<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.159939">
<title confidence="0.995148">
Generative CCG Parsing with OOV Prediction
</title>
<author confidence="0.995503">
Huijia Wu
</author>
<affiliation confidence="0.994109">
Institute of Automation, Chinese Academy of Science
</affiliation>
<email confidence="0.974919">
huijia.wul@ia.ac.cn
</email>
<sectionHeader confidence="0.99716" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999179">
This paper presents our system for the
CIPS-SIGHAN-2014 bakeoff task of Sim-
plified Chinese Parsing (Task 3). The sys-
tem adopts a generative model with OOV
prediction model. The former has a PCFG
form while the latter uses a three-layer hi-
erarchical Bayesian model. The final per-
formance on the test corpus is reported to-
gether with the performance of the OOV
model.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999817857142857">
Statistical parsing is the process of discovering the
syntactic relations in a sentence, according to the
rules of a formal grammar. There exist a body
of parsers based on various linguistic formalisms,
such as LFG, HPSG, TAG and CCG. (Riezler et
al., 2002; Sarkar and Joshi, 2003; Cahill et al.,
2004; Miyao and Tsujii, 2005; Clark and Curran,
2007). The parsing techniques also vary from the
generative model to the discriminative model. The
former uses a joint probability distribution includ-
ing both the observations and the targets, while
the latter only models the conditional probability
measure to describe the randomness of the targets
based on the observations (Hockenmaier, 2003a,
2003b; Clark and Curran, 2007).
The out-of-vocabulary (OOV) problem is far-
from solved in statistical parsing, especially in
CCG. There are lots of categories such that a com-
puter would be less likely to remember a word.
Clark proposed a supertagger to assignment sev-
eral possible categories to a word which provides
highly accurate and efficient results (Clark, 2002).
In this task we propose a three layer hierarchi-
cal Bayesian model to predict the OOV, using the
POS tag as the hidden layer. Further, we estimate
a OOV’s category through integrating all possible
POS tags, which means that we need to find rela-
tions between OOV and POS. To achieve this goal,
</bodyText>
<table confidence="0.920301666666667">
Leaf nodes Unary trees Head left: Head right:
(S\NP)/NP S/(S\NP) S\NP S
喜欢 NP (S\NP)/NP NP NP S\NP
</table>
<tableCaption confidence="0.999843">
Table 1: The four different kinds of expansion
</tableCaption>
<bodyText confidence="0.999809166666667">
we create a mapping between a CCG tree and a
TCT tree, which is another kind of syntactic tree
according the Tsinghua Chinese Treebank (TCT).
The final report has two parts, one is the eval-
uation performance based on the test corpus, the
other is the performance on OOV prediction.
</bodyText>
<sectionHeader confidence="0.942846" genericHeader="method">
2 Our System
</sectionHeader>
<bodyText confidence="0.9999574">
Our system combines a generative model for pars-
ing with a OOV prediction model. The former
follows heavily from (Hockenmaier, 2003a) with
slightly modification, which includes the defini-
tion of head nodes, using Dirichlet prior as the
smoothing technique. The latter is a three-layer hi-
erarchical Bayesian model: the input and the out-
put layer corresponds to a OOV and its category,
respectively, composed with a POS tag as the hid-
den layer.
</bodyText>
<subsectionHeader confidence="0.547132">
2.1 Generative Model for Parsing
</subsectionHeader>
<bodyText confidence="0.999359769230769">
In this evaluation task, we adopt a generative
model as the CCG parsing algorithm. One advan-
tage of the generative model is it needs less hu-
man intervention than the discriminative model,
which means that, if we have enough data, to-
gether with the proper generative model, the algo-
rithm can learn from the data, of the data and for
the data with a competitive performance, while the
discriminative model needs a lot of manual fea-
ture templates, which sounds like cheating since
the features are designed by human, rather than
the computer itself.
Our generative model bases on (Hockenmaier
</bodyText>
<page confidence="0.988702">
153
</page>
<note confidence="0.6536405">
Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 153–156,
Wuhan, China, 20-21 October 2014
</note>
<bodyText confidence="0.99791247368421">
, 2003a), which defines a generative model over
CCG derivation trees. This model acts like a
PCFG form, which does not incorporate the no-
tion of combinatory trees. Instead, it is a gener-
ative model over sub-trees. By contrast to Hock-
enmaier, we use a different approach of defining
head node, which is a functor categories (cate-
gories that accept arguments). Since from a mod-
elling point of view, isolating a head node from
a non-head one just make a generative process
more hierarchical, there is no statistically signif-
icant differences between a head node and a non-
head node.
The derivations of a CCG tree can be repre-
sented by top-down expansions. As mentioned in
(Hockenmaier, 2003a), there are four kinds of leaf
nodes in a CCG tree, which corresponds to four
kinds of expansion (Table 1). Follow this conven-
tion, we have the following generating process:
</bodyText>
<listItem confidence="0.999308727272727">
1. Expansion probabiltiy: Start from a root,
choose a type of expansion N by P(exp|C)
with exp E {left, right, unary, leaf} and C E
C.
2. Lexical probability: If it meets a leaf
node, a word w is generated with probability
P(w|C, exp = leaf), stop.
3. Head probability: Otherwise, choose a head
node with probability P(H|C, exp).
4. Non-head probability: Finally, generate a
non-head node w.p. P(D|C, exp, H).
</listItem>
<subsectionHeader confidence="0.983202">
2.2 Inference and Learning
</subsectionHeader>
<bodyText confidence="0.999632173913043">
The parameter estimation step is similar to a
PCFG parser based on the maximum likelihood
estimation (MLE), but the estimator may become
sparsity due to the huge number of parameters.
This may cause the problem of overestimation. To
avoid this, we can use a regularization term or a
prior as the smoothing technique.
In this task, we prefer a Dirichlet distribution
as the prior to other smoothing methods. Since
it is easy to implement and forms a conjugate
prior to a multinomial distribution. We put a
Dirichlet prior Dir(α) on a lexical distribution
P(w|C, exp = leaf). In the experiment we set the
α = (1, 1, ... ,1) as a uniform distribution.
The learning or decoding algorithm is the well-
known CKY algorithm. But efficiency is still a
problem, since the number of categories is large,
for a long sentence more computing steps will be
needed to compose two adjacent cells in a chart
than other lexicon-based parsers. Fortunately,
Clark and Curran proposed an log-likelihood CCG
parser which is efficient enough to large-scale
NLP tasks (Clark and Curran, 2007).
</bodyText>
<subsectionHeader confidence="0.99624">
2.3 Estimating the OOV
</subsectionHeader>
<bodyText confidence="0.999961185185185">
The supertagger proposed by Clark uses a maxi-
mum entropy model to predict a word’s categories,
based on the idea that given a set of manual fea-
tures, we need to find a category distribution re-
stricted on the set acts an uniform predictor to un-
known words. This maximum entropy principle
may not apply to OOV estimating, for the reasons
that the OOV is rare, statistically insignificant and
unable to catch by a statistical model.
Manual rules can get a more accurate prediction
than the statistical model, but these rules are also
non-flexible, time-confusing and heavy-lifting. To
overcome this problem, we propose a mapping be-
tween a CCG tree and a TCT tree with the same
terminal nodes.
To make this mapping possible we first need to
verify the existence, uniqueness and reversibility
of such a mapping. Luckily such a mapping is ex-
ist since the CCG tree is generated by a TCT tree.
To make it simpler we omit the condition of the
uniqueness and reversibility. Now the problem is:
Can we find a such a mapping to help us to predict
the OOV?
Obviously, the mapping is the relation between
the syntactic symbols (POS) and the semantic
symbols (category). If we can find the estimator
of P(cat|pos) our problem is easily solved by:
</bodyText>
<equation confidence="0.995496222222222">
P(C|O)P(O)
P (O|C) =
(1)
EO∈{OOV }
P(C|O)P(O)
P(O)ES∈{POS} P(C|5)P(5|O)
∈ {OOV } {P( O ) E S∈{POS} P(C|5)P(5|O) }
(2)
EO
</equation>
<bodyText confidence="0.9957736">
In the above equations, O stands for the OOV,
which is a random variable assigned values from
all possible OOV. C indicates the category and 5
stands for POS tag.
How to create such a mapping matrix? We start
from the root node, using adepth-first search algo-
rithm to find the correspondence between nodes in
each tree. Notice that the CCG tree is binary
, while
the TCT tree is not. To find the correct map, we
</bodyText>
<page confidence="0.999015">
154
</page>
<bodyText confidence="0.999968727272727">
first need to binarize the TCT tree. But the set of
all possible binary trees may become huge when
there are many children of a node. Fortunately we
just need to expand all binary nodes through one
direction.
This model acts like the maximum entropy
model, since they all use the context features, but
the difference is the former focuses on a more
restricted conditions based on the tree structure,
while the features in the latter is at the sentence
level.
</bodyText>
<sectionHeader confidence="0.998313" genericHeader="method">
3 Experiment
</sectionHeader>
<subsectionHeader confidence="0.982524">
3.1 Datesets
</subsectionHeader>
<bodyText confidence="0.999960727272727">
The data uses in the system composed of two parts,
one is for the parser, the other is for the OOV
prediction model. The data used by the former
comes from the sponsor (CCG bank) with 17558
parsed sentences, 984 categories, while the latter
uses data from both the CCG bank and the TCT
bank with 9034 sentences. To find the mapping
tree with the same leaf nodes, we extract such tree
pairs from the two data sets. Finally we get a data
set for the OOV prediction model with 5360 tree
pairs.
</bodyText>
<subsectionHeader confidence="0.999089">
3.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.998247454545455">
There are two kinds of metrics to be evaluated,
one is the syntactic category evaluation metrics,
the other is the parsing tree evaluation metrics. We
report both of these metrics, together with the per-
formance of the OOV prediction model.
Table 2 and 3 gives the performance of the
parser on the test set, based on the syntactic cat-
egory evaluation metrics and the parsing tree eval-
uation metrics, respectively.
The notations in Table 3 are explained as fol-
lows (Qiang Zhou, 2014):
</bodyText>
<listItem confidence="0.999626090909091">
• LDP CE stands for the lexical dependency
pairs (LDPs) with complex event relations in
the sentence levels.
• LDP CC stands for the LDPs with concept
compound relations in the chunk levels.
• LDP PA stands for the LDPs with predicate-
argument relations in the clause levels, in-
cluding head-complement and adjunct-head
relations.
• LDP MO stands for the LDPs with other
non-PA relations in the chunk and clause
</listItem>
<table confidence="0.9997676">
Category Precision Recall F1
NP 79.71 89.07 84.13
NP/NP 63.31 67.63 65.4
Others 70.57 67.47 68.99
All 71.80 71.81 71.81
</table>
<tableCaption confidence="0.966893">
Table 2: The performance based on the syntactic
category evaluation metrics
</tableCaption>
<table confidence="0.999974428571428">
Relation Precision Recall F1
LDP CE 12.98 11.92 12.43
LDP CC 26.80 36.87 31.04
LDP PA 40.69 40.47 40.58
LDP MO 45.99 45.33 45.66
Others 45.81 43.62 44.69
All 42.31 42.27 42.29
</table>
<tableCaption confidence="0.9541405">
Table 3: The performance based on the parsing
tree evaluation metrics
</tableCaption>
<bodyText confidence="0.994197888888889">
levels, including modifier-head and operator-
complement relations.
Table 4 shows the performance of the OOV es-
timation model, OOV-POS is the baseline model,
which means that a node’s category is taken ex-
actly on the corresponding POS tag, +head means
such a category is not just on its POS tag, but also
with its parent’s node’s POS tag. +sister has the
similar meaning.
</bodyText>
<table confidence="0.9967132">
Model Precision Recall F1
OOV-POS 60.02 72.10 65.46
+parent 83.15 88.12 85.56
+sister 76.2 82.41 79.18
+parent, sister 86.67 90.2 88.39
</table>
<tableCaption confidence="0.99955">
Table 4: The results of OOV prediction model
</tableCaption>
<sectionHeader confidence="0.989303" genericHeader="method">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999923666666667">
This report has shown a generative CCG parser
with a OOV prediction model. One contribution of
this report is the development of a Bayesian model
to predict the OOV with high accuracy. The tech-
niques we use is easy to extend to a more compli-
cated system.
</bodyText>
<sectionHeader confidence="0.888383" genericHeader="conclusions">
Acknowledge
</sectionHeader>
<bodyText confidence="0.997831">
We would like to thank Qiang Zhou for helpful
discussion.
</bodyText>
<page confidence="0.999038">
155
</page>
<sectionHeader confidence="0.985654" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999718357894737">
Clark, Stephen. 2002. A maximum-entropy-inspired
parser . In Proceedings of the 1st Meeting of the
NAACL, pages 132–139, Seattle, WA.
Clark, Stephen and James R. Curran. 2003. Log-linear
models for wide-coverage CCG parsing.. In Pro-
ceedings of the EMNLP Conference, pages 97–104,
Sapporo, Japan.
Clark, Stephen and James R. Curran. 2004b. Pars-
ing the WSJ using CCG and log-linear models. In
Proceedings of the 42nd Meeting of the ACL, pages
104–111, Barcelona, Spain.
Clark, Stephen, Julia Hockenmaier, and Mark Steed-
man. 2002. Building deep dependency structures
with a wide-coverage CCG parser. In Proceedings
of the 40th Meeting of the ACL, pages 327–334,
Philadelphia, PA.
Geman, Stuart and Mark Johnson. 2002. Dynamic pro-
gramming for parsing and estimation of stochastic
unification-based grammars. In Proceedings of the
40th Meeting of the ACL, pages 279–286, Philadel-
phia, PA.
Collins, Michael. 1996. A new statistical parser based
on bigram lexical dependencies. In Proceedings of
the 34th Meeting of the ACL, pages 184–191, Santa
Cruz, CA.
Collins, Michael. 1999. Head-driven statistical mod-
els for natural language parsing. PhD thesis, Uni-
versity of Pennsylvania.
Collins, Michael. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589–637.
Hockenmaier, Julia and Mark Steedman. 2002a
Acquiring compact lexicalized grammars from a
cleaner treebank. In Proceedings of the Third LREC
Conference, pages 1974–1981, Las Palmas, Spain.
Hockenmaier, Julia and Mark Steedman. 2002b Gen-
erative models for statistical parsing with Combi-
natory Categorial Grammar. In Proceedings of the
40th Meeting of the ACL, pages 335–342, Philadel-
phia, PA.
Hockenmaier, Julia. 2003a Data and Models for Sta-
tistical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Hockenmaier, Julia. 2003b Parsing with generative
models of predicate-argument structure. In Pro-
ceedings of the 41st Meeting of the ACL, pages
359–366, Sapporo, Japan.
Lari, K. and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4(1):35–56.
Mark Johnson, Thomas L. Griffiths and Sharon Gold-
water. 2007. Inference for PCFGs via Markov
Chain Monte Carlo. Human Language Technolo-
gies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics; Proceedings of the Main Confer- ence,
pages 139-146.
Percy Liang, Slav Petrov, Michael I. Jordan, Dan Klein.
2007. The infinite PCFG using hierarchical Dirich-
let processes. Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning(EMNLP/CoNLL).
Qiang Zhou. 2004. Chinese Treebank Annotation
Scheme. Journal of Chinese Information, 18(4), p1-
8.
Qiang Zhou. 2011. Automatically transform the TCT
data into a CCG bank: designation specification Ver
3.0. Technical Report CSLT-20110512, Center for
speech and language technology, Research Institute
of Information Technology, Tsinghua University.
Ratnaparkhi, Adwait, Salim Roukos, and Todd Ward.
1994. A maximum entropy model for parsing. In
Proceedings of the International Conference on Spo-
ken Language Processing, pages 803–806, Yoko-
hama, Japan.
Sarkar, A. and Joshi, A. 2003. Tree-adjoining gram-
mars and its application to statistical parsing. In
Bod, R., Scha, R., and Sima’an, K., editors, Data-
oriented parsing. CSLI.
Steedman, Mark. 1996. Surface Structure and Inter-
pretation. MIT Press, Cambridge, MA.
Steedman, Mark. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA.
Steedman, Mark, Steven Baker, Stephen Clark,
Jeremiah Crim, Julia Hockenmaier, Rebecca Hwa,
Miles Osborne, Paul Ruhlen, and Anoop Sarkar.
2002. Semi-supervised training for statistical pars-
ing: Final report. Technical report, Center for Lan-
guage and Speech Processing, Johns Hopkins Uni-
versity, Baltimore, MD.
Yonatan Bisk and Julia Hockenmaier. 2013. An
HDP Model for Inducing Com- binatory Catego-
rial Grammars. Transactions of the Association for
Compu- tational Linguistics Vol 1.
</reference>
<page confidence="0.998784">
156
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.723680">
<title confidence="0.999862">Generative CCG Parsing with OOV Prediction</title>
<author confidence="0.997511">Huijia Wu</author>
<affiliation confidence="0.999087">Institute of Automation, Chinese Academy of Science</affiliation>
<email confidence="0.769209">huijia.wul@ia.ac.cn</email>
<abstract confidence="0.994629272727273">This paper presents our system for the CIPS-SIGHAN-2014 bakeoff task of Simplified Chinese Parsing (Task 3). The system adopts a generative model with OOV prediction model. The former has a PCFG form while the latter uses a three-layer hierarchical Bayesian model. The final performance on the test corpus is reported together with the performance of the OOV model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>A maximum-entropy-inspired parser .</title>
<date>2002</date>
<booktitle>In Proceedings of the 1st Meeting of the NAACL,</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="1574" citStr="Clark, 2002" startWordPosition="248" endWordPosition="249">mer uses a joint probability distribution including both the observations and the targets, while the latter only models the conditional probability measure to describe the randomness of the targets based on the observations (Hockenmaier, 2003a, 2003b; Clark and Curran, 2007). The out-of-vocabulary (OOV) problem is farfrom solved in statistical parsing, especially in CCG. There are lots of categories such that a computer would be less likely to remember a word. Clark proposed a supertagger to assignment several possible categories to a word which provides highly accurate and efficient results (Clark, 2002). In this task we propose a three layer hierarchical Bayesian model to predict the OOV, using the POS tag as the hidden layer. Further, we estimate a OOV’s category through integrating all possible POS tags, which means that we need to find relations between OOV and POS. To achieve this goal, Leaf nodes Unary trees Head left: Head right: (S\NP)/NP S/(S\NP) S\NP S 喜欢 NP (S\NP)/NP NP NP S\NP Table 1: The four different kinds of expansion we create a mapping between a CCG tree and a TCT tree, which is another kind of syntactic tree according the Tsinghua Chinese Treebank (TCT). The final report h</context>
</contexts>
<marker>Clark, 2002</marker>
<rawString>Clark, Stephen. 2002. A maximum-entropy-inspired parser . In Proceedings of the 1st Meeting of the NAACL, pages 132–139, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Log-linear models for wide-coverage CCG parsing..</title>
<date>2003</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>97--104</pages>
<location>Sapporo, Japan.</location>
<marker>Clark, Curran, 2003</marker>
<rawString>Clark, Stephen and James R. Curran. 2003. Log-linear models for wide-coverage CCG parsing.. In Proceedings of the EMNLP Conference, pages 97–104, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the ACL,</booktitle>
<pages>104--111</pages>
<location>Barcelona,</location>
<marker>Clark, Curran, 2004</marker>
<rawString>Clark, Stephen and James R. Curran. 2004b. Parsing the WSJ using CCG and log-linear models. In Proceedings of the 42nd Meeting of the ACL, pages 104–111, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Building deep dependency structures with a wide-coverage CCG parser.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<pages>327--334</pages>
<location>Philadelphia, PA.</location>
<marker>Clark, Hockenmaier, Steedman, 2002</marker>
<rawString>Clark, Stephen, Julia Hockenmaier, and Mark Steedman. 2002. Building deep dependency structures with a wide-coverage CCG parser. In Proceedings of the 40th Meeting of the ACL, pages 327–334, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Mark Johnson</author>
</authors>
<title>Dynamic programming for parsing and estimation of stochastic unification-based grammars.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<pages>279--286</pages>
<location>Philadelphia, PA.</location>
<marker>Geman, Johnson, 2002</marker>
<rawString>Geman, Stuart and Mark Johnson. 2002. Dynamic programming for parsing and estimation of stochastic unification-based grammars. In Proceedings of the 40th Meeting of the ACL, pages 279–286, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Meeting of the ACL,</booktitle>
<pages>184--191</pages>
<location>Santa Cruz, CA.</location>
<marker>Collins, 1996</marker>
<rawString>Collins, Michael. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th Meeting of the ACL, pages 184–191, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Collins, 1999</marker>
<rawString>Collins, Michael. 1999. Head-driven statistical models for natural language parsing. PhD thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<marker>Collins, 2003</marker>
<rawString>Collins, Michael. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Acquiring compact lexicalized grammars from a cleaner treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third LREC Conference,</booktitle>
<pages>1974--1981</pages>
<location>Las Palmas,</location>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Hockenmaier, Julia and Mark Steedman. 2002a Acquiring compact lexicalized grammars from a cleaner treebank. In Proceedings of the Third LREC Conference, pages 1974–1981, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative models for statistical parsing with Combinatory Categorial Grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<pages>335--342</pages>
<location>Philadelphia, PA.</location>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Hockenmaier, Julia and Mark Steedman. 2002b Generative models for statistical parsing with Combinatory Categorial Grammar. In Proceedings of the 40th Meeting of the ACL, pages 335–342, Philadelphia, PA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>2003a Data and Models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<marker>Hockenmaier, </marker>
<rawString>Hockenmaier, Julia. 2003a Data and Models for Statistical Parsing with Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>2003b Parsing with generative models of predicate-argument structure.</title>
<booktitle>In Proceedings of the 41st Meeting of the ACL,</booktitle>
<pages>359--366</pages>
<location>Sapporo, Japan.</location>
<marker>Hockenmaier, </marker>
<rawString>Hockenmaier, Julia. 2003b Parsing with generative models of predicate-argument structure. In Proceedings of the 41st Meeting of the ACL, pages 359–366, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the insideoutside algorithm.</title>
<date>1990</date>
<journal>Computer Speech and Language,</journal>
<volume>4</volume>
<issue>1</issue>
<marker>Lari, Young, 1990</marker>
<rawString>Lari, K. and S. J. Young. 1990. The estimation of stochastic context-free grammars using the insideoutside algorithm. Computer Speech and Language, 4(1):35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Inference for PCFGs via Markov Chain Monte Carlo. Human Language Technologies</title>
<date>2007</date>
<booktitle>The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Confer- ence,</booktitle>
<pages>139--146</pages>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths and Sharon Goldwater. 2007. Inference for PCFGs via Markov Chain Monte Carlo. Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Confer- ence, pages 139-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Slav Petrov</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>The infinite PCFG using hierarchical Dirichlet processes.</title>
<date>2007</date>
<booktitle>Empirical Methods in Natural Language Processing and Computational Natural Language Learning(EMNLP/CoNLL).</booktitle>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>Percy Liang, Slav Petrov, Michael I. Jordan, Dan Klein. 2007. The infinite PCFG using hierarchical Dirichlet processes. Empirical Methods in Natural Language Processing and Computational Natural Language Learning(EMNLP/CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiang Zhou</author>
</authors>
<title>Chinese Treebank Annotation Scheme.</title>
<date>2004</date>
<journal>Journal of Chinese Information,</journal>
<volume>18</volume>
<issue>4</issue>
<pages>1--8</pages>
<marker>Zhou, 2004</marker>
<rawString>Qiang Zhou. 2004. Chinese Treebank Annotation Scheme. Journal of Chinese Information, 18(4), p1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiang Zhou</author>
</authors>
<title>Automatically transform the TCT data into a CCG bank: designation specification Ver 3.0.</title>
<date>2011</date>
<tech>Technical Report CSLT-20110512,</tech>
<institution>Center</institution>
<marker>Zhou, 2011</marker>
<rawString>Qiang Zhou. 2011. Automatically transform the TCT data into a CCG bank: designation specification Ver 3.0. Technical Report CSLT-20110512, Center for speech and language technology, Research Institute of Information Technology, Tsinghua University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
</authors>
<title>A maximum entropy model for parsing.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>803--806</pages>
<location>Yokohama, Japan.</location>
<marker>Ratnaparkhi, Roukos, Ward, 1994</marker>
<rawString>Ratnaparkhi, Adwait, Salim Roukos, and Todd Ward. 1994. A maximum entropy model for parsing. In Proceedings of the International Conference on Spoken Language Processing, pages 803–806, Yokohama, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sarkar</author>
<author>A Joshi</author>
</authors>
<title>Tree-adjoining grammars and its application to statistical parsing.</title>
<date>2003</date>
<editor>In Bod, R., Scha, R., and Sima’an, K., editors, Dataoriented parsing.</editor>
<publisher>CSLI.</publisher>
<contexts>
<context position="795" citStr="Sarkar and Joshi, 2003" startWordPosition="125" endWordPosition="128">CIPS-SIGHAN-2014 bakeoff task of Simplified Chinese Parsing (Task 3). The system adopts a generative model with OOV prediction model. The former has a PCFG form while the latter uses a three-layer hierarchical Bayesian model. The final performance on the test corpus is reported together with the performance of the OOV model. 1 Introduction Statistical parsing is the process of discovering the syntactic relations in a sentence, according to the rules of a formal grammar. There exist a body of parsers based on various linguistic formalisms, such as LFG, HPSG, TAG and CCG. (Riezler et al., 2002; Sarkar and Joshi, 2003; Cahill et al., 2004; Miyao and Tsujii, 2005; Clark and Curran, 2007). The parsing techniques also vary from the generative model to the discriminative model. The former uses a joint probability distribution including both the observations and the targets, while the latter only models the conditional probability measure to describe the randomness of the targets based on the observations (Hockenmaier, 2003a, 2003b; Clark and Curran, 2007). The out-of-vocabulary (OOV) problem is farfrom solved in statistical parsing, especially in CCG. There are lots of categories such that a computer would be </context>
</contexts>
<marker>Sarkar, Joshi, 2003</marker>
<rawString>Sarkar, A. and Joshi, A. 2003. Tree-adjoining grammars and its application to statistical parsing. In Bod, R., Scha, R., and Sima’an, K., editors, Dataoriented parsing. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Surface Structure and Interpretation.</title>
<date>1996</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Steedman, 1996</marker>
<rawString>Steedman, Mark. 1996. Surface Structure and Interpretation. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Steedman, 2000</marker>
<rawString>Steedman, Mark. 2000. The Syntactic Process. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Steven Baker</author>
<author>Stephen Clark</author>
<author>Jeremiah Crim</author>
<author>Julia Hockenmaier</author>
<author>Rebecca Hwa</author>
<author>Miles Osborne</author>
<author>Paul Ruhlen</author>
<author>Anoop Sarkar</author>
</authors>
<title>Semi-supervised training for statistical parsing: Final report.</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>Center for Language and Speech Processing, Johns Hopkins University,</institution>
<location>Baltimore, MD.</location>
<marker>Steedman, Baker, Clark, Crim, Hockenmaier, Hwa, Osborne, Ruhlen, Sarkar, 2002</marker>
<rawString>Steedman, Mark, Steven Baker, Stephen Clark, Jeremiah Crim, Julia Hockenmaier, Rebecca Hwa, Miles Osborne, Paul Ruhlen, and Anoop Sarkar. 2002. Semi-supervised training for statistical parsing: Final report. Technical report, Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonatan Bisk</author>
<author>Julia Hockenmaier</author>
</authors>
<title>An HDP Model for Inducing Com- binatory Categorial Grammars.</title>
<date>2013</date>
<journal>Transactions of the Association for Compu- tational Linguistics</journal>
<volume>1</volume>
<marker>Bisk, Hockenmaier, 2013</marker>
<rawString>Yonatan Bisk and Julia Hockenmaier. 2013. An HDP Model for Inducing Com- binatory Categorial Grammars. Transactions of the Association for Compu- tational Linguistics Vol 1.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
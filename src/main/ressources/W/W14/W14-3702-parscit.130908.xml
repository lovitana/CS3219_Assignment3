<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000051">
<title confidence="0.997591">
Exploiting Timegraphs in Temporal Relation Classification
</title>
<author confidence="0.995665">
Natsuda Laokulrat†, Makoto Miwa‡, and Yoshimasa Tsuruoka†
</author>
<affiliation confidence="0.997911">
†The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
</affiliation>
<email confidence="0.971129">
{natsuda,tsuruoka}@logos.t.u-tokyo.ac.jp
</email>
<note confidence="0.474763">
‡Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan
</note>
<email confidence="0.846684">
miwa@toyota-ti.ac.jp
</email>
<sectionHeader confidence="0.989209" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954739130435">
Most of the recent work on machine
learning-based temporal relation classifi-
cation has been done by considering only
a given pair of temporal entities (events or
temporal expressions) at a time. Entities
that have temporal connections to the pair
of temporal entities under inspection are
not considered even though they provide
valuable clues to the prediction. In this
paper, we present a new approach for ex-
ploiting knowledge obtained from nearby
entities by making use of timegraphs and
applying the stacked learning method to
the temporal relation classification task.
By performing 10-fold cross validation
on the Timebank corpus, we achieved an
F1 score of 59.61% based on the graph-
based evaluation, which is 0.16 percent-
age points higher than that of the local
approach. Our system outperformed the
state-of-the-art system that utilizes global
information and achieved about 1.4 per-
centage points higher accuracy.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998936727272728">
Temporal relationships between entities, namely
temporal expressions and events, are regarded as
important information for deep understanding of
documents. Being able to predict temporal re-
lations between events and temporal expressions
within a piece of text can support various NLP ap-
plications such as textual entailment (Bos et al.,
2005), multi-document summarization (Bollegala
et al., 2010), and question answering (Ravichan-
dran and Hovy, 2002).
Temporal relation classification, which is one of
the subtasks TempEval-3 (UzZaman et al., 2013),
aims to classify temporal relationships between
pairs of temporal entities into one of the 14 re-
lation types according to the TimeML specifica-
tion (Pustejovsky et al., 2005), e.g., BEFORE, AF-
TER, DURING, and BEGINS.
The Timebank corpus introduced by Puste-
jovsky et al. (2003) has enabled the machine
learning-based classification of temporal relation-
ship. By learning from the annotated relation
types in the documents, it is possible to predict
the temporal relation of a given pair of temporal
entities (Mani et al., 2006).
However, most of the existing machine
learning-based systems use local information
alone, i.e., they consider only a given pair of tem-
poral entities at a time. Entities that have tem-
poral connections to the entities in the given pair
are not considered at all even though they provide
valuable clues to the prediction. Hence, the lo-
cal approach often produces contradictions. For
instance, the system may predict that A happens
before B, that B happens before C, and that A hap-
pens after C, which are mutually contradictory.
In order to tackle the contradiction problem,
global approaches have been proposed by Cham-
bers and Jurafsky (2008) and Yoshikawa et al.
(2009). Chamber and Jurafsky proposed a global
model based on Integer Linear Programming that
combines the output of local classifiers and max-
imizes the global confidence scores. While they
focused only on the temporal relations between
events, Yoshikawa et al. proposed a Markov Logic
model to jointly predict the temporal relations be-
tween events and time expressions.
In this paper, we propose an approach that
utilizes timegraphs (Miller and Schubert, 1999),
which represent temporal connectivity of all tem-
poral entities in each document, for the relation
classification. Our method differs from the pre-
vious work in that their methods used transition
rules to enforce consistency within each triplet of
relations, but our method can also work with a set
consisting of more than three relations. Moreover,
</bodyText>
<page confidence="0.972982">
6
</page>
<subsubsectionHeader confidence="0.432232">
Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 6–14,
</subsubsectionHeader>
<page confidence="0.186427">
October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</page>
<figureCaption confidence="0.999304">
Figure 1: An example from the Timebank corpus
</figureCaption>
<bodyText confidence="0.999604615384615">
in our work, the full set of temporal relations spec-
ified in TimeML are used, rather than the reduced
set used in the previous work.
We evaluate our method on the TempEval-3’s
Task C-relation-only data, which provides a sys-
tem with all the appropriate temporal links and
only needs the system to classify the relation
types. The result shows that by exploiting the
timegraph features in the stacked learning ap-
proach, the classification performance improves
significantly. By performing 10-fold cross valida-
tion on the Timebank corpus, we can achieve an F1
score of 59.61% based on the graph-based evalu-
ation, which is 0.16 percentage points (pp) higher
than that of the local approach. We compared the
results of our system to those of Yoshikawa et al.
(2009) and achieved about 1.4 pp higher accuracy.
The remainder of the paper is organized as fol-
lows. Section 2 explains the temporal relation
classification task and the pairwise classifier. Sec-
tion 3 and Section 4 describe our proposed time-
graph features and the application to the stacked
learning approach. Section 5 shows the experi-
ment setup and presents the results. Finally, we
discuss the results in 6 and conclude with direc-
tions for future work in Section 7.
</bodyText>
<sectionHeader confidence="0.976433" genericHeader="introduction">
2 Temporal Relation Classification
</sectionHeader>
<bodyText confidence="0.999774538461538">
According to TempEval-3, a temporal annotation
task consists of several subtasks, including tempo-
ral expression extraction (Task A), event extrac-
tion (Task B), and temporal link identification and
relation classification (Task C). Our work, as with
the previous work mentioned in Section 1, only
focuses on the relation classification task (Task C-
relation only). The system does not extract events
and temporal expressions automatically.
A pair of temporal entities, including events and
temporal expressions, that is annotated as a tem-
poral relation is called a TLINK. Temporal rela-
tion classification is a task to classify TLINKs into
</bodyText>
<listItem confidence="0.961616875">
temporal relation types.
Following TempEval-3, all possible TLINKs
are between:
• Event and Document Creation Time (DCT)
• Events in the same sentence
• Event and temporal expression in the same
sentence
• Events in consecutive sentences
</listItem>
<subsectionHeader confidence="0.973069">
2.1 The Timebank corpus
</subsectionHeader>
<bodyText confidence="0.933828566666667">
The Timebank corpus is a human-annotated cor-
pus commonly used in training and evaluating a
temporal relation classifier. It is annotated follow-
ing the TimeML specification to indicate events,
temporal expressions, and temporal relations. It
also provides five attributes, namely, class, tense,
aspect, modality, and polarity, associated with
each event (EVENT), and four attributes, namely,
type, value, functionInDocument, and temporal-
Function, associated with each temporal expres-
sion (TIMEX3). An example of the annotated event
and temporal expression is shown in Figure 1.
The sentence is brought from wsj 0292.tml in the
Timebank corpus.
There is no modal word in the sentence, so the
attribute modality does not appear.
We use the complete set of the TimeML rela-
tions, which has 14 types of temporal relations in-
cluding BEFORE, AFTER, IMMEDIATELY BEFORE, IM-
MEDIATELY AFTER, INCLUDES, IS INCLUDED, DUR-
ING, DURING INVERSE, SIMULTANEOUS, IDENTITY,
BEGINS, BEGUN BY, END, and ENDED BY. However,
in TempEval-3, SIMULTANEOUS and IDENTITY are
regarded as the same relation type, so we change
all IDENTITY relations into SIMULTANEOUS.
Given the example mentioned above, the tem-
poral relation is annotated as shown in the last line
of Figure 1. From the annotated relation, the event
rose (e30) happens DURING the temporal expres-
sion the first nine months (t88).
</bodyText>
<page confidence="0.999283">
7
</page>
<table confidence="0.999800911764706">
Feature E-E E-T Description
Event attributes
Class X X All attributes associated with events. The ex-
Tense X X planation of each attribute can be found in
Aspect X X (Pustejovsky et al., 2005).
Modality X X
Polarity X X
Timex attributes
Type X All attributes associated with temporal ex-
Value X pressions. The explanation of each attribute
onInDocument X be found in (Pustejovsky et al., 2005).
Functican
TemporalFunction
X
Morphosyntactic information
Words X X Words, POS, lemmas within a window be-
Part of speech tags X X fore/after event words extracted using Stan-
Lemmas X X ford coreNLP (Stanford NLP Group, 2012)
Lexical semantic information
Synonyms of event word tokens X X WordNet lexical database (Fellbaum, 1998)
Synonyms of temporal expressions X
Event-Event information
Class match X X Details are described in (Chambers et al.,
Tense match X 2007)
Aspect match X True if both temporal entities are in the same
Class bigram X sentence
Tense bigram X
Aspect bigram X
Same sentence X
Deep syntactic information
Phrase structure X X Deep syntactic information extracted from
Predicate-argument structure X X Enju Parser (Miyao and Tsujii, 2008). The
details are described in (Laokulrat et al.,
2013)
</table>
<tableCaption confidence="0.966217">
Table 1: Local features
</tableCaption>
<table confidence="0.998937666666667">
Feature E-E E-T Description
Adjacent nodes and links X X The details are described in Subsection 3.2
Other paths X X
Generalized paths X X
(E,V,E) tuples X X
(V,E,V) tuples X X
</table>
<tableCaption confidence="0.997415">
Table 2: Timegraph features
</tableCaption>
<page confidence="0.993696">
8
</page>
<figureCaption confidence="0.999999333333333">
Figure 2: path length &lt; 2 Figure 5: Local pairwise classification. Each
TLINK is classified separately.
Figure 3: path length &lt; 3
</figureCaption>
<sectionHeader confidence="0.981903" genericHeader="method">
3 Proposed method
</sectionHeader>
<bodyText confidence="0.998367807692308">
Rather than using only local information on
two entities in a TLINK, our goal is to exploit
more global information which can be extracted
from a document’s timegraph. Our motivation
is that temporal relations of nearby TLINKs in
a timegraph provide very useful information for
predicting the relation type of a given TLINK. For
instance, consider the following sentence and the
temporal connectivity shown in Figure 2.
About 500 people attended (e1) a Sunday
night memorial for the Buffalo-area physician
who performed abortions, one year (t1) after he
was killed (e2) by a sniper’s bullet.
It can be seen that the relation between e1 and
t1 and the relation between t1 and e2 are useful
for predicting the relation between e1 and e2.
Another more-complicated example is shown
below with temporal connectivity in Figure 3.
“The Congress of the United States is af-
fording(e1) Elian Gonzalez what INS and this
administration has not, which is his legal right
and his right to due process,” said(e2) Jorge
Mas Santos, chairman of the Cuban American
National Foundation. “This gives(e3) him the
protection that he will not be repatriated(e4) to
Cuba between now and Feb. 10.”
</bodyText>
<figureCaption confidence="0.9801195">
Figure 6: Timegraph constructed from a docu-
ment’s TLINKs
</figureCaption>
<bodyText confidence="0.6963134">
Again, the relation between e4 and e3
can be inferred from the nearby relations,
i.e., (1) e4 AFTER e2 and e2 AFTER e1
imply e4 AFTER e1, (2) e4 AFTER e1 and
e1 SIMULTANEOUS e3 imply e4 AFTER e3.
</bodyText>
<subsectionHeader confidence="0.99897">
3.1 Overview of our framework
</subsectionHeader>
<bodyText confidence="0.999897">
Our framework is based on the stacked learn-
ing method (Wolpert, 1992), which employs two
stages of classification as illustrated in Figure 4.
</bodyText>
<subsectionHeader confidence="0.81723">
3.1.1 Local pairwise model
</subsectionHeader>
<bodyText confidence="0.999571846153846">
In a local pairwise model, temporal relation clas-
sification is done by considering only a given pair
of temporal entities at a time as illustrated in Fig-
ure 5. We use a supervised machine learning ap-
proach and employ the basic feature set that can
be easily extracted from the document’s text and
the set of features proposed in our previous work
(Laokulrat et al., 2013), which utilizes deep syn-
tactic information, as baselines. The local features
at different linguistic levels are listed in Table 1.
Two classifiers are used: one for Event-Event
TLINKs (E-E), and the other for Event-Time
TLINKs (E-T).
</bodyText>
<subsectionHeader confidence="0.517839">
3.1.2 Stacked learning
</subsectionHeader>
<bodyText confidence="0.996051333333333">
Stacked learning is a machine learning method
that enables the learner to be aware of the labels
of nearby examples.
</bodyText>
<page confidence="0.990492">
9
</page>
<figureCaption confidence="0.7679675">
Figure 4: Stacked learning. The output from the first stage is treated as features for the second stage.
The final output is predicted using label information of nearby TLINKs.
</figureCaption>
<bodyText confidence="0.999839333333333">
The first stage, as shown in Figure 5, uses the
local classifiers and predicts the relation types of
all TLINKs. In the second stage, the document’s
timegraph is constructed and the output from the
first stage is associated with TLINKs in the graph.
The classifiers in the second stage use the infor-
mation from the nearby TLINKs and predict the
final output. We exploit features extracted from
the documents’ timegraphs, as listed in Section 3.2
in the second stage of the stacked learning.
An example of a document’s timegraph is
shown in Figure 6.
</bodyText>
<subsectionHeader confidence="0.997655">
3.2 Timegraph features
</subsectionHeader>
<bodyText confidence="0.999475428571429">
We treat timegraphs as directed graphs and double
the number of edges by adding new edges with
opposite relation types/directions to every existing
edge. For example, if the graph contains an edge
e1 BEFORE e2, we add a new edge e2 AFTER e1.
Our proposed timegraph features are described
below.
</bodyText>
<listItem confidence="0.981782">
• Adjacent nodes and links
</listItem>
<bodyText confidence="0.988721461538461">
The features are the concatenation of the di-
rections to the adjacent links to the pair of en-
tities, the relation types of the links, and the
information on the adjacent nodes, i.e., word
tokens, part of speech tags, lemmas. For ex-
ample, the features for predicting the relation
between e1 and e2 in Figure 6 are SRC OUT-
IS INCLUDED-(Type of t0), DEST IN-BEFORE-
(Type of t0), and so on.
In this work, only Type of temporal expres-
sion (an attribute given in the Timebank cor-
pus), Tense and Part-of-speech tag are ap-
plied but other attributes could also be used.
</bodyText>
<listItem confidence="0.992758">
• Other paths
</listItem>
<bodyText confidence="0.607040571428571">
Paths with certain path lengths (in this work,
2 G path length G 4) between the temporal
entities are used as features. The paths must
not contain cycles. For example, the path
features of the relation between e1 and e2
are IS INCLUDED-BEFORE and SIMULTANEOUS-
BEFORE-BEFORE.
</bodyText>
<listItem confidence="0.974286">
• Generalized paths
</listItem>
<bodyText confidence="0.650780666666667">
A generalized version of the path features,
e.g., the IS INCLUDED-BEFORE path is gener-
alized to *-BEFORE and IS INCLUDED-*.
</bodyText>
<listItem confidence="0.988734">
• (E,V,E) tuples
</listItem>
<bodyText confidence="0.970274">
The (E,V,E) tuples of the edges and ver-
tices on the path are used as features, e.g.,
IS INCLUDED (Type of t0) BEFORE.
</bodyText>
<listItem confidence="0.967926">
• (V,E,V) tuples
</listItem>
<bodyText confidence="0.9988035">
The (V,E,V) tuples of the edges and vertices
on the path are used as features, e.g., (Tense
of e1) IS INCLUDED (Type of t0) and (Type of
t0) BEFORE (Tense of e2).
The summary of the timegraph features is
shown in Table 2.
</bodyText>
<sectionHeader confidence="0.992232" genericHeader="method">
4 Relation inference and time-time
connection
</sectionHeader>
<bodyText confidence="0.99992825">
We call TLINKs that have more than one path be-
tween the temporal entities “multi-path TLINKs”.
The coverage of the multi-path TLINKs is pre-
sented in Table 3. The annotated entities in
</bodyText>
<page confidence="0.996802">
10
</page>
<bodyText confidence="0.9998864">
the Timebank corpus create loosely connected
timegraphs as we can see from the table that only
5.65% of all the annotated TLINKs have multiple
paths between given pairs of temporal entities.
Since most of the timegraph features are only
applicable for multi-path TLINKs, it is important
to have dense timegraphs. In order to increase
the numbers of connections, we employ two ap-
proaches: relation inference and time-time con-
nection.
</bodyText>
<subsectionHeader confidence="0.990858">
4.1 Relation inference
</subsectionHeader>
<bodyText confidence="0.999994181818182">
We create new E-E and E-T connections between
entities in a timegraph by following a set of infer-
ence rules. For example, if e1 happens AFTER e2
and e2 happens IMMEDIATELY AFTER e3, then we
infer a new temporal relation “e1 happens AFTER
e3”. In this paper, we add a new connection only
when the inference gives only one type of tem-
poral relation as a result from the relation infer-
ence. Figure 7b shows the timegraph after adding
new inference relations to the original timegraph
in Figure 7a.
</bodyText>
<subsectionHeader confidence="0.973945">
4.2 Time-time connection
</subsectionHeader>
<bodyText confidence="0.998527130434782">
As with Chambers et al. (2007) and Tatu and
Srikanth (2008), we also create new connections
between time entities in a timegraph by applying
some rules to normalized values of time entities
provided in the corpus.
Figure 7c shows the timegraph after adding a
time-time link and new inference relations to the
original timegraph in Figure 7a. When the nor-
malized value of t2 is more than the value of t1,
a TLINK with the relation type AFTER is added
between them. After that, as introduced in Sub-
section 4.2, new inference relations (e1-e2, e1-e3,
e2-e3) are added.
As the number of relations grows too large af-
ter performing time-time connection and infer-
ence relation recursively, we limited the number of
TLINKs for each document’s timegraph to 10,000
relations. The total number of TLINKs for all doc-
uments in the corpus is presented in Table 4. The
first row is the number of the human-annotated re-
lations. The second and third rows show the to-
tal number after performing relation inference and
time-time connection.
</bodyText>
<figure confidence="0.6272562">
(a) Original timegraph
(b) After relation inference. Two relations (e1-e2, e1-e3)
are added.
(c) After time-time connection (t1-t2) and relation inference.
Three relations (e1-e2, e1-e3, e2-e3) are added.
</figure>
<figureCaption confidence="0.996587">
Figure 7: Increasing number of TLINKs
</figureCaption>
<table confidence="0.9916655">
No. of TLINKs E-E E-T Total
All TLINKs 2,520 2,463 4,983
Multi-path TLINKs 119 163 282
Percentage 4.72 6.62 5.65
</table>
<tableCaption confidence="0.99928">
Table 3: Coverage of multi-path TLINKs
</tableCaption>
<figure confidence="0.6711245">
after
after
</figure>
<page confidence="0.941649">
11
</page>
<table confidence="0.999532">
Approach Graph-based evaluation
F1(%) P(%) R(%)
Local - baseline features 58.15 58.17 58.13
Local - baseline + deep features 59.45 59.48 59.42
Stacked - baseline features 58.33 58.37 58.29
Stacked (inference) - baseline features 58.30 58.32 58.27
Stacked (inference, time-time) - baseline features 58.29 58.31 58.27
Stacked - baseline + deep features 59.55 59.51 59.58
Stacked (inference) - baseline + deep features 59.55 59.57 59.52
Stacked (inference, time-time) - baseline + deep features 59.61 59.63 59.58
</table>
<tableCaption confidence="0.993799">
Table 5: Ten-fold cross validation results on the training set
</tableCaption>
<table confidence="0.99969025">
No. of TLINKs Total
Annotated 4,983
+Inference 24,788
+Inference + time-time connection 87,992
</table>
<tableCaption confidence="0.9409715">
Table 4: Number of TLINKs in the Timebank cor-
pus
</tableCaption>
<sectionHeader confidence="0.991593" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.99968925">
For the baselines and both stages of the stacked
learning, we have used the LIBLINEAR (Fan
et al., 2008) and configured it to work as L2-
regularized logistic regression classifiers.
We trained our models on the Timebank corpus,
introduced in Subsection 2.1, which was provided
by the TempEval-3 organiser. The corpus contains
183 newswire articles in total.
</bodyText>
<subsectionHeader confidence="0.94992">
5.1 Results on the training data
</subsectionHeader>
<bodyText confidence="0.999963642857143">
The performance analysis is performed based on
10-fold cross validation over the training data. The
classification F1 score improves by 0.18 pp and
0.16 pp compared to the local pairwise models
with/without deep syntactic features.
We evaluated the system using a graph-based
evaluation metric proposed by UzZaman and
Allen (2011). Table 5 shows the classification
accuracy over the training set using graph-based
evaluation.
The stacked model affected the relation classi-
fication output of the local model, changing the
relation types of 390 (out of 2520) E-E TLINKs
and 169 (out of 2463) E-T TLINKs.
</bodyText>
<subsectionHeader confidence="0.996927">
5.2 Comparison with the state of the art
</subsectionHeader>
<bodyText confidence="0.998402230769231">
We compared our system to that of Yoshikawa
et al. (2009) which uses global information to
improve the accuracy of temporal relation clas-
sification. Their system was evaluated based on
TempEval-2’s rules and data set (Verhagen et al.,
2007), in which the relation types were reduced to
six relations: BEFORE, OVERLAP, AFTER, BEFORE-
OR-OVERLAP, OVERLAP-OR-AFTER, and VAGUE. The
evaluation was done using 10-fold cross validation
over the same data set as that of their reported re-
sults.
According to TempEval-2’s rules, there are
three tasks as follows:
</bodyText>
<listItem confidence="0.997781714285714">
• Task A: Temporal relations between events
and all time expressions appearing in the
same sentence.
• Task B: Temporal relations between events
and the DCT.
• Task C: Temporal relations betweeen main
verbs of adjacent sentences.
</listItem>
<bodyText confidence="0.99994047368421">
The number of TLINKs annotated by the orga-
nizer, after relation inference, and after time-time
connection for each task is summarized in Table
7. Table 8 shows the number of TLINKs after per-
forming relation inference and time-time connec-
tion.
As shown in Table 6, our system can achieve
better results in task B and C even without deep
syntactic features but performs worse than their
system in task A. Compared to the baselines, the
overall improvement is statistically significant* (p
&lt; 10−4, McNemar’s test, two-tailed) without deep
syntactic features and gets more statistically sig-
nificant** (p &lt; 10−5, McNemar’s test, two-tailed)
when applying deep syntactic information to the
system. The overall result has about 1.4 pp higher
accuracy than the result from their global model.
Note that Yoshikawa et al. (2009) did not apply
deep syntactic features in their system.
</bodyText>
<page confidence="0.995984">
12
</page>
<table confidence="0.990543230769231">
Approach Task A Task B Task C Overall
Yoshikawa et al. (2009) (local) 61.3 78.9 53.3 66.7
Yoshikawa et al. (2009) (global) 66.2 79.9 55.2 68.9
Our system (local) - baseline features 59.9 80.3 58.5 68.5
Our system (local) - baseline + deep features 62.1 80.3 58.4 69.0
Our system (stacked) - baseline features 59.5 79.9 58.5 68.2
Our system (stacked, inference) - baseline features 59.9 80.0 59.7 68.7
Our system (stacked, inference, time-time) - baseline fea- 63.8 80.0 58.9 69.5*
tures
Our system (stacked) - baseline + deep features 63.5 79.4 58.0 68.9
Our system (stacked, inference) - baseline + deep features 63.7 80.3 59.2 69.7
Our system (stacked, inference, time-time) - baseline + 65.9 80.5 58.9 70.3**
deep features
</table>
<tableCaption confidence="0.996253">
Table 6: Comparison of the stacked model to the state of the art and to our local model (F1 score(%))
</tableCaption>
<table confidence="0.978544">
No. of TLINKs Task A Task B Task C
Annotated 1,490 2,556 1,744
</table>
<tableCaption confidence="0.949004">
Table 7: TempEval-2 data set
</tableCaption>
<table confidence="0.998777">
No. of TLINKs Total
Annotated 5,970
+Inference 156,654
+Inference + time-time connection 167,875
</table>
<tableCaption confidence="0.8021565">
Table 8: Number of relations in TempEval-2 data
set
</tableCaption>
<bodyText confidence="0.999945636363637">
The stacked model enhances the classification
accuracy of task A when timegraphs are dense
enough. Deep syntactic features can be extracted
only when temporal entities are in the same sen-
tences so they improve the model for task A
(event-time pairs in the same sentences) but these
features clearly lower the accuracy of task C, since
there are very few event-event pairs that appear
in the same sentences (and break the definition
of task C). This is probably because the sparse-
ness of the deep features degrades the performance
in task C. Moreover, these features do not help
task B in the local model because we cannot ex-
tract any deep syntactic features from TLINKs be-
tween events and DCT. However, they contribute
slightly to the improvement in the stacked model
since deep syntactic features increase the accuracy
of the prediction of task A in the first stage of the
stacked model. As a result, timegraph features ex-
tracted from the output of the first stage are better
than those extracted from the local model trained
on only baseline features.
</bodyText>
<sectionHeader confidence="0.999856" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999986208333333">
As we can see from Table 5 and 6, although
deep syntactic features can improve the classifi-
cation accuracy significantly, some additional pre-
processing is required. Moreover, deep parsers
are not able to parse sentences in some specific
domains. Thus, sometimes it is not practical to
use this kind of features in real-world temporal
relation classification problems. By applying the
stacked learning approach to the temporal relation
classification task, the system with only baseline
features is able to achieve good classification re-
sults compared to the system with deep syntactic
features.
Again, from Table 5 and 6, the inference and
time-time connection, described in Section 4,
sometimes degrade the performance. This is pre-
sumably because the number of features increases
severely as the number of TLINKs increased.
The stacked model also has another advantage
that it is easy to build and does not consume too
much training time compared to MLNs used by
Yoshikawa et al. (2009), which are, in general,
computationally expensive and infeasible for large
training sets.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9998534">
In this paper, we present an approach for exploit-
ing timegraph features in the temporal relation
classification task. We employ the stacked learn-
ing approach to make use of information obtained
from nearby entities in timegraphs. The results
</bodyText>
<page confidence="0.996728">
13
</page>
<bodyText confidence="0.9999956">
show that our system can outperform the state-of-
the-art system and achieve good accuracy by us-
ing only baseline features. We also apply the rela-
tion inference rules and the time-time connection
to tackle the timegraphs’ sparseness problem.
In future work, we hope to improve the classi-
fication performance by making use of probability
values of prediction results obtained from the first
stage of the stacked learning and applying the full
set of inference relations to the system.
</bodyText>
<sectionHeader confidence="0.972903" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999962">
The authors would like to thank the anonymous re-
viewers for their insightful comments and sugges-
tions, which were helpful in improving the quality
of the paper.
</bodyText>
<sectionHeader confidence="0.999436" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998988478873239">
Danushka Bollegala, Naoaki Okazaki, and Mitsuru
Ishizuka. 2010. A bottom-up approach to sentence
ordering for multi-document summarization. In In-
formation Processing &amp; Management, Volume 46,
Issue 1, January 2010, pages 89–109.
Johan Bos and Katja Markert. 2005. Recognis-
ing textual entailment with logical inference. In
HLT/EMNLP 2005, pages 628–635.
Nathanael Chambers, Shan Wang and Dan Juraf-
sky. 2007. Classifying temporal relations between
events. In ACL 2007, pages 173–176.
Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal
ordering. In EMNLP 2008, pages 698–706.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Cambridge, MA: MIT Press.
Natsuda Laokulrat, Makoto Miwa, Yoshimasa Tsu-
ruoka and Takashi Chikayama. 2013. UTTime:
Temporal relation classification using deep syntac-
tic features. In SemEval 2013, pages 89–92.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong
Min Lee and James Pustejovsky. 2006. Machine
Learning of Temporal Relations. In ACL 2006,
pages 753–760.
Stephanie A. Miller and Lenhart K. Schubert. 1999.
Time revisited. In Computational Intelligence 6,
pages 108–118.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. In Com-
putational Linguistics. 34(1). pages 35–80, MIT
Press.
James Pustejovsky, Patrick Hanks, Roser Saur´ı, An-
dew See, Rob Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro and
Marcia Lazo. 2003. The TIMEBANK Corpus.
In Proceedings of Corpus Linguistics 2003 (March
2003), pages 545–557.
James Pustejovsky, Robert Ingria, Roser Saur´ı, Jos´e
Casta˜no, Jessica Littman, Rob Gaizauskas, Andrea
Setzer, Graham Katz and Inderjeet Mani. 2005. The
specification language TimeML. In The Language
of Time: A reader, pages 545–557.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In ACL 2002, pages 41–47.
Stanford Natural Language Processing Group. 2012.
Stanford CoreNLP.
Marta Tatu and Munirathnam Srikanth. 2008. Experi-
ments with reasoning for temporal relations between
events. In COLING 2008, pages 857–864.
Naushad UzZaman and James F. Allen. 2011. Tempo-
ral evaluation. In ACL 2011, pages 351–356.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, Marc Verhagen, James Allen and James Puste-
jovsky. 2013. SemEval-2013 Task 1: TempEval-3:
Evaluating time expressions, events, and temporal
relations. In SemEval 2013, pages 2–9.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz and James Pustejovsky.
2007. SemEval-2007 task 15: TempEval temporal
relation identification. In SemEval 2007, pages 75–
80.
David H. Wolpert. 1992. Stacked generalization. In
Neural Networks, volume 5, pages 241–259.
Katsumasa Yoshikawa, Sebastian Riedel ,Masayuki
Asahara and Yuji Matsumoto. 2009. Jointly iden-
tifying temporal relations with Markov Logic. In
ACL 2009, pages 405–413.
</reference>
<page confidence="0.999257">
14
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.771538">
<title confidence="0.999952">Exploiting Timegraphs in Temporal Relation Classification</title>
<author confidence="0.920056">Makoto</author>
<author confidence="0.920056">Yoshimasa</author>
<affiliation confidence="0.991112">University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo,</affiliation>
<address confidence="0.858591">Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya,</address>
<email confidence="0.995666">miwa@toyota-ti.ac.jp</email>
<abstract confidence="0.999359625">Most of the recent work on machine learning-based temporal relation classification has been done by considering only a given pair of temporal entities (events or temporal expressions) at a time. Entities that have temporal connections to the pair of temporal entities under inspection are not considered even though they provide valuable clues to the prediction. In this paper, we present a new approach for exploiting knowledge obtained from nearby entities by making use of timegraphs and applying the stacked learning method to the temporal relation classification task. By performing 10-fold cross validation on the Timebank corpus, we achieved an F1 score of 59.61% based on the graphbased evaluation, which is 0.16 percentage points higher than that of the local approach. Our system outperformed the state-of-the-art system that utilizes global information and achieved about 1.4 percentage points higher accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>Naoaki Okazaki</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>A bottom-up approach to sentence ordering for multi-document summarization.</title>
<date>2010</date>
<booktitle>In Information Processing &amp; Management, Volume 46, Issue 1,</booktitle>
<pages>89--109</pages>
<contexts>
<context position="1660" citStr="Bollegala et al., 2010" startWordPosition="230" endWordPosition="233">h is 0.16 percentage points higher than that of the local approach. Our system outperformed the state-of-the-art system that utilizes global information and achieved about 1.4 percentage points higher accuracy. 1 Introduction Temporal relationships between entities, namely temporal expressions and events, are regarded as important information for deep understanding of documents. Being able to predict temporal relations between events and temporal expressions within a piece of text can support various NLP applications such as textual entailment (Bos et al., 2005), multi-document summarization (Bollegala et al., 2010), and question answering (Ravichandran and Hovy, 2002). Temporal relation classification, which is one of the subtasks TempEval-3 (UzZaman et al., 2013), aims to classify temporal relationships between pairs of temporal entities into one of the 14 relation types according to the TimeML specification (Pustejovsky et al., 2005), e.g., BEFORE, AFTER, DURING, and BEGINS. The Timebank corpus introduced by Pustejovsky et al. (2003) has enabled the machine learning-based classification of temporal relationship. By learning from the annotated relation types in the documents, it is possible to predict </context>
</contexts>
<marker>Bollegala, Okazaki, Ishizuka, 2010</marker>
<rawString>Danushka Bollegala, Naoaki Okazaki, and Mitsuru Ishizuka. 2010. A bottom-up approach to sentence ordering for multi-document summarization. In Information Processing &amp; Management, Volume 46, Issue 1, January 2010, pages 89–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>Recognising textual entailment with logical inference.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP</booktitle>
<pages>628--635</pages>
<marker>Bos, Markert, 2005</marker>
<rawString>Johan Bos and Katja Markert. 2005. Recognising textual entailment with logical inference. In HLT/EMNLP 2005, pages 628–635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Shan Wang</author>
<author>Dan Jurafsky</author>
</authors>
<title>Classifying temporal relations between events.</title>
<date>2007</date>
<booktitle>In ACL</booktitle>
<pages>173--176</pages>
<contexts>
<context position="15203" citStr="Chambers et al. (2007)" startWordPosition="2461" endWordPosition="2464">es: relation inference and time-time connection. 4.1 Relation inference We create new E-E and E-T connections between entities in a timegraph by following a set of inference rules. For example, if e1 happens AFTER e2 and e2 happens IMMEDIATELY AFTER e3, then we infer a new temporal relation “e1 happens AFTER e3”. In this paper, we add a new connection only when the inference gives only one type of temporal relation as a result from the relation inference. Figure 7b shows the timegraph after adding new inference relations to the original timegraph in Figure 7a. 4.2 Time-time connection As with Chambers et al. (2007) and Tatu and Srikanth (2008), we also create new connections between time entities in a timegraph by applying some rules to normalized values of time entities provided in the corpus. Figure 7c shows the timegraph after adding a time-time link and new inference relations to the original timegraph in Figure 7a. When the normalized value of t2 is more than the value of t1, a TLINK with the relation type AFTER is added between them. After that, as introduced in Subsection 4.2, new inference relations (e1-e2, e1-e3, e2-e3) are added. As the number of relations grows too large after performing time</context>
</contexts>
<marker>Chambers, Wang, Jurafsky, 2007</marker>
<rawString>Nathanael Chambers, Shan Wang and Dan Jurafsky. 2007. Classifying temporal relations between events. In ACL 2007, pages 173–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Jointly combining implicit constraints improves temporal ordering.</title>
<date>2008</date>
<booktitle>In EMNLP</booktitle>
<pages>698--706</pages>
<contexts>
<context position="2979" citStr="Chambers and Jurafsky (2008)" startWordPosition="441" endWordPosition="445">st of the existing machine learning-based systems use local information alone, i.e., they consider only a given pair of temporal entities at a time. Entities that have temporal connections to the entities in the given pair are not considered at all even though they provide valuable clues to the prediction. Hence, the local approach often produces contradictions. For instance, the system may predict that A happens before B, that B happens before C, and that A happens after C, which are mutually contradictory. In order to tackle the contradiction problem, global approaches have been proposed by Chambers and Jurafsky (2008) and Yoshikawa et al. (2009). Chamber and Jurafsky proposed a global model based on Integer Linear Programming that combines the output of local classifiers and maximizes the global confidence scores. While they focused only on the temporal relations between events, Yoshikawa et al. proposed a Markov Logic model to jointly predict the temporal relations between events and time expressions. In this paper, we propose an approach that utilizes timegraphs (Miller and Schubert, 1999), which represent temporal connectivity of all temporal entities in each document, for the relation classification. O</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008. Jointly combining implicit constraints improves temporal ordering. In EMNLP 2008, pages 698–706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<contexts>
<context position="17439" citStr="Fan et al., 2008" startWordPosition="2822" endWordPosition="2825">ne features 58.30 58.32 58.27 Stacked (inference, time-time) - baseline features 58.29 58.31 58.27 Stacked - baseline + deep features 59.55 59.51 59.58 Stacked (inference) - baseline + deep features 59.55 59.57 59.52 Stacked (inference, time-time) - baseline + deep features 59.61 59.63 59.58 Table 5: Ten-fold cross validation results on the training set No. of TLINKs Total Annotated 4,983 +Inference 24,788 +Inference + time-time connection 87,992 Table 4: Number of TLINKs in the Timebank corpus 5 Evaluation For the baselines and both stages of the stacked learning, we have used the LIBLINEAR (Fan et al., 2008) and configured it to work as L2- regularized logistic regression classifiers. We trained our models on the Timebank corpus, introduced in Subsection 2.1, which was provided by the TempEval-3 organiser. The corpus contains 183 newswire articles in total. 5.1 Results on the training data The performance analysis is performed based on 10-fold cross validation over the training data. The classification F1 score improves by 0.18 pp and 0.16 pp compared to the local pairwise models with/without deep syntactic features. We evaluated the system using a graph-based evaluation metric proposed by UzZama</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="8289" citStr="Fellbaum, 1998" startWordPosition="1283" endWordPosition="1284">e X X planation of each attribute can be found in Aspect X X (Pustejovsky et al., 2005). Modality X X Polarity X X Timex attributes Type X All attributes associated with temporal exValue X pressions. The explanation of each attribute onInDocument X be found in (Pustejovsky et al., 2005). Functican TemporalFunction X Morphosyntactic information Words X X Words, POS, lemmas within a window bePart of speech tags X X fore/after event words extracted using StanLemmas X X ford coreNLP (Stanford NLP Group, 2012) Lexical semantic information Synonyms of event word tokens X X WordNet lexical database (Fellbaum, 1998) Synonyms of temporal expressions X Event-Event information Class match X X Details are described in (Chambers et al., Tense match X 2007) Aspect match X True if both temporal entities are in the same Class bigram X sentence Tense bigram X Aspect bigram X Same sentence X Deep syntactic information Phrase structure X X Deep syntactic information extracted from Predicate-argument structure X X Enju Parser (Miyao and Tsujii, 2008). The details are described in (Laokulrat et al., 2013) Table 1: Local features Feature E-E E-T Description Adjacent nodes and links X X The details are described in Sub</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natsuda Laokulrat</author>
</authors>
<title>Makoto Miwa, Yoshimasa Tsuruoka and Takashi Chikayama.</title>
<date>2013</date>
<pages>89--92</pages>
<marker>Laokulrat, 2013</marker>
<rawString>Natsuda Laokulrat, Makoto Miwa, Yoshimasa Tsuruoka and Takashi Chikayama. 2013. UTTime: Temporal relation classification using deep syntactic features. In SemEval 2013, pages 89–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Marc Verhagen</author>
<author>Ben Wellner</author>
<author>Chong Min Lee</author>
<author>James Pustejovsky</author>
</authors>
<title>Machine Learning of Temporal Relations. In ACL</title>
<date>2006</date>
<pages>753--760</pages>
<contexts>
<context position="2338" citStr="Mani et al., 2006" startWordPosition="336" endWordPosition="339">oral relation classification, which is one of the subtasks TempEval-3 (UzZaman et al., 2013), aims to classify temporal relationships between pairs of temporal entities into one of the 14 relation types according to the TimeML specification (Pustejovsky et al., 2005), e.g., BEFORE, AFTER, DURING, and BEGINS. The Timebank corpus introduced by Pustejovsky et al. (2003) has enabled the machine learning-based classification of temporal relationship. By learning from the annotated relation types in the documents, it is possible to predict the temporal relation of a given pair of temporal entities (Mani et al., 2006). However, most of the existing machine learning-based systems use local information alone, i.e., they consider only a given pair of temporal entities at a time. Entities that have temporal connections to the entities in the given pair are not considered at all even though they provide valuable clues to the prediction. Hence, the local approach often produces contradictions. For instance, the system may predict that A happens before B, that B happens before C, and that A happens after C, which are mutually contradictory. In order to tackle the contradiction problem, global approaches have been</context>
</contexts>
<marker>Mani, Verhagen, Wellner, Lee, Pustejovsky, 2006</marker>
<rawString>Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min Lee and James Pustejovsky. 2006. Machine Learning of Temporal Relations. In ACL 2006, pages 753–760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie A Miller</author>
<author>Lenhart K Schubert</author>
</authors>
<title>Time revisited.</title>
<date>1999</date>
<journal>In Computational Intelligence</journal>
<volume>6</volume>
<pages>108--118</pages>
<contexts>
<context position="3462" citStr="Miller and Schubert, 1999" startWordPosition="517" endWordPosition="520"> are mutually contradictory. In order to tackle the contradiction problem, global approaches have been proposed by Chambers and Jurafsky (2008) and Yoshikawa et al. (2009). Chamber and Jurafsky proposed a global model based on Integer Linear Programming that combines the output of local classifiers and maximizes the global confidence scores. While they focused only on the temporal relations between events, Yoshikawa et al. proposed a Markov Logic model to jointly predict the temporal relations between events and time expressions. In this paper, we propose an approach that utilizes timegraphs (Miller and Schubert, 1999), which represent temporal connectivity of all temporal entities in each document, for the relation classification. Our method differs from the previous work in that their methods used transition rules to enforce consistency within each triplet of relations, but our method can also work with a set consisting of more than three relations. Moreover, 6 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 6–14, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: An example from the Timebank corpus in our work,</context>
</contexts>
<marker>Miller, Schubert, 1999</marker>
<rawString>Stephanie A. Miller and Lenhart K. Schubert. 1999. Time revisited. In Computational Intelligence 6, pages 108–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature forest models for probabilistic HPSG parsing.</title>
<date>2008</date>
<journal>In Computational Linguistics.</journal>
<volume>34</volume>
<issue>1</issue>
<pages>35--80</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8720" citStr="Miyao and Tsujii, 2008" startWordPosition="1350" endWordPosition="1353">fter event words extracted using StanLemmas X X ford coreNLP (Stanford NLP Group, 2012) Lexical semantic information Synonyms of event word tokens X X WordNet lexical database (Fellbaum, 1998) Synonyms of temporal expressions X Event-Event information Class match X X Details are described in (Chambers et al., Tense match X 2007) Aspect match X True if both temporal entities are in the same Class bigram X sentence Tense bigram X Aspect bigram X Same sentence X Deep syntactic information Phrase structure X X Deep syntactic information extracted from Predicate-argument structure X X Enju Parser (Miyao and Tsujii, 2008). The details are described in (Laokulrat et al., 2013) Table 1: Local features Feature E-E E-T Description Adjacent nodes and links X X The details are described in Subsection 3.2 Other paths X X Generalized paths X X (E,V,E) tuples X X (V,E,V) tuples X X Table 2: Timegraph features 8 Figure 2: path length &lt; 2 Figure 5: Local pairwise classification. Each TLINK is classified separately. Figure 3: path length &lt; 3 3 Proposed method Rather than using only local information on two entities in a TLINK, our goal is to exploit more global information which can be extracted from a document’s timegrap</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. In Computational Linguistics. 34(1). pages 35–80, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Saur´ı</author>
<author>Andew See</author>
<author>Rob Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Dragomir Radev</author>
<author>Beth Sundheim</author>
<author>David Day</author>
<author>Lisa Ferro</author>
<author>Marcia Lazo</author>
</authors>
<title>The TIMEBANK Corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of Corpus Linguistics</booktitle>
<pages>545--557</pages>
<marker>Pustejovsky, Hanks, Saur´ı, See, Gaizauskas, Setzer, Radev, Sundheim, Day, Ferro, Lazo, 2003</marker>
<rawString>James Pustejovsky, Patrick Hanks, Roser Saur´ı, Andew See, Rob Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro and Marcia Lazo. 2003. The TIMEBANK Corpus. In Proceedings of Corpus Linguistics 2003 (March 2003), pages 545–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Robert Ingria</author>
<author>Roser Saur´ı</author>
<author>Jos´e Casta˜no</author>
<author>Jessica Littman</author>
<author>Rob Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Graham Katz</author>
<author>Inderjeet Mani</author>
</authors>
<title>The specification language TimeML. In The Language of Time: A reader,</title>
<date>2005</date>
<pages>545--557</pages>
<marker>Pustejovsky, Ingria, Saur´ı, Casta˜no, Littman, Gaizauskas, Setzer, Katz, Mani, 2005</marker>
<rawString>James Pustejovsky, Robert Ingria, Roser Saur´ı, Jos´e Casta˜no, Jessica Littman, Rob Gaizauskas, Andrea Setzer, Graham Katz and Inderjeet Mani. 2005. The specification language TimeML. In The Language of Time: A reader, pages 545–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In ACL</booktitle>
<pages>41--47</pages>
<contexts>
<context position="1714" citStr="Ravichandran and Hovy, 2002" startWordPosition="237" endWordPosition="241">he local approach. Our system outperformed the state-of-the-art system that utilizes global information and achieved about 1.4 percentage points higher accuracy. 1 Introduction Temporal relationships between entities, namely temporal expressions and events, are regarded as important information for deep understanding of documents. Being able to predict temporal relations between events and temporal expressions within a piece of text can support various NLP applications such as textual entailment (Bos et al., 2005), multi-document summarization (Bollegala et al., 2010), and question answering (Ravichandran and Hovy, 2002). Temporal relation classification, which is one of the subtasks TempEval-3 (UzZaman et al., 2013), aims to classify temporal relationships between pairs of temporal entities into one of the 14 relation types according to the TimeML specification (Pustejovsky et al., 2005), e.g., BEFORE, AFTER, DURING, and BEGINS. The Timebank corpus introduced by Pustejovsky et al. (2003) has enabled the machine learning-based classification of temporal relationship. By learning from the annotated relation types in the documents, it is possible to predict the temporal relation of a given pair of temporal enti</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In ACL 2002, pages 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanford</author>
</authors>
<title>Natural Language Processing Group.</title>
<date>2012</date>
<institution>Stanford CoreNLP.</institution>
<marker>Stanford, 2012</marker>
<rawString>Stanford Natural Language Processing Group. 2012. Stanford CoreNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Tatu</author>
<author>Munirathnam Srikanth</author>
</authors>
<title>Experiments with reasoning for temporal relations between events.</title>
<date>2008</date>
<booktitle>In COLING</booktitle>
<pages>857--864</pages>
<contexts>
<context position="15232" citStr="Tatu and Srikanth (2008)" startWordPosition="2466" endWordPosition="2469">time-time connection. 4.1 Relation inference We create new E-E and E-T connections between entities in a timegraph by following a set of inference rules. For example, if e1 happens AFTER e2 and e2 happens IMMEDIATELY AFTER e3, then we infer a new temporal relation “e1 happens AFTER e3”. In this paper, we add a new connection only when the inference gives only one type of temporal relation as a result from the relation inference. Figure 7b shows the timegraph after adding new inference relations to the original timegraph in Figure 7a. 4.2 Time-time connection As with Chambers et al. (2007) and Tatu and Srikanth (2008), we also create new connections between time entities in a timegraph by applying some rules to normalized values of time entities provided in the corpus. Figure 7c shows the timegraph after adding a time-time link and new inference relations to the original timegraph in Figure 7a. When the normalized value of t2 is more than the value of t1, a TLINK with the relation type AFTER is added between them. After that, as introduced in Subsection 4.2, new inference relations (e1-e2, e1-e3, e2-e3) are added. As the number of relations grows too large after performing time-time connection and inferenc</context>
</contexts>
<marker>Tatu, Srikanth, 2008</marker>
<rawString>Marta Tatu and Munirathnam Srikanth. 2008. Experiments with reasoning for temporal relations between events. In COLING 2008, pages 857–864.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naushad UzZaman</author>
<author>James F Allen</author>
</authors>
<title>Temporal evaluation.</title>
<date>2011</date>
<booktitle>In ACL 2011,</booktitle>
<pages>351--356</pages>
<contexts>
<context position="18057" citStr="UzZaman and Allen (2011)" startWordPosition="2916" endWordPosition="2919"> 2008) and configured it to work as L2- regularized logistic regression classifiers. We trained our models on the Timebank corpus, introduced in Subsection 2.1, which was provided by the TempEval-3 organiser. The corpus contains 183 newswire articles in total. 5.1 Results on the training data The performance analysis is performed based on 10-fold cross validation over the training data. The classification F1 score improves by 0.18 pp and 0.16 pp compared to the local pairwise models with/without deep syntactic features. We evaluated the system using a graph-based evaluation metric proposed by UzZaman and Allen (2011). Table 5 shows the classification accuracy over the training set using graph-based evaluation. The stacked model affected the relation classification output of the local model, changing the relation types of 390 (out of 2520) E-E TLINKs and 169 (out of 2463) E-T TLINKs. 5.2 Comparison with the state of the art We compared our system to that of Yoshikawa et al. (2009) which uses global information to improve the accuracy of temporal relation classification. Their system was evaluated based on TempEval-2’s rules and data set (Verhagen et al., 2007), in which the relation types were reduced to s</context>
</contexts>
<marker>UzZaman, Allen, 2011</marker>
<rawString>Naushad UzZaman and James F. Allen. 2011. Temporal evaluation. In ACL 2011, pages 351–356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naushad UzZaman</author>
<author>Hector Llorens</author>
<author>Leon Derczynski</author>
<author>Marc Verhagen</author>
<author>James Allen</author>
<author>James Pustejovsky</author>
</authors>
<title>SemEval-2013 Task 1: TempEval-3: Evaluating time expressions, events, and temporal relations. In SemEval</title>
<date>2013</date>
<pages>2--9</pages>
<contexts>
<context position="1812" citStr="UzZaman et al., 2013" startWordPosition="252" endWordPosition="255">nd achieved about 1.4 percentage points higher accuracy. 1 Introduction Temporal relationships between entities, namely temporal expressions and events, are regarded as important information for deep understanding of documents. Being able to predict temporal relations between events and temporal expressions within a piece of text can support various NLP applications such as textual entailment (Bos et al., 2005), multi-document summarization (Bollegala et al., 2010), and question answering (Ravichandran and Hovy, 2002). Temporal relation classification, which is one of the subtasks TempEval-3 (UzZaman et al., 2013), aims to classify temporal relationships between pairs of temporal entities into one of the 14 relation types according to the TimeML specification (Pustejovsky et al., 2005), e.g., BEFORE, AFTER, DURING, and BEGINS. The Timebank corpus introduced by Pustejovsky et al. (2003) has enabled the machine learning-based classification of temporal relationship. By learning from the annotated relation types in the documents, it is possible to predict the temporal relation of a given pair of temporal entities (Mani et al., 2006). However, most of the existing machine learning-based systems use local i</context>
</contexts>
<marker>UzZaman, Llorens, Derczynski, Verhagen, Allen, Pustejovsky, 2013</marker>
<rawString>Naushad UzZaman, Hector Llorens, Leon Derczynski, Marc Verhagen, James Allen and James Pustejovsky. 2013. SemEval-2013 Task 1: TempEval-3: Evaluating time expressions, events, and temporal relations. In SemEval 2013, pages 2–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Robert Gaizauskas</author>
<author>Frank Schilder</author>
<author>Mark Hepple</author>
<author>Graham Katz</author>
<author>James Pustejovsky</author>
</authors>
<title>SemEval-2007 task 15: TempEval temporal relation identification. In SemEval</title>
<date>2007</date>
<pages>75--80</pages>
<contexts>
<context position="18610" citStr="Verhagen et al., 2007" startWordPosition="3007" endWordPosition="3010"> a graph-based evaluation metric proposed by UzZaman and Allen (2011). Table 5 shows the classification accuracy over the training set using graph-based evaluation. The stacked model affected the relation classification output of the local model, changing the relation types of 390 (out of 2520) E-E TLINKs and 169 (out of 2463) E-T TLINKs. 5.2 Comparison with the state of the art We compared our system to that of Yoshikawa et al. (2009) which uses global information to improve the accuracy of temporal relation classification. Their system was evaluated based on TempEval-2’s rules and data set (Verhagen et al., 2007), in which the relation types were reduced to six relations: BEFORE, OVERLAP, AFTER, BEFOREOR-OVERLAP, OVERLAP-OR-AFTER, and VAGUE. The evaluation was done using 10-fold cross validation over the same data set as that of their reported results. According to TempEval-2’s rules, there are three tasks as follows: • Task A: Temporal relations between events and all time expressions appearing in the same sentence. • Task B: Temporal relations between events and the DCT. • Task C: Temporal relations betweeen main verbs of adjacent sentences. The number of TLINKs annotated by the organizer, after rel</context>
</contexts>
<marker>Verhagen, Gaizauskas, Schilder, Hepple, Katz, Pustejovsky, 2007</marker>
<rawString>Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark Hepple, Graham Katz and James Pustejovsky. 2007. SemEval-2007 task 15: TempEval temporal relation identification. In SemEval 2007, pages 75– 80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H Wolpert</author>
</authors>
<title>Stacked generalization.</title>
<date>1992</date>
<journal>In Neural Networks,</journal>
<volume>5</volume>
<pages>241--259</pages>
<contexts>
<context position="10680" citStr="Wolpert, 1992" startWordPosition="1686" endWordPosition="1687">ministration has not, which is his legal right and his right to due process,” said(e2) Jorge Mas Santos, chairman of the Cuban American National Foundation. “This gives(e3) him the protection that he will not be repatriated(e4) to Cuba between now and Feb. 10.” Figure 6: Timegraph constructed from a document’s TLINKs Again, the relation between e4 and e3 can be inferred from the nearby relations, i.e., (1) e4 AFTER e2 and e2 AFTER e1 imply e4 AFTER e1, (2) e4 AFTER e1 and e1 SIMULTANEOUS e3 imply e4 AFTER e3. 3.1 Overview of our framework Our framework is based on the stacked learning method (Wolpert, 1992), which employs two stages of classification as illustrated in Figure 4. 3.1.1 Local pairwise model In a local pairwise model, temporal relation classification is done by considering only a given pair of temporal entities at a time as illustrated in Figure 5. We use a supervised machine learning approach and employ the basic feature set that can be easily extracted from the document’s text and the set of features proposed in our previous work (Laokulrat et al., 2013), which utilizes deep syntactic information, as baselines. The local features at different linguistic levels are listed in Table </context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>David H. Wolpert. 1992. Stacked generalization. In Neural Networks, volume 5, pages 241–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsumasa Yoshikawa</author>
</authors>
<title>Sebastian Riedel ,Masayuki Asahara and Yuji Matsumoto.</title>
<date>2009</date>
<booktitle>In ACL</booktitle>
<pages>405--413</pages>
<marker>Yoshikawa, 2009</marker>
<rawString>Katsumasa Yoshikawa, Sebastian Riedel ,Masayuki Asahara and Yuji Matsumoto. 2009. Jointly identifying temporal relations with Markov Logic. In ACL 2009, pages 405–413.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
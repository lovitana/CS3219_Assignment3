<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012399">
<title confidence="0.997799">
An automated method to build a corpus of rhetorically-classified
sentences in biomedical texts
</title>
<author confidence="0.994716">
Hospice Houngbo
</author>
<affiliation confidence="0.9977065">
Department of Computer Science
The University of Western Ontario
</affiliation>
<email confidence="0.99475">
hhoungbo@uwo.ca
</email>
<sectionHeader confidence="0.993812" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999689047619048">
The rhetorical classification of sentences
in biomedical texts is an important task
in the recognition of the components of
a scientific argument. Generating super-
vised machine learned models to do this
recognition requires corpora annotated for
the rhetorical categories Introduction (or
Background), Method, Result, Discus-
sion (or Conclusion). Currently, a few,
small annotated corpora exist. We use
a straightforward feature of co-referring
text using the word “this” to build a self-
annotating corpus extracted from a large
biomedical research paper dataset. The
corpus is annotated for all of the rhetori-
cal categories except Introduction with-
out involving domain experts. In a 10-fold
cross-validation, we report an overall F-
score of 97% with Naive Bayes and 98.7%
with SVM, far above those previously re-
ported.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999660764705882">
Sentence classification is an important pre-
processing task in the recognition of the compo-
nents of an argument in scientific text. For in-
stance, sentences that are deemed as conclusions
of a research paper can be used to validate or re-
fute an hypothesis presented in background or in-
troduction sentences in that paper. Therefore, in
order to understand the argumentation flow in sci-
entific publications, we need to understand how
different sentences fit into the complete rhetorical
structure of scientific writing.
To perform sentence classification using su-
pervised machine learning techniques requires a
large training corpus annotated with the appropri-
ate classification tags. In the biomedical domain,
some corpora already exist, but many of these cor-
pora are still limited and cannot be generalized to
</bodyText>
<author confidence="0.623236">
Robert E. Mercer
</author>
<affiliation confidence="0.9563175">
Department of Computer Science
The University of Western Ontario
</affiliation>
<email confidence="0.977211">
mercer@csd.uwo.ca
</email>
<bodyText confidence="0.999928073170732">
every context. The task of sentence classification
in various rhetorical categories is often performed
on ad hoc corpora derived from a limited num-
ber of papers that don’t necessarily represent all
of the text in the biomedical domain. For instance,
the corpus used by Agarwal and Yu (2009) for the
task of sentence classification into the IMRaD cat-
egories, is composed of only 1131 sentences.
In this study, we hypothesize that using a simple
linguistically-based heuristic, we can build a sig-
nificantly larger corpus comprising sentences that
belong to specific categories of the IMRaD rhetor-
ical structure of the biomedical research text, that
will not need domain experts to annotate them,
and will represent a wider range of publications in
the biomedical literature. We have collected pairs
of sequential sentences where the second sentence
begins with “This method...”, “This result...”,
“This conclusion... ”. Our hypothesis is that the
first sentence in each pair is a sentence that can be
categorized respectively as Method, Result and
Conclusion sentences.
We have a number of motivations for this work.
First, sentences are the basis for most text min-
ing and extraction systems. The second motiva-
tion is that biomedical texts are the reports of sci-
entific investigations and their discourse structures
should represent the scientific method that drives
these investigations. The third and last motivation
is that categorizing sentences into the IMRaD cat-
egories can help in the task of extracting knowl-
edge discovery elements from scientific papers.
The contribution of our work is twofold. First,
we have used a simple linguistic filter to automati-
cally select thousands of sentences that have a high
probability of being correctly categorized in the
IMRAD scheme, and second, we have used ma-
chine learning techniques to classify sentences in
order to validate our hypothesis that this linguis-
tic filter works. The rest of this paper is organized
as follows. The next section reviews some related
</bodyText>
<page confidence="0.988298">
19
</page>
<bodyText confidence="0.848458333333333">
Proceedings of the First Workshop on Argumentation Mining, pages 19–23,
Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics
work. In Section 3, a detailed methodology of cor-
pus construction and sentence classification tech-
niques is presented. In Section 4, the results are
described.
</bodyText>
<sectionHeader confidence="0.999272" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999912233333333">
The classification of sentences from scientific re-
search papers into different categories has been in-
vestigated in previous works. Many schemes have
been used and currently no standard classification
scheme has been agreed upon. Teufel et al. (1999)
use a classification scheme termed Argumentative
Zoning (AZ) to model the rhetorical and argumen-
tative aspects of scientific writing in order to easily
detect the different claims that are mentioned in a
scientific research paper. AZ has been modified
for the annotation of biology articles (Yoko et al.,
2006) and chemistry articles (Teufel et al., 2009).
Scientific discourse has also been studied in
terms of speculation and modality by Kilicoglu
and Bergler (2008) and Medlock and Briscoe
(2007). Also, Shatkay et al. (2008) and Wilbur
et al. (2006) have proposed an annotation scheme
that categorizes sentences according to various di-
mensions such as focus, polarity and certainty.
Many annotation units have also be proposed in
previous studies. Sentence level annotation is used
in Teufel et al. (1999) whereas de Waard et al.
(2009) used a multi-dimensional scheme for the
annotation of biomedical events (bio-events) in
texts.
Liakata et al. (2012) attempt to classify sen-
tences into the Core Scientific Concept (CoreSC)
scheme. This classification scheme consists of a
number of categories distributed into hierarchical
layers. The first layer consists of 11 categories,
which describe the main components of a sci-
entific investigation, the second layer consists of
properties of those categories (e.g. Novelty, Ad-
vantage), and the third layer provides identifiers
that link together instances of the same concept.
Some other recent works have focussed on the
classification of sentences from biomedical arti-
cles into the IMRaD (Introduction, Methods, Re-
search, and, Discussion) categories. Agarwal and
Yu (2009) use a corpus of 1131 sentences to clas-
sify sentences from biomedical research papers
into these categories. In this study, sentence level
annotation is used and multinomial Naive Bayes
machine learning has proved to perform better
than simple Naive Bayes. The authors report an
overall F-measure score of 91.55% with a mu-
tual information feature selection technique. The
present study provides an alternative way to build
a larger IMRaD annotated corpus, which com-
bined with existing corpora achieves a better per-
formance.
Methods for training supervised machine-
learning systems on non-annotated data, were pre-
sented in (Yu and Hatzivassiloglou, 2003), which
assumed that in a full-text, IMRaD-structured ar-
ticle, the majority of sentences in each section
will be classified into their respective IMRaD cate-
gory. Also, Agarwal and Yu (2009) used the same
method to build a baseline classifier that achieved
about 77.81% accuracy on their corpus.
</bodyText>
<sectionHeader confidence="0.999792" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.999256">
3.1 Constructing a self-annotating corpus
from a biomedical dataset
</subsectionHeader>
<bodyText confidence="0.99938815625">
The goal of this study is to show that the classi-
fication of sentences from scientific research pa-
pers to match the IMRaD rhetorical structure with
supervised machine learning can be enhanced us-
ing a self-annotating corpus. The first task con-
sists of the curation of a corpus that contains sen-
tences representative of the defined categorization
scheme. We have chosen to build the corpus by ex-
tracting sentences from a large repository of full-
text scientific research papers, a publicly available
full-text subset of the PubMed repository.
Since most demonstrative pronouns are co-
referential, a sentence that begins with the demon-
strative noun phrase “This method... ” or “This re-
sult... ” or “This conclusion... ” is co-referential
and its antecedents are likely to be found in previ-
ous sentences. Torii and Vijay-Shanker (2005) re-
ported that nearly all antecedents of such demon-
strative phrases can be found within two sen-
tences. As well, Hunston (2008) reported that
interpreting recurring phrases in a large corpus
enables us to capture the consistency in mean-
ing as well as the role of specific words in such
phrases. So, the recurring semantic sequences
“This method...” or “This result...” or “This
conclusion...” in the Pubmed corpus can help
us to capture valuable information in the context
of their usage. A similar technique was used in
(Houngbo and Mercer, 2012), to build a corpus
for method mention extraction from biomedical
research papers.
Our assumption is that a sentence that appears
</bodyText>
<page confidence="0.972832">
20
</page>
<bodyText confidence="0.996579166666667">
in the co-referential context of the co-referencing
phrase “This method...”, will likely talk about
a methodology used in a research experiment or
analysis. Similarly, a sentence that starts with the
expression “This result...” is likely to refer to
a result. And, similarly, for sentences that be-
gin with “This conclusion... ”. The Introduction
(Background) rhetorical category does not have a
similar co-referential structure. We have chosen to
only consider the immediately preceding sentence
to the “This” referencing sentence. Some exam-
ples are shown below.
</bodyText>
<table confidence="0.9983956">
Category # of Sentences Proportion
Method 3163 31.9%
Result 6288 62.7%
Conclusion 534 5.4%
Total 9985 100%
</table>
<tableCaption confidence="0.99766">
Table 1: Initial Self-annotated Corpus Statistics
</tableCaption>
<listItem confidence="0.866728909090909">
1. We have developed a DNA microarray-based
method for measuring transcript length ...
This method, called the Virtual Northern, is
a complementary approach ...
2. Interestingly, Drice the downstream caspase
activated ... was not affected by inhibition of
Dronc and Dredd.
This result, ... suggests that some other
mechanism activates Drice.
3. We obtained a long-range PCR product from
the latter interval, that appeared to encom-
</listItem>
<bodyText confidence="0.960058">
pass the breakpoint on chromosome 2 ...
This conclusion, however , was regarded
with caution, since ...
Table 1 shows the number of sentences per cate-
gory in this initial self-annotated corpus.
</bodyText>
<subsectionHeader confidence="0.878293">
3.1.1 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999159363636364">
We have used the set of features extracted from
the Agarwal and Yu (2009) IMRaD corpus. The
reason for this choice is to be able to validate our
claim against this previous work. Agarwal and
Yu (2009) experimented with mutual information
and chi-squared for feature selection and obtained
their best performance using the top 2500 features
comprised of a combination of individual words
as well as bigrams and trigrams. A feature that
indicates the presence of a citation in a sentence
is also used as it can be an important feature for
</bodyText>
<table confidence="0.952452416666667">
(a) Classification with Multinomial Naive Bayes.
Class Precision Recall F-Measure
Method 0.923 0.661 0.77
Result 0.627 0.813 0.708
Conclusion 0.68 0.821 0.744
Average 0.779 0.74 0.744
(b) Classification with Support Vector Machine
Class Precision Recall F-Measure
Method 0.818 0.521 0.636
Result 0.511 0.908 0.654
Conclusion 0.923 0.226 0.364
Average 0.72 0.621 0.604
</table>
<tableCaption confidence="0.990320666666667">
Table 2: Precision, Recall, F-measure : Classifier trained
with the initial self-annotated corpus and tested on a reduced
Agarwal and Yu (2009) corpus (Method, Result, Conclusion)
</tableCaption>
<bodyText confidence="0.999875285714286">
distinguishing some categories; for example, ci-
tations are more frequently used in Introduction
than in Results. All numbers were replaced by a
unique symbol #NuMBeR. Stop words were not
removed since certain stop words are also more
likely to be associated with certain IMRaD cate-
gories. Words that refer to a figure or table are not
removed, since such references are more likely to
occur in sentences indicating the outcome of the
study. We also used verb tense features as some
categories may be associated with the presence of
the present tense or the past tense in the sentence.
We used the Stanford parser (Klein and Manning,
2003) to identify these tenses.
</bodyText>
<subsectionHeader confidence="0.977699">
3.1.2 Self-annotation
</subsectionHeader>
<bodyText confidence="0.999913">
In our first experiment we trained a model on the
initial self-annotated corpus discussed above and
tested the model on the Agarwal and Yu (2009)
corpus. Table 2 shows F-measures that are below
the baseline classifier levels. We suggest that there
are two causes: many of the important n-grams
in the larger corpus are not present in the 2500
n-gram feature set; and there is noise in the ini-
tial self-annotated corpus. To reduce the noise in
the initial self-annotated corpus and to maintain
the 2500 n-gram feature set we pruned our ini-
tial self-annotated corpus using a semi-supervised
learning step using an initial model based on the
Agarwal and Yu feature set and learned from the
Agarwal and Yu corpus. We describe below the
semi-supervised method to do this pruning of the
initial self-annotated corpus.
</bodyText>
<page confidence="0.998064">
21
</page>
<bodyText confidence="0.999835866666667">
Our method for categorizing sentences into the
IMRaD categories does not work for the Intro-
duction category, so from the Agarwal and Yu
(2009) IMRaD corpus, we have extracted in-
stances belonging to the Method, Result and
Conclusion categories and have used this corpus
to build a model with a supervised multinomial
Naive Bayes method. This model is then used
to classify sentences in the initial self-annotated
corpus. When the model matches the initial self-
annotated corpus category with a confidence level
greater than 98%, this instance is added to what we
will now call the model-validated self-annotated
corpus. The composition of this model-validated
corpus is presented in Table 3.
</bodyText>
<table confidence="0.996598">
Category # of Sentences Proportion
Method 878 23.6%
Result 2399 64.5%
Conclusion 443 11.9%
Total 3719 100%
</table>
<tableCaption confidence="0.999016">
Table 3: Model-validated Self-annotated Corpus Statistics
</tableCaption>
<subsectionHeader confidence="0.99969">
3.2 Automatic text classification
</subsectionHeader>
<bodyText confidence="0.998465333333333">
For all supervised learning, we have used two
popular supervised machine-learning algorithms,
multinomial Naive Bayes (NB) and Support Vec-
tor Machine (SVM), provided by the open-source
Java-based machine-learning library Weka 3.7
(Witten and Frank, 2005).
</bodyText>
<sectionHeader confidence="0.999183" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999837111111111">
In the first classification task a classifier is trained
with the model-validated self-annotated corpus us-
ing 10-fold cross-validation. The model achieves
an F-measure score of 97% with NB and 98.7%
with SVM. See Table 4. The average F-measure
that Agarwal and Yu (2009) report for their 10-fold
cross-validation (which includes Introduction) is
91.55. The category F-measures that Agarwal and
Yu (2009) report for their 10-fold cross-validation
with the features that we use are: Method: 91.4
(95.04) (their best scores, in parentheses, require
inclusion of the IMRaD section as a feature), Re-
sult: 88.3 (92.24), and Conclusion: 69.03 (73.77).
In the last classification task, a classifier is
trained with the model-validated self-annotated
corpus and tested on the Agarwal and Yu (2009)
corpus. The F-measures in Table 5 are a substan-
tial improvement over those in Table 2.
</bodyText>
<table confidence="0.9621095">
(a) Classification with Multinomial Naive Bayes.
Class Precision Recall F-Measure
Method 0.981 0.957 0.969
Result 0.966 0.992 0.979
Conclusion 0.98 0.885 0.93
Average 0.971 0.971 0.971
(b) Classification with Support Vector Machine
Class Precision Recall F-Measure
Method 0.986 0.984 0.985
Result 0.988 0.995 0.992
Conclusion 0.986 0.95 0.968
Average 0.987 0.987 0.987
</table>
<tableCaption confidence="0.995657">
Table 4: Precision, Recall, F-measure : Classifier trained
with the model-validated self-annotated corpus (Method, Re-
sult, Conclusion) using 10-fold cross-validation
</tableCaption>
<table confidence="0.983345416666667">
(a) Classification with Multinomial Naive Bayes.
Class Precision Recall F-Measure
Method 0.937 0.806 0.866
Result 0.763 0.873 0.814
Conclusion 0.836 0.911 0.872
Average 0.858 0.847 0.848
(b) Classification with Support Vector Machine
Class Precision Recall F-Measure
Method 0.893 0.824 0.857
Result 0.763 0.85 0.804
Conclusion 0.835 0.811 0.823
Average 0.837 0.832 0.833
</table>
<tableCaption confidence="0.9854165">
Table 5: Precision, Recall, F-measure : Classifier trained
with the model-validated self-annotated corpus and tested on
a reduced Agarwal and Yu (2009) corpus (Method, Result,
Conclusion)
</tableCaption>
<bodyText confidence="0.999840777777778">
Sentence classification is important in determin-
ing the different components of argumentation.
We have suggested a method to annotate sentences
from scientific research papers into their IMRaD
categories, excluding Introduction. Our results
show that it is possible to extract a large self-
annotated corpus automatically from a large repos-
itory of scientific research papers that generates
very good supervised machine learned models.
</bodyText>
<sectionHeader confidence="0.998378" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.860418">
This work was partially funded through a Natu-
ral Sciences and Engineering Research Council of
Canada (NSERC) Discovery Grant to R. Mercer.
</bodyText>
<page confidence="0.996185">
22
</page>
<sectionHeader confidence="0.989932" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998596646341464">
Shashank Agarwal and Hong Yu. 2009. Automatically
classifying sentences in full-text biomedical articles
into introduction, methods, results and discussion.
Bioinformatics, 25(23):3174–3180.
Anita de Waard, Paul Buitelaar, and Thomas Eigner.
2009. Identifying the epistemic value of discourse
segments in biology texts. In Proceedings of the
Eighth International Conference on Computational
Semantics, IWCS-8 ’09, pages 351–354. Associa-
tion for Computational Linguistics.
Hospice Houngbo and Robert E. Mercer. 2012.
Method mention extraction from scientific research
papers. In Proceedings of COLING 2012, pages
1211–1222, Mumbai, India.
Susan Hunston. 2008. Starting with the small words.
Patterns, lexis and semantic sequences. Interna-
tional Journal of Corpus Linguistics, 13:271–295.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: A linguistically motivated perspective. BMC
Bioinformatics, 9(S-11):S10.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, ACL ’03, pages 423–430. Asso-
ciation for Computational Linguistics.
Maria Liakata, Shyamasree Saha, Simon Dob-
nik, Colin R. Batchelor, and Dietrich Rebholz-
Schuhmann. 2012. Automatic recognition of con-
ceptualization zones in scientific articles and two life
science applications. Bioinformatics, 28(7):991–
1000.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
ACL ’07, pages 992–999. Association for Computa-
tional Linguistics.
Hagit Shatkay, Fengxia Pan, Andrey Rzhetsky, and
W. John Wilbur. 2008. Multi-dimensional classifi-
cation of biomedical text: Toward automated, prac-
tical provision of high-utility text to diverse users.
Bioinformatics, 24(18):2086–2093.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumen-
tation in research articles. In Proceedings of the
Ninth Conference of the European Chapter of the
Association for Computational Linguistics, pages
110–117. Association for Computational Linguis-
tics.
Simone Teufel, Advaith Siddharthan, and Colin Batch-
elor. 2009. Towards discipline-independent ar-
gumentative zoning: Evidence from chemistry and
computational linguistics. In Proceedings of the
2009 Conference on Empirical Methods in Natu-
ral Language Processing: Volume 3, EMNLP ’09,
pages 1493–1502. Association for Computational
Linguistics.
Manabu Torii and K. Vijay-Shanker. 2005. Anaphora
resolution of demonstrative noun phrases in Medline
abstracts. In Proceedings of PACLING 2005, pages
332–339.
W. John Wilbur, Andrey Rzhetsky, and Hagit Shatkay.
2006. New directions in biomedical text annota-
tion: definitions, guidelines and corpus construction.
BMC Bioinformatics, 7:356.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann Series in Data Management Sys-
tems. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA, 2nd edition.
Mizuta Yoko, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Jour-
nal of Medical Informatics, 75:468–487.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opin-
ion sentences. In Proceedings of the 2003 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ’03, pages 129–136. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.998936">
23
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.684436">
<title confidence="0.9507315">An automated method to build a corpus of sentences in biomedical texts</title>
<author confidence="0.871542">Hospice</author>
<affiliation confidence="0.999287">Department of Computer The University of Western</affiliation>
<email confidence="0.986048">hhoungbo@uwo.ca</email>
<abstract confidence="0.992226681818182">The rhetorical classification of sentences in biomedical texts is an important task in the recognition of the components of a scientific argument. Generating supervised machine learned models to do this recognition requires corpora annotated for rhetorical categories Discus- Conclusion). Currently, a few, small annotated corpora exist. We use a straightforward feature of co-referring using the word “this” to build a selfextracted from a large biomedical research paper dataset. The corpus is annotated for all of the rhetoricategories except without involving domain experts. In a 10-fold cross-validation, we report an overall Fscore of 97% with Naive Bayes and 98.7% with SVM, far above those previously reported.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shashank Agarwal</author>
<author>Hong Yu</author>
</authors>
<title>Automatically classifying sentences in full-text biomedical articles into introduction, methods, results and discussion.</title>
<date>2009</date>
<journal>Bioinformatics,</journal>
<volume>25</volume>
<issue>23</issue>
<contexts>
<context position="2231" citStr="Agarwal and Yu (2009)" startWordPosition="335" endWordPosition="338">earning techniques requires a large training corpus annotated with the appropriate classification tags. In the biomedical domain, some corpora already exist, but many of these corpora are still limited and cannot be generalized to Robert E. Mercer Department of Computer Science The University of Western Ontario mercer@csd.uwo.ca every context. The task of sentence classification in various rhetorical categories is often performed on ad hoc corpora derived from a limited number of papers that don’t necessarily represent all of the text in the biomedical domain. For instance, the corpus used by Agarwal and Yu (2009) for the task of sentence classification into the IMRaD categories, is composed of only 1131 sentences. In this study, we hypothesize that using a simple linguistically-based heuristic, we can build a significantly larger corpus comprising sentences that belong to specific categories of the IMRaD rhetorical structure of the biomedical research text, that will not need domain experts to annotate them, and will represent a wider range of publications in the biomedical literature. We have collected pairs of sequential sentences where the second sentence begins with “This method...”, “This result.</context>
<context position="6162" citStr="Agarwal and Yu (2009)" startWordPosition="942" endWordPosition="945"> Scientific Concept (CoreSC) scheme. This classification scheme consists of a number of categories distributed into hierarchical layers. The first layer consists of 11 categories, which describe the main components of a scientific investigation, the second layer consists of properties of those categories (e.g. Novelty, Advantage), and the third layer provides identifiers that link together instances of the same concept. Some other recent works have focussed on the classification of sentences from biomedical articles into the IMRaD (Introduction, Methods, Research, and, Discussion) categories. Agarwal and Yu (2009) use a corpus of 1131 sentences to classify sentences from biomedical research papers into these categories. In this study, sentence level annotation is used and multinomial Naive Bayes machine learning has proved to perform better than simple Naive Bayes. The authors report an overall F-measure score of 91.55% with a mutual information feature selection technique. The present study provides an alternative way to build a larger IMRaD annotated corpus, which combined with existing corpora achieves a better performance. Methods for training supervised machinelearning systems on non-annotated dat</context>
<context position="10141" citStr="Agarwal and Yu (2009)" startWordPosition="1569" endWordPosition="1572">alled the Virtual Northern, is a complementary approach ... 2. Interestingly, Drice the downstream caspase activated ... was not affected by inhibition of Dronc and Dredd. This result, ... suggests that some other mechanism activates Drice. 3. We obtained a long-range PCR product from the latter interval, that appeared to encompass the breakpoint on chromosome 2 ... This conclusion, however , was regarded with caution, since ... Table 1 shows the number of sentences per category in this initial self-annotated corpus. 3.1.1 Feature Extraction We have used the set of features extracted from the Agarwal and Yu (2009) IMRaD corpus. The reason for this choice is to be able to validate our claim against this previous work. Agarwal and Yu (2009) experimented with mutual information and chi-squared for feature selection and obtained their best performance using the top 2500 features comprised of a combination of individual words as well as bigrams and trigrams. A feature that indicates the presence of a citation in a sentence is also used as it can be an important feature for (a) Classification with Multinomial Naive Bayes. Class Precision Recall F-Measure Method 0.923 0.661 0.77 Result 0.627 0.813 0.708 Concl</context>
<context position="11985" citStr="Agarwal and Yu (2009)" startWordPosition="1864" endWordPosition="1867">op words are also more likely to be associated with certain IMRaD categories. Words that refer to a figure or table are not removed, since such references are more likely to occur in sentences indicating the outcome of the study. We also used verb tense features as some categories may be associated with the presence of the present tense or the past tense in the sentence. We used the Stanford parser (Klein and Manning, 2003) to identify these tenses. 3.1.2 Self-annotation In our first experiment we trained a model on the initial self-annotated corpus discussed above and tested the model on the Agarwal and Yu (2009) corpus. Table 2 shows F-measures that are below the baseline classifier levels. We suggest that there are two causes: many of the important n-grams in the larger corpus are not present in the 2500 n-gram feature set; and there is noise in the initial self-annotated corpus. To reduce the noise in the initial self-annotated corpus and to maintain the 2500 n-gram feature set we pruned our initial self-annotated corpus using a semi-supervised learning step using an initial model based on the Agarwal and Yu feature set and learned from the Agarwal and Yu corpus. We describe below the semi-supervis</context>
<context position="14095" citStr="Agarwal and Yu (2009)" startWordPosition="2195" endWordPosition="2198">ed Self-annotated Corpus Statistics 3.2 Automatic text classification For all supervised learning, we have used two popular supervised machine-learning algorithms, multinomial Naive Bayes (NB) and Support Vector Machine (SVM), provided by the open-source Java-based machine-learning library Weka 3.7 (Witten and Frank, 2005). 4 Results and Discussion In the first classification task a classifier is trained with the model-validated self-annotated corpus using 10-fold cross-validation. The model achieves an F-measure score of 97% with NB and 98.7% with SVM. See Table 4. The average F-measure that Agarwal and Yu (2009) report for their 10-fold cross-validation (which includes Introduction) is 91.55. The category F-measures that Agarwal and Yu (2009) report for their 10-fold cross-validation with the features that we use are: Method: 91.4 (95.04) (their best scores, in parentheses, require inclusion of the IMRaD section as a feature), Result: 88.3 (92.24), and Conclusion: 69.03 (73.77). In the last classification task, a classifier is trained with the model-validated self-annotated corpus and tested on the Agarwal and Yu (2009) corpus. The F-measures in Table 5 are a substantial improvement over those in Tab</context>
<context position="15758" citStr="Agarwal and Yu (2009)" startWordPosition="2434" endWordPosition="2437"> model-validated self-annotated corpus (Method, Result, Conclusion) using 10-fold cross-validation (a) Classification with Multinomial Naive Bayes. Class Precision Recall F-Measure Method 0.937 0.806 0.866 Result 0.763 0.873 0.814 Conclusion 0.836 0.911 0.872 Average 0.858 0.847 0.848 (b) Classification with Support Vector Machine Class Precision Recall F-Measure Method 0.893 0.824 0.857 Result 0.763 0.85 0.804 Conclusion 0.835 0.811 0.823 Average 0.837 0.832 0.833 Table 5: Precision, Recall, F-measure : Classifier trained with the model-validated self-annotated corpus and tested on a reduced Agarwal and Yu (2009) corpus (Method, Result, Conclusion) Sentence classification is important in determining the different components of argumentation. We have suggested a method to annotate sentences from scientific research papers into their IMRaD categories, excluding Introduction. Our results show that it is possible to extract a large selfannotated corpus automatically from a large repository of scientific research papers that generates very good supervised machine learned models. Acknowledgments This work was partially funded through a Natural Sciences and Engineering Research Council of Canada (NSERC) Disc</context>
</contexts>
<marker>Agarwal, Yu, 2009</marker>
<rawString>Shashank Agarwal and Hong Yu. 2009. Automatically classifying sentences in full-text biomedical articles into introduction, methods, results and discussion. Bioinformatics, 25(23):3174–3180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anita de Waard</author>
<author>Paul Buitelaar</author>
<author>Thomas Eigner</author>
</authors>
<title>Identifying the epistemic value of discourse segments in biology texts.</title>
<date>2009</date>
<booktitle>In Proceedings of the Eighth International Conference on Computational Semantics, IWCS-8 ’09,</booktitle>
<pages>351--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>de Waard, Buitelaar, Eigner, 2009</marker>
<rawString>Anita de Waard, Paul Buitelaar, and Thomas Eigner. 2009. Identifying the epistemic value of discourse segments in biology texts. In Proceedings of the Eighth International Conference on Computational Semantics, IWCS-8 ’09, pages 351–354. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hospice Houngbo</author>
<author>Robert E Mercer</author>
</authors>
<title>Method mention extraction from scientific research papers.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>1211--1222</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="8566" citStr="Houngbo and Mercer, 2012" startWordPosition="1326" endWordPosition="1329">ts are likely to be found in previous sentences. Torii and Vijay-Shanker (2005) reported that nearly all antecedents of such demonstrative phrases can be found within two sentences. As well, Hunston (2008) reported that interpreting recurring phrases in a large corpus enables us to capture the consistency in meaning as well as the role of specific words in such phrases. So, the recurring semantic sequences “This method...” or “This result...” or “This conclusion...” in the Pubmed corpus can help us to capture valuable information in the context of their usage. A similar technique was used in (Houngbo and Mercer, 2012), to build a corpus for method mention extraction from biomedical research papers. Our assumption is that a sentence that appears 20 in the co-referential context of the co-referencing phrase “This method...”, will likely talk about a methodology used in a research experiment or analysis. Similarly, a sentence that starts with the expression “This result...” is likely to refer to a result. And, similarly, for sentences that begin with “This conclusion... ”. The Introduction (Background) rhetorical category does not have a similar co-referential structure. We have chosen to only consider the im</context>
</contexts>
<marker>Houngbo, Mercer, 2012</marker>
<rawString>Hospice Houngbo and Robert E. Mercer. 2012. Method mention extraction from scientific research papers. In Proceedings of COLING 2012, pages 1211–1222, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Hunston</author>
</authors>
<title>Starting with the small words. Patterns, lexis and semantic sequences.</title>
<date>2008</date>
<journal>International Journal of Corpus Linguistics,</journal>
<pages>13--271</pages>
<contexts>
<context position="8146" citStr="Hunston (2008)" startWordPosition="1259" endWordPosition="1260">ion scheme. We have chosen to build the corpus by extracting sentences from a large repository of fulltext scientific research papers, a publicly available full-text subset of the PubMed repository. Since most demonstrative pronouns are coreferential, a sentence that begins with the demonstrative noun phrase “This method... ” or “This result... ” or “This conclusion... ” is co-referential and its antecedents are likely to be found in previous sentences. Torii and Vijay-Shanker (2005) reported that nearly all antecedents of such demonstrative phrases can be found within two sentences. As well, Hunston (2008) reported that interpreting recurring phrases in a large corpus enables us to capture the consistency in meaning as well as the role of specific words in such phrases. So, the recurring semantic sequences “This method...” or “This result...” or “This conclusion...” in the Pubmed corpus can help us to capture valuable information in the context of their usage. A similar technique was used in (Houngbo and Mercer, 2012), to build a corpus for method mention extraction from biomedical research papers. Our assumption is that a sentence that appears 20 in the co-referential context of the co-referen</context>
</contexts>
<marker>Hunston, 2008</marker>
<rawString>Susan Hunston. 2008. Starting with the small words. Patterns, lexis and semantic sequences. International Journal of Corpus Linguistics, 13:271–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halil Kilicoglu</author>
<author>Sabine Bergler</author>
</authors>
<title>Recognizing speculative language in biomedical research articles: A linguistically motivated perspective.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<pages>9--11</pages>
<contexts>
<context position="5008" citStr="Kilicoglu and Bergler (2008)" startWordPosition="767" endWordPosition="770">has been investigated in previous works. Many schemes have been used and currently no standard classification scheme has been agreed upon. Teufel et al. (1999) use a classification scheme termed Argumentative Zoning (AZ) to model the rhetorical and argumentative aspects of scientific writing in order to easily detect the different claims that are mentioned in a scientific research paper. AZ has been modified for the annotation of biology articles (Yoko et al., 2006) and chemistry articles (Teufel et al., 2009). Scientific discourse has also been studied in terms of speculation and modality by Kilicoglu and Bergler (2008) and Medlock and Briscoe (2007). Also, Shatkay et al. (2008) and Wilbur et al. (2006) have proposed an annotation scheme that categorizes sentences according to various dimensions such as focus, polarity and certainty. Many annotation units have also be proposed in previous studies. Sentence level annotation is used in Teufel et al. (1999) whereas de Waard et al. (2009) used a multi-dimensional scheme for the annotation of biomedical events (bio-events) in texts. Liakata et al. (2012) attempt to classify sentences into the Core Scientific Concept (CoreSC) scheme. This classification scheme con</context>
</contexts>
<marker>Kilicoglu, Bergler, 2008</marker>
<rawString>Halil Kilicoglu and Sabine Bergler. 2008. Recognizing speculative language in biomedical research articles: A linguistically motivated perspective. BMC Bioinformatics, 9(S-11):S10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, ACL ’03,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11791" citStr="Klein and Manning, 2003" startWordPosition="1833" endWordPosition="1836"> some categories; for example, citations are more frequently used in Introduction than in Results. All numbers were replaced by a unique symbol #NuMBeR. Stop words were not removed since certain stop words are also more likely to be associated with certain IMRaD categories. Words that refer to a figure or table are not removed, since such references are more likely to occur in sentences indicating the outcome of the study. We also used verb tense features as some categories may be associated with the presence of the present tense or the past tense in the sentence. We used the Stanford parser (Klein and Manning, 2003) to identify these tenses. 3.1.2 Self-annotation In our first experiment we trained a model on the initial self-annotated corpus discussed above and tested the model on the Agarwal and Yu (2009) corpus. Table 2 shows F-measures that are below the baseline classifier levels. We suggest that there are two causes: many of the important n-grams in the larger corpus are not present in the 2500 n-gram feature set; and there is noise in the initial self-annotated corpus. To reduce the noise in the initial self-annotated corpus and to maintain the 2500 n-gram feature set we pruned our initial self-ann</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, ACL ’03, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Liakata</author>
<author>Shyamasree Saha</author>
<author>Simon Dobnik</author>
<author>Colin R Batchelor</author>
<author>Dietrich RebholzSchuhmann</author>
</authors>
<title>Automatic recognition of conceptualization zones in scientific articles and two life science applications.</title>
<date>2012</date>
<journal>Bioinformatics,</journal>
<volume>28</volume>
<issue>7</issue>
<pages>1000</pages>
<contexts>
<context position="5497" citStr="Liakata et al. (2012)" startWordPosition="844" endWordPosition="847">eufel et al., 2009). Scientific discourse has also been studied in terms of speculation and modality by Kilicoglu and Bergler (2008) and Medlock and Briscoe (2007). Also, Shatkay et al. (2008) and Wilbur et al. (2006) have proposed an annotation scheme that categorizes sentences according to various dimensions such as focus, polarity and certainty. Many annotation units have also be proposed in previous studies. Sentence level annotation is used in Teufel et al. (1999) whereas de Waard et al. (2009) used a multi-dimensional scheme for the annotation of biomedical events (bio-events) in texts. Liakata et al. (2012) attempt to classify sentences into the Core Scientific Concept (CoreSC) scheme. This classification scheme consists of a number of categories distributed into hierarchical layers. The first layer consists of 11 categories, which describe the main components of a scientific investigation, the second layer consists of properties of those categories (e.g. Novelty, Advantage), and the third layer provides identifiers that link together instances of the same concept. Some other recent works have focussed on the classification of sentences from biomedical articles into the IMRaD (Introduction, Meth</context>
</contexts>
<marker>Liakata, Saha, Dobnik, Batchelor, RebholzSchuhmann, 2012</marker>
<rawString>Maria Liakata, Shyamasree Saha, Simon Dobnik, Colin R. Batchelor, and Dietrich RebholzSchuhmann. 2012. Automatic recognition of conceptualization zones in scientific articles and two life science applications. Bioinformatics, 28(7):991– 1000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Medlock</author>
<author>Ted Briscoe</author>
</authors>
<title>Weakly supervised learning for hedge classification in scientific literature.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL ’07,</booktitle>
<pages>992--999</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5039" citStr="Medlock and Briscoe (2007)" startWordPosition="772" endWordPosition="775"> works. Many schemes have been used and currently no standard classification scheme has been agreed upon. Teufel et al. (1999) use a classification scheme termed Argumentative Zoning (AZ) to model the rhetorical and argumentative aspects of scientific writing in order to easily detect the different claims that are mentioned in a scientific research paper. AZ has been modified for the annotation of biology articles (Yoko et al., 2006) and chemistry articles (Teufel et al., 2009). Scientific discourse has also been studied in terms of speculation and modality by Kilicoglu and Bergler (2008) and Medlock and Briscoe (2007). Also, Shatkay et al. (2008) and Wilbur et al. (2006) have proposed an annotation scheme that categorizes sentences according to various dimensions such as focus, polarity and certainty. Many annotation units have also be proposed in previous studies. Sentence level annotation is used in Teufel et al. (1999) whereas de Waard et al. (2009) used a multi-dimensional scheme for the annotation of biomedical events (bio-events) in texts. Liakata et al. (2012) attempt to classify sentences into the Core Scientific Concept (CoreSC) scheme. This classification scheme consists of a number of categories</context>
</contexts>
<marker>Medlock, Briscoe, 2007</marker>
<rawString>Ben Medlock and Ted Briscoe. 2007. Weakly supervised learning for hedge classification in scientific literature. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL ’07, pages 992–999. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hagit Shatkay</author>
<author>Fengxia Pan</author>
<author>Andrey Rzhetsky</author>
<author>W John Wilbur</author>
</authors>
<title>Multi-dimensional classification of biomedical text: Toward automated, practical provision of high-utility text to diverse users.</title>
<date>2008</date>
<journal>Bioinformatics,</journal>
<volume>24</volume>
<issue>18</issue>
<contexts>
<context position="5068" citStr="Shatkay et al. (2008)" startWordPosition="777" endWordPosition="780">d and currently no standard classification scheme has been agreed upon. Teufel et al. (1999) use a classification scheme termed Argumentative Zoning (AZ) to model the rhetorical and argumentative aspects of scientific writing in order to easily detect the different claims that are mentioned in a scientific research paper. AZ has been modified for the annotation of biology articles (Yoko et al., 2006) and chemistry articles (Teufel et al., 2009). Scientific discourse has also been studied in terms of speculation and modality by Kilicoglu and Bergler (2008) and Medlock and Briscoe (2007). Also, Shatkay et al. (2008) and Wilbur et al. (2006) have proposed an annotation scheme that categorizes sentences according to various dimensions such as focus, polarity and certainty. Many annotation units have also be proposed in previous studies. Sentence level annotation is used in Teufel et al. (1999) whereas de Waard et al. (2009) used a multi-dimensional scheme for the annotation of biomedical events (bio-events) in texts. Liakata et al. (2012) attempt to classify sentences into the Core Scientific Concept (CoreSC) scheme. This classification scheme consists of a number of categories distributed into hierarchica</context>
</contexts>
<marker>Shatkay, Pan, Rzhetsky, Wilbur, 2008</marker>
<rawString>Hagit Shatkay, Fengxia Pan, Andrey Rzhetsky, and W. John Wilbur. 2008. Multi-dimensional classification of biomedical text: Toward automated, practical provision of high-utility text to diverse users. Bioinformatics, 24(18):2086–2093.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Jean Carletta</author>
<author>Marc Moens</author>
</authors>
<title>An annotation scheme for discourse-level argumentation in research articles.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>110--117</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4539" citStr="Teufel et al. (1999)" startWordPosition="694" endWordPosition="697">xt section reviews some related 19 Proceedings of the First Workshop on Argumentation Mining, pages 19–23, Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics work. In Section 3, a detailed methodology of corpus construction and sentence classification techniques is presented. In Section 4, the results are described. 2 Related Work The classification of sentences from scientific research papers into different categories has been investigated in previous works. Many schemes have been used and currently no standard classification scheme has been agreed upon. Teufel et al. (1999) use a classification scheme termed Argumentative Zoning (AZ) to model the rhetorical and argumentative aspects of scientific writing in order to easily detect the different claims that are mentioned in a scientific research paper. AZ has been modified for the annotation of biology articles (Yoko et al., 2006) and chemistry articles (Teufel et al., 2009). Scientific discourse has also been studied in terms of speculation and modality by Kilicoglu and Bergler (2008) and Medlock and Briscoe (2007). Also, Shatkay et al. (2008) and Wilbur et al. (2006) have proposed an annotation scheme that categ</context>
</contexts>
<marker>Teufel, Carletta, Moens, 1999</marker>
<rawString>Simone Teufel, Jean Carletta, and Marc Moens. 1999. An annotation scheme for discourse-level argumentation in research articles. In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics, pages 110–117. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Advaith Siddharthan</author>
<author>Colin Batchelor</author>
</authors>
<title>Towards discipline-independent argumentative zoning: Evidence from chemistry and computational linguistics.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3, EMNLP ’09,</booktitle>
<pages>1493--1502</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4895" citStr="Teufel et al., 2009" startWordPosition="750" endWordPosition="753">2 Related Work The classification of sentences from scientific research papers into different categories has been investigated in previous works. Many schemes have been used and currently no standard classification scheme has been agreed upon. Teufel et al. (1999) use a classification scheme termed Argumentative Zoning (AZ) to model the rhetorical and argumentative aspects of scientific writing in order to easily detect the different claims that are mentioned in a scientific research paper. AZ has been modified for the annotation of biology articles (Yoko et al., 2006) and chemistry articles (Teufel et al., 2009). Scientific discourse has also been studied in terms of speculation and modality by Kilicoglu and Bergler (2008) and Medlock and Briscoe (2007). Also, Shatkay et al. (2008) and Wilbur et al. (2006) have proposed an annotation scheme that categorizes sentences according to various dimensions such as focus, polarity and certainty. Many annotation units have also be proposed in previous studies. Sentence level annotation is used in Teufel et al. (1999) whereas de Waard et al. (2009) used a multi-dimensional scheme for the annotation of biomedical events (bio-events) in texts. Liakata et al. (201</context>
</contexts>
<marker>Teufel, Siddharthan, Batchelor, 2009</marker>
<rawString>Simone Teufel, Advaith Siddharthan, and Colin Batchelor. 2009. Towards discipline-independent argumentative zoning: Evidence from chemistry and computational linguistics. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3, EMNLP ’09, pages 1493–1502. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Torii</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Anaphora resolution of demonstrative noun phrases in Medline abstracts.</title>
<date>2005</date>
<booktitle>In Proceedings of PACLING</booktitle>
<pages>332--339</pages>
<contexts>
<context position="8020" citStr="Torii and Vijay-Shanker (2005)" startWordPosition="1235" endWordPosition="1238"> self-annotating corpus. The first task consists of the curation of a corpus that contains sentences representative of the defined categorization scheme. We have chosen to build the corpus by extracting sentences from a large repository of fulltext scientific research papers, a publicly available full-text subset of the PubMed repository. Since most demonstrative pronouns are coreferential, a sentence that begins with the demonstrative noun phrase “This method... ” or “This result... ” or “This conclusion... ” is co-referential and its antecedents are likely to be found in previous sentences. Torii and Vijay-Shanker (2005) reported that nearly all antecedents of such demonstrative phrases can be found within two sentences. As well, Hunston (2008) reported that interpreting recurring phrases in a large corpus enables us to capture the consistency in meaning as well as the role of specific words in such phrases. So, the recurring semantic sequences “This method...” or “This result...” or “This conclusion...” in the Pubmed corpus can help us to capture valuable information in the context of their usage. A similar technique was used in (Houngbo and Mercer, 2012), to build a corpus for method mention extraction from</context>
</contexts>
<marker>Torii, Vijay-Shanker, 2005</marker>
<rawString>Manabu Torii and K. Vijay-Shanker. 2005. Anaphora resolution of demonstrative noun phrases in Medline abstracts. In Proceedings of PACLING 2005, pages 332–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W John Wilbur</author>
<author>Andrey Rzhetsky</author>
<author>Hagit Shatkay</author>
</authors>
<title>New directions in biomedical text annotation: definitions, guidelines and corpus construction.</title>
<date>2006</date>
<journal>BMC Bioinformatics,</journal>
<pages>7--356</pages>
<contexts>
<context position="5093" citStr="Wilbur et al. (2006)" startWordPosition="782" endWordPosition="785">d classification scheme has been agreed upon. Teufel et al. (1999) use a classification scheme termed Argumentative Zoning (AZ) to model the rhetorical and argumentative aspects of scientific writing in order to easily detect the different claims that are mentioned in a scientific research paper. AZ has been modified for the annotation of biology articles (Yoko et al., 2006) and chemistry articles (Teufel et al., 2009). Scientific discourse has also been studied in terms of speculation and modality by Kilicoglu and Bergler (2008) and Medlock and Briscoe (2007). Also, Shatkay et al. (2008) and Wilbur et al. (2006) have proposed an annotation scheme that categorizes sentences according to various dimensions such as focus, polarity and certainty. Many annotation units have also be proposed in previous studies. Sentence level annotation is used in Teufel et al. (1999) whereas de Waard et al. (2009) used a multi-dimensional scheme for the annotation of biomedical events (bio-events) in texts. Liakata et al. (2012) attempt to classify sentences into the Core Scientific Concept (CoreSC) scheme. This classification scheme consists of a number of categories distributed into hierarchical layers. The first layer</context>
</contexts>
<marker>Wilbur, Rzhetsky, Shatkay, 2006</marker>
<rawString>W. John Wilbur, Andrey Rzhetsky, and Hagit Shatkay. 2006. New directions in biomedical text annotation: definitions, guidelines and corpus construction. BMC Bioinformatics, 7:356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data Mining: Practical Machine Learning Tools and Techniques.</title>
<date>2005</date>
<booktitle>Series in Data Management Systems.</booktitle>
<publisher>Morgan Kaufmann</publisher>
<location>San Francisco, CA, USA,</location>
<note>2nd edition.</note>
<contexts>
<context position="13798" citStr="Witten and Frank, 2005" startWordPosition="2148" endWordPosition="2151">n 98%, this instance is added to what we will now call the model-validated self-annotated corpus. The composition of this model-validated corpus is presented in Table 3. Category # of Sentences Proportion Method 878 23.6% Result 2399 64.5% Conclusion 443 11.9% Total 3719 100% Table 3: Model-validated Self-annotated Corpus Statistics 3.2 Automatic text classification For all supervised learning, we have used two popular supervised machine-learning algorithms, multinomial Naive Bayes (NB) and Support Vector Machine (SVM), provided by the open-source Java-based machine-learning library Weka 3.7 (Witten and Frank, 2005). 4 Results and Discussion In the first classification task a classifier is trained with the model-validated self-annotated corpus using 10-fold cross-validation. The model achieves an F-measure score of 97% with NB and 98.7% with SVM. See Table 4. The average F-measure that Agarwal and Yu (2009) report for their 10-fold cross-validation (which includes Introduction) is 91.55. The category F-measures that Agarwal and Yu (2009) report for their 10-fold cross-validation with the features that we use are: Method: 91.4 (95.04) (their best scores, in parentheses, require inclusion of the IMRaD sect</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann Series in Data Management Systems. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mizuta Yoko</author>
<author>Anna Korhonen</author>
<author>Tony Mullen</author>
<author>Nigel Collier</author>
</authors>
<title>Zone analysis in biology articles as a basis for information extraction.</title>
<date>2006</date>
<journal>International Journal of Medical Informatics,</journal>
<pages>75--468</pages>
<contexts>
<context position="4850" citStr="Yoko et al., 2006" startWordPosition="743" endWordPosition="746">. In Section 4, the results are described. 2 Related Work The classification of sentences from scientific research papers into different categories has been investigated in previous works. Many schemes have been used and currently no standard classification scheme has been agreed upon. Teufel et al. (1999) use a classification scheme termed Argumentative Zoning (AZ) to model the rhetorical and argumentative aspects of scientific writing in order to easily detect the different claims that are mentioned in a scientific research paper. AZ has been modified for the annotation of biology articles (Yoko et al., 2006) and chemistry articles (Teufel et al., 2009). Scientific discourse has also been studied in terms of speculation and modality by Kilicoglu and Bergler (2008) and Medlock and Briscoe (2007). Also, Shatkay et al. (2008) and Wilbur et al. (2006) have proposed an annotation scheme that categorizes sentences according to various dimensions such as focus, polarity and certainty. Many annotation units have also be proposed in previous studies. Sentence level annotation is used in Teufel et al. (1999) whereas de Waard et al. (2009) used a multi-dimensional scheme for the annotation of biomedical even</context>
</contexts>
<marker>Yoko, Korhonen, Mullen, Collier, 2006</marker>
<rawString>Mizuta Yoko, Anna Korhonen, Tony Mullen, and Nigel Collier. 2006. Zone analysis in biology articles as a basis for information extraction. International Journal of Medical Informatics, 75:468–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, EMNLP ’03,</booktitle>
<pages>129--136</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6814" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="1042" endWordPosition="1045">sentences to classify sentences from biomedical research papers into these categories. In this study, sentence level annotation is used and multinomial Naive Bayes machine learning has proved to perform better than simple Naive Bayes. The authors report an overall F-measure score of 91.55% with a mutual information feature selection technique. The present study provides an alternative way to build a larger IMRaD annotated corpus, which combined with existing corpora achieves a better performance. Methods for training supervised machinelearning systems on non-annotated data, were presented in (Yu and Hatzivassiloglou, 2003), which assumed that in a full-text, IMRaD-structured article, the majority of sentences in each section will be classified into their respective IMRaD category. Also, Agarwal and Yu (2009) used the same method to build a baseline classifier that achieved about 77.81% accuracy on their corpus. 3 Methodology 3.1 Constructing a self-annotating corpus from a biomedical dataset The goal of this study is to show that the classification of sentences from scientific research papers to match the IMRaD rhetorical structure with supervised machine learning can be enhanced using a self-annotating corpus.</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, EMNLP ’03, pages 129–136. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
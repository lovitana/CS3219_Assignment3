<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000320">
<title confidence="0.981054">
The PAISA` Corpus of Italian Web Texts
</title>
<author confidence="0.952107">
Verena Lyding * Egon Stemle* Claudia Borghetti†
</author>
<email confidence="0.951614">
verena.lyding@eurac.edu egon.stemle@eurac.edu claudia.borghetti@unibo.it
</email>
<author confidence="0.638955">
Marco Brunello� Sara Castagnoli† Felice Dell’Orletta§
</author>
<email confidence="0.74366">
marcobrunello84@gmail.com s.castagnoli@unibo.it felice.dellorletta@ilc.cnr.it
</email>
<author confidence="0.81576">
Henrik Dittmann� Alessandro Lencill
</author>
<email confidence="0.735158">
henrik.dittmann@bordet.be alessandro.lenci@ling.unipi.it
</email>
<author confidence="0.523089">
Vito Pirrelli§
</author>
<email confidence="0.947335">
vito.pirrelli@ilc.cnr.it
</email>
<sectionHeader confidence="0.991803" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99987425">
PAISA` is a Creative Commons licensed,
large web corpus of contemporary Italian.
We describe the design, harvesting, and
processing steps involved in its creation.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967692307692">
This paper provides an overview of the PAISA` cor-
pus of Italian web texts and an introductory de-
scription of the motivation, procedures and facili-
ties for its creation and delivery.
Developed within the PAISA` project, the cor-
pus is intended to meet the objective to help over-
come the technological barriers that still prevent
web users from making use of large quantities of
contemporary Italian texts for language and cul-
tural education, by creating a comprehensive and
easily accessible corpus resource of Italian.
The initial motivation of the initiative stemmed
from the awareness that any static repertoire of
digital data, however carefully designed and de-
veloped, is doomed to fast obsolescence, if con-
tents are not freely available for public usage, con-
tinuously updated and checked for quality, incre-
mentally augmented with new texts and annota-
tion metadata for intelligent indexing and brows-
ing. These requirements brought us to design a
resource that was (1) freely available and freely
re-publishable, (2) comprehensively covering con-
temporary common language and cultural content
and (3) enhanced with a rich set of automatically-
annotated linguistic information to enable ad-
vanced querying and retrieving of data. On top
</bodyText>
<affiliation confidence="0.960316428571429">
*EURAC Research Bolzano/Bozen, IT
†University of Bologna, IT
$University of Leeds, UK
§ Institute of Computational Linguistics “Antonio Zam-
polli” - CNR, IT
¶Institut Jules Bordet, BE
IIUniversity of Pisa, IT
</affiliation>
<bodyText confidence="0.999921133333334">
of that, we set out to develop (4) a dedicated in-
terface with a low entry barrier for different target
groups. The end result of this original plan repre-
sents an unprecedented digital language resource
in the Italian scenario.
The main novelty of the PAISA` web corpus is
that it exclusively draws on Creative Commons li-
censed data, provides advanced linguistic annota-
tions with respect to corpora of comparable size
and corpora of web data, and invests in a carefully
designed query interface, targeted at different user
groups. In particular, the integration of richly an-
notated language content with an easily accessible,
user-oriented interface makes PAISA` a unique and
flexible resource for language teaching.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999957666666667">
The world wide web, with its inexhaustible
amount of natural language data, has become an
established source for efficiently building large
corpora (Kilgarriff and Grefenstette, 2003). Tools
are available that make it convenient to bootstrap
corpora from the web based on mere seed term
lists, such as the BootCaT toolkit (Baroni and
Bernardini, 2004). The huge corpora created by
the WaCky project (Baroni et al., 2009) are an ex-
ample of such an approach.
A large number of papers have recently been
published on the harvesting, cleaning and pro-
cessing of web corpora.1 However, freely avail-
able, large, contemporary, linguistically anno-
tated, easily accessible web corpora are still miss-
ing for many languages; but cf. e.g. (G´en´ereux
et al., 2012) and the Common Crawl Foundations
(CCF) web crawl2.
</bodyText>
<footnote confidence="0.9909065">
1cf. the Special Interest Group of the Association for
Computational Linguistics on Web as Corpus (SIGWAC)
http://sigwac.org.uk/
2CCF produces and maintains a repository of web crawl
data that is openly accessible: http://commoncrawl.
org/
</footnote>
<page confidence="0.972209">
36
</page>
<note confidence="0.9160345">
Felix Bildhauer &amp; Roland Schäfer (eds.), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 36–43,
Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.842343" genericHeader="method">
3 Corpus Composition
</sectionHeader>
<subsectionHeader confidence="0.992655">
3.1 Corpus design
</subsectionHeader>
<bodyText confidence="0.999965117647059">
PAISA` aimed at creating a comprehensive corpus
resource of Italian web texts which adheres to the
criteria laid out in section 1. For these criteria to
be fully met, we had to address a wide variety of
issues covering the entire life-cycle of a digital text
resource, ranging from robust algorithms for web
navigation and harvesting, to adaptive annotation
tools for advanced text indexing and querying and
user-friendly accessing and rendering online inter-
faces customisable for different target groups.
Initially, we targeted a size of 100M tokens, and
planned to automatically annotate the data with
lemma, part-of-speech, structural dependency, and
advanced linguistic information, using and adapt-
ing standard annotation tools (cf. section 4). In-
tegration into a querying environment and a dedi-
cated online interface were planned.
</bodyText>
<subsectionHeader confidence="0.996324">
3.2 Licenses
</subsectionHeader>
<bodyText confidence="0.999536428571429">
A crucial point when planning to compile a cor-
pus that is free to redistribute without encounter-
ing legal copyright issues is to collect texts that are
in the public domain or at least, have been made
available in a copyleft regime. This is the case
when the author of a certain document decided to
share some rights (copy and/or distribute, adapt
etc.) on her work with the public, in a way that
end users do not need to ask permission to the cre-
ator/owner of the original work. This is possible
by employing licenses other than the traditional
“all right reserved” copyright, i.e. GNU, Creative
Commons etc., which found a wide use especially
on the web. Exploratory studies (Brunello, 2009)
have shown that Creative Commons licenses are
widely employed throughout the web (at least on
the Italian webspace), enough to consider the pos-
sibility to build a large corpus from the web ex-
clusively made of documents released under such
licenses.
In particular, Creative Commons provides five
basic “baseline rights”: Attribution (BY), Share
Alike (SA), Non Commercial (NC), No Deriva-
tive Works (ND). The licenses themselves are
composed of at least Attribution (which can be
used even alone) plus the other elements, al-
lowing six different combinations:3 (1) Attribu-
tion (CC BY), (2) Attribution-NonCommercial
</bodyText>
<footnote confidence="0.9594415">
3For detailed descriptions of each license see http://
creativecommons.org/licenses/
</footnote>
<bodyText confidence="0.987539583333333">
(CC BY-NC), (3) Attribution-ShareAlike (CC BY-
SA), (4) Attribution-NoDerivs (CC BY-ND), (5)
Attribution-NonCommercial-ShareAlike (CC BY-
NC-SA), and (6) Attribution-NonCommercial-
NoDerivs (CC BY-NC-ND).
Some combinations are not possible because
certain elements are not compatible, e.g. Share
Alike and No Derivative Works. For our purposes
we decided to discard documents released with the
two licenses containing the No Derivative Works
option, because our corpus is in fact a derivative
work of collected documents.
</bodyText>
<subsectionHeader confidence="0.999842">
3.3 The final corpus
</subsectionHeader>
<bodyText confidence="0.996931147058824">
The corpus contains approximately 388,000 docu-
ments from 1,067 different websites, for a total of
about 250M tokens. All documents contained in
the PAISA` corpus date back to Sept./Oct. 2010.
The documents come from several web sources
which, at the time of corpus collection, provided
their content under Creative Commons license
(see section 3.2 for details). About 269,000 texts
are from Wikimedia Foundation projects, with
approximately 263,300 pages from Wikipedia,
2380 pages from Wikibooks, 1680 pages from
Wikinews, 740 pages from Wikiversity, 410 pages
from Wikisource, and 390 Wikivoyage pages.
The remaining 119,000 documents come
from guide.supereva.it (ca. 19,000),
italy.indymedia.org (ca. 10,000) and
several blog services from more than another
1,000 different sites (e.g. www.tvblog.it
(9,088 pages), www.motoblog.it (3,300),
www.ecowebnews.it (3,220), and
www.webmasterpoint.org (3,138).
Texts included in PAISA` have an average length
of 683 words, with the longest text4 counting
66,380 running tokens. A non exhaustive list of
average text lengths by source type is provided in
table 1 by way of illustration.
The corpus has been annotated for lemma, part-
of-speech and dependency information (see sec-
tion 4.2 for details). At the document level, the
corpus contains information on the URL of origin
and a set of descriptive statistics of the text, includ-
ing text length, rate of advanced vocabulary, read-
ability parameters, etc. (see section 4.3). Also,
each document is marked with a unique identifier.
</bodyText>
<footnote confidence="0.992819333333333">
4The European Constitution from wikisource.org:
http://it.wikisource.org/wiki/Trattato_
che_adotta_una_Costituzione_per_l’Europa
</footnote>
<page confidence="0.998882">
37
</page>
<table confidence="0.9998126">
Document source Avg text length
PAISA` total 683 words
Wikipedia 693 words
Wikibooks 1844 words
guide.supereva.it 378 words
italy.indymedia.it 1147 words
tvblog.it 1472 words
motoblog.it 421 words
ecowebnews.it 347 words
webmasterpoint.org 332 words
</table>
<tableCaption confidence="0.999949">
Table 1: Average text length by source
</tableCaption>
<bodyText confidence="0.935679333333333">
The annotated corpus adheres to the stan-
dard CoNLL column-based format (Buchholz and
Marsi, 2006), is encoded in UTF-8.
</bodyText>
<sectionHeader confidence="0.997369" genericHeader="method">
4 Corpus Creation
</sectionHeader>
<subsectionHeader confidence="0.999722">
4.1 Collecting and cleaning web data
</subsectionHeader>
<bodyText confidence="0.9999636">
The web pages for PAISA` were selected in two
ways: part of the corpus collection was made
through CC-focused web crawling, and another
part through a targeted collection of documents
from specific websites.
</bodyText>
<subsectionHeader confidence="0.551694">
4.1.1 Seed-term based harvesting
</subsectionHeader>
<bodyText confidence="0.99994145">
At the time of corpus collection (2010), we used
the BootCaT toolkit mainly because collecting
URLs could be based on the public Yahoo! search
API5, including the option to restrict search to CC-
licensed pages (including the possibility to specify
even the particular licenses). Unfortunately, Ya-
hoo! discontinued the free availability of this API,
and BootCaT’s remaining search engines do not
provide this feature.
An earlier version of the corpus was collected
using the tuple list originally employed to build
itWaC6. As we noticed that the use of this list, in
combination with the restriction to CC, biased the
final results (i.e. specific websites occurred very
often as top results) , we provided as input 50,000
medium frequent seed terms from a basic Italian
vocabulary list7, in order to get a wider distribu-
tion of search queries, and, ultimately, of texts.
As introduced in section 3.2, we restricted the
selection not just to Creative Commons-licensed
</bodyText>
<footnote confidence="0.9298002">
5http://developer.yahoo.com/boss/
6http://wacky.sslmit.unibo.it/doku.
php?id=seed_words_and_tuples
7http://ppbm.paravia.it/dib_lemmario.
php
</footnote>
<bodyText confidence="0.999481">
texts, but specifically to those licenses allowing
redistribution: namely, CC BY, CC BY-SA, CC
BY-NC-SA, and CC BY-NC.
Results were downloaded and automatically
cleaned with the KrdWrd system, an environment
for the unified processing of web content (Steger
and Stemle, 2009).
Wrongly CC-tagged pages were eliminated us-
ing a black-list that had been manually populated
following inspection of earlier corpus versions.
</bodyText>
<subsectionHeader confidence="0.554997">
4.1.2 Targeted
</subsectionHeader>
<bodyText confidence="0.99988">
In September 2009, the Wikimedia Foundation de-
cided to release the content of their wikis under
CC BY-SA8, so we decided to download the large
and varied amount of texts made available through
the Italian versions of these websites. This was
done using the Wikipedia Extractor9 on official
dumps10 of Wikipedia, Wikinews, Wikisource,
Wikibooks, Wikiversity and Wikivoyage.
</bodyText>
<subsectionHeader confidence="0.95334">
4.2 Linguistic annotation and tools
adaptation
</subsectionHeader>
<bodyText confidence="0.999835727272727">
The corpus was automatically annotated with
lemma, part-of-speech and dependency infor-
mation, using state-of-the-art annotation tools
for Italian. Part-of-speech tagging was per-
formed with the Part-Of-Speech tagger described
in Dell’Orletta (2009) and dependency-parsed by
the DeSR parser (Attardi et al., 2009), using Mul-
tilayer Perceptron as the learning algorithm. The
systems used the ISST-TANL part-of-speech11
and dependency tagsets12. In particular, the pos-
tagger achieves a performance of 96.34% and
DeSR, trained on the ISST-TANL treebank con-
sisting of articles from newspapers and period-
icals, achieves a performance of 83.38% and
87.71% in terms of LAS (labelled attachment
score) and UAS (unlabelled attachment score) re-
spectively, when tested on texts of the same type.
However, since Gildea (2001), it is widely ac-
knowledged that statistical NLP tools have a drop
of accuracy when tested against corpora differing
from the typology of texts on which they were
trained. This also holds true for PAIS `A: it contains
</bodyText>
<footnote confidence="0.97278675">
8Previously under GNU Free Documentation License.
9http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
10http://dumps.wikimedia.org/
11http://www.italianlp.it/docs/
ISST-TANL-POStagset.pdf
12http://www.italianlp.it/docs/
ISST-TANL-DEPtagset.pdf
</footnote>
<page confidence="0.998637">
38
</page>
<bodyText confidence="0.999884948275862">
lexical and syntactic structures of non-canonical
languages such as the language of social media,
blogs, forum posts, consumer reviews, etc. As re-
ported in Petrov and McDonald (2012), there are
multiple reasons why parsing the web texts is dif-
ficult: punctuation and capitalization are often in-
consistent, there is a lexical shift due to increased
use of slang and technical jargon, some syntactic
constructions are more frequent in web text than
in newswire, etc.
In order to overcome this problem, two main ty-
pologies of methods and techniques have been de-
veloped: Self-training (McClosky et al., 2006) and
Active Learning (Thompson et al., 1999).
For the specific purpose of the NLP tools adap-
tation to the Italian web texts, we adopted two dif-
ferent strategies for the pos-tagger and the parser.
For what concerns pos-tagging, we used an active
learning approach: given a subset of automatically
pos-tagged sentences of PAIS `A, we selected the
ones with the lowest likelihood, where the sen-
tence likelihood was computed as the product of
the probabilities of the assignments of the pos-
tagger for all the tokens. These sentences were
manually revised and added to the training corpus
in order to build a new pos-tagger model incor-
porating some new knowledge from the target do-
main.
For what concerns parsing, we used a self-
training approach to domain adaptation described
in Dell’Orletta et al. (2013), based on ULISSE
(Dell’Orletta et al., 2011). ULISSE is an unsu-
pervised linguistically-driven algorithm to select
reliable parses from a collection of dependency
annotated texts. It assigns to each dependency
tree a score quantifying its reliability based on a
wide range of linguistic features. After collect-
ing statistics about selected features from a cor-
pus of automatically parsed sentences, for each
newly parsed sentence ULISSE computes a reli-
ability score using the previously extracted feature
statistics. From the top of the parses (ranked ac-
cording to their reliability score) different pools of
parses were selected to be used for training. The
new training contains the original training set as
well as the new selected parses which include lex-
ical and syntactic characteristics specific of the tar-
get domain (Italian web texts). The parser trained
on this new training set improves its performance
when tested on the target domain.
We used this domain adaptation approach for
the following three main reasons: a) it is unsuper-
vised (i.e. no need for manually annotated training
data); b) unlike the Active Learning approach used
for pos-tagging, it does not need manual revision
of the automatically parsed samples to be used for
training; c) it was previously tested on Italian texts
with good results (Dell’Orletta et al., 2013).
</bodyText>
<subsectionHeader confidence="0.99985">
4.3 Readability analysis of corpus documents
</subsectionHeader>
<bodyText confidence="0.99995468">
For each corpus document, we calculated several
text statistics indicative of the linguistic complex-
ity, or ’readability’ of a text.
The applied measures include, (1) text length in
tokens, that is the number of tokens per text, (2)
sentences per text, that is a sentence count, and (3)
type-token ratio indicated as a percentage value.
In addition, we calculated (4) the advanced vo-
cabulary per text, that is a word count of the text
vocabulary which is not part of the the basic Ital-
ian vocabulary (’vocabolario di base’) for written
texts, as defined by De Mauro (1991)13, and (5)
the Gulpease Index (’Indice Gulpease’) (Lucisano
and Piemontese, 1988), which is a measure for the
readability of text that is based on frequency rela-
tions between the number of sentences, words and
letters of a text.
All values are encoded as metadata for the cor-
pus. Via the PAISA` online interface, they can
be employed for filtering documents and building
subcorpora. This facility was implemented with
the principal target group of PAISA` users in mind,
as the selection of language examples according
to their readability level is particularly relevant for
language learning and teaching.
</bodyText>
<subsectionHeader confidence="0.475611">
4.4 Attempts at text classification for genre,
topic, and function
</subsectionHeader>
<bodyText confidence="0.761709071428571">
Lack of information about the composition of cor-
pora collected from the web using unsupervised
methods is probably one of the major limitations
of current web corpora vis-`a-vis more traditional,
carefully constructed corpora, most notably when
applications to language teaching and learning are
envisaged. This also holds true for PAIS `A, es-
13The advanced vocabulary was calculated on the ba-
sis of a word list consisting of De Mauro’s ’vocabolario
fondamentale’ (http://it.wikipedia.org/wiki/
Vocabolario_fondamentale) and ’vocabolario
di alto uso’ (http://it.wikipedia.org/wiki/
Vocabolario_di_alto_uso), together with high
frequent function words not contained in those two lists.
</bodyText>
<page confidence="0.998044">
39
</page>
<bodyText confidence="0.999985541666667">
pecially for the harvested14 subcorpus that was
downloaded as described in section 4.1. We there-
fore carried out some experiments with the ulti-
mate aim to enrich the corpus with metadata about
text genre, topic and function, using automated
techniques.
In order to gain some insights into the com-
position of PAIS `A, we first conducted some man-
ual investigations. Drawing on existing literature
on web genres (e.g. (Santini, 2005; Rehm et al.,
2008; Santini et al., 2010)) and text classification
according to text function and topic (e.g. (Sharoff,
2006)), we developed a tentative three-fold taxon-
omy to be used for text classification. Following
four cycles of sample manual annotation by three
annotators, categories were adjusted in order to
better reflect the nature of PAIS `A’s web documents
(cf. (Sharoff, 2010) about differences between do-
mains covered in the BNC and in the web-derived
ukWaC). Details about the taxonomy are provided
in Borghetti et al. (2011). Then, we started to
cross-check whether the devised taxonomy was
indeed appropriate to describe PAIS `A’s composi-
tion by comparing its categories with data result-
ing from the application of unsupervised methods
for text classification.
Interesting insights have emerged so far re-
garding the topic category. Following Sharoff
(2010), we used topic modelling based on La-
tent Dirichlet Allocation for the detection of top-
ics: 20 clusters/topics were identified on the ba-
sis of keywords (the number of clusters to re-
trieve is a user-defined parameter) and projected
onto the manually defined taxonomy. This re-
vealed that most of the 20 automatically iden-
tified topics could be reasonably matched to
one of the 8 categories included in the tax-
onomy; exceptions were represented by clus-
ters characterised by proper nouns and gen-
eral language words such bambino/uomo/famiglia
(’child’/’man’/’family’) or credere/sentire/sperare
(’to believe’/’feel’/’hope’), which may in fact be
indicative of genres such as diary or personal com-
ment (e.g. personal blog). Only one of the cate-
gories originally included in the taxonomy – natu-
ral sciences – was not represented in the clusters,
which may indicate that there are few texts within
PAISA` belonging to this domain. One of the ma-
</bodyText>
<footnote confidence="0.750583">
14In fact, even the nature of the targeted texts is not pre-
cisely defined: for instance, Wikipedia articles can actually
encompass a variety of text types such as biographies, intro-
ductions to academic theories etc. (Santini et al., 2010, p. 15)
</footnote>
<bodyText confidence="0.999554857142857">
jor advantages of topic models is that each corpus
document can be associated – to varying degrees –
to several topics/clusters: if encoded as metadata,
this information makes it possible not only to fil-
ter texts according to their prevailing domain, but
also to represent the heterogeneous nature of many
web documents.
</bodyText>
<sectionHeader confidence="0.842066" genericHeader="method">
5 Corpus Access and Usage
</sectionHeader>
<subsectionHeader confidence="0.99564">
5.1 Corpus distribution
</subsectionHeader>
<bodyText confidence="0.999894">
The PAISA` corpus is distributed in two ways: it is
made available for download and it can be queried
via its online interface. For both cases, no restric-
tions on its usage apply other than those defined
by the Creative Commons BY-NC-SA license. For
corpus download, both the raw text version and the
annotated corpus in CoNLL format are provided.
The PAISA` corpus together with all project-
related information is accessible via the project
web site at http://www.corpusitaliano.it
</bodyText>
<subsectionHeader confidence="0.997739">
5.2 Corpus interface
</subsectionHeader>
<bodyText confidence="0.99998348275862">
The creation of a dedicated open online interface
for the PAISA` corpus has been a declared primary
objective of the project.
The interface is aimed at providing a power-
ful, effective and easy-to-employ tool for mak-
ing full use of the resource, without having to go
through downloading, installation or registration
procedures. It is targeted at different user groups,
particularly language learners, teachers, and lin-
guists. As users of PAISA` are expected to show
varying levels of proficiency in terms of language
competence, linguistic knowledge, and concern-
ing the use of online search tools, the interface
has been designed to provide four separate search
components, implementing different query modes.
Initially, the user is directed to a basic keyword
search that adopts a ’Google-style’ search box.
Single search terms, as well as multi-word combi-
nations or sequences can be searched by inserting
them in a simple text box.
The second component is an advanced graph-
ical search form. It provides elaborated search
options for querying linguistic annotation layers
and allows for defining distances between search
terms as well as repetitions or optionally occurring
terms. Furthermore, the advanced search supports
regular expressions.
The third component emulates a command-line
search via the powerful CQP query language of
</bodyText>
<page confidence="0.996311">
40
</page>
<bodyText confidence="0.999805304347826">
the Open Corpus Workbench (Evert and Hardie,
2011). It allows for complex search queries in
CQP syntax that rely on linguistic annotation lay-
ers as well as on metadata information.
Finally, a filter interface is presented in a fourth
component. It serves the purpose of retriev-
ing full-text corpus documents based on keyword
searches as well as text statistics (see section 4.3).
Like the CQP interface, the filter interface is also
supporting the building of temporary subcorpora
for subsequent querying.
By default, search results are displayed as
KWIC (KeyWord In Context) lines, centred
around the search expression. Each search hit can
be expanded to its full sentence view. In addition,
the originating full text document can be accessed
and its source URL is provided.
Based on an interactive visualisation for depen-
dency graphs (Culy et al., 2011) for each search
result a graphical representations of dependency
relations together with the sentence and associated
lemma and part-of-speech information can be gen-
erated (see Figure 1).
</bodyText>
<figureCaption confidence="0.99969">
Figure 1: Dependency diagram
</figureCaption>
<bodyText confidence="0.999406142857143">
Targeted at novice language learners of Italian,
a filter for automatically restricting search results
to sentences of limited complexity has been in-
tegrated into each search component. When ac-
tivated, search results are automatically filtered
based on a combination of the complexity mea-
sures introduced in section 4.3.
</bodyText>
<subsectionHeader confidence="0.999228">
5.3 Technical details
</subsectionHeader>
<bodyText confidence="0.999896857142857">
The PAISA` online interface has been developed in
several layers: in essence, it provides a front-end
to the corpus as indexed in Open Corpus Work-
bench (Evert and Hardie, 2011). This corpus
query engine provides the fundamental search ca-
pabilities through the CQP language. Based on
the CWB/Perl API that is part of the Open Corpus
Workbench package, a web service has been de-
veloped at EURAC which exposes a large part of
the CQP language15 through a RESTful API.16
The four types of searches provided by the on-
line interface are developed on top of this web ser-
vice. The user queries are translated into CQP
queries and passed to the web service. In many
cases, such as the free word order queries in the
simple and advanced search forms, more than one
CQP query is necessary to produce the desired
result. Other functionalities implemented in this
layer are the management of subcorpora and the
filtering by complexity. The results returned by
the web service are then formatted and presented
to the user.
The user interface as well as the mechanisms
for translation of queries from the web forms into
CQP have been developed server-side in PHP.
The visualizations are implemented client-side in
JavaScript and jQuery, the dependency graphs
based on the xLDD framework (Culy et al., 2011).
</bodyText>
<subsectionHeader confidence="0.999816">
5.4 Extraction of lexico-syntactic information
</subsectionHeader>
<bodyText confidence="0.99969535">
PAISA` is currently used in the CombiNet project
“Word Combinations in Italian – Theoretical and
descriptive analysis, computational models, lexi-
cographic layout and creation of a dictionary”.17
The project goal is to study the combinatory prop-
erties of Italian words by developing advanced
computational linguistics methods for extracting
distributional information from PAIS `A.
In particular, CombiNet uses a pattern-based
approach to extract a wide range of multiword
expressions, such as phrasal lexemes, colloca-
tions, and usual combinations. POS n-grams
are automatically extracted from PAIS `A, and then
ranked according to different types of associa-
tion measures (e.g., pointwise mutual informa-
tion, log-likelihood ratios, etc.). Extending the
LexIt methodology (Lenci et al., 2012), CombiNet
also extracts distributional profiles from the parsed
layer of PAIS `A, including the following types of
information:
</bodyText>
<listItem confidence="0.625111">
1. syntactic slots (subject, complements, modi-
</listItem>
<footnote confidence="0.9982656">
15To safeguard the system against malicious attacks, secu-
rity measures had to be taken at several of the layers, which
unfortunately also make some of the more advanced CQP fea-
tures inaccessible to the user.
16Web services based on REST (Representational State
Transfer) principles employ standard concepts such as a URI
and standard HTTP methods to provide an interface to func-
tionalities on a remote host.
173-year PRIN(2010/2011)-project, coordination by Raf-
faele Simone – University of Rome Tre
</footnote>
<page confidence="0.999173">
41
</page>
<bodyText confidence="0.884564">
fiers, etc.) and subcategorization frames;
</bodyText>
<listItem confidence="0.938876428571429">
2. lexical sets filling syntactic slots (e.g. proto-
typical subjects of a target verb);
3. semantic classes describing selectional pref-
erences of syntactic slots (e.g. the direct obj.
of mangiare/’to eat’ typically selects nouns
referring to food, while its subject selects an-
imate nouns); semantic roles of predicates.
</listItem>
<bodyText confidence="0.999863666666667">
The saliency and typicality of combinatory pat-
terns are weighted by means of different statisti-
cal indexes and the resulting profiles will be used
to define a distributional semantic classification of
Italian verbs, comparable to the one elaborated in
the VerbNet project (Kipper et al., 2008).
</bodyText>
<sectionHeader confidence="0.999645" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999996">
We performed post-crawl evaluations on the data.
For licensing, we analysed 200,534 pages that
were originally collected for the PAISA` corpus,
and only 1,060 were identified as containing no
CC license link (99.95% with CC mark-up). Then,
from 10,000 randomly selected non-CC-licensed
Italian pages 15 were wrongly identified as CC li-
censed containing CC mark-up (0.15% error). For
language identification we checked the harvested
corpus part with the CLD2 toolkit18, and &gt; 99%
of the data was identified as Italian.
The pos-tagger has been adapted to peculiari-
ties of the PAISA` web texts, by manually correct-
ing sample annotation output and re-training the
tagger accordingly. Following the active learning
approach as described in section 4.2 we built a new
pos-tagger model based on 40.000 manually re-
vised tokens. With the new model, we obtained
an improvement in accuracy of 1% on a test-set
of 5000 tokens extracted from PAIS `A. Final tag-
ger accuracy reached 96.03%.
</bodyText>
<sectionHeader confidence="0.995289" genericHeader="conclusions">
7 Conclusion / Future Work
</sectionHeader>
<bodyText confidence="0.998429888888889">
In this paper we showed how a contemporary and
free language resource of Italian with linguistic
annotations can be designed, implemented and de-
veloped from the web and made available for dif-
ferent types of language users.
Future work will focus on enriching the cor-
pus with metadata by means of automatic clas-
sification techniques, so as to make a better as-
sessment of corpus composition. A multi-faceted
</bodyText>
<footnote confidence="0.96443">
18Compact Language Detection 2, http://code.
google.com/p/cld2/
</footnote>
<bodyText confidence="0.999986230769231">
approach combining linguistic features extracted
from texts (content/function words ratio, sentence
length, word frequency, etc.) and information
extracted from document URLs (e.g., tags like
”wiki“, ”blog“) might be particularly suitable for
genre and function annotation.
Metadata annotation will enable more advanced
applications of the corpus for language teaching
and learning purposes. In this respect, existing
exemplifications of the use of the PAISA` inter-
face for language learning and teaching (Lyding et
al., 2013) could be followed by further pedagogi-
cal proposals as well as empowered by dedicated
teaching guidelines for the exploitation of the cor-
pus and its web interface in the class of Italian as
a second language.
In a more general perspective, we envisage
a tighter integration between acquisition of new
texts, automated text annotation and development
of lexical and language learning resources allow-
ing even non-specialised users to carve out and
develop their own language data. This ambitious
goal points in the direction of a fully-automatised
control of the entire life-cycle of open-access Ital-
ian language resources with a view to address an
increasingly wider range of potential demands.
</bodyText>
<sectionHeader confidence="0.994727" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996742">
The three years PAISA` project19, concluded in
January 2013, received funding from the Italian
Ministry of Education, Universities and Research
(MIUR)20, by the FIRB program (Fondo per gli
Investimenti della Ricerca di Base)21.
</bodyText>
<sectionHeader confidence="0.998599" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993769785714286">
G. Attardi, F. Dell’Orletta, M. Simi, and J. Turian.
2009. Accurate dependency parsing with a stacked
multilayer perceptron. In Proc. of Evalita’09, Eval-
uation of NLP and Speech Tools for Italian, Reggio
Emilia.
M. Baroni and S. Bernardini. 2004. Bootcat: Boot-
strapping corpora and terms from the web. In Proc.
of LREC 2004, pages 1313–1316. ELDA.
M. Baroni, S. Bernardini, A. Ferraresi, and
E. Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed
19An effort of four Italian research units: University of
Bologna, CNR Pisa, University of Trento and European
Academy of Bolzano/Bozen.
</reference>
<footnote confidence="0.952016666666667">
20http://www.istruzione.it/
21http://hubmiur.pubblica.istruzione.
it/web/ricerca/firb
</footnote>
<page confidence="0.997455">
42
</page>
<reference confidence="0.997551339622642">
web-crawled corpora. Journal of LRE, 43(3):209–
226.
C. Borghetti, S. Castagnoli, and M. Brunello. 2011. I
testi del web: una proposta di classificazione sulla
base del corpus pais`a. In M. Cerruti, E. Corino,
and C. Onesti, editors, Formale e informale. La vari-
azione di registro nella comunicazione elettronica.,
pages 147–170. Carocci, Roma.
M. Brunello. 2009. The creation of free linguistic cor-
pora from the web. In I. Alegria, I. Leturia, and
S. Sharoff, editors, Proc. of the Fifth Web as Corpus
Workshop (WAC5), pages 9–16. Elhuyar Fundazioa.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
Tenth Conf. Comput. Nat. Lang. Learn., number
June in CoNLL-X ’06, pages 149–164. Association
for Computational Linguistics.
C. Culy, V. Lyding, and H. Dittmann. 2011. xldd:
Extended linguistic dependency diagrams. In Proc.
of the 15th International Conference on Information
Visualisation IV2011, pages 164–169, London, UK.
T. De Mauro. 1991. Guida all’uso delle parole. Edi-
tori Riuniti, Roma.
F. Dell’Orletta, G. Venturi, and S. Montemagni. 2011.
Ulisse: an unsupervised algorithm for detecting re-
liable dependency parses. In Proc. of CoNLL 2011,
Conferences on Natural Language Learning, Port-
land, Oregon.
F. Dell’Orletta, G. Venturi, and S. Montemagni. 2013.
Unsupervised linguistically-driven reliable depen-
dency parses detection and self-training for adapta-
tion to the biomedical domain. In Proc. of BioNLP
2013, Workshop on Biomedical NLP, Sofia.
F. Dell’Orletta. 2009. Ensemble system for part-of-
speech tagging. In Proceedings of Evalita’09, Eval-
uation of NLP and Speech Tools for Italian, Reggio
Emilia.
S. Evert and A. Hardie. 2011. Twenty-first century
corpus workbench: Updating a query architecture
for the new millennium. In Proc. of the Corpus Lin-
guistics 2011, Birmingham, UK.
M. G´en´ereux, I. Hendrickx, and A. Mendes. 2012.
A large portuguese corpus on-line: Cleaning and
preprocessing. In PROPOR, volume 7243 of Lec-
ture Notes in Computer Science, pages 113–120.
Springer.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the special issue on the web as corpus. Computa-
tional Linguistics, 29(3):333–347.
K. Kipper, A. Korhonen, N. Ryant, and M. Palmer.
2008. A large-scale classification of english verbs.
Journal of LRE, 42:21–40.
A. Lenci, G. Lapesa, and G. Bonansinga. 2012. Lexit:
A computational resource on italian argument struc-
ture. In N. Calzolari, K. Choukri, T. Declerck,
M. U˘gur Do˘gan, B. Maegaard, J. Mariani, J. Odijk,
and S. Piperidis, editors, Proc. of LREC 2012, pages
3712–3718, Istanbul, Turkey, May. ELRA.
P. Lucisano and M. E. Piemontese. 1988. Gulpease:
una formula per la predizione della difficolt dei testi
in lingua italiana. Scuola e citt`a, 39(3):110–124.
V. Lyding, C. Borghetti, H. Dittmann, L. Nicolas, and
E. Stemle. 2013. Open corpus interface for italian
language learning. In Proc. of the ICTfor Language
Learning Conference, 6th Edition, Florence, Italy.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proc. of ACL 2006, ACL, Sydney.
S. Petrov and R. McDonald. 2012. Overview of the
2012 shared task on parsing the web. In Proc. of
SANCL 2012, First Workshop on Syntactic Analysis
of Non-Canonical Language, Montreal.
G. Rehm, M. Santini, A. Mehler, P. Braslavski,
R. Gleim, A. Stubbe, S. Symonenko, M. Tavosanis,
and V. Vidulin. 2008. Towards a reference corpus of
web genres for the evaluation of genre identification
systems. In Proc. of LREC 2008, pages 351–358,
Marrakech, Morocco.
M. Santini, A. Mehler, and S. Sharoff. 2010. Riding
the Rough Waves of Genre on the Web. Concepts
and Research Questions. In A. Mehler, S. Sharoff,
and M. Santini, editors, Genres on the Web: Compu-
tational Models and Empirical Studies., pages 3–33.
Springer, Dordrecht.
M. Santini. 2005. Genres in formation? an ex-
ploratory study of web pages using cluster analysis.
In Proc. of the 8th Annual Colloquium for the UK
Special Interest Group for Computational Linguis-
tics (CLUK05), Manchester, UK.
S. Sharoff. 2006. Creating General-Purpose Corpora
Using Automated Search Engine Queries. In M. Ba-
roni and S. Bernardini, editors, Wacky! Working
Papers on the Web as Corpus, pages 63–98. Gedit,
Bologna.
S. Sharoff. 2010. Analysing similarities and differ-
ences between corpora. In 7th Language Technolo-
gies Conference, Ljubljana.
J. M. Steger and E. W. Stemle. 2009. KrdWrd – The
Architecture for Unified Processing of Web Content.
In Proc. Fifth Web as Corpus Work., Donostia-San
Sebastian, Basque Country.
C. A. Thompson, M. E. Califf, and R. J. Mooney. 1999.
Active learning for natural language parsing and in-
formation extraction. In Proc. of ICML99, the Six-
teenth International Conference on Machine Learn-
ing, San Francisco, CA.
</reference>
<page confidence="0.999833">
43
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.448399">
<title confidence="0.8082895">of Italian Web Texts Lyding</title>
<abstract confidence="0.964784555555556">verena.lyding@eurac.edu egon.stemle@eurac.edu claudia.borghetti@unibo.it marcobrunello84@gmail.com s.castagnoli@unibo.it felice.dellorletta@ilc.cnr.it henrik.dittmann@bordet.be alessandro.lenci@ling.unipi.it vito.pirrelli@ilc.cnr.it Abstract a Creative Commons licensed, large web corpus of contemporary Italian. We describe the design, harvesting, and processing steps involved in its creation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Attardi</author>
<author>F Dell’Orletta</author>
<author>M Simi</author>
<author>J Turian</author>
</authors>
<title>Accurate dependency parsing with a stacked multilayer perceptron.</title>
<date>2009</date>
<booktitle>In Proc. of Evalita’09, Evaluation of NLP and Speech Tools for Italian,</booktitle>
<location>Reggio Emilia.</location>
<marker>Attardi, Dell’Orletta, Simi, Turian, 2009</marker>
<rawString>G. Attardi, F. Dell’Orletta, M. Simi, and J. Turian. 2009. Accurate dependency parsing with a stacked multilayer perceptron. In Proc. of Evalita’09, Evaluation of NLP and Speech Tools for Italian, Reggio Emilia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>S Bernardini</author>
</authors>
<title>Bootcat: Bootstrapping corpora and terms from the web. In</title>
<date>2004</date>
<booktitle>Proc. of LREC</booktitle>
<pages>1313--1316</pages>
<publisher>ELDA.</publisher>
<contexts>
<context position="3138" citStr="Baroni and Bernardini, 2004" startWordPosition="454" endWordPosition="457">in a carefully designed query interface, targeted at different user groups. In particular, the integration of richly annotated language content with an easily accessible, user-oriented interface makes PAISA` a unique and flexible resource for language teaching. 2 Related Work The world wide web, with its inexhaustible amount of natural language data, has become an established source for efficiently building large corpora (Kilgarriff and Grefenstette, 2003). Tools are available that make it convenient to bootstrap corpora from the web based on mere seed term lists, such as the BootCaT toolkit (Baroni and Bernardini, 2004). The huge corpora created by the WaCky project (Baroni et al., 2009) are an example of such an approach. A large number of papers have recently been published on the harvesting, cleaning and processing of web corpora.1 However, freely available, large, contemporary, linguistically annotated, easily accessible web corpora are still missing for many languages; but cf. e.g. (G´en´ereux et al., 2012) and the Common Crawl Foundations (CCF) web crawl2. 1cf. the Special Interest Group of the Association for Computational Linguistics on Web as Corpus (SIGWAC) http://sigwac.org.uk/ 2CCF produces and m</context>
</contexts>
<marker>Baroni, Bernardini, 2004</marker>
<rawString>M. Baroni and S. Bernardini. 2004. Bootcat: Bootstrapping corpora and terms from the web. In Proc. of LREC 2004, pages 1313–1316. ELDA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>S Bernardini</author>
<author>A Ferraresi</author>
<author>E Zanchetta</author>
</authors>
<title>The wacky wide web: A collection of very large linguistically processed 19An effort of four Italian research units:</title>
<date>2009</date>
<journal>Journal of LRE,</journal>
<volume>43</volume>
<issue>3</issue>
<pages>226</pages>
<institution>University of Bologna, CNR Pisa, University</institution>
<contexts>
<context position="3207" citStr="Baroni et al., 2009" startWordPosition="466" endWordPosition="469">n particular, the integration of richly annotated language content with an easily accessible, user-oriented interface makes PAISA` a unique and flexible resource for language teaching. 2 Related Work The world wide web, with its inexhaustible amount of natural language data, has become an established source for efficiently building large corpora (Kilgarriff and Grefenstette, 2003). Tools are available that make it convenient to bootstrap corpora from the web based on mere seed term lists, such as the BootCaT toolkit (Baroni and Bernardini, 2004). The huge corpora created by the WaCky project (Baroni et al., 2009) are an example of such an approach. A large number of papers have recently been published on the harvesting, cleaning and processing of web corpora.1 However, freely available, large, contemporary, linguistically annotated, easily accessible web corpora are still missing for many languages; but cf. e.g. (G´en´ereux et al., 2012) and the Common Crawl Foundations (CCF) web crawl2. 1cf. the Special Interest Group of the Association for Computational Linguistics on Web as Corpus (SIGWAC) http://sigwac.org.uk/ 2CCF produces and maintains a repository of web crawl data that is openly accessible: ht</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta. 2009. The wacky wide web: A collection of very large linguistically processed 19An effort of four Italian research units: University of Bologna, CNR Pisa, University of Trento and European Academy of Bolzano/Bozen. web-crawled corpora. Journal of LRE, 43(3):209– 226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Borghetti</author>
<author>S Castagnoli</author>
<author>M Brunello</author>
</authors>
<title>I testi del web: una proposta di classificazione sulla base del corpus pais`a.</title>
<date>2011</date>
<booktitle>Formale e informale. La variazione di registro nella comunicazione elettronica.,</booktitle>
<pages>147--170</pages>
<editor>In M. Cerruti, E. Corino, and C. Onesti, editors,</editor>
<location>Carocci, Roma.</location>
<contexts>
<context position="18157" citStr="Borghetti et al. (2011)" startWordPosition="2724" endWordPosition="2727">investigations. Drawing on existing literature on web genres (e.g. (Santini, 2005; Rehm et al., 2008; Santini et al., 2010)) and text classification according to text function and topic (e.g. (Sharoff, 2006)), we developed a tentative three-fold taxonomy to be used for text classification. Following four cycles of sample manual annotation by three annotators, categories were adjusted in order to better reflect the nature of PAIS `A’s web documents (cf. (Sharoff, 2010) about differences between domains covered in the BNC and in the web-derived ukWaC). Details about the taxonomy are provided in Borghetti et al. (2011). Then, we started to cross-check whether the devised taxonomy was indeed appropriate to describe PAIS `A’s composition by comparing its categories with data resulting from the application of unsupervised methods for text classification. Interesting insights have emerged so far regarding the topic category. Following Sharoff (2010), we used topic modelling based on Latent Dirichlet Allocation for the detection of topics: 20 clusters/topics were identified on the basis of keywords (the number of clusters to retrieve is a user-defined parameter) and projected onto the manually defined taxonomy. </context>
</contexts>
<marker>Borghetti, Castagnoli, Brunello, 2011</marker>
<rawString>C. Borghetti, S. Castagnoli, and M. Brunello. 2011. I testi del web: una proposta di classificazione sulla base del corpus pais`a. In M. Cerruti, E. Corino, and C. Onesti, editors, Formale e informale. La variazione di registro nella comunicazione elettronica., pages 147–170. Carocci, Roma.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brunello</author>
</authors>
<title>The creation of free linguistic corpora from the web. In</title>
<date>2009</date>
<booktitle>Proc. of the Fifth Web as Corpus Workshop (WAC5),</booktitle>
<pages>9--16</pages>
<editor>I. Alegria, I. Leturia, and S. Sharoff, editors,</editor>
<publisher>Elhuyar Fundazioa.</publisher>
<contexts>
<context position="5619" citStr="Brunello, 2009" startWordPosition="850" endWordPosition="851"> without encountering legal copyright issues is to collect texts that are in the public domain or at least, have been made available in a copyleft regime. This is the case when the author of a certain document decided to share some rights (copy and/or distribute, adapt etc.) on her work with the public, in a way that end users do not need to ask permission to the creator/owner of the original work. This is possible by employing licenses other than the traditional “all right reserved” copyright, i.e. GNU, Creative Commons etc., which found a wide use especially on the web. Exploratory studies (Brunello, 2009) have shown that Creative Commons licenses are widely employed throughout the web (at least on the Italian webspace), enough to consider the possibility to build a large corpus from the web exclusively made of documents released under such licenses. In particular, Creative Commons provides five basic “baseline rights”: Attribution (BY), Share Alike (SA), Non Commercial (NC), No Derivative Works (ND). The licenses themselves are composed of at least Attribution (which can be used even alone) plus the other elements, allowing six different combinations:3 (1) Attribution (CC BY), (2) Attribution-</context>
</contexts>
<marker>Brunello, 2009</marker>
<rawString>M. Brunello. 2009. The creation of free linguistic corpora from the web. In I. Alegria, I. Leturia, and S. Sharoff, editors, Proc. of the Fifth Web as Corpus Workshop (WAC5), pages 9–16. Elhuyar Fundazioa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X Shared Task on Multilingual Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proc. Tenth Conf. Comput. Nat. Lang. Learn., number June in CoNLL-X ’06,</booktitle>
<pages>149--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8898" citStr="Buchholz and Marsi, 2006" startWordPosition="1316" endWordPosition="1319">y, readability parameters, etc. (see section 4.3). Also, each document is marked with a unique identifier. 4The European Constitution from wikisource.org: http://it.wikisource.org/wiki/Trattato_ che_adotta_una_Costituzione_per_l’Europa 37 Document source Avg text length PAISA` total 683 words Wikipedia 693 words Wikibooks 1844 words guide.supereva.it 378 words italy.indymedia.it 1147 words tvblog.it 1472 words motoblog.it 421 words ecowebnews.it 347 words webmasterpoint.org 332 words Table 1: Average text length by source The annotated corpus adheres to the standard CoNLL column-based format (Buchholz and Marsi, 2006), is encoded in UTF-8. 4 Corpus Creation 4.1 Collecting and cleaning web data The web pages for PAISA` were selected in two ways: part of the corpus collection was made through CC-focused web crawling, and another part through a targeted collection of documents from specific websites. 4.1.1 Seed-term based harvesting At the time of corpus collection (2010), we used the BootCaT toolkit mainly because collecting URLs could be based on the public Yahoo! search API5, including the option to restrict search to CClicensed pages (including the possibility to specify even the particular licenses). Unf</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X Shared Task on Multilingual Dependency Parsing. In Proc. Tenth Conf. Comput. Nat. Lang. Learn., number June in CoNLL-X ’06, pages 149–164. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Culy</author>
<author>V Lyding</author>
<author>H Dittmann</author>
</authors>
<title>xldd: Extended linguistic dependency diagrams.</title>
<date>2011</date>
<booktitle>In Proc. of the 15th International Conference on Information Visualisation IV2011,</booktitle>
<pages>164--169</pages>
<location>London, UK.</location>
<contexts>
<context position="22739" citStr="Culy et al., 2011" startWordPosition="3448" endWordPosition="3451">ponent. It serves the purpose of retrieving full-text corpus documents based on keyword searches as well as text statistics (see section 4.3). Like the CQP interface, the filter interface is also supporting the building of temporary subcorpora for subsequent querying. By default, search results are displayed as KWIC (KeyWord In Context) lines, centred around the search expression. Each search hit can be expanded to its full sentence view. In addition, the originating full text document can be accessed and its source URL is provided. Based on an interactive visualisation for dependency graphs (Culy et al., 2011) for each search result a graphical representations of dependency relations together with the sentence and associated lemma and part-of-speech information can be generated (see Figure 1). Figure 1: Dependency diagram Targeted at novice language learners of Italian, a filter for automatically restricting search results to sentences of limited complexity has been integrated into each search component. When activated, search results are automatically filtered based on a combination of the complexity measures introduced in section 4.3. 5.3 Technical details The PAISA` online interface has been dev</context>
<context position="24590" citStr="Culy et al., 2011" startWordPosition="3749" endWordPosition="3752">he free word order queries in the simple and advanced search forms, more than one CQP query is necessary to produce the desired result. Other functionalities implemented in this layer are the management of subcorpora and the filtering by complexity. The results returned by the web service are then formatted and presented to the user. The user interface as well as the mechanisms for translation of queries from the web forms into CQP have been developed server-side in PHP. The visualizations are implemented client-side in JavaScript and jQuery, the dependency graphs based on the xLDD framework (Culy et al., 2011). 5.4 Extraction of lexico-syntactic information PAISA` is currently used in the CombiNet project “Word Combinations in Italian – Theoretical and descriptive analysis, computational models, lexicographic layout and creation of a dictionary”.17 The project goal is to study the combinatory properties of Italian words by developing advanced computational linguistics methods for extracting distributional information from PAIS `A. In particular, CombiNet uses a pattern-based approach to extract a wide range of multiword expressions, such as phrasal lexemes, collocations, and usual combinations. POS</context>
</contexts>
<marker>Culy, Lyding, Dittmann, 2011</marker>
<rawString>C. Culy, V. Lyding, and H. Dittmann. 2011. xldd: Extended linguistic dependency diagrams. In Proc. of the 15th International Conference on Information Visualisation IV2011, pages 164–169, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T De Mauro</author>
</authors>
<title>Guida all’uso delle parole. Editori Riuniti,</title>
<date>1991</date>
<location>Roma.</location>
<marker>De Mauro, 1991</marker>
<rawString>T. De Mauro. 1991. Guida all’uso delle parole. Editori Riuniti, Roma.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Dell’Orletta</author>
<author>G Venturi</author>
<author>S Montemagni</author>
</authors>
<title>Ulisse: an unsupervised algorithm for detecting reliable dependency parses.</title>
<date>2011</date>
<booktitle>In Proc. of CoNLL 2011, Conferences on Natural Language Learning,</booktitle>
<location>Portland, Oregon.</location>
<marker>Dell’Orletta, Venturi, Montemagni, 2011</marker>
<rawString>F. Dell’Orletta, G. Venturi, and S. Montemagni. 2011. Ulisse: an unsupervised algorithm for detecting reliable dependency parses. In Proc. of CoNLL 2011, Conferences on Natural Language Learning, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Dell’Orletta</author>
<author>G Venturi</author>
<author>S Montemagni</author>
</authors>
<title>Unsupervised linguistically-driven reliable dependency parses detection and self-training for adaptation to the biomedical domain.</title>
<date>2013</date>
<booktitle>In Proc. of BioNLP 2013, Workshop on Biomedical NLP,</booktitle>
<location>Sofia.</location>
<marker>Dell’Orletta, Venturi, Montemagni, 2013</marker>
<rawString>F. Dell’Orletta, G. Venturi, and S. Montemagni. 2013. Unsupervised linguistically-driven reliable dependency parses detection and self-training for adaptation to the biomedical domain. In Proc. of BioNLP 2013, Workshop on Biomedical NLP, Sofia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Dell’Orletta</author>
</authors>
<title>Ensemble system for part-ofspeech tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of Evalita’09, Evaluation of NLP and Speech Tools for Italian,</booktitle>
<location>Reggio Emilia.</location>
<marker>Dell’Orletta, 2009</marker>
<rawString>F. Dell’Orletta. 2009. Ensemble system for part-ofspeech tagging. In Proceedings of Evalita’09, Evaluation of NLP and Speech Tools for Italian, Reggio Emilia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Evert</author>
<author>A Hardie</author>
</authors>
<title>Twenty-first century corpus workbench: Updating a query architecture for the new millennium.</title>
<date>2011</date>
<booktitle>In Proc. of the Corpus Linguistics 2011,</booktitle>
<location>Birmingham, UK.</location>
<contexts>
<context position="21934" citStr="Evert and Hardie, 2011" startWordPosition="3319" endWordPosition="3322">ch that adopts a ’Google-style’ search box. Single search terms, as well as multi-word combinations or sequences can be searched by inserting them in a simple text box. The second component is an advanced graphical search form. It provides elaborated search options for querying linguistic annotation layers and allows for defining distances between search terms as well as repetitions or optionally occurring terms. Furthermore, the advanced search supports regular expressions. The third component emulates a command-line search via the powerful CQP query language of 40 the Open Corpus Workbench (Evert and Hardie, 2011). It allows for complex search queries in CQP syntax that rely on linguistic annotation layers as well as on metadata information. Finally, a filter interface is presented in a fourth component. It serves the purpose of retrieving full-text corpus documents based on keyword searches as well as text statistics (see section 4.3). Like the CQP interface, the filter interface is also supporting the building of temporary subcorpora for subsequent querying. By default, search results are displayed as KWIC (KeyWord In Context) lines, centred around the search expression. Each search hit can be expand</context>
<context position="23475" citStr="Evert and Hardie, 2011" startWordPosition="3560" endWordPosition="3563">ated lemma and part-of-speech information can be generated (see Figure 1). Figure 1: Dependency diagram Targeted at novice language learners of Italian, a filter for automatically restricting search results to sentences of limited complexity has been integrated into each search component. When activated, search results are automatically filtered based on a combination of the complexity measures introduced in section 4.3. 5.3 Technical details The PAISA` online interface has been developed in several layers: in essence, it provides a front-end to the corpus as indexed in Open Corpus Workbench (Evert and Hardie, 2011). This corpus query engine provides the fundamental search capabilities through the CQP language. Based on the CWB/Perl API that is part of the Open Corpus Workbench package, a web service has been developed at EURAC which exposes a large part of the CQP language15 through a RESTful API.16 The four types of searches provided by the online interface are developed on top of this web service. The user queries are translated into CQP queries and passed to the web service. In many cases, such as the free word order queries in the simple and advanced search forms, more than one CQP query is necessar</context>
</contexts>
<marker>Evert, Hardie, 2011</marker>
<rawString>S. Evert and A. Hardie. 2011. Twenty-first century corpus workbench: Updating a query architecture for the new millennium. In Proc. of the Corpus Linguistics 2011, Birmingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G´en´ereux</author>
<author>I Hendrickx</author>
<author>A Mendes</author>
</authors>
<title>A large portuguese corpus on-line: Cleaning and preprocessing.</title>
<date>2012</date>
<booktitle>In PROPOR,</booktitle>
<volume>7243</volume>
<pages>113--120</pages>
<publisher>Springer.</publisher>
<marker>G´en´ereux, Hendrickx, Mendes, 2012</marker>
<rawString>M. G´en´ereux, I. Hendrickx, and A. Mendes. 2012. A large portuguese corpus on-line: Cleaning and preprocessing. In PROPOR, volume 7243 of Lecture Notes in Computer Science, pages 113–120. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>G Grefenstette</author>
</authors>
<title>Introduction to the special issue on the web as corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="2970" citStr="Kilgarriff and Grefenstette, 2003" startWordPosition="426" endWordPosition="429">exclusively draws on Creative Commons licensed data, provides advanced linguistic annotations with respect to corpora of comparable size and corpora of web data, and invests in a carefully designed query interface, targeted at different user groups. In particular, the integration of richly annotated language content with an easily accessible, user-oriented interface makes PAISA` a unique and flexible resource for language teaching. 2 Related Work The world wide web, with its inexhaustible amount of natural language data, has become an established source for efficiently building large corpora (Kilgarriff and Grefenstette, 2003). Tools are available that make it convenient to bootstrap corpora from the web based on mere seed term lists, such as the BootCaT toolkit (Baroni and Bernardini, 2004). The huge corpora created by the WaCky project (Baroni et al., 2009) are an example of such an approach. A large number of papers have recently been published on the harvesting, cleaning and processing of web corpora.1 However, freely available, large, contemporary, linguistically annotated, easily accessible web corpora are still missing for many languages; but cf. e.g. (G´en´ereux et al., 2012) and the Common Crawl Foundation</context>
</contexts>
<marker>Kilgarriff, Grefenstette, 2003</marker>
<rawString>A. Kilgarriff and G. Grefenstette. 2003. Introduction to the special issue on the web as corpus. Computational Linguistics, 29(3):333–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
<author>A Korhonen</author>
<author>N Ryant</author>
<author>M Palmer</author>
</authors>
<title>A large-scale classification of english verbs.</title>
<date>2008</date>
<journal>Journal of LRE,</journal>
<pages>42--21</pages>
<contexts>
<context position="26761" citStr="Kipper et al., 2008" startWordPosition="4068" endWordPosition="4071">frames; 2. lexical sets filling syntactic slots (e.g. prototypical subjects of a target verb); 3. semantic classes describing selectional preferences of syntactic slots (e.g. the direct obj. of mangiare/’to eat’ typically selects nouns referring to food, while its subject selects animate nouns); semantic roles of predicates. The saliency and typicality of combinatory patterns are weighted by means of different statistical indexes and the resulting profiles will be used to define a distributional semantic classification of Italian verbs, comparable to the one elaborated in the VerbNet project (Kipper et al., 2008). 6 Evaluation We performed post-crawl evaluations on the data. For licensing, we analysed 200,534 pages that were originally collected for the PAISA` corpus, and only 1,060 were identified as containing no CC license link (99.95% with CC mark-up). Then, from 10,000 randomly selected non-CC-licensed Italian pages 15 were wrongly identified as CC licensed containing CC mark-up (0.15% error). For language identification we checked the harvested corpus part with the CLD2 toolkit18, and &gt; 99% of the data was identified as Italian. The pos-tagger has been adapted to peculiarities of the PAISA` web </context>
</contexts>
<marker>Kipper, Korhonen, Ryant, Palmer, 2008</marker>
<rawString>K. Kipper, A. Korhonen, N. Ryant, and M. Palmer. 2008. A large-scale classification of english verbs. Journal of LRE, 42:21–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lenci</author>
<author>G Lapesa</author>
<author>G Bonansinga</author>
</authors>
<title>Lexit: A computational resource on italian argument structure.</title>
<date>2012</date>
<booktitle>Proc. of LREC 2012,</booktitle>
<pages>3712--3718</pages>
<editor>In N. Calzolari, K. Choukri, T. Declerck, M. U˘gur Do˘gan, B. Maegaard, J. Mariani, J. Odijk, and S. Piperidis, editors,</editor>
<publisher>ELRA.</publisher>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="25429" citStr="Lenci et al., 2012" startWordPosition="3866" endWordPosition="3869">ion of a dictionary”.17 The project goal is to study the combinatory properties of Italian words by developing advanced computational linguistics methods for extracting distributional information from PAIS `A. In particular, CombiNet uses a pattern-based approach to extract a wide range of multiword expressions, such as phrasal lexemes, collocations, and usual combinations. POS n-grams are automatically extracted from PAIS `A, and then ranked according to different types of association measures (e.g., pointwise mutual information, log-likelihood ratios, etc.). Extending the LexIt methodology (Lenci et al., 2012), CombiNet also extracts distributional profiles from the parsed layer of PAIS `A, including the following types of information: 1. syntactic slots (subject, complements, modi15To safeguard the system against malicious attacks, security measures had to be taken at several of the layers, which unfortunately also make some of the more advanced CQP features inaccessible to the user. 16Web services based on REST (Representational State Transfer) principles employ standard concepts such as a URI and standard HTTP methods to provide an interface to functionalities on a remote host. 173-year PRIN(201</context>
</contexts>
<marker>Lenci, Lapesa, Bonansinga, 2012</marker>
<rawString>A. Lenci, G. Lapesa, and G. Bonansinga. 2012. Lexit: A computational resource on italian argument structure. In N. Calzolari, K. Choukri, T. Declerck, M. U˘gur Do˘gan, B. Maegaard, J. Mariani, J. Odijk, and S. Piperidis, editors, Proc. of LREC 2012, pages 3712–3718, Istanbul, Turkey, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lucisano</author>
<author>M E Piemontese</author>
</authors>
<title>Gulpease: una formula per la predizione della difficolt dei testi in lingua italiana.</title>
<date>1988</date>
<booktitle>Scuola e citt`a,</booktitle>
<pages>39--3</pages>
<contexts>
<context position="15906" citStr="Lucisano and Piemontese, 1988" startWordPosition="2380" endWordPosition="2383">, we calculated several text statistics indicative of the linguistic complexity, or ’readability’ of a text. The applied measures include, (1) text length in tokens, that is the number of tokens per text, (2) sentences per text, that is a sentence count, and (3) type-token ratio indicated as a percentage value. In addition, we calculated (4) the advanced vocabulary per text, that is a word count of the text vocabulary which is not part of the the basic Italian vocabulary (’vocabolario di base’) for written texts, as defined by De Mauro (1991)13, and (5) the Gulpease Index (’Indice Gulpease’) (Lucisano and Piemontese, 1988), which is a measure for the readability of text that is based on frequency relations between the number of sentences, words and letters of a text. All values are encoded as metadata for the corpus. Via the PAISA` online interface, they can be employed for filtering documents and building subcorpora. This facility was implemented with the principal target group of PAISA` users in mind, as the selection of language examples according to their readability level is particularly relevant for language learning and teaching. 4.4 Attempts at text classification for genre, topic, and function Lack of </context>
</contexts>
<marker>Lucisano, Piemontese, 1988</marker>
<rawString>P. Lucisano and M. E. Piemontese. 1988. Gulpease: una formula per la predizione della difficolt dei testi in lingua italiana. Scuola e citt`a, 39(3):110–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lyding</author>
<author>C Borghetti</author>
<author>H Dittmann</author>
<author>L Nicolas</author>
<author>E Stemle</author>
</authors>
<title>Open corpus interface for italian language learning.</title>
<date>2013</date>
<booktitle>In Proc. of the ICTfor Language Learning Conference, 6th Edition,</booktitle>
<location>Florence, Italy.</location>
<contexts>
<context position="28775" citStr="Lyding et al., 2013" startWordPosition="4378" endWordPosition="4381"> composition. A multi-faceted 18Compact Language Detection 2, http://code. google.com/p/cld2/ approach combining linguistic features extracted from texts (content/function words ratio, sentence length, word frequency, etc.) and information extracted from document URLs (e.g., tags like ”wiki“, ”blog“) might be particularly suitable for genre and function annotation. Metadata annotation will enable more advanced applications of the corpus for language teaching and learning purposes. In this respect, existing exemplifications of the use of the PAISA` interface for language learning and teaching (Lyding et al., 2013) could be followed by further pedagogical proposals as well as empowered by dedicated teaching guidelines for the exploitation of the corpus and its web interface in the class of Italian as a second language. In a more general perspective, we envisage a tighter integration between acquisition of new texts, automated text annotation and development of lexical and language learning resources allowing even non-specialised users to carve out and develop their own language data. This ambitious goal points in the direction of a fully-automatised control of the entire life-cycle of open-access Italia</context>
</contexts>
<marker>Lyding, Borghetti, Dittmann, Nicolas, Stemle, 2013</marker>
<rawString>V. Lyding, C. Borghetti, H. Dittmann, L. Nicolas, and E. Stemle. 2013. Open corpus interface for italian language learning. In Proc. of the ICTfor Language Learning Conference, 6th Edition, Florence, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL</booktitle>
<location>ACL, Sydney.</location>
<contexts>
<context position="13059" citStr="McClosky et al., 2006" startWordPosition="1918" endWordPosition="1921">-DEPtagset.pdf 38 lexical and syntactic structures of non-canonical languages such as the language of social media, blogs, forum posts, consumer reviews, etc. As reported in Petrov and McDonald (2012), there are multiple reasons why parsing the web texts is difficult: punctuation and capitalization are often inconsistent, there is a lexical shift due to increased use of slang and technical jargon, some syntactic constructions are more frequent in web text than in newswire, etc. In order to overcome this problem, two main typologies of methods and techniques have been developed: Self-training (McClosky et al., 2006) and Active Learning (Thompson et al., 1999). For the specific purpose of the NLP tools adaptation to the Italian web texts, we adopted two different strategies for the pos-tagger and the parser. For what concerns pos-tagging, we used an active learning approach: given a subset of automatically pos-tagged sentences of PAIS `A, we selected the ones with the lowest likelihood, where the sentence likelihood was computed as the product of the probabilities of the assignments of the postagger for all the tokens. These sentences were manually revised and added to the training corpus in order to buil</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006. Reranking and self-training for parser adaptation. In Proc. of ACL 2006, ACL, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>R McDonald</author>
</authors>
<title>Overview of the 2012 shared task on parsing the web.</title>
<date>2012</date>
<booktitle>In Proc. of SANCL 2012, First Workshop on Syntactic Analysis of Non-Canonical Language,</booktitle>
<location>Montreal.</location>
<contexts>
<context position="12637" citStr="Petrov and McDonald (2012)" startWordPosition="1849" endWordPosition="1852">ged that statistical NLP tools have a drop of accuracy when tested against corpora differing from the typology of texts on which they were trained. This also holds true for PAIS `A: it contains 8Previously under GNU Free Documentation License. 9http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor 10http://dumps.wikimedia.org/ 11http://www.italianlp.it/docs/ ISST-TANL-POStagset.pdf 12http://www.italianlp.it/docs/ ISST-TANL-DEPtagset.pdf 38 lexical and syntactic structures of non-canonical languages such as the language of social media, blogs, forum posts, consumer reviews, etc. As reported in Petrov and McDonald (2012), there are multiple reasons why parsing the web texts is difficult: punctuation and capitalization are often inconsistent, there is a lexical shift due to increased use of slang and technical jargon, some syntactic constructions are more frequent in web text than in newswire, etc. In order to overcome this problem, two main typologies of methods and techniques have been developed: Self-training (McClosky et al., 2006) and Active Learning (Thompson et al., 1999). For the specific purpose of the NLP tools adaptation to the Italian web texts, we adopted two different strategies for the pos-tagge</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>S. Petrov and R. McDonald. 2012. Overview of the 2012 shared task on parsing the web. In Proc. of SANCL 2012, First Workshop on Syntactic Analysis of Non-Canonical Language, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Rehm</author>
<author>M Santini</author>
<author>A Mehler</author>
<author>P Braslavski</author>
<author>R Gleim</author>
<author>A Stubbe</author>
<author>S Symonenko</author>
<author>M Tavosanis</author>
<author>V Vidulin</author>
</authors>
<title>Towards a reference corpus of web genres for the evaluation of genre identification systems.</title>
<date>2008</date>
<booktitle>In Proc. of LREC</booktitle>
<pages>351--358</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="17634" citStr="Rehm et al., 2008" startWordPosition="2642" endWordPosition="2645">) and ’vocabolario di alto uso’ (http://it.wikipedia.org/wiki/ Vocabolario_di_alto_uso), together with high frequent function words not contained in those two lists. 39 pecially for the harvested14 subcorpus that was downloaded as described in section 4.1. We therefore carried out some experiments with the ultimate aim to enrich the corpus with metadata about text genre, topic and function, using automated techniques. In order to gain some insights into the composition of PAIS `A, we first conducted some manual investigations. Drawing on existing literature on web genres (e.g. (Santini, 2005; Rehm et al., 2008; Santini et al., 2010)) and text classification according to text function and topic (e.g. (Sharoff, 2006)), we developed a tentative three-fold taxonomy to be used for text classification. Following four cycles of sample manual annotation by three annotators, categories were adjusted in order to better reflect the nature of PAIS `A’s web documents (cf. (Sharoff, 2010) about differences between domains covered in the BNC and in the web-derived ukWaC). Details about the taxonomy are provided in Borghetti et al. (2011). Then, we started to cross-check whether the devised taxonomy was indeed app</context>
</contexts>
<marker>Rehm, Santini, Mehler, Braslavski, Gleim, Stubbe, Symonenko, Tavosanis, Vidulin, 2008</marker>
<rawString>G. Rehm, M. Santini, A. Mehler, P. Braslavski, R. Gleim, A. Stubbe, S. Symonenko, M. Tavosanis, and V. Vidulin. 2008. Towards a reference corpus of web genres for the evaluation of genre identification systems. In Proc. of LREC 2008, pages 351–358, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Santini</author>
<author>A Mehler</author>
<author>S Sharoff</author>
</authors>
<title>Riding the Rough Waves of Genre on the Web. Concepts and Research Questions. In</title>
<date>2010</date>
<booktitle>Genres on the Web: Computational Models and Empirical Studies.,</booktitle>
<pages>3--33</pages>
<editor>A. Mehler, S. Sharoff, and M. Santini, editors,</editor>
<publisher>Springer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="17657" citStr="Santini et al., 2010" startWordPosition="2646" endWordPosition="2649">di alto uso’ (http://it.wikipedia.org/wiki/ Vocabolario_di_alto_uso), together with high frequent function words not contained in those two lists. 39 pecially for the harvested14 subcorpus that was downloaded as described in section 4.1. We therefore carried out some experiments with the ultimate aim to enrich the corpus with metadata about text genre, topic and function, using automated techniques. In order to gain some insights into the composition of PAIS `A, we first conducted some manual investigations. Drawing on existing literature on web genres (e.g. (Santini, 2005; Rehm et al., 2008; Santini et al., 2010)) and text classification according to text function and topic (e.g. (Sharoff, 2006)), we developed a tentative three-fold taxonomy to be used for text classification. Following four cycles of sample manual annotation by three annotators, categories were adjusted in order to better reflect the nature of PAIS `A’s web documents (cf. (Sharoff, 2010) about differences between domains covered in the BNC and in the web-derived ukWaC). Details about the taxonomy are provided in Borghetti et al. (2011). Then, we started to cross-check whether the devised taxonomy was indeed appropriate to describe PA</context>
<context position="19663" citStr="Santini et al., 2010" startWordPosition="2963" endWordPosition="2966">’family’) or credere/sentire/sperare (’to believe’/’feel’/’hope’), which may in fact be indicative of genres such as diary or personal comment (e.g. personal blog). Only one of the categories originally included in the taxonomy – natural sciences – was not represented in the clusters, which may indicate that there are few texts within PAISA` belonging to this domain. One of the ma14In fact, even the nature of the targeted texts is not precisely defined: for instance, Wikipedia articles can actually encompass a variety of text types such as biographies, introductions to academic theories etc. (Santini et al., 2010, p. 15) jor advantages of topic models is that each corpus document can be associated – to varying degrees – to several topics/clusters: if encoded as metadata, this information makes it possible not only to filter texts according to their prevailing domain, but also to represent the heterogeneous nature of many web documents. 5 Corpus Access and Usage 5.1 Corpus distribution The PAISA` corpus is distributed in two ways: it is made available for download and it can be queried via its online interface. For both cases, no restrictions on its usage apply other than those defined by the Creative </context>
</contexts>
<marker>Santini, Mehler, Sharoff, 2010</marker>
<rawString>M. Santini, A. Mehler, and S. Sharoff. 2010. Riding the Rough Waves of Genre on the Web. Concepts and Research Questions. In A. Mehler, S. Sharoff, and M. Santini, editors, Genres on the Web: Computational Models and Empirical Studies., pages 3–33. Springer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Santini</author>
</authors>
<title>Genres in formation? an exploratory study of web pages using cluster analysis.</title>
<date>2005</date>
<booktitle>In Proc. of the 8th Annual Colloquium for the UK Special Interest Group for Computational Linguistics (CLUK05),</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="17615" citStr="Santini, 2005" startWordPosition="2640" endWordPosition="2641">io_fondamentale) and ’vocabolario di alto uso’ (http://it.wikipedia.org/wiki/ Vocabolario_di_alto_uso), together with high frequent function words not contained in those two lists. 39 pecially for the harvested14 subcorpus that was downloaded as described in section 4.1. We therefore carried out some experiments with the ultimate aim to enrich the corpus with metadata about text genre, topic and function, using automated techniques. In order to gain some insights into the composition of PAIS `A, we first conducted some manual investigations. Drawing on existing literature on web genres (e.g. (Santini, 2005; Rehm et al., 2008; Santini et al., 2010)) and text classification according to text function and topic (e.g. (Sharoff, 2006)), we developed a tentative three-fold taxonomy to be used for text classification. Following four cycles of sample manual annotation by three annotators, categories were adjusted in order to better reflect the nature of PAIS `A’s web documents (cf. (Sharoff, 2010) about differences between domains covered in the BNC and in the web-derived ukWaC). Details about the taxonomy are provided in Borghetti et al. (2011). Then, we started to cross-check whether the devised taxo</context>
</contexts>
<marker>Santini, 2005</marker>
<rawString>M. Santini. 2005. Genres in formation? an exploratory study of web pages using cluster analysis. In Proc. of the 8th Annual Colloquium for the UK Special Interest Group for Computational Linguistics (CLUK05), Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sharoff</author>
</authors>
<title>Creating General-Purpose Corpora Using Automated Search Engine Queries.</title>
<date>2006</date>
<booktitle>Working Papers on the Web as Corpus,</booktitle>
<pages>63--98</pages>
<editor>In M. Baroni and S. Bernardini, editors, Wacky!</editor>
<publisher>Gedit,</publisher>
<location>Bologna.</location>
<contexts>
<context position="17741" citStr="Sharoff, 2006" startWordPosition="2660" endWordPosition="2661">equent function words not contained in those two lists. 39 pecially for the harvested14 subcorpus that was downloaded as described in section 4.1. We therefore carried out some experiments with the ultimate aim to enrich the corpus with metadata about text genre, topic and function, using automated techniques. In order to gain some insights into the composition of PAIS `A, we first conducted some manual investigations. Drawing on existing literature on web genres (e.g. (Santini, 2005; Rehm et al., 2008; Santini et al., 2010)) and text classification according to text function and topic (e.g. (Sharoff, 2006)), we developed a tentative three-fold taxonomy to be used for text classification. Following four cycles of sample manual annotation by three annotators, categories were adjusted in order to better reflect the nature of PAIS `A’s web documents (cf. (Sharoff, 2010) about differences between domains covered in the BNC and in the web-derived ukWaC). Details about the taxonomy are provided in Borghetti et al. (2011). Then, we started to cross-check whether the devised taxonomy was indeed appropriate to describe PAIS `A’s composition by comparing its categories with data resulting from the applica</context>
</contexts>
<marker>Sharoff, 2006</marker>
<rawString>S. Sharoff. 2006. Creating General-Purpose Corpora Using Automated Search Engine Queries. In M. Baroni and S. Bernardini, editors, Wacky! Working Papers on the Web as Corpus, pages 63–98. Gedit, Bologna.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sharoff</author>
</authors>
<title>Analysing similarities and differences between corpora.</title>
<date>2010</date>
<booktitle>In 7th Language Technologies Conference,</booktitle>
<location>Ljubljana.</location>
<contexts>
<context position="18006" citStr="Sharoff, 2010" startWordPosition="2701" endWordPosition="2702">opic and function, using automated techniques. In order to gain some insights into the composition of PAIS `A, we first conducted some manual investigations. Drawing on existing literature on web genres (e.g. (Santini, 2005; Rehm et al., 2008; Santini et al., 2010)) and text classification according to text function and topic (e.g. (Sharoff, 2006)), we developed a tentative three-fold taxonomy to be used for text classification. Following four cycles of sample manual annotation by three annotators, categories were adjusted in order to better reflect the nature of PAIS `A’s web documents (cf. (Sharoff, 2010) about differences between domains covered in the BNC and in the web-derived ukWaC). Details about the taxonomy are provided in Borghetti et al. (2011). Then, we started to cross-check whether the devised taxonomy was indeed appropriate to describe PAIS `A’s composition by comparing its categories with data resulting from the application of unsupervised methods for text classification. Interesting insights have emerged so far regarding the topic category. Following Sharoff (2010), we used topic modelling based on Latent Dirichlet Allocation for the detection of topics: 20 clusters/topics were </context>
</contexts>
<marker>Sharoff, 2010</marker>
<rawString>S. Sharoff. 2010. Analysing similarities and differences between corpora. In 7th Language Technologies Conference, Ljubljana.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Steger</author>
<author>E W Stemle</author>
</authors>
<title>KrdWrd – The Architecture for Unified Processing of Web Content.</title>
<date>2009</date>
<booktitle>In Proc. Fifth Web as Corpus Work.,</booktitle>
<location>Donostia-San Sebastian, Basque Country.</location>
<contexts>
<context position="10598" citStr="Steger and Stemle, 2009" startWordPosition="1567" endWordPosition="1570">ic Italian vocabulary list7, in order to get a wider distribution of search queries, and, ultimately, of texts. As introduced in section 3.2, we restricted the selection not just to Creative Commons-licensed 5http://developer.yahoo.com/boss/ 6http://wacky.sslmit.unibo.it/doku. php?id=seed_words_and_tuples 7http://ppbm.paravia.it/dib_lemmario. php texts, but specifically to those licenses allowing redistribution: namely, CC BY, CC BY-SA, CC BY-NC-SA, and CC BY-NC. Results were downloaded and automatically cleaned with the KrdWrd system, an environment for the unified processing of web content (Steger and Stemle, 2009). Wrongly CC-tagged pages were eliminated using a black-list that had been manually populated following inspection of earlier corpus versions. 4.1.2 Targeted In September 2009, the Wikimedia Foundation decided to release the content of their wikis under CC BY-SA8, so we decided to download the large and varied amount of texts made available through the Italian versions of these websites. This was done using the Wikipedia Extractor9 on official dumps10 of Wikipedia, Wikinews, Wikisource, Wikibooks, Wikiversity and Wikivoyage. 4.2 Linguistic annotation and tools adaptation The corpus was automat</context>
</contexts>
<marker>Steger, Stemle, 2009</marker>
<rawString>J. M. Steger and E. W. Stemle. 2009. KrdWrd – The Architecture for Unified Processing of Web Content. In Proc. Fifth Web as Corpus Work., Donostia-San Sebastian, Basque Country.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C A Thompson</author>
<author>M E Califf</author>
<author>R J Mooney</author>
</authors>
<title>Active learning for natural language parsing and information extraction.</title>
<date>1999</date>
<booktitle>In Proc. of ICML99, the Sixteenth International Conference on Machine Learning,</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="13103" citStr="Thompson et al., 1999" startWordPosition="1925" endWordPosition="1928">ctures of non-canonical languages such as the language of social media, blogs, forum posts, consumer reviews, etc. As reported in Petrov and McDonald (2012), there are multiple reasons why parsing the web texts is difficult: punctuation and capitalization are often inconsistent, there is a lexical shift due to increased use of slang and technical jargon, some syntactic constructions are more frequent in web text than in newswire, etc. In order to overcome this problem, two main typologies of methods and techniques have been developed: Self-training (McClosky et al., 2006) and Active Learning (Thompson et al., 1999). For the specific purpose of the NLP tools adaptation to the Italian web texts, we adopted two different strategies for the pos-tagger and the parser. For what concerns pos-tagging, we used an active learning approach: given a subset of automatically pos-tagged sentences of PAIS `A, we selected the ones with the lowest likelihood, where the sentence likelihood was computed as the product of the probabilities of the assignments of the postagger for all the tokens. These sentences were manually revised and added to the training corpus in order to build a new pos-tagger model incorporating some </context>
</contexts>
<marker>Thompson, Califf, Mooney, 1999</marker>
<rawString>C. A. Thompson, M. E. Califf, and R. J. Mooney. 1999. Active learning for natural language parsing and information extraction. In Proc. of ICML99, the Sixteenth International Conference on Machine Learning, San Francisco, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.049870">
<title confidence="0.987548">
Machine Translation of Medical Texts in the Khresmoi Project
</title>
<author confidence="0.9892075">
Ondˇrej Duˇsek, Jan Hajiˇc, Jaroslava Hlav´aˇcov´a, Michal Nov´ak,
Pavel Pecina, Rudolf Rosa, Aleˇs Tamchyna, Zdeˇnka Ureˇsov´a, Daniel Zeman
</author>
<affiliation confidence="0.9884935">
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
</affiliation>
<address confidence="0.911685">
Malostransk´e n´amˇest´ı 25, 11800 Prague, Czech Republic
</address>
<email confidence="0.993196">
{odusek,hajic,hlavacova,mnovak,pecina,rosa,tamchyna,uresova,zeman}@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.993682" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999887888888889">
This paper presents the participation of
the Charles University team in the WMT
2014 Medical Translation Task. Our sys-
tems are developed within the Khresmoi
project, a large integrated project aim-
ing to deliver a multi-lingual multi-modal
search and access system for biomedical
information and documents. Being in-
volved in the organization of the Medi-
cal Translation Task, our primary goal is
to set up a baseline for both its subtasks
(summary translation and query transla-
tion) and for all translation directions.
Our systems are based on the phrase-
based Moses system and standard meth-
ods for domain adaptation. The con-
strained/unconstrained systems differ in
the training data only.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919472727273">
The WMT 2014 Medical Translation Task poses
an interesting challenge for Machine Translation
(MT). In the “standard” translation task, the end
application is the translation itself. In the Medi-
cal Translation Task, the MT system is considered
a part of a larger system for Cross-Lingual Infor-
mation Retrieval (CLIR) and is used to solve two
different problems: (i) translation of user search
queries, and (ii) translation of summaries of re-
trieved documents.
In query translation, the end user does not even
necessarily see the MT output as their queries are
translated and search is performed on documents
in the target language. In summary translation, the
sentences to be translated come from document
summaries (snippets) displayed to provide infor-
mation on each of the documents retrieved by the
search. Therefore, translation quality may not be
the most important measure in this task – the per-
formance of the CLIR system as a whole is the
final criterion. Another fundamental difference
from the standard task is the nature of the trans-
lated texts. While we can consider document sum-
maries to be ordinary texts (despite their higher in-
formation density in terms of terminology from a
narrow domain), search queries in the medical do-
main are an extremely specific type of data, and
traditional techniques for system development and
domain adaptation are truly put to a test here.
This work is a part of the of the large integrated
EU-funded Khresmoi project.1 Among other
goals, such as joint text and image retrieval of ra-
diodiagnostic records, Khresmoi aims to develop
technology for transparent cross-lingual search of
medical sources for both professionals and laypeo-
ple, with the emphasis primarily on publicly avail-
able web sources.
In this paper, we describe the Khresmoi sys-
tems submitted to the WMT 2014 Medical Trans-
lation Task. We participate in both subtasks (sum-
mary translation and query translation) for all
language pairs (Czech–English, German–English,
and French–English) in both directions (to English
and from English). Our systems are based on the
Moses phrase-based translation toolkit and stan-
dard methods for domain adaptation. We submit
one constrained and one unconstrained system for
each subtask and translation direction. The con-
strained and unconstrained systems differ in train-
ing data only: The former use all allowed training
data, the latter take advantage of additional web-
crawled data.
We first summarize previous works in MT do-
main adaptation in Section 2, then describe the
data we used for our systems in Section 3. Sec-
</bodyText>
<footnote confidence="0.984691">
1http://www.khresmoi.eu/
</footnote>
<page confidence="0.939617">
221
</page>
<note confidence="0.719178">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 221–228,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99970775">
tion 4 contains an account of the submitted sys-
tems and their performance in translation of search
queries and document summaries. Section 5 con-
cludes the paper.
</bodyText>
<sectionHeader confidence="0.998773" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.9998034">
To put our work in the context of other approaches,
we first describe previous work on domain adap-
tation in Statistical Machine Translation (SMT),
then focus specifically on SMT in the medical do-
main.
</bodyText>
<subsectionHeader confidence="0.8894415">
2.1 Domain adaptation of Statistical machine
translation
</subsectionHeader>
<bodyText confidence="0.999976">
Many works on domain adaptation examine the
usage of available in-domain data to directly im-
prove in-domain performance of SMT. Some au-
thors attempt to combine the predictions of two
separate (in-domain and general-domain) transla-
tion models (Langlais, 2002; Sanchis-Trilles and
Casacuberta, 2010; Bisazza et al., 2011; Nakov,
2008) or language models (Koehn and Schroeder,
2007). Wu and Wang (2004) use in-domain data
to improve word alignment in the training phase.
Carpuat et al. (2012) explore the possibility of us-
ing word sense disambiguation to discriminate be-
tween domains.
Other approaches concentrate on the acquisition
of larger in-domain corpora. Some of them ex-
ploit existing general-domain corpora by select-
ing data that resemble the properties of in-domain
data (e.g., using cross-entropy), thus building a
larger pseudo-in-domain training corpus. This
technique is used to adapt language models (Eck
et al., 2004b; Moore and Lewis, 2010) as well as
translation models (Hildebrand et al., 2005; Axel-
rod et al., 2011) or their combination (Mansour et
al., 2011). Similar approaches to domain adapta-
tion are also applied in other tasks, e.g., automatic
speech recognition (Byrne et al., 2004).
</bodyText>
<subsectionHeader confidence="0.9983575">
2.2 Statistical machine translation in the
medical domain
</subsectionHeader>
<bodyText confidence="0.9999548">
Eck et al. (2004a) employ an SMT system for the
translation of dialogues between doctors and pa-
tients and show that according to automatic met-
rics, a dictionary extracted from the Unified Medi-
cal Language System (UMLS) Metathesaurus and
its semantic type classification (U.S. National Li-
brary of Medicine, 2009) significantly improves
translation quality from Spanish to English when
applied to generalize the training data.
Wu et al. (2011) analyze the quality of MT on
PubMed2 titles and whether it is sufficient for pa-
tients. The conclusions are very positive espe-
cially for languages with large training resources
(English, Spanish, German) – the average fluency
and content scores (based on human evaluation)
are above four on a five-point scale. In automatic
evaluation, their systems substantially outperform
Google Translate. However, the SMT systems are
specifically trained, tuned, and tested on the do-
main of PubMed titles, and it is not evident how
they would perform on other medical texts.
Costa-juss`a et al. (2012) are less optimistic re-
garding SMT quality in the medical domain. They
analyze and evaluate the quality of public web-
based MT systems (such as Google Translate) and
conclude that in both automatic and manual eval-
uation (on 7 language pairs), the performance of
these systems is still not good enough to be used
in daily routines of medical doctors in hospitals.
Jimeno Yepes et al. (2013) propose a method
for obtaining in-domain parallel corpora from ti-
tles and abstracts of publications in the MED-
LINE3 database. The acquired corpora contain
from 30,000 to 130,000 sentence pairs (depending
on the language pair) and are reported to improve
translation quality when used for SMT training,
compared to a baseline trained on out-of-domain
data. However, the authors use only one source
of in-domain parallel data to adapt the translation
model, and do not use any in-domain monolingual
data to adapt the language model.
In this work, we investigate methods combining
the different kinds of data – general-domain, in-
domain, and pseudo-in-domain – to find the opti-
mal approach to this problem.
</bodyText>
<sectionHeader confidence="0.980195" genericHeader="method">
3 Data description
</sectionHeader>
<bodyText confidence="0.999942333333333">
This section includes an overview of the parallel
and monolingual data sources used to train our
systems. Following the task specification, they
are split into constrained and unconstrained sec-
tions. The constrained section includes medical-
domain data provided for this task (extracted by
the provided scripts), and general-domain texts
provided as constrained data for the standard task
(“general domain” here is used to denote data
</bodyText>
<footnote confidence="0.994244">
2http://www.ncbi.nlm.nih.gov/pubmed/
3http://www.nlm.nih.gov/pubs/
factsheets/medline.html
</footnote>
<page confidence="0.994758">
222
</page>
<figure confidence="0.344487">
Czech–English German–English French–English
dom set pairs source target pairs source target pairs source target
med con 2,498 18,126 19,964 4,998 123,686 130,598 6,139 202,245 171,928
gen con 15,788 226,711 260,505 4,520 112,818 119,404 40,842 1,470,016 1,211,516
gen unc – – – 9,320 525,782 574,373 13,809 961,991 808,222
</figure>
<tableCaption confidence="0.968274">
Table 1: Number of sentence pairs and tokens (source/target) in parallel training data (in thousands).
</tableCaption>
<bodyText confidence="0.9851748">
dom set English Czech German French
med con 172,991 1,848 63,499 63,022
gen con 6,132,107 627,493 1,728,065 1,837,457
med unc 3,275,272 36,348 361,881 908,911
gen unc 618,084 – 339,595 204,025
</bodyText>
<tableCaption confidence="0.930233">
Table 2: Number of tokens in monolingual training data (in thousands).
</tableCaption>
<bodyText confidence="0.999928833333333">
which comes from a mixture of various different
domains, mostly news, parliament proceedings,
web-crawls, etc.). The unconstrained section con-
tains automatically crawled data from medical and
health websites and non-medical data from patent
collections.
</bodyText>
<subsectionHeader confidence="0.999574">
3.1 Parallel data
</subsectionHeader>
<bodyText confidence="0.999901653846154">
The parallel data summary is presented in Table 1.
The main sources of the medical-domain data
for all the language pairs include the EMEA cor-
pus (Tiedemann, 2009), the UMLS metathesaurus
of health and biomedical vocabularies and stan-
dards (U.S. National Library of Medicine, 2009),
and bilingual titles of Wikipedia articles belonging
to the categories identified to be medical domain.
Additional medical-domain data comes from the
MAREC patent collection: PatTR (W¨aschle and
Riezler, 2012) available for DE–EN and FR–EN,
and COPPA (Pouliquen and Mazenc, 2011) for
FR–EN (only patents from the medical categories
A61, C12N, and C12P are allowed in the con-
strained systems).
The constrained general-domain data include
three parallel corpora for all the language pairs:
CommonCrawl (Smith et al., 2013), Europarl ver-
sion 6 (Koehn, 2005), the News Commentary cor-
pus (Callison-Burch et al., 2012). Further, the con-
strained data include CzEng (Bojar et al., 2012)
for CS–EN and the UN corpus for FR–EN.
For our unconstrained experiments, we also em-
ploy parallel data from the non-medical patents
from the PatTR and COPPA collections (other cat-
egories than A61, C12N, and C12P).
</bodyText>
<subsectionHeader confidence="0.999526">
3.2 Monolingual data
</subsectionHeader>
<bodyText confidence="0.999957419354839">
The monolingual data is summarized in Table 2.
The main sources of the medical-domain mono-
lingual data for all languages involve Wikipedia
pages, UMLS concept descriptions, and non-
parallel texts extracted from the medical patents
of the PatTR collections. For English, the main
source is the AACT collection of texts from Clin-
icalTrials.gov. Smaller resources include: Drug-
Bank (Knox et al., 2011), GENIA (Kim et al.,
2003), FMA (Rosse and Mejino Jr., 2008), GREC
(Thompson et al., 2009), and PIL (Bouayad-Agha
et al., 2000).
In the unconstrained systems, we use additional
monolingual data from web pages crawled within
the Khresmoi project: a collection of about one
million HON-certified4 webpages in English re-
leased as the test collection for the CLEF 2013
eHealth Task 3 evaluation campaign,5 additional
web-crawled HON-certified pages (not publicly
available), and other webcrawled medical-domain
related webpages.
The constrained general-domain resources in-
clude: the News corpus for CS, DE, EN, and FR
collected for the purpose of the WMT 2014 Stan-
dard Task, monolingual parts of the Europarl and
News-Commentary corpora, and the Gigaword for
EN and FR.
For the FR–EN and DE–EN unconstrained sys-
tems, the additional general domain monolingual
data is taken from monolingual texts of non-
medical patents in the PatTR collection.
</bodyText>
<footnote confidence="0.998032">
4https://www.hon.ch/
5https://sites.google.com/site/
shareclefehealth/
</footnote>
<page confidence="0.998097">
223
</page>
<figure confidence="0.9994245">
medical general medical general
constrained
unconstrained
−10
−15
−10
−15
−5
−5
15
10
15
10
5
0
5
0
−10
−15
−10
−15
−5
−5
15
10
15
10
5
0
5
0
15
10
5
0
−5
−10
5
0
−5
constrained
−10
−15
unconstrained
−15
15
10
5
0
−5
−10
−15
15
10
</figure>
<figureCaption confidence="0.9965885">
Figure 1: Distribution of the domain-specificity
scores in the English–French parallel data sets.
Figure 2: Distribution of the domain-specificity
scores in the French monolingual data sets.
</figureCaption>
<subsectionHeader confidence="0.998899">
3.3 Data preprocessing
</subsectionHeader>
<bodyText confidence="0.99997165625">
The data consisting of crawled web pages, namely
CLEF, HON, and non-HON, needed to be cleaned
and transformed into a set of sentences. The
Boilerpipe (Kohlsch¨utter et al., 2010) and Justext
(Pomik´alek, 2011) tools were used to remove boil-
erplate texts and extract just the main content from
the web pages. The YALI language detection tool
(Majliˇs, 2012) trained on both in-domain and gen-
eral domain data then filtered out those cleaned
pages which were not identified as written in one
of the concerned languages.
The rest of the preprocessing procedure was ap-
plied to all the datasets mentioned above, both
parallel and monolingual. The data were tok-
enized and normalized by converting or omit-
ting some (mostly punctuation) characters. A
set of language-dependent heuristics was applied
in an attempt to restore and normalize the open-
ing/closing quotation marks, i.e. convert &amp;quot;quoted&amp;quot;
to “quoted” (Zeman, 2012). The motivation here
is twofold: First, we hope that paired quota-
tion marks could occasionally work as brackets
and better denote parallel phrases for Moses; sec-
ond, if Moses learns to output directed quotation
marks, the subsequent detokenization will be eas-
ier. For all systems which translate from German,
decompounding is employed to reduce source-side
data sparsity. We used BananaSplit for this task
(M¨uller and Gurevych, 2006).
We perform all training and internal evaluation
on lowercased data; we trained recasers to post-
process the final submissions.
</bodyText>
<sectionHeader confidence="0.995173" genericHeader="method">
4 Submitted systems
</sectionHeader>
<bodyText confidence="0.9999895">
We first describe our technique of psedo-in-
domain data selection in Section 4.1, then com-
pare two methods of combining the selected data
in Section 4.2. This, along with using constrained
and unconstrained data sets to train the systems
(see Section 3), amounts to a total of four system
variants submitted for each task. A description of
the system settings used is given in Section 4.3.
</bodyText>
<subsectionHeader confidence="0.995779">
4.1 Data selection
</subsectionHeader>
<bodyText confidence="0.999689523809524">
We follow an approach originally proposed for
selection of monolingual sentences for language
modeling (Moore and Lewis, 2010) and its modi-
fication applied to selection of parallel sentences
(Axelrod et al., 2011). This technique assumes
two language models for sentence scoring, one
trained on (true) in-domain text and one trained
on (any) general-domain text in the same lan-
guage (e.g., English). For both data domains
(general and medical), we score each sentence
by the difference of its cross-perplexity given the
in-domain language model and cross-perplexity
given the general-domain language model (in this
order). We only keep sentences with a negative
score in our data, assuming that these are the
most “medical-like”. Visualisation of the domain-
specificity scores (cross-perplexity difference) in
the FR–EN parallel data and FR monolingual data
is illustrated in Figures 1 and 2, respectively.6 The
scores (Y axis) are presented for each sentence in
increasing order from left to right (X axis).
</bodyText>
<footnote confidence="0.9871325">
6For the medical domain, constrained and unconstrained
parallel data are identical.
</footnote>
<page confidence="0.993307">
224
</page>
<table confidence="0.9978482">
cs→en de→en en→cs en→de en→fr fr→en
con concat 33.64±1.14 32.84±1.24 18.10±0.94 18.29±0.92 33.39±1.11 36.71±1.17
con interpol 32.94±1.11 32.31±1.20 18.96±0.93 18.41±0.93 34.06 ±1.11 37.42±1.21
unc concat 34.10±1.11 34.52±1.20 21.12±1.03 19.76±0.92 36.23 ±1.03 38.15±1.16
unc interpol 34.48±1.16 34.92±1.17 22.15±1.06 20.81±0.95 36.26 ±1.13 37.91±1.13
</table>
<tableCaption confidence="0.971768">
Table 3: BLEU scores of summary translations.
</tableCaption>
<table confidence="0.9992862">
cs→en de→en en→cs en→de en→fr fr→en
con concat 30.87±4.70 33.21±5.03 23.25±4.85 17.72±4.75 28.64±3.77 35.56±4.94
con interpol 32.46±5.05 33.74±4.97 21.56±4.80 16.90±4.39 29.34±3.73 35.28±5.26
unc concat 34.88±5.04 31.24±5.59 22.61±4.91 19.13±5.66 33.08±3.80 36.73±4.88
unc interpol 33.82±5.16 34.19±5.27 23.93±5.16 15.87±11.31 31.19±3.73 40.25±5.14
</table>
<tableCaption confidence="0.999685">
Table 4: BLEU scores of query translations.
</tableCaption>
<bodyText confidence="0.9999815">
The two language models for sentence scoring
are trained with a restricted vocabulary extracted
from the in-domain training data as words occur-
ring at least twice (singletons and other words are
treated as out-of-vocabulary). In our experiments,
we apply this technique to select both monolin-
gual data for language models and parallel data
for translation models. Selection of parallel data
is based on the English side only. The in-domain
models are trained on the monolingual data in the
target language (constrained or unconstrained, de-
pending on the setting). The general-domain mod-
els are trained on the WMT News data.
Compared to the approach of Moore and Lewis
(2010) and Axelrod et al. (2011), we prune the
model vocabulary more aggressively – we discard
not only the singletons, but also all words with
non-Latin characters, which helps clean the mod-
els from noise introduced by the automatic process
of data acquisition by web crawling.
</bodyText>
<subsectionHeader confidence="0.995345">
4.2 Data combination
</subsectionHeader>
<bodyText confidence="0.997616">
For both parallel and monolingual data, we obtain
two data sets after applying the data selection:
</bodyText>
<listItem confidence="0.999804">
• “medical-like” data from the medical domain
• “medical-like” data from the general domain.
</listItem>
<bodyText confidence="0.999956071428571">
For each language pair and for each system
type (constrained/unconstrained), we submitted
two system variants which differ in how the se-
lected data are combined. The first variant uses
a simple concatenation of the two datasets both
for parallel data and for language model data. In
the second variant, we train separate models for
each section and use linear interpolation to com-
bine them into a single model. For language mod-
els, we use the SRILM linear interpolation feature
(Stolcke, 2002). We interpolate phrase tables us-
ing Tmcombine (Sennrich, 2012). In both cases,
the held-out set for minimizing the perplexity is
the system development set.
</bodyText>
<subsectionHeader confidence="0.999282">
4.3 System details
</subsectionHeader>
<bodyText confidence="0.999997666666667">
We compute word alignment on lowercase 4-cha-
racter stems using fast align (Dyer et al., 2013).
We create phrase tables using the Moses toolkit
(Koehn et al., 2007) with standard settings. We
train 5-gram language models on the target-side
lowercase forms using SRILM. We use MERT
(Och, 2003) to tune model weights in our systems
on the development data provided for the task.
The only difference between the system variants
for query and summary translation is the tuning
set. In both cases, we use the respective sets pro-
vided offcially for the shared task.
</bodyText>
<subsectionHeader confidence="0.735543">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999539">
Tables 3 and 4 show case-insensitive BLEU scores
of our systems.7 As expected, the unconstrained
systems outperform the constrained ones. Linear
interpolation outperforms data concatenation quite
reliably across language pairs for summary trans-
lation. While the picture for query translation is
similar, there is more variance in the results, so
we cannot state that interpolation definitely works
</bodyText>
<footnote confidence="0.966635">
7As we use the same recasers for both summary and query
translation, our systems are heavily penalized for wrong let-
ter case in query translation. However, letter case is not taken
into account in most CLIR systems. All BLEU scores re-
ported in this paper will be case-insensitive for this reason.
</footnote>
<page confidence="0.998142">
225
</page>
<bodyText confidence="0.999897857142857">
better in this case. This is due to the sizes of the
development and test sets and most importantly
due to sentence lengths – queries are very short,
making BLEU unreliable, MERT unstable, and
bootstrap resampling intervals wide.
If we compare our score to the other competi-
tors, we are clearly worse than the best systems for
summary translation. From this perspective, our
data filtering seems overly eager (i.e., discarding
all sentence pairs with a positive perplexity differ-
ence). An experiment which we leave for future
work is doing one more round of interpolation to
combine a model trained on the data with negative
perplexity with models trained on the remainder.
</bodyText>
<sectionHeader confidence="0.998202" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999968090909091">
We described the Charles University MT system
used in the Shared Medical Translation Task of
WMT 2014. Our primary goal was to set up a
baseline for both the subtasks and all translation
directions. The systems are based on the Moses
toolkit, pseudo-in-domain data selection based on
perplexity difference and two different methods of
in-domain and out-of-domain data combination:
simple data concatenation and linear model inter-
polation.
We report results of constrained and uncon-
strained systems which differ in the training data
only. In most experiments, using additional data
improved the results compared to the constrained
systems and using linear model interpolation out-
performed data concatenation. While our systems
are on par with best results for case-insensitive
BLEU score in query translation, our overly ea-
ger data selection techniques caused lower scores
in summary translation. In future work, we plan
to include a special out-of-domain model in our
setup to compensate for this problem.
</bodyText>
<sectionHeader confidence="0.984405" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999262625">
This work was supported by the EU FP7 project
Khresmoi (contract no. 257528), the Czech Sci-
ence Foundation (grant no. P103/12/G084), and
SVV project number 260 104. This work has
been using language resources developed, stored,
and distributed by the LINDAT/CLARIN project
of the Ministry of Education, Youth and Sports of
the Czech Republic (project LM2010013).
</bodyText>
<sectionHeader confidence="0.950944" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996354436363636">
A. Axelrod, X. He, and J. Gao. 2011. Domain adap-
tation via pseudo in-domain data selection. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 355–
362, Edinburgh, United Kingdom. ACL.
A. Bisazza, N. Ruiz, and M. Federico. 2011. Fill-
up versus interpolation methods for phrase-based
SMT adaptation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 136–143, San Francisco, CA, USA. Interna-
tional Speech Communication Association.
O. Bojar, Z. &amp;quot;Zabokrtsk´y, O. Du&amp;quot;sek, P. Galu&amp;quot;s&amp;quot;c´akov´a,
M. Majli&amp;quot;s, D. Mare&amp;quot;cek, J. Mar&amp;quot;s´ık, M. Nov´ak,
M. Popel, and A. Tamchyna. 2012. The joy of
parallelism with CzEng 1.0. In Proceedings of the
Eighth International Conference on Language Re-
sources and Evaluation, pages 3921–3928, Istanbul,
Turkey. European Language Resources Association.
N. Bouayad-Agha, D. R. Scott, and R. Power. 2000.
Integrating content and style in documents: A case
study of patient information leaflets. Information
Design Journal, 9(2–3):161–176.
W. Byrne, D. S. Doermann, M. Franz, S. Gustman,
J. Haji&amp;quot;c, D. W. Oard, et al. 2004. Automatic recog-
nition of spontaneous speech for access to multilin-
gual oral history archives. Speech and Audio Pro-
cessing, IEEE Transactions on, 12(4):420–435.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, pages 10–51, Montr´eal,
Canada. ACL.
M. Carpuat, H. Daum´e III, A. Fraser, C. Quirk,
F. Braune, A. Clifton, et al. 2012. Domain adap-
tation in machine translation: Final report. In
2012 Johns Hopkins Summer Workshop Final Re-
port, pages 61–72. Johns Hopkins University.
M. R. Costa-juss`a, M. Farris, and J. Serrano Pons.
2012. Machine translation in medicine. A qual-
ity analysis of statistical machine translation in the
medical domain. In Proceedings of the 1st Virtual
International Conference on Advanced Research in
Scientific Areas, pages 1995–1998, &amp;quot;Zilina, Slovakia.
&amp;quot;Zilinsk´a univerzita.
C. Dyer, V. Chahuneau, and N. A. Smith. 2013. A sim-
ple, fast, and effective reparameterization of IBM
model 2. In Proceedings of NAACL-HLT, pages
644–648.
M. Eck, S. Vogel, and A. Waibel. 2004a. Improv-
ing statistical machine translation in the medical do-
main using the Unified Medical Language System.
In COLING 2004: Proceedings of the 20th Inter-
national Conference on Computational Linguistics,
pages 792–798, Geneva, Switzerland. ACL.
</reference>
<page confidence="0.992507">
226
</page>
<reference confidence="0.996423554545454">
M. Eck, S. Vogel, and A. Waibel. 2004b. Language
model adaptation for statistical machine translation
based on information retrieval. In Maria Teresa
Lino, Maria Francisca Xavier, F´atima Ferreira, Rute
Costa, and Raquel Silva, editors, Proceedings of the
International Conference on Language Resources
and Evaluation, pages 327–330, Lisbon, Portugal.
European Language Resources Association.
A. S. Hildebrand, M. Eck, S. Vogel, and A. Waibel.
2005. Adaptation of the translation model for statis-
tical machine translation based on information re-
trieval. In Proceedings of the 10th Annual Con-
ference of the European Association for Machine
Translation, pages 133–142, Budapest, Hungary.
European Association for Machine Translation.
A. Jimeno Yepes, ´E. Prieur-Gaston, and A. N´ev´eol.
2013. Combining MEDLINE and publisher data to
create parallel corpora for the automatic translation
of biomedical text. BMC Bioinformatics, 14(1):1–
10.
J.-D Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GE-
NIA corpus – a semantically annotated corpus for
bio-textmining. Bioinformatics, 19(suppl 1):i180–
i182.
C. Knox, V. Law, T. Jewison, P. Liu, Son Ly, A. Frolkis,
A. Pon, K. Banco, C. Mak, V. Neveu, Y. Djoum-
bou, R. Eisner, A. C. Guo, and D. S. Wishart.
2011. DrugBank 3.0: a comprehensive resource for
‘Omics’ research on drugs. Nucleic acids research,
39(suppl 1):D1035–D1041.
P. Koehn and J. Schroeder. 2007. Experiments in do-
main adaptation for statistical machine translation.
In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 224–227, Prague,
Czech Republic. ACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages
177–180, Praha, Czechia, June. ACL.
P. Koehn. 2005. Europarl: a parallel corpus for sta-
tistical machine translation. In Conference Proceed-
ings: the tenth Machine Translation Summit, pages
79–86, Phuket, Thailand. Asia-Pacific Association
for Machine Translation.
C. Kohlsch¨utter, P. Fankhauser, and W. Nejdl. 2010.
Boilerplate detection using shallow text features. In
Proceedings of the ThirdACMInternational Confer-
ence on Web Search and Data Mining, WSDM ’10,
pages 441–450, New York, NY, USA. ACM.
P. Langlais. 2002. Improving a general-purpose statis-
tical translation engine by terminological lexicons.
In COLING-02 on COMPUTERM 2002: second
international workshop on computational terminol-
ogy, volume 14, pages 1–7, Taipei, Taiwan. ACL.
M. Majliˇs. 2012. Yet another language identifier. In
Proceedings of the Student Research Workshop at
the 13th Conference of the European Chapter of the
Association for Computational Linguistics, pages
46–54, Avignon, France. ACL.
S. Mansour, J. Wuebker, and H. Ney. 2011. Com-
bining translation and language model scoring for
domain-specific data filtering. In International
Workshop on Spoken Language Translation, pages
222–229, San Francisco, CA, USA. ISCA.
R. C. Moore and W. Lewis. 2010. Intelligent selection
of language model training data. In Proceedings of
the ACL 2010 Conference Short Papers, pages 220–
224, Uppsala, Sweden. ACL.
C. M¨uller and I. Gurevych. 2006. Exploring the po-
tential of semantic relatedness in information re-
trieval. In LWA 2006 Lernen – Wissensentdeck-
ung – Adaptivit¨at, 9.-11.10.2006, Hildesheimer In-
formatikberichte, pages 126–131, Hildesheim, Ger-
many. Universit¨at Hildesheim.
P. Nakov. 2008. Improving English–Spanish statistical
machine translation: Experiments in domain adapta-
tion, sentence paraphrasing, tokenization, and recas-
ing. In Proceedings of the Third Workshop on Statis-
tical Machine Translation, pages 147–150, Colum-
bus, OH, USA. ACL.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In ACL ’03: Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160–167, Morristown,
NJ, USA. ACL.
J. Pomik´alek. 2011. Removing Boilerplate and Du-
plicate Content from Web Corpora. PhD thesis,
Masaryk University, Faculty of Informatics, Brno.
B. Pouliquen and C. Mazenc. 2011. COPPA, CLIR
and TAPTA: three tools to assist in overcoming the
patent barrier at WIPO. In Proceedings of the Thir-
teenth Machine Translation Summit, pages 24–30,
Xiamen, China. Asia-Pacific Association for Ma-
chine Translation.
C. Rosse and Jos´e L. V. Mejino Jr. 2008. The foun-
dational model of anatomy ontology. In A. Burger,
D. Davidson, and R. Baldock, editors, Anatomy On-
tologies for Bioinformatics, volume 6 of Computa-
tional Biology, pages 59–117. Springer London.
G. Sanchis-Trilles and F. Casacuberta. 2010. Log-
linear weight optimisation via Bayesian adaptation
in statistical machine translation. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1077–1085, Bei-
jing, China. ACL.
</reference>
<page confidence="0.967448">
227
</page>
<reference confidence="0.999922854166667">
R. Sennrich. 2012. Perplexity minimization for trans-
lation model domain adaptation in statistical ma-
chine translation. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 539–549. ACL.
J. R. Smith, H. Saint-Amand, M. Plamada, P. Koehn,
C. Callison-Burch, and A. Lopez. 2013. Dirt cheap
web-scale parallel text from the common crawl. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1374–1383, Sofia, Bulgaria.
ACL.
A. Stolcke. 2002. SRILM – an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, Den-
ver, Colorado, USA.
P. Thompson, S. Iqbal, J. McNaught, and Sophia Ana-
niadou. 2009. Construction of an annotated corpus
to support biomedical information extraction. BMC
bioinformatics, 10(1):349.
J. Tiedemann. 2009. News from OPUS – a collection
of multilingual parallel corpora with tools and in-
terfaces. In Recent Advances in Natural Language
Processing, volume 5, pages 237–248, Borovets,
Bulgaria. John Benjamins.
U.S. National Library of Medicine. 2009. UMLS
reference manual. Metathesaurus. Bethesda, MD,
USA.
K. W¨aschle and S. Riezler. 2012. Analyzing paral-
lelism and domain similarities in the MAREC patent
corpus. In M. Salampasis and B. Larsen, edi-
tors, Multidisciplinary Information Retrieval, vol-
ume 7356 of Lecture Notes in Computer Science,
pages 12–27. Springer Berlin Heidelberg.
H. Wu and H. Wang. 2004. Improving domain-specific
word alignment with a general bilingual corpus. In
Robert E. Frederking and Kathryn B. Taylor, editors,
Machine Translation: From Real Users to Research,
volume 3265 of Lecture Notes in Computer Science,
pages 262–271. Springer Berlin Heidelberg.
C. Wu, F. Xia, L. Deleger, and I. Solti. 2011. Statistical
machine translation for biomedical text: are we there
yet? AMIA Annual Symposium proceedings, pages
1290–1299.
D. Zeman. 2012. Data issues of the multilingual trans-
lation matrix. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 395–
400, Montr´eal, Canada. ACL.
</reference>
<page confidence="0.997671">
228
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.438886">
<title confidence="0.8054515">Machine Translation of Medical Texts in the Khresmoi Project Ondˇrej Duˇsek, Jan Hajiˇc, Jaroslava Hlav´aˇcov´a, Michal</title>
<author confidence="0.982855">Pavel Pecina</author>
<author confidence="0.982855">Rudolf Rosa</author>
<author confidence="0.982855">Aleˇs Tamchyna</author>
<author confidence="0.982855">Zdeˇnka Ureˇsov´a</author>
<author confidence="0.982855">Daniel</author>
<affiliation confidence="0.9262725">Charles University in Prague, Faculty of Mathematics and Institute of Formal and Applied</affiliation>
<address confidence="0.743284">Malostransk´e n´amˇest´ı 25, 11800 Prague, Czech Republic</address>
<abstract confidence="0.998095684210526">This paper presents the participation of the Charles University team in the WMT 2014 Medical Translation Task. Our systems are developed within the Khresmoi project, a large integrated project aiming to deliver a multi-lingual multi-modal search and access system for biomedical information and documents. Being involved in the organization of the Medical Translation Task, our primary goal is to set up a baseline for both its subtasks (summary translation and query translation) and for all translation directions. Our systems are based on the phrasebased Moses system and standard methods for domain adaptation. The constrained/unconstrained systems differ in the training data only.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Axelrod</author>
<author>X He</author>
<author>J Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>355--362</pages>
<publisher>ACL.</publisher>
<location>Edinburgh, United Kingdom.</location>
<contexts>
<context position="5401" citStr="Axelrod et al., 2011" startWordPosition="822" endWordPosition="826">ain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 Statistical machine translation in the medical domain Eck et al. (2004a) employ an SMT system for the translation of dialogues between doctors and patients and show that according to automatic metrics, a dictionary extracted from the Unified Medical Language System (UMLS) Metathesaurus and its semantic type classification (U.S. National Library of Medicine, 2009) significantly improves translation quality from Spanis</context>
<context position="14438" citStr="Axelrod et al., 2011" startWordPosition="2231" endWordPosition="2234">We first describe our technique of psedo-indomain data selection in Section 4.1, then compare two methods of combining the selected data in Section 4.2. This, along with using constrained and unconstrained data sets to train the systems (see Section 3), amounts to a total of four system variants submitted for each task. A description of the system settings used is given in Section 4.3. 4.1 Data selection We follow an approach originally proposed for selection of monolingual sentences for language modeling (Moore and Lewis, 2010) and its modification applied to selection of parallel sentences (Axelrod et al., 2011). This technique assumes two language models for sentence scoring, one trained on (true) in-domain text and one trained on (any) general-domain text in the same language (e.g., English). For both data domains (general and medical), we score each sentence by the difference of its cross-perplexity given the in-domain language model and cross-perplexity given the general-domain language model (in this order). We only keep sentences with a negative score in our data, assuming that these are the most “medical-like”. Visualisation of the domainspecificity scores (cross-perplexity difference) in the </context>
<context position="16811" citStr="Axelrod et al. (2011)" startWordPosition="2571" endWordPosition="2574">bulary extracted from the in-domain training data as words occurring at least twice (singletons and other words are treated as out-of-vocabulary). In our experiments, we apply this technique to select both monolingual data for language models and parallel data for translation models. Selection of parallel data is based on the English side only. The in-domain models are trained on the monolingual data in the target language (constrained or unconstrained, depending on the setting). The general-domain models are trained on the WMT News data. Compared to the approach of Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 Data combination For both parallel and monolingual data, we obtain two data sets after applying the data selection: • “medical-like” data from the medical domain • “medical-like” data from the general domain. For each language pair and for each system type (constrained/unconstrained), we submitted two system variants which differ in how the select</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>A. Axelrod, X. He, and J. Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 355– 362, Edinburgh, United Kingdom. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bisazza</author>
<author>N Ruiz</author>
<author>M Federico</author>
</authors>
<title>Fillup versus interpolation methods for phrase-based SMT adaptation.</title>
<date>2011</date>
<journal>International Speech Communication Association.</journal>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<pages>136--143</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="4688" citStr="Bisazza et al., 2011" startWordPosition="712" endWordPosition="715">ction 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et</context>
</contexts>
<marker>Bisazza, Ruiz, Federico, 2011</marker>
<rawString>A. Bisazza, N. Ruiz, and M. Federico. 2011. Fillup versus interpolation methods for phrase-based SMT adaptation. In Proceedings of the International Workshop on Spoken Language Translation, pages 136–143, San Francisco, CA, USA. International Speech Communication Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>Z Zabokrtsk´y</author>
<author>O Dusek</author>
<author>P Galusc´akov´a</author>
<author>M Majlis</author>
<author>D Marecek</author>
<author>J Mars´ık</author>
<author>M Nov´ak</author>
<author>M Popel</author>
<author>A Tamchyna</author>
</authors>
<title>The joy of parallelism with CzEng 1.0.</title>
<date>2012</date>
<journal>European Language Resources Association.</journal>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation,</booktitle>
<pages>3921--3928</pages>
<location>Istanbul,</location>
<marker>Bojar, Zabokrtsk´y, Dusek, Galusc´akov´a, Majlis, Marecek, Mars´ık, Nov´ak, Popel, Tamchyna, 2012</marker>
<rawString>O. Bojar, Z. &amp;quot;Zabokrtsk´y, O. Du&amp;quot;sek, P. Galu&amp;quot;s&amp;quot;c´akov´a, M. Majli&amp;quot;s, D. Mare&amp;quot;cek, J. Mar&amp;quot;s´ık, M. Nov´ak, M. Popel, and A. Tamchyna. 2012. The joy of parallelism with CzEng 1.0. In Proceedings of the Eighth International Conference on Language Resources and Evaluation, pages 3921–3928, Istanbul, Turkey. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bouayad-Agha</author>
<author>D R Scott</author>
<author>R Power</author>
</authors>
<title>Integrating content and style in documents: A case study of patient information leaflets.</title>
<date>2000</date>
<journal>Information Design Journal,</journal>
<pages>9--2</pages>
<contexts>
<context position="10988" citStr="Bouayad-Agha et al., 2000" startWordPosition="1682" endWordPosition="1685">rom the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 3.2 Monolingual data The monolingual data is summarized in Table 2. The main sources of the medical-domain monolingual data for all languages involve Wikipedia pages, UMLS concept descriptions, and nonparallel texts extracted from the medical patents of the PatTR collections. For English, the main source is the AACT collection of texts from ClinicalTrials.gov. Smaller resources include: DrugBank (Knox et al., 2011), GENIA (Kim et al., 2003), FMA (Rosse and Mejino Jr., 2008), GREC (Thompson et al., 2009), and PIL (Bouayad-Agha et al., 2000). In the unconstrained systems, we use additional monolingual data from web pages crawled within the Khresmoi project: a collection of about one million HON-certified4 webpages in English released as the test collection for the CLEF 2013 eHealth Task 3 evaluation campaign,5 additional web-crawled HON-certified pages (not publicly available), and other webcrawled medical-domain related webpages. The constrained general-domain resources include: the News corpus for CS, DE, EN, and FR collected for the purpose of the WMT 2014 Standard Task, monolingual parts of the Europarl and News-Commentary co</context>
</contexts>
<marker>Bouayad-Agha, Scott, Power, 2000</marker>
<rawString>N. Bouayad-Agha, D. R. Scott, and R. Power. 2000. Integrating content and style in documents: A case study of patient information leaflets. Information Design Journal, 9(2–3):161–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Byrne</author>
<author>D S Doermann</author>
<author>M Franz</author>
<author>S Gustman</author>
<author>J Hajic</author>
<author>D W Oard</author>
</authors>
<title>Automatic recognition of spontaneous speech for access to multilingual oral history archives. Speech and Audio Processing,</title>
<date>2004</date>
<journal>IEEE Transactions on,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="5575" citStr="Byrne et al., 2004" startWordPosition="850" endWordPosition="853">r approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 Statistical machine translation in the medical domain Eck et al. (2004a) employ an SMT system for the translation of dialogues between doctors and patients and show that according to automatic metrics, a dictionary extracted from the Unified Medical Language System (UMLS) Metathesaurus and its semantic type classification (U.S. National Library of Medicine, 2009) significantly improves translation quality from Spanish to English when applied to generalize the training data. Wu et al. (2011) analyze the quality of MT on PubMed2 titles and whether it is sufficient for patients. The conclus</context>
</contexts>
<marker>Byrne, Doermann, Franz, Gustman, Hajic, Oard, 2004</marker>
<rawString>W. Byrne, D. S. Doermann, M. Franz, S. Gustman, J. Haji&amp;quot;c, D. W. Oard, et al. 2004. Automatic recognition of spontaneous speech for access to multilingual oral history archives. Speech and Audio Processing, IEEE Transactions on, 12(4):420–435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>M Post</author>
<author>R Soricut</author>
<author>L Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>ACL.</publisher>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="10162" citStr="Callison-Burch et al., 2012" startWordPosition="1548" endWordPosition="1551">edicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 3.2 Monolingual data The monolingual data is summarized in Table 2. The main sources of the medical-domain monolingual data for all languages involve Wikipedia pages, UMLS concept descriptions, and nonparallel texts extracted from the medical patents of the PatTR collections. For English, the main source is the AACT c</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Soricut, and L. Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>H Daum´e A Fraser</author>
<author>C Quirk</author>
<author>F Braune</author>
<author>A Clifton</author>
</authors>
<title>Domain adaptation in machine translation: Final report.</title>
<date>2012</date>
<booktitle>In 2012 Johns Hopkins Summer Workshop Final Report,</booktitle>
<pages>61--72</pages>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="4859" citStr="Carpuat et al. (2012)" startWordPosition="740" endWordPosition="743">e Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar appr</context>
</contexts>
<marker>Carpuat, Fraser, Quirk, Braune, Clifton, 2012</marker>
<rawString>M. Carpuat, H. Daum´e III, A. Fraser, C. Quirk, F. Braune, A. Clifton, et al. 2012. Domain adaptation in machine translation: Final report. In 2012 Johns Hopkins Summer Workshop Final Report, pages 61–72. Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Costa-juss`a</author>
<author>M Farris</author>
<author>J Serrano Pons</author>
</authors>
<title>Machine translation in medicine. A quality analysis of statistical machine translation in the medical domain.</title>
<date>2012</date>
<booktitle>In Proceedings of the 1st Virtual International Conference on Advanced Research in Scientific Areas,</booktitle>
<pages>1995--1998</pages>
<location>Zilina, Slovakia. &amp;quot;Zilinsk´a univerzita.</location>
<marker>Costa-juss`a, Farris, Pons, 2012</marker>
<rawString>M. R. Costa-juss`a, M. Farris, and J. Serrano Pons. 2012. Machine translation in medicine. A quality analysis of statistical machine translation in the medical domain. In Proceedings of the 1st Virtual International Conference on Advanced Research in Scientific Areas, pages 1995–1998, &amp;quot;Zilina, Slovakia. &amp;quot;Zilinsk´a univerzita.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
<author>V Chahuneau</author>
<author>N A Smith</author>
</authors>
<title>A simple, fast, and effective reparameterization of IBM model 2.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>644--648</pages>
<contexts>
<context position="18034" citStr="Dyer et al., 2013" startWordPosition="2768" endWordPosition="2771">ta are combined. The first variant uses a simple concatenation of the two datasets both for parallel data and for language model data. In the second variant, we train separate models for each section and use linear interpolation to combine them into a single model. For language models, we use the SRILM linear interpolation feature (Stolcke, 2002). We interpolate phrase tables using Tmcombine (Sennrich, 2012). In both cases, the held-out set for minimizing the perplexity is the system development set. 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. 4.4 Results Tables 3 and 4 show case-insensitive BLEU scores of our systems.7 As expected, the unconstrained systems outperform the co</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>C. Dyer, V. Chahuneau, and N. A. Smith. 2013. A simple, fast, and effective reparameterization of IBM model 2. In Proceedings of NAACL-HLT, pages 644–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Eck</author>
<author>S Vogel</author>
<author>A Waibel</author>
</authors>
<title>Improving statistical machine translation in the medical domain using the Unified Medical Language System. In</title>
<date>2004</date>
<booktitle>COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>792--798</pages>
<publisher>ACL.</publisher>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="5298" citStr="Eck et al., 2004" startWordPosition="805" endWordPosition="808">, 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 Statistical machine translation in the medical domain Eck et al. (2004a) employ an SMT system for the translation of dialogues between doctors and patients and show that according to automatic metrics, a dictionary extracted from the Unified Medical Language System (UMLS) Metathesaurus and its semantic type classifi</context>
</contexts>
<marker>Eck, Vogel, Waibel, 2004</marker>
<rawString>M. Eck, S. Vogel, and A. Waibel. 2004a. Improving statistical machine translation in the medical domain using the Unified Medical Language System. In COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, pages 792–798, Geneva, Switzerland. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Eck</author>
<author>S Vogel</author>
<author>A Waibel</author>
</authors>
<title>Language model adaptation for statistical machine translation based on information retrieval.</title>
<date>2004</date>
<journal>European Language Resources Association.</journal>
<booktitle>In Maria</booktitle>
<pages>327--330</pages>
<editor>Costa, and Raquel Silva, editors,</editor>
<location>Teresa Lino, Maria Francisca Xavier, F´atima Ferreira, Rute</location>
<contexts>
<context position="5298" citStr="Eck et al., 2004" startWordPosition="805" endWordPosition="808">, 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 Statistical machine translation in the medical domain Eck et al. (2004a) employ an SMT system for the translation of dialogues between doctors and patients and show that according to automatic metrics, a dictionary extracted from the Unified Medical Language System (UMLS) Metathesaurus and its semantic type classifi</context>
</contexts>
<marker>Eck, Vogel, Waibel, 2004</marker>
<rawString>M. Eck, S. Vogel, and A. Waibel. 2004b. Language model adaptation for statistical machine translation based on information retrieval. In Maria Teresa Lino, Maria Francisca Xavier, F´atima Ferreira, Rute Costa, and Raquel Silva, editors, Proceedings of the International Conference on Language Resources and Evaluation, pages 327–330, Lisbon, Portugal. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Hildebrand</author>
<author>M Eck</author>
<author>S Vogel</author>
<author>A Waibel</author>
</authors>
<title>Adaptation of the translation model for statistical machine translation based on information retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Annual Conference of the European Association for Machine Translation,</booktitle>
<pages>133--142</pages>
<location>Budapest,</location>
<contexts>
<context position="5378" citStr="Hildebrand et al., 2005" startWordPosition="818" endWordPosition="821">nd Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 Statistical machine translation in the medical domain Eck et al. (2004a) employ an SMT system for the translation of dialogues between doctors and patients and show that according to automatic metrics, a dictionary extracted from the Unified Medical Language System (UMLS) Metathesaurus and its semantic type classification (U.S. National Library of Medicine, 2009) significantly improves translat</context>
</contexts>
<marker>Hildebrand, Eck, Vogel, Waibel, 2005</marker>
<rawString>A. S. Hildebrand, M. Eck, S. Vogel, and A. Waibel. 2005. Adaptation of the translation model for statistical machine translation based on information retrieval. In Proceedings of the 10th Annual Conference of the European Association for Machine Translation, pages 133–142, Budapest, Hungary. European Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Jimeno Yepes</author>
<author>´E Prieur-Gaston</author>
<author>A N´ev´eol</author>
</authors>
<title>Combining MEDLINE and publisher data to create parallel corpora for the automatic translation of biomedical text.</title>
<date>2013</date>
<journal>BMC Bioinformatics,</journal>
<volume>14</volume>
<issue>1</issue>
<pages>10</pages>
<marker>Yepes, Prieur-Gaston, N´ev´eol, 2013</marker>
<rawString>A. Jimeno Yepes, ´E. Prieur-Gaston, and A. N´ev´eol. 2013. Combining MEDLINE and publisher data to create parallel corpora for the automatic translation of biomedical text. BMC Bioinformatics, 14(1):1– 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-D Kim</author>
<author>T Ohta</author>
<author>Y Tateisi</author>
<author>J Tsujii</author>
</authors>
<title>GENIA corpus – a semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl 1):i180– i182.</title>
<date>2003</date>
<contexts>
<context position="10887" citStr="Kim et al., 2003" startWordPosition="1665" endWordPosition="1668">r our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 3.2 Monolingual data The monolingual data is summarized in Table 2. The main sources of the medical-domain monolingual data for all languages involve Wikipedia pages, UMLS concept descriptions, and nonparallel texts extracted from the medical patents of the PatTR collections. For English, the main source is the AACT collection of texts from ClinicalTrials.gov. Smaller resources include: DrugBank (Knox et al., 2011), GENIA (Kim et al., 2003), FMA (Rosse and Mejino Jr., 2008), GREC (Thompson et al., 2009), and PIL (Bouayad-Agha et al., 2000). In the unconstrained systems, we use additional monolingual data from web pages crawled within the Khresmoi project: a collection of about one million HON-certified4 webpages in English released as the test collection for the CLEF 2013 eHealth Task 3 evaluation campaign,5 additional web-crawled HON-certified pages (not publicly available), and other webcrawled medical-domain related webpages. The constrained general-domain resources include: the News corpus for CS, DE, EN, and FR collected fo</context>
</contexts>
<marker>Kim, Ohta, Tateisi, Tsujii, 2003</marker>
<rawString>J.-D Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GENIA corpus – a semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl 1):i180– i182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Knox</author>
<author>V Law</author>
<author>T Jewison</author>
<author>P Liu</author>
<author>Son Ly</author>
<author>A Frolkis</author>
<author>A Pon</author>
<author>K Banco</author>
<author>C Mak</author>
<author>V Neveu</author>
<author>Y Djoumbou</author>
<author>R Eisner</author>
<author>A C Guo</author>
<author>D S Wishart</author>
</authors>
<title>DrugBank 3.0: a comprehensive resource for ‘Omics’ research on drugs. Nucleic acids research, 39(suppl 1):D1035–D1041.</title>
<date>2011</date>
<contexts>
<context position="10861" citStr="Knox et al., 2011" startWordPosition="1660" endWordPosition="1663">the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 3.2 Monolingual data The monolingual data is summarized in Table 2. The main sources of the medical-domain monolingual data for all languages involve Wikipedia pages, UMLS concept descriptions, and nonparallel texts extracted from the medical patents of the PatTR collections. For English, the main source is the AACT collection of texts from ClinicalTrials.gov. Smaller resources include: DrugBank (Knox et al., 2011), GENIA (Kim et al., 2003), FMA (Rosse and Mejino Jr., 2008), GREC (Thompson et al., 2009), and PIL (Bouayad-Agha et al., 2000). In the unconstrained systems, we use additional monolingual data from web pages crawled within the Khresmoi project: a collection of about one million HON-certified4 webpages in English released as the test collection for the CLEF 2013 eHealth Task 3 evaluation campaign,5 additional web-crawled HON-certified pages (not publicly available), and other webcrawled medical-domain related webpages. The constrained general-domain resources include: the News corpus for CS, D</context>
</contexts>
<marker>Knox, Law, Jewison, Liu, Ly, Frolkis, Pon, Banco, Mak, Neveu, Djoumbou, Eisner, Guo, Wishart, 2011</marker>
<rawString>C. Knox, V. Law, T. Jewison, P. Liu, Son Ly, A. Frolkis, A. Pon, K. Banco, C. Mak, V. Neveu, Y. Djoumbou, R. Eisner, A. C. Guo, and D. S. Wishart. 2011. DrugBank 3.0: a comprehensive resource for ‘Omics’ research on drugs. Nucleic acids research, 39(suppl 1):D1035–D1041.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>J Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>224--227</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4749" citStr="Koehn and Schroeder, 2007" startWordPosition="721" endWordPosition="724">work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation mo</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>P. Koehn and J. Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 224–227, Prague, Czech Republic. ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>ACL.</publisher>
<location>Praha, Czechia,</location>
<contexts>
<context position="18104" citStr="Koehn et al., 2007" startWordPosition="2780" endWordPosition="2783"> two datasets both for parallel data and for language model data. In the second variant, we train separate models for each section and use linear interpolation to combine them into a single model. For language models, we use the SRILM linear interpolation feature (Stolcke, 2002). We interpolate phrase tables using Tmcombine (Sennrich, 2012). In both cases, the held-out set for minimizing the perplexity is the system development set. 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. 4.4 Results Tables 3 and 4 show case-insensitive BLEU scores of our systems.7 As expected, the unconstrained systems outperform the constrained ones. Linear interpolation outperforms data concatenation qu</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Praha, Czechia, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: a parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Conference Proceedings: the tenth Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<location>Phuket,</location>
<contexts>
<context position="10104" citStr="Koehn, 2005" startWordPosition="1541" endWordPosition="1542"> and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 3.2 Monolingual data The monolingual data is summarized in Table 2. The main sources of the medical-domain monolingual data for all languages involve Wikipedia pages, UMLS concept descriptions, and nonparallel texts extracted from the medical patents of the Pat</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: a parallel corpus for statistical machine translation. In Conference Proceedings: the tenth Machine Translation Summit, pages 79–86, Phuket, Thailand. Asia-Pacific Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kohlsch¨utter</author>
<author>P Fankhauser</author>
<author>W Nejdl</author>
</authors>
<title>Boilerplate detection using shallow text features.</title>
<date>2010</date>
<booktitle>In Proceedings of the ThirdACMInternational Conference on Web Search and Data Mining, WSDM ’10,</booktitle>
<pages>441--450</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Kohlsch¨utter, Fankhauser, Nejdl, 2010</marker>
<rawString>C. Kohlsch¨utter, P. Fankhauser, and W. Nejdl. 2010. Boilerplate detection using shallow text features. In Proceedings of the ThirdACMInternational Conference on Web Search and Data Mining, WSDM ’10, pages 441–450, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Langlais</author>
</authors>
<title>Improving a general-purpose statistical translation engine by terminological lexicons.</title>
<date>2002</date>
<booktitle>In COLING-02 on COMPUTERM 2002: second international workshop on computational terminology,</booktitle>
<volume>14</volume>
<pages>1--7</pages>
<publisher>ACL.</publisher>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="4627" citStr="Langlais, 2002" startWordPosition="706" endWordPosition="707">ranslation of search queries and document summaries. Section 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training co</context>
</contexts>
<marker>Langlais, 2002</marker>
<rawString>P. Langlais. 2002. Improving a general-purpose statistical translation engine by terminological lexicons. In COLING-02 on COMPUTERM 2002: second international workshop on computational terminology, volume 14, pages 1–7, Taipei, Taiwan. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Majliˇs</author>
</authors>
<title>Yet another language identifier.</title>
<date>2012</date>
<booktitle>In Proceedings of the Student Research Workshop at the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>46--54</pages>
<publisher>ACL.</publisher>
<location>Avignon, France.</location>
<marker>Majliˇs, 2012</marker>
<rawString>M. Majliˇs. 2012. Yet another language identifier. In Proceedings of the Student Research Workshop at the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 46–54, Avignon, France. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mansour</author>
<author>J Wuebker</author>
<author>H Ney</author>
</authors>
<title>Combining translation and language model scoring for domain-specific data filtering.</title>
<date>2011</date>
<booktitle>In International Workshop on Spoken Language Translation,</booktitle>
<pages>222--229</pages>
<publisher>ISCA.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="5445" citStr="Mansour et al., 2011" startWordPosition="830" endWordPosition="833">aining phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 Statistical machine translation in the medical domain Eck et al. (2004a) employ an SMT system for the translation of dialogues between doctors and patients and show that according to automatic metrics, a dictionary extracted from the Unified Medical Language System (UMLS) Metathesaurus and its semantic type classification (U.S. National Library of Medicine, 2009) significantly improves translation quality from Spanish to English when applied to generalize the </context>
</contexts>
<marker>Mansour, Wuebker, Ney, 2011</marker>
<rawString>S. Mansour, J. Wuebker, and H. Ney. 2011. Combining translation and language model scoring for domain-specific data filtering. In International Workshop on Spoken Language Translation, pages 222–229, San Francisco, CA, USA. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>W Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>220--224</pages>
<publisher>ACL.</publisher>
<location>Uppsala,</location>
<contexts>
<context position="5323" citStr="Moore and Lewis, 2010" startWordPosition="809" endWordPosition="812">) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 Statistical machine translation in the medical domain Eck et al. (2004a) employ an SMT system for the translation of dialogues between doctors and patients and show that according to automatic metrics, a dictionary extracted from the Unified Medical Language System (UMLS) Metathesaurus and its semantic type classification (U.S. National Lib</context>
<context position="14351" citStr="Moore and Lewis, 2010" startWordPosition="2217" endWordPosition="2220">sed data; we trained recasers to postprocess the final submissions. 4 Submitted systems We first describe our technique of psedo-indomain data selection in Section 4.1, then compare two methods of combining the selected data in Section 4.2. This, along with using constrained and unconstrained data sets to train the systems (see Section 3), amounts to a total of four system variants submitted for each task. A description of the system settings used is given in Section 4.3. 4.1 Data selection We follow an approach originally proposed for selection of monolingual sentences for language modeling (Moore and Lewis, 2010) and its modification applied to selection of parallel sentences (Axelrod et al., 2011). This technique assumes two language models for sentence scoring, one trained on (true) in-domain text and one trained on (any) general-domain text in the same language (e.g., English). For both data domains (general and medical), we score each sentence by the difference of its cross-perplexity given the in-domain language model and cross-perplexity given the general-domain language model (in this order). We only keep sentences with a negative score in our data, assuming that these are the most “medical-lik</context>
<context position="16785" citStr="Moore and Lewis (2010)" startWordPosition="2566" endWordPosition="2569">ined with a restricted vocabulary extracted from the in-domain training data as words occurring at least twice (singletons and other words are treated as out-of-vocabulary). In our experiments, we apply this technique to select both monolingual data for language models and parallel data for translation models. Selection of parallel data is based on the English side only. The in-domain models are trained on the monolingual data in the target language (constrained or unconstrained, depending on the setting). The general-domain models are trained on the WMT News data. Compared to the approach of Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 Data combination For both parallel and monolingual data, we obtain two data sets after applying the data selection: • “medical-like” data from the medical domain • “medical-like” data from the general domain. For each language pair and for each system type (constrained/unconstrained), we submitted two system variants whic</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>R. C. Moore and W. Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the ACL 2010 Conference Short Papers, pages 220– 224, Uppsala, Sweden. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M¨uller</author>
<author>I Gurevych</author>
</authors>
<title>Exploring the potential of semantic relatedness in information retrieval.</title>
<date>2006</date>
<booktitle>In LWA 2006 Lernen – Wissensentdeckung – Adaptivit¨at, 9.-11.10.2006, Hildesheimer Informatikberichte,</booktitle>
<pages>126--131</pages>
<location>Hildesheim, Germany. Universit¨at Hildesheim.</location>
<marker>M¨uller, Gurevych, 2006</marker>
<rawString>C. M¨uller and I. Gurevych. 2006. Exploring the potential of semantic relatedness in information retrieval. In LWA 2006 Lernen – Wissensentdeckung – Adaptivit¨at, 9.-11.10.2006, Hildesheimer Informatikberichte, pages 126–131, Hildesheim, Germany. Universit¨at Hildesheim.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nakov</author>
</authors>
<title>Improving English–Spanish statistical machine translation: Experiments in domain adaptation, sentence paraphrasing, tokenization, and recasing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>147--150</pages>
<publisher>ACL.</publisher>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="4702" citStr="Nakov, 2008" startWordPosition="716" endWordPosition="717">paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; M</context>
</contexts>
<marker>Nakov, 2008</marker>
<rawString>P. Nakov. 2008. Improving English–Spanish statistical machine translation: Experiments in domain adaptation, sentence paraphrasing, tokenization, and recasing. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 147–150, Columbus, OH, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>ACL.</publisher>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="18232" citStr="Och, 2003" startWordPosition="2802" endWordPosition="2803">e linear interpolation to combine them into a single model. For language models, we use the SRILM linear interpolation feature (Stolcke, 2002). We interpolate phrase tables using Tmcombine (Sennrich, 2012). In both cases, the held-out set for minimizing the perplexity is the system development set. 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. 4.4 Results Tables 3 and 4 show case-insensitive BLEU scores of our systems.7 As expected, the unconstrained systems outperform the constrained ones. Linear interpolation outperforms data concatenation quite reliably across language pairs for summary translation. While the picture for query translation is similar, there is more va</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160–167, Morristown, NJ, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pomik´alek</author>
</authors>
<title>Removing Boilerplate and Duplicate Content from Web Corpora.</title>
<date>2011</date>
<tech>PhD thesis,</tech>
<institution>Masaryk University, Faculty of Informatics,</institution>
<location>Brno.</location>
<marker>Pomik´alek, 2011</marker>
<rawString>J. Pomik´alek. 2011. Removing Boilerplate and Duplicate Content from Web Corpora. PhD thesis, Masaryk University, Faculty of Informatics, Brno.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pouliquen</author>
<author>C Mazenc</author>
</authors>
<title>COPPA, CLIR and TAPTA: three tools to assist in overcoming the patent barrier at WIPO.</title>
<date>2011</date>
<booktitle>In Proceedings of the Thirteenth Machine Translation Summit,</booktitle>
<pages>24--30</pages>
<location>Xiamen,</location>
<contexts>
<context position="9829" citStr="Pouliquen and Mazenc, 2011" startWordPosition="1496" endWordPosition="1499"> and non-medical data from patent collections. 3.1 Parallel data The parallel data summary is presented in Table 1. The main sources of the medical-domain data for all the language pairs include the EMEA corpus (Tiedemann, 2009), the UMLS metathesaurus of health and biomedical vocabularies and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12</context>
</contexts>
<marker>Pouliquen, Mazenc, 2011</marker>
<rawString>B. Pouliquen and C. Mazenc. 2011. COPPA, CLIR and TAPTA: three tools to assist in overcoming the patent barrier at WIPO. In Proceedings of the Thirteenth Machine Translation Summit, pages 24–30, Xiamen, China. Asia-Pacific Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Rosse</author>
<author>Jos´e L V Mejino Jr</author>
</authors>
<title>The foundational model of anatomy ontology.</title>
<date>2008</date>
<booktitle>Anatomy Ontologies for Bioinformatics,</booktitle>
<volume>6</volume>
<pages>59--117</pages>
<editor>In A. Burger, D. Davidson, and R. Baldock, editors,</editor>
<publisher>Springer</publisher>
<location>London.</location>
<marker>Rosse, Jr, 2008</marker>
<rawString>C. Rosse and Jos´e L. V. Mejino Jr. 2008. The foundational model of anatomy ontology. In A. Burger, D. Davidson, and R. Baldock, editors, Anatomy Ontologies for Bioinformatics, volume 6 of Computational Biology, pages 59–117. Springer London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sanchis-Trilles</author>
<author>F Casacuberta</author>
</authors>
<title>Loglinear weight optimisation via Bayesian adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>1077--1085</pages>
<publisher>ACL.</publisher>
<location>Beijing, China.</location>
<contexts>
<context position="4666" citStr="Sanchis-Trilles and Casacuberta, 2010" startWordPosition="708" endWordPosition="711">arch queries and document summaries. Section 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt l</context>
</contexts>
<marker>Sanchis-Trilles, Casacuberta, 2010</marker>
<rawString>G. Sanchis-Trilles and F. Casacuberta. 2010. Loglinear weight optimisation via Bayesian adaptation in statistical machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 1077–1085, Beijing, China. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sennrich</author>
</authors>
<title>Perplexity minimization for translation model domain adaptation in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>539--549</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="17827" citStr="Sennrich, 2012" startWordPosition="2736" endWordPosition="2737">ical domain • “medical-like” data from the general domain. For each language pair and for each system type (constrained/unconstrained), we submitted two system variants which differ in how the selected data are combined. The first variant uses a simple concatenation of the two datasets both for parallel data and for language model data. In the second variant, we train separate models for each section and use linear interpolation to combine them into a single model. For language models, we use the SRILM linear interpolation feature (Stolcke, 2002). We interpolate phrase tables using Tmcombine (Sennrich, 2012). In both cases, the held-out set for minimizing the perplexity is the system development set. 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both c</context>
</contexts>
<marker>Sennrich, 2012</marker>
<rawString>R. Sennrich. 2012. Perplexity minimization for translation model domain adaptation in statistical machine translation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 539–549. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Smith</author>
<author>H Saint-Amand</author>
<author>M Plamada</author>
<author>P Koehn</author>
<author>C Callison-Burch</author>
<author>A Lopez</author>
</authors>
<title>Dirt cheap web-scale parallel text from the common crawl.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1374--1383</pages>
<publisher>ACL.</publisher>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="10070" citStr="Smith et al., 2013" startWordPosition="1533" endWordPosition="1536">rus of health and biomedical vocabularies and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 3.2 Monolingual data The monolingual data is summarized in Table 2. The main sources of the medical-domain monolingual data for all languages involve Wikipedia pages, UMLS concept descriptions, and nonparallel texts extracted f</context>
</contexts>
<marker>Smith, Saint-Amand, Plamada, Koehn, Callison-Burch, Lopez, 2013</marker>
<rawString>J. R. Smith, H. Saint-Amand, M. Plamada, P. Koehn, C. Callison-Burch, and A. Lopez. 2013. Dirt cheap web-scale parallel text from the common crawl. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1374–1383, Sofia, Bulgaria. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="17764" citStr="Stolcke, 2002" startWordPosition="2727" endWordPosition="2728">pplying the data selection: • “medical-like” data from the medical domain • “medical-like” data from the general domain. For each language pair and for each system type (constrained/unconstrained), we submitted two system variants which differ in how the selected data are combined. The first variant uses a simple concatenation of the two datasets both for parallel data and for language model data. In the second variant, we train separate models for each section and use linear interpolation to combine them into a single model. For language models, we use the SRILM linear interpolation feature (Stolcke, 2002). We interpolate phrase tables using Tmcombine (Sennrich, 2012). In both cases, the held-out set for minimizing the perplexity is the system development set. 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings of International Conference on Spoken Language Processing, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Thompson</author>
<author>S Iqbal</author>
<author>J McNaught</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Construction of an annotated corpus to support biomedical information extraction.</title>
<date>2009</date>
<journal>BMC bioinformatics,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="10951" citStr="Thompson et al., 2009" startWordPosition="1676" endWordPosition="1679">ta from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 3.2 Monolingual data The monolingual data is summarized in Table 2. The main sources of the medical-domain monolingual data for all languages involve Wikipedia pages, UMLS concept descriptions, and nonparallel texts extracted from the medical patents of the PatTR collections. For English, the main source is the AACT collection of texts from ClinicalTrials.gov. Smaller resources include: DrugBank (Knox et al., 2011), GENIA (Kim et al., 2003), FMA (Rosse and Mejino Jr., 2008), GREC (Thompson et al., 2009), and PIL (Bouayad-Agha et al., 2000). In the unconstrained systems, we use additional monolingual data from web pages crawled within the Khresmoi project: a collection of about one million HON-certified4 webpages in English released as the test collection for the CLEF 2013 eHealth Task 3 evaluation campaign,5 additional web-crawled HON-certified pages (not publicly available), and other webcrawled medical-domain related webpages. The constrained general-domain resources include: the News corpus for CS, DE, EN, and FR collected for the purpose of the WMT 2014 Standard Task, monolingual parts o</context>
</contexts>
<marker>Thompson, Iqbal, McNaught, Ananiadou, 2009</marker>
<rawString>P. Thompson, S. Iqbal, J. McNaught, and Sophia Ananiadou. 2009. Construction of an annotated corpus to support biomedical information extraction. BMC bioinformatics, 10(1):349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tiedemann</author>
</authors>
<title>News from OPUS – a collection of multilingual parallel corpora with tools and interfaces.</title>
<date>2009</date>
<booktitle>In Recent Advances in Natural Language Processing,</booktitle>
<volume>5</volume>
<pages>237--248</pages>
<location>Borovets, Bulgaria. John Benjamins.</location>
<contexts>
<context position="9430" citStr="Tiedemann, 2009" startWordPosition="1441" endWordPosition="1442">32,107 627,493 1,728,065 1,837,457 med unc 3,275,272 36,348 361,881 908,911 gen unc 618,084 – 339,595 204,025 Table 2: Number of tokens in monolingual training data (in thousands). which comes from a mixture of various different domains, mostly news, parliament proceedings, web-crawls, etc.). The unconstrained section contains automatically crawled data from medical and health websites and non-medical data from patent collections. 3.1 Parallel data The parallel data summary is presented in Table 1. The main sources of the medical-domain data for all the language pairs include the EMEA corpus (Tiedemann, 2009), the UMLS metathesaurus of health and biomedical vocabularies and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language</context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>J. Tiedemann. 2009. News from OPUS – a collection of multilingual parallel corpora with tools and interfaces. In Recent Advances in Natural Language Processing, volume 5, pages 237–248, Borovets, Bulgaria. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U S National</author>
</authors>
<title>Library of Medicine.</title>
<date>2009</date>
<booktitle>UMLS reference manual. Metathesaurus.</booktitle>
<location>Bethesda, MD, USA.</location>
<marker>National, 2009</marker>
<rawString>U.S. National Library of Medicine. 2009. UMLS reference manual. Metathesaurus. Bethesda, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W¨aschle</author>
<author>S Riezler</author>
</authors>
<title>Analyzing parallelism and domain similarities in the MAREC patent corpus.</title>
<date>2012</date>
<journal>Multidisciplinary Information Retrieval,</journal>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>7356</volume>
<pages>12--27</pages>
<editor>In M. Salampasis and B. Larsen, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<marker>W¨aschle, Riezler, 2012</marker>
<rawString>K. W¨aschle and S. Riezler. 2012. Analyzing parallelism and domain similarities in the MAREC patent corpus. In M. Salampasis and B. Larsen, editors, Multidisciplinary Information Retrieval, volume 7356 of Lecture Notes in Computer Science, pages 12–27. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wu</author>
<author>H Wang</author>
</authors>
<title>Improving domain-specific word alignment with a general bilingual corpus.</title>
<date>2004</date>
<booktitle>Machine Translation: From Real Users to Research,</booktitle>
<volume>3265</volume>
<pages>262--271</pages>
<editor>In Robert E. Frederking and Kathryn B. Taylor, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="4769" citStr="Wu and Wang (2004)" startWordPosition="725" endWordPosition="728"> approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et </context>
</contexts>
<marker>Wu, Wang, 2004</marker>
<rawString>H. Wu and H. Wang. 2004. Improving domain-specific word alignment with a general bilingual corpus. In Robert E. Frederking and Kathryn B. Taylor, editors, Machine Translation: From Real Users to Research, volume 3265 of Lecture Notes in Computer Science, pages 262–271. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wu</author>
<author>F Xia</author>
<author>L Deleger</author>
<author>I Solti</author>
</authors>
<title>Statistical machine translation for biomedical text: are we there yet? AMIA Annual Symposium proceedings,</title>
<date>2011</date>
<pages>1290--1299</pages>
<contexts>
<context position="6076" citStr="Wu et al. (2011)" startWordPosition="927" endWordPosition="930">aches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 Statistical machine translation in the medical domain Eck et al. (2004a) employ an SMT system for the translation of dialogues between doctors and patients and show that according to automatic metrics, a dictionary extracted from the Unified Medical Language System (UMLS) Metathesaurus and its semantic type classification (U.S. National Library of Medicine, 2009) significantly improves translation quality from Spanish to English when applied to generalize the training data. Wu et al. (2011) analyze the quality of MT on PubMed2 titles and whether it is sufficient for patients. The conclusions are very positive especially for languages with large training resources (English, Spanish, German) – the average fluency and content scores (based on human evaluation) are above four on a five-point scale. In automatic evaluation, their systems substantially outperform Google Translate. However, the SMT systems are specifically trained, tuned, and tested on the domain of PubMed titles, and it is not evident how they would perform on other medical texts. Costa-juss`a et al. (2012) are less o</context>
</contexts>
<marker>Wu, Xia, Deleger, Solti, 2011</marker>
<rawString>C. Wu, F. Xia, L. Deleger, and I. Solti. 2011. Statistical machine translation for biomedical text: are we there yet? AMIA Annual Symposium proceedings, pages 1290–1299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zeman</author>
</authors>
<title>Data issues of the multilingual translation matrix.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>395--400</pages>
<publisher>ACL.</publisher>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="13234" citStr="Zeman, 2012" startWordPosition="2043" endWordPosition="2044">ages. The YALI language detection tool (Majliˇs, 2012) trained on both in-domain and general domain data then filtered out those cleaned pages which were not identified as written in one of the concerned languages. The rest of the preprocessing procedure was applied to all the datasets mentioned above, both parallel and monolingual. The data were tokenized and normalized by converting or omitting some (mostly punctuation) characters. A set of language-dependent heuristics was applied in an attempt to restore and normalize the opening/closing quotation marks, i.e. convert &amp;quot;quoted&amp;quot; to “quoted” (Zeman, 2012). The motivation here is twofold: First, we hope that paired quotation marks could occasionally work as brackets and better denote parallel phrases for Moses; second, if Moses learns to output directed quotation marks, the subsequent detokenization will be easier. For all systems which translate from German, decompounding is employed to reduce source-side data sparsity. We used BananaSplit for this task (M¨uller and Gurevych, 2006). We perform all training and internal evaluation on lowercased data; we trained recasers to postprocess the final submissions. 4 Submitted systems We first describe</context>
</contexts>
<marker>Zeman, 2012</marker>
<rawString>D. Zeman. 2012. Data issues of the multilingual translation matrix. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 395– 400, Montr´eal, Canada. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
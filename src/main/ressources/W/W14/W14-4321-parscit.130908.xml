<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001673">
<title confidence="0.961416">
The Role of Polarity in Inferring Acceptance and Rejection in Dialogue
</title>
<author confidence="0.949136">
Julian J. Schl¨oder and Raquel Fern´andez
</author>
<affiliation confidence="0.9879875">
Institute for Logic, Language &amp; Computation
University of Amsterdam
</affiliation>
<email confidence="0.974427">
julian.schloeder@gmail.com, raquel.fernandez@uva.nl
</email>
<sectionHeader confidence="0.993485" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965615384615">
We study the role that logical polarity
plays in determining the rejection or ac-
ceptance function of an utterance in dia-
logue. We develop a model inspired by re-
cent work on the semantics of negation
and polarity particles and test it on annota-
ted data from two spoken dialogue corpo-
ra: the Switchboard Corpus and the AMI
Meeting Corpus. Our experiments show
that taking into account the relative pola-
rity of a proposal under discussion and of
its response greatly helps to distinguish re-
jections from acceptances in both corpora.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952821428571">
In order to establish and maintain coherence, dia-
logue participants need to keep track of the infor-
mation they jointly take for granted—their com-
mon ground (Stalnaker, 1978). As a dialogue
progresses, the common ground typically evolves.
New information becomes shared as the interlocu-
tors exchange moves (such as assertions, ques-
tions, acceptances, and rejections) through the col-
laborative process of grounding (Clark and Schae-
fer, 1989; Clark, 1996). To keep track of the com-
mon ground, speakers must identify which infor-
mation is accepted or rejected by their addressees.
The basic idea is simple: If a proposal is rejected,
its content does not enter the common ground,
while if it is accepted, its content does become
common belief.
Yet, determining whether a response to a move
counts as an acceptance or a rejection is far from
trivial. In many cases, the surface form of an
utterance is not explicit enough to determine its
acceptance or rejection force and inference is re-
quired (Horn, 1989; Lascarides and Asher, 2009;
Walker, 1996). For instance, B’s utterance in (1),
extracted from the AMI Meeting Corpus (Carletta,
2007), exemplifies what Walker (1996) calls im-
plicature rejection (the rejection arises from an
inferred scalar implicature: “normal” implicates
“not interesting”; see also Hirschberg (1985)).
</bodyText>
<listItem confidence="0.995716">
(1) A: This is a very interesting design.
B: It’s just the same as normal.
</listItem>
<bodyText confidence="0.9979975">
The goal of this paper is to investigate the role of
logical polarity in distinguishing rejections from
acceptances. Consider the following dialogue ex-
cerpts, again from AMI, where the same utterance
form (“Yes it is”) acts as an acceptance in (2) and
as a rejection in (3):
</bodyText>
<listItem confidence="0.9986082">
(2) A: But it’s uh yeah it’s uh original idea.
B: Yes it is.
(3) A: the shape of a banana is not it’s not really
handy .
B: Yes it is.
</listItem>
<bodyText confidence="0.999909652173913">
To determine whether B’s utterance in either case
above functions as an acceptance or a rejection,
it is critical to not only look beyond the utterance
itself and take into account the proposal under dis-
cussion (A’s utterance), but also to specify (a) the
polarity (positive vs. negative) of both the proposal
and the response, and (b) how these polarities in-
teract to give rise to a particular interpretation. Our
aim in this paper is to develop a model of how
logical polarity influences acceptance/rejection in-
terpretation, inspired by recent work on the se-
mantics of negation and polarity particles (Cooper
and Ginzburg, 2011; Cooper and Ginzburg, 2012;
Farkas and Roelofsen, 2013), and to test it on an-
notated data from two spoken dialogue corpora:
the Switchboard Corpus (Godfrey et al., 1992) and
the AMI Meeting Corpus (Carletta, 2007).
In the next section, we give an overview of re-
lated computational work on acceptance/rejection
detection. In Section 3, we first briefly review re-
cent formal semantics approaches to polarity and
then present our model of logical polarity in ac-
ceptance and rejection moves. Section 4 describes
</bodyText>
<page confidence="0.978614">
151
</page>
<note confidence="0.7324995">
Proceedings of the SIGDIAL 2014 Conference, pages 151–160,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9977765">
our experiments: We derive machine learning fea-
tures from our polarity theory and test them in
Switchboard and AMI datasets, achieving compet-
itive F-scores of around 60 on the task of retriev-
ing rejections. We conclude in Section 5 with a
discussion of our results.
</bodyText>
<sectionHeader confidence="0.998207" genericHeader="introduction">
2 Related Computational Work
</sectionHeader>
<bodyText confidence="0.999839053191489">
The first attempts to automatically identify accep-
tances and rejections (often referred to as agree-
ments and disagreements) were carried out in the
context of multiparty meetings for the purpose
of dialogue summarisation tasks. Hillard et al.
(2003) and Hahn et al. (2006) used the ICSI Meet-
ing Corpus (Janin et al., 2003) to develop sys-
tems that would classify utterances into agree-
ments, disagreements, backchannels, and ‘other’.
While these authors only leveraged lexical and
prosodic features of the utterance to be classified
(i.e., local features), Galley et al. (2004) showed
that accuracy could be improved by taking into ac-
count contextual dependencies, in particular pre-
vious (dis)agreements between the dialogue par-
ticipants, achieving an overall accuracy of 86.9%.
Subsequent work built on Galley et al.’s approach
showed that detecting agreement acts helped to
identify public commitments to tasks (Purver et
al., 2007) and other decisions made in a meeting
(Fern´andez et al., 2008).
A difficulty shared by all approaches mentioned
above is the skewness of the data, not only regard-
ing (dis)agreement vs. other types of acts, but also
agreement vs. disagreement. In the dialogue set-
tings considered, acceptance/agreement is much
more common than rejection/disagreement (e.g.,
11.9% vs. 6.7% in the portion of the ICSI Meet-
ing Corpus used by Galley et al. (2004) and 3.6%
vs. 0.4% in the section of the AMI Meeting Cor-
pus used by Germesin and Wilson (2009)). This
can lead to reasonable overall accuracy but poor
results on recognising rejections. Indeed, Ger-
mesin and Wilson (2009), who apply an approach
based on Galley et al. (2004) to the AMI Meet-
ing Corpus, achieve 98.1% accuracy, but report
0% recall for rejections/disagreements. Wang et
al. (2011), who also work with AMI data, use dif-
ferent resampling methods to balance their dataset
and then apply Conditional Random Fields (using
therefore contextual information from sequences
of utterances), achieving 56.9% recall and 55.9 F1
for disagreement detection.
Some recent work has moved away from spoken
dialogue to address similar tasks in online discus-
sion forums. An advantage of this kind of sce-
narios is that they seem to offer more opportu-
nity for disagreement/rejection, thereby yielding
more inherently balanced datasets. Abbott et al.
(2011) and Misra and Walker (2013) use the In-
ternet Argument Corpus (Walker et al., 2012), an
annotated collection of posts in discussion forums
with a balanced distribution of agreeing and dis-
agreeing posts. They address a 2-way classifica-
tion task—determining whether each response to
a post (or to a quoted portion of a post in the case
of Abbott et al. (2011)) is either an agreement or
a disagreement—using a collection of features in-
spired by previous computational and theoretical
approaches. The system developed by Misra and
Walker (2013) uses only local features of the to-
be-classified post, achieving an accuracy of 66%
(over a 50% baseline). Abbott et al. (2011)’s best
system uses features from both the quoted post and
the response post, achieving an accuracy of 68.2%.
However adding this contextual information does
not significantly outperform a system based only
on local features of the response, which yields
66.6% accuracy. Using both features from the post
and the post response, Yin et al. (2012) obtain sim-
ilar results: 68% accuracy on a different online
corpus (the Political Forum), where the datasets
are not balanced (they report a ratio of about 2 to
1 for agreement vs. disagreement).
All in all, this body of work has identified
several linguistic features that are useful for in-
ferring acceptances and rejections, often build-
ing on observations made by conversational an-
alysts (Pomerantz, 1984; Brown and Levinson,
1987). Furthermore, recent work by Bousmalis
et al. (2013) suggests that there are specific non-
verbal behaviours associated with agreement and
disagreement, such as different types of head, lip,
and hand movements. However, to our knowl-
edge, the role of logical polarity has not been
investigated in any detail by computational ap-
proaches. Several systems make use of subjective
polarity, i.e., sentiment. For instance, Galley et al.
(2004) use the list of subjective adjectives com-
piled by Hatzivassiloglou and McKeown (1997)
to assign a positive and a negative polarity value
to an utterance given the number of subjective
positive/negative adjectives it contains. Similarly,
Misra and Walker (2013) use the MPQA Subjec-
</bodyText>
<page confidence="0.997461">
152
</page>
<bodyText confidence="0.999674928571428">
tivity Lexicon (Wilson et al., 2005) to capture the
local sentiment of an online post response given
the number of words in the response with strongly
subjective positive/negative polarity according to
the subjectivity lexicon. Yin et al. (2012) assign a
positive and a negative score to a post by aggregat-
ing the sentiment scores of those words that can be
found in SentiWordNet (Baccianella et al., 2010).
Although subjective polarity may be helpful
(e.g., utterances with a high positive sentiment
score may be more likely to be acceptances), this
is not the kind of polarity that concerns us in this
paper. Note, furthermore, that local sentiment in-
formation may be superseded by logical polarity.
</bodyText>
<listItem confidence="0.997978666666667">
(4) A: But then it wouldn’t sit as comfortably in
your hand.
B: It would still be comfortable.
</listItem>
<bodyText confidence="0.988861625">
Despite the fact that B’s utterance in (4)—
extracted from the AMI corpus—would be as-
signed a positive sentiment score (given the pres-
ence of the word “comfortable”, classified as pos-
itive in the MPQA Subjectivity Lexicon, and the
absence of negative subjective words), the utter-
ance acts as a rejection due to logical polarity con-
straints, as we shall make clear in the next section.
</bodyText>
<sectionHeader confidence="0.939431" genericHeader="method">
3 Polarity in Acceptances and Rejections
</sectionHeader>
<bodyText confidence="0.9999312">
In this section, we first give a brief overview of
some of the main ideas put forward in recent theo-
retical approaches to polarity. Afterwards, we in-
troduce our approach to logical polarity in the con-
text of acceptance and rejection moves.
</bodyText>
<subsectionHeader confidence="0.997367">
3.1 Formal Semantics Approaches
</subsectionHeader>
<bodyText confidence="0.997614333333333">
Polarity and in particular negation are central con-
cepts in formal semantics and pragmatics (Horn,
1989). Recent work independently put forward
within the frameworks of Type Theory with
Records (Cooper and Ginzburg, 2011; 2012) and
of Inquisitive Semantics (Farkas and Roelofsen,
2013) has proposed to semantically distinguish be-
tween positive and negative propositions. Such a
proposal departs from the traditional view in for-
mal semantics where propositions are taken to de-
note sets of possible worlds (see Partee (1989) for
a survey). According to this traditional view, the
meaning of (5a) would be indistinguishable from
that of (5b), given that the two propositions are
true in exactly the same possible worlds:
</bodyText>
<listItem confidence="0.8974125">
(5) a. Sue failed the exam.
b. Sue didn’t pass the exam.
</listItem>
<bodyText confidence="0.999774043478261">
These utterances, however, license different types
of responses. For instance, responding “no” to
(5a) would assert that Sue did pass the exam,
while the same response to (5b) would typically
be understood as asserting the opposite. Leaving
aside many details that distinguish the two theo-
ries, Cooper/Ginzburg and Farkas/Roelofsen pro-
pose that polarity particles—words like “yes” and
“no”—are sensitive to the polarity of their an-
tecedent: “yes” presupposes that a positive propo-
sition is under discussion, while “no” presupposes
a negative proposition. If the presupposition is
met, both “yes” and “no” assert the proposition
under discussion (i.e., in our terms, they act as ac-
ceptances); if the presupposition fails, they assert
the negation of the proposition under discussion
(i.e., they act as rejections).
This characterises the standard behaviour of po-
larity particles. However, the picture is slightly
more complicated since, when the proposition un-
der discussion is negative, in English “yes” and
“no” can also be used to agree or disagree, respec-
tively (contrary to the standard case):
</bodyText>
<listItem confidence="0.9556732">
(6) Sue didn’t pass the exam.
a. No (she didn’t). --* standard acceptance
Yes, she didn’t.
b. Yes, she did. / #Yes. --* standard rejection
No, she did.
</listItem>
<bodyText confidence="0.994288086956521">
According to Farkas and Roelofsen (2013), this
ambiguity of use makes bare forms of “yes”/“no”
less likely in the non-standard cases exemplified
in (6) and favours more explicit sentential forms
where the presence of the verb disambiguates the
intended interpretation. In this respect, however,
the standard rejection in (6b) constitutes a spe-
cial case: While in standard acceptances the sen-
tential form is not required, in standard rejections
it seems needed. According to these authors, in
English the positive polarity particle “yes” has a
strong preference for realising an agreement move
and therefore its use as a rejection is marked.1
This makes the explicit sentential form “Yes, she
did” in (6b) more felicitous than the bare form
“Yes”. Thus, the two types of rejections to a neg-
ative proposition we see in (6b)—with “yes” and
1The special status of English “yes” for rejection seems
to be supported by cross-linguistic evidence. For instance,
German has a special positive polarity particle “doch” for
rejecting a negative proposition: in response to the assertion
in (6), “doch” would be used to disagree (“yes, she did”)
while “ja” would be used to agree (“yes, she didn’t”).
</bodyText>
<page confidence="0.995429">
153
</page>
<subsectionHeader confidence="0.594767">
Polarity of P-R Type Example from AMI Meeting Corpus
</subsectionHeader>
<bodyText confidence="0.96794625">
positive – positive default relative acceptance A: And then you can buy the covers. B: Yes
negative– negative reverse relative acceptance A: It’s not very well advertised. B: No, it’s not.
positive – negative default relative rejection A: It’s a frog. B: No, it’s a turtle.
negative– positive reverse relative rejection A: TVs aren’t capable of sending. B: Yes, they are.
</bodyText>
<tableCaption confidence="0.98758">
Table 1: Relative response types.
</tableCaption>
<bodyText confidence="0.997939666666667">
“no”—are expected to contain an explicit verbal
constituent. We refer to this as the markedness ex-
pectation.
</bodyText>
<subsectionHeader confidence="0.992668">
3.2 Our Model
</subsectionHeader>
<bodyText confidence="0.999773277777778">
Our aim is to exploit insights from the theo-
ries sketched above to develop a model that can
be operationalised in a computational setting to
test whether information regarding logical polarity
can contribute to automatically distinguish accep-
tances from rejections.
We focus on proposal-response pairs (P-R),
where R either accepts or rejects P. We pro-
pose to assign both the proposal and the response
a logical polarity: either positive or negative. Fur-
thermore, we differentiate absolute (polarity inde-
pendent) from relative (polarity independent) re-
sponses. A response type R is absolute if its ac-
ceptance/rejection function does not depend on the
polarity of P, and it is relative if it does. Formally,
we say that a proposal P is rejected by a response
R if P A R is inconsistent. This gives us the fol-
lowing four possible responses to P:
</bodyText>
<listItem confidence="0.99981675">
• R - T : absolute acceptance
• R - L : absolute rejection
• R - P : relative acceptance
• R -¬P: relative rejection
</listItem>
<bodyText confidence="0.999903482758621">
Our focus of attention is on relative responses.
Given a P-R pair with a relative response, we infer
an acceptance if the polarities of P and R align,
and a rejection if the polarities differ. This gives
us four possible relative responses, shown in Ta-
ble 1. In the default cases, where P is positive,
positive responses act as acceptances and nega-
tive responses as rejections—exactly as absolute
response types would act. When P is negative
(i.e., P - ¬P0), we are faced with what we call
reverse relative responses: Negative polarity re-
sponses act as acceptances and positive polarity
responses as rejections. An acceptance can have
the form R - P - ¬P0 while a rejection can have
the form R - ¬P - P0 (with R being positive,
i.e., with the double negation ¬¬P0 eliminated).
We call these cases reverse responses because their
polarity signature is precisely the negation of the
respective default cases (cf. Table 1).
The next obvious question to address is how the
polarity of proposals and responses can be deter-
mined. Clearly, this will differ across languages.
For the case of English, we shall assume that po-
larity is linked to the presence of particular par-
ticles and grammatical indicators. In particular,
we consider the words in Table 2 to be positive
and negative polarity markers.2 Amongst negative
polarity markers, we distinguish between negative
polarity particles and negation indicators.
</bodyText>
<construct confidence="0.85151125">
positive particles: yes, yeah, yep
negative particles: no, nope, nah
negation: not, -n’t, never,
nothing, nobody, nowhere
</construct>
<tableCaption confidence="0.976451">
Table 2: Polarity markers.
</tableCaption>
<bodyText confidence="0.9995565">
All markers in Table 2 are key cues of polar-
ity. However, they do not straightforwardly deter-
mine the polarity of a contribution. Firstly, there
are cases where the presence of a marker does not
have the expected effect on polarity. For instance,
a negative tag question (“isn’t it?”) at the end of
an utterance does not mark that utterance as neg-
ative. Also, the polarity effect of a marker can be
invalidated if it is followed by the contrast con-
nective “but”. For instance, in the following AMI
examples, “but” cancels out the effect of the neg-
ative polarity particle “no” in (7), making B’s ut-
terance positive, and the effect of the positive po-
larity particle “yeah” in (8), making B’s utterance
negative (in conjunction with the verbal negation
in this case):
</bodyText>
<listItem confidence="0.97823725">
(7) Reverse rejection: negative–positive
A: Yes, but some televisions don’t support it.
B: No, but then they would also support that
button, because it’s the same thing.
(8) Default rejection: positive–negative
A: Yeah, uh materials like wood that
B: Yeah, but wood is not a not a material you
which you build a a remote control of .
</listItem>
<footnote confidence="0.921914">
2We do not claim that this list is exhaustive.
</footnote>
<page confidence="0.999341">
154
</page>
<bodyText confidence="0.9940674">
Secondly, it is important to take into account that
a large amount of acceptances and rejections do
not include any marker of polarity at all. For in-
stance, in our datasets extracted from the AMI and
Switchboard (SWB) corpora (which we will de-
scribe in detail in Section 4.1), 49% and 70% of
acceptances in AMI and SWB, respectively, do not
contain any explicit polarity marker; and similarly
for 40% (AMI) and 15% (SWB) of rejections.
In part this is due to the fact that in English (as
in most languages) there is no morphologically-
realised positive counterpart of verbal negation.
Given the observations above, we adopt the heu-
ristics in Figure 1 to assign a polarity to P and R.
Since this heuristics is intended to be applicable
to dialogue corpora, we forgo the use of deep se-
mantic analysis, which is difficult to achieve when
dealing with naturally occurring spoken language.3
P-polarity: A proposal P has negative polarity if it con-
tains a negation indicator (excluding tag questions); oth-
erwise, P has positive polarity.
R-polarity: We define a precedence order on polarity
markers: negative polarity particles take precedence over
positive polarity particles, which in turn take precedence
over negation indicators.
</bodyText>
<listItem confidence="0.986647">
• If a response R contains a negative polarity particle
(not followed by “but”), its polarity is negative.
• Else, if R contains a positive polarity marker (not fol-
lowed by “but”), its polarity is positive.
• Else, if R contains a negation indicator, its polarity is
negative.
• Otherwise, R has positive polarity.
</listItem>
<figureCaption confidence="0.99889">
Figure 1: Heuristics for polarity determination.
</figureCaption>
<bodyText confidence="0.999969214285714">
Drawing on the notion of markedness expecta-
tion we introduced at the end of Section 3.1, we
hypothesise that the lack of explicit positive polar-
ity markers will be compensated for by the pres-
ence of sentential similarity patterns between pro-
posals and responses. It follows from our descrip-
tion of relative responses (see Table 1) that they
will either semantically mirror the proposal (ac-
ceptances) or negate it (rejections). In the absence
of an explicit positive polarity marker in the pro-
posal or the response, therefore, we expect to find
some form of sentential parallelism, potentially in
both cases—when P-R polarities align, as in (9),
and when they differ, as in (10):4
</bodyText>
<footnote confidence="0.987443333333333">
3Amongst other things, this means we do not account for
the scope of negation.
4Both examples are extracted from the AMI corpus.
</footnote>
<listItem confidence="0.99930525">
(9) A: It’s still it’s still working,
B: It is.
(10) A: It’s a fat cat.
B: It is not a fat cat.
</listItem>
<bodyText confidence="0.999648285714286">
According to the markedness expectation, this
type of parallelism is expected in reverse relative
responses even when polarity particles are present
as in (11) from Switchboard and in the reverse re-
sponse examples in Table 1. Hence, we conjec-
ture that parallelism will be present with higher
frequency in the reverse cases.
</bodyText>
<listItem confidence="0.998887">
(11) A: They wouldn’t be able to own a house.
B: Yes, they would.
</listItem>
<sectionHeader confidence="0.998783" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999993125">
In order to automatically test the extent to which
logical polarity plays a role in determining the
function of naturally occurring acceptances and
rejections, we conduct machine learning experi-
ments on dialogue corpus data. We first explain
how we create our dataset, then describe how we
devise features that encode polarity information,
and finally report the results obtained.
</bodyText>
<subsectionHeader confidence="0.933891">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9997135">
We test our model on two different corpora: The
Switchboard Corpus (SWB) (Godfrey et al., 1992)
and the AMI Meeting Corpus (Carletta, 2007).
SWD is a collection of around 2400 recorded and
transcribed telephone conversations between two
dialogue participants. The speakers are provided
with a topic and then converse freely. In con-
trast, AMI contains transcriptions from around
100 hours of recorded multiparty conversations
amongst four dialogue participants who interact
face-to-face in a meeting setting. The speakers
converse freely, but they play roles (such as in-
dustrial designer or project manager) in a ficti-
tious design team whose goal is to design a re-
mote control. Therefore the dialogue is mildly
task-oriented. Both corpora have been annotated
with dialogue acts (DAs), albeit with slightly dif-
ferent DA annotation schemes: SWD is annotated
with the SWBD-DAMSL tagset (Jurafsky et al.,
1997), while AMI uses a coarser-grained tagset
but includes relations between some DAs (loosely
called adjacency pair annotations).5
</bodyText>
<footnote confidence="0.978693666666667">
5The AMI DA annotation manual is available at
http://mmm.idiap.ch/private/ami/annotation/dialogue_
acts_manual_1.0.pdf
</footnote>
<page confidence="0.995872">
155
</page>
<table confidence="0.859349666666667">
acceptances rejections total P-R
SWB 4534 (97%) 145 (3%) 4679
AMI 7405 (91%) 697 (9%) 8102
</table>
<tableCaption confidence="0.999515">
Table 3: Class distribution in our datasets.
</tableCaption>
<bodyText confidence="0.994574265306123">
We use the DA annotations to extract a
dataset of proposal-response (P-R) pairs for
each corpus as follows. To construct the SWB
dataset, we extract all utterances u annotated
as Agree/Accept or Reject that are turn-
initial and that are immediately preceded by
a turn whose last utterance u&apos; is annotated
as Statement-non-opinion, Statement-
opinion or Summarize/Reformulate. To
construct the AMI dataset, we extract all ut-
terances u annotated as Assessment that are
turn initial and that are linked with the re-
lations Support/Positive Assessment or
Objection/Negative Assessment to an
earlier utterance u&apos; that is not annotated as
Elicit Inform or Elicit Assessment (i.e.,
that is not a question). In both cases, P cor-
responds to u&apos; and R to the first five words
of u. We consider R an acceptance if u is
annotated as Agree/Accept in SWB or as
Support/Positive Assessment in AMI, and a
rejection if it is annotated as Reject in SWB or
as Objection/Negative Assessment in AMI.
We take the first five words of a turn-initial ut-
terance to be the most relevant ones for convey-
ing acceptance or rejection. This is motivated by
the fact that dialogue participants typically provide
evidence of understanding—and, by extension, of
agreement or disagreement—at the earliest oppor-
tunity in order to avoid misunderstandings on what
they take to be common ground (Pomerantz, 1984;
Clark, 1996). However, when extracting our P-R
pairs we retain the entire utterance u (of which R
is a prefix) in order to be able to take its length into
account in the automatic classification experiment,
as explained in the next section.
Finally, we observe that in the two corpora all
the P-R pairs where R is just a single “yeah” are
acceptances. Thus, in the terminology we intro-
duced in Section 3.2, bare “yeah” seems to be an
absolute response type, whose acceptance func-
tion is independent of the polarity of P (in contrast
to the relative response types in Table 1). Since
identification of these acceptances is trivial, we
discard them from our datasets. The final distri-
bution of acceptances and rejections in each of the
datasets is shown in Table 3. As can be seen, the
data is highly skewed, with less than 10% of P-R
pairs corresponding to rejections.
</bodyText>
<subsectionHeader confidence="0.839158">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.9999922">
We derive different types of features to test our
model. We are not interested in using large
amounts of unmotivated features, but rather in ex-
ploiting a small set of meaningful domain- and
setting-independent features that can help us to in-
vestigate the impact of logical polarity. The fea-
ture we use are summarised in Figure 2. We con-
sider several local features of the response. Most
of these features are inspired by earlier approaches
reviewed in Section 2, such as those by Galley et
al. (2004) and Misra and Walker (2013). We use
several lexical features that act as cues for accep-
tance or rejection. For instance, the presence of
“yeah” is a good cue for acceptance, while the
presence of “but” is a strong cue for rejection.
The bigram “yeah, but” is in turn a good indica-
tor for rejection—the “yeah” in such cases seems
to be an attempt at politeness (Brown and Levin-
son, 1987; Bousfield, 2008). Since rejections are
dispreferred moves, they are frequently initiated
with a hedging such as “well” or with hesitation
or stalling (Byron and Heeman, 1997). These
utterance-initial cues are aggregated into one fea-
ture. Rejections also tend to be longer than ac-
ceptances since the speaker feels the need to jus-
tify the unexpected move (Pomerantz, 1984). We
take into account the length of the entire utter-
ance containing R with three binary features.6 We
also consider less frequent semantic indicators for
acceptance and rejection, respectively, which we
group into two aggregate features that record the
presence of agreement words such as “okay” or
“correct” and contrast words such as “however”
or “although”. Given our observations regard-
ing polarity and polarity particles in Section 3, in
contrast to previous approaches we don’t include
“yes” and “no” as local lexical cues. Instead,
we add a new local feature encoding the polarity
of the response as determined by the R-polarity
heuristics in Figure 1.7
</bodyText>
<footnote confidence="0.8656765">
6The use of Boolean features here is motivated by our
choice of classifier, as we point out in the next subsection.
The length thresholds have been set up manually after quali-
tative examination of several examples.
7We have tested other theoretically motivated local fea-
tures, such as turn-length and number of disfluencies. The
local R features in Fig. 2 combined with the local R polarity
feature correspond to the best performing local feature set.
</footnote>
<page confidence="0.998484">
156
</page>
<bodyText confidence="0.999955315789474">
Local features cannot capture the most interes-
ting aspects of logical polarity, which originate
from the interaction between the polarities of the
proposal and the response in relative response
types. To account for this, we introduce four rela-
tive P-R polarity features corresponding to the re-
sponse types described in Table 1. Finally, we in-
troduce a feature that records the presence of some
form of parallelism between P and R. As we men-
tioned at the end of Section 3.2, the markedness
expectation predicts that sentential parallelism
will occur more frequently in reverse relative re-
sponses, i.e., responses to negative proposals. The
parallelism feature targets such cases. We restrict
ourselves to strict identity between P and R of
a pronominal subject and a verb (in negative vs.
positive form).8 The feature therefore is only able
to capture examples such as (12a) but not (12b),
where anaphora resolution would be required.9
</bodyText>
<listItem confidence="0.92922725">
(12) a. A: But it wouldn’t be very attractive.
B: No, it would.
b. A: TVs aren’t capable of sending.
B: Yes, they are.
</listItem>
<subsectionHeader confidence="0.960094">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999900090909091">
We conducted the machine learning experiment
using BernoulliNB, the Bernoulli-distributed
Naive Bayesian classifier from scikit-learn (Pe-
dregosa et al., 2011), which outperformed sev-
eral other classifiers, including Random Forests
and a Support Vector Machine. We chose this
classifier because our main features—the relative
polarities—are Boolean and our data is highly im-
balanced.10 Given the high relative frequency of
acceptances over rejections in our datasets (see
Table 3), measuring accuracy or retrieving accep-
tances would yield very good results. Hence, as
discussed in section 2, we believe that the most
discerning task is the retrieval of rejections. Pre-
cision, recall and F-scores for this task, with the
classifier trained on different combinations of fea-
ture sets, are shown in Table 4. We developed the
classifier on the whole AMI dataset, as the small
number of rejections makes splitting up the cor-
pus into a development and a test set infeasible.
The SWB corpus was exclusively used for testing.
In the AMI dataset we tested the classifier with
</bodyText>
<footnote confidence="0.999212666666667">
8We use the NLTK POS tagger to implement this feature
(Bird et al., 2009).
9Given the high frequency of pronominal forms in spoken
dialogue, pronoun identity turns out to be reasonably useful.
10The scikit-learn documentation indicates that this classi-
fier is particularly suited for sparse data and Boolean features.
</footnote>
<table confidence="0.83952">
LOCAL R FEATURES
Length of utterance containing R in number of words:
</table>
<listItem confidence="0.9895351">
• Three features: l &gt;2, l &gt;12, l &gt;24
Acceptance Indicators:
• R contains yeah
• R contains any of absolutely, okay, accept, agree, cor-
rect, either, true, sure, not preceded by not
Rejection Indicators:
• R contains but
• R contains the bigram ‘yeah, but’
• R starts with any of well, oh, uh, mm
• R contains any of actually, however, though, although
</listItem>
<figure confidence="0.696318875">
LOCAL R POLARITY FEATURE
• positive or negative, according to R-polarity in Fig. 1
RELATIVE P-R POLARITY FEATURES (cf. Fig. 1)
• positive–positive
• positive–negative
• negative–negative
• negative–positive
RELATIVE P-R PARALLELISM FEATURE
</figure>
<bodyText confidence="0.949773333333333">
One of the following patterns appears in P-R, where a
pronoun p, an auxiliary verb aux and a main verb v are
identical in P and R:
</bodyText>
<listItem confidence="0.888278333333333">
• ‘p aux not’ – ‘p aux’ not followed by {n’t |not}
• ‘p (aux) not v’ – ‘p v’
• ‘I do{n’t |not} {think|know} {that|if} p aux’ –
</listItem>
<bodyText confidence="0.22198">
‘p aux’ not followed by {n’t |not}
</bodyText>
<figureCaption confidence="0.992174">
Figure 2: Feature types (all features are Boolean).
</figureCaption>
<bodyText confidence="0.999196869565217">
10-fold cross-validation and in the SWB dataset
with 5-fold cross-validation, due to the more lim-
ited amount of rejections in this corpus. Also, due
to the lack of training data, the more specific Rel-
ative P-R Parallelism feature could not be applied
to the SWB corpus.
For comparison we report the results of a sim-
ple unigram baseline: Each content word that oc-
curs at least 5 times in the dataset is used as a
Boolean feature (occurrence vs. non-occurrence).
This achieves F-scores of 31.66 in AMI and 16.63
in SWB. As a more substantial baseline we con-
sider a system that uses only local features of the
response, including local polarity. This feature-
set is expected to capture relatively well the ac-
cepting/rejecting function of absolute responses
and default relative responses, since their function
aligns with their local polarity. This yields an F-
score of 52.24 in AMI and of 33 in SWB. The Rel-
ative Polarity features were conceived to reduce
classification confusion grounded in reverse polar-
ity: If only local features are considered, a reverse
polarity acceptance would appear to be a rejection,
</bodyText>
<page confidence="0.992088">
157
</page>
<table confidence="0.999605166666667">
AMI SWB
Feature sets Precision Recall F1 Precision Recall F1
Unigrams 35.61% 28.97% 31.66 24.20% 12.93% 16.63
Local + Local Polarity 44.13% 64.12% 52.24 20.80% 82.46% 33.00
Local + Relative Polarity 58.08% 61.63% 59.75 49.12% 72.93% 58.49
Local + Relative Pol. + Parallelism 58.23% 64.04% 60.96 n/a n/a n/a
</table>
<tableCaption confidence="0.999747">
Table 4: Precision, Recall, and F-scores for rejection identification.
</tableCaption>
<bodyText confidence="0.9998773">
while a reverse polarity rejection would seem to
be an acceptance. Moving from local to relative
polarity features should therefore reduce this con-
fusion. Indeed, in both corpora the precision is in-
creased substantially (from 44.13% to 58.08% in
AMI and from 20.8% to 49.12% in SWB), caus-
ing a great increase in F-scores: 59.75 in AMI
and 58.49 in SWB (paired t-tests show all these
increases are significant, with p &lt; 0.001).
However, in both datasets we observe a reduc-
tion in recall when moving from local to relative
polarity. We believe that this is in part due to the
relative polarity features ignoring some absolute
uses of polarity particles, which may have been
captured by Local Polarity.11 The Relative Paral-
lelism feature should be able to help in such cases.
For instance, in example (12a) B’s utterance would
be assigned negative polarity and therefore the rel-
ative polarity features would contribute to clas-
sify it as an acceptance (since in the large major-
ity of cases negative-negative P-R pairs do cor-
respond to acceptances). In this case, however,
“no” is used absolutely, i.e., as a rejection. Due to
the markedness expectation, this is likely to show
up in the form of contrastive parallelism, which
we can—at least in part—capture with our sim-
ple feature. Indeed, adding this feature to the AMI
dataset raises recall back to baseline level: 64.04%
vs. 61.63% (p &lt; 0.005). This, in turn, increase the
AMI F-score from 59.75 to 60.96 (p &lt; 0.05).
</bodyText>
<sectionHeader confidence="0.999436" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999935555555556">
The overall aim of this paper has been to investi-
gate the influence of logical polarity in interpret-
ing utterances as acceptance or rejection moves
in dialogue. We have built on recent work on
the semantics of negation and polarity particles by
Cooper and Ginzburg (2011; 2012) and Farkas and
Roelofsen (2013) to develop an approach to polar-
ity that is theoretically motivated and that can be
computationally tested on corpus data. Although
</bodyText>
<footnote confidence="0.98054925">
11We note that the featureset Local + Local Polarity + Rel-
ative Polarity does not outperform Local + Local Polarity in
the classification experiments. We believe this indicates that
polarity is indeed mostly a contextual phenomenon.
</footnote>
<bodyText confidence="0.999748863636364">
there is a substantial amount of previous work on
automatically detecting agreement and disagree-
ment in dialogue corpora, to our knowledge the
role of logical polarity had not been explicitly in-
vestigated before.
Our focus has been on relative responses, i.e.,
responses where simply taking into account clues
from the utterance to be classified is insufficient—
or can even be misleading—to infer acceptance or
rejection. We have argued that relative responses
require taking into account how the polarities of
the response and of the current proposal under dis-
cussion interact, and have put forward a model that
captures such interaction. Our experiments show
that the use of information on relative polarity sub-
stantially helps to distinguish acceptances from re-
jections. This indicates, on the one hand, that our
model does a reasonably good job at capturing this
phenomenon, and on the other hand, that relative
polarity responses are not merely a theoretically
interesting phenomenon but are in fact widespread
in actual dialogue.
There is certainly room for improving the im-
plementation of our heuristics, for instance by
using finer-grained semantic and syntactic infor-
mation: e.g., we cannot currently capture accep-
tance/rejection of a subclause, implicature rejec-
tions, rhetorical questions, nor sarcasm—all of
which affect the recall of our system. Interestingly,
the classification experiments yield very similar
results in the two corpora with the Local + Relative
Polarity feature set—F-scores of 59.75 in AMI
and 58.49 in SWB. This indicates that our theo-
retical observations are applicable independently
of setting, domain and number of speakers. There
seem to be some differences across the two cor-
pora, however, since the impact of relative polarity
information is much higher in SWB than in AMI
(the F-score goes up around 7 in AMI when mov-
ing from local to relative polarity, while in SWB it
increases by 25). A deeper investigation into the
shortcomings of our implemented model and of
where these shortcomings affect AMI differently
than SWB are issues we leave for future work.
</bodyText>
<page confidence="0.997587">
158
</page>
<sectionHeader confidence="0.989249" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99929908411215">
Rob Abbott, Marilyn Walker, Pranav Anand, Jean E.
Fox Tree, Robeson Bowmani, and Joseph King.
2011. How Can You Say Such Things?!?: Rec-
ognizing Disagreement in Informal Political Argu-
ment. In Proceedings of the Workshop on Lan-
guages in Social Media, pages 2–11.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of LREC, pages 2200–2204.
Steven Bird, Edward Loper, and Ewan Klein.
2009. Natural Language Processing with Python.
O’Reilly Media Inc.
Derek Bousfield. 2008. Impoliteness in Interaction.
John Benjamins.
Konstantinos Bousmalis, Marc Mehu, and Maja Pan-
tic. 2013. Towards the Automatic Detection of
Spontaneous Agreement and Disagreement Based
on Nonverbal Behaviour: A Survey of Related Cues,
Databases, and Tools. Image Vision Computing,
31(2):203–221.
Penelope Brown and Stephen Levinson. 1987. Po-
liteness: Some universals in language usage. Cam-
bridge University Press.
Donna K. Byron and Peter A. Heeman. 1997. Dis-
course marker use in task-oriented spoken dialog. In
Proceedings of Eurospeech, pages 2223–2226.
Jean Carletta. 2007. Unleashing the killer corpus:
experiences in creating the multi-everything AMI
Meeting Corpus. Language Resources and Evalu-
ation, 41(2):181–190.
Herbert H. Clark and Edward F. Schaefer. 1989.
Contributing to discourse. Cognitive Science,
13(2):259–294.
Herbert H. Clark. 1996. Using language. Cambridge
University Press.
Robin Cooper and Jonathan Ginzburg. 2011. Nega-
tion in dialogue. In Proceedings of the 15th SemDial
Workshop (Los Angelogue), pages 130–139.
Robin Cooper and Jonathan Ginzburg. 2012. Nega-
tive inquisitiveness and alternatives-based negation.
In Logic, Language and Meaning: Proceedings of
the 18th Amsterdam Colloquium, Lecture Notes in
Computer Science, pages 32–41. Springer.
Donka Farkas and Floris Roelofsen. 2013. Polar ini-
tiatives and polar particle responses in an inquisi-
tive discourse model. Available from http://www.
illc.uva.nl/inquisitivesemantics/.
Raquel Fern´andez, Matthew Frampton, Patrick Ehlen,
Matthew Purver, and Stanley Peters. 2008. Mod-
elling and Detecting Decisions in Multi-Party Dia-
logue. In Proceedings of the 9th SIGdial Workshop
on Discourse and Dialogue, pages 156–163.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Meeting
of the Association for Computational Linguistics
(ACL’04), pages 669–676.
Sebastian Germesin and Theresa Wilson. 2009.
Agreement detection in multiparty conversation. In
Proceedings of the 2009 international conference on
Multimodal interfaces, pages 7–14. ACM.
John J. Godfrey, E. C. Holliman, and J. McDaniel.
1992. SWITCHBOARD: Telephone Speech Cor-
pus for Research and Development. IEEE Confer-
ence on Acoustics, Speech, and Signal Processing,
1:517–520.
Sangyun Hahn, Richard Ladner, and Mari Ostendorf.
2006. Agreement/disagreement classification: Ex-
ploiting unlabeled data using contrast classifiers. In
Proceedings of the HLT-NAACL, pages 53–56.
Vasileios Hatzivassiloglou and Kathleen R McKeown.
1997. Predicting the semantic orientation of ad-
jectives. In Proceedings of the 35th Annual Meet-
ing of the Association for Computational Linguis-
tics and Eighth Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 174–181.
Dustin Hillard, Mari Ostendorf, and Elizabeth
Shriberg. 2003. Detection of agreement vs. dis-
agreement in meetings: training with unlabeled data.
In Proceedings of the HLT-NAACL 2003, pages 34–
36.
Julia L. B. Hirschberg. 1985. A theory of scalar impli-
cature. Ph.D. thesis, University of Pennsylvania.
Laurence R. Horn. 1989. A Natural History of Nega-
tion. University of Chicago Press.
Adam Janin, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin,
Thilo Pfau, Elisabeth Shriberg, Andreas Stolcke,
and Chuck Wooters. 2003. The ICSI Meeting Cor-
pus. In Proceedings of ICASSP’03, pages 364–367.
Dan Jurafsky, Elizabeth Shriberg, and Debra Bi-
asca. 1997. Switchboard SWBD-DAMSL shallow-
discourse-function-annotation coder’s manual, draft
13. Technical Report TR 97-02, Institute for Cogni-
tive Science, University of Colorado at Boulder.
Alex Lascarides and Nicholas Asher. 2009. Agree-
ment, disputes and commitments in dialogue. Jour-
nal of Semantics, 26(2):109–158.
Amita Misra and Marilyn Walker. 2013. Topic in-
dependent identification of agreement and disagree-
ment in social media dialogue. In Proceedings of
the SIGDIAL 2013 Conference, pages 41–50, Metz,
France. Association for Computational Linguistics.
</reference>
<page confidence="0.986209">
159
</page>
<reference confidence="0.9995368">
Barbara Partee. 1989. Possible worlds in model-
theoretic semantics: A linguistic perspective. In
S. Allen, editor, Possible Worlds in Humanities, Arts
and Sciences, pages 93–123. Walter de Gruyter.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
Anita Pomerantz. 1984. Agreeing and disagree-
ing with assessments: Some features of pre-
ferred/dispreferred turn shapes. In Structures of So-
cial Action. Cambridge University Press.
Matthew Purver, John Dowding, John Niekrasz,
Patrick Ehlen, Sharareh Noorbaloochi, and Stanley
Peters. 2007. Detecting and summarizing action
items in multi-party dialogue. In Proceedings of the
8th SIGdial Workshop on Discourse and Dialogue,
pages 18–25.
Robert Stalnaker. 1978. Assertion. In P. Cole, edi-
tor, Pragmatics, volume 9 of Syntax and Semantics,
pages 315–332. New York Academic Press.
Marilyn A Walker, Jean E Fox Tree, Pranav Anand,
Rob Abbott, and Joseph King. 2012. A corpus for
research on deliberation and debate. In LREC, pages
812–817.
Marilyn A. Walker. 1996. Inferring acceptance and re-
jection in dialog by default rules of inference. Lan-
guage and Speech, 39(2-3):265–304.
Wen Wang, Sibel Yaman, Kristin Precoda, Colleen
Richey, and Geoffrey Raymond. 2011. Detection
of agreement and disagreement in broadcast conver-
sations. In Proceedings of ACL, pages 374–378.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP, pages 347–354.
Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris.
2012. Unifying Local and Global Agreement and
Disagreement Classification in Online Debates. In
Proceedings of the 3rd Workshop in Computational
Approaches to Subjectivity and Sentiment Analysis,
pages 61–69.
</reference>
<page confidence="0.99743">
160
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.933424">
<title confidence="0.999968">The Role of Polarity in Inferring Acceptance and Rejection in Dialogue</title>
<author confidence="0.99982">J Schl¨oder</author>
<affiliation confidence="0.9978515">Institute for Logic, Language &amp; University of</affiliation>
<abstract confidence="0.995520142857143">We study the role that logical polarity plays in determining the rejection or acceptance function of an utterance in dialogue. We develop a model inspired by recent work on the semantics of negation and polarity particles and test it on annotated data from two spoken dialogue corpora: the Switchboard Corpus and the AMI Meeting Corpus. Our experiments show that taking into account the relative polarity of a proposal under discussion and of its response greatly helps to distinguish rejections from acceptances in both corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rob Abbott</author>
<author>Marilyn Walker</author>
<author>Pranav Anand</author>
<author>Jean E Fox Tree</author>
<author>Robeson Bowmani</author>
<author>Joseph King</author>
</authors>
<title>How Can You Say Such Things?!?: Recognizing Disagreement in Informal Political Argument.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media,</booktitle>
<pages>2--11</pages>
<contexts>
<context position="6502" citStr="Abbott et al. (2011)" startWordPosition="1042" endWordPosition="1045"> report 0% recall for rejections/disagreements. Wang et al. (2011), who also work with AMI data, use different resampling methods to balance their dataset and then apply Conditional Random Fields (using therefore contextual information from sequences of utterances), achieving 56.9% recall and 55.9 F1 for disagreement detection. Some recent work has moved away from spoken dialogue to address similar tasks in online discussion forums. An advantage of this kind of scenarios is that they seem to offer more opportunity for disagreement/rejection, thereby yielding more inherently balanced datasets. Abbott et al. (2011) and Misra and Walker (2013) use the Internet Argument Corpus (Walker et al., 2012), an annotated collection of posts in discussion forums with a balanced distribution of agreeing and disagreeing posts. They address a 2-way classification task—determining whether each response to a post (or to a quoted portion of a post in the case of Abbott et al. (2011)) is either an agreement or a disagreement—using a collection of features inspired by previous computational and theoretical approaches. The system developed by Misra and Walker (2013) uses only local features of the tobe-classified post, achi</context>
</contexts>
<marker>Abbott, Walker, Anand, Tree, Bowmani, King, 2011</marker>
<rawString>Rob Abbott, Marilyn Walker, Pranav Anand, Jean E. Fox Tree, Robeson Bowmani, and Joseph King. 2011. How Can You Say Such Things?!?: Recognizing Disagreement in Informal Political Argument. In Proceedings of the Workshop on Languages in Social Media, pages 2–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>2200--2204</pages>
<contexts>
<context position="9074" citStr="Baccianella et al., 2010" startWordPosition="1456" endWordPosition="1459">u and McKeown (1997) to assign a positive and a negative polarity value to an utterance given the number of subjective positive/negative adjectives it contains. Similarly, Misra and Walker (2013) use the MPQA Subjec152 tivity Lexicon (Wilson et al., 2005) to capture the local sentiment of an online post response given the number of words in the response with strongly subjective positive/negative polarity according to the subjectivity lexicon. Yin et al. (2012) assign a positive and a negative score to a post by aggregating the sentiment scores of those words that can be found in SentiWordNet (Baccianella et al., 2010). Although subjective polarity may be helpful (e.g., utterances with a high positive sentiment score may be more likely to be acceptances), this is not the kind of polarity that concerns us in this paper. Note, furthermore, that local sentiment information may be superseded by logical polarity. (4) A: But then it wouldn’t sit as comfortably in your hand. B: It would still be comfortable. Despite the fact that B’s utterance in (4)— extracted from the AMI corpus—would be assigned a positive sentiment score (given the presence of the word “comfortable”, classified as positive in the MPQA Subjecti</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. In Proceedings of LREC, pages 2200–2204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Edward Loper</author>
<author>Ewan Klein</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media Inc.</booktitle>
<contexts>
<context position="29046" citStr="Bird et al., 2009" startWordPosition="4741" endWordPosition="4744">ances would yield very good results. Hence, as discussed in section 2, we believe that the most discerning task is the retrieval of rejections. Precision, recall and F-scores for this task, with the classifier trained on different combinations of feature sets, are shown in Table 4. We developed the classifier on the whole AMI dataset, as the small number of rejections makes splitting up the corpus into a development and a test set infeasible. The SWB corpus was exclusively used for testing. In the AMI dataset we tested the classifier with 8We use the NLTK POS tagger to implement this feature (Bird et al., 2009). 9Given the high frequency of pronominal forms in spoken dialogue, pronoun identity turns out to be reasonably useful. 10The scikit-learn documentation indicates that this classifier is particularly suited for sparse data and Boolean features. LOCAL R FEATURES Length of utterance containing R in number of words: • Three features: l &gt;2, l &gt;12, l &gt;24 Acceptance Indicators: • R contains yeah • R contains any of absolutely, okay, accept, agree, correct, either, true, sure, not preceded by not Rejection Indicators: • R contains but • R contains the bigram ‘yeah, but’ • R starts with any of well, o</context>
</contexts>
<marker>Bird, Loper, Klein, 2009</marker>
<rawString>Steven Bird, Edward Loper, and Ewan Klein. 2009. Natural Language Processing with Python. O’Reilly Media Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derek Bousfield</author>
</authors>
<title>Impoliteness in Interaction.</title>
<date>2008</date>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="25369" citStr="Bousfield, 2008" startWordPosition="4150" endWordPosition="4151">eature we use are summarised in Figure 2. We consider several local features of the response. Most of these features are inspired by earlier approaches reviewed in Section 2, such as those by Galley et al. (2004) and Misra and Walker (2013). We use several lexical features that act as cues for acceptance or rejection. For instance, the presence of “yeah” is a good cue for acceptance, while the presence of “but” is a strong cue for rejection. The bigram “yeah, but” is in turn a good indicator for rejection—the “yeah” in such cases seems to be an attempt at politeness (Brown and Levinson, 1987; Bousfield, 2008). Since rejections are dispreferred moves, they are frequently initiated with a hedging such as “well” or with hesitation or stalling (Byron and Heeman, 1997). These utterance-initial cues are aggregated into one feature. Rejections also tend to be longer than acceptances since the speaker feels the need to justify the unexpected move (Pomerantz, 1984). We take into account the length of the entire utterance containing R with three binary features.6 We also consider less frequent semantic indicators for acceptance and rejection, respectively, which we group into two aggregate features that rec</context>
</contexts>
<marker>Bousfield, 2008</marker>
<rawString>Derek Bousfield. 2008. Impoliteness in Interaction. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konstantinos Bousmalis</author>
<author>Marc Mehu</author>
<author>Maja Pantic</author>
</authors>
<title>Towards the Automatic Detection of Spontaneous Agreement and Disagreement Based on Nonverbal Behaviour: A Survey of Related Cues, Databases, and Tools.</title>
<date>2013</date>
<journal>Image Vision Computing,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="8001" citStr="Bousmalis et al. (2013)" startWordPosition="1286" endWordPosition="1289">n local features of the response, which yields 66.6% accuracy. Using both features from the post and the post response, Yin et al. (2012) obtain similar results: 68% accuracy on a different online corpus (the Political Forum), where the datasets are not balanced (they report a ratio of about 2 to 1 for agreement vs. disagreement). All in all, this body of work has identified several linguistic features that are useful for inferring acceptances and rejections, often building on observations made by conversational analysts (Pomerantz, 1984; Brown and Levinson, 1987). Furthermore, recent work by Bousmalis et al. (2013) suggests that there are specific nonverbal behaviours associated with agreement and disagreement, such as different types of head, lip, and hand movements. However, to our knowledge, the role of logical polarity has not been investigated in any detail by computational approaches. Several systems make use of subjective polarity, i.e., sentiment. For instance, Galley et al. (2004) use the list of subjective adjectives compiled by Hatzivassiloglou and McKeown (1997) to assign a positive and a negative polarity value to an utterance given the number of subjective positive/negative adjectives it c</context>
</contexts>
<marker>Bousmalis, Mehu, Pantic, 2013</marker>
<rawString>Konstantinos Bousmalis, Marc Mehu, and Maja Pantic. 2013. Towards the Automatic Detection of Spontaneous Agreement and Disagreement Based on Nonverbal Behaviour: A Survey of Related Cues, Databases, and Tools. Image Vision Computing, 31(2):203–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penelope Brown</author>
<author>Stephen Levinson</author>
</authors>
<title>Politeness: Some universals in language usage.</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7948" citStr="Brown and Levinson, 1987" startWordPosition="1278" endWordPosition="1281">does not significantly outperform a system based only on local features of the response, which yields 66.6% accuracy. Using both features from the post and the post response, Yin et al. (2012) obtain similar results: 68% accuracy on a different online corpus (the Political Forum), where the datasets are not balanced (they report a ratio of about 2 to 1 for agreement vs. disagreement). All in all, this body of work has identified several linguistic features that are useful for inferring acceptances and rejections, often building on observations made by conversational analysts (Pomerantz, 1984; Brown and Levinson, 1987). Furthermore, recent work by Bousmalis et al. (2013) suggests that there are specific nonverbal behaviours associated with agreement and disagreement, such as different types of head, lip, and hand movements. However, to our knowledge, the role of logical polarity has not been investigated in any detail by computational approaches. Several systems make use of subjective polarity, i.e., sentiment. For instance, Galley et al. (2004) use the list of subjective adjectives compiled by Hatzivassiloglou and McKeown (1997) to assign a positive and a negative polarity value to an utterance given the n</context>
<context position="25351" citStr="Brown and Levinson, 1987" startWordPosition="4145" endWordPosition="4149">of logical polarity. The feature we use are summarised in Figure 2. We consider several local features of the response. Most of these features are inspired by earlier approaches reviewed in Section 2, such as those by Galley et al. (2004) and Misra and Walker (2013). We use several lexical features that act as cues for acceptance or rejection. For instance, the presence of “yeah” is a good cue for acceptance, while the presence of “but” is a strong cue for rejection. The bigram “yeah, but” is in turn a good indicator for rejection—the “yeah” in such cases seems to be an attempt at politeness (Brown and Levinson, 1987; Bousfield, 2008). Since rejections are dispreferred moves, they are frequently initiated with a hedging such as “well” or with hesitation or stalling (Byron and Heeman, 1997). These utterance-initial cues are aggregated into one feature. Rejections also tend to be longer than acceptances since the speaker feels the need to justify the unexpected move (Pomerantz, 1984). We take into account the length of the entire utterance containing R with three binary features.6 We also consider less frequent semantic indicators for acceptance and rejection, respectively, which we group into two aggregate</context>
</contexts>
<marker>Brown, Levinson, 1987</marker>
<rawString>Penelope Brown and Stephen Levinson. 1987. Politeness: Some universals in language usage. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna K Byron</author>
<author>Peter A Heeman</author>
</authors>
<title>Discourse marker use in task-oriented spoken dialog.</title>
<date>1997</date>
<booktitle>In Proceedings of Eurospeech,</booktitle>
<pages>2223--2226</pages>
<contexts>
<context position="25527" citStr="Byron and Heeman, 1997" startWordPosition="4172" endWordPosition="4175"> reviewed in Section 2, such as those by Galley et al. (2004) and Misra and Walker (2013). We use several lexical features that act as cues for acceptance or rejection. For instance, the presence of “yeah” is a good cue for acceptance, while the presence of “but” is a strong cue for rejection. The bigram “yeah, but” is in turn a good indicator for rejection—the “yeah” in such cases seems to be an attempt at politeness (Brown and Levinson, 1987; Bousfield, 2008). Since rejections are dispreferred moves, they are frequently initiated with a hedging such as “well” or with hesitation or stalling (Byron and Heeman, 1997). These utterance-initial cues are aggregated into one feature. Rejections also tend to be longer than acceptances since the speaker feels the need to justify the unexpected move (Pomerantz, 1984). We take into account the length of the entire utterance containing R with three binary features.6 We also consider less frequent semantic indicators for acceptance and rejection, respectively, which we group into two aggregate features that record the presence of agreement words such as “okay” or “correct” and contrast words such as “however” or “although”. Given our observations regarding polarity </context>
</contexts>
<marker>Byron, Heeman, 1997</marker>
<rawString>Donna K. Byron and Peter A. Heeman. 1997. Discourse marker use in task-oriented spoken dialog. In Proceedings of Eurospeech, pages 2223–2226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus. Language Resources and Evaluation,</title>
<date>2007</date>
<pages>41--2</pages>
<contexts>
<context position="1922" citStr="Carletta, 2007" startWordPosition="305" endWordPosition="306">nformation is accepted or rejected by their addressees. The basic idea is simple: If a proposal is rejected, its content does not enter the common ground, while if it is accepted, its content does become common belief. Yet, determining whether a response to a move counts as an acceptance or a rejection is far from trivial. In many cases, the surface form of an utterance is not explicit enough to determine its acceptance or rejection force and inference is required (Horn, 1989; Lascarides and Asher, 2009; Walker, 1996). For instance, B’s utterance in (1), extracted from the AMI Meeting Corpus (Carletta, 2007), exemplifies what Walker (1996) calls implicature rejection (the rejection arises from an inferred scalar implicature: “normal” implicates “not interesting”; see also Hirschberg (1985)). (1) A: This is a very interesting design. B: It’s just the same as normal. The goal of this paper is to investigate the role of logical polarity in distinguishing rejections from acceptances. Consider the following dialogue excerpts, again from AMI, where the same utterance form (“Yes it is”) acts as an acceptance in (2) and as a rejection in (3): (2) A: But it’s uh yeah it’s uh original idea. B: Yes it is. (</context>
<context position="3438" citStr="Carletta, 2007" startWordPosition="562" endWordPosition="563">but also to specify (a) the polarity (positive vs. negative) of both the proposal and the response, and (b) how these polarities interact to give rise to a particular interpretation. Our aim in this paper is to develop a model of how logical polarity influences acceptance/rejection interpretation, inspired by recent work on the semantics of negation and polarity particles (Cooper and Ginzburg, 2011; Cooper and Ginzburg, 2012; Farkas and Roelofsen, 2013), and to test it on annotated data from two spoken dialogue corpora: the Switchboard Corpus (Godfrey et al., 1992) and the AMI Meeting Corpus (Carletta, 2007). In the next section, we give an overview of related computational work on acceptance/rejection detection. In Section 3, we first briefly review recent formal semantics approaches to polarity and then present our model of logical polarity in acceptance and rejection moves. Section 4 describes 151 Proceedings of the SIGDIAL 2014 Conference, pages 151–160, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics our experiments: We derive machine learning features from our polarity theory and test them in Switchboard and AMI datasets, achieving competitive F-score</context>
<context position="21063" citStr="Carletta, 2007" startWordPosition="3443" endWordPosition="3444">. (11) A: They wouldn’t be able to own a house. B: Yes, they would. 4 Experiments In order to automatically test the extent to which logical polarity plays a role in determining the function of naturally occurring acceptances and rejections, we conduct machine learning experiments on dialogue corpus data. We first explain how we create our dataset, then describe how we devise features that encode polarity information, and finally report the results obtained. 4.1 Datasets We test our model on two different corpora: The Switchboard Corpus (SWB) (Godfrey et al., 1992) and the AMI Meeting Corpus (Carletta, 2007). SWD is a collection of around 2400 recorded and transcribed telephone conversations between two dialogue participants. The speakers are provided with a topic and then converse freely. In contrast, AMI contains transcriptions from around 100 hours of recorded multiparty conversations amongst four dialogue participants who interact face-to-face in a meeting setting. The speakers converse freely, but they play roles (such as industrial designer or project manager) in a fictitious design team whose goal is to design a remote control. Therefore the dialogue is mildly task-oriented. Both corpora h</context>
</contexts>
<marker>Carletta, 2007</marker>
<rawString>Jean Carletta. 2007. Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus. Language Resources and Evaluation, 41(2):181–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Edward F Schaefer</author>
</authors>
<title>Contributing to discourse.</title>
<date>1989</date>
<journal>Cognitive Science,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="1225" citStr="Clark and Schaefer, 1989" startWordPosition="184" endWordPosition="188">hat taking into account the relative polarity of a proposal under discussion and of its response greatly helps to distinguish rejections from acceptances in both corpora. 1 Introduction In order to establish and maintain coherence, dialogue participants need to keep track of the information they jointly take for granted—their common ground (Stalnaker, 1978). As a dialogue progresses, the common ground typically evolves. New information becomes shared as the interlocutors exchange moves (such as assertions, questions, acceptances, and rejections) through the collaborative process of grounding (Clark and Schaefer, 1989; Clark, 1996). To keep track of the common ground, speakers must identify which information is accepted or rejected by their addressees. The basic idea is simple: If a proposal is rejected, its content does not enter the common ground, while if it is accepted, its content does become common belief. Yet, determining whether a response to a move counts as an acceptance or a rejection is far from trivial. In many cases, the surface form of an utterance is not explicit enough to determine its acceptance or rejection force and inference is required (Horn, 1989; Lascarides and Asher, 2009; Walker, </context>
</contexts>
<marker>Clark, Schaefer, 1989</marker>
<rawString>Herbert H. Clark and Edward F. Schaefer. 1989. Contributing to discourse. Cognitive Science, 13(2):259–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
</authors>
<title>Using language.</title>
<date>1996</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1239" citStr="Clark, 1996" startWordPosition="189" endWordPosition="190">e relative polarity of a proposal under discussion and of its response greatly helps to distinguish rejections from acceptances in both corpora. 1 Introduction In order to establish and maintain coherence, dialogue participants need to keep track of the information they jointly take for granted—their common ground (Stalnaker, 1978). As a dialogue progresses, the common ground typically evolves. New information becomes shared as the interlocutors exchange moves (such as assertions, questions, acceptances, and rejections) through the collaborative process of grounding (Clark and Schaefer, 1989; Clark, 1996). To keep track of the common ground, speakers must identify which information is accepted or rejected by their addressees. The basic idea is simple: If a proposal is rejected, its content does not enter the common ground, while if it is accepted, its content does become common belief. Yet, determining whether a response to a move counts as an acceptance or a rejection is far from trivial. In many cases, the surface form of an utterance is not explicit enough to determine its acceptance or rejection force and inference is required (Horn, 1989; Lascarides and Asher, 2009; Walker, 1996). For ins</context>
<context position="23596" citStr="Clark, 1996" startWordPosition="3837" endWordPosition="3838">e consider R an acceptance if u is annotated as Agree/Accept in SWB or as Support/Positive Assessment in AMI, and a rejection if it is annotated as Reject in SWB or as Objection/Negative Assessment in AMI. We take the first five words of a turn-initial utterance to be the most relevant ones for conveying acceptance or rejection. This is motivated by the fact that dialogue participants typically provide evidence of understanding—and, by extension, of agreement or disagreement—at the earliest opportunity in order to avoid misunderstandings on what they take to be common ground (Pomerantz, 1984; Clark, 1996). However, when extracting our P-R pairs we retain the entire utterance u (of which R is a prefix) in order to be able to take its length into account in the automatic classification experiment, as explained in the next section. Finally, we observe that in the two corpora all the P-R pairs where R is just a single “yeah” are acceptances. Thus, in the terminology we introduced in Section 3.2, bare “yeah” seems to be an absolute response type, whose acceptance function is independent of the polarity of P (in contrast to the relative response types in Table 1). Since identification of these accep</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>Herbert H. Clark. 1996. Using language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Cooper</author>
<author>Jonathan Ginzburg</author>
</authors>
<title>Negation in dialogue.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th SemDial Workshop (Los Angelogue),</booktitle>
<pages>130--139</pages>
<contexts>
<context position="3224" citStr="Cooper and Ginzburg, 2011" startWordPosition="525" endWordPosition="528">etermine whether B’s utterance in either case above functions as an acceptance or a rejection, it is critical to not only look beyond the utterance itself and take into account the proposal under discussion (A’s utterance), but also to specify (a) the polarity (positive vs. negative) of both the proposal and the response, and (b) how these polarities interact to give rise to a particular interpretation. Our aim in this paper is to develop a model of how logical polarity influences acceptance/rejection interpretation, inspired by recent work on the semantics of negation and polarity particles (Cooper and Ginzburg, 2011; Cooper and Ginzburg, 2012; Farkas and Roelofsen, 2013), and to test it on annotated data from two spoken dialogue corpora: the Switchboard Corpus (Godfrey et al., 1992) and the AMI Meeting Corpus (Carletta, 2007). In the next section, we give an overview of related computational work on acceptance/rejection detection. In Section 3, we first briefly review recent formal semantics approaches to polarity and then present our model of logical polarity in acceptance and rejection moves. Section 4 describes 151 Proceedings of the SIGDIAL 2014 Conference, pages 151–160, Philadelphia, U.S.A., 18-20 </context>
<context position="10383" citStr="Cooper and Ginzburg, 2011" startWordPosition="1672" endWordPosition="1675"> rejection due to logical polarity constraints, as we shall make clear in the next section. 3 Polarity in Acceptances and Rejections In this section, we first give a brief overview of some of the main ideas put forward in recent theoretical approaches to polarity. Afterwards, we introduce our approach to logical polarity in the context of acceptance and rejection moves. 3.1 Formal Semantics Approaches Polarity and in particular negation are central concepts in formal semantics and pragmatics (Horn, 1989). Recent work independently put forward within the frameworks of Type Theory with Records (Cooper and Ginzburg, 2011; 2012) and of Inquisitive Semantics (Farkas and Roelofsen, 2013) has proposed to semantically distinguish between positive and negative propositions. Such a proposal departs from the traditional view in formal semantics where propositions are taken to denote sets of possible worlds (see Partee (1989) for a survey). According to this traditional view, the meaning of (5a) would be indistinguishable from that of (5b), given that the two propositions are true in exactly the same possible worlds: (5) a. Sue failed the exam. b. Sue didn’t pass the exam. These utterances, however, license different </context>
<context position="33538" citStr="Cooper and Ginzburg (2011" startWordPosition="5500" endWordPosition="5503">markedness expectation, this is likely to show up in the form of contrastive parallelism, which we can—at least in part—capture with our simple feature. Indeed, adding this feature to the AMI dataset raises recall back to baseline level: 64.04% vs. 61.63% (p &lt; 0.005). This, in turn, increase the AMI F-score from 59.75 to 60.96 (p &lt; 0.05). 5 Conclusions The overall aim of this paper has been to investigate the influence of logical polarity in interpreting utterances as acceptance or rejection moves in dialogue. We have built on recent work on the semantics of negation and polarity particles by Cooper and Ginzburg (2011; 2012) and Farkas and Roelofsen (2013) to develop an approach to polarity that is theoretically motivated and that can be computationally tested on corpus data. Although 11We note that the featureset Local + Local Polarity + Relative Polarity does not outperform Local + Local Polarity in the classification experiments. We believe this indicates that polarity is indeed mostly a contextual phenomenon. there is a substantial amount of previous work on automatically detecting agreement and disagreement in dialogue corpora, to our knowledge the role of logical polarity had not been explicitly inve</context>
</contexts>
<marker>Cooper, Ginzburg, 2011</marker>
<rawString>Robin Cooper and Jonathan Ginzburg. 2011. Negation in dialogue. In Proceedings of the 15th SemDial Workshop (Los Angelogue), pages 130–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Cooper</author>
<author>Jonathan Ginzburg</author>
</authors>
<title>Negative inquisitiveness and alternatives-based negation.</title>
<date>2012</date>
<booktitle>In Logic, Language and Meaning: Proceedings of the 18th Amsterdam Colloquium, Lecture Notes in Computer Science,</booktitle>
<pages>32--41</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3251" citStr="Cooper and Ginzburg, 2012" startWordPosition="529" endWordPosition="532">nce in either case above functions as an acceptance or a rejection, it is critical to not only look beyond the utterance itself and take into account the proposal under discussion (A’s utterance), but also to specify (a) the polarity (positive vs. negative) of both the proposal and the response, and (b) how these polarities interact to give rise to a particular interpretation. Our aim in this paper is to develop a model of how logical polarity influences acceptance/rejection interpretation, inspired by recent work on the semantics of negation and polarity particles (Cooper and Ginzburg, 2011; Cooper and Ginzburg, 2012; Farkas and Roelofsen, 2013), and to test it on annotated data from two spoken dialogue corpora: the Switchboard Corpus (Godfrey et al., 1992) and the AMI Meeting Corpus (Carletta, 2007). In the next section, we give an overview of related computational work on acceptance/rejection detection. In Section 3, we first briefly review recent formal semantics approaches to polarity and then present our model of logical polarity in acceptance and rejection moves. Section 4 describes 151 Proceedings of the SIGDIAL 2014 Conference, pages 151–160, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Associati</context>
</contexts>
<marker>Cooper, Ginzburg, 2012</marker>
<rawString>Robin Cooper and Jonathan Ginzburg. 2012. Negative inquisitiveness and alternatives-based negation. In Logic, Language and Meaning: Proceedings of the 18th Amsterdam Colloquium, Lecture Notes in Computer Science, pages 32–41. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donka Farkas</author>
<author>Floris Roelofsen</author>
</authors>
<title>Polar initiatives and polar particle responses in an inquisitive discourse model.</title>
<date>2013</date>
<note>Available from http://www. illc.uva.nl/inquisitivesemantics/.</note>
<contexts>
<context position="3280" citStr="Farkas and Roelofsen, 2013" startWordPosition="533" endWordPosition="536">nctions as an acceptance or a rejection, it is critical to not only look beyond the utterance itself and take into account the proposal under discussion (A’s utterance), but also to specify (a) the polarity (positive vs. negative) of both the proposal and the response, and (b) how these polarities interact to give rise to a particular interpretation. Our aim in this paper is to develop a model of how logical polarity influences acceptance/rejection interpretation, inspired by recent work on the semantics of negation and polarity particles (Cooper and Ginzburg, 2011; Cooper and Ginzburg, 2012; Farkas and Roelofsen, 2013), and to test it on annotated data from two spoken dialogue corpora: the Switchboard Corpus (Godfrey et al., 1992) and the AMI Meeting Corpus (Carletta, 2007). In the next section, we give an overview of related computational work on acceptance/rejection detection. In Section 3, we first briefly review recent formal semantics approaches to polarity and then present our model of logical polarity in acceptance and rejection moves. Section 4 describes 151 Proceedings of the SIGDIAL 2014 Conference, pages 151–160, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguist</context>
<context position="10448" citStr="Farkas and Roelofsen, 2013" startWordPosition="1681" endWordPosition="1684">e clear in the next section. 3 Polarity in Acceptances and Rejections In this section, we first give a brief overview of some of the main ideas put forward in recent theoretical approaches to polarity. Afterwards, we introduce our approach to logical polarity in the context of acceptance and rejection moves. 3.1 Formal Semantics Approaches Polarity and in particular negation are central concepts in formal semantics and pragmatics (Horn, 1989). Recent work independently put forward within the frameworks of Type Theory with Records (Cooper and Ginzburg, 2011; 2012) and of Inquisitive Semantics (Farkas and Roelofsen, 2013) has proposed to semantically distinguish between positive and negative propositions. Such a proposal departs from the traditional view in formal semantics where propositions are taken to denote sets of possible worlds (see Partee (1989) for a survey). According to this traditional view, the meaning of (5a) would be indistinguishable from that of (5b), given that the two propositions are true in exactly the same possible worlds: (5) a. Sue failed the exam. b. Sue didn’t pass the exam. These utterances, however, license different types of responses. For instance, responding “no” to (5a) would a</context>
<context position="12228" citStr="Farkas and Roelofsen (2013)" startWordPosition="1964" endWordPosition="1967">s, they act as acceptances); if the presupposition fails, they assert the negation of the proposition under discussion (i.e., they act as rejections). This characterises the standard behaviour of polarity particles. However, the picture is slightly more complicated since, when the proposition under discussion is negative, in English “yes” and “no” can also be used to agree or disagree, respectively (contrary to the standard case): (6) Sue didn’t pass the exam. a. No (she didn’t). --* standard acceptance Yes, she didn’t. b. Yes, she did. / #Yes. --* standard rejection No, she did. According to Farkas and Roelofsen (2013), this ambiguity of use makes bare forms of “yes”/“no” less likely in the non-standard cases exemplified in (6) and favours more explicit sentential forms where the presence of the verb disambiguates the intended interpretation. In this respect, however, the standard rejection in (6b) constitutes a special case: While in standard acceptances the sentential form is not required, in standard rejections it seems needed. According to these authors, in English the positive polarity particle “yes” has a strong preference for realising an agreement move and therefore its use as a rejection is marked.</context>
<context position="33577" citStr="Farkas and Roelofsen (2013)" startWordPosition="5506" endWordPosition="5509">y to show up in the form of contrastive parallelism, which we can—at least in part—capture with our simple feature. Indeed, adding this feature to the AMI dataset raises recall back to baseline level: 64.04% vs. 61.63% (p &lt; 0.005). This, in turn, increase the AMI F-score from 59.75 to 60.96 (p &lt; 0.05). 5 Conclusions The overall aim of this paper has been to investigate the influence of logical polarity in interpreting utterances as acceptance or rejection moves in dialogue. We have built on recent work on the semantics of negation and polarity particles by Cooper and Ginzburg (2011; 2012) and Farkas and Roelofsen (2013) to develop an approach to polarity that is theoretically motivated and that can be computationally tested on corpus data. Although 11We note that the featureset Local + Local Polarity + Relative Polarity does not outperform Local + Local Polarity in the classification experiments. We believe this indicates that polarity is indeed mostly a contextual phenomenon. there is a substantial amount of previous work on automatically detecting agreement and disagreement in dialogue corpora, to our knowledge the role of logical polarity had not been explicitly investigated before. Our focus has been on </context>
</contexts>
<marker>Farkas, Roelofsen, 2013</marker>
<rawString>Donka Farkas and Floris Roelofsen. 2013. Polar initiatives and polar particle responses in an inquisitive discourse model. Available from http://www. illc.uva.nl/inquisitivesemantics/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Fern´andez</author>
<author>Matthew Frampton</author>
<author>Patrick Ehlen</author>
<author>Matthew Purver</author>
<author>Stanley Peters</author>
</authors>
<title>Modelling and Detecting Decisions in Multi-Party Dialogue.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>156--163</pages>
<marker>Fern´andez, Frampton, Ehlen, Purver, Peters, 2008</marker>
<rawString>Raquel Fern´andez, Matthew Frampton, Patrick Ehlen, Matthew Purver, and Stanley Peters. 2008. Modelling and Detecting Decisions in Multi-Party Dialogue. In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 156–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
<author>Julia Hirschberg</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Identifying agreement and disagreement in conversational speech: Use of bayesian networks to model pragmatic dependencies.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04),</booktitle>
<pages>669--676</pages>
<contexts>
<context position="4754" citStr="Galley et al. (2004)" startWordPosition="765" endWordPosition="768">of our results. 2 Related Computational Work The first attempts to automatically identify acceptances and rejections (often referred to as agreements and disagreements) were carried out in the context of multiparty meetings for the purpose of dialogue summarisation tasks. Hillard et al. (2003) and Hahn et al. (2006) used the ICSI Meeting Corpus (Janin et al., 2003) to develop systems that would classify utterances into agreements, disagreements, backchannels, and ‘other’. While these authors only leveraged lexical and prosodic features of the utterance to be classified (i.e., local features), Galley et al. (2004) showed that accuracy could be improved by taking into account contextual dependencies, in particular previous (dis)agreements between the dialogue participants, achieving an overall accuracy of 86.9%. Subsequent work built on Galley et al.’s approach showed that detecting agreement acts helped to identify public commitments to tasks (Purver et al., 2007) and other decisions made in a meeting (Fern´andez et al., 2008). A difficulty shared by all approaches mentioned above is the skewness of the data, not only regarding (dis)agreement vs. other types of acts, but also agreement vs. disagreement</context>
<context position="8383" citStr="Galley et al. (2004)" startWordPosition="1345" endWordPosition="1348"> linguistic features that are useful for inferring acceptances and rejections, often building on observations made by conversational analysts (Pomerantz, 1984; Brown and Levinson, 1987). Furthermore, recent work by Bousmalis et al. (2013) suggests that there are specific nonverbal behaviours associated with agreement and disagreement, such as different types of head, lip, and hand movements. However, to our knowledge, the role of logical polarity has not been investigated in any detail by computational approaches. Several systems make use of subjective polarity, i.e., sentiment. For instance, Galley et al. (2004) use the list of subjective adjectives compiled by Hatzivassiloglou and McKeown (1997) to assign a positive and a negative polarity value to an utterance given the number of subjective positive/negative adjectives it contains. Similarly, Misra and Walker (2013) use the MPQA Subjec152 tivity Lexicon (Wilson et al., 2005) to capture the local sentiment of an online post response given the number of words in the response with strongly subjective positive/negative polarity according to the subjectivity lexicon. Yin et al. (2012) assign a positive and a negative score to a post by aggregating the s</context>
<context position="24965" citStr="Galley et al. (2004)" startWordPosition="4075" endWordPosition="4078">3. As can be seen, the data is highly skewed, with less than 10% of P-R pairs corresponding to rejections. 4.2 Features We derive different types of features to test our model. We are not interested in using large amounts of unmotivated features, but rather in exploiting a small set of meaningful domain- and setting-independent features that can help us to investigate the impact of logical polarity. The feature we use are summarised in Figure 2. We consider several local features of the response. Most of these features are inspired by earlier approaches reviewed in Section 2, such as those by Galley et al. (2004) and Misra and Walker (2013). We use several lexical features that act as cues for acceptance or rejection. For instance, the presence of “yeah” is a good cue for acceptance, while the presence of “but” is a strong cue for rejection. The bigram “yeah, but” is in turn a good indicator for rejection—the “yeah” in such cases seems to be an attempt at politeness (Brown and Levinson, 1987; Bousfield, 2008). Since rejections are dispreferred moves, they are frequently initiated with a hedging such as “well” or with hesitation or stalling (Byron and Heeman, 1997). These utterance-initial cues are agg</context>
</contexts>
<marker>Galley, McKeown, Hirschberg, Shriberg, 2004</marker>
<rawString>Michel Galley, Kathleen McKeown, Julia Hirschberg, and Elizabeth Shriberg. 2004. Identifying agreement and disagreement in conversational speech: Use of bayesian networks to model pragmatic dependencies. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), pages 669–676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Germesin</author>
<author>Theresa Wilson</author>
</authors>
<title>Agreement detection in multiparty conversation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 international conference on Multimodal interfaces,</booktitle>
<pages>7--14</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5648" citStr="Germesin and Wilson (2009)" startWordPosition="909" endWordPosition="912">cting agreement acts helped to identify public commitments to tasks (Purver et al., 2007) and other decisions made in a meeting (Fern´andez et al., 2008). A difficulty shared by all approaches mentioned above is the skewness of the data, not only regarding (dis)agreement vs. other types of acts, but also agreement vs. disagreement. In the dialogue settings considered, acceptance/agreement is much more common than rejection/disagreement (e.g., 11.9% vs. 6.7% in the portion of the ICSI Meeting Corpus used by Galley et al. (2004) and 3.6% vs. 0.4% in the section of the AMI Meeting Corpus used by Germesin and Wilson (2009)). This can lead to reasonable overall accuracy but poor results on recognising rejections. Indeed, Germesin and Wilson (2009), who apply an approach based on Galley et al. (2004) to the AMI Meeting Corpus, achieve 98.1% accuracy, but report 0% recall for rejections/disagreements. Wang et al. (2011), who also work with AMI data, use different resampling methods to balance their dataset and then apply Conditional Random Fields (using therefore contextual information from sequences of utterances), achieving 56.9% recall and 55.9 F1 for disagreement detection. Some recent work has moved away from</context>
</contexts>
<marker>Germesin, Wilson, 2009</marker>
<rawString>Sebastian Germesin and Theresa Wilson. 2009. Agreement detection in multiparty conversation. In Proceedings of the 2009 international conference on Multimodal interfaces, pages 7–14. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>E C Holliman</author>
<author>J McDaniel</author>
</authors>
<date>1992</date>
<booktitle>SWITCHBOARD: Telephone Speech Corpus for Research and Development. IEEE Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>1--517</pages>
<contexts>
<context position="3394" citStr="Godfrey et al., 1992" startWordPosition="553" endWordPosition="556">nt the proposal under discussion (A’s utterance), but also to specify (a) the polarity (positive vs. negative) of both the proposal and the response, and (b) how these polarities interact to give rise to a particular interpretation. Our aim in this paper is to develop a model of how logical polarity influences acceptance/rejection interpretation, inspired by recent work on the semantics of negation and polarity particles (Cooper and Ginzburg, 2011; Cooper and Ginzburg, 2012; Farkas and Roelofsen, 2013), and to test it on annotated data from two spoken dialogue corpora: the Switchboard Corpus (Godfrey et al., 1992) and the AMI Meeting Corpus (Carletta, 2007). In the next section, we give an overview of related computational work on acceptance/rejection detection. In Section 3, we first briefly review recent formal semantics approaches to polarity and then present our model of logical polarity in acceptance and rejection moves. Section 4 describes 151 Proceedings of the SIGDIAL 2014 Conference, pages 151–160, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics our experiments: We derive machine learning features from our polarity theory and test them in Switchboard and</context>
<context position="21019" citStr="Godfrey et al., 1992" startWordPosition="3434" endWordPosition="3437">present with higher frequency in the reverse cases. (11) A: They wouldn’t be able to own a house. B: Yes, they would. 4 Experiments In order to automatically test the extent to which logical polarity plays a role in determining the function of naturally occurring acceptances and rejections, we conduct machine learning experiments on dialogue corpus data. We first explain how we create our dataset, then describe how we devise features that encode polarity information, and finally report the results obtained. 4.1 Datasets We test our model on two different corpora: The Switchboard Corpus (SWB) (Godfrey et al., 1992) and the AMI Meeting Corpus (Carletta, 2007). SWD is a collection of around 2400 recorded and transcribed telephone conversations between two dialogue participants. The speakers are provided with a topic and then converse freely. In contrast, AMI contains transcriptions from around 100 hours of recorded multiparty conversations amongst four dialogue participants who interact face-to-face in a meeting setting. The speakers converse freely, but they play roles (such as industrial designer or project manager) in a fictitious design team whose goal is to design a remote control. Therefore the dial</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>John J. Godfrey, E. C. Holliman, and J. McDaniel. 1992. SWITCHBOARD: Telephone Speech Corpus for Research and Development. IEEE Conference on Acoustics, Speech, and Signal Processing, 1:517–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sangyun Hahn</author>
<author>Richard Ladner</author>
<author>Mari Ostendorf</author>
</authors>
<title>Agreement/disagreement classification: Exploiting unlabeled data using contrast classifiers.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL,</booktitle>
<pages>53--56</pages>
<contexts>
<context position="4451" citStr="Hahn et al. (2006)" startWordPosition="718" endWordPosition="721">. c�2014 Association for Computational Linguistics our experiments: We derive machine learning features from our polarity theory and test them in Switchboard and AMI datasets, achieving competitive F-scores of around 60 on the task of retrieving rejections. We conclude in Section 5 with a discussion of our results. 2 Related Computational Work The first attempts to automatically identify acceptances and rejections (often referred to as agreements and disagreements) were carried out in the context of multiparty meetings for the purpose of dialogue summarisation tasks. Hillard et al. (2003) and Hahn et al. (2006) used the ICSI Meeting Corpus (Janin et al., 2003) to develop systems that would classify utterances into agreements, disagreements, backchannels, and ‘other’. While these authors only leveraged lexical and prosodic features of the utterance to be classified (i.e., local features), Galley et al. (2004) showed that accuracy could be improved by taking into account contextual dependencies, in particular previous (dis)agreements between the dialogue participants, achieving an overall accuracy of 86.9%. Subsequent work built on Galley et al.’s approach showed that detecting agreement acts helped t</context>
</contexts>
<marker>Hahn, Ladner, Ostendorf, 2006</marker>
<rawString>Sangyun Hahn, Richard Ladner, and Mari Ostendorf. 2006. Agreement/disagreement classification: Exploiting unlabeled data using contrast classifiers. In Proceedings of the HLT-NAACL, pages 53–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>174--181</pages>
<contexts>
<context position="8469" citStr="Hatzivassiloglou and McKeown (1997)" startWordPosition="1358" endWordPosition="1361">ctions, often building on observations made by conversational analysts (Pomerantz, 1984; Brown and Levinson, 1987). Furthermore, recent work by Bousmalis et al. (2013) suggests that there are specific nonverbal behaviours associated with agreement and disagreement, such as different types of head, lip, and hand movements. However, to our knowledge, the role of logical polarity has not been investigated in any detail by computational approaches. Several systems make use of subjective polarity, i.e., sentiment. For instance, Galley et al. (2004) use the list of subjective adjectives compiled by Hatzivassiloglou and McKeown (1997) to assign a positive and a negative polarity value to an utterance given the number of subjective positive/negative adjectives it contains. Similarly, Misra and Walker (2013) use the MPQA Subjec152 tivity Lexicon (Wilson et al., 2005) to capture the local sentiment of an online post response given the number of words in the response with strongly subjective positive/negative polarity according to the subjectivity lexicon. Yin et al. (2012) assign a positive and a negative score to a post by aggregating the sentiment scores of those words that can be found in SentiWordNet (Baccianella et al., </context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, pages 174–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dustin Hillard</author>
<author>Mari Ostendorf</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Detection of agreement vs. disagreement in meetings: training with unlabeled data.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 2003,</booktitle>
<pages>34--36</pages>
<contexts>
<context position="4428" citStr="Hillard et al. (2003)" startWordPosition="713" endWordPosition="716">a, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics our experiments: We derive machine learning features from our polarity theory and test them in Switchboard and AMI datasets, achieving competitive F-scores of around 60 on the task of retrieving rejections. We conclude in Section 5 with a discussion of our results. 2 Related Computational Work The first attempts to automatically identify acceptances and rejections (often referred to as agreements and disagreements) were carried out in the context of multiparty meetings for the purpose of dialogue summarisation tasks. Hillard et al. (2003) and Hahn et al. (2006) used the ICSI Meeting Corpus (Janin et al., 2003) to develop systems that would classify utterances into agreements, disagreements, backchannels, and ‘other’. While these authors only leveraged lexical and prosodic features of the utterance to be classified (i.e., local features), Galley et al. (2004) showed that accuracy could be improved by taking into account contextual dependencies, in particular previous (dis)agreements between the dialogue participants, achieving an overall accuracy of 86.9%. Subsequent work built on Galley et al.’s approach showed that detecting </context>
</contexts>
<marker>Hillard, Ostendorf, Shriberg, 2003</marker>
<rawString>Dustin Hillard, Mari Ostendorf, and Elizabeth Shriberg. 2003. Detection of agreement vs. disagreement in meetings: training with unlabeled data. In Proceedings of the HLT-NAACL 2003, pages 34– 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia L B Hirschberg</author>
</authors>
<title>A theory of scalar implicature.</title>
<date>1985</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2107" citStr="Hirschberg (1985)" startWordPosition="329" endWordPosition="330"> content does become common belief. Yet, determining whether a response to a move counts as an acceptance or a rejection is far from trivial. In many cases, the surface form of an utterance is not explicit enough to determine its acceptance or rejection force and inference is required (Horn, 1989; Lascarides and Asher, 2009; Walker, 1996). For instance, B’s utterance in (1), extracted from the AMI Meeting Corpus (Carletta, 2007), exemplifies what Walker (1996) calls implicature rejection (the rejection arises from an inferred scalar implicature: “normal” implicates “not interesting”; see also Hirschberg (1985)). (1) A: This is a very interesting design. B: It’s just the same as normal. The goal of this paper is to investigate the role of logical polarity in distinguishing rejections from acceptances. Consider the following dialogue excerpts, again from AMI, where the same utterance form (“Yes it is”) acts as an acceptance in (2) and as a rejection in (3): (2) A: But it’s uh yeah it’s uh original idea. B: Yes it is. (3) A: the shape of a banana is not it’s not really handy . B: Yes it is. To determine whether B’s utterance in either case above functions as an acceptance or a rejection, it is critica</context>
</contexts>
<marker>Hirschberg, 1985</marker>
<rawString>Julia L. B. Hirschberg. 1985. A theory of scalar implicature. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurence R Horn</author>
</authors>
<title>A Natural History of Negation.</title>
<date>1989</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="1787" citStr="Horn, 1989" startWordPosition="285" endWordPosition="286">e process of grounding (Clark and Schaefer, 1989; Clark, 1996). To keep track of the common ground, speakers must identify which information is accepted or rejected by their addressees. The basic idea is simple: If a proposal is rejected, its content does not enter the common ground, while if it is accepted, its content does become common belief. Yet, determining whether a response to a move counts as an acceptance or a rejection is far from trivial. In many cases, the surface form of an utterance is not explicit enough to determine its acceptance or rejection force and inference is required (Horn, 1989; Lascarides and Asher, 2009; Walker, 1996). For instance, B’s utterance in (1), extracted from the AMI Meeting Corpus (Carletta, 2007), exemplifies what Walker (1996) calls implicature rejection (the rejection arises from an inferred scalar implicature: “normal” implicates “not interesting”; see also Hirschberg (1985)). (1) A: This is a very interesting design. B: It’s just the same as normal. The goal of this paper is to investigate the role of logical polarity in distinguishing rejections from acceptances. Consider the following dialogue excerpts, again from AMI, where the same utterance fo</context>
<context position="10267" citStr="Horn, 1989" startWordPosition="1657" endWordPosition="1658"> the MPQA Subjectivity Lexicon, and the absence of negative subjective words), the utterance acts as a rejection due to logical polarity constraints, as we shall make clear in the next section. 3 Polarity in Acceptances and Rejections In this section, we first give a brief overview of some of the main ideas put forward in recent theoretical approaches to polarity. Afterwards, we introduce our approach to logical polarity in the context of acceptance and rejection moves. 3.1 Formal Semantics Approaches Polarity and in particular negation are central concepts in formal semantics and pragmatics (Horn, 1989). Recent work independently put forward within the frameworks of Type Theory with Records (Cooper and Ginzburg, 2011; 2012) and of Inquisitive Semantics (Farkas and Roelofsen, 2013) has proposed to semantically distinguish between positive and negative propositions. Such a proposal departs from the traditional view in formal semantics where propositions are taken to denote sets of possible worlds (see Partee (1989) for a survey). According to this traditional view, the meaning of (5a) would be indistinguishable from that of (5b), given that the two propositions are true in exactly the same pos</context>
</contexts>
<marker>Horn, 1989</marker>
<rawString>Laurence R. Horn. 1989. A Natural History of Negation. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Janin</author>
<author>Don Baron</author>
<author>Jane Edwards</author>
<author>Dan Ellis</author>
<author>David Gelbart</author>
<author>Nelson Morgan</author>
<author>Barbara Peskin</author>
<author>Thilo Pfau</author>
<author>Elisabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Chuck Wooters</author>
</authors>
<title>The ICSI Meeting Corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of ICASSP’03,</booktitle>
<pages>364--367</pages>
<contexts>
<context position="4501" citStr="Janin et al., 2003" startWordPosition="728" endWordPosition="731">s our experiments: We derive machine learning features from our polarity theory and test them in Switchboard and AMI datasets, achieving competitive F-scores of around 60 on the task of retrieving rejections. We conclude in Section 5 with a discussion of our results. 2 Related Computational Work The first attempts to automatically identify acceptances and rejections (often referred to as agreements and disagreements) were carried out in the context of multiparty meetings for the purpose of dialogue summarisation tasks. Hillard et al. (2003) and Hahn et al. (2006) used the ICSI Meeting Corpus (Janin et al., 2003) to develop systems that would classify utterances into agreements, disagreements, backchannels, and ‘other’. While these authors only leveraged lexical and prosodic features of the utterance to be classified (i.e., local features), Galley et al. (2004) showed that accuracy could be improved by taking into account contextual dependencies, in particular previous (dis)agreements between the dialogue participants, achieving an overall accuracy of 86.9%. Subsequent work built on Galley et al.’s approach showed that detecting agreement acts helped to identify public commitments to tasks (Purver et </context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>Adam Janin, Don Baron, Jane Edwards, Dan Ellis, David Gelbart, Nelson Morgan, Barbara Peskin, Thilo Pfau, Elisabeth Shriberg, Andreas Stolcke, and Chuck Wooters. 2003. The ICSI Meeting Corpus. In Proceedings of ICASSP’03, pages 364–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>Elizabeth Shriberg</author>
<author>Debra Biasca</author>
</authors>
<title>Switchboard SWBD-DAMSL shallowdiscourse-function-annotation coder’s manual, draft 13.</title>
<date>1997</date>
<tech>Technical Report TR 97-02,</tech>
<institution>Institute for Cognitive Science, University of Colorado at Boulder.</institution>
<contexts>
<context position="21829" citStr="Jurafsky et al., 1997" startWordPosition="3558" endWordPosition="3561">ed with a topic and then converse freely. In contrast, AMI contains transcriptions from around 100 hours of recorded multiparty conversations amongst four dialogue participants who interact face-to-face in a meeting setting. The speakers converse freely, but they play roles (such as industrial designer or project manager) in a fictitious design team whose goal is to design a remote control. Therefore the dialogue is mildly task-oriented. Both corpora have been annotated with dialogue acts (DAs), albeit with slightly different DA annotation schemes: SWD is annotated with the SWBD-DAMSL tagset (Jurafsky et al., 1997), while AMI uses a coarser-grained tagset but includes relations between some DAs (loosely called adjacency pair annotations).5 5The AMI DA annotation manual is available at http://mmm.idiap.ch/private/ami/annotation/dialogue_ acts_manual_1.0.pdf 155 acceptances rejections total P-R SWB 4534 (97%) 145 (3%) 4679 AMI 7405 (91%) 697 (9%) 8102 Table 3: Class distribution in our datasets. We use the DA annotations to extract a dataset of proposal-response (P-R) pairs for each corpus as follows. To construct the SWB dataset, we extract all utterances u annotated as Agree/Accept or Reject that are tu</context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>Dan Jurafsky, Elizabeth Shriberg, and Debra Biasca. 1997. Switchboard SWBD-DAMSL shallowdiscourse-function-annotation coder’s manual, draft 13. Technical Report TR 97-02, Institute for Cognitive Science, University of Colorado at Boulder.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Nicholas Asher</author>
</authors>
<title>Agreement, disputes and commitments in dialogue.</title>
<date>2009</date>
<journal>Journal of Semantics,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="1815" citStr="Lascarides and Asher, 2009" startWordPosition="287" endWordPosition="290"> grounding (Clark and Schaefer, 1989; Clark, 1996). To keep track of the common ground, speakers must identify which information is accepted or rejected by their addressees. The basic idea is simple: If a proposal is rejected, its content does not enter the common ground, while if it is accepted, its content does become common belief. Yet, determining whether a response to a move counts as an acceptance or a rejection is far from trivial. In many cases, the surface form of an utterance is not explicit enough to determine its acceptance or rejection force and inference is required (Horn, 1989; Lascarides and Asher, 2009; Walker, 1996). For instance, B’s utterance in (1), extracted from the AMI Meeting Corpus (Carletta, 2007), exemplifies what Walker (1996) calls implicature rejection (the rejection arises from an inferred scalar implicature: “normal” implicates “not interesting”; see also Hirschberg (1985)). (1) A: This is a very interesting design. B: It’s just the same as normal. The goal of this paper is to investigate the role of logical polarity in distinguishing rejections from acceptances. Consider the following dialogue excerpts, again from AMI, where the same utterance form (“Yes it is”) acts as an </context>
</contexts>
<marker>Lascarides, Asher, 2009</marker>
<rawString>Alex Lascarides and Nicholas Asher. 2009. Agreement, disputes and commitments in dialogue. Journal of Semantics, 26(2):109–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amita Misra</author>
<author>Marilyn Walker</author>
</authors>
<title>Topic independent identification of agreement and disagreement in social media dialogue.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>41--50</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Metz, France.</location>
<contexts>
<context position="6530" citStr="Misra and Walker (2013)" startWordPosition="1047" endWordPosition="1050">ections/disagreements. Wang et al. (2011), who also work with AMI data, use different resampling methods to balance their dataset and then apply Conditional Random Fields (using therefore contextual information from sequences of utterances), achieving 56.9% recall and 55.9 F1 for disagreement detection. Some recent work has moved away from spoken dialogue to address similar tasks in online discussion forums. An advantage of this kind of scenarios is that they seem to offer more opportunity for disagreement/rejection, thereby yielding more inherently balanced datasets. Abbott et al. (2011) and Misra and Walker (2013) use the Internet Argument Corpus (Walker et al., 2012), an annotated collection of posts in discussion forums with a balanced distribution of agreeing and disagreeing posts. They address a 2-way classification task—determining whether each response to a post (or to a quoted portion of a post in the case of Abbott et al. (2011)) is either an agreement or a disagreement—using a collection of features inspired by previous computational and theoretical approaches. The system developed by Misra and Walker (2013) uses only local features of the tobe-classified post, achieving an accuracy of 66% (ov</context>
<context position="8644" citStr="Misra and Walker (2013)" startWordPosition="1384" endWordPosition="1387"> are specific nonverbal behaviours associated with agreement and disagreement, such as different types of head, lip, and hand movements. However, to our knowledge, the role of logical polarity has not been investigated in any detail by computational approaches. Several systems make use of subjective polarity, i.e., sentiment. For instance, Galley et al. (2004) use the list of subjective adjectives compiled by Hatzivassiloglou and McKeown (1997) to assign a positive and a negative polarity value to an utterance given the number of subjective positive/negative adjectives it contains. Similarly, Misra and Walker (2013) use the MPQA Subjec152 tivity Lexicon (Wilson et al., 2005) to capture the local sentiment of an online post response given the number of words in the response with strongly subjective positive/negative polarity according to the subjectivity lexicon. Yin et al. (2012) assign a positive and a negative score to a post by aggregating the sentiment scores of those words that can be found in SentiWordNet (Baccianella et al., 2010). Although subjective polarity may be helpful (e.g., utterances with a high positive sentiment score may be more likely to be acceptances), this is not the kind of polari</context>
<context position="24993" citStr="Misra and Walker (2013)" startWordPosition="4080" endWordPosition="4083">ta is highly skewed, with less than 10% of P-R pairs corresponding to rejections. 4.2 Features We derive different types of features to test our model. We are not interested in using large amounts of unmotivated features, but rather in exploiting a small set of meaningful domain- and setting-independent features that can help us to investigate the impact of logical polarity. The feature we use are summarised in Figure 2. We consider several local features of the response. Most of these features are inspired by earlier approaches reviewed in Section 2, such as those by Galley et al. (2004) and Misra and Walker (2013). We use several lexical features that act as cues for acceptance or rejection. For instance, the presence of “yeah” is a good cue for acceptance, while the presence of “but” is a strong cue for rejection. The bigram “yeah, but” is in turn a good indicator for rejection—the “yeah” in such cases seems to be an attempt at politeness (Brown and Levinson, 1987; Bousfield, 2008). Since rejections are dispreferred moves, they are frequently initiated with a hedging such as “well” or with hesitation or stalling (Byron and Heeman, 1997). These utterance-initial cues are aggregated into one feature. Re</context>
</contexts>
<marker>Misra, Walker, 2013</marker>
<rawString>Amita Misra and Marilyn Walker. 2013. Topic independent identification of agreement and disagreement in social media dialogue. In Proceedings of the SIGDIAL 2013 Conference, pages 41–50, Metz, France. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Partee</author>
</authors>
<title>Possible worlds in modeltheoretic semantics: A linguistic perspective.</title>
<date>1989</date>
<booktitle>Possible Worlds in Humanities, Arts and Sciences,</booktitle>
<pages>93--123</pages>
<editor>In S. Allen, editor,</editor>
<note>Walter de Gruyter.</note>
<contexts>
<context position="10685" citStr="Partee (1989)" startWordPosition="1720" endWordPosition="1721">polarity in the context of acceptance and rejection moves. 3.1 Formal Semantics Approaches Polarity and in particular negation are central concepts in formal semantics and pragmatics (Horn, 1989). Recent work independently put forward within the frameworks of Type Theory with Records (Cooper and Ginzburg, 2011; 2012) and of Inquisitive Semantics (Farkas and Roelofsen, 2013) has proposed to semantically distinguish between positive and negative propositions. Such a proposal departs from the traditional view in formal semantics where propositions are taken to denote sets of possible worlds (see Partee (1989) for a survey). According to this traditional view, the meaning of (5a) would be indistinguishable from that of (5b), given that the two propositions are true in exactly the same possible worlds: (5) a. Sue failed the exam. b. Sue didn’t pass the exam. These utterances, however, license different types of responses. For instance, responding “no” to (5a) would assert that Sue did pass the exam, while the same response to (5b) would typically be understood as asserting the opposite. Leaving aside many details that distinguish the two theories, Cooper/Ginzburg and Farkas/Roelofsen propose that po</context>
</contexts>
<marker>Partee, 1989</marker>
<rawString>Barbara Partee. 1989. Possible worlds in modeltheoretic semantics: A linguistic perspective. In S. Allen, editor, Possible Worlds in Humanities, Arts and Sciences, pages 93–123. Walter de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="28066" citStr="Pedregosa et al., 2011" startWordPosition="4580" endWordPosition="4584"> responses to negative proposals. The parallelism feature targets such cases. We restrict ourselves to strict identity between P and R of a pronominal subject and a verb (in negative vs. positive form).8 The feature therefore is only able to capture examples such as (12a) but not (12b), where anaphora resolution would be required.9 (12) a. A: But it wouldn’t be very attractive. B: No, it would. b. A: TVs aren’t capable of sending. B: Yes, they are. 4.3 Results We conducted the machine learning experiment using BernoulliNB, the Bernoulli-distributed Naive Bayesian classifier from scikit-learn (Pedregosa et al., 2011), which outperformed several other classifiers, including Random Forests and a Support Vector Machine. We chose this classifier because our main features—the relative polarities—are Boolean and our data is highly imbalanced.10 Given the high relative frequency of acceptances over rejections in our datasets (see Table 3), measuring accuracy or retrieving acceptances would yield very good results. Hence, as discussed in section 2, we believe that the most discerning task is the retrieval of rejections. Precision, recall and F-scores for this task, with the classifier trained on different combina</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anita Pomerantz</author>
</authors>
<title>Agreeing and disagreeing with assessments: Some features of preferred/dispreferred turn shapes. In Structures of Social Action.</title>
<date>1984</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7921" citStr="Pomerantz, 1984" startWordPosition="1276" endWordPosition="1277">tual information does not significantly outperform a system based only on local features of the response, which yields 66.6% accuracy. Using both features from the post and the post response, Yin et al. (2012) obtain similar results: 68% accuracy on a different online corpus (the Political Forum), where the datasets are not balanced (they report a ratio of about 2 to 1 for agreement vs. disagreement). All in all, this body of work has identified several linguistic features that are useful for inferring acceptances and rejections, often building on observations made by conversational analysts (Pomerantz, 1984; Brown and Levinson, 1987). Furthermore, recent work by Bousmalis et al. (2013) suggests that there are specific nonverbal behaviours associated with agreement and disagreement, such as different types of head, lip, and hand movements. However, to our knowledge, the role of logical polarity has not been investigated in any detail by computational approaches. Several systems make use of subjective polarity, i.e., sentiment. For instance, Galley et al. (2004) use the list of subjective adjectives compiled by Hatzivassiloglou and McKeown (1997) to assign a positive and a negative polarity value </context>
<context position="23582" citStr="Pomerantz, 1984" startWordPosition="3835" endWordPosition="3836">ive words of u. We consider R an acceptance if u is annotated as Agree/Accept in SWB or as Support/Positive Assessment in AMI, and a rejection if it is annotated as Reject in SWB or as Objection/Negative Assessment in AMI. We take the first five words of a turn-initial utterance to be the most relevant ones for conveying acceptance or rejection. This is motivated by the fact that dialogue participants typically provide evidence of understanding—and, by extension, of agreement or disagreement—at the earliest opportunity in order to avoid misunderstandings on what they take to be common ground (Pomerantz, 1984; Clark, 1996). However, when extracting our P-R pairs we retain the entire utterance u (of which R is a prefix) in order to be able to take its length into account in the automatic classification experiment, as explained in the next section. Finally, we observe that in the two corpora all the P-R pairs where R is just a single “yeah” are acceptances. Thus, in the terminology we introduced in Section 3.2, bare “yeah” seems to be an absolute response type, whose acceptance function is independent of the polarity of P (in contrast to the relative response types in Table 1). Since identification </context>
<context position="25723" citStr="Pomerantz, 1984" startWordPosition="4206" endWordPosition="4207">h” is a good cue for acceptance, while the presence of “but” is a strong cue for rejection. The bigram “yeah, but” is in turn a good indicator for rejection—the “yeah” in such cases seems to be an attempt at politeness (Brown and Levinson, 1987; Bousfield, 2008). Since rejections are dispreferred moves, they are frequently initiated with a hedging such as “well” or with hesitation or stalling (Byron and Heeman, 1997). These utterance-initial cues are aggregated into one feature. Rejections also tend to be longer than acceptances since the speaker feels the need to justify the unexpected move (Pomerantz, 1984). We take into account the length of the entire utterance containing R with three binary features.6 We also consider less frequent semantic indicators for acceptance and rejection, respectively, which we group into two aggregate features that record the presence of agreement words such as “okay” or “correct” and contrast words such as “however” or “although”. Given our observations regarding polarity and polarity particles in Section 3, in contrast to previous approaches we don’t include “yes” and “no” as local lexical cues. Instead, we add a new local feature encoding the polarity of the resp</context>
</contexts>
<marker>Pomerantz, 1984</marker>
<rawString>Anita Pomerantz. 1984. Agreeing and disagreeing with assessments: Some features of preferred/dispreferred turn shapes. In Structures of Social Action. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>John Dowding</author>
<author>John Niekrasz</author>
<author>Patrick Ehlen</author>
<author>Sharareh Noorbaloochi</author>
<author>Stanley Peters</author>
</authors>
<title>Detecting and summarizing action items in multi-party dialogue.</title>
<date>2007</date>
<booktitle>In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>18--25</pages>
<contexts>
<context position="5111" citStr="Purver et al., 2007" startWordPosition="818" endWordPosition="821">al., 2003) to develop systems that would classify utterances into agreements, disagreements, backchannels, and ‘other’. While these authors only leveraged lexical and prosodic features of the utterance to be classified (i.e., local features), Galley et al. (2004) showed that accuracy could be improved by taking into account contextual dependencies, in particular previous (dis)agreements between the dialogue participants, achieving an overall accuracy of 86.9%. Subsequent work built on Galley et al.’s approach showed that detecting agreement acts helped to identify public commitments to tasks (Purver et al., 2007) and other decisions made in a meeting (Fern´andez et al., 2008). A difficulty shared by all approaches mentioned above is the skewness of the data, not only regarding (dis)agreement vs. other types of acts, but also agreement vs. disagreement. In the dialogue settings considered, acceptance/agreement is much more common than rejection/disagreement (e.g., 11.9% vs. 6.7% in the portion of the ICSI Meeting Corpus used by Galley et al. (2004) and 3.6% vs. 0.4% in the section of the AMI Meeting Corpus used by Germesin and Wilson (2009)). This can lead to reasonable overall accuracy but poor result</context>
</contexts>
<marker>Purver, Dowding, Niekrasz, Ehlen, Noorbaloochi, Peters, 2007</marker>
<rawString>Matthew Purver, John Dowding, John Niekrasz, Patrick Ehlen, Sharareh Noorbaloochi, and Stanley Peters. 2007. Detecting and summarizing action items in multi-party dialogue. In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue, pages 18–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Stalnaker</author>
</authors>
<title>Assertion. In</title>
<date>1978</date>
<booktitle>of Syntax and Semantics,</booktitle>
<volume>9</volume>
<pages>315--332</pages>
<editor>P. Cole, editor, Pragmatics,</editor>
<publisher>Academic Press.</publisher>
<location>New York</location>
<contexts>
<context position="960" citStr="Stalnaker, 1978" startWordPosition="148" endWordPosition="149">n utterance in dialogue. We develop a model inspired by recent work on the semantics of negation and polarity particles and test it on annotated data from two spoken dialogue corpora: the Switchboard Corpus and the AMI Meeting Corpus. Our experiments show that taking into account the relative polarity of a proposal under discussion and of its response greatly helps to distinguish rejections from acceptances in both corpora. 1 Introduction In order to establish and maintain coherence, dialogue participants need to keep track of the information they jointly take for granted—their common ground (Stalnaker, 1978). As a dialogue progresses, the common ground typically evolves. New information becomes shared as the interlocutors exchange moves (such as assertions, questions, acceptances, and rejections) through the collaborative process of grounding (Clark and Schaefer, 1989; Clark, 1996). To keep track of the common ground, speakers must identify which information is accepted or rejected by their addressees. The basic idea is simple: If a proposal is rejected, its content does not enter the common ground, while if it is accepted, its content does become common belief. Yet, determining whether a respons</context>
</contexts>
<marker>Stalnaker, 1978</marker>
<rawString>Robert Stalnaker. 1978. Assertion. In P. Cole, editor, Pragmatics, volume 9 of Syntax and Semantics, pages 315–332. New York Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Jean E Fox Tree</author>
<author>Pranav Anand</author>
<author>Rob Abbott</author>
<author>Joseph King</author>
</authors>
<title>A corpus for research on deliberation and debate.</title>
<date>2012</date>
<booktitle>In LREC,</booktitle>
<pages>812--817</pages>
<contexts>
<context position="6585" citStr="Walker et al., 2012" startWordPosition="1057" endWordPosition="1060">ith AMI data, use different resampling methods to balance their dataset and then apply Conditional Random Fields (using therefore contextual information from sequences of utterances), achieving 56.9% recall and 55.9 F1 for disagreement detection. Some recent work has moved away from spoken dialogue to address similar tasks in online discussion forums. An advantage of this kind of scenarios is that they seem to offer more opportunity for disagreement/rejection, thereby yielding more inherently balanced datasets. Abbott et al. (2011) and Misra and Walker (2013) use the Internet Argument Corpus (Walker et al., 2012), an annotated collection of posts in discussion forums with a balanced distribution of agreeing and disagreeing posts. They address a 2-way classification task—determining whether each response to a post (or to a quoted portion of a post in the case of Abbott et al. (2011)) is either an agreement or a disagreement—using a collection of features inspired by previous computational and theoretical approaches. The system developed by Misra and Walker (2013) uses only local features of the tobe-classified post, achieving an accuracy of 66% (over a 50% baseline). Abbott et al. (2011)’s best system </context>
</contexts>
<marker>Walker, Tree, Anand, Abbott, King, 2012</marker>
<rawString>Marilyn A Walker, Jean E Fox Tree, Pranav Anand, Rob Abbott, and Joseph King. 2012. A corpus for research on deliberation and debate. In LREC, pages 812–817.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
</authors>
<title>Inferring acceptance and rejection in dialog by default rules of inference.</title>
<date>1996</date>
<journal>Language and Speech,</journal>
<pages>39--2</pages>
<contexts>
<context position="1830" citStr="Walker, 1996" startWordPosition="291" endWordPosition="292">er, 1989; Clark, 1996). To keep track of the common ground, speakers must identify which information is accepted or rejected by their addressees. The basic idea is simple: If a proposal is rejected, its content does not enter the common ground, while if it is accepted, its content does become common belief. Yet, determining whether a response to a move counts as an acceptance or a rejection is far from trivial. In many cases, the surface form of an utterance is not explicit enough to determine its acceptance or rejection force and inference is required (Horn, 1989; Lascarides and Asher, 2009; Walker, 1996). For instance, B’s utterance in (1), extracted from the AMI Meeting Corpus (Carletta, 2007), exemplifies what Walker (1996) calls implicature rejection (the rejection arises from an inferred scalar implicature: “normal” implicates “not interesting”; see also Hirschberg (1985)). (1) A: This is a very interesting design. B: It’s just the same as normal. The goal of this paper is to investigate the role of logical polarity in distinguishing rejections from acceptances. Consider the following dialogue excerpts, again from AMI, where the same utterance form (“Yes it is”) acts as an acceptance in (</context>
</contexts>
<marker>Walker, 1996</marker>
<rawString>Marilyn A. Walker. 1996. Inferring acceptance and rejection in dialog by default rules of inference. Language and Speech, 39(2-3):265–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Sibel Yaman</author>
<author>Kristin Precoda</author>
<author>Colleen Richey</author>
<author>Geoffrey Raymond</author>
</authors>
<title>Detection of agreement and disagreement in broadcast conversations.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>374--378</pages>
<contexts>
<context position="5948" citStr="Wang et al. (2011)" startWordPosition="957" endWordPosition="960">reement vs. disagreement. In the dialogue settings considered, acceptance/agreement is much more common than rejection/disagreement (e.g., 11.9% vs. 6.7% in the portion of the ICSI Meeting Corpus used by Galley et al. (2004) and 3.6% vs. 0.4% in the section of the AMI Meeting Corpus used by Germesin and Wilson (2009)). This can lead to reasonable overall accuracy but poor results on recognising rejections. Indeed, Germesin and Wilson (2009), who apply an approach based on Galley et al. (2004) to the AMI Meeting Corpus, achieve 98.1% accuracy, but report 0% recall for rejections/disagreements. Wang et al. (2011), who also work with AMI data, use different resampling methods to balance their dataset and then apply Conditional Random Fields (using therefore contextual information from sequences of utterances), achieving 56.9% recall and 55.9 F1 for disagreement detection. Some recent work has moved away from spoken dialogue to address similar tasks in online discussion forums. An advantage of this kind of scenarios is that they seem to offer more opportunity for disagreement/rejection, thereby yielding more inherently balanced datasets. Abbott et al. (2011) and Misra and Walker (2013) use the Internet </context>
</contexts>
<marker>Wang, Yaman, Precoda, Richey, Raymond, 2011</marker>
<rawString>Wen Wang, Sibel Yaman, Kristin Precoda, Colleen Richey, and Geoffrey Raymond. 2011. Detection of agreement and disagreement in broadcast conversations. In Proceedings of ACL, pages 374–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="8704" citStr="Wilson et al., 2005" startWordPosition="1395" endWordPosition="1398">d disagreement, such as different types of head, lip, and hand movements. However, to our knowledge, the role of logical polarity has not been investigated in any detail by computational approaches. Several systems make use of subjective polarity, i.e., sentiment. For instance, Galley et al. (2004) use the list of subjective adjectives compiled by Hatzivassiloglou and McKeown (1997) to assign a positive and a negative polarity value to an utterance given the number of subjective positive/negative adjectives it contains. Similarly, Misra and Walker (2013) use the MPQA Subjec152 tivity Lexicon (Wilson et al., 2005) to capture the local sentiment of an online post response given the number of words in the response with strongly subjective positive/negative polarity according to the subjectivity lexicon. Yin et al. (2012) assign a positive and a negative score to a post by aggregating the sentiment scores of those words that can be found in SentiWordNet (Baccianella et al., 2010). Although subjective polarity may be helpful (e.g., utterances with a high positive sentiment score may be more likely to be acceptances), this is not the kind of polarity that concerns us in this paper. Note, furthermore, that l</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of HLTEMNLP, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Yin</author>
<author>Paul Thomas</author>
<author>Nalin Narang</author>
<author>Cecile Paris</author>
</authors>
<title>Unifying Local and Global Agreement and Disagreement Classification in Online Debates.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis,</booktitle>
<pages>61--69</pages>
<contexts>
<context position="7515" citStr="Yin et al. (2012)" startWordPosition="1208" endWordPosition="1211">ent—using a collection of features inspired by previous computational and theoretical approaches. The system developed by Misra and Walker (2013) uses only local features of the tobe-classified post, achieving an accuracy of 66% (over a 50% baseline). Abbott et al. (2011)’s best system uses features from both the quoted post and the response post, achieving an accuracy of 68.2%. However adding this contextual information does not significantly outperform a system based only on local features of the response, which yields 66.6% accuracy. Using both features from the post and the post response, Yin et al. (2012) obtain similar results: 68% accuracy on a different online corpus (the Political Forum), where the datasets are not balanced (they report a ratio of about 2 to 1 for agreement vs. disagreement). All in all, this body of work has identified several linguistic features that are useful for inferring acceptances and rejections, often building on observations made by conversational analysts (Pomerantz, 1984; Brown and Levinson, 1987). Furthermore, recent work by Bousmalis et al. (2013) suggests that there are specific nonverbal behaviours associated with agreement and disagreement, such as differe</context>
<context position="8913" citStr="Yin et al. (2012)" startWordPosition="1427" endWordPosition="1430">ms make use of subjective polarity, i.e., sentiment. For instance, Galley et al. (2004) use the list of subjective adjectives compiled by Hatzivassiloglou and McKeown (1997) to assign a positive and a negative polarity value to an utterance given the number of subjective positive/negative adjectives it contains. Similarly, Misra and Walker (2013) use the MPQA Subjec152 tivity Lexicon (Wilson et al., 2005) to capture the local sentiment of an online post response given the number of words in the response with strongly subjective positive/negative polarity according to the subjectivity lexicon. Yin et al. (2012) assign a positive and a negative score to a post by aggregating the sentiment scores of those words that can be found in SentiWordNet (Baccianella et al., 2010). Although subjective polarity may be helpful (e.g., utterances with a high positive sentiment score may be more likely to be acceptances), this is not the kind of polarity that concerns us in this paper. Note, furthermore, that local sentiment information may be superseded by logical polarity. (4) A: But then it wouldn’t sit as comfortably in your hand. B: It would still be comfortable. Despite the fact that B’s utterance in (4)— extr</context>
</contexts>
<marker>Yin, Thomas, Narang, Paris, 2012</marker>
<rawString>Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris. 2012. Unifying Local and Global Agreement and Disagreement Classification in Online Debates. In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, pages 61–69.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
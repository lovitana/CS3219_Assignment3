<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003035">
<title confidence="0.9960655">
Detecting sociostructural beliefs about group status differences in online
discussions
</title>
<author confidence="0.939424">
Brian Riordan Heather Wade
</author>
<affiliation confidence="0.632752">
Aptima, Inc.
</affiliation>
<address confidence="0.54549">
3100 Presidential Drive
Fairborn, OH 45324
</address>
<email confidence="0.995449">
{briordan, hwade}@aptima.com
</email>
<note confidence="0.78302175">
Afzal Upal
Defence R&amp;D Canada Toronto
1133 Sheppard Ave W
Toronto, ON, M3K 2C9
</note>
<email confidence="0.730525">
Afzal.Upal@drdc-rddc.gc.ca
</email>
<sectionHeader confidence="0.985279" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999701529411765">
Detection of fine-grained opinions and
beliefs holds promise for improved so-
cial media analysis for social science re-
search, business intelligence, and govern-
ment decision-makers. While commercial
applications focus on mapping landscapes
of opinions towards brands and products,
our goal is to map “sociostructural” land-
scapes of perceptions of social groups. In
this work, we focus on the detection of
views of social group status differences.
We report an analysis of methods for de-
tecting views of the legitimacy of income
inequality in the U.S. from online dis-
cussions, and demonstrate detection rates
competitive with results from similar tasks
such as debate stance classification.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958375">
Social media and the internet continue to be a vast
resource for exploring and analyzing public opin-
ion. While there has been a longstanding focus
on detecting sentiment for commercial applica-
tions (Liu, 2012), in recent years there has been in-
creased interest in detecting opinions and perspec-
tives in politics and social science more generally
(Grimmer &amp; Stewart, 2013). Examples include
analyzing people’s perceptions of particular politi-
cal issues by classifying debate stances (Hasan and
Ng, 2013) and detecting the expression of ideol-
ogy (Sim et al., 2013). Research has increasingly
turned from detecting opinions and beliefs in gen-
eral (Prabhakaran et al., 2010) to discerning par-
ticular types of opinions or beliefs for specific ap-
plications.
The goal of our work is to detect indicators of
people’s views of social conditions and intergroup
perceptions in social media. Working within the
framework of Social Identity Theory (Tajfel and
Turner, 1979; Tajfel and Turner, 1986; Turner,
1999), we explore detection of the linguistic cor-
relates of sociostructural beliefs. Sociostructural
beliefs are abstract theoretical constructs in Social
Identity Theory that underpin individual and so-
cial identity formation and individual actions that
affect the relations between social groups.
For this study, we focus on class-based social
groups and the views of individuals on the issue
of income inequality. We seek to detect people’s
views of the legitimacy of the socio-economic
structure that has resulted in increasing income in-
equality, particularly in the U.S. Our approach fo-
cuses on comments on news articles related to the
issue of income inequality. We develop a series of
supervised classifiers for detecting the expression
of views on the legitimacy of income inequality.
We show promising results comparable to detec-
tion rates for other studies of social and political
perspectives.
</bodyText>
<sectionHeader confidence="0.971499" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.998802894736842">
Social Identity Theory attempts to account for how
subjectively perceived social structure can lead
people to define themselves in terms of a shared
social identity and thereby produce forms of in-
tergroup behavior. Social identity – how people
perceive their relations to the multiple groups to
which they belong – is argued to be a crucial part
of a person’s self-concept. People invoke part of
their social identities whenever they think of them-
selves as belonging to one gender, ethnicity, social
class, religion, etc. Group membership and social
identity play a role in shaping interpersonal inter-
actions.
Social Identity Theory (as well as social catego-
rization theory) holds that people are sensitive to
group status differences and are motivated to view
their own social groups positively. These two fac-
tors are key drivers of individuals’ social identity
management strategies. For example, membership
</bodyText>
<page confidence="0.795021">
1
</page>
<bodyText confidence="0.942343769230769">
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 1–6,
Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics
in a relatively low-status group may engender per-
ceptions of deprivation, which in turn may result in
individuals taking actions to increase their group’s
status or diminish the status of other groups (Tajfel
and Turner, 1979; Tajfel and Turner, 1986). Ac-
cording to Social Identity Theory, a group mem-
ber’s expectations of rewards of group member-
ship are importantly affected by sociostructural
beliefs about the nature of group status differ-
ences. Group status differences are thought to be
shaped by three types of these beliefs:
</bodyText>
<listItem confidence="0.9996182">
• Legitimacy: the degree to which people be-
lieve that group status differences are valid.
• Stability: people’s sense of how likely the sta-
tus hierarchy is to last into the future.
• Permeability: the perception of how easy it is
</listItem>
<bodyText confidence="0.914602">
for outsiders to enter or leave the group.
Based on these sociostructural beliefs and percep-
tions of the relative deprivation of one’s group,
people are motivated to take actions to maintain
and enhance their group’s image.
</bodyText>
<sectionHeader confidence="0.931116" genericHeader="method">
3 Detecting sociostructural beliefs
</sectionHeader>
<bodyText confidence="0.999863484848485">
A central challenge for extracting sociostructural
beliefs is determining where they are likely to
occur in natural discourse on the internet. So-
ciostructural beliefs relate to group status differ-
ences – for example, in terms of wealth, power, or
prestige. Hence, the most likely context for so-
ciostructural belief expressions is discussions of
issues that relate to such social differences.
While debate-focused websites (e.g., createde-
bate.com, debate.org) hold potential as a data
source, we found that in practice such websites
had few discussions of issues that might relate to
sociostructural beliefs and, furthermore, the num-
ber of posts for each topic was generally small.
In contrast, we found that highly relevant data can
be harvested from comments on news or opinion
articles from large newspapers or popular media
websites. Articles and op-eds commonly generate
hundreds of responses. We considered a variety
of topics related to social differences in ethnicity,
gender, religion, etc., but found the most data on
the topic of income inequality in the U.S. We col-
lected comments across several news articles and
op-ed pieces that focused on income inequality.
In the context of income inequality, the social
groups are hard to rigorously define, but in com-
ments it was common to observe a dichotomy be-
tween “rich” and “poor,” or “the 1 percent” and
“everyone else”. We observed comments on each
of the three types of sociostructural beliefs – legit-
imacy, stability, permeability – but by far the most
common topic of discussion was the legitimacy of
a large income gap. Therefore, we focused on de-
tecting expressions of legitimacy and leave the ex-
traction of expressions of stability and permeabil-
ity to future work.
In past survey research related to sociostructural
beliefs (Kessler and Mummendey, 2002; Mum-
mendey et al., 1999), participants were asked
to respond to explicit statements reflecting so-
ciostructural beliefs – for example, It is [justi-
fied|right|legitimate|accurate|fair] that [proposi-
tion]. However, we found no instances of such
explicit expressions in our data. Nevertheless,
beliefs about legitimacy are implicit in many in-
stances. For example, consider this comment:
Now we are all victims and we should
be given our fair share instead of earn-
ing our fair share. All the wealth should
be redistributed. The wealthy are vil-
ianized. The ones who have been able
to rely on their vision, innovation, self
motivation, sacrifice and wits are be-
ing called out by the envious.Like it or
not, the one-percenters are the ones who
have advanced humanity to the highest
standard ofliving - ever.
Although there is no explicit articulation of a
belief that it is legitimate for income inequality to
exist across social groups, for human annotators,
it is not difficult to infer that this author likely
believes that this is the case. Our goal is to un-
cover cases like this where sociostructural beliefs
are strongly implicit.
We formulated the problem as staged text clas-
sification (cf. Lamb et al. (2013)):
</bodyText>
<listItem confidence="0.955295833333333">
1. Finding comments that implicity express the
sociostructural belief in the legitimacy or il-
legitimacy of income inequality (+/-E);
2. Making a binary classification of the author’s
sociostructural belief (income inequality is
legitimate or not) (+/-L).
</listItem>
<sectionHeader confidence="0.969357" genericHeader="method">
4 Data Collection
</sectionHeader>
<bodyText confidence="0.9997265">
We scraped more than 10,000 comments from ar-
ticles from major internet media outlets related to
</bodyText>
<page confidence="0.962556">
2
</page>
<bodyText confidence="0.99535255">
the income inequality issue in the U.S., including
CNN, The New York Times, Daily Finance, and
marketwatch.com (The Wall Street Journal). For
example, we collected comments from the CNN
op-ed “Is income inequality ‘morally wrong’?”1,
which had attracted several thousand comments at
the time of data collection (and continues to re-
ceive more).
An initial set was randomly selected for annota-
tion for +/-E and +/-L by one of the authors. An-
other author independently annotated a subset of
these comments (N=100) and agreement was as-
sessed. While the agreement was low for the +/-
E label (κ = .282), for comments that the anno-
tators agreed were +E, the inter-annotator agree-
ment was high (κ = .916). After the annotators
discussed and resolved differences in the +/-E an-
notation guidelines, the first annotator continued
the annotation process to compile a final dataset.
Table 1 gives a summary of the final corpus.
</bodyText>
<table confidence="0.9993945">
+ - Total
Expression related to le- 400 1,088 1,488
gitimacy (E)
Support for legitimacy (L) 174 226 400
</table>
<tableCaption confidence="0.995919">
Table 1: Dataset annotation statistics.
</tableCaption>
<sectionHeader confidence="0.997358" genericHeader="method">
5 Features
</sectionHeader>
<subsectionHeader confidence="0.910197">
5.1 N-grams
</subsectionHeader>
<bodyText confidence="0.999947375">
As with similar tasks such as debate stance classi-
fication and sentiment tagging, token-level differ-
ences should provide a strong baseline for discrim-
inating between the classes of belief expression
(+/-E) and the belief in legitimacy (+/-L). There-
fore, we explored a variety of combinations of n-
gram features, including surface tokens, lemmas,
and parts of speech.
</bodyText>
<subsectionHeader confidence="0.998795">
5.2 Word classes
</subsectionHeader>
<bodyText confidence="0.999770666666667">
Beyond n-gram features, we expected that co-
herent sets of tokens would pattern together for
implicit beliefs about legitimacy of status differ-
ences. One of the authors coded a total of 24
classes for the income inequality setting based on
annotating a subset of about 100 comments. Ex-
amples are shown in Table 2. The classes reflected
both semantic similarity and, for some, polarity of
the sociostructural belief.
</bodyText>
<footnote confidence="0.910949">
1http://www.cnn.com/2013/07/25/opinion/sutter-income-
inequality-moral-obama/
</footnote>
<table confidence="0.999961884615384">
Word class Example words
income inequality gap, widening, inequality
lack of income in- equal chance, never fair,
equality free society
the non-rich (+) the 99%, have-nots
the non-rich (+/-) the poor, middle-class
the non-rich (-) lazy, dumb
change (+) fix, make changes
change(-) redistribution, impose
greed greed, exploit
hardship can’t afford, cost of living
rich – epithets shameful, evil, no empathy
poor – epithets soviet, communist, envy
rich individuals Buffet, Gates, Bloomberg
society safety net, playing field
business companies, profit
money wealth, income level, salary
the rich (+) wealthy, those with means
the rich (+/-) upper middle class
the rich (-) extreme rich, the 1%
deserve deserve, earn
work / effort work harder, effort
success success, fortune, move up
government regulation, bloated
taxes taxes, taxpayer, pay most of
lifestyle save, budget, responsibility
</table>
<tableCaption confidence="0.998045">
Table 2: Example word classes.
</tableCaption>
<subsectionHeader confidence="0.859574">
5.3 Quotation-related features
</subsectionHeader>
<bodyText confidence="0.998665578947368">
Excerpts from other posters’ comments and quo-
tations of famous individuals are common in our
dataset. For example:
“Everyone in America has an equal
chance an equal opportunity to suc-
ceed.” Dont know if Id go THAT far.
The author quotes a previous post’s words in or-
der to explicitly disagree with a statement. In this
case, n-gram features might indicate that the com-
ment should be labeled +L (since comments dis-
cussing an “equal opportunity to succeed” typi-
cally expressed this belief). However, the sec-
ond sentence expresses a negation of the ideas in
the quoted text. This issue is common in dia-
logic social media settings, particularly when de-
bating political or social issues, and poses a chal-
lenge to surface-oriented classifiers (Malouf and
Mullen, 2008). To address this issue, n-gram fea-
tures were computed specifically for text inside
</bodyText>
<page confidence="0.99737">
3
</page>
<bodyText confidence="0.9939215">
quotes (“quote features”) and text outside quotes
(“nonquote features”). In the quote above, the
words Everyone in America has an equal chance...
would contribute to the quote n-grams.
</bodyText>
<sectionHeader confidence="0.999077" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99968652173913">
For classification, we experimented with Naive
Bayes and MaxEnt (via MALLET2) and SVMs
(via LIBSVM3). Our baseline was a majority class
predictor. We began by comparing the results of
several different n-gram sets, including n-grams
from surface text or lemmatization, binary labels
or count features, combinations of unigrams, bi-
grams, trigrams, and 4-grams, and the inclusion
or exclusion of stopwords. We found that the n-
grams set of binary labels for unigrams, bigrams,
trigrams, and 4-grams after lemmatization had the
highest performance. The inclusion of stopwords
generally afforded better performance; hence we
do not remove stopwords.
We explored the hypothesis that this result was
due to the inclusion of negation operators among
stopwords. Negation may be useful to retain in
n-grams to distinguish expressions such as didn’t
earn from earned. We removed negation operators
from the stopword list. However, other than Max-
Ent, performance was worse4. What stylometric
features that stopwords capture to distinguish au-
thors’ beliefs in this task is left for future work.
</bodyText>
<table confidence="0.9964218">
Classifier +/-E +/-L
MLE 73.1 56.5
MaxEnt 79.9 66.0
Naive Bayes 75.9 68.3
SVM 80.1 66.3
</table>
<tableCaption confidence="0.998921">
Table 3: Comparison of classifiers by accuracy on
</tableCaption>
<bodyText confidence="0.997041545454546">
the +/-E and +/-L task with a feature set of: uni-
gram, bigram, trigram, and 4-gram lemma labels,
stopwords included. MLE = majority class.
The results for both the +/-E and +/-L tasks are
shown in Table 3. We report accuracy following
previous related work. We only report results for
the staged classifier setting (-E posts were not an-
notated for +/-L). For the +/-E task, absolute ac-
curacy values were high due to the very unbal-
anced dataset (cf. Table 1). On the +/-L task,
Naive Bayes achieved the highest accuracy score.
</bodyText>
<footnote confidence="0.998671666666667">
2http://mallet.cs.umass.edu/
3http://www.csie.ntu.edu.tw/ cjlin/libsvm/
4ME = 66.5, NB = 65.8, SVM = 63.0
</footnote>
<bodyText confidence="0.999700565217391">
Our dataset consisted of a mix of short and long
comments (M = 45.4 tokens, SD = 37.5 tokens),
which, interestingly, was not unfavorable to Naive
Bayes (cf. Wang and Manning (2012)). All classi-
fiers were significantly better than the baseline (by
paired samples t-tests on accuracy across folds in
cross-validation with p &lt;.05) in both tasks. On +/-
E, MaxEnt and SVM were not significantly differ-
ent; both performed better than Naive Bayes. On
+/-L, there were no significant differences.
Tables 4 and 5 report the results after adding
the +/-L problem-specific features to the best n-
gram set. The addition of the word class features
provides a small improvement in accuracy across
the classifiers. MaxEnt’s performance approached
significance compared to the others (p &lt;.1) These
results confirm that, for the task of detecting so-
ciostructural beliefs about legitimacy in this do-
main, words tokens do tend to co-occur in topical
and polarity-based word classes. However, it is
likely that our word class feature set suffered from
limited coverage relative to the diversity of expres-
sions used in the domain.
</bodyText>
<table confidence="0.976447">
Feature set MLE ME NB SVM
n-grams 56.5 66.0 68.3 66.3
+ WC counts 56.5 70.8 68.8 67.0
+ WC lab. 56.5 69.5 68.0 67.0
+ WC counts, lab. 56.5 69.8 68.8 66.8
</table>
<tableCaption confidence="0.954668666666667">
Table 4: Classification accuracies for the +/-L task
on variants of word class (WC) feature sets for
MaxEnt, NB, and SVM. MLE = majority class.
</tableCaption>
<bodyText confidence="0.955472944444445">
Table 5 reports the results of adding quotation
features. Performance improved with the addition
of these features, most notably with the addition
of both quote and nonquote features. While these
results suggest that accounting for quotations is
important, the inclusion of quotation-related fea-
tures only differentiates between words appearing
in quotations from those outside quotations, and
does not represent any relationship between the
two sets of features. The appearance of terms in a
quotation that are typically not found in quotations
and that are used by people expressing a particular
stance is often a strong indicator that the opinion
of the text surrounding the quotation is the oppo-
site of that in the quotation a relationship found
by Malouf and Mullen (2008)). Hence, more re-
search that explores relations between terms in and
outside of quotations would seem worthwhile.
</bodyText>
<page confidence="0.995651">
4
</page>
<bodyText confidence="0.99834525">
Finally, we experimented with combining both
word class features and quotation features, but per-
formance did not improve over the results for word
class features or quote features alone.
</bodyText>
<table confidence="0.991588625">
Feature set MLE ME NB SVM
n-grams 56.5 66.0 68.3 66.3
+ Q count 56.5 67.0 68.3 66.8
+ Q labels 56.5 66.0 68.8 66.3
+ Q count &amp; lab. 56.5 66.5 69.3 65.3
+ NQ labels 56.5 66.3 69.0 65.3
+ Q &amp; NQ 56.5 67.3 70.0 66.3
repl. w/ Q &amp; NQ 56.5 67.3 70.5 66.3
</table>
<tableCaption confidence="0.97819">
Table 5: Classification accuracies for the +/-L task
on variants of quotation (Q, NQ) feature sets for
MaxEnt, NB, and SVM. MLE = majority class.
</tableCaption>
<sectionHeader confidence="0.995837" genericHeader="method">
7 Error analysis
</sectionHeader>
<subsectionHeader confidence="0.998926">
7.1 Focus on a specific sub-issue
</subsectionHeader>
<bodyText confidence="0.9999911">
In discussions on income inequality, there are
“sub-issues” that are repeatedly discussed in com-
ments, including taxes, welfare, the U.S. economy,
and business owners. The difficulty of classifying
these kinds of comments stems from the difficulty
of deciding whether the comments contain an im-
plicit expression of a sociostructural belief, i.e.
the +/- E classification problem. Inference based
on world knowledge may be required to chain to-
gether the steps that link expressions to beliefs.
</bodyText>
<subsectionHeader confidence="0.996693">
7.2 Personal stories used as examples
</subsectionHeader>
<bodyText confidence="0.999320222222222">
In discussions involving social status, we observed
that people often use personal examples to support
their positions.
My Dad slept in a dresser drawer on the
floor with cotton stuffed under a sheet...
He graduated with an engineering de-
gree summa cum laude and has never
been un-employed for 45 years because
he always worked harder and made him-
self more valuable than his peers. No GI
Bill No Pell Grants No Welfare...
While a human annotator can usually infer which
view on legitimacy such a story supports, the con-
tent can seem unrelated to the issue of interest.
Similar behavior occurs on debate websites, where
descriptions of personal experiences add material
irrelevant to stance, often leading to misclassifica-
tion (Hasan and Ng, 2013).
</bodyText>
<subsectionHeader confidence="0.997397">
7.3 Importance of context
</subsectionHeader>
<bodyText confidence="0.99952475">
While we considered comments independently for
our classification task, comments can refer to or
reply to previous comments, such that the meaning
of a comment can be obscured without the con-
tent of these related comments. To address this is-
sue, techniques for incorporating other comments
in dialog threads may be fruitful (Walker et al.,
2012; Hasan and Ng, 2013).
</bodyText>
<sectionHeader confidence="0.999917" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999588571428572">
The goal of detection of sociostructural beliefs in
the context of Social Identity Theory is similar to
work in debate stance classification (Anand et al.,
2011; Hasan and Ng, 2013; Somasundaran and
Wiebe, 2009; Walker et al., 2012). For example,
Hasan and Ng (2013) developed methods for clas-
sifying author postings on debate websites into bi-
nary classes reflecting opposing stances on polit-
ical issues (e.g., gay marriage). Our setting dif-
fers in that “sides” of the issue are only hypoth-
esized (i.e., legitimate/illegitimate) and not given,
and stances are never explicitly observed. How-
ever, the behavior of posters appears to be similar
across debate sites and comments on news articles.
The work here also fits into the increasing focus
on content analysis for political and social science
analysis (Grimmer and Stewart, 2013). Much re-
cent work has focused on analysis of artifacts from
the political arena, such as speeches, floor debates,
or press releases (Gerrish and Blei, 2012; Sim et
al., 2013; Thomas et al., 2006).
</bodyText>
<sectionHeader confidence="0.998939" genericHeader="conclusions">
9 Discussion
</sectionHeader>
<bodyText confidence="0.999957375">
This work explored the task of detecting latent au-
thor beliefs in social media analysis. We focused
on the specific problem of detecting and classi-
fying sociostructural beliefs from Social Identity
Theory – beliefs about the legitimacy, stability,
and permeability of social groups and their status.
We collected and analyzed a dataset of social me-
dia comments centering on the issue of income in-
equality and sought to classify implicit author be-
liefs on the legitimacy of class-based income dis-
parity. Because of the heavily implicit nature of
sociostructural belief expression, we formulated
the detection problem as a form of text classifi-
cation. Our approach achieved classification accu-
racies competitive with results from similar tasks
such as debate stance classification.
</bodyText>
<page confidence="0.991183">
5
</page>
<sectionHeader confidence="0.989977" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999895191780822">
Anand, Pranav, Walker, Marilyn, Abbott, Rob, Tree,
Jean. E. Fox, Bowmani, Robeson, and Minor,
Michael. 2011. Classifying stance in online de-
bate. In Proceedings of the 2nd workshop on com-
putational approaches to subjectivity and sentiment
analysis.
Gerrish, Sean M., and Blei, David M. 2012. How They
Vote: Issue-Adjusted Models of Legislative Behav-
ior. In Advances in Neural Information Processing
Systems.
Grimmer, Justin, and Stewart, Brandon M. 2013. Text
as data: The promise and pitfalls of automatic con-
tent analysis methods for political texts. Political
Analysis, 21(3), 267297.
Hasan, Kazi Saidul, and Ng, Vincent. 2013. Frame
Semantics for Stance Classification. CoNLL-2013,
124.
Kessler, Thomas, and Mummendey, Am´elie. 2002.
Sequential or parallel processes? A longitudi-
nal field study concerning determinants of identity-
management strategies. Journal of Personality and
Social Psychology, 82(1), 75-88.
Lamb, Alex, Paul, Michael J., and Dredze, Mark.
2013. Separating fact from fear: Tracking flu in-
fections on Twitter. In Proceedings of NAACL-HLT.
Liu, Bing. 2012. Sentiment analysis and opinion min-
ing. Morgan &amp; Claypool.
Malouf, Robert, and Mullen, Tony. 2008. Taking
sides: User classification for informal online polit-
ical discourse. Internet Research, 18(2), 177-190.
Mummendey, Am´elie, Klink, Andreas, Mielke, Rose-
marie, Wenzel, Michael, and Blanz, Mathias. 1999.
Sociostructural characteristics of intergroup rela-
tions and identity management strategies: results
from a field study in East Germany. European Jour-
nal of Social Psychology, 29(2-3), 259285.
Prabhakaran, Vinodkumar, Rambow, Owen, and Diab,
Mona. 2010. Automatic committed belief tagging.
In Proceedings of COLING.
Sim, Yanchuan, Acree, Brice, Gross, Justin H., and
Smith, Noah A. 2013. Measuring ideological pro-
portions in political speeches. In Proceedings of
EMNLP.
Somasundaran, Swapna, and Wiebe, Janyce. 2009.
Recognizing stances in online debates. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP.
Tajfel, Henri and Turner, John C. 1979. An integrative
theory of intergroup conflict. In W. G. Austin and S.
Worchel (Eds.), The social psychology of intergroup
relations (pp. 3347). Monterey, CA: Brooks-Cole.
Tajfel, Henri and Turner, John C. 1986. The so-
cial identity theory of intergroup behaviour. In S.
Worchel, and W. G. Austin (Eds.), Psychology of in-
tergroup relations (pp. 724). Chicago, IL: Nelson-
Hall.
Thomas, Matt, Pang, Bo, and Lee, Lillian. 2006.
Get out the vote: Determining support or opposi-
tion from Congressional floor-debate transcripts. In
Proceedings of EMNLP.
Turner, John C. 1999. Some current issues in research
on social identity and self-categorization thoeries In
Ellemers, N., Spears, R., Doosje, B. Social identity
(pp. 6-34). Oxford: Blackwell.
Walker, Marilyn A., Anand, Pranav, Abbott, Robert,
and Grant, Ricky. 2012. Stance classification using
dialogic properties of persuasion. In Proceedings of
NAACL-HLT.
Wang, Sida I., and Manning, Christopher D. 2012.
Baselines and Bigrams: Simple, Good Sentiment
and Topic Classification. In Proceedings of ACL.
</reference>
<page confidence="0.998785">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.417616">
<title confidence="0.9981875">Detecting sociostructural beliefs about group status differences in online discussions</title>
<author confidence="0.999972">Brian Riordan Heather Wade</author>
<affiliation confidence="0.877267">Aptima,</affiliation>
<address confidence="0.9763225">3100 Presidential Fairborn, OH</address>
<affiliation confidence="0.7876635">Afzal Defence R&amp;D Canada</affiliation>
<address confidence="0.9876365">1133 Sheppard Ave Toronto, ON, M3K</address>
<email confidence="0.867327">Afzal.Upal@drdc-rddc.gc.ca</email>
<abstract confidence="0.999310444444444">Detection of fine-grained opinions and beliefs holds promise for improved social media analysis for social science research, business intelligence, and government decision-makers. While commercial applications focus on mapping landscapes of opinions towards brands and products, our goal is to map “sociostructural” landscapes of perceptions of social groups. In this work, we focus on the detection of views of social group status differences. We report an analysis of methods for detecting views of the legitimacy of income inequality in the U.S. from online discussions, and demonstrate detection rates competitive with results from similar tasks such as debate stance classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Fox</author>
<author>Robeson Bowmani</author>
<author>Michael Minor</author>
</authors>
<title>Classifying stance in online debate.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd</booktitle>
<marker>Fox, Bowmani, Minor, 2011</marker>
<rawString>Anand, Pranav, Walker, Marilyn, Abbott, Rob, Tree, Jean. E. Fox, Bowmani, Robeson, and Minor, Michael. 2011. Classifying stance in online debate. In Proceedings of the 2nd workshop on computational approaches to subjectivity and sentiment analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean M Gerrish</author>
<author>David M Blei</author>
</authors>
<title>How They Vote: Issue-Adjusted Models of Legislative Behavior.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="19868" citStr="Gerrish and Blei, 2012" startWordPosition="3164" endWordPosition="3167"> opposing stances on political issues (e.g., gay marriage). Our setting differs in that “sides” of the issue are only hypothesized (i.e., legitimate/illegitimate) and not given, and stances are never explicitly observed. However, the behavior of posters appears to be similar across debate sites and comments on news articles. The work here also fits into the increasing focus on content analysis for political and social science analysis (Grimmer and Stewart, 2013). Much recent work has focused on analysis of artifacts from the political arena, such as speeches, floor debates, or press releases (Gerrish and Blei, 2012; Sim et al., 2013; Thomas et al., 2006). 9 Discussion This work explored the task of detecting latent author beliefs in social media analysis. We focused on the specific problem of detecting and classifying sociostructural beliefs from Social Identity Theory – beliefs about the legitimacy, stability, and permeability of social groups and their status. We collected and analyzed a dataset of social media comments centering on the issue of income inequality and sought to classify implicit author beliefs on the legitimacy of class-based income disparity. Because of the heavily implicit nature of </context>
</contexts>
<marker>Gerrish, Blei, 2012</marker>
<rawString>Gerrish, Sean M., and Blei, David M. 2012. How They Vote: Issue-Adjusted Models of Legislative Behavior. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Grimmer</author>
<author>Brandon M Stewart</author>
</authors>
<title>Text as data: The promise and pitfalls of automatic content analysis methods for political texts. Political Analysis,</title>
<date>2013</date>
<volume>21</volume>
<issue>3</issue>
<pages>267297</pages>
<contexts>
<context position="19712" citStr="Grimmer and Stewart, 2013" startWordPosition="3138" endWordPosition="3141">2009; Walker et al., 2012). For example, Hasan and Ng (2013) developed methods for classifying author postings on debate websites into binary classes reflecting opposing stances on political issues (e.g., gay marriage). Our setting differs in that “sides” of the issue are only hypothesized (i.e., legitimate/illegitimate) and not given, and stances are never explicitly observed. However, the behavior of posters appears to be similar across debate sites and comments on news articles. The work here also fits into the increasing focus on content analysis for political and social science analysis (Grimmer and Stewart, 2013). Much recent work has focused on analysis of artifacts from the political arena, such as speeches, floor debates, or press releases (Gerrish and Blei, 2012; Sim et al., 2013; Thomas et al., 2006). 9 Discussion This work explored the task of detecting latent author beliefs in social media analysis. We focused on the specific problem of detecting and classifying sociostructural beliefs from Social Identity Theory – beliefs about the legitimacy, stability, and permeability of social groups and their status. We collected and analyzed a dataset of social media comments centering on the issue of in</context>
<context position="1388" citStr="Grimmer &amp; Stewart, 2013" startWordPosition="202" endWordPosition="205">port an analysis of methods for detecting views of the legitimacy of income inequality in the U.S. from online discussions, and demonstrate detection rates competitive with results from similar tasks such as debate stance classification. 1 Introduction Social media and the internet continue to be a vast resource for exploring and analyzing public opinion. While there has been a longstanding focus on detecting sentiment for commercial applications (Liu, 2012), in recent years there has been increased interest in detecting opinions and perspectives in politics and social science more generally (Grimmer &amp; Stewart, 2013). Examples include analyzing people’s perceptions of particular political issues by classifying debate stances (Hasan and Ng, 2013) and detecting the expression of ideology (Sim et al., 2013). Research has increasingly turned from detecting opinions and beliefs in general (Prabhakaran et al., 2010) to discerning particular types of opinions or beliefs for specific applications. The goal of our work is to detect indicators of people’s views of social conditions and intergroup perceptions in social media. Working within the framework of Social Identity Theory (Tajfel and Turner, 1979; Tajfel and</context>
</contexts>
<marker>Grimmer, Stewart, 2013</marker>
<rawString>Grimmer, Justin, and Stewart, Brandon M. 2013. Text as data: The promise and pitfalls of automatic content analysis methods for political texts. Political Analysis, 21(3), 267297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazi Saidul Hasan</author>
<author>Vincent Ng</author>
</authors>
<title>Frame Semantics for Stance Classification.</title>
<date>2013</date>
<booktitle>CoNLL-2013,</booktitle>
<pages>124</pages>
<contexts>
<context position="1519" citStr="Hasan and Ng, 2013" startWordPosition="220" endWordPosition="223">ate detection rates competitive with results from similar tasks such as debate stance classification. 1 Introduction Social media and the internet continue to be a vast resource for exploring and analyzing public opinion. While there has been a longstanding focus on detecting sentiment for commercial applications (Liu, 2012), in recent years there has been increased interest in detecting opinions and perspectives in politics and social science more generally (Grimmer &amp; Stewart, 2013). Examples include analyzing people’s perceptions of particular political issues by classifying debate stances (Hasan and Ng, 2013) and detecting the expression of ideology (Sim et al., 2013). Research has increasingly turned from detecting opinions and beliefs in general (Prabhakaran et al., 2010) to discerning particular types of opinions or beliefs for specific applications. The goal of our work is to detect indicators of people’s views of social conditions and intergroup perceptions in social media. Working within the framework of Social Identity Theory (Tajfel and Turner, 1979; Tajfel and Turner, 1986; Turner, 1999), we explore detection of the linguistic correlates of sociostructural beliefs. Sociostructural beliefs</context>
<context position="18470" citStr="Hasan and Ng, 2013" startWordPosition="2938" endWordPosition="2941">slept in a dresser drawer on the floor with cotton stuffed under a sheet... He graduated with an engineering degree summa cum laude and has never been un-employed for 45 years because he always worked harder and made himself more valuable than his peers. No GI Bill No Pell Grants No Welfare... While a human annotator can usually infer which view on legitimacy such a story supports, the content can seem unrelated to the issue of interest. Similar behavior occurs on debate websites, where descriptions of personal experiences add material irrelevant to stance, often leading to misclassification (Hasan and Ng, 2013). 7.3 Importance of context While we considered comments independently for our classification task, comments can refer to or reply to previous comments, such that the meaning of a comment can be obscured without the content of these related comments. To address this issue, techniques for incorporating other comments in dialog threads may be fruitful (Walker et al., 2012; Hasan and Ng, 2013). 8 Related Work The goal of detection of sociostructural beliefs in the context of Social Identity Theory is similar to work in debate stance classification (Anand et al., 2011; Hasan and Ng, 2013; Somasund</context>
</contexts>
<marker>Hasan, Ng, 2013</marker>
<rawString>Hasan, Kazi Saidul, and Ng, Vincent. 2013. Frame Semantics for Stance Classification. CoNLL-2013, 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Kessler</author>
<author>Am´elie Mummendey</author>
</authors>
<title>Sequential or parallel processes? A longitudinal field study concerning determinants of identitymanagement strategies.</title>
<date>2002</date>
<journal>Journal of Personality and Social Psychology,</journal>
<volume>82</volume>
<issue>1</issue>
<pages>75--88</pages>
<contexts>
<context position="6872" citStr="Kessler and Mummendey, 2002" startWordPosition="1063" endWordPosition="1066"> income inequality, the social groups are hard to rigorously define, but in comments it was common to observe a dichotomy between “rich” and “poor,” or “the 1 percent” and “everyone else”. We observed comments on each of the three types of sociostructural beliefs – legitimacy, stability, permeability – but by far the most common topic of discussion was the legitimacy of a large income gap. Therefore, we focused on detecting expressions of legitimacy and leave the extraction of expressions of stability and permeability to future work. In past survey research related to sociostructural beliefs (Kessler and Mummendey, 2002; Mummendey et al., 1999), participants were asked to respond to explicit statements reflecting sociostructural beliefs – for example, It is [justified|right|legitimate|accurate|fair] that [proposition]. However, we found no instances of such explicit expressions in our data. Nevertheless, beliefs about legitimacy are implicit in many instances. For example, consider this comment: Now we are all victims and we should be given our fair share instead of earning our fair share. All the wealth should be redistributed. The wealthy are vilianized. The ones who have been able to rely on their vision,</context>
</contexts>
<marker>Kessler, Mummendey, 2002</marker>
<rawString>Kessler, Thomas, and Mummendey, Am´elie. 2002. Sequential or parallel processes? A longitudinal field study concerning determinants of identitymanagement strategies. Journal of Personality and Social Psychology, 82(1), 75-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lamb</author>
<author>Michael J Paul</author>
<author>Mark Dredze</author>
</authors>
<title>Separating fact from fear: Tracking flu infections on Twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="8081" citStr="Lamb et al. (2013)" startWordPosition="1261" endWordPosition="1264">r vision, innovation, self motivation, sacrifice and wits are being called out by the envious.Like it or not, the one-percenters are the ones who have advanced humanity to the highest standard ofliving - ever. Although there is no explicit articulation of a belief that it is legitimate for income inequality to exist across social groups, for human annotators, it is not difficult to infer that this author likely believes that this is the case. Our goal is to uncover cases like this where sociostructural beliefs are strongly implicit. We formulated the problem as staged text classification (cf. Lamb et al. (2013)): 1. Finding comments that implicity express the sociostructural belief in the legitimacy or illegitimacy of income inequality (+/-E); 2. Making a binary classification of the author’s sociostructural belief (income inequality is legitimate or not) (+/-L). 4 Data Collection We scraped more than 10,000 comments from articles from major internet media outlets related to 2 the income inequality issue in the U.S., including CNN, The New York Times, Daily Finance, and marketwatch.com (The Wall Street Journal). For example, we collected comments from the CNN op-ed “Is income inequality ‘morally wro</context>
</contexts>
<marker>Lamb, Paul, Dredze, 2013</marker>
<rawString>Lamb, Alex, Paul, Michael J., and Dredze, Mark. 2013. Separating fact from fear: Tracking flu infections on Twitter. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool.</publisher>
<contexts>
<context position="1226" citStr="Liu, 2012" startWordPosition="178" endWordPosition="179">iostructural” landscapes of perceptions of social groups. In this work, we focus on the detection of views of social group status differences. We report an analysis of methods for detecting views of the legitimacy of income inequality in the U.S. from online discussions, and demonstrate detection rates competitive with results from similar tasks such as debate stance classification. 1 Introduction Social media and the internet continue to be a vast resource for exploring and analyzing public opinion. While there has been a longstanding focus on detecting sentiment for commercial applications (Liu, 2012), in recent years there has been increased interest in detecting opinions and perspectives in politics and social science more generally (Grimmer &amp; Stewart, 2013). Examples include analyzing people’s perceptions of particular political issues by classifying debate stances (Hasan and Ng, 2013) and detecting the expression of ideology (Sim et al., 2013). Research has increasingly turned from detecting opinions and beliefs in general (Prabhakaran et al., 2010) to discerning particular types of opinions or beliefs for specific applications. The goal of our work is to detect indicators of people’s </context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Liu, Bing. 2012. Sentiment analysis and opinion mining. Morgan &amp; Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>Tony Mullen</author>
</authors>
<title>Taking sides: User classification for informal online political discourse.</title>
<date>2008</date>
<journal>Internet Research,</journal>
<volume>18</volume>
<issue>2</issue>
<pages>177--190</pages>
<contexts>
<context position="12122" citStr="Malouf and Mullen, 2008" startWordPosition="1895" endWordPosition="1898">s an equal chance an equal opportunity to succeed.” Dont know if Id go THAT far. The author quotes a previous post’s words in order to explicitly disagree with a statement. In this case, n-gram features might indicate that the comment should be labeled +L (since comments discussing an “equal opportunity to succeed” typically expressed this belief). However, the second sentence expresses a negation of the ideas in the quoted text. This issue is common in dialogic social media settings, particularly when debating political or social issues, and poses a challenge to surface-oriented classifiers (Malouf and Mullen, 2008). To address this issue, n-gram features were computed specifically for text inside 3 quotes (“quote features”) and text outside quotes (“nonquote features”). In the quote above, the words Everyone in America has an equal chance... would contribute to the quote n-grams. 6 Experiments For classification, we experimented with Naive Bayes and MaxEnt (via MALLET2) and SVMs (via LIBSVM3). Our baseline was a majority class predictor. We began by comparing the results of several different n-gram sets, including n-grams from surface text or lemmatization, binary labels or count features, combinations </context>
<context position="16444" citStr="Malouf and Mullen (2008)" startWordPosition="2594" endWordPosition="2597">h quote and nonquote features. While these results suggest that accounting for quotations is important, the inclusion of quotation-related features only differentiates between words appearing in quotations from those outside quotations, and does not represent any relationship between the two sets of features. The appearance of terms in a quotation that are typically not found in quotations and that are used by people expressing a particular stance is often a strong indicator that the opinion of the text surrounding the quotation is the opposite of that in the quotation a relationship found by Malouf and Mullen (2008)). Hence, more research that explores relations between terms in and outside of quotations would seem worthwhile. 4 Finally, we experimented with combining both word class features and quotation features, but performance did not improve over the results for word class features or quote features alone. Feature set MLE ME NB SVM n-grams 56.5 66.0 68.3 66.3 + Q count 56.5 67.0 68.3 66.8 + Q labels 56.5 66.0 68.8 66.3 + Q count &amp; lab. 56.5 66.5 69.3 65.3 + NQ labels 56.5 66.3 69.0 65.3 + Q &amp; NQ 56.5 67.3 70.0 66.3 repl. w/ Q &amp; NQ 56.5 67.3 70.5 66.3 Table 5: Classification accuracies for the +/-L </context>
</contexts>
<marker>Malouf, Mullen, 2008</marker>
<rawString>Malouf, Robert, and Mullen, Tony. 2008. Taking sides: User classification for informal online political discourse. Internet Research, 18(2), 177-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Am´elie Mummendey</author>
<author>Andreas Klink</author>
<author>Rosemarie Mielke</author>
<author>Michael Wenzel</author>
<author>Mathias Blanz</author>
</authors>
<title>Sociostructural characteristics of intergroup relations and identity management strategies: results from a field study in East Germany.</title>
<date>1999</date>
<journal>European Journal of Social Psychology,</journal>
<volume>29</volume>
<issue>2</issue>
<pages>259285</pages>
<contexts>
<context position="6897" citStr="Mummendey et al., 1999" startWordPosition="1067" endWordPosition="1071">l groups are hard to rigorously define, but in comments it was common to observe a dichotomy between “rich” and “poor,” or “the 1 percent” and “everyone else”. We observed comments on each of the three types of sociostructural beliefs – legitimacy, stability, permeability – but by far the most common topic of discussion was the legitimacy of a large income gap. Therefore, we focused on detecting expressions of legitimacy and leave the extraction of expressions of stability and permeability to future work. In past survey research related to sociostructural beliefs (Kessler and Mummendey, 2002; Mummendey et al., 1999), participants were asked to respond to explicit statements reflecting sociostructural beliefs – for example, It is [justified|right|legitimate|accurate|fair] that [proposition]. However, we found no instances of such explicit expressions in our data. Nevertheless, beliefs about legitimacy are implicit in many instances. For example, consider this comment: Now we are all victims and we should be given our fair share instead of earning our fair share. All the wealth should be redistributed. The wealthy are vilianized. The ones who have been able to rely on their vision, innovation, self motivat</context>
</contexts>
<marker>Mummendey, Klink, Mielke, Wenzel, Blanz, 1999</marker>
<rawString>Mummendey, Am´elie, Klink, Andreas, Mielke, Rosemarie, Wenzel, Michael, and Blanz, Mathias. 1999. Sociostructural characteristics of intergroup relations and identity management strategies: results from a field study in East Germany. European Journal of Social Psychology, 29(2-3), 259285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Owen Rambow</author>
<author>Mona Diab</author>
</authors>
<title>Automatic committed belief tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1687" citStr="Prabhakaran et al., 2010" startWordPosition="247" endWordPosition="250">a vast resource for exploring and analyzing public opinion. While there has been a longstanding focus on detecting sentiment for commercial applications (Liu, 2012), in recent years there has been increased interest in detecting opinions and perspectives in politics and social science more generally (Grimmer &amp; Stewart, 2013). Examples include analyzing people’s perceptions of particular political issues by classifying debate stances (Hasan and Ng, 2013) and detecting the expression of ideology (Sim et al., 2013). Research has increasingly turned from detecting opinions and beliefs in general (Prabhakaran et al., 2010) to discerning particular types of opinions or beliefs for specific applications. The goal of our work is to detect indicators of people’s views of social conditions and intergroup perceptions in social media. Working within the framework of Social Identity Theory (Tajfel and Turner, 1979; Tajfel and Turner, 1986; Turner, 1999), we explore detection of the linguistic correlates of sociostructural beliefs. Sociostructural beliefs are abstract theoretical constructs in Social Identity Theory that underpin individual and social identity formation and individual actions that affect the relations b</context>
</contexts>
<marker>Prabhakaran, Rambow, Diab, 2010</marker>
<rawString>Prabhakaran, Vinodkumar, Rambow, Owen, and Diab, Mona. 2010. Automatic committed belief tagging. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanchuan Sim</author>
<author>Brice Acree</author>
<author>Justin H Gross</author>
<author>Noah A Smith</author>
</authors>
<title>Measuring ideological proportions in political speeches.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1579" citStr="Sim et al., 2013" startWordPosition="231" endWordPosition="234">s such as debate stance classification. 1 Introduction Social media and the internet continue to be a vast resource for exploring and analyzing public opinion. While there has been a longstanding focus on detecting sentiment for commercial applications (Liu, 2012), in recent years there has been increased interest in detecting opinions and perspectives in politics and social science more generally (Grimmer &amp; Stewart, 2013). Examples include analyzing people’s perceptions of particular political issues by classifying debate stances (Hasan and Ng, 2013) and detecting the expression of ideology (Sim et al., 2013). Research has increasingly turned from detecting opinions and beliefs in general (Prabhakaran et al., 2010) to discerning particular types of opinions or beliefs for specific applications. The goal of our work is to detect indicators of people’s views of social conditions and intergroup perceptions in social media. Working within the framework of Social Identity Theory (Tajfel and Turner, 1979; Tajfel and Turner, 1986; Turner, 1999), we explore detection of the linguistic correlates of sociostructural beliefs. Sociostructural beliefs are abstract theoretical constructs in Social Identity Theo</context>
<context position="19886" citStr="Sim et al., 2013" startWordPosition="3168" endWordPosition="3171">itical issues (e.g., gay marriage). Our setting differs in that “sides” of the issue are only hypothesized (i.e., legitimate/illegitimate) and not given, and stances are never explicitly observed. However, the behavior of posters appears to be similar across debate sites and comments on news articles. The work here also fits into the increasing focus on content analysis for political and social science analysis (Grimmer and Stewart, 2013). Much recent work has focused on analysis of artifacts from the political arena, such as speeches, floor debates, or press releases (Gerrish and Blei, 2012; Sim et al., 2013; Thomas et al., 2006). 9 Discussion This work explored the task of detecting latent author beliefs in social media analysis. We focused on the specific problem of detecting and classifying sociostructural beliefs from Social Identity Theory – beliefs about the legitimacy, stability, and permeability of social groups and their status. We collected and analyzed a dataset of social media comments centering on the issue of income inequality and sought to classify implicit author beliefs on the legitimacy of class-based income disparity. Because of the heavily implicit nature of sociostructural be</context>
</contexts>
<marker>Sim, Acree, Gross, Smith, 2013</marker>
<rawString>Sim, Yanchuan, Acree, Brice, Gross, Justin H., and Smith, Noah A. 2013. Measuring ideological proportions in political speeches. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</booktitle>
<contexts>
<context position="19090" citStr="Somasundaran and Wiebe, 2009" startWordPosition="3039" endWordPosition="3042">g, 2013). 7.3 Importance of context While we considered comments independently for our classification task, comments can refer to or reply to previous comments, such that the meaning of a comment can be obscured without the content of these related comments. To address this issue, techniques for incorporating other comments in dialog threads may be fruitful (Walker et al., 2012; Hasan and Ng, 2013). 8 Related Work The goal of detection of sociostructural beliefs in the context of Social Identity Theory is similar to work in debate stance classification (Anand et al., 2011; Hasan and Ng, 2013; Somasundaran and Wiebe, 2009; Walker et al., 2012). For example, Hasan and Ng (2013) developed methods for classifying author postings on debate websites into binary classes reflecting opposing stances on political issues (e.g., gay marriage). Our setting differs in that “sides” of the issue are only hypothesized (i.e., legitimate/illegitimate) and not given, and stances are never explicitly observed. However, the behavior of posters appears to be similar across debate sites and comments on news articles. The work here also fits into the increasing focus on content analysis for political and social science analysis (Grim</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>Somasundaran, Swapna, and Wiebe, Janyce. 2009. Recognizing stances in online debates. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henri Tajfel</author>
<author>John C Turner</author>
</authors>
<title>An integrative theory of intergroup conflict. In</title>
<date>1979</date>
<pages>3347</pages>
<publisher>Brooks-Cole.</publisher>
<location>Monterey, CA:</location>
<contexts>
<context position="1976" citStr="Tajfel and Turner, 1979" startWordPosition="293" endWordPosition="296">generally (Grimmer &amp; Stewart, 2013). Examples include analyzing people’s perceptions of particular political issues by classifying debate stances (Hasan and Ng, 2013) and detecting the expression of ideology (Sim et al., 2013). Research has increasingly turned from detecting opinions and beliefs in general (Prabhakaran et al., 2010) to discerning particular types of opinions or beliefs for specific applications. The goal of our work is to detect indicators of people’s views of social conditions and intergroup perceptions in social media. Working within the framework of Social Identity Theory (Tajfel and Turner, 1979; Tajfel and Turner, 1986; Turner, 1999), we explore detection of the linguistic correlates of sociostructural beliefs. Sociostructural beliefs are abstract theoretical constructs in Social Identity Theory that underpin individual and social identity formation and individual actions that affect the relations between social groups. For this study, we focus on class-based social groups and the views of individuals on the issue of income inequality. We seek to detect people’s views of the legitimacy of the socio-economic structure that has resulted in increasing income inequality, particularly in</context>
<context position="4258" citStr="Tajfel and Turner, 1979" startWordPosition="643" endWordPosition="646">to group status differences and are motivated to view their own social groups positively. These two factors are key drivers of individuals’ social identity management strategies. For example, membership 1 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 1–6, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics in a relatively low-status group may engender perceptions of deprivation, which in turn may result in individuals taking actions to increase their group’s status or diminish the status of other groups (Tajfel and Turner, 1979; Tajfel and Turner, 1986). According to Social Identity Theory, a group member’s expectations of rewards of group membership are importantly affected by sociostructural beliefs about the nature of group status differences. Group status differences are thought to be shaped by three types of these beliefs: • Legitimacy: the degree to which people believe that group status differences are valid. • Stability: people’s sense of how likely the status hierarchy is to last into the future. • Permeability: the perception of how easy it is for outsiders to enter or leave the group. Based on these socio</context>
</contexts>
<marker>Tajfel, Turner, 1979</marker>
<rawString>Tajfel, Henri and Turner, John C. 1979. An integrative theory of intergroup conflict. In W. G. Austin and S. Worchel (Eds.), The social psychology of intergroup relations (pp. 3347). Monterey, CA: Brooks-Cole.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henri Tajfel</author>
<author>John C Turner</author>
</authors>
<title>The social identity theory of intergroup behaviour. In</title>
<date>1986</date>
<pages>724</pages>
<publisher>NelsonHall.</publisher>
<location>Chicago, IL:</location>
<contexts>
<context position="2001" citStr="Tajfel and Turner, 1986" startWordPosition="297" endWordPosition="300">art, 2013). Examples include analyzing people’s perceptions of particular political issues by classifying debate stances (Hasan and Ng, 2013) and detecting the expression of ideology (Sim et al., 2013). Research has increasingly turned from detecting opinions and beliefs in general (Prabhakaran et al., 2010) to discerning particular types of opinions or beliefs for specific applications. The goal of our work is to detect indicators of people’s views of social conditions and intergroup perceptions in social media. Working within the framework of Social Identity Theory (Tajfel and Turner, 1979; Tajfel and Turner, 1986; Turner, 1999), we explore detection of the linguistic correlates of sociostructural beliefs. Sociostructural beliefs are abstract theoretical constructs in Social Identity Theory that underpin individual and social identity formation and individual actions that affect the relations between social groups. For this study, we focus on class-based social groups and the views of individuals on the issue of income inequality. We seek to detect people’s views of the legitimacy of the socio-economic structure that has resulted in increasing income inequality, particularly in the U.S. Our approach fo</context>
<context position="4284" citStr="Tajfel and Turner, 1986" startWordPosition="647" endWordPosition="650">es and are motivated to view their own social groups positively. These two factors are key drivers of individuals’ social identity management strategies. For example, membership 1 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 1–6, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics in a relatively low-status group may engender perceptions of deprivation, which in turn may result in individuals taking actions to increase their group’s status or diminish the status of other groups (Tajfel and Turner, 1979; Tajfel and Turner, 1986). According to Social Identity Theory, a group member’s expectations of rewards of group membership are importantly affected by sociostructural beliefs about the nature of group status differences. Group status differences are thought to be shaped by three types of these beliefs: • Legitimacy: the degree to which people believe that group status differences are valid. • Stability: people’s sense of how likely the status hierarchy is to last into the future. • Permeability: the perception of how easy it is for outsiders to enter or leave the group. Based on these sociostructural beliefs and per</context>
</contexts>
<marker>Tajfel, Turner, 1986</marker>
<rawString>Tajfel, Henri and Turner, John C. 1986. The social identity theory of intergroup behaviour. In S. Worchel, and W. G. Austin (Eds.), Psychology of intergroup relations (pp. 724). Chicago, IL: NelsonHall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from Congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="19908" citStr="Thomas et al., 2006" startWordPosition="3172" endWordPosition="3175">., gay marriage). Our setting differs in that “sides” of the issue are only hypothesized (i.e., legitimate/illegitimate) and not given, and stances are never explicitly observed. However, the behavior of posters appears to be similar across debate sites and comments on news articles. The work here also fits into the increasing focus on content analysis for political and social science analysis (Grimmer and Stewart, 2013). Much recent work has focused on analysis of artifacts from the political arena, such as speeches, floor debates, or press releases (Gerrish and Blei, 2012; Sim et al., 2013; Thomas et al., 2006). 9 Discussion This work explored the task of detecting latent author beliefs in social media analysis. We focused on the specific problem of detecting and classifying sociostructural beliefs from Social Identity Theory – beliefs about the legitimacy, stability, and permeability of social groups and their status. We collected and analyzed a dataset of social media comments centering on the issue of income inequality and sought to classify implicit author beliefs on the legitimacy of class-based income disparity. Because of the heavily implicit nature of sociostructural belief expression, we fo</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Thomas, Matt, Pang, Bo, and Lee, Lillian. 2006. Get out the vote: Determining support or opposition from Congressional floor-debate transcripts. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Turner</author>
</authors>
<title>Some current issues in research on social identity and self-categorization thoeries In</title>
<date>1999</date>
<pages>6--34</pages>
<publisher>Blackwell.</publisher>
<location>Oxford:</location>
<contexts>
<context position="2016" citStr="Turner, 1999" startWordPosition="301" endWordPosition="302">ude analyzing people’s perceptions of particular political issues by classifying debate stances (Hasan and Ng, 2013) and detecting the expression of ideology (Sim et al., 2013). Research has increasingly turned from detecting opinions and beliefs in general (Prabhakaran et al., 2010) to discerning particular types of opinions or beliefs for specific applications. The goal of our work is to detect indicators of people’s views of social conditions and intergroup perceptions in social media. Working within the framework of Social Identity Theory (Tajfel and Turner, 1979; Tajfel and Turner, 1986; Turner, 1999), we explore detection of the linguistic correlates of sociostructural beliefs. Sociostructural beliefs are abstract theoretical constructs in Social Identity Theory that underpin individual and social identity formation and individual actions that affect the relations between social groups. For this study, we focus on class-based social groups and the views of individuals on the issue of income inequality. We seek to detect people’s views of the legitimacy of the socio-economic structure that has resulted in increasing income inequality, particularly in the U.S. Our approach focuses on commen</context>
</contexts>
<marker>Turner, 1999</marker>
<rawString>Turner, John C. 1999. Some current issues in research on social identity and self-categorization thoeries In Ellemers, N., Spears, R., Doosje, B. Social identity (pp. 6-34). Oxford: Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Pranav Anand</author>
<author>Robert Abbott</author>
<author>Ricky Grant</author>
</authors>
<title>Stance classification using dialogic properties of persuasion.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="18842" citStr="Walker et al., 2012" startWordPosition="2998" endWordPosition="3001">ry supports, the content can seem unrelated to the issue of interest. Similar behavior occurs on debate websites, where descriptions of personal experiences add material irrelevant to stance, often leading to misclassification (Hasan and Ng, 2013). 7.3 Importance of context While we considered comments independently for our classification task, comments can refer to or reply to previous comments, such that the meaning of a comment can be obscured without the content of these related comments. To address this issue, techniques for incorporating other comments in dialog threads may be fruitful (Walker et al., 2012; Hasan and Ng, 2013). 8 Related Work The goal of detection of sociostructural beliefs in the context of Social Identity Theory is similar to work in debate stance classification (Anand et al., 2011; Hasan and Ng, 2013; Somasundaran and Wiebe, 2009; Walker et al., 2012). For example, Hasan and Ng (2013) developed methods for classifying author postings on debate websites into binary classes reflecting opposing stances on political issues (e.g., gay marriage). Our setting differs in that “sides” of the issue are only hypothesized (i.e., legitimate/illegitimate) and not given, and stances are ne</context>
</contexts>
<marker>Walker, Anand, Abbott, Grant, 2012</marker>
<rawString>Walker, Marilyn A., Anand, Pranav, Abbott, Robert, and Grant, Ricky. 2012. Stance classification using dialogic properties of persuasion. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida I Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Baselines and Bigrams: Simple, Good Sentiment and Topic Classification.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="14439" citStr="Wang and Manning (2012)" startWordPosition="2265" endWordPosition="2268">hown in Table 3. We report accuracy following previous related work. We only report results for the staged classifier setting (-E posts were not annotated for +/-L). For the +/-E task, absolute accuracy values were high due to the very unbalanced dataset (cf. Table 1). On the +/-L task, Naive Bayes achieved the highest accuracy score. 2http://mallet.cs.umass.edu/ 3http://www.csie.ntu.edu.tw/ cjlin/libsvm/ 4ME = 66.5, NB = 65.8, SVM = 63.0 Our dataset consisted of a mix of short and long comments (M = 45.4 tokens, SD = 37.5 tokens), which, interestingly, was not unfavorable to Naive Bayes (cf. Wang and Manning (2012)). All classifiers were significantly better than the baseline (by paired samples t-tests on accuracy across folds in cross-validation with p &lt;.05) in both tasks. On +/- E, MaxEnt and SVM were not significantly different; both performed better than Naive Bayes. On +/-L, there were no significant differences. Tables 4 and 5 report the results after adding the +/-L problem-specific features to the best ngram set. The addition of the word class features provides a small improvement in accuracy across the classifiers. MaxEnt’s performance approached significance compared to the others (p &lt;.1) Thes</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Wang, Sida I., and Manning, Christopher D. 2012. Baselines and Bigrams: Simple, Good Sentiment and Topic Classification. In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000059">
<title confidence="0.9965985">
Social and Semantic Diversity:
Socio-semantic Representation of a Scientific Corpus
</title>
<author confidence="0.717834">
Elisa Omodei
</author>
<note confidence="0.6138135">
LATTICE and ISC-PIF
CNRS &amp; ENS &amp; U. Sorbonne Nouvelle
</note>
<address confidence="0.8606385">
1 rue Mauriece Arnoux
92120 Montrouge France
</address>
<email confidence="0.996815">
elisa.omodei@ens.fr
</email>
<author confidence="0.837673">
Jean-Philippe Cointet
</author>
<affiliation confidence="0.609761">
INRA Sens and ISC-PIF
</affiliation>
<address confidence="0.732515">
Cit´e Descartes, 5 boulevard Descartes
77454 Marne-la-Vall´ee Cedex France
75013 Paris France
</address>
<email confidence="0.996059">
jphcoi@yahoo.fr
</email>
<author confidence="0.995462">
Yufan Guo
</author>
<affiliation confidence="0.975122">
University of Washington
Computer Science
</affiliation>
<address confidence="0.713278">
Engineering
Box 352350 Seattle, WA 98195-2350
</address>
<email confidence="0.998354">
yufanguo@cs.washington.edu
</email>
<author confidence="0.919934">
Thierry Poibeau
</author>
<affiliation confidence="0.70506">
LATTICE
</affiliation>
<address confidence="0.721025666666667">
CNRS &amp; ENS &amp; U. Sorbonne Nouvelle
1 rue Mauriece Arnoux
92120 Montrouge France
</address>
<email confidence="0.992904">
thierry.poibeau@ens.fr
</email>
<sectionHeader confidence="0.995549" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999549230769231">
We propose a new method to extract key-
words from texts and categorize these
keywords according to their informational
value, derived from the analysis of the ar-
gumentative goal of the sentences they ap-
pear in. The method is applied to the ACL
Anthology corpus, containing papers on
the computational linguistic domain pub-
lished between 1980 and 2008. We show
that our approach allows to highlight inter-
esting facts concerning the evolution of the
topics and methods used in computational
linguistics.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956782608696">
Big data makes it possible to observe in vivo the
dynamics of a large number of different domains.
It is particularly the case in the scientific field,
where researchers produce a prolific literature but
also other kinds of data like numbers, figures, im-
ages and so on. For a number of domains, large
scientific archives are now available over several
decades.
This is for example the case for computational
linguistics. The ACL Anthology contains more
than 24,500 papers, for the most part in PDF for-
mat. The oldest ones date back to 1965 (first edi-
tion of the COLING conference) but it is mostly
after 1980 that data are available in large volumes
so that they can be exploited in evolution studies.
The volume of data increases over time, which
means there is a wide diversity in the number of
papers available depending on the given period of
time. There are similar archives for different do-
mains like, e.g. physics (the APS database pro-
vided by the American Physical Society) or the
bio-medical domain (with Medline).
These scientific archives have already given
birth to a large number of different pieces of work.
Collaboration networks have for example been au-
tomatically extracted so as to study the topology
of the domain (Girvan and Newman, 2002) or
its morphogenesis (Guimera et al., 2005). Ref-
erencing has also been the subject of numerous
studies on inter-citation (Garfield, 1972) and co-
citation (Small, 1973). Other variables can be
taken into account like the nationality of the au-
thors, the projects they are involved in or the re-
search institutions they belong to, but it is the anal-
ysis of the textual content (mostly titles, abstracts
and keywords provided with the papers) that have
attracted the most part of the research in the area
since the seminal work of Callon (Callon et al.,
1986; Callon et al., 1991).
In this paper, our goal is to investigate the evo-
lution of the field of computational linguistics,
which means that text will play a crucial role. Tex-
tual analysis is then mixed with the study of indi-
vidual trajectories in the semantic space: our goal
is to propose possible avenues for the study of the
dynamics of innovation in the computational lin-
</bodyText>
<page confidence="0.986662">
71
</page>
<note confidence="0.993321">
Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 71–79,
Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.994177631578947">
guistics domain.
The ACL Anthology has been the subject of
several studies in 2012, for the 50 years of the
ACL. More specifically, a workshop called “Re-
discovering 50 Years of Discoveries” was orga-
nized to examine 50 years of research in NLP
(but, for the reasons given above, the workshop
mostly focused on the evolution of the domain
since 1980). This workshop was also an oppor-
tunity to study a large scientific collection with re-
cent NLP techniques and see how these techniques
can be applied to study the dynamics of a scientific
domain.
The analysis of this kind of data is generally
based on the extraction of key information (au-
thors, keywords) and the discovery of their rela-
tionships. The data can be represented as a graph,
therefore graph algorithmics can be used to study
the topology and the evolution of the graph of col-
laborations or the graph of linked authors. It is
thus possible to observe the evolution of the do-
main, check some hypotheses or common assump-
tions about this evolution and provide a strong em-
pirical basis to epistemology studies.
The paper “Towards a computational History of
the ACL: 1980-2008” is very relevant from this
point of view (Anderson et al., 2012). The au-
thors try to determine the evolution of the main
sub-domains of research within NLP since 1980
and they obtain very interesting results. For ex-
ample, they show the influence of the American
evaluation campaigns on the domain: when a US
agency sponsored a sub-domain of NLP, one can
observe a quick concentration effect since a wide
number of research groups suddenly concentrated
their efforts on the topic; when no evaluation cam-
paign was organized, research was much more
widespread across the different sub-domains of
NLP. Even if this is partially predictable, it was
not obvious to be able to show this in a collection
of papers as large as the ACL Anthology.
Our study has been profoundly influenced by
the study by Anderson et al. However, our goal
here is to characterize automatically the keywords
based on the information they carry. We will thus
combine keyword extraction with text zoning so
as to categorize the keywords depending on their
context of use.
The rest of the paper is organized as follows.
We first present an analysis of the structure of ab-
stracts so as to better characterize their content by
mixing keyword extraction with text zoning. We
show how these techniques can be applied to the
ACL Anthology in order to examine specific facts,
more specifically concerning the evolution of the
techniques used in the computational linguistics
domain.
</bodyText>
<sectionHeader confidence="0.9739755" genericHeader="method">
2 A Text Zoning Analysis of the ACL
Anthology
</sectionHeader>
<bodyText confidence="0.999234">
The study of the evolution of topics in large cor-
pora is usually done through keyword extraction.
This is also our goal, but we would like to be able
to better characterize these keywords and make a
difference, for example, between keywords refer-
ring to concepts and keywords referring to meth-
ods. Hence, the context of these keywords seems
highly important. Consequently, we propose to
use Text Zoning that can provide an accurate char-
acterization of the argumentative goal of each sen-
tence in a scientific abstract.
</bodyText>
<subsectionHeader confidence="0.825623">
2.1 Previous work
</subsectionHeader>
<bodyText confidence="0.999763655172414">
The first important contributions in text zoning are
probably the experiments by S. Teufel who pro-
posed to categorize sentences in scientific papers
(and more specifically, in the NLP domain) ac-
cording to different categories (Teufel, 1999) like
BKG: General scientific background, AIM: State-
ments of the particular aim of the current paper or
CTR: Contrastive or comparative statements about
other work. This task is called Rhetorical zoning
or Argumentative zoning since the goal is to iden-
tify the rhetoric or argumentative role of each sen-
tence of the text.
The initial work of Teufel was based on the
manual annotation of 80 papers representing the
different areas of NLP (the corpus was made of
papers published within the ACL conferences or
Computational Linguistics). A classifier was then
trained on this manually annotated corpus. The
author reported interesting results despite “a 20%
diference between [the] system and human perfor-
mance” (Teufel and Moens, 2002). The learning
method used a Naive Bayesian model since more
sophisticated methods tested by the author did not
obtain better results. Teufel in subsequent publica-
tions showed that the technique can be used to pro-
duce high quality summaries (Teufel and Moens,
2002) or precisely characterize the different cita-
tions in a paper (Ritchie et al., 2008).
The seminal work of Teufel has since then given
</bodyText>
<page confidence="0.996798">
72
</page>
<bodyText confidence="0.99993209375">
rise to different kinds of works, on the one hand
to refine the annotation method, and on the other
hand to check its applicability to different scien-
tific domains. Concerning the first point, research
has focused on the identification of relevant fea-
tures for classification, on the evaluation of dif-
ferent learning algorithms for the task and more
importantly on the reduction of the volume of text
to be annotated. Concerning the second point, it
is mostly the biological and bio-medical domains
that have attracted attention, since scientists in
these domains often have to access the literature
“vertically” (i.e. experts may need to have access
to all the methods and protocols that have been
used in a specific domain) (Mizuta et al., 2006;
Tbahriti et al., 2006).
Guo has since developed a similar trend of re-
search to extend the initial work of Teufel (Guo
et al., 2011; Guo et al., 2013): she has tested a
large list of features to analyze the zones, evalu-
ated different learning algorithms for the task and
proposed new methods to decrease the number of
texts to be annotated. The features used for learn-
ing are of three categories: i) positional (location
of the sentence inside the paper), ii) lexical (words,
classes of words, bigrams, etc. are taken into con-
sideration) and iii) syntactic (the different syntac-
tic relations as well as the class of words appear-
ing in subject or object positions are taken into ac-
count). The analysis is thus based on more fea-
tures than in Teufel’s initial work and requires a
parser.
</bodyText>
<subsectionHeader confidence="0.996749">
2.2 Application to the ACL Anthology corpus
</subsectionHeader>
<bodyText confidence="0.999980340425532">
In our experiment, we only used the abstracts of
the papers. Our hypothesis is that abstracts con-
tain enough information and are redundant enough
to study the evolution of the domain. Taking into
consideration the full text would probably give too
many details and thus introduce noise in the anal-
ysis.
The annotation scheme includes five different
categories, which are the following: OBJEC-
TIVE (objectives of the paper), METHOD (meth-
ods used in the paper), RESULTS (main results),
CONCLUSION (conclusion of the paper), BACK-
GROUND (general context), as in (Reichart and
Korhonen, 2012). These categories are also close
to those of (Mizuta et al., 2006; Guo et al., 2011;
Guo et al., 2013) and have been adapted to ab-
stracts (as opposed to full text1). It seems relevant
to take into consideration an annotation scheme
that has already been used by various authors so
that the results are easy to compare to others.
Around one hundred abstracts from the ACL
Anthology have then been manually annotated us-
ing this scheme (-500 sentences; ACL abstracts
are generally quite short since most of them are
related to conference papers). The selection of the
abstracts has been done using stratified sampling
over time and journals, so as to obtain a represen-
tative corpus (papers must be related to different
periods of time and different sub-areas of the do-
main). The annotation has been done according
to the annotation guideline defined by Y. Guo, es-
pecially for long sentences when more than one
category could be applied (preferences are defined
to solve complex cases2).
The algorithm defined by (Guo et al., 2011) is
then adapted to our corpus. The analysis is based
on positional, lexical and syntactic features, as ex-
plained above. No domain specific information
was added, which makes the whole process easy
to reproduce. As for parsing, we used the C&amp;C
parser (James Curran and Stephen Clark and Johan
Bos, 2007). All the implementation details can be
found in (Guo et al., 2011), especially concerning
annotation and the learning algorithm. As a result,
each sentence is associated with a tag correspond-
ing to one of the zones defined in the annotation
scheme.
</bodyText>
<subsectionHeader confidence="0.950194">
2.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.9999308">
In order to evaluate the text zoning task, a num-
ber of abstracts were chosen randomly (-300 sen-
tences that do not overlap with the training set).
CONCLUSION represented less than 3% of the
sentences and was then dropped for the rest of
the analysis. The four remaining zones are un-
equaly represented: 18.05 % of the sentences re-
fer to BACKGROUND, 14.35% to OBJECTIVE,
14.81 % to RESULT and 52.77 % to METHOD.
Just by looking at these numbers, one can see how
</bodyText>
<footnote confidence="0.6971451">
1The categories used in (Teufel, 1999) were not relevant
since this model focused on full text papers, with a special
emphasis on the novelty of the author’s work and the attitude
towards other people’s work, which is not the case here.
2The task is to assign the sentence only a single category.
The choice of the category should be made according to the
following priority list: Conclusion &gt; Objective &gt; Result &gt;
Method &gt; Background. The only exception is that when 75%
or more of the sentence belongs to a less preferred category,
then that category will be assigned to the sentence.
</footnote>
<page confidence="0.999314">
73
</page>
<tableCaption confidence="0.8520515">
Table 1: Result of the text zoning analysis (preci-
sion)
</tableCaption>
<table confidence="0.6900768">
Category Precision
Objective 83,87 %
Background 81,25 %
Method 71,05 %
Results 82,05 %
</table>
<figureCaption confidence="0.986756">
Figure 1: An abstract annotated with text zoning
information. Categories are indicated in bold face.
</figureCaption>
<bodyText confidence="0.999141863636364">
Most of errors in Korean morphological analysis and
POS ( Part-of-Speech ) tagging are caused by unknown
morphemes. BACKGROUND
This paper presents a generalized unknown morpheme
handling method with POSTAG(POStech TAGger )
which is a statistical/rule based hybrid POS tagging
system. OBJECTIVE
The generalized unknown morpheme guessing is based
on a combination of a morpheme pattern dictionary
which encodes general lexical patterns of Korean
morphemes with a posteriori syllable tri-gram estimation
.METHOD
The syllable tri-grams help to calculate lexical proba-
bilities of the unknown morphemes and are utilized to
search the best tagging result. METHOD
In our scheme , we can guess the POS’s of unknown
morphemes regardless of their numbers and positions
in an eojeol , which was not possible before in Korean
tagging systems. RESULTS
In a series of experiments using three different domain
corpora , we can achieve 97% tagging accuracy regard-
less of many unknown morphemes in test corpora .
</bodyText>
<sectionHeader confidence="0.962983" genericHeader="method">
RESULTS
</sectionHeader>
<bodyText confidence="0.999926028571429">
methodological issues are important for the do-
main.
We then calculate for each of the categories, the
percentage of sentences that received the right la-
bel, which allows us to calculate precision. The
results are given in table 1.
These results are similar to the state of the art
(Guo et al., 2011), which is positive taking into
consideration the small number of sentences an-
notated for training. The diversity of the features
used makes it easy to transfer the technique from
one domain to the other without any heavy anno-
tation phase. Results are slightly worse for the
METHOD category, probably because this cate-
gory is more diverse and thus more difficult to rec-
ognize. The fact that NLP terms can refer either to
objectives or to methods also contributes render-
ing the recognition of this category more difficult.
Figure 1 shows an abstract annotated by the text
zoning module (the paper is (Lee et al., 2002): it
has been chosen randomly between those contain-
ing the different types of zones). One category
is associated with each sentence but this is some-
times problematic: for example the fact that a hy-
brid method is used is mentioned in a sentence that
is globally tagged as OBJECTIVE by the system.
However, sentences tagged as METHOD contain
relevant keywords like lexical pattern or tri-gram
estimation, which makes it possible to infer that
the approach is hybrid. One can also spot some
problems with digitization, which are typical of
this corpus: the ACL Anthology contains automat-
ically converted files to PDF, which means texts
are not perfect and may contain some digitization
errors.
</bodyText>
<sectionHeader confidence="0.9903775" genericHeader="method">
3 Contribution to the Study of the
Evolution ACL Anthology
</sectionHeader>
<bodyText confidence="0.999983571428571">
As said above, we are largely inspired by (Ander-
son et al., 2012). We think the ACL Anthology
is typical since it contains papers spanning over
more than 30 years: it is thus interesting to use it
as a way to study the main evolutions of the com-
putational linguistics domain. The method can of
course also be applied to other scientific corpora.
</bodyText>
<subsectionHeader confidence="0.999304">
3.1 Keyword extraction and characterization
</subsectionHeader>
<bodyText confidence="0.999930625">
The first step consists in identifying the main key-
words of the domain. We then want to more pre-
cisely categorize these keywords so as to identify
the ones specifically referring to methods for ex-
ample. From this perspective, keywords appear-
ing in the METHOD sections are thus particularly
interesting for us. However, one major problem is
that there is no clear-cut difference between goals
and methods in NLP since most systems are made
of different layers and require various NLP tech-
niques. For example, a semantic analyzer may use
a part-of-speech tagger and a parser, which means
NLP tools can appear as part of the method.
Keyword extraction aims at automatically ex-
tracting relevant keywords from a collection of
texts. A popular approach consists in first extract-
ing typical sequences of tags that are then filtered
according to specific criteria (these criteria can in-
clude the use of external resources but they are
more generally based on scores mixing frequency
and specificity (Bourigault and Jacquemin, 1999;
Frantzi and Ananiadou, 2000)). In this study, we
voluntarily used a minimal approach for keyword
extraction and filtering since we want to keep most
</bodyText>
<page confidence="0.998782">
74
</page>
<tableCaption confidence="0.984565">
Table 2: Most specific keywords found in the METHOD sections.
</tableCaption>
<table confidence="0.999721837837838">
Methods
Category Method N-grams
Machine learning Bayesian methods baesyan
Vector Space model space model, vector space, cosine
Genetic algorithms genetic algorithms
HMM hidden markov models, markov model
CRF conditional random fields
SVM support vector machines
MaxEnt maximum entropy model, maximum entropy approach, maximum entropy
Clustering clustering algorithm, clustering method, word clusters, classification problem
Speech &amp; Mach. Trans. Language models large-vocabulary, n-gram language model, Viterbi
Parallel Corpora parallel corpus, bilingual corpus, phrase pairs, source and target languages, sentence pairs, word pairs,
Alignment source sentence
phrase alignment, alignment algorithm, alignment models, ibm model, phrase translation, translation
candidates, sentence alignment
NLP Methods POS tagging part-of-speech tagger, part-of-speech tags
Morphology two-level morphology, morphological analyzer, morphological rules
FST finite-state transducers, regular expressions, state automata, rule-based approach
Syntax syntactic categories, syntactic patterns, extraction patterns
Dependency parsing dependency parser, dependency graphs, prague dependency, dependency treebank, derivation trees, parse
Parsing trees
Semantics grammar rules, parser output, parsing process, parsed sentences, transfer rules
logical forms, inference rules, generative lexicon, lexical rules, lexico-syntactic, predicate argument
Applications IE and IR entity recognition, answer candidates, temporal information, web search, query expansion, google, user
Discourse queries, keywords, query terms, term recognition
Segmentation generation component, dialogue acts, centering theory, lexical chains, resolution algorithm, generation
process, discourse model, lexical choice
machine transliteration, phonological rules, segmentation algorithm, word boundaries
Words and Resource Lexical knowledge bases lexical knowledge base, semantic network, machine readable dictionaries, eurowordnet, lexical entries,
Word similarity dictionary entries, lexical units, representation structures, lookup
Corpora word associations, mutual information, semantic relationships, word similarity, semantic similarity,
semeval-2007, word co-occurrence, synonymy
brown corpus, dialogue corpus, annotation scheme, tagged corpus
Evaluation Evaluation score, gold standard, evaluation measures, estimation method
Calculation &amp; complexity Software tool development, polynomial time, software tools, series of experiments, system architecture, runtime,
Constraints programming language
relaxation, constraint satisfaction, semantic constraints
</table>
<bodyText confidence="0.999608166666667">
of the information for the subsequent text zoning
phase. We thus used NLTK for part-of-speech tag-
ging and from this result extracted the most com-
mon noun phrases. We used a pre-defined set
of grammatical patterns to extract noun phrases
defined as sequences of simple sequences (e.g.
adjectives + nouns, “phrase pairs”, “dependency
graph”, etc.) possibly connected to other such pat-
terns through propositions to form longer phrases
(e.g. ”series of experiments”). Only the noun
phrases appearing in more than 10 papers are kept
for subsequent processing.
Candidate keywords are then ranked per zone,
according to their specificity (the zone they are
the most specific of) . Specificity corresponds to
the Kolmogorov-Smirnov test that quantifies a dis-
tance between the empirical distribution functions
of two samples. The test is calculated as follows:
</bodyText>
<equation confidence="0.997353">
|SN1(x) − SN2(x) |(1)
</equation>
<bodyText confidence="0.99995445">
where SN1(x) et SN2(x) are the empirical distri-
bution function of the two samples (that corre-
spond in our case to the number of occurrences
of the keyword in a given zone, and to the total
number of occurrences of all the keywords in the
same zone, respectively) (Press et al., 2007). A
high value of D for a given keyword means that it
is highly specific of the considered zone. At the
opposite, a low value means that the keyword is
spread over the different zones and not really spe-
cific of any zone.
The first keywords of each category are then
categorized by an expert of the domain. For the
METHOD category, we obtain Table 2. Logically,
given our approach, the table does not contain all
the keywords relevant for the computational lin-
guistics domain, but it contains the mots specific
ones according to the above approach. One should
thus not be surprised not to see all the keywords
used in the domain.
</bodyText>
<subsectionHeader confidence="0.999681">
3.2 Evolution of methods over time
</subsectionHeader>
<bodyText confidence="0.999812428571428">
The automatic analysis of the corpus allows us to
track the main evolutions of the field over time.
During the last 30 years, the methods used have
changed to a large extent, the most notable fact be-
ing probably the generalization of machine learn-
ing methods since the late 1990s. This is outlined
by the fact that papers in the domain nowadays
nearly always include a section that describes an
experiment and some results.
To confirm this hypothesis, we observe the rel-
ative frequency of sentences tagged as RESULTS
in the papers over time. In the figure 3, we see that
the curve increases almost linearly from the early
1980s until the late 2000s.
</bodyText>
<figure confidence="0.53681925">
D=max
X
75
NLP Methods Applications Machine Learning
</figure>
<figureCaption confidence="0.998464">
Figure 2: Evolution of the relative frequency of the different groups of methods over time.
</figureCaption>
<figure confidence="0.999697774509804">
Year
Year
Clustering
MaxEnt
SVM
CRF
HMM
Genetic algorithms
Vector Space model
Calculation &amp; Complexity
Resources
1
0.9
0.8
0.7
0.6
Relative Frequency
0.5
Word similarity
Lexical knowledge bases
0.4
0.3
0.2
0.1
0
1
1
0.9
0.9
0.8
0.8
0.7
0.7
0.6
0.6
Relative Frequency
Segmentation
Discourse
IE and IR
0.5
0.5
0.4
0.4
0.3
0.3
0.2
0.2
0.1
0.1
0
0
Year
Year
1
0.9
0.8
0.7
Parsing
Dependency parsing
Syntax
FST
Morphology
POS tagging
0.6
Relative Frequency
Relative Frequency
0.5
0.4
0.3
0.2
0.1
0
Year
Speech &amp; machine translation specific
1
0.9
0.8
0.7
0.6
Relative Frequency
Relative Frequency
Alignment
Parallel Corpora
Language models
0.5
0.4
0.3
0.2
0.1
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Year
</figure>
<bodyText confidence="0.99987">
It is also possible to make more fine-grained ob-
servations, for example to follow over time the dif-
ferent kinds of methods under consideration. The
results are shown in figure 2. Rule based methods
and manually crafted resources are used all over
the period, while machine learning based meth-
ods are more and more successful after the late
1990s. This is not surprising since we know that
machine learning is now highly popular within the
field. However, symbolic methods are still used,
sometimes in conjunction with learning methods.
The two kinds of methods are thus more comple-
mentary than antagonistic.
One could observe details that should be
checked through a more thorough study. We ob-
serve for example the success of dependency pars-
ing in the end of the 1980s (probably due to the
success of the Tree Adjoining Grammars at the
time) and the new popularity of this area of re-
search in the early 2000s (dependency parsing has
been the subject of several evaluation campaigns
in the 2000s, see for example for the CONLL
shared tasks from 2006 to 2009).
Different machine learning methods have been
popular over time but each of them continues to be
used after a first wave corresponding to their ini-
tial success. Hidden Markov Models and n-grams
are highly popular in the 1990s, probably thanks
to the experiments made by Jelinek and his col-
leagues, which will open the field of statistical ma-
chine translation (Brown et al., 1990). SVM and
CRF have had a more recent success as everybody
knows.
We are also interested in the distribution of
these methods between papers and authors. Fig-
ure 4 shows the average number of keywords
</bodyText>
<figure confidence="0.7231495">
Results
Year
</figure>
<figureCaption confidence="0.942178">
Figure 3: Evolution of the relative frequency of
sentences tagged as RESULTS in the abstracts of
the papers
</figureCaption>
<bodyText confidence="0.999842785714286">
appearing in the METHOD section of the papers
over time. We see that this number regularly in-
creases, especially during the 1980s, showing pos-
sibly a gradually increasing complexity of the sys-
tems under consideration.
Lastly, figure 5 shows the number of authors
who are specialists of one or several methods.
Most of the authors just mention one method in
their papers and, logically, the curves decrease,
which means that there are few authors who are
really specialists of many methods. This result
should be confirmed by a larger scale study tak-
ing into account a larger number of keywords but
the trend seems however interesting.
</bodyText>
<subsectionHeader confidence="0.99948">
3.3 The dynamics of the authors in the
method space
</subsectionHeader>
<bodyText confidence="0.9996745">
One could say that the results we have reported in
the previous section are not new but rather confirm
some already well known facts. Our method al-
lows to go one step further and try to answer more
</bodyText>
<figure confidence="0.995249894736842">
Relative Frequency
0.15
0.1
0.25
0.2
0.05
0
76
1 Fraction of pionners that are
0.9 new to the field
0.8 Fraction of authors that enter
0.7 the field in those years
0.6
0.5
0.4
0.3
0.2
0.1
0
</figure>
<figureCaption confidence="0.974571166666667">
Figure 4: Evolution of the number of keywords
related to methods over time.
Figure 6: For each “new method”, number of “pi-
oneers” not having published any paper before
(compared to the total number of new authors dur-
ing the same period of time).
</figureCaption>
<figure confidence="0.999660789473684">
Number of methods
0.6
0.5
0.4
Proportion of authors
0.3
0.2
0.1
Authors who published 5 papers
Authors who published 6 papers
Authors who published 7 papers
Authors who published 8 papers
Authors who published 9 papers
Authors who published 10 papers
Authors who published 11 papers
Authors who published 12 papers
Authors who published 13 papers
0
1 2 3 4 5 6 7 8
</figure>
<figureCaption confidence="0.877394">
Figure 5: Proportion of authors specialized in
a given number of methods (i.e. mentioning
</figureCaption>
<bodyText confidence="0.992032645161291">
frequently the name of the method in the ab-
stracts), for different categories of researchers.
challenging questions. How are new methods in-
troduced in the field? Are they mainly brought
by young researchers or is it mainly confirmed re-
searchers who develop new techniques (or import
them from related fields)? Are NLP experts spe-
cialized in one field or in a wide variety of differ-
ent fields?
These questions are of course quite complex.
Each individual has his own expertise and his
own history but we think that automatic meth-
ods can provide some interesting trends over time.
For example, (Anderson et al., 2012) show that
evaluation campaigns have played a central role
at certain periods of time, which does not mean
of course that there was no independent research
outside these campaigns at the time. Our goal
is thus to exhibit some tendencies that could be
interpreted or even make it possible to compare
the evolution of the computational linguistics field
with other fields. Out tools provide some hypothe-
ses that must of course be confirmed by further ob-
servations and analysis. We do not claim that they
provide an exact and accurate view of the domain.
For this study we only take into account authors
who have published at least 5 papers in the ACL
Anthology, in order to take into consideration au-
thors who have contributed to the domain during a
period of time relevant for the study. We consider
as “pioneers” the authors of the first 25% of pa-
pers in which a keyword referring to a method is
introduced (for example, the first papers where the
keywords support vector machine or SVM appear).
We then calculate, among this set of authors, the
ones who can be considered as new authors, which
means people who have not published before in
the field. Since there are every year a large number
of new authors (who use standard techniques) we
compare the ratio of new authors using new tech-
niques with the number of authors using already
known techniques over the considered period. Re-
sults are visible in figure 6.
Results are variable depending on the method
under consideration but some of them seem inter-
esting. Papers with the keyword Hidden Markov
Model in the 1990s seem to be largely written
by new comers, probably by researchers having
tested this method in related fields before (and
we know that it was the case of Jelinek’s team
who was largely involved in speech processing, a
domain not so well represented in the ACL An-
thology before the 1990s. Of course, Jelinek and
colleague were confirmed and even highly estab-
lished researchers already at the beginning of the
1990s). We observe a similar patten for genetic
algorithms but the number of authors is too lim-
ited to say if the trend is really meaningful. SVM
also seem to have been popularized by new com-
ers but it is not the case of language models or of
the vector space model. A more thorough study is
of course needed to confirm and better understand
</bodyText>
<page confidence="0.988408">
77
</page>
<figure confidence="0.995530090909091">
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
Pioneers proportion
Total authors proportion
Cumulative Distribution Function
0.8
0.6
0.4
0.2
0
1
Proportion of authors
Number of methods per author
1 2 3 4 5 6 7 8
0 0.2 0.4 0.6 0.8 1
Fraction of total production of author already published
</figure>
<figureCaption confidence="0.6526455">
Figure 7: Distribution function of the number of
papers already published by “pioneers” when they
have published their paper on the new method,
compared to the total production of their career.
</figureCaption>
<bodyText confidence="0.998330171428571">
these results.
We then do a similar experiment to try to de-
termine when, during their career, researchers use
new methods. Practically, we examine at what
point of their career the authors who are character-
ized as “pioneers” in our study (what refers to the
first authors using a new method) have published
the papers containing new methods (for example,
if an author is one of the first who employed the
keyword SVM, has he done this at the beginning
of his career or later on?). The result is visible in
figure 7 and shows that 60% of pioneers had pub-
lished less than a third of their scientific produc-
tion when they use the new method. We thus ob-
serve a similar set of authors between the pioneers
and researchers having published so far in related
but nevertheless different communities. To con-
firm this result, it would be useful to study other
domains and other corpora (in computer science,
linguistics, cognitive sciences) so as to get a better
picture of the domain, but the task is then highly
challenging.
One may want then to observe the diversity of
methods employed in the domain, especially by
the set of people called “pioneers” in our study.
Figure 8 shows in blue the number of methods
detected for the pioneers and in red the number of
methods used by all the authors.
We see that pioneers, when taking into consid-
eration the whole set of papers in the ACL An-
thology, are using a larger number of methods.
They are over represented among authors using 3
methods and more. This group of people also con-
tribute to a larger number of sub-areas in the do-
mains compared to the set of other authors.
</bodyText>
<figureCaption confidence="0.971548">
Figure 8: Proportion of “pioneers” experts in a
given number of methods compared to all the other
authors in the corpus.
</figureCaption>
<sectionHeader confidence="0.998775" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999987625">
We have presented in this paper an analysis of the
ACL Anthology corpus. Our analysis is based on
the identification of keywords which are catego-
rized according to their informational status. Cate-
gorization is done according to a Text Zoning anal-
ysis of the papers’ abstracts, which provides very
relevant information for the study. We have shown
that coupling keyword extraction with Text Zon-
ing makes it possible to observe fine grained facts
in the dynamics of a scientific domain.
These tools only give pieces of information that
should be confirmed by subsequent studies. It
is necessary to go back to the texts themselves,
consult domain experts and probably the larger
context to be able to get a really accurate pic-
ture of the evolution of a scientific domain. This
multi-disciplinary research means that to collabo-
rate with people from other fields is needed, espe-
cially with the history of science and epistemol-
ogy. However, the platforms and the techniques
we have described in this paper are now available
and can be re-used for other kinds of studies, mak-
ing it possible to reproduce similar experiments
across different domains.
</bodyText>
<sectionHeader confidence="0.999479" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9989902">
Ashton Anderson, Dan Jurafsky, and Daniel A. McFar-
land. 2012. Towards a computational history of the
acl: 1980-2008. In Proceedings of the ACL-2012
Special Workshop on Rediscovering 50 Years of Dis-
coveries, pages 13–21, Jeju Island, Core. Associa-
tion for Computational Linguistics.
Didier Bourigault and Christian Jacquemin. 1999.
Term extraction + term clustering: An integrated
platform for computer-aided terminology. In Pro-
ceedings of the Ninth Conference on European
</reference>
<page confidence="0.974381">
78
</page>
<reference confidence="0.999751370786517">
Chapter of the Association for Computational Lin-
guistics, EACL ’99, pages 15–22.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Computational Linguistics, 16(2):79–85.
Michel Callon, John Law, and Arie Rip. 1986.
Mapping the dynamics of science and technology.
McMillan, London.
Michel Callon, Jean-Pierre Courtial, and Franc¸oise
Laville. 1991. Co-word analysis as a tool for de-
scribing the network of interaction between basic
and technological research: The case of polymer
chemistry. Scientometrics, 22(1):155–205.
Katarina Frantzi and Sophia Ananiadou. 2000. Au-
tomatic recognition of multi-word terms:. the C-
value/NC-value method. International Journal on
Digital Libraries, 3(2):115–130.
Eugene Garfield. 1972. Citation Analysis as a Tool in
Journal Evaluation. Science, 178(4060):471–479.
Michelle Girvan and Mark E J Newman. 2002. Com-
munity structure in social and biological networks.
Proceedings of the National Academy of Sciences of
the United States of America, 99:7821–7826.
Roger Guimera, Brian Uzzi, Jarrett Spiro, and Luis
A. Nunes Amaral. 2005. Team Assembly Mech-
anisms Determine Collaboration Network Structure
and Team Performance. Science, 308(5722):697–
702.
Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011. A weakly-supervised approach to argumenta-
tive zoning of scientific documents. In Proceedings
of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 273–283, Ed-
inburgh.
Yufan Guo, Roi Reichart, and Anna Korhonen. 2013.
Improved information structure analysis of scien-
tific documents through discourse and lexical con-
straints. In Proceedings of Human Language Tech-
nologies: Conference of the North American Chap-
ter of the Association of Computational Linguistics
(HLT-NAACL), pages 928–937.
James Curran and Stephen Clark and Johan Bos.
2007. Linguistically Motivated Large-Scale NLP
with C&amp;C and Boxer. In Proceedings of the 45th
Meeting of the Association for Computation Linguis-
tics (ACL), pages 33–36.
Gary Geunbae Lee, Jong-Hyeok Lee, and Jeong-
won Cha. 2002. Syllable-pattern-based unknown-
morpheme segmentation and estimation for hybrid
part-of-speech tagging of korean. Computational
Linguistics, 28(1):53–70.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Jour-
nal of Medical Informatics, 75(6):468–487.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2007. Numerical
Recipes 3rd Edition: The Art of Scientific Comput-
ing. Cambridge University Press, New York, NY,
USA, 3 edition.
Roi Reichart and Anna Korhonen. 2012. Docu-
ment and corpus level inference for unsupervised
and transductive learning of information structure of
scientific documents. In Proceedings of COLING
(Posters), pages 995–1006, Mumbai.
Anna Ritchie, Stephen Robertson, and Simone Teufel.
2008. Comparing citation contexts for information
retrieval. In Proeedings of the 17th Conference on
Information and Knowledge Management (CIKM),
pages 213–222, Napa Valley.
Henry G Small. 1973. Co-citation in the scientific lit-
erature: A new measure of the relationship between
two documents. Journal ofAmerican Society forIn-
formation Science, 24(4):265–269.
Imad Tbahriti, Christine Chichester, Fr´ed´erique
Lisacek, and Patrick Ruch. 2006. Using argumen-
tation to retrieve articles with similar citations: An
inquiry into improving related articles search in the
medline digital library. I. J. Medical Informatics,
75(6):488–495.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: Experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409–445.
Simone Teufel. 1999. Argumentative Zoning: Infor-
mation Extraction from Scientific Articles. Univer-
sity of Edinburgh.
</reference>
<page confidence="0.999049">
79
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.138207">
<title confidence="0.996711">Social and Semantic Socio-semantic Representation of a Scientific Corpus</title>
<author confidence="0.911559">Elisa</author>
<affiliation confidence="0.695861">LATTICE and CNRS &amp; ENS &amp; U. Sorbonne</affiliation>
<address confidence="0.9089735">1 rue Mauriece 92120 Montrouge</address>
<email confidence="0.994642">elisa.omodei@ens.fr</email>
<author confidence="0.9075">Jean-Philippe</author>
<affiliation confidence="0.983649">INRA Sens and</affiliation>
<address confidence="0.830600666666667">Cit´e Descartes, 5 boulevard 77454 Marne-la-Vall´ee Cedex 75013 Paris</address>
<email confidence="0.993961">jphcoi@yahoo.fr</email>
<author confidence="0.766794">Yufan</author>
<affiliation confidence="0.9929215">University of Computer</affiliation>
<address confidence="0.996905">Box 352350 Seattle, WA</address>
<email confidence="0.9997">yufanguo@cs.washington.edu</email>
<author confidence="0.887578">Thierry</author>
<affiliation confidence="0.733937">CNRS &amp; ENS &amp; U. Sorbonne</affiliation>
<address confidence="0.914631">1 rue Mauriece 92120 Montrouge</address>
<email confidence="0.994775">thierry.poibeau@ens.fr</email>
<abstract confidence="0.995725928571428">We propose a new method to extract keywords from texts and categorize these keywords according to their informational value, derived from the analysis of the argumentative goal of the sentences they appear in. The method is applied to the ACL Anthology corpus, containing papers on the computational linguistic domain published between 1980 and 2008. We show that our approach allows to highlight interesting facts concerning the evolution of the topics and methods used in computational linguistics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ashton Anderson</author>
<author>Dan Jurafsky</author>
<author>Daniel A McFarland</author>
</authors>
<title>Towards a computational history of the acl: 1980-2008.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries,</booktitle>
<pages>13--21</pages>
<institution>Jeju Island, Core. Association for Computational Linguistics.</institution>
<contexts>
<context position="4761" citStr="Anderson et al., 2012" startWordPosition="776" endWordPosition="779">ta is generally based on the extraction of key information (authors, keywords) and the discovery of their relationships. The data can be represented as a graph, therefore graph algorithmics can be used to study the topology and the evolution of the graph of collaborations or the graph of linked authors. It is thus possible to observe the evolution of the domain, check some hypotheses or common assumptions about this evolution and provide a strong empirical basis to epistemology studies. The paper “Towards a computational History of the ACL: 1980-2008” is very relevant from this point of view (Anderson et al., 2012). The authors try to determine the evolution of the main sub-domains of research within NLP since 1980 and they obtain very interesting results. For example, they show the influence of the American evaluation campaigns on the domain: when a US agency sponsored a sub-domain of NLP, one can observe a quick concentration effect since a wide number of research groups suddenly concentrated their efforts on the topic; when no evaluation campaign was organized, research was much more widespread across the different sub-domains of NLP. Even if this is partially predictable, it was not obvious to be ab</context>
<context position="15897" citStr="Anderson et al., 2012" startWordPosition="2633" endWordPosition="2637">a hybrid method is used is mentioned in a sentence that is globally tagged as OBJECTIVE by the system. However, sentences tagged as METHOD contain relevant keywords like lexical pattern or tri-gram estimation, which makes it possible to infer that the approach is hybrid. One can also spot some problems with digitization, which are typical of this corpus: the ACL Anthology contains automatically converted files to PDF, which means texts are not perfect and may contain some digitization errors. 3 Contribution to the Study of the Evolution ACL Anthology As said above, we are largely inspired by (Anderson et al., 2012). We think the ACL Anthology is typical since it contains papers spanning over more than 30 years: it is thus interesting to use it as a way to study the main evolutions of the computational linguistics domain. The method can of course also be applied to other scientific corpora. 3.1 Keyword extraction and characterization The first step consists in identifying the main keywords of the domain. We then want to more precisely categorize these keywords so as to identify the ones specifically referring to methods for example. From this perspective, keywords appearing in the METHOD sections are thu</context>
<context position="27533" citStr="Anderson et al., 2012" startWordPosition="4478" endWordPosition="4481">ioning frequently the name of the method in the abstracts), for different categories of researchers. challenging questions. How are new methods introduced in the field? Are they mainly brought by young researchers or is it mainly confirmed researchers who develop new techniques (or import them from related fields)? Are NLP experts specialized in one field or in a wide variety of different fields? These questions are of course quite complex. Each individual has his own expertise and his own history but we think that automatic methods can provide some interesting trends over time. For example, (Anderson et al., 2012) show that evaluation campaigns have played a central role at certain periods of time, which does not mean of course that there was no independent research outside these campaigns at the time. Our goal is thus to exhibit some tendencies that could be interpreted or even make it possible to compare the evolution of the computational linguistics field with other fields. Out tools provide some hypotheses that must of course be confirmed by further observations and analysis. We do not claim that they provide an exact and accurate view of the domain. For this study we only take into account authors</context>
</contexts>
<marker>Anderson, Jurafsky, McFarland, 2012</marker>
<rawString>Ashton Anderson, Dan Jurafsky, and Daniel A. McFarland. 2012. Towards a computational history of the acl: 1980-2008. In Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 13–21, Jeju Island, Core. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Didier Bourigault</author>
<author>Christian Jacquemin</author>
</authors>
<title>Term extraction + term clustering: An integrated platform for computer-aided terminology.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Conference on European Chapter of the Association for Computational Linguistics, EACL ’99,</booktitle>
<pages>15--22</pages>
<contexts>
<context position="17244" citStr="Bourigault and Jacquemin, 1999" startWordPosition="2856" endWordPosition="2859">and methods in NLP since most systems are made of different layers and require various NLP techniques. For example, a semantic analyzer may use a part-of-speech tagger and a parser, which means NLP tools can appear as part of the method. Keyword extraction aims at automatically extracting relevant keywords from a collection of texts. A popular approach consists in first extracting typical sequences of tags that are then filtered according to specific criteria (these criteria can include the use of external resources but they are more generally based on scores mixing frequency and specificity (Bourigault and Jacquemin, 1999; Frantzi and Ananiadou, 2000)). In this study, we voluntarily used a minimal approach for keyword extraction and filtering since we want to keep most 74 Table 2: Most specific keywords found in the METHOD sections. Methods Category Method N-grams Machine learning Bayesian methods baesyan Vector Space model space model, vector space, cosine Genetic algorithms genetic algorithms HMM hidden markov models, markov model CRF conditional random fields SVM support vector machines MaxEnt maximum entropy model, maximum entropy approach, maximum entropy Clustering clustering algorithm, clustering method</context>
</contexts>
<marker>Bourigault, Jacquemin, 1999</marker>
<rawString>Didier Bourigault and Christian Jacquemin. 1999. Term extraction + term clustering: An integrated platform for computer-aided terminology. In Proceedings of the Ninth Conference on European Chapter of the Association for Computational Linguistics, EACL ’99, pages 15–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Fredrick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="24822" citStr="Brown et al., 1990" startWordPosition="4002" endWordPosition="4005">g Grammars at the time) and the new popularity of this area of research in the early 2000s (dependency parsing has been the subject of several evaluation campaigns in the 2000s, see for example for the CONLL shared tasks from 2006 to 2009). Different machine learning methods have been popular over time but each of them continues to be used after a first wave corresponding to their initial success. Hidden Markov Models and n-grams are highly popular in the 1990s, probably thanks to the experiments made by Jelinek and his colleagues, which will open the field of statistical machine translation (Brown et al., 1990). SVM and CRF have had a more recent success as everybody knows. We are also interested in the distribution of these methods between papers and authors. Figure 4 shows the average number of keywords Results Year Figure 3: Evolution of the relative frequency of sentences tagged as RESULTS in the abstracts of the papers appearing in the METHOD section of the papers over time. We see that this number regularly increases, especially during the 1980s, showing possibly a gradually increasing complexity of the systems under consideration. Lastly, figure 5 shows the number of authors who are specialis</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Callon</author>
<author>John Law</author>
<author>Arie Rip</author>
</authors>
<title>Mapping the dynamics of science and technology.</title>
<date>1986</date>
<location>McMillan, London.</location>
<contexts>
<context position="2957" citStr="Callon et al., 1986" startWordPosition="470" endWordPosition="473">o as to study the topology of the domain (Girvan and Newman, 2002) or its morphogenesis (Guimera et al., 2005). Referencing has also been the subject of numerous studies on inter-citation (Garfield, 1972) and cocitation (Small, 1973). Other variables can be taken into account like the nationality of the authors, the projects they are involved in or the research institutions they belong to, but it is the analysis of the textual content (mostly titles, abstracts and keywords provided with the papers) that have attracted the most part of the research in the area since the seminal work of Callon (Callon et al., 1986; Callon et al., 1991). In this paper, our goal is to investigate the evolution of the field of computational linguistics, which means that text will play a crucial role. Textual analysis is then mixed with the study of individual trajectories in the semantic space: our goal is to propose possible avenues for the study of the dynamics of innovation in the computational lin71 Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 71–79, Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Lingu</context>
</contexts>
<marker>Callon, Law, Rip, 1986</marker>
<rawString>Michel Callon, John Law, and Arie Rip. 1986. Mapping the dynamics of science and technology. McMillan, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Callon</author>
<author>Jean-Pierre Courtial</author>
<author>Franc¸oise Laville</author>
</authors>
<title>Co-word analysis as a tool for describing the network of interaction between basic and technological research: The case of polymer chemistry.</title>
<date>1991</date>
<journal>Scientometrics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="2979" citStr="Callon et al., 1991" startWordPosition="474" endWordPosition="477">ology of the domain (Girvan and Newman, 2002) or its morphogenesis (Guimera et al., 2005). Referencing has also been the subject of numerous studies on inter-citation (Garfield, 1972) and cocitation (Small, 1973). Other variables can be taken into account like the nationality of the authors, the projects they are involved in or the research institutions they belong to, but it is the analysis of the textual content (mostly titles, abstracts and keywords provided with the papers) that have attracted the most part of the research in the area since the seminal work of Callon (Callon et al., 1986; Callon et al., 1991). In this paper, our goal is to investigate the evolution of the field of computational linguistics, which means that text will play a crucial role. Textual analysis is then mixed with the study of individual trajectories in the semantic space: our goal is to propose possible avenues for the study of the dynamics of innovation in the computational lin71 Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 71–79, Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics guistics domain</context>
</contexts>
<marker>Callon, Courtial, Laville, 1991</marker>
<rawString>Michel Callon, Jean-Pierre Courtial, and Franc¸oise Laville. 1991. Co-word analysis as a tool for describing the network of interaction between basic and technological research: The case of polymer chemistry. Scientometrics, 22(1):155–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katarina Frantzi</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Automatic recognition of multi-word terms:.</title>
<date>2000</date>
<booktitle>the Cvalue/NC-value method. International Journal on Digital Libraries,</booktitle>
<pages>3--2</pages>
<contexts>
<context position="17274" citStr="Frantzi and Ananiadou, 2000" startWordPosition="2860" endWordPosition="2863">stems are made of different layers and require various NLP techniques. For example, a semantic analyzer may use a part-of-speech tagger and a parser, which means NLP tools can appear as part of the method. Keyword extraction aims at automatically extracting relevant keywords from a collection of texts. A popular approach consists in first extracting typical sequences of tags that are then filtered according to specific criteria (these criteria can include the use of external resources but they are more generally based on scores mixing frequency and specificity (Bourigault and Jacquemin, 1999; Frantzi and Ananiadou, 2000)). In this study, we voluntarily used a minimal approach for keyword extraction and filtering since we want to keep most 74 Table 2: Most specific keywords found in the METHOD sections. Methods Category Method N-grams Machine learning Bayesian methods baesyan Vector Space model space model, vector space, cosine Genetic algorithms genetic algorithms HMM hidden markov models, markov model CRF conditional random fields SVM support vector machines MaxEnt maximum entropy model, maximum entropy approach, maximum entropy Clustering clustering algorithm, clustering method, word clusters, classificatio</context>
</contexts>
<marker>Frantzi, Ananiadou, 2000</marker>
<rawString>Katarina Frantzi and Sophia Ananiadou. 2000. Automatic recognition of multi-word terms:. the Cvalue/NC-value method. International Journal on Digital Libraries, 3(2):115–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Garfield</author>
</authors>
<title>Citation Analysis as a Tool in</title>
<date>1972</date>
<journal>Journal Evaluation. Science,</journal>
<volume>178</volume>
<issue>4060</issue>
<contexts>
<context position="2542" citStr="Garfield, 1972" startWordPosition="398" endWordPosition="399">n the number of papers available depending on the given period of time. There are similar archives for different domains like, e.g. physics (the APS database provided by the American Physical Society) or the bio-medical domain (with Medline). These scientific archives have already given birth to a large number of different pieces of work. Collaboration networks have for example been automatically extracted so as to study the topology of the domain (Girvan and Newman, 2002) or its morphogenesis (Guimera et al., 2005). Referencing has also been the subject of numerous studies on inter-citation (Garfield, 1972) and cocitation (Small, 1973). Other variables can be taken into account like the nationality of the authors, the projects they are involved in or the research institutions they belong to, but it is the analysis of the textual content (mostly titles, abstracts and keywords provided with the papers) that have attracted the most part of the research in the area since the seminal work of Callon (Callon et al., 1986; Callon et al., 1991). In this paper, our goal is to investigate the evolution of the field of computational linguistics, which means that text will play a crucial role. Textual analys</context>
</contexts>
<marker>Garfield, 1972</marker>
<rawString>Eugene Garfield. 1972. Citation Analysis as a Tool in Journal Evaluation. Science, 178(4060):471–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle Girvan</author>
<author>Mark E J Newman</author>
</authors>
<title>Community structure in social and biological networks.</title>
<date>2002</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America,</booktitle>
<pages>99--7821</pages>
<contexts>
<context position="2404" citStr="Girvan and Newman, 2002" startWordPosition="375" endWordPosition="378">n large volumes so that they can be exploited in evolution studies. The volume of data increases over time, which means there is a wide diversity in the number of papers available depending on the given period of time. There are similar archives for different domains like, e.g. physics (the APS database provided by the American Physical Society) or the bio-medical domain (with Medline). These scientific archives have already given birth to a large number of different pieces of work. Collaboration networks have for example been automatically extracted so as to study the topology of the domain (Girvan and Newman, 2002) or its morphogenesis (Guimera et al., 2005). Referencing has also been the subject of numerous studies on inter-citation (Garfield, 1972) and cocitation (Small, 1973). Other variables can be taken into account like the nationality of the authors, the projects they are involved in or the research institutions they belong to, but it is the analysis of the textual content (mostly titles, abstracts and keywords provided with the papers) that have attracted the most part of the research in the area since the seminal work of Callon (Callon et al., 1986; Callon et al., 1991). In this paper, our goal</context>
</contexts>
<marker>Girvan, Newman, 2002</marker>
<rawString>Michelle Girvan and Mark E J Newman. 2002. Community structure in social and biological networks. Proceedings of the National Academy of Sciences of the United States of America, 99:7821–7826.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Guimera</author>
<author>Brian Uzzi</author>
<author>Jarrett Spiro</author>
<author>Luis A Nunes Amaral</author>
</authors>
<title>Team Assembly Mechanisms Determine Collaboration Network Structure and Team Performance.</title>
<date>2005</date>
<journal>Science,</journal>
<volume>308</volume>
<issue>5722</issue>
<pages>702</pages>
<contexts>
<context position="2448" citStr="Guimera et al., 2005" startWordPosition="382" endWordPosition="385">n evolution studies. The volume of data increases over time, which means there is a wide diversity in the number of papers available depending on the given period of time. There are similar archives for different domains like, e.g. physics (the APS database provided by the American Physical Society) or the bio-medical domain (with Medline). These scientific archives have already given birth to a large number of different pieces of work. Collaboration networks have for example been automatically extracted so as to study the topology of the domain (Girvan and Newman, 2002) or its morphogenesis (Guimera et al., 2005). Referencing has also been the subject of numerous studies on inter-citation (Garfield, 1972) and cocitation (Small, 1973). Other variables can be taken into account like the nationality of the authors, the projects they are involved in or the research institutions they belong to, but it is the analysis of the textual content (mostly titles, abstracts and keywords provided with the papers) that have attracted the most part of the research in the area since the seminal work of Callon (Callon et al., 1986; Callon et al., 1991). In this paper, our goal is to investigate the evolution of the fiel</context>
</contexts>
<marker>Guimera, Uzzi, Spiro, Amaral, 2005</marker>
<rawString>Roger Guimera, Brian Uzzi, Jarrett Spiro, and Luis A. Nunes Amaral. 2005. Team Assembly Mechanisms Determine Collaboration Network Structure and Team Performance. Science, 308(5722):697– 702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yufan Guo</author>
<author>Anna Korhonen</author>
<author>Thierry Poibeau</author>
</authors>
<title>A weakly-supervised approach to argumentative zoning of scientific documents.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>273--283</pages>
<location>Edinburgh.</location>
<contexts>
<context position="8978" citStr="Guo et al., 2011" startWordPosition="1474" endWordPosition="1477">lassification, on the evaluation of different learning algorithms for the task and more importantly on the reduction of the volume of text to be annotated. Concerning the second point, it is mostly the biological and bio-medical domains that have attracted attention, since scientists in these domains often have to access the literature “vertically” (i.e. experts may need to have access to all the methods and protocols that have been used in a specific domain) (Mizuta et al., 2006; Tbahriti et al., 2006). Guo has since developed a similar trend of research to extend the initial work of Teufel (Guo et al., 2011; Guo et al., 2013): she has tested a large list of features to analyze the zones, evaluated different learning algorithms for the task and proposed new methods to decrease the number of texts to be annotated. The features used for learning are of three categories: i) positional (location of the sentence inside the paper), ii) lexical (words, classes of words, bigrams, etc. are taken into consideration) and iii) syntactic (the different syntactic relations as well as the class of words appearing in subject or object positions are taken into account). The analysis is thus based on more features</context>
<context position="10346" citStr="Guo et al., 2011" startWordPosition="1704" endWordPosition="1707">apers. Our hypothesis is that abstracts contain enough information and are redundant enough to study the evolution of the domain. Taking into consideration the full text would probably give too many details and thus introduce noise in the analysis. The annotation scheme includes five different categories, which are the following: OBJECTIVE (objectives of the paper), METHOD (methods used in the paper), RESULTS (main results), CONCLUSION (conclusion of the paper), BACKGROUND (general context), as in (Reichart and Korhonen, 2012). These categories are also close to those of (Mizuta et al., 2006; Guo et al., 2011; Guo et al., 2013) and have been adapted to abstracts (as opposed to full text1). It seems relevant to take into consideration an annotation scheme that has already been used by various authors so that the results are easy to compare to others. Around one hundred abstracts from the ACL Anthology have then been manually annotated using this scheme (-500 sentences; ACL abstracts are generally quite short since most of them are related to conference papers). The selection of the abstracts has been done using stratified sampling over time and journals, so as to obtain a representative corpus (pap</context>
<context position="11660" citStr="Guo et al., 2011" startWordPosition="1926" endWordPosition="1929">tation has been done according to the annotation guideline defined by Y. Guo, especially for long sentences when more than one category could be applied (preferences are defined to solve complex cases2). The algorithm defined by (Guo et al., 2011) is then adapted to our corpus. The analysis is based on positional, lexical and syntactic features, as explained above. No domain specific information was added, which makes the whole process easy to reproduce. As for parsing, we used the C&amp;C parser (James Curran and Stephen Clark and Johan Bos, 2007). All the implementation details can be found in (Guo et al., 2011), especially concerning annotation and the learning algorithm. As a result, each sentence is associated with a tag corresponding to one of the zones defined in the annotation scheme. 2.3 Results and Discussion In order to evaluate the text zoning task, a number of abstracts were chosen randomly (-300 sentences that do not overlap with the training set). CONCLUSION represented less than 3% of the sentences and was then dropped for the rest of the analysis. The four remaining zones are unequaly represented: 18.05 % of the sentences refer to BACKGROUND, 14.35% to OBJECTIVE, 14.81 % to RESULT and </context>
<context position="14465" citStr="Guo et al., 2011" startWordPosition="2393" endWordPosition="2396">the POS’s of unknown morphemes regardless of their numbers and positions in an eojeol , which was not possible before in Korean tagging systems. RESULTS In a series of experiments using three different domain corpora , we can achieve 97% tagging accuracy regardless of many unknown morphemes in test corpora . RESULTS methodological issues are important for the domain. We then calculate for each of the categories, the percentage of sentences that received the right label, which allows us to calculate precision. The results are given in table 1. These results are similar to the state of the art (Guo et al., 2011), which is positive taking into consideration the small number of sentences annotated for training. The diversity of the features used makes it easy to transfer the technique from one domain to the other without any heavy annotation phase. Results are slightly worse for the METHOD category, probably because this category is more diverse and thus more difficult to recognize. The fact that NLP terms can refer either to objectives or to methods also contributes rendering the recognition of this category more difficult. Figure 1 shows an abstract annotated by the text zoning module (the paper is (</context>
</contexts>
<marker>Guo, Korhonen, Poibeau, 2011</marker>
<rawString>Yufan Guo, Anna Korhonen, and Thierry Poibeau. 2011. A weakly-supervised approach to argumentative zoning of scientific documents. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 273–283, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yufan Guo</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Improved information structure analysis of scientific documents through discourse and lexical constraints.</title>
<date>2013</date>
<booktitle>In Proceedings of Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL),</booktitle>
<pages>928--937</pages>
<contexts>
<context position="8997" citStr="Guo et al., 2013" startWordPosition="1478" endWordPosition="1481">the evaluation of different learning algorithms for the task and more importantly on the reduction of the volume of text to be annotated. Concerning the second point, it is mostly the biological and bio-medical domains that have attracted attention, since scientists in these domains often have to access the literature “vertically” (i.e. experts may need to have access to all the methods and protocols that have been used in a specific domain) (Mizuta et al., 2006; Tbahriti et al., 2006). Guo has since developed a similar trend of research to extend the initial work of Teufel (Guo et al., 2011; Guo et al., 2013): she has tested a large list of features to analyze the zones, evaluated different learning algorithms for the task and proposed new methods to decrease the number of texts to be annotated. The features used for learning are of three categories: i) positional (location of the sentence inside the paper), ii) lexical (words, classes of words, bigrams, etc. are taken into consideration) and iii) syntactic (the different syntactic relations as well as the class of words appearing in subject or object positions are taken into account). The analysis is thus based on more features than in Teufel’s i</context>
<context position="10365" citStr="Guo et al., 2013" startWordPosition="1708" endWordPosition="1711">sis is that abstracts contain enough information and are redundant enough to study the evolution of the domain. Taking into consideration the full text would probably give too many details and thus introduce noise in the analysis. The annotation scheme includes five different categories, which are the following: OBJECTIVE (objectives of the paper), METHOD (methods used in the paper), RESULTS (main results), CONCLUSION (conclusion of the paper), BACKGROUND (general context), as in (Reichart and Korhonen, 2012). These categories are also close to those of (Mizuta et al., 2006; Guo et al., 2011; Guo et al., 2013) and have been adapted to abstracts (as opposed to full text1). It seems relevant to take into consideration an annotation scheme that has already been used by various authors so that the results are easy to compare to others. Around one hundred abstracts from the ACL Anthology have then been manually annotated using this scheme (-500 sentences; ACL abstracts are generally quite short since most of them are related to conference papers). The selection of the abstracts has been done using stratified sampling over time and journals, so as to obtain a representative corpus (papers must be related</context>
</contexts>
<marker>Guo, Reichart, Korhonen, 2013</marker>
<rawString>Yufan Guo, Roi Reichart, and Anna Korhonen. 2013. Improved information structure analysis of scientific documents through discourse and lexical constraints. In Proceedings of Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL), pages 928–937.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
<author>Stephen Clark</author>
<author>Johan Bos</author>
</authors>
<title>Linguistically Motivated Large-Scale NLP with C&amp;C and Boxer.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Meeting of the Association for Computation Linguistics (ACL),</booktitle>
<pages>33--36</pages>
<marker>Curran, Clark, Bos, 2007</marker>
<rawString>James Curran and Stephen Clark and Johan Bos. 2007. Linguistically Motivated Large-Scale NLP with C&amp;C and Boxer. In Proceedings of the 45th Meeting of the Association for Computation Linguistics (ACL), pages 33–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary Geunbae Lee</author>
<author>Jong-Hyeok Lee</author>
<author>Jeongwon Cha</author>
</authors>
<title>Syllable-pattern-based unknownmorpheme segmentation and estimation for hybrid part-of-speech tagging of korean.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="15082" citStr="Lee et al., 2002" startWordPosition="2498" endWordPosition="2501">, which is positive taking into consideration the small number of sentences annotated for training. The diversity of the features used makes it easy to transfer the technique from one domain to the other without any heavy annotation phase. Results are slightly worse for the METHOD category, probably because this category is more diverse and thus more difficult to recognize. The fact that NLP terms can refer either to objectives or to methods also contributes rendering the recognition of this category more difficult. Figure 1 shows an abstract annotated by the text zoning module (the paper is (Lee et al., 2002): it has been chosen randomly between those containing the different types of zones). One category is associated with each sentence but this is sometimes problematic: for example the fact that a hybrid method is used is mentioned in a sentence that is globally tagged as OBJECTIVE by the system. However, sentences tagged as METHOD contain relevant keywords like lexical pattern or tri-gram estimation, which makes it possible to infer that the approach is hybrid. One can also spot some problems with digitization, which are typical of this corpus: the ACL Anthology contains automatically converted</context>
</contexts>
<marker>Lee, Lee, Cha, 2002</marker>
<rawString>Gary Geunbae Lee, Jong-Hyeok Lee, and Jeongwon Cha. 2002. Syllable-pattern-based unknownmorpheme segmentation and estimation for hybrid part-of-speech tagging of korean. Computational Linguistics, 28(1):53–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoko Mizuta</author>
<author>Anna Korhonen</author>
<author>Tony Mullen</author>
<author>Nigel Collier</author>
</authors>
<title>Zone analysis in biology articles as a basis for information extraction.</title>
<date>2006</date>
<journal>International Journal of Medical Informatics,</journal>
<volume>75</volume>
<issue>6</issue>
<contexts>
<context position="8846" citStr="Mizuta et al., 2006" startWordPosition="1449" endWordPosition="1452">lity to different scientific domains. Concerning the first point, research has focused on the identification of relevant features for classification, on the evaluation of different learning algorithms for the task and more importantly on the reduction of the volume of text to be annotated. Concerning the second point, it is mostly the biological and bio-medical domains that have attracted attention, since scientists in these domains often have to access the literature “vertically” (i.e. experts may need to have access to all the methods and protocols that have been used in a specific domain) (Mizuta et al., 2006; Tbahriti et al., 2006). Guo has since developed a similar trend of research to extend the initial work of Teufel (Guo et al., 2011; Guo et al., 2013): she has tested a large list of features to analyze the zones, evaluated different learning algorithms for the task and proposed new methods to decrease the number of texts to be annotated. The features used for learning are of three categories: i) positional (location of the sentence inside the paper), ii) lexical (words, classes of words, bigrams, etc. are taken into consideration) and iii) syntactic (the different syntactic relations as well</context>
<context position="10328" citStr="Mizuta et al., 2006" startWordPosition="1700" endWordPosition="1703">he abstracts of the papers. Our hypothesis is that abstracts contain enough information and are redundant enough to study the evolution of the domain. Taking into consideration the full text would probably give too many details and thus introduce noise in the analysis. The annotation scheme includes five different categories, which are the following: OBJECTIVE (objectives of the paper), METHOD (methods used in the paper), RESULTS (main results), CONCLUSION (conclusion of the paper), BACKGROUND (general context), as in (Reichart and Korhonen, 2012). These categories are also close to those of (Mizuta et al., 2006; Guo et al., 2011; Guo et al., 2013) and have been adapted to abstracts (as opposed to full text1). It seems relevant to take into consideration an annotation scheme that has already been used by various authors so that the results are easy to compare to others. Around one hundred abstracts from the ACL Anthology have then been manually annotated using this scheme (-500 sentences; ACL abstracts are generally quite short since most of them are related to conference papers). The selection of the abstracts has been done using stratified sampling over time and journals, so as to obtain a represen</context>
</contexts>
<marker>Mizuta, Korhonen, Mullen, Collier, 2006</marker>
<rawString>Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel Collier. 2006. Zone analysis in biology articles as a basis for information extraction. International Journal of Medical Informatics, 75(6):468–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
<author>Brian P Flannery</author>
</authors>
<title>Numerical Recipes 3rd Edition: The Art of Scientific Computing.</title>
<date>2007</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA, 3 edition.</location>
<contexts>
<context position="21224" citStr="Press et al., 2007" startWordPosition="3374" endWordPosition="3377">uent processing. Candidate keywords are then ranked per zone, according to their specificity (the zone they are the most specific of) . Specificity corresponds to the Kolmogorov-Smirnov test that quantifies a distance between the empirical distribution functions of two samples. The test is calculated as follows: |SN1(x) − SN2(x) |(1) where SN1(x) et SN2(x) are the empirical distribution function of the two samples (that correspond in our case to the number of occurrences of the keyword in a given zone, and to the total number of occurrences of all the keywords in the same zone, respectively) (Press et al., 2007). A high value of D for a given keyword means that it is highly specific of the considered zone. At the opposite, a low value means that the keyword is spread over the different zones and not really specific of any zone. The first keywords of each category are then categorized by an expert of the domain. For the METHOD category, we obtain Table 2. Logically, given our approach, the table does not contain all the keywords relevant for the computational linguistics domain, but it contains the mots specific ones according to the above approach. One should thus not be surprised not to see all the </context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 2007</marker>
<rawString>William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 2007. Numerical Recipes 3rd Edition: The Art of Scientific Computing. Cambridge University Press, New York, NY, USA, 3 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Document and corpus level inference for unsupervised and transductive learning of information structure of scientific documents.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING (Posters),</booktitle>
<pages>995--1006</pages>
<location>Mumbai.</location>
<contexts>
<context position="10262" citStr="Reichart and Korhonen, 2012" startWordPosition="1688" endWordPosition="1691">2 Application to the ACL Anthology corpus In our experiment, we only used the abstracts of the papers. Our hypothesis is that abstracts contain enough information and are redundant enough to study the evolution of the domain. Taking into consideration the full text would probably give too many details and thus introduce noise in the analysis. The annotation scheme includes five different categories, which are the following: OBJECTIVE (objectives of the paper), METHOD (methods used in the paper), RESULTS (main results), CONCLUSION (conclusion of the paper), BACKGROUND (general context), as in (Reichart and Korhonen, 2012). These categories are also close to those of (Mizuta et al., 2006; Guo et al., 2011; Guo et al., 2013) and have been adapted to abstracts (as opposed to full text1). It seems relevant to take into consideration an annotation scheme that has already been used by various authors so that the results are easy to compare to others. Around one hundred abstracts from the ACL Anthology have then been manually annotated using this scheme (-500 sentences; ACL abstracts are generally quite short since most of them are related to conference papers). The selection of the abstracts has been done using stra</context>
</contexts>
<marker>Reichart, Korhonen, 2012</marker>
<rawString>Roi Reichart and Anna Korhonen. 2012. Document and corpus level inference for unsupervised and transductive learning of information structure of scientific documents. In Proceedings of COLING (Posters), pages 995–1006, Mumbai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Ritchie</author>
<author>Stephen Robertson</author>
<author>Simone Teufel</author>
</authors>
<title>Comparing citation contexts for information retrieval.</title>
<date>2008</date>
<booktitle>In Proeedings of the 17th Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>213--222</pages>
<location>Napa Valley.</location>
<contexts>
<context position="8047" citStr="Ritchie et al., 2008" startWordPosition="1316" endWordPosition="1319">apers published within the ACL conferences or Computational Linguistics). A classifier was then trained on this manually annotated corpus. The author reported interesting results despite “a 20% diference between [the] system and human performance” (Teufel and Moens, 2002). The learning method used a Naive Bayesian model since more sophisticated methods tested by the author did not obtain better results. Teufel in subsequent publications showed that the technique can be used to produce high quality summaries (Teufel and Moens, 2002) or precisely characterize the different citations in a paper (Ritchie et al., 2008). The seminal work of Teufel has since then given 72 rise to different kinds of works, on the one hand to refine the annotation method, and on the other hand to check its applicability to different scientific domains. Concerning the first point, research has focused on the identification of relevant features for classification, on the evaluation of different learning algorithms for the task and more importantly on the reduction of the volume of text to be annotated. Concerning the second point, it is mostly the biological and bio-medical domains that have attracted attention, since scientists </context>
</contexts>
<marker>Ritchie, Robertson, Teufel, 2008</marker>
<rawString>Anna Ritchie, Stephen Robertson, and Simone Teufel. 2008. Comparing citation contexts for information retrieval. In Proeedings of the 17th Conference on Information and Knowledge Management (CIKM), pages 213–222, Napa Valley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry G Small</author>
</authors>
<title>Co-citation in the scientific literature: A new measure of the relationship between two documents.</title>
<date>1973</date>
<journal>Journal ofAmerican Society forInformation Science,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="2571" citStr="Small, 1973" startWordPosition="403" endWordPosition="404"> depending on the given period of time. There are similar archives for different domains like, e.g. physics (the APS database provided by the American Physical Society) or the bio-medical domain (with Medline). These scientific archives have already given birth to a large number of different pieces of work. Collaboration networks have for example been automatically extracted so as to study the topology of the domain (Girvan and Newman, 2002) or its morphogenesis (Guimera et al., 2005). Referencing has also been the subject of numerous studies on inter-citation (Garfield, 1972) and cocitation (Small, 1973). Other variables can be taken into account like the nationality of the authors, the projects they are involved in or the research institutions they belong to, but it is the analysis of the textual content (mostly titles, abstracts and keywords provided with the papers) that have attracted the most part of the research in the area since the seminal work of Callon (Callon et al., 1986; Callon et al., 1991). In this paper, our goal is to investigate the evolution of the field of computational linguistics, which means that text will play a crucial role. Textual analysis is then mixed with the stu</context>
</contexts>
<marker>Small, 1973</marker>
<rawString>Henry G Small. 1973. Co-citation in the scientific literature: A new measure of the relationship between two documents. Journal ofAmerican Society forInformation Science, 24(4):265–269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imad Tbahriti</author>
<author>Christine Chichester</author>
<author>Fr´ed´erique Lisacek</author>
<author>Patrick Ruch</author>
</authors>
<title>Using argumentation to retrieve articles with similar citations: An inquiry into improving related articles search in the medline digital library.</title>
<date>2006</date>
<journal>I. J. Medical Informatics,</journal>
<volume>75</volume>
<issue>6</issue>
<contexts>
<context position="8870" citStr="Tbahriti et al., 2006" startWordPosition="1453" endWordPosition="1456">entific domains. Concerning the first point, research has focused on the identification of relevant features for classification, on the evaluation of different learning algorithms for the task and more importantly on the reduction of the volume of text to be annotated. Concerning the second point, it is mostly the biological and bio-medical domains that have attracted attention, since scientists in these domains often have to access the literature “vertically” (i.e. experts may need to have access to all the methods and protocols that have been used in a specific domain) (Mizuta et al., 2006; Tbahriti et al., 2006). Guo has since developed a similar trend of research to extend the initial work of Teufel (Guo et al., 2011; Guo et al., 2013): she has tested a large list of features to analyze the zones, evaluated different learning algorithms for the task and proposed new methods to decrease the number of texts to be annotated. The features used for learning are of three categories: i) positional (location of the sentence inside the paper), ii) lexical (words, classes of words, bigrams, etc. are taken into consideration) and iii) syntactic (the different syntactic relations as well as the class of words a</context>
</contexts>
<marker>Tbahriti, Chichester, Lisacek, Ruch, 2006</marker>
<rawString>Imad Tbahriti, Christine Chichester, Fr´ed´erique Lisacek, and Patrick Ruch. 2006. Using argumentation to retrieve articles with similar citations: An inquiry into improving related articles search in the medline digital library. I. J. Medical Informatics, 75(6):488–495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Summarizing scientific articles: Experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="7698" citStr="Teufel and Moens, 2002" startWordPosition="1259" endWordPosition="1262">ntrastive or comparative statements about other work. This task is called Rhetorical zoning or Argumentative zoning since the goal is to identify the rhetoric or argumentative role of each sentence of the text. The initial work of Teufel was based on the manual annotation of 80 papers representing the different areas of NLP (the corpus was made of papers published within the ACL conferences or Computational Linguistics). A classifier was then trained on this manually annotated corpus. The author reported interesting results despite “a 20% diference between [the] system and human performance” (Teufel and Moens, 2002). The learning method used a Naive Bayesian model since more sophisticated methods tested by the author did not obtain better results. Teufel in subsequent publications showed that the technique can be used to produce high quality summaries (Teufel and Moens, 2002) or precisely characterize the different citations in a paper (Ritchie et al., 2008). The seminal work of Teufel has since then given 72 rise to different kinds of works, on the one hand to refine the annotation method, and on the other hand to check its applicability to different scientific domains. Concerning the first point, resea</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>Simone Teufel and Marc Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical status. Computational Linguistics, 28(4):409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
</authors>
<title>Argumentative Zoning: Information Extraction from Scientific Articles.</title>
<date>1999</date>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="6964" citStr="Teufel, 1999" startWordPosition="1144" endWordPosition="1145">racterize these keywords and make a difference, for example, between keywords referring to concepts and keywords referring to methods. Hence, the context of these keywords seems highly important. Consequently, we propose to use Text Zoning that can provide an accurate characterization of the argumentative goal of each sentence in a scientific abstract. 2.1 Previous work The first important contributions in text zoning are probably the experiments by S. Teufel who proposed to categorize sentences in scientific papers (and more specifically, in the NLP domain) according to different categories (Teufel, 1999) like BKG: General scientific background, AIM: Statements of the particular aim of the current paper or CTR: Contrastive or comparative statements about other work. This task is called Rhetorical zoning or Argumentative zoning since the goal is to identify the rhetoric or argumentative role of each sentence of the text. The initial work of Teufel was based on the manual annotation of 80 papers representing the different areas of NLP (the corpus was made of papers published within the ACL conferences or Computational Linguistics). A classifier was then trained on this manually annotated corpus.</context>
<context position="12367" citStr="Teufel, 1999" startWordPosition="2051" endWordPosition="2052">ssociated with a tag corresponding to one of the zones defined in the annotation scheme. 2.3 Results and Discussion In order to evaluate the text zoning task, a number of abstracts were chosen randomly (-300 sentences that do not overlap with the training set). CONCLUSION represented less than 3% of the sentences and was then dropped for the rest of the analysis. The four remaining zones are unequaly represented: 18.05 % of the sentences refer to BACKGROUND, 14.35% to OBJECTIVE, 14.81 % to RESULT and 52.77 % to METHOD. Just by looking at these numbers, one can see how 1The categories used in (Teufel, 1999) were not relevant since this model focused on full text papers, with a special emphasis on the novelty of the author’s work and the attitude towards other people’s work, which is not the case here. 2The task is to assign the sentence only a single category. The choice of the category should be made according to the following priority list: Conclusion &gt; Objective &gt; Result &gt; Method &gt; Background. The only exception is that when 75% or more of the sentence belongs to a less preferred category, then that category will be assigned to the sentence. 73 Table 1: Result of the text zoning analysis (pre</context>
</contexts>
<marker>Teufel, 1999</marker>
<rawString>Simone Teufel. 1999. Argumentative Zoning: Information Extraction from Scientific Articles. University of Edinburgh.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
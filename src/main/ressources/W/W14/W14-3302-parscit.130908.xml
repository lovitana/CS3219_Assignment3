<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.810931">
Findings of the 2014 Workshop on Statistical Machine Translation
</title>
<author confidence="0.675181">
Ondˇrej Bojar Christian Buck Christian Federmann Barry Haddow
</author>
<affiliation confidence="0.634675">
Charles University in Prague University of Edinburgh Microsoft Research University of Edinburgh
</affiliation>
<note confidence="0.867446">
Philipp Koehn
JHU / Edinburgh
</note>
<author confidence="0.935479">
Pavel Pecina
</author>
<affiliation confidence="0.91309">
Charles University in Prague
</affiliation>
<author confidence="0.61756">
Radu Soricut
</author>
<affiliation confidence="0.502635">
Google
</affiliation>
<author confidence="0.928792">
Johannes Leveling
</author>
<affiliation confidence="0.944817">
Dublin City University
</affiliation>
<author confidence="0.956787">
Matt Post
</author>
<affiliation confidence="0.939577">
Johns Hopkins University
</affiliation>
<author confidence="0.985672">
Lucia Specia
</author>
<affiliation confidence="0.994249">
University of Sheffield
</affiliation>
<author confidence="0.932521">
Christof Monz
</author>
<affiliation confidence="0.956629">
University of Amsterdam
</affiliation>
<author confidence="0.978734">
Herve Saint-Amand
</author>
<affiliation confidence="0.994041">
University of Edinburgh
</affiliation>
<author confidence="0.79698">
Aleˇs Tamchyna
</author>
<affiliation confidence="0.900882">
Charles University in Prague
</affiliation>
<sectionHeader confidence="0.968867" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998476">
This paper presents the results of the
WMT14 shared tasks, which included a
standard news translation task, a sepa-
rate medical translation task, a task for
run-time estimation of machine translation
quality, and a metrics task. This year, 143
machine translation systems from 23 insti-
tutions were submitted to the ten transla-
tion directions in the standard translation
task. An additional 6 anonymized sys-
tems were included, and were then evalu-
ated both automatically and manually. The
quality estimation task had four subtasks,
with a total of 10 teams, submitting 57 en-
tries.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999845105263158">
We present the results of the shared tasks of
the Workshop on Statistical Machine Translation
(WMT) held at ACL 2014. This workshop builds
on eight previous WMT workshops (Koehn and
Monz, 2006; Callison-Burch et al., 2007, 2008,
2009, 2010, 2011, 2012; Bojar et al., 2013).
This year we conducted four official tasks: a
translation task, a quality estimation task, a met-
rics task1 and a medical translation task. In the
translation task (§2), participants were asked to
translate a shared test set, optionally restricting
themselves to the provided training data. We held
ten translation tasks this year, between English and
each of Czech, French, German, Hindi, and Rus-
sian. The Hindi translation tasks were new this
year, providing a lesser resourced data condition
on a challenging language pair. The system out-
puts for each task were evaluated both automati-
cally and manually.
</bodyText>
<footnote confidence="0.8082935">
1The metrics task is reported in a separate paper
(Mach´aˇcek and Bojar, 2014).
</footnote>
<bodyText confidence="0.999951">
The human evaluation (§3) involves asking
human judges to rank sentences output by
anonymized systems. We obtained large num-
bers of rankings from researchers who contributed
evaluations proportional to the number of tasks
they entered. Last year, we dramatically increased
the number of judgments, achieving much more
meaningful rankings. This year, we developed a
new ranking method that allows us to achieve the
same with fewer judgments.
The quality estimation task (§4) this year
included sentence- and word-level subtasks:
sentence-level prediction of 1-3 likert scores,
sentence-level prediction of percentage of word
edits necessary to fix a sentence, sentence-level
prediction of post-editing time, and word-level
prediction of scores at different levels of granular-
ity (correct/incorrect, accuracy/fluency errors, and
specific types of errors). Datasets were released
with English-Spanish, English-German, Spanish-
English and German-English news translations
produced by 2-3 machine translation systems and,
for some subtasks, a human translation.
The medical translation task (§5) was intro-
duced this year. Unlike the “standard” translation
task, the test sets come from the very specialized
domain of medical texts. The aim of this task was
not only domain adaptation but also the utilization
of translation systems in a larger scenario, namely
cross-lingual information retrieval (IR). Extrinsic
evaluation in an IR setting was a part of this task
(on the other hand, manual evaluation of transla-
tion quality was not carried out).
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dis-
seminate common test sets and public training data
with published performance numbers, and to re-
fine evaluation and estimation methodologies for
machine translation. As before, all of the data,
</bodyText>
<page confidence="0.984761">
12
</page>
<note confidence="0.714303">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12–58,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9802594">
translations, and collected human judgments are
publicly available.2 We hope these datasets serve
as a valuable resource for research into statistical
machine translation and automatic evaluation or
prediction of translation quality.
2 Overview of the Translation Task
The recurring task of the workshop examines
translation between English and other languages.
As in the previous years, the other languages in-
clude German, French, Czech and Russian.
We dropped Spanish and added Hindi this year.
From a linguistic point of view, Spanish poses
similar problems as French, making its prior in-
clusion less valuable. Hindi is not only interest-
ing since it is a more distant language than the
European languages we include, but also because
we have much less training data, thus forcing re-
searchers to deal with low resource conditions, but
also providing them with a language pair that does
not suffer from the computational complexities of
having to deal with massive amounts of training
data.
We created a test set for each language pair by
translating newspaper articles and provided train-
ing data.
</bodyText>
<subsectionHeader confidence="0.999138">
2.1 Test data
</subsectionHeader>
<bodyText confidence="0.973381545454545">
The test data for this year’s task was selected from
news stories from online sources, as before. How-
ever, we changed our method to create the test sets.
In previous years, we took equal amounts of
source sentences from all six languages involved
(around 500 sentences each), and translated them
into all other languages. While this produced a
multi-parallel test corpus that could be also used
for language pairs (such as Czech-Russian) that
we did not include in the evaluation, it did suf-
fer from artifacts from the larger distance between
source and target sentences. Most test sentences
involved the translation a source sentence that
was translated from a their language into a tar-
get sentence (which was compared against a trans-
lation from that third language as well). Ques-
tions have been raised, if the evaluation of, say,
French-English translation is best served when
testing on sentences that have been originally writ-
ten in, say, Czech. For discussions about trans-
lationese please for instance refer to Koppel and
Ordan (2011).
</bodyText>
<footnote confidence="0.727556">
2http://statmt.org/wmt14/results.html
</footnote>
<bodyText confidence="0.999657266666667">
This year, we took about 1500 English sen-
tences and translated them into the other 5 lan-
guages, and then additional 1500 sentences from
each of the other languages and translated them
into English. This gave us test sets of about 3000
sentences for our English-X language pairs, which
have been either written originally written in En-
glish and translated into X, or vice versa.
The composition of the test documents is shown
in Table 1. The stories were translated by the pro-
fessional translation agency Capita, funded by the
EU Framework Programme 7 project MosesCore,
and by Yandex, a Russian search engine com-
pany.3 All of the translations were done directly,
and not via an intermediate language.
</bodyText>
<subsectionHeader confidence="0.998549">
2.2 Training data
</subsectionHeader>
<bodyText confidence="0.999983533333333">
As in past years we provided parallel corpora
to train translation models, monolingual cor-
pora to train language models, and development
sets to tune system parameters. Some train-
ing corpora were identical from last year (Eu-
roparl4, United Nations, French-English 109 cor-
pus, CzEng, Common Crawl, Russian-English
Wikipedia Headlines provided by CMU), some
were updated (Russian-English parallel data pro-
vided by Yandex, News Commentary, monolin-
gual data), and a new corpus was added (Hindi-
English corpus, Bojar et al. (2014)), Hindi-English
Wikipedia Headline corpus).
Some statistics about the training materials are
given in Figure 1.
</bodyText>
<subsectionHeader confidence="0.997817">
2.3 Submitted systems
</subsectionHeader>
<bodyText confidence="0.9999315">
We received 143 submissions from 23 institu-
tions. The participating institutions and their entry
names are listed in Table 2; each system did not
necessarily appear in all translation tasks. We also
included four commercial off-the-shelf MT sys-
tems and four online statistical MT systems, which
we anonymized.
For presentation of the results, systems are
treated as either constrained or unconstrained, de-
pending on whether their models were trained only
on the provided data. Since we do not know how
they were built, these online and commercial sys-
tems are treated as unconstrained during the auto-
matic and human evaluations.
</bodyText>
<footnote confidence="0.942601666666667">
3http://www.yandex.com/
4As of Fall 2011, the proceedings of the European Parlia-
ment are no longer translated into all official languages.
</footnote>
<page confidence="0.997142">
13
</page>
<table confidence="0.9622048">
Europarl Parallel Corpus
French +-+ English German +-+ English Czech +-+ English
Sentences 2,007,723 1,920,209 646,605
Words 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Parallel Corpus
French +-+ English German +-+ English Czech +-+ English Russian +-+ English
Sentences 183,251 201,288 146,549 165,602
Words 5,688,656 4,659,619 5,105,101 5,046,157 3,288,645 3,590,287 4,153,847 4,339,974
Distinct words 72,863 62,673 150,760 65,520 139,477 55,547 151,101 60,801
Common Crawl Parallel Corpus
French +-+ English German +-+ English Czech +-+ English Russian +-+ English
Sentences 3,244,152 2,399,123 161,838 878,386
Words 91,328,790 81,096,306 54,575,405 58,870,638 3,529,783 3,927,378 21,018,793 21,535,122
Distinct words 889,291 859,017 1,640,835 823,480 210,170 128,212 764,203 432,062
United Nations Parallel Corpus Hindi-English Parallel Corpus
French +-+ English Hindi +-+ English
Sentences 12,886,831 Sentences 287,202
Words 411,916,781 360,341,450 Words 6,002,418 3,953,851
Distinct words 565,553 666,077 Distinct words 121,236 105,330
109 Word Parallel Corpus Yandex 1M Parallel Corpus
French +-+ English Russian +-+ English
Sentences 22,520,400 Sentences 1,000,000
Words 811,203,407 668,412,817 Words 24,121,459 26,107,293
Distinct words 2,738,882 2,861,836 Distinct words 701,809 387,646
Sentences
14,833,358
Wiki Headlines Parallel Corpus
Russian +-+ English Hindi +-+ English
Sentences 514,859 32,863
Words 1,191,474 1,230,644 141,042 70,075
Distinct words 282,989 251,328 25,678 26,989
CzEng Parallel Corpus
Czech +-+ English
Words
Distinct words
200,658,857
1,389,803
228,040,794
920,824
Europarl Language Model Data
English French German Czech
Sentence 2,218,201 2,190,579 2,176,537 668,595
Words 59,848,044 63,439,791 53,534,167 14,946,399
Distinct words 123,059 145,496 394,781 172,461
News Language Model Data
English French German Czech Russian Hindi
Sentence 90,209,983 30,451,749 89,634,193 36,426,900 32,245,651 1,275,921
Words 2,109,603,244 748,852,739 1,606,506,785 602,950,410 575,423,682 36,297,394
Distinct words 4,089,792 1,906,470 10,248,707 3,101,846 2,860,837 258,759
News Test Set
French +-+ English German +-+ English Czech +-+ English Russian +-+ English Hindi +-+ English
Sentences 3003 3003 3003 3003 2507
Words 81,194 71,147 63,078 67,624 60,240 68,866 62,107 69,329 86,974 55,822
Distinct words 11,715 10,610 13,930 10,458 16,774 9,893 17,009 9,938 8,292 9,217
</table>
<figureCaption confidence="0.923647">
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct
words (case-insensitive) is based on the provided tokenizer.
</figureCaption>
<page confidence="0.988075">
14
</page>
<table confidence="0.99900192">
Language Sources (Number of Documents)
Czech aktu´alnˇe.cz (2), blesk.cz (3), blisty.cz (1), denik.cz (9), e15.cz (1), iDNES.cz (17), ihned.cz (14), lidovky.cz (8), medi-
afax.cz (2), metro.cz (1), Novinky.cz (5), pravo.novinky.cz (6), reflex.cz (2), tyden.cz (1), zdn.cz (1).
French BBC French Africa (1), Canoe (9), Croix (4), Cyber Presse (12), Dernieres Nouvelles (1), dhnet.be (5), Equipe (1),
Euronews (6), Journal Metro.com (1), La Libre.be (2), La Meuse.be (2), Le Devoir (3), Le Figaro (8), Le Monde (3),
Les Echos (15), Lexpress.fr (3), Liberation (1), L’independant (2), Metro France (1), Nice-Matin (6), Le Nouvel Ob-
servateur (3), Radio Canada (6), Reuters (7).
English ABC News (5), BBC (5), CBS News (5), CNN (5), Daily Mail (5), Financial Times (5), Fox News (2), Globe and
Mail (1), Independent (1), Los Angeles Times (1), New Yorker (1), News.com Australia (16), Reuters (3), Scotsman (2),
smh.com.au (2), stv.tv (1), Telegraph (6), UPI (2).
German Abendzeitung N¨urnberg (1), all-in.de (2), Augsburger Allgemeine (1), AZ Online (1), B¨orsenzeitung (1), come-
on.de (1), Der Westen (2), DZ Online (1), Reutlinger General-Anzeiger (1), Generalanzeiger Bonn (1), Giessener
Anzeiger (1), Goslarsche Zeitung (1), Hersfelder Zeitung (1), J¨udische Allgemeine (1), Kreisanzeiger (2),
Kreiszeitung (2), Krone (1), Lampertheimer Zeitung (2), Lausitzer Rundschau (1), Mittelbayerische (1), Morgen-
post (1), nachrichten.at (1), Neue Presse (1), OP Online (1), Potsdamer Neueste Nachrichten (1), Passauer Neue
Presse (1), Recklingh¨auser Zeitung (1), Rhein Zeitung (1), salzburg.com (1), Schwarzw¨alder Bote (29), Segeberger
Zeitung (1), Soester Anzeiger (1), S¨udkurier (17), svz.de (1), Tagesspiegel (1), Usinger Anzeiger (3), Volksblatt.li (1),
Westf¨alischen Anzeiger (3), Wiener Zeitung (1), Wiesbadener Kurier (1), Westdeutsche Zeitung (1), Wilhelmshavener
Zeitung (1), Yahoo Deutschland (1).
Hindi Bhaskar (24), Jagran (61), Navbharat Times / India Times (4), ndtv (2).
Russian 168.ru (1), aif (3), altapress.ru (2), argumenti.ru (2), BBC Russian (3), belta.by (2), communa.ru (1), dp.ru (1), eg-
online.ru (1), Euronews (2), fakty.ua (2), gazeta.ru (1), inotv.rt.com (1), interfax (1), Izvestiya (1), Kommersant (7),
kp (2), lenta.ru (4), lgng (1), litrossia.ru (1), mirnov.ru (5), mk (8), mn.ru (2), newizv (2), nov-pravda.ru (1), no-
vayagazeta (1), nr2.ru (8), pnp.ru (1), rbc.ru (3), ria.ru (4), rosbalt.ru (1), sovsport.ru (6), Sport Express (10), trud.ru (4),
tumentoday.ru (1), vesti.ru (10), zr.ru (1).
</table>
<tableCaption confidence="0.974174">
Table 1: Composition of the test set. For more details see the XML test files. The docid tag gives the source and the date for
each document in the test set, and the origlang tag indicates the original source language.
</tableCaption>
<sectionHeader confidence="0.961133" genericHeader="method">
3 Human Evaluation
</sectionHeader>
<bodyText confidence="0.999891181818182">
As with past workshops, we contend that auto-
matic measures of machine translation quality are
an imperfect substitute for human assessments.
We therefore conduct a manual evaluation of the
system outputs and define its results to be the prin-
cipal ranking of the workshop. In this section, we
describe how we collected this data and compute
the results, and then present the official results of
the ranking.
This year’s evaluation was conducted a bit dif-
ferently. The main differences are:
</bodyText>
<listItem confidence="0.8959095625">
• In contrast to the past two years, we collected
judgments entirely from researchers partici-
pating in the shared tasks and trusted friends
of the community. Last year, about two thirds
of the data were solicited from random volun-
teers on the Amazon Mechanical Turk. For
some language pairs, the Turkers data had
much lower inter-annotator agreement com-
pared to the researchers.
• As a result, we collected about seventy-five
percent less data, but were able to obtain
good confidence intervals on the clusters with
the use of new approaches to ranking.
• We compared three different ranking method-
ologies, selecting the one with the highest ac-
curacy on held-out data.
</listItem>
<bodyText confidence="0.999408833333333">
We also maintain many of our customs from
prior years, including the presentation of the re-
sults in terms of a partial ordering (clustering) of
the systems. Systems in the same cluster could not
be meaningfully distinguished and should be con-
sidered ties.
</bodyText>
<subsectionHeader confidence="0.997858">
3.1 Data collection
</subsectionHeader>
<bodyText confidence="0.999986875">
The system ranking is produced from a large set of
pairwise annotations between system pairs. These
pairwise annotations are collected in an evaluation
campaign that enlists participants in the shared
task to contribute one hundred “Human Intelli-
gence Tasks” (HITs) per system submitted. Each
HIT consists of three ranking tasks. In a rank-
ing task, an annotator is presented with a source
segment, a human reference translation, and the
outputs of five anonymized systems, randomly se-
lected from the set of participating systems, and
randomly ordered.
To run the evaluation, we use Appraise5 (Fe-
dermann, 2012), an open-source tool built on
Python’s Django framework. At the top of each
HIT, the following instructions are provided:
</bodyText>
<footnote confidence="0.7095112">
You are shown a source sentence fol-
lowed by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
5https://github.com/cfedermann/Appraise
</footnote>
<page confidence="0.990836">
15
</page>
<note confidence="0.814138516129032">
ID Institution
AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014)
CIMS University of Stuttgart / University of Munich (Cap et al., 2014)
CMU Carnegie Mellon University (Matthews et al., 2014)
CU-* Charles University, Prague (Tamchyna et al., 2014)
DCU-FDA Dublin City University (Bicici et al., 2014)
DCU-ICTCAS Dublin City University (Li et al., 2014b)
DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014)
EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014)
KIT Karlsruhe Institute of Technology (Herrmann et al., 2014)
IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014)
IIIT-HYDERABAD IIIT Hyderabad
IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014)
IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014)
KAZNU Amandyk Kartbayev, FBK
LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014)
MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014)
MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014)
PROMT-RULE, PROMT
PROMT-HYBRID
RWTH RWTH Aachen (Peitz et al., 2014)
STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014)
UA-* University of Alicante (S´anchez-Cartagena et al., 2014)
UEDIN-PHRASE, University of Edinburgh (Durrani et al., 2014b)
UEDIN-UNCNSTR
UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014)
UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014)
YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014)
COMMERCIAL-[1,2] Two commercial machine translation systems
ONLINE-[A,B,C,G] Four online statistical machine translation systems
RBMT-[1,4] Two rule-based statistical machine translation systems
</note>
<tableCaption confidence="0.994545333333333">
Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the
commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore
anonymized in a fashion consistent with previous years of the workshop.
</tableCaption>
<page confidence="0.995073">
16
</page>
<figureCaption confidence="0.9161664">
Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a
source segment, a reference translation, and the outputs of five systems (anonymized and randomly ordered), and is asked to
rank these according to their translation quality, with ties allowed.
A screenshot of the ranking interface is shown in
Figure 2. Annotators are asked to rank the sys-
</figureCaption>
<bodyText confidence="0.967593857142857">
tems from 1 (best) to 5 (worst), with ties permit-
ted. Note that a lower rank is better. The rankings
provided by a ranking task are then reduced to a
set of ten pairwise rankings produced by consider-
ing all (2) combinations of systems in the ranking
task. For example, consider the following annota-
tion provided among systems A, B, F, H, and J:
</bodyText>
<figure confidence="0.994156">
1 2 3 4 5
F •
A •
B •
J •
H •
</figure>
<bodyText confidence="0.4325915">
This is reduced to the following set of pairwise
judgments:
</bodyText>
<construct confidence="0.84975275">
A &gt; B, A = F, A &gt; H, A &lt; J
B&lt;F,B&lt;H,B&lt;J
F&gt;H,F&lt;J
H &lt; J
</construct>
<bodyText confidence="0.999684">
Here, A &gt; B should be read is “A is ranked higher
than (worse than) B”. Note that by this procedure,
the absolute value of ranks and the magnitude of
their differences are discarded.
For WMT13, nearly a million pairwise anno-
tations were collected from both researchers and
paid workers on Amazon’s Mechanical Turk, in
a roughly 1:2 ratio. This year, we collected data
from researchers only, an ability that was enabled
by the use of a new technique for producing the
partial ranking for each task (§3.3.3). Table 3 con-
tains more detail.
</bodyText>
<subsectionHeader confidence="0.998135">
3.2 Annotator agreement
</subsectionHeader>
<bodyText confidence="0.999653571428571">
Each year we calculate annotator agreement
scores for the human evaluation as a measure of
the reliability of the rankings. We measured pair-
wise agreement among annotators using Cohen’s
kappa coefficient (n) (Cohen, 1960). If P(A) be
the proportion of times that the annotators agree,
and P(E) is the proportion of time that they would
</bodyText>
<page confidence="0.998639">
17
</page>
<table confidence="0.996650466666667">
LANGUAGE PAIR Systems Rankings Average
Czech–English 5 21,130 4,226.0
English–Czech 10 55,900 5,590.0
German–English 13 25,260 1,943.0
English–German 18 54,660 3,036.6
French–English 8 26,090 3,261.2
English–French 13 33,350 2,565.3
Russian–English 13 34,460 2,650.7
English–Russian 9 28,960 3,217.7
Hindi–English 9 20,900 2,322.2
English–Hindi 12 28,120 2,343.3
TOTAL WMT 14 110 328,830 2,989.3
WMT13 148 942,840 6,370.5
WMT12 103 101,969 999.6
WMT11 133 63,045 474.0
</table>
<tableCaption confidence="0.9771145">
Table 3: Amount of data collected in the WMT14 manual evaluation. The final three rows report summary information from
the previous two workshops.
</tableCaption>
<bodyText confidence="0.564577">
agree by chance, then Cohen’s kappa is:
</bodyText>
<equation confidence="0.978339">
P(A) − P(E)
1 − P(E)
</equation>
<bodyText confidence="0.999842266666667">
Note that n is basically a normalized version of
P(A), one which takes into account how mean-
ingful it is for annotators to agree with each other
by incorporating P(E). The values for n range
from 0 to 1, with zero indicating no agreement and
1 perfect agreement.
We calculate P(A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A &lt; B, A = B, or A &gt; B. In
other words, P(A) is the empirical, observed rate
at which annotators agree, in the context of pair-
wise comparisons.
As for P(E), it captures the probability that two
annotators would agree randomly. Therefore:
</bodyText>
<equation confidence="0.998065">
P(E) = P(A&lt;B)2 + P(A=B)2 + P(A&gt;B)2
</equation>
<bodyText confidence="0.9999684">
Note that each of the three probabilities in P(E)’s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is
computed empirically, by observing how often an-
notators actually rank two systems as being tied.
Table 4 gives n values for inter-annotator agree-
ment for WMT11–WMT14 while Table 5 de-
tails intra-annotator agreement scores, including
the division of researchers (WMT13r) and MTurk
(WMT13m) data. The exact interpretation of the
kappa coefficient is difficult, but according to Lan-
dis and Koch (1977), 0–0.2 is slight, 0.2–0.4 is
fair, 0.4–0.6 is moderate, 0.6–0.8 is substantial,
and 0.8–1.0 is almost perfect. The agreement rates
are more or less in line with prior years: worse for
some tasks, better for others, and on average, the
best since WMT11 (where agreement scores were
likely inflated due to inclusion of reference trans-
lations in the comparisons).
</bodyText>
<subsectionHeader confidence="0.999923">
3.3 Models of System Rankings
</subsectionHeader>
<bodyText confidence="0.999763590909091">
The collected pairwise rankings are used to pro-
duce a ranking of the systems. Machine transla-
tion evaluation has always been a subject of con-
tention, and no exception to this rule exists for the
WMT manual evaluation. While the precise met-
ric has varied over the years, it has always shared
a common idea of computing the average num-
ber of times each system was judged better than
other systems, and ranking from highest to low-
est. For example, in WMT11 Callison-Burch et al.
(2011), the metric computed the percentage of the
time each system was ranked better than or equal
to other systems, and included comparisons to hu-
man references. In WMT12 Callison-Burch et al.
(2012), comparisons to references were dropped.
In WMT13, rankings were produced over 1,000
bootstrap-resampled sets of the training data. A
rank range was collected for each system across
these folds; the average value was used to order
the systems, and a 95% confidence interval across
these ranks was used to organize the systems into
equivalence classes containing systems with over-
</bodyText>
<equation confidence="0.954972">
n =
</equation>
<page confidence="0.996818">
18
</page>
<table confidence="0.999924666666666">
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13r WMT13m WMT14
Czech–English 0.400 0.311 0.244 0.342 0.279 0.305
English–Czech 0.460 0.359 0.168 0.408 0.075 0.360
German–English 0.324 0.385 0.299 0.443 0.324 0.368
English–German 0.378 0.356 0.267 0.457 0.239 0.427
French–English 0.402 0.272 0.275 0.405 0.321 0.357
English–French 0.406 0.296 0.231 0.434 0.237 0.302
Hindi–English — — — — — 0.400
English–Hindi — — — — — 0.413
Russian–English — 0.278 0.315 0.324 0.324
English–Russian — 0.243 0.416 0.207 0.418
MEAN 0.395 0.330 0.260 0.367
</table>
<tableCaption confidence="0.994539">
Table 4: κ scores measuring inter-annotator agreement. See Table 5 for corresponding intra-annotator agreement scores.
</tableCaption>
<table confidence="0.999871">
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13r WMT13m WMT14
Czech–English 0.597 0.454 0.479 0.483 0.478 0.382
English–Czech 0.601 0.390 0.290 0.547 0.242 0.448
German–English 0.576 0.392 0.535 0.643 0.515 0.344
English–German 0.528 0.433 0.498 0.649 0.452 0.576
French–English 0.673 0.360 0.578 0.585 0.565 0.629
English–French 0.524 0.414 0.495 0.630 0.486 0.507
Hindi–English — — — — — 0.605
English–Hindi — — — — — 0.535
Russian–English — 0.450 0.363 0.477 0.629
English–Russian — 0.513 0.582 0.500 0.570
MEAN 0.583 0.407 0.479 0.522
</table>
<tableCaption confidence="0.9944035">
Table 5: κ scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the
human evaluation.
</tableCaption>
<bodyText confidence="0.997482090909091">
lapping ranges.
This year, we introduce two new changes. First,
we pit the WMT13 method against two new ap-
proaches: that of Hopkins and May (2013, §3.3.2),
and another based on TrueSkill (Sakaguchi et al.,
2014, §3.3.3). Second, we compare these two
methods against WMT13’s “Expected Wins” ap-
proach, and then select among them by determin-
ing which of them has the highest accuracy in
terms of predicting annotations on a held-out set
of pairwise judgments.
</bodyText>
<subsectionHeader confidence="0.983948">
3.3.1 Method 1: Expected Wins (EW)
</subsectionHeader>
<bodyText confidence="0.9945454">
Introduced for WMT13, the EXPECTED WINS has
an intuitive score demonstrated to be accurate in
ranking systems according to an underlying model
of “relative ability” (Koehn, 2012a). The idea is
to gauge the probability that a system Si will be
ranked better than another system randomly cho-
sen from a pool of opponents {Sj : j =6 i}. If
we define the function win(A, B) as the number
of times system A is ranked better than system B,
then we can define this as follows:
</bodyText>
<equation confidence="0.996492">
scoreEW (Si) =
1 � win(Si, Sj)
|{Sj} |win(Si,Sj) + win(Sj, Si)
j,j7i
</equation>
<bodyText confidence="0.972195">
Note that this score ignores ties.
</bodyText>
<subsectionHeader confidence="0.993934">
3.3.2 Method 2: Hopkins and May (HM)
</subsectionHeader>
<bodyText confidence="0.9987915">
Hopkins and May (2013) introduced a graphical
model formulation of the task, which makes the
notion of underlying system ability even more ex-
plicit. Each system SJ in the pool {Sj} is repre-
sented by an associated relative ability pj and a
variance σ2a (fixed across all systems) which serve
as the parameters of a Gaussian distribution. Sam-
ples from this distribution represent the quality
of sentence translations, with higher quality sam-
ples having higher values. Pairwise annotations
(S1, S2, π) are generated according to the follow-
ing process:
</bodyText>
<page confidence="0.996374">
19
</page>
<listItem confidence="0.871153166666667">
1. Select two systems S1 and S2 from the pool
of systems {Sj}
2. Draw two “translations”, adding random
Gaussian noise with variance σ2obs to simulate
the subjectivity of the task and the differences
among annotators:
</listItem>
<equation confidence="0.9979055">
q1 ∼ N(µS1, σ2a) + N(0, σ2obs)
q2 ∼ N(µS2, σ2a) + N(0, σ2obs)
</equation>
<listItem confidence="0.893638333333333">
3. Let d be a nonzero real number that defines
a fixed decision radius. Produce a rating π
according to:
</listItem>
<equation confidence="0.934576666666667">
&lt; q1 − q2 &gt; d
&gt; q2 − q1 &gt; d
= otherwise
</equation>
<bodyText confidence="0.970709166666667">
Hopkins and May use Gibbs sampling to infer
the set of system means from an annotated dataset.
Details of this inference procedure can be found in
Sakaguchi et al. (2014). The score used to produce
the rankings is simply the system mean associated
with each system:
</bodyText>
<equation confidence="0.961155">
scoreHM(Si) = µSi
</equation>
<subsectionHeader confidence="0.792398">
3.3.3 Method 3: TrueSkill (TS)
</subsectionHeader>
<bodyText confidence="0.979028125">
TrueSkill is an adaptive, online system that em-
ploys a similar model of relative ability Herbrich
et al. (2006). It was initially developed for Xbox
Live’s online player community, where it is used
to model player ability, assign levels, and select
competitive matches. Each player Sj is modeled
by two parameters: TrueSkill’s current estimate
of each system’s relative ability, µSj, and a per-
system measure of TrueSkill’s uncertainty of those
estimates, σ2. When the outcome of a match is
j
observed, TrueSkill uses the relative status of the
two systems to update these estimates. If a trans-
lation from a system with a high mean is judged
better than a system with a greatly lower mean, the
result is not surprising, and the update size for the
corresponding system means will be small. On the
other hand, when an upset occurs in a competition,
the means will receive larger updates. Sakaguchi
et al. (2014) provide an adaptation of this approach
to the WMT manual evaluation, and showed that
it performed well on WMT13 data.
Similar to the Hopkins and May model,
TrueSkill scores systems by their inferred means:
</bodyText>
<equation confidence="0.920661">
scoreTS(Si) = µSi
</equation>
<bodyText confidence="0.9956285">
This score is then used to sort the systems and pro-
duce the ranking.
</bodyText>
<subsectionHeader confidence="0.999021">
3.4 Method Selection
</subsectionHeader>
<bodyText confidence="0.999991571428571">
We have three methods which, provided with the
collected data, produce different rankings of the
systems. Which of them is correct? More imme-
diately, which one of them should we publish as
the official ranking for the WMT14 manual eval-
uation? As discussed, the method used to com-
pute the ranking has been tweaked a bit each year
over the past few years in response to criticisms
(e.g., Lopez (2012); Bojar et al. (2011)). While the
changes were reasonable (and later corroborated),
Hopkins and May (2013) pointed out that this task
of model selection should be driven by empirical
evaluation on held-out data, and suggested per-
plexity as the metric of choice.
We choose instead a more direct gold-standard
evaluation metric: the accuracy of the rankings
produced by each method in predicting pairwise
judgments. We use each method to produce a par-
tial ordering of the systems, grouping them into
equivalence classes. This partial ordering unam-
biguously assigns a prediction πP between any
pair of systems (Si, Sj). By comparing the pre-
dicted relationship πP to the actual annotation for
each pairwise judgment in the test data (by token),
we can compute an accuracy score for each model.
We predict accuracy in this manner using 100-
fold cross-validation. For each task, we split the
data into a fixed set of 100 randomly-selected
folds. Each fold serves as a test set, with the
remaining ninety-nine folds available as training
data for each method. Note that the total order-
ing over systems provided by the score∗ functions
defined do not predict ties. In order to do enable
the models to predict ties, we produce equivalence
classes using the following procedure:
</bodyText>
<listItem confidence="0.9219525">
• Assign S1 to a cluster
• For each system Si, assign it to the current
cluster if score(Si−1) − score(Si) ≤ r; oth-
erwise, assign it to a new cluster
</listItem>
<bodyText confidence="0.971155333333333">
The value of r (the decision radius for ties)
is tuned using accuracy on the entire training
data using grid search over the values r E
0,0.01,0.02, ... , .25 (26 values in total). This
value is tuned separately for each method on each
fold. Table 6 contains an example partial ordering.
</bodyText>
<equation confidence="0.869919916666667">
⎧
⎨
⎩
π =
20
System Score Rank
B 0.60 1
D 0.44 2
E 0.39 2
A 0.25 2
F -0.09 3
C -0.22 3
</equation>
<tableCaption confidence="0.925549">
Table 6: The partial ordering computed with the provided
scores when r = 0.15.
</tableCaption>
<table confidence="0.999200333333333">
Task EW HM TS Oracle
Czech–English 40.4 41.1 41.1 41.2
English–Czech 45.3 45.6 45.9 46.8
French–English 49.0 49.4 49.3 50.3
English–French 44.6 44.4 44.7 46.0
German–English 43.5 43.7 43.7 45.2
English–German 47.3 47.4 47.2 48.2
Hindi–English 62.5 62.2 62.5 62.6
English–Hindi 53.3 53.7 53.5 55.7
Russian–English 47.6 47.7 47.7 50.6
English–Russian 46.5 46.1 46.4 48.2
MEAN 48.0 48.1 48.2 49.2
</table>
<tableCaption confidence="0.98357025">
Table 7: Accuracies for each method across 100 folds, for
each translation task. The oracle uses the most frequent out-
come between each pair of systems, and therefore might not
constitute a feasible ranking.
</tableCaption>
<bodyText confidence="0.999673761904762">
After training, each model has defined a partial
ordering over systems.6 This is then used to com-
pute accuracy on all the pairwise judgments in the
test fold. This process yields 100 accuracies for
each method; the average accuracy across all the
folds can then be used to compute the best method.
Table 7 contains accuracy results for the three
methods on the WMT14 tasks. On average, there
is a small improvement in accuracy moving from
Expected Wins to the H&amp;M model, and then again
to the TrueSkill model; however, there is no pat-
tern to the best model for each class. The Oracle
column is computed by selecting the most prob-
able outcome (π ∈ {&lt;, =, &gt;}) for each system
pair, and provides an upper bound on accuracy
when predicting outcomes using only system-level
information. Furthermore, this method of oracle
computation might not represent a feasible rank-
ing or clustering,7.
The TrueSkill approach was best overall, so we
used it to produce the official rankings for all lan-
</bodyText>
<footnote confidence="0.997866">
6It is a total ordering when r = 0, or when all the system
scores are outside the decision radius.
7For example, if there were a cycle of “better than” judg-
ments among a set of systems.
</footnote>
<bodyText confidence="0.559869">
guage pairs.
</bodyText>
<subsectionHeader confidence="0.9984">
3.5 Rank Ranges and Clusters
</subsectionHeader>
<bodyText confidence="0.99996265625">
Above we saw how to produce system scores for
each method, which provides a total ordering of
the systems. But we would also like to know if the
obtained system ranking is statistically significant.
Given the large number of systems that participate,
and the similarity of the underlying systems result-
ing from the common training data condition and
(often) toolsets, there will be some systems that
will be very close in quality. These systems should
be grouped together in equivalence classes.
To establish the reliability of the obtained sys-
tem ranking, we use bootstrap resampling. We
sample from the set of pairwise rankings an equal
sized set of pairwise rankings (allowing for multi-
ple drawings of the same pairwise ranking), com-
pute a TrueSkill model score for each system
based on this sample, and then rank the systems
from 1..|{Sj}|. By repeating this procedure 1,000
times, we can determine a range of ranks, into
which system falls at least 95% of the time (i.e.,
at least 950 times) — corresponding to a p-level
of p ≤ 0.05. Furthermore, given the rank ranges
for each system, we can cluster systems with over-
lapping rank ranges.8
Table 8 reports all system scores, rank ranges,
and clusters for all language pairs and all systems.
The official interpretation of these results is that
systems in the same cluster are considered tied.
Given the large number of judgments that we col-
lected, it was possible to group on average about
two systems in a cluster, even though the systems
in the middle are typically in larger clusters.
</bodyText>
<subsectionHeader confidence="0.999876">
3.6 Cluster analysis
</subsectionHeader>
<bodyText confidence="0.998057888888889">
The official ranking results for English-German
produced clusters compute at the 90% confidence
level due to the presence of a very large cluster
(of nine systems). While there is always the pos-
sibility that this cluster reflects a true ambiguity, it
is more likely due to the fact that we didn’t have
enough data: English–German had the most sys-
8Formally, given ranges defined by start(Si) and end(Si),
we seek the largest set of clusters {CcI that satisfies:
</bodyText>
<equation confidence="0.9932445">
VS EIC : SEC
S ECa,S E Cb - Ca = Cb
Ca =�Cb - VSi ECa, Sj ECb:
start(Si) &gt; end(Sj) or start(Sj) &gt; end(Si)
</equation>
<page confidence="0.999047">
21
</page>
<figure confidence="0.996053256198347">
German–English
LIMSI-KIT
UEDIN-PHRASE
EU-BRIDGE
KIT
RWTH
# score range system
UEDIN-SYNTAX
ONLINE-A
2
0.267
0.258
2-3
2-3
0.451
1
ONLINE-B
1
3 0.147
0.146
0.138
4-6
4-6
4-6
4 0.026
-0.049
7-8
7-8
7 -0.604
13
ONLINE-C
DCU-ICTCAS
CMU
RBMT4
6 -0.306
12
RBMT1
5 -0.125
-0.157
-0.192
9-11
9-11
9-11
French–English
#
score
range
system
1
1
0.608
UEDIN-PHRASE
2
ONLINE-B
KIT
STANFORD
0.331
4 -0.389
5 -0.648
6 -1.284
5
6
7
8
3
RBMT1
RBMT4
ONLINE-A
ONLINE-C
2-4
2-4
2-4
0.479
0.475
0.428
English–French
# score range system
0.327
1
11
12
13
4 -0.215
5 -0.328
6 -0.540
2
3
ONLINE-B
UEDIN-PHRASE
KIT
MATRAN
MATRAN-RULES
ONLINE-A
UU-DOCENT
PROMT-HYBRID
UA
PROMT-RULE
RBMT1
RBMT4
ONLINE-C
1
0.232
0.194
0.185
0.142
0.120
0.003
-0.019
-0.033
-0.069
2-4
2-5
2-5
4-6
4-6
7-9
7-10
7-10
8-10
Czech–English
English–German
</figure>
<table confidence="0.953308409090909">
# score range system
1 0.264 1-2 UEDIN-SYNTAX
0.242 1-2 ONLINE-B
2 0.167 3-6 ONLINE-A
0.156 3-6 PROMT-HYBRID
0.155 3-6 PROMT-RULE
0.155 3-6 UEDIN-STANFORD
3 0.094 7 EU-BRIDGE
4 0.033 8-10 RBMT4
0.031 8-10 UEDIN-PHRASE
0.012 8-10 RBMT1
5 -0.032 11-12 KIT
-0.069 11-13 STANFORD-UNC
-0.100 12-14 CIMS
-0.126 13-15 STANFORD
-0.158 14-16 UU
-0.191 15-16 ONLINE-C
6 -0.307 17-18 IMS-TTT
-0.325 17-18 UU-DOCENT
Hindi–English
# score range system
1 1.326 1 ONLINE-B
2 0.559 2-3 ONLINE-A
0.476 2-4 UEDIN-SYNTAX
0.434 3-4 CMU
3 0.323 5 UEDIN-PHRASE
4 -0.198 6-7 AFRL
-0.280 6-7 IIT-BOMBAY
5 -0.549 8 DCU-LINGO24
6 -2.092 9 IIIT-HYDERABAD
English–Hindi
# score range system
1 1.008 1 ONLINE-B
2 0.915 2 ONLINE-A
3 0.214 3 UEDIN-UNCNSTR
4 0.120 4-5 UEDIN-PHRASE
0.054 4-5 CU-MOSES
5 -0.111 6-7 IIT-BOMBAY
-0.142 6-7 IPN-UPV-CNTXT
6 -0.233 8-9 DCU-LINGO24
-0.261 8-9 IPN-UPV-NODEV
7 -0.449 10-11 MANAWI-H1
-0.494 10-11 MANAWI
8 -0.622 12 MANAWI-RMOOV
</table>
<figure confidence="0.996030428571429">
#
system
score
range
#
score
range
system
2
2
0.290
UEDIN-PHRASE
4 -0.468
5
CU-MOSES
0.591
1
ONLINE-B
3 -0.171
-0.243
UEDIN-SYNTAX
ONLINE-A
1
3-4
3-4
English–Czech
# score range system
1 0.371 1-3 CU-DEPFIX
0.356 1-3 UEDIN-UNCNSTR
0.333 1-4 CU-BOJAR
0.287 3-4 CU-FUNKY
2 0.169 5-6 ONLINE-B
0.113 5-6 UEDIN-PHRASE
3 0.030 7 ONLINE-A
4 -0.175 8 CU-TECTO
5 -0.534 9 COMMERCIAL1
6 -0.950 10 COMMERCIAL2
Russian–English
AFRL-PE
ONLINE-B
ONLINE-A
PROMT-HYBRID
PROMT-RULE
UEDIN-PHRASE
YANDEX
ONLINE-G
AFRL
UEDIN-SYNTAX
KAZNU
RBMT1
RBMT4
0.583
0.299
1
2
9
10
11
12
13
1
3
4 -0.017
5 -0.159
6 -0.306
7 -0.487
8 -0.642
2
0.190
0.178
0.123
0.104
0.069
0.066
3-5
3-5
4-7
5-8
5-8
5-8
English–Russian
# score range system
1 0.575 1-2 PROMT-RULE
0.547 1-2 ONLINE-B
2 0.426 3 PROMT-HYBRID
3 0.305 4-5 UEDIN-UNCNSTR
0.231 4-5 ONLINE-G
4 0.089 6-7 ONLINE-A
0.031 6-7 UEDIN-PHRASE
5 -0.920 8 RBMT4
6 -1.284 9 RBMT1
</figure>
<tableCaption confidence="0.8532305">
Table 8: Official results for the WMT14 translation task. Systems are ordered by their inferred system means. Lines between
systems indicate clusters according to bootstrap resampling at p-level P &lt; .05, except for English–German, where P &lt; 0.1.
This method is also used to determine the range of ranks into which system falls. Systems with grey background indicate use
of resources that fall outside the constraints provided for the shared task.
</tableCaption>
<page confidence="0.982768">
22
</page>
<bodyText confidence="0.99997692">
tems (18, compared to 13 for the next languages),
yet only an average amount of per-system data.
Here, we look at this language pair in more detail,
in order to justify this decision, and to shed light
on the differences between the ranking methods.
Table 9 presents the 95% confidence-level clus-
terings for English–German computed with each
of the three methods, along with lines that show
the reorderings of the systems between them. Re-
orderings of this type have been used to argue
against the reliability of the official WMT rank-
ing (Lopez, 2012; Hopkins and May, 2013). This
table shows that these reorderings are captured en-
tirely by the clustering approach we used. This rel-
ative consensus of these independently-computed
and somewhat different models suggests that the
published ranking is approaching the true ambigu-
ity underlying systems within the same cluster.
Looking across all language pairs, we find that
the total ordering predicted by EW and TS is ex-
actly the same for eight of the ten language pair
tasks, and is constrained to reorderings within
the official cluster for the other two (German-
English —just one adjacent swap — and English-
German, depicted in Table 9).
</bodyText>
<subsectionHeader confidence="0.98583">
3.7 Conclusions
</subsectionHeader>
<bodyText confidence="0.999970607142857">
The official ranking method employed by WMT
over the past few years has changed a few times as
a result of error analysis and introspection. Until
this year, these results were largely based on the
intuitions of the community and organizers about
deficiencies in the models. In addition to their in-
tuitive appeal, many of these changes (such as the
decision to throw out comparisons against refer-
ences) have been empirically validated Hopkins
and May (2013). The actual effect of the refine-
ments in the ranking metric has been minor pertur-
bations in the permutation of systems. The cluster-
ing method of Koehn (2012b), in which the official
rankings are presented as a partial (instead of to-
tal) ordering, alleviated many of the problems ob-
served by Lopez (2012), and also capture all the
variance across the new systems introduced this
year. In addition, presenting systems as clusters
appeals to intuition. As such, we disagree with
claims that there is a problem with irreproducibil-
ity of the results of the workshop evaluation task,
and especially disagree that there is anything ap-
proaching a “crisis of confidence” (Hopkins and
May, 2013). These claims seem to us to be over-
stated.
Conducting proper model selection by compar-
ison on held-out data, however, is a welcome sug-
gestion, and our inclusion of this process supports
improved confidence in the ranking results. That
said, it is notable that the different methods com-
pute very similar orderings. This avoids hallu-
cinating distinctions among systems that are not
really there, and captures the intuition that some
systems are basically equivalent. The chief ben-
efit of the TrueSkill model is not in outputting a
better complete ranking of the systems, but lies in
its reduced variance, which allow us to cluster the
systems with less data. There is also the unex-
plored avenue of using TrueSkill to drive the data
collection, steering the annotations of judges to-
wards evenly matched systems during the collec-
tion phase, potentially allowing confident results
to be presented while collecting even less data.
There is, of course, more work to be done.
We have produced this year statistically significant
clusters with a third of the data required last year,
which is an improvement. Models of relative abil-
ity are a natural fit for the manual evaluation, and
the introduction of an online Bayesian approach
to data collection present further opportunities to
reduce the amount of data needed. These methods
also provide a framework for extending the models
in a variety of potentially useful ways, including
modeling annotator bias, incorporating sentence
metadata (such as length, difficulty, or subtopic),
and adding features of the sentence pairs.
</bodyText>
<sectionHeader confidence="0.977613" genericHeader="method">
4 Quality Estimation Task
</sectionHeader>
<bodyText confidence="0.999720769230769">
Machine translation quality estimation is the task
of predicting a quality score for a machine trans-
lated text without access to reference translations.
The most common approach is to treat the problem
as a supervised machine learning task, using stan-
dard regression or classification algorithms. The
third edition of the WMT shared task on qual-
ity estimation builds on the previous editions of
the task (Callison-Burch et al., 2012; Bojar et al.,
2013), with tasks including both sentence-level
and word-level estimation, with new training and
test datasets.
The goals of this year’s shared task were:
</bodyText>
<listItem confidence="0.999864">
• To investigate the effectiveness of different
quality labels.
• To explore word-level quality prediction at
</listItem>
<page confidence="0.993714">
23
</page>
<table confidence="0.999070913043478">
Expected Wins Hopkins &amp; May TrueSkill
UEDIN-SYNTAX UEDIN-SYNTAX UEDIN-SYNTAX
ONLINE ONLINE ONLINE
-B -B -B
ONLINE-A UEDIN-STANFORD ONLINE-A
UEDIN-STANFORD PROMT-HYBRID PROMT-HYBRID
PROMT-RULE ONLINE-A PROMT-RULE
PROMT-HYBRID PROMT-RULE UEDIN-STANFORD
EU-BRIDGE EU-BRIDGE EU-BRIDGE
RBMT4 UEDIN-PHRASE RBMT4
UEDIN-PHRASE RBMT4 UEDIN-PHRASE
1 1 1
RBMT RBMT RBMT
KIT KIT KIT
STANFORD STANFORD STANFORD
-UNC -UNC -UNC
CIMS CIMS CIMS
STANFORD STANFORD STANFORD
UU UU UU
ONLINE ONLINE ONLINE
-C -C -C
IMS-TTT UU-DOCENT IMS-TTT
UU-DOCENT IMS-TTT UU-DOCENT
</table>
<tableCaption confidence="0.995385">
Table 9: A comparison of the rankings produced by Expected Wins, Hopkins &amp; May, and TrueSkill for English–German (the
</tableCaption>
<bodyText confidence="0.6534318">
task with the most systems and the largest cluster). The lines extending all the way across mark the official English–German
clustering (computed from TrueSkill with 90% confidence intervals), while bold entries mark the start of new clusters within
each method or column (computed at the 95% confidence level). The TrueSkill clusterings contain all the system reorderings
across the other two ranking methods.
different levels of granularity.
</bodyText>
<listItem confidence="0.9361816">
• To study the effects of training and test
datasets with mixed domains, language pairs
and MT systems.
• To examine the effectiveness of quality pre-
diction methods on human translations.
</listItem>
<bodyText confidence="0.999870133333333">
Four tasks were proposed: Tasks 1.1, 1.2, 1.3
are defined at the sentence-level (Sections 4.1),
while Task 2, at the word-level (Section 4.2). Each
task provides one or more datasets with up to four
language pairs each: English-Spanish, English-
German, German-English, Spanish-English, and
up to four alternative translations generated by:
a statistical MT system (SMT), a rule-based MT
system (RBMT), a hybrid MT system, and a hu-
man. These datasets were annotated with differ-
ent labels for quality by professional translators as
part of the QTLaunchPad9 project. External re-
sources (e.g. parallel corpora) were provided to
participants. Any additional resources, including
additional quality estimation training data, could
</bodyText>
<footnote confidence="0.733575">
9http://www.qt21.eu/launchpad/
</footnote>
<bodyText confidence="0.999954764705882">
be used by participants (no distinction between
open and close tracks is made). Participants were
also provided with a software package to extract
quality estimation features and perform model
learning, with a suggested list of baseline features
and learning method for sentence-level prediction.
Participants, described in Section 4.3, could sub-
mit up to two systems for each task.
Data used for building specific MT systems or
internal system information (such as n-best lists)
were not made available this year as multiple MT
systems were used to produced the datasets, in-
cluding rule-based systems. In addition, part of
the translations were produced by humans. Infor-
mation on the sources of translations was not pro-
vided either. Therefore, as a general rule, partici-
pants were only allowed to use black-box features.
</bodyText>
<subsectionHeader confidence="0.999364">
4.1 Sentence-level Quality Estimation
</subsectionHeader>
<bodyText confidence="0.943125333333333">
For the sentence-level tasks, two variants of the
results could be submitted for each task and lan-
guage pair:
</bodyText>
<listItem confidence="0.974594">
• Scoring: An absolute quality score for each
sentence translation according to the type of
</listItem>
<page confidence="0.993708">
24
</page>
<bodyText confidence="0.982233">
prediction, to be interpreted as an error met-
ric: lower scores mean better translations.
</bodyText>
<listItem confidence="0.626055">
• Ranking: A ranking of sentence translations
for all source test sentences from best to
worst. For this variant, it does not matter how
the ranking is produced (from HTER predic-
tions, likert predictions, or even without ma-
chine learning).
</listItem>
<bodyText confidence="0.998733666666667">
Evaluation was performed against the true label
and/or HTER ranking using the same metrics as in
previous years:
For the training of prediction models, we pro-
vide a new dataset consisting of source sen-
tences and their human translations, as well as
two-three versions of machine translations (by an
SMT system, an RBMT system and, for English-
Spanish/German only, a hybrid system), all in the
news domain, extracted from tests sets of various
WMT years and MT systems that participated in
the translation shared task:
</bodyText>
<listItem confidence="0.726525357142857">
# Source sentences # Target sentences
954 English
350 English
3,816 Spanish
1,400 German
• Scoring: Mean Average Error (MAE) (pri-
mary metric), Root Mean Squared Error
(RMSE).
350 German
350 Spanish
1,050 English
1,050 English
• Ranking: DeltaAvg (primary metric) (Bojar
et al., 2013) and Spearman’s rank correlation.
</listItem>
<bodyText confidence="0.999599166666667">
For all sentence-level these tasks, the same 17
features as in WMT12-13 were used to build base-
line systems. The SVM regression algorithm
within QUEST (Specia et al., 2013)10 was applied
for that with RBF kernel and grid search for pa-
rameter optimisation.
</bodyText>
<subsectionHeader confidence="0.544115">
Task 1.1 Predicting post-editing effort
</subsectionHeader>
<bodyText confidence="0.993754333333333">
Data in this task is labelled with discrete and
absolute scores for perceived post-editing effort,
where:
</bodyText>
<listItem confidence="0.9972545">
• 1 = Perfect translation, no post-editing
needed at all.
• 2 = Near miss translation: translation con-
tains maximum of 2-3 errors, and possibly
additional errors that can be easily fixed (cap-
italisation, punctuation, etc.).
• 3 = Very low quality translation, cannot be
easily fixed.
</listItem>
<bodyText confidence="0.9998576">
The datasets were annotated in a “triage” phase
aimed at selecting translations of type “2” (near
miss) that could be annotated for errors at the
word-level using the MQM metric (see Task 2, be-
low) for a more fine-grained and systematic trans-
lation quality analysis. Word-level errors in trans-
lations of type “3” are too difficult if not impos-
sible to annotate and classify, particularly as they
often contain inter-related errors in contiguous or
overlapping word spans.
</bodyText>
<footnote confidence="0.344263">
10http://www.quest.dcs.shef.ac.uk/
</footnote>
<bodyText confidence="0.999888">
As test data, for each language pair and MT sys-
tem (or human translation) we provide a new set
of translations produced by the same MT systems
(and humans) as those used for the training data:
</bodyText>
<figure confidence="0.633702">
# Source sentences # Target sentences
150 English
150 English
150 German
150 Spanish
</figure>
<figureCaption confidence="0.659773">
The distribution of true scores in both training
and test sets for each language pair is given in Fig-
ures 3.
Figure 3: Distribution of true 1-3 scores by langauge pair.
</figureCaption>
<bodyText confidence="0.999983">
Additionally, we provide some out of domain
test data. These translations were annotated in
the same way as above, each dataset by one Lan-
guage Service Provider (LSP), i.e, one profes-
sional translator, with two LPSs producing data in-
dependently for English-Spanish. They were gen-
erated using the LSPs’ own source data (a different
domain from news), and own MT system (differ-
ent from the three used for the official datasets).
The results on these datasets were not considered
</bodyText>
<figure confidence="0.998219416666667">
40%
60%
50%
30%
20%
10%
0%
{en-de-1}
{en-de-2}
{en-de-3}
{de-en-1}
{de-en-2}
{de-en-3}
Training Test
{en-es-1}
{en-es-2}
{en-es-3}
{es-en-1}
{es-en-2}
{es-en-3}
600 Spanish
600 German
450 English
450 English
</figure>
<page confidence="0.98598">
25
</page>
<bodyText confidence="0.986521563636364">
for the official ranking of the participating sys-
tems:
Task 1.2 Predicting percentage of edits
In this task we use HTER (Snover et al., 2006) as
quality score. This score is to be interpreted as
the minimum edit distance between the machine
translation and its manually post-edited version,
and its range is [0, 1] (0 when no edit needs to
be made, and 1 when all words need to be edited).
We used TERp (default settings: tokenised, case
insensitive, etc., but capped to 1)11 to compute the
HTER scores.
For practical reasons, the data is a subset of
Task 1.1’s dataset: only translations produced
by the SMT system English-Spanish. As train-
ing data, we provide 896 English-Spanish trans-
lation suggestions and their post-editions. As
test data, we provide a new set of 208 English-
Spanish translations produced by the same SMT
system. Each of the training and test translations
was post-edited by a professional translator using
the CASMACAT12 web-based tool, which also col-
lects post-editing time on a sentence-basis.
Task 1.3 Predicting post-editing time
For this task systems are required to produce, for
each translation, a real valued estimate of the time
(in milliseconds) it takes a translator to post-edit
the translation. The training and test sets are a sub-
set of that uses in Task 1.2 (subject to filtering of
outliers). The difference is that the labels are now
the number of milliseconds that were necessary to
post-edit each translation.
As training data, we provide 650 English-
Spanish translation suggestions and their post-
editions. As test data, we provide a new set of 208
English-Spanish translations (same test data as for
Task 1.2).
human and machine translations: those transla-
tions labelled “2” (near misses), plus additional
data provided by industry (either on the news do-
main or on other domains, such as technical doc-
umentation, produced using their own MT sys-
tems, and also pre-labelled as “2”). All seg-
ments were annotated with word-level labels by
professional translators using the core categories
in MQM (Multidimensional Quality Metrics)13 as
error typology (see Figure 4). Each word or se-
quence of words was annotated with a single error.
For (supposedly rare) cases where a decision be-
tween multiple fine-grained error types could not
be made, annotators were requested to choose a
coarser error category in the hierarchy.
Participants are asked to produce a label for
each token that indicates quality at different lev-
els of granularity:
</bodyText>
<listItem confidence="0.99279875">
• Binary classification: an OK / bad label,
where bad indicates the need for editing the
token.
• Level 1 classification: an OK / accuracy /
fluency label, specifying coarser level cate-
gories of errors for each token, or “OK” for
tokens with no error.
• Multi-class classification: one of the labels
</listItem>
<bodyText confidence="0.963227">
specifying the error type for the token (termi-
nology, mistranslation, missing word, etc.) in
Figure 4, or “OK” for tokens with no error.
As training data, we provide tokenised transla-
tion output for all language pairs, human and ma-
chine translations, with tokens annotated with all
issue types listed above, or “OK”. The annotation
was performed manually by professional transla-
tors as part of the QTLaunchPad project. For
the coarser variants, fine-grained errors are gen-
eralised to Accuracy or Fluency, or “bad” for the
binary variant. The amount of available training
data varies by language pair:
</bodyText>
<figure confidence="0.988854">
# Source sentences # Target sentences
# Source sentences
# Target sentences
971 English
297 English
388 Spanish
971 Spanish
297 German
388 English
</figure>
<subsectionHeader confidence="0.9961">
4.2 Word-level Quality Estimation
</subsectionHeader>
<bodyText confidence="0.996581">
The data for this task is based on a subset of the
datasets used for Task 1.1, for all language pairs,
</bodyText>
<footnote confidence="0.819993666666667">
1,957 English
715 English
350 German
900 Spanish
1,957 Spanish
715 German
350 English
900 English
11http://www.umiacs.umd.edu/˜snover/terp/
12http://casmacat.eu/
13http://www.qt21.eu/launchpad/content/
training
</footnote>
<page confidence="0.996315">
26
</page>
<figureCaption confidence="0.99962">
Figure 4: MQM metric as error typology.
</figureCaption>
<bodyText confidence="0.899530666666667">
As test data, we provide additional data points
for all language pairs, human and machine trans-
lations:
</bodyText>
<figure confidence="0.9983758">
# Source sentences
# Target sentences
382 English
150 English
100 German
150 Spanish
382 Spanish
150 German
100 English
150 English
</figure>
<equation confidence="0.9154537">
trix as follows:
tpO = 4172
fpO = 1482 + 193 = 1675
fnO = 1819 + 198 = 2017
tpF = 1333
fpF = 1819 + 214 = 2033
fnF = 1482 + 133 = 1615
tpA = 69
fpA = 198 + 133 = 331
fnA = 193 + 214 = 407
</equation>
<bodyText confidence="0.993397421052632">
In contrast to Tasks 1.1–1.3, no baseline feature
set is provided to the participants.
Similar to last year (Bojar et al., 2013), the
word-level task is primarily evaluated by macro-
averaged F-measure (in %). Because the class dis-
tribution is skewed – in the test data about 78% of
the tokens are marked as “OK” – we compute pre-
cision, recall, and F1 for each class individually,
weighting F1 scores by the frequency of the class
in the test data. This avoids giving undue impor-
tance to less frequent classes. Consider the follow-
ing confusion matrix for Level 1 annotation, i.e.
the three classes (O)K, (F)luency, and (A)ccuracy:
For each of the three classes we assume a binary
setting (one-vs-all) and derive true-positive (tp),
false-positive (fp), and false-negative (fn) counts
from the rows and columns of the confusion ma-
We continue to compute F1 scores for each
class c E {O, F, Al:
</bodyText>
<equation confidence="0.975558285714286">
precisionc = tpc/(tpc + fpc)
recallc = tpc/(tpc + fnc)
F1,c = 2 · precisionc · recallc
precisionc + recallc
yielding:
precisionO = 4172/(4172 + 1675) = 0.7135
recallO = 4172/(4172 + 2017) = 0.6741
2 · 0.7135 · 0.6741
0.7135 + 0.6741
· · ·
F1,F = 0.4222
F1,A = 0.1575
F1,O =
= 0.6932
</equation>
<figure confidence="0.917727142857143">
reference A
O F
O 4172 1482 193 Finally, we compute the average of F1,c scores
predicted F 1819 1333 214 weighted by the occurrence count N(c) of c:
A 198 133 69
c
1
</figure>
<equation confidence="0.808514666666667">
weighted F1,ERR =
weighted F1 ,ALL = E
1N(c)1:Nc · F1,c
c
1: Nc · F1
Ec:c=,4O N(c) c:c=,4O
</equation>
<page confidence="0.971309">
27
</page>
<bodyText confidence="0.99999625">
We choose F1,ERR as our primary evaluation mea-
sure because it most closely mimics the common
application of F1 scores in binary classification:
one is interested in the performance in detecting a
positive class, which in this case would be erro-
neous words. This does, however, ignore the num-
ber of correctly classified words of the OK class,
which is why we also report F1,ALL. In addition,
we follow Powers (2011) and report Matthews
Correlation Coefficient (MCC), averaged in the
same way as F1, as our secondary metric. Finally,
for contrast we also report Accuracy (ACC).
</bodyText>
<subsectionHeader confidence="0.99344">
4.3 Participants
</subsectionHeader>
<bodyText confidence="0.9989365">
Table 10 lists all participating teams. Each team
was allowed up to two submissions for each task
and language pair. In the descriptions below, par-
ticipation in specific tasks is denoted by a task
identifier: T1.1, T1.2, T1.3, and T2.
Sentence-level baseline system (T1.1, T1.2,
T1.3): QUEST is used to extract 17 system-
independent features from source and trans-
lation sentences and parallel corpora (same
features as in the WMT12 shared task):
</bodyText>
<listItem confidence="0.96836175">
• number of tokens in the source and tar-
get sentences.
• average source token length.
• average number of occurrences of the
target word within the target sentence.
• number of punctuation marks in source
and target sentences.
• language model (LM) probability of
source and target sentences based on
models for the WMT News Commen-
tary corpus.
• average number of translations per
source word in the sentence as given by
IBM Model 1 extracted from the WMT
News Commentary parallel corpus, and
thresholded so that P(t|s) &gt; 0.2, or
so that P(t|s) &gt; 0.01 weighted by the
inverse frequency of each word in the
source side of the parallel corpus.
• percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower
frequency words) and 4 (higher fre-
quency words) in the source language
extracted from the WMT News Com-
mentary corpus.
• percentage of unigrams in the source
sentence seen in the source side of the
WMT News Commentary corpus.
</listItem>
<bodyText confidence="0.997666444444445">
These features are used to train a Support
Vector Machine (SVM) regression algorithm
using a radial basis function kernel within
the SCIKIT-LEARN toolkit. The γ, E and C
parameters were optimised via grid search
with 5-fold cross validation on the training
set. We note that although the system is re-
ferred to as “baseline”, it is in fact a strong
system. It has proved robust across a range
of language pairs, MT systems, and text do-
mains for predicting various forms of post-
editing effort (Callison-Burch et al., 2012;
Bojar et al., 2013).
DCU (T1.1): DCU-MIXED and DCU-SVR use
a selection of features available in QUEST,
such as punctuation statistics, LM perplex-
ity, n-gram frequency quartile statistics and
coarse-grained POS frequency ratios, and
four additional feature types: combined POS
and stop word LM features, source-side
pseudo-reference features, inverse glass-box
features for translating the translation and er-
ror grammar parsing features. For machine
learning, the QUEST framework is expanded
to combine logistic regression and support
vector regression and to handle cross- valida-
tion and randomisation in a way that training
items with the same source side are kept to-
gether. External resources are monolingual
corpora taken from the WMT 2014 transla-
tion task for LMs, the MT system used for the
inverse glass-box features (Li et al., 2014b)
and, for error grammar parsing, the Penn-
Treebank and an error grammar derived from
it (Foster, 2007).
which for the above example gives:
</bodyText>
<equation confidence="0.490527636363636">
1
weighted F1,ALL =
weighted F1,ERR =
6189 + 2948 + 476
(6189 · 0.6932 + 2948 · 0.4222
+476 · 0.1575) = 0.5836
1
2948 + 476·
(2948 · 0.4222 + 476 · 0.1575)
= 0.3854
·
</equation>
<page confidence="0.957867">
28
</page>
<table confidence="0.71844932">
Participating team
Dublin City University Team 1, Ireland (Hokamp et al., 2014)
German Research Centre for Artificial Intelligence, Germany (Avramidis,
2014)
Fondazione Bruno Kessler, Italy, UPV Universitat Polit`ecnica de Val`encia,
Spain &amp; University of Edinburgh, UK (Camargo de Souza et al., 2014)
Laboratoire d’Informatique Grenoble, France (Luong et al., 2014)
Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur,
France (Wisniewski et al., 2014)
Multilizer, Finland
Dublin City University Team 2, Ireland (Bicici and Way, 2014)
University of Sheffield Team 1, UK (Beck et al., 2014)
University of Sheffield Team 2, UK (Scarton and Specia, 2014)
Yandex, Russia
DCU
DFKI
FBK-UPV-UEDIN
LIG
LIMSI
MULTILIZER
RTM-DCU
SHEF-lite
USHEFF
YANDEX
ID
</table>
<tableCaption confidence="0.998348">
Table 10: Participants in the WMT14 Quality Estimation shared task.
</tableCaption>
<bodyText confidence="0.998612455882353">
DFKI (T1.2): DFKI/SVR builds upon the base-
line system (above) by adding non-redundant
data from the WMT13 task for predicting
the same label (HTER) and additional fea-
tures such as (a) rule-based language cor-
rections (language tool) (b), PCFG parsing
statistics and counts of tree labels, (c) po-
sition statistics of parsing labels, (d) posi-
tion statistics of trigrams with low probabil-
ity. DFKI/SVRxdata uses a similar setting,
with the addition of more training data from
non-minimally post-edited translation out-
puts (references), filtered based on a thresh-
old on the edit distance between the MT out-
put and the freely-translated reference.
FBK-UPV-UEDIN (T1.2, T1.3, T2): The sub-
missions for the word-level task (T2) use fea-
tures extracted from word posterior probabil-
ities and confusion network descriptors com-
puted over the 100k-best hypothesis transla-
tions generated by a phrase-based SMT sys-
tem. They also use features from word lexi-
cons, and POS tags of each word for source
and translation sentences. The predictions of
the Binary model are used as a feature for the
Level 1 and Multi-class settings. Both condi-
tional random fields (CRF) and bidirectional
long short-term memory recurrent neural net-
works (BLSTM-RNNs) are used for the Bi-
nary setting, and BLSTM-RNNs only for the
Level 1 and Multi-class settings.
The sentence-level QE submissions (T1.2
and T1.3) are trained on black-box features
extracted using QUEST in addition to fea-
tures based on word alignments, word poste-
rior probabilities and diversity scores (Souza
et al., 2013). These features are computed
over 100k-best hypothesis translations also
used for task 2. In addition, a set of ratios
computed from the word-level predictions of
the model trained on the binary setting of
task 2 is used. A total of 221 features and
the extremely randomised trees (Geurts et al.,
2006) learning algorithm are used to train re-
gression models.
LIG (T2): Conditional Random Fields classi-
fiers are trained with features used in LIG’s
WMT13 systems (Luong et al., 2013): tar-
get and source words, alignment informa-
tion, source and target alignment context,
LM scores, target and source POS tags,
lexical categorisations (stopword, punctua-
tion, proper name, numerical), constituent
label, depth in the constituent tree, target
polysemy count, pseudo reference. These
are combined with novel features: word
occurrence in multiple translation systems
and POS tag-based LM scores (longest tar-
get/source n-gram length and backoff score
for POS tag). These features require external
NLP tools and resources such as: TreeTag-
ger, GIZA++, Bekerley parser, Link Gram-
mar parser, WordNet and BabelNet, Google
Translate (pseudo-reference). For the binary
task, the optimal classification threshold is
tuned based on a development set split from
the original training set. Feature selection is
employed over the all features (for the binary
</bodyText>
<page confidence="0.993537">
29
</page>
<bodyText confidence="0.99983225">
task only), with the Sequential Backward Se-
lection algorithm. The best performing fea-
ture set is then also used for the Level 1 and
Multi-class variants.
LIMSI (T2): The submission relies on a ran-
dom forest classifier and considers only 16
dense and continuous features. To prevent
sparsity issues, lexicalised information such
as the word or the previous word identities
is not included. The features considered are
mostly classic MT features and can be cat-
egorised into two classes: association fea-
tures, which describe the quality of the as-
sociation between the source sentence and
each target word, and fluency features, which
describe the ’quality’ of the translation hy-
potheses. The latter rely on different lan-
guage models (either on POS or on words)
and the former on IBM Model 1 translation
probabilities and on pseudo- references, i.e.
translation produced by an independent MT
system. Random forests are known to per-
form well in tasks like this one, in which
only a few dense and continuous features are
available, possibly because of their ability to
take into account complex interactions be-
tween features and to automatically partition
the continuous feature values into a discrete
set of intervals that achieves the best classifi-
cation performance. Since they predict the
class probabilities, it is possible to directly
optimize the F1 score during training by find-
ing, with a grid search method, the decision
threshold that achieved the best F1 score on
the training set.
MULTILIZER (T1.2, T1.3): The 80 black-box
features from QUEST are used in addition to
new features based on using other MT en-
gines for forward and backward translations.
In forward translations, the idea is that dif-
ferent MT engines make different mistakes.
Therefore, when several forward translations
are similar to each other, these translations
are more likely to be correct. This is con-
firmed by the Pearson correlation of similar-
ities between the forward translations against
the true scores (above 0.5). A backward
translation is very error-prone and therefore
it has to be used in combination with for-
ward translations. A single back-translation
similar to original source segment does not
bring much information. Instead, when sev-
eral MT engines give back-translations simi-
lar to this source segment, one can conclude
that the translation is reliable. Those transla-
tions where similarities both in forward trans-
lation and backward translation are high are
intuitively more likely to be good. A simple
feature selection method that omits all fea-
tures with Pearson correlation against the true
scores below 0.2 is used. The systems sub-
mitted are obtained using linear regression
models.
RTM-DCU (T1.1, T1.2, T1.3, T2): RTM-DCU
systems are based on referential translation
machines (RTM) (Bic¸ici, 2013) and parallel
feature decay algorithms (ParFDA5) (Bic¸ici
et al., 2014), which allow language and MT
system-independent predictions. For each
task, individual RTM models are developed
using the parallel corpora and the language
model corpora distributed by the WMT14
translation task and the language model cor-
pora provided by LDC for English and Span-
ish. RTMs use 337 to 437 sentence-level fea-
tures for coverage and diversity, IBM1 and
sentence translation performance, retrieval
closeness and minimum Bayes retrieval risk,
distributional similarity and entropy, IBM2
alignment, character n-grams, sentence read-
ability, and parse output tree structures. The
features use ngrams defined over text or com-
mon cover link (CCL) (Seginer, 2007) struc-
tures as the basic units of information over
which similarity calculations are performed.
Learning models include ridge regression
(RR), support vector machines (SVR), and
regression trees (TREE), which are applied
after partial least squares (PLS) or feature
selection (FS). For word-level prediction,
generalised linear models (GLM) (Collins,
2002) and GLM with dynamic learning
(GLMd) (Bic¸ici, 2013) are used with word-
level features including CCL links, word
length, location, prefix, suffix, form, context,
and alignment, totalling up to a couple of mil-
lion features.
SHEF-lite (T1.1, T1.2, T1.3): These submis-
sions use the framework of Multi-task Gaus-
sian Processes, where multiple datasets are
</bodyText>
<page confidence="0.991718">
30
</page>
<bodyText confidence="0.9999628">
combined in a multi-task setting similar to
the one used by Cohn and Specia (2013).
For T1.1, data for all language pairs is put
together, and each language is considered a
task. For T1.2 and T1.3, additional datasets
from previous shared task years are used,
each encoded as a different task. For all tasks,
the QUEST framework is used to extract a set
of 80 black-box features (a superset of the 17
baseline features). To cope with the large size
of the datasets, the SHEF-lite-sparse submis-
sion uses Sparse Gaussian Processes, which
provide sensible sparse approximations using
only a subset of instances (inducing inputs)
to speed up training and prediction. For this
“sparse” submission, feature selection is per-
formed following the approach of Shah et al.
(2013) by ranking features according to their
learned length-scales and selecting the top 40
features.
USHEFF (T1.1, T1.2, T1.3): USHEFF submis-
sions exploit the use of consensus among
MT systems by comparing the MT sys-
tem output to several alternative translations
generated by other MT systems (pseudo-
references). The comparison is done using
standard evaluation metrics (BLEU, TER,
METEOR, ROUGE for all tasks, and two
metrics based on syntactic similarities from
shallow and dependency parser information
for T1.2 and T1.3). Figures extracted from
such metrics are used as features to com-
plement prediction models trained on the 17
baseline features. Different from the standard
use of pseudo-reference features, these fea-
tures do not assume that the alternative MT
systems are better than the system of inter-
est. A more realistic scenario is considered
where the quality of the pseudo-references is
not known. For T1, no external systems in
addition to those provided for the shared task
are used: for a given translation, all alter-
native translations for the same source seg-
ment (two or three, depending on the lan-
guage pair) are used as pseudo-references.
For T1.2 and T1.3, for each source sentence,
all alternative translations produced by MT
systems on the same data (WMT12/13) are
used as pseudo-references. The hypothesis
is that by using translations from several MT
systems one can find consensual information
and this can smooth out the effect of “coinci-
dences” in the similarities between systems’
translations. SVM regression with radial ba-
sis function kernel and hyper-parameters op-
timised via grid search is used to build the
models.
YANDEX (T1.1): Both submissions are based
on the the 80 black-box features, plus an
LM score from a larger language model,
a pseudo-reference, and several additional
features based on POS tags and syntactic
parsers. The first attempt uses an extract
of the top 5 features selected with a greedy
search from the set of all features. SVM re-
gression is used as machine learning algo-
rithm. The second attempt uses the same
features processed with Yandex’ implemen-
tation of the gradient tree boosting (Ma-
trixNet).
</bodyText>
<subsectionHeader confidence="0.515869">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999953333333333">
In what follows we give the official results for all
tasks followed by a discussion that highlights the
main findings for each of the tasks.
</bodyText>
<subsectionHeader confidence="0.44271">
Task 1.1 Predicting post-editing effort
</subsectionHeader>
<bodyText confidence="0.98303964">
Table 11 summarises the results for the ranking
variant of Task 1.1. They are sorted from best to
worst using the DeltaAvg metric scores as primary
key and the Spearman’s rank correlation scores as
secondary key.
The winning submissions for the ranking vari-
ant of Task 1.1 are as follows: for English-Spanish
it is RTM-DCU/RTM-TREE, with a DeltaAvg
score of 0.26; for Spanish-English it is USH-
EFF, with a DeltaAvg score of 0.23; for English-
German it is again RTM-DCU/RTM-TREE, with a
DeltaAvg score of 0.39; and for German-English it
is RTM-DCU/RTM-RR, with a DeltaAvg score of
0.38. These winning submissions are better than
the baseline system by a large margin, which indi-
cates that current best performance in MT quality
estimation has reached levels that are clearly be-
yond what the baseline system can produce. As for
the other systems, according to DeltaAvg, com-
pared to the previous year results a smaller per-
centage of systems is able to beat the baseline.
This might be a consequence of the use of the met-
ric for the prediction of only three discrete labels.
The results for the scoring task are presented in
Table 12, sorted from best to worst using the MAE
</bodyText>
<page confidence="0.997648">
31
</page>
<table confidence="0.999953451612903">
System ID DeltaAvg Spearman Corr
English-Spanish
• RTM-DCU/RTM-PLS-TREE 0.26 0.38
• RTM-DCU/RTM-TREE 0.26 0.41
• YANDEX/SHAD BOOSTEDTREES2 0.23 0.35
USHEFF 0.21 0.33
SHEFF-lite 0.21 0.33
YANDEX/SHAD SVR1 0.18 0.29
SHEFF-lite-sparse 0.17 0.27
Baseline SVM 0.14 0.22
Spanish-English
• USHEFF 0.23 0.30
• RTM-DCU/RTM-PLS-RR 0.20 0.35
• RTM-DCU/RTM-FS-RR 0.19 0.36
Baseline SVM 0.12 0.21
SHEFF-lite-sparse 0.12 0.17
SHEFF-lite 0.11 0.15
English-German
• RTM-DCU/RTM-TREE 0.39 0.54
RTM-DCU/RTM-PLS-TREE 0.33 0.42
USHEFF 0.26 0.41
SHEFF-lite 0.26 0.36
Baseline SVM 0.23 0.34
SHEFF-lite-sparse 0.23 0.33
German-English
• RTM-DCU/RTM-RR 0.38 0.51
• RTM-DCU/RTM-PLS-RR 0.35 0.45
USHEFF 0.28 0.30
SHEFF-lite 0.24 0.27
Baseline SVM 0.21 0.25
SHEFF-lite-sparse 0.14 0.17
</table>
<tableCaption confidence="0.98858975">
Table 11: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.1. The winning submissions
are indicated by a •. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (1M times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
</tableCaption>
<page confidence="0.877051">
32
</page>
<table confidence="0.9999465">
System ID MAE RMSE
English-Spanish
• RTM-DCU/RTM-PLS-TREE 0.49 0.61
• SHEFF-lite 0.49 0.63
• USHEFF 0.49 0.63
• SHEFF-lite/sparse 0.49 0.69
• RTM-DCU/RTM-TREE 0.49 0.61
Baseline SVM 0.52 0.66
YANDEX/SHAD BOOSTEDTREES2 0.56 0.68
YANDEX/SHAD SVR1 0.64 0.81
DCU-Chris/SVR 0.66 0.88
DCU-Chris/MIXED 0.94 1.14
Spanish-English
• RTM-DCU/RTM-FS-RR 0.53 0.64
• SHEFF-lite/sparse 0.54 0.69
• RTM-DCU/RTM-PLS-RR 0.55 0.71
USHEFF 0.57 0.67
Baseline SVM 0.57 0.68
SHEFF-lite 0.62 0.77
DCU-Chris/MIXED 0.65 0.91
English-German
• RTM-DCU/RTM-TREE 0.58 0.68
RTM-DCU/RTM-PLS-TREE 0.60 0.71
SHEFF-lite 0.63 0.74
USHEFF 0.64 0.75
SHEFF-lite/sparse 0.64 0.75
Baseline SVM 0.64 0.76
DCU-Chris/MIXED 0.69 0.98
German-English
• RTM-DCU/RTM-RR 0.55 0.67
• RTM-DCU/RTM-PLS-RR 0.57 0.74
USHEFF 0.63 0.76
SHEFF-lite 0.65 0.77
Baseline SVM 0.65 0.78
</table>
<tableCaption confidence="0.982657">
Table 12: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.1. The winning submissions
are indicated by a •. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (1M times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
</tableCaption>
<page confidence="0.99811">
33
</page>
<bodyText confidence="0.990051415841584">
metric scores as primary key and the RMSE metric
scores as secondary key.
The winning submissions for the scoring variant
of Task 1.1 are as follows: for English-Spanish it
is RTM-DCU/RTM-TREE with a MAE of 0.49;
for Spanish-English it is RTM-DCU/RTM-FS-
RR with a MAE of 0.53; for English-German
it is again RTM-DCU/RTM-TREE, with a MAE
of 0.58; and for German-English it is RTM-
DCU/RTM-RR with a MAE of 0.55. These sub-
missions are again much better than the baseline
system, which under the scoring variant seems
to perform at a middle-of-the-pack level or lower
compared to the overall pool of submissions.
Overall, more systems are able to outperform the
baseline according to the scoring metric.
The top system for most language pairs are
essentially based on the same core techniques
(RTM-DCU) according to both the DeltaAvg and
MAE metrics. The ranking of other systems, how-
ever, can be substantially different according to the
two metrics.
Task 1.2 Predicting percentage of edits
Table 13 summarises the results for the ranking
variant of Task 1.2. For readability purposes we
have used a multiplication-factor of 100 in the
scoring script, which makes the HTER numbers
(both predicted and gold) to be in the [0, 100]
range. They are sorted from best to worst using
the DeltaAvg metric scores as primary key and the
Spearman’s rank correlation scores as secondary
key.
The winning submission for the ranking vari-
ant of Task 1.2 is RTM-DCU/RTM-SVR, with a
DeltaAvg score of 9.31. There is a large mar-
gin between this score and the baseline score of
DeltaAvg 5.08, which indicates again that current
best performance has reached levels that are much
beyond what this baseline system can produce.
The vast majority of the submissions perform bet-
ter than the baseline (the only exception is the sub-
mission from SHEFF-lite, for which the authors
report a major issue with the learning algorithm).
The results for the scoring variant are presented
in Table 14, sorted from best to worst by using the
MAE metric scores as primary key and the RMSE
metric scores as secondary key.
The winning submission for the scoring variant
of Task 1.2 is FBK-UPV-UEDIN/WP with a MAE
of 12.89, while the baseline system has a MAE
of 15.23. Most of the submissions perform better
than the baseline.
Task 1.3 Predicting post-editing time
Table 15 summarises the results for the ranking
variant of Task 1.3. For readability purposes, we
have used a multiplication-factor of 0.001 in the
scoring script, which makes the time (both pre-
dicted and gold) to be measured in seconds. They
are sorted from best to worst using the DeltaAvg
metric scores as primary key and the Spearman’s
rank correlation scores as secondary key.
The winning submission for the ranking vari-
ant of Task 1.3 is RTM-DCU/RTM-RR, with a
DeltaAvg score of 17.02 (when predicting sec-
onds). The interesting aspect of these results is
that the DeltaAvg numbers have a direct real-
world interpretation, in terms of time spent (or
saved, depending on one’s view-point) for post-
editing machine-produced translations. A more
elaborate discussion on this point can be found in
Section 4.5.
The winning submission for the scoring variant
of Task 1.3 is RTM-DCU/RTM-SVR, with a MAE
of 16.77. Note that all of the submissions perform
significantly better than the baseline, which has a
MAE of 21.49, and that the majority is not signif-
icantly worse than the top scoring submission.
Task 2 Predicting word-level edits
The results for Task 2 are summarised in Tables
17–19. The results are ordered by F1 score for
the Error (BAD) class. For comparison, two triv-
ial baselines are included, one that marks every
word as correct and that marks every word with
the most common error class found in the training
data. Both baselines are clearly useless for any ap-
plication, but help put the results in perspective.
Most teams submitted systems for a single lan-
guage pair: English-Spanish; only a single team
produced predictions for all four pairs.
Table 17 gives the results of the binary (OK vs.
BAD) classification variant of Task 2. The win-
ning submissions for this variant are as follows:
for English-Spanish it is FBK-UPV-UEDIN/RNN
with a weighted F1 of 48.73; for Spanish-
English it is RTM-DCU/RTM-GLMd with a
weighted F1 of 29.14; for English-German it is
RTM-DCU/RTM-GLM with a weighted F1 of
45.30; and for German-English it is again RTM-
DCU/RTM-GLM with a weighted F1 of 26.13.
Remarkably, for three out of four language
pairs, the systems fail to beat our trivial baseline of
</bodyText>
<page confidence="0.998143">
34
</page>
<table confidence="0.9995095">
System ID DeltaAvg Spearman Corr
English-Spanish
• RTM-DCU/RTM-SVR 9.31 0.53
• RTM-DCU/RTM-TREE 8.57 0.48
• USHEFF 7.93 0.45
SHEFF-lite/sparse 7.69 0.43
Baseline 5.08 0.31
SHEFF-lite 0.72 0.09
</table>
<tableCaption confidence="0.99759">
Table 13: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.2. The winning submissions
are indicated by a •. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (100k times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
</tableCaption>
<table confidence="0.999038153846154">
System ID MAE RMSE
English-Spanish
• FBK-UPV-UEDIN/WP 12.89 16.74
• RTM-DCU/RTM-SVR 13.40 16.69
• USHEFF 13.61 17.84
RTM-DCU/RTM-TREE 14.03 17.48
DFKI/SVR 14.32 17.74
FBK-UPV-UEDIN/NOWP 14.38 18.10
SHEFF-lite/sparse 15.04 18.38
MULTILIZER 15.04 20.86
Baseline 15.23 19.48
DFKI/SVRxdata 16.01 19.52
SHEFF-lite 18.15 23.41
</table>
<tableCaption confidence="0.92194025">
Table 14: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.2. The winning submissions
are indicated by a •. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
</tableCaption>
<table confidence="0.999815375">
System ID DeltaAvg Spearman Corr
English-Spanish
• RTM-DCU/RTM-RR 17.02 0.68
• RTM-DCU/RTM-SVR 16.60 0.67
SHEFF-lite/sparse 16.33 0.63
SHEFF-lite 16.08 0.64
USHEFF 14.98 0.59
Baseline 14.71 0.57
</table>
<tableCaption confidence="0.959284">
Table 15: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.3. The winning submissions
</tableCaption>
<bodyText confidence="0.824821666666667">
are indicated by a •. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with a 95% confidence interval. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
</bodyText>
<page confidence="0.988392">
35
</page>
<table confidence="0.998927166666666">
System ID MAE RMSE
English-Spanish
• RTM-DCU/RTM-SVR 16.77 26.17
• MULTILIZER/MLZ2 17.07 25.83
• SHEFF-lite 17.13 27.33
• MULTILIZER/MLZ1 17.31 25.51
• SHEFF-lite/sparse 17.42 27.35
• FBK-UPV-UEDIN/WP 17.48 25.31
RTM-DCU/RTM-RR 17.50 25.97
FBK-UPV-UEDIN/NOWP 18.69 26.58
USHEFF 21.48 34.28
Baseline 21.49 34.28
</table>
<tableCaption confidence="0.86462325">
Table 16: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.3. The winning submissions
are indicated by a •. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with a 95% confidence interval. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
</tableCaption>
<table confidence="0.999695481481482">
System ID weighted F1 F1 MCC ACC
All Bad T
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 64.38
Baseline (always Bad) 18.71 52.53 0.00 35.62
• FBK-UPV-UEDIN/RNN 62.00 48.73 18.23 61.62
LIMSI/RF 60.55 47.32 15.44 60.09
LIG/FS 63.55 44.47 19.41 64.67
LIG/BL ALL 63.77 44.11 19.91 65.12
FBK-UPV-UEDIN/RNN+tandem+crf 62.17 42.63 16.32 63.26
RTM-DCU/RTM-GLM 60.68 35.08 13.45 63.74
RTM-DCU/RTM-GLMd 60.24 32.89 12.98 63.97
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 82.37
Baseline (always Bad) 5.28 29.98 0.00 17.63
• RTM-DCU/RTM-GLMd 79.54 29.14 25.47 82.98
RTM-DCU/RTM-GLM 79.42 26.91 25.93 83.43
English-German
Baseline (always OK) 59.39 0.00 0.00 71.33
Baseline (always Bad) 12.78 44.57 0.00 28.67
• RTM-DCU/RTM-GLM 71.51 45.30 28.61 72.97
RTM-DCU/RTM-GLMd 68.73 36.91 21.32 71.41
German-English
Baseline (always OK) 67.82 0.00 0.00 77.60
Baseline (always Bad) 8.20 36.60 0.00 22.40
• RTM-DCU/RTM-GLM 72.41 26.13 16.08 76.14
RTM-DCU/RTM-GLMd 71.42 22.97 12.63 75.46
</table>
<tableCaption confidence="0.9737545">
Table 17: Official results for the binary part of the WMT14 Quality Evaluation Task 2. The winning submissions are indicated
by a •. All values are given as percentages.
</tableCaption>
<page confidence="0.99842">
36
</page>
<bodyText confidence="0.999964886363637">
marking all the words as wrong. This may either
indicate that the predictions themselves are of low
quality or the chosen evaluation approach is mis-
leading. On the other hand F1 scores are a com-
mon measure of binary classification performance
and no averaging is performed here.
Table 18 gives the results of the Level 1
classification (OK, Fluency, Accuracy) variant
of Task 2. Here the second baseline is to
always predict Fluency errors, as this is the
most common error category in the training
data. The winning submissions of this vari-
ant are as follows: for English-Spanish it
is FBK-UPV-UEDIN/RNN+tandem+crf with a
weighted F1 of 23.94 and for Spanish-English,
English-German, and German-English it is RTM-
DCU/RTM-GLMd with weighted F1 scores of
23.94, 21.94, and 8.57 respectively.
As before, all systems fail to outperform the
single-class baseline for the Spanish-English lan-
guage pair according to our primary metric. How-
ever, for Spanish-English and English-German
both submissions are able to beat the baseline by
large margin. We also observe that the absolute
numbers vary greatly between language pairs.
Table 19 gives the results of the Multi-class
classification variant of Task 2. Again, the sec-
ond baseline is to always predict the most common
error category in the training data, which varies
depending on language pair and produces and in-
creasingly weak baseline as the number of classes
rises.
The winning submissions of this variant are
as follows: for English-Spanish, Spanish-English,
and English-German it is RTM-DCU/RTM-GLM
with weighted F1 scores of 26.84, 8.75, and 15.02
respectively and and for German-English it is
RTM-DCU/RTM-GLMd with a weighted F1 of
3.08. Not only do these systems perform above
our baselines for all but the German-English lan-
guage pair, they also outperform all other sub-
missions for English-Spanish. Remarkably, RTM-
DCU/RTM-GLM wins English-Spanish for all of
the proposed metrics by a sizeable margin.
</bodyText>
<subsectionHeader confidence="0.80279">
4.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999898351851852">
In what follows, we discuss the main accomplish-
ments of this year’s shared task starting from the
goals we had previously identified for it.
Investigating the effectiveness of different
quality labels
For the sentence-level tasks, the results of this
year’s shared task allow us to investigate the ef-
fectiveness of predicting translation quality using
three very different quality labels: perceived post-
editing effort on a scale of [1-3] (Task 1.1); HTER
scores (Task 1.2); and the time that a translator
takes to post-edit the translation (Task 1.3). One of
the ways one can compare the effectiveness across
all these different labels is to look at how well
the models can produce predictions that correlate
with the gold label that we have at our disposal.
A measure of correlation that does not depend
on the value of the labels is Spearman’s ranking
correlation. From this perspective, the label that
seems the most effective appears to be post-editing
time (Task 1.3), with the best system (RTM-
DCU/RTM-RR) producing a Spearman’s p of 0.68
(English-Spanish translations, see Table 15). In
comparison, when perceived post-editing effort la-
bels are used (Task 1.1), the best systems achieve
a Spearman’s p of 0.38 and 0.30 for English-
Spanish and Spanish-English translations, respec-
tively, and p of 0.54 and 0.51 for English-German
and German-English, respectively (Table 11); for
HTER scores (Task 1.2) the best systems achieve
a Spearman’s p of 0.53 for English-Spanish trans-
lations (Table 13).
This comparison across tasks seems to indicate
that, among the three labels we have proposed,
post-editing time seems to be the most learnable,
in the sense that automatic predictions can vest
match the gold labels (in this case, with respect
to the rankings they induce). A possible reason
for this is that post-editing time correlates with the
length of the source sentence whereas HTER is a
normalised measure.
Compared to the results regarding time predic-
tion in the Quality Evaluation shared task from
2013 (Bojar et al., 2013), we note that this time
all submissions were able to beat the baseline sys-
tem (compared to only 1/3 of the submissions in
2013). In addition, better handling of the data
acquisition reduced the number of outliers in this
year’s dataset allowing for numbers that are more
reliably interpretable. As an example of its in-
terpretability, consider the following: the winning
submission for the ranking variant of Task 1.3 is
RTM-DCU/RTM-RR, with a a Spearman’s p of
0.68 and a DeltaAvg score of 17.02 (when predict-
</bodyText>
<page confidence="0.995698">
37
</page>
<figure confidence="0.70841654">
System ID
weighted F1
All Errors T
weighted MCC
All Errors
ACC
English-Spanish
Baseline (always OK) 50.43
14.39
Baseline (always fluency)
• FBK-UPV-UEDIN/RNN+tandem+crf 58.36
FBK-UPV-UEDIN/RNN 60.32
LIG/BL ALL 58.97
LIG/FS 58.95
RTM-DCU/RTM-GLMd 58.23
RTM-DCU/RTM-GLM 56.47
0.00 0.00 0.00 64.38
40.41 0.00 0.00 30.67
38.54 16.63 13.89 57.98
37.25 18.22 15.51 61.75
31.79 14.95 11.48 61.13
31.78 14.92 11.46 61.10
26.62 12.60 12.76 62.94
29.91 8.11 7.96 58.56
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 0.00 82.37
Baseline (always fluency) 2.67 15.13 0.00 0.00 12.24
• RTM-DCU/RTM-GLMd 78.89 23.94 25.41 25.45 83.17
RTM-DCU/RTM-GLM 78.78 21.96 26.31 26.99 83.69
English-German
Baseline (always OK) 59.39
3.83
Baseline (always fluency)
64.58
64.43
0.00
0.00
15.92
14.93
0.00 0.00
13.35 0.00
21.94 17.69
21.10 16.99
71.33
14.82
69.26
69.34
• RTM-DCU/RTM-GLMd
RTM-DCU/RTM-GLM
German-English
</figure>
<table confidence="0.95139275">
Baseline (always OK) 67.82 0.00 0.00 0.00 77.60
Baseline (always fluency) 3.34 14.92 0.00 0.00 13.79
• RTM-DCU/RTM-GLMd 69.17 8.57 10.61 5.76 75.91
RTM-DCU/RTM-GLM 69.09 8.26 9.95 5.76 75.97
</table>
<tableCaption confidence="0.921338">
Table 18: Official results for the Level
classification part of the WMT14 Quality Evaluation Task 2. The winning submissions
</tableCaption>
<figure confidence="0.914620196428571">
All values are given as
1
•.
percentages.
are indicated by a
38
System ID
weighted F1
All Errors T
weighted MCC
All Errors
ACC
English-Spanish
Baseline (always OK) 50.43
7.93
Baseline (always unintelligible)
• RTM-DCU/RTM-GLM 60.52
FBK-UPV-UEDIN/RNN+tandem+crf 52.96
LIG/BL ALL 56.66
LIG/FS 56.66
FBK-UPV-UEDIN/RNN 52.84
RTM-DCU/RTM-GLMd 51.87
0.00 0.00 0.00 64.38
22.26 0.00 0.00 21.99
26.84 23.77 21.45 66.83
23.07 15.17 10.74 52.13
20.50 18.56 13.39 60.39
20.50 18.56 13.39 60.39
17.09 7.66 4.24 57.18
3.22 10.16 4.04 64.42
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 0.00 82.37
Baseline (always word order) 0.34 1.96 0.00 0.00 4.24
• RTM-DCU/RTM-GLM 76.34 8.75 19.82 13.43 83.27
RTM-DCU/RTM-GLMd 76.21 8.19 19.35 15.32 83.17
English-German
Baseline (always OK) 59.39
2.48
slation)
63.57
63.33
0.00
0.00
15.08
13.20
0.00 0.00
8.66 0.00
15.02 17.57
12.48 18.70
71.33
11.78
70.82
71.45
• RTM-DCU/RTM-GLM
RTM-DCU/RTM-GLMd
German-English
</figure>
<table confidence="0.973188">
Baseline (always OK) 67.82 0.00 0.00 0.00 77.60
Baseline (always word order) 1.56 6.96 0.00 0.00 9.23
• RTM-DCU/RTM-GLMd 67.62 3.08 7.19 1.48 74.73
RTM-DCU/RTM-GLM 67.86 2.36 7.55 1.79 75.75
</table>
<tableCaption confidence="0.990615">
Table 19: Official results for the Multi-class classification part of the
</tableCaption>
<figure confidence="0.397179285714286">
Quality Evaluation Task 2. The winning
All values are given as perc
WMT14
•.
entages.
Baseline (always mistran
submissions are indicated by a
</figure>
<page confidence="0.993196">
39
</page>
<bodyText confidence="0.995381549019608">
ing seconds). This number has a direct real-world
interpretation: using the order proposed by this
system, a human translator would spend, on av-
erage, about 17 seconds less on a sentence taken
from the top of the ranking compared to a sen-
tence picked randomly from the set.14 To put this
number into perspective, for this dataset the av-
erage time to complete a sentence post-editing is
39 seconds. As such, one has an immediate inter-
pretation for the usefulness of using such a rank-
ing: translating around 100 sentences taken from
the top of the rankings would take around 36min
(at about 22 seconds/sentence), while translating
the same number of sentences extracted randomly
from the same dataset would take around 1h5min
(at about 39 seconds/sentence). It is in this sense
that we consider post-editing time an interpretable
label.
Another desirable property of label predictions
is usefulness; this property, however, it highly
task-dependent and therefore cannot be judged in
the absence of a specific task. For instance, an in-
terpretable label like post-editing time may not be
that useful in a task the requires one to place the
machine translations into “ready to publish” and
“not ready to publish” bins. For such an appli-
cation, labels such as the ones used by Task 1.1
are clearly more useful, and also very much inter-
pretable within the scope of the task. Our attempt
at presenting the Quality Prediction task with a va-
riety of prediction labels illustrates a good range
of properties for the proposed labels and enables
one to draw certain conclusions depending on the
needs of the specific task at hand.
For the word-level tasks, different quality labels
equate with using different levels of granularity for
the predictions, which we discuss next.
Exploring word-level quality prediction at
different levels of granularity
Previous work on word-level predictions, e.g. (Bo-
jar et al., 2013) has focused on prediction of auto-
matically derived labels, generally due to practical
considerations as the manual annotation is labour
intensive. While easily applicable, automatic an-
notations, using for example TER alignment be-
tween the machine translation and reference (or
post-edition), face the same problems as automatic
14Note that the 17.02 seconds figure is a difference in real-
time, not predicted time; what is considered in this variant of
Task 1.3 is only the predicted ranking of data points, not the
absolute values of the predictions.
MT evaluation metrics as they fail to account for
different word choices and lack the ability to re-
liably distinguish meaning preserving reorderings
from those that change the semantics of the out-
put. Furthermore, previous automatic annotation
for word-level quality estimation has focused on
binary labels: correct / incorrect, or at most, the
main edit operations that can be captured by align-
ment metrics like TER: correct, insertion, dele-
tion, substitution.
In this year’s task we were able to provide
manual fine-grained annotations at the word-level
produced by humans irrespective of references or
post-editions. Error categories range from fre-
quent ones, such as unintelligible, mistranslation,
and terminology, to rare ones such as additions or
omissions. For example, only 10 out of more than
3,400 errors in the English-Spanish test set fall
into the latter categories, while over 2,000 words
are marked as unintelligible. By hierarchically
grouping errors into coarser categories we aimed
to find a compromise between data sparsity and
the expressiveness of the labels. What marks a
good compromise depends on the use case, which
we do not specify here, and the quality of the finer
grained predictions: if a system is able to predict
even rare errors these may be grouped later if nec-
essary.
Overall, word-level error prediction seems to re-
main a challenging task as evidenced by the fact
that many submissions were unable to beat a triv-
ial baseline. We hypothesise that this is at least
partially due to a mismatch in loss-functions used
in training and testing. We know from the sys-
tem descriptions that some systems were tuned to
optimise squared error or accuracy, while evalua-
tion was performed using weighted F1 scores. On
the other hand, even a comparison of just accuracy
shows that systems struggle to obtain a lower error
rates than the “all-OK” baseline.
Such performance problems are consistent over
the three levels of granularity, contrary to the in-
tuition that binary classification would be easier.
A notable exception is the RTM-DCU/RTM-GLM
system, which is able to beat both the baseline and
all other systems on the Multi-Class variant of the
English-Spanish task – cf. Table 19 – with regard
to all metrics. For this and most other submis-
sions we observe that labels are not consistent for
different granularities, i.e. at token marked with a
specific error in the multi-class variant may still
</bodyText>
<page confidence="0.996307">
40
</page>
<bodyText confidence="0.999922323529412">
carry an “OK” label in binary annotation. Thus,
additional coarse grained annotations may be de-
rived by automatic means. For example, mapping
the multi-class predictions of the above system to
coarser categories improves the F1,ERR score in
Table 17 from 35.08 to 37.02 but does not change
the rank with respect to the other entries.
The fact that coarse grained predictions seem
not to be derived from the fine-grained ones leads
us to believe that most participants treated the
different granularities as independent classifica-
tion tasks. The FBK-UPV-UEDIN team trans-
fers information in the opposite direction by using
their binary predictions as features for Level-1 and
multi-class.
Given the current quality of word-level predic-
tion it remains unclear if these systems can already
be employed in a practical setting, e.g. to focus the
attention of post-editors.
Studying the effects of training and test
datasets with mixed domains, language pairs
and MT systems
This year’s shared task made available datasets for
more than one language pair with the same or dif-
ferent types of annotation, 2-3 multiple MT sys-
tems (plus a human translation) per language pair,
and out-of-domain test data (Tasks 1.1 and 2). In-
stances for each language pair were kept in sep-
arate datasets and thus the “language pair” vari-
able can be analysed independently. However, for
a given language pair, datasets mix translation sys-
tems (and humans) in Task 1.1, and also text do-
mains in Task 2.
Directly comparing the performance across lan-
guage pairs is not possible, given that their
datasets have different numbers of instances (pro-
duced by 3 or 4 systems) and/or different true
score distributions (see Figure 3). For a relative
comparison (although not all systems submitted
results for all language pairs, which is especially
true in Task 2), we observe in Task 1.1 that for all
language pairs generally at least half of the sys-
tems did better than the baseline. To our surprise,
only one submission combined data for multiple
languages together for Task 1.1: SHEF-lite, treat-
ing each language pair data as a different task in
a multi-task learning setting. However, only for
the ’sparse’ variant of the submission significant
gains were reported over modelling each task in-
dependently (with the tasks still sharing the same
data kernel and the same hyperparameters).
The interpretation of the results for Task 2 is
very dependent on the evaluation metric used,
but generally speaking a large variation in per-
formance was found between different languages,
with English-Spanish performing the best, possi-
bly given the much larger number of training in-
stances. Data for Task 2 also presented varied true
score distributions (as shown by the performance
of the baseline (e.g. always “OK”) in Tables 17-
19.
One of the main goals with Task 1.1 (and Task 2
to some extent) was to test the robustness of mod-
els in a blind setting where multiple MT systems
(and human translations) are put together and their
identifiers are now known. All submissions for
these tasks were therefore translation system ag-
nostic, with no submission attempting to perform
meta-identification of the origins of the transla-
tions. For Task 1.1, data from multiple MT sys-
tems was explicitly used by USHEFF though the
idea of consensus translations. Translations from
all but the system of interest for the same source
segment were used as pseudo-references. The
submission significantly outperformed the base-
line for all language pairs and did particularly well
for Spanish-English and English-Spanish.
An in depth analysis of Task 1.1’s datasets on
the difference in prediction performance between
models built and applied for individual transla-
tion systems and models built and tested for all
translations pooled together is presented in (Shah
and Specia, 2014). Not surprisingly, the former
models perform significantly better, with MAE
scores ranging between 0.35 and 0.5 for differ-
ent language pairs and MT systems, and signifi-
cantly lower scores for models trained and tested
on human translations only (MAE scores between
0.2 and 0.35 for different language pairs), against
MAE scores ranging between 0.5 and 0.65 for
models with pooled data.
For Tasks 1.2 and 1.3, two submissions included
English-Spanish data which had been produced by
yet different MT systems (SHEF-lite and DFKI).
While using these additional instances seemed at-
tractive given the small number of instances avail-
able for these tasks, it is not clear what their contri-
bution was. For example, with a reduced set of in-
stances (only 400) from the combined sets, SHEF-
lite/sparse performed significantly better than its
variant SHEF-lite.
Finally, with respect to out-of-domain (different
</bodyText>
<page confidence="0.998671">
41
</page>
<bodyText confidence="0.999877916666667">
text domain and MT system) test data, for Task
1.1, none of the papers submitted included experi-
ments. (Shah and Specia, 2014) applied the mod-
els trained on pooled datasets (as explained above)
for each language pair to the out-of-domain test
sets. The results were surprisingly positive, with
average MAE score of 0.5, compared to the 0.5-
0.65 range for in-domain data (see above). Further
analysis is necessary to understand the reasons for
that.
In Task 2, the official training and test sets al-
ready include out-of-domain data because of the
very small amount of in-domain data available,
and thus is is hard to isolate the effect of this data
on the results.
Examining the effectiveness of quality
prediction methods on human translations
Datasets for Tasks 1.1 and 2 contain human trans-
lations, in addition to the automatic translations
from various MT systems. Predicting human
translation quality is an area that has been largely
unexplored. Previous work has looked into dis-
tinguishing human from machine translations (e.g.
(Gamon et al., 2005)), but this problem setting is
somehow artificial, and moreover arguably harder
to solve nowadays given the higher general qual-
ity of current MT systems (Shah and Specia,
2014). Although human translations are obviously
of higher quality in general, many segments are
translated by MT systems with the same or similar
levels of quality as human translation. This is par-
ticularly true for Task 2, since data had been pre-
viously categorised and only “near misses” were
selected for the word-level annotation, i.e., human
and machine translations that were both nearly
perfect in this case.
While no distinction was made between human
and machine translations in our tasks, we believe
the mix of these two types of translations has had
a negative impact in prediction performance. Intu-
itively, one can expect errors in human translation
to be more subtle, and hence more difficult to cap-
ture via standard quality estimation features. For
example, an incorrect lexical choice (due to, e.g.,
ambiguity) which still fits the context and does not
make the translation ungrammatical is unlikely to
be captured. We hoped that participants would de-
sign features for this particular type of translation,
but although linguistically motivated features have
been exploited, they did not seem appropriate for
human translations.
It is interesting to mention the indirect use of
human translations by USHEFF for Tasks 1.1-1.3:
given a translation for a source segment, all other
translations for the same segment were used as
pseudo-references. Apart from when this transla-
tion was actually the human translation, the hu-
man translation was effectively used as a refer-
ence. While this reference was mixed with 2-
3 other pseudo-references (other machine transla-
tions) for the feature computations, these features
led to significant gains in performance over the
baseline features Scarton and Specia (2014).
We believe that more investigation is needed for
human translation quality prediction. Tasks ded-
icated to this type of data at both sentence- and
word-level in the next editions of this shared task
would be a possible starting point. The acquisi-
tion of such data is however much more costly, as
it is arguably hard to find examples of low quality
human translation, unless specific settings, such as
translation learner corpora, are considered.
</bodyText>
<sectionHeader confidence="0.966644" genericHeader="method">
5 Medical Translation Task
</sectionHeader>
<bodyText confidence="0.99953605">
The Medical Translation Task addresses the prob-
lem of domain-specific and genre-specific ma-
chine translation. The task is split into two sub-
tasks: summary translation, focused on transla-
tion of sentences from summaries of medical ar-
ticles, and query translation, focused on transla-
tion of queries entered by users into medical infor-
mation search engines.
In general, texts of specific domains and gen-
res are characterized by the occurrence of special
vocabulary and syntactic constructions which are
rare or even absent in traditional (general-domain)
training data and therefore difficult for MT. Spe-
cific training data (containing such vocabulary and
syntactic constructions) is usually scarce or not
available at all. Medicine, however, is an exam-
ple of a domain for which in-domain training data
(both parallel and monolingual) is publicly avail-
able in amounts which allow to train a complete
SMT system or to adapt an existing one.
</bodyText>
<subsectionHeader confidence="0.993393">
5.1 Task Description
</subsectionHeader>
<bodyText confidence="0.999602333333333">
In the Medical Translation Task, we provided links
to various medical-domain training resources and
asked participants to use the data to train or adapt
their systems to translate unseen test sets for both
subtasks between English and Czech (CS), Ger-
man (DE), and French (FR), in both directions.
</bodyText>
<page confidence="0.997929">
42
</page>
<bodyText confidence="0.999855631578947">
The summary translation test data is domain-
specific, but otherwise can be considered as ordi-
nary sentences. On the other hand, the query trans-
lation test data is also specific for its genre (gen-
eral style) – it contains short sequences of (more
or less) of independent terms rather than complete
and grammatical sentences, the usual target of cur-
rent MT systems.
Similarly to the standard Translation Task, the
participants of the Medical Translation Task were
allowed to use only the provided resources in the
constrained task (in addition to data allowed in
the constrained standard Translation Task), but
could exploit any additional resources in the un-
constrained task. The submissions were expected
with true letter casing and detokenized. The trans-
lation quality was measured using automatic eval-
uation metrics, manual evaluation was not per-
formed.
</bodyText>
<subsectionHeader confidence="0.999635">
5.2 Test and Development Data
</subsectionHeader>
<bodyText confidence="0.99984025925926">
The test and development data sets for this task
were provided by the EU FP7 project Khres-
moi.15 This projects develops a multi-lingual
multi-modal search and access system for biomed-
ical information and documents and its MT com-
ponent allows users to use non-English queries to
search in English documents and see summaries
of retrieved documents in their preferred language
(Czech, German, or French). The statistics of the
data sets are presented in Tables 20 and 21.
For the summary translation subtask, 1,000
and 500 sentences were provided for test devel-
opment purposes, respectively. The sentences
were randomly sampled from automatically gen-
erated summaries (extracts) of English documents
(web pages) containing medical information rel-
evant to 50 topics provided for the CLEF 2013
eHealth Task 3.16 Out-of-domain and ungram-
matical sentences were manually removed. The
sentences were then translated by medical experts
into Czech, German and French, and the transla-
tions were reviewed. Each sentence was provided
with the corresponding document ID and topic ID.
The set also included a description for each of the
50 topics. The data package (Khresmoi Summary
Translation Test Data 1.1) is now available from
the LINDAT/CLARIN repository17 and more de-
</bodyText>
<footnote confidence="0.9975695">
15http://khresmoi.eu/
16https://sites.google.com/site/
shareclefehealth/
17http://hdl.handle.net/11858/
</footnote>
<bodyText confidence="0.998550208333333">
tails can be found in Zdeˇnka Ureˇsov´a and Pecina
(2014).
For the query translation subtask, the main
test set contains 1,000 queries for test and 508
queries for development purposes. The original
English queries were extracted at random from
real user query logs provided by the Health on the
Net foundation18 (queries by general public) and
the Trip database19 (queries by medical experts).
Each query was translated into Czech, German,
and French by medical experts and the transla-
tions were reviewed. The data package (Khresmoi
Query Translation Test Data 1.0) is available from
the LINDAT/CLARIN repository.20
An additional test set for the query translation
subtask was adopted from the CLEF 2013 eHealth
Task 3 (Pecina et al., 2014). It contains 50 queries
constructed from titles of the test topics (originally
in English) translated into Czech, German, and
French by medical experts. The participants were
asked to translate the queries back to English and
the resulting translations were used in an informa-
tion retrieval (IR) experiment for extrinsic evalua-
tion.
</bodyText>
<subsectionHeader confidence="0.999763">
5.3 Training Data
</subsectionHeader>
<bodyText confidence="0.999526909090909">
This section reviews the in-domain resources
which were allowed for the constrained Medical
Translation Task in addition to resources for the
constrained standard Translation Task (see Section
2). Most of the corpora are available for direct
download, others can be obtained upon registra-
tion. The corpora usually employ their own, more
or less complex data format. To lower the entry
barrier, we provided a set of easy-to-use scripts to
convert the data to a plain text format suitable for
MT training.
</bodyText>
<subsectionHeader confidence="0.886837">
5.3.1 Parallel Training Data
</subsectionHeader>
<bodyText confidence="0.999908875">
The medical-domain parallel data includes the fol-
lowing corpora (see Table 22 for statistics): The
EMEA corpus (Tiedemann, 2009) contains doc-
uments from the European Medicines Agency,
automatically processed and aligned on sentence
level. It is available for many language pairs, in-
cluding those relevant to this task. UMLS is a
multilingual metathesaurus of health and biomed-
</bodyText>
<footnote confidence="0.9418828">
00-097C-0000-0023-866E-1
18http://www.hon.ch/
19http://www.tripdatabase.com/
20http://hdl.handle.net/11858/
00-097C-0000-0022-D9BF-5
</footnote>
<page confidence="0.999018">
43
</page>
<table confidence="0.995313">
sents tokens
total Czech German French English
dev 500 9,209 9,924 12,369 10,350
test 1,000 19,191 20,831 26,183 21,423
</table>
<tableCaption confidence="0.967509">
Table 20: Statistics of summary test data.
</tableCaption>
<table confidence="0.997114">
queries tokens
total general expert Czech German French English
dev 508 249 259 1,128 1,041 1,335 1,084
test 1,000 500 500 2,121 1,951 2,490 2,067
</table>
<tableCaption confidence="0.966428">
Table 21: Statistics of query test data.
</tableCaption>
<table confidence="0.999938875">
L1–L2 Czech–English DE–EN FR–EN
data set sents L1 tokens L2 tokens sents L1 tokens L2 tokens sents L1 tokens L2 tokens
EMEA 1,053 13,872 14,378 1,108 13,946 14,953 1,092 17,605 14,786
UMLS 1,441 4,248 5,579 2,001 6,613 8,153 2,171 8,505 8,524
Wiki 3 5 6 10 19 22 8 19 17
MuchMore 29 688 740
PatTr 1,848 102,418 106,727 2,201 127,098 108,665
COPPA 664 49,016 39,933
</table>
<tableCaption confidence="0.980165">
Table 22: Statistics of the in-domain parallel training data allowed for the constrained task (in thousands).
</tableCaption>
<table confidence="0.999741">
data set English Czech German French
PatTR 121,592 53,242 54,608
UMLS 7,991 63 24 37
Wiki 26,945 1,784 10,232 8,376
AACT 13,341
DrugBank 953
FMA 884
GENIA 557
GREC 62
PIL 662
</table>
<tableCaption confidence="0.9940965">
Table 23: Sizes of monolingual training data allowed for the
constrained tasks (in thousands of tokens).
</tableCaption>
<bodyText confidence="0.999962956521739">
ical vocabularies and standards (U.S. National Li-
brary of Medicine, 2009). The UMLS dataset
was constructed by selecting the concepts which
have translations in the respective languages. The
Wiki dataset contains bilingual pairs of titles of
Wikipedia articles belonging to the categories
identified to be medical-domain within the Khres-
moi project. It is available for all three lan-
guage pairs. The MuchMore Springer Corpus
is a German–English parallel corpus of medical
journals abstracts published by Springer (Buitelaar
et al., 2003). PatTR is a parallel corpus extracted
from the MAREC patent collection (W¨aschle and
Riezler, 2012). It is available for German–English
and French–English. For the medical domain,
we only consider text from patents indicated to
be from the medicine-related categories (A61,
C12N, C12P). COPPA (Corpus of Parallel Patent
Applications (Pouliquen and Mazenc, 2011) is a
French–English parallel corpus extracted from the
MAREC patent collection (W¨aschle and Riezler,
2012). The medical-domain subset is identified by
the same categories as in PatTR.
</bodyText>
<subsectionHeader confidence="0.932487">
5.3.2 Monolingual Training Data
</subsectionHeader>
<bodyText confidence="0.998981433333333">
The medical-domain monolingual data consists of
the following corpora (statistics are presented in
Table 23): The monolingual UMLS dataset con-
tains concept descriptions in CS, DE, and FR ex-
tracted from the UMLS Metathesaurus (see Sec-
tion 5.3.1). The monolingual Wiki dataset con-
sists of articles belonging to the categories iden-
tified to be medical-domain within the Khresmoi
project. The PatTR dataset contains non-parallel
data extracted from the medical patents included
in the PatTR corpus (see Section 5.3.1). AACT is a
collection of restructured and reformatted English
texts publicly available and downloadable from
ClinicalTrials.gov, containing clinical studies con-
ducted around the world. DrugBank is a bioin-
formatics and cheminformatics resource contain-
ing drug descriptions (Knox et al., 2011). GENIA
is a corpus of biomedical literature compiled and
annotated within the GENIA project (Kim et al.,
2003). FMA stands for the Foundational Model
of Anatomy Ontology, a knowledge source for
biomedical informatics concerned with symbolic
representation of the phenotypic structure of the
human body (Rosse and Mejino Jr., 2008). GREC
(Gene Regulation Event Corpus) is a semantically
annotated English corpus of abstracts of biomedi-
cal papers (Thompson et al., 2009). The PIL cor-
pus is a collection of documents giving instruc-
tions to patients about their medication (Bouayad-
Agha et al., 2000).
</bodyText>
<subsectionHeader confidence="0.981626">
5.4 Participants
</subsectionHeader>
<bodyText confidence="0.999388111111111">
A total of eight teams participated in the Medical
Translation Task by submitting their systems to at
least one subtask for one or more translation direc-
tions. A list of the participants is given in Table 24;
we provide short descriptions of their systems in
the following.
CUNI was involved in the organization of the task,
and their primary goal was to set up a baseline for
both the subtasks and for all translation directions.
</bodyText>
<page confidence="0.996911">
44
</page>
<figure confidence="0.816469">
ID
Participating team
</figure>
<affiliation confidence="0.959969125">
Charles University in Prague (Duˇsek et al., 2014)
Dublin City University (Okita et al., 2014)
Dublin City University (Zhang et al., 2014)
Laboratoire dInformatique pour la Mecanique et les Sciences de lIng´enieur (P´echeux et al., 2014)
Pohang University of Science and Technology (Li et al., 2014a)
University of Edinburgh (Durrani et al., 2014a)
University of Macau (Wang et al., 2014)
University of Macau (Lu et al., 2014)
</affiliation>
<figure confidence="0.45731725">
CUNI
DCU-Q
DCU-S
LIMSI
POSTECH
UEDIN
UM-DA
UM-WDA
</figure>
<tableCaption confidence="0.99757">
Table 24: Participants in the WMT14 Medical Translation Task.
</tableCaption>
<bodyText confidence="0.999881492957747">
Their systems are based on the Moses phrase-
based toolkit and linear interpolation of in-domain
and out-of-domain language models and phrase ta-
bles. The constrained/unconstrained systems dif-
fer in the training data only. The constrained
ones are built using all allowed training data; the
unconstrained ones take advantage of additional
web-crawled monolingual data used for training of
the language models, and additional parallel non-
medical data from the PatTr and COPPA patent
collections.
DCU-Q submitted a system designed specifically
for terminology translation in the query translation
task for EN–FR and FR–EN. This system supports
six terminology extraction methods and is able to
detect rare word pairs including zero-appearance
word pairs. It uses monotonic decoding with lat-
tice inputs, avoiding unnecessary hypothesis ex-
pansions by the reordering model.
DCU-S submitted a system to the FR–EN sum-
mary translation subtask only. The system is
similar to DCU’s system for patent translation
(phrased-based using Moses) but adapted to trans-
late medical summaries and reports.
LIMSI took part in the summary translation sub-
task for English to French.Their primary submis-
sion uses a combination of two translation sys-
tems: NCODE, based on bilingual n-gram trans-
lation models; and an on-the-fly estimation of
the parameters of Moses along with a vector
space model to perform domain adaptation. A
continuous-space language model is also used in
a post-processing step for each system.
POSTECH submitted a phrase-based SMT sys-
tem and query translation system for the DE–EN
language pair in both subtasks. They analysed
three types of query formation, generated query
translation candidates using term-to-term dictio-
naries and a phrase-based system, and then scored
them using a co-occurrence word frequency mea-
sure to select the best candidate.
UEDIN applied the Moses phrase-based system to
all language pairs and both subtasks. They used
the hierarchical reordering model and the OSM
feature, same as in UEDIN’s news translation sys-
tem, and applied compound splitting to German
input. They used separate language models built
on in-domain and out-of-domain data with linear
interpolation. For all language pairs except CS-
EN and DE-EN, they selected data for the transla-
tion model using modified Moore-Lewis filtering.
For DE-EN and CS-EN, they concatenated all the
supplied parallel training data.
UM-DA submitted systems for all language pairs
in the summary translation subtask based on a
combination of different adaptation steps, namely
domain-specific pre-processing, language model
adaptation, translation model adaptation, numeric
adaptation, and hyphenated word adaptation. Data
for the domain-adapted language and translation
models were selected using various data selection
techniques.
UM-WDA submitted systems for all language
pairs in the summary translation subtask. Their
systems are domain-adapted using web-crawled
in-domain resources: bilingual dictionaries and
monolingual data. The translation model and lan-
guage model trained on the crawled data were in-
terpolated with the best-performing language and
translation model employed in the UM-DA sys-
tems.
</bodyText>
<subsectionHeader confidence="0.557844">
5.5 Results
</subsectionHeader>
<bodyText confidence="0.999824909090909">
MT quality in the Medical Translation Task
is evaluated using automatic evaluation metrics:
BLEU (Papineni et al., 2002), TER (Snover et al.,
2006), PER (Tillmann et al., 1997), and CDER
(Leusch et al., 2006). BLEU scores are reported as
percentage and all error rates are reported as one
minus the original value, also as percentage, so
that all metrics are in the 0-100 range, and higher
scores indicate better translations.
The main reason for not conducting human
evaluation, as it happens in the standard Trans-
</bodyText>
<page confidence="0.998467">
45
</page>
<table confidence="0.998659819672132">
original normalized truecased normalized lowercased
ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER
Czech--+English
CUNI 29.64 29.79±1.07 47.45±1.15 61.64±1.06 52.18±0.98 31.68±1.14 49.84±1.10 64.38±1.06 54.10±0.96
CUNI 22.44 22.57±0.95 41.43±1.16 55.46±1.09 46.42±0.96 32.34±1.12 50.24±1.20 65.07±1.10 54.42±0.96
UEDIN 36.65 36.87±1.23 54.35±1.19 67.16±1.00 57.61±1.01 38.02±1.24 56.14±1.17 69.24±1.01 58.96±0.96
UM-DA 37.62 37.79±1.26 54.55±1.20 68.29±0.88 57.28±1.03 38.81±1.28 56.04±1.20 70.06±0.82 58.45±1.05
CUNI 22.92 23.06±0.97 42.49±1.10 56.10±1.12 47.13±0.95 33.18±1.15 51.48±1.15 66.00±1.03 55.30±0.96
CUNI 22.69 22.84±0.98 42.21±1.14 56.01±1.11 46.79±0.94 32.84±1.13 51.10±1.11 65.79±1.07 54.81±0.96
UM-WDA 37.35 37.53±1.26 54.39±1.19 68.21±0.83 57.16±1.07 38.61±1.27 55.92±1.17 70.02±0.81 58.36±1.07
ONLINE 39.57±1.21 58.24±1.14 70.16±0.78 60.04±1.02 40.62±1.23 59.72±1.11 71.94±0.74 61.26±1.01
German--+English
CUNI 28.20 28.34±1.12 46.66±1.13 61.53±1.03 50.57±0.93 30.69±1.19 48.91±1.16 64.12±1.04 52.52±0.95
CUNI 28.85 28.99±1.15 47.12±1.15 61.98±1.07 50.72±0.98 31.37±1.21 49.29±1.13 64.53±1.05 52.64±0.98
POSTECH 25.92 25.99±1.06 43.66±1.14 59.62±0.92 47.13±0.90 26.97±1.06 45.13±1.12 61.53±0.89 48.37±0.88
UEDIN 37.31 37.53±1.19 55.72±1.14 68.82±0.99 58.35±0.95 38.60±1.25 57.18±1.12 70.46±0.98 59.53±0.94
UM-DA 35.71 35.81±1.23 53.08±1.16 66.82±0.98 55.91±0.96 36.55±1.27 54.01±1.13 68.05±0.97 56.78±0.95
CUNI 30.58 30.71±1.10 48.68±1.09 63.19±1.08 52.72±0.94 33.14±1.19 50.98±1.06 65.88±1.04 54.74±0.94
CUNI 30.22 30.32±1.12 47.71±1.18 62.20±1.10 52.17±0.91 32.75±1.20 50.00±1.14 64.87±1.06 54.19±0.92
UM-WDA 32.70 32.88±1.19 49.60±1.18 63.74±1.01 53.50±0.96 33.95±1.23 51.05±1.19 65.54±0.98 54.73±0.96
ONLINE 41.18±1.24 59.33±1.09 70.95±0.92 61.92±1.01 42.29±1.23 60.76±1.08 72.51±0.88 63.06±0.96
French--+English
CUNI 34.42 34.55 ±1.20 52.24 ±1.17 64.52 ±1.03 56.48 ±0.91 36.52 ±1.23 54.35 ±1.12 67.07 ±1.00 58.34 ±0.91
CUNI 33.67 33.59 ±1.16 50.39 ±1.23 61.75 ±1.16 56.74 ±0.97 35.55 ±1.21 52.55 ±1.26 64.45 ±1.13 58.63 ±0.91
DCU-B 44.85 45.01 ±1.24 62.57 ±1.12 74.11 ±0.78 64.33 ±0.99 46.12 ±1.26 64.04 ±1.06 75.84 ±0.74 65.55 ±0.94
UEDIN 46.44 46.68 ±1.26 64.12 ±1.16 74.47 ±0.87 66.40 ±0.96 48.01 ±1.29 65.70 ±1.15 76.30 ±0.86 67.76 ±0.91
UM-DA 47.08 47.22 ±1.33 64.08 ±1.16 75.41 ±0.88 66.15 ±0.96 48.23 ±1.31 65.36 ±1.10 76.95 ±0.89 67.18 ±0.93
CUNI 34.74 34.89 ±1.12 52.39 ±1.16 63.76 ±1.09 57.29 ±0.94 36.84 ±1.17 54.56 ±1.13 66.43 ±1.07 59.14 ±0.90
CUNI 35.04 34.99 ±1.18 52.11 ±1.24 63.24 ±1.09 57.51 ±0.97 37.04 ±1.18 54.38 ±1.17 66.02 ±1.05 59.55 ±0.93
UM-WDA 43.84 44.06 ±1.32 61.14 ±1.18 73.13 ±0.87 63.09 ±1.00 45.17 ±1.36 62.63 ±1.15 74.94 ±0.84 64.37 ±0.99
ONLINE 46.99 ±1.35 64.31 ±1.12 76.07 ±0.78 66.09 ±1.00 47.99 ±1.33 65.65 ±1.07 77.65 ±0.75 67.20 ±0.96
English--+Czech
CUNI 17.36 17.65 ±0.96 37.17 ±1.02 49.13 ±0.98 40.31 ±0.95 18.75 ±0.96 38.32 ±1.02 50.82±0.91 41.39 ±0.94
CUNI 16.64 16.89 ±0.93 36.57 ±1.05 48.79 ±0.98 39.46 ±0.90 17.94 ±0.96 37.74 ±1.03 50.50±0.97 40.59 ±0.91
UEDIN 23.45 23.74 ±1.00 44.20 ±1.10 55.38 ±0.88 46.23 ±0.99 24.20 ±1.00 44.92 ±1.08 56.38±0.90 46.78 ±1.00
UM-DA 22.61 22.72 ±0.98 42.73 ±1.16 54.12 ±0.93 44.73 ±1.01 23.12 ±1.01 43.41 ±1.14 55.11±0.93 45.32 ±1.02
CUNI 20.56 20.84 ±1.01 39.98 ±1.09 51.98 ±0.99 42.86 ±1.00 22.03 ±1.05 41.19 ±1.08 53.66±0.97 43.93 ±1.01
CUNI 19.50 19.72 ±0.97 38.09 ±1.10 50.12 ±1.06 41.50 ±0.96 20.91 ±1.02 39.26 ±1.12 51.79±1.04 42.59 ±0.96
UM-WDA 22.14 22.33 ±0.96 42.30 ±1.11 53.89 ±0.92 44.48 ±1.01 22.72 ±0.97 43.02 ±1.09 54.89±0.95 45.08 ±0.99
ONLINE 33.45 ±1.28 51.64 ±1.28 61.82 ±1.10 53.97 ±1.18 34.02 ±1.31 52.35 ±1.22 62.84±1.08 54.52 ±1.18
English--+German
CUNI 12.52 12.64 ±0.77 29.84 ±0.99 45.38 ±1.14 34.69 ±0.81 16.63 ±0.91 33.63 ±1.07 50.03 ±1.24 38.43 ±0.87
CUNI 12.42 12.53 ±0.77 29.02 ±1.05 44.27 ±1.16 34.62 ±0.78 16.41 ±0.91 32.87 ±1.08 48.99 ±1.21 38.37 ±0.86
POSTECH 15.46 15.59 ±0.91 34.41 ±1.01 49.00 ±0.83 37.11 ±0.90 15.98 ±0.92 34.98 ±1.00 49.94 ±0.81 37.60 ±0.87
UEDIN 20.88 21.01 ±1.03 40.03 ±1.08 55.54 ±0.91 42.95 ±0.90 21.40 ±1.03 40.55 ±1.08 56.33 ±0.92 43.41 ±0.90
UM-DA 20.89 21.09 ±1.07 40.76 ±1.03 55.45 ±0.89 43.02 ±0.93 21.52 ±1.08 41.31 ±1.01 56.38 ±0.90 43.58 ±0.91
CUNI 14.29 14.42 ±0.81 31.82 ±1.03 47.01 ±1.13 36.81 ±0.79 18.87 ±0.90 35.76 ±1.11 51.76 ±1.17 40.65 ±0.87
CUNI 13.44 13.58 ±0.75 30.37 ±1.03 45.80 ±1.14 35.80 ±0.76 17.84 ±0.89 34.41 ±1.13 50.75 ±1.18 39.85 ±0.78
UM-WDA 18.77 18.91 ±1.00 37.92 ±1.02 53.59 ±0.85 40.90 ±0.86 19.30 ±1.02 38.42 ±1.01 54.40 ±0.85 41.34 ±0.86
ONLINE 23.92 ±1.06 44.33 ±0.97 57.47 ±0.80 46.35 ±0.91 24.29 ±1.07 44.83 ±0.98 58.20 ±0.80 46.71 ±0.92
English--+French
CUNI 30.30 30.67 ±1.11 46.59 ±1.09 59.83 ±1.04 50.51 ±0.93 32.06 ±1.12 48.01 ±1.09 61.66 ±1.00 51.83 ±0.94
CUNI 29.35 29.71 ±1.10 45.84 ±1.07 58.81 ±1.04 50.00 ±0.96 31.02 ±1.10 47.24 ±1.09 60.57 ±1.02 51.31 ±0.94
LIMSI 40.14 43.54 ±1.22 59.70 ±1.04 69.45 ±0.86 61.35 ±0.96 44.04 ±1.22 60.32 ±1.03 70.20 ±0.85 61.90 ±0.94
LIMSI 38.83 42.21 ±1.13 58.88 ±1.01 68.70 ±0.81 60.59 ±0.93 42.69 ±1.12 59.53 ±0.98 69.50 ±0.80 61.17 ±0.91
UEDIN 40.74 44.24 ±1.16 60.66 ±1.07 70.35 ±0.82 62.28 ±0.95 44.85 ±1.17 61.43 ±1.05 71.27 ±0.81 62.94 ±0.91
UM-DA 41.24 41.68 ±1.12 58.72 ±1.06 69.37 ±0.78 60.12 ±0.95 42.16 ±1.11 59.39 ±1.05 70.21 ±0.77 60.71 ±0.92
CUNI 32.23 32.61 ±1.09 48.48 ±1.08 61.13 ±1.01 52.24 ±0.93 34.08 ±1.10 49.93 ±1.11 62.92 ±0.99 53.65 ±0.92
CUNI 32.45 32.84 ±1.06 48.68 ±1.06 61.32 ±0.98 52.35 ±0.94 34.22 ±1.07 50.09 ±1.04 63.04 ±0.96 53.67 ±0.91
UM-WDA 40.78 41.16 ±1.13 58.20 ±0.99 68.93 ±0.84 59.64 ±0.94 41.79 ±1.12 59.10 ±0.96 70.01 ±0.84 60.39 ±0.91
ONLINE 58.63 ±1.26 70.70 ±1.12 78.22 ±0.81 71.89 ±0.96 59.27 ±1.26 71.50 ±1.10 79.16 ±0.81 72.63 ±0.94
</table>
<tableCaption confidence="0.995442">
Table 25: Official results of translation quality evaluation in the medical summary translation subtask.
</tableCaption>
<page confidence="0.969407">
46
</page>
<table confidence="0.999164134615385">
original normalized truecased normalized lowercased
ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER
Czech--+English
CUNI 10.71 10.57±3.42 15.72±2.77 23.37±3.03 18.68±2.42 30.13±4.85 53.38±3.01 62.53±2.84 55.44±2.87
CUNI 9.92 9.78±3.04 16.84 ±2.84 23.80 ±3.08 19.85 ±2.40 28.21 ±4.56 54.15 ±3.04 62.56 ±2.99 55.91 ±2.79
UEDIN 24.66 24.68 ±4.52 39.88 ±3.05 49.97 ±3.29 41.81 ±2.80 28.25 ±4.94 45.31 ±3.14 55.66 ±3.06 46.67 ±2.77
CUNI 12.00 11.86 ±3.42 18.49 ±2.74 24.67 ±2.85 21.08 ±2.29 31.91 ±4.81 57.61 ±3.13 65.02 ±2.99 59.24 ±2.69
CUNI 10.54 10.39 ±3.48 18.86 ±2.48 26.65 ±2.05 20.53 ±2.08 32.39 ±5.45 56.79 ±3.02 65.52 ±2.26 57.96 ±2.56
ONLINE 28.88 ±4.96 47.31 ±3.35 55.19 ±3.21 49.88 ±2.89 35.33 ±5.20 55.80 ±3.20 64.05 ±2.97 57.94 ±2.85
German--+English
CUNI 10.90 10.74 ±3.41 18.89 ±2.39 26.09 ±2.00 20.29 ±2.07 32.15 ±5.23 55.56 ±2.90 63.68 ±2.34 56.45 ±2.62
CUNI 10.71 10.55 ±3.47 18.40 ±2.35 25.45 ±2.04 19.84 ±2.07 32.06 ±5.19 54.85 ±2.91 62.87 ±2.39 55.52 ±2.61
POSTECH 18.06 17.97 ±4.38 28.57 ±3.30 40.38 ±2.77 31.79 ±2.80 21.99 ±4.65 35.76 ±3.35 47.84 ±2.82 38.84 ±2.92
POSTECH 17.99 17.88 ±4.72 29.79 ±3.04 41.15 ±2.48 32.49 ±2.63 24.41 ±4.83 41.72 ±3.19 53.33 ±2.55 44.06 ±2.88
UEDIN 23.33 23.39 ±4.37 38.55 ±3.65 48.21 ±3.43 40.75 ±3.05 27.17 ±4.63 43.87 ±3.52 53.76 ±3.48 45.72 ±3.03
CUNI 10.54 10.39 ±3.48 18.86 ±2.48 26.65 ±2.05 20.53 ±2.08 32.39 ±5.45 56.79 ±3.02 65.52 ±2.26 57.96 ±2.56
CUNI 8.75 8.49±3.60 19.10 ±2.27 24.98 ±1.95 19.95 ±2.02 30.00 ±5.59 56.07 ±2.92 62.92 ±2.32 56.27 ±2.56
ONLINE 19.97±4.46 37.03 ±3.26 43.91 ±3.22 40.95 ±2.93 33.86 ±4.87 53.28 ±3.28 60.86 ±3.22 56.33 ±2.98
French--+English
CUNI 13.90 13.79 ±3.61 18.49 ±2.55 28.35 ±2.81 20.36 ±2.20 34.97 ±5.34 59.54 ±2.94 72.30 ±2.63 58.86 ±2.76
CUNI 12.10 11.95 ±3.41 17.23 ±2.57 27.12 ±2.88 19.15 ±2.28 33.74 ±5.01 58.95 ±2.96 71.25 ±2.76 58.20 ±2.81
DCU-Q 30.85 31.24 ±5.08 58.88 ±2.97 67.94 ±2.62 59.19 ±2.62 36.88 ±5.07 66.38 ±2.85 75.86 ±2.37 66.29 ±2.55
DCU-Q 26.51 26.16 ±4.40 48.02 ±3.72 57.34 ±3.24 53.56 ±2.79 28.61 ±4.52 53.65 ±3.73 63.51 ±3.21 59.07 ±2.79
UEDIN 27.20 27.60 ±3.98 38.54 ±3.22 48.81 ±3.26 39.77 ±2.95 32.23 ±4.27 43.66 ±3.20 54.31 ±3.17 44.53 ±2.79
CUNI 14.03 14.00 ±3.30 20.11 ±2.38 29.00 ±2.71 21.62 ±2.22 38.98 ±5.08 62.90 ±2.87 74.49 ±2.45 62.12 ±2.64
CUNI 13.38 13.16 ±3.52 17.79 ±2.56 28.84 ±2.81 19.17 ±2.23 35.00 ±5.20 59.52 ±2.98 73.08 ±2.57 58.41 ±2.68
ONLINE 32.96 ±5.04 53.68 ±3.21 64.27 ±2.80 54.40 ±2.66 38.09 ±5.52 61.44 ±3.08 72.59 ±2.61 61.60 ±2.78
English--+Czech
CUNI 8.37 8.00±3.65 17.74±2.23 26.46±1.96 19.48±2.10 19.49±4.60 41.53±2.94 51.34±2.51 42.54±2.74
CUNI 9.04 8.75±3.64 18.25±2.27 26.97±1.92 19.69±2.11 21.46±5.05 42.36±3.09 51.99±2.40 43.18±2.68
UEDIN 12.57 12.40±3.61 21.15±2.96 33.56±2.80 22.30±2.67 14.06±3.80 24.92±2.90 37.85±2.72 25.58±2.70
UEDIN 6.64 6.21±4.73 -2.35±3.06 5.95±3.48 -0.97±3.12 14.35±3.52 14.51±3.19 24.96±3.50 15.11±3.10
CUNI 9.06 8.64±3.82 19.92±2.24 26.97±1.94 20.82±2.06 22.42±5.24 44.89±2.94 52.89±2.40 45.36±2.78
CUNI 8.49 8.01±6.05 18.13±2.28 25.19±1.86 19.19±2.01 21.04±4.80 42.66±2.87 50.34±2.47 43.30±2.74
ONLINE 21.09±4.60 48.56±2.82 54.72±2.51 48.30±2.83 24.37±4.80 51.93±2.74 58.10±2.50 51.62±2.80
English--+German
CUNI 10.17 10.01±3.92 26.48±3.24 36.71±3.37 29.26±2.96 13.02±4.17 31.96±3.41 42.39±3.21 34.61±2.95
CUNI 9.98 9.69±3.94 26.16±3.19 35.50±3.23 28.86±2.94 12.90±4.28 31.75±3.33 41.24±3.21 34.38±3.05
POSTECH 13.43 13.01±5.91 26.38±3.09 35.75±3.16 27.86±2.82 15.05±5.71 30.45±3.10 39.89±3.14 31.79±3.00
POSTECH 13.41 13.15±5.21 22.18±3.09 30.89±3.31 24.17±3.06 14.96±5.15 26.13±3.19 34.92±3.40 27.98±3.12
UEDIN 10.45 10.14±3.86 23.44±3.43 34.55±3.34 25.46±3.17 11.91±4.42 27.91±3.45 39.08±3.42 29.63±3.31
CUNI 8.91 7.72±6.48 30.05±3.22 40.65±2.71 31.91±2.88 13.66±5.37 35.51±3.28 46.12±2.74 37.27±3.01
CUNI 9.14 8.69±6.44 27.66±3.31 37.95±3.45 31.00±2.82 14.03±5.92 33.53±3.45 44.03±3.53 36.73±3.00
ONLINE 20.07±6.06 41.07±3.23 47.41±2.86 41.61±3.02 21.67±6.23 43.78±3.23 50.18±2.95 44.26±3.06
English--+French
CUNI 13.12 12.92±2.84 21.95±2.41 33.19±2.09 23.70±2.24 28.42±3.98 51.43±2.90 63.74±2.35 52.64±2.58
CUNI 12.80 12.65±2.81 19.16±2.61 31.61±2.21 21.91±2.32 27.52±4.05 47.47±3.08 61.43±2.37 49.82±2.72
DCU-Q 27.69 27.84±4.11 48.97±3.06 60.90±2.55 51.84±2.83 28.98±4.16 51.73±3.10 63.84±2.47 54.43±2.76
UEDIN 20.16 21.76±3.42 31.66±4.23 44.37±4.13 44.29±2.73 23.25±3.49 35.38±4.19 48.52±4.07 47.94±2.75
CUNI 13.78 13.57±3.00 21.92±2.51 33.47±2.03 24.16±2.32 30.07±4.10 51.12±3.08 63.61±2.45 52.96±2.67
CUNI 15.27 15.24±3.12 23.58±2.54 34.39±2.54 25.79±2.32 31.40±4.15 53.60±2.96 65.39±2.57 55.47±2.69
ONLINE 28.93±3.66 49.20±3.08 60.85±2.69 51.68±2.78 30.88±3.66 52.25±3.08 64.06±2.62 54.59±2.68
</table>
<tableCaption confidence="0.914332">
Table 26: Official results of translation quality evaluation in the medical query translation subtask.
</tableCaption>
<table confidence="0.999681571428572">
source lang. ID P@5 P@10 NDCG@5 NDCG@10 MAP Rprec bpref rel
Czech--+English CUNI 0.3280 0.3340 0.2873 0.2936 0.2217 0.2362 0.3473 1461
German--+English CUNI 0.2800 0.3000 0.2467 0.2630 0.2057 0.2077 0.3310 1426
French--+English CUNI 0.3280 0.3380 0.2811 0.2882 0.2206 0.2284 0.3504 1481
DCU-Q 0.3480 0.3460 0.3060 0.3072 0.2252 0.2358 0.3659 1524
UEDIN 0.4440 0.4300 0.3793 0.3826 0.2843 0.2935 0.3936 1544
English (monolingual) 0.4600 0.4700 0.4091 0.4205 0.3035 0.3198 0.3858 1638
</table>
<tableCaption confidence="0.996945">
Table 27: Official results of retrieval evaluation in the query translation subtask.
</tableCaption>
<page confidence="0.999045">
47
</page>
<bodyText confidence="0.999985944444445">
lation Task, was the lack of domain expertise of
prospective raters. While in the standard task, the
only requirement for the raters was to be a na-
tive speaker of the target language, in the Med-
ical Translation Task, a very good knowledge of
the domain would be necessary to provide reli-
able judgements and the raters with such an ex-
pertise (medical doctors and native speakers) were
not available.
The complete results of the task are presented
in Table 25 (for summary translation) and Ta-
bles 26 and 27 (for query translation). Partici-
pant IDs given in bold indicate primary submis-
sions, IDs in normal font refer to contrastive sub-
missions. The first section for each translation di-
rection (white background) refers to constrained
submissions and the second one (light-gray back-
ground) to unconstrained submissions. The col-
umn denoted as “original” contains BLEU scores
as reported by the Matrix submission system ob-
tained on the original submitted translations. Due
to punctuation inconsistency in the original refer-
ence translations, we decided to perform punctu-
ation normalization before calculating the official
scores. The columns denoted as “normalized true-
cased” contain scores obtained on the submitted
translations after punctuation normalization and
the columns denoted as “normalized lowercased”
contain scores obtained after punctuation normal-
ization and lowercasing. The normalization script
is available in the package with summary transla-
tion test data. The confidence intervals were ob-
tained by bootstrap resampling with a confidence
level of 95%. Figures in bold denote the best con-
strained system and, if its score is higher, the best
unconstrained system for each translation direc-
tion and each metric. For comparison, we also
present results of a major on-line translation sys-
tem (denoted as ONLINE).
The results of the extrinsic evaluation of query
translation submissions are given in 27. We used
the CLEF 2013 eHealth Task 3 test collection con-
taining about 1 million web pages (in English),
50 test queries (originally in English and trans-
lated to Czech, German, and French), and their
relevance assessments. Some of the participants
of the WMT Medical Task (three teams with five
submissions in total) submitted translations of the
queries (from Czech, German, and French) into
English and these translations were used to query
the CLEF 2013 eHealth Task 3 test collection us-
ing a state-of-the-art system based on a BM25
model, described in Pecina et al. (2014). Origi-
nally, we asked for 10 best translations for each
query, but only the best one were used for the
evaluation. The results are provided in terms of
standard IR evaluation measures: precision at a
cut-off of 5 and 10 documents (P@5, P@10),
normalized discounted cumulative gain (J¨arvelin
and Kek¨al¨ainen, 2002) at 5 and 10 documents
(NDCG@5, NDCG@10), mean average precision
(MAP) (Voorhees and Harman, 2005), precision
reached after R documents retrieved, where R in-
dicates the number of the relevant documents for
each query in the entire collection (Rprec), binary
preference (bpref) (Buckley and Voorhees, 2004),
and number or relevant documents retrieved (rel).
The cross-lingual results are also compared with
the monolingual one (obtained by using the refer-
ence (English) translations of the test topics) to see
how the system would perform if the queries were
translated perfectly.
</bodyText>
<subsectionHeader confidence="0.923258">
5.6 Discussion and Conclusion
</subsectionHeader>
<bodyText confidence="0.999676464285714">
Both the subtasks turned out to be quite challeng-
ing not only because of the specific domain – in
summary sentences, we can observe much higher
density of terminology than in ordinary sentences;
the queries, which are also rich in terminology, do
not form sentences at all.
Most submissions were based on systems par-
ticipating in the standard Translation Task and
trained on the provided data or its subsets CUNI
provided baseline systems for all language pairs in
both subtasks, which turned to be relatively strong
for the query translation task, especially in trans-
lation to English, but only in terms of scores ob-
tained on normalized and lowercased translations
since their truecasing component did not perform
well.
In the summary translation subtask, the best
overall results were achieved by the UEDIN team
which won for DE–EN, EN–CS, and EN–FR, fol-
lowed by the UM-DA team, which performed on
par with UEDIN in all other translation.
The unconstrained submissions in almost all
cases did not outperform the results of the con-
strained submissions. Some improvements were
observed in the query translations subtasks by the
CUNI’s unconstrained system with language mod-
els trained on larger in-domain data.
The ONLINE system outperforms all other sub-
</bodyText>
<page confidence="0.997265">
48
</page>
<bodyText confidence="0.9997621">
missions with only two exceptions – the UM-DA’s
and UEDIN’s systems for the summary translation
in the FR–EN direction, though the score differ-
ences are within the 95% confidence interval.
In the query translation subtask, DCU-Q built
a system designed specifically for terminology
translation between French and English and out-
performed all other participants in translation into
English; however, the confidence intervals in the
query translation task are much wider and most of
the differences in scores of the automatic metrics
are not statistically significant.
The extrinsic evaluation in the cross-lingual in-
formation retrieval was conducted for translations
into English only. CUNI provided the baselines
for all directions, but other submissions were done
for FR–EN only. Here, the winner is UEDIN, who
outperformed both CUNI and DCU-Q, and their
scores are very close to those obtained using the
reference English translations.
</bodyText>
<sectionHeader confidence="0.997235" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999847125">
This work was supported in parts by the
MosesCore, Casmacat, Khresmoi, Matecat and
QTLaunchPad projects funded by the European
Commission (7th Framework Programme), and by
gifts from Yandex.
We would also like to thank our colleagues Ma-
touˇs Mach´aˇcek and Martin Popel for detailed dis-
cussions.
</bodyText>
<sectionHeader confidence="0.988846" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.961908343283582">
Avramidis, E. (2014). Efforts on machine learning
over human-mediated translation edit rate. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Beck, D., Shah, K., and Specia, L. (2014). Shef-
lite 2.0: Sparse multi-task gaussian processes
for translation quality estimation. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Bic¸ici, E. (2013). Referential translation machines
for quality estimation. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, Sofia, Bulgaria.
Bic¸ici, E., Liu, Q., and Way, A. (2014). Parallel
FDA5 for fast deployment of accurate statisti-
cal machine translation systems. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, USA. Association
for Computational Linguistics.
Bicici, E., Liu, Q., and Way, A. (2014). Parallel
fda5 for fast deployment of accurate statistical
machine translation systems. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Bicici, E. and Way, A. (2014). Referential transla-
tion machines for predicting translation quality.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Bojar, O., Buck, C., Callison-Burch, C., Feder-
mann, C., Haddow, B., Koehn, P., Monz, C.,
Post, M., Soricut, R., and Specia, L. (2013).
Findings of the 2013 Workshop on Statistical
Machine Translation. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 1–42, Sofia, Bulgaria. Association
for Computational Linguistics.
Bojar, O., Diatka, V., Rychl´y, P., Straˇn´ak, P.,
Tamchyna, A., and Zeman, D. (2014). Hindi-
English and Hindi-only Corpus for Machine
Translation. In Proceedings of the Ninth Inter-
national Language Resources and Evaluation
Conference, Reykjavik, Iceland. ELRA.
Bojar, O., Ercegovˇcevi´c, M., Popel, M., and
Zaidan, O. (2011). A grain of salt for the WMT
manual evaluation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation,
pages 1–11, Edinburgh, Scotland. Association
for Computational Linguistics.
Borisov, A. and Galinskaya, I. (2014). Yandex
school of data analysis russian-english machine
translation system for wmt14. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Bouayad-Agha, N., Scott, D. R., and Power, R.
(2000). Integrating content and style in doc-
uments: A case study of patient information
leaflets. Information Design Journal, 9(2–
3):161–176.
Buckley, C. and Voorhees, E. M. (2004). Re-
trieval evaluation with incomplete information.
</reference>
<page confidence="0.99713">
49
</page>
<reference confidence="0.919856809523809">
In Proceedings of the 27th Annual International
ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 25–
32, Sheffield, United Kingdom.
Buitelaar, P., Sacaleanu, B., ˇSpela Vintar, Stef-
fen, D., Volk, M., Dejean, H., Gaussier, E.,
Widdows, D., Weiser, O., and Frederking, R.
(2003). Multilingual concept hierarchies for
medical information organization and retrieval.
Public deliverable, MuchMore project.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2007). (Meta-) evaluation
of machine translation. In Proceedings of the
Second Workshop on Statistical Machine Trans-
lation (WMT07), Prague, Czech Republic.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2008). Further meta-
evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Ma-
chine Translation (WMT08), Colmbus, Ohio.
Callison-Burch, C., Koehn, P., Monz, C., Pe-
terson, K., Przybocki, M., and Zaidan, O. F.
(2010). Findings of the 2010 joint workshop
on statistical machine translation and metrics
for machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
lation (WMT10), Uppsala, Sweden.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of
the 2012 workshop on statistical machine trans-
lation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 10–
51, Montr´eal, Canada. Association for Compu-
tational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., and
Schroeder, J. (2009). Findings of the 2009
workshop on statistical machine translation. In
Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT09), Athens,
Greece.
Callison-Burch, C., Koehn, P., Monz, C., and
Zaidan, O. (2011). Findings of the 2011 work-
shop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22–64, Edinburgh,
Scotland.
Camargo de Souza, J. G., Gonz´alez-Rubio, J.,
Buck, C., Turchi, M., and Negri, M. (2014).
Fbk-upv-uedin participation in the wmt14 qual-
ity estimation shared-task. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Cap, F., Weller, M., Ramm, A., and Fraser, A.
(2014). Cims – the cis and ims joint submis-
sion to wmt 2014 translating from english into
german. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Cohen, J. (1960). A coefficient of agreement for
nominal scales. Educational and Psychological
Measurment, 20(1):37–46.
</reference>
<bodyText confidence="0.97315985">
Cohn, T. and Specia, L. (2013). Modelling an-
notator bias with multi-task gaussian processes:
An application to machine translation quality
estimation. In Proceedings of the 51st An-
nual Meeting of the Association for Compu-
tational Linguistics, ACL-2013, pages 32–42,
Sofia, Bulgaria.
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: theory and ex-
periments with perceptron algorithms. In Pro-
ceedings of the ACL-02 conference on Empir-
ical methods in natural language processing -
Volume 10, EMNLP ’02, pages 1–8, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Costa-juss`a, M. R., Gupta, P., Rosso, P., and
Banchs, R. E. (2014). English-to-hindi sys-
tem description for wmt 2014: Deep source-
context features for moses. In Proceedings
of the Ninth Workshop on Statistical Machine
</bodyText>
<reference confidence="0.874174125">
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Do, Q. K., Herrmann, T., Niehues, J., Allauzen,
A., Yvon, F., and Waibel, A. (2014). The
kit-limsi translation system for wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Dungarwal, P., Chatterjee, R., Mishra, A.,
Kunchukuttan, A., Shah, R., and Bhattacharyya,
P. (2014). The iit bombay hindi-english transla-
tion system at wmt 2014. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
</reference>
<page confidence="0.979059">
50
</page>
<reference confidence="0.998925653061225">
Durrani, N., Haddow, B., Koehn, P., and Heafield,
K. (2014a). Edinburgh’s phrase-based machine
translation systems for wmt-14. In Proceedings
of the ACL 2014 Ninth Workshop of Statistical
Machine Translation, Baltimore, USA.
Durrani, N., Haddow, B., Koehn, P., and Heafield,
K. (2014b). Edinburghs phrase-based machine
translation systems for wmt-14. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Duˇsek, O., Hajiˇc, J., Hlav´aˇcov´a, J., Nov´ak, M.,
Pecina, P., Rosa, R., Tamchyna, A., Ureˇsov´a,
Z., and Zeman, D. (2014). Machine transla-
tion of medical texts in the khresmoi project. In
Proceedings of the ACL 2014 Ninth Workshop
of Statistical Machine Translation, Baltimore,
USA.
Federmann, C. (2012). Appraise: An Open-
Source Toolkit for Manual Evaluation of Ma-
chine Translation Output. The Prague Bulletin
of Mathematical Linguistics (PBML), 98:25–
35.
Foster, J. (2007). Treebanks gone bad: Parser eval-
uation and retraining using a treebank of un-
grammatical sentences. International Journal
on Document Analysis and Recognition, 10(3-
4):129–145.
Freitag, M., Peitz, S., Wuebker, J., Ney, H., Huck,
M., Sennrich, R., Durrani, N., Nadejde, M.,
Williams, P., Koehn, P., Herrmann, T., Cho,
E., and Waibel, A. (2014). Eu-bridge mt:
Combined machine translation. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Gamon, M., Aue, A., and Smets, M. (2005).
Sentence-level MT evaluation without reference
translations: beyond language modeling. In
Proceedings of the Annual Conference of the
European Association for Machine Translation,
Budapest.
Geurts, P., Ernst, D., and Wehenkel, L. (2006). Ex-
tremely randomized trees. Machine Learning,
63(1):3–42.
Green, S., Cer, D., and Manning, C. (2014).
Phrasal: A toolkit for new directions in statis-
tical machine translation. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Hardmeier, C., Stymne, S., Tiedemann, J., Smith,
A., and Nivre, J. (2014). Anaphora models and
reordering for phrase-based smt. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Herbrich, R., Minka, T., and Graepel, T. (2006).
TrueSkillTM: A Bayesian Skill Rating Sys-
tem. In Proceedings of the Twentieth Annual
Conference on Neural Information Processing
Systems, pages 569–576, Vancouver, British
Columbia, Canada. MIT Press.
Herrmann, T., Mediani, M., Cho, E., Ha, T.-L.,
Niehues, J., Slawik, I., Zhang, Y., and Waibel,
A. (2014). The karlsruhe institute of technol-
ogy translation systems for the wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Hokamp, C., Calixto, I., Wagner, J., and Zhang,
J. (2014). Target-centric features for transla-
tion quality estimation. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Hopkins, M. and May, J. (2013). Models of trans-
lation competitions. In Proceedings of the 51st
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 1416–1424, Sofia, Bulgaria.
J¨arvelin, K. and Kek¨al¨ainen, J. (2002). Cumu-
lated gain-based evaluation of ir techniques.
ACM Transactions on Information Systems,
20(4):422–446.
Kim, J.-D., Ohta, T., Tateisi, Y., and Tsujii, J.
(2003). GENIA corpus – a semantically anno-
tated corpus for bio-textmining. Bioinformatics,
19(suppl 1):i180–i182.
Knox, C., Law, V., Jewison, T., Liu, P., Ly,
S., Frolkis, A., Pon, A., Banco, K., Mak, C.,
Neveu, V., Djoumbou, Y., Eisner, R., Guo,
A. C., and Wishart, D. S. (2011). DrugBank 3.0:
a comprehensive resource for Omics research
on drugs. Nucleic acids research, 39(suppl
1):D1035–D1041.
Koehn, P. (2012a). Simulating human judgment in
</reference>
<page confidence="0.980204">
51
</page>
<reference confidence="0.999780010204081">
machine translation evaluation campaigns. In
International Workshop on Spoken Language
Translation (IWSLT).
Koehn, P. (2012b). Simulating Human Judgment
in Machine Translation Evaluation Campaigns.
In Proceedings of the Ninth International Work-
shop on Spoken Language Translation, pages
179–184, Hong Kong, China.
Koehn, P. and Monz, C. (2006). Manual and au-
tomatic evaluation of machine translation be-
tween European languages. In Proceedings of
NAACL 2006 Workshop on Statistical Machine
Translation, New York, New York.
Koppel, M. and Ordan, N. (2011). Translationese
and its dialects. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Techolo-
gies, pages 1318–1326, Portland, Oregon.
Landis, J. R. and Koch, G. G. (1977). The mea-
surement of observer agreement for categorical
data. Biometrics, 33:159–174.
Leusch, G., Ueffing, N., and Ney, H. (2006). Cder:
Efficient mt evaluation using block movements.
In Proceedings of the 11th Conference of the
European Chapter of the Association for Com-
putational Linguistics, pages 241–248, Trento,
Italy.
Li, J., Kim, S.-J., Na, H., and Lee, J.-H. (2014a).
Postech’s system description for medical text
translation task. In Proceedings of the ACL
2014 Ninth Workshop of Statistical Machine
Translation, Baltimore, USA.
Li, L., Wu, X., Vaillo, S. C., Xie, J., Way, A., and
Liu, Q. (2014b). The dcu-ictcas mt system at
wmt 2014 on german-english translation task.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Lopez, A. (2012). Putting Human Assessments of
Machine Translation Systems in Order. In Pro-
ceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 1–9, Montr´eal,
Canada. Association for Computational Lin-
guistics.
Lu, Y., Wang, L., Wong, D. F., Chao, L. S., Wang,
Y., and Oliveira, F. (2014). Domain adapta-
tion for medical text translation using web re-
sources. In Proceedings of the ACL 2014 Ninth
Workshop of Statistical Machine Translation,
Baltimore, USA.
Luong, N. Q., Besacier, L., and Lecouteux, B.
(2014). Lig system for word level qe task at
wmt14. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Luong, N. Q., Lecouteux, B., and Besacier, L.
(2013). LIG system for WMT13 QE task: In-
vestigating the usefulness of features in word
confidence estimation for MT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 384–389, Sofia, Bulgaria.
Association for Computational Linguistics.
Mach´aˇcek, M. and Bojar, O. (2014). Results of
the wmt14 metrics shared task. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Matthews, A., Ammar, W., Bhatia, A., Feely, W.,
Hanneman, G., Schlinger, E., Swayamdipta, S.,
Tsvetkov, Y., Lavie, A., and Dyer, C. (2014).
The cmu machine translation systems at wmt
2014. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Neidert, J., Schuster, S., Green, S., Heafield, K.,
and Manning, C. (2014). Stanford universitys
submissions to the wmt 2014 translation task. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Okita, T., Vahid, A. H., Way, A., and Liu, Q.
(2014). Dcu terminology translation system for
medical query subtask at wmt14. In Proceed-
ings of the ACL 2014 Ninth Workshop of Statis-
tical Machine Translation, Baltimore, USA.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a method for automatic eval-
uation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 311–318,
Philadelphia, PA, USA. Association for Com-
putational Linguistics.
P´echeux, N., Gong, L., Do, Q. K., Marie, B.,
Ivanishcheva, Y., Allauzen, A., Lavergne, T.,
</reference>
<page confidence="0.973627">
52
</page>
<reference confidence="0.990537846938776">
Niehues, J., Max, A., and Yvon, Y. (2014).
LIMSI @ WMT’14 Medical Translation Task.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, USA.
Pecina, P., Duˇsek, O., Goeuriot, L., Hajiˇc, J.,
Hlav´aˇcov´a, J., Jones, G., Kelly, L., Leveling, J.,
Mareˇcek, D., Nov´ak, M., Popel, M., Rosa, R.,
Tamchyna, A., and Ureˇsov´a, Z. (2014). Adapta-
tion of machine translation for multilingual in-
formation retrieval in the medical domain. Arti-
ficial Intelligence in Medicine, (0):–.
Peitz, S., Wuebker, J., Freitag, M., and Ney, H.
(2014). The rwth aachen german-english ma-
chine translation system for wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Pouliquen, B. and Mazenc, C. (2011). COPPA,
CLIR and TAPTA: three tools to assist in over-
coming the patent barrier at WIPO. In Pro-
ceedings of the Thirteenth Machine Translation
Summit, pages 24–30, Xiamen, China. Asia-
Pacific Association for Machine Translation.
Powers, D. M. W. (2011). Evaluation: from preci-
sion, recall and f-measure to roc, informedness,
markedness &amp; correlation. Journal of Machine
Learning Technologies.
Quernheim, D. and Cap, F. (2014). Large-scale ex-
act decoding: The ims-ttt submission to wmt14.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Rosse, C. and Mejino Jr., J. L. V. (2008). The
foundational model of anatomy ontology. In
Burger, A., Davidson, D., and Baldock, R., ed-
itors, Anatomy Ontologies for Bioinformatics,
volume 6 of Computational Biology, pages 59–
117. Springer London.
Rubino, R., Toral, A., S´anchez-Cartagena, V. M.,
Ferr´andez-Tordera, J., Ortiz Rojas, S., Ramirez-
S´anchez, G., S´anchez-Martinez, F., and Way,
A. (2014). Abu-matran at wmt 2014 transla-
tion task: Two-step data selection and rbmt-
style synthetic rules. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Sakaguchi, K., Post, M., and Van Durme, B.
(2014). Efficient elicitation of annotations for
human evaluation of machine translation. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland.
S´anchez-Cartagena, V. M., P´erez-Ortiz, J. A., and
S´anchez-Martinez, F. (2014). The ua-prompsit
hybrid machine translation system for the 2014
workshop on statistical machine translation. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Scarton, C. and Specia, L. (2014). Exploring con-
sensus in machine translation for quality esti-
mation. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Schwartz, L., Anderson, T., Gwinnup, J., and
Young, K. (2014). Machine translation and
monolingual postediting: The afrl wmt-14 sys-
tem. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Seginer, Y. (2007). Learning Syntactic Structure.
PhD thesis, University of Amsterdam.
Shah, K., Cohn, T., and Specia, L. (2013). An
investigation on the effectiveness of features for
translation quality estimation. In Proceedings
of the Machine Translation Summit XIV, pages
167–174, Nice, France.
Shah, K. and Specia, L. (2014). Quality estimation
for translation selection. In Proceedings of the
17th Annual Conference of the European As-
sociation for Machine Translation, Dubrovnik,
Croatia.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference
of the Association for Machine Translation in
the Americas (AMTA-2006), Cambridge, Mas-
sachusetts.
Souza, J. G. C. d., Espl-Gomis, M., Turchi, M.,
and Negri, M. (2013). Exploiting qualitative in-
formation from automatic word alignment for
cross-lingual nlp tasks. In The 51st Annual
</reference>
<page confidence="0.98764">
53
</page>
<reference confidence="0.999737252747253">
Meeting of the Association for Computational
Linguistics - Short Papers (ACL Short Papers
2013).
Specia, L., Shah, K., de Souza, J. G. C., and Cohn,
T. (2013). QuEst - A Translation Quality Esti-
mation Framework. In Proceedings of the 51th
Conference of the Association for Computa-
tional Linguistics (ACL), Demo Session, Sofia,
Bulgaria.
Tamchyna, A., Popel, M., Rosa, R., and Bojar, O.
(2014). Cuni in wmt14: Chimera still awaits
bellerophon. In Proceedings of the Ninth Work-
shop on Statistical Machine Translation, Balti-
more, Maryland, USA. Association for Compu-
tational Linguistics.
Tan, L. and Pal, S. (2014). Manawi: Using
multi-word expressions and named entities to
improve machine translation. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Thompson, P., Iqbal, S., McNaught, J., and Ana-
niadou, S. (2009). Construction of an annotated
corpus to support biomedical information ex-
traction. BMC bioinformatics, 10(1):349.
Tiedemann, J. (2009). News from OPUS – a
collection of multilingual parallel corpora with
tools and interfaces. In Recent Advances in
Natural Language Processing, volume 5, pages
237–248, Borovets, Bulgaria. John Benjamins.
Tillmann, C., Vogel, S., Ney, H., Zubiaga, A.,
and Sawaf, H. (1997). Accelerated DP based
search for statistical translation. In Kokki-
nakis, G., Fakotakis, N., and Dermatas, E., edi-
tors, Proceedings of the Fifth European Confer-
ence on Speech Communication and Technol-
ogy, pages 2667–2670, Rhodes, Greece. Inter-
national Speech Communication Association.
U.S. National Library of Medicine (2009). UMLS
reference manual. Metathesaurus. Bethesda,
MD, USA.
Voorhees, E. M. and Harman, D. K., editors
(2005). TREC: Experiment and evaluation in
information retrieval, volume 63 of Digital li-
braries and electronic publishing series. MIT
press Cambridge, Cambridge, MA, USA.
Wang, L., Lu, Y., Wong, D. F., Chao, L. S., Wang,
Y., and Oliveira., F. (2014). Combining domain
adaptation approaches for medical text transla-
tion. In Proceedings of the ACL 2014 Ninth
Workshop of Statistical Machine Translation,
Baltimore, USA.
W¨aschle, K. and Riezler, S. (2012). Analyz-
ing parallelism and domain similarities in the
MAREC patent corpus. In Salampasis, M. and
Larsen, B., editors, Multidisciplinary Informa-
tion Retrieval, volume 7356 of Lecture Notes
in Computer Science, pages 12–27. Springer
Berlin Heidelberg.
Williams, P., Sennrich, R., Nadejde, M., Huck, M.,
Hasler, E., and Koehn, P. (2014). Edinburghs
syntax-based systems at wmt 2014. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Wisniewski, G., P´echeux, N., Allauzen, A., and
Yvon, F. (2014). Limsi submission for wmt’14
qe task. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
wu, x., Haque, R., Okita, T., Arora, P., Way, A.,
and Liu, Q. (2014). Dcu-lingo24 participation
in wmt 2014 hindi-english translation task. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Zdeˇnka Ureˇsov´a, Ondˇrej Duˇsek, J. H. and Pecina,
P. (2014). Multilingual test sets for machine
translation of search queries for cross-lingual
information retrieval in the medical domain. In
To appear in Proceedings of the Ninth Interna-
tional Conference on Language Resources and
Evaluation, Reykjavik, Iceland.
Zhang, J., Wu, X., Calixto, I., Vahid, A. H., Zhang,
X., Way, A., and Liu, Q. (2014). Experiments in
medical translation shared task at wmt 2014. In
Proceedings of the ACL 2014 Ninth Workshop
of Statistical Machine Translation, Baltimore,
USA.
</reference>
<page confidence="0.999242">
54
</page>
<sectionHeader confidence="0.700289" genericHeader="method">
A Pairwise System Comparisons by Human Judges
</sectionHeader>
<bodyText confidence="0.9529609">
Tables 28–37 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables’ cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row, ignoring ties. Bolding indicates the winner of the two systems.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables * indicates sta-
tistical significance at p G 0.10, † indicates statistical significance at p G 0.05, and ‡ indicates statistical
significance at p G 0.01, according to the Sign Test.
Each table contains final rows showing how likely a system would win when paired against a randomly
selected system (the expected win ratio score) and the rank range according the official method used in
</bodyText>
<tableCaption confidence="0.963566">
Table 8. Gray lines separate clusters based on non-overlapping rank ranges.
</tableCaption>
<table confidence="0.999778555555556">
ONLINE-B UEDIN-PHRASE UEDIN-SYNTAX CU-MOSES
ONLINE-A
ONLINE-B – .47$ .43$ .42$ .39$
UEDIN-PHRASE .53$ – .44$ .44$ .41$
UEDIN-SYNTAX .57$ .56$ – .49 .48†
ONLINE-A .58$ .56$ .51 – .48*
CU-MOSES .61$ .59$ .52† .52* –
score .57 .54 .47 .46 .44
rank 1 2 3-4 3-4 5
</table>
<tableCaption confidence="0.989487">
Table 28: Head to head comparison, ignoring ties, for Czech-English systems
</tableCaption>
<table confidence="0.9996488125">
CU-DEPFIX ONLINE-B ONLINE-A CU-TECTO COMMERCIAL1 COMMERCIAL2
UEDIN-UNCNSTR UEDIN-PHRASE
CU-BOJAR
CU-FUNKY
CU-DEPFIX – .50 .42$ .48 .44$ .43$ .41$ .35$ .30$ .24$
UEDIN-UNCNSTR .50 – .51 .48 .42$ .37$ .42$ .39$ .31$ .26$
CU-BOJAR .58$ .49 – .49 .45$ .44$ .40$ .36$ .32$ .24$
CU-FUNKY .52 .52 .51 – .48 .47† .44$ .34$ .33$ .26$
ONLINE-B .56$ .58$ .55$ .52 – .48 .47† .41$ .31$ .26$
UEDIN-PHRASE .57$ .63$ .56$ .53† .52 – .48 .44$ .32$ .27$
ONLINE-A .59$ .58$ .60$ .56$ .53† .52 – .45$ .37$ .30$
CU-TECTO .65$ .61$ .64$ .66$ .59$ .56$ .55$ – .42$ .30$
COMMERCIAL1 .70$ .69$ .68$ .67$ .69$ .68$ .63$ .58$ – .40$
COMMERCIAL2 .76$ .74$ .76$ .74$ .74$ .73$ .70$ .70$ .60$ –
score .60 .59 .58 .57 .54 .52 .50 .44 .36 .28
rank 1-3 1-3 1-4 3-4 5-6 5-6 7 8 9 10
</table>
<tableCaption confidence="0.998919">
Table 29: Head to head comparison, ignoring ties, for English-Czech systems
</tableCaption>
<page confidence="0.965061">
55
</page>
<table confidence="0.998953388888889">
ONLINE-B UEDIN-SYNTAX LIMSI-KIT KIT DCU-ICTCAS RBMT1 ONLINE-C
ONLINE-A EU-BRIDGE RWTH CMU
UEDIN-PHRASE RBMT4
ONLINE-B – .46 .40$ .41$ .35$ .42$ .38$ .35$ .40$ .31$ .33$ .32$ .22$
UEDIN-SYNTAX .54 – .51 .47 .47 .45 .45* .39$ .36$ .38$ .35$ .34$ .27$
ONLINE-A .60$ .49 – .42† .44† .51 .41$ .38$ .44* .42$ .38$ .31$ .20$
LIMSI-KIT .59$ .53 .58† – .55 .53 .31$ .45* .39$ .41$ .37$ .35$ .29$
EU-BRIDGE .65$ .53 .56† .45 – .45 .44* .48 .40$ .37$ .39$ .37$ .30$
UEDIN-PHRASE .58$ .55 .49 .47 .55 – .48 .39$ .34$ .45* .40$ .40$ .34$
KIT .62$ .55* .59$ .69$ .56* .52 – .45* .41$ .45* .47 .40$ .31$
RWTH .65$ .61$ .62$ .55* .52 .61$ .55* – .54 .44† .44† .38$ .37$
DCU-ICTCAS .60$ .64$ .56* .61$ .60$ .66$ .59$ .46 – .51 .49 .46* .40$
CMU .69$ .62$ .58$ .59$ .63$ .55* .55* .56† .49 – .53 .42$ .43†
RBMT4 .67$ .65$ .62$ .63$ .61$ .60$ .53 .56† .51 .47 – .51 .37$
RBMT1 .68$ .66$ .69$ .65$ .63$ .60$ .60$ .62$ .54* .58$ .49 – .38$
ONLINE-C .78$ .73$ .80$ .71$ .70$ .66$ .69$ .63$ .60$ .57† .63$ .62$ –
score .63 .58 .58 .55 .55 .54 .49 .47 .45 .44 .44 .40 .32
rank 1 2-3 2-3 4-6 4-6 4-6 7-8 7-8 9-11 9-11 9-11 12 13
</table>
<tableCaption confidence="0.993539">
Table 30: Head to head comparison, ignoring ties, for German-English systems
</tableCaption>
<table confidence="0.996190269230769">
UEDIN-SYNTAX ONLINE-A EU-BRIDGE RBMT4 KIT IMS-TTT
ONLINE-B PROMT-HYBRID UEDIN-PHRASE STANFORD-UNC UU-DOCENT
PROMT-RULE RBMT1 CIMS
UEDIN-STANFORD STANFORD
UU
ONLINE-C
UEDIN-SYNTAX – .55* .46* .45* .46* .44† .41$ .45† .43$ .41$ .38$ .38$ .36$ .33$ .38$ .30$ .30$ .25$
ONLINE-B .45* – .50 .48 .50 .47 .43† .46* .41$ .45† .39$ .39$ .37$ .32$ .35$ .34$ .30$ .29$
ONLINE-A .54* .50 – .44† .52 .50 .45* .43$ .43† .42$ .39$ .41$ .42$ .42$ .37$ .44† .38$ .33$
PROMT-HYBRID .55* .52 .56† – .45* .47 .47 .46* .50 .44† .42$ .40$ .41$ .38$ .39$ .39$ .33$ .34$
PROMT-RULE .54* .50 .48 .55* – .51 .47 .47 .45* .38$ .42$ .40$ .43† .41$ .43† .38$ .35$ .29$
UEDIN-STANFORD .56† .53 .50 .53 .49 – .48 .50 .47 .44† .46 .36$ .36$ .36$ .36$ .35$ .30$ .32$
EU-BRIDGE .59$ .57† .55* .53 .53 .52 – .46* .43† .52 .42$ .42$ .45* .35$ .36$ .41$ .38$ .30$
RBMT4 .55† .54* .57$ .54* .53 .50 .54* – .53 .49 .44† .49 .50 .47 .40$ .42$ .38$ .40$
UEDIN-PHRASE .57$ .59$ .57† .50 .55* .53 .57† .47 – .50 .55* .47 .45* .44† .43† .42$ .37$ .34$
RBMT1 .59$ .55† .58$ .56† .62$ .56† .48 .51 .50 – .47 .47 .45† .47 .43$ .42$ .38$ .41$
KIT .62$ .61$ .61$ .58$ .58$ .54 .58$ .56† .45* .53 – .47 .49 .46 .43$ .48 .34$ .37$
STANFORD-UNC .62$ .61$ .59$ .60$ .60$ .64$ .58$ .51 .53 .53 .53 – .48 .47 .45† .45* .39$ .41$
CIMS .64$ .63$ .58$ .59$ .57† .64$ .55* .50 .55* .55† .51 .52 – .53 .42$ .52 .47 .42$
STANFORD .67$ .68$ .58$ .62$ .59$ .64$ .65$ .53 .56† .53 .54 .53 .47 – .53 .42$ .39$ .48
UU .62$ .65$ .62$ .61$ .57† .64$ .64$ .60$ .57† .57$ .57$ .55† .58$ .47 – .46* .45† .38$
ONLINE-C .70$ .66$ .56† .61$ .62$ .65$ .59$ .58$ .58$ .58$ .52 .55* .48 .58$ .54* – .48 .47
IMS-TTT .70$ .70$ .62$ .67$ .65$ .70$ .62$ .62$ .63$ .62$ .66$ .61$ .53 .61$ .55† .52 – .49
UU-DOCENT .75$ .71$ .67$ .66$ .71$ .68$ .70$ .60$ .66$ .59$ .63$ .59$ .58$ .52 .62$ .53 .51 –
score .60 .59 .56 .56 .56 .56 .54 .51 .51 .50 .48 .47 .46 .44 .43 .42 .38 .37
rank 1-2 1-2 3-6 3-6 3-6 3-6 7 8-10 8-10 8-10 11-12 11-13 12-14 13-15 14-16 15-16 17-18 17-18
</table>
<tableCaption confidence="0.998992">
Table 31: Head to head comparison, ignoring ties, for English-German systems
</tableCaption>
<page confidence="0.946857">
56
</page>
<table confidence="0.998949230769231">
UEDIN-PHRASE KIT ONLINE-A RBMT1 RBMT4 ONLINE-C
ONLINE-B
STANFORD
UEDIN-PHRASE – .48 .48 .45$ .43$ .28$ .28$ .19$
KIT .52 – .54† .48 .44$ .31$ .29$ .21$
ONLINE-B .52 .46† – .51 .47 .31$ .30$ .24$
STANFORD .55$ .52 .49 – .46† .34$ .30$ .23$
ONLINE-A .57$ .56$ .53 .54† – .32$ .29$ .21$
RBMT1 .72$ .69$ .69$ .66$ .68$ – .42$ .33$
RBMT4 .72$ .71$ .70$ .70$ .71$ .58$ – .39$
ONLINE-C .81$ .79$ .76$ .77$ .79$ .67$ .61$ –
score .63 .60 .59 .58 .57 .40 .35 .25
rank 1 2-4 2-4 2-4 5 6 7 8
</table>
<tableCaption confidence="0.99293">
Table 32: Head to head comparison, ignoring ties, for French-English systems
</tableCaption>
<table confidence="0.99944555">
ONLINE-B UEDIN-PHRASE UU-DOCENT RBMT1 RBMT4 ONLINE-C
KIT PROMT-HYBRID
MATRAN UA
MATRAN-RULES PROMT-RULE
ONLINE-A
ONLINE-B – .46* .48 .46* .50 .41$ .39$ .39$ .37$ .38$ .37$ .35$ .27$
UEDIN-PHRASE .54* – .50 .47 .46 .46* .42$ .41$ .46* .42$ .35$ .34$ .33$
KIT .52 .50 – .53 .51 .50 .43$ .49 .41$ .42$ .35$ .37$ .29$
MATRAN .54* .53 .47 – .49 .50 .43$ .43$ .38$ .48 .40$ .34$ .32$
MATRAN-RULES .50 .54 .49 .51 – .53 .40$ .45† .46* .42$ .44† .40$ .34$
ONLINE-A .59$ .54* .50 .50 .47 – .44† .49 .47 .45* .42$ .37$ .34$
UU-DOCENT .61$ .58$ .57$ .57$ .60$ .56† – .43$ .52 .46* .39$ .44† .33$
PROMT-HYBRID .61$ .59$ .51 .57$ .55† .51 .57$ – .50 .41$ .46* .44† .35$
UA .63$ .54* .59$ .62$ .54* .53 .48 .50 – .49 .46* .43$ .34$
PROMT-RULE .62$ .58$ .58$ .52 .58$ .55* .54* .59$ .51 – .47 .39$ .37$
RBMT1 .63$ .65$ .65$ .60$ .56† .58$ .61$ .54* .54* .53 – .46* .45†
RBMT4 .65$ .66$ .63$ .66$ .60$ .63$ .56† .56† .57$ .61$ .54* – .45*
ONLINE-C .73$ .67$ .71$ .67$ .66$ .66$ .67$ .65$ .66$ .63$ .55† .55* –
score .59 .57 .55 .55 .54 .53 .49 .49 .48 .47 .43 .40 .34
rank 1 2-4 2-5 2-5 4-6 4-6 7-9 7-10 7-10 8-10 11 12 13
</table>
<tableCaption confidence="0.995259">
Table 33: Head to head comparison, ignoring ties, for English-French systems
</tableCaption>
<table confidence="0.948426714285714">
ONLINE-B ONLINE-A UEDIN-PHRASE AFRL DCU-LINGO24 IIIT-HYDERABAD
UEDIN-SYNTAX IIT-BOMBAY
CMU
ONLINE-B – .36$ .33$ .37$ .31$ .21$ .20$ .14$ .00
ONLINE-A .64$ – .48 .47* .44$ .31$ .30$ .24$ .12$
UEDIN-SYNTAX .67$ .52 – .47 .46† .33$ .29$ .24$ .12$
CMU .63$ .53* .53 – .47 .37$ .31$ .26$ .11$
UEDIN-PHRASE .69$ .56$ .54† .53 – .40$ .33$ .25$ .11$
AFRL .79$ .69$ .67$ .63$ .60$ – .53 .40$ .16$
IIT-BOMBAY .80$ .70$ .71$ .69$ .67$ .47 – .44$ .19$
DCU-LINGO24 .86$ .76$ .76$ .74$ .75$ .60$ .56$ – .19$
IIIT-HYDERABAD .94$ .88$ .88$ .89$ .89$ .84$ .81$ .81$ –
score .75 .62 .61 .60 .57 .44 .41 .34 .13
rank 1 2-3 2-4 3-4 5 6-7 6-7 8 9
</table>
<tableCaption confidence="0.999167">
Table 34: Head to head comparison, ignoring ties, for Hindi-English systems
</tableCaption>
<page confidence="0.971005">
57
</page>
<table confidence="0.9945035">
ONLINE-B ONLINE-A UEDIN-UNCNSTR UEDIN-PHRASE IIT-BOMBAY DCU-LINGO24 MANAWI-H1 MANAWI MANAWI-RMOOV
CU-MOSES IPN-UPV-CNTXT IPN-UPV-NODEV
ONLINE-B – .49 .28$ .29$ .27$ .23$ .22$ .20$ .17$ .12$ .13$ .13$
ONLINE-A .51 – .31$ .29$ .27$ .25$ .20$ .20$ .21$ .19$ .16$ .15$
UEDIN-UNCNSTR .72$ .69$ – .44† .49 .39$ .40$ .34$ .39$ .29$ .30$ .27$
UEDIN-PHRASE .71$ .71$ .56† – .48 .45† .44† .39$ .37$ .31$ .31$ .32$
CU-MOSES .73$ .73$ .51 .52 – .47 .42$ .40$ .45* .36$ .35$ .33$
IIT-BOMBAY .77$ .75$ .61$ .55† .53 – .50 .47 .45† .41$ .40$ .36$
IPN-UPV-CNTXT .78$ .80$ .60$ .56† .58$ .50 – .51 .41$ .40$ .40$ .37$
DCU-LINGO24 .80$ .80$ .66$ .61$ .60$ .53 .49 – .52 .41$ .41$ .39$
IPN-UPV-NODEV .83$ .79$ .61$ .63$ .55* .55† .59$ .48 – .46* .44† .38$
MANAWI-H1 .88$ .81$ .71$ .69$ .64$ .59$ .60$ .59$ .54* – .35$ .34$
MANAWI .87$ .84$ .70$ .69$ .65$ .60$ .60$ .59$ .56† .65$ – .39$
MANAWI-RMOOV .87$ .85$ .73$ .68$ .67$ .64$ .63$ .61$ .62$ .66$ .61$ –
score .77 .75 .57 .54 .52 .47 .46 .43 .42 .38 .35 .31
rank 1 2 3 4-5 4-5 6-7 6-7 8-9 8-9 10-11 10-11 12
</table>
<tableCaption confidence="0.993388">
Table 35: Head to head comparison, ignoring ties, for English-Hindi systems
</tableCaption>
<table confidence="0.999497142857143">
AFRL-PE ONLINE-B ONLINE-A AFRL UEDIN-SYNTAX KAZNU RBMT1 RBMT4
PROMT-HYBRID
PROMT-RULE
UEDIN-PHRASE
YANDEX
ONLINE-G
AFRL-PE – .42$ .40$ .39$ .39$ .41$ .35$ .39$ .28$ .26$ .26$ .29$ .21$
ONLINE-B .58$ – .42$ .43$ .45† .45† .42$ .43$ .46* .37$ .33$ .29$ .31$
ONLINE-A .60$ .58$ – .50 .45† .51 .47 .45† .42$ .40$ .33$ .32$ .30$
PROMT-HYBRID .61$ .57$ .50 – .47 .45* .49 .44† .43$ .44† .39$ .31$ .27$
PROMT-RULE .61$ .55† .55† .53 – .46* .47 .49 .48 .42$ .36$ .34$ .30$
UEDIN-PHRASE .59$ .55† .49 .55* .54* – .49 .50 .47 .44† .32$ .37$ .29$
YANDEX .65$ .58$ .53 .51 .53 .51 – .48 .50 .43$ .34$ .36$ .34$
ONLINE-G .61$ .57$ .55† .56† .51 .50 .52 – .48 .43$ .39$ .35$ .30$
AFRL .72$ .54* .58$ .57$ .52 .53 .50 .52 – .44† .41$ .41$ .37$
UEDIN-SYNTAX .74$ .63$ .60$ .56† .58$ .56† .57$ .57$ .56† – .51 .36$ .37$
KAZNU .74$ .67$ .67$ .61$ .64$ .68$ .66$ .61$ .59$ .49 – .44† .38$
RBMT1 .71$ .71$ .68$ .69$ .66$ .63$ .64$ .65$ .59$ .64$ .56† – .47
RBMT4 .79$ .69$ .70$ .73$ .70$ .71$ .66$ .70$ .63$ .63$ .62$ .53 –
score .66 .58 .55 .55 .53 .53 .52 .51 .49 .45 .40 .36 .32
rank 1 2 3-5 3-5 4-7 5-8 5-8 5-8 9 10 11 12 13
</table>
<tableCaption confidence="0.99197">
Table 36: Head to head comparison, ignoring ties, for Russian-English systems
</tableCaption>
<table confidence="0.999650384615385">
PROMT-RULE PROMT-HYBRID UEDIN-UNCNSTR ONLINE-A RBMT4 RBMT1
ONLINE-B ONLINE-G UEDIN-PHRASE
PROMT-RULE – .51 .45† .43$ .43$ .39$ .38$ .15$ .00
ONLINE-B .49 – .50 .47* .38$ .36$ .38$ .16$ .13$
PROMT-HYBRID .55† .50 – .49 .47 .39$ .40$ .18$ .15$
UEDIN-UNCNSTR .57$ .53* .51 – .50 .44$ .36$ .25$ .18$
ONLINE-G .57$ .62$ .53 .50 – .46* .44$ .23$ .18$
ONLINE-A .61$ .64$ .61$ .56$ .54* – .49 .24$ .18$
UEDIN-PHRASE .62$ .62$ .60$ .64$ .56$ .51 – .30$ .21$
RBMT4 .85$ .84$ .82$ .75$ .77$ .76$ .70$ – .42$
RBMT1 .91$ .87$ .85$ .82$ .82$ .82$ .79$ .58$ –
score .64 .64 .61 .58 .55 .51 .49 .26 .19
rank 1-2 1-2 3 4-5 4-5 6-7 6-7 8 9
</table>
<tableCaption confidence="0.999233">
Table 37: Head to head comparison, ignoring ties, for English-Russian systems
</tableCaption>
<page confidence="0.998499">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.008206">
<title confidence="0.996507">Findings of the 2014 Workshop on Statistical Machine Translation</title>
<author confidence="0.978733">Ondˇrej Bojar Christian Buck Christian Federmann Barry Haddow</author>
<affiliation confidence="0.985707">Charles University in Prague University of Edinburgh Microsoft Research University of Edinburgh</affiliation>
<author confidence="0.520977">Philipp</author>
<affiliation confidence="0.600825">JHU / Edinburgh</affiliation>
<author confidence="0.982127">Pavel Pecina</author>
<affiliation confidence="0.946276">Charles University in Prague</affiliation>
<email confidence="0.823865">Radu</email>
<title confidence="0.41883">Google</title>
<author confidence="0.536948">Johannes</author>
<affiliation confidence="0.98495">Dublin City University</affiliation>
<author confidence="0.999309">Matt Post</author>
<affiliation confidence="0.99837">Johns Hopkins University</affiliation>
<author confidence="0.637585">Lucia</author>
<affiliation confidence="0.9198808">University of Sheffield Christof University of Amsterdam Herve University of Edinburgh</affiliation>
<note confidence="0.237935">Aleˇs Charles University in Prague</note>
<abstract confidence="0.9913921875">This paper presents the results of the WMT14 shared tasks, which included a standard news translation task, a separate medical translation task, a task for run-time estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Avramidis</author>
</authors>
<title>Efforts on machine learning over human-mediated translation edit rate.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="58547" citStr="Avramidis, 2014" startWordPosition="9440" endWordPosition="9441">rces are monolingual corpora taken from the WMT 2014 translation task for LMs, the MT system used for the inverse glass-box features (Li et al., 2014b) and, for error grammar parsing, the PennTreebank and an error grammar derived from it (Foster, 2007). which for the above example gives: 1 weighted F1,ALL = weighted F1,ERR = 6189 + 2948 + 476 (6189 · 0.6932 + 2948 · 0.4222 +476 · 0.1575) = 0.5836 1 2948 + 476· (2948 · 0.4222 + 476 · 0.1575) = 0.3854 · 28 Participating team Dublin City University Team 1, Ireland (Hokamp et al., 2014) German Research Centre for Artificial Intelligence, Germany (Avramidis, 2014) Fondazione Bruno Kessler, Italy, UPV Universitat Polit`ecnica de Val`encia, Spain &amp; University of Edinburgh, UK (Camargo de Souza et al., 2014) Laboratoire d’Informatique Grenoble, France (Luong et al., 2014) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2014) Multilizer, Finland Dublin City University Team 2, Ireland (Bicici and Way, 2014) University of Sheffield Team 1, UK (Beck et al., 2014) University of Sheffield Team 2, UK (Scarton and Specia, 2014) Yandex, Russia DCU DFKI FBK-UPV-UEDIN LIG LIMSI MULTILIZER RTM-DCU SHEF-lite US</context>
</contexts>
<marker>Avramidis, 2014</marker>
<rawString>Avramidis, E. (2014). Efforts on machine learning over human-mediated translation edit rate. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Beck</author>
<author>K Shah</author>
<author>L Specia</author>
</authors>
<title>Sheflite 2.0: Sparse multi-task gaussian processes for translation quality estimation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="59005" citStr="Beck et al., 2014" startWordPosition="9504" endWordPosition="9507">28 Participating team Dublin City University Team 1, Ireland (Hokamp et al., 2014) German Research Centre for Artificial Intelligence, Germany (Avramidis, 2014) Fondazione Bruno Kessler, Italy, UPV Universitat Polit`ecnica de Val`encia, Spain &amp; University of Edinburgh, UK (Camargo de Souza et al., 2014) Laboratoire d’Informatique Grenoble, France (Luong et al., 2014) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2014) Multilizer, Finland Dublin City University Team 2, Ireland (Bicici and Way, 2014) University of Sheffield Team 1, UK (Beck et al., 2014) University of Sheffield Team 2, UK (Scarton and Specia, 2014) Yandex, Russia DCU DFKI FBK-UPV-UEDIN LIG LIMSI MULTILIZER RTM-DCU SHEF-lite USHEFF YANDEX ID Table 10: Participants in the WMT14 Quality Estimation shared task. DFKI (T1.2): DFKI/SVR builds upon the baseline system (above) by adding non-redundant data from the WMT13 task for predicting the same label (HTER) and additional features such as (a) rule-based language corrections (language tool) (b), PCFG parsing statistics and counts of tree labels, (c) position statistics of parsing labels, (d) position statistics of trigrams with low</context>
</contexts>
<marker>Beck, Shah, Specia, 2014</marker>
<rawString>Beck, D., Shah, K., and Specia, L. (2014). Sheflite 2.0: Sparse multi-task gaussian processes for translation quality estimation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bic¸ici</author>
</authors>
<title>Referential translation machines for quality estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<location>Sofia, Bulgaria.</location>
<marker>Bic¸ici, 2013</marker>
<rawString>Bic¸ici, E. (2013). Referential translation machines for quality estimation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bic¸ici</author>
<author>Q Liu</author>
<author>A Way</author>
</authors>
<title>Parallel FDA5 for fast deployment of accurate statistical machine translation systems.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA.</location>
<marker>Bic¸ici, Liu, Way, 2014</marker>
<rawString>Bic¸ici, E., Liu, Q., and Way, A. (2014). Parallel FDA5 for fast deployment of accurate statistical machine translation systems. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bicici</author>
<author>Q Liu</author>
<author>A Way</author>
</authors>
<title>Parallel fda5 for fast deployment of accurate statistical machine translation systems.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="16583" citStr="Bicici et al., 2014" startWordPosition="2484" endWordPosition="2487">-source tool built on Python’s Django framework. At the top of each HIT, the following instructions are provided: You are shown a source sentence followed by several candidate translations. Your task is to rank the translations from best to worst (ties are allowed). 5https://github.com/cfedermann/Appraise 15 ID Institution AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014) CIMS University of Stuttgart / University of Munich (Cap et al., 2014) CMU Carnegie Mellon University (Matthews et al., 2014) CU-* Charles University, Prague (Tamchyna et al., 2014) DCU-FDA Dublin City University (Bicici et al., 2014) DCU-ICTCAS Dublin City University (Li et al., 2014b) DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014) EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014) KIT Karlsruhe Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu</context>
</contexts>
<marker>Bicici, Liu, Way, 2014</marker>
<rawString>Bicici, E., Liu, Q., and Way, A. (2014). Parallel fda5 for fast deployment of accurate statistical machine translation systems. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bicici</author>
<author>A Way</author>
</authors>
<title>Referential translation machines for predicting translation quality.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="58950" citStr="Bicici and Way, 2014" startWordPosition="9494" endWordPosition="9497">6 1 2948 + 476· (2948 · 0.4222 + 476 · 0.1575) = 0.3854 · 28 Participating team Dublin City University Team 1, Ireland (Hokamp et al., 2014) German Research Centre for Artificial Intelligence, Germany (Avramidis, 2014) Fondazione Bruno Kessler, Italy, UPV Universitat Polit`ecnica de Val`encia, Spain &amp; University of Edinburgh, UK (Camargo de Souza et al., 2014) Laboratoire d’Informatique Grenoble, France (Luong et al., 2014) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2014) Multilizer, Finland Dublin City University Team 2, Ireland (Bicici and Way, 2014) University of Sheffield Team 1, UK (Beck et al., 2014) University of Sheffield Team 2, UK (Scarton and Specia, 2014) Yandex, Russia DCU DFKI FBK-UPV-UEDIN LIG LIMSI MULTILIZER RTM-DCU SHEF-lite USHEFF YANDEX ID Table 10: Participants in the WMT14 Quality Estimation shared task. DFKI (T1.2): DFKI/SVR builds upon the baseline system (above) by adding non-redundant data from the WMT13 task for predicting the same label (HTER) and additional features such as (a) rule-based language corrections (language tool) (b), PCFG parsing statistics and counts of tree labels, (c) position statistics of parsi</context>
</contexts>
<marker>Bicici, Way, 2014</marker>
<rawString>Bicici, E. and Way, A. (2014). Referential translation machines for predicting translation quality. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>C Buck</author>
<author>C Callison-Burch</author>
<author>C Federmann</author>
<author>B Haddow</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>M Post</author>
<author>R Soricut</author>
<author>L Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--42</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<contexts>
<context position="1426" citStr="Bojar et al., 2013" startWordPosition="211" endWordPosition="214">hine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2014. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013). This year we conducted four official tasks: a translation task, a quality estimation task, a metrics task1 and a medical translation task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Hindi, and Russian. The Hindi translation tasks were new this year, providing a lesser resourced data condition on a challenging language pair. The system outputs for each task were evaluated both automatically and m</context>
<context position="42096" citStr="Bojar et al., 2013" startWordPosition="6746" endWordPosition="6749">eling annotator bias, incorporating sentence metadata (such as length, difficulty, or subtopic), and adding features of the sentence pairs. 4 Quality Estimation Task Machine translation quality estimation is the task of predicting a quality score for a machine translated text without access to reference translations. The most common approach is to treat the problem as a supervised machine learning task, using standard regression or classification algorithms. The third edition of the WMT shared task on quality estimation builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013), with tasks including both sentence-level and word-level estimation, with new training and test datasets. The goals of this year’s shared task were: • To investigate the effectiveness of different quality labels. • To explore word-level quality prediction at 23 Expected Wins Hopkins &amp; May TrueSkill UEDIN-SYNTAX UEDIN-SYNTAX UEDIN-SYNTAX ONLINE ONLINE ONLINE -B -B -B ONLINE-A UEDIN-STANFORD ONLINE-A UEDIN-STANFORD PROMT-HYBRID PROMT-HYBRID PROMT-RULE ONLINE-A PROMT-RULE PROMT-HYBRID PROMT-RULE UEDIN-STANFORD EU-BRIDGE EU-BRIDGE EU-BRIDGE RBMT4 UEDIN-PHRASE RBMT4 UEDIN-PHRASE RBMT4 UEDIN-PHRASE</context>
<context position="46606" citStr="Bojar et al., 2013" startWordPosition="7433" endWordPosition="7436">isting of source sentences and their human translations, as well as two-three versions of machine translations (by an SMT system, an RBMT system and, for EnglishSpanish/German only, a hybrid system), all in the news domain, extracted from tests sets of various WMT years and MT systems that participated in the translation shared task: # Source sentences # Target sentences 954 English 350 English 3,816 Spanish 1,400 German • Scoring: Mean Average Error (MAE) (primary metric), Root Mean Squared Error (RMSE). 350 German 350 Spanish 1,050 English 1,050 English • Ranking: DeltaAvg (primary metric) (Bojar et al., 2013) and Spearman’s rank correlation. For all sentence-level these tasks, the same 17 features as in WMT12-13 were used to build baseline systems. The SVM regression algorithm within QUEST (Specia et al., 2013)10 was applied for that with RBF kernel and grid search for parameter optimisation. Task 1.1 Predicting post-editing effort Data in this task is labelled with discrete and absolute scores for perceived post-editing effort, where: • 1 = Perfect translation, no post-editing needed at all. • 2 = Near miss translation: translation contains maximum of 2-3 errors, and possibly additional errors th</context>
<context position="53431" citStr="Bojar et al., 2013" startWordPosition="8561" endWordPosition="8564">chpad/content/ training 26 Figure 4: MQM metric as error typology. As test data, we provide additional data points for all language pairs, human and machine translations: # Source sentences # Target sentences 382 English 150 English 100 German 150 Spanish 382 Spanish 150 German 100 English 150 English trix as follows: tpO = 4172 fpO = 1482 + 193 = 1675 fnO = 1819 + 198 = 2017 tpF = 1333 fpF = 1819 + 214 = 2033 fnF = 1482 + 133 = 1615 tpA = 69 fpA = 198 + 133 = 331 fnA = 193 + 214 = 407 In contrast to Tasks 1.1–1.3, no baseline feature set is provided to the participants. Similar to last year (Bojar et al., 2013), the word-level task is primarily evaluated by macroaveraged F-measure (in %). Because the class distribution is skewed – in the test data about 78% of the tokens are marked as “OK” – we compute precision, recall, and F1 for each class individually, weighting F1 scores by the frequency of the class in the test data. This avoids giving undue importance to less frequent classes. Consider the following confusion matrix for Level 1 annotation, i.e. the three classes (O)K, (F)luency, and (A)ccuracy: For each of the three classes we assume a binary setting (one-vs-all) and derive true-positive (tp)</context>
<context position="57260" citStr="Bojar et al., 2013" startWordPosition="9231" endWordPosition="9234">the source sentence seen in the source side of the WMT News Commentary corpus. These features are used to train a Support Vector Machine (SVM) regression algorithm using a radial basis function kernel within the SCIKIT-LEARN toolkit. The γ, E and C parameters were optimised via grid search with 5-fold cross validation on the training set. We note that although the system is referred to as “baseline”, it is in fact a strong system. It has proved robust across a range of language pairs, MT systems, and text domains for predicting various forms of postediting effort (Callison-Burch et al., 2012; Bojar et al., 2013). DCU (T1.1): DCU-MIXED and DCU-SVR use a selection of features available in QUEST, such as punctuation statistics, LM perplexity, n-gram frequency quartile statistics and coarse-grained POS frequency ratios, and four additional feature types: combined POS and stop word LM features, source-side pseudo-reference features, inverse glass-box features for translating the translation and error grammar parsing features. For machine learning, the QUEST framework is expanded to combine logistic regression and support vector regression and to handle cross- validation and randomisation in a way that tra</context>
<context position="85496" citStr="Bojar et al., 2013" startWordPosition="13672" endWordPosition="13675">st systems achieve a Spearman’s p of 0.53 for English-Spanish translations (Table 13). This comparison across tasks seems to indicate that, among the three labels we have proposed, post-editing time seems to be the most learnable, in the sense that automatic predictions can vest match the gold labels (in this case, with respect to the rankings they induce). A possible reason for this is that post-editing time correlates with the length of the source sentence whereas HTER is a normalised measure. Compared to the results regarding time prediction in the Quality Evaluation shared task from 2013 (Bojar et al., 2013), we note that this time all submissions were able to beat the baseline system (compared to only 1/3 of the submissions in 2013). In addition, better handling of the data acquisition reduced the number of outliers in this year’s dataset allowing for numbers that are more reliably interpretable. As an example of its interpretability, consider the following: the winning submission for the ranking variant of Task 1.3 is RTM-DCU/RTM-RR, with a a Spearman’s p of 0.68 and a DeltaAvg score of 17.02 (when predict37 System ID weighted F1 All Errors T weighted MCC All Errors ACC English-Spanish Baseline</context>
<context position="90482" citStr="Bojar et al., 2013" startWordPosition="14452" endWordPosition="14456">learly more useful, and also very much interpretable within the scope of the task. Our attempt at presenting the Quality Prediction task with a variety of prediction labels illustrates a good range of properties for the proposed labels and enables one to draw certain conclusions depending on the needs of the specific task at hand. For the word-level tasks, different quality labels equate with using different levels of granularity for the predictions, which we discuss next. Exploring word-level quality prediction at different levels of granularity Previous work on word-level predictions, e.g. (Bojar et al., 2013) has focused on prediction of automatically derived labels, generally due to practical considerations as the manual annotation is labour intensive. While easily applicable, automatic annotations, using for example TER alignment between the machine translation and reference (or post-edition), face the same problems as automatic 14Note that the 17.02 seconds figure is a difference in realtime, not predicted time; what is considered in this variant of Task 1.3 is only the predicted ranking of data points, not the absolute values of the predictions. MT evaluation metrics as they fail to account fo</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Bojar, O., Buck, C., Callison-Burch, C., Federmann, C., Haddow, B., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. (2013). Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1–42, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>V Diatka</author>
<author>P Rychl´y</author>
<author>P Straˇn´ak</author>
<author>A Tamchyna</author>
<author>D Zeman</author>
</authors>
<title>HindiEnglish and Hindi-only Corpus for Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Language Resources and Evaluation Conference,</booktitle>
<location>Reykjavik, Iceland. ELRA.</location>
<marker>Bojar, Diatka, Rychl´y, Straˇn´ak, Tamchyna, Zeman, 2014</marker>
<rawString>Bojar, O., Diatka, V., Rychl´y, P., Straˇn´ak, P., Tamchyna, A., and Zeman, D. (2014). HindiEnglish and Hindi-only Corpus for Machine Translation. In Proceedings of the Ninth International Language Resources and Evaluation Conference, Reykjavik, Iceland. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>M Ercegovˇcevi´c</author>
<author>M Popel</author>
<author>O Zaidan</author>
</authors>
<title>A grain of salt for the WMT manual evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland.</location>
<marker>Bojar, Ercegovˇcevi´c, Popel, Zaidan, 2011</marker>
<rawString>Bojar, O., Ercegovˇcevi´c, M., Popel, M., and Zaidan, O. (2011). A grain of salt for the WMT manual evaluation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 1–11, Edinburgh, Scotland. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borisov</author>
<author>I Galinskaya</author>
</authors>
<title>Yandex school of data analysis russian-english machine translation system for wmt14.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="17707" citStr="Borisov and Galinskaya, 2014" startWordPosition="2645" endWordPosition="2648">utute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014) PROMT-RULE, PROMT PROMT-HYBRID RWTH RWTH Aachen (Peitz et al., 2014) STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014) UA-* University of Alicante (S´anchez-Cartagena et al., 2014) UEDIN-PHRASE, University of Edinburgh (Durrani et al., 2014b) UEDIN-UNCNSTR UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014) UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014) YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014) COMMERCIAL-[1,2] Two commercial machine translation systems ONLINE-[A,B,C,G] Four online statistical machine translation systems RBMT-[1,4] Two rule-based statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 16 Figure 2: Screenshot of the Appraise interface used in the human evaluation c</context>
</contexts>
<marker>Borisov, Galinskaya, 2014</marker>
<rawString>Borisov, A. and Galinskaya, I. (2014). Yandex school of data analysis russian-english machine translation system for wmt14. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bouayad-Agha</author>
<author>D R Scott</author>
<author>R Power</author>
</authors>
<title>Integrating content and style in documents: A case study of patient information leaflets.</title>
<date>2000</date>
<journal>Information Design Journal,</journal>
<volume>9</volume>
<issue>2</issue>
<pages>3--161</pages>
<marker>Bouayad-Agha, Scott, Power, 2000</marker>
<rawString>Bouayad-Agha, N., Scott, D. R., and Power, R. (2000). Integrating content and style in documents: A case study of patient information leaflets. Information Design Journal, 9(2– 3):161–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Buckley</author>
<author>E M Voorhees</author>
</authors>
<title>Retrieval evaluation with incomplete information.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>25--32</pages>
<location>Sheffield, United Kingdom.</location>
<contexts>
<context position="129933" citStr="Buckley and Voorhees, 2004" startWordPosition="20431" endWordPosition="20434">(2014). Originally, we asked for 10 best translations for each query, but only the best one were used for the evaluation. The results are provided in terms of standard IR evaluation measures: precision at a cut-off of 5 and 10 documents (P@5, P@10), normalized discounted cumulative gain (J¨arvelin and Kek¨al¨ainen, 2002) at 5 and 10 documents (NDCG@5, NDCG@10), mean average precision (MAP) (Voorhees and Harman, 2005), precision reached after R documents retrieved, where R indicates the number of the relevant documents for each query in the entire collection (Rprec), binary preference (bpref) (Buckley and Voorhees, 2004), and number or relevant documents retrieved (rel). The cross-lingual results are also compared with the monolingual one (obtained by using the reference (English) translations of the test topics) to see how the system would perform if the queries were translated perfectly. 5.6 Discussion and Conclusion Both the subtasks turned out to be quite challenging not only because of the specific domain – in summary sentences, we can observe much higher density of terminology than in ordinary sentences; the queries, which are also rich in terminology, do not form sentences at all. Most submissions were</context>
</contexts>
<marker>Buckley, Voorhees, 2004</marker>
<rawString>Buckley, C. and Voorhees, E. M. (2004). Retrieval evaluation with incomplete information. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 25– 32, Sheffield, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
<author>B Sacaleanu</author>
<author>ˇSpela Vintar</author>
<author>D Steffen</author>
<author>M Volk</author>
<author>H Dejean</author>
<author>E Gaussier</author>
<author>D Widdows</author>
<author>O Weiser</author>
<author>R Frederking</author>
</authors>
<title>Multilingual concept hierarchies for medical information organization and retrieval. Public deliverable, MuchMore project.</title>
<date>2003</date>
<contexts>
<context position="108890" citStr="Buitelaar et al., 2003" startWordPosition="17367" endWordPosition="17370">monolingual training data allowed for the constrained tasks (in thousands of tokens). ical vocabularies and standards (U.S. National Library of Medicine, 2009). The UMLS dataset was constructed by selecting the concepts which have translations in the respective languages. The Wiki dataset contains bilingual pairs of titles of Wikipedia articles belonging to the categories identified to be medical-domain within the Khresmoi project. It is available for all three language pairs. The MuchMore Springer Corpus is a German–English parallel corpus of medical journals abstracts published by Springer (Buitelaar et al., 2003). PatTR is a parallel corpus extracted from the MAREC patent collection (W¨aschle and Riezler, 2012). It is available for German–English and French–English. For the medical domain, we only consider text from patents indicated to be from the medicine-related categories (A61, C12N, C12P). COPPA (Corpus of Parallel Patent Applications (Pouliquen and Mazenc, 2011) is a French–English parallel corpus extracted from the MAREC patent collection (W¨aschle and Riezler, 2012). The medical-domain subset is identified by the same categories as in PatTR. 5.3.2 Monolingual Training Data The medical-domain m</context>
</contexts>
<marker>Buitelaar, Sacaleanu, Vintar, Steffen, Volk, Dejean, Gaussier, Widdows, Weiser, Frederking, 2003</marker>
<rawString>Buitelaar, P., Sacaleanu, B., ˇSpela Vintar, Steffen, D., Volk, M., Dejean, H., Gaussier, E., Widdows, D., Weiser, O., and Frederking, R. (2003). Multilingual concept hierarchies for medical information organization and retrieval. Public deliverable, MuchMore project.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>C Fordyce</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>J Schroeder</author>
</authors>
<title>(Meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation (WMT07),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1375" citStr="Callison-Burch et al., 2007" startWordPosition="202" endWordPosition="205">translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2014. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013). This year we conducted four official tasks: a translation task, a quality estimation task, a metrics task1 and a medical translation task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Hindi, and Russian. The Hindi translation tasks were new this year, providing a lesser resourced data condition on a challenging language pair. The system outputs fo</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Callison-Burch, C., Fordyce, C., Koehn, P., Monz, C., and Schroeder, J. (2007). (Meta-) evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation (WMT07), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>C Fordyce</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>J Schroeder</author>
</authors>
<title>Further metaevaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation (WMT08), Colmbus,</booktitle>
<location>Ohio.</location>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Callison-Burch, C., Fordyce, C., Koehn, P., Monz, C., and Schroeder, J. (2008). Further metaevaluation of machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation (WMT08), Colmbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>K Peterson</author>
<author>M Przybocki</author>
<author>O F Zaidan</author>
</authors>
<title>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation (WMT10),</booktitle>
<location>Uppsala,</location>
<marker>Callison-Burch, Koehn, Monz, Peterson, Przybocki, Zaidan, 2010</marker>
<rawString>Callison-Burch, C., Koehn, P., Monz, C., Peterson, K., Przybocki, M., and Zaidan, O. F. (2010). Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation (WMT10), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>M Post</author>
<author>R Soricut</author>
<author>L Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="23068" citStr="Callison-Burch et al. (2012)" startWordPosition="3544" endWordPosition="3547">uce a ranking of the systems. Machine translation evaluation has always been a subject of contention, and no exception to this rule exists for the WMT manual evaluation. While the precise metric has varied over the years, it has always shared a common idea of computing the average number of times each system was judged better than other systems, and ranking from highest to lowest. For example, in WMT11 Callison-Burch et al. (2011), the metric computed the percentage of the time each system was ranked better than or equal to other systems, and included comparisons to human references. In WMT12 Callison-Burch et al. (2012), comparisons to references were dropped. In WMT13, rankings were produced over 1,000 bootstrap-resampled sets of the training data. A rank range was collected for each system across these folds; the average value was used to order the systems, and a 95% confidence interval across these ranks was used to organize the systems into equivalence classes containing systems with overn = 18 LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13r WMT13m WMT14 Czech–English 0.400 0.311 0.244 0.342 0.279 0.305 English–Czech 0.460 0.359 0.168 0.408 0.075 0.360 German–English 0.324 0.385 0.299 0.443 0.324 0.368 English–Ge</context>
<context position="42075" citStr="Callison-Burch et al., 2012" startWordPosition="6742" endWordPosition="6745">ly useful ways, including modeling annotator bias, incorporating sentence metadata (such as length, difficulty, or subtopic), and adding features of the sentence pairs. 4 Quality Estimation Task Machine translation quality estimation is the task of predicting a quality score for a machine translated text without access to reference translations. The most common approach is to treat the problem as a supervised machine learning task, using standard regression or classification algorithms. The third edition of the WMT shared task on quality estimation builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013), with tasks including both sentence-level and word-level estimation, with new training and test datasets. The goals of this year’s shared task were: • To investigate the effectiveness of different quality labels. • To explore word-level quality prediction at 23 Expected Wins Hopkins &amp; May TrueSkill UEDIN-SYNTAX UEDIN-SYNTAX UEDIN-SYNTAX ONLINE ONLINE ONLINE -B -B -B ONLINE-A UEDIN-STANFORD ONLINE-A UEDIN-STANFORD PROMT-HYBRID PROMT-HYBRID PROMT-RULE ONLINE-A PROMT-RULE PROMT-HYBRID PROMT-RULE UEDIN-STANFORD EU-BRIDGE EU-BRIDGE EU-BRIDGE RBMT4 UEDIN-PHRASE RBMT4 UEDIN-PHRA</context>
<context position="57239" citStr="Callison-Burch et al., 2012" startWordPosition="9227" endWordPosition="9230"> • percentage of unigrams in the source sentence seen in the source side of the WMT News Commentary corpus. These features are used to train a Support Vector Machine (SVM) regression algorithm using a radial basis function kernel within the SCIKIT-LEARN toolkit. The γ, E and C parameters were optimised via grid search with 5-fold cross validation on the training set. We note that although the system is referred to as “baseline”, it is in fact a strong system. It has proved robust across a range of language pairs, MT systems, and text domains for predicting various forms of postediting effort (Callison-Burch et al., 2012; Bojar et al., 2013). DCU (T1.1): DCU-MIXED and DCU-SVR use a selection of features available in QUEST, such as punctuation statistics, LM perplexity, n-gram frequency quartile statistics and coarse-grained POS frequency ratios, and four additional feature types: combined POS and stop word LM features, source-side pseudo-reference features, inverse glass-box features for translating the translation and error grammar parsing features. For machine learning, the QUEST framework is expanded to combine logistic regression and support vector regression and to handle cross- validation and randomisat</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Callison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. (2012). Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10– 51, Montr´eal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>J Schroeder</author>
</authors>
<title>Findings of the 2009 workshop on statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation (WMT09),</booktitle>
<location>Athens, Greece.</location>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Callison-Burch, C., Koehn, P., Monz, C., and Schroeder, J. (2009). Findings of the 2009 workshop on statistical machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation (WMT09), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>O Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>22--64</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="22874" citStr="Callison-Burch et al. (2011)" startWordPosition="3512" endWordPosition="3515"> WMT11 (where agreement scores were likely inflated due to inclusion of reference translations in the comparisons). 3.3 Models of System Rankings The collected pairwise rankings are used to produce a ranking of the systems. Machine translation evaluation has always been a subject of contention, and no exception to this rule exists for the WMT manual evaluation. While the precise metric has varied over the years, it has always shared a common idea of computing the average number of times each system was judged better than other systems, and ranking from highest to lowest. For example, in WMT11 Callison-Burch et al. (2011), the metric computed the percentage of the time each system was ranked better than or equal to other systems, and included comparisons to human references. In WMT12 Callison-Burch et al. (2012), comparisons to references were dropped. In WMT13, rankings were produced over 1,000 bootstrap-resampled sets of the training data. A rank range was collected for each system across these folds; the average value was used to order the systems, and a 95% confidence interval across these ranks was used to organize the systems into equivalence classes containing systems with overn = 18 LANGUAGE PAIR WMT11</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Callison-Burch, C., Koehn, P., Monz, C., and Zaidan, O. (2011). Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22–64, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Camargo de Souza</author>
<author>J G</author>
<author>J Gonz´alez-Rubio</author>
<author>C Buck</author>
<author>M Turchi</author>
<author>M Negri</author>
</authors>
<title>Fbk-upv-uedin participation in the wmt14 quality estimation shared-task.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<marker>de Souza, G, Gonz´alez-Rubio, Buck, Turchi, Negri, 2014</marker>
<rawString>Camargo de Souza, J. G., Gonz´alez-Rubio, J., Buck, C., Turchi, M., and Negri, M. (2014). Fbk-upv-uedin participation in the wmt14 quality estimation shared-task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Cap</author>
<author>M Weller</author>
<author>A Ramm</author>
<author>A Fraser</author>
</authors>
<title>Cims – the cis and ims joint submission to wmt 2014 translating from english into german.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="16419" citStr="Cap et al., 2014" startWordPosition="2460" endWordPosition="2463">ymized systems, randomly selected from the set of participating systems, and randomly ordered. To run the evaluation, we use Appraise5 (Federmann, 2012), an open-source tool built on Python’s Django framework. At the top of each HIT, the following instructions are provided: You are shown a source sentence followed by several candidate translations. Your task is to rank the translations from best to worst (ties are allowed). 5https://github.com/cfedermann/Appraise 15 ID Institution AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014) CIMS University of Stuttgart / University of Munich (Cap et al., 2014) CMU Carnegie Mellon University (Matthews et al., 2014) CU-* Charles University, Prague (Tamchyna et al., 2014) DCU-FDA Dublin City University (Bicici et al., 2014) DCU-ICTCAS Dublin City University (Li et al., 2014b) DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014) EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014) KIT Karlsruhe Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KA</context>
</contexts>
<marker>Cap, Weller, Ramm, Fraser, 2014</marker>
<rawString>Cap, F., Weller, M., Ramm, A., and Fraser, A. (2014). Cims – the cis and ims joint submission to wmt 2014 translating from english into german. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurment,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="19910" citStr="Cohen, 1960" startWordPosition="3021" endWordPosition="3022">e discarded. For WMT13, nearly a million pairwise annotations were collected from both researchers and paid workers on Amazon’s Mechanical Turk, in a roughly 1:2 ratio. This year, we collected data from researchers only, an ability that was enabled by the use of a new technique for producing the partial ranking for each task (§3.3.3). Table 3 contains more detail. 3.2 Annotator agreement Each year we calculate annotator agreement scores for the human evaluation as a measure of the reliability of the rankings. We measured pairwise agreement among annotators using Cohen’s kappa coefficient (n) (Cohen, 1960). If P(A) be the proportion of times that the annotators agree, and P(E) is the proportion of time that they would 17 LANGUAGE PAIR Systems Rankings Average Czech–English 5 21,130 4,226.0 English–Czech 10 55,900 5,590.0 German–English 13 25,260 1,943.0 English–German 18 54,660 3,036.6 French–English 8 26,090 3,261.2 English–French 13 33,350 2,565.3 Russian–English 13 34,460 2,650.7 English–Russian 9 28,960 3,217.7 Hindi–English 9 20,900 2,322.2 English–Hindi 12 28,120 2,343.3 TOTAL WMT 14 110 328,830 2,989.3 WMT13 148 942,840 6,370.5 WMT12 103 101,969 999.6 WMT11 133 63,045 474.0 Table 3: Amou</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurment, 20(1):37–46.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Baltimore Translation</author>
</authors>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Maryland, USA.</location>
<marker>Translation, </marker>
<rawString>Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q K Do</author>
<author>T Herrmann</author>
<author>J Niehues</author>
<author>A Allauzen</author>
<author>F Yvon</author>
<author>A Waibel</author>
</authors>
<title>The kit-limsi translation system for wmt</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="17115" citStr="Do et al., 2014" startWordPosition="2561" endWordPosition="2564"> Prague (Tamchyna et al., 2014) DCU-FDA Dublin City University (Bicici et al., 2014) DCU-ICTCAS Dublin City University (Li et al., 2014b) DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014) EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014) KIT Karlsruhe Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014) PROMT-RULE, PROMT PROMT-HYBRID RWTH RWTH Aachen (Peitz et al., 2014) STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014) UA-* University of Alicante (S´anchez-Cartagena et al., 2014) UEDIN-PHRASE, University of Edinburgh (Durrani et al., 2014b) UEDIN-UNCNSTR UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014) UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014) YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014) COMMERC</context>
</contexts>
<marker>Do, Herrmann, Niehues, Allauzen, Yvon, Waibel, 2014</marker>
<rawString>Do, Q. K., Herrmann, T., Niehues, J., Allauzen, A., Yvon, F., and Waibel, A. (2014). The kit-limsi translation system for wmt 2014. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dungarwal</author>
<author>R Chatterjee</author>
<author>A Mishra</author>
<author>A Kunchukuttan</author>
<author>R Shah</author>
<author>P Bhattacharyya</author>
</authors>
<title>The iit bombay hindi-english translation system at wmt</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="16859" citStr="Dungarwal et al., 2014" startWordPosition="2525" endWordPosition="2528">://github.com/cfedermann/Appraise 15 ID Institution AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014) CIMS University of Stuttgart / University of Munich (Cap et al., 2014) CMU Carnegie Mellon University (Matthews et al., 2014) CU-* Charles University, Prague (Tamchyna et al., 2014) DCU-FDA Dublin City University (Bicici et al., 2014) DCU-ICTCAS Dublin City University (Li et al., 2014b) DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014) EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014) KIT Karlsruhe Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014) PROMT-RULE, PROMT PROMT-HYBRID RWTH RWTH Aachen (Peitz et al., 2014) STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014) UA-* University of Alicante (S´anchez-Cartagena et al., 2014) UEDIN-PHRASE, U</context>
</contexts>
<marker>Dungarwal, Chatterjee, Mishra, Kunchukuttan, Shah, Bhattacharyya, 2014</marker>
<rawString>Dungarwal, P., Chatterjee, R., Mishra, A., Kunchukuttan, A., Shah, R., and Bhattacharyya, P. (2014). The iit bombay hindi-english translation system at wmt 2014. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Durrani</author>
<author>B Haddow</author>
<author>P Koehn</author>
<author>K Heafield</author>
</authors>
<title>Edinburgh’s phrase-based machine translation systems for wmt-14.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Ninth Workshop of Statistical Machine Translation,</booktitle>
<location>Baltimore, USA.</location>
<contexts>
<context position="17503" citStr="Durrani et al., 2014" startWordPosition="2618" endWordPosition="2621">erabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014) PROMT-RULE, PROMT PROMT-HYBRID RWTH RWTH Aachen (Peitz et al., 2014) STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014) UA-* University of Alicante (S´anchez-Cartagena et al., 2014) UEDIN-PHRASE, University of Edinburgh (Durrani et al., 2014b) UEDIN-UNCNSTR UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014) UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014) YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014) COMMERCIAL-[1,2] Two commercial machine translation systems ONLINE-[A,B,C,G] Four online statistical machine translation systems RBMT-[1,4] Two rule-based statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective c</context>
<context position="111692" citStr="Durrani et al., 2014" startWordPosition="17795" endWordPosition="17798">ants is given in Table 24; we provide short descriptions of their systems in the following. CUNI was involved in the organization of the task, and their primary goal was to set up a baseline for both the subtasks and for all translation directions. 44 ID Participating team Charles University in Prague (Duˇsek et al., 2014) Dublin City University (Okita et al., 2014) Dublin City University (Zhang et al., 2014) Laboratoire dInformatique pour la Mecanique et les Sciences de lIng´enieur (P´echeux et al., 2014) Pohang University of Science and Technology (Li et al., 2014a) University of Edinburgh (Durrani et al., 2014a) University of Macau (Wang et al., 2014) University of Macau (Lu et al., 2014) CUNI DCU-Q DCU-S LIMSI POSTECH UEDIN UM-DA UM-WDA Table 24: Participants in the WMT14 Medical Translation Task. Their systems are based on the Moses phrasebased toolkit and linear interpolation of in-domain and out-of-domain language models and phrase tables. The constrained/unconstrained systems differ in the training data only. The constrained ones are built using all allowed training data; the unconstrained ones take advantage of additional web-crawled monolingual data used for training of the language models, </context>
</contexts>
<marker>Durrani, Haddow, Koehn, Heafield, 2014</marker>
<rawString>Durrani, N., Haddow, B., Koehn, P., and Heafield, K. (2014a). Edinburgh’s phrase-based machine translation systems for wmt-14. In Proceedings of the ACL 2014 Ninth Workshop of Statistical Machine Translation, Baltimore, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Durrani</author>
<author>B Haddow</author>
<author>P Koehn</author>
<author>K Heafield</author>
</authors>
<title>Edinburghs phrase-based machine translation systems for wmt-14.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="17503" citStr="Durrani et al., 2014" startWordPosition="2618" endWordPosition="2621">erabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014) PROMT-RULE, PROMT PROMT-HYBRID RWTH RWTH Aachen (Peitz et al., 2014) STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014) UA-* University of Alicante (S´anchez-Cartagena et al., 2014) UEDIN-PHRASE, University of Edinburgh (Durrani et al., 2014b) UEDIN-UNCNSTR UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014) UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014) YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014) COMMERCIAL-[1,2] Two commercial machine translation systems ONLINE-[A,B,C,G] Four online statistical machine translation systems RBMT-[1,4] Two rule-based statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective c</context>
<context position="111692" citStr="Durrani et al., 2014" startWordPosition="17795" endWordPosition="17798">ants is given in Table 24; we provide short descriptions of their systems in the following. CUNI was involved in the organization of the task, and their primary goal was to set up a baseline for both the subtasks and for all translation directions. 44 ID Participating team Charles University in Prague (Duˇsek et al., 2014) Dublin City University (Okita et al., 2014) Dublin City University (Zhang et al., 2014) Laboratoire dInformatique pour la Mecanique et les Sciences de lIng´enieur (P´echeux et al., 2014) Pohang University of Science and Technology (Li et al., 2014a) University of Edinburgh (Durrani et al., 2014a) University of Macau (Wang et al., 2014) University of Macau (Lu et al., 2014) CUNI DCU-Q DCU-S LIMSI POSTECH UEDIN UM-DA UM-WDA Table 24: Participants in the WMT14 Medical Translation Task. Their systems are based on the Moses phrasebased toolkit and linear interpolation of in-domain and out-of-domain language models and phrase tables. The constrained/unconstrained systems differ in the training data only. The constrained ones are built using all allowed training data; the unconstrained ones take advantage of additional web-crawled monolingual data used for training of the language models, </context>
</contexts>
<marker>Durrani, Haddow, Koehn, Heafield, 2014</marker>
<rawString>Durrani, N., Haddow, B., Koehn, P., and Heafield, K. (2014b). Edinburghs phrase-based machine translation systems for wmt-14. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Duˇsek</author>
<author>J Hajiˇc</author>
<author>J Hlav´aˇcov´a</author>
<author>M Nov´ak</author>
<author>P Pecina</author>
<author>R Rosa</author>
<author>A Tamchyna</author>
<author>Z Ureˇsov´a</author>
<author>D Zeman</author>
</authors>
<title>Machine translation of medical texts in the khresmoi project.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Ninth Workshop of Statistical Machine Translation,</booktitle>
<location>Baltimore, USA.</location>
<marker>Duˇsek, Hajiˇc, Hlav´aˇcov´a, Nov´ak, Pecina, Rosa, Tamchyna, Ureˇsov´a, Zeman, 2014</marker>
<rawString>Duˇsek, O., Hajiˇc, J., Hlav´aˇcov´a, J., Nov´ak, M., Pecina, P., Rosa, R., Tamchyna, A., Ureˇsov´a, Z., and Zeman, D. (2014). Machine translation of medical texts in the khresmoi project. In Proceedings of the ACL 2014 Ninth Workshop of Statistical Machine Translation, Baltimore, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Federmann</author>
</authors>
<title>Appraise: An OpenSource Toolkit for Manual Evaluation of Machine Translation Output. The Prague Bulletin of Mathematical Linguistics (PBML),</title>
<date>2012</date>
<pages>98--25</pages>
<contexts>
<context position="15954" citStr="Federmann, 2012" startWordPosition="2390" endWordPosition="2392">ection The system ranking is produced from a large set of pairwise annotations between system pairs. These pairwise annotations are collected in an evaluation campaign that enlists participants in the shared task to contribute one hundred “Human Intelligence Tasks” (HITs) per system submitted. Each HIT consists of three ranking tasks. In a ranking task, an annotator is presented with a source segment, a human reference translation, and the outputs of five anonymized systems, randomly selected from the set of participating systems, and randomly ordered. To run the evaluation, we use Appraise5 (Federmann, 2012), an open-source tool built on Python’s Django framework. At the top of each HIT, the following instructions are provided: You are shown a source sentence followed by several candidate translations. Your task is to rank the translations from best to worst (ties are allowed). 5https://github.com/cfedermann/Appraise 15 ID Institution AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014) CIMS University of Stuttgart / University of Munich (Cap et al., 2014) CMU Carnegie Mellon University (Matthews et al., 2014) CU-* Charles University, Prague (Tamchyna et al., 2014) DCU-FDA Dublin City Uni</context>
</contexts>
<marker>Federmann, 2012</marker>
<rawString>Federmann, C. (2012). Appraise: An OpenSource Toolkit for Manual Evaluation of Machine Translation Output. The Prague Bulletin of Mathematical Linguistics (PBML), 98:25– 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Foster</author>
</authors>
<title>Treebanks gone bad: Parser evaluation and retraining using a treebank of ungrammatical sentences.</title>
<date>2007</date>
<journal>International Journal on Document Analysis and Recognition,</journal>
<pages>10--3</pages>
<contexts>
<context position="58183" citStr="Foster, 2007" startWordPosition="9374" endWordPosition="9375">ce features, inverse glass-box features for translating the translation and error grammar parsing features. For machine learning, the QUEST framework is expanded to combine logistic regression and support vector regression and to handle cross- validation and randomisation in a way that training items with the same source side are kept together. External resources are monolingual corpora taken from the WMT 2014 translation task for LMs, the MT system used for the inverse glass-box features (Li et al., 2014b) and, for error grammar parsing, the PennTreebank and an error grammar derived from it (Foster, 2007). which for the above example gives: 1 weighted F1,ALL = weighted F1,ERR = 6189 + 2948 + 476 (6189 · 0.6932 + 2948 · 0.4222 +476 · 0.1575) = 0.5836 1 2948 + 476· (2948 · 0.4222 + 476 · 0.1575) = 0.3854 · 28 Participating team Dublin City University Team 1, Ireland (Hokamp et al., 2014) German Research Centre for Artificial Intelligence, Germany (Avramidis, 2014) Fondazione Bruno Kessler, Italy, UPV Universitat Polit`ecnica de Val`encia, Spain &amp; University of Edinburgh, UK (Camargo de Souza et al., 2014) Laboratoire d’Informatique Grenoble, France (Luong et al., 2014) Laboratoire d’Informatique</context>
</contexts>
<marker>Foster, 2007</marker>
<rawString>Foster, J. (2007). Treebanks gone bad: Parser evaluation and retraining using a treebank of ungrammatical sentences. International Journal on Document Analysis and Recognition, 10(3-4):129–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Freitag</author>
<author>S Peitz</author>
<author>J Wuebker</author>
<author>H Ney</author>
<author>M Huck</author>
<author>R Sennrich</author>
<author>N Durrani</author>
<author>M Nadejde</author>
<author>P Williams</author>
<author>P Koehn</author>
<author>T Herrmann</author>
<author>E Cho</author>
<author>A Waibel</author>
</authors>
<title>Eu-bridge mt: Combined machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="16750" citStr="Freitag et al., 2014" startWordPosition="2509" endWordPosition="2512">candidate translations. Your task is to rank the translations from best to worst (ties are allowed). 5https://github.com/cfedermann/Appraise 15 ID Institution AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014) CIMS University of Stuttgart / University of Munich (Cap et al., 2014) CMU Carnegie Mellon University (Matthews et al., 2014) CU-* Charles University, Prague (Tamchyna et al., 2014) DCU-FDA Dublin City University (Bicici et al., 2014) DCU-ICTCAS Dublin City University (Li et al., 2014b) DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014) EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014) KIT Karlsruhe Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014) PROMT-RULE, PROMT PROMT-HYBRID RWTH RWTH Aachen (Peitz et al., 2014) STANFORD Stanford University (Neidert et</context>
</contexts>
<marker>Freitag, Peitz, Wuebker, Ney, Huck, Sennrich, Durrani, Nadejde, Williams, Koehn, Herrmann, Cho, Waibel, 2014</marker>
<rawString>Freitag, M., Peitz, S., Wuebker, J., Ney, H., Huck, M., Sennrich, R., Durrani, N., Nadejde, M., Williams, P., Koehn, P., Herrmann, T., Cho, E., and Waibel, A. (2014). Eu-bridge mt: Combined machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>A Aue</author>
<author>M Smets</author>
</authors>
<title>Sentence-level MT evaluation without reference translations: beyond language modeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Conference of the European Association for Machine Translation,</booktitle>
<location>Budapest.</location>
<contexts>
<context position="99241" citStr="Gamon et al., 2005" startWordPosition="15865" endWordPosition="15868">sons for that. In Task 2, the official training and test sets already include out-of-domain data because of the very small amount of in-domain data available, and thus is is hard to isolate the effect of this data on the results. Examining the effectiveness of quality prediction methods on human translations Datasets for Tasks 1.1 and 2 contain human translations, in addition to the automatic translations from various MT systems. Predicting human translation quality is an area that has been largely unexplored. Previous work has looked into distinguishing human from machine translations (e.g. (Gamon et al., 2005)), but this problem setting is somehow artificial, and moreover arguably harder to solve nowadays given the higher general quality of current MT systems (Shah and Specia, 2014). Although human translations are obviously of higher quality in general, many segments are translated by MT systems with the same or similar levels of quality as human translation. This is particularly true for Task 2, since data had been previously categorised and only “near misses” were selected for the word-level annotation, i.e., human and machine translations that were both nearly perfect in this case. While no dis</context>
</contexts>
<marker>Gamon, Aue, Smets, 2005</marker>
<rawString>Gamon, M., Aue, A., and Smets, M. (2005). Sentence-level MT evaluation without reference translations: beyond language modeling. In Proceedings of the Annual Conference of the European Association for Machine Translation, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Geurts</author>
<author>D Ernst</author>
<author>L Wehenkel</author>
</authors>
<title>Extremely randomized trees.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>63--1</pages>
<contexts>
<context position="61080" citStr="Geurts et al., 2006" startWordPosition="9834" endWordPosition="9837">used for the Binary setting, and BLSTM-RNNs only for the Level 1 and Multi-class settings. The sentence-level QE submissions (T1.2 and T1.3) are trained on black-box features extracted using QUEST in addition to features based on word alignments, word posterior probabilities and diversity scores (Souza et al., 2013). These features are computed over 100k-best hypothesis translations also used for task 2. In addition, a set of ratios computed from the word-level predictions of the model trained on the binary setting of task 2 is used. A total of 221 features and the extremely randomised trees (Geurts et al., 2006) learning algorithm are used to train regression models. LIG (T2): Conditional Random Fields classifiers are trained with features used in LIG’s WMT13 systems (Luong et al., 2013): target and source words, alignment information, source and target alignment context, LM scores, target and source POS tags, lexical categorisations (stopword, punctuation, proper name, numerical), constituent label, depth in the constituent tree, target polysemy count, pseudo reference. These are combined with novel features: word occurrence in multiple translation systems and POS tag-based LM scores (longest target</context>
</contexts>
<marker>Geurts, Ernst, Wehenkel, 2006</marker>
<rawString>Geurts, P., Ernst, D., and Wehenkel, L. (2006). Extremely randomized trees. Machine Learning, 63(1):3–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Green</author>
<author>D Cer</author>
<author>C Manning</author>
</authors>
<title>Phrasal: A toolkit for new directions in statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="17381" citStr="Green et al., 2014" startWordPosition="2602" endWordPosition="2605">he Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014) PROMT-RULE, PROMT PROMT-HYBRID RWTH RWTH Aachen (Peitz et al., 2014) STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014) UA-* University of Alicante (S´anchez-Cartagena et al., 2014) UEDIN-PHRASE, University of Edinburgh (Durrani et al., 2014b) UEDIN-UNCNSTR UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014) UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014) YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014) COMMERCIAL-[1,2] Two commercial machine translation systems ONLINE-[A,B,C,G] Four online statistical machine translation systems RBMT-[1,4] Two rule-based statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participat</context>
</contexts>
<marker>Green, Cer, Manning, 2014</marker>
<rawString>Green, S., Cer, D., and Manning, C. (2014). Phrasal: A toolkit for new directions in statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hardmeier</author>
<author>S Stymne</author>
<author>J Tiedemann</author>
<author>A Smith</author>
<author>J Nivre</author>
</authors>
<title>Anaphora models and reordering for phrase-based smt.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="17638" citStr="Hardmeier et al., 2014" startWordPosition="2635" endWordPosition="2638">) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014) PROMT-RULE, PROMT PROMT-HYBRID RWTH RWTH Aachen (Peitz et al., 2014) STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014) UA-* University of Alicante (S´anchez-Cartagena et al., 2014) UEDIN-PHRASE, University of Edinburgh (Durrani et al., 2014b) UEDIN-UNCNSTR UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014) UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014) YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014) COMMERCIAL-[1,2] Two commercial machine translation systems ONLINE-[A,B,C,G] Four online statistical machine translation systems RBMT-[1,4] Two rule-based statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 16 Figure 2</context>
</contexts>
<marker>Hardmeier, Stymne, Tiedemann, Smith, Nivre, 2014</marker>
<rawString>Hardmeier, C., Stymne, S., Tiedemann, J., Smith, A., and Nivre, J. (2014). Anaphora models and reordering for phrase-based smt. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Herbrich</author>
<author>T Minka</author>
<author>T Graepel</author>
</authors>
<title>TrueSkillTM: A Bayesian Skill Rating System.</title>
<date>2006</date>
<booktitle>In Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems,</booktitle>
<pages>569--576</pages>
<publisher>MIT Press.</publisher>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="27281" citStr="Herbrich et al. (2006)" startWordPosition="4244" endWordPosition="4247"> σ2a) + N(0, σ2obs) q2 ∼ N(µS2, σ2a) + N(0, σ2obs) 3. Let d be a nonzero real number that defines a fixed decision radius. Produce a rating π according to: &lt; q1 − q2 &gt; d &gt; q2 − q1 &gt; d = otherwise Hopkins and May use Gibbs sampling to infer the set of system means from an annotated dataset. Details of this inference procedure can be found in Sakaguchi et al. (2014). The score used to produce the rankings is simply the system mean associated with each system: scoreHM(Si) = µSi 3.3.3 Method 3: TrueSkill (TS) TrueSkill is an adaptive, online system that employs a similar model of relative ability Herbrich et al. (2006). It was initially developed for Xbox Live’s online player community, where it is used to model player ability, assign levels, and select competitive matches. Each player Sj is modeled by two parameters: TrueSkill’s current estimate of each system’s relative ability, µSj, and a persystem measure of TrueSkill’s uncertainty of those estimates, σ2. When the outcome of a match is j observed, TrueSkill uses the relative status of the two systems to update these estimates. If a translation from a system with a high mean is judged better than a system with a greatly lower mean, the result is not surp</context>
</contexts>
<marker>Herbrich, Minka, Graepel, 2006</marker>
<rawString>Herbrich, R., Minka, T., and Graepel, T. (2006). TrueSkillTM: A Bayesian Skill Rating System. In Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, pages 569–576, Vancouver, British Columbia, Canada. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Herrmann</author>
<author>M Mediani</author>
<author>E Cho</author>
<author>T-L Ha</author>
<author>J Niehues</author>
<author>I Slawik</author>
<author>Y Zhang</author>
<author>A Waibel</author>
</authors>
<title>The karlsruhe institute of technology translation systems for the wmt 2014.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="16812" citStr="Herrmann et al., 2014" startWordPosition="2518" endWordPosition="2521"> from best to worst (ties are allowed). 5https://github.com/cfedermann/Appraise 15 ID Institution AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014) CIMS University of Stuttgart / University of Munich (Cap et al., 2014) CMU Carnegie Mellon University (Matthews et al., 2014) CU-* Charles University, Prague (Tamchyna et al., 2014) DCU-FDA Dublin City University (Bicici et al., 2014) DCU-ICTCAS Dublin City University (Li et al., 2014b) DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014) EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014) KIT Karlsruhe Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014) PROMT-RULE, PROMT PROMT-HYBRID RWTH RWTH Aachen (Peitz et al., 2014) STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014) UA-* University of Alicante (S</context>
</contexts>
<marker>Herrmann, Mediani, Cho, Ha, Niehues, Slawik, Zhang, Waibel, 2014</marker>
<rawString>Herrmann, T., Mediani, M., Cho, E., Ha, T.-L., Niehues, J., Slawik, I., Zhang, Y., and Waibel, A. (2014). The karlsruhe institute of technology translation systems for the wmt 2014. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hokamp</author>
<author>I Calixto</author>
<author>J Wagner</author>
<author>J Zhang</author>
</authors>
<title>Target-centric features for translation quality estimation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="58469" citStr="Hokamp et al., 2014" startWordPosition="9429" endWordPosition="9432">ay that training items with the same source side are kept together. External resources are monolingual corpora taken from the WMT 2014 translation task for LMs, the MT system used for the inverse glass-box features (Li et al., 2014b) and, for error grammar parsing, the PennTreebank and an error grammar derived from it (Foster, 2007). which for the above example gives: 1 weighted F1,ALL = weighted F1,ERR = 6189 + 2948 + 476 (6189 · 0.6932 + 2948 · 0.4222 +476 · 0.1575) = 0.5836 1 2948 + 476· (2948 · 0.4222 + 476 · 0.1575) = 0.3854 · 28 Participating team Dublin City University Team 1, Ireland (Hokamp et al., 2014) German Research Centre for Artificial Intelligence, Germany (Avramidis, 2014) Fondazione Bruno Kessler, Italy, UPV Universitat Polit`ecnica de Val`encia, Spain &amp; University of Edinburgh, UK (Camargo de Souza et al., 2014) Laboratoire d’Informatique Grenoble, France (Luong et al., 2014) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2014) Multilizer, Finland Dublin City University Team 2, Ireland (Bicici and Way, 2014) University of Sheffield Team 1, UK (Beck et al., 2014) University of Sheffield Team 2, UK (Scarton and Specia, 2014) Y</context>
</contexts>
<marker>Hokamp, Calixto, Wagner, Zhang, 2014</marker>
<rawString>Hokamp, C., Calixto, I., Wagner, J., and Zhang, J. (2014). Target-centric features for translation quality estimation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hopkins</author>
<author>J May</author>
</authors>
<title>Models of translation competitions.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1416--1424</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="24921" citStr="Hopkins and May (2013" startWordPosition="3830" endWordPosition="3833">0.344 English–German 0.528 0.433 0.498 0.649 0.452 0.576 French–English 0.673 0.360 0.578 0.585 0.565 0.629 English–French 0.524 0.414 0.495 0.630 0.486 0.507 Hindi–English — — — — — 0.605 English–Hindi — — — — — 0.535 Russian–English — 0.450 0.363 0.477 0.629 English–Russian — 0.513 0.582 0.500 0.570 MEAN 0.583 0.407 0.479 0.522 Table 5: κ scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the human evaluation. lapping ranges. This year, we introduce two new changes. First, we pit the WMT13 method against two new approaches: that of Hopkins and May (2013, §3.3.2), and another based on TrueSkill (Sakaguchi et al., 2014, §3.3.3). Second, we compare these two methods against WMT13’s “Expected Wins” approach, and then select among them by determining which of them has the highest accuracy in terms of predicting annotations on a held-out set of pairwise judgments. 3.3.1 Method 1: Expected Wins (EW) Introduced for WMT13, the EXPECTED WINS has an intuitive score demonstrated to be accurate in ranking systems according to an underlying model of “relative ability” (Koehn, 2012a). The idea is to gauge the probability that a system Si will be ranked bet</context>
<context position="28898" citStr="Hopkins and May (2013)" startWordPosition="4519" endWordPosition="4522">ed means: scoreTS(Si) = µSi This score is then used to sort the systems and produce the ranking. 3.4 Method Selection We have three methods which, provided with the collected data, produce different rankings of the systems. Which of them is correct? More immediately, which one of them should we publish as the official ranking for the WMT14 manual evaluation? As discussed, the method used to compute the ranking has been tweaked a bit each year over the past few years in response to criticisms (e.g., Lopez (2012); Bojar et al. (2011)). While the changes were reasonable (and later corroborated), Hopkins and May (2013) pointed out that this task of model selection should be driven by empirical evaluation on held-out data, and suggested perplexity as the metric of choice. We choose instead a more direct gold-standard evaluation metric: the accuracy of the rankings produced by each method in predicting pairwise judgments. We use each method to produce a partial ordering of the systems, grouping them into equivalence classes. This partial ordering unambiguously assigns a prediction πP between any pair of systems (Si, Sj). By comparing the predicted relationship πP to the actual annotation for each pairwise jud</context>
<context position="38275" citStr="Hopkins and May, 2013" startWordPosition="6125" endWordPosition="6128">the constraints provided for the shared task. 22 tems (18, compared to 13 for the next languages), yet only an average amount of per-system data. Here, we look at this language pair in more detail, in order to justify this decision, and to shed light on the differences between the ranking methods. Table 9 presents the 95% confidence-level clusterings for English–German computed with each of the three methods, along with lines that show the reorderings of the systems between them. Reorderings of this type have been used to argue against the reliability of the official WMT ranking (Lopez, 2012; Hopkins and May, 2013). This table shows that these reorderings are captured entirely by the clustering approach we used. This relative consensus of these independently-computed and somewhat different models suggests that the published ranking is approaching the true ambiguity underlying systems within the same cluster. Looking across all language pairs, we find that the total ordering predicted by EW and TS is exactly the same for eight of the ten language pair tasks, and is constrained to reorderings within the official cluster for the other two (GermanEnglish —just one adjacent swap — and EnglishGerman, depicted</context>
<context position="40048" citStr="Hopkins and May, 2013" startWordPosition="6417" endWordPosition="6420"> been minor perturbations in the permutation of systems. The clustering method of Koehn (2012b), in which the official rankings are presented as a partial (instead of total) ordering, alleviated many of the problems observed by Lopez (2012), and also capture all the variance across the new systems introduced this year. In addition, presenting systems as clusters appeals to intuition. As such, we disagree with claims that there is a problem with irreproducibility of the results of the workshop evaluation task, and especially disagree that there is anything approaching a “crisis of confidence” (Hopkins and May, 2013). These claims seem to us to be overstated. Conducting proper model selection by comparison on held-out data, however, is a welcome suggestion, and our inclusion of this process supports improved confidence in the ranking results. That said, it is notable that the different methods compute very similar orderings. This avoids hallucinating distinctions among systems that are not really there, and captures the intuition that some systems are basically equivalent. The chief benefit of the TrueSkill model is not in outputting a better complete ranking of the systems, but lies in its reduced varian</context>
</contexts>
<marker>Hopkins, May, 2013</marker>
<rawString>Hopkins, M. and May, J. (2013). Models of translation competitions. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1416–1424, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K J¨arvelin</author>
<author>J Kek¨al¨ainen</author>
</authors>
<title>Cumulated gain-based evaluation of ir techniques.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>4</issue>
<marker>J¨arvelin, Kek¨al¨ainen, 2002</marker>
<rawString>J¨arvelin, K. and Kek¨al¨ainen, J. (2002). Cumulated gain-based evaluation of ir techniques. ACM Transactions on Information Systems, 20(4):422–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-D Kim</author>
<author>T Ohta</author>
<author>Y Tateisi</author>
<author>J Tsujii</author>
</authors>
<title>GENIA corpus – a semantically annotated corpus for bio-textmining. Bioinformatics,</title>
<date>2003</date>
<pages>1--180</pages>
<contexts>
<context position="110386" citStr="Kim et al., 2003" startWordPosition="17586" endWordPosition="17589">onging to the categories identified to be medical-domain within the Khresmoi project. The PatTR dataset contains non-parallel data extracted from the medical patents included in the PatTR corpus (see Section 5.3.1). AACT is a collection of restructured and reformatted English texts publicly available and downloadable from ClinicalTrials.gov, containing clinical studies conducted around the world. DrugBank is a bioinformatics and cheminformatics resource containing drug descriptions (Knox et al., 2011). GENIA is a corpus of biomedical literature compiled and annotated within the GENIA project (Kim et al., 2003). FMA stands for the Foundational Model of Anatomy Ontology, a knowledge source for biomedical informatics concerned with symbolic representation of the phenotypic structure of the human body (Rosse and Mejino Jr., 2008). GREC (Gene Regulation Event Corpus) is a semantically annotated English corpus of abstracts of biomedical papers (Thompson et al., 2009). The PIL corpus is a collection of documents giving instructions to patients about their medication (BouayadAgha et al., 2000). 5.4 Participants A total of eight teams participated in the Medical Translation Task by submitting their systems </context>
</contexts>
<marker>Kim, Ohta, Tateisi, Tsujii, 2003</marker>
<rawString>Kim, J.-D., Ohta, T., Tateisi, Y., and Tsujii, J. (2003). GENIA corpus – a semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl 1):i180–i182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Knox</author>
<author>V Law</author>
<author>T Jewison</author>
<author>P Liu</author>
<author>S Ly</author>
<author>A Frolkis</author>
<author>A Pon</author>
<author>K Banco</author>
<author>C Mak</author>
<author>V Neveu</author>
<author>Y Djoumbou</author>
<author>R Eisner</author>
<author>A C Guo</author>
<author>D S Wishart</author>
</authors>
<title>DrugBank 3.0: a comprehensive resource for Omics research on drugs. Nucleic acids research, 39(suppl 1):D1035–D1041.</title>
<date>2011</date>
<contexts>
<context position="110275" citStr="Knox et al., 2011" startWordPosition="17568" endWordPosition="17571">extracted from the UMLS Metathesaurus (see Section 5.3.1). The monolingual Wiki dataset consists of articles belonging to the categories identified to be medical-domain within the Khresmoi project. The PatTR dataset contains non-parallel data extracted from the medical patents included in the PatTR corpus (see Section 5.3.1). AACT is a collection of restructured and reformatted English texts publicly available and downloadable from ClinicalTrials.gov, containing clinical studies conducted around the world. DrugBank is a bioinformatics and cheminformatics resource containing drug descriptions (Knox et al., 2011). GENIA is a corpus of biomedical literature compiled and annotated within the GENIA project (Kim et al., 2003). FMA stands for the Foundational Model of Anatomy Ontology, a knowledge source for biomedical informatics concerned with symbolic representation of the phenotypic structure of the human body (Rosse and Mejino Jr., 2008). GREC (Gene Regulation Event Corpus) is a semantically annotated English corpus of abstracts of biomedical papers (Thompson et al., 2009). The PIL corpus is a collection of documents giving instructions to patients about their medication (BouayadAgha et al., 2000). 5.</context>
</contexts>
<marker>Knox, Law, Jewison, Liu, Ly, Frolkis, Pon, Banco, Mak, Neveu, Djoumbou, Eisner, Guo, Wishart, 2011</marker>
<rawString>Knox, C., Law, V., Jewison, T., Liu, P., Ly, S., Frolkis, A., Pon, A., Banco, K., Mak, C., Neveu, V., Djoumbou, Y., Eisner, R., Guo, A. C., and Wishart, D. S. (2011). DrugBank 3.0: a comprehensive resource for Omics research on drugs. Nucleic acids research, 39(suppl 1):D1035–D1041.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Simulating human judgment in machine translation evaluation campaigns.</title>
<date>2012</date>
<booktitle>In International Workshop on Spoken Language Translation (IWSLT).</booktitle>
<contexts>
<context position="25445" citStr="Koehn, 2012" startWordPosition="3915" endWordPosition="3916">irst, we pit the WMT13 method against two new approaches: that of Hopkins and May (2013, §3.3.2), and another based on TrueSkill (Sakaguchi et al., 2014, §3.3.3). Second, we compare these two methods against WMT13’s “Expected Wins” approach, and then select among them by determining which of them has the highest accuracy in terms of predicting annotations on a held-out set of pairwise judgments. 3.3.1 Method 1: Expected Wins (EW) Introduced for WMT13, the EXPECTED WINS has an intuitive score demonstrated to be accurate in ranking systems according to an underlying model of “relative ability” (Koehn, 2012a). The idea is to gauge the probability that a system Si will be ranked better than another system randomly chosen from a pool of opponents {Sj : j =6 i}. If we define the function win(A, B) as the number of times system A is ranked better than system B, then we can define this as follows: scoreEW (Si) = 1 � win(Si, Sj) |{Sj} |win(Si,Sj) + win(Sj, Si) j,j7i Note that this score ignores ties. 3.3.2 Method 2: Hopkins and May (HM) Hopkins and May (2013) introduced a graphical model formulation of the task, which makes the notion of underlying system ability even more explicit. Each system SJ in </context>
<context position="39519" citStr="Koehn (2012" startWordPosition="6333" endWordPosition="6334"> official ranking method employed by WMT over the past few years has changed a few times as a result of error analysis and introspection. Until this year, these results were largely based on the intuitions of the community and organizers about deficiencies in the models. In addition to their intuitive appeal, many of these changes (such as the decision to throw out comparisons against references) have been empirically validated Hopkins and May (2013). The actual effect of the refinements in the ranking metric has been minor perturbations in the permutation of systems. The clustering method of Koehn (2012b), in which the official rankings are presented as a partial (instead of total) ordering, alleviated many of the problems observed by Lopez (2012), and also capture all the variance across the new systems introduced this year. In addition, presenting systems as clusters appeals to intuition. As such, we disagree with claims that there is a problem with irreproducibility of the results of the workshop evaluation task, and especially disagree that there is anything approaching a “crisis of confidence” (Hopkins and May, 2013). These claims seem to us to be overstated. Conducting proper model sel</context>
</contexts>
<marker>Koehn, 2012</marker>
<rawString>Koehn, P. (2012a). Simulating human judgment in machine translation evaluation campaigns. In International Workshop on Spoken Language Translation (IWSLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Simulating Human Judgment in Machine Translation Evaluation Campaigns.</title>
<date>2012</date>
<booktitle>In Proceedings of the Ninth International Workshop on Spoken Language Translation,</booktitle>
<pages>179--184</pages>
<location>Hong Kong, China.</location>
<contexts>
<context position="25445" citStr="Koehn, 2012" startWordPosition="3915" endWordPosition="3916">irst, we pit the WMT13 method against two new approaches: that of Hopkins and May (2013, §3.3.2), and another based on TrueSkill (Sakaguchi et al., 2014, §3.3.3). Second, we compare these two methods against WMT13’s “Expected Wins” approach, and then select among them by determining which of them has the highest accuracy in terms of predicting annotations on a held-out set of pairwise judgments. 3.3.1 Method 1: Expected Wins (EW) Introduced for WMT13, the EXPECTED WINS has an intuitive score demonstrated to be accurate in ranking systems according to an underlying model of “relative ability” (Koehn, 2012a). The idea is to gauge the probability that a system Si will be ranked better than another system randomly chosen from a pool of opponents {Sj : j =6 i}. If we define the function win(A, B) as the number of times system A is ranked better than system B, then we can define this as follows: scoreEW (Si) = 1 � win(Si, Sj) |{Sj} |win(Si,Sj) + win(Sj, Si) j,j7i Note that this score ignores ties. 3.3.2 Method 2: Hopkins and May (HM) Hopkins and May (2013) introduced a graphical model formulation of the task, which makes the notion of underlying system ability even more explicit. Each system SJ in </context>
<context position="39519" citStr="Koehn (2012" startWordPosition="6333" endWordPosition="6334"> official ranking method employed by WMT over the past few years has changed a few times as a result of error analysis and introspection. Until this year, these results were largely based on the intuitions of the community and organizers about deficiencies in the models. In addition to their intuitive appeal, many of these changes (such as the decision to throw out comparisons against references) have been empirically validated Hopkins and May (2013). The actual effect of the refinements in the ranking metric has been minor perturbations in the permutation of systems. The clustering method of Koehn (2012b), in which the official rankings are presented as a partial (instead of total) ordering, alleviated many of the problems observed by Lopez (2012), and also capture all the variance across the new systems introduced this year. In addition, presenting systems as clusters appeals to intuition. As such, we disagree with claims that there is a problem with irreproducibility of the results of the workshop evaluation task, and especially disagree that there is anything approaching a “crisis of confidence” (Hopkins and May, 2013). These claims seem to us to be overstated. Conducting proper model sel</context>
</contexts>
<marker>Koehn, 2012</marker>
<rawString>Koehn, P. (2012b). Simulating Human Judgment in Machine Translation Evaluation Campaigns. In Proceedings of the Ninth International Workshop on Spoken Language Translation, pages 179–184, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>C Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between European languages.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL 2006 Workshop on Statistical Machine Translation,</booktitle>
<location>New York, New York.</location>
<contexts>
<context position="1346" citStr="Koehn and Monz, 2006" startWordPosition="198" endWordPosition="201">estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2014. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013). This year we conducted four official tasks: a translation task, a quality estimation task, a metrics task1 and a medical translation task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Hindi, and Russian. The Hindi translation tasks were new this year, providing a lesser resourced data condition on a challenging languag</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Koehn, P. and Monz, C. (2006). Manual and automatic evaluation of machine translation between European languages. In Proceedings of NAACL 2006 Workshop on Statistical Machine Translation, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Koppel</author>
<author>N Ordan</author>
</authors>
<title>Translationese and its dialects.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Techologies,</booktitle>
<pages>1318--1326</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="6278" citStr="Koppel and Ordan (2011)" startWordPosition="964" endWordPosition="967">uch as Czech-Russian) that we did not include in the evaluation, it did suffer from artifacts from the larger distance between source and target sentences. Most test sentences involved the translation a source sentence that was translated from a their language into a target sentence (which was compared against a translation from that third language as well). Questions have been raised, if the evaluation of, say, French-English translation is best served when testing on sentences that have been originally written in, say, Czech. For discussions about translationese please for instance refer to Koppel and Ordan (2011). 2http://statmt.org/wmt14/results.html This year, we took about 1500 English sentences and translated them into the other 5 languages, and then additional 1500 sentences from each of the other languages and translated them into English. This gave us test sets of about 3000 sentences for our English-X language pairs, which have been either written originally written in English and translated into X, or vice versa. The composition of the test documents is shown in Table 1. The stories were translated by the professional translation agency Capita, funded by the EU Framework Programme 7 project M</context>
</contexts>
<marker>Koppel, Ordan, 2011</marker>
<rawString>Koppel, M. and Ordan, N. (2011). Translationese and its dialects. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Techologies, pages 1318–1326, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<pages>33--159</pages>
<contexts>
<context position="22000" citStr="Landis and Koch (1977)" startWordPosition="3361" endWordPosition="3365"> + P(A=B)2 + P(A&gt;B)2 Note that each of the three probabilities in P(E)’s definition are squared to reflect the fact that we are considering the chance that two annotators would agree by chance. Each of these probabilities is computed empirically, by observing how often annotators actually rank two systems as being tied. Table 4 gives n values for inter-annotator agreement for WMT11–WMT14 while Table 5 details intra-annotator agreement scores, including the division of researchers (WMT13r) and MTurk (WMT13m) data. The exact interpretation of the kappa coefficient is difficult, but according to Landis and Koch (1977), 0–0.2 is slight, 0.2–0.4 is fair, 0.4–0.6 is moderate, 0.6–0.8 is substantial, and 0.8–1.0 is almost perfect. The agreement rates are more or less in line with prior years: worse for some tasks, better for others, and on average, the best since WMT11 (where agreement scores were likely inflated due to inclusion of reference translations in the comparisons). 3.3 Models of System Rankings The collected pairwise rankings are used to produce a ranking of the systems. Machine translation evaluation has always been a subject of contention, and no exception to this rule exists for the WMT manual ev</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>Landis, J. R. and Koch, G. G. (1977). The measurement of observer agreement for categorical data. Biometrics, 33:159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leusch</author>
<author>N Ueffing</author>
<author>H Ney</author>
</authors>
<title>Cder: Efficient mt evaluation using block movements.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>241--248</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="115282" citStr="Leusch et al., 2006" startWordPosition="18330" endWordPosition="18333">ta selection techniques. UM-WDA submitted systems for all language pairs in the summary translation subtask. Their systems are domain-adapted using web-crawled in-domain resources: bilingual dictionaries and monolingual data. The translation model and language model trained on the crawled data were interpolated with the best-performing language and translation model employed in the UM-DA systems. 5.5 Results MT quality in the Medical Translation Task is evaluated using automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), PER (Tillmann et al., 1997), and CDER (Leusch et al., 2006). BLEU scores are reported as percentage and all error rates are reported as one minus the original value, also as percentage, so that all metrics are in the 0-100 range, and higher scores indicate better translations. The main reason for not conducting human evaluation, as it happens in the standard Trans45 original normalized truecased normalized lowercased ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER Czech--+English CUNI 29.64 29.79±1.07 47.45±1.15 61.64±1.06 52.18±0.98 31.68±1.14 49.84±1.10 64.38±1.06 54.10±0.96 CUNI 22.44 22.57±0.95 41.43±1.16 55.46±1.09 46.42±0.96 32.34±1.12 5</context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2006</marker>
<rawString>Leusch, G., Ueffing, N., and Ney, H. (2006). Cder: Efficient mt evaluation using block movements. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 241–248, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Li</author>
<author>S-J Kim</author>
<author>H Na</author>
<author>J-H Lee</author>
</authors>
<title>Postech’s system description for medical text translation task.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Ninth Workshop of Statistical Machine Translation,</booktitle>
<location>Baltimore, USA.</location>
<contexts>
<context position="16634" citStr="Li et al., 2014" startWordPosition="2492" endWordPosition="2495">top of each HIT, the following instructions are provided: You are shown a source sentence followed by several candidate translations. Your task is to rank the translations from best to worst (ties are allowed). 5https://github.com/cfedermann/Appraise 15 ID Institution AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014) CIMS University of Stuttgart / University of Munich (Cap et al., 2014) CMU Carnegie Mellon University (Matthews et al., 2014) CU-* Charles University, Prague (Tamchyna et al., 2014) DCU-FDA Dublin City University (Bicici et al., 2014) DCU-ICTCAS Dublin City University (Li et al., 2014b) DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014) EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014) KIT Karlsruhe Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al.,</context>
<context position="58080" citStr="Li et al., 2014" startWordPosition="9355" endWordPosition="9358">os, and four additional feature types: combined POS and stop word LM features, source-side pseudo-reference features, inverse glass-box features for translating the translation and error grammar parsing features. For machine learning, the QUEST framework is expanded to combine logistic regression and support vector regression and to handle cross- validation and randomisation in a way that training items with the same source side are kept together. External resources are monolingual corpora taken from the WMT 2014 translation task for LMs, the MT system used for the inverse glass-box features (Li et al., 2014b) and, for error grammar parsing, the PennTreebank and an error grammar derived from it (Foster, 2007). which for the above example gives: 1 weighted F1,ALL = weighted F1,ERR = 6189 + 2948 + 476 (6189 · 0.6932 + 2948 · 0.4222 +476 · 0.1575) = 0.5836 1 2948 + 476· (2948 · 0.4222 + 476 · 0.1575) = 0.3854 · 28 Participating team Dublin City University Team 1, Ireland (Hokamp et al., 2014) German Research Centre for Artificial Intelligence, Germany (Avramidis, 2014) Fondazione Bruno Kessler, Italy, UPV Universitat Polit`ecnica de Val`encia, Spain &amp; University of Edinburgh, UK (Camargo de Souza et</context>
<context position="111644" citStr="Li et al., 2014" startWordPosition="17788" endWordPosition="17791">nslation directions. A list of the participants is given in Table 24; we provide short descriptions of their systems in the following. CUNI was involved in the organization of the task, and their primary goal was to set up a baseline for both the subtasks and for all translation directions. 44 ID Participating team Charles University in Prague (Duˇsek et al., 2014) Dublin City University (Okita et al., 2014) Dublin City University (Zhang et al., 2014) Laboratoire dInformatique pour la Mecanique et les Sciences de lIng´enieur (P´echeux et al., 2014) Pohang University of Science and Technology (Li et al., 2014a) University of Edinburgh (Durrani et al., 2014a) University of Macau (Wang et al., 2014) University of Macau (Lu et al., 2014) CUNI DCU-Q DCU-S LIMSI POSTECH UEDIN UM-DA UM-WDA Table 24: Participants in the WMT14 Medical Translation Task. Their systems are based on the Moses phrasebased toolkit and linear interpolation of in-domain and out-of-domain language models and phrase tables. The constrained/unconstrained systems differ in the training data only. The constrained ones are built using all allowed training data; the unconstrained ones take advantage of additional web-crawled monolingual</context>
</contexts>
<marker>Li, Kim, Na, Lee, 2014</marker>
<rawString>Li, J., Kim, S.-J., Na, H., and Lee, J.-H. (2014a). Postech’s system description for medical text translation task. In Proceedings of the ACL 2014 Ninth Workshop of Statistical Machine Translation, Baltimore, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Li</author>
<author>X Wu</author>
<author>S C Vaillo</author>
<author>J Xie</author>
<author>A Way</author>
<author>Q Liu</author>
</authors>
<title>The dcu-ictcas mt system at wmt 2014 on german-english translation task.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="16634" citStr="Li et al., 2014" startWordPosition="2492" endWordPosition="2495">top of each HIT, the following instructions are provided: You are shown a source sentence followed by several candidate translations. Your task is to rank the translations from best to worst (ties are allowed). 5https://github.com/cfedermann/Appraise 15 ID Institution AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014) CIMS University of Stuttgart / University of Munich (Cap et al., 2014) CMU Carnegie Mellon University (Matthews et al., 2014) CU-* Charles University, Prague (Tamchyna et al., 2014) DCU-FDA Dublin City University (Bicici et al., 2014) DCU-ICTCAS Dublin City University (Li et al., 2014b) DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014) EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014) KIT Karlsruhe Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al.,</context>
<context position="58080" citStr="Li et al., 2014" startWordPosition="9355" endWordPosition="9358">os, and four additional feature types: combined POS and stop word LM features, source-side pseudo-reference features, inverse glass-box features for translating the translation and error grammar parsing features. For machine learning, the QUEST framework is expanded to combine logistic regression and support vector regression and to handle cross- validation and randomisation in a way that training items with the same source side are kept together. External resources are monolingual corpora taken from the WMT 2014 translation task for LMs, the MT system used for the inverse glass-box features (Li et al., 2014b) and, for error grammar parsing, the PennTreebank and an error grammar derived from it (Foster, 2007). which for the above example gives: 1 weighted F1,ALL = weighted F1,ERR = 6189 + 2948 + 476 (6189 · 0.6932 + 2948 · 0.4222 +476 · 0.1575) = 0.5836 1 2948 + 476· (2948 · 0.4222 + 476 · 0.1575) = 0.3854 · 28 Participating team Dublin City University Team 1, Ireland (Hokamp et al., 2014) German Research Centre for Artificial Intelligence, Germany (Avramidis, 2014) Fondazione Bruno Kessler, Italy, UPV Universitat Polit`ecnica de Val`encia, Spain &amp; University of Edinburgh, UK (Camargo de Souza et</context>
<context position="111644" citStr="Li et al., 2014" startWordPosition="17788" endWordPosition="17791">nslation directions. A list of the participants is given in Table 24; we provide short descriptions of their systems in the following. CUNI was involved in the organization of the task, and their primary goal was to set up a baseline for both the subtasks and for all translation directions. 44 ID Participating team Charles University in Prague (Duˇsek et al., 2014) Dublin City University (Okita et al., 2014) Dublin City University (Zhang et al., 2014) Laboratoire dInformatique pour la Mecanique et les Sciences de lIng´enieur (P´echeux et al., 2014) Pohang University of Science and Technology (Li et al., 2014a) University of Edinburgh (Durrani et al., 2014a) University of Macau (Wang et al., 2014) University of Macau (Lu et al., 2014) CUNI DCU-Q DCU-S LIMSI POSTECH UEDIN UM-DA UM-WDA Table 24: Participants in the WMT14 Medical Translation Task. Their systems are based on the Moses phrasebased toolkit and linear interpolation of in-domain and out-of-domain language models and phrase tables. The constrained/unconstrained systems differ in the training data only. The constrained ones are built using all allowed training data; the unconstrained ones take advantage of additional web-crawled monolingual</context>
</contexts>
<marker>Li, Wu, Vaillo, Xie, Way, Liu, 2014</marker>
<rawString>Li, L., Wu, X., Vaillo, S. C., Xie, J., Way, A., and Liu, Q. (2014b). The dcu-ictcas mt system at wmt 2014 on german-english translation task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
</authors>
<title>Putting Human Assessments of Machine Translation Systems in Order.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>1--9</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="28792" citStr="Lopez (2012)" startWordPosition="4505" endWordPosition="4506">ll on WMT13 data. Similar to the Hopkins and May model, TrueSkill scores systems by their inferred means: scoreTS(Si) = µSi This score is then used to sort the systems and produce the ranking. 3.4 Method Selection We have three methods which, provided with the collected data, produce different rankings of the systems. Which of them is correct? More immediately, which one of them should we publish as the official ranking for the WMT14 manual evaluation? As discussed, the method used to compute the ranking has been tweaked a bit each year over the past few years in response to criticisms (e.g., Lopez (2012); Bojar et al. (2011)). While the changes were reasonable (and later corroborated), Hopkins and May (2013) pointed out that this task of model selection should be driven by empirical evaluation on held-out data, and suggested perplexity as the metric of choice. We choose instead a more direct gold-standard evaluation metric: the accuracy of the rankings produced by each method in predicting pairwise judgments. We use each method to produce a partial ordering of the systems, grouping them into equivalence classes. This partial ordering unambiguously assigns a prediction πP between any pair of s</context>
<context position="38251" citStr="Lopez, 2012" startWordPosition="6123" endWordPosition="6124">fall outside the constraints provided for the shared task. 22 tems (18, compared to 13 for the next languages), yet only an average amount of per-system data. Here, we look at this language pair in more detail, in order to justify this decision, and to shed light on the differences between the ranking methods. Table 9 presents the 95% confidence-level clusterings for English–German computed with each of the three methods, along with lines that show the reorderings of the systems between them. Reorderings of this type have been used to argue against the reliability of the official WMT ranking (Lopez, 2012; Hopkins and May, 2013). This table shows that these reorderings are captured entirely by the clustering approach we used. This relative consensus of these independently-computed and somewhat different models suggests that the published ranking is approaching the true ambiguity underlying systems within the same cluster. Looking across all language pairs, we find that the total ordering predicted by EW and TS is exactly the same for eight of the ten language pair tasks, and is constrained to reorderings within the official cluster for the other two (GermanEnglish —just one adjacent swap — and</context>
<context position="39666" citStr="Lopez (2012)" startWordPosition="6358" endWordPosition="6359">is year, these results were largely based on the intuitions of the community and organizers about deficiencies in the models. In addition to their intuitive appeal, many of these changes (such as the decision to throw out comparisons against references) have been empirically validated Hopkins and May (2013). The actual effect of the refinements in the ranking metric has been minor perturbations in the permutation of systems. The clustering method of Koehn (2012b), in which the official rankings are presented as a partial (instead of total) ordering, alleviated many of the problems observed by Lopez (2012), and also capture all the variance across the new systems introduced this year. In addition, presenting systems as clusters appeals to intuition. As such, we disagree with claims that there is a problem with irreproducibility of the results of the workshop evaluation task, and especially disagree that there is anything approaching a “crisis of confidence” (Hopkins and May, 2013). These claims seem to us to be overstated. Conducting proper model selection by comparison on held-out data, however, is a welcome suggestion, and our inclusion of this process supports improved confidence in the rank</context>
</contexts>
<marker>Lopez, 2012</marker>
<rawString>Lopez, A. (2012). Putting Human Assessments of Machine Translation Systems in Order. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 1–9, Montr´eal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lu</author>
<author>L Wang</author>
<author>D F Wong</author>
<author>L S Chao</author>
<author>Y Wang</author>
<author>F Oliveira</author>
</authors>
<title>Domain adaptation for medical text translation using web resources.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Ninth Workshop of Statistical Machine Translation,</booktitle>
<location>Baltimore, USA.</location>
<contexts>
<context position="111772" citStr="Lu et al., 2014" startWordPosition="17809" endWordPosition="17812">lowing. CUNI was involved in the organization of the task, and their primary goal was to set up a baseline for both the subtasks and for all translation directions. 44 ID Participating team Charles University in Prague (Duˇsek et al., 2014) Dublin City University (Okita et al., 2014) Dublin City University (Zhang et al., 2014) Laboratoire dInformatique pour la Mecanique et les Sciences de lIng´enieur (P´echeux et al., 2014) Pohang University of Science and Technology (Li et al., 2014a) University of Edinburgh (Durrani et al., 2014a) University of Macau (Wang et al., 2014) University of Macau (Lu et al., 2014) CUNI DCU-Q DCU-S LIMSI POSTECH UEDIN UM-DA UM-WDA Table 24: Participants in the WMT14 Medical Translation Task. Their systems are based on the Moses phrasebased toolkit and linear interpolation of in-domain and out-of-domain language models and phrase tables. The constrained/unconstrained systems differ in the training data only. The constrained ones are built using all allowed training data; the unconstrained ones take advantage of additional web-crawled monolingual data used for training of the language models, and additional parallel nonmedical data from the PatTr and COPPA patent collecti</context>
</contexts>
<marker>Lu, Wang, Wong, Chao, Wang, Oliveira, 2014</marker>
<rawString>Lu, Y., Wang, L., Wong, D. F., Chao, L. S., Wang, Y., and Oliveira, F. (2014). Domain adaptation for medical text translation using web resources. In Proceedings of the ACL 2014 Ninth Workshop of Statistical Machine Translation, Baltimore, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Q Luong</author>
<author>L Besacier</author>
<author>B Lecouteux</author>
</authors>
<title>Lig system for word level qe task at wmt14.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="58756" citStr="Luong et al., 2014" startWordPosition="9467" endWordPosition="9470">n error grammar derived from it (Foster, 2007). which for the above example gives: 1 weighted F1,ALL = weighted F1,ERR = 6189 + 2948 + 476 (6189 · 0.6932 + 2948 · 0.4222 +476 · 0.1575) = 0.5836 1 2948 + 476· (2948 · 0.4222 + 476 · 0.1575) = 0.3854 · 28 Participating team Dublin City University Team 1, Ireland (Hokamp et al., 2014) German Research Centre for Artificial Intelligence, Germany (Avramidis, 2014) Fondazione Bruno Kessler, Italy, UPV Universitat Polit`ecnica de Val`encia, Spain &amp; University of Edinburgh, UK (Camargo de Souza et al., 2014) Laboratoire d’Informatique Grenoble, France (Luong et al., 2014) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2014) Multilizer, Finland Dublin City University Team 2, Ireland (Bicici and Way, 2014) University of Sheffield Team 1, UK (Beck et al., 2014) University of Sheffield Team 2, UK (Scarton and Specia, 2014) Yandex, Russia DCU DFKI FBK-UPV-UEDIN LIG LIMSI MULTILIZER RTM-DCU SHEF-lite USHEFF YANDEX ID Table 10: Participants in the WMT14 Quality Estimation shared task. DFKI (T1.2): DFKI/SVR builds upon the baseline system (above) by adding non-redundant data from the WMT13 task for predicting </context>
</contexts>
<marker>Luong, Besacier, Lecouteux, 2014</marker>
<rawString>Luong, N. Q., Besacier, L., and Lecouteux, B. (2014). Lig system for word level qe task at wmt14. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Q Luong</author>
<author>B Lecouteux</author>
<author>L Besacier</author>
</authors>
<title>LIG system for WMT13 QE task: Investigating the usefulness of features in word confidence estimation for MT.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>384--389</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<contexts>
<context position="61259" citStr="Luong et al., 2013" startWordPosition="9863" endWordPosition="9866">cted using QUEST in addition to features based on word alignments, word posterior probabilities and diversity scores (Souza et al., 2013). These features are computed over 100k-best hypothesis translations also used for task 2. In addition, a set of ratios computed from the word-level predictions of the model trained on the binary setting of task 2 is used. A total of 221 features and the extremely randomised trees (Geurts et al., 2006) learning algorithm are used to train regression models. LIG (T2): Conditional Random Fields classifiers are trained with features used in LIG’s WMT13 systems (Luong et al., 2013): target and source words, alignment information, source and target alignment context, LM scores, target and source POS tags, lexical categorisations (stopword, punctuation, proper name, numerical), constituent label, depth in the constituent tree, target polysemy count, pseudo reference. These are combined with novel features: word occurrence in multiple translation systems and POS tag-based LM scores (longest target/source n-gram length and backoff score for POS tag). These features require external NLP tools and resources such as: TreeTagger, GIZA++, Bekerley parser, Link Grammar parser, Wo</context>
</contexts>
<marker>Luong, Lecouteux, Besacier, 2013</marker>
<rawString>Luong, N. Q., Lecouteux, B., and Besacier, L. (2013). LIG system for WMT13 QE task: Investigating the usefulness of features in word confidence estimation for MT. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 384–389, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mach´aˇcek</author>
<author>O Bojar</author>
</authors>
<title>Results of the wmt14 metrics shared task.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<marker>Mach´aˇcek, Bojar, 2014</marker>
<rawString>Mach´aˇcek, M. and Bojar, O. (2014). Results of the wmt14 metrics shared task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Matthews</author>
<author>W Ammar</author>
<author>A Bhatia</author>
<author>W Feely</author>
<author>G Hanneman</author>
<author>E Schlinger</author>
<author>S Swayamdipta</author>
<author>Y Tsvetkov</author>
<author>A Lavie</author>
<author>C Dyer</author>
</authors>
<title>The cmu machine translation systems at wmt</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="16474" citStr="Matthews et al., 2014" startWordPosition="2468" endWordPosition="2471">participating systems, and randomly ordered. To run the evaluation, we use Appraise5 (Federmann, 2012), an open-source tool built on Python’s Django framework. At the top of each HIT, the following instructions are provided: You are shown a source sentence followed by several candidate translations. Your task is to rank the translations from best to worst (ties are allowed). 5https://github.com/cfedermann/Appraise 15 ID Institution AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014) CIMS University of Stuttgart / University of Munich (Cap et al., 2014) CMU Carnegie Mellon University (Matthews et al., 2014) CU-* Charles University, Prague (Tamchyna et al., 2014) DCU-FDA Dublin City University (Bicici et al., 2014) DCU-ICTCAS Dublin City University (Li et al., 2014b) DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014) EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014) KIT Karlsruhe Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe </context>
</contexts>
<marker>Matthews, Ammar, Bhatia, Feely, Hanneman, Schlinger, Swayamdipta, Tsvetkov, Lavie, Dyer, 2014</marker>
<rawString>Matthews, A., Ammar, W., Bhatia, A., Feely, W., Hanneman, G., Schlinger, E., Swayamdipta, S., Tsvetkov, Y., Lavie, A., and Dyer, C. (2014). The cmu machine translation systems at wmt 2014. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Neidert</author>
<author>S Schuster</author>
<author>S Green</author>
<author>K Heafield</author>
<author>C Manning</author>
</authors>
<title>Stanford universitys submissions to the wmt 2014 translation task.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="17360" citStr="Neidert et al., 2014" startWordPosition="2598" endWordPosition="2601">al., 2014) KIT Karlsruhe Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014) PROMT-RULE, PROMT PROMT-HYBRID RWTH RWTH Aachen (Peitz et al., 2014) STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014) UA-* University of Alicante (S´anchez-Cartagena et al., 2014) UEDIN-PHRASE, University of Edinburgh (Durrani et al., 2014b) UEDIN-UNCNSTR UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014) UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014) YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014) COMMERCIAL-[1,2] Two commercial machine translation systems ONLINE-[A,B,C,G] Four online statistical machine translation systems RBMT-[1,4] Two rule-based statistical machine translation systems Table 2: Participants in the shared translation task. Not</context>
</contexts>
<marker>Neidert, Schuster, Green, Heafield, Manning, 2014</marker>
<rawString>Neidert, J., Schuster, S., Green, S., Heafield, K., and Manning, C. (2014). Stanford universitys submissions to the wmt 2014 translation task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Okita</author>
<author>A H Vahid</author>
<author>A Way</author>
<author>Q Liu</author>
</authors>
<title>Dcu terminology translation system for medical query subtask at wmt14.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Ninth Workshop of Statistical Machine Translation,</booktitle>
<location>Baltimore, USA.</location>
<contexts>
<context position="111440" citStr="Okita et al., 2014" startWordPosition="17757" endWordPosition="17760"> about their medication (BouayadAgha et al., 2000). 5.4 Participants A total of eight teams participated in the Medical Translation Task by submitting their systems to at least one subtask for one or more translation directions. A list of the participants is given in Table 24; we provide short descriptions of their systems in the following. CUNI was involved in the organization of the task, and their primary goal was to set up a baseline for both the subtasks and for all translation directions. 44 ID Participating team Charles University in Prague (Duˇsek et al., 2014) Dublin City University (Okita et al., 2014) Dublin City University (Zhang et al., 2014) Laboratoire dInformatique pour la Mecanique et les Sciences de lIng´enieur (P´echeux et al., 2014) Pohang University of Science and Technology (Li et al., 2014a) University of Edinburgh (Durrani et al., 2014a) University of Macau (Wang et al., 2014) University of Macau (Lu et al., 2014) CUNI DCU-Q DCU-S LIMSI POSTECH UEDIN UM-DA UM-WDA Table 24: Participants in the WMT14 Medical Translation Task. Their systems are based on the Moses phrasebased toolkit and linear interpolation of in-domain and out-of-domain language models and phrase tables. The con</context>
</contexts>
<marker>Okita, Vahid, Way, Liu, 2014</marker>
<rawString>Okita, T., Vahid, A. H., Way, A., and Liu, Q. (2014). Dcu terminology translation system for medical query subtask at wmt14. In Proceedings of the ACL 2014 Ninth Workshop of Statistical Machine Translation, Baltimore, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="115194" citStr="Papineni et al., 2002" startWordPosition="18314" endWordPosition="18317">Data for the domain-adapted language and translation models were selected using various data selection techniques. UM-WDA submitted systems for all language pairs in the summary translation subtask. Their systems are domain-adapted using web-crawled in-domain resources: bilingual dictionaries and monolingual data. The translation model and language model trained on the crawled data were interpolated with the best-performing language and translation model employed in the UM-DA systems. 5.5 Results MT quality in the Medical Translation Task is evaluated using automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), PER (Tillmann et al., 1997), and CDER (Leusch et al., 2006). BLEU scores are reported as percentage and all error rates are reported as one minus the original value, also as percentage, so that all metrics are in the 0-100 range, and higher scores indicate better translations. The main reason for not conducting human evaluation, as it happens in the standard Trans45 original normalized truecased normalized lowercased ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER Czech--+English CUNI 29.64 29.79±1.07 47.45±1.15 61.64±1.06 52.18±0.98 31.68±1.14 49.84±1.10 6</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>N P´echeux</author>
<author>L Gong</author>
<author>Q K Do</author>
<author>B Marie</author>
<author>Y Ivanishcheva</author>
<author>A Allauzen</author>
<author>T Lavergne</author>
</authors>
<marker>P´echeux, Gong, Do, Marie, Ivanishcheva, Allauzen, Lavergne, </marker>
<rawString>P´echeux, N., Gong, L., Do, Q. K., Marie, B., Ivanishcheva, Y., Allauzen, A., Lavergne, T.,</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Niehues</author>
<author>A Max</author>
<author>Y Yvon</author>
</authors>
<title>LIMSI @ WMT’14 Medical Translation Task.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<location>Baltimore, USA.</location>
<marker>Niehues, Max, Yvon, 2014</marker>
<rawString>Niehues, J., Max, A., and Yvon, Y. (2014). LIMSI @ WMT’14 Medical Translation Task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pecina</author>
<author>O Duˇsek</author>
<author>L Goeuriot</author>
<author>J Hajiˇc</author>
<author>J Hlav´aˇcov´a</author>
<author>G Jones</author>
<author>L Kelly</author>
<author>J Leveling</author>
<author>D Mareˇcek</author>
<author>M Nov´ak</author>
<author>M Popel</author>
<author>R Rosa</author>
<author>A Tamchyna</author>
<author>Z Ureˇsov´a</author>
</authors>
<title>Adaptation of machine translation for multilingual information retrieval in the medical domain.</title>
<date>2014</date>
<journal>Artificial Intelligence in Medicine,</journal>
<volume>0</volume>
<marker>Pecina, Duˇsek, Goeuriot, Hajiˇc, Hlav´aˇcov´a, Jones, Kelly, Leveling, Mareˇcek, Nov´ak, Popel, Rosa, Tamchyna, Ureˇsov´a, 2014</marker>
<rawString>Pecina, P., Duˇsek, O., Goeuriot, L., Hajiˇc, J., Hlav´aˇcov´a, J., Jones, G., Kelly, L., Leveling, J., Mareˇcek, D., Nov´ak, M., Popel, M., Rosa, R., Tamchyna, A., and Ureˇsov´a, Z. (2014). Adaptation of machine translation for multilingual information retrieval in the medical domain. Artificial Intelligence in Medicine, (0):–.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Peitz</author>
<author>J Wuebker</author>
<author>M Freitag</author>
<author>H Ney</author>
</authors>
<title>The rwth aachen german-english machine translation system for wmt</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="17309" citStr="Peitz et al., 2014" startWordPosition="2591" endWordPosition="2594">l., 2014) EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014) KIT Karlsruhe Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014) PROMT-RULE, PROMT PROMT-HYBRID RWTH RWTH Aachen (Peitz et al., 2014) STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014) UA-* University of Alicante (S´anchez-Cartagena et al., 2014) UEDIN-PHRASE, University of Edinburgh (Durrani et al., 2014b) UEDIN-UNCNSTR UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014) UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014) YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014) COMMERCIAL-[1,2] Two commercial machine translation systems ONLINE-[A,B,C,G] Four online statistical machine translation systems RBMT-[1,4] Two rule-based statistical machine translation systems Table </context>
</contexts>
<marker>Peitz, Wuebker, Freitag, Ney, 2014</marker>
<rawString>Peitz, S., Wuebker, J., Freitag, M., and Ney, H. (2014). The rwth aachen german-english machine translation system for wmt 2014. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pouliquen</author>
<author>C Mazenc</author>
</authors>
<title>COPPA, CLIR and TAPTA: three tools to assist in overcoming the patent barrier at WIPO.</title>
<date>2011</date>
<booktitle>In Proceedings of the Thirteenth Machine Translation Summit,</booktitle>
<pages>24--30</pages>
<location>Xiamen,</location>
<contexts>
<context position="109252" citStr="Pouliquen and Mazenc, 2011" startWordPosition="17419" endWordPosition="17422">o the categories identified to be medical-domain within the Khresmoi project. It is available for all three language pairs. The MuchMore Springer Corpus is a German–English parallel corpus of medical journals abstracts published by Springer (Buitelaar et al., 2003). PatTR is a parallel corpus extracted from the MAREC patent collection (W¨aschle and Riezler, 2012). It is available for German–English and French–English. For the medical domain, we only consider text from patents indicated to be from the medicine-related categories (A61, C12N, C12P). COPPA (Corpus of Parallel Patent Applications (Pouliquen and Mazenc, 2011) is a French–English parallel corpus extracted from the MAREC patent collection (W¨aschle and Riezler, 2012). The medical-domain subset is identified by the same categories as in PatTR. 5.3.2 Monolingual Training Data The medical-domain monolingual data consists of the following corpora (statistics are presented in Table 23): The monolingual UMLS dataset contains concept descriptions in CS, DE, and FR extracted from the UMLS Metathesaurus (see Section 5.3.1). The monolingual Wiki dataset consists of articles belonging to the categories identified to be medical-domain within the Khresmoi projec</context>
</contexts>
<marker>Pouliquen, Mazenc, 2011</marker>
<rawString>Pouliquen, B. and Mazenc, C. (2011). COPPA, CLIR and TAPTA: three tools to assist in overcoming the patent barrier at WIPO. In Proceedings of the Thirteenth Machine Translation Summit, pages 24–30, Xiamen, China. AsiaPacific Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M W Powers</author>
</authors>
<title>Evaluation: from precision, recall and f-measure to roc, informedness, markedness &amp; correlation.</title>
<date>2011</date>
<journal>Journal of Machine Learning Technologies.</journal>
<contexts>
<context position="55150" citStr="Powers (2011)" startWordPosition="8876" endWordPosition="8877">ge of F1,c scores predicted F 1819 1333 214 weighted by the occurrence count N(c) of c: A 198 133 69 c 1 weighted F1,ERR = weighted F1 ,ALL = E 1N(c)1:Nc · F1,c c 1: Nc · F1 Ec:c=,4O N(c) c:c=,4O 27 We choose F1,ERR as our primary evaluation measure because it most closely mimics the common application of F1 scores in binary classification: one is interested in the performance in detecting a positive class, which in this case would be erroneous words. This does, however, ignore the number of correctly classified words of the OK class, which is why we also report F1,ALL. In addition, we follow Powers (2011) and report Matthews Correlation Coefficient (MCC), averaged in the same way as F1, as our secondary metric. Finally, for contrast we also report Accuracy (ACC). 4.3 Participants Table 10 lists all participating teams. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier: T1.1, T1.2, T1.3, and T2. Sentence-level baseline system (T1.1, T1.2, T1.3): QUEST is used to extract 17 systemindependent features from source and translation sentences and parallel corpora (same features as in </context>
</contexts>
<marker>Powers, 2011</marker>
<rawString>Powers, D. M. W. (2011). Evaluation: from precision, recall and f-measure to roc, informedness, markedness &amp; correlation. Journal of Machine Learning Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Quernheim</author>
<author>F Cap</author>
</authors>
<title>Large-scale exact decoding: The ims-ttt submission to wmt14.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="16970" citStr="Quernheim and Cap, 2014" startWordPosition="2540" endWordPosition="2543">4) CIMS University of Stuttgart / University of Munich (Cap et al., 2014) CMU Carnegie Mellon University (Matthews et al., 2014) CU-* Charles University, Prague (Tamchyna et al., 2014) DCU-FDA Dublin City University (Bicici et al., 2014) DCU-ICTCAS Dublin City University (Li et al., 2014b) DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014) EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014) KIT Karlsruhe Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014) PROMT-RULE, PROMT PROMT-HYBRID RWTH RWTH Aachen (Peitz et al., 2014) STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014) UA-* University of Alicante (S´anchez-Cartagena et al., 2014) UEDIN-PHRASE, University of Edinburgh (Durrani et al., 2014b) UEDIN-UNCNSTR UEDIN-SYNTAX University of Edinburgh (Williams et </context>
</contexts>
<marker>Quernheim, Cap, 2014</marker>
<rawString>Quernheim, D. and Cap, F. (2014). Large-scale exact decoding: The ims-ttt submission to wmt14. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Rosse</author>
<author>Mejino Jr</author>
<author>J L V</author>
</authors>
<title>The foundational model of anatomy ontology.</title>
<date>2008</date>
<booktitle>Anatomy Ontologies for Bioinformatics,</booktitle>
<volume>6</volume>
<pages>59--117</pages>
<editor>In Burger, A., Davidson, D., and Baldock, R., editors,</editor>
<publisher>Springer</publisher>
<location>London.</location>
<marker>Rosse, Jr, V, 2008</marker>
<rawString>Rosse, C. and Mejino Jr., J. L. V. (2008). The foundational model of anatomy ontology. In Burger, A., Davidson, D., and Baldock, R., editors, Anatomy Ontologies for Bioinformatics, volume 6 of Computational Biology, pages 59– 117. Springer London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rubino</author>
<author>A Toral</author>
<author>V M S´anchez-Cartagena</author>
<author>J Ferr´andez-Tordera</author>
<author>Ortiz Rojas</author>
<author>S RamirezS´anchez</author>
<author>G S´anchez-Martinez</author>
<author>F</author>
<author>A Way</author>
</authors>
<title>Abu-matran at wmt 2014 translation task: Two-step data selection and rbmtstyle synthetic rules.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<marker>Rubino, Toral, S´anchez-Cartagena, Ferr´andez-Tordera, Rojas, RamirezS´anchez, S´anchez-Martinez, F, Way, 2014</marker>
<rawString>Rubino, R., Toral, A., S´anchez-Cartagena, V. M., Ferr´andez-Tordera, J., Ortiz Rojas, S., RamirezS´anchez, G., S´anchez-Martinez, F., and Way, A. (2014). Abu-matran at wmt 2014 translation task: Two-step data selection and rbmtstyle synthetic rules. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sakaguchi</author>
<author>M Post</author>
<author>B Van Durme</author>
</authors>
<title>Efficient elicitation of annotations for human evaluation of machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<location>Baltimore, Maryland.</location>
<marker>Sakaguchi, Post, Van Durme, 2014</marker>
<rawString>Sakaguchi, K., Post, M., and Van Durme, B. (2014). Efficient elicitation of annotations for human evaluation of machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V M S´anchez-Cartagena</author>
<author>J A P´erez-Ortiz</author>
<author>F S´anchez-Martinez</author>
</authors>
<title>The ua-prompsit hybrid machine translation system for the 2014 workshop on statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<marker>S´anchez-Cartagena, P´erez-Ortiz, S´anchez-Martinez, 2014</marker>
<rawString>S´anchez-Cartagena, V. M., P´erez-Ortiz, J. A., and S´anchez-Martinez, F. (2014). The ua-prompsit hybrid machine translation system for the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Scarton</author>
<author>L Specia</author>
</authors>
<title>Exploring consensus in machine translation for quality estimation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="59067" citStr="Scarton and Specia, 2014" startWordPosition="9514" endWordPosition="9517">eland (Hokamp et al., 2014) German Research Centre for Artificial Intelligence, Germany (Avramidis, 2014) Fondazione Bruno Kessler, Italy, UPV Universitat Polit`ecnica de Val`encia, Spain &amp; University of Edinburgh, UK (Camargo de Souza et al., 2014) Laboratoire d’Informatique Grenoble, France (Luong et al., 2014) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2014) Multilizer, Finland Dublin City University Team 2, Ireland (Bicici and Way, 2014) University of Sheffield Team 1, UK (Beck et al., 2014) University of Sheffield Team 2, UK (Scarton and Specia, 2014) Yandex, Russia DCU DFKI FBK-UPV-UEDIN LIG LIMSI MULTILIZER RTM-DCU SHEF-lite USHEFF YANDEX ID Table 10: Participants in the WMT14 Quality Estimation shared task. DFKI (T1.2): DFKI/SVR builds upon the baseline system (above) by adding non-redundant data from the WMT13 task for predicting the same label (HTER) and additional features such as (a) rule-based language corrections (language tool) (b), PCFG parsing statistics and counts of tree labels, (c) position statistics of parsing labels, (d) position statistics of trigrams with low probability. DFKI/SVRxdata uses a similar setting, with the a</context>
<context position="101130" citStr="Scarton and Specia (2014)" startWordPosition="16162" endWordPosition="16165">y did not seem appropriate for human translations. It is interesting to mention the indirect use of human translations by USHEFF for Tasks 1.1-1.3: given a translation for a source segment, all other translations for the same segment were used as pseudo-references. Apart from when this translation was actually the human translation, the human translation was effectively used as a reference. While this reference was mixed with 2- 3 other pseudo-references (other machine translations) for the feature computations, these features led to significant gains in performance over the baseline features Scarton and Specia (2014). We believe that more investigation is needed for human translation quality prediction. Tasks dedicated to this type of data at both sentence- and word-level in the next editions of this shared task would be a possible starting point. The acquisition of such data is however much more costly, as it is arguably hard to find examples of low quality human translation, unless specific settings, such as translation learner corpora, are considered. 5 Medical Translation Task The Medical Translation Task addresses the problem of domain-specific and genre-specific machine translation. The task is spli</context>
</contexts>
<marker>Scarton, Specia, 2014</marker>
<rawString>Scarton, C. and Specia, L. (2014). Exploring consensus in machine translation for quality estimation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Schwartz</author>
<author>T Anderson</author>
<author>J Gwinnup</author>
<author>K Young</author>
</authors>
<title>Machine translation and monolingual postediting: The afrl wmt-14 system.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="16348" citStr="Schwartz et al., 2014" startWordPosition="2448" endWordPosition="2451"> source segment, a human reference translation, and the outputs of five anonymized systems, randomly selected from the set of participating systems, and randomly ordered. To run the evaluation, we use Appraise5 (Federmann, 2012), an open-source tool built on Python’s Django framework. At the top of each HIT, the following instructions are provided: You are shown a source sentence followed by several candidate translations. Your task is to rank the translations from best to worst (ties are allowed). 5https://github.com/cfedermann/Appraise 15 ID Institution AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014) CIMS University of Stuttgart / University of Munich (Cap et al., 2014) CMU Carnegie Mellon University (Matthews et al., 2014) CU-* Charles University, Prague (Tamchyna et al., 2014) DCU-FDA Dublin City University (Bicici et al., 2014) DCU-ICTCAS Dublin City University (Li et al., 2014b) DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014) EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014) KIT Karlsruhe Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Qu</context>
</contexts>
<marker>Schwartz, Anderson, Gwinnup, Young, 2014</marker>
<rawString>Schwartz, L., Anderson, T., Gwinnup, J., and Young, K. (2014). Machine translation and monolingual postediting: The afrl wmt-14 system. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Seginer</author>
</authors>
<title>Learning Syntactic Structure.</title>
<date>2007</date>
<tech>PhD thesis,</tech>
<institution>University of Amsterdam.</institution>
<contexts>
<context position="65655" citStr="Seginer, 2007" startWordPosition="10546" endWordPosition="10547">ctions. For each task, individual RTM models are developed using the parallel corpora and the language model corpora distributed by the WMT14 translation task and the language model corpora provided by LDC for English and Spanish. RTMs use 337 to 437 sentence-level features for coverage and diversity, IBM1 and sentence translation performance, retrieval closeness and minimum Bayes retrieval risk, distributional similarity and entropy, IBM2 alignment, character n-grams, sentence readability, and parse output tree structures. The features use ngrams defined over text or common cover link (CCL) (Seginer, 2007) structures as the basic units of information over which similarity calculations are performed. Learning models include ridge regression (RR), support vector machines (SVR), and regression trees (TREE), which are applied after partial least squares (PLS) or feature selection (FS). For word-level prediction, generalised linear models (GLM) (Collins, 2002) and GLM with dynamic learning (GLMd) (Bic¸ici, 2013) are used with wordlevel features including CCL links, word length, location, prefix, suffix, form, context, and alignment, totalling up to a couple of million features. SHEF-lite (T1.1, T1.2</context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Seginer, Y. (2007). Learning Syntactic Structure. PhD thesis, University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Shah</author>
<author>T Cohn</author>
<author>L Specia</author>
</authors>
<title>An investigation on the effectiveness of features for translation quality estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Machine Translation Summit XIV,</booktitle>
<pages>167--174</pages>
<location>Nice, France.</location>
<contexts>
<context position="67133" citStr="Shah et al. (2013)" startWordPosition="10775" endWordPosition="10778">nguage is considered a task. For T1.2 and T1.3, additional datasets from previous shared task years are used, each encoded as a different task. For all tasks, the QUEST framework is used to extract a set of 80 black-box features (a superset of the 17 baseline features). To cope with the large size of the datasets, the SHEF-lite-sparse submission uses Sparse Gaussian Processes, which provide sensible sparse approximations using only a subset of instances (inducing inputs) to speed up training and prediction. For this “sparse” submission, feature selection is performed following the approach of Shah et al. (2013) by ranking features according to their learned length-scales and selecting the top 40 features. USHEFF (T1.1, T1.2, T1.3): USHEFF submissions exploit the use of consensus among MT systems by comparing the MT system output to several alternative translations generated by other MT systems (pseudoreferences). The comparison is done using standard evaluation metrics (BLEU, TER, METEOR, ROUGE for all tasks, and two metrics based on syntactic similarities from shallow and dependency parser information for T1.2 and T1.3). Figures extracted from such metrics are used as features to complement predict</context>
</contexts>
<marker>Shah, Cohn, Specia, 2013</marker>
<rawString>Shah, K., Cohn, T., and Specia, L. (2013). An investigation on the effectiveness of features for translation quality estimation. In Proceedings of the Machine Translation Summit XIV, pages 167–174, Nice, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Shah</author>
<author>L Specia</author>
</authors>
<title>Quality estimation for translation selection.</title>
<date>2014</date>
<booktitle>In Proceedings of the 17th Annual Conference of the European Association for Machine Translation,</booktitle>
<location>Dubrovnik, Croatia.</location>
<contexts>
<context position="97284" citStr="Shah and Specia, 2014" startWordPosition="15552" endWordPosition="15555">1.1, data from multiple MT systems was explicitly used by USHEFF though the idea of consensus translations. Translations from all but the system of interest for the same source segment were used as pseudo-references. The submission significantly outperformed the baseline for all language pairs and did particularly well for Spanish-English and English-Spanish. An in depth analysis of Task 1.1’s datasets on the difference in prediction performance between models built and applied for individual translation systems and models built and tested for all translations pooled together is presented in (Shah and Specia, 2014). Not surprisingly, the former models perform significantly better, with MAE scores ranging between 0.35 and 0.5 for different language pairs and MT systems, and significantly lower scores for models trained and tested on human translations only (MAE scores between 0.2 and 0.35 for different language pairs), against MAE scores ranging between 0.5 and 0.65 for models with pooled data. For Tasks 1.2 and 1.3, two submissions included English-Spanish data which had been produced by yet different MT systems (SHEF-lite and DFKI). While using these additional instances seemed attractive given the sma</context>
<context position="99417" citStr="Shah and Specia, 2014" startWordPosition="15893" endWordPosition="15896"> hard to isolate the effect of this data on the results. Examining the effectiveness of quality prediction methods on human translations Datasets for Tasks 1.1 and 2 contain human translations, in addition to the automatic translations from various MT systems. Predicting human translation quality is an area that has been largely unexplored. Previous work has looked into distinguishing human from machine translations (e.g. (Gamon et al., 2005)), but this problem setting is somehow artificial, and moreover arguably harder to solve nowadays given the higher general quality of current MT systems (Shah and Specia, 2014). Although human translations are obviously of higher quality in general, many segments are translated by MT systems with the same or similar levels of quality as human translation. This is particularly true for Task 2, since data had been previously categorised and only “near misses” were selected for the word-level annotation, i.e., human and machine translations that were both nearly perfect in this case. While no distinction was made between human and machine translations in our tasks, we believe the mix of these two types of translations has had a negative impact in prediction performance</context>
</contexts>
<marker>Shah, Specia, 2014</marker>
<rawString>Shah, K. and Specia, L. (2014). Quality estimation for translation selection. In Proceedings of the 17th Annual Conference of the European Association for Machine Translation, Dubrovnik, Croatia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA-2006),</booktitle>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="49109" citStr="Snover et al., 2006" startWordPosition="7841" endWordPosition="7844">o LPSs producing data independently for English-Spanish. They were generated using the LSPs’ own source data (a different domain from news), and own MT system (different from the three used for the official datasets). The results on these datasets were not considered 40% 60% 50% 30% 20% 10% 0% {en-de-1} {en-de-2} {en-de-3} {de-en-1} {de-en-2} {de-en-3} Training Test {en-es-1} {en-es-2} {en-es-3} {es-en-1} {es-en-2} {es-en-3} 600 Spanish 600 German 450 English 450 English 25 for the official ranking of the participating systems: Task 1.2 Predicting percentage of edits In this task we use HTER (Snover et al., 2006) as quality score. This score is to be interpreted as the minimum edit distance between the machine translation and its manually post-edited version, and its range is [0, 1] (0 when no edit needs to be made, and 1 when all words need to be edited). We used TERp (default settings: tokenised, case insensitive, etc., but capped to 1)11 to compute the HTER scores. For practical reasons, the data is a subset of Task 1.1’s dataset: only translations produced by the SMT system English-Spanish. As training data, we provide 896 English-Spanish translation suggestions and their post-editions. As test da</context>
<context position="115221" citStr="Snover et al., 2006" startWordPosition="18319" endWordPosition="18322">anguage and translation models were selected using various data selection techniques. UM-WDA submitted systems for all language pairs in the summary translation subtask. Their systems are domain-adapted using web-crawled in-domain resources: bilingual dictionaries and monolingual data. The translation model and language model trained on the crawled data were interpolated with the best-performing language and translation model employed in the UM-DA systems. 5.5 Results MT quality in the Medical Translation Task is evaluated using automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), PER (Tillmann et al., 1997), and CDER (Leusch et al., 2006). BLEU scores are reported as percentage and all error rates are reported as one minus the original value, also as percentage, so that all metrics are in the 0-100 range, and higher scores indicate better translations. The main reason for not conducting human evaluation, as it happens in the standard Trans45 original normalized truecased normalized lowercased ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER Czech--+English CUNI 29.64 29.79±1.07 47.45±1.15 61.64±1.06 52.18±0.98 31.68±1.14 49.84±1.10 64.38±1.06 54.10±0.96 CUNI 2</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Snover, M., Dorr, B., Schwartz, R., Micciulla, L., and Makhoul, J. (2006). A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA-2006), Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G C d Souza</author>
<author>M Espl-Gomis</author>
<author>M Turchi</author>
<author>M Negri</author>
</authors>
<title>Exploiting qualitative information from automatic word alignment for cross-lingual nlp tasks.</title>
<date>2013</date>
<booktitle>In The 51st Annual Meeting of the Association for Computational Linguistics - Short Papers (ACL Short Papers</booktitle>
<contexts>
<context position="60777" citStr="Souza et al., 2013" startWordPosition="9783" endWordPosition="9786">m word lexicons, and POS tags of each word for source and translation sentences. The predictions of the Binary model are used as a feature for the Level 1 and Multi-class settings. Both conditional random fields (CRF) and bidirectional long short-term memory recurrent neural networks (BLSTM-RNNs) are used for the Binary setting, and BLSTM-RNNs only for the Level 1 and Multi-class settings. The sentence-level QE submissions (T1.2 and T1.3) are trained on black-box features extracted using QUEST in addition to features based on word alignments, word posterior probabilities and diversity scores (Souza et al., 2013). These features are computed over 100k-best hypothesis translations also used for task 2. In addition, a set of ratios computed from the word-level predictions of the model trained on the binary setting of task 2 is used. A total of 221 features and the extremely randomised trees (Geurts et al., 2006) learning algorithm are used to train regression models. LIG (T2): Conditional Random Fields classifiers are trained with features used in LIG’s WMT13 systems (Luong et al., 2013): target and source words, alignment information, source and target alignment context, LM scores, target and source PO</context>
</contexts>
<marker>Souza, Espl-Gomis, Turchi, Negri, 2013</marker>
<rawString>Souza, J. G. C. d., Espl-Gomis, M., Turchi, M., and Negri, M. (2013). Exploiting qualitative information from automatic word alignment for cross-lingual nlp tasks. In The 51st Annual Meeting of the Association for Computational Linguistics - Short Papers (ACL Short Papers 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Specia</author>
<author>K Shah</author>
<author>J G C de Souza</author>
<author>T Cohn</author>
</authors>
<title>QuEst - A Translation Quality Estimation Framework.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Conference of the Association for Computational Linguistics (ACL),</booktitle>
<location>Demo Session, Sofia, Bulgaria.</location>
<marker>Specia, Shah, de Souza, Cohn, 2013</marker>
<rawString>Specia, L., Shah, K., de Souza, J. G. C., and Cohn, T. (2013). QuEst - A Translation Quality Estimation Framework. In Proceedings of the 51th Conference of the Association for Computational Linguistics (ACL), Demo Session, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tamchyna</author>
<author>M Popel</author>
<author>R Rosa</author>
<author>O Bojar</author>
</authors>
<title>Cuni in wmt14: Chimera still awaits bellerophon.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="16530" citStr="Tamchyna et al., 2014" startWordPosition="2476" endWordPosition="2479">evaluation, we use Appraise5 (Federmann, 2012), an open-source tool built on Python’s Django framework. At the top of each HIT, the following instructions are provided: You are shown a source sentence followed by several candidate translations. Your task is to rank the translations from best to worst (ties are allowed). 5https://github.com/cfedermann/Appraise 15 ID Institution AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014) CIMS University of Stuttgart / University of Munich (Cap et al., 2014) CMU Carnegie Mellon University (Matthews et al., 2014) CU-* Charles University, Prague (Tamchyna et al., 2014) DCU-FDA Dublin City University (Bicici et al., 2014) DCU-ICTCAS Dublin City University (Li et al., 2014b) DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014) EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014) KIT Karlsruhe Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Unive</context>
</contexts>
<marker>Tamchyna, Popel, Rosa, Bojar, 2014</marker>
<rawString>Tamchyna, A., Popel, M., Rosa, R., and Bojar, O. (2014). Cuni in wmt14: Chimera still awaits bellerophon. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tan</author>
<author>S Pal</author>
</authors>
<title>Manawi: Using multi-word expressions and named entities to improve machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="17172" citStr="Tan and Pal, 2014" startWordPosition="2569" endWordPosition="2572">versity (Bicici et al., 2014) DCU-ICTCAS Dublin City University (Li et al., 2014b) DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014) EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014) KIT Karlsruhe Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014) PROMT-RULE, PROMT PROMT-HYBRID RWTH RWTH Aachen (Peitz et al., 2014) STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014) UA-* University of Alicante (S´anchez-Cartagena et al., 2014) UEDIN-PHRASE, University of Edinburgh (Durrani et al., 2014b) UEDIN-UNCNSTR UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014) UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014) YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014) COMMERCIAL-[1,2] Two commercial machine translation systems ONLI</context>
</contexts>
<marker>Tan, Pal, 2014</marker>
<rawString>Tan, L. and Pal, S. (2014). Manawi: Using multi-word expressions and named entities to improve machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Thompson</author>
<author>S Iqbal</author>
<author>J McNaught</author>
<author>S Ananiadou</author>
</authors>
<title>Construction of an annotated corpus to support biomedical information extraction.</title>
<date>2009</date>
<journal>BMC bioinformatics,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="110744" citStr="Thompson et al., 2009" startWordPosition="17639" endWordPosition="17642">g clinical studies conducted around the world. DrugBank is a bioinformatics and cheminformatics resource containing drug descriptions (Knox et al., 2011). GENIA is a corpus of biomedical literature compiled and annotated within the GENIA project (Kim et al., 2003). FMA stands for the Foundational Model of Anatomy Ontology, a knowledge source for biomedical informatics concerned with symbolic representation of the phenotypic structure of the human body (Rosse and Mejino Jr., 2008). GREC (Gene Regulation Event Corpus) is a semantically annotated English corpus of abstracts of biomedical papers (Thompson et al., 2009). The PIL corpus is a collection of documents giving instructions to patients about their medication (BouayadAgha et al., 2000). 5.4 Participants A total of eight teams participated in the Medical Translation Task by submitting their systems to at least one subtask for one or more translation directions. A list of the participants is given in Table 24; we provide short descriptions of their systems in the following. CUNI was involved in the organization of the task, and their primary goal was to set up a baseline for both the subtasks and for all translation directions. 44 ID Participating tea</context>
</contexts>
<marker>Thompson, Iqbal, McNaught, Ananiadou, 2009</marker>
<rawString>Thompson, P., Iqbal, S., McNaught, J., and Ananiadou, S. (2009). Construction of an annotated corpus to support biomedical information extraction. BMC bioinformatics, 10(1):349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tiedemann</author>
</authors>
<title>News from OPUS – a collection of multilingual parallel corpora with tools and interfaces.</title>
<date>2009</date>
<booktitle>In Recent Advances in Natural Language Processing,</booktitle>
<volume>5</volume>
<pages>237--248</pages>
<location>Borovets, Bulgaria. John Benjamins.</location>
<contexts>
<context position="106863" citStr="Tiedemann, 2009" startWordPosition="17057" endWordPosition="17058">which were allowed for the constrained Medical Translation Task in addition to resources for the constrained standard Translation Task (see Section 2). Most of the corpora are available for direct download, others can be obtained upon registration. The corpora usually employ their own, more or less complex data format. To lower the entry barrier, we provided a set of easy-to-use scripts to convert the data to a plain text format suitable for MT training. 5.3.1 Parallel Training Data The medical-domain parallel data includes the following corpora (see Table 22 for statistics): The EMEA corpus (Tiedemann, 2009) contains documents from the European Medicines Agency, automatically processed and aligned on sentence level. It is available for many language pairs, including those relevant to this task. UMLS is a multilingual metathesaurus of health and biomed00-097C-0000-0023-866E-1 18http://www.hon.ch/ 19http://www.tripdatabase.com/ 20http://hdl.handle.net/11858/ 00-097C-0000-0022-D9BF-5 43 sents tokens total Czech German French English dev 500 9,209 9,924 12,369 10,350 test 1,000 19,191 20,831 26,183 21,423 Table 20: Statistics of summary test data. queries tokens total general expert Czech German Fren</context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>Tiedemann, J. (2009). News from OPUS – a collection of multilingual parallel corpora with tools and interfaces. In Recent Advances in Natural Language Processing, volume 5, pages 237–248, Borovets, Bulgaria. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>S Vogel</author>
<author>H Ney</author>
<author>A Zubiaga</author>
<author>H Sawaf</author>
</authors>
<title>Accelerated DP based search for statistical translation.</title>
<date>1997</date>
<booktitle>Proceedings of the Fifth European Conference on Speech Communication and Technology,</booktitle>
<pages>2667--2670</pages>
<editor>In Kokkinakis, G., Fakotakis, N., and Dermatas, E., editors,</editor>
<publisher>International Speech Communication Association.</publisher>
<contexts>
<context position="115250" citStr="Tillmann et al., 1997" startWordPosition="18324" endWordPosition="18327">els were selected using various data selection techniques. UM-WDA submitted systems for all language pairs in the summary translation subtask. Their systems are domain-adapted using web-crawled in-domain resources: bilingual dictionaries and monolingual data. The translation model and language model trained on the crawled data were interpolated with the best-performing language and translation model employed in the UM-DA systems. 5.5 Results MT quality in the Medical Translation Task is evaluated using automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), PER (Tillmann et al., 1997), and CDER (Leusch et al., 2006). BLEU scores are reported as percentage and all error rates are reported as one minus the original value, also as percentage, so that all metrics are in the 0-100 range, and higher scores indicate better translations. The main reason for not conducting human evaluation, as it happens in the standard Trans45 original normalized truecased normalized lowercased ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER Czech--+English CUNI 29.64 29.79±1.07 47.45±1.15 61.64±1.06 52.18±0.98 31.68±1.14 49.84±1.10 64.38±1.06 54.10±0.96 CUNI 22.44 22.57±0.95 41.43±1.16 55</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, Sawaf, 1997</marker>
<rawString>Tillmann, C., Vogel, S., Ney, H., Zubiaga, A., and Sawaf, H. (1997). Accelerated DP based search for statistical translation. In Kokkinakis, G., Fakotakis, N., and Dermatas, E., editors, Proceedings of the Fifth European Conference on Speech Communication and Technology, pages 2667–2670, Rhodes, Greece. International Speech Communication Association.</rawString>
</citation>
<citation valid="true">
<date>2009</date>
<booktitle>UMLS reference manual. Metathesaurus.</booktitle>
<institution>U.S. National Library of Medicine</institution>
<location>Bethesda, MD, USA.</location>
<marker>2009</marker>
<rawString>U.S. National Library of Medicine (2009). UMLS reference manual. Metathesaurus. Bethesda, MD, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E M Voorhees</author>
<author>D K Harman</author>
</authors>
<title>editors (2005). TREC: Experiment and evaluation in information retrieval,</title>
<booktitle>of Digital libraries and electronic publishing series. MIT press Cambridge,</booktitle>
<volume>63</volume>
<location>Cambridge, MA, USA.</location>
<marker>Voorhees, Harman, </marker>
<rawString>Voorhees, E. M. and Harman, D. K., editors (2005). TREC: Experiment and evaluation in information retrieval, volume 63 of Digital libraries and electronic publishing series. MIT press Cambridge, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wang</author>
<author>Y Lu</author>
<author>D F Wong</author>
<author>L S Chao</author>
<author>Y Wang</author>
<author>F Oliveira</author>
</authors>
<title>Combining domain adaptation approaches for medical text translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Ninth Workshop of Statistical Machine Translation,</booktitle>
<location>Baltimore, USA.</location>
<contexts>
<context position="111734" citStr="Wang et al., 2014" startWordPosition="17802" endWordPosition="17805">descriptions of their systems in the following. CUNI was involved in the organization of the task, and their primary goal was to set up a baseline for both the subtasks and for all translation directions. 44 ID Participating team Charles University in Prague (Duˇsek et al., 2014) Dublin City University (Okita et al., 2014) Dublin City University (Zhang et al., 2014) Laboratoire dInformatique pour la Mecanique et les Sciences de lIng´enieur (P´echeux et al., 2014) Pohang University of Science and Technology (Li et al., 2014a) University of Edinburgh (Durrani et al., 2014a) University of Macau (Wang et al., 2014) University of Macau (Lu et al., 2014) CUNI DCU-Q DCU-S LIMSI POSTECH UEDIN UM-DA UM-WDA Table 24: Participants in the WMT14 Medical Translation Task. Their systems are based on the Moses phrasebased toolkit and linear interpolation of in-domain and out-of-domain language models and phrase tables. The constrained/unconstrained systems differ in the training data only. The constrained ones are built using all allowed training data; the unconstrained ones take advantage of additional web-crawled monolingual data used for training of the language models, and additional parallel nonmedical data fr</context>
</contexts>
<marker>Wang, Lu, Wong, Chao, Wang, Oliveira, 2014</marker>
<rawString>Wang, L., Lu, Y., Wong, D. F., Chao, L. S., Wang, Y., and Oliveira., F. (2014). Combining domain adaptation approaches for medical text translation. In Proceedings of the ACL 2014 Ninth Workshop of Statistical Machine Translation, Baltimore, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W¨aschle</author>
<author>S Riezler</author>
</authors>
<title>Analyzing parallelism and domain similarities in the MAREC patent corpus.</title>
<date>2012</date>
<journal>Multidisciplinary Information Retrieval,</journal>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>7356</volume>
<pages>12--27</pages>
<editor>In Salampasis, M. and Larsen, B., editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<marker>W¨aschle, Riezler, 2012</marker>
<rawString>W¨aschle, K. and Riezler, S. (2012). Analyzing parallelism and domain similarities in the MAREC patent corpus. In Salampasis, M. and Larsen, B., editors, Multidisciplinary Information Retrieval, volume 7356 of Lecture Notes in Computer Science, pages 12–27. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Williams</author>
<author>R Sennrich</author>
<author>M Nadejde</author>
<author>M Huck</author>
<author>E Hasler</author>
<author>P Koehn</author>
</authors>
<title>Edinburghs syntax-based systems at wmt</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="17580" citStr="Williams et al., 2014" startWordPosition="2627" endWordPosition="2630">d Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014) PROMT-RULE, PROMT PROMT-HYBRID RWTH RWTH Aachen (Peitz et al., 2014) STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014) UA-* University of Alicante (S´anchez-Cartagena et al., 2014) UEDIN-PHRASE, University of Edinburgh (Durrani et al., 2014b) UEDIN-UNCNSTR UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014) UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014) YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014) COMMERCIAL-[1,2] Two commercial machine translation systems ONLINE-[A,B,C,G] Four online statistical machine translation systems RBMT-[1,4] Two rule-based statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion c</context>
</contexts>
<marker>Williams, Sennrich, Nadejde, Huck, Hasler, Koehn, 2014</marker>
<rawString>Williams, P., Sennrich, R., Nadejde, M., Huck, M., Hasler, E., and Koehn, P. (2014). Edinburghs syntax-based systems at wmt 2014. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Wisniewski</author>
<author>N P´echeux</author>
<author>A Allauzen</author>
<author>F Yvon</author>
</authors>
<title>Limsi submission for wmt’14 qe task.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<marker>Wisniewski, P´echeux, Allauzen, Yvon, 2014</marker>
<rawString>Wisniewski, G., P´echeux, N., Allauzen, A., and Yvon, F. (2014). Limsi submission for wmt’14 qe task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>x wu</author>
<author>R Haque</author>
<author>T Okita</author>
<author>P Arora</author>
<author>A Way</author>
<author>Q Liu</author>
</authors>
<title>Dcu-lingo24 participation in wmt 2014 hindi-english translation task.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="16699" citStr="wu et al., 2014" startWordPosition="2502" endWordPosition="2505">e shown a source sentence followed by several candidate translations. Your task is to rank the translations from best to worst (ties are allowed). 5https://github.com/cfedermann/Appraise 15 ID Institution AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014) CIMS University of Stuttgart / University of Munich (Cap et al., 2014) CMU Carnegie Mellon University (Matthews et al., 2014) CU-* Charles University, Prague (Tamchyna et al., 2014) DCU-FDA Dublin City University (Bicici et al., 2014) DCU-ICTCAS Dublin City University (Li et al., 2014b) DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014) EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014) KIT Karlsruhe Institute of Technology (Herrmann et al., 2014) IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014) IIIT-HYDERABAD IIIT Hyderabad IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014) IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014) KAZNU Amandyk Kartbayev, FBK LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014) MANAWI-* Universit¨at des Saarlandes (Tan and Pal, 2014) MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014) PROMT-RULE, PROMT PROMT-HYBRID RWTH RWTH Aachen (Peitz et </context>
</contexts>
<marker>wu, Haque, Okita, Arora, Way, Liu, 2014</marker>
<rawString>wu, x., Haque, R., Okita, T., Arora, P., Way, A., and Liu, Q. (2014). Dcu-lingo24 participation in wmt 2014 hindi-english translation task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zdeˇnka Ureˇsov´a</author>
<author>Ondˇrej Duˇsek</author>
<author>J H</author>
<author>P Pecina</author>
</authors>
<title>Multilingual test sets for machine translation of search queries for cross-lingual information retrieval in the medical domain.</title>
<date>2014</date>
<booktitle>In To appear in Proceedings of the Ninth International Conference on Language Resources and Evaluation,</booktitle>
<location>Reykjavik, Iceland.</location>
<marker>Ureˇsov´a, Duˇsek, H, Pecina, 2014</marker>
<rawString>Zdeˇnka Ureˇsov´a, Ondˇrej Duˇsek, J. H. and Pecina, P. (2014). Multilingual test sets for machine translation of search queries for cross-lingual information retrieval in the medical domain. In To appear in Proceedings of the Ninth International Conference on Language Resources and Evaluation, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhang</author>
<author>X Wu</author>
<author>I Calixto</author>
<author>A H Vahid</author>
<author>X Zhang</author>
<author>A Way</author>
<author>Q Liu</author>
</authors>
<title>Experiments in medical translation shared task at wmt</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Ninth Workshop of Statistical Machine Translation,</booktitle>
<location>Baltimore, USA.</location>
<contexts>
<context position="111484" citStr="Zhang et al., 2014" startWordPosition="17764" endWordPosition="17767"> 2000). 5.4 Participants A total of eight teams participated in the Medical Translation Task by submitting their systems to at least one subtask for one or more translation directions. A list of the participants is given in Table 24; we provide short descriptions of their systems in the following. CUNI was involved in the organization of the task, and their primary goal was to set up a baseline for both the subtasks and for all translation directions. 44 ID Participating team Charles University in Prague (Duˇsek et al., 2014) Dublin City University (Okita et al., 2014) Dublin City University (Zhang et al., 2014) Laboratoire dInformatique pour la Mecanique et les Sciences de lIng´enieur (P´echeux et al., 2014) Pohang University of Science and Technology (Li et al., 2014a) University of Edinburgh (Durrani et al., 2014a) University of Macau (Wang et al., 2014) University of Macau (Lu et al., 2014) CUNI DCU-Q DCU-S LIMSI POSTECH UEDIN UM-DA UM-WDA Table 24: Participants in the WMT14 Medical Translation Task. Their systems are based on the Moses phrasebased toolkit and linear interpolation of in-domain and out-of-domain language models and phrase tables. The constrained/unconstrained systems differ in the</context>
</contexts>
<marker>Zhang, Wu, Calixto, Vahid, Zhang, Way, Liu, 2014</marker>
<rawString>Zhang, J., Wu, X., Calixto, I., Vahid, A. H., Zhang, X., Way, A., and Liu, Q. (2014). Experiments in medical translation shared task at wmt 2014. In Proceedings of the ACL 2014 Ninth Workshop of Statistical Machine Translation, Baltimore, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
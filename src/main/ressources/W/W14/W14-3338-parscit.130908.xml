<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000242">
<title confidence="0.986237">
SHEF-Lite 2.0: Sparse Multi-task Gaussian Processes for Translation
Quality Estimation
</title>
<author confidence="0.996963">
Daniel Beck and Kashif Shah and Lucia Specia
</author>
<affiliation confidence="0.879296666666667">
Department of Computer Science
University of Sheffield
Sheffield, United Kingdom
</affiliation>
<email confidence="0.998737">
{debeck1,kashif.shah,l.specia}@sheffield.ac.uk
</email>
<sectionHeader confidence="0.997387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999846181818182">
We describe our systems for the WMT14
Shared Task on Quality Estimation (sub-
tasks 1.1, 1.2 and 1.3). Our submissions
use the framework of Multi-task Gaus-
sian Processes, where we combine multi-
ple datasets in a multi-task setting. Due to
the large size of our datasets we also ex-
periment with Sparse Gaussian Processes,
which aim to speed up training and predic-
tion by providing sensible sparse approxi-
mations.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938269230769">
The purpose of machine translation (MT) quality
estimation (QE) is to provide a quality prediction
for new, unseen machine translated texts, with-
out relying on reference translations (Blatz et al.,
2004; Specia et al., 2009; Bojar et al., 2013). A
common use of quality predictions is the decision
between post-editing a given machine translated
sentence and translating its source from scratch,
based on whether its post-editing effort is esti-
mated to be lower than the effort of translating the
source sentence.
The WMT 2014 QE shared task defined a group
of tasks related to QE. In this paper, we de-
scribe our submissions for subtasks 1.1, 1.2 and
1.3. Our models are based on Gaussian Pro-
cesses (GPs) (Rasmussen and Williams, 2006),
a non-parametric kernelised probabilistic frame-
work. We propose to combine multiple datasets
to improve our QE models by applying GPs in
a multi-task setting. Our hypothesis is that us-
ing sensible multi-task learning settings gives im-
provements over simply pooling all datasets to-
gether.
Task 1.1 focuses on predicting post-editing ef-
fort for four language pairs: English-Spanish
(en-es), Spanish-English (es-en), English-German
(en-de), and German-English (de-en). Each con-
tains a different number of source sentences and
their human translations, as well as 2-3 versions
of machine translations: by a statistical (SMT)
system, a rule-based system (RBMT) system and,
for en-es/de only, a hybrid system. Source sen-
tences were extracted from tests sets of WMT13
and WMT12, and the translations were produced
by top MT systems of each type and a human
translator. Labels range from 1 to 3, with 1 in-
dicating a perfect translation and 3, a low quality
translation.
The purpose of task 1.2 is to predict HTER
scores (Human Translation Error Rate) (Snover
et al., 2006) using a dataset composed of 896
English-Spanish sentences translated by a MT sys-
tem and post-edited by a professional translator.
Finally, task 1.3 aims at predicting post-editing
time, using a subset of 650 sentences from the
Task 1.2 dataset.
For each task, participants can submit two types
of results: scoring and ranking. For scoring, eval-
uation is made in terms of Mean Absolute Error
(MAE) and Root Mean Square Error (RMSE). For
ranking, DeltaAvg and Spearman’s rank correla-
tion were used as evaluation metrics.
</bodyText>
<sectionHeader confidence="0.982268" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.999394333333333">
Gaussian Processes are a Bayesian non-parametric
machine learning framework considered the state-
of-the-art for regression. They assume the pres-
ence of a latent function f : RF → R, which maps
a vector x from feature space F to a scalar value.
Formally, this function is drawn from a GP prior:
</bodyText>
<equation confidence="0.912068">
f(x) ∼ GP(0, k(x, x&apos;)),
</equation>
<footnote confidence="0.670599">
which is parameterised by a mean function (here,
0) and a covariance kernel function k(x, x&apos;). Each
response value is then generated from the function
evaluated at the corresponding input, yi = f(xi)+
77, where 77 ∼ N(0, σ2n) is added white-noise.
</footnote>
<page confidence="0.970531">
307
</page>
<bodyText confidence="0.56637775">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 307–312,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
Prediction is formulated as a Bayesian inference
under the posterior:
</bodyText>
<equation confidence="0.911434">
p(y∗ |x∗,D) = if
p(y∗|x∗,f)p(f|D),
</equation>
<bodyText confidence="0.943297">
where x∗ is a test input, y∗ is the test response
value and D is the training set. The predictive pos-
terior can be solved analitically, resulting in:
</bodyText>
<equation confidence="0.83823">
y∗ — jV(kT∗ (K + σ2nI)−1y,
k(x∗, x∗) — kT∗ (K + σ2nI)−1k∗),
</equation>
<bodyText confidence="0.999493636363636">
where k∗ = [k(x∗, x1)k(x∗, x2) ... k(x∗, xn)]T
is the vector of kernel evaluations between the
training set and the test input and K is the kernel
matrix over the training inputs (the Gram matrix).
The kernel function encodes the covariance
(similarity) between each input pair. While a vari-
ety of kernel functions are available, here we fol-
lowed previous work in QE using GP (Cohn and
Specia, 2013; Shah et al., 2013) and employed
a squared exponential (SE) kernel with automatic
relevance determination (ARD):
</bodyText>
<equation confidence="0.990648">
1 F
k(x, x0) = σ 2f exp —
2
i=1
</equation>
<bodyText confidence="0.999630333333333">
where F is the number of features, σ2 f is the co-
variance magnitude and li &gt; 0 are the feature
lengthscales.
The resulting model hyperparameters (SE vari-
ance σ2f, noise variance σ2n and SE lengthscales li)
were learned from data by maximising the model
likelihood. All our models were trained using the
GPy1 toolkit, an open source implementation of
GPs written in Python.
</bodyText>
<subsectionHeader confidence="0.979136">
2.1 Multi-task learning
</subsectionHeader>
<bodyText confidence="0.972801076923077">
The GP regression framework can be extended to
multiple outputs by assuming f(x) to be a vec-
tor valued function. These models are commonly
referred as coregionalization models in the GP lit-
erature ( ´Alvarez et al., 2012). Here we refer to
them as multi-task kernels, to emphasize our ap-
plication.
In this work, we employ a separable multi-task
kernel, similar to the one used by Bonilla et al.
(2008) and Cohn and Specia (2013). Consider-
ing a set of D tasks, we define the corresponding
multi-task kernel as:
k((x, d), (x0, d0)) = kdata(x, x0) x Bd,d�, (1)
</bodyText>
<footnote confidence="0.710064">
1http://sheffieldml.github.io/GPy/
</footnote>
<bodyText confidence="0.999857857142857">
where kdata is a kernel on the input points, d and
d0 are task or metadata information for each input
and B E RD×D is the multi-task matrix, which
encodes task covariances. For task 1.1, we con-
sider each language pair as a different task, while
for tasks 1.2 and 1.3 we use additional datasets
for the same language pair (en-es), treating each
dataset as a different task.
To perform the learning procedure the multi-
task matrix should be parameterised in a sensible
way. We follow the parameterisations proposed
by Cohn and Specia (2013), which we briefly de-
scribe here:
Independent: B = I. In this setting each task is
modelled independently. This is not strictly
equivalent to independent model training be-
cause the tasks share the same data kernel
(and the same hyperparameters);
Pooled: B = 1. Here the task identity is ignored.
This is equivalent to pooling all datasets in a
single task model;
Combined: B = 1 + αI. This setting lever-
ages between independent and pooled mod-
els. Here, α &gt; 0 is treated as an hyperparam-
eter;
Combined+: B = 1 + diag(α). Same as “com-
bined”, but allowing one different α value per
task.
</bodyText>
<subsectionHeader confidence="0.999746">
2.2 Sparse Gaussian Processes
</subsectionHeader>
<bodyText confidence="0.999993857142857">
The performance bottleneck for GP models is the
Gram matrix inversion, which is O(n3) for stan-
dard GPs, with n being the number of training in-
stances. For multi-task settings this can be a po-
tential issue because these models replicate the in-
stances for each task and the resulting Gram ma-
trix has dimensionality nd x nd, where d is the
number of tasks.
Sparse GPs tackle this problem by approximat-
ing the Gram matrix using only a subset of m in-
ducing inputs. Without loss of generalisation, con-
sider these m points as the first instances in the
training data. We can then expand the Gram ma-
trix in the following way:
</bodyText>
<equation confidence="0.442372">
r Kmm K
K(n−m)m K(n
</equation>
<bodyText confidence="0.9988685">
Following the notation in (Rasmussen and
Williams, 2006), we refer Km(n−m) as Kmn and
</bodyText>
<equation confidence="0.986576">
xi — x0i
li
�,
K=
�
m(n−m)
−m)(n−m)
.
</equation>
<page confidence="0.975152">
308
</page>
<bodyText confidence="0.998033666666667">
its transpose as Knm. The block structure of K
forms the basis of the so-called Nystr¨om approxi-
mation:
</bodyText>
<equation confidence="0.998590285714286">
K˜ = KnmK−1
mmKmn, (2)
which results in the following predictive posterior:
y∗ ∼ N(kTm∗ ˜G−1Kmny, (3)
k(x∗, x∗) − kTm∗K−1mmkm∗+
2 T1
ankm∗˜G km∗),
</equation>
<bodyText confidence="0.997889">
where G˜ = a2 nKmm + KmnKnm and km∗ is the
vector of kernel evaluations between test input x∗
and the m inducing inputs. The resulting training
complexity is O(m2n).
The remaining question is how to choose the in-
ducing inputs. We follow the approach of Snelson
and Ghahramani (2006), which note that these in-
ducing inputs do not need to be a subset of the
training data. Their method considers each in-
put as a hyperparameter, which is then optimised
jointly with the kernel hyperparameters.
</bodyText>
<subsectionHeader confidence="0.932185">
2.3 Features
</subsectionHeader>
<bodyText confidence="0.9995058">
For all tasks we used the QuEst framework (Spe-
cia et al., 2013) to extract a set of 80 black-box
features as in Shah et al. (2013), for which we had
all the necessary resources available. Examples of
the features extracted include:
</bodyText>
<listItem confidence="0.770206809523809">
• N-gram-based features:
– Number of tokens in source and target
segments;
– Language model (LM) probability of
source and target segments;
– Percentage of source 1–3-grams ob-
served in different frequency quartiles of
a large corpus of the source language;
– Average number of translations per
source word in the segment as given by
IBM 1 model from a large parallel cor-
pus of the language, with probabilities
thresholded in different ways.
• POS-based features:
– Ratio of percentage of nouns/verbs/etc
in the source and target segments;
– Ratio of punctuation symbols in source
and target segments;
– Percentage of direct object personal or
possessive pronouns incorrectly trans-
lated.
</listItem>
<bodyText confidence="0.999576">
For the full set of features we refer readers to
QuEst website.2
To perform feature selection, we followed the
approach used in Shah et al. (2013) and ranked
the features according to their learned lengthscales
(from the lowest to the highest). The lengthscale
of a feature can be interpreted as the relevance of
such feature for the model. Therefore, the out-
come of a GP model using an ARD kernel can be
viewed as a list of features ranked by relevance,
and this information can be used for feature selec-
tion by discarding the lowest ranked (least useful)
ones.
</bodyText>
<sectionHeader confidence="0.996587" genericHeader="method">
3 Preliminary Experiments
</sectionHeader>
<bodyText confidence="0.999959090909091">
Our submissions are based on multi-task settings.
For task 1.1, we consider each language pair as a
different task, training one model for all pairs. For
tasks 1.2 and 1.3, we used additional datasets and
encoded each one as a different task (totalling 3
tasks):
WMT13: these are the datasets provided in last
year’s QE shared task (Bojar et al., 2013).
We combined training and test sets, totalling
2, 754 sentences for HTER prediction and
1, 003 sentences for post-editing time predic-
tion, both for English-Spanish.
EAMT11: this dataset is provided by Specia
(2011) and is composed of 1, 000 English-
Spanish sentences annotated in terms of
HTER and post-editing time.
For each task we prepared two submissions: one
trained on a standard GP with the full 80 features
set and another one trained on a sparse GP with
a subset of 40 features. The features were chosen
by training a smaller model on a subset of 400 in-
stances and following the procedure explained in
Section 2.3 for feature selection, with a pre-define
cutoff point on the number of features (40), based
on previous experiments. The sparse models were
trained using 400 inducing inputs.
To select an appropriate multi-task setting for
our submissions we performed preliminary exper-
iments using a 90%/10% split on the correspond-
ing training set for each task. The resulting MAE
scores are shown in Tables 1 and 2, for standard
and sparse GPs, respectively. The boldface fig-
ures correspond to the settings we choose for the
</bodyText>
<footnote confidence="0.9744155">
2http://www.quest.dcs.shef.ac.uk/
quest—files/features—blackbox
</footnote>
<page confidence="0.99687">
309
</page>
<table confidence="0.9776714">
Task 1.1 Task 1.2 Task 1.3
Independent 0.4905 0.5325 0.5962 0.5452 0.2047 0.4486
Pooled 0.4957 0.5171 0.6012 0.5612 0.2036 0.8599
Combined 0.4939 0.5162 0.6007 0.5550 0.2321 0.7489
Combined+ 0.4932 0.5182 0.5990 0.5514 0.2296 0.4472
</table>
<tableCaption confidence="0.9967945">
Table 1: MAE results for preliminary experiments on standard GPs. Post-editing time scores for task 1.3
are shown on log time per word.
</tableCaption>
<table confidence="0.9695624">
Task 1.1 Task 1.2 Task 1.3
Independent 0.5036 0.5274 0.6002 0.5532 0.3432 0.3906
Pooled 0.4890 0.5131 0.5927 0.5532 0.1597 0.6410
Combined 0.4872 0.5183 0.5871 0.5451 0.2871 0.6449
Combined+ 0.4935 0.5255 0.5864 0.5458 0.1659 0.4040
</table>
<tableCaption confidence="0.9937285">
Table 2: MAE results for preliminary experiments on sparse GPs. Post-editing time scores for task 1.3
are shown on log time per word.
</tableCaption>
<figure confidence="0.996088">
en-es
en-es
es-en
en-de
de-en
en-es
en-es
en-es
es-en
en-de
de-en
en-es
</figure>
<figureCaption confidence="0.6021295">
official submissions, after re-training on the corre-
sponding full training sets.
</figureCaption>
<bodyText confidence="0.999942444444444">
To check the speed-ups obtained from using
sparse GPs, we measured wall clock times for
training and prediction in Task 1.1 using the “In-
dependent” multi-task setting. Table 3 shows the
resulting times and the corresponding speed-ups
when comparing to the standard GP. For compar-
ison, we also trained a model using 200 inducing
inputs, although we did not use the results of this
model in our submissions.
</bodyText>
<table confidence="0.71930425">
Time (secs) Speed-up
Standard GP
Sparse GP (m=400)
Sparse GP (m=200)
</table>
<tableCaption confidence="0.9824955">
Table 3: Wall clock times and speed-ups for GPs
training and prediction: full versus sparse GPs.
</tableCaption>
<sectionHeader confidence="0.909512" genericHeader="method">
4 Official Results and Discussion
</sectionHeader>
<bodyText confidence="0.999928414634147">
Table 4 shows the results for Task 1.1. Us-
ing standard GPs we obtained improved results
over the baseline for English-Spanish and English-
German only, with particularly substantial im-
provements for English-Spanish, which also hap-
pens for sparse GPs. This may be related to the
larger size of this dataset when compared to the
others. Our results here are mostly inconclusive
though and we plan to investigate this setting more
in depth in the future. Specifically, due to the
coarse behaviour of the labels, ordinal regression
GP models (like the one proposed in (Chu et al.,
2005)) could be useful for this task.
Results for Task 1.2 are shown in Table 5. The
standard GP model performed unusually poorly
when compared to the baseline or the sparse GP
model. To investigate this, we inspected the re-
sulting model hyperparameters. We found out that
the noise σ2 was optimised to a very low value,
close to zero, which characterises overfitting. The
same behaviour was not observed with the sparse
model, even though it had a much higher number
of hyperparameters to optimise, and was therefore
more prone to overfitting. We plan to investigate
this issue further but a possible cause could be bad
starting values for the hyperparameters.
Table 6 shows results for Task 1.3. In this task,
the standard GP model outperformed the base-
line, with the sparse GP model following very
closely. These figures represent significant im-
provements compared to our submission to the
same task in last year’s shared task (Beck et al.,
2013), where we were not able to beat the baseline.
The main differences between last year’s and this
year’s models are the use of additional datasets
and a higher number of features (25 vs. 40). The
competitive results for the sparse GP models are
very promising because they show we can com-
bine multiple datasets to improve post-editing time
prediction while employing a sparse model to cope
with speed issues.
</bodyText>
<figure confidence="0.723992666666667">
12122 –
3376 3.59x
978 12.39x
</figure>
<page confidence="0.870361">
310
</page>
<table confidence="0.9994679">
en-es es-en en-de de-en
A ρ A ρ A ρ A ρ
Standard GP 0.21 -0.33 0.11 -0.15 0.26 -0.36 0.24 -0.27
Sparse GP 0.17 0.27 0.12 -0.17 0.23 -0.33 0.14 -0.17
Baseline 0.14 -0.22 0.12 -0.21 0.23 -0.34 0.21 -0.25
en-es es-en en-de de-en
MAE RMSE MAE RMSE MAE RMSE MAE RMSE
Standard GP 0.49 0.63 0.62 0.77 0.63 0.74 0.65 0.77
Sparse GP 0.54 0.69 0.54 0.69 0.64 0.75 0.66 0.79
Baseline 0.52 0.66 0.57 0.68 0.64 0.76 0.65 0.78
</table>
<tableCaption confidence="0.949414">
Table 4: Official results for task 1.1. The top table shows results for the ranking subtask (A: DeltaAvg;
ρ: Spearman’s correlation). The bottom table shows results for the scoring subtask.
</tableCaption>
<table confidence="0.9996962">
Ranking Scoring
A ρ MAE RMSE
Standard GP 0.72 0.09 18.15 23.41
Sparse GP 7.69 0.43 15.04 18.38
Baseline 5.08 0.31 15.23 19.48
</table>
<tableCaption confidence="0.91304">
Table 5: Official results for task 1.2.
</tableCaption>
<table confidence="0.9996596">
Ranking Scoring
A ρ MAE RMSE
Standard GP 16.08 0.64 17.13 27.33
Sparse GP 16.33 0.63 17.42 27.35
Baseline 14.71 0.57 21.49 34.28
</table>
<tableCaption confidence="0.991763">
Table 6: Official results for task 1.3.
</tableCaption>
<sectionHeader confidence="0.99823" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999835">
We proposed a new setting for training QE mod-
els based on Multi-task Gaussian Processes. Our
settings combined different datasets in a sensible
way, by considering each dataset as a different
task and learning task covariances. We also pro-
posed to speed-up training and prediction times
by employing sparse GPs, which becomes crucial
in multi-task settings. The results obtained are
specially promising in the post-editing time task,
where we obtained the same results as with stan-
dard GPs and improved over our models from the
last evaluation campaign.
In the future, we plan to employ our multi-task
models in large-scale settings, like datasets an-
notated through crowdsourcing platforms. These
datasets are usually labelled by dozens of annota-
tors and multi-task GPs have proved an interest-
ing framework for learning the annotation noise
(Cohn and Specia, 2013). However, multiple tasks
can easily make training and prediction times pro-
hibitive, and thus another direction if work is to
use recent advances in sparse GPs, like the one
proposed by Hensman et al. (2013). We believe
that the combination of these approaches could
further improve the state-of-the-art performance in
these tasks.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999821666666667">
This work was supported by funding from
CNPq/Brazil (No. 237999/2012-9, Daniel Beck)
and from European Union’s Seventh Framework
Programme for research, technological develop-
ment and demonstration under grant agreement
no. 296347 (QTLaunchPad).
</bodyText>
<sectionHeader confidence="0.999551" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999881555555556">
Mauricio A. ´Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2012. Kernels for Vector-Valued Func-
tions: a Review. Foundations and Trends in Ma-
chine Learning, pages 1–37.
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia
Specia. 2013. SHEF-Lite : When Less is More for
Translation Quality Estimation. In Proceedings of
WMT13, pages 337–342.
John Blatz, Erin Fitzgerald, and George Foster. 2004.
Confidence estimation for machine translation. In
Proceedings of the 20th Conference on Computa-
tional Linguistics, pages 315–321.
Ondej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Workshop
on Statistical Machine Translation. In Proceedings
of WMT13, pages 1–44.
</reference>
<page confidence="0.984482">
311
</page>
<reference confidence="0.999833536585366">
Edwin V. Bonilla, Kian Ming A. Chai, and Christopher
K. I. Williams. 2008. Multi-task Gaussian Process
Prediction. Advances in Neural Information Pro-
cessing Systems.
Wei Chu, Zoubin Ghahramani, Francesco Falciani, and
David L Wild. 2005. Biomarker discovery in mi-
croarray gene expression data with Gaussian pro-
cesses. Bioinformatics, 21(16):3385–93, August.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of ACL.
James Hensman, Nicol`o Fusi, and Neil D. Lawrence.
2013. Gaussian Processes for Big Data. In Pro-
ceedings of UAI.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian processes for machine
learning, volume 1. MIT Press Cambridge.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An Investigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings of
MT Summit XIV.
Edward Snelson and Zoubin Ghahramani. 2006.
Sparse Gaussian Processes using Pseudo-inputs. In
Proceedings of NIPS.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving
the confidence of machine translation quality esti-
mates. In Proceedings of MT Summit XII.
Lucia Specia, Kashif Shah, Jos´e G. C. De Souza, and
Trevor Cohn. 2013. QuEst - A translation qual-
ity estimation framework. In Proceedings of ACL
Demo Session.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of EAMT.
</reference>
<page confidence="0.998664">
312
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.374749">
<title confidence="0.992384">SHEF-Lite 2.0: Sparse Multi-task Gaussian Processes for Quality Estimation</title>
<author confidence="0.999995">Beck Shah</author>
<affiliation confidence="0.9932465">Department of Computer University of</affiliation>
<address confidence="0.43399">Sheffield, United</address>
<abstract confidence="0.989614833333333">We describe our systems for the WMT14 Shared Task on Quality Estimation (subtasks 1.1, 1.2 and 1.3). Our submissions use the framework of Multi-task Gaussian Processes, where we combine multiple datasets in a multi-task setting. Due to the large size of our datasets we also experiment with Sparse Gaussian Processes, which aim to speed up training and prediction by providing sensible sparse approximations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mauricio A ´Alvarez</author>
<author>Lorenzo Rosasco</author>
<author>Neil D Lawrence</author>
</authors>
<title>Kernels for Vector-Valued Functions: a Review. Foundations and Trends</title>
<date>2012</date>
<booktitle>in Machine Learning,</booktitle>
<pages>1--37</pages>
<marker>´Alvarez, Rosasco, Lawrence, 2012</marker>
<rawString>Mauricio A. ´Alvarez, Lorenzo Rosasco, and Neil D. Lawrence. 2012. Kernels for Vector-Valued Functions: a Review. Foundations and Trends in Machine Learning, pages 1–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Beck</author>
<author>Kashif Shah</author>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>SHEF-Lite : When Less is More for Translation Quality Estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of WMT13,</booktitle>
<pages>337--342</pages>
<contexts>
<context position="14370" citStr="Beck et al., 2013" startWordPosition="2394" endWordPosition="2397">ero, which characterises overfitting. The same behaviour was not observed with the sparse model, even though it had a much higher number of hyperparameters to optimise, and was therefore more prone to overfitting. We plan to investigate this issue further but a possible cause could be bad starting values for the hyperparameters. Table 6 shows results for Task 1.3. In this task, the standard GP model outperformed the baseline, with the sparse GP model following very closely. These figures represent significant improvements compared to our submission to the same task in last year’s shared task (Beck et al., 2013), where we were not able to beat the baseline. The main differences between last year’s and this year’s models are the use of additional datasets and a higher number of features (25 vs. 40). The competitive results for the sparse GP models are very promising because they show we can combine multiple datasets to improve post-editing time prediction while employing a sparse model to cope with speed issues. 12122 – 3376 3.59x 978 12.39x 310 en-es es-en en-de de-en A ρ A ρ A ρ A ρ Standard GP 0.21 -0.33 0.11 -0.15 0.26 -0.36 0.24 -0.27 Sparse GP 0.17 0.27 0.12 -0.17 0.23 -0.33 0.14 -0.17 Baseline </context>
</contexts>
<marker>Beck, Shah, Cohn, Specia, 2013</marker>
<rawString>Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia Specia. 2013. SHEF-Lite : When Less is More for Translation Quality Estimation. In Proceedings of WMT13, pages 337–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th Conference on Computational Linguistics,</booktitle>
<pages>315--321</pages>
<contexts>
<context position="895" citStr="Blatz et al., 2004" startWordPosition="128" endWordPosition="131">ibe our systems for the WMT14 Shared Task on Quality Estimation (subtasks 1.1, 1.2 and 1.3). Our submissions use the framework of Multi-task Gaussian Processes, where we combine multiple datasets in a multi-task setting. Due to the large size of our datasets we also experiment with Sparse Gaussian Processes, which aim to speed up training and prediction by providing sensible sparse approximations. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Bojar et al., 2013). A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence. The WMT 2014 QE shared task defined a group of tasks related to QE. In this paper, we describe our submissions for subtasks 1.1, 1.2 and 1.3. Our models are based on Gaussian Processes (GPs) (Rasmussen and Williams, 2006), a non-parametric kernelised probabilistic framework. We propose t</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, 2004</marker>
<rawString>John Blatz, Erin Fitzgerald, and George Foster. 2004. Confidence estimation for machine translation. In Proceedings of the 20th Conference on Computational Linguistics, pages 315–321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of WMT13,</booktitle>
<pages>1--44</pages>
<contexts>
<context position="937" citStr="Bojar et al., 2013" startWordPosition="136" endWordPosition="139"> on Quality Estimation (subtasks 1.1, 1.2 and 1.3). Our submissions use the framework of Multi-task Gaussian Processes, where we combine multiple datasets in a multi-task setting. Due to the large size of our datasets we also experiment with Sparse Gaussian Processes, which aim to speed up training and prediction by providing sensible sparse approximations. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Bojar et al., 2013). A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence. The WMT 2014 QE shared task defined a group of tasks related to QE. In this paper, we describe our submissions for subtasks 1.1, 1.2 and 1.3. Our models are based on Gaussian Processes (GPs) (Rasmussen and Williams, 2006), a non-parametric kernelised probabilistic framework. We propose to combine multiple datasets to improve our</context>
<context position="10150" citStr="Bojar et al., 2013" startWordPosition="1708" endWordPosition="1711">ure for the model. Therefore, the outcome of a GP model using an ARD kernel can be viewed as a list of features ranked by relevance, and this information can be used for feature selection by discarding the lowest ranked (least useful) ones. 3 Preliminary Experiments Our submissions are based on multi-task settings. For task 1.1, we consider each language pair as a different task, training one model for all pairs. For tasks 1.2 and 1.3, we used additional datasets and encoded each one as a different task (totalling 3 tasks): WMT13: these are the datasets provided in last year’s QE shared task (Bojar et al., 2013). We combined training and test sets, totalling 2, 754 sentences for HTER prediction and 1, 003 sentences for post-editing time prediction, both for English-Spanish. EAMT11: this dataset is provided by Specia (2011) and is composed of 1, 000 EnglishSpanish sentences annotated in terms of HTER and post-editing time. For each task we prepared two submissions: one trained on a standard GP with the full 80 features set and another one trained on a sparse GP with a subset of 40 features. The features were chosen by training a smaller model on a subset of 400 instances and following the procedure ex</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of WMT13, pages 1–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edwin V Bonilla</author>
<author>Kian Ming A Chai</author>
<author>Christopher K I Williams</author>
</authors>
<date>2008</date>
<booktitle>Multi-task Gaussian Process Prediction. Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="5425" citStr="Bonilla et al. (2008)" startWordPosition="884" endWordPosition="887"> σ2n and SE lengthscales li) were learned from data by maximising the model likelihood. All our models were trained using the GPy1 toolkit, an open source implementation of GPs written in Python. 2.1 Multi-task learning The GP regression framework can be extended to multiple outputs by assuming f(x) to be a vector valued function. These models are commonly referred as coregionalization models in the GP literature ( ´Alvarez et al., 2012). Here we refer to them as multi-task kernels, to emphasize our application. In this work, we employ a separable multi-task kernel, similar to the one used by Bonilla et al. (2008) and Cohn and Specia (2013). Considering a set of D tasks, we define the corresponding multi-task kernel as: k((x, d), (x0, d0)) = kdata(x, x0) x Bd,d�, (1) 1http://sheffieldml.github.io/GPy/ where kdata is a kernel on the input points, d and d0 are task or metadata information for each input and B E RD×D is the multi-task matrix, which encodes task covariances. For task 1.1, we consider each language pair as a different task, while for tasks 1.2 and 1.3 we use additional datasets for the same language pair (en-es), treating each dataset as a different task. To perform the learning procedure t</context>
</contexts>
<marker>Bonilla, Chai, Williams, 2008</marker>
<rawString>Edwin V. Bonilla, Kian Ming A. Chai, and Christopher K. I. Williams. 2008. Multi-task Gaussian Process Prediction. Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Chu</author>
<author>Zoubin Ghahramani</author>
<author>Francesco Falciani</author>
<author>David L Wild</author>
</authors>
<title>Biomarker discovery in microarray gene expression data with Gaussian processes.</title>
<date>2005</date>
<journal>Bioinformatics,</journal>
<volume>21</volume>
<issue>16</issue>
<contexts>
<context position="13426" citStr="Chu et al., 2005" startWordPosition="2235" endWordPosition="2238">s. 4 Official Results and Discussion Table 4 shows the results for Task 1.1. Using standard GPs we obtained improved results over the baseline for English-Spanish and EnglishGerman only, with particularly substantial improvements for English-Spanish, which also happens for sparse GPs. This may be related to the larger size of this dataset when compared to the others. Our results here are mostly inconclusive though and we plan to investigate this setting more in depth in the future. Specifically, due to the coarse behaviour of the labels, ordinal regression GP models (like the one proposed in (Chu et al., 2005)) could be useful for this task. Results for Task 1.2 are shown in Table 5. The standard GP model performed unusually poorly when compared to the baseline or the sparse GP model. To investigate this, we inspected the resulting model hyperparameters. We found out that the noise σ2 was optimised to a very low value, close to zero, which characterises overfitting. The same behaviour was not observed with the sparse model, even though it had a much higher number of hyperparameters to optimise, and was therefore more prone to overfitting. We plan to investigate this issue further but a possible cau</context>
</contexts>
<marker>Chu, Ghahramani, Falciani, Wild, 2005</marker>
<rawString>Wei Chu, Zoubin Ghahramani, Francesco Falciani, and David L Wild. 2005. Biomarker discovery in microarray gene expression data with Gaussian processes. Bioinformatics, 21(16):3385–93, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4481" citStr="Cohn and Specia, 2013" startWordPosition="720" endWordPosition="723">x∗,f)p(f|D), where x∗ is a test input, y∗ is the test response value and D is the training set. The predictive posterior can be solved analitically, resulting in: y∗ — jV(kT∗ (K + σ2nI)−1y, k(x∗, x∗) — kT∗ (K + σ2nI)−1k∗), where k∗ = [k(x∗, x1)k(x∗, x2) ... k(x∗, xn)]T is the vector of kernel evaluations between the training set and the test input and K is the kernel matrix over the training inputs (the Gram matrix). The kernel function encodes the covariance (similarity) between each input pair. While a variety of kernel functions are available, here we followed previous work in QE using GP (Cohn and Specia, 2013; Shah et al., 2013) and employed a squared exponential (SE) kernel with automatic relevance determination (ARD): 1 F k(x, x0) = σ 2f exp — 2 i=1 where F is the number of features, σ2 f is the covariance magnitude and li &gt; 0 are the feature lengthscales. The resulting model hyperparameters (SE variance σ2f, noise variance σ2n and SE lengthscales li) were learned from data by maximising the model likelihood. All our models were trained using the GPy1 toolkit, an open source implementation of GPs written in Python. 2.1 Multi-task learning The GP regression framework can be extended to multiple o</context>
<context position="6154" citStr="Cohn and Specia (2013)" startWordPosition="1008" endWordPosition="1011">: k((x, d), (x0, d0)) = kdata(x, x0) x Bd,d�, (1) 1http://sheffieldml.github.io/GPy/ where kdata is a kernel on the input points, d and d0 are task or metadata information for each input and B E RD×D is the multi-task matrix, which encodes task covariances. For task 1.1, we consider each language pair as a different task, while for tasks 1.2 and 1.3 we use additional datasets for the same language pair (en-es), treating each dataset as a different task. To perform the learning procedure the multitask matrix should be parameterised in a sensible way. We follow the parameterisations proposed by Cohn and Specia (2013), which we briefly describe here: Independent: B = I. In this setting each task is modelled independently. This is not strictly equivalent to independent model training because the tasks share the same data kernel (and the same hyperparameters); Pooled: B = 1. Here the task identity is ignored. This is equivalent to pooling all datasets in a single task model; Combined: B = 1 + αI. This setting leverages between independent and pooled models. Here, α &gt; 0 is treated as an hyperparameter; Combined+: B = 1 + diag(α). Same as “combined”, but allowing one different α value per task. 2.2 Sparse Gaus</context>
<context position="16628" citStr="Cohn and Specia, 2013" startWordPosition="2780" endWordPosition="2783">ed to speed-up training and prediction times by employing sparse GPs, which becomes crucial in multi-task settings. The results obtained are specially promising in the post-editing time task, where we obtained the same results as with standard GPs and improved over our models from the last evaluation campaign. In the future, we plan to employ our multi-task models in large-scale settings, like datasets annotated through crowdsourcing platforms. These datasets are usually labelled by dozens of annotators and multi-task GPs have proved an interesting framework for learning the annotation noise (Cohn and Specia, 2013). However, multiple tasks can easily make training and prediction times prohibitive, and thus another direction if work is to use recent advances in sparse GPs, like the one proposed by Hensman et al. (2013). We believe that the combination of these approaches could further improve the state-of-the-art performance in these tasks. Acknowledgments This work was supported by funding from CNPq/Brazil (No. 237999/2012-9, Daniel Beck) and from European Union’s Seventh Framework Programme for research, technological development and demonstration under grant agreement no. 296347 (QTLaunchPad). Referen</context>
</contexts>
<marker>Cohn, Specia, 2013</marker>
<rawString>Trevor Cohn and Lucia Specia. 2013. Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Hensman</author>
<author>Nicol`o Fusi</author>
<author>Neil D Lawrence</author>
</authors>
<title>Gaussian Processes for Big Data.</title>
<date>2013</date>
<booktitle>In Proceedings of UAI.</booktitle>
<marker>Hensman, Fusi, Lawrence, 2013</marker>
<rawString>James Hensman, Nicol`o Fusi, and Neil D. Lawrence. 2013. Gaussian Processes for Big Data. In Proceedings of UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Edward Rasmussen</author>
<author>Christopher K I Williams</author>
</authors>
<title>Gaussian processes for machine learning, volume 1.</title>
<date>2006</date>
<publisher>MIT Press</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1428" citStr="Rasmussen and Williams, 2006" startWordPosition="219" endWordPosition="222">, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Bojar et al., 2013). A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence. The WMT 2014 QE shared task defined a group of tasks related to QE. In this paper, we describe our submissions for subtasks 1.1, 1.2 and 1.3. Our models are based on Gaussian Processes (GPs) (Rasmussen and Williams, 2006), a non-parametric kernelised probabilistic framework. We propose to combine multiple datasets to improve our QE models by applying GPs in a multi-task setting. Our hypothesis is that using sensible multi-task learning settings gives improvements over simply pooling all datasets together. Task 1.1 focuses on predicting post-editing effort for four language pairs: English-Spanish (en-es), Spanish-English (es-en), English-German (en-de), and German-English (de-en). Each contains a different number of source sentences and their human translations, as well as 2-3 versions of machine translations: </context>
<context position="7463" citStr="Rasmussen and Williams, 2006" startWordPosition="1245" endWordPosition="1248">sion, which is O(n3) for standard GPs, with n being the number of training instances. For multi-task settings this can be a potential issue because these models replicate the instances for each task and the resulting Gram matrix has dimensionality nd x nd, where d is the number of tasks. Sparse GPs tackle this problem by approximating the Gram matrix using only a subset of m inducing inputs. Without loss of generalisation, consider these m points as the first instances in the training data. We can then expand the Gram matrix in the following way: r Kmm K K(n−m)m K(n Following the notation in (Rasmussen and Williams, 2006), we refer Km(n−m) as Kmn and xi — x0i li �, K= � m(n−m) −m)(n−m) . 308 its transpose as Knm. The block structure of K forms the basis of the so-called Nystr¨om approximation: K˜ = KnmK−1 mmKmn, (2) which results in the following predictive posterior: y∗ ∼ N(kTm∗ ˜G−1Kmny, (3) k(x∗, x∗) − kTm∗K−1mmkm∗+ 2 T1 ankm∗˜G km∗), where G˜ = a2 nKmm + KmnKnm and km∗ is the vector of kernel evaluations between test input x∗ and the m inducing inputs. The resulting training complexity is O(m2n). The remaining question is how to choose the inducing inputs. We follow the approach of Snelson and Ghahramani (</context>
</contexts>
<marker>Rasmussen, Williams, 2006</marker>
<rawString>Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian processes for machine learning, volume 1. MIT Press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashif Shah</author>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>An Investigation on the Effectiveness of Features for Translation Quality Estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of MT Summit XIV.</booktitle>
<contexts>
<context position="4501" citStr="Shah et al., 2013" startWordPosition="724" endWordPosition="727">s a test input, y∗ is the test response value and D is the training set. The predictive posterior can be solved analitically, resulting in: y∗ — jV(kT∗ (K + σ2nI)−1y, k(x∗, x∗) — kT∗ (K + σ2nI)−1k∗), where k∗ = [k(x∗, x1)k(x∗, x2) ... k(x∗, xn)]T is the vector of kernel evaluations between the training set and the test input and K is the kernel matrix over the training inputs (the Gram matrix). The kernel function encodes the covariance (similarity) between each input pair. While a variety of kernel functions are available, here we followed previous work in QE using GP (Cohn and Specia, 2013; Shah et al., 2013) and employed a squared exponential (SE) kernel with automatic relevance determination (ARD): 1 F k(x, x0) = σ 2f exp — 2 i=1 where F is the number of features, σ2 f is the covariance magnitude and li &gt; 0 are the feature lengthscales. The resulting model hyperparameters (SE variance σ2f, noise variance σ2n and SE lengthscales li) were learned from data by maximising the model likelihood. All our models were trained using the GPy1 toolkit, an open source implementation of GPs written in Python. 2.1 Multi-task learning The GP regression framework can be extended to multiple outputs by assuming f</context>
<context position="8420" citStr="Shah et al. (2013)" startWordPosition="1420" endWordPosition="1423">mm + KmnKnm and km∗ is the vector of kernel evaluations between test input x∗ and the m inducing inputs. The resulting training complexity is O(m2n). The remaining question is how to choose the inducing inputs. We follow the approach of Snelson and Ghahramani (2006), which note that these inducing inputs do not need to be a subset of the training data. Their method considers each input as a hyperparameter, which is then optimised jointly with the kernel hyperparameters. 2.3 Features For all tasks we used the QuEst framework (Specia et al., 2013) to extract a set of 80 black-box features as in Shah et al. (2013), for which we had all the necessary resources available. Examples of the features extracted include: • N-gram-based features: – Number of tokens in source and target segments; – Language model (LM) probability of source and target segments; – Percentage of source 1–3-grams observed in different frequency quartiles of a large corpus of the source language; – Average number of translations per source word in the segment as given by IBM 1 model from a large parallel corpus of the language, with probabilities thresholded in different ways. • POS-based features: – Ratio of percentage of nouns/verb</context>
</contexts>
<marker>Shah, Cohn, Specia, 2013</marker>
<rawString>Kashif Shah, Trevor Cohn, and Lucia Specia. 2013. An Investigation on the Effectiveness of Features for Translation Quality Estimation. In Proceedings of MT Summit XIV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Snelson</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Sparse Gaussian Processes using Pseudo-inputs.</title>
<date>2006</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="8068" citStr="Snelson and Ghahramani (2006)" startWordPosition="1355" endWordPosition="1358">ssen and Williams, 2006), we refer Km(n−m) as Kmn and xi — x0i li �, K= � m(n−m) −m)(n−m) . 308 its transpose as Knm. The block structure of K forms the basis of the so-called Nystr¨om approximation: K˜ = KnmK−1 mmKmn, (2) which results in the following predictive posterior: y∗ ∼ N(kTm∗ ˜G−1Kmny, (3) k(x∗, x∗) − kTm∗K−1mmkm∗+ 2 T1 ankm∗˜G km∗), where G˜ = a2 nKmm + KmnKnm and km∗ is the vector of kernel evaluations between test input x∗ and the m inducing inputs. The resulting training complexity is O(m2n). The remaining question is how to choose the inducing inputs. We follow the approach of Snelson and Ghahramani (2006), which note that these inducing inputs do not need to be a subset of the training data. Their method considers each input as a hyperparameter, which is then optimised jointly with the kernel hyperparameters. 2.3 Features For all tasks we used the QuEst framework (Specia et al., 2013) to extract a set of 80 black-box features as in Shah et al. (2013), for which we had all the necessary resources available. Examples of the features extracted include: • N-gram-based features: – Number of tokens in source and target segments; – Language model (LM) probability of source and target segments; – Perc</context>
</contexts>
<marker>Snelson, Ghahramani, 2006</marker>
<rawString>Edward Snelson and Zoubin Ghahramani. 2006. Sparse Gaussian Processes using Pseudo-inputs. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="2494" citStr="Snover et al., 2006" startWordPosition="387" endWordPosition="390">rman-English (de-en). Each contains a different number of source sentences and their human translations, as well as 2-3 versions of machine translations: by a statistical (SMT) system, a rule-based system (RBMT) system and, for en-es/de only, a hybrid system. Source sentences were extracted from tests sets of WMT13 and WMT12, and the translations were produced by top MT systems of each type and a human translator. Labels range from 1 to 3, with 1 indicating a perfect translation and 3, a low quality translation. The purpose of task 1.2 is to predict HTER scores (Human Translation Error Rate) (Snover et al., 2006) using a dataset composed of 896 English-Spanish sentences translated by a MT system and post-edited by a professional translator. Finally, task 1.3 aims at predicting post-editing time, using a subset of 650 sentences from the Task 1.2 dataset. For each task, participants can submit two types of results: scoring and ranking. For scoring, evaluation is made in terms of Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). For ranking, DeltaAvg and Spearman’s rank correlation were used as evaluation metrics. 2 Model Gaussian Processes are a Bayesian non-parametric machine learning framew</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Craig Saunders</author>
<author>Marco Turchi</author>
<author>Zhuoran Wang</author>
<author>John Shawe-Taylor</author>
</authors>
<title>Improving the confidence of machine translation quality estimates.</title>
<date>2009</date>
<booktitle>In Proceedings of MT Summit XII.</booktitle>
<contexts>
<context position="916" citStr="Specia et al., 2009" startWordPosition="132" endWordPosition="135">the WMT14 Shared Task on Quality Estimation (subtasks 1.1, 1.2 and 1.3). Our submissions use the framework of Multi-task Gaussian Processes, where we combine multiple datasets in a multi-task setting. Due to the large size of our datasets we also experiment with Sparse Gaussian Processes, which aim to speed up training and prediction by providing sensible sparse approximations. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Bojar et al., 2013). A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence. The WMT 2014 QE shared task defined a group of tasks related to QE. In this paper, we describe our submissions for subtasks 1.1, 1.2 and 1.3. Our models are based on Gaussian Processes (GPs) (Rasmussen and Williams, 2006), a non-parametric kernelised probabilistic framework. We propose to combine multiple da</context>
</contexts>
<marker>Specia, Saunders, Turchi, Wang, Shawe-Taylor, 2009</marker>
<rawString>Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran Wang, and John Shawe-Taylor. 2009. Improving the confidence of machine translation quality estimates. In Proceedings of MT Summit XII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Kashif Shah</author>
<author>Jos´e G C De Souza</author>
<author>Trevor Cohn</author>
</authors>
<title>QuEst - A translation quality estimation framework.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL Demo Session.</booktitle>
<marker>Specia, Shah, De Souza, Cohn, 2013</marker>
<rawString>Lucia Specia, Kashif Shah, Jos´e G. C. De Souza, and Trevor Cohn. 2013. QuEst - A translation quality estimation framework. In Proceedings of ACL Demo Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
</authors>
<title>Exploiting objective annotations for measuring translation post-editing effort.</title>
<date>2011</date>
<booktitle>In Proceedings of EAMT.</booktitle>
<contexts>
<context position="10365" citStr="Specia (2011)" startWordPosition="1743" endWordPosition="1744">least useful) ones. 3 Preliminary Experiments Our submissions are based on multi-task settings. For task 1.1, we consider each language pair as a different task, training one model for all pairs. For tasks 1.2 and 1.3, we used additional datasets and encoded each one as a different task (totalling 3 tasks): WMT13: these are the datasets provided in last year’s QE shared task (Bojar et al., 2013). We combined training and test sets, totalling 2, 754 sentences for HTER prediction and 1, 003 sentences for post-editing time prediction, both for English-Spanish. EAMT11: this dataset is provided by Specia (2011) and is composed of 1, 000 EnglishSpanish sentences annotated in terms of HTER and post-editing time. For each task we prepared two submissions: one trained on a standard GP with the full 80 features set and another one trained on a sparse GP with a subset of 40 features. The features were chosen by training a smaller model on a subset of 400 instances and following the procedure explained in Section 2.3 for feature selection, with a pre-define cutoff point on the number of features (40), based on previous experiments. The sparse models were trained using 400 inducing inputs. To select an appr</context>
</contexts>
<marker>Specia, 2011</marker>
<rawString>Lucia Specia. 2011. Exploiting objective annotations for measuring translation post-editing effort. In Proceedings of EAMT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000147">
<title confidence="0.9990385">
Rule-based Syntactic Preprocessing
for Syntax-based Machine Translation
</title>
<author confidence="0.999482">
Yuto Hatakoshi, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura
</author>
<affiliation confidence="0.9883495">
Nara Institute of Science and Technology
Graduate School of Information Science
</affiliation>
<address confidence="0.601669">
Takayama, Ikoma, Nara 630-0192, Japan
</address>
<email confidence="0.999291">
{hatakoshi.yuto.hq8,neubig,ssakti,tomoki,s-nakamura}@is.naist.jp
</email>
<sectionHeader confidence="0.997398" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999791833333334">
Several preprocessing techniques using
syntactic information and linguistically
motivated rules have been proposed to im-
prove the quality of phrase-based machine
translation (PBMT) output. On the other
hand, there has been little work on similar
techniques in the context of other trans-
lation formalisms such as syntax-based
SMT. In this paper, we examine whether
the sort of rule-based syntactic preprocess-
ing approaches that have proved beneficial
for PBMT can contribute to syntax-based
SMT. Specifically, we tailor a highly suc-
cessful preprocessing method for English-
Japanese PBMT to syntax-based SMT,
and find that while the gains achievable are
smaller than those for PBMT, significant
improvements in accuracy can be realized.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983213114755">
In the widely-studied framework of phrase-based
machine translation (PBMT) (Koehn et al., 2003),
translation probabilities between phrases consist-
ing of multiple words are calculated, and trans-
lated phrases are rearranged by the reordering
model in the appropriate target language order.
While PBMT provides a light-weight framework
to learn translation models and achieves high
translation quality in many language pairs, it does
not directly incorporate morphological or syntac-
tic information. Thus, many preprocessing meth-
ods for PBMT using these types of information
have been proposed. Methods include preprocess-
ing to obtain accurate word alignments by the divi-
sion of the prefix of verbs (Nießen and Ney, 2000),
preprocessing to reduce the errors in verb conju-
gation and noun case agreement (Avramidis and
Koehn, 2008), and many others. The effectiveness
of the syntactic preprocessing for PBMT has been
supported by these and various related works.
In particular, much attention has been paid to
preordering (Xia and McCord, 2004; Collins et
al., 2005), a class of preprocessing methods for
PBMT. PBMT has well-known problems with lan-
guage pairs that have very different word order,
due to the fact that the reordering model has dif-
ficulty estimating the probability of long distance
reorderings. Therefore, preordering methods at-
tempt to improve the translation quality of PBMT
by rearranging source language sentences into an
order closer to that of the target language. It’s of-
ten the case that preordering methods are based
on rule-based approaches, and these methods have
achieved great success in ameliorating the word
ordering problems faced by PBMT (Collins et al.,
2005; Xu et al., 2009; Isozaki et al., 2010b).
One particularly successful example of rule-
based syntactic preprocessing is Head Finalization
(Isozaki et al., 2010b), a method of syntactic pre-
processing for English to Japanese translation that
has significantly improved translation quality of
English-Japanese PBMT using simple rules based
on the syntactic structure of the two languages.
The most central part of the method, as indicated
by its name, is a reordering rule that moves the
English head word to the end of the corresponding
syntactic constituents to match the head-final syn-
tactic structure of Japanese sentences. Head Final-
ization also contains some additional preprocess-
ing steps such as determiner elimination, parti-
cle insertion and singularization to generate a sen-
tence that is closer to Japanese grammatical struc-
ture.
In addition to PBMT, there has also recently
been interest in syntax-based SMT (Yamada and
Knight, 2001; Liu et al., 2006), which translates
using syntactic information. However, few at-
tempts have been made at syntactic preprocessing
for syntax-based SMT, as the syntactic informa-
tion given by the parser is already incorporated
directly in the translation model. Notable excep-
</bodyText>
<page confidence="0.983509">
34
</page>
<note confidence="0.782277">
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999748344827586">
tions include methods to perform tree transforma-
tions improving correspondence between the sen-
tence structure and word alignment (Burkett and
Klein, 2012), methods for binarizing parse trees to
match word alignments (Zhang et al., 2006), and
methods for adjusting label sets to be more ap-
propriate for syntax-based SMT (Hanneman and
Lavie, 2011; Tamura et al., 2013). It should be
noted that these methods of syntactic preprocess-
ing for syntax-based SMT are all based on auto-
matically learned rules, and there has been little in-
vestigation of the manually-created linguistically-
motivated rules that have proved useful in prepro-
cessing for PBMT.
In this paper, we examine whether rule-based
syntactic preprocessing methods designed for
PBMT can contribute anything to syntax-based
machine translation. Specifically, we examine
whether the reordering and lexical processing of
Head Finalization contributes to the improvement
of syntax-based machine translation as it did for
PBMT. Additionally, we examine whether it is
possible to incorporate the intuitions behind the
Head Finalization reordering rules as soft con-
straints by incorporating them as a decoder fea-
ture. As a result of our experiments, we demon-
strate that rule-based lexical processing can con-
tribute to improvement of translation quality of
syntax-based machine translation.
</bodyText>
<sectionHeader confidence="0.970306" genericHeader="method">
2 Head Finalization
</sectionHeader>
<bodyText confidence="0.999967066666667">
Head Finalization is a syntactic preprocessing
method for English to Japanese PBMT, reducing
grammatical errors through reordering and lexi-
cal processing. Isozaki et al. (2010b) have re-
ported that translation quality of English-Japanese
PBMT is significantly improved using a transla-
tion model learned by English sentences prepro-
cessed by Head Finalization and Japanese sen-
tences. In fact, this method achieved the highest
results in the large scale NTCIR 2011 evaluation
(Sudoh et al., 2011), the first time a statistical ma-
chine translation (SMT) surpassed rule-based sys-
tems for this very difficult language pair, demon-
strating the utility of these simple syntactic trans-
formations from the point of view of PBMT.
</bodyText>
<subsectionHeader confidence="0.978408">
2.1 Reordering
</subsectionHeader>
<bodyText confidence="0.994316333333333">
The reordering process of Head Finalization uses
a simple rule based on the features of Japanese
grammar. To convert English sentence into
</bodyText>
<figure confidence="0.559088">
Original English
</figure>
<figureCaption confidence="0.999419">
Figure 1: Head Finalization
</figureCaption>
<bodyText confidence="0.999943611111111">
Japanese word order, the English sentence is first
parsed using a syntactic parser, and then head
words are moved to the end of the corresponding
syntactic constituents in each non-terminal node
of the English syntax tree. This helps replicate
the ordering of words in Japanese grammar, where
syntactic head words come after non-head (depen-
dent) words.
Figure 1 shows an example of the application
of Head Finalization to an English sentence. The
head node of the English syntax tree is connected
to the parent node by a bold line. When this node
is the first child node, we move it behind the de-
pendent node in order to convert the English sen-
tence into head final order. In this case, moving
the head node VBD of black node VP to the end of
this node, we can obtain the sentence “John a ball
hit” which is in a word order similar to Japanese.
</bodyText>
<subsectionHeader confidence="0.998589">
2.2 Lexical Processing
</subsectionHeader>
<bodyText confidence="0.99806">
In addition to reordering, Head Finalization con-
ducts the following three steps that do not affect
word order. These steps do not change the word
</bodyText>
<figure confidence="0.999093391304348">
S
VP
NP
NP
NN VBD DT NN
John hit a ball
Head Final English
S
VP
NP
NP
NN
DT NN
VBD
Reordering
Add Japanese Particles
Singularize,
Eliminate Determiners
John a ball hit
John a ball hit
va0 va2
John a ball hit
va0 va2
</figure>
<page confidence="0.997767">
35
</page>
<bodyText confidence="0.999709428571429">
ordering, but still result in an improvement of
translation quality, and it can be assumed that the
effect of this variety of syntactic preprocessing is
not only applicable to PBMT but also other trans-
lation methods that do not share PBMT’s problems
of reordering such as syntax-based SMT. The three
steps included are as follows:
</bodyText>
<listItem confidence="0.993483">
1. Pseudo-particle insertion
2. Determiner (“a”, “an”, “the”) elimination
3. Singularization
</listItem>
<bodyText confidence="0.999888">
The motivation for the first step is that in con-
trast to English, which has relatively rigid word
order and marks grammatical cases of many noun
phrases according to their position relative to the
verb, Japanese marks the topic, subject, and object
using case marking particles. As Japanese parti-
cles are not found in English, Head Finalization
inserts “pseudo-particles” to prevent a mistransla-
tion or lack of particles in the translation process.
In the pseudo-particle insertion process (1), we in-
sert the following three types of pseudo-particles
equivalent to Japanese case markers “wa” (topic),
“ga” (subject) or “wo” (object).
</bodyText>
<listItem confidence="0.999989">
• va0: Subject particle of the main verb
• va1: Subject particle of other verbs
• va2: Object particle of any verb
</listItem>
<bodyText confidence="0.999921411764706">
In the example of Figure 1, we insert the topic par-
ticle va0 behind of “John”, which is a subject of a
verb “hit” and object particle va2 at the back of
object “ball.”
Another source of divergence between the two
languages stems from the fact that Japanese does
not contain determiners or makes distinctions be-
tween singular and plural by inflection of nouns.
Thus, to generate a sentence that is closer to
Japanese, Head Finalization eliminates determin-
ers (2) and singularizes plural nouns (3) in addi-
tion to the pseudo-particle insertion.
In Figure 1, we can see that applying these
three processes to the source English sentence re-
sults in the sentence “John va0 (wa) ball va2 (wo)
hit” which closely resembles the structure of the
Japanese translation “jon wa bo-ru wo utta.”
</bodyText>
<sectionHeader confidence="0.9333085" genericHeader="method">
3 Syntax-based Statistical Machine
Translation
</sectionHeader>
<bodyText confidence="0.987218236842105">
Syntax-based SMT is a method for statistical
translation using syntactic information of the sen-
tence (Yamada and Knight, 2001; Liu et al., 2006).
By using translation patterns following the struc-
ture of linguistic syntax trees, syntax-based trans-
lations often makes it possible to achieve more
grammatical translations and reorderings com-
pared with PBMT. In this section, we describe
tree-to-string (T2S) machine translation based on
synchronous tree substitution grammars (STSG)
(Graehl et al., 2008), the variety of syntax-based
SMT that we use in our experiments.
T2S captures the syntactic relationship between
two languages by using the syntactic structure of
parsing results of the source sentence. Each trans-
lation pattern is expressed as a source sentence
subtree using rules including variables. The fol-
lowing example of a translation pattern include
two noun phrases NP0 and NP1, which are trans-
lated and inserted into the target placeholders X0
and X1 respectively. The decoder generates the
translated sentence in consideration of the proba-
bility of translation pattern itself and translations
of the subtrees of NP0 and NP1.
S((NP0) (VP(VBD hit) (NP1)))
—* X0 wa X1 wo utta
T2S has several advantages over PBMT. First,
because the space of translation candidates is re-
duced using the source sentence subtree, it is often
possible to generate translations that are more ac-
curate, particularly with regards to long-distance
reordering, as long as the source parse is correct.
Second, the time to generate translation results is
also reduced because the search space is smaller
than PBMT. On the other hand, because T2S gen-
erates translation results using the result of auto-
matic parsing, translation quality highly depends
on the accuracy of the parser.
</bodyText>
<sectionHeader confidence="0.8063745" genericHeader="method">
4 Applying Syntactic Preprocessing to
Syntax-based Machine Translation
</sectionHeader>
<bodyText confidence="0.999971">
In this section, we describe our proposed method
to apply Head Finalization to T2S translation.
Specifically, we examine two methods for incor-
porating the Head Finalization rules into syntax-
based SMT: through applying them as preprocess-
ing step to the trees used in T2S translation, and
</bodyText>
<page confidence="0.989567">
36
</page>
<bodyText confidence="0.999429">
through adding reordering information as a feature
of the translation patterns.
</bodyText>
<subsectionHeader confidence="0.996658">
4.1 Syntactic Preprocessing for T2S
</subsectionHeader>
<bodyText confidence="0.999859125">
We applied the two types of processing shown in
Table 1 as preprocessing for T2S. This is similar
to preprocessing for PBMT with the exception that
preprocessing for PBMT results in a transformed
string, and preprocessing for T2S results in a trans-
formed tree. In the following sections, we elabo-
rate on methods for applying these preprocessing
steps to T2S and some effects expected therefrom.
</bodyText>
<tableCaption confidence="0.994573">
Table 1: Syntactic preprocessing applied to T2S
</tableCaption>
<subsubsectionHeader confidence="0.372302">
4.1.1 Reordering for T2S
</subsubsectionHeader>
<bodyText confidence="0.999988142857143">
In the case of PBMT, reordering is used to change
the source sentence word order to be closer to
that of the target, reducing the burden on the rel-
atively weak PBMT reordering models. On the
other hand, because translation patterns of T2S
are expressed by using source sentence subtrees,
the effect of reordering problems are relatively
small, and the majority of reordering rules spec-
ified by hand can be automatically learned in a
well-trained T2S model. Therefore, preordering
is not expected to cause large gains, unlike in the
case of PBMT.
However, it can also be thought that preordering
can still have a positive influence on the translation
model training process, particularly by increasing
alignment accuracy. For example, training meth-
ods for word alignment such as the IBM or HMM
models (Och and Ney, 2003) are affected by word
order, and word alignment may be improved by
moving word order closer between the two lan-
guages. As alignment accuracy plays a important
role in T2S translation (Neubig and Duh, 2014), it
is reasonable to hypothesize that reordering may
also have a positive effect on T2S. In terms of the
actual incorporation with the T2S system, we sim-
ply follow the process in Figure 1, but output the
reordered tree instead of only the reordered termi-
nal nodes as is done for PBMT.
</bodyText>
<figure confidence="0.582182">
Original English
</figure>
<figureCaption confidence="0.9849435">
Figure 2: A method of applying Lexical Process-
ing
</figureCaption>
<subsubsectionHeader confidence="0.742912">
4.1.2 Lexical Processing for T2S
</subsubsectionHeader>
<bodyText confidence="0.99993762962963">
In comparison to reordering, Lexical Processing
may be expected to have a larger effect on T2S,
as it will both have the potential to increase align-
ment accuracy, and remove the burden of learning
rules to perform simple systematic changes that
can be written by hand. Figure 2 shows an ex-
ample of the application of Lexical Processing to
transform not strings, but trees.
In the pseudo-particle insertion component,
three pseudo particles “va0,” “va1,” and “va2” (as
shown in Section 2.2) are added in the source En-
glish syntax tree as terminal nodes with the non-
terminal node “VA”. As illustrated in Figure 2, par-
ticles are inserted as children at the end of the cor-
responding NP node. For example, in the figure
the topic particle “va0” is inserted after “John,”
subject of the verb “hit,” and the object particle
“va2” is inserted at the end of the NP for “ball,”
the object.
In the determiner elimination process, terminal
nodes “a,” “an,” and “the” are eliminated along
with non-terminal node DT. Determiner “a” and
its corresponding non-terminal DT are eliminated
in the Figure 2 example.
Singularization, like in the processing for
PBMT, simply changes plural noun terminals to
their base form.
</bodyText>
<subsectionHeader confidence="0.6782835">
4.2 Reordering Information as Soft
Constraints
</subsectionHeader>
<bodyText confidence="0.999643111111111">
As described in section 4.1.1, T2S work well on
language pairs that have very different word order,
but is sensitive to alignment accuracy. On the other
hand, we know that in most cases Japanese word
order tends to be head final, and thus any rules that
do not obey head final order may be the result of
bad alignments. On the other hand, there are some
cases where head final word order is not applica-
ble (such as sentences that contain the determiner
</bodyText>
<table confidence="0.5862449">
Preprocessing
Description
Reordering Reordering based on Japanese
typical head-final grammatical
structure
Lexical Processing Pseudo-particle insertion, deter-
miner elimination, singulariza-
tion
John hit a ball John va0 hit ball va2
NP
NN VBD DT NN
S
VP
NP
lexical Processing
NN VA VBD NN VA
NP
S
VP
NP
</table>
<page confidence="0.996874">
37
</page>
<bodyText confidence="0.999724235294118">
“no,” or situations where non-literal translations
are necessary) and a hard constraint to obey head-
final word order could be detrimental.
In order to incorporate this intuition, we add
a feature (HF-feature) to translation patterns that
conform to the reordering rules of Head Final-
ization. This gives the decoder ability to discern
translation patterns that follow the canonical re-
ordering patterns in English-Japanese translation,
and has the potential to improve translation quality
in the T2S translation model.
We use the log-linear approach (Och, 2003) to
add the Head Finalization feature (HF-feature). As
in the standard log-linear model, a source sen-
tence f is translated into a target language sen-
tence e, by searching for the sentence maximizing
the score:
</bodyText>
<equation confidence="0.9873485">
e� = arg max w� - h(f, e). (1)
e
</equation>
<bodyText confidence="0.999988580645161">
where h(f, e) is a feature function vector. w is
a weight vector that scales the contribution from
each feature. Each feature can take any real value
which is useful to improve translation quality, such
as the log of the n-gram language model proba-
bility to represent fluency, or lexical/phrase trans-
lation probability to capture the word or phrase-
wise correspondence. Thus, if we can incorporate
the information about reordering expressed by the
Head Finalization reordering rule as a features in
this model, we can learn weights to inform the de-
coder that it should generally follow this canonical
ordering.
Figure 3 shows a procedure of Head Finaliza-
tion feature (HF-feature) addition. To add the
HF-feature to translation patterns, we examine
the translation rules, along with the alignments
between target and source terminals and non-
terminals. First, we apply the Reordering to the
source side of the translation pattern subtree ac-
cording to the canonical head-final reordering rule.
Second, we examine whether the word order of the
reordered translation pattern matches with that of
the target translation pattern for which the word
alignment is non-crossing, indicating that the tar-
get string is also in head-final word order. Finally,
we set a binary feature (hHF(f, e) = 1) if the tar-
get word order obeys the head final order. This
feature is only applied to translation patterns for
which the number of target side words is greater
than or equal to two.
</bodyText>
<figureCaption confidence="0.77569">
Figure 3: Procedure of HF-feature addition
Table 2: The details of NTCIR7
</figureCaption>
<table confidence="0.999533375">
Dataset Lang Words Sentences Average
length
train En 99.0M 3.08M 32.13
Ja 117M 3.08M 37.99
dev En 28.6k 0.82k 34.83
Ja 33.5k 0.82k 40.77
test En 44.3k 1.38k 32.11
Ja 52.4k 1.38k 37.99
</table>
<sectionHeader confidence="0.9985" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<bodyText confidence="0.999970833333333">
In our experiment, we examined how much each
of the preprocessing steps (Reordering, Lexical
Processing) contribute to improve the translation
quality of PBMT and T2S. We also examined the
improvement in translation quality of T2S by the
introduction of the Head Finalization feature.
</bodyText>
<subsectionHeader confidence="0.950256">
5.1 Experimental Environment
</subsectionHeader>
<bodyText confidence="0.9993095">
For our English to Japanese translation experi-
ments, we used NTCIR7 PATENT-MT’s Patent
corpus (Fujii et al., 2008). Table 2 shows the
details of training data (train), development data
(dev), and test data (test).
As the PBMT and T2S engines, we used the
Moses (Koehn et al., 2007) and Travatar (Neubig,
2013) translation toolkits with the default settings.
</bodyText>
<figure confidence="0.9982465">
utta
x0 wo
x0:NP
hit
Source side of
translation pattern
Target side of
translation pattern
VP
VBD NP
hit x0:NP
Word alignment
1. Apply Reordering to
source translation pattern
VP
NP VBD
Reordered
translation pattern
x0 wo utta
�������������������
2. Add HF-feature
if ward alignment is
non-crossing
Target side of
</figure>
<page confidence="0.997285">
38
</page>
<bodyText confidence="0.999819222222222">
Enju (Miyao and Tsujii, 2002) is used to parse En-
glish sentences and KyTea (Neubig et al., 2011) is
used as a Japanese tokenizer. We generated word
alignments using GIZA++ (Och and Ney, 2003)
and trained a Kneser-Ney smoothed 5-gram LM
using SRILM (Stolcke et al., 2011). Minimum
Error Rate Training (MERT) (Och, 2003) is used
for tuning to optimize BLEU. MERT is replicated
three times to provide performance stability on test
set evaluation (Clark et al., 2011).
We used BLEU (Papineni et al., 2002) and
RIBES (Isozaki et al., 2010a) as evaluation mea-
sures of translation quality. RIBES is an eval-
uation method that focuses on word reordering
information, and is known to have high correla-
tion with human judgement for language pairs that
have very different word order such as English-
Japanese.
</bodyText>
<subsectionHeader confidence="0.982577">
5.2 Result
</subsectionHeader>
<bodyText confidence="0.999939730769231">
Table 3 shows translation quality for each com-
bination of HF-feature, Reordering, and Lexical
Processing. Scores in boldface indicate no sig-
nificant difference in comparison with the con-
dition that has highest translation quality using
the bootstrap resampling method (Koehn, 2004)
(p &lt; 0.05).
For PBMT, we can see that reordering plays an
extremely important role, with the highest BLEU
and RIBES scores being achieved when using Re-
ordering preprocessing (line 3, 4). Lexical Pro-
cessing also provided a slight performance gain
for PBMT. When we applied Lexical Processing to
PBMT, BLEU and RIBES scores were improved
(line 1 vs 2), although this gain was not significant
when Reordering was performed as well.
Overall T2S without any preprocessing
achieved better translation quality than all con-
ditions of PBMT (line 1 of T2S vs line 1-4 of
PBMT). In addition, BLEU and RIBES score of
T2S were clearly improved by Lexical Processing
(line 2, 4, 6, 8 vs line 1, 3, 5, 7), and these scores
are the highest of all conditions. On the other
hand, Reordering and HF-Feature addition had no
positive effect, and actually tended to slightly hurt
translation accuracy.
</bodyText>
<subsectionHeader confidence="0.999921">
5.3 Analysis of Preprocessing
</subsectionHeader>
<bodyText confidence="0.983846930232558">
With regards to PBMT, as previous works on
preordering have already indicated, BLEU and
RIBES scores were significantly improved by Re-
ordering. In addition, Lexical Processing also con-
Table 5: Optimized weight of HF-feature in each
condition
tributed to improve translation quality of PBMT
slightly. We also investigated the influence
that each element of Lexical Processing (pseudo-
particle insertion, determiner elimination, singu-
larization) had on translation quality, and found
that the gains were mainly provided by particle
insertion, with little effect from determiner elim-
ination or singularization.
Although Reordering was effective for PBMT,
it did not provide any benefit for T2S. This in-
dicates that T2S can already conduct long dis-
tance word reordering relatively correctly, and
word alignment quality was not improved as much
as expected by closing the gap in word order be-
tween the two languages. This was verified by a
subjective evaluation of the data, finding very few
major reordering issues in the sentences translated
by T2S.
On the other hand, Lexical Processing func-
tioned effectively for not only PBMT but also T2S.
When added to the baseline, lexical processing on
its own resulted in a gain of 0.57 BLEU, and 0.99
RIBES points, a significant improvement, with
similar gains being seen in other settings as well.
Table 4 demonstrates a typical example of the
improvement of the translation result due to Lex-
ical Processing. It can be seen that translation
performance of particles (indicated by underlined
words) was improved. The underlined particle is
in the direct object position of the verb that corre-
sponds to “comprises” in English, and thus should
be given the object particle “を wo” as in the refer-
ence and the system using Lexical Processing. On
the other hand, in the baseline system the genitive
“と to” is generated instead due to misaligned par-
ticles being inserted in an incorrect position in the
translation rules.
</bodyText>
<subsectionHeader confidence="0.999684">
5.4 Analysis of Feature Addition
</subsectionHeader>
<bodyText confidence="0.984755">
Our experimental results indicated that translation
quality is not improved by HF-feature addition
(line 1-4 vs line 5-8). We conjecture that the rea-
son why HF-feature did not contribute to an im-
</bodyText>
<figure confidence="0.673109307692308">
Word
HF-feature Reordering
Processing
+ - -
+ - +
+ + -
+ + +
Weight of
HF-feature
-0.00707078
0.00524676
0.156724
-0.121326
</figure>
<page confidence="0.996613">
39
</page>
<tableCaption confidence="0.890042333333333">
Table 3: Translation quality by combination of HF-feature, Reordering, and Lexical Processing. Bold
indicates results that are not statistically significantly different from the best result (39.60 BLEU in line
4 and 79.47 RIBES in line 2).
</tableCaption>
<figure confidence="0.8813152">
ID HF-feature Reordering Lexical Processing PBMT T2S
BLEU RIBES BLEU RIBES
1 - - - 32.11 69.06 38.94 78.48
2 - - + 33.16 70.19 39.51 79.47
3 - + - 37.62 77.56 38.44 78.48
4 - + + 37.77 77.71 39.60 79.26
5 + - - — — 38.74 78.33
6 + - + — — 39.29 79.23
7 + + - — — 38.48 78.44
8 + + + — — 39.38 79.21
</figure>
<tableCaption confidence="0.862474">
Table 4: Improvement of translation results due to Lexical Processing
</tableCaption>
<figure confidence="0.9175886">
Source another connector 96 , which is matable with this cable connector 90 , comprises a plurality of
male contacts 98 aligned in a row in an electrically insulative housing 97 as shown in the figure.
Reference この ケーブル コネクタ 90 と 嵌合 接続さ れ る 相手 コネクタ 96 は 、 図示 の よう に
、 絶縁 ハウジング 97 内 に雄 コンタクト 98 を 整列 保持 し て 構成 さ れ る 。
- Lexical Processing この ケーブル コネクタ 90 は 相手 コネクタ 96 は 、 図 に 示 す よう に 、 電気 絶縁
性 の ハウジング 97に 一 列 に 並 ぶ 複数 の 雄型 コンタクト 98 と から 構成 され
い る 。
+ Lexical Processing この ケーブル コネクタ 90 と 相手 コネクタ 96 は 、 図 に 示 す よう に 、 電気 絶縁
性の ハウジング 97に 一 列 に 並 ぶ 複数 の 雄型 コンタクト 98 を 有 し て 構成さ
る 。
</figure>
<bodyText confidence="0.9985469375">
provement in translation quality is that the reorder-
ing quality achieved by T2S translation was al-
ready sufficiently high, and the initial feature led
to confusion in MERT optimization.
Table 5 shows the optimized weight of the HF
feature in each condition. From this table, we can
see that in two of the conditions positive weights
are learned, and in two of the conditions negative
weights are learned. This indicates that there is no
consistent pattern of learning weights that corre-
spond to our intuition that head-final rules should
receive higher preference.
It is possible that other optimization methods,
or a more sophisticated way of inserting these fea-
tures into the translation rules could help alleviate
these problems.
</bodyText>
<sectionHeader confidence="0.999596" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999927518518518">
In this paper, we analyzed the effect of applying
syntactic preprocessing methods to syntax-based
SMT. Additionally, we have adapted reordering
rules as a decoder feature. The results showed
that lexical processing, specifically insertion of
pseudo-particles, contributed to improving trans-
lation quality, and it was effective as preprocessing
for T2S.
It should be noted that this paper, while demon-
strating that the simple rule-based syntactic pro-
cessing methods that have been useful for PBMT
can also contribute to T2S in English-Japanese
translation, more work is required to ensure that
this will generalize to other settings. A next step in
our inquiry is the generalization of these results to
other proposed preprocessing techniques and other
language pairs. In addition, we would like to try
two ways described below. First, it is likely that
other tree transformations, for example changing
the internal structure of the tree by moving chil-
dren to different nodes, would help in cases where
it is common to translate into highly divergent syn-
tactic structures between the source and target lan-
guages. Second, we plan to investigate other ways
of incorporating the preprocessing rules as a soft
constraints, such as using n-best lists or forests to
enode many possible sentence interpretations.
</bodyText>
<sectionHeader confidence="0.984595" genericHeader="references">
References
</sectionHeader>
<footnote confidence="0.670108">
Eleftherios Avramidis and Philipp Koehn. 2008. En-
riching morphologically poor languages for statisti-
cal machine translation. In Annual Meeting of the
</footnote>
<page confidence="0.995816">
40
</page>
<reference confidence="0.998632357798165">
Association for Computational Linguistics (ACL),
pages 763–770.
David Burkett and Dan Klein. 2012. Transforming
trees to improve syntactic convergence. In Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 863–872.
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: Controlling for optimizer
instability. In Annual Meeting of the Association for
Computational Linguistics (ACL), pages 176–181.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Annual Meeting of the Association
for Computational Linguistics (ACL), pages 531–
540.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-
ya, and Sayori Shimohata. 2008. Overview of the
patent translation task at the NTCIR-7 workshop. In
Proceedings of the 7th NTCIR Workshop Meeting,
pages 389–400.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, pages 391–427.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Workshop on Syntax and Structure in
Statistical Translation, pages 98–106.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 944–952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple
reordering rule for SOV languages. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 244–251.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In North
American Chapter of the Association for Computa-
tional Linguistics, pages 48–54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 177–180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 388–395.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Annual Meeting of the Association
for Computational Linguistics (ACL), pages 609–
616.
Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the second international conference on Hu-
man Language Technology Research, pages 292–
297.
Graham Neubig and Kevin Duh. 2014. On the ele-
ments of an accurate tree-to-string machine transla-
tion system. In Annual Meeting of the Association
for Computational Linguistics (ACL), pages 143–
149.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 529–533.
Graham Neubig. 2013. Travatar: A forest-to-string
machine translation engine based on tree transduc-
ers. Annual Meeting of the Association for Compu-
tational Linguistics (ACL), page 91.
Sonja Nießen and Hermann Ney. 2000. Improving
SMT quality with morpho-syntactic analysis. In
Proceedings of the 18th conference on Computa-
tional linguistics-Volume 2, pages 1081–1085.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, pages 19–51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311–318.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at sixteen: Update and out-
look. In IEEE Automatic Speech Recognition and
Understanding Workshop (ASRU), page 5.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada,
Masaaki Nagata, Xianchao Wu, Takuya Matsuzaki,
and Jun ’ichi Tsujii. 2011. NTT-UT statistical ma-
chine translation in NTCIR-9 PatentMT. In Pro-
ceedings of NTCIR, pages 585–592.
Akihiro Tamura, Taro Watanabe, Eiichiro Sumita, Hi-
roya Takamura, and Manabu Okumura. 2013. Part-
of-speech induction in dependency trees for statisti-
cal machine translation. In Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 841–851.
</reference>
<page confidence="0.987675">
41
</page>
<reference confidence="0.999448777777778">
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In International Conference on Computa-
tional Linguistics (COLING), page 508.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In North
American Chapter of the Association for Computa-
tional Linguistics, pages 245–253.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 523–530.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In North American Chapter of the
Association for Computational Linguistics, pages
256–263.
</reference>
<page confidence="0.999297">
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.920844">
<title confidence="0.9984255">Rule-based Syntactic for Syntax-based Machine Translation</title>
<author confidence="0.990828">Yuto Hatakoshi</author>
<author confidence="0.990828">Graham Neubig</author>
<author confidence="0.990828">Sakriani Sakti</author>
<author confidence="0.990828">Tomoki Toda</author>
<author confidence="0.990828">Satoshi Nakamura</author>
<affiliation confidence="0.9984845">Nara Institute of Science and Technology Graduate School of Information Science</affiliation>
<address confidence="0.996687">Takayama, Ikoma, Nara 630-0192, Japan</address>
<abstract confidence="0.996681736842105">Several preprocessing techniques using syntactic information and linguistically motivated rules have been proposed to improve the quality of phrase-based machine translation (PBMT) output. On the other hand, there has been little work on similar techniques in the context of other translation formalisms such as syntax-based SMT. In this paper, we examine whether the sort of rule-based syntactic preprocessing approaches that have proved beneficial for PBMT can contribute to syntax-based SMT. Specifically, we tailor a highly successful preprocessing method for English- Japanese PBMT to syntax-based SMT, and find that while the gains achievable are smaller than those for PBMT, significant improvements in accuracy can be realized.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Association for Computational Linguistics (ACL),</title>
<pages>763--770</pages>
<marker></marker>
<rawString>Association for Computational Linguistics (ACL), pages 763–770.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Transforming trees to improve syntactic convergence.</title>
<date>2012</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>863--872</pages>
<contexts>
<context position="4341" citStr="Burkett and Klein, 2012" startWordPosition="635" endWordPosition="638">Liu et al., 2006), which translates using syntactic information. However, few attempts have been made at syntactic preprocessing for syntax-based SMT, as the syntactic information given by the parser is already incorporated directly in the translation model. Notable excep34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), methods for binarizing parse trees to match word alignments (Zhang et al., 2006), and methods for adjusting label sets to be more appropriate for syntax-based SMT (Hanneman and Lavie, 2011; Tamura et al., 2013). It should be noted that these methods of syntactic preprocessing for syntax-based SMT are all based on automatically learned rules, and there has been little investigation of the manually-created linguisticallymotivated rules that have proved useful in preprocessing for PBMT. In this paper, we examine whether rule-based syntactic preprocessing methods designed for PBMT can contribute</context>
</contexts>
<marker>Burkett, Klein, 2012</marker>
<rawString>David Burkett and Dan Klein. 2012. Transforming trees to improve syntactic convergence. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 863–872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>176--181</pages>
<contexts>
<context position="19814" citStr="Clark et al., 2011" startWordPosition="3161" endWordPosition="3164">lation pattern VP NP VBD Reordered translation pattern x0 wo utta ������������������� 2. Add HF-feature if ward alignment is non-crossing Target side of 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapanese. 5.2 Result Table 3 shows translation quality for each combination of HF-feature, Reordering, and Lexical Processing. Scores in boldface indicate no significant difference in comparison with the condition that has highest translation quality using the bootstrap resam</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 176–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>531--540</pages>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 531– 540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
</authors>
<title>Masao Utiyama, Mikio Yamamoto, Takehito Utsuro, Terumasa Ehara, Hiroshi Echizenya, and Sayori Shimohata.</title>
<date>2008</date>
<booktitle>In Proceedings of the 7th NTCIR Workshop Meeting,</booktitle>
<pages>389--400</pages>
<marker>Fujii, 2008</marker>
<rawString>Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Takehito Utsuro, Terumasa Ehara, Hiroshi Echizenya, and Sayori Shimohata. 2008. Overview of the patent translation task at the NTCIR-7 workshop. In Proceedings of the 7th NTCIR Workshop Meeting, pages 389–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Jonathan May</author>
</authors>
<title>Training tree transducers. Computational Linguistics,</title>
<date>2008</date>
<pages>391--427</pages>
<contexts>
<context position="10198" citStr="Graehl et al., 2008" startWordPosition="1583" endWordPosition="1586">mbles the structure of the Japanese translation “jon wa bo-ru wo utta.” 3 Syntax-based Statistical Machine Translation Syntax-based SMT is a method for statistical translation using syntactic information of the sentence (Yamada and Knight, 2001; Liu et al., 2006). By using translation patterns following the structure of linguistic syntax trees, syntax-based translations often makes it possible to achieve more grammatical translations and reorderings compared with PBMT. In this section, we describe tree-to-string (T2S) machine translation based on synchronous tree substitution grammars (STSG) (Graehl et al., 2008), the variety of syntax-based SMT that we use in our experiments. T2S captures the syntactic relationship between two languages by using the syntactic structure of parsing results of the source sentence. Each translation pattern is expressed as a source sentence subtree using rules including variables. The following example of a translation pattern include two noun phrases NP0 and NP1, which are translated and inserted into the target placeholders X0 and X1 respectively. The decoder generates the translated sentence in consideration of the probability of translation pattern itself and translat</context>
</contexts>
<marker>Graehl, Knight, May, 2008</marker>
<rawString>Jonathan Graehl, Kevin Knight, and Jonathan May. 2008. Training tree transducers. Computational Linguistics, pages 391–427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Hanneman</author>
<author>Alon Lavie</author>
</authors>
<title>Automatic category label coarsening for syntax-based machine translation.</title>
<date>2011</date>
<booktitle>In Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>98--106</pages>
<contexts>
<context position="4531" citStr="Hanneman and Lavie, 2011" startWordPosition="666" endWordPosition="669">he parser is already incorporated directly in the translation model. Notable excep34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), methods for binarizing parse trees to match word alignments (Zhang et al., 2006), and methods for adjusting label sets to be more appropriate for syntax-based SMT (Hanneman and Lavie, 2011; Tamura et al., 2013). It should be noted that these methods of syntactic preprocessing for syntax-based SMT are all based on automatically learned rules, and there has been little investigation of the manually-created linguisticallymotivated rules that have proved useful in preprocessing for PBMT. In this paper, we examine whether rule-based syntactic preprocessing methods designed for PBMT can contribute anything to syntax-based machine translation. Specifically, we examine whether the reordering and lexical processing of Head Finalization contributes to the improvement of syntax-based mach</context>
</contexts>
<marker>Hanneman, Lavie, 2011</marker>
<rawString>Greg Hanneman and Alon Lavie. 2011. Automatic category label coarsening for syntax-based machine translation. In Workshop on Syntax and Structure in Statistical Translation, pages 98–106.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hideki Isozaki</author>
<author>Tsutomu Hirao</author>
<author>Kevin Duh</author>
</authors>
<title>Katsuhito Sudoh, and Hajime Tsukada. 2010a. Automatic evaluation of translation quality for distant language pairs.</title>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>944--952</pages>
<marker>Isozaki, Hirao, Duh, </marker>
<rawString>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010a. Automatic evaluation of translation quality for distant language pairs. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 944–952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
<author>Kevin Duh</author>
</authors>
<title>Head finalization: A simple reordering rule for SOV languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>244--251</pages>
<contexts>
<context position="2815" citStr="Isozaki et al., 2010" startWordPosition="409" endWordPosition="412">PBMT has well-known problems with language pairs that have very different word order, due to the fact that the reordering model has difficulty estimating the probability of long distance reorderings. Therefore, preordering methods attempt to improve the translation quality of PBMT by rearranging source language sentences into an order closer to that of the target language. It’s often the case that preordering methods are based on rule-based approaches, and these methods have achieved great success in ameliorating the word ordering problems faced by PBMT (Collins et al., 2005; Xu et al., 2009; Isozaki et al., 2010b). One particularly successful example of rulebased syntactic preprocessing is Head Finalization (Isozaki et al., 2010b), a method of syntactic preprocessing for English to Japanese translation that has significantly improved translation quality of English-Japanese PBMT using simple rules based on the syntactic structure of the two languages. The most central part of the method, as indicated by its name, is a reordering rule that moves the English head word to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sentences. Head Finalizati</context>
<context position="5722" citStr="Isozaki et al. (2010" startWordPosition="841" endWordPosition="844">rovement of syntax-based machine translation as it did for PBMT. Additionally, we examine whether it is possible to incorporate the intuitions behind the Head Finalization reordering rules as soft constraints by incorporating them as a decoder feature. As a result of our experiments, we demonstrate that rule-based lexical processing can contribute to improvement of translation quality of syntax-based machine translation. 2 Head Finalization Head Finalization is a syntactic preprocessing method for English to Japanese PBMT, reducing grammatical errors through reordering and lexical processing. Isozaki et al. (2010b) have reported that translation quality of English-Japanese PBMT is significantly improved using a translation model learned by English sentences preprocessed by Head Finalization and Japanese sentences. In fact, this method achieved the highest results in the large scale NTCIR 2011 evaluation (Sudoh et al., 2011), the first time a statistical machine translation (SMT) surpassed rule-based systems for this very difficult language pair, demonstrating the utility of these simple syntactic transformations from the point of view of PBMT. 2.1 Reordering The reordering process of Head Finalization</context>
<context position="19884" citStr="Isozaki et al., 2010" startWordPosition="3174" endWordPosition="3177">���������������� 2. Add HF-feature if ward alignment is non-crossing Target side of 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapanese. 5.2 Result Table 3 shows translation quality for each combination of HF-feature, Reordering, and Lexical Processing. Scores in boldface indicate no significant difference in comparison with the condition that has highest translation quality using the bootstrap resampling method (Koehn, 2004) (p &lt; 0.05). For PBMT, we can see that reord</context>
</contexts>
<marker>Isozaki, Sudoh, Tsukada, Duh, 2010</marker>
<rawString>Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and Kevin Duh. 2010b. Head finalization: A simple reordering rule for SOV languages. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 244–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="1185" citStr="Koehn et al., 2003" startWordPosition="155" endWordPosition="158">ttle work on similar techniques in the context of other translation formalisms such as syntax-based SMT. In this paper, we examine whether the sort of rule-based syntactic preprocessing approaches that have proved beneficial for PBMT can contribute to syntax-based SMT. Specifically, we tailor a highly successful preprocessing method for EnglishJapanese PBMT to syntax-based SMT, and find that while the gains achievable are smaller than those for PBMT, significant improvements in accuracy can be realized. 1 Introduction In the widely-studied framework of phrase-based machine translation (PBMT) (Koehn et al., 2003), translation probabilities between phrases consisting of multiple words are calculated, and translated phrases are rearranged by the reordering model in the appropriate target language order. While PBMT provides a light-weight framework to learn translation models and achieves high translation quality in many language pairs, it does not directly incorporate morphological or syntactic information. Thus, many preprocessing methods for PBMT using these types of information have been proposed. Methods include preprocessing to obtain accurate word alignments by the division of the prefix of verbs </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In North American Chapter of the Association for Computational Linguistics, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>177--180</pages>
<contexts>
<context position="18957" citStr="Koehn et al., 2007" startWordPosition="3022" endWordPosition="3025">.38k 37.99 5 Experiment In our experiment, we examined how much each of the preprocessing steps (Reordering, Lexical Processing) contribute to improve the translation quality of PBMT and T2S. We also examined the improvement in translation quality of T2S by the introduction of the Head Finalization feature. 5.1 Experimental Environment For our English to Japanese translation experiments, we used NTCIR7 PATENT-MT’s Patent corpus (Fujii et al., 2008). Table 2 shows the details of training data (train), development data (dev), and test data (test). As the PBMT and T2S engines, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. utta x0 wo x0:NP hit Source side of translation pattern Target side of translation pattern VP VBD NP hit x0:NP Word alignment 1. Apply Reordering to source translation pattern VP NP VBD Reordered translation pattern x0 wo utta ������������������� 2. Add HF-feature if ward alignment is non-crossing Target side of 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>388--395</pages>
<contexts>
<context position="20440" citStr="Koehn, 2004" startWordPosition="3262" endWordPosition="3263"> (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapanese. 5.2 Result Table 3 shows translation quality for each combination of HF-feature, Reordering, and Lexical Processing. Scores in boldface indicate no significant difference in comparison with the condition that has highest translation quality using the bootstrap resampling method (Koehn, 2004) (p &lt; 0.05). For PBMT, we can see that reordering plays an extremely important role, with the highest BLEU and RIBES scores being achieved when using Reordering preprocessing (line 3, 4). Lexical Processing also provided a slight performance gain for PBMT. When we applied Lexical Processing to PBMT, BLEU and RIBES scores were improved (line 1 vs 2), although this gain was not significant when Reordering was performed as well. Overall T2S without any preprocessing achieved better translation quality than all conditions of PBMT (line 1 of T2S vs line 1-4 of PBMT). In addition, BLEU and RIBES sco</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>609--616</pages>
<contexts>
<context position="3734" citStr="Liu et al., 2006" startWordPosition="550" endWordPosition="553">e syntactic structure of the two languages. The most central part of the method, as indicated by its name, is a reordering rule that moves the English head word to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sentences. Head Finalization also contains some additional preprocessing steps such as determiner elimination, particle insertion and singularization to generate a sentence that is closer to Japanese grammatical structure. In addition to PBMT, there has also recently been interest in syntax-based SMT (Yamada and Knight, 2001; Liu et al., 2006), which translates using syntactic information. However, few attempts have been made at syntactic preprocessing for syntax-based SMT, as the syntactic information given by the parser is already incorporated directly in the translation model. Notable excep34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein</context>
<context position="9841" citStr="Liu et al., 2006" startWordPosition="1533" endWordPosition="1536">uns. Thus, to generate a sentence that is closer to Japanese, Head Finalization eliminates determiners (2) and singularizes plural nouns (3) in addition to the pseudo-particle insertion. In Figure 1, we can see that applying these three processes to the source English sentence results in the sentence “John va0 (wa) ball va2 (wo) hit” which closely resembles the structure of the Japanese translation “jon wa bo-ru wo utta.” 3 Syntax-based Statistical Machine Translation Syntax-based SMT is a method for statistical translation using syntactic information of the sentence (Yamada and Knight, 2001; Liu et al., 2006). By using translation patterns following the structure of linguistic syntax trees, syntax-based translations often makes it possible to achieve more grammatical translations and reorderings compared with PBMT. In this section, we describe tree-to-string (T2S) machine translation based on synchronous tree substitution grammars (STSG) (Graehl et al., 2008), the variety of syntax-based SMT that we use in our experiments. T2S captures the syntactic relationship between two languages by using the syntactic structure of parsing results of the source sentence. Each translation pattern is expressed a</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 609– 616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research,</booktitle>
<pages>292--297</pages>
<contexts>
<context position="19380" citStr="Miyao and Tsujii, 2002" startWordPosition="3088" endWordPosition="3091">atent corpus (Fujii et al., 2008). Table 2 shows the details of training data (train), development data (dev), and test data (test). As the PBMT and T2S engines, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. utta x0 wo x0:NP hit Source side of translation pattern Target side of translation pattern VP VBD NP hit x0:NP Word alignment 1. Apply Reordering to source translation pattern VP NP VBD Reordered translation pattern x0 wo utta ������������������� 2. Add HF-feature if ward alignment is non-crossing Target side of 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on </context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum entropy estimation for feature forests. In Proceedings of the second international conference on Human Language Technology Research, pages 292– 297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Kevin Duh</author>
</authors>
<title>On the elements of an accurate tree-to-string machine translation system.</title>
<date>2014</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>143--149</pages>
<contexts>
<context position="13431" citStr="Neubig and Duh, 2014" startWordPosition="2103" endWordPosition="2106">be automatically learned in a well-trained T2S model. Therefore, preordering is not expected to cause large gains, unlike in the case of PBMT. However, it can also be thought that preordering can still have a positive influence on the translation model training process, particularly by increasing alignment accuracy. For example, training methods for word alignment such as the IBM or HMM models (Och and Ney, 2003) are affected by word order, and word alignment may be improved by moving word order closer between the two languages. As alignment accuracy plays a important role in T2S translation (Neubig and Duh, 2014), it is reasonable to hypothesize that reordering may also have a positive effect on T2S. In terms of the actual incorporation with the T2S system, we simply follow the process in Figure 1, but output the reordered tree instead of only the reordered terminal nodes as is done for PBMT. Original English Figure 2: A method of applying Lexical Processing 4.1.2 Lexical Processing for T2S In comparison to reordering, Lexical Processing may be expected to have a larger effect on T2S, as it will both have the potential to increase alignment accuracy, and remove the burden of learning rules to perform </context>
</contexts>
<marker>Neubig, Duh, 2014</marker>
<rawString>Graham Neubig and Kevin Duh. 2014. On the elements of an accurate tree-to-string machine translation system. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 143– 149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Yosuke Nakata</author>
<author>Shinsuke Mori</author>
</authors>
<title>Pointwise prediction for robust, adaptable japanese morphological analysis.</title>
<date>2011</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>529--533</pages>
<contexts>
<context position="19447" citStr="Neubig et al., 2011" startWordPosition="3101" endWordPosition="3104">ng data (train), development data (dev), and test data (test). As the PBMT and T2S engines, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. utta x0 wo x0:NP hit Source side of translation pattern Target side of translation pattern VP VBD NP hit x0:NP Word alignment 1. Apply Reordering to source translation pattern VP NP VBD Reordered translation pattern x0 wo utta ������������������� 2. Add HF-feature if ward alignment is non-crossing Target side of 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation </context>
</contexts>
<marker>Neubig, Nakata, Mori, 2011</marker>
<rawString>Graham Neubig, Yosuke Nakata, and Shinsuke Mori. 2011. Pointwise prediction for robust, adaptable japanese morphological analysis. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 529–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
</authors>
<title>Travatar: A forest-to-string machine translation engine based on tree transducers.</title>
<date>2013</date>
<booktitle>Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>91</pages>
<contexts>
<context position="18985" citStr="Neubig, 2013" startWordPosition="3028" endWordPosition="3029">eriment, we examined how much each of the preprocessing steps (Reordering, Lexical Processing) contribute to improve the translation quality of PBMT and T2S. We also examined the improvement in translation quality of T2S by the introduction of the Head Finalization feature. 5.1 Experimental Environment For our English to Japanese translation experiments, we used NTCIR7 PATENT-MT’s Patent corpus (Fujii et al., 2008). Table 2 shows the details of training data (train), development data (dev), and test data (test). As the PBMT and T2S engines, we used the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. utta x0 wo x0:NP hit Source side of translation pattern Target side of translation pattern VP VBD NP hit x0:NP Word alignment 1. Apply Reordering to source translation pattern VP NP VBD Reordered translation pattern x0 wo utta ������������������� 2. Add HF-feature if ward alignment is non-crossing Target side of 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram L</context>
</contexts>
<marker>Neubig, 2013</marker>
<rawString>Graham Neubig. 2013. Travatar: A forest-to-string machine translation engine based on tree transducers. Annual Meeting of the Association for Computational Linguistics (ACL), page 91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Hermann Ney</author>
</authors>
<title>Improving SMT quality with morpho-syntactic analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics-Volume 2,</booktitle>
<pages>1081--1085</pages>
<contexts>
<context position="1807" citStr="Nießen and Ney, 2000" startWordPosition="248" endWordPosition="251"> translation probabilities between phrases consisting of multiple words are calculated, and translated phrases are rearranged by the reordering model in the appropriate target language order. While PBMT provides a light-weight framework to learn translation models and achieves high translation quality in many language pairs, it does not directly incorporate morphological or syntactic information. Thus, many preprocessing methods for PBMT using these types of information have been proposed. Methods include preprocessing to obtain accurate word alignments by the division of the prefix of verbs (Nießen and Ney, 2000), preprocessing to reduce the errors in verb conjugation and noun case agreement (Avramidis and Koehn, 2008), and many others. The effectiveness of the syntactic preprocessing for PBMT has been supported by these and various related works. In particular, much attention has been paid to preordering (Xia and McCord, 2004; Collins et al., 2005), a class of preprocessing methods for PBMT. PBMT has well-known problems with language pairs that have very different word order, due to the fact that the reordering model has difficulty estimating the probability of long distance reorderings. Therefore, p</context>
</contexts>
<marker>Nießen, Ney, 2000</marker>
<rawString>Sonja Nießen and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In Proceedings of the 18th conference on Computational linguistics-Volume 2, pages 1081–1085.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models. Computational linguistics,</title>
<date>2003</date>
<pages>pages</pages>
<contexts>
<context position="13226" citStr="Och and Ney, 2003" startWordPosition="2068" endWordPosition="2071">, because translation patterns of T2S are expressed by using source sentence subtrees, the effect of reordering problems are relatively small, and the majority of reordering rules specified by hand can be automatically learned in a well-trained T2S model. Therefore, preordering is not expected to cause large gains, unlike in the case of PBMT. However, it can also be thought that preordering can still have a positive influence on the translation model training process, particularly by increasing alignment accuracy. For example, training methods for word alignment such as the IBM or HMM models (Och and Ney, 2003) are affected by word order, and word alignment may be improved by moving word order closer between the two languages. As alignment accuracy plays a important role in T2S translation (Neubig and Duh, 2014), it is reasonable to hypothesize that reordering may also have a positive effect on T2S. In terms of the actual incorporation with the T2S system, we simply follow the process in Figure 1, but output the reordered tree instead of only the reordered terminal nodes as is done for PBMT. Original English Figure 2: A method of applying Lexical Processing 4.1.2 Lexical Processing for T2S In compar</context>
<context position="19542" citStr="Och and Ney, 2003" startWordPosition="3117" endWordPosition="3120">ed the Moses (Koehn et al., 2007) and Travatar (Neubig, 2013) translation toolkits with the default settings. utta x0 wo x0:NP hit Source side of translation pattern Target side of translation pattern VP VBD NP hit x0:NP Word alignment 1. Apply Reordering to source translation pattern VP NP VBD Reordered translation pattern x0 wo utta ������������������� 2. Add HF-feature if ward alignment is non-crossing Target side of 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapa</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, pages 19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<contexts>
<context position="16383" citStr="Och, 2003" startWordPosition="2597" endWordPosition="2598">P lexical Processing NN VA VBD NN VA NP S VP NP 37 “no,” or situations where non-literal translations are necessary) and a hard constraint to obey headfinal word order could be detrimental. In order to incorporate this intuition, we add a feature (HF-feature) to translation patterns that conform to the reordering rules of Head Finalization. This gives the decoder ability to discern translation patterns that follow the canonical reordering patterns in English-Japanese translation, and has the potential to improve translation quality in the T2S translation model. We use the log-linear approach (Och, 2003) to add the Head Finalization feature (HF-feature). As in the standard log-linear model, a source sentence f is translated into a target language sentence e, by searching for the sentence maximizing the score: e� = arg max w� - h(f, e). (1) e where h(f, e) is a feature function vector. w is a weight vector that scales the contribution from each feature. Each feature can take any real value which is useful to improve translation quality, such as the log of the n-gram language model probability to represent fluency, or lexical/phrase translation probability to capture the word or phrasewise corr</context>
<context position="19669" citStr="Och, 2003" startWordPosition="3139" endWordPosition="3140">ce side of translation pattern Target side of translation pattern VP VBD NP hit x0:NP Word alignment 1. Apply Reordering to source translation pattern VP NP VBD Reordered translation pattern x0 wo utta ������������������� 2. Add HF-feature if ward alignment is non-crossing Target side of 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapanese. 5.2 Result Table 3 shows translation quality for each combination of HF-feature, Reordering, and Lexical Processing. Scor</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<contexts>
<context position="19852" citStr="Papineni et al., 2002" startWordPosition="3168" endWordPosition="3171">translation pattern x0 wo utta ������������������� 2. Add HF-feature if ward alignment is non-crossing Target side of 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapanese. 5.2 Result Table 3 shows translation quality for each combination of HF-feature, Reordering, and Lexical Processing. Scores in boldface indicate no significant difference in comparison with the condition that has highest translation quality using the bootstrap resampling method (Koehn, 2004) (p &lt; 0.05).</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Jing Zheng</author>
<author>Wen Wang</author>
<author>Victor Abrash</author>
</authors>
<title>SRILM at sixteen: Update and outlook.</title>
<date>2011</date>
<booktitle>In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),</booktitle>
<pages>5</pages>
<contexts>
<context position="19621" citStr="Stolcke et al., 2011" startWordPosition="3130" endWordPosition="3133">olkits with the default settings. utta x0 wo x0:NP hit Source side of translation pattern Target side of translation pattern VP VBD NP hit x0:NP Word alignment 1. Apply Reordering to source translation pattern VP NP VBD Reordered translation pattern x0 wo utta ������������������� 2. Add HF-feature if ward alignment is non-crossing Target side of 38 Enju (Miyao and Tsujii, 2002) is used to parse English sentences and KyTea (Neubig et al., 2011) is used as a Japanese tokenizer. We generated word alignments using GIZA++ (Och and Ney, 2003) and trained a Kneser-Ney smoothed 5-gram LM using SRILM (Stolcke et al., 2011). Minimum Error Rate Training (MERT) (Och, 2003) is used for tuning to optimize BLEU. MERT is replicated three times to provide performance stability on test set evaluation (Clark et al., 2011). We used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) as evaluation measures of translation quality. RIBES is an evaluation method that focuses on word reordering information, and is known to have high correlation with human judgement for language pairs that have very different word order such as EnglishJapanese. 5.2 Result Table 3 shows translation quality for each combination of HF-f</context>
</contexts>
<marker>Stolcke, Zheng, Wang, Abrash, 2011</marker>
<rawString>Andreas Stolcke, Jing Zheng, Wen Wang, and Victor Abrash. 2011. SRILM at sixteen: Update and outlook. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), page 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsuhito Sudoh</author>
<author>Kevin Duh</author>
</authors>
<title>Hajime Tsukada, Masaaki Nagata, Xianchao Wu, Takuya Matsuzaki, and Jun ’ichi Tsujii.</title>
<date>2011</date>
<booktitle>In Proceedings of NTCIR,</booktitle>
<pages>585--592</pages>
<marker>Sudoh, Duh, 2011</marker>
<rawString>Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Masaaki Nagata, Xianchao Wu, Takuya Matsuzaki, and Jun ’ichi Tsujii. 2011. NTT-UT statistical machine translation in NTCIR-9 PatentMT. In Proceedings of NTCIR, pages 585–592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akihiro Tamura</author>
</authors>
<title>Taro Watanabe, Eiichiro Sumita, Hiroya Takamura, and Manabu Okumura.</title>
<date>2013</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>841--851</pages>
<marker>Tamura, 2013</marker>
<rawString>Akihiro Tamura, Taro Watanabe, Eiichiro Sumita, Hiroya Takamura, and Manabu Okumura. 2013. Partof-speech induction in dependency trees for statistical machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 841–851.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical mt system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In International Conference on Computational Linguistics (COLING),</booktitle>
<pages>508</pages>
<contexts>
<context position="2127" citStr="Xia and McCord, 2004" startWordPosition="298" endWordPosition="301">airs, it does not directly incorporate morphological or syntactic information. Thus, many preprocessing methods for PBMT using these types of information have been proposed. Methods include preprocessing to obtain accurate word alignments by the division of the prefix of verbs (Nießen and Ney, 2000), preprocessing to reduce the errors in verb conjugation and noun case agreement (Avramidis and Koehn, 2008), and many others. The effectiveness of the syntactic preprocessing for PBMT has been supported by these and various related works. In particular, much attention has been paid to preordering (Xia and McCord, 2004; Collins et al., 2005), a class of preprocessing methods for PBMT. PBMT has well-known problems with language pairs that have very different word order, due to the fact that the reordering model has difficulty estimating the probability of long distance reorderings. Therefore, preordering methods attempt to improve the translation quality of PBMT by rearranging source language sentences into an order closer to that of the target language. It’s often the case that preordering methods are based on rule-based approaches, and these methods have achieved great success in ameliorating the word orde</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical mt system with automatically learned rewrite patterns. In International Conference on Computational Linguistics (COLING), page 508.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Jaeho Kang</author>
<author>Michael Ringgaard</author>
<author>Franz Och</author>
</authors>
<title>Using a dependency parser to improve smt for subject-object-verb languages.</title>
<date>2009</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>245--253</pages>
<contexts>
<context position="2793" citStr="Xu et al., 2009" startWordPosition="405" endWordPosition="408">ethods for PBMT. PBMT has well-known problems with language pairs that have very different word order, due to the fact that the reordering model has difficulty estimating the probability of long distance reorderings. Therefore, preordering methods attempt to improve the translation quality of PBMT by rearranging source language sentences into an order closer to that of the target language. It’s often the case that preordering methods are based on rule-based approaches, and these methods have achieved great success in ameliorating the word ordering problems faced by PBMT (Collins et al., 2005; Xu et al., 2009; Isozaki et al., 2010b). One particularly successful example of rulebased syntactic preprocessing is Head Finalization (Isozaki et al., 2010b), a method of syntactic preprocessing for English to Japanese translation that has significantly improved translation quality of English-Japanese PBMT using simple rules based on the syntactic structure of the two languages. The most central part of the method, as indicated by its name, is a reordering rule that moves the English head word to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sent</context>
</contexts>
<marker>Xu, Kang, Ringgaard, Och, 2009</marker>
<rawString>Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a dependency parser to improve smt for subject-object-verb languages. In North American Chapter of the Association for Computational Linguistics, pages 245–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>523--530</pages>
<contexts>
<context position="3715" citStr="Yamada and Knight, 2001" startWordPosition="546" endWordPosition="549"> simple rules based on the syntactic structure of the two languages. The most central part of the method, as indicated by its name, is a reordering rule that moves the English head word to the end of the corresponding syntactic constituents to match the head-final syntactic structure of Japanese sentences. Head Finalization also contains some additional preprocessing steps such as determiner elimination, particle insertion and singularization to generate a sentence that is closer to Japanese grammatical structure. In addition to PBMT, there has also recently been interest in syntax-based SMT (Yamada and Knight, 2001; Liu et al., 2006), which translates using syntactic information. However, few attempts have been made at syntactic preprocessing for syntax-based SMT, as the syntactic information given by the parser is already incorporated directly in the translation model. Notable excep34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment</context>
<context position="9822" citStr="Yamada and Knight, 2001" startWordPosition="1529" endWordPosition="1532">lural by inflection of nouns. Thus, to generate a sentence that is closer to Japanese, Head Finalization eliminates determiners (2) and singularizes plural nouns (3) in addition to the pseudo-particle insertion. In Figure 1, we can see that applying these three processes to the source English sentence results in the sentence “John va0 (wa) ball va2 (wo) hit” which closely resembles the structure of the Japanese translation “jon wa bo-ru wo utta.” 3 Syntax-based Statistical Machine Translation Syntax-based SMT is a method for statistical translation using syntactic information of the sentence (Yamada and Knight, 2001; Liu et al., 2006). By using translation patterns following the structure of linguistic syntax trees, syntax-based translations often makes it possible to achieve more grammatical translations and reorderings compared with PBMT. In this section, we describe tree-to-string (T2S) machine translation based on synchronous tree substitution grammars (STSG) (Graehl et al., 2008), the variety of syntax-based SMT that we use in our experiments. T2S captures the syntactic relationship between two languages by using the syntactic structure of parsing results of the source sentence. Each translation pat</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>256--263</pages>
<contexts>
<context position="4423" citStr="Zhang et al., 2006" startWordPosition="648" endWordPosition="651">have been made at syntactic preprocessing for syntax-based SMT, as the syntactic information given by the parser is already incorporated directly in the translation model. Notable excep34 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34–42, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics tions include methods to perform tree transformations improving correspondence between the sentence structure and word alignment (Burkett and Klein, 2012), methods for binarizing parse trees to match word alignments (Zhang et al., 2006), and methods for adjusting label sets to be more appropriate for syntax-based SMT (Hanneman and Lavie, 2011; Tamura et al., 2013). It should be noted that these methods of syntactic preprocessing for syntax-based SMT are all based on automatically learned rules, and there has been little investigation of the manually-created linguisticallymotivated rules that have proved useful in preprocessing for PBMT. In this paper, we examine whether rule-based syntactic preprocessing methods designed for PBMT can contribute anything to syntax-based machine translation. Specifically, we examine whether th</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In North American Chapter of the Association for Computational Linguistics, pages 256–263.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
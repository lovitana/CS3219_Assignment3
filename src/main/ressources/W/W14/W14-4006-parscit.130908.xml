<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004505">
<title confidence="0.950864">
Reducing the Impact of Data Sparsity in Statistical Machine Translation
</title>
<author confidence="0.819964">
Karan Singla1, Kunal Sachdeva1, Diksha Yadav1, Srinivas Bangalore2, Dipti Misra Sharma1
</author>
<affiliation confidence="0.257838">
1LTRC IIIT Hyderabad,2AT&amp;T Labs-Research
</affiliation>
<sectionHeader confidence="0.966613" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999530933333333">
Morphologically rich languages generally
require large amounts of parallel data to
adequately estimate parameters in a statis-
tical Machine Translation(SMT) system.
However, it is time consuming and expen-
sive to create large collections of parallel
data. In this paper, we explore two strate-
gies for circumventing sparsity caused by
lack of large parallel corpora. First, we ex-
plore the use of distributed representations
in an Recurrent Neural Network based lan-
guage model with different morphological
features and second, we explore the use of
lexical resources such as WordNet to over-
come sparsity of content words.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999879966666667">
Statistical machine translation (SMT) models es-
timate parameters (lexical models, and distortion
model) from parallel corpora. The reliability of
these parameter estimates is dependent on the size
of the corpora. In morphologically rich languages,
this sparsity is compounded further due to lack of
large parallel corpora.
In this paper, we present two approaches that
address the issue of sparsity in SMT models for
morphologically rich languages. First, we use an
Recurrent Neural Network (RNN) based language
model (LM) to re-rank the output of a phrase-
based SMT (PB-SMT) system and second we use
lexical resources such as WordNet to minimize the
impact of Out-of-Vocabulary(OOV) words on MT
quality. We further improve the accuracy of MT
using a model combination approach.
The rest of the paper is organized as follows.
We first present our approach of training the base-
line model and source side reordering. In Section
4, we present our experiments and results on re-
ranking the MT output using RNNLM. In Section
5, we discuss our approach to increase the cover-
age of the model by using synset ID’s from the
English WordNet (EWN). Section 6 describes our
experiments on combining the model with synset
ID’s and baseline model to further improve the
translation accuracy followed by results and obser-
vations sections.We conclude the paper with future
work and conclusions.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999953129032258">
In this paper, we present our efforts of re-
ranking the n-best hypotheses produced by a PB-
MT (Phrase-Based MT) system using RNNLM
(Mikolov et al., 2010) in the context of an English-
Hindi SMT system. The re-ranking task in ma-
chine translation can be defined as re-scoring the
n-best list of translations, wherein a number of
language models are deployed along with fea-
tures of source or target language. (Dungarwal
et al., 2014) described the benefits of re-ranking
the translation hypothesis using simple n-gram
based language model. In recent years, the use
of RNNLM have shown significant improvements
over the traditional n-gram models (Sundermeyer
et al., 2013). (Mikolov et al., 2010) and (Liu et
al., 2014) have shown significant improvements in
speech recognition accuracy using RNNLM . Shi
(2012) also showed the benefits of using RNNLM
with contextual and linguistic features. We have
also explored the use of morphological features
(Hindi being a morphologically rich language) in
RNNLM and deduced that these features further
improve the baseline RNNLM in re-ranking the n-
best hypothesis.
Words in natural languages are richly diverse
so it is not possible to cover all source language
words when training an MT system. Untranslated
out-of-vocabulary (OOV) words tend to degrade
the accuracy of the output produced by an MT
model. Huang (2010) pointed to various types
of OOV words which occur in a data set – seg-
</bodyText>
<page confidence="0.979873">
51
</page>
<subsubsectionHeader confidence="0.276553">
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 51–56,
</subsubsectionHeader>
<bodyText confidence="0.98114276">
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
mentation error in source language, named enti-
ties, combination forms (e.g. widebody) and ab-
breviations. Apart from these issues, Hindi being
a low-resourced language in terms of parallel cor-
pora suffers from data sparsity.
In the second part of the paper, we address the
problem of data sparsity with the help of English
WordNet (EWN) for English-Hindi PB-SMT. We
increase the coverage of content words (exclud-
ing Named-Entities) by incorporating sysnset in-
formation in the source sentences.
Combining Machine Translation (MT) systems
has become an important part of statistical MT in
past few years. Works by (Razmara and Sarkar,
2013; Cohn and Lapata, 2007) have shown that
there is an increase in phrase coverage when com-
bining different systems. To get more coverage of
unigrams in phrase-table, we have explored sys-
tem combination approaches to combine models
trained with synset information and without synset
information. We have explored two methodolo-
gies for system combination based on confusion
matrix(dynamic) (Ghannay et al., 2014) and mix-
ing models (Cohn and Lapata, 2007).
</bodyText>
<sectionHeader confidence="0.999094" genericHeader="method">
3 Baseline Components
</sectionHeader>
<subsectionHeader confidence="0.99968">
3.1 Baseline Model and Corpus Statistics
</subsectionHeader>
<bodyText confidence="0.999976277777778">
We have used the ILCI corpora (Choudhary and
Jha, 2011) for our experiments, which contains
English-Hindi parallel sentences from tourism and
health domain. We randomly divided the data into
training (48970), development (500) and testing
(500) sentences and for language modelling we
used news corpus of English which is distributed
as a part of WMT’14 translation task. The data is
about 3 million sentences which also contains MT
training data.
We trained a phrase based (Koehn et al., 2003)
MT system using the Moses toolkit with word-
alignments extracted from GIZA++ (Och and Ney,
2000). We have used the SRILM (Stolcke and
others, 2002) with Kneser-Ney smoothing (Kneser
and Ney, 1995) for training a language model for
the first stage of decoding. The result of this base-
line system is shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.998105">
3.2 English Transformation Module
</subsectionHeader>
<bodyText confidence="0.99685275">
Hindi is a relatively free-word order language and
generally tends to follow SOV (Subject-Object-
Verb) order and English tends to follow SVO
(Subject-Verb-Object) word order. Research has
</bodyText>
<table confidence="0.99469375">
Number of Number of Number of BLEU
Training Development Evaluation
Sentences Sentences Sentences
48970 500 500 20.04
</table>
<tableCaption confidence="0.8323355">
Table 1: Baseline Scores for Phrase-based Moses
Model
</tableCaption>
<bodyText confidence="0.99934175">
shown that pre-ordering source language to con-
form to target language word order significantly
improves translation quality (Collins et al., 2005).
We created a re-ordering module for transform-
ing an English sentence to be in the Hindi order
based on reordering rules provided by Anusaaraka
(Chaudhury et al., 2010). The reordering rules are
based on parse output produced by the Stanford
Parser (Klein and Manning, 2003).
The transformation module requires the text to
contain only surface form of words, however, we
extended it to support surface form along with its
factors such as lemma and Part of Speech (POS).
Input: the girl in blue shirt is my sister
Output: in blue shirt the girl is my sister.
Hindi : neele shirt waali ladki meri bahen hai (
blue) ( shirt) (Mod)(girl)(my)(sister)(Vaux)
With this transformation, the English sentence
is structurally closer to the Hindi sentence which
leads to better phrase alignments. The model
trained with the transformed corpus produces a
new baseline score of 21.84 BLEU score an
improvement over the earlier baseline of 20.04
BLEU points.
</bodyText>
<sectionHeader confidence="0.995734" genericHeader="method">
4 Re-Ranking Experiments
</sectionHeader>
<bodyText confidence="0.999957882352941">
In this section, we describe the results of re-
ranking the output of the translation model us-
ing Recurrent Neural Networks (RNN) based lan-
guage models using the same data which is used
for language modelling in the baseline models.
Unlike traditional n-gram based discrete lan-
guage models, RNN do not make the Markov as-
sumption and potentially can take into account
long-term dependencies between words. Since the
words in RNNs are represented as continuous val-
ued vectors in low dimensions allowing for the
possibility of smoothing using syntactic and se-
mantic features. In practice, however, learning
long-term dependencies with gradient descent is
difficult as described by (Bengio et al., 1994) due
to diminishing gradients.
We have integrated the approach of re-scoring
</bodyText>
<page confidence="0.97975">
52
</page>
<figure confidence="0.947121">
BLEU scores
</figure>
<figureCaption confidence="0.956535666666667">
Figure 1: BLEU Scores for Re-ranking experi-
ments with RNNLM using different feature com-
binations.
</figureCaption>
<bodyText confidence="0.999404">
n-best output using RNNLM which has also been
shown to be helpful by (Liu et al., 2014). Shi
(2012) also showed the benefits of using RNNLM
with contextual and linguistic features. Follow-
ing their work, we used three type of features for
building an RNNLM for Hindi : lemma (root),
POS, NC (number-case). The data used was a
Wikipedia dump, MT training data, news arti-
cles which had approximately 500,000 Hindi sen-
tences. Features were extracted using paradigm-
based Hindi Morphological Analyzer 1
Figure 1 illustrates the results of re-ranking per-
formed using RNNLM trained with various fea-
tures. The Oracle score is the highest achievable
score in a re-ranking experiment. This score is
computed based on the best translation out of n-
best translations. The best translation is found us-
ing the cosine similarity between the hypothesis
and the reference translation. It can be seen from
Figure 1, that the LM with only word and POS in-
formation is inferior to all other models. However,
morphological features like lemma, number and
case information help in re-ranking the hypothesis
significantly. The RNNLM which uses all the fea-
tures performed the best for the re-ranking exper-
iments achieving a BLEU score of 26.91, after re-
scoring 500-best obtained from the pre-order SMT
model.
</bodyText>
<footnote confidence="0.863048">
1We have used the HCU morph-analyzer.
</footnote>
<table confidence="0.999666142857143">
System BLEU
Baseline 21.84
Rescoring 500-best with RNNLM
Features NONE 25.77
POS 24.36
Lemma(root) 26.32
ALL(POS+Lemma+NC) 26.91
</table>
<tableCaption confidence="0.950827">
Table 2: Rescoring results of 500-best hypotheses
using RNNLM with different features
</tableCaption>
<sectionHeader confidence="0.9237545" genericHeader="method">
5 Using WordNet to Reduce Data
Sparsity
</sectionHeader>
<bodyText confidence="0.999957428571429">
We extend the coverage of our source data by us-
ing synonyms from the English WordNet (EWN).
Our main motivation is to reduce the impact of
OOV words on output quality by replacing words
in a source sentence with their corresponding
synset IDs. However, choosing the appropriate
synset ID based upon its context and morphologi-
cal information is important. For sense selection,
we followed the approach used by (Tammewar et
al., 2013), which is also described further in this
section in the context of our task. We ignored
words that are regarded as Named-Entities as in-
dicated by Stanford NER tagger, as they should
not have synonyms in any case.
</bodyText>
<subsectionHeader confidence="0.99325">
5.1 Sense Selection
</subsectionHeader>
<bodyText confidence="0.9999498125">
Words are ambiguous, independent of their sen-
tence context. To choose an appropriate sense ac-
cording to the context for a lexical item is a chal-
lenging task typically termed as word-sense dis-
ambiguation. However, the syntactic category of
a lexical item provides an initial cue for disam-
biguating a lexical item. Among the varied senses,
we filter out the senses that are not the same POS
tag as the lexical item. But words are not just am-
biguous across different syntactic categories but
are also ambiguous within a syntactic category. In
the following, we discuss our approaches to select
the sense of a lexical item best suited in a given
context within a given category. Also categories
were filtered so that only content words get re-
placed with synset IDs.
</bodyText>
<subsubsectionHeader confidence="0.802232">
5.1.1 Intra-Category Sense Selection
</subsubsectionHeader>
<bodyText confidence="0.9933105">
First Sense: Among the different senses,we se-
lect the first sense listed in EWN corresponding to
the POS-tag of a given lexical item. The choice is
motivated by our observation that the senses of a
</bodyText>
<figure confidence="0.927765923076923">
30
28
26
24
22
100 200 300 400 500
Number of Hypotheses
Baseline
POS
NONE
Lemma
Oracle
All
</figure>
<page confidence="0.994472">
53
</page>
<bodyText confidence="0.99916225">
lexical item are ordered in the descending order of
their frequencies of usage in the lexical resource.
Merged Sense: In this approach, we merge all
the senses listed in EWN corresponding to the
POS-tag of the given lexical item. The motivation
behind this strategy is that the senses in the EWN
for a particular word-POS pair are too finely clas-
sified resulting in classification of words that may
represent the same concept, are classified into dif-
ferent synsets. For example : travel and go can
mean the same concept in a similar context but the
first sense given by EWN is different for these two
words. Therefore, we merge all the senses for a
word into a super sense ( synset ID of first word
occurred in data), which is given to all its syn-
onyms even if it occurs in different synset IDs.
</bodyText>
<subsectionHeader confidence="0.885141">
5.2 Factored Model
</subsectionHeader>
<bodyText confidence="0.997262333333333">
Techniques such as factored modelling (Koehn
and Hoang, 2007) are quite beneficial for Trans-
lation from English to Hindi language as shown
by (Ramanathan et al., 2008). When we replace
words in a source sentence with the synset IDˆas,
we tend to lose morphological information associ-
ated with that word. We add inflections as features
in a factored SMT model to minimize the impact
of this replacement.
We show the results of the processing steps on
an example sentence below.
Original Sentence : Ram is going to market to
buy apples
New Sentence : Ram is Synset(go.v.1)
to Synset(market.n.0) to Synset(buy.v.1)
Synset(apple.n.1)
Sentence with synset ID: Ram E is E
Synset(go.v.1) ing to E Synset(market.n.0) E
to E Synset(buy.v.1) E Synset(apple.n.1) s
Then English sentences were reordered to Hindi
word-order using the module discussed in Section
3.
Reordered Sentence: Ram E Synset(apple.n.1) s
Synset(buy.v.1) E to E Synset(market.n.0) E to E
Synset(go.v.1) ing is E
In Table 3, the second row shows the BLEU
scores for the models in which there are synset IDs
for the source side. It can be seen that the factored
model also shows significant improvement in the
results.
</bodyText>
<sectionHeader confidence="0.992338" genericHeader="method">
6 Combining MT Models
</sectionHeader>
<bodyText confidence="0.999886444444444">
Combining Machine translation (MT) systems has
become an important part of Statistical MT in
the past few years. There are two dominant ap-
proaches. (1) a system combination approach
based on confusion networks (CN) (Rosti et al.,
2007), which can work dynamically in combin-
ing the systems. (2) Combine the models by lin-
early interpolating and then using MERT to tune
the combined system.
</bodyText>
<subsectionHeader confidence="0.958153">
6.1 Combination based on confusion
networks
</subsectionHeader>
<bodyText confidence="0.999946125">
We used the tool MANY (Barrault, 2010) for sys-
tem combination. However, since the tool is con-
figured to work with TERp evaluation metric, we
modified it to use METEOR (Gupta et al., 2010)
metric since it has been shown by (Kalyani et al.,
2014), that METEOR evaluation metric is better
correlated to human evaluation for morphologi-
cally rich Indian Languages.
</bodyText>
<subsectionHeader confidence="0.998493">
6.2 Linearly Interpolated Combination
</subsectionHeader>
<bodyText confidence="0.999875142857143">
In this approach, we combined phrase-tables of
the two models (Eng (sysnset) - Hindi and Base-
line) using linear interpolation. We combined the
two models with uniform weights – 0.5 for each
model, in our case. We again tuned this model
with the new interpolated phrase-table using stan-
dard algorithm MERT.
</bodyText>
<sectionHeader confidence="0.994297" genericHeader="evaluation">
7 Experiments and Results
</sectionHeader>
<bodyText confidence="0.998750684210526">
As can be seen in Table 3, the model with synset
information led to reduction in OOV words. Even
though BLEU score decreased, but METEOR
score improved for all the experiments based on
using synset IDs in the source sentence, but it has
been shown by (Gupta et al., 2010) that METEOR
is a better evaluation metrics for morphologically
rich languages. Also, when synset IDˆas are used
instead of words in the source language, the sys-
tem makes incorrect morphological choices. Ex-
ample : going and goes will be replaced by same
synset ID ˆaSynset(go.v.1)ˆa, so this has lead to loss
of information in the phrase-table but METEOR
catches these complexities as it considers features
like stems, synonyms for its evaluation metrics
and hence showed better improvements compared
to BLEU metric. Last two rows of Table 3 show
results for combination experiments and Mixture
Model (linearly interpolated model) showed best
</bodyText>
<page confidence="0.99726">
54
</page>
<table confidence="0.999736833333333">
System #OOV words BLEU Meteor
Baseline 253 21.8 .492
Eng(Synset ID)-Hindi Baseline 237 19.2 .494
*factor(inflections) 225 20.3 .506
Ensembled Decoding 213 21.0 .511
Mixture Model 210 21.2 .519
</table>
<tableCaption confidence="0.998627">
Table 3: Results for the model in which there were Synset ID’s instead of word in English data
</tableCaption>
<bodyText confidence="0.9591865">
results with significant reduction in OOV words
and also some gains in METEOR score.
</bodyText>
<sectionHeader confidence="0.983834" genericHeader="evaluation">
8 Observations
</sectionHeader>
<bodyText confidence="0.997097666666667">
In this section, we study the coverage of different
models by categorizing the OOV words into 5 cat-
egories.
</bodyText>
<listItem confidence="0.998469333333333">
• NE(Named Entities) : As the data was
from Health &amp; Tourism domain, these words
were mainly the names of the places and
medicines.
• VB :types of verb forms
• NN : types of nouns and pronouns
• ADJ : all adjectives
• AD : adverbs
• OTH : there were some words which did not
mean anything in English
• SM : There were some occasional spelling
mistakes seen in the test data.
</listItem>
<bodyText confidence="0.510661">
Note: There were no function words seen in the
OOV(un-translated) words
</bodyText>
<table confidence="0.999321625">
Cat. Baseline Eng(synset)-Hin MixtureModel
NE 120 121 115
VB 47 37 27
NN 76 60 47
ADJ 22 15 12
AD 5 5 4
OTH 2 2 2
SM 8 8 8
</table>
<tableCaption confidence="0.999716">
Table 4: OOV words in Different Models
</tableCaption>
<bodyText confidence="0.9999375">
As this analysis was done on a small dataset and
for a fixed domain, the OOV words were few in
number as it can be seen in Table 4. But the OOV
words across the different models reduced as ex-
pected. The NE words remained almost the same
for all the three models but OOV words from cate-
gory VB,NN,ADJ decreased for Eng(synset)-Hin
model and Mixture model significantly.
</bodyText>
<sectionHeader confidence="0.99195" genericHeader="discussions">
9 Future Work
</sectionHeader>
<bodyText confidence="0.999898461538462">
In the future, we will work on using the two ap-
proaches discussed: Re-Ranking &amp; using lexical
resources to reduce sparsity together in a system.
We will work on exploring syntax based features
for RNNLM and we are planning to use a better
method for sense selection and extending this con-
cept for more language pairs. Word-sense disam-
biguation can be used for choosing more appro-
priate sense when the translation model is trained
on a bigger data data set. Also we are looking for
unsupervised techniques to learn the replacements
for words to reduce sparsity and ways to adapt our
system to different domains.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
10 Conclusions
</sectionHeader>
<bodyText confidence="0.999972777777778">
In this paper, we have discussed two approaches
to address sparsity issues encountered in training
SMT models for morphologically rich languages
with limited amounts of parallel corpora. In the
first approach we used an RNNLM enriched with
morphological features of the target words and
show the BLEU score to improve by 5 points. In
the second approach we use lexical resource such
as WordNet to alleviate sparsity.
</bodyText>
<sectionHeader confidence="0.99858" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9991426">
Loic Barrault. 2010. Many: Open source machine
translation system combination. The Prague Bul-
letin of Mathematical Linguistics, 93:147–155.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157–166.
Sriram Chaudhury, Ankitha Rao, and Dipti M Sharma.
2010. Anusaaraka: An expert system based machine
translation system. In Natural Language Processing
</reference>
<page confidence="0.991688">
55
</page>
<reference confidence="0.998255727272727">
and Knowledge Engineering (NLP-KE), 2010 Inter-
national Conference on, pages 1–6. IEEE.
Narayan Choudhary and Girish Nath Jha. 2011. Cre-
ating multilingual parallel corpora in indian lan-
guages. In Proceedings of Language and Technol-
ogy Conference.
Trevor Cohn and Mirella Lapata. 2007. Ma-
chine translation by triangulation: Making ef-
fective use of multi-parallel corpora. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 728.
Citeseer.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd annual
meeting on association for computational linguis-
tics, pages 531–540. Association for Computational
Linguistics.
Piyush Dungarwal, Rajen Chatterjee, Abhijit Mishra,
Anoop Kunchukuttan, Ritesh Shah, and Pushpak
Bhattacharyya. 2014. The iit bombay hindi-english
translation system at wmt 2014. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, pages 90–96, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.
Sahar Ghannay, France Le Mans, and Loıc Barrault.
2014. Using hypothesis selection based features for
confusion network mt system combination. In Pro-
ceedings of the 3rd Workshop on Hybrid Approaches
to Translation (HyTra)@ EACL, pages 1–5.
Ankush Gupta, Sriram Venkatapathy, and Rajeev San-
gal. 2010. Meteor-hindi: Automatic mt evaluation
metric for hindi as a target language. In Proceed-
ings of ICON-2010: 8th International Conference
on Natural Language Processing.
Chung-chi Huang, Ho-ching Yen, and Jason S Chang.
2010. Using sublexical translations to handle the
oov problem in mt. In Proceedings of The Ninth
Conference of the Association for Machine Transla-
tion in the Americas (AMTA).
Aditi Kalyani, Hemant Kamud, Sashi Pal Singh, and
Ajai Kumar. 2014. Assessing the quality of mt
systems for hindi to english translation. In In-
ternational Journal of Computer Applications, vol-
ume 89.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Asso-
ciation for Computational Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181–184. IEEE.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In EMNLP-CoNLL, pages 868–876.
Citeseer.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48–54. Association for Computa-
tional Linguistics.
X Liu, Y Wang, X Chen, MJF Gales, and PC Wood-
land. 2014. Efficient lattice rescoring using recur-
rent neural network language models.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045–1048.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 440–447. Association for
Computational Linguistics.
Ananthakrishnan Ramanathan, Jayprasad Hegde,
Ritesh M Shah, Pushpak Bhattacharyya, and
M Sasikumar. 2008. Simple syntactic and morpho-
logical processing can help english-hindi statistical
machine translation. In IJCNLP, pages 513–520.
Majid Razmara and Anoop Sarkar. 2013. Ensemble
triangulation for statistical machine translation. In
Proceedings of the Sixth International Joint Confer-
ence on Natural Language Processing, pages 252–
260.
Antti-Veikko I Rosti, Spyridon Matsoukas, and
Richard Schwartz. 2007. Improved word-level sys-
tem combination for machine translation. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 312.
Citeseer.
Yangyang Shi, Pascal Wiggers, and Catholijn M
Jonker. 2012. Towards recurrent neural networks
language models with linguistic and contextual fea-
tures. In INTERSPEECH.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In INTERSPEECH.
Martin Sundermeyer, Ilya Oparin, J-L Gauvain, Ben
Freiberg, R Schluter, and Hermann Ney. 2013.
Comparison of feedforward and recurrent neural
network language models. In Acoustics, Speech and
Signal Processing (ICASSP), 2013 IEEE Interna-
tional Conference on, pages 8430–8434. IEEE.
Aniruddha Tammewar, Karan Singla, Srinivas Banga-
lore, and Michael Carl. 2013. Enhancing asr by
mt using semantic information from hindiwordnet.
In Proceedings of ICON-2013: 10th International
Conference on Natural Language Processing.
</reference>
<page confidence="0.998423">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.963149">
<title confidence="0.999923">Reducing the Impact of Data Sparsity in Statistical Machine Translation</title>
<author confidence="0.995273">Kunal Diksha Srinivas Dipti Misra</author>
<affiliation confidence="0.994876">IIIT Labs-Research</affiliation>
<abstract confidence="0.998254625">Morphologically rich languages generally require large amounts of parallel data to adequately estimate parameters in a statistical Machine Translation(SMT) system. However, it is time consuming and expensive to create large collections of parallel data. In this paper, we explore two strategies for circumventing sparsity caused by lack of large parallel corpora. First, we explore the use of distributed representations in an Recurrent Neural Network based language model with different morphological features and second, we explore the use of lexical resources such as WordNet to overcome sparsity of content words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Loic Barrault</author>
</authors>
<title>Many: Open source machine translation system combination.</title>
<date>2010</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>93--147</pages>
<contexts>
<context position="14029" citStr="Barrault, 2010" startWordPosition="2269" endWordPosition="2270"> synset IDs for the source side. It can be seen that the factored model also shows significant improvement in the results. 6 Combining MT Models Combining Machine translation (MT) systems has become an important part of Statistical MT in the past few years. There are two dominant approaches. (1) a system combination approach based on confusion networks (CN) (Rosti et al., 2007), which can work dynamically in combining the systems. (2) Combine the models by linearly interpolating and then using MERT to tune the combined system. 6.1 Combination based on confusion networks We used the tool MANY (Barrault, 2010) for system combination. However, since the tool is configured to work with TERp evaluation metric, we modified it to use METEOR (Gupta et al., 2010) metric since it has been shown by (Kalyani et al., 2014), that METEOR evaluation metric is better correlated to human evaluation for morphologically rich Indian Languages. 6.2 Linearly Interpolated Combination In this approach, we combined phrase-tables of the two models (Eng (sysnset) - Hindi and Baseline) using linear interpolation. We combined the two models with uniform weights – 0.5 for each model, in our case. We again tuned this model with</context>
</contexts>
<marker>Barrault, 2010</marker>
<rawString>Loic Barrault. 2010. Many: Open source machine translation system combination. The Prague Bulletin of Mathematical Linguistics, 93:147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Patrice Simard</author>
<author>Paolo Frasconi</author>
</authors>
<title>Learning long-term dependencies with gradient descent is difficult. Neural Networks,</title>
<date>1994</date>
<journal>IEEE Transactions on,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="8023" citStr="Bengio et al., 1994" startWordPosition="1260" endWordPosition="1263">lation model using Recurrent Neural Networks (RNN) based language models using the same data which is used for language modelling in the baseline models. Unlike traditional n-gram based discrete language models, RNN do not make the Markov assumption and potentially can take into account long-term dependencies between words. Since the words in RNNs are represented as continuous valued vectors in low dimensions allowing for the possibility of smoothing using syntactic and semantic features. In practice, however, learning long-term dependencies with gradient descent is difficult as described by (Bengio et al., 1994) due to diminishing gradients. We have integrated the approach of re-scoring 52 BLEU scores Figure 1: BLEU Scores for Re-ranking experiments with RNNLM using different feature combinations. n-best output using RNNLM which has also been shown to be helpful by (Liu et al., 2014). Shi (2012) also showed the benefits of using RNNLM with contextual and linguistic features. Following their work, we used three type of features for building an RNNLM for Hindi : lemma (root), POS, NC (number-case). The data used was a Wikipedia dump, MT training data, news articles which had approximately 500,000 Hindi</context>
</contexts>
<marker>Bengio, Simard, Frasconi, 1994</marker>
<rawString>Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sriram Chaudhury</author>
<author>Ankitha Rao</author>
<author>Dipti M Sharma</author>
</authors>
<title>Anusaaraka: An expert system based machine translation system.</title>
<date>2010</date>
<booktitle>In Natural Language Processing and Knowledge Engineering (NLP-KE), 2010 International Conference on,</booktitle>
<pages>1--6</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6525" citStr="Chaudhury et al., 2010" startWordPosition="1019" endWordPosition="1022"> generally tends to follow SOV (Subject-ObjectVerb) order and English tends to follow SVO (Subject-Verb-Object) word order. Research has Number of Number of Number of BLEU Training Development Evaluation Sentences Sentences Sentences 48970 500 500 20.04 Table 1: Baseline Scores for Phrase-based Moses Model shown that pre-ordering source language to conform to target language word order significantly improves translation quality (Collins et al., 2005). We created a re-ordering module for transforming an English sentence to be in the Hindi order based on reordering rules provided by Anusaaraka (Chaudhury et al., 2010). The reordering rules are based on parse output produced by the Stanford Parser (Klein and Manning, 2003). The transformation module requires the text to contain only surface form of words, however, we extended it to support surface form along with its factors such as lemma and Part of Speech (POS). Input: the girl in blue shirt is my sister Output: in blue shirt the girl is my sister. Hindi : neele shirt waali ladki meri bahen hai ( blue) ( shirt) (Mod)(girl)(my)(sister)(Vaux) With this transformation, the English sentence is structurally closer to the Hindi sentence which leads to better ph</context>
</contexts>
<marker>Chaudhury, Rao, Sharma, 2010</marker>
<rawString>Sriram Chaudhury, Ankitha Rao, and Dipti M Sharma. 2010. Anusaaraka: An expert system based machine translation system. In Natural Language Processing and Knowledge Engineering (NLP-KE), 2010 International Conference on, pages 1–6. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Narayan Choudhary</author>
<author>Girish Nath Jha</author>
</authors>
<title>Creating multilingual parallel corpora in indian languages.</title>
<date>2011</date>
<booktitle>In Proceedings of Language and Technology Conference.</booktitle>
<contexts>
<context position="5063" citStr="Choudhary and Jha, 2011" startWordPosition="790" endWordPosition="793">in past few years. Works by (Razmara and Sarkar, 2013; Cohn and Lapata, 2007) have shown that there is an increase in phrase coverage when combining different systems. To get more coverage of unigrams in phrase-table, we have explored system combination approaches to combine models trained with synset information and without synset information. We have explored two methodologies for system combination based on confusion matrix(dynamic) (Ghannay et al., 2014) and mixing models (Cohn and Lapata, 2007). 3 Baseline Components 3.1 Baseline Model and Corpus Statistics We have used the ILCI corpora (Choudhary and Jha, 2011) for our experiments, which contains English-Hindi parallel sentences from tourism and health domain. We randomly divided the data into training (48970), development (500) and testing (500) sentences and for language modelling we used news corpus of English which is distributed as a part of WMT’14 translation task. The data is about 3 million sentences which also contains MT training data. We trained a phrase based (Koehn et al., 2003) MT system using the Moses toolkit with wordalignments extracted from GIZA++ (Och and Ney, 2000). We have used the SRILM (Stolcke and others, 2002) with Kneser-N</context>
</contexts>
<marker>Choudhary, Jha, 2011</marker>
<rawString>Narayan Choudhary and Girish Nath Jha. 2011. Creating multilingual parallel corpora in indian languages. In Proceedings of Language and Technology Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Machine translation by triangulation: Making effective use of multi-parallel corpora.</title>
<date>2007</date>
<booktitle>In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,</booktitle>
<volume>45</volume>
<pages>728</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="4516" citStr="Cohn and Lapata, 2007" startWordPosition="705" endWordPosition="708">med entities, combination forms (e.g. widebody) and abbreviations. Apart from these issues, Hindi being a low-resourced language in terms of parallel corpora suffers from data sparsity. In the second part of the paper, we address the problem of data sparsity with the help of English WordNet (EWN) for English-Hindi PB-SMT. We increase the coverage of content words (excluding Named-Entities) by incorporating sysnset information in the source sentences. Combining Machine Translation (MT) systems has become an important part of statistical MT in past few years. Works by (Razmara and Sarkar, 2013; Cohn and Lapata, 2007) have shown that there is an increase in phrase coverage when combining different systems. To get more coverage of unigrams in phrase-table, we have explored system combination approaches to combine models trained with synset information and without synset information. We have explored two methodologies for system combination based on confusion matrix(dynamic) (Ghannay et al., 2014) and mixing models (Cohn and Lapata, 2007). 3 Baseline Components 3.1 Baseline Model and Corpus Statistics We have used the ILCI corpora (Choudhary and Jha, 2011) for our experiments, which contains English-Hindi pa</context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2007. Machine translation by triangulation: Making effective use of multi-parallel corpora. In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, volume 45, page 728. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd annual meeting on association for computational linguistics,</booktitle>
<pages>531--540</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd annual meeting on association for computational linguistics, pages 531–540. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piyush Dungarwal</author>
<author>Rajen Chatterjee</author>
<author>Abhijit Mishra</author>
<author>Anoop Kunchukuttan</author>
<author>Ritesh Shah</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>The iit bombay hindi-english translation system at wmt</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>90--96</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="2661" citStr="Dungarwal et al., 2014" startWordPosition="417" endWordPosition="420">with synset ID’s and baseline model to further improve the translation accuracy followed by results and observations sections.We conclude the paper with future work and conclusions. 2 Related Work In this paper, we present our efforts of reranking the n-best hypotheses produced by a PBMT (Phrase-Based MT) system using RNNLM (Mikolov et al., 2010) in the context of an EnglishHindi SMT system. The re-ranking task in machine translation can be defined as re-scoring the n-best list of translations, wherein a number of language models are deployed along with features of source or target language. (Dungarwal et al., 2014) described the benefits of re-ranking the translation hypothesis using simple n-gram based language model. In recent years, the use of RNNLM have shown significant improvements over the traditional n-gram models (Sundermeyer et al., 2013). (Mikolov et al., 2010) and (Liu et al., 2014) have shown significant improvements in speech recognition accuracy using RNNLM . Shi (2012) also showed the benefits of using RNNLM with contextual and linguistic features. We have also explored the use of morphological features (Hindi being a morphologically rich language) in RNNLM and deduced that these feature</context>
</contexts>
<marker>Dungarwal, Chatterjee, Mishra, Kunchukuttan, Shah, Bhattacharyya, 2014</marker>
<rawString>Piyush Dungarwal, Rajen Chatterjee, Abhijit Mishra, Anoop Kunchukuttan, Ritesh Shah, and Pushpak Bhattacharyya. 2014. The iit bombay hindi-english translation system at wmt 2014. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 90–96, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sahar Ghannay</author>
<author>France Le Mans</author>
<author>Loıc Barrault</author>
</authors>
<title>Using hypothesis selection based features for confusion network mt system combination.</title>
<date>2014</date>
<booktitle>In Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra)@ EACL,</booktitle>
<pages>1--5</pages>
<marker>Ghannay, Le Mans, Barrault, 2014</marker>
<rawString>Sahar Ghannay, France Le Mans, and Loıc Barrault. 2014. Using hypothesis selection based features for confusion network mt system combination. In Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra)@ EACL, pages 1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ankush Gupta</author>
<author>Sriram Venkatapathy</author>
<author>Rajeev Sangal</author>
</authors>
<title>Meteor-hindi: Automatic mt evaluation metric for hindi as a target language.</title>
<date>2010</date>
<booktitle>In Proceedings of ICON-2010: 8th International Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="14178" citStr="Gupta et al., 2010" startWordPosition="2294" endWordPosition="2297">Combining Machine translation (MT) systems has become an important part of Statistical MT in the past few years. There are two dominant approaches. (1) a system combination approach based on confusion networks (CN) (Rosti et al., 2007), which can work dynamically in combining the systems. (2) Combine the models by linearly interpolating and then using MERT to tune the combined system. 6.1 Combination based on confusion networks We used the tool MANY (Barrault, 2010) for system combination. However, since the tool is configured to work with TERp evaluation metric, we modified it to use METEOR (Gupta et al., 2010) metric since it has been shown by (Kalyani et al., 2014), that METEOR evaluation metric is better correlated to human evaluation for morphologically rich Indian Languages. 6.2 Linearly Interpolated Combination In this approach, we combined phrase-tables of the two models (Eng (sysnset) - Hindi and Baseline) using linear interpolation. We combined the two models with uniform weights – 0.5 for each model, in our case. We again tuned this model with the new interpolated phrase-table using standard algorithm MERT. 7 Experiments and Results As can be seen in Table 3, the model with synset informat</context>
</contexts>
<marker>Gupta, Venkatapathy, Sangal, 2010</marker>
<rawString>Ankush Gupta, Sriram Venkatapathy, and Rajeev Sangal. 2010. Meteor-hindi: Automatic mt evaluation metric for hindi as a target language. In Proceedings of ICON-2010: 8th International Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung-chi Huang</author>
<author>Ho-ching Yen</author>
<author>Jason S Chang</author>
</authors>
<title>Using sublexical translations to handle the oov problem in mt.</title>
<date>2010</date>
<booktitle>In Proceedings of The Ninth Conference of the Association for Machine Translation in the Americas (AMTA).</booktitle>
<marker>Huang, Yen, Chang, 2010</marker>
<rawString>Chung-chi Huang, Ho-ching Yen, and Jason S Chang. 2010. Using sublexical translations to handle the oov problem in mt. In Proceedings of The Ninth Conference of the Association for Machine Translation in the Americas (AMTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aditi Kalyani</author>
<author>Hemant Kamud</author>
<author>Sashi Pal Singh</author>
<author>Ajai Kumar</author>
</authors>
<title>Assessing the quality of mt systems for hindi to english translation.</title>
<date>2014</date>
<journal>In International Journal of Computer Applications,</journal>
<volume>89</volume>
<contexts>
<context position="14235" citStr="Kalyani et al., 2014" startWordPosition="2305" endWordPosition="2308">n important part of Statistical MT in the past few years. There are two dominant approaches. (1) a system combination approach based on confusion networks (CN) (Rosti et al., 2007), which can work dynamically in combining the systems. (2) Combine the models by linearly interpolating and then using MERT to tune the combined system. 6.1 Combination based on confusion networks We used the tool MANY (Barrault, 2010) for system combination. However, since the tool is configured to work with TERp evaluation metric, we modified it to use METEOR (Gupta et al., 2010) metric since it has been shown by (Kalyani et al., 2014), that METEOR evaluation metric is better correlated to human evaluation for morphologically rich Indian Languages. 6.2 Linearly Interpolated Combination In this approach, we combined phrase-tables of the two models (Eng (sysnset) - Hindi and Baseline) using linear interpolation. We combined the two models with uniform weights – 0.5 for each model, in our case. We again tuned this model with the new interpolated phrase-table using standard algorithm MERT. 7 Experiments and Results As can be seen in Table 3, the model with synset information led to reduction in OOV words. Even though BLEU score</context>
</contexts>
<marker>Kalyani, Kamud, Singh, Kumar, 2014</marker>
<rawString>Aditi Kalyani, Hemant Kamud, Sashi Pal Singh, and Ajai Kumar. 2014. Assessing the quality of mt systems for hindi to english translation. In International Journal of Computer Applications, volume 89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6631" citStr="Klein and Manning, 2003" startWordPosition="1036" endWordPosition="1039">bject) word order. Research has Number of Number of Number of BLEU Training Development Evaluation Sentences Sentences Sentences 48970 500 500 20.04 Table 1: Baseline Scores for Phrase-based Moses Model shown that pre-ordering source language to conform to target language word order significantly improves translation quality (Collins et al., 2005). We created a re-ordering module for transforming an English sentence to be in the Hindi order based on reordering rules provided by Anusaaraka (Chaudhury et al., 2010). The reordering rules are based on parse output produced by the Stanford Parser (Klein and Manning, 2003). The transformation module requires the text to contain only surface form of words, however, we extended it to support surface form along with its factors such as lemma and Part of Speech (POS). Input: the girl in blue shirt is my sister Output: in blue shirt the girl is my sister. Hindi : neele shirt waali ladki meri bahen hai ( blue) ( shirt) (Mod)(girl)(my)(sister)(Vaux) With this transformation, the English sentence is structurally closer to the Hindi sentence which leads to better phrase alignments. The model trained with the transformed corpus produces a new baseline score of 21.84 BLEU</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5698" citStr="Kneser and Ney, 1995" startWordPosition="891" endWordPosition="894">iments, which contains English-Hindi parallel sentences from tourism and health domain. We randomly divided the data into training (48970), development (500) and testing (500) sentences and for language modelling we used news corpus of English which is distributed as a part of WMT’14 translation task. The data is about 3 million sentences which also contains MT training data. We trained a phrase based (Koehn et al., 2003) MT system using the Moses toolkit with wordalignments extracted from GIZA++ (Och and Ney, 2000). We have used the SRILM (Stolcke and others, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) for training a language model for the first stage of decoding. The result of this baseline system is shown in Table 1. 3.2 English Transformation Module Hindi is a relatively free-word order language and generally tends to follow SOV (Subject-ObjectVerb) order and English tends to follow SVO (Subject-Verb-Object) word order. Research has Number of Number of Number of BLEU Training Development Evaluation Sentences Sentences Sentences 48970 500 500 20.04 Table 1: Baseline Scores for Phrase-based Moses Model shown that pre-ordering source language to conform to target language word order signifi</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pages 181–184. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>868--876</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="12421" citStr="Koehn and Hoang, 2007" startWordPosition="2002" endWordPosition="2005">is strategy is that the senses in the EWN for a particular word-POS pair are too finely classified resulting in classification of words that may represent the same concept, are classified into different synsets. For example : travel and go can mean the same concept in a similar context but the first sense given by EWN is different for these two words. Therefore, we merge all the senses for a word into a super sense ( synset ID of first word occurred in data), which is given to all its synonyms even if it occurs in different synset IDs. 5.2 Factored Model Techniques such as factored modelling (Koehn and Hoang, 2007) are quite beneficial for Translation from English to Hindi language as shown by (Ramanathan et al., 2008). When we replace words in a source sentence with the synset IDˆas, we tend to lose morphological information associated with that word. We add inflections as features in a factored SMT model to minimize the impact of this replacement. We show the results of the processing steps on an example sentence below. Original Sentence : Ram is going to market to buy apples New Sentence : Ram is Synset(go.v.1) to Synset(market.n.0) to Synset(buy.v.1) Synset(apple.n.1) Sentence with synset ID: Ram E </context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In EMNLP-CoNLL, pages 868–876. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5502" citStr="Koehn et al., 2003" startWordPosition="859" endWordPosition="862">nnay et al., 2014) and mixing models (Cohn and Lapata, 2007). 3 Baseline Components 3.1 Baseline Model and Corpus Statistics We have used the ILCI corpora (Choudhary and Jha, 2011) for our experiments, which contains English-Hindi parallel sentences from tourism and health domain. We randomly divided the data into training (48970), development (500) and testing (500) sentences and for language modelling we used news corpus of English which is distributed as a part of WMT’14 translation task. The data is about 3 million sentences which also contains MT training data. We trained a phrase based (Koehn et al., 2003) MT system using the Moses toolkit with wordalignments extracted from GIZA++ (Och and Ney, 2000). We have used the SRILM (Stolcke and others, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) for training a language model for the first stage of decoding. The result of this baseline system is shown in Table 1. 3.2 English Transformation Module Hindi is a relatively free-word order language and generally tends to follow SOV (Subject-ObjectVerb) order and English tends to follow SVO (Subject-Verb-Object) word order. Research has Number of Number of Number of BLEU Training Development Evaluat</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 48–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Liu</author>
<author>Y Wang</author>
<author>X Chen</author>
<author>MJF Gales</author>
<author>PC Woodland</author>
</authors>
<title>Efficient lattice rescoring using recurrent neural network language models.</title>
<date>2014</date>
<contexts>
<context position="2946" citStr="Liu et al., 2014" startWordPosition="460" endWordPosition="463">hrase-Based MT) system using RNNLM (Mikolov et al., 2010) in the context of an EnglishHindi SMT system. The re-ranking task in machine translation can be defined as re-scoring the n-best list of translations, wherein a number of language models are deployed along with features of source or target language. (Dungarwal et al., 2014) described the benefits of re-ranking the translation hypothesis using simple n-gram based language model. In recent years, the use of RNNLM have shown significant improvements over the traditional n-gram models (Sundermeyer et al., 2013). (Mikolov et al., 2010) and (Liu et al., 2014) have shown significant improvements in speech recognition accuracy using RNNLM . Shi (2012) also showed the benefits of using RNNLM with contextual and linguistic features. We have also explored the use of morphological features (Hindi being a morphologically rich language) in RNNLM and deduced that these features further improve the baseline RNNLM in re-ranking the nbest hypothesis. Words in natural languages are richly diverse so it is not possible to cover all source language words when training an MT system. Untranslated out-of-vocabulary (OOV) words tend to degrade the accuracy of the ou</context>
<context position="8300" citStr="Liu et al., 2014" startWordPosition="1306" endWordPosition="1309">account long-term dependencies between words. Since the words in RNNs are represented as continuous valued vectors in low dimensions allowing for the possibility of smoothing using syntactic and semantic features. In practice, however, learning long-term dependencies with gradient descent is difficult as described by (Bengio et al., 1994) due to diminishing gradients. We have integrated the approach of re-scoring 52 BLEU scores Figure 1: BLEU Scores for Re-ranking experiments with RNNLM using different feature combinations. n-best output using RNNLM which has also been shown to be helpful by (Liu et al., 2014). Shi (2012) also showed the benefits of using RNNLM with contextual and linguistic features. Following their work, we used three type of features for building an RNNLM for Hindi : lemma (root), POS, NC (number-case). The data used was a Wikipedia dump, MT training data, news articles which had approximately 500,000 Hindi sentences. Features were extracted using paradigmbased Hindi Morphological Analyzer 1 Figure 1 illustrates the results of re-ranking performed using RNNLM trained with various features. The Oracle score is the highest achievable score in a re-ranking experiment. This score is</context>
</contexts>
<marker>Liu, Wang, Chen, Gales, Woodland, 2014</marker>
<rawString>X Liu, Y Wang, X Chen, MJF Gales, and PC Woodland. 2014. Efficient lattice rescoring using recurrent neural network language models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5598" citStr="Och and Ney, 2000" startWordPosition="875" endWordPosition="878">Model and Corpus Statistics We have used the ILCI corpora (Choudhary and Jha, 2011) for our experiments, which contains English-Hindi parallel sentences from tourism and health domain. We randomly divided the data into training (48970), development (500) and testing (500) sentences and for language modelling we used news corpus of English which is distributed as a part of WMT’14 translation task. The data is about 3 million sentences which also contains MT training data. We trained a phrase based (Koehn et al., 2003) MT system using the Moses toolkit with wordalignments extracted from GIZA++ (Och and Ney, 2000). We have used the SRILM (Stolcke and others, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) for training a language model for the first stage of decoding. The result of this baseline system is shown in Table 1. 3.2 English Transformation Module Hindi is a relatively free-word order language and generally tends to follow SOV (Subject-ObjectVerb) order and English tends to follow SVO (Subject-Verb-Object) word order. Research has Number of Number of Number of BLEU Training Development Evaluation Sentences Sentences Sentences 48970 500 500 20.04 Table 1: Baseline Scores for Phrase-based </context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 440–447. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananthakrishnan Ramanathan</author>
<author>Jayprasad Hegde</author>
<author>Ritesh M Shah</author>
<author>Pushpak Bhattacharyya</author>
<author>M Sasikumar</author>
</authors>
<title>Simple syntactic and morphological processing can help english-hindi statistical machine translation.</title>
<date>2008</date>
<booktitle>In IJCNLP,</booktitle>
<pages>513--520</pages>
<contexts>
<context position="12527" citStr="Ramanathan et al., 2008" startWordPosition="2020" endWordPosition="2023">ting in classification of words that may represent the same concept, are classified into different synsets. For example : travel and go can mean the same concept in a similar context but the first sense given by EWN is different for these two words. Therefore, we merge all the senses for a word into a super sense ( synset ID of first word occurred in data), which is given to all its synonyms even if it occurs in different synset IDs. 5.2 Factored Model Techniques such as factored modelling (Koehn and Hoang, 2007) are quite beneficial for Translation from English to Hindi language as shown by (Ramanathan et al., 2008). When we replace words in a source sentence with the synset IDˆas, we tend to lose morphological information associated with that word. We add inflections as features in a factored SMT model to minimize the impact of this replacement. We show the results of the processing steps on an example sentence below. Original Sentence : Ram is going to market to buy apples New Sentence : Ram is Synset(go.v.1) to Synset(market.n.0) to Synset(buy.v.1) Synset(apple.n.1) Sentence with synset ID: Ram E is E Synset(go.v.1) ing to E Synset(market.n.0) E to E Synset(buy.v.1) E Synset(apple.n.1) s Then English </context>
</contexts>
<marker>Ramanathan, Hegde, Shah, Bhattacharyya, Sasikumar, 2008</marker>
<rawString>Ananthakrishnan Ramanathan, Jayprasad Hegde, Ritesh M Shah, Pushpak Bhattacharyya, and M Sasikumar. 2008. Simple syntactic and morphological processing can help english-hindi statistical machine translation. In IJCNLP, pages 513–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Majid Razmara</author>
<author>Anoop Sarkar</author>
</authors>
<title>Ensemble triangulation for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>252--260</pages>
<contexts>
<context position="4492" citStr="Razmara and Sarkar, 2013" startWordPosition="701" endWordPosition="704">ror in source language, named entities, combination forms (e.g. widebody) and abbreviations. Apart from these issues, Hindi being a low-resourced language in terms of parallel corpora suffers from data sparsity. In the second part of the paper, we address the problem of data sparsity with the help of English WordNet (EWN) for English-Hindi PB-SMT. We increase the coverage of content words (excluding Named-Entities) by incorporating sysnset information in the source sentences. Combining Machine Translation (MT) systems has become an important part of statistical MT in past few years. Works by (Razmara and Sarkar, 2013; Cohn and Lapata, 2007) have shown that there is an increase in phrase coverage when combining different systems. To get more coverage of unigrams in phrase-table, we have explored system combination approaches to combine models trained with synset information and without synset information. We have explored two methodologies for system combination based on confusion matrix(dynamic) (Ghannay et al., 2014) and mixing models (Cohn and Lapata, 2007). 3 Baseline Components 3.1 Baseline Model and Corpus Statistics We have used the ILCI corpora (Choudhary and Jha, 2011) for our experiments, which c</context>
</contexts>
<marker>Razmara, Sarkar, 2013</marker>
<rawString>Majid Razmara and Anoop Sarkar. 2013. Ensemble triangulation for statistical machine translation. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 252– 260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Spyridon Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved word-level system combination for machine translation.</title>
<date>2007</date>
<booktitle>In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,</booktitle>
<volume>45</volume>
<pages>312</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="13794" citStr="Rosti et al., 2007" startWordPosition="2228" endWordPosition="2231">ng the module discussed in Section 3. Reordered Sentence: Ram E Synset(apple.n.1) s Synset(buy.v.1) E to E Synset(market.n.0) E to E Synset(go.v.1) ing is E In Table 3, the second row shows the BLEU scores for the models in which there are synset IDs for the source side. It can be seen that the factored model also shows significant improvement in the results. 6 Combining MT Models Combining Machine translation (MT) systems has become an important part of Statistical MT in the past few years. There are two dominant approaches. (1) a system combination approach based on confusion networks (CN) (Rosti et al., 2007), which can work dynamically in combining the systems. (2) Combine the models by linearly interpolating and then using MERT to tune the combined system. 6.1 Combination based on confusion networks We used the tool MANY (Barrault, 2010) for system combination. However, since the tool is configured to work with TERp evaluation metric, we modified it to use METEOR (Gupta et al., 2010) metric since it has been shown by (Kalyani et al., 2014), that METEOR evaluation metric is better correlated to human evaluation for morphologically rich Indian Languages. 6.2 Linearly Interpolated Combination In th</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Antti-Veikko I Rosti, Spyridon Matsoukas, and Richard Schwartz. 2007. Improved word-level system combination for machine translation. In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, volume 45, page 312. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangyang Shi</author>
<author>Pascal Wiggers</author>
<author>Catholijn M Jonker</author>
</authors>
<title>Towards recurrent neural networks language models with linguistic and contextual features.</title>
<date>2012</date>
<booktitle>In INTERSPEECH.</booktitle>
<marker>Shi, Wiggers, Jonker, 2012</marker>
<rawString>Yangyang Shi, Pascal Wiggers, and Catholijn M Jonker. 2012. Towards recurrent neural networks language models with linguistic and contextual features. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In INTERSPEECH.</booktitle>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke et al. 2002. Srilm-an extensible language modeling toolkit. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Ilya Oparin</author>
<author>J-L Gauvain</author>
<author>Ben Freiberg</author>
<author>R Schluter</author>
<author>Hermann Ney</author>
</authors>
<title>Comparison of feedforward and recurrent neural network language models.</title>
<date>2013</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,</booktitle>
<pages>8430--8434</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="2899" citStr="Sundermeyer et al., 2013" startWordPosition="451" endWordPosition="454">f reranking the n-best hypotheses produced by a PBMT (Phrase-Based MT) system using RNNLM (Mikolov et al., 2010) in the context of an EnglishHindi SMT system. The re-ranking task in machine translation can be defined as re-scoring the n-best list of translations, wherein a number of language models are deployed along with features of source or target language. (Dungarwal et al., 2014) described the benefits of re-ranking the translation hypothesis using simple n-gram based language model. In recent years, the use of RNNLM have shown significant improvements over the traditional n-gram models (Sundermeyer et al., 2013). (Mikolov et al., 2010) and (Liu et al., 2014) have shown significant improvements in speech recognition accuracy using RNNLM . Shi (2012) also showed the benefits of using RNNLM with contextual and linguistic features. We have also explored the use of morphological features (Hindi being a morphologically rich language) in RNNLM and deduced that these features further improve the baseline RNNLM in re-ranking the nbest hypothesis. Words in natural languages are richly diverse so it is not possible to cover all source language words when training an MT system. Untranslated out-of-vocabulary (OO</context>
</contexts>
<marker>Sundermeyer, Oparin, Gauvain, Freiberg, Schluter, Ney, 2013</marker>
<rawString>Martin Sundermeyer, Ilya Oparin, J-L Gauvain, Ben Freiberg, R Schluter, and Hermann Ney. 2013. Comparison of feedforward and recurrent neural network language models. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8430–8434. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aniruddha Tammewar</author>
<author>Karan Singla</author>
<author>Srinivas Bangalore</author>
<author>Michael Carl</author>
</authors>
<title>Enhancing asr by mt using semantic information from hindiwordnet.</title>
<date>2013</date>
<booktitle>In Proceedings of ICON-2013: 10th International Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="10220" citStr="Tammewar et al., 2013" startWordPosition="1615" endWordPosition="1618"> NONE 25.77 POS 24.36 Lemma(root) 26.32 ALL(POS+Lemma+NC) 26.91 Table 2: Rescoring results of 500-best hypotheses using RNNLM with different features 5 Using WordNet to Reduce Data Sparsity We extend the coverage of our source data by using synonyms from the English WordNet (EWN). Our main motivation is to reduce the impact of OOV words on output quality by replacing words in a source sentence with their corresponding synset IDs. However, choosing the appropriate synset ID based upon its context and morphological information is important. For sense selection, we followed the approach used by (Tammewar et al., 2013), which is also described further in this section in the context of our task. We ignored words that are regarded as Named-Entities as indicated by Stanford NER tagger, as they should not have synonyms in any case. 5.1 Sense Selection Words are ambiguous, independent of their sentence context. To choose an appropriate sense according to the context for a lexical item is a challenging task typically termed as word-sense disambiguation. However, the syntactic category of a lexical item provides an initial cue for disambiguating a lexical item. Among the varied senses, we filter out the senses tha</context>
</contexts>
<marker>Tammewar, Singla, Bangalore, Carl, 2013</marker>
<rawString>Aniruddha Tammewar, Karan Singla, Srinivas Bangalore, and Michael Carl. 2013. Enhancing asr by mt using semantic information from hindiwordnet. In Proceedings of ICON-2013: 10th International Conference on Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
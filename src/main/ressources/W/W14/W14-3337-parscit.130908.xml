<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001452">
<title confidence="0.9990655">
Efforts on Machine Learning over
Human-mediated Translation Edit Rate
</title>
<author confidence="0.808181">
Eleftherios Avramidis
</author>
<affiliation confidence="0.7967025">
German Research Center for Artificial Intelligence (DFKI)
Language Technology Lab
</affiliation>
<address confidence="0.909325">
Alt Moabit 91c, 10559 Berlin, Germany
</address>
<email confidence="0.997997">
eleftherios.avramidis@dfki.de
</email>
<sectionHeader confidence="0.993921" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999479090909091">
In this paper we describe experiments on
predicting HTER, as part of our submis-
sion in the Shared Task on Quality Esti-
mation, in the frame of the 9th Workshop
on Statistical Machine Translation. In our
experiment we check whether it is possi-
ble to achieve better HTER prediction by
training four individual regression models
for each one of the edit types (deletions,
insertions, substitutions, shifts), however
no improvements were yielded. We also
had no improvements when investigat-
ing the possibility of adding more data
from other non-minimally post-edited and
freely translated datasets. Best HTER pre-
diction was achieved by adding dedupli-
cated WMT13 data and additional features
such as (a) rule-based language correc-
tions (language tool) (b) PCFG parsing
statistics and count of tree labels (c) posi-
tion statistics of parsing labels (d) position
statistics of tri-grams with low probability.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949485714286">
As Machine Translation (MT) gets integrated into
regular translation workflows, its use as base for
post-editing is radically increased. As a result,
there is a great demand for methods that can auto-
matically assess the MT outcome and ensure that
it is useful for the translator and can lead to more
productive translation work.
Although many agree that the quality of the
MT output itself is not adequate for the profes-
sional standards, there has not yet been a widely-
accepted way to measure its quality on par with
human translations. One such metric, the Hu-
man Translation Edit Rate (HTER) (Snover et
al., 2006), is the focus of the current submission.
HTER is highly relevant to the need of adapting
MT to the needs of translators, as it aims to mea-
sure how far it is from an acceptable equivalent
translation done by humans.
HTER is used here in the frame of Quality Es-
timation, i.e. having the goal of being able to pre-
dict the post-editing effort in a real case environ-
ment, right before the translation is given to the
user, without real access to the correct translation.
For this purpose the text of the source and the pro-
duced translation is analyzed by automatic tools
in order to infer indications (numerical features)
that may be relevant to the quality of the transla-
tion. These features are used in a statistical model
whose parameters are estimated with common su-
pervised Machine Learning techniques.
This work presents an extensive search over var-
ious set-ups and parameters for such techniques,
aiming to build a model that better predicts HTER
over the data of the Shared Task of the 9th Work-
shop on Statistical Machine Translation.
</bodyText>
<sectionHeader confidence="0.880798" genericHeader="method">
2 New approaches being tested
</sectionHeader>
<subsectionHeader confidence="0.96554">
2.1 Break HTER apart
</subsectionHeader>
<bodyText confidence="0.962869">
HTER is a complex metric, in the sense that it is
calculated as a linear function over specific types
of edit distance. The official algorithm performs
a comparison between the MT output and the cor-
rected version of this output by a human translator,
who performed the minimum number of changes.
The comparison results in counting the number of
insertions, deletions, substitutions and shifts (e.g.
reordering). The final HTER score is the total
number of edits divided by the number of refer-
ence words.
HTER = #insertions + #dels + #subs + #shifts
#reference words
We notice that the metric is clearly based on four
edit types that are seemingly independent of each
other. This poses the question whether the existing
</bodyText>
<page confidence="0.984199">
302
</page>
<bodyText confidence="0.9196225625">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 302–306,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
approach of learning the entire metric altogether
introduces way too much complexity in the ma-
chine learning process. Instead, we test the hy-
pothesis that it is more effective to build a separate
model for each error type and then put the output
of each model on the overall HTER fraction shown
above.
Following this idea, we score the given transla-
tions again in order to produce all four HTER fac-
tors (insertions, deletions, substitutions and shifts)
and we train four regression models accordingly.
This way, each model can be optimized separately,
in order to better fit the particular error type, unaf-
fected by the noise that other error types may infer.
</bodyText>
<subsectionHeader confidence="0.964438">
2.2 Rounding of individual edit type
predictions
</subsectionHeader>
<bodyText confidence="0.987253428571429">
Due to the separate model per error type, it is pos-
sible to perform corrections on the predicted error
count for each error type, before the calculation of
the entire HTER score. This may be helpful, given
the observation that continuous statistical models
may produce a real number as prediction for the
count of edits, whereas the actual requirement is
an integer.
Here, we take this opportunity and test the hy-
pothesis that prediction of the overall HTER is bet-
ter, if the output of the four individual models is
rounded to the closest integer, before entered in
the HTER ratio.
2.3 More data by approximating minimal
post-edits
We investigate whether prediction performance
can be improved by adding further data. This rises
from the fact that the original number of sentences
is relatively small, given the amount of usable fea-
tures. Unfortunately, the amount of openly avail-
able resources of minimally post-edited transla-
tions are few, given the fact that this relies on a
costly manual process usually done by profession-
als.
Consequently, we add more training samples,
using reference translations of the source which
are not post-edited. In order to ensure that the ad-
ditional data still resemble minimally post-edited
translations as required for HTER, we include
those additional sentences only if they match spe-
cific similarity criteria. In particular, the trans-
lations are filtered, based on the amount of edits
between the MT output and the reference transla-
tion; sentences with an amount of edits above the
threshold are omitted.
</bodyText>
<sectionHeader confidence="0.997923" genericHeader="method">
3 Methods
</sectionHeader>
<subsectionHeader confidence="0.804295">
3.1 Machine Learning on a regression
problem
</subsectionHeader>
<bodyText confidence="0.999902866666667">
Fitting a statistical model in order to predict con-
tinuous values is clearly a regression problem. The
task takes place on a sentence level, given a set of
features describing the source and translation text,
and the respective edit score for the particular sen-
tence.
For this purpose we use Support Vector Regres-
sion - SVR (Basak et al., 2007), which uses lin-
ear learning machines in order to map a non-linear
function into a feature space induce by a high-
dimensional kernel. Similar to the baseline, the
RBF kernel was used, whose parameters where
adjusted via a grid search, cross-validated (10
folds) on all data that was available for each vari-
ation of the training.
</bodyText>
<subsectionHeader confidence="0.984847">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.999961666666667">
As explained, the statistical model predicts the
edit counts based on a set of features. Our anal-
ysis focuses on “black-box” features, which only
look superficially on the given text and the pro-
duced translation, without further knowledge on
how this translation was produced. These features
depend on several automatic extraction mecha-
nisms, mostly based on existing language process-
ing tools.
</bodyText>
<subsectionHeader confidence="0.619025">
3.2.1 Baseline features
</subsectionHeader>
<bodyText confidence="0.997211823529412">
A big set of features is adopted from the baseline
of the Shared Task description:
Language models: provide the smoothed n-
gram probability and the n-gram perplexity of the
sentence.
Source frequency: A set of eight features in-
cludes the percentage of uni-grams, bi-grams and
tri-grams of the processed sentence in frequency
quartiles 1 (lower frequency words) and 4 (higher
frequency words) in the source side of a parallel
corpus (Callison-Burch et al., 2012).
Count-based features include count and per-
centage of tokens, unknown words, punctuation
marks, numbers, tokens which do or do not con-
tain characters “a-z”; the absolute difference be-
tween number of tokens in source and target nor-
malized by source length, number of occurrences
</bodyText>
<page confidence="0.99705">
303
</page>
<bodyText confidence="0.989001666666667">
of the target word within the target hypothesis av-
eraged for all words in the hypothesis (type/token
ratio).
</bodyText>
<subsectionHeader confidence="0.655935">
3.2.2 Additional features
</subsectionHeader>
<bodyText confidence="0.999865608695652">
Additionally to the baseline features, the following
feature groups are considered:
Rule-based language correction is a result of
hand-written controlled language rules, that indi-
cate mistakes on several pre-defined error cate-
gories (Naber, 2003). We include the number of
errors of each category as a feature.
Parsing Features: We parse the text with a
PCFG grammar (Petrov et al., 2006) and we de-
rive the counts of all node labels (e.g. count of
verb phrases, noun phrases etc.), the parse log-
likelihood and the number of the n-best parse trees
generated (Avramidis et al., 2011). In order to re-
duce unnecessary noise, in some experiments we
separate a group of “basic” parsing labels, which
include only verb phrases, noun phrases, adjec-
tives and subordinate clauses.
Position statistics: This are derivatives of the
previous feature categories and focus on the po-
sition of unknown words, or node tree tags. For
each of them, we calculate the average position in-
dex over the sentence and the standard deviation of
these indices.
</bodyText>
<subsectionHeader confidence="0.978386">
3.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999277857142857">
All specific model parameters were tested with
cross validation with 10 equal folds on the train-
ing data. Cross validation is useful as it reduces
the possibility of overfitting, yet using the entire
amount of data.
The regression task is evaluated in terms of
Mean Average Error (MAE).
</bodyText>
<sectionHeader confidence="0.995073" genericHeader="method">
4 Experiment setup
</sectionHeader>
<subsectionHeader confidence="0.934626">
4.1 Implementation
</subsectionHeader>
<bodyText confidence="0.998956888888889">
The open source language tool1 is used to an-
notate source and target sentences with automati-
cally detected monolingual error tags. Language
model features are computed with the SRILM
toolkit (Stolcke, 2002) with an order of 5, based on
monolingual training material from Europarl v7.0
(Koehn, 2005) and News Commentary (Callison-
Burch et al., 2011). For the parsing parsing fea-
tures we used the Berkeley Parser (Petrov and
</bodyText>
<footnote confidence="0.921904">
1Open source at http://languagetool.org
</footnote>
<table confidence="0.9930852">
datasets feature set MAE
wmt14 baseline 0.142
wmt14 all features 0.143
wmt14,wmt13 baseline 0.140
wmt14,wmt13 all features 0.138
</table>
<tableCaption confidence="0.842446">
Table 1: Better scores are achieved when training
with both WMT14 and deduplicated WMT13 data
</tableCaption>
<bodyText confidence="0.93815525">
Klein, 2007) trained over an English and a Span-
ish treebank (Taul´e et al., 2008).2 Baseline fea-
tures are extracted using Quest and HTER edits
and scores are recalculated by modifying the orig-
inal TERp code. The annotation process is or-
ganised with the Ruffus library (Goodstadt, 2010)
and the learning algorithms are executed using the
Scikit Learn toolkit (Pedregosa et al., 2011).
</bodyText>
<subsectionHeader confidence="0.921318">
4.2 Data
</subsectionHeader>
<bodyText confidence="0.999980904761905">
In our effort to reproduce HTER in a higher gran-
ularity, we noticed that HTER scoring on the of-
ficial data was reversed: the calculation was per-
formed by using the MT output as reference and
the human post-edition as hypothesis. Therefore,
the denominator on the “official” scores is the
number of tokens on the MT output. This makes
the prediction even easier, as this number of tokens
is always known.
Apart from the data provided by the WMT14,
we include additional minimally post-edited data
from WMT13. It was observed that about 30% of
the WMT13 data already occurred in the WMT14
set. Since this would negatively affect the credibil-
ity of the cross-fold evaluation (section 3.3) and
also create duplicates, we filtered out incoming
sentences with a string match higher than 85% to
the existing ones.
The rest of the additional data (section 2.3)
was extracted from the test-sets of shared tasks
WMT2008-2011.
</bodyText>
<sectionHeader confidence="0.999985" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.999857">
5.1 Adding data from previous year
</subsectionHeader>
<bodyText confidence="0.999105833333333">
Adding deduplicated data from the HTER predic-
tion task of WMT13 (Section 4.2) leads to an im-
provement of about 0.004 of MAE for the best
feature-set, as it can be seen by comparing the re-
spective entries of the two horizontal blocks of Ta-
ble 1.
</bodyText>
<footnote confidence="0.9650085">
2although the Spanish grammar performed purely in this
case and was eliminated as a feature
</footnote>
<page confidence="0.998025">
304
</page>
<figure confidence="0.884674454545455">
feature set MAE
baseline (b) 0.140
b + language tool 0.141
b + source parse 0.141
b + parse pos 0.142
b + basic parse pos 0.139
b + parse count 0.139
b + low prob trigram pos 0.139
all without char count 0.139
all without lang. tool 0.139
all features 0.138
</figure>
<tableCaption confidence="0.8534634">
Table 2: Comparing models built with several dif-
ferent feature sets, including various combinations
of the features described in section 3.2. All models
trained on combination of WMT14 and WMT13
data
</tableCaption>
<subsectionHeader confidence="0.999014">
5.2 Feature sets
</subsectionHeader>
<bodyText confidence="0.999995615384615">
We tested separately several feature sets, addition-
ally to the baseline feature set and the feature set
containing all features. The feature sets tested
are based on the feature categories explained in
Section 3.2.2 and the results are seen in Table 2.
One can see that there is little improvement on the
MAE score, which is achieved best by using all
features.
Adding individual categories of features on the
baseline has little effect. Namely, the language
tool annotation, the source parse features and the
source and target parse positional features deteri-
orate the MAE score, when added to the baseline
features.
On the contrary, there is a small positive con-
tribution by using the position statistics of only
the “basic” parsing nodes (i.e. noun phrases, verb
phrases, adjectives and subordinate clauses). Sim-
ilarly positive is the effect of the count of parsed
node labels for source and target and the features
indicating the position of tri-grams with low prob-
ability (lower than the deviation of the mean). Al-
though language tool features deteriorate the score
of the baseline model when added, their absense
has a negative effect when compared to the full
feature set.
</bodyText>
<subsectionHeader confidence="0.998529">
5.3 Separate vs. single HTER predictor
</subsectionHeader>
<bodyText confidence="0.9559915">
Table 3 includes comparisons of models that test
the hypothesis mentioned in Section 2.1. For both
models trained over the baseline or with additional
features, the MAE score is higher (worse), when
</bodyText>
<table confidence="0.998925285714286">
features mode MAE std +/-
baseline single 0.140 0.012
baseline combined 0.148 0.018
baseline combined round 0.152 0.018
all single 0.138 0.009
all combined 0.160 0.019
all combined round 0.162 0.020
</table>
<tableCaption confidence="0.999283">
Table 3: The combination of 4 different estima-
</tableCaption>
<bodyText confidence="0.898798888888889">
tors (combined) does not bring any improvement,
when compared to the single HTER estimator.
Models trained on both WMT14 and WMT13 data
separate models are trained. This indicates that
our hypothesis does not hold, at least for the cur-
rent setting of learning method and feature sets.
Rounding up individual edit type predictions to the
closes integer, before the calculation of the HTER
ratio, deteriorates the scores even more.
</bodyText>
<subsectionHeader confidence="0.999575">
5.4 Effect of adding non-postedited sentences
</subsectionHeader>
<bodyText confidence="0.999980866666667">
In Table 4 we can see that adding more data, which
are not minimally post-edited (but normal refer-
ences), does not contribute to a better model, even
if we limit the number of edits. The lowest MAE
is 0.176, when compared to the one of our best
model which is 0.138.
The best score when additional sentences are
imported, is achieved by allowing sentences that
have between up to edits, and particularly up to 3
substitutions and up to 1 deletion. Increasing the
number of edits on more than 4, leads to a further
deterioration of the model. One can also see that
adding training instances where MT outputs did
not require any edit, also yields scores worse than
the baseline.
</bodyText>
<sectionHeader confidence="0.988454" genericHeader="conclusions">
6 Conclusion and further work
</sectionHeader>
<bodyText confidence="0.997609636363637">
In our submission, we process the test set with the
model using all features (Table 2). We addition-
ally submit the model trained with additional fil-
tered sentences, as indicated in the second row of
Table 4.
One of the basic hypothesis of this experiment,
that each edit type can better be learned individu-
ally, was not confirmed given these data and set-
tings. Further work could include more focus on
the individual models and more elaborating on
features that may be specific for each error type.
</bodyText>
<page confidence="0.996041">
305
</page>
<table confidence="0.99879275">
del ins sub shifts total add. sentences MAE std+/-
0 0 0 0 0 275 0.177 0.049
1 0 3 0 4 480 0.176 0.040
1 0 2 0 3 433 0.177 0.040
0 0 4 0 4 432 0.177 0.040
2 1 0 0 3 296 0.177 0.048
2 0 3 0 5 530 0.178 0.038
4 0 2 0 6 485 0.178 0.041
4 4 0 0 8 310 0.178 0.046
2 1 0 1 4 309 0.178 0.047
1 0 5 0 6 558 0.179 0.039
1 4 5 0 10 1019 0.200 0.031
</table>
<tableCaption confidence="0.999631">
Table 4: Indicative MAE scores achieved by adding filtered not minimally post-edited WMT translation
</tableCaption>
<sectionHeader confidence="0.995414" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999761194444445">
Eleftherios Avramidis, Maja Popovi´c, David Vilar, and
Aljoscha Burchardt. 2011. Evaluate with Confi-
dence Estimation : Machine ranking of translation
outputs using grammatical features. In Proceedings
of the Sixth Workshop on Statistical Machine Trans-
lation, pages 65–70, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Debasish Basak, Srimanta Pal, and Dipak Chandra Pa-
tranabis. 2007. Support vector regression. Neu-
ral Information Processing-Letters and Reviews,
11(10):203–224.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
Workshop on Statistical Machine Translation. In
Proceedings of the Sixth Workshop on Statisti-
cal Machine Translation, pages 22–64, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10–51, Montr´eal, Canada, June. Association for
Computational Linguistics.
Leo Goodstadt. 2010. Ruffus: a lightweight Python
library for computational pipelines. Bioinformatics,
26(21):2778–2779.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. Proceedings of the
tenth Machine Translation Summit, 5:79–86.
Daniel Naber. 2003. A rule-based style and gram-
mar checker. Technical report, Bielefeld University,
Bielefeld, Germany.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and ´Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in python. Journal
of Machine Learning Research, 12:2825–2830.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
the 2007 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, Rochester, New York. Association for
Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433–440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Matthew Snover, B Dorr, Richard Schwartz, L Micci-
ulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of Association for Machine Translation
in the Americas, pages 223–231.
Andreas Stolcke. 2002. SRILM – An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing, pages 901–904. ISCA, September.
Mariona Taul´e, Ant`onia Marti, and Marta Recasens.
2008. AnCora: Multilevel Annotated Corpora for
Catalan and Spanish. In Proceedings of the Sixth
International Conference on Language Resources
and Evaluation (LREC’08), Marrakech, Morocco,
May. European Language Resources Association
(ELRA).
</reference>
<page confidence="0.999211">
306
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.347705">
<title confidence="0.7639498">Efforts on Machine Learning Human-mediated Translation Edit Rate Eleftherios German Research Center for Artificial Intelligence Language Technology</title>
<author confidence="0.388498">Alt Moabit c</author>
<author confidence="0.388498">Berlin</author>
<email confidence="0.99829">eleftherios.avramidis@dfki.de</email>
<abstract confidence="0.999327695652174">In this paper we describe experiments on predicting HTER, as part of our submission in the Shared Task on Quality Estimation, in the frame of the 9th Workshop on Statistical Machine Translation. In our experiment we check whether it is possible to achieve better HTER prediction by training four individual regression models for each one of the edit types (deletions, insertions, substitutions, shifts), however no improvements were yielded. We also had no improvements when investigating the possibility of adding more data from other non-minimally post-edited and freely translated datasets. Best HTER prediction was achieved by adding deduplicated WMT13 data and additional features such as (a) rule-based language corrections (language tool) (b) PCFG parsing statistics and count of tree labels (c) position statistics of parsing labels (d) position statistics of tri-grams with low probability.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eleftherios Avramidis</author>
<author>Maja Popovi´c</author>
<author>David Vilar</author>
<author>Aljoscha Burchardt</author>
</authors>
<title>Evaluate with Confidence Estimation : Machine ranking of translation outputs using grammatical features.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>65--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>Avramidis, Popovi´c, Vilar, Burchardt, 2011</marker>
<rawString>Eleftherios Avramidis, Maja Popovi´c, David Vilar, and Aljoscha Burchardt. 2011. Evaluate with Confidence Estimation : Machine ranking of translation outputs using grammatical features. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 65–70, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Debasish Basak</author>
</authors>
<title>Srimanta Pal, and Dipak Chandra Patranabis.</title>
<date>2007</date>
<booktitle>Neural Information Processing-Letters and Reviews,</booktitle>
<pages>11--10</pages>
<marker>Basak, 2007</marker>
<rawString>Debasish Basak, Srimanta Pal, and Dipak Chandra Patranabis. 2007. Support vector regression. Neural Information Processing-Letters and Reviews, 11(10):203–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<date>2011</date>
<booktitle>Findings of the 2011 Workshop on Statistical Machine Translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>22--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 Workshop on Statistical Machine Translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22–64, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="7642" citStr="Callison-Burch et al., 2012" startWordPosition="1243" endWordPosition="1246">translation was produced. These features depend on several automatic extraction mechanisms, mostly based on existing language processing tools. 3.2.1 Baseline features A big set of features is adopted from the baseline of the Shared Task description: Language models: provide the smoothed ngram probability and the n-gram perplexity of the sentence. Source frequency: A set of eight features includes the percentage of uni-grams, bi-grams and tri-grams of the processed sentence in frequency quartiles 1 (lower frequency words) and 4 (higher frequency words) in the source side of a parallel corpus (Callison-Burch et al., 2012). Count-based features include count and percentage of tokens, unknown words, punctuation marks, numbers, tokens which do or do not contain characters “a-z”; the absolute difference between number of tokens in source and target normalized by source length, number of occurrences 303 of the target word within the target hypothesis averaged for all words in the hypothesis (type/token ratio). 3.2.2 Additional features Additionally to the baseline features, the following feature groups are considered: Rule-based language correction is a result of hand-written controlled language rules, that indicat</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Goodstadt</author>
</authors>
<title>Ruffus: a lightweight Python library for computational pipelines.</title>
<date>2010</date>
<journal>Bioinformatics,</journal>
<volume>26</volume>
<issue>21</issue>
<contexts>
<context position="10402" citStr="Goodstadt, 2010" startWordPosition="1688" endWordPosition="1689">he parsing parsing features we used the Berkeley Parser (Petrov and 1Open source at http://languagetool.org datasets feature set MAE wmt14 baseline 0.142 wmt14 all features 0.143 wmt14,wmt13 baseline 0.140 wmt14,wmt13 all features 0.138 Table 1: Better scores are achieved when training with both WMT14 and deduplicated WMT13 data Klein, 2007) trained over an English and a Spanish treebank (Taul´e et al., 2008).2 Baseline features are extracted using Quest and HTER edits and scores are recalculated by modifying the original TERp code. The annotation process is organised with the Ruffus library (Goodstadt, 2010) and the learning algorithms are executed using the Scikit Learn toolkit (Pedregosa et al., 2011). 4.2 Data In our effort to reproduce HTER in a higher granularity, we noticed that HTER scoring on the official data was reversed: the calculation was performed by using the MT output as reference and the human post-edition as hypothesis. Therefore, the denominator on the “official” scores is the number of tokens on the MT output. This makes the prediction even easier, as this number of tokens is always known. Apart from the data provided by the WMT14, we include additional minimally post-edited d</context>
</contexts>
<marker>Goodstadt, 2010</marker>
<rawString>Leo Goodstadt. 2010. Ruffus: a lightweight Python library for computational pipelines. Bioinformatics, 26(21):2778–2779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>Proceedings of the tenth Machine Translation Summit,</booktitle>
<pages>5--79</pages>
<contexts>
<context position="9730" citStr="Koehn, 2005" startWordPosition="1582" endWordPosition="1583">c model parameters were tested with cross validation with 10 equal folds on the training data. Cross validation is useful as it reduces the possibility of overfitting, yet using the entire amount of data. The regression task is evaluated in terms of Mean Average Error (MAE). 4 Experiment setup 4.1 Implementation The open source language tool1 is used to annotate source and target sentences with automatically detected monolingual error tags. Language model features are computed with the SRILM toolkit (Stolcke, 2002) with an order of 5, based on monolingual training material from Europarl v7.0 (Koehn, 2005) and News Commentary (CallisonBurch et al., 2011). For the parsing parsing features we used the Berkeley Parser (Petrov and 1Open source at http://languagetool.org datasets feature set MAE wmt14 baseline 0.142 wmt14 all features 0.143 wmt14,wmt13 baseline 0.140 wmt14,wmt13 all features 0.138 Table 1: Better scores are achieved when training with both WMT14 and deduplicated WMT13 data Klein, 2007) trained over an English and a Spanish treebank (Taul´e et al., 2008).2 Baseline features are extracted using Quest and HTER edits and scores are recalculated by modifying the original TERp code. The a</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. Proceedings of the tenth Machine Translation Summit, 5:79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Naber</author>
</authors>
<title>A rule-based style and grammar checker.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Bielefeld University,</institution>
<location>Bielefeld, Germany.</location>
<contexts>
<context position="8306" citStr="Naber, 2003" startWordPosition="1345" endWordPosition="1346">of tokens, unknown words, punctuation marks, numbers, tokens which do or do not contain characters “a-z”; the absolute difference between number of tokens in source and target normalized by source length, number of occurrences 303 of the target word within the target hypothesis averaged for all words in the hypothesis (type/token ratio). 3.2.2 Additional features Additionally to the baseline features, the following feature groups are considered: Rule-based language correction is a result of hand-written controlled language rules, that indicate mistakes on several pre-defined error categories (Naber, 2003). We include the number of errors of each category as a feature. Parsing Features: We parse the text with a PCFG grammar (Petrov et al., 2006) and we derive the counts of all node labels (e.g. count of verb phrases, noun phrases etc.), the parse loglikelihood and the number of the n-best parse trees generated (Avramidis et al., 2011). In order to reduce unnecessary noise, in some experiments we separate a group of “basic” parsing labels, which include only verb phrases, noun phrases, adjectives and subordinate clauses. Position statistics: This are derivatives of the previous feature categorie</context>
</contexts>
<marker>Naber, 2003</marker>
<rawString>Daniel Naber. 2003. A rule-based style and grammar checker. Technical report, Bielefeld University, Bielefeld, Germany.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Fabian Pedregosa</author>
</authors>
<title>Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer,</title>
<location>Ron</location>
<marker>Pedregosa, </marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Dubourg Weiss</author>
<author>Jake Vanderplas</author>
<author>Alexandre Passos</author>
<author>David Cournapeau</author>
<author>Matthieu Brucher</author>
<author>Matthieu Perrot</author>
<author>´Edouard Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<marker>Weiss, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and ´Edouard Duchesnay. 2011. Scikit-learn: Machine learning in python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York.</location>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of the 2007 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Rochester, New York. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="8448" citStr="Petrov et al., 2006" startWordPosition="1369" endWordPosition="1372">een number of tokens in source and target normalized by source length, number of occurrences 303 of the target word within the target hypothesis averaged for all words in the hypothesis (type/token ratio). 3.2.2 Additional features Additionally to the baseline features, the following feature groups are considered: Rule-based language correction is a result of hand-written controlled language rules, that indicate mistakes on several pre-defined error categories (Naber, 2003). We include the number of errors of each category as a feature. Parsing Features: We parse the text with a PCFG grammar (Petrov et al., 2006) and we derive the counts of all node labels (e.g. count of verb phrases, noun phrases etc.), the parse loglikelihood and the number of the n-best parse trees generated (Avramidis et al., 2011). In order to reduce unnecessary noise, in some experiments we separate a group of “basic” parsing labels, which include only verb phrases, noun phrases, adjectives and subordinate clauses. Position statistics: This are derivatives of the previous feature categories and focus on the position of unknown words, or node tree tags. For each of them, we calculate the average position index over the sentence a</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>B Dorr</author>
<author>Richard Schwartz</author>
<author>L Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="1780" citStr="Snover et al., 2006" startWordPosition="273" endWordPosition="276">ction As Machine Translation (MT) gets integrated into regular translation workflows, its use as base for post-editing is radically increased. As a result, there is a great demand for methods that can automatically assess the MT outcome and ensure that it is useful for the translator and can lead to more productive translation work. Although many agree that the quality of the MT output itself is not adequate for the professional standards, there has not yet been a widelyaccepted way to measure its quality on par with human translations. One such metric, the Human Translation Edit Rate (HTER) (Snover et al., 2006), is the focus of the current submission. HTER is highly relevant to the need of adapting MT to the needs of translators, as it aims to measure how far it is from an acceptable equivalent translation done by humans. HTER is used here in the frame of Quality Estimation, i.e. having the goal of being able to predict the post-editing effort in a real case environment, right before the translation is given to the user, without real access to the correct translation. For this purpose the text of the source and the produced translation is analyzed by automatic tools in order to infer indications (nu</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, B Dorr, Richard Schwartz, L Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the Seventh International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<publisher>ISCA,</publisher>
<contexts>
<context position="9638" citStr="Stolcke, 2002" startWordPosition="1567" endWordPosition="1568">ndex over the sentence and the standard deviation of these indices. 3.3 Evaluation All specific model parameters were tested with cross validation with 10 equal folds on the training data. Cross validation is useful as it reduces the possibility of overfitting, yet using the entire amount of data. The regression task is evaluated in terms of Mean Average Error (MAE). 4 Experiment setup 4.1 Implementation The open source language tool1 is used to annotate source and target sentences with automatically detected monolingual error tags. Language model features are computed with the SRILM toolkit (Stolcke, 2002) with an order of 5, based on monolingual training material from Europarl v7.0 (Koehn, 2005) and News Commentary (CallisonBurch et al., 2011). For the parsing parsing features we used the Berkeley Parser (Petrov and 1Open source at http://languagetool.org datasets feature set MAE wmt14 baseline 0.142 wmt14 all features 0.143 wmt14,wmt13 baseline 0.140 wmt14,wmt13 all features 0.138 Table 1: Better scores are achieved when training with both WMT14 and deduplicated WMT13 data Klein, 2007) trained over an English and a Spanish treebank (Taul´e et al., 2008).2 Baseline features are extracted using</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Proceedings of the Seventh International Conference on Spoken Language Processing, pages 901–904. ISCA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taul´e</author>
<author>Ant`onia Marti</author>
<author>Marta Recasens</author>
</authors>
<title>AnCora: Multilevel Annotated Corpora for Catalan and Spanish.</title>
<date>2008</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech, Morocco,</location>
<marker>Taul´e, Marti, Recasens, 2008</marker>
<rawString>Mariona Taul´e, Ant`onia Marti, and Marta Recasens. 2008. AnCora: Multilevel Annotated Corpora for Catalan and Spanish. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08), Marrakech, Morocco, May. European Language Resources Association (ELRA).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
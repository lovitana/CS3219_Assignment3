<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001418">
<title confidence="0.9704815">
Similarity-Based Non-Scorable Response Detection for Automated Speech
Scoring
</title>
<author confidence="0.882004">
Su-Youn Yoon
</author>
<affiliation confidence="0.785171">
Educational Testing Service
</affiliation>
<address confidence="0.870731">
Princeton, NJ, USA
</address>
<email confidence="0.997728">
syoon@ets.org
</email>
<sectionHeader confidence="0.993869" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999939413793103">
This study provides a method that iden-
tifies problematic responses which make
automated speech scoring difficult. When
automated scoring is used in the context
of a high stakes language proficiency as-
sessment, for which the scores are used to
make consequential decisions, some test
takers may have an incentive to try to game
the system in order to artificially inflate
their scores. Since many automated pro-
ficiency scoring systems use fluency fea-
tures such as speaking rate as one of the
important features, students may engage
in strategies designed to manipulate their
speaking rate as measured by the system.
In order to address this issue, we de-
veloped a method which filters out non-
scorable responses based on text similar-
ity measures. Given a test response, the
method generated a set of features which
calculated the topic similarity with the
prompt question or the sample responses
including relevant content. Next, an au-
tomated filter which identified these prob-
lematic responses was implemented us-
ing the similarity features. This filter im-
proved the performance of the baseline
filter in identifying responses with topic
problems.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996527625">
In spoken language proficiency assessment, some
responses may include sub-optimal characteristics
which make it difficult for the automated scor-
ing system to provide a valid score. For instance,
some test takers may try to game the system by
speaking in their native languages or by citing
memorized responses for unrelated topics. Oth-
ers may repeat questions or part of questions with
</bodyText>
<note confidence="0.633472333333333">
Shasha Xie
Microsoft
Sunnyvale, CA, USA
</note>
<email confidence="0.813775">
shxie@microsoft.com
</email>
<bodyText confidence="0.999352243902439">
modifications instead of generating his/her own
response. Hereafter, we call these problematic
responses non-scorable (NS) responses. By us-
ing these strategies, test takers can generate flu-
ent speech, and the automated proficiency scoring
system, which utilizes fluency as one of the im-
portant factors, may assign a high score. In or-
der to address this issue, the automated proficiency
scoring system in this study used a two-step ap-
proach: these problematic responses were filtered
out by a “filtering model,” and only the remaining
responses were scored using the automated scor-
ing model. By filtering out these responses, the
robustness of the automated scoring system can be
improved.
The proportion of NS responses, in the assess-
ment of which the responses are scored by human
raters, are likely to be low. For instance, the pro-
portion of NS responses in the international En-
glish language assessment used in this study was
2%. Despite this low proportion, it is a serious
problem which has a strong impact on the validity
of the test. In addition, the likelihood of students
engaging in gaming strategies may increase with
the use of automated scoring. Therefore, an au-
tomated filtering model with a high accuracy is a
necessary step to use the automated scoring sys-
tem as a sole rater.
Both off-topic and copy responses have topic-
related problems, although they are at the two ex-
tremes in the degree of similarity. Focusing on
the intermediate levels of similarity, Metzler et al.
(2005) presented a hierarchy of five similarity lev-
els: unrelated, on the general topic, on the spe-
cific topic, same facts, and copied. In the auto-
mated scoring of spontaneous speech, responses
that fell into unrelated can be considered as off-
topic, while the ones that fell into copied can be
considered as repetition or plagiarism. Follow-
ing this approach, we developed a non-scorable
response identification method utilizing similar-
</bodyText>
<page confidence="0.986711">
116
</page>
<affiliation confidence="0.348388">
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications, pages 116–123,
Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics
</affiliation>
<figureCaption confidence="0.9892795">
Figure 1: A diagram of the overall architecture of
our method.
</figureCaption>
<bodyText confidence="0.98319378125">
ity measures. We will show that this similarity
based method is highly efficient in identifying off-
topic or repetition responses. Furthermore, we
will show that the method can effectively detect
NS responses that are not directly related to the
topicality issue (e.g, non-English responses).
Figure 1 shows the overall architecture of our
method including the automated speech profi-
ciency scoring system. For a given spoken re-
sponse, the system performs speech processing in-
cluding speech recognition and generates a word
hypotheses and time stamps. In addition, the sys-
tem computes pitch and power; the system calcu-
lates descriptive statistics such as the mean and
standard deviation of pitch and power at both the
word level and response level. Given the word hy-
potheses and descriptive features of pitch/power,
it derives features for automated proficiency scor-
ing. In addition, the similarity features are gener-
ated based on the word hypotheses and topic mod-
els. Finally, given both sets of features, the filter-
ing model filters out non-scorable responses, and
the remainder of the responses are scored using a
scoring model. A detailed description of the sys-
tem is available from Zechner et al. (2009). In this
study, we will only focus on the filtering model.
This paper will proceed as follows: we first re-
view previous studies in section 2, then describe
the data in section 3, and present the method and
experiment set-up in sections 4 and 5. The results
and discussion are presented in section 6, and the
conclusions are presented in section 7.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999871689655173">
Filtering of NS responses for automated speech
scoring has been rarely recognized. Only a few
pieces of research have focused on this task,
and most studies have targeted highly restricted
speech. van Doremalen et al. (2009) and Lo et
al. (2010) used normalized confidence scores of
a speech recognizer in recasting speech. They
identified non-scorable responses with promising
performances (equal error rates ranged from 10
to 20%). Cheng and Shen (2011) extended these
studies and combined an acoustic model score, a
language model score, and a garbage model score
with confidence scores. They applied this new fil-
ter to less constrained items (e.g., picture descrip-
tion) and identified off-topic responses with an ac-
curacy rate of 90% with a false positive rate of 5%.
Although normalized confidence scores
achieved promising performances in restricted
speech, they may not be appropriate for the items
that elicit unconstrained spontaneous speech.
Low confidence scores signal the use of words
or phrases not covered by the language model
(LM) and this is strongly associated with off-topic
responses in restricted speech in which the target
sentence is given. However, in spontaneous
speech, this is not trivial; it may be associated
with not only off-topic speech but also mismatch
between the LM and speech input due to the low
coverage of the LM. Due to the latter case, the
decision based on the confidence score may not
be effective in measuring topic similarity.
The topic similarity between two documents
has been frequently modeled by relative-frequency
measures (Hoad and Zobel, 2003; Shivakumar and
Garcia-Molina, 1995), document fingerprinting
(Brin et al., 1995; Shivakumar and Garcia-Molina,
1995; Shivakumar and Garcia-Molina, 1996)), and
query based information retrieval methods using
vector space models or language model (Sander-
son, 1997; Hoad and Zobel, 2003).
Document similarity measures have been ap-
plied in automated scoring. Foltz et al. (1999)
evaluated the content of written essays using latent
semantic analysis (LSA) by comparing the test es-
says with essays of known quality in regard to their
degree of conceptual relevance and the amount of
relevant content. In another approach, the lexical
content of an essay was evaluated by comparing
the words contained in each essay to the words
found in a sample of essays from each score cat-
egory (Attali and Burstein, 2006). More recently,
Xie et al. (2012) used a similar approach in au-
tomated speech scoring; they measured the sim-
ilarity using three similarity measures, including
a lexical matching method (Vector Space Model)
and two semantic similarity measures (Latent Se-
mantic Analysis and Pointwise Mutual Informa-
tion). They showed moderately high correlations
</bodyText>
<page confidence="0.988415">
117
</page>
<bodyText confidence="0.999987166666667">
between the similarity features and human profi-
ciency scores on even the output of an automatic
speech recognition system. Similarity measures
have also been used in off-topic detection for non-
native speakers’ essays. Higgins et al. (2006) cal-
culated overlaps between the question and content
words from the essay and obtained an error rate of
10%.
Given the promising performance in both auto-
mated scoring and off-topic essay detection, we
will expand these similarity measures in NS re-
sponse detection for speech scoring.
</bodyText>
<sectionHeader confidence="0.996491" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999942156862745">
In this study, we used a collection of responses
from an international English language assess-
ment. The assessment was composed of items in
which speakers were prompted to provide sponta-
neous speech.
Approximately 48,000 responses from 8,000
non-native speakers were collected and used for
training the automated speech recognizer (ASR
set). Among 24 items in the ASR set, four items
were randomly selected. For these items, a total
of 11,560 responses were collected and used for
the training and evaluation of filtering model (FM
set). Due to the extremely skewed distribution of
NS responses (2% in the ASR set), it was not easy
to train and evaluate the filtering model. In or-
der to address this issue, we modified the distribu-
tion of NS responses in the FM set. Initially, we
collected 90,000 responses including 1,560 NS re-
sponses. While maintaining all NS responses, we
downsampled the scorable responses in the FM set
to include 10,000 responses. Finally, the propor-
tion of NS responses was 6 times higher in FM
set (13%) than ASR set. This artificial increase of
the NS responses reduces the current problem of
the skewed NS distribution and may make the task
easier. However, the likelihood of students engag-
ing in gaming strategies may increase with the use
of automated scoring, and this increased NS dis-
tribution may be close to this situation.
Each response was rated by trained human
raters using a 4-point scoring scale, where 1 indi-
cated a low speaking proficiency and 4 indicated a
high speaking proficiency. The raters also labeled
responses as NS, when appropriate. NS responses
are defined as responses that cannot be given a
score according to the rubrics of the four-point
scale. NS responses were responses with tech-
nical difficulties (TDs) that obscured the content
of the responses or responses that would receive
a score of 0 due to participants’ inappropriate be-
haviors. The speakers, item information, and dis-
tribution of proficiency scores are presented in Ta-
ble 1. There was no overlap in the sets of speakers
in the ASR and FM sets.
In addition, 1,560 NS responses from the FM
set were further classified into six types by two
raters with backgrounds in linguistics using the
rubrics presented in Table 2. This annotation was
used for the purpose of analysis: to identify the
frequent types of NS responses and prioritize the
research effort.
</bodyText>
<table confidence="0.990017388888889">
Type Proportion Description
in total
NSs
NR 73% No response. Test taker doesn‘t
speak.
OR 16% Off-topic responses. The re-
sponse is not related to the
prompt.
TR 5% Generic responses. The re-
sponse only include filler words
or generic responses such as, “I
don‘t know, it is too difficult to
answer, well”, etc.
RE 4% Question copy. Full or partial
repetition of question.
NE 1% Non-English. Responses is in a
language other than English.
OT 1% Others
</table>
<tableCaption confidence="0.999273">
Table 2: Types of zero responses and proportions
</tableCaption>
<bodyText confidence="0.996789285714286">
Some responses belonged to more than one
type, and this increased complexity of the anno-
tation task. For instance, one response was com-
prised of a question copy and generic sentences,
while another response was comprised of a ques-
tion copy and off-topic sentences. An example of
this type was presented in Table 3. This was a re-
sponse for the question “Talk about an interesting
book that you read recently. Explain why it was
interesting).”
For these responses, annotators first segmented
them into sentences and assigned the type that was
most dominant.
Each rater annotated approximately 1,000 re-
sponses, and 586 responses were rated by both
&apos;In order to not reveal the real test question administered
in the operational test, we invented this question. Based on
the question, we also modified a sample response; the ques-
tion copy part was changed to avoid disclosure of the test
question, but the other part remained the same as the original
response.
</bodyText>
<page confidence="0.983768">
118
</page>
<table confidence="0.999979">
Data set Num. responses Num. speakers Num. items Average score Score distribution
NS 1 2 3 4
ASR 48,000 8,000 24 2.63 773 1953 16834 23106 5334
2% 4% 35% 48% 11%
FM 11,560 11,390 4 2.15 1560 734 4328 4263 675
13% 6% 37% 37% 6%
</table>
<tableCaption confidence="0.983094">
Table 1: Data size and score distribution
</tableCaption>
<table confidence="0.999673846153846">
Sentence Type
Well in my opinion are the inter- RE
esting books that I read recently
is.
Talking about a interesting book. RE
One interesting book oh God in- RE
teresting book that had read re-
cently.
Oh my God. TR
I really don’t know how to an- TR
swer this question.
Well I don’t know. TR
Sorry. TR
</table>
<tableCaption confidence="0.774225">
Table 3: Manual transcription of complex-type re-
sponse
</tableCaption>
<bodyText confidence="0.999190833333333">
raters. The Cohen’s kappa between two raters was
0.76. Among five different NS responses, non-
response was the most frequent type (73%), fol-
lowed by off-topic (16%). The combination of the
two types was approximately 90% of the entire NS
responses.
</bodyText>
<sectionHeader confidence="0.996314" genericHeader="method">
4 Method
</sectionHeader>
<bodyText confidence="0.999986">
In this study, we generated two different types of
features. First, we developed similarity features
(both chunk-based and response-based) to identify
the responses with problems in topicality. Sec-
ondly, we generated acoustic, fluency, and ASR-
confidence features using a state-of-art automated
speech scoring system. Finally, using both feature
sets, classifiers were trained to make a binary dis-
tinction of NS response vs. scorable response.
</bodyText>
<subsectionHeader confidence="0.995873">
4.1 Chunk-based similarity features
</subsectionHeader>
<bodyText confidence="0.999908421052632">
Some responses in this study included more than
two different types of the topicality problems. For
instance, the first three sentences in Table 3 be-
longed to the “copied” category, while the other
sentences fell into “unrelated”. If the similarity
features were calculated based on the entire re-
sponse, the feature values may fall into neither
the “copied” nor “unrelated” range because of the
trade-off between the two types at two extremes.
In order to address this issue, we calculated chunk-
based similarity features similar to Metzler et al.
(2005)’s sentence-based features.
First, the response was split into the chunks
which were surrounded by long silences with du-
rations longer than 0.6 sec. For each chunk,
the proportion of word overlap with the question
(WOL) was calculated based on the formula (1).
Next, chunks with a WOL higher than 0.5 were
considered as question copies.
</bodyText>
<equation confidence="0.9929385">
WOL = |Sn�|
|S|
</equation>
<bodyText confidence="0.91529025">
where S is a response and Q is a question,
|5 n Q |is the number of word types that appear
both in S and Q,
|5 |is the number of word types in S
(1)
Finally, the following three features were de-
rived for each response based on the chunk-based
WOL.
</bodyText>
<listItem confidence="0.999138875">
• numwds: the number of word tokens after re-
moving question copies, fillers, and typical
generic sentences2;
• copyR: the proportion of question copies in
the response in terms of number of word to-
kens;
• meanWOL: the mean of WOLs for all chunks
in the response.
</listItem>
<subsectionHeader confidence="0.992726">
4.2 Response-based similarity features
</subsectionHeader>
<bodyText confidence="0.9997864">
We implemented three features based on a vector
space model (VSM) using cosine similarity and
term frequency-inverse document frequency (tf-
idf) weighting to estimate the topic relevance at
the response-level.
</bodyText>
<tableCaption confidence="0.478363">
2Five sentences “it is too difficult”, “thank you”, “I don’t
know”, “I am sorry”, and “oh my God” were stored as typical
sentences and removed from responses
</tableCaption>
<page confidence="0.998879">
119
</page>
<bodyText confidence="0.999644">
Since the topics of each question were differ-
ent from each other, we trained a VSM for each
question separately. For the four items in the
FM set, we selected a total of 485 responses (125
responses per item) from the ASR set for topic
model training. Assuming that the responses with
the highest proficiency scores contain the most di-
verse and appropriate words related to the topic,
we only selected responses with a score of 4.
We obtained the manual transcriptions of the re-
sponses, and all responses about the same ques-
tion were converted into a single vector. In this
study, the term was a unigram word, and the doc-
ument was the response. idf was trained from the
entire set of 48,000 responses in the ASR training
partition, while tf was trained from the question-
specific topic model training set.
In addition to the response-based VSM, we
trained a question-based VSM. Each question was
composed of two sentences. Each question was
converted into a single vector, and a total of four
VSMs were trained. idf was trained in the same
way as the response-based VSMs, while tf was
trained only using the question sentences.
Using these two different types of VSMs, the
following three features were generated for each
response.
</bodyText>
<listItem confidence="0.888147217391304">
• sampleCosine: a similarity score based on
the response-based VSM. Assuming that two
documents with the same topic shared com-
mon words, it measured the similarity in the
words used in a test response and the sample
responses. The feature was implemented to
identify off-topic responses (OR);
• qCosine: a similarity score based on the
question-based VSM. It measured the simi-
larity between a test response and its ques-
tion. The feature was implemented to iden-
tify both off-topic responses (OR) and ques-
tion copy responses (RE); a low score is
highly likely to be an off-topic response,
while a high score signals a full or partial
copy;
• meanIDF: mean of idfs for all word tokens
in the response. Generic responses (TR) tend
to include many high frequency words such
as articles and pronouns, and the mean idf
value of these responses may be low.
4.3 Features from the automated speech
scoring system
</listItem>
<bodyText confidence="0.9999836">
A total of 61 features (hereafter, A/S features)
were generated using a state-of-the-art automated
speech scoring system. A detailed description
of the system is available from (Jeon and Yoon,
2012). Among these features, many features were
conceptually similar but based on different nor-
malization methods, and they showed a strong
inter-correlation. For this study, 30 features were
selected and classified into three groups according
to their characteristics: acoustic features, fluency
features, and ASR-confidence features.
The acoustic features were related to power,
pitch, and MFCC. First, power, pitch and
MFCC were extracted at each frame using
Praat (Boersma, 2002). Next, we generated
response-level features from these frame-level fea-
tures by calculating mean and variation. These
features captured the overall distribution of energy
and voiced regions in a speaker’s response. These
features are relevant since NS responses may have
an abnormal distribution in energy. For instance,
non-responses contain very low energy. In order
to detect these abnormalities in the speech signal,
pitch and power related features were calculated.
The fluency features measure the length of a re-
sponse in terms of duration and number of words.
In addition, this group contains features related
to speaking rate and silences, such as mean du-
ration and number of silences. In particular, these
features are effective in identifying non-responses
which contain zero or only a few words.
The ASR-confidence group contains features
predicting the performance of the speech recog-
nizer. Low confidence scores signal low speech
recognition accuracy.
</bodyText>
<subsectionHeader confidence="0.999828">
4.4 Model training
</subsectionHeader>
<bodyText confidence="0.999929857142857">
Three filtering models were trained to investigate
the impact of each feature group: a filtering model
using similarity features (hereafter, the Similarity-
filter), a filtering model using A/S features (here-
after, the A/S-filter), and a filtering model using a
combination of the two groups of features (here-
after, the Combined-filter).
</bodyText>
<sectionHeader confidence="0.998999" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.997757666666667">
An HMM-based speech recognizer was trained us-
ing the ASR set. A gender independent triphone
acoustic model and a combination of bigram, tri-
</bodyText>
<page confidence="0.988253">
120
</page>
<bodyText confidence="0.999779642857143">
gram, and four-gram language models were used.
A word error rate (WER) of 27% on the held-out
test dataset was observed.
For each response in the FM set, the word
hypotheses was generated using this recognizer.
From this ASR-based transcription, the six simi-
larity features were generated. In addition, the 30
A/S features described in 4.3 were generated.
Using these two sets of features, filtering mod-
els were trained using the Support Vector Ma-
chine algorithm (SVM) with the RBF kernel of
the WEKA machine-learning toolkit (Hall et al.,
2009). A 10 fold cross-validation was conducted
using the FM dataset.
</bodyText>
<sectionHeader confidence="0.999096" genericHeader="evaluation">
6 Results and discussion
</sectionHeader>
<bodyText confidence="0.976360346938776">
First, we will report the performance for the sub-
set only topic-related NS responses. The sim-
ilarity features were designed to detect NS re-
sponses with topicality issues, but the majority in
the FM set were non-response (73%). The topic-
related NS responses (off-topic responses, generic
responses, and question copy responses) were only
25%. In the entire set, the advantage of the simi-
larity features over the A/S features might not be
salient due to the high proportion of non-response.
In order to investigate the performance of the sim-
ilarity features in the topic related NS responses,
we excluded all responses other than ‘OR’, ‘TR’,
and ‘RE’ from the FM set and conducted a 10 fold
cross-validation.
Table 4 presents the average of the 10 fold
cross-validation results in this subset. In this set,
the total number of NS responses is 314, and the
accuracy of the majority voting (to classify all re-
sponses as scorable responses) is 0.962.
acc. prec. recall fscore
Similarity- 0.975 0.731 0.548 0.626
filter
A/S-filter 0.971 0.767 0.341 0.472
Combined- 0.977 0.780 0.566 0.656
filter
Table 4: Performance of filters in topic-related NS
detection
Not surprisingly, the Similarity-filter outper-
formed the A/S-filter: the F-score was approxi-
mately 0.63 which was 0.15 higher than that of
the A/S-filter in absolute value. The lack of fea-
tures specialized for detection of topic abnormal-
ity resulted in the low recall of the A/S-filter. The
combination of the two features achieved a slight
improvement: the F-score was 0.66 and it was 0.03
higher than the Similarity-filter.
In Metzler et al. (2005)’s study, the system us-
ing both sentence-based features and document-
based features did not achieve further improve-
ment over the system based on the document-
based features alone. In order to explore the im-
pact of chunk-based features, similarity features
were classified into two groups (chunk-based fea-
tures vs. document-based features), and two fil-
ters were trained using each group separately. Ta-
ble 5 compares the performance of the two filters
(Similarity-chunk and Similarity-doc) with the fil-
ter using all similarity features (Similarity).
</bodyText>
<table confidence="0.861215166666667">
acc. prec. recall fscore
Similarity- 0.972 0.700 0.442 0.542
chunk
Similarity- 0.971 0.730 0.396 0.514
doc
Similarity 0.975 0.731 0.548 0.626
</table>
<tableCaption confidence="0.950229">
Table 5: Comparison of chunk-based and
document-based similarity features
</tableCaption>
<bodyText confidence="0.986176681818182">
In this study, the chunk-based features were
comparable to the document-based features. Fur-
thermore, combination of the two features im-
proved F-score. The performance improvement
mostly resulted from higher recall.
Finally, Table 6 presents the results using the
entire FM set, including the OR, TR, and RE re-
sponses that were not included in the previous
experiment. The accuracy of the majority class
baseline (classifying all responses as scorable re-
sponses) is 0.865.
acc. prec. recall fscore
Similarity- 0.976 0.926 0.895 0.910
filter
A/S-filter 0.974 0.953 0.849 0.898
Combined- 0.977 0.941 0.884 0.911
filter
Table 6: Performance of filters in all types of NS
detection
Both the Similarity-filter and the A/S-
filter achieved high performance. Both accuracies
and F-scores were similar and the difference
</bodyText>
<page confidence="0.994118">
121
</page>
<bodyText confidence="0.99998575">
between the two filters was approximately 0.01.
The Similarity-filter achieved better performance
than the A/S-filter in recall: it was 0.89, which
was substantially higher than the A/S-filter (0.85).
It is an encouraging result that the Similarity-
filter could achieve a performance comparable
to the A/S-filter, which was based on multi-
ple resources such as signal processing, forced-
alignment, and ASR. But, the combination of the
two feature groups did not achieve further im-
provement: the increase in both accuracy and F-
measure was less than 0.01.
</bodyText>
<sectionHeader confidence="0.999086" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99998947826087">
In this study, filtering models were implemented
as a supplementary module for an automated
speech proficiency scoring system. In addition to
A/S features, which have shown promising perfor-
mance in previous studies, a set of similarity fea-
tures were implemented and a filtering model was
developed. The Similarity-filter was more accu-
rate than the A/S-filter in identifying the responses
with topical problems. This result is encouraging
since the proportion of these responses is likely to
increase when the automated speech scoring sys-
tem becomes a sole rater of the assessment.
Although the Similarity-filter achieved better
performance than the A/S-filter, it should be fur-
ther improved. The recall of the system was low,
and approximately 45% of NS responses could
not be identified. In addition, the model requires
substantial amount of sample responses for each
item, and it will cause serious difficulty when it is
used the real test situation. In future, we will ex-
plore the similarity features trained only using the
prompt question or the additional prompt materi-
als such as visual and audio materials.
</bodyText>
<sectionHeader confidence="0.999265" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999812546875">
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e–rater R v.2. The Journal of Technol-
ogy, Learning, and Assessment, 4(3).
Paul Boersma. 2002. Praat, a system for doing phonet-
ics by computer. Glot International, 5(9/10):341–
345.
Sergey Brin, James Davis, and Hector Garcia-Molina.
1995. Copy detection mechanisms for digital docu-
ments. In ACM SIGMOD Record, volume 24, pages
398–409. ACM.
Jian Cheng and Jianqiang Shen. 2011. Off-topic detec-
tion in automated speech assessment applications.
In Proceedings of InterSpeech, pages 1597–1600.
IEEE.
Peter W. Foltz, Darrell Laham, and Thomas K. Lan-
dauer. 1999. The Intelligent Essay Assessor: Appli-
cations to educational technology. Interactive mul-
timedia Electronic Journal of Computer-Enhanced
Learning, 1(2).
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10–
18.
Derrick Higgins, Jill Burstein, and Yigal Attali. 2006.
Identifying off-topic student essays without topic-
specific training data. Natural Language Engineer-
ing, 12(02):145–159.
Timothy C Hoad and Justin Zobel. 2003. Meth-
ods for identifying versioned and plagiarized doc-
uments. Journal of the American society for infor-
mation science and technology, 54(3):203–215.
Je Hun Jeon and Su-Youn Yoon. 2012. Acoustic
feature-based non-scorable response detection for an
automated speaking proficiency assessment. In Pro-
ceedings of the InterSpeech, pages 1275–1278.
Wai-Kit Lo, Alissa M Harrison, and Helen Meng.
2010. Statistical phone duration modeling to filter
for intact utterances in a computer-assisted pronun-
ciation training system. In Proceedings of Acous-
tics Speech and Signal Processing (ICASSP), 2010
IEEE International Conference on, pages 5238–
5241. IEEE.
Donald Metzler, Yaniv Bernstein, W Bruce Croft, Al-
istair Moffat, and Justin Zobel. 2005. Similarity
measures for tracking information flow. In Proceed-
ings of the 14th ACM international conference on In-
formation and knowledge management, pages 517–
524. ACM.
Mark Sanderson. 1997. Duplicate detection in the
reuters collection. ” Technical Report (TR-1997-5)
of the Department of Computing Science at the Uni-
versity of Glasgow G12 8QQ, UK”.
Narayanan Shivakumar and Hector Garcia-Molina.
1995. Scam: A copy detection mechanism for digi-
tal documents.
Narayanan Shivakumar and Hector Garcia-Molina.
1996. Building a scalable and accurate copy detec-
tion mechanism. In Proceedings of the first ACM
international conference on Digital libraries, pages
160–168. ACM.
Joost van Doremalen, Helmet Strik, and Cartia Cuc-
chiarini. 2009. Utterance verification in language
learning applications. In Proceedings of the SLaTE.
</reference>
<page confidence="0.971127">
122
</page>
<reference confidence="0.999237454545455">
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 103–111. Association for Computa-
tional Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring
of non-native spontaneous speech in tests of spoken
English. Speech Communication, 51(10):883–895.
</reference>
<page confidence="0.998952">
123
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.268656">
<title confidence="0.9936275">Similarity-Based Non-Scorable Response Detection for Automated Speech Scoring</title>
<author confidence="0.377437">Su-Youn</author>
<affiliation confidence="0.422111">Educational Testing</affiliation>
<address confidence="0.585834">Princeton, NJ,</address>
<email confidence="0.987514">syoon@ets.org</email>
<abstract confidence="0.998895533333333">This study provides a method that identifies problematic responses which make automated speech scoring difficult. When automated scoring is used in the context of a high stakes language proficiency assessment, for which the scores are used to make consequential decisions, some test takers may have an incentive to try to game the system in order to artificially inflate their scores. Since many automated proficiency scoring systems use fluency features such as speaking rate as one of the important features, students may engage in strategies designed to manipulate their speaking rate as measured by the system. In order to address this issue, we developed a method which filters out nonscorable responses based on text similarity measures. Given a test response, the method generated a set of features which calculated the topic similarity with the prompt question or the sample responses including relevant content. Next, an automated filter which identified these problematic responses was implemented using the similarity features. This filter improved the performance of the baseline filter in identifying responses with topic problems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yigal Attali</author>
<author>Jill Burstein</author>
</authors>
<title>Automated essay scoring with e–rater R v.2.</title>
<date>2006</date>
<journal>The Journal of Technology, Learning, and Assessment,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="7931" citStr="Attali and Burstein, 2006" startWordPosition="1257" endWordPosition="1260">ation retrieval methods using vector space models or language model (Sanderson, 1997; Hoad and Zobel, 2003). Document similarity measures have been applied in automated scoring. Foltz et al. (1999) evaluated the content of written essays using latent semantic analysis (LSA) by comparing the test essays with essays of known quality in regard to their degree of conceptual relevance and the amount of relevant content. In another approach, the lexical content of an essay was evaluated by comparing the words contained in each essay to the words found in a sample of essays from each score category (Attali and Burstein, 2006). More recently, Xie et al. (2012) used a similar approach in automated speech scoring; they measured the similarity using three similarity measures, including a lexical matching method (Vector Space Model) and two semantic similarity measures (Latent Semantic Analysis and Pointwise Mutual Information). They showed moderately high correlations 117 between the similarity features and human proficiency scores on even the output of an automatic speech recognition system. Similarity measures have also been used in off-topic detection for nonnative speakers’ essays. Higgins et al. (2006) calculated</context>
</contexts>
<marker>Attali, Burstein, 2006</marker>
<rawString>Yigal Attali and Jill Burstein. 2006. Automated essay scoring with e–rater R v.2. The Journal of Technology, Learning, and Assessment, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Boersma</author>
</authors>
<title>Praat, a system for doing phonetics by computer.</title>
<date>2002</date>
<journal>Glot International,</journal>
<volume>5</volume>
<issue>9</issue>
<pages>345</pages>
<contexts>
<context position="18643" citStr="Boersma, 2002" startWordPosition="3047" endWordPosition="3048">d using a state-of-the-art automated speech scoring system. A detailed description of the system is available from (Jeon and Yoon, 2012). Among these features, many features were conceptually similar but based on different normalization methods, and they showed a strong inter-correlation. For this study, 30 features were selected and classified into three groups according to their characteristics: acoustic features, fluency features, and ASR-confidence features. The acoustic features were related to power, pitch, and MFCC. First, power, pitch and MFCC were extracted at each frame using Praat (Boersma, 2002). Next, we generated response-level features from these frame-level features by calculating mean and variation. These features captured the overall distribution of energy and voiced regions in a speaker’s response. These features are relevant since NS responses may have an abnormal distribution in energy. For instance, non-responses contain very low energy. In order to detect these abnormalities in the speech signal, pitch and power related features were calculated. The fluency features measure the length of a response in terms of duration and number of words. In addition, this group contains </context>
</contexts>
<marker>Boersma, 2002</marker>
<rawString>Paul Boersma. 2002. Praat, a system for doing phonetics by computer. Glot International, 5(9/10):341– 345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>James Davis</author>
<author>Hector Garcia-Molina</author>
</authors>
<title>Copy detection mechanisms for digital documents.</title>
<date>1995</date>
<journal>In ACM SIGMOD Record,</journal>
<volume>24</volume>
<pages>398--409</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7207" citStr="Brin et al., 1995" startWordPosition="1142" endWordPosition="1145">trongly associated with off-topic responses in restricted speech in which the target sentence is given. However, in spontaneous speech, this is not trivial; it may be associated with not only off-topic speech but also mismatch between the LM and speech input due to the low coverage of the LM. Due to the latter case, the decision based on the confidence score may not be effective in measuring topic similarity. The topic similarity between two documents has been frequently modeled by relative-frequency measures (Hoad and Zobel, 2003; Shivakumar and Garcia-Molina, 1995), document fingerprinting (Brin et al., 1995; Shivakumar and Garcia-Molina, 1995; Shivakumar and Garcia-Molina, 1996)), and query based information retrieval methods using vector space models or language model (Sanderson, 1997; Hoad and Zobel, 2003). Document similarity measures have been applied in automated scoring. Foltz et al. (1999) evaluated the content of written essays using latent semantic analysis (LSA) by comparing the test essays with essays of known quality in regard to their degree of conceptual relevance and the amount of relevant content. In another approach, the lexical content of an essay was evaluated by comparing the</context>
</contexts>
<marker>Brin, Davis, Garcia-Molina, 1995</marker>
<rawString>Sergey Brin, James Davis, and Hector Garcia-Molina. 1995. Copy detection mechanisms for digital documents. In ACM SIGMOD Record, volume 24, pages 398–409. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Cheng</author>
<author>Jianqiang Shen</author>
</authors>
<title>Off-topic detection in automated speech assessment applications.</title>
<date>2011</date>
<booktitle>In Proceedings of InterSpeech,</booktitle>
<pages>1597--1600</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5984" citStr="Cheng and Shen (2011)" startWordPosition="950" endWordPosition="953">thod and experiment set-up in sections 4 and 5. The results and discussion are presented in section 6, and the conclusions are presented in section 7. 2 Related Work Filtering of NS responses for automated speech scoring has been rarely recognized. Only a few pieces of research have focused on this task, and most studies have targeted highly restricted speech. van Doremalen et al. (2009) and Lo et al. (2010) used normalized confidence scores of a speech recognizer in recasting speech. They identified non-scorable responses with promising performances (equal error rates ranged from 10 to 20%). Cheng and Shen (2011) extended these studies and combined an acoustic model score, a language model score, and a garbage model score with confidence scores. They applied this new filter to less constrained items (e.g., picture description) and identified off-topic responses with an accuracy rate of 90% with a false positive rate of 5%. Although normalized confidence scores achieved promising performances in restricted speech, they may not be appropriate for the items that elicit unconstrained spontaneous speech. Low confidence scores signal the use of words or phrases not covered by the language model (LM) and thi</context>
</contexts>
<marker>Cheng, Shen, 2011</marker>
<rawString>Jian Cheng and Jianqiang Shen. 2011. Off-topic detection in automated speech assessment applications. In Proceedings of InterSpeech, pages 1597–1600. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
<author>Thomas K Landauer</author>
</authors>
<title>The Intelligent Essay Assessor: Applications to educational technology. Interactive multimedia</title>
<date>1999</date>
<journal>Electronic Journal of Computer-Enhanced Learning,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="7502" citStr="Foltz et al. (1999)" startWordPosition="1184" endWordPosition="1187"> Due to the latter case, the decision based on the confidence score may not be effective in measuring topic similarity. The topic similarity between two documents has been frequently modeled by relative-frequency measures (Hoad and Zobel, 2003; Shivakumar and Garcia-Molina, 1995), document fingerprinting (Brin et al., 1995; Shivakumar and Garcia-Molina, 1995; Shivakumar and Garcia-Molina, 1996)), and query based information retrieval methods using vector space models or language model (Sanderson, 1997; Hoad and Zobel, 2003). Document similarity measures have been applied in automated scoring. Foltz et al. (1999) evaluated the content of written essays using latent semantic analysis (LSA) by comparing the test essays with essays of known quality in regard to their degree of conceptual relevance and the amount of relevant content. In another approach, the lexical content of an essay was evaluated by comparing the words contained in each essay to the words found in a sample of essays from each score category (Attali and Burstein, 2006). More recently, Xie et al. (2012) used a similar approach in automated speech scoring; they measured the similarity using three similarity measures, including a lexical m</context>
</contexts>
<marker>Foltz, Laham, Landauer, 1999</marker>
<rawString>Peter W. Foltz, Darrell Laham, and Thomas K. Landauer. 1999. The Intelligent Essay Assessor: Applications to educational technology. Interactive multimedia Electronic Journal of Computer-Enhanced Learning, 1(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<pages>18</pages>
<contexts>
<context position="20665" citStr="Hall et al., 2009" startWordPosition="3359" endWordPosition="3362"> independent triphone acoustic model and a combination of bigram, tri120 gram, and four-gram language models were used. A word error rate (WER) of 27% on the held-out test dataset was observed. For each response in the FM set, the word hypotheses was generated using this recognizer. From this ASR-based transcription, the six similarity features were generated. In addition, the 30 A/S features described in 4.3 were generated. Using these two sets of features, filtering models were trained using the Support Vector Machine algorithm (SVM) with the RBF kernel of the WEKA machine-learning toolkit (Hall et al., 2009). A 10 fold cross-validation was conducted using the FM dataset. 6 Results and discussion First, we will report the performance for the subset only topic-related NS responses. The similarity features were designed to detect NS responses with topicality issues, but the majority in the FM set were non-response (73%). The topicrelated NS responses (off-topic responses, generic responses, and question copy responses) were only 25%. In the entire set, the advantage of the similarity features over the A/S features might not be salient due to the high proportion of non-response. In order to investiga</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. The weka data mining software: an update. ACM SIGKDD Explorations Newsletter, 11(1):10– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derrick Higgins</author>
<author>Jill Burstein</author>
<author>Yigal Attali</author>
</authors>
<title>Identifying off-topic student essays without topicspecific training data.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>02</issue>
<contexts>
<context position="8520" citStr="Higgins et al. (2006)" startWordPosition="1346" endWordPosition="1349">gory (Attali and Burstein, 2006). More recently, Xie et al. (2012) used a similar approach in automated speech scoring; they measured the similarity using three similarity measures, including a lexical matching method (Vector Space Model) and two semantic similarity measures (Latent Semantic Analysis and Pointwise Mutual Information). They showed moderately high correlations 117 between the similarity features and human proficiency scores on even the output of an automatic speech recognition system. Similarity measures have also been used in off-topic detection for nonnative speakers’ essays. Higgins et al. (2006) calculated overlaps between the question and content words from the essay and obtained an error rate of 10%. Given the promising performance in both automated scoring and off-topic essay detection, we will expand these similarity measures in NS response detection for speech scoring. 3 Data In this study, we used a collection of responses from an international English language assessment. The assessment was composed of items in which speakers were prompted to provide spontaneous speech. Approximately 48,000 responses from 8,000 non-native speakers were collected and used for training the autom</context>
</contexts>
<marker>Higgins, Burstein, Attali, 2006</marker>
<rawString>Derrick Higgins, Jill Burstein, and Yigal Attali. 2006. Identifying off-topic student essays without topicspecific training data. Natural Language Engineering, 12(02):145–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy C Hoad</author>
<author>Justin Zobel</author>
</authors>
<title>Methods for identifying versioned and plagiarized documents.</title>
<date>2003</date>
<journal>Journal of the American</journal>
<pages>54--3</pages>
<contexts>
<context position="7126" citStr="Hoad and Zobel, 2003" startWordPosition="1132" endWordPosition="1135">nal the use of words or phrases not covered by the language model (LM) and this is strongly associated with off-topic responses in restricted speech in which the target sentence is given. However, in spontaneous speech, this is not trivial; it may be associated with not only off-topic speech but also mismatch between the LM and speech input due to the low coverage of the LM. Due to the latter case, the decision based on the confidence score may not be effective in measuring topic similarity. The topic similarity between two documents has been frequently modeled by relative-frequency measures (Hoad and Zobel, 2003; Shivakumar and Garcia-Molina, 1995), document fingerprinting (Brin et al., 1995; Shivakumar and Garcia-Molina, 1995; Shivakumar and Garcia-Molina, 1996)), and query based information retrieval methods using vector space models or language model (Sanderson, 1997; Hoad and Zobel, 2003). Document similarity measures have been applied in automated scoring. Foltz et al. (1999) evaluated the content of written essays using latent semantic analysis (LSA) by comparing the test essays with essays of known quality in regard to their degree of conceptual relevance and the amount of relevant content. In</context>
</contexts>
<marker>Hoad, Zobel, 2003</marker>
<rawString>Timothy C Hoad and Justin Zobel. 2003. Methods for identifying versioned and plagiarized documents. Journal of the American society for information science and technology, 54(3):203–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Je Hun Jeon</author>
<author>Su-Youn Yoon</author>
</authors>
<title>Acoustic feature-based non-scorable response detection for an automated speaking proficiency assessment.</title>
<date>2012</date>
<booktitle>In Proceedings of the InterSpeech,</booktitle>
<pages>1275--1278</pages>
<contexts>
<context position="18165" citStr="Jeon and Yoon, 2012" startWordPosition="2977" endWordPosition="2980">esponses (OR) and question copy responses (RE); a low score is highly likely to be an off-topic response, while a high score signals a full or partial copy; • meanIDF: mean of idfs for all word tokens in the response. Generic responses (TR) tend to include many high frequency words such as articles and pronouns, and the mean idf value of these responses may be low. 4.3 Features from the automated speech scoring system A total of 61 features (hereafter, A/S features) were generated using a state-of-the-art automated speech scoring system. A detailed description of the system is available from (Jeon and Yoon, 2012). Among these features, many features were conceptually similar but based on different normalization methods, and they showed a strong inter-correlation. For this study, 30 features were selected and classified into three groups according to their characteristics: acoustic features, fluency features, and ASR-confidence features. The acoustic features were related to power, pitch, and MFCC. First, power, pitch and MFCC were extracted at each frame using Praat (Boersma, 2002). Next, we generated response-level features from these frame-level features by calculating mean and variation. These feat</context>
</contexts>
<marker>Jeon, Yoon, 2012</marker>
<rawString>Je Hun Jeon and Su-Youn Yoon. 2012. Acoustic feature-based non-scorable response detection for an automated speaking proficiency assessment. In Proceedings of the InterSpeech, pages 1275–1278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wai-Kit Lo</author>
<author>Alissa M Harrison</author>
<author>Helen Meng</author>
</authors>
<title>Statistical phone duration modeling to filter for intact utterances in a computer-assisted pronunciation training system.</title>
<date>2010</date>
<booktitle>In Proceedings of Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on,</booktitle>
<pages>5238--5241</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5774" citStr="Lo et al. (2010)" startWordPosition="920" endWordPosition="923"> (2009). In this study, we will only focus on the filtering model. This paper will proceed as follows: we first review previous studies in section 2, then describe the data in section 3, and present the method and experiment set-up in sections 4 and 5. The results and discussion are presented in section 6, and the conclusions are presented in section 7. 2 Related Work Filtering of NS responses for automated speech scoring has been rarely recognized. Only a few pieces of research have focused on this task, and most studies have targeted highly restricted speech. van Doremalen et al. (2009) and Lo et al. (2010) used normalized confidence scores of a speech recognizer in recasting speech. They identified non-scorable responses with promising performances (equal error rates ranged from 10 to 20%). Cheng and Shen (2011) extended these studies and combined an acoustic model score, a language model score, and a garbage model score with confidence scores. They applied this new filter to less constrained items (e.g., picture description) and identified off-topic responses with an accuracy rate of 90% with a false positive rate of 5%. Although normalized confidence scores achieved promising performances in </context>
</contexts>
<marker>Lo, Harrison, Meng, 2010</marker>
<rawString>Wai-Kit Lo, Alissa M Harrison, and Helen Meng. 2010. Statistical phone duration modeling to filter for intact utterances in a computer-assisted pronunciation training system. In Proceedings of Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on, pages 5238– 5241. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>Yaniv Bernstein</author>
<author>W Bruce Croft</author>
<author>Alistair Moffat</author>
<author>Justin Zobel</author>
</authors>
<title>Similarity measures for tracking information flow.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM international conference on Information and knowledge management,</booktitle>
<pages>517--524</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3261" citStr="Metzler et al. (2005)" startWordPosition="514" endWordPosition="517">onal English language assessment used in this study was 2%. Despite this low proportion, it is a serious problem which has a strong impact on the validity of the test. In addition, the likelihood of students engaging in gaming strategies may increase with the use of automated scoring. Therefore, an automated filtering model with a high accuracy is a necessary step to use the automated scoring system as a sole rater. Both off-topic and copy responses have topicrelated problems, although they are at the two extremes in the degree of similarity. Focusing on the intermediate levels of similarity, Metzler et al. (2005) presented a hierarchy of five similarity levels: unrelated, on the general topic, on the specific topic, same facts, and copied. In the automated scoring of spontaneous speech, responses that fell into unrelated can be considered as offtopic, while the ones that fell into copied can be considered as repetition or plagiarism. Following this approach, we developed a non-scorable response identification method utilizing similar116 Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications, pages 116–123, Baltimore, Maryland USA, June 26, 2014. c�2014 Associa</context>
<context position="14559" citStr="Metzler et al. (2005)" startWordPosition="2360" endWordPosition="2363">response vs. scorable response. 4.1 Chunk-based similarity features Some responses in this study included more than two different types of the topicality problems. For instance, the first three sentences in Table 3 belonged to the “copied” category, while the other sentences fell into “unrelated”. If the similarity features were calculated based on the entire response, the feature values may fall into neither the “copied” nor “unrelated” range because of the trade-off between the two types at two extremes. In order to address this issue, we calculated chunkbased similarity features similar to Metzler et al. (2005)’s sentence-based features. First, the response was split into the chunks which were surrounded by long silences with durations longer than 0.6 sec. For each chunk, the proportion of word overlap with the question (WOL) was calculated based on the formula (1). Next, chunks with a WOL higher than 0.5 were considered as question copies. WOL = |Sn�| |S| where S is a response and Q is a question, |5 n Q |is the number of word types that appear both in S and Q, |5 |is the number of word types in S (1) Finally, the following three features were derived for each response based on the chunk-based WOL.</context>
<context position="22357" citStr="Metzler et al. (2005)" startWordPosition="3635" endWordPosition="3638">- 0.975 0.731 0.548 0.626 filter A/S-filter 0.971 0.767 0.341 0.472 Combined- 0.977 0.780 0.566 0.656 filter Table 4: Performance of filters in topic-related NS detection Not surprisingly, the Similarity-filter outperformed the A/S-filter: the F-score was approximately 0.63 which was 0.15 higher than that of the A/S-filter in absolute value. The lack of features specialized for detection of topic abnormality resulted in the low recall of the A/S-filter. The combination of the two features achieved a slight improvement: the F-score was 0.66 and it was 0.03 higher than the Similarity-filter. In Metzler et al. (2005)’s study, the system using both sentence-based features and documentbased features did not achieve further improvement over the system based on the documentbased features alone. In order to explore the impact of chunk-based features, similarity features were classified into two groups (chunk-based features vs. document-based features), and two filters were trained using each group separately. Table 5 compares the performance of the two filters (Similarity-chunk and Similarity-doc) with the filter using all similarity features (Similarity). acc. prec. recall fscore Similarity- 0.972 0.700 0.442</context>
</contexts>
<marker>Metzler, Bernstein, Croft, Moffat, Zobel, 2005</marker>
<rawString>Donald Metzler, Yaniv Bernstein, W Bruce Croft, Alistair Moffat, and Justin Zobel. 2005. Similarity measures for tracking information flow. In Proceedings of the 14th ACM international conference on Information and knowledge management, pages 517– 524. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Sanderson</author>
</authors>
<title>Duplicate detection in the reuters collection. ”</title>
<date>1997</date>
<booktitle>of the Department of Computing Science at the University of Glasgow G12 8QQ,</booktitle>
<tech>Technical Report (TR-1997-5)</tech>
<publisher>UK”.</publisher>
<contexts>
<context position="7389" citStr="Sanderson, 1997" startWordPosition="1167" endWordPosition="1169">ot only off-topic speech but also mismatch between the LM and speech input due to the low coverage of the LM. Due to the latter case, the decision based on the confidence score may not be effective in measuring topic similarity. The topic similarity between two documents has been frequently modeled by relative-frequency measures (Hoad and Zobel, 2003; Shivakumar and Garcia-Molina, 1995), document fingerprinting (Brin et al., 1995; Shivakumar and Garcia-Molina, 1995; Shivakumar and Garcia-Molina, 1996)), and query based information retrieval methods using vector space models or language model (Sanderson, 1997; Hoad and Zobel, 2003). Document similarity measures have been applied in automated scoring. Foltz et al. (1999) evaluated the content of written essays using latent semantic analysis (LSA) by comparing the test essays with essays of known quality in regard to their degree of conceptual relevance and the amount of relevant content. In another approach, the lexical content of an essay was evaluated by comparing the words contained in each essay to the words found in a sample of essays from each score category (Attali and Burstein, 2006). More recently, Xie et al. (2012) used a similar approach</context>
</contexts>
<marker>Sanderson, 1997</marker>
<rawString>Mark Sanderson. 1997. Duplicate detection in the reuters collection. ” Technical Report (TR-1997-5) of the Department of Computing Science at the University of Glasgow G12 8QQ, UK”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Narayanan Shivakumar</author>
<author>Hector Garcia-Molina</author>
</authors>
<title>Scam: A copy detection mechanism for digital documents.</title>
<date>1995</date>
<contexts>
<context position="7163" citStr="Shivakumar and Garcia-Molina, 1995" startWordPosition="1136" endWordPosition="1139">r phrases not covered by the language model (LM) and this is strongly associated with off-topic responses in restricted speech in which the target sentence is given. However, in spontaneous speech, this is not trivial; it may be associated with not only off-topic speech but also mismatch between the LM and speech input due to the low coverage of the LM. Due to the latter case, the decision based on the confidence score may not be effective in measuring topic similarity. The topic similarity between two documents has been frequently modeled by relative-frequency measures (Hoad and Zobel, 2003; Shivakumar and Garcia-Molina, 1995), document fingerprinting (Brin et al., 1995; Shivakumar and Garcia-Molina, 1995; Shivakumar and Garcia-Molina, 1996)), and query based information retrieval methods using vector space models or language model (Sanderson, 1997; Hoad and Zobel, 2003). Document similarity measures have been applied in automated scoring. Foltz et al. (1999) evaluated the content of written essays using latent semantic analysis (LSA) by comparing the test essays with essays of known quality in regard to their degree of conceptual relevance and the amount of relevant content. In another approach, the lexical conten</context>
</contexts>
<marker>Shivakumar, Garcia-Molina, 1995</marker>
<rawString>Narayanan Shivakumar and Hector Garcia-Molina. 1995. Scam: A copy detection mechanism for digital documents.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Narayanan Shivakumar</author>
<author>Hector Garcia-Molina</author>
</authors>
<title>Building a scalable and accurate copy detection mechanism.</title>
<date>1996</date>
<booktitle>In Proceedings of the first ACM international conference on Digital libraries,</booktitle>
<pages>160--168</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7280" citStr="Shivakumar and Garcia-Molina, 1996" startWordPosition="1150" endWordPosition="1153">ed speech in which the target sentence is given. However, in spontaneous speech, this is not trivial; it may be associated with not only off-topic speech but also mismatch between the LM and speech input due to the low coverage of the LM. Due to the latter case, the decision based on the confidence score may not be effective in measuring topic similarity. The topic similarity between two documents has been frequently modeled by relative-frequency measures (Hoad and Zobel, 2003; Shivakumar and Garcia-Molina, 1995), document fingerprinting (Brin et al., 1995; Shivakumar and Garcia-Molina, 1995; Shivakumar and Garcia-Molina, 1996)), and query based information retrieval methods using vector space models or language model (Sanderson, 1997; Hoad and Zobel, 2003). Document similarity measures have been applied in automated scoring. Foltz et al. (1999) evaluated the content of written essays using latent semantic analysis (LSA) by comparing the test essays with essays of known quality in regard to their degree of conceptual relevance and the amount of relevant content. In another approach, the lexical content of an essay was evaluated by comparing the words contained in each essay to the words found in a sample of essays f</context>
</contexts>
<marker>Shivakumar, Garcia-Molina, 1996</marker>
<rawString>Narayanan Shivakumar and Hector Garcia-Molina. 1996. Building a scalable and accurate copy detection mechanism. In Proceedings of the first ACM international conference on Digital libraries, pages 160–168. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joost van Doremalen</author>
<author>Helmet Strik</author>
<author>Cartia Cucchiarini</author>
</authors>
<title>Utterance verification in language learning applications.</title>
<date>2009</date>
<booktitle>In Proceedings of the SLaTE.</booktitle>
<marker>van Doremalen, Strik, Cucchiarini, 2009</marker>
<rawString>Joost van Doremalen, Helmet Strik, and Cartia Cucchiarini. 2009. Utterance verification in language learning applications. In Proceedings of the SLaTE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Xie</author>
<author>Keelan Evanini</author>
<author>Klaus Zechner</author>
</authors>
<title>Exploring content features for automated speech scoring.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>103--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7965" citStr="Xie et al. (2012)" startWordPosition="1263" endWordPosition="1266">models or language model (Sanderson, 1997; Hoad and Zobel, 2003). Document similarity measures have been applied in automated scoring. Foltz et al. (1999) evaluated the content of written essays using latent semantic analysis (LSA) by comparing the test essays with essays of known quality in regard to their degree of conceptual relevance and the amount of relevant content. In another approach, the lexical content of an essay was evaluated by comparing the words contained in each essay to the words found in a sample of essays from each score category (Attali and Burstein, 2006). More recently, Xie et al. (2012) used a similar approach in automated speech scoring; they measured the similarity using three similarity measures, including a lexical matching method (Vector Space Model) and two semantic similarity measures (Latent Semantic Analysis and Pointwise Mutual Information). They showed moderately high correlations 117 between the similarity features and human proficiency scores on even the output of an automatic speech recognition system. Similarity measures have also been used in off-topic detection for nonnative speakers’ essays. Higgins et al. (2006) calculated overlaps between the question and</context>
</contexts>
<marker>Xie, Evanini, Zechner, 2012</marker>
<rawString>Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012. Exploring content features for automated speech scoring. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 103–111. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
<author>Derrick Higgins</author>
<author>Xiaoming Xi</author>
<author>David M Williamson</author>
</authors>
<title>Automatic scoring of non-native spontaneous speech in tests of spoken English.</title>
<date>2009</date>
<journal>Speech Communication,</journal>
<volume>51</volume>
<issue>10</issue>
<contexts>
<context position="5165" citStr="Zechner et al. (2009)" startWordPosition="814" endWordPosition="817">tch and power; the system calculates descriptive statistics such as the mean and standard deviation of pitch and power at both the word level and response level. Given the word hypotheses and descriptive features of pitch/power, it derives features for automated proficiency scoring. In addition, the similarity features are generated based on the word hypotheses and topic models. Finally, given both sets of features, the filtering model filters out non-scorable responses, and the remainder of the responses are scored using a scoring model. A detailed description of the system is available from Zechner et al. (2009). In this study, we will only focus on the filtering model. This paper will proceed as follows: we first review previous studies in section 2, then describe the data in section 3, and present the method and experiment set-up in sections 4 and 5. The results and discussion are presented in section 6, and the conclusions are presented in section 7. 2 Related Work Filtering of NS responses for automated speech scoring has been rarely recognized. Only a few pieces of research have focused on this task, and most studies have targeted highly restricted speech. van Doremalen et al. (2009) and Lo et a</context>
</contexts>
<marker>Zechner, Higgins, Xi, Williamson, 2009</marker>
<rawString>Klaus Zechner, Derrick Higgins, Xiaoming Xi, and David M. Williamson. 2009. Automatic scoring of non-native spontaneous speech in tests of spoken English. Speech Communication, 51(10):883–895.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
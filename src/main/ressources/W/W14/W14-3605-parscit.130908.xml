<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000049">
<title confidence="0.96909">
The First QALB Shared Task on Automatic Text Correction for Arabic
</title>
<author confidence="0.998586">
Behrang Mohit&apos;∗, Alla Rozovskaya2∗, Nizar Habash3, Wajdi Zaghouani&apos;, Ossama Obeid&apos;
</author>
<affiliation confidence="0.936032666666667">
&apos;Carnegie Mellon University in Qatar
2Center for Computational Learning Systems, Columbia University
3New York University Abu Dhabi
</affiliation>
<email confidence="0.9887135">
behrang@cmu.edu, alla@ccls.columbia.edu, nizar.habash@nyu.edu
wajdiz@qatar.cmu.edu,owo@qatar.cmu.edu
</email>
<sectionHeader confidence="0.993826" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989909090909">
We present a summary of the first shared
task on automatic text correction for Ara-
bic text. The shared task received 18 sys-
tems submissions from nine teams in six
countries and represented a diversity of ap-
proaches. Our report includes an overview
of the QALB corpus which was the source
of the datasets used for training and eval-
uation, an overview of participating sys-
tems, results of the competition and an
analysis of the results and systems.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999942125">
The task of text correction has recently gained a
lot of attention in the Natural Language Process-
ing (NLP) community. Most of the effort in this
area concentrated on English, especially on errors
made by learners of English as a Second Lan-
guage. Four competitions devoted to error cor-
rection for non-native English writers took place
recently: HOO (Dale and Kilgarriff, 2011; Dale
et al., 2012) and CoNLL (Ng et al., 2013; Ng et
al., 2014). Shared tasks of this kind are extremely
important, as they bring together researchers who
focus on this problem and promote development
and dissemination of key resources, such as bench-
mark datasets.
Recently, there have been several efforts aimed
at creating data resources related to the correc-
tion of Arabic text. Those include human anno-
tated corpora (Zaghouani et al., 2014; Alfaifi and
Atwell, 2012), spell-checking lexicon (Attia et al.,
2012) and unannotated language learner corpora
(Farwaneh and Tamimi, 2012). A natural exten-
sion to these resource production efforts is the cre-
ation of robust automatic systems for error correc-
tion.
</bodyText>
<note confidence="0.645088">
* These authors contributed equally to this work.
</note>
<bodyText confidence="0.999825625">
In this paper, we present a summary of the
QALB shared task on automatic text correction
for Arabic. The Qatar Arabic Language Bank
(QALB) project1 is one of the first large scale data
and system development efforts for automatic cor-
rection of Arabic which has resulted in annota-
tion of the QALB corpus. In conjunction with the
EMNLP Arabic NLP workshop, the QALB shared
task is the first community effort for construction
and evaluation of automatic correction systems for
Arabic.
The results of the competition indicate that the
shared task attracted a lot of interest and generated
a diverse set of approaches from the participating
teams.
In the next section, we present the shared task
framework. This is followed by an overview of
the QALB corpus (Section 3). Section 4 describes
the shared task data, and Section 5 presents the ap-
proaches adopted by the participating teams. Sec-
tion 6 discusses the results of the competition. Fi-
nally, in Section 7, we offer a brief analysis and
present preliminary experiments on system com-
bination.
</bodyText>
<sectionHeader confidence="0.990605" genericHeader="introduction">
2 Task Description
</sectionHeader>
<bodyText confidence="0.975495416666667">
The QALB shared task was created as a forum for
competition and collaboration on automatic error
correction in Modern Standard Arabic. The shared
task makes use of the QALB corpus (Zaghouani et
al., 2014), which is a manually-corrected collec-
tion of Arabic texts. The shared task participants
were provided with training and development data
to build their systems, but were also free to make
use of additional resources, including corpora, lin-
guistic resources, and software, as long as these
were publicly available.
For evaluation, a standard framework devel-
</bodyText>
<footnote confidence="0.987212">
1http://nlp.qatar.cmu.edu/qalb/
</footnote>
<page confidence="0.986599">
39
</page>
<note confidence="0.571891928571429">
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 39–47,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
Original
Corrected
éª�K@QË@ �HCJ�Êj�JË@ ~è �Yë �é�K@Q�¯ Y�J« ú
��GXAªƒ ø
YÓ @ðPñ’�J�K B
�
ø� Xj�@ �@ éiË@ �áÓ ú��æÒ�JK. �I�J» ðH. Aƒ� ú��G@B ;ÓQ��jÖÏ@ ð
YJ
ªK. @Aë 0@ @ðYJ.K
or ð ú
æ”~¯B@ Yj. ‚ÖÏAK. @PðQÓ 3QÒªË@
áºÜØ 1/2�K@ Èñ�®J�K.JA¿ ;J�:ÓB@ ©Ò‚�� Yg ú�
</note>
<equation confidence="0.982893650000001">
�¯ AÓ É3/4&apos; ÈA:ÖÏ@
�
. éÊJ�j��‚Ó 1/2�J���JÓ@ à
�BAëñ�®�®m�
�� 1/4XA�®k�@ XA�®k@ à@ ú
�æÒ�J�K
lA ttSwrwA mdy1 sςAdty ςnd qrAˆyh2 h6h3
AltHlylAt AlrAˆyςh w AlmHtrmh4 lÂAny6 šAb
w knt7 btmny8 mn Allh An9 Â ˆwdy Alςmrh mr-
wrA bAlmsjd AlAqSy10 w kAn12 ybdwA13 An14
h6A bςyd AlmnAl fkl mA16 fy17 Hd18 ysmς
AlAmnyh19 kAn byqwl20 Ank21 mmkn ttmny23
An24 ÂHfAd ÂHfAdk yHqqwhAlÂn25 Amnytk26
mstHylh.
éª�K@QË@ �HCJ�Êj�JË@ è �Yë �èZ@Q�¯ Y�J« ú
��GXAªƒ øYÓ @ðPñ’�J�K B
��
èQÒªË@ ø
Xð@ à@ é�&lt;Ë@ áÓ ú æÖ
ß~
@�I�J»ð H. Aƒ~ ú
�æ�KB . �éÓQ��jÖÏ@ð
�
,ÈA�JÖÏ@ YJ�ªK. @ �Yë à@ ðYJ.K� �àA¿ð ,úæ”~¯B@ Yj. ‚ÖÏAK. @PðQÓ
�
à @ ú �æÒ�J�K à@ áºÜØ 1/2 K@ Èñ�®K� �àA¿ �éJ��JÓB@ ©Ò‚~
Yg@ð É3/4 ¯
�
. éÊJ�j��‚Ó 1/2�J���JÓ
��@ �àB Aëñ�®�®m�
�� 1/4XA�®k�@ XA�®k@
lA ttSwrwA mdý1 sςAdty ςnd qrA’h2 h6h3
AltHlylAt AlrAˆyςh wAlmHtrmh4.5 lÂnny6 šAb
wknt7 Âtmný8 mn Allh Ân9 Â ˆwdy Alςmrh
mrwrA bAlmsjd AlÂqSý10,11 wkAn12 ybdw13
Ân14 h6A bςyd AlmnAl,15 fkl wAHd18 ysmς
AlÂmnyh19 kAn yqwl20 Ânk21 mmkn Ân22
ttmný23 Ân24 ÂHfAd ÂHfAdk yHqqwhA lÂn25
Âmnytk26 mstHylh.
Translation
</equation>
<construct confidence="0.60070825">
You cannot imagine the extent of my happiness when I read these wonderful and respectful analyses
because I am a young man and I wish from God to perform Umrah passing through the Al-Aqsa
Mosque; and it seemed that this was elusive that when anyone heard the wish, he would say that you
can wish that your great grandchildren may achieve it because your wish is impossible.
</construct>
<tableCaption confidence="0.9622065">
Table 1: A sample of an original (erroneous) text along with its manual correction and English translation.
The indices in the table are linked with those in Table 2 and the Appendix.
</tableCaption>
<table confidence="0.9997402">
# Error Correction Error Type Correction Action
#1 mdy ø mdý øYÓ Ya/Alif-Maqsura Spelling Edit
YÓ
#9 An v@ Ân v@ Alif-Hamza Spelling Edit
#11 Missing Comma , Punctuation Add_before
#12 w kAn oA¿ ð wkAn vA¿ð Extra Space Merge
#13 ybdwA @ðYJ.K� ybdw ðYJ.K Morphology Edit
#20 yqwl Èñ�®K� Dialectal Edit
byqwl Èñ�®J�K. �
#25 yHqqwhAlÂn 0�BAëñ dm�� yHqqwhA lÂn 0B Aëñ�®�®m� � Missing Space Split
</table>
<tableCaption confidence="0.713643857142857">
Table 2: Error type and correction action for a few examples extracted from the sentence pair in Table 1.
The indices are linked to those in Table 1 and the Appendix.
oped for similar error correction competitions is
adopted: system outputs are compared against
gold annotations using Precision, Recall and F1.
Systems are ranked based on the F1 scores ob-
tained on the test set.
</tableCaption>
<bodyText confidence="0.999845777777778">
After the initial registration, the participants
were provided with training and development sets
and evaluation scripts. During the test period, the
teams were given test data on which they needed
to run their systems. Following the announcement
of system results, the answer key to the test set was
released. Participants authored description papers
which will be presented in the Arabic NLP work-
shop.
</bodyText>
<sectionHeader confidence="0.993659" genericHeader="method">
3 The QALB Corpus
</sectionHeader>
<bodyText confidence="0.999681833333333">
One of the goals of the QALB project is to create
a large manually corrected corpus of errors for a
variety of Arabic texts, including user comments
on news web sites, native and non-native speaker
essays, and machine translation output. Within the
framework of this project, comprehensive annota-
tion guidelines and a specialized web-based anno-
tation interface have been developed (Zaghouani
et al., 2014; Obeid et al., 2013).
The annotation process includes an initial au-
tomatic pre-processing step followed by an auto-
matic correction of common spelling errors by the
</bodyText>
<page confidence="0.996851">
40
</page>
<bodyText confidence="0.999984342105263">
morphological analysis and disambiguation sys-
tem MADA (v3.2) (Habash and Rambow, 2005;
Habash et al., 2009). The pre-processed files are
then assigned to a team of expert (human) annota-
tors.
For a given sentence, the annotators were
instructed to correct all errors; these include
spelling, punctuation, word choice, morphology,
syntax, and dialectal usage. It should be noted that
the error classification was only used for guiding
the annotation process; the annotators were not in-
structed to mark the type of error but only needed
to specify an appropriate correction.
Once the annotation was complete, the correc-
tions were automatically grouped into the follow-
ing seven action categories based on the action
required to correct the error: {Edit, Add, Merge,
Split, Delete, Move, Other}.2 Table 1 presents
a sample erroneous Arabic news comment along
with its manually corrected form, its romanized
transliteration,3 and the English translation. The
errors in the original and the corrected forms are
underlined and co-indexed. Table 2 presents a sub-
set of the errors from the example shown in Table 1
along with the error types and annotation actions.
The Appendix at the end of the paper lists all an-
notation actions for that example.
To ensure high annotation quality, the annota-
tors went through multiple phases of training; the
inter-annotator agreement was reviewed routinely.
Zaghouani et al. (2014) report an average Word
Error Rate (WER) of 3.8% for all words (exclud-
ing punctuation), which is quite high. When punc-
tuation was included, the WER rose to 11.3%. The
high level of agreement indicates that the anno-
tations are reliable and the guidelines are useful
in producing homogeneous and consistent data.
Punctuation, however, remains a challenge.
</bodyText>
<sectionHeader confidence="0.978061" genericHeader="method">
4 Shared Task Data
</sectionHeader>
<bodyText confidence="0.9993616">
The shared task data comes from the QALB cor-
pus and consists of user comments from the Al-
jazeera News webpage written in Modern Stan-
dard Arabic.
Comments belonging to the same article were
</bodyText>
<footnote confidence="0.523990571428571">
2In the shared task, we specified two Add categories: ad-
dBefore and addAfter. Most of the add errors fall into the
first category, and we combine these here into a single Add
category.
3Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al., 2007): (in alphabetical
order) AbtθjHxdðrzsšSDT ˇDςγfqklmnhwy and the additional
</footnote>
<table confidence="0.7424746">
symbols: ’ r., Â I, AˇI�, A¯�I, wˆ j, yˆSS , ¯h o, ý S.
Statistics Train. Dev. Test
Number of docs. 19,411 1,017 968
Number of words 1M 54K 51K
Number of errors 306K 16K 16K
</table>
<tableCaption confidence="0.998314">
Table 3: Statistics on the shared task data.
</tableCaption>
<bodyText confidence="0.999954795454545">
included only in one of the shared task subsets
(i.e., training, development or test). Furthermore,
we split the data by the annotation time. Conse-
quently, the training data is comprised of com-
ments annotated between June and December,
2013; the development data includes texts anno-
tated in December 2013; the test data includes
documents annotated in the Spring of 2014.
We refer to each comment in the shared task
data as document and assign it a special ID that
indicates the ID of the article to which the com-
ment refers and the comment’s number.
The data was made available to the participants
in three versions: (1) plain text, one document per
line; (2) text with annotations specifying errors
and the corresponding corrections; (3) feature files
specifying morphological information obtained by
running MADAMIRA, a tool for morphological
analysis and disambiguation of Modern Standard
Arabic (Pasha et al., 2014). MADAMIRA per-
forms morphological analysis and contextual dis-
ambiguation. Using the output of MADAMIRA,
we generated for each word thirty-three features.
The features specify various properties: the part-
of-speech (POS), lemma, aspect, person, gender,
number, and so on.
Among its features, MADAMIRA produces
forms that correct a large subset of a special class
of spelling mistakes in words containing the let-
ters Alif and final Ya. These letters are a source of
the most common spelling types of spelling errors
in Arabic and involve Hamzated Alifs and Alif-
Maqsura/Ya confusion (Habash, 2010; El Kholy
and Habash, 2012). We refer to these errors as
Alif/Ya errors (see also Section 6).
Table 3 presents statistics on the shared task
data. The training data contains over one mil-
lion words of text; the development and test data
contain slightly over 50,000 words each. Table 4
shows the distribution of annotations by the action
type. The majority of corrections (over 50%) be-
long to the type Edit. This is followed by mistakes
that require several words to be merged together
(about a third of all errors).
</bodyText>
<page confidence="0.998699">
41
</page>
<table confidence="0.999324">
Data Edit Add Error type (%) Move Other
Merge Split Delete
Train. 55.34 32.36 5.95 3.48 2.21 0.14 0.50
Dev. 53.51 34.24 5.97 3.67 2.03 0.08 0.49
Test 51.94 34.73 5.89 3.48 3.32 0.15 0.49
</table>
<tableCaption confidence="0.965611">
Table 4: Distribution of annotations by type in the shared task data. Error types denotes the action
required in order to correct the error.
</tableCaption>
<table confidence="0.995776923076923">
Team Name Affiliation
CLMB (Rozovskaya et al., 2014) Columbia University (USA)
CMUQ (Jeblee et al., 2014) Carnegie Mellon University in Qatar (Qatar)
CP13 (Tomeh et al., 2014) Université Paris 13 (France)
CUFE (Nawar and Ragheb, 2014) Computer Engineering Department, Cairo University
(Egypt)
GLTW (Zerrouki et al., 2014) Bouira University (Algeria), The National Computer Sci-
ence Engineering School (Algeria), and Tabuk University
(KSA)
GWU (Attia et al., 2014) George Washington University (USA)
QCRI (Mubarak and Darwish, 2014) Qatar Computing Research Institute (Qatar)
TECH (Mostefa et al., 2014) Techlimed.com (France)
YAM (Hassan et al., 2014) Faculty of Engineering, Cairo University (Egypt)
</table>
<tableCaption confidence="0.963114">
Table 5: List of the nine teams that participated in the shared task.
</tableCaption>
<table confidence="0.999632833333333">
Team Approach External Resources
CLMB Corrections proposed by MADAMIRA; a Maximum Likeli- Arabic Gigaword Fourth Edition (Parker et al.,
hood model trained on the training data; regular expressions; 2009)
a decision-tree classifier for punctuation errors trained on the
training data; an SVM character-level error correction model; a
Naïve Bayes classifier trained on the training data and the Ara-
bic Gigaword corpus
CMUQ A pipeline consisting of rules, corrections proposed by AraComLex dictionary (Attia et al., 2012)
MADAMIRA, a language model for spelling mistakes, and a
statistical machine-translation system
CP13 None
A pipeline that consists of an error detection SVM clas-
sifier that uses MADAMIRA features and language model
scores; a character-level back-off correction model imple-
mented as a weighted finite-state transducer; a statistical
machine-translation system; a discriminative re-ranker; a de-
cision tree classifier for inserting missing punctuation
CUFE Rules extracted from the Buckwalter morphological analyser; Buckwalter morphological analyzer Version 2.0
their probabilities are learned using the training data (Buckwalter, 2004)
GLTW Regular expressions and word lists AraComLex dictionary (Attia et al., 2012); in-
house resources; Ayaspell dictionary
GWU A CRF model for punctuation errors; a dictionary and a lan- AraComLex Extended dictionary (Attia et al.,
guage model for spelling errors; normalization rules 2012); Arabic Gigaword Fourth Edition (Parker
et al., 2009)
QCRI Word errors: a language model trained on Arabic Wikipedia Arabic Wikipedia; Aljazeera articles
and Aljazeera data; punctuation mistakes: a CRF model and a
frequency-based model trained on the shared task data
TECH Off-the-shelf spell checkers and a statistical machine- Newspaper articles from Open Source Arabic
translation model Corpora; other corpora collected online; Hun-
spell
YAM Edit errors: a Naïve Bayes classifier that uses the following fea- AraComLex dictionary (Attia et al., 2012);
tures: a character confusion matrix based on the training data; a Buckwalter Analyzer Version 1.0 (Buckwalter,
collocation model that uses the target lemma and the surround- 2002); Arabic stoplists
ing POS tags; a co-occurrence model that uses lemmata of the
surrounding words; Split and Merge errors: a language model
trained on the training data; Add errors: a classifier
</table>
<tableCaption confidence="0.999037">
Table 6: Approaches adopted by the participating teams.
</tableCaption>
<page confidence="0.998468">
42
</page>
<sectionHeader confidence="0.967754" genericHeader="method">
5 Participants and Approaches
</sectionHeader>
<bodyText confidence="0.999977809523809">
Nine teams from six countries participated in the
shared task. Table 5 presents the list of partici-
pating institutions and their names in the shared
task. Each team was allowed to submit up to
three outputs. Overall, we received eighteen out-
puts. The submitted systems included a diverse
set of approaches that incorporated rule-based
frameworks, statistical machine translation and
machine learning models, as well as hybrid sys-
tems. The teams that scored at the top employed
a variety of techniques and attempted to classify
the errors in some way using that classification
in developing their systems: the CLMB system
combined machine-learning modules with rules
and MADAMIRA corrections; the CUFE system
extracted rules from the morphological analyzer
and learned their probabilities using the training
data; and the CMUQ system combined statistical
machine-translation with a language model, rules,
and MADAMIRA corrections. Table 6 summa-
rizes the approaches adopted by each team.
</bodyText>
<sectionHeader confidence="0.999952" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.971684676923077">
In this section, we present the results of the com-
petition. For evaluation, we adopted the stan-
dard Precision (P), Recall (R), and F1 metric that
was used in recent shared tasks on grammatical
error correction in English: HOO competitions
(Dale and Kilgarriff, 2011; Dale et al., 2012) and
CoNLL (Ng et al., 2013). The results are com-
puted using the M2 scorer (Dahlmeier and Ng,
2012) that was also used in the CoNLL shared
tasks.
Table 7 presents the official results of the evalu-
ation on the test set. The results are sorted accord-
ing to the F1 scores obtained by the systems. The
range of the scores is quite wide – from 20 to 67
F1 – but the the majority of the systems stay in the
50-60 range.
It is interesting to note that these results are con-
siderably higher than those shown on the similar
shared tasks on English non-native data. For in-
stance, the highest performance in the CoNLL-
2013 shared task that also used the same evalua-
tion metric was 31.20 (Rozovskaya et al., 2013).4
The highest score in the HOO-2011 shared task
(Dale and Kilgarriff, 2011) that addressed all er-
4This year CoNLL was an extension of the CoNLL-2013
competition for all errors but in its evaluation favored preci-
sion twice as much as recall, so we are not comparing to this
setting.
rors was 21.1 (Rozovskaya et al., 2011). Of
course, the setting was different,as we are deal-
ing with texts written by native speakers. But, in
addition to that, we hypothesize that our data con-
tains a set of language-specific errors that may be
“easier”, e.g Alif/Ya errors.
We also asked the participants for the outputs of
their systems on the development set. We show the
results in Table 8. While these results are not used
for ranking, since the development set was used
for tuning the parameters of the systems, it is in-
teresting to see how much the performance differs
from the results obtained on the test. In general,
we do not observe substantial differences in the
performance and the rankings, with a few excep-
tions. In particular, CP13 submissions did much
better on the development set, as well as the CUFE
system: the CUFE system suffers a major drop in
precision on the test set, while the CP13 systems
lose in recall. For more details, we refer the reader
to the system description papers.
In addition to the official rankings, it is also in-
teresting to analyze system performance for dif-
ferent types of mistakes. Note that here we are
not interested in the annotation classification by
action type. Instead, we automatically assign er-
rors to one of the following categories: punctu-
ation errors;errors involving Alif and Ya; and all
other errors. Punctuation errors account for 39%
of all errors in the data5 . Table 7 shows the perfor-
mance of the teams in three settings: with punctu-
ation errors removed; with Alif/Ya errors removed;
and when both punctuation and Alif/Ya errors are
removed. Observe that when punctuation errors
are not taken into account, the CUFE team gets
the first ranking (for each the results of the best-
performing system were chosen).
</bodyText>
<subsectionHeader confidence="0.721174">
7 Analysis of System Output
</subsectionHeader>
<bodyText confidence="0.999268777777778">
We conducted a couple of experiments to analyze
the task challenges and system errors.
The Most and Least Challenging Sentences
We examined some of the most, and the least chal-
lenging parts of the test data for the shared task
systems. To identify these subsets, we ranked all
sentences using their average sentence-level F1
score and selected the top and bottom 50 sen-
tences. Our manual examination of these two
</bodyText>
<footnote confidence="0.971518">
5For example, there are a lot of missing periods at the end
of a sentence that may be due to the fact that the data was
collected online.
</footnote>
<page confidence="0.999359">
43
</page>
<table confidence="0.999440263157895">
Rank Team P R F1
1 CLMB-1 73.34 63.23 67.91
2 CLMB-2 70.86 62.21 66.25
3 CUFE-1 87.49 52.63 65.73
4 CMUQ-1 77.97 56.35 65.42
5 CLMB-3 71.45 60.00 65.22
6 QCRI-1 71.70 56.86 63.43
7 GWU-1 75.47 52.98 62.25
8 GWU-2 75.34 52.99 62.22
9 QCRI-2 62.86 60.32 61.57
10 YAM-1 63.52 57.61 60.42
11 QCRI-3 60.66 59.28 59.96
12 TECH-1 73.46 50.56 59.89
13 TECH-2 73.50 50.53 59.88
14 TECH-3 72.34 50.51 59.49
15 CP13-2 76.85 47.33 58.58
16 CP13-1 77.85 38.77 51.76
17 GLTW-1 75.15 23.15 35.40
18 GLTW-2 69.80 12.33 20.96
</table>
<tableCaption confidence="0.965818">
Table 7: Official results on the test set. Column 1
</tableCaption>
<bodyText confidence="0.97833">
shows the system rank according to the F1 score.
sets shows that the differences between them re-
late to both the density and the type of errors.
The more challenging sentences (with the lowest
system performance) contain more errors in gen-
eral, and their errors tend to be complex and chal-
lenging, e.g., the correction of the erroneous two-
token string ~I« uX@ (Âdt St) requires a charac-
ter deletion and a merge to produce :,_rX@ (AdSt).
In contrast the less challenging sentences tend to
have fewer and simpler errors such as the common
Alif/Ya errors.
System Combination We took the 18 systems’
output and conducted two system combination ex-
periments: (a) an oracle upper-bound estimation
and (b) a simple majority vote system combina-
tion. For these experiments we isolated and eval-
uated each sentence output individually to form a
new combined system output.
In the oracle experiment, we combined differ-
ent systems by selecting the output of the best per-
forming system for each individual sentence. For
that, we evaluated sentences individually for each
system and chose the system output with the high-
est F1 score. The combined output holds the best
output of all systems for the test set. This is an
oracle system combination which allows us to es-
timate an upper-bound combination of all 18 sys-
tems.
</bodyText>
<table confidence="0.999708789473684">
Rank (test) Rank (dev) Team P R F1
1 2 CLMB-1 72.22 62.79 67.18
2 3 CLMB-2 69.49 61.73 65.38
3 1 CUFE-1 94.11 53.74 68.42
4 4 CMUQ-176.17 56.59 64.94
5 5 CLMB-3 69.71 59.42 64.15
6 6 QCRI-1 70.83 57.34 63.38
7 9 GWU-1 73.15 53.18 61.59
8 10 GWU-2 73.01 53.13 61.50
9 8 QCRI-2 62.21 61.30 61.75
10 14 YAM-1 57.81 59.19 58.49
11 12 QCRI-3 60.47 60.65 60.56
12 13 TECH-1 70.86 50.04 58.66
13 15 TECH-2 70.66 49.65 58.32
14 16 TECH-3 70.65 48.83 57.75
15 7 CP13-2 74.85 54.15 62.84
16 11 CP13-1 75.73 51.33 61.19
17 17 GLTW-1 73.83 22.80 34.84
18 18 GLTW-2 67.85 11.09 19.06
</table>
<tableCaption confidence="0.996512">
Table 8: Results on the development set.
</tableCaption>
<bodyText confidence="0.996125708333333">
Columns 1 and 2 show the rank of the system ac-
cording to F1 score obtained on the test set shown
in Table 7 and the development set, respectively.
In the majority vote experiment, we combined
system output based on majority vote of various
systems at sentence level. For every sentence,
we choose the output that is agreed by most sys-
tems. If all systems have different output for a sen-
tence, we back-off to the best performing system
(CLMB-1).
Table 10 compares the results of these two
experiments against the best performing system
(CLMB-1). We observe a large boost of perfor-
mance in the oracle experiment. This promis-
ing result reflects the complementary nature of the
different methods that have been applied to the
shared task, and it motivates further research on
system combination. The result for the majority-
vote system combination is very close to the
CLMB-1’s performance. This is not surpris-
ing; since, for 92% of sentences, there was no
sentence-level agreement among systems. As a re-
sult, the combined system was very close to the
back-off CLMB-1 system.
</bodyText>
<sectionHeader confidence="0.991093" genericHeader="method">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99998225">
We have described the framework and results of
the first shared task on automatic correction of
Arabic, which used data from the QALB corpus.
The shared task received 18 systems submissions
</bodyText>
<page confidence="0.998393">
44
</page>
<table confidence="0.999730833333333">
Team No punc. R errors No Alif/Ya R errors No Alif/Ya No errors
P F1 P F1 P punc. F1
R
CLMB-1 82.63 72.50 77.24 64.05 50.86 56.70 76.99 49.91 60.56
CMUQ-1 82.89 68.69 75.12 68.32 40.51 50.86 74.25 41.46 53.21
CP13-2 80.51 59.97 68.74 65.09 28.00 39.16 68.67 25.34 37.02
CUFE-1 85.22 78.79 81.88 83.34 36.21 50.48 80.63 63.25 70.89
GLTW-1 65.18 34.84 45.41 48.52 15.29 23.26 49.25 26.78 34.70
GWU-1 76.28 64.17 69.70 64.67 39.61 49.13 59.07 41.48 48.74
QCRI-1 76.74 74.93 75.82 59.66 41.90 49.23 63.22 55.10 58.88
TECH-1 81.23 62.99 70.95 59.39 34.59 43.72 64.93 35.69 46.06
YAM-1 77.38 63.99 70.05 50.77 43.43 46.81 64.63 34.71 45.17
</table>
<tableCaption confidence="0.934148">
Table 9: Results on the test set in different settings: with punctuation errors removed from evaluation;
normalization errors removed; and when both punctuation and normalization errors are removed. Only
the best output from each team is shown.
</tableCaption>
<table confidence="0.99949525">
System Precision Recall Fi
Oracle 83.25 68.72 75.29
Majority-Vote 73.96 62.88 67.97
CLMB-1 73.34 63.23 67.91
</table>
<tableCaption confidence="0.6433795">
Table 10: Comparing the best performing system
with two experimental hybrid systems.
</tableCaption>
<bodyText confidence="0.9995122">
from nine teams in six countries. We are pleased
with the extent of participation, the quality of re-
sults and the diversity of approaches. We plan to
release the output of all systems. Such dataset and
all the methods used in this shared task are ex-
pected to introduce new directions in automatic
correction of Arabic. We feel motivated to ex-
tend the shared task’s framework and text domain
to conduct new research competitions in the near
future.
</bodyText>
<sectionHeader confidence="0.975255" genericHeader="method">
9 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999898944444445">
We would like to thank the organizing commit-
tee of EMNLP-2014 and its Arabic NLP work-
shop and also the shared task participants for their
ideas and support. We thank Al Jazeera News (and
especially, Khalid Judia) for providing the user
comments portion of the QALB corpus. We also
thank the QALB project annotators: Hoda Fathy,
Dhoha Abid, Mariem Fekih, Anissa Jrad, Hoda
Ibrahim, Noor Alzeer, Samah Lakhal, Jihene Wefi,
Elsherif Mahmoud and Hossam El-Husseini. This
publication was made possible by grants NPRP-
4-1058-1-168 and YSREP-1-018-1-004 from the
Qatar National Research Fund (a member of the
Qatar Foundation). The statements made herein
are solely the responsibility of the authors. Nizar
Habash performed most of his contribution to this
paper while he was at the Center for Computa-
tional Learning Systems at Columbia University.
</bodyText>
<sectionHeader confidence="0.998089" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.998027321428571">
A. Alfaifi and E. Atwell. 2012. Arabic Learner Cor-
pora (ALC): A Taxonomy of Coding Errors. In The
8th International Computing Conference in Arabic.
M. Attia, P. Pecina, Y. Samih, K. Shaalan, and J. van
Genabith. 2012. Improved Spelling Error Detection
and Correction for Arabic. In Proceedings of COL-
ING.
M. Attia, M. Al-Badrashiny, and M. Diab. 2014.
GWU-HASP: Hybrid Arabic Spelling and Punctua-
tion Corrector. In Proceedings of EMNLP Workshop
on Arabic Natural Language Processing: QALB
Shared Task.
T. Buckwalter. 2002. Buckwalter Arabic Morpho-
logical Analyzer Version 1.0.
T. Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0.
D. Dahlmeier and H. T. Ng. 2012. Better Evaluation
for Grammatical Error Correction. In Proceedings
of NAACL.
R. Dale and A. Kilgarriff. 2011. Helping Our Own:
The HOO 2011 Pilot Shared Task. In Proceedings of
the 13th European Workshop on Natural Language
Generation.
R. Dale, I. Anisimoff, and G. Narroway. 2012. A Re-
port on the Preposition and Determiner Error Cor-
rection Shared Task. In Proceedings of the NAACL
Workshop on Innovative Use of NLP for Building
Educational Applications.
</reference>
<page confidence="0.994074">
45
</page>
<reference confidence="0.997987805825242">
A. El Kholy and N. Habash. 2012. Orthographic and
morphological processing for English–Arabic sta-
tistical machine translation. Machine Translation,
26(1-2).
S. Farwaneh and M. Tamimi. 2012. Arabic Learn-
ers Written Corpus: A Resource for Research and
Learning. The Center for Educational Resources in
Culture, Language and Literacy.
N. Habash and O. Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proceed-
ings of ACL.
N. Habash, A. Soudi, and T. Buckwalter. 2007.
On Arabic Transliteration. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
N. Habash, O. Rambow, and R. Roth. 2009.
MADA+TOKAN: A Toolkit for Arabic Tokeniza-
tion, Diacritization, Morphological Disambiguation,
POS Tagging, Stemming and Lemmatization. In
Proceedings of the Second International Conference
on Arabic Language Resources and Tools.
N. Habash. 2010. Introduction to Arabic Natural Lan-
guage Processing. Morgan &amp; Claypool Publishers.
Y. Hassan, M. Aly, and A. Atiya. 2014. Arabic
Spelling Correction using Supervised Learning. In
Proceedings of EMNLP Workshop on Arabic Natu-
ral Language Processing: QALB Shared Task.
S. Jeblee, H. Bouamor, W. Zaghouani, and K. Oflazer.
2014. CMUQ@QALB-2014: An SMT-based Sys-
tem for Automatic Arabic Error Correction. In Pro-
ceedings of EMNLP Workshop on Arabic Natural
Language Processing: QALB Shared Task.
D. Mostefa, O. Asbayou, and R. Abbes. 2014. TECH-
LIMED System Description for the Shared Task on
Automatic Arabic Error Correction. In Proceedings
of EMNLP Workshop on Arabic Natural Language
Processing: QALB Shared Task.
H. Mubarak and K. Darwish. 2014. Automatic Cor-
rection of Arabic Text: a Cascaded Approach. In
Proceedings of EMNLP Workshop on Arabic Natu-
ral Language Processing: QALB Shared Task.
M. Nawar and M. Ragheb. 2014. Fast and Robust
Arabic Error Correction System. In Proceedings
of EMNLP Workshop on Arabic Natural Language
Processing: QALB Shared Task.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The CoNLL-2013 Shared Task
on Grammatical Error Correction. In Proceedings
of CoNLL: Shared Task.
H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H.
Susanto, and C. Bryant. 2014. The CoNLL-2014
Shared Task on Grammatical Error Correction. In
Proceedings of CoNLL: Shared Task.
O. Obeid, W. Zaghouani, B. Mohit, N. Habash,
K. Oflazer, and N. Tomeh. 2013. A Web-based An-
notation Framework For Large-Scale Text Correc-
tion. In The Companion Volume of the Proceedings
of IJCNLP 2013: System Demonstrations. Asian
Federation of Natural Language Processing.
R. Parker, D. Graff, K. Chen, J. Kong, and K. Maeda.
2009. Arabic Gigaword Fourth Edition. LDC Cata-
log No.: LDC2009T30, ISBN: 1-58563-532-4.
A. Pasha, M. Al-Badrashiny, M. Diab, A. El Kholy,
R. Eskander, N. Habash, M. Pooleery, O. Rambow,
and R. Roth. 2014. MADAMIRA: A Fast, Compre-
hensive Tool for Morphological Analysis and Dis-
ambiguation of Arabic. In Proceedings of the Ninth
International Conference on Language Resources
and Evaluation (LREC).
A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth.
2011. University of Illinois System in HOO Text
Correction Shared Task. In Proceedings of the Eu-
ropean Workshop on Natural Language Generation
(ENLG).
A. Rozovskaya, K.-W. Chang, M. Sammons, and
D. Roth. 2013. The University of Illinois System
in the CoNLL-2013 Shared Task. In Proceedings of
CoNLL Shared Task.
A. Rozovskaya, N. Habash, R. Eskander, N. Farra, and
W. Salloum. 2014. The Columbia System in the
QALB-2014 Shared Task on Arabic Error Correc-
tion. In Proceedings of EMNLP Workshop on Ara-
bic Natural Language Processing: QALB Shared
Task.
N. Tomeh, N. Habash, R. Eskander, and J. Le Roux.
2014. A Pipeline Approach to Supervised Error
Correction for the QALB-2014 Shared Task. In Pro-
ceedings of EMNLP Workshop on Arabic Natural
Language Processing: QALB Shared Task.
W. Zaghouani, B. Mohit, N. Habash, O. Obeid,
N. Tomeh, A. Rozovskaya, N. Farra, S. Alkuhlani,
and K. Oflazer. 2014. Large Scale Arabic Error An-
notation: Guidelines and Framework. In Proceed-
ings of the Ninth International Conference on Lan-
guage Resources and Evaluation (LREC). European
Language Resources Association (ELRA).
T. Zerrouki, K. Alhawiti, and A. Balla. 2014. Au-
tocorrection Of Arabic Common Errors For Large
Text Corpus. In Proceedings of EMNLP Workshop
on Arabic Natural Language Processing: QALB
Shared Task.
</reference>
<page confidence="0.99982">
46
</page>
<sectionHeader confidence="0.976609" genericHeader="method">
Appendix A: Sample annotation file
</sectionHeader>
<bodyText confidence="0.999804166666667">
Below is the complete list of correction actions for the example in Table 1 as they appear in the training
and evaluation data. The first two columns are the error index linking to Table 1 and the original word,
respectively. Only the column titled Correction Action is in the training and evaluation data. The two
numbers following the A specify the start and end positions of the sentence token string to change.
Following that (and delimited by  ) are the action type and the correction string. The last three fields
are irrelevant to this discussion.
</bodyText>
<figure confidence="0.998878333333334">
Error Index Original Word Correction Action
#1 ø �Yë REQUIRED
#2 YÓ úB REQUIRED
#3 é úæ”�B@ REQUIRED
#4 �K@Q�¯ ��
#5 ~ B
#6 è Yë A 2 3 Edit øYÓ REQUIRED -NONE- 0
#7 ��jÖÏ@ ð A 5 6 Edit �èZ@Q�¯ REQUIRED -NONE- 0
#8 éÓQ� �
#9 �� A 6 7 Edit è -NONE- 0
#10 � G@B A 9 11 Merge �éÓQ��jÖÏ@ð REQUIRED -NONE- 0
#11 � A 11 11 Add_before . REQUIRED -NONE- 0
#12 I�J» ð A 11 12 Edit ú� æ K
#13 �. -NONE- 0
#14 �JKA A 13 15 Merge  �I�J»ð REQUIRED -NONE- 0
#15 úæÒ �
#16 � 15 16 Edit ú æÖ~ß @ REQUIRED -NONE- 0
#17 � �
#18 à@ A 18 19 Edit  �à@ REQUIRED -NONE- 0
#19 ¯B@ A 23 24 Edit úæ”�¯�
#20 � -NONE- 0
#21 àA¿ ð A 24 24 Add_before , REQUIRED -NONE- 0
#22 @ðYJ.K A 24 26 Merge �àA¿ð REQUIRED -NONE- 0
#23 � A 26 27 Edit ðYJ.K� REQUIRED -NONE- 0
#24 à@ �
#25 AÓ A 27 28 Edit  �à@ REQUIRED -NONE- 0
#26 ú¯ A 31 31 Add_before , REQUIRED -NONE- 0
� A 32 33 Delete  REQUIRED -NONE- 0
Yg A 33 34 Delete  REQUIRED -NONE- 0
� A 34 35 Edit Yg@ð REQUIRED -NONE- 0
éJ�JÓB@ A 36 37 Edit +J��JÓB@ REQUIRED -NONE- 0
Èñ�®J�K. A 38 39 Edit Èñ�®K� REQUIRED -NONE- 0
1/2;@ A 39 40 Edit 1/2%@ REQUIRED -NONE- 0
� �� �
úJ A 41 41 Add_before  �à@ REQUIRED -NONE- 0
� æÒK A 41 42 Edit ú æÒ~J~K REQUIRED -NONE- 0
� �
à@ A 42 43 Edit  �à@ REQUIRED -NONE- 0
àBAëñ�®�®m� � Aëñ�®�®m�
1/2�J���JÓ@ A 45 46 Split  �à�� REQUIRED -NONE- 0
�
A 46 47 Edit 1/2�J���JÓ@ REQUIRED -NONE- 0
</figure>
<page confidence="0.987191">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.368670">
<title confidence="0.999679">The First QALB Shared Task on Automatic Text Correction for Arabic</title>
<author confidence="0.924649">Alla Nizar Wajdi Ossama</author>
<affiliation confidence="0.676498666666667">Mellon University in Qatar for Computational Learning Systems, Columbia York University Abu Dhabi</affiliation>
<email confidence="0.9707235">behrang@cmu.edu,alla@ccls.columbia.edu,wajdiz@qatar.cmu.edu,owo@qatar.cmu.edu</email>
<abstract confidence="0.999768833333333">We present a summary of the first shared task on automatic text correction for Arabic text. The shared task received 18 systems submissions from nine teams in six countries and represented a diversity of approaches. Our report includes an overview of the QALB corpus which was the source of the datasets used for training and evaluation, an overview of participating systems, results of the competition and an analysis of the results and systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>pora</author>
</authors>
<title>A Taxonomy of Coding Errors.</title>
<booktitle>In The 8th International Computing Conference in Arabic.</booktitle>
<marker>pora, </marker>
<rawString>pora (ALC): A Taxonomy of Coding Errors. In The 8th International Computing Conference in Arabic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Attia</author>
<author>P Pecina</author>
<author>Y Samih</author>
<author>K Shaalan</author>
<author>J van Genabith</author>
</authors>
<title>Improved Spelling Error Detection and Correction for Arabic.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING.</booktitle>
<marker>Attia, Pecina, Samih, Shaalan, van Genabith, 2012</marker>
<rawString>M. Attia, P. Pecina, Y. Samih, K. Shaalan, and J. van Genabith. 2012. Improved Spelling Error Detection and Correction for Arabic. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Attia</author>
<author>M Al-Badrashiny</author>
<author>M Diab</author>
</authors>
<title>GWU-HASP: Hybrid Arabic Spelling and Punctuation Corrector.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</booktitle>
<contexts>
<context position="12808" citStr="Attia et al., 2014" startWordPosition="2082" endWordPosition="2085"> 3.32 0.15 0.49 Table 4: Distribution of annotations by type in the shared task data. Error types denotes the action required in order to correct the error. Team Name Affiliation CLMB (Rozovskaya et al., 2014) Columbia University (USA) CMUQ (Jeblee et al., 2014) Carnegie Mellon University in Qatar (Qatar) CP13 (Tomeh et al., 2014) Université Paris 13 (France) CUFE (Nawar and Ragheb, 2014) Computer Engineering Department, Cairo University (Egypt) GLTW (Zerrouki et al., 2014) Bouira University (Algeria), The National Computer Science Engineering School (Algeria), and Tabuk University (KSA) GWU (Attia et al., 2014) George Washington University (USA) QCRI (Mubarak and Darwish, 2014) Qatar Computing Research Institute (Qatar) TECH (Mostefa et al., 2014) Techlimed.com (France) YAM (Hassan et al., 2014) Faculty of Engineering, Cairo University (Egypt) Table 5: List of the nine teams that participated in the shared task. Team Approach External Resources CLMB Corrections proposed by MADAMIRA; a Maximum Likeli- Arabic Gigaword Fourth Edition (Parker et al., hood model trained on the training data; regular expressions; 2009) a decision-tree classifier for punctuation errors trained on the training data; an SVM </context>
</contexts>
<marker>Attia, Al-Badrashiny, Diab, 2014</marker>
<rawString>M. Attia, M. Al-Badrashiny, and M. Diab. 2014. GWU-HASP: Hybrid Arabic Spelling and Punctuation Corrector. In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Buckwalter</author>
</authors>
<date>2002</date>
<journal>Buckwalter Arabic Morphological Analyzer Version</journal>
<volume>1</volume>
<marker>Buckwalter, 2002</marker>
<rawString>T. Buckwalter. 2002. Buckwalter Arabic Morphological Analyzer Version 1.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Buckwalter</author>
</authors>
<date>2004</date>
<journal>Buckwalter Arabic Morphological Analyzer Version</journal>
<volume>2</volume>
<contexts>
<context position="14269" citStr="Buckwalter, 2004" startWordPosition="2289" endWordPosition="2290">uage model for spelling mistakes, and a statistical machine-translation system CP13 None A pipeline that consists of an error detection SVM classifier that uses MADAMIRA features and language model scores; a character-level back-off correction model implemented as a weighted finite-state transducer; a statistical machine-translation system; a discriminative re-ranker; a decision tree classifier for inserting missing punctuation CUFE Rules extracted from the Buckwalter morphological analyser; Buckwalter morphological analyzer Version 2.0 their probabilities are learned using the training data (Buckwalter, 2004) GLTW Regular expressions and word lists AraComLex dictionary (Attia et al., 2012); inhouse resources; Ayaspell dictionary GWU A CRF model for punctuation errors; a dictionary and a lan- AraComLex Extended dictionary (Attia et al., guage model for spelling errors; normalization rules 2012); Arabic Gigaword Fourth Edition (Parker et al., 2009) QCRI Word errors: a language model trained on Arabic Wikipedia Arabic Wikipedia; Aljazeera articles and Aljazeera data; punctuation mistakes: a CRF model and a frequency-based model trained on the shared task data TECH Off-the-shelf spell checkers and a s</context>
</contexts>
<marker>Buckwalter, 2004</marker>
<rawString>T. Buckwalter. 2004. Buckwalter Arabic Morphological Analyzer Version 2.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahlmeier</author>
<author>H T Ng</author>
</authors>
<title>Better Evaluation for Grammatical Error Correction.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="16957" citStr="Dahlmeier and Ng, 2012" startWordPosition="2700" endWordPosition="2703">nd learned their probabilities using the training data; and the CMUQ system combined statistical machine-translation with a language model, rules, and MADAMIRA corrections. Table 6 summarizes the approaches adopted by each team. 6 Results In this section, we present the results of the competition. For evaluation, we adopted the standard Precision (P), Recall (R), and F1 metric that was used in recent shared tasks on grammatical error correction in English: HOO competitions (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013). The results are computed using the M2 scorer (Dahlmeier and Ng, 2012) that was also used in the CoNLL shared tasks. Table 7 presents the official results of the evaluation on the test set. The results are sorted according to the F1 scores obtained by the systems. The range of the scores is quite wide – from 20 to 67 F1 – but the the majority of the systems stay in the 50-60 range. It is interesting to note that these results are considerably higher than those shown on the similar shared tasks on English non-native data. For instance, the highest performance in the CoNLL2013 shared task that also used the same evaluation metric was 31.20 (Rozovskaya et al., 2013</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>D. Dahlmeier and H. T. Ng. 2012. Better Evaluation for Grammatical Error Correction. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>A Kilgarriff</author>
</authors>
<title>Helping Our Own: The HOO 2011 Pilot Shared Task.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="1229" citStr="Dale and Kilgarriff, 2011" startWordPosition="181" endWordPosition="184">es. Our report includes an overview of the QALB corpus which was the source of the datasets used for training and evaluation, an overview of participating systems, results of the competition and an analysis of the results and systems. 1 Introduction The task of text correction has recently gained a lot of attention in the Natural Language Processing (NLP) community. Most of the effort in this area concentrated on English, especially on errors made by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers who focus on this problem and promote development and dissemination of key resources, such as benchmark datasets. Recently, there have been several efforts aimed at creating data resources related to the correction of Arabic text. Those include human annotated corpora (Zaghouani et al., 2014; Alfaifi and Atwell, 2012), spell-checking lexicon (Attia et al., 2012) and unannotated language learner corpora (Farwaneh and Tamimi, 2012). A natural ex</context>
<context position="16838" citStr="Dale and Kilgarriff, 2011" startWordPosition="2677" endWordPosition="2680">e-learning modules with rules and MADAMIRA corrections; the CUFE system extracted rules from the morphological analyzer and learned their probabilities using the training data; and the CMUQ system combined statistical machine-translation with a language model, rules, and MADAMIRA corrections. Table 6 summarizes the approaches adopted by each team. 6 Results In this section, we present the results of the competition. For evaluation, we adopted the standard Precision (P), Recall (R), and F1 metric that was used in recent shared tasks on grammatical error correction in English: HOO competitions (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013). The results are computed using the M2 scorer (Dahlmeier and Ng, 2012) that was also used in the CoNLL shared tasks. Table 7 presents the official results of the evaluation on the test set. The results are sorted according to the F1 scores obtained by the systems. The range of the scores is quite wide – from 20 to 67 F1 – but the the majority of the systems stay in the 50-60 range. It is interesting to note that these results are considerably higher than those shown on the similar shared tasks on English non-native data. For instance, the highes</context>
</contexts>
<marker>Dale, Kilgarriff, 2011</marker>
<rawString>R. Dale and A. Kilgarriff. 2011. Helping Our Own: The HOO 2011 Pilot Shared Task. In Proceedings of the 13th European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>I Anisimoff</author>
<author>G Narroway</author>
</authors>
<title>A Report on the Preposition and Determiner Error Correction Shared Task.</title>
<date>2012</date>
<journal>Machine Translation,</journal>
<booktitle>In Proceedings of the NAACL Workshop on Innovative Use of NLP for Building Educational</booktitle>
<pages>26--1</pages>
<contexts>
<context position="1249" citStr="Dale et al., 2012" startWordPosition="185" endWordPosition="188">overview of the QALB corpus which was the source of the datasets used for training and evaluation, an overview of participating systems, results of the competition and an analysis of the results and systems. 1 Introduction The task of text correction has recently gained a lot of attention in the Natural Language Processing (NLP) community. Most of the effort in this area concentrated on English, especially on errors made by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers who focus on this problem and promote development and dissemination of key resources, such as benchmark datasets. Recently, there have been several efforts aimed at creating data resources related to the correction of Arabic text. Those include human annotated corpora (Zaghouani et al., 2014; Alfaifi and Atwell, 2012), spell-checking lexicon (Attia et al., 2012) and unannotated language learner corpora (Farwaneh and Tamimi, 2012). A natural extension to these res</context>
<context position="16858" citStr="Dale et al., 2012" startWordPosition="2681" endWordPosition="2684">es and MADAMIRA corrections; the CUFE system extracted rules from the morphological analyzer and learned their probabilities using the training data; and the CMUQ system combined statistical machine-translation with a language model, rules, and MADAMIRA corrections. Table 6 summarizes the approaches adopted by each team. 6 Results In this section, we present the results of the competition. For evaluation, we adopted the standard Precision (P), Recall (R), and F1 metric that was used in recent shared tasks on grammatical error correction in English: HOO competitions (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013). The results are computed using the M2 scorer (Dahlmeier and Ng, 2012) that was also used in the CoNLL shared tasks. Table 7 presents the official results of the evaluation on the test set. The results are sorted according to the F1 scores obtained by the systems. The range of the scores is quite wide – from 20 to 67 F1 – but the the majority of the systems stay in the 50-60 range. It is interesting to note that these results are considerably higher than those shown on the similar shared tasks on English non-native data. For instance, the highest performance in the</context>
</contexts>
<marker>Dale, Anisimoff, Narroway, 2012</marker>
<rawString>R. Dale, I. Anisimoff, and G. Narroway. 2012. A Report on the Preposition and Determiner Error Correction Shared Task. In Proceedings of the NAACL Workshop on Innovative Use of NLP for Building Educational Applications. morphological processing for English–Arabic statistical machine translation. Machine Translation, 26(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Farwaneh</author>
<author>M Tamimi</author>
</authors>
<title>Arabic Learners Written Corpus: A Resource for Research and Learning. The Center for Educational Resources in Culture, Language and Literacy.</title>
<date>2012</date>
<contexts>
<context position="1815" citStr="Farwaneh and Tamimi, 2012" startWordPosition="274" endWordPosition="277">recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers who focus on this problem and promote development and dissemination of key resources, such as benchmark datasets. Recently, there have been several efforts aimed at creating data resources related to the correction of Arabic text. Those include human annotated corpora (Zaghouani et al., 2014; Alfaifi and Atwell, 2012), spell-checking lexicon (Attia et al., 2012) and unannotated language learner corpora (Farwaneh and Tamimi, 2012). A natural extension to these resource production efforts is the creation of robust automatic systems for error correction. * These authors contributed equally to this work. In this paper, we present a summary of the QALB shared task on automatic text correction for Arabic. The Qatar Arabic Language Bank (QALB) project1 is one of the first large scale data and system development efforts for automatic correction of Arabic which has resulted in annotation of the QALB corpus. In conjunction with the EMNLP Arabic NLP workshop, the QALB shared task is the first community effort for construction an</context>
</contexts>
<marker>Farwaneh, Tamimi, 2012</marker>
<rawString>S. Farwaneh and M. Tamimi. 2012. Arabic Learners Written Corpus: A Resource for Research and Learning. The Center for Educational Resources in Culture, Language and Literacy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>O Rambow</author>
</authors>
<title>Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="7525" citStr="Habash and Rambow, 2005" startWordPosition="1221" endWordPosition="1224">ct is to create a large manually corrected corpus of errors for a variety of Arabic texts, including user comments on news web sites, native and non-native speaker essays, and machine translation output. Within the framework of this project, comprehensive annotation guidelines and a specialized web-based annotation interface have been developed (Zaghouani et al., 2014; Obeid et al., 2013). The annotation process includes an initial automatic pre-processing step followed by an automatic correction of common spelling errors by the 40 morphological analysis and disambiguation system MADA (v3.2) (Habash and Rambow, 2005; Habash et al., 2009). The pre-processed files are then assigned to a team of expert (human) annotators. For a given sentence, the annotators were instructed to correct all errors; these include spelling, punctuation, word choice, morphology, syntax, and dialectal usage. It should be noted that the error classification was only used for guiding the annotation process; the annotators were not instructed to mark the type of error but only needed to specify an appropriate correction. Once the annotation was complete, the corrections were automatically grouped into the following seven action cate</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>N. Habash and O. Rambow. 2005. Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>A Soudi</author>
<author>T Buckwalter</author>
</authors>
<title>On Arabic Transliteration.</title>
<date>2007</date>
<booktitle>Arabic Computational Morphology: Knowledge-based and Empirical Methods.</booktitle>
<editor>In A. van den Bosch and A. Soudi, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="9691" citStr="Habash et al., 2007" startWordPosition="1572" endWordPosition="1575">are reliable and the guidelines are useful in producing homogeneous and consistent data. Punctuation, however, remains a challenge. 4 Shared Task Data The shared task data comes from the QALB corpus and consists of user comments from the Aljazeera News webpage written in Modern Standard Arabic. Comments belonging to the same article were 2In the shared task, we specified two Add categories: addBefore and addAfter. Most of the add errors fall into the first category, and we combine these here into a single Add category. 3Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical order) AbtθjHxdðrzsšSDT ˇDςγfqklmnhwy and the additional symbols: ’ r., Â I, AˇI�, A¯�I, wˆ j, yˆSS , ¯h o, ý S. Statistics Train. Dev. Test Number of docs. 19,411 1,017 968 Number of words 1M 54K 51K Number of errors 306K 16K 16K Table 3: Statistics on the shared task data. included only in one of the shared task subsets (i.e., training, development or test). Furthermore, we split the data by the annotation time. Consequently, the training data is comprised of comments annotated between June and December, 2013; the development data includes texts annotated in December 2013;</context>
</contexts>
<marker>Habash, Soudi, Buckwalter, 2007</marker>
<rawString>N. Habash, A. Soudi, and T. Buckwalter. 2007. On Arabic Transliteration. In A. van den Bosch and A. Soudi, editors, Arabic Computational Morphology: Knowledge-based and Empirical Methods. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>O Rambow</author>
<author>R Roth</author>
</authors>
<title>MADA+TOKAN: A Toolkit for Arabic Tokenization, Diacritization, Morphological Disambiguation, POS Tagging, Stemming and Lemmatization.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second International Conference on Arabic Language Resources and Tools.</booktitle>
<contexts>
<context position="7547" citStr="Habash et al., 2009" startWordPosition="1225" endWordPosition="1228">anually corrected corpus of errors for a variety of Arabic texts, including user comments on news web sites, native and non-native speaker essays, and machine translation output. Within the framework of this project, comprehensive annotation guidelines and a specialized web-based annotation interface have been developed (Zaghouani et al., 2014; Obeid et al., 2013). The annotation process includes an initial automatic pre-processing step followed by an automatic correction of common spelling errors by the 40 morphological analysis and disambiguation system MADA (v3.2) (Habash and Rambow, 2005; Habash et al., 2009). The pre-processed files are then assigned to a team of expert (human) annotators. For a given sentence, the annotators were instructed to correct all errors; these include spelling, punctuation, word choice, morphology, syntax, and dialectal usage. It should be noted that the error classification was only used for guiding the annotation process; the annotators were not instructed to mark the type of error but only needed to specify an appropriate correction. Once the annotation was complete, the corrections were automatically grouped into the following seven action categories based on the ac</context>
</contexts>
<marker>Habash, Rambow, Roth, 2009</marker>
<rawString>N. Habash, O. Rambow, and R. Roth. 2009. MADA+TOKAN: A Toolkit for Arabic Tokenization, Diacritization, Morphological Disambiguation, POS Tagging, Stemming and Lemmatization. In Proceedings of the Second International Conference on Arabic Language Resources and Tools.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
</authors>
<title>Introduction to Arabic Natural Language Processing.</title>
<date>2010</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="11498" citStr="Habash, 2010" startWordPosition="1867" endWordPosition="1868">(Pasha et al., 2014). MADAMIRA performs morphological analysis and contextual disambiguation. Using the output of MADAMIRA, we generated for each word thirty-three features. The features specify various properties: the partof-speech (POS), lemma, aspect, person, gender, number, and so on. Among its features, MADAMIRA produces forms that correct a large subset of a special class of spelling mistakes in words containing the letters Alif and final Ya. These letters are a source of the most common spelling types of spelling errors in Arabic and involve Hamzated Alifs and AlifMaqsura/Ya confusion (Habash, 2010; El Kholy and Habash, 2012). We refer to these errors as Alif/Ya errors (see also Section 6). Table 3 presents statistics on the shared task data. The training data contains over one million words of text; the development and test data contain slightly over 50,000 words each. Table 4 shows the distribution of annotations by the action type. The majority of corrections (over 50%) belong to the type Edit. This is followed by mistakes that require several words to be merged together (about a third of all errors). 41 Data Edit Add Error type (%) Move Other Merge Split Delete Train. 55.34 32.36 5.</context>
</contexts>
<marker>Habash, 2010</marker>
<rawString>N. Habash. 2010. Introduction to Arabic Natural Language Processing. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Hassan</author>
<author>M Aly</author>
<author>A Atiya</author>
</authors>
<title>Arabic Spelling Correction using Supervised Learning.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</booktitle>
<contexts>
<context position="12996" citStr="Hassan et al., 2014" startWordPosition="2108" endWordPosition="2111">zovskaya et al., 2014) Columbia University (USA) CMUQ (Jeblee et al., 2014) Carnegie Mellon University in Qatar (Qatar) CP13 (Tomeh et al., 2014) Université Paris 13 (France) CUFE (Nawar and Ragheb, 2014) Computer Engineering Department, Cairo University (Egypt) GLTW (Zerrouki et al., 2014) Bouira University (Algeria), The National Computer Science Engineering School (Algeria), and Tabuk University (KSA) GWU (Attia et al., 2014) George Washington University (USA) QCRI (Mubarak and Darwish, 2014) Qatar Computing Research Institute (Qatar) TECH (Mostefa et al., 2014) Techlimed.com (France) YAM (Hassan et al., 2014) Faculty of Engineering, Cairo University (Egypt) Table 5: List of the nine teams that participated in the shared task. Team Approach External Resources CLMB Corrections proposed by MADAMIRA; a Maximum Likeli- Arabic Gigaword Fourth Edition (Parker et al., hood model trained on the training data; regular expressions; 2009) a decision-tree classifier for punctuation errors trained on the training data; an SVM character-level error correction model; a Naïve Bayes classifier trained on the training data and the Arabic Gigaword corpus CMUQ A pipeline consisting of rules, corrections proposed by Ar</context>
</contexts>
<marker>Hassan, Aly, Atiya, 2014</marker>
<rawString>Y. Hassan, M. Aly, and A. Atiya. 2014. Arabic Spelling Correction using Supervised Learning. In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jeblee</author>
<author>H Bouamor</author>
<author>W Zaghouani</author>
<author>K Oflazer</author>
</authors>
<title>CMUQ@QALB-2014: An SMT-based System for Automatic Arabic Error Correction.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</booktitle>
<contexts>
<context position="12451" citStr="Jeblee et al., 2014" startWordPosition="2031" endWordPosition="2034">tion type. The majority of corrections (over 50%) belong to the type Edit. This is followed by mistakes that require several words to be merged together (about a third of all errors). 41 Data Edit Add Error type (%) Move Other Merge Split Delete Train. 55.34 32.36 5.95 3.48 2.21 0.14 0.50 Dev. 53.51 34.24 5.97 3.67 2.03 0.08 0.49 Test 51.94 34.73 5.89 3.48 3.32 0.15 0.49 Table 4: Distribution of annotations by type in the shared task data. Error types denotes the action required in order to correct the error. Team Name Affiliation CLMB (Rozovskaya et al., 2014) Columbia University (USA) CMUQ (Jeblee et al., 2014) Carnegie Mellon University in Qatar (Qatar) CP13 (Tomeh et al., 2014) Université Paris 13 (France) CUFE (Nawar and Ragheb, 2014) Computer Engineering Department, Cairo University (Egypt) GLTW (Zerrouki et al., 2014) Bouira University (Algeria), The National Computer Science Engineering School (Algeria), and Tabuk University (KSA) GWU (Attia et al., 2014) George Washington University (USA) QCRI (Mubarak and Darwish, 2014) Qatar Computing Research Institute (Qatar) TECH (Mostefa et al., 2014) Techlimed.com (France) YAM (Hassan et al., 2014) Faculty of Engineering, Cairo University (Egypt) Table</context>
</contexts>
<marker>Jeblee, Bouamor, Zaghouani, Oflazer, 2014</marker>
<rawString>S. Jeblee, H. Bouamor, W. Zaghouani, and K. Oflazer. 2014. CMUQ@QALB-2014: An SMT-based System for Automatic Arabic Error Correction. In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mostefa</author>
<author>O Asbayou</author>
<author>R Abbes</author>
</authors>
<title>TECHLIMED System Description for the Shared Task on Automatic Arabic Error Correction.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</booktitle>
<contexts>
<context position="12947" citStr="Mostefa et al., 2014" startWordPosition="2101" endWordPosition="2104"> correct the error. Team Name Affiliation CLMB (Rozovskaya et al., 2014) Columbia University (USA) CMUQ (Jeblee et al., 2014) Carnegie Mellon University in Qatar (Qatar) CP13 (Tomeh et al., 2014) Université Paris 13 (France) CUFE (Nawar and Ragheb, 2014) Computer Engineering Department, Cairo University (Egypt) GLTW (Zerrouki et al., 2014) Bouira University (Algeria), The National Computer Science Engineering School (Algeria), and Tabuk University (KSA) GWU (Attia et al., 2014) George Washington University (USA) QCRI (Mubarak and Darwish, 2014) Qatar Computing Research Institute (Qatar) TECH (Mostefa et al., 2014) Techlimed.com (France) YAM (Hassan et al., 2014) Faculty of Engineering, Cairo University (Egypt) Table 5: List of the nine teams that participated in the shared task. Team Approach External Resources CLMB Corrections proposed by MADAMIRA; a Maximum Likeli- Arabic Gigaword Fourth Edition (Parker et al., hood model trained on the training data; regular expressions; 2009) a decision-tree classifier for punctuation errors trained on the training data; an SVM character-level error correction model; a Naïve Bayes classifier trained on the training data and the Arabic Gigaword corpus CMUQ A pipelin</context>
</contexts>
<marker>Mostefa, Asbayou, Abbes, 2014</marker>
<rawString>D. Mostefa, O. Asbayou, and R. Abbes. 2014. TECHLIMED System Description for the Shared Task on Automatic Arabic Error Correction. In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Mubarak</author>
<author>K Darwish</author>
</authors>
<title>Automatic Correction of Arabic Text: a Cascaded Approach.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</booktitle>
<contexts>
<context position="12876" citStr="Mubarak and Darwish, 2014" startWordPosition="2091" endWordPosition="2094">in the shared task data. Error types denotes the action required in order to correct the error. Team Name Affiliation CLMB (Rozovskaya et al., 2014) Columbia University (USA) CMUQ (Jeblee et al., 2014) Carnegie Mellon University in Qatar (Qatar) CP13 (Tomeh et al., 2014) Université Paris 13 (France) CUFE (Nawar and Ragheb, 2014) Computer Engineering Department, Cairo University (Egypt) GLTW (Zerrouki et al., 2014) Bouira University (Algeria), The National Computer Science Engineering School (Algeria), and Tabuk University (KSA) GWU (Attia et al., 2014) George Washington University (USA) QCRI (Mubarak and Darwish, 2014) Qatar Computing Research Institute (Qatar) TECH (Mostefa et al., 2014) Techlimed.com (France) YAM (Hassan et al., 2014) Faculty of Engineering, Cairo University (Egypt) Table 5: List of the nine teams that participated in the shared task. Team Approach External Resources CLMB Corrections proposed by MADAMIRA; a Maximum Likeli- Arabic Gigaword Fourth Edition (Parker et al., hood model trained on the training data; regular expressions; 2009) a decision-tree classifier for punctuation errors trained on the training data; an SVM character-level error correction model; a Naïve Bayes classifier tra</context>
</contexts>
<marker>Mubarak, Darwish, 2014</marker>
<rawString>H. Mubarak and K. Darwish. 2014. Automatic Correction of Arabic Text: a Cascaded Approach. In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nawar</author>
<author>M Ragheb</author>
</authors>
<title>Fast and Robust Arabic Error Correction System. In</title>
<date>2014</date>
<booktitle>Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</booktitle>
<contexts>
<context position="12580" citStr="Nawar and Ragheb, 2014" startWordPosition="2051" endWordPosition="2054">ords to be merged together (about a third of all errors). 41 Data Edit Add Error type (%) Move Other Merge Split Delete Train. 55.34 32.36 5.95 3.48 2.21 0.14 0.50 Dev. 53.51 34.24 5.97 3.67 2.03 0.08 0.49 Test 51.94 34.73 5.89 3.48 3.32 0.15 0.49 Table 4: Distribution of annotations by type in the shared task data. Error types denotes the action required in order to correct the error. Team Name Affiliation CLMB (Rozovskaya et al., 2014) Columbia University (USA) CMUQ (Jeblee et al., 2014) Carnegie Mellon University in Qatar (Qatar) CP13 (Tomeh et al., 2014) Université Paris 13 (France) CUFE (Nawar and Ragheb, 2014) Computer Engineering Department, Cairo University (Egypt) GLTW (Zerrouki et al., 2014) Bouira University (Algeria), The National Computer Science Engineering School (Algeria), and Tabuk University (KSA) GWU (Attia et al., 2014) George Washington University (USA) QCRI (Mubarak and Darwish, 2014) Qatar Computing Research Institute (Qatar) TECH (Mostefa et al., 2014) Techlimed.com (France) YAM (Hassan et al., 2014) Faculty of Engineering, Cairo University (Egypt) Table 5: List of the nine teams that participated in the shared task. Team Approach External Resources CLMB Corrections proposed by MA</context>
</contexts>
<marker>Nawar, Ragheb, 2014</marker>
<rawString>M. Nawar and M. Ragheb. 2014. Fast and Robust Arabic Error Correction System. In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hadiwinoto</author>
<author>J Tetreault</author>
</authors>
<date>2013</date>
<booktitle>The CoNLL-2013 Shared Task on Grammatical Error Correction. In Proceedings of CoNLL: Shared Task.</booktitle>
<marker>Hadiwinoto, Tetreault, 2013</marker>
<rawString>H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and J. Tetreault. 2013. The CoNLL-2013 Shared Task on Grammatical Error Correction. In Proceedings of CoNLL: Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>S M Wu</author>
<author>T Briscoe</author>
<author>C Hadiwinoto</author>
<author>R H Susanto</author>
<author>C Bryant</author>
</authors>
<date>2014</date>
<booktitle>The CoNLL-2014 Shared Task on Grammatical Error Correction. In Proceedings of CoNLL: Shared Task.</booktitle>
<contexts>
<context position="1294" citStr="Ng et al., 2014" startWordPosition="195" endWordPosition="198">e of the datasets used for training and evaluation, an overview of participating systems, results of the competition and an analysis of the results and systems. 1 Introduction The task of text correction has recently gained a lot of attention in the Natural Language Processing (NLP) community. Most of the effort in this area concentrated on English, especially on errors made by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers who focus on this problem and promote development and dissemination of key resources, such as benchmark datasets. Recently, there have been several efforts aimed at creating data resources related to the correction of Arabic text. Those include human annotated corpora (Zaghouani et al., 2014; Alfaifi and Atwell, 2012), spell-checking lexicon (Attia et al., 2012) and unannotated language learner corpora (Farwaneh and Tamimi, 2012). A natural extension to these resource production efforts is the creation of r</context>
</contexts>
<marker>Ng, Wu, Briscoe, Hadiwinoto, Susanto, Bryant, 2014</marker>
<rawString>H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H. Susanto, and C. Bryant. 2014. The CoNLL-2014 Shared Task on Grammatical Error Correction. In Proceedings of CoNLL: Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Obeid</author>
<author>W Zaghouani</author>
<author>B Mohit</author>
<author>N Habash</author>
<author>K Oflazer</author>
<author>N Tomeh</author>
</authors>
<title>A Web-based Annotation Framework For Large-Scale Text Correction.</title>
<date>2013</date>
<booktitle>In The Companion Volume of the Proceedings of IJCNLP 2013: System Demonstrations. Asian Federation of Natural Language Processing.</booktitle>
<contexts>
<context position="7293" citStr="Obeid et al., 2013" startWordPosition="1186" endWordPosition="1189">g the announcement of system results, the answer key to the test set was released. Participants authored description papers which will be presented in the Arabic NLP workshop. 3 The QALB Corpus One of the goals of the QALB project is to create a large manually corrected corpus of errors for a variety of Arabic texts, including user comments on news web sites, native and non-native speaker essays, and machine translation output. Within the framework of this project, comprehensive annotation guidelines and a specialized web-based annotation interface have been developed (Zaghouani et al., 2014; Obeid et al., 2013). The annotation process includes an initial automatic pre-processing step followed by an automatic correction of common spelling errors by the 40 morphological analysis and disambiguation system MADA (v3.2) (Habash and Rambow, 2005; Habash et al., 2009). The pre-processed files are then assigned to a team of expert (human) annotators. For a given sentence, the annotators were instructed to correct all errors; these include spelling, punctuation, word choice, morphology, syntax, and dialectal usage. It should be noted that the error classification was only used for guiding the annotation proce</context>
</contexts>
<marker>Obeid, Zaghouani, Mohit, Habash, Oflazer, Tomeh, 2013</marker>
<rawString>O. Obeid, W. Zaghouani, B. Mohit, N. Habash, K. Oflazer, and N. Tomeh. 2013. A Web-based Annotation Framework For Large-Scale Text Correction. In The Companion Volume of the Proceedings of IJCNLP 2013: System Demonstrations. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Parker</author>
<author>D Graff</author>
<author>K Chen</author>
<author>J Kong</author>
<author>K Maeda</author>
</authors>
<date>2009</date>
<booktitle>Arabic Gigaword Fourth Edition. LDC Catalog No.: LDC2009T30, ISBN:</booktitle>
<pages>1--58563</pages>
<contexts>
<context position="14613" citStr="Parker et al., 2009" startWordPosition="2338" endWordPosition="2341">; a discriminative re-ranker; a decision tree classifier for inserting missing punctuation CUFE Rules extracted from the Buckwalter morphological analyser; Buckwalter morphological analyzer Version 2.0 their probabilities are learned using the training data (Buckwalter, 2004) GLTW Regular expressions and word lists AraComLex dictionary (Attia et al., 2012); inhouse resources; Ayaspell dictionary GWU A CRF model for punctuation errors; a dictionary and a lan- AraComLex Extended dictionary (Attia et al., guage model for spelling errors; normalization rules 2012); Arabic Gigaword Fourth Edition (Parker et al., 2009) QCRI Word errors: a language model trained on Arabic Wikipedia Arabic Wikipedia; Aljazeera articles and Aljazeera data; punctuation mistakes: a CRF model and a frequency-based model trained on the shared task data TECH Off-the-shelf spell checkers and a statistical machine- Newspaper articles from Open Source Arabic translation model Corpora; other corpora collected online; Hunspell YAM Edit errors: a Naïve Bayes classifier that uses the following fea- AraComLex dictionary (Attia et al., 2012); tures: a character confusion matrix based on the training data; a Buckwalter Analyzer Version 1.0 (</context>
</contexts>
<marker>Parker, Graff, Chen, Kong, Maeda, 2009</marker>
<rawString>R. Parker, D. Graff, K. Chen, J. Kong, and K. Maeda. 2009. Arabic Gigaword Fourth Edition. LDC Catalog No.: LDC2009T30, ISBN: 1-58563-532-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pasha</author>
<author>M Al-Badrashiny</author>
<author>M Diab</author>
<author>A El Kholy</author>
<author>R Eskander</author>
<author>N Habash</author>
<author>M Pooleery</author>
<author>O Rambow</author>
<author>R Roth</author>
</authors>
<title>MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation of Arabic.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC).</booktitle>
<marker>Pasha, Al-Badrashiny, Diab, El Kholy, Eskander, Habash, Pooleery, Rambow, Roth, 2014</marker>
<rawString>A. Pasha, M. Al-Badrashiny, M. Diab, A. El Kholy, R. Eskander, N. Habash, M. Pooleery, O. Rambow, and R. Roth. 2014. MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation of Arabic. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>M Sammons</author>
<author>J Gioja</author>
<author>D Roth</author>
</authors>
<date>2011</date>
<booktitle>University of Illinois System in HOO Text Correction Shared Task. In Proceedings of the European Workshop on Natural Language Generation (ENLG).</booktitle>
<contexts>
<context position="17880" citStr="Rozovskaya et al., 2011" startWordPosition="2870" endWordPosition="2873">he 50-60 range. It is interesting to note that these results are considerably higher than those shown on the similar shared tasks on English non-native data. For instance, the highest performance in the CoNLL2013 shared task that also used the same evaluation metric was 31.20 (Rozovskaya et al., 2013).4 The highest score in the HOO-2011 shared task (Dale and Kilgarriff, 2011) that addressed all er4This year CoNLL was an extension of the CoNLL-2013 competition for all errors but in its evaluation favored precision twice as much as recall, so we are not comparing to this setting. rors was 21.1 (Rozovskaya et al., 2011). Of course, the setting was different,as we are dealing with texts written by native speakers. But, in addition to that, we hypothesize that our data contains a set of language-specific errors that may be “easier”, e.g Alif/Ya errors. We also asked the participants for the outputs of their systems on the development set. We show the results in Table 8. While these results are not used for ranking, since the development set was used for tuning the parameters of the systems, it is interesting to see how much the performance differs from the results obtained on the test. In general, we do not ob</context>
</contexts>
<marker>Rozovskaya, Sammons, Gioja, Roth, 2011</marker>
<rawString>A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth. 2011. University of Illinois System in HOO Text Correction Shared Task. In Proceedings of the European Workshop on Natural Language Generation (ENLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>K-W Chang</author>
<author>M Sammons</author>
<author>D Roth</author>
</authors>
<date>2013</date>
<booktitle>The University of Illinois System in the CoNLL-2013 Shared Task. In Proceedings of CoNLL Shared Task.</booktitle>
<contexts>
<context position="17558" citStr="Rozovskaya et al., 2013" startWordPosition="2814" endWordPosition="2817">Dahlmeier and Ng, 2012) that was also used in the CoNLL shared tasks. Table 7 presents the official results of the evaluation on the test set. The results are sorted according to the F1 scores obtained by the systems. The range of the scores is quite wide – from 20 to 67 F1 – but the the majority of the systems stay in the 50-60 range. It is interesting to note that these results are considerably higher than those shown on the similar shared tasks on English non-native data. For instance, the highest performance in the CoNLL2013 shared task that also used the same evaluation metric was 31.20 (Rozovskaya et al., 2013).4 The highest score in the HOO-2011 shared task (Dale and Kilgarriff, 2011) that addressed all er4This year CoNLL was an extension of the CoNLL-2013 competition for all errors but in its evaluation favored precision twice as much as recall, so we are not comparing to this setting. rors was 21.1 (Rozovskaya et al., 2011). Of course, the setting was different,as we are dealing with texts written by native speakers. But, in addition to that, we hypothesize that our data contains a set of language-specific errors that may be “easier”, e.g Alif/Ya errors. We also asked the participants for the out</context>
</contexts>
<marker>Rozovskaya, Chang, Sammons, Roth, 2013</marker>
<rawString>A. Rozovskaya, K.-W. Chang, M. Sammons, and D. Roth. 2013. The University of Illinois System in the CoNLL-2013 Shared Task. In Proceedings of CoNLL Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>N Habash</author>
<author>R Eskander</author>
<author>N Farra</author>
<author>W Salloum</author>
</authors>
<date>2014</date>
<booktitle>The Columbia System in the QALB-2014 Shared Task on Arabic Error Correction. In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</booktitle>
<contexts>
<context position="12398" citStr="Rozovskaya et al., 2014" startWordPosition="2023" endWordPosition="2026">. Table 4 shows the distribution of annotations by the action type. The majority of corrections (over 50%) belong to the type Edit. This is followed by mistakes that require several words to be merged together (about a third of all errors). 41 Data Edit Add Error type (%) Move Other Merge Split Delete Train. 55.34 32.36 5.95 3.48 2.21 0.14 0.50 Dev. 53.51 34.24 5.97 3.67 2.03 0.08 0.49 Test 51.94 34.73 5.89 3.48 3.32 0.15 0.49 Table 4: Distribution of annotations by type in the shared task data. Error types denotes the action required in order to correct the error. Team Name Affiliation CLMB (Rozovskaya et al., 2014) Columbia University (USA) CMUQ (Jeblee et al., 2014) Carnegie Mellon University in Qatar (Qatar) CP13 (Tomeh et al., 2014) Université Paris 13 (France) CUFE (Nawar and Ragheb, 2014) Computer Engineering Department, Cairo University (Egypt) GLTW (Zerrouki et al., 2014) Bouira University (Algeria), The National Computer Science Engineering School (Algeria), and Tabuk University (KSA) GWU (Attia et al., 2014) George Washington University (USA) QCRI (Mubarak and Darwish, 2014) Qatar Computing Research Institute (Qatar) TECH (Mostefa et al., 2014) Techlimed.com (France) YAM (Hassan et al., 2014) F</context>
</contexts>
<marker>Rozovskaya, Habash, Eskander, Farra, Salloum, 2014</marker>
<rawString>A. Rozovskaya, N. Habash, R. Eskander, N. Farra, and W. Salloum. 2014. The Columbia System in the QALB-2014 Shared Task on Arabic Error Correction. In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Tomeh</author>
<author>N Habash</author>
<author>R Eskander</author>
<author>J Le Roux</author>
</authors>
<title>A Pipeline Approach to Supervised Error Correction for the QALB-2014 Shared Task.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</booktitle>
<marker>Tomeh, Habash, Eskander, Le Roux, 2014</marker>
<rawString>N. Tomeh, N. Habash, R. Eskander, and J. Le Roux. 2014. A Pipeline Approach to Supervised Error Correction for the QALB-2014 Shared Task. In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Zaghouani</author>
<author>B Mohit</author>
<author>N Habash</author>
<author>O Obeid</author>
<author>N Tomeh</author>
<author>A Rozovskaya</author>
<author>N Farra</author>
<author>S Alkuhlani</author>
<author>K Oflazer</author>
</authors>
<title>Large Scale Arabic Error Annotation: Guidelines and Framework.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC). European Language Resources Association (ELRA).</booktitle>
<contexts>
<context position="1674" citStr="Zaghouani et al., 2014" startWordPosition="255" endWordPosition="258">ade by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers who focus on this problem and promote development and dissemination of key resources, such as benchmark datasets. Recently, there have been several efforts aimed at creating data resources related to the correction of Arabic text. Those include human annotated corpora (Zaghouani et al., 2014; Alfaifi and Atwell, 2012), spell-checking lexicon (Attia et al., 2012) and unannotated language learner corpora (Farwaneh and Tamimi, 2012). A natural extension to these resource production efforts is the creation of robust automatic systems for error correction. * These authors contributed equally to this work. In this paper, we present a summary of the QALB shared task on automatic text correction for Arabic. The Qatar Arabic Language Bank (QALB) project1 is one of the first large scale data and system development efforts for automatic correction of Arabic which has resulted in annotation </context>
<context position="3255" citStr="Zaghouani et al., 2014" startWordPosition="514" endWordPosition="517"> In the next section, we present the shared task framework. This is followed by an overview of the QALB corpus (Section 3). Section 4 describes the shared task data, and Section 5 presents the approaches adopted by the participating teams. Section 6 discusses the results of the competition. Finally, in Section 7, we offer a brief analysis and present preliminary experiments on system combination. 2 Task Description The QALB shared task was created as a forum for competition and collaboration on automatic error correction in Modern Standard Arabic. The shared task makes use of the QALB corpus (Zaghouani et al., 2014), which is a manually-corrected collection of Arabic texts. The shared task participants were provided with training and development data to build their systems, but were also free to make use of additional resources, including corpora, linguistic resources, and software, as long as these were publicly available. For evaluation, a standard framework devel1http://nlp.qatar.cmu.edu/qalb/ 39 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 39–47, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Original Corrected éª�K@QË@ �H</context>
<context position="7272" citStr="Zaghouani et al., 2014" startWordPosition="1182" endWordPosition="1185"> their systems. Following the announcement of system results, the answer key to the test set was released. Participants authored description papers which will be presented in the Arabic NLP workshop. 3 The QALB Corpus One of the goals of the QALB project is to create a large manually corrected corpus of errors for a variety of Arabic texts, including user comments on news web sites, native and non-native speaker essays, and machine translation output. Within the framework of this project, comprehensive annotation guidelines and a specialized web-based annotation interface have been developed (Zaghouani et al., 2014; Obeid et al., 2013). The annotation process includes an initial automatic pre-processing step followed by an automatic correction of common spelling errors by the 40 morphological analysis and disambiguation system MADA (v3.2) (Habash and Rambow, 2005; Habash et al., 2009). The pre-processed files are then assigned to a team of expert (human) annotators. For a given sentence, the annotators were instructed to correct all errors; these include spelling, punctuation, word choice, morphology, syntax, and dialectal usage. It should be noted that the error classification was only used for guiding</context>
<context position="8849" citStr="Zaghouani et al. (2014)" startWordPosition="1431" endWordPosition="1434">er}.2 Table 1 presents a sample erroneous Arabic news comment along with its manually corrected form, its romanized transliteration,3 and the English translation. The errors in the original and the corrected forms are underlined and co-indexed. Table 2 presents a subset of the errors from the example shown in Table 1 along with the error types and annotation actions. The Appendix at the end of the paper lists all annotation actions for that example. To ensure high annotation quality, the annotators went through multiple phases of training; the inter-annotator agreement was reviewed routinely. Zaghouani et al. (2014) report an average Word Error Rate (WER) of 3.8% for all words (excluding punctuation), which is quite high. When punctuation was included, the WER rose to 11.3%. The high level of agreement indicates that the annotations are reliable and the guidelines are useful in producing homogeneous and consistent data. Punctuation, however, remains a challenge. 4 Shared Task Data The shared task data comes from the QALB corpus and consists of user comments from the Aljazeera News webpage written in Modern Standard Arabic. Comments belonging to the same article were 2In the shared task, we specified two </context>
</contexts>
<marker>Zaghouani, Mohit, Habash, Obeid, Tomeh, Rozovskaya, Farra, Alkuhlani, Oflazer, 2014</marker>
<rawString>W. Zaghouani, B. Mohit, N. Habash, O. Obeid, N. Tomeh, A. Rozovskaya, N. Farra, S. Alkuhlani, and K. Oflazer. 2014. Large Scale Arabic Error Annotation: Guidelines and Framework. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC). European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zerrouki</author>
<author>K Alhawiti</author>
<author>A Balla</author>
</authors>
<title>Autocorrection Of Arabic Common Errors For Large Text Corpus.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</booktitle>
<contexts>
<context position="12667" citStr="Zerrouki et al., 2014" startWordPosition="2062" endWordPosition="2065">) Move Other Merge Split Delete Train. 55.34 32.36 5.95 3.48 2.21 0.14 0.50 Dev. 53.51 34.24 5.97 3.67 2.03 0.08 0.49 Test 51.94 34.73 5.89 3.48 3.32 0.15 0.49 Table 4: Distribution of annotations by type in the shared task data. Error types denotes the action required in order to correct the error. Team Name Affiliation CLMB (Rozovskaya et al., 2014) Columbia University (USA) CMUQ (Jeblee et al., 2014) Carnegie Mellon University in Qatar (Qatar) CP13 (Tomeh et al., 2014) Université Paris 13 (France) CUFE (Nawar and Ragheb, 2014) Computer Engineering Department, Cairo University (Egypt) GLTW (Zerrouki et al., 2014) Bouira University (Algeria), The National Computer Science Engineering School (Algeria), and Tabuk University (KSA) GWU (Attia et al., 2014) George Washington University (USA) QCRI (Mubarak and Darwish, 2014) Qatar Computing Research Institute (Qatar) TECH (Mostefa et al., 2014) Techlimed.com (France) YAM (Hassan et al., 2014) Faculty of Engineering, Cairo University (Egypt) Table 5: List of the nine teams that participated in the shared task. Team Approach External Resources CLMB Corrections proposed by MADAMIRA; a Maximum Likeli- Arabic Gigaword Fourth Edition (Parker et al., hood model tra</context>
</contexts>
<marker>Zerrouki, Alhawiti, Balla, 2014</marker>
<rawString>T. Zerrouki, K. Alhawiti, and A. Balla. 2014. Autocorrection Of Arabic Common Errors For Large Text Corpus. In Proceedings of EMNLP Workshop on Arabic Natural Language Processing: QALB Shared Task.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
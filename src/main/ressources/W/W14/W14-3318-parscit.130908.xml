<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007518">
<title confidence="0.980236">
Large-scale Exact Decoding: The IMS-TTT submission to WMT14∗
</title>
<author confidence="0.905995">
Daniel Quernheim Fabienne Cap
</author>
<affiliation confidence="0.8794285">
IMS CIS
University of Stuttgart Ludwig-Maximilian University of Munich
</affiliation>
<email confidence="0.991086">
daniel@ims.uni-stuttgart.de cap@cis.uni-muenchen.de
</email>
<sectionHeader confidence="0.995521" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999868">
We present the IMS-TTT submission to
WMT14, an experimental statistical tree-
to-tree machine translation system based
on the multi-bottom up tree transducer in-
cluding rule extraction, tuning and decod-
ing. Thanks to input parse forests and
a “no pruning” strategy during decoding,
the obtained translations are competitive.
The drawbacks are a restricted coverage
of 70% on test data, in part due to ex-
act input parse tree matching, and a rela-
tively high runtime. Advantages include
easy redecoding with a different weight
vector, since the full translation forests can
be stored after the first decoding pass.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958142857143">
In this contribution, we present an implementation
of a translation model that is based on MMBOT
(the multi bottom-up tree transducer of Arnold and
Dauchet (1982) and Lilin (1978)). Intuitively, an
MBOT is a synchronous tree sequence substitution
grammar (STSSG, Zhang et al. (2008a); Zhang et
al. (2008b); Sun et al. (2009)) that has discon-
tiguities only on the target side (Maletti, 2011).
From an algorithmic point of view, this makes the
MBOT more appealing than STSSG as demon-
strated by Maletti (2010). Formally, MBOT is
expressive enough to express all sensible trans-
lations (Maletti, 2012)1. Figure 2 displays sam-
ple rules of the MBOT variant, called MMBOT,
</bodyText>
<footnote confidence="0.9908985">
∗This work was supported by Deutsche Forschungsge-
meinschaft grants Models of Morphosyntax for Statistical
Machine Translation (Phase 2) and MA/4959/1–1.
1A translation is sensible if it is of linear size increase
and can be computed by some (potentially copying) top-down
tree transducer.
</footnote>
<bodyText confidence="0.998900472222222">
that we use (in a graphical representation of the
trees and the alignment). Recently, a shallow ver-
sion of MBOT has been integrated into the popular
Moses toolkit (Braune et al., 2013). Our imple-
mentation is exact in the sense that it does abso-
lutely no pruning during decoding and thus pre-
serves all translation candidates, while having no
mechanism to handle unknown structures. (We
added dummy rules that leave unseen lexical ma-
terial untranslated.) The coverage is thus limited,
but still considerably high. Source-side and target-
side syntax restrict the search space so that decod-
ing stays tractable. Only the language model scor-
ing is implemented as a separate reranker2. This
has several advantages: (1) We can use input parse
forests (Liu et al., 2009). (2) Not only is the out-
put optimal with regard to the theoretical model,
also the space of translation candidates can be ef-
ficiently stored as a weighted regular tree gram-
mar. The best translations can then be extracted
using the k-best algorithm by Huang and Chiang
(2005). Rule weights can be changed without the
need for explicit redecoding, the parameters of the
log-linear model can be changed, and even new
features can be added. These properties are espe-
cially helpful in tuning, where only the k-best al-
gorithm has to be re-run in each iteration. A model
in similar spirit has been described by Huang et al.
(2006); however, it used target syntax only (using
a top-down tree-to-string transducer backwards),
and was restricted to sentences of length at most
25. We do not make such restrictions.
The theoretical aspects of MMBOT and their use
in our translation model are presented in Section 2.
Based on this, we implemented a machine transla-
tion system that we are going to make available to
</bodyText>
<footnote confidence="0.991915">
2Strictly speaking, this does introduce pruning into the
pipeline.
</footnote>
<page confidence="0.964564">
163
</page>
<note confidence="0.4419075">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 163–170,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999587">
the public. Section 4 presents the most important
components of our `MBOT implementation, and
Section 5 presents our submission to the WMT14
shared translation task.
</bodyText>
<sectionHeader confidence="0.99402" genericHeader="method">
2 Theoretical Model
</sectionHeader>
<bodyText confidence="0.999968888888889">
In this section, we present the theoretical genera-
tive model that is used in our approach to syntax-
based machine translation: the multi bottom-up
tree transducer (Maletti, 2011). We omit the tech-
nical details and give graphical examples only to
illustrate how the device works, but refer to the lit-
erature for the theoretical background. Roughly
speaking, a local multi bottom-up tree transducer
(`MBOT) has rules that replace one nonterminal
symbol N on the source side by a tree, and a se-
quence of nonterminal symbols on the target side
linked to N by one tree each. These trees again
have linked nonterminals, thus allowing further
rule applications.
Our `MBOT rules are obtained automatically
from data like that in Figure 1. Thus, we (word)
align the bilingual text and parse it in both the
source and the target language. In this manner we
obtain sentence pairs like the one shown in Fig-
ure 1. To these sentence pairs we apply the rule
extraction method of Maletti (2011). The rules
extracted from the sentence pair of Figure 1 are
shown in Figure 2. Note the discontiguous align-
ment of went to ist and gegangen, resulting in dis-
contiguous rules.
The application of those rules is illustrated in
Figure 3 (a pre-translation is a pair consisting of a
source tree and a sequence of target trees). While
it shows a synchronous derivation, our main use
case of `MBOT rules is forward application or in-
put restriction, that is the calculation of all target
trees that can be derived given a source tree. For
a given synchronous derivation d, the source tree
generated by d is s(d), and the target tree is t(d).
The yield of a tree is the string obtained by con-
catenating its leaves.
Apart from `MBOT application to input trees,
we can even apply `MBOT to parse forests and
even weighted regular tree grammars (RTGs)
(F¨ul¨op and Vogler, 2009). RTGs offer an ef-
ficient representation of weighted forests, which
are sets of trees such that each individual tree is
equipped with a weight. This representation is
even more efficient than packed forests (Mi et al.,
2008) and moreover can represent an infinite num-
ber of weighted trees. The most important prop-
erty that we utilize is that the output tree language
is regular, so we can represent it by an RTG (cf.
preservation of regularity (Maletti, 2011)). In-
deed, every input tree can only be transformed into
finitely many output trees by our model, so for a
given finite input forest (which the output of the
parser is) the computed output forest will also be
finite and thus regular.
</bodyText>
<sectionHeader confidence="0.9987" genericHeader="method">
3 Translation Model
</sectionHeader>
<bodyText confidence="0.99997075">
Given a source language sentence e and corre-
sponding weighted parse forest F(e), our trans-
lation model aims to find the best corresponding
target language translation ˆg;3 i.e.,
</bodyText>
<equation confidence="0.686484">
gˆ = arg maxg p(g|e) .
</equation>
<bodyText confidence="0.898651857142857">
We estimate the probability p(g|e) through a log-
linear combination of component models with pa-
rameters λm scored on the derivations d such that
the source tree of d is in the parse forest of e and
the yield of the target tree reads g. With
D(e, g) = {d  |s(d) E F(e) and yield(t(d)) = g},
we thus have: 4
</bodyText>
<equation confidence="0.9994835">
p(g|e) ∝ 1: 11 hm(d)λ—
d∈D(e,g) m=1
</equation>
<bodyText confidence="0.962956">
Our model uses the following features hm(�) for a
derivation:
</bodyText>
<listItem confidence="0.9996112">
(1) Translation weight normalized by source root
symbol
(2) Translation weight normalized by all root
symbols
(3) Translation weight normalized by leaves on
the source side
(4) Lexical translation weight source —* target
(5) Lexical translation weight target —* source
(6) Target side language model: p(g)
(7) Number of words in g
(8) Number of rules used in the derivation
(9) Number of gaps in the target side sequences
(10) Penalty for rules that have more lexical ma-
terial on the source side than on the target side
or vice versa (absolute value)
</listItem>
<footnote confidence="0.9844146">
3Our main translation direction is English to German.
4While this is the clean theoretical formulation, we make
two approximations to D(e, g): (1) The parser we use returns
a pruned parse forest. (2) We only sum over derivations with
the same target sentence that actually appear in the k-best list.
</footnote>
<page confidence="0.99498">
164
</page>
<figureCaption confidence="0.999993">
Figure 1: Aligned parsed sentences.
Figure 2: Extracted rules.
</figureCaption>
<figure confidence="0.999133509090909">
NP
NNP
Max
NP
NN
home
VP
VBD
nach
hause
Max
ADJD-HD-Pos/N
gegangen
NE-HD-Nom.Sg.Masc
ist
PP-MO/V
VVPP-HD
S
went
APPR-AC
PN-SB-Nom.Sg.Masc
VP-OC/pp
S-TOP
VAFIN-HD-Sg
NP
NNP
Max
�→
PN-SB-Nom.Sg.Masc
NE-HD-Nom.Sg.Masc
Max
NP
NN
home
�
�→
VAFIN-HD-Sg VVPP-HD
,
gegangen
ist
VAFIN-HD-Sg �
VP-OC/pp S —� ( S-TOP
→ ( ) )
PP-MO/V VVPP-HD NP VP PN-SB-Nom.Sg.Masc VAFIN-HD-Sg VP-OC/pp
VP
VBD NP
�→
\ VBD
went
PP-MO/V
ADJD-HD-Pos/N
hause
�
APPR-AC
nach
</figure>
<listItem confidence="0.703169">
(11) Input parse tree probability assigned to s(t)
by the parser of e
</listItem>
<bodyText confidence="0.994896384615385">
The rule weights required for (1) are relative
frequencies normalized over all extracted rules
with the same root symbol on the left-hand side. In
the same fashion the rule weights required for (2)
are relative frequencies normalized over all rules
with the same root symbols on both sides. The
lexical weights for (4) and (5) are obtained by mul-
tiplying the word translations w(gi|ej) [respec-
tively, w(ej|gi)] of lexically aligned words (gi, ej)
across (possibly discontiguous) target side se-
quences.5 Whenever a source word ej is aligned
to multiple target words, we average over the word
translations:6
</bodyText>
<equation confidence="0.96775275">
h4(d)
= Y average {w(g|e)  |g aligned to e}
lexical item
e occurs in s(d)
</equation>
<sectionHeader confidence="0.998526" genericHeader="method">
4 Implementation
</sectionHeader>
<bodyText confidence="0.996553">
Our implementation is very close to the theoretical
model and consists of several independent compo-
</bodyText>
<footnote confidence="0.9982754">
5The lexical alignments are different from the links used
to link nonterminals.
6If the word ej has no alignment to a target word, then
it is assumed to be aligned to a special NULL word and this
alignment is scored.
</footnote>
<bodyText confidence="0.999791192307692">
nents, most of which are implemented in Python.
The system does not have any dependencies other
than the need for parsers for the source and tar-
get language, a word alignment tool and option-
ally an implementation of some tuning algorithm.
A schematic depiction of the training and decod-
ing pipeline can be seen in Figure 4.
Rule extraction From a parallel corpus of
which both halves have been parsed and word
aligned, multi bottom-up tree transducer rules are
extracted according to the procedure laid out in
(Maletti, 2011). In order to handle unknown
words, we add dummy identity translation rules
for lexical material that was not present in the
training data.
Translation model building Given a set of
rules, translation weights (see above) are com-
puted for each unique rule. The translation model
is then converted into a source, a weight and a tar-
get model. The source model (an RTG represented
in an efficient binary format) is used for decod-
ing and maps input trees to trees over rule iden-
tifiers representing derivations. The weight model
and the target model can be used to reconstruct the
weight and the target realization of a given deriva-
tion.
</bodyText>
<page confidence="0.983631">
165
</page>
<figure confidence="0.9990116">
Composing 3 rules:
Obtained pre-translation:
NP
NN
home
(y
PP-MO/V
�
VAFIN-HD-Sg VVPP-HD
y ( gegangen
ist
ADJD-HD-Pos/N
hause
APPR-AC
nach
VP
VBD NP
(y VAFIN-HD-Sg
�
PP-MO/V VVPP-HD
VP-OC/pp
VBD
went
APPR-AC
nach
ADJD-HD-Pos/N
hause
VAFIN-HD-Sg
ist
VP-OC/pp
NP
NN
home
(y
VP
VBD
went
PP-MO/V
VVPP-HD \
gegangen )
</figure>
<figureCaption confidence="0.999786">
Figure 3: Synchronous rule application.
Figure 4: Our machine translation system.
</figureCaption>
<page confidence="0.996649">
166
</page>
<bodyText confidence="0.997753224489796">
Decoder The decoder transforms a forest of in-
put sentence parse trees to a forest of transla-
tion derivations by means of forward application.
These derivations are trees over the set of rules
(represented by rule identifiers). One of the most
useful aspects of our model is the fact that decod-
ing is completely independent of the weights, as
no pruning is performed and all translation candi-
dates are preserved in the translation forest. Thus,
even after decoding, the weight model can be
changed, augmented by new features, etc.; even
the target model can be changed, e.g. to support
parse tree output instead of string output. In all
of our experiments, we used string output, but it is
conceivable to use other realizations. For instance,
a syntactic language model could be used for out-
put tree scoring. Also, recasing is extremely easy
when we have part-of-speech tags to base our de-
cision on (proper names are typically uppercase,
as are all nouns in German).
Another benefit of having a packed representa-
tion of all candidates is that we can easily check
whether the reference translation is included in the
candidate set (“force decoding”). The freedom to
allow arbitrary target models that rewrite deriva-
tions is related to current work on interpreted reg-
ular tree grammars (Koller and Kuhlmann, 2011),
where arbitrary algebras can be used to compute a
realization of the output tree.
k-best extractor From the translation derivation
RTGs, a k-best list of derivations can be extracted
(Huang and Chiang, 2005) very efficiently. This
is the only step that has to be repeated if the rule
weights or the parameters of the log-linear model
change. The derivations are then mapped to tar-
get language sentences (if several derivations re-
alize the same target sentence, their weights are
summed) and reranked according to a language
model (as was done in Huang et al. (2006)). This
is the only part of the pipeline where we deviate
from the theoretical log-linear model, and this is
where we might make search errors. In principle,
one could integrate the language model by inter-
section with the translation model (as the stateful
MBOT model is closed under intersection with fi-
nite automata), but this is (currently) not computa-
tionally feasible due to the size of models.
Tuning Minimum error rate training (Och,
2003) is implemented using Z-MERT7 (Zaidan,
</bodyText>
<footnote confidence="0.833119">
7http://cs.jhu.edu/˜ozaidan/zmert/
</footnote>
<bodyText confidence="0.9992738">
2009). A set of source sentences has to be (forest-
)parsed and decoded; the translation forests are
stored on disk. Then, in each iteration of Z-MERT,
it suffices to extract k-best lists from the transla-
tion forests according to the current weight vector.
</bodyText>
<sectionHeader confidence="0.874083" genericHeader="method">
5 WMT14 Experimental setup
</sectionHeader>
<bodyText confidence="0.990083325">
We used the training data that was made avail-
able for the WMT14 shared translation task on
English–German8. It consists of three parallel cor-
pora (1.9M sentences of European parliament pro-
ceedings, 201K sentences of newswire text, and
2M sentences of web text) and additional mono-
lingual news data for language model training.
The English half of the parallel data was parsed
using Egret9 which is a re-implementation of the
Berkeley parser (Petrov et al., 2006). For the Ger-
man parse, we used the BitPar parser (Schmid,
2004; Schmid, 2006). The BitPar German gram-
mar is highly detailed, which makes the syntac-
tic information contained in the parses extremely
useful. Part-of-speech tags and category label are
augmented by case, number and gender informa-
tion, as can be seen in the German parse tree in
Figure 1. We only kept the best parse for each
sentence during training. After parsing, we pre-
pared three versions of the German corpus: a)
RAW, with no morphological post-processing; b)
UNSPLIT, using SMOR, a rule-based morpho-
logical analyser (Schmid et al., 2004), to reduce
words to their base form; c) SPLIT, using SMOR
to reduce words to their base form and split com-
pound nouns. After translation, compounds were
merged again, and words were re-inflected. Pre-
vious experiments using SMOR to lemmatise and
split compounds in phrase-based SMT showed im-
proved translation performances, see (Cap et al.,
2014a) for details.
We then trained three 5-gram language models
on monolingual data using KenLM10 (Heafield,
2011; Heafield et al., 2013 to appear) for the
three setups. For SPLIT and UNSPLIT, we were
only able to use the German side of the parallel
data, since parsing is a prerequisite for our mor-
phological post-processing and we did not have
the resources to parse more data. For RAW, we
additionally used the monolingual German data
</bodyText>
<footnote confidence="0.9991026">
8http://www.statmt.org/wmt14/
translation-task.html
9https://sites.google.com/site/
zhangh1982/egret
10http://kheafield.com/code/kenlm/
</footnote>
<page confidence="0.982385">
167
</page>
<table confidence="0.999427">
system BLEU BLEU-cased TER
RAW 17.0 16.4 .770
UNSPLIT 16.4 15.8 .773
SPLIT 16.3 15.7 .773
</table>
<tableCaption confidence="0.8664275">
Table 1: BLEU and TER scores of the submitted
systems.
</tableCaption>
<bodyText confidence="0.998881476190476">
that was distributed for the shared task. Word
alignment for all three setups was achieved using
GIZA++11. As usual, we discarded sentence pairs
where one sentence was significantly longer than
the other, as well as those that were too long or too
short.
For tuning, we chose the WMT12 test set (3,003
sentences of newswire text), available as part
of the development data for the WMT13 shared
translation task. Since our system had limited cov-
erage on this tuning set, we limited ourselves to
the first a subset of sentences we could translate.
When translating the test set, our models used
parse trees delivered by the Egret parser. After
translation, recasing was done by examining the
output syntax tree, using a simple heuristics look-
ing for nouns and sentence boundaries. Since cov-
erage on the test set was also limited, we used the
systems as described in (Cap et al., 2014b)12 as a
fallback to translate sentences that our system was
not able to translate.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9999756">
We report the overall translation quality, as listed
on http://matrix.statmt.org/, mea-
sured using BLEU (Papineni et al., 2002) and
TER (Snover et al., 2006), in Table 1.
We assume that the poor performance of UN-
SPLIT and SPLIT compared to RAW is due to the
fact that we use a significantly smaller language
model (as explained above) for these two settings.
A detailed analysis will follow after the end of the
manual evaluation period.
</bodyText>
<sectionHeader confidence="0.892346" genericHeader="conclusions">
7 Conclusion and further work
</sectionHeader>
<bodyText confidence="0.9991965">
We presented our submission to the WMT14
shared translation task based on a novel, promising
“full syntax, no pruning” tree-to-tree approach to
statistical machine translation, inspired by Huang
</bodyText>
<footnote confidence="0.839135666666667">
11https://code.google.com/p/giza-pp/
12We use raw as described in (Cap et al., 2014b) as a fall-
back for RAW, RI for UNSPLIT and CoRI for SPLIT.
</footnote>
<bodyText confidence="0.999988039215686">
et al. (2006). There are, however, still major draw-
backs and open problems associated with our ap-
proach. Firstly, the coverage can still be signifi-
cantly improved. In these experiments, our model
was able to translate only 70% of the test sen-
tences. To some extent, this number can be im-
proved by providing more training data. Also,
more rules can be extracted if we not only use the
best parse for rule extraction, but multiple parse
trees, or even switch to forest-based rule extrac-
tion (Mi and Huang, 2008). Finally, the size of the
input parse forest plays a role. For instance, if we
only supply the best parse to our model, transla-
tion will fail for approximately half of the input.
However, there are inherent coverage limits.
Since our model is extremely strict, it will never
be able to translate sentences whose parse trees
contain structures it has never seen before, since
it has to match at least one input parse tree ex-
actly. While we implemented a simple solution to
handle unknown words, the issue with unknown
structures is not so easy to solve without breaking
the otherwise theoretically sound approach. Pos-
sibly, glue rules can help.
The second drawback is runtime. We were
able to translate about 15 sentences per hour on
one processor. Distributing the translation task
on different machines, we were able to translate
the WMT14 test set (10k sentences) in roughly
four days. Given that the trend goes towards par-
allel programming, and considering the fact that
our decoder is written in the rather slow language
Python, we are confident that this is not a major
problem. We were able to run the whole pipeline
of training, tuning and evaluation on the WMT14
shared task data in less than one week. We are cur-
rently investigating whether A* k-best algorithms
(Pauls and Klein, 2009; Pauls et al., 2010) can help
to guide the translation process while maintaining
optimality.
Thirdly, currently the language model is not in-
tegrated, but implemented as a separate rerank-
ing component. We are aware that this might in-
troduce search errors, and that an integrated lan-
guage model might improve translation quality
(see e.g. Chiang (2007) where 3–4 BLEU points
are gained by LM integration). Some research on
this topic already exists, e.g. (Rush and Collins,
2011) who use dual decomposition, and (Aziz et
al., 2013) who replace intersection with an upper
bound which is easier to compute.
</bodyText>
<page confidence="0.997645">
168
</page>
<sectionHeader confidence="0.990217" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999870431372549">
Andr´e Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d’arbres. Theoret. Comput. Sci.,
20(1):33–93.
Wilker Aziz, Marc Dymetman, and Sriram Venkatap-
athy. 2013. Investigations in exact inference for
hierarchical translation. In Proc. 8th WMT, pages
472–483.
Fabienne Braune, Nina Seemann, Daniel Quernheim,
and Andreas Maletti. 2013. Shallow local multi-
bottom-up tree transducers in statistical machine
translation. In Proc. 51th ACL, pages 811–821.
Fabienne Cap, Alexander Fraser, Marion Weller, and
Aoife Cahill. 2014a. How to Produce Unseen
Teddy Bears: Improved Morphological Processing
of Compounds in SMT. In Proc. 14th EACL.
Fabienne Cap, Marion Weller, Anita Ramm, and
Alexander Fraser. 2014b. CimS – The CIS and IMS
joint submission to WMT 2014 translating from En-
glish into German. In Proc. 9th WMT.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computat. Linguist., 33(2):201–228.
Zolt´an F¨ul¨op and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Hand-
book of Weighted Automata, EATCS Monographs
on Theoret. Comput. Sci., chapter 9, pages 313–403.
Springer.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013 (to appear). Scal-
able modified Kneser-Ney language model estima-
tion. In Proc. 51stACL.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proc. 6th WMT, pages
187–197.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. IWPT, pages 53–64.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. 7th Conf. AMTA, pages
66–73.
Alexander Koller and Marco Kuhlmann. 2011. A gen-
eralized view on parsing and translation. In Proc.
IWPT, pages 2–13.
Eric Lilin. 1978. Une g´en´eralisation des transducteurs
d’´etats finis d’arbres: les S-transducteurs. Th`ese
3`eme cycle, Universit´e de Lille.
Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
47th ACL, pages 558–566.
Andreas Maletti. 2010. Why synchronous tree sub-
stitution grammars? In Proc. HLT-NAACL, pages
876–884.
Andreas Maletti. 2011. How to train your multi
bottom-up tree transducer. In Proc. 49th ACL, pages
825–834.
Andreas Maletti. 2012. Every sensible extended top-
down tree transducer is a multi bottom-up tree trans-
ducer. In Proc. HLT-NAACL, pages 263–273.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proc. EMNLP, pages 206–
214.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. 46th ACL, pages 192–
199. ACL.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. 41st ACL,
pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. 40th
ACL, pages 311–318.
Adam Pauls and Dan Klein. 2009. K-best A* parsing.
In Proc. 47th ACL, pages 958–966.
Adam Pauls, Dan Klein, and Chris Quirk. 2010. Top-
down k-best A* parsing. In Proc. 48th ACL, pages
200–204.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. COLING-ACL,
pages 433–440.
Alexander M. Rush and Michael Collins. 2011. Ex-
act decoding of syntactic translation models through
lagrangian relaxation. In Proc. 49th ACL, pages 72–
82.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: A German Computational Morphol-
ogy Covering Derivation, Composition and Inflec-
tion. In Proc. 4th LREC.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
Proc. 20th COLING, pages 162–168.
Helmut Schmid. 2006. Trace prediction and recov-
ery with unlexicalized PCFGs and slash features. In
Proc. 44th ACL.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA.
Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A non-
contiguous tree sequence alignment-based model for
statistical machine translation. In Proc. 47th ACL,
pages 914–922.
</reference>
<page confidence="0.986011">
169
</page>
<reference confidence="0.999711384615385">
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008a. A tree
sequence alignment-based tree-to-tree translation
model. In Proc. 46th ACL, pages 559–567.
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, and
Sheng Li. 2008b. Grammar comparison study
for translational equivalence modeling and statistical
machine translation. In Proc. 22nd COLING, pages
1097–1104.
</reference>
<page confidence="0.99742">
170
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.767086">
<title confidence="0.996589">Exact Decoding: The IMS-TTT submission to</title>
<author confidence="0.993345">Daniel Quernheim Fabienne Cap</author>
<affiliation confidence="0.9984515">IMS CIS University of Stuttgart Ludwig-Maximilian University of Munich</affiliation>
<email confidence="0.78178">daniel@ims.uni-stuttgart.decap@cis.uni-muenchen.de</email>
<abstract confidence="0.9995030625">We present the IMS-TTT submission to WMT14, an experimental statistical treeto-tree machine translation system based on the multi-bottom up tree transducer including rule extraction, tuning and decoding. Thanks to input parse forests and a “no pruning” strategy during decoding, the obtained translations are competitive. The drawbacks are a restricted coverage of 70% on test data, in part due to exact input parse tree matching, and a relatively high runtime. Advantages include easy redecoding with a different weight vector, since the full translation forests can be stored after the first decoding pass.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andr´e Arnold</author>
<author>Max Dauchet</author>
</authors>
<title>Morphismes et bimorphismes d’arbres.</title>
<date>1982</date>
<journal>Theoret. Comput. Sci.,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="1009" citStr="Arnold and Dauchet (1982)" startWordPosition="144" endWordPosition="147">ding rule extraction, tuning and decoding. Thanks to input parse forests and a “no pruning” strategy during decoding, the obtained translations are competitive. The drawbacks are a restricted coverage of 70% on test data, in part due to exact input parse tree matching, and a relatively high runtime. Advantages include easy redecoding with a different weight vector, since the full translation forests can be stored after the first decoding pass. 1 Introduction In this contribution, we present an implementation of a translation model that is based on MMBOT (the multi bottom-up tree transducer of Arnold and Dauchet (1982) and Lilin (1978)). Intuitively, an MBOT is a synchronous tree sequence substitution grammar (STSSG, Zhang et al. (2008a); Zhang et al. (2008b); Sun et al. (2009)) that has discontiguities only on the target side (Maletti, 2011). From an algorithmic point of view, this makes the MBOT more appealing than STSSG as demonstrated by Maletti (2010). Formally, MBOT is expressive enough to express all sensible translations (Maletti, 2012)1. Figure 2 displays sample rules of the MBOT variant, called MMBOT, ∗This work was supported by Deutsche Forschungsgemeinschaft grants Models of Morphosyntax for Sta</context>
</contexts>
<marker>Arnold, Dauchet, 1982</marker>
<rawString>Andr´e Arnold and Max Dauchet. 1982. Morphismes et bimorphismes d’arbres. Theoret. Comput. Sci., 20(1):33–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilker Aziz</author>
<author>Marc Dymetman</author>
<author>Sriram Venkatapathy</author>
</authors>
<title>Investigations in exact inference for hierarchical translation.</title>
<date>2013</date>
<booktitle>In Proc. 8th WMT,</booktitle>
<pages>472--483</pages>
<marker>Aziz, Dymetman, Venkatapathy, 2013</marker>
<rawString>Wilker Aziz, Marc Dymetman, and Sriram Venkatapathy. 2013. Investigations in exact inference for hierarchical translation. In Proc. 8th WMT, pages 472–483.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabienne Braune</author>
<author>Nina Seemann</author>
<author>Daniel Quernheim</author>
<author>Andreas Maletti</author>
</authors>
<title>Shallow local multibottom-up tree transducers in statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proc. 51th ACL,</booktitle>
<pages>811--821</pages>
<contexts>
<context position="1985" citStr="Braune et al., 2013" startWordPosition="300" endWordPosition="303">MBOT is expressive enough to express all sensible translations (Maletti, 2012)1. Figure 2 displays sample rules of the MBOT variant, called MMBOT, ∗This work was supported by Deutsche Forschungsgemeinschaft grants Models of Morphosyntax for Statistical Machine Translation (Phase 2) and MA/4959/1–1. 1A translation is sensible if it is of linear size increase and can be computed by some (potentially copying) top-down tree transducer. that we use (in a graphical representation of the trees and the alignment). Recently, a shallow version of MBOT has been integrated into the popular Moses toolkit (Braune et al., 2013). Our implementation is exact in the sense that it does absolutely no pruning during decoding and thus preserves all translation candidates, while having no mechanism to handle unknown structures. (We added dummy rules that leave unseen lexical material untranslated.) The coverage is thus limited, but still considerably high. Source-side and targetside syntax restrict the search space so that decoding stays tractable. Only the language model scoring is implemented as a separate reranker2. This has several advantages: (1) We can use input parse forests (Liu et al., 2009). (2) Not only is the ou</context>
</contexts>
<marker>Braune, Seemann, Quernheim, Maletti, 2013</marker>
<rawString>Fabienne Braune, Nina Seemann, Daniel Quernheim, and Andreas Maletti. 2013. Shallow local multibottom-up tree transducers in statistical machine translation. In Proc. 51th ACL, pages 811–821.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabienne Cap</author>
<author>Alexander Fraser</author>
<author>Marion Weller</author>
<author>Aoife Cahill</author>
</authors>
<title>How to Produce Unseen Teddy Bears: Improved Morphological Processing of Compounds in SMT.</title>
<date>2014</date>
<booktitle>In Proc. 14th EACL.</booktitle>
<contexts>
<context position="15254" citStr="Cap et al., 2014" startWordPosition="2524" endWordPosition="2527">ee in Figure 1. We only kept the best parse for each sentence during training. After parsing, we prepared three versions of the German corpus: a) RAW, with no morphological post-processing; b) UNSPLIT, using SMOR, a rule-based morphological analyser (Schmid et al., 2004), to reduce words to their base form; c) SPLIT, using SMOR to reduce words to their base form and split compound nouns. After translation, compounds were merged again, and words were re-inflected. Previous experiments using SMOR to lemmatise and split compounds in phrase-based SMT showed improved translation performances, see (Cap et al., 2014a) for details. We then trained three 5-gram language models on monolingual data using KenLM10 (Heafield, 2011; Heafield et al., 2013 to appear) for the three setups. For SPLIT and UNSPLIT, we were only able to use the German side of the parallel data, since parsing is a prerequisite for our morphological post-processing and we did not have the resources to parse more data. For RAW, we additionally used the monolingual German data 8http://www.statmt.org/wmt14/ translation-task.html 9https://sites.google.com/site/ zhangh1982/egret 10http://kheafield.com/code/kenlm/ 167 system BLEU BLEU-cased TE</context>
<context position="16855" citStr="Cap et al., 2014" startWordPosition="2780" endWordPosition="2783">, we chose the WMT12 test set (3,003 sentences of newswire text), available as part of the development data for the WMT13 shared translation task. Since our system had limited coverage on this tuning set, we limited ourselves to the first a subset of sentences we could translate. When translating the test set, our models used parse trees delivered by the Egret parser. After translation, recasing was done by examining the output syntax tree, using a simple heuristics looking for nouns and sentence boundaries. Since coverage on the test set was also limited, we used the systems as described in (Cap et al., 2014b)12 as a fallback to translate sentences that our system was not able to translate. 6 Results We report the overall translation quality, as listed on http://matrix.statmt.org/, measured using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), in Table 1. We assume that the poor performance of UNSPLIT and SPLIT compared to RAW is due to the fact that we use a significantly smaller language model (as explained above) for these two settings. A detailed analysis will follow after the end of the manual evaluation period. 7 Conclusion and further work We presented our submission to the WMT</context>
</contexts>
<marker>Cap, Fraser, Weller, Cahill, 2014</marker>
<rawString>Fabienne Cap, Alexander Fraser, Marion Weller, and Aoife Cahill. 2014a. How to Produce Unseen Teddy Bears: Improved Morphological Processing of Compounds in SMT. In Proc. 14th EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabienne Cap</author>
<author>Marion Weller</author>
<author>Anita Ramm</author>
<author>Alexander Fraser</author>
</authors>
<title>translating from English into German.</title>
<date>2014</date>
<booktitle>2014b. CimS – The CIS and IMS joint submission to WMT</booktitle>
<contexts>
<context position="15254" citStr="Cap et al., 2014" startWordPosition="2524" endWordPosition="2527">ee in Figure 1. We only kept the best parse for each sentence during training. After parsing, we prepared three versions of the German corpus: a) RAW, with no morphological post-processing; b) UNSPLIT, using SMOR, a rule-based morphological analyser (Schmid et al., 2004), to reduce words to their base form; c) SPLIT, using SMOR to reduce words to their base form and split compound nouns. After translation, compounds were merged again, and words were re-inflected. Previous experiments using SMOR to lemmatise and split compounds in phrase-based SMT showed improved translation performances, see (Cap et al., 2014a) for details. We then trained three 5-gram language models on monolingual data using KenLM10 (Heafield, 2011; Heafield et al., 2013 to appear) for the three setups. For SPLIT and UNSPLIT, we were only able to use the German side of the parallel data, since parsing is a prerequisite for our morphological post-processing and we did not have the resources to parse more data. For RAW, we additionally used the monolingual German data 8http://www.statmt.org/wmt14/ translation-task.html 9https://sites.google.com/site/ zhangh1982/egret 10http://kheafield.com/code/kenlm/ 167 system BLEU BLEU-cased TE</context>
<context position="16855" citStr="Cap et al., 2014" startWordPosition="2780" endWordPosition="2783">, we chose the WMT12 test set (3,003 sentences of newswire text), available as part of the development data for the WMT13 shared translation task. Since our system had limited coverage on this tuning set, we limited ourselves to the first a subset of sentences we could translate. When translating the test set, our models used parse trees delivered by the Egret parser. After translation, recasing was done by examining the output syntax tree, using a simple heuristics looking for nouns and sentence boundaries. Since coverage on the test set was also limited, we used the systems as described in (Cap et al., 2014b)12 as a fallback to translate sentences that our system was not able to translate. 6 Results We report the overall translation quality, as listed on http://matrix.statmt.org/, measured using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), in Table 1. We assume that the poor performance of UNSPLIT and SPLIT compared to RAW is due to the fact that we use a significantly smaller language model (as explained above) for these two settings. A detailed analysis will follow after the end of the manual evaluation period. 7 Conclusion and further work We presented our submission to the WMT</context>
</contexts>
<marker>Cap, Weller, Ramm, Fraser, 2014</marker>
<rawString>Fabienne Cap, Marion Weller, Anita Ramm, and Alexander Fraser. 2014b. CimS – The CIS and IMS joint submission to WMT 2014 translating from English into German. In Proc. 9th WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computat. Linguist.,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computat. Linguist., 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zolt´an F¨ul¨op</author>
<author>Heiko Vogler</author>
</authors>
<title>Weighted tree automata and tree transducers.</title>
<date>2009</date>
<booktitle>Handbook of Weighted Automata, EATCS Monographs on Theoret. Comput. Sci., chapter 9,</booktitle>
<pages>313--403</pages>
<editor>In Manfred Droste, Werner Kuich, and Heiko Vogler, editors,</editor>
<publisher>Springer.</publisher>
<marker>F¨ul¨op, Vogler, 2009</marker>
<rawString>Zolt´an F¨ul¨op and Heiko Vogler. 2009. Weighted tree automata and tree transducers. In Manfred Droste, Werner Kuich, and Heiko Vogler, editors, Handbook of Weighted Automata, EATCS Monographs on Theoret. Comput. Sci., chapter 9, pages 313–403. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Ivan Pouzyrevsky</author>
<author>Jonathan H Clark</author>
<author>Philipp Koehn</author>
</authors>
<title>(to appear). Scalable modified Kneser-Ney language model estimation. In</title>
<date>2013</date>
<booktitle>Proc. 51stACL.</booktitle>
<contexts>
<context position="15387" citStr="Heafield et al., 2013" startWordPosition="2544" endWordPosition="2547">German corpus: a) RAW, with no morphological post-processing; b) UNSPLIT, using SMOR, a rule-based morphological analyser (Schmid et al., 2004), to reduce words to their base form; c) SPLIT, using SMOR to reduce words to their base form and split compound nouns. After translation, compounds were merged again, and words were re-inflected. Previous experiments using SMOR to lemmatise and split compounds in phrase-based SMT showed improved translation performances, see (Cap et al., 2014a) for details. We then trained three 5-gram language models on monolingual data using KenLM10 (Heafield, 2011; Heafield et al., 2013 to appear) for the three setups. For SPLIT and UNSPLIT, we were only able to use the German side of the parallel data, since parsing is a prerequisite for our morphological post-processing and we did not have the resources to parse more data. For RAW, we additionally used the monolingual German data 8http://www.statmt.org/wmt14/ translation-task.html 9https://sites.google.com/site/ zhangh1982/egret 10http://kheafield.com/code/kenlm/ 167 system BLEU BLEU-cased TER RAW 17.0 16.4 .770 UNSPLIT 16.4 15.8 .773 SPLIT 16.3 15.7 .773 Table 1: BLEU and TER scores of the submitted systems. that was dist</context>
</contexts>
<marker>Heafield, Pouzyrevsky, Clark, Koehn, 2013</marker>
<rawString>Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013 (to appear). Scalable modified Kneser-Ney language model estimation. In Proc. 51stACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proc. 6th WMT,</booktitle>
<pages>187--197</pages>
<contexts>
<context position="15364" citStr="Heafield, 2011" startWordPosition="2542" endWordPosition="2543">versions of the German corpus: a) RAW, with no morphological post-processing; b) UNSPLIT, using SMOR, a rule-based morphological analyser (Schmid et al., 2004), to reduce words to their base form; c) SPLIT, using SMOR to reduce words to their base form and split compound nouns. After translation, compounds were merged again, and words were re-inflected. Previous experiments using SMOR to lemmatise and split compounds in phrase-based SMT showed improved translation performances, see (Cap et al., 2014a) for details. We then trained three 5-gram language models on monolingual data using KenLM10 (Heafield, 2011; Heafield et al., 2013 to appear) for the three setups. For SPLIT and UNSPLIT, we were only able to use the German side of the parallel data, since parsing is a prerequisite for our morphological post-processing and we did not have the resources to parse more data. For RAW, we additionally used the monolingual German data 8http://www.statmt.org/wmt14/ translation-task.html 9https://sites.google.com/site/ zhangh1982/egret 10http://kheafield.com/code/kenlm/ 167 system BLEU BLEU-cased TER RAW 17.0 16.4 .770 UNSPLIT 16.4 15.8 .773 SPLIT 16.3 15.7 .773 Table 1: BLEU and TER scores of the submitted</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proc. 6th WMT, pages 187–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proc. IWPT,</booktitle>
<pages>53--64</pages>
<contexts>
<context position="2836" citStr="Huang and Chiang (2005)" startWordPosition="442" endWordPosition="445">e unseen lexical material untranslated.) The coverage is thus limited, but still considerably high. Source-side and targetside syntax restrict the search space so that decoding stays tractable. Only the language model scoring is implemented as a separate reranker2. This has several advantages: (1) We can use input parse forests (Liu et al., 2009). (2) Not only is the output optimal with regard to the theoretical model, also the space of translation candidates can be efficiently stored as a weighted regular tree grammar. The best translations can then be extracted using the k-best algorithm by Huang and Chiang (2005). Rule weights can be changed without the need for explicit redecoding, the parameters of the log-linear model can be changed, and even new features can be added. These properties are especially helpful in tuning, where only the k-best algorithm has to be re-run in each iteration. A model in similar spirit has been described by Huang et al. (2006); however, it used target syntax only (using a top-down tree-to-string transducer backwards), and was restricted to sentences of length at most 25. We do not make such restrictions. The theoretical aspects of MMBOT and their use in our translation mod</context>
<context position="12680" citStr="Huang and Chiang, 2005" startWordPosition="2102" endWordPosition="2105">n (proper names are typically uppercase, as are all nouns in German). Another benefit of having a packed representation of all candidates is that we can easily check whether the reference translation is included in the candidate set (“force decoding”). The freedom to allow arbitrary target models that rewrite derivations is related to current work on interpreted regular tree grammars (Koller and Kuhlmann, 2011), where arbitrary algebras can be used to compute a realization of the output tree. k-best extractor From the translation derivation RTGs, a k-best list of derivations can be extracted (Huang and Chiang, 2005) very efficiently. This is the only step that has to be repeated if the rule weights or the parameters of the log-linear model change. The derivations are then mapped to target language sentences (if several derivations realize the same target sentence, their weights are summed) and reranked according to a language model (as was done in Huang et al. (2006)). This is the only part of the pipeline where we deviate from the theoretical log-linear model, and this is where we might make search errors. In principle, one could integrate the language model by intersection with the translation model (a</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proc. IWPT, pages 53–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proc. 7th Conf. AMTA,</booktitle>
<pages>66--73</pages>
<contexts>
<context position="3185" citStr="Huang et al. (2006)" startWordPosition="503" endWordPosition="506"> Not only is the output optimal with regard to the theoretical model, also the space of translation candidates can be efficiently stored as a weighted regular tree grammar. The best translations can then be extracted using the k-best algorithm by Huang and Chiang (2005). Rule weights can be changed without the need for explicit redecoding, the parameters of the log-linear model can be changed, and even new features can be added. These properties are especially helpful in tuning, where only the k-best algorithm has to be re-run in each iteration. A model in similar spirit has been described by Huang et al. (2006); however, it used target syntax only (using a top-down tree-to-string transducer backwards), and was restricted to sentences of length at most 25. We do not make such restrictions. The theoretical aspects of MMBOT and their use in our translation model are presented in Section 2. Based on this, we implemented a machine translation system that we are going to make available to 2Strictly speaking, this does introduce pruning into the pipeline. 163 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 163–170, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association fo</context>
<context position="13038" citStr="Huang et al. (2006)" startWordPosition="2164" endWordPosition="2167">ed regular tree grammars (Koller and Kuhlmann, 2011), where arbitrary algebras can be used to compute a realization of the output tree. k-best extractor From the translation derivation RTGs, a k-best list of derivations can be extracted (Huang and Chiang, 2005) very efficiently. This is the only step that has to be repeated if the rule weights or the parameters of the log-linear model change. The derivations are then mapped to target language sentences (if several derivations realize the same target sentence, their weights are summed) and reranked according to a language model (as was done in Huang et al. (2006)). This is the only part of the pipeline where we deviate from the theoretical log-linear model, and this is where we might make search errors. In principle, one could integrate the language model by intersection with the translation model (as the stateful MBOT model is closed under intersection with finite automata), but this is (currently) not computationally feasible due to the size of models. Tuning Minimum error rate training (Och, 2003) is implemented using Z-MERT7 (Zaidan, 7http://cs.jhu.edu/˜ozaidan/zmert/ 2009). A set of source sentences has to be (forest)parsed and decoded; the trans</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proc. 7th Conf. AMTA, pages 66–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Marco Kuhlmann</author>
</authors>
<title>A generalized view on parsing and translation.</title>
<date>2011</date>
<booktitle>In Proc. IWPT,</booktitle>
<pages>2--13</pages>
<contexts>
<context position="12471" citStr="Koller and Kuhlmann, 2011" startWordPosition="2069" endWordPosition="2072">t is conceivable to use other realizations. For instance, a syntactic language model could be used for output tree scoring. Also, recasing is extremely easy when we have part-of-speech tags to base our decision on (proper names are typically uppercase, as are all nouns in German). Another benefit of having a packed representation of all candidates is that we can easily check whether the reference translation is included in the candidate set (“force decoding”). The freedom to allow arbitrary target models that rewrite derivations is related to current work on interpreted regular tree grammars (Koller and Kuhlmann, 2011), where arbitrary algebras can be used to compute a realization of the output tree. k-best extractor From the translation derivation RTGs, a k-best list of derivations can be extracted (Huang and Chiang, 2005) very efficiently. This is the only step that has to be repeated if the rule weights or the parameters of the log-linear model change. The derivations are then mapped to target language sentences (if several derivations realize the same target sentence, their weights are summed) and reranked according to a language model (as was done in Huang et al. (2006)). This is the only part of the p</context>
</contexts>
<marker>Koller, Kuhlmann, 2011</marker>
<rawString>Alexander Koller and Marco Kuhlmann. 2011. A generalized view on parsing and translation. In Proc. IWPT, pages 2–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Lilin</author>
</authors>
<title>Une g´en´eralisation des transducteurs d’´etats finis d’arbres: les S-transducteurs. Th`ese 3`eme cycle, Universit´e de Lille.</title>
<date>1978</date>
<contexts>
<context position="1026" citStr="Lilin (1978)" startWordPosition="149" endWordPosition="150">nd decoding. Thanks to input parse forests and a “no pruning” strategy during decoding, the obtained translations are competitive. The drawbacks are a restricted coverage of 70% on test data, in part due to exact input parse tree matching, and a relatively high runtime. Advantages include easy redecoding with a different weight vector, since the full translation forests can be stored after the first decoding pass. 1 Introduction In this contribution, we present an implementation of a translation model that is based on MMBOT (the multi bottom-up tree transducer of Arnold and Dauchet (1982) and Lilin (1978)). Intuitively, an MBOT is a synchronous tree sequence substitution grammar (STSSG, Zhang et al. (2008a); Zhang et al. (2008b); Sun et al. (2009)) that has discontiguities only on the target side (Maletti, 2011). From an algorithmic point of view, this makes the MBOT more appealing than STSSG as demonstrated by Maletti (2010). Formally, MBOT is expressive enough to express all sensible translations (Maletti, 2012)1. Figure 2 displays sample rules of the MBOT variant, called MMBOT, ∗This work was supported by Deutsche Forschungsgemeinschaft grants Models of Morphosyntax for Statistical Machine </context>
</contexts>
<marker>Lilin, 1978</marker>
<rawString>Eric Lilin. 1978. Une g´en´eralisation des transducteurs d’´etats finis d’arbres: les S-transducteurs. Th`ese 3`eme cycle, Universit´e de Lille.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Improving tree-to-tree translation with packed forests.</title>
<date>2009</date>
<booktitle>In Proc. 47th ACL,</booktitle>
<pages>558--566</pages>
<marker>Liu, L¨u, Liu, 2009</marker>
<rawString>Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proc. 47th ACL, pages 558–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>Why synchronous tree substitution grammars?</title>
<date>2010</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>876--884</pages>
<contexts>
<context position="1353" citStr="Maletti (2010)" startWordPosition="203" endWordPosition="204">ight vector, since the full translation forests can be stored after the first decoding pass. 1 Introduction In this contribution, we present an implementation of a translation model that is based on MMBOT (the multi bottom-up tree transducer of Arnold and Dauchet (1982) and Lilin (1978)). Intuitively, an MBOT is a synchronous tree sequence substitution grammar (STSSG, Zhang et al. (2008a); Zhang et al. (2008b); Sun et al. (2009)) that has discontiguities only on the target side (Maletti, 2011). From an algorithmic point of view, this makes the MBOT more appealing than STSSG as demonstrated by Maletti (2010). Formally, MBOT is expressive enough to express all sensible translations (Maletti, 2012)1. Figure 2 displays sample rules of the MBOT variant, called MMBOT, ∗This work was supported by Deutsche Forschungsgemeinschaft grants Models of Morphosyntax for Statistical Machine Translation (Phase 2) and MA/4959/1–1. 1A translation is sensible if it is of linear size increase and can be computed by some (potentially copying) top-down tree transducer. that we use (in a graphical representation of the trees and the alignment). Recently, a shallow version of MBOT has been integrated into the popular Mos</context>
</contexts>
<marker>Maletti, 2010</marker>
<rawString>Andreas Maletti. 2010. Why synchronous tree substitution grammars? In Proc. HLT-NAACL, pages 876–884.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>How to train your multi bottom-up tree transducer.</title>
<date>2011</date>
<booktitle>In Proc. 49th ACL,</booktitle>
<pages>825--834</pages>
<contexts>
<context position="1237" citStr="Maletti, 2011" startWordPosition="183" endWordPosition="184">act input parse tree matching, and a relatively high runtime. Advantages include easy redecoding with a different weight vector, since the full translation forests can be stored after the first decoding pass. 1 Introduction In this contribution, we present an implementation of a translation model that is based on MMBOT (the multi bottom-up tree transducer of Arnold and Dauchet (1982) and Lilin (1978)). Intuitively, an MBOT is a synchronous tree sequence substitution grammar (STSSG, Zhang et al. (2008a); Zhang et al. (2008b); Sun et al. (2009)) that has discontiguities only on the target side (Maletti, 2011). From an algorithmic point of view, this makes the MBOT more appealing than STSSG as demonstrated by Maletti (2010). Formally, MBOT is expressive enough to express all sensible translations (Maletti, 2012)1. Figure 2 displays sample rules of the MBOT variant, called MMBOT, ∗This work was supported by Deutsche Forschungsgemeinschaft grants Models of Morphosyntax for Statistical Machine Translation (Phase 2) and MA/4959/1–1. 1A translation is sensible if it is of linear size increase and can be computed by some (potentially copying) top-down tree transducer. that we use (in a graphical represen</context>
<context position="4176" citStr="Maletti, 2011" startWordPosition="656" endWordPosition="657">to 2Strictly speaking, this does introduce pruning into the pipeline. 163 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 163–170, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics the public. Section 4 presents the most important components of our `MBOT implementation, and Section 5 presents our submission to the WMT14 shared translation task. 2 Theoretical Model In this section, we present the theoretical generative model that is used in our approach to syntaxbased machine translation: the multi bottom-up tree transducer (Maletti, 2011). We omit the technical details and give graphical examples only to illustrate how the device works, but refer to the literature for the theoretical background. Roughly speaking, a local multi bottom-up tree transducer (`MBOT) has rules that replace one nonterminal symbol N on the source side by a tree, and a sequence of nonterminal symbols on the target side linked to N by one tree each. These trees again have linked nonterminals, thus allowing further rule applications. Our `MBOT rules are obtained automatically from data like that in Figure 1. Thus, we (word) align the bilingual text and pa</context>
<context position="6301" citStr="Maletti, 2011" startWordPosition="1026" endWordPosition="1027">t from `MBOT application to input trees, we can even apply `MBOT to parse forests and even weighted regular tree grammars (RTGs) (F¨ul¨op and Vogler, 2009). RTGs offer an efficient representation of weighted forests, which are sets of trees such that each individual tree is equipped with a weight. This representation is even more efficient than packed forests (Mi et al., 2008) and moreover can represent an infinite number of weighted trees. The most important property that we utilize is that the output tree language is regular, so we can represent it by an RTG (cf. preservation of regularity (Maletti, 2011)). Indeed, every input tree can only be transformed into finitely many output trees by our model, so for a given finite input forest (which the output of the parser is) the computed output forest will also be finite and thus regular. 3 Translation Model Given a source language sentence e and corresponding weighted parse forest F(e), our translation model aims to find the best corresponding target language translation ˆg;3 i.e., gˆ = arg maxg p(g|e) . We estimate the probability p(g|e) through a loglinear combination of component models with parameters λm scored on the derivations d such that t</context>
<context position="10125" citStr="Maletti, 2011" startWordPosition="1679" endWordPosition="1680">d, then it is assumed to be aligned to a special NULL word and this alignment is scored. nents, most of which are implemented in Python. The system does not have any dependencies other than the need for parsers for the source and target language, a word alignment tool and optionally an implementation of some tuning algorithm. A schematic depiction of the training and decoding pipeline can be seen in Figure 4. Rule extraction From a parallel corpus of which both halves have been parsed and word aligned, multi bottom-up tree transducer rules are extracted according to the procedure laid out in (Maletti, 2011). In order to handle unknown words, we add dummy identity translation rules for lexical material that was not present in the training data. Translation model building Given a set of rules, translation weights (see above) are computed for each unique rule. The translation model is then converted into a source, a weight and a target model. The source model (an RTG represented in an efficient binary format) is used for decoding and maps input trees to trees over rule identifiers representing derivations. The weight model and the target model can be used to reconstruct the weight and the target re</context>
</contexts>
<marker>Maletti, 2011</marker>
<rawString>Andreas Maletti. 2011. How to train your multi bottom-up tree transducer. In Proc. 49th ACL, pages 825–834.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>Every sensible extended topdown tree transducer is a multi bottom-up tree transducer.</title>
<date>2012</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>263--273</pages>
<contexts>
<context position="1443" citStr="Maletti, 2012" startWordPosition="216" endWordPosition="217">s. 1 Introduction In this contribution, we present an implementation of a translation model that is based on MMBOT (the multi bottom-up tree transducer of Arnold and Dauchet (1982) and Lilin (1978)). Intuitively, an MBOT is a synchronous tree sequence substitution grammar (STSSG, Zhang et al. (2008a); Zhang et al. (2008b); Sun et al. (2009)) that has discontiguities only on the target side (Maletti, 2011). From an algorithmic point of view, this makes the MBOT more appealing than STSSG as demonstrated by Maletti (2010). Formally, MBOT is expressive enough to express all sensible translations (Maletti, 2012)1. Figure 2 displays sample rules of the MBOT variant, called MMBOT, ∗This work was supported by Deutsche Forschungsgemeinschaft grants Models of Morphosyntax for Statistical Machine Translation (Phase 2) and MA/4959/1–1. 1A translation is sensible if it is of linear size increase and can be computed by some (potentially copying) top-down tree transducer. that we use (in a graphical representation of the trees and the alignment). Recently, a shallow version of MBOT has been integrated into the popular Moses toolkit (Braune et al., 2013). Our implementation is exact in the sense that it does ab</context>
</contexts>
<marker>Maletti, 2012</marker>
<rawString>Andreas Maletti. 2012. Every sensible extended topdown tree transducer is a multi bottom-up tree transducer. In Proc. HLT-NAACL, pages 263–273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>206--214</pages>
<contexts>
<context position="18265" citStr="Mi and Huang, 2008" startWordPosition="3017" endWordPosition="3020">p/ 12We use raw as described in (Cap et al., 2014b) as a fallback for RAW, RI for UNSPLIT and CoRI for SPLIT. et al. (2006). There are, however, still major drawbacks and open problems associated with our approach. Firstly, the coverage can still be significantly improved. In these experiments, our model was able to translate only 70% of the test sentences. To some extent, this number can be improved by providing more training data. Also, more rules can be extracted if we not only use the best parse for rule extraction, but multiple parse trees, or even switch to forest-based rule extraction (Mi and Huang, 2008). Finally, the size of the input parse forest plays a role. For instance, if we only supply the best parse to our model, translation will fail for approximately half of the input. However, there are inherent coverage limits. Since our model is extremely strict, it will never be able to translate sentences whose parse trees contain structures it has never seen before, since it has to match at least one input parse tree exactly. While we implemented a simple solution to handle unknown words, the issue with unknown structures is not so easy to solve without breaking the otherwise theoretically so</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proc. EMNLP, pages 206– 214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proc. 46th ACL,</booktitle>
<pages>192</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="6066" citStr="Mi et al., 2008" startWordPosition="983" endWordPosition="986">target trees that can be derived given a source tree. For a given synchronous derivation d, the source tree generated by d is s(d), and the target tree is t(d). The yield of a tree is the string obtained by concatenating its leaves. Apart from `MBOT application to input trees, we can even apply `MBOT to parse forests and even weighted regular tree grammars (RTGs) (F¨ul¨op and Vogler, 2009). RTGs offer an efficient representation of weighted forests, which are sets of trees such that each individual tree is equipped with a weight. This representation is even more efficient than packed forests (Mi et al., 2008) and moreover can represent an infinite number of weighted trees. The most important property that we utilize is that the output tree language is regular, so we can represent it by an RTG (cf. preservation of regularity (Maletti, 2011)). Indeed, every input tree can only be transformed into finitely many output trees by our model, so for a given finite input forest (which the output of the parser is) the computed output forest will also be finite and thus regular. 3 Translation Model Given a source language sentence e and corresponding weighted parse forest F(e), our translation model aims to </context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proc. 46th ACL, pages 192– 199. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. 41st ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="13484" citStr="Och, 2003" startWordPosition="2240" endWordPosition="2241">s (if several derivations realize the same target sentence, their weights are summed) and reranked according to a language model (as was done in Huang et al. (2006)). This is the only part of the pipeline where we deviate from the theoretical log-linear model, and this is where we might make search errors. In principle, one could integrate the language model by intersection with the translation model (as the stateful MBOT model is closed under intersection with finite automata), but this is (currently) not computationally feasible due to the size of models. Tuning Minimum error rate training (Och, 2003) is implemented using Z-MERT7 (Zaidan, 7http://cs.jhu.edu/˜ozaidan/zmert/ 2009). A set of source sentences has to be (forest)parsed and decoded; the translation forests are stored on disk. Then, in each iteration of Z-MERT, it suffices to extract k-best lists from the translation forests according to the current weight vector. 5 WMT14 Experimental setup We used the training data that was made available for the WMT14 shared translation task on English–German8. It consists of three parallel corpora (1.9M sentences of European parliament proceedings, 201K sentences of newswire text, and 2M senten</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. 41st ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. 40th ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="17076" citStr="Papineni et al., 2002" startWordPosition="2814" endWordPosition="2817"> ourselves to the first a subset of sentences we could translate. When translating the test set, our models used parse trees delivered by the Egret parser. After translation, recasing was done by examining the output syntax tree, using a simple heuristics looking for nouns and sentence boundaries. Since coverage on the test set was also limited, we used the systems as described in (Cap et al., 2014b)12 as a fallback to translate sentences that our system was not able to translate. 6 Results We report the overall translation quality, as listed on http://matrix.statmt.org/, measured using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), in Table 1. We assume that the poor performance of UNSPLIT and SPLIT compared to RAW is due to the fact that we use a significantly smaller language model (as explained above) for these two settings. A detailed analysis will follow after the end of the manual evaluation period. 7 Conclusion and further work We presented our submission to the WMT14 shared translation task based on a novel, promising “full syntax, no pruning” tree-to-tree approach to statistical machine translation, inspired by Huang 11https://code.google.com/p/giza-pp/ 12We use raw as described i</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. 40th ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>K-best A* parsing.</title>
<date>2009</date>
<booktitle>In Proc. 47th ACL,</booktitle>
<pages>958--966</pages>
<contexts>
<context position="19559" citStr="Pauls and Klein, 2009" startWordPosition="3238" endWordPosition="3241">me. We were able to translate about 15 sentences per hour on one processor. Distributing the translation task on different machines, we were able to translate the WMT14 test set (10k sentences) in roughly four days. Given that the trend goes towards parallel programming, and considering the fact that our decoder is written in the rather slow language Python, we are confident that this is not a major problem. We were able to run the whole pipeline of training, tuning and evaluation on the WMT14 shared task data in less than one week. We are currently investigating whether A* k-best algorithms (Pauls and Klein, 2009; Pauls et al., 2010) can help to guide the translation process while maintaining optimality. Thirdly, currently the language model is not integrated, but implemented as a separate reranking component. We are aware that this might introduce search errors, and that an integrated language model might improve translation quality (see e.g. Chiang (2007) where 3–4 BLEU points are gained by LM integration). Some research on this topic already exists, e.g. (Rush and Collins, 2011) who use dual decomposition, and (Aziz et al., 2013) who replace intersection with an upper bound which is easier to compu</context>
</contexts>
<marker>Pauls, Klein, 2009</marker>
<rawString>Adam Pauls and Dan Klein. 2009. K-best A* parsing. In Proc. 47th ACL, pages 958–966.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
<author>Chris Quirk</author>
</authors>
<title>Topdown k-best A* parsing.</title>
<date>2010</date>
<booktitle>In Proc. 48th ACL,</booktitle>
<pages>200--204</pages>
<marker>Pauls, Klein, Quirk, 2010</marker>
<rawString>Adam Pauls, Dan Klein, and Chris Quirk. 2010. Topdown k-best A* parsing. In Proc. 48th ACL, pages 200–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL,</booktitle>
<pages>433--440</pages>
<contexts>
<context position="14302" citStr="Petrov et al., 2006" startWordPosition="2367" endWordPosition="2370">each iteration of Z-MERT, it suffices to extract k-best lists from the translation forests according to the current weight vector. 5 WMT14 Experimental setup We used the training data that was made available for the WMT14 shared translation task on English–German8. It consists of three parallel corpora (1.9M sentences of European parliament proceedings, 201K sentences of newswire text, and 2M sentences of web text) and additional monolingual news data for language model training. The English half of the parallel data was parsed using Egret9 which is a re-implementation of the Berkeley parser (Petrov et al., 2006). For the German parse, we used the BitPar parser (Schmid, 2004; Schmid, 2006). The BitPar German grammar is highly detailed, which makes the syntactic information contained in the parses extremely useful. Part-of-speech tags and category label are augmented by case, number and gender information, as can be seen in the German parse tree in Figure 1. We only kept the best parse for each sentence during training. After parsing, we prepared three versions of the German corpus: a) RAW, with no morphological post-processing; b) UNSPLIT, using SMOR, a rule-based morphological analyser (Schmid et al.</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. COLING-ACL, pages 433–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
</authors>
<title>Exact decoding of syntactic translation models through lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In Proc. 49th ACL,</booktitle>
<pages>72--82</pages>
<marker>Rush, Collins, 2011</marker>
<rawString>Alexander M. Rush and Michael Collins. 2011. Exact decoding of syntactic translation models through lagrangian relaxation. In Proc. 49th ACL, pages 72– 82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
<author>Arne Fitschen</author>
<author>Ulrich Heid</author>
</authors>
<title>SMOR: A German Computational Morphology Covering Derivation, Composition and Inflection.</title>
<date>2004</date>
<booktitle>In Proc. 4th LREC.</booktitle>
<contexts>
<context position="14909" citStr="Schmid et al., 2004" startWordPosition="2468" endWordPosition="2471">et al., 2006). For the German parse, we used the BitPar parser (Schmid, 2004; Schmid, 2006). The BitPar German grammar is highly detailed, which makes the syntactic information contained in the parses extremely useful. Part-of-speech tags and category label are augmented by case, number and gender information, as can be seen in the German parse tree in Figure 1. We only kept the best parse for each sentence during training. After parsing, we prepared three versions of the German corpus: a) RAW, with no morphological post-processing; b) UNSPLIT, using SMOR, a rule-based morphological analyser (Schmid et al., 2004), to reduce words to their base form; c) SPLIT, using SMOR to reduce words to their base form and split compound nouns. After translation, compounds were merged again, and words were re-inflected. Previous experiments using SMOR to lemmatise and split compounds in phrase-based SMT showed improved translation performances, see (Cap et al., 2014a) for details. We then trained three 5-gram language models on monolingual data using KenLM10 (Heafield, 2011; Heafield et al., 2013 to appear) for the three setups. For SPLIT and UNSPLIT, we were only able to use the German side of the parallel data, si</context>
</contexts>
<marker>Schmid, Fitschen, Heid, 2004</marker>
<rawString>Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004. SMOR: A German Computational Morphology Covering Derivation, Composition and Inflection. In Proc. 4th LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Efficient parsing of highly ambiguous context-free grammars with bit vectors.</title>
<date>2004</date>
<booktitle>In Proc. 20th COLING,</booktitle>
<pages>162--168</pages>
<contexts>
<context position="14365" citStr="Schmid, 2004" startWordPosition="2381" endWordPosition="2382">translation forests according to the current weight vector. 5 WMT14 Experimental setup We used the training data that was made available for the WMT14 shared translation task on English–German8. It consists of three parallel corpora (1.9M sentences of European parliament proceedings, 201K sentences of newswire text, and 2M sentences of web text) and additional monolingual news data for language model training. The English half of the parallel data was parsed using Egret9 which is a re-implementation of the Berkeley parser (Petrov et al., 2006). For the German parse, we used the BitPar parser (Schmid, 2004; Schmid, 2006). The BitPar German grammar is highly detailed, which makes the syntactic information contained in the parses extremely useful. Part-of-speech tags and category label are augmented by case, number and gender information, as can be seen in the German parse tree in Figure 1. We only kept the best parse for each sentence during training. After parsing, we prepared three versions of the German corpus: a) RAW, with no morphological post-processing; b) UNSPLIT, using SMOR, a rule-based morphological analyser (Schmid et al., 2004), to reduce words to their base form; c) SPLIT, using SM</context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Helmut Schmid. 2004. Efficient parsing of highly ambiguous context-free grammars with bit vectors. In Proc. 20th COLING, pages 162–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Trace prediction and recovery with unlexicalized PCFGs and slash features.</title>
<date>2006</date>
<booktitle>In Proc. 44th ACL.</booktitle>
<contexts>
<context position="14380" citStr="Schmid, 2006" startWordPosition="2383" endWordPosition="2384">rests according to the current weight vector. 5 WMT14 Experimental setup We used the training data that was made available for the WMT14 shared translation task on English–German8. It consists of three parallel corpora (1.9M sentences of European parliament proceedings, 201K sentences of newswire text, and 2M sentences of web text) and additional monolingual news data for language model training. The English half of the parallel data was parsed using Egret9 which is a re-implementation of the Berkeley parser (Petrov et al., 2006). For the German parse, we used the BitPar parser (Schmid, 2004; Schmid, 2006). The BitPar German grammar is highly detailed, which makes the syntactic information contained in the parses extremely useful. Part-of-speech tags and category label are augmented by case, number and gender information, as can be seen in the German parse tree in Figure 1. We only kept the best parse for each sentence during training. After parsing, we prepared three versions of the German corpus: a) RAW, with no morphological post-processing; b) UNSPLIT, using SMOR, a rule-based morphological analyser (Schmid et al., 2004), to reduce words to their base form; c) SPLIT, using SMOR to reduce wo</context>
</contexts>
<marker>Schmid, 2006</marker>
<rawString>Helmut Schmid. 2006. Trace prediction and recovery with unlexicalized PCFGs and slash features. In Proc. 44th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proc. AMTA.</booktitle>
<contexts>
<context position="17106" citStr="Snover et al., 2006" startWordPosition="2820" endWordPosition="2823"> of sentences we could translate. When translating the test set, our models used parse trees delivered by the Egret parser. After translation, recasing was done by examining the output syntax tree, using a simple heuristics looking for nouns and sentence boundaries. Since coverage on the test set was also limited, we used the systems as described in (Cap et al., 2014b)12 as a fallback to translate sentences that our system was not able to translate. 6 Results We report the overall translation quality, as listed on http://matrix.statmt.org/, measured using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), in Table 1. We assume that the poor performance of UNSPLIT and SPLIT compared to RAW is due to the fact that we use a significantly smaller language model (as explained above) for these two settings. A detailed analysis will follow after the end of the manual evaluation period. 7 Conclusion and further work We presented our submission to the WMT14 shared translation task based on a novel, promising “full syntax, no pruning” tree-to-tree approach to statistical machine translation, inspired by Huang 11https://code.google.com/p/giza-pp/ 12We use raw as described in (Cap et al., 2014b) as a fal</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proc. AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Sun</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
</authors>
<title>A noncontiguous tree sequence alignment-based model for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. 47th ACL,</booktitle>
<pages>914--922</pages>
<contexts>
<context position="1171" citStr="Sun et al. (2009)" startWordPosition="170" endWordPosition="173">acks are a restricted coverage of 70% on test data, in part due to exact input parse tree matching, and a relatively high runtime. Advantages include easy redecoding with a different weight vector, since the full translation forests can be stored after the first decoding pass. 1 Introduction In this contribution, we present an implementation of a translation model that is based on MMBOT (the multi bottom-up tree transducer of Arnold and Dauchet (1982) and Lilin (1978)). Intuitively, an MBOT is a synchronous tree sequence substitution grammar (STSSG, Zhang et al. (2008a); Zhang et al. (2008b); Sun et al. (2009)) that has discontiguities only on the target side (Maletti, 2011). From an algorithmic point of view, this makes the MBOT more appealing than STSSG as demonstrated by Maletti (2010). Formally, MBOT is expressive enough to express all sensible translations (Maletti, 2012)1. Figure 2 displays sample rules of the MBOT variant, called MMBOT, ∗This work was supported by Deutsche Forschungsgemeinschaft grants Models of Morphosyntax for Statistical Machine Translation (Phase 2) and MA/4959/1–1. 1A translation is sensible if it is of linear size increase and can be computed by some (potentially copyi</context>
</contexts>
<marker>Sun, Zhang, Tan, 2009</marker>
<rawString>Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A noncontiguous tree sequence alignment-based model for statistical machine translation. In Proc. 47th ACL, pages 914–922.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
</authors>
<title>Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>91--79</pages>
<marker>Zaidan, 2009</marker>
<rawString>Omar F. Zaidan. 2009. Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. The Prague Bulletin of Mathematical Linguistics, 91:79–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<booktitle>In Proc. 46th ACL,</booktitle>
<pages>559--567</pages>
<contexts>
<context position="1128" citStr="Zhang et al. (2008" startWordPosition="162" endWordPosition="165">ined translations are competitive. The drawbacks are a restricted coverage of 70% on test data, in part due to exact input parse tree matching, and a relatively high runtime. Advantages include easy redecoding with a different weight vector, since the full translation forests can be stored after the first decoding pass. 1 Introduction In this contribution, we present an implementation of a translation model that is based on MMBOT (the multi bottom-up tree transducer of Arnold and Dauchet (1982) and Lilin (1978)). Intuitively, an MBOT is a synchronous tree sequence substitution grammar (STSSG, Zhang et al. (2008a); Zhang et al. (2008b); Sun et al. (2009)) that has discontiguities only on the target side (Maletti, 2011). From an algorithmic point of view, this makes the MBOT more appealing than STSSG as demonstrated by Maletti (2010). Formally, MBOT is expressive enough to express all sensible translations (Maletti, 2012)1. Figure 2 displays sample rules of the MBOT variant, called MMBOT, ∗This work was supported by Deutsche Forschungsgemeinschaft grants Models of Morphosyntax for Statistical Machine Translation (Phase 2) and MA/4959/1–1. 1A translation is sensible if it is of linear size increase and</context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008a. A tree sequence alignment-based tree-to-tree translation model. In Proc. 46th ACL, pages 559–567.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Haizhou Li</author>
<author>Aiti Aw</author>
<author>Sheng Li</author>
</authors>
<title>Grammar comparison study for translational equivalence modeling and statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. 22nd COLING,</booktitle>
<pages>1097--1104</pages>
<contexts>
<context position="1128" citStr="Zhang et al. (2008" startWordPosition="162" endWordPosition="165">ined translations are competitive. The drawbacks are a restricted coverage of 70% on test data, in part due to exact input parse tree matching, and a relatively high runtime. Advantages include easy redecoding with a different weight vector, since the full translation forests can be stored after the first decoding pass. 1 Introduction In this contribution, we present an implementation of a translation model that is based on MMBOT (the multi bottom-up tree transducer of Arnold and Dauchet (1982) and Lilin (1978)). Intuitively, an MBOT is a synchronous tree sequence substitution grammar (STSSG, Zhang et al. (2008a); Zhang et al. (2008b); Sun et al. (2009)) that has discontiguities only on the target side (Maletti, 2011). From an algorithmic point of view, this makes the MBOT more appealing than STSSG as demonstrated by Maletti (2010). Formally, MBOT is expressive enough to express all sensible translations (Maletti, 2012)1. Figure 2 displays sample rules of the MBOT variant, called MMBOT, ∗This work was supported by Deutsche Forschungsgemeinschaft grants Models of Morphosyntax for Statistical Machine Translation (Phase 2) and MA/4959/1–1. 1A translation is sensible if it is of linear size increase and</context>
</contexts>
<marker>Zhang, Jiang, Li, Aw, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, and Sheng Li. 2008b. Grammar comparison study for translational equivalence modeling and statistical machine translation. In Proc. 22nd COLING, pages 1097–1104.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
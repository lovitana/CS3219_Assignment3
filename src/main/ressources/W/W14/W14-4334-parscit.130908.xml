<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.306795">
<title confidence="0.99629">
A Demonstration of Dialogue Processing in SimSensei Kiosk
</title>
<author confidence="0.976677">
Fabrizio Morbini, David DeVault, Kallirroi Georgila,
Ron Artstein, David Traum, Louis-Philippe Morency
</author>
<affiliation confidence="0.972391">
USC Institute for Creative Technologies
</affiliation>
<address confidence="0.939677">
12015 Waterfront Dr., Playa Vista, CA 90094
</address>
<email confidence="0.999817">
{morbini,devault,kgeorgila,artstein,traum,morency}@ict.usc.edu
</email>
<sectionHeader confidence="0.997402" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999865388888889">
This demonstration highlights the dia-
logue processing in SimSensei Kiosk, a
virtual human dialogue system that con-
ducts interviews related to psychologi-
cal distress conditions such as depression,
anxiety, and post-traumatic stress disorder
(PTSD). The dialogue processing in Sim-
Sensei Kiosk allows the system to con-
duct coherent spoken interviews of human
users that are 15-25 minutes in length,
and in which users feel comfortable talk-
ing and openly sharing information. We
present the design of the individual dia-
logue components, and show examples of
natural conversation flow between the sys-
tem and users, including expressions of
empathy, follow-up responses and contin-
uation prompts, and turn-taking.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999856894736842">
This demonstration highlights the dialogue pro-
cessing in SimSensei Kiosk, a virtual human di-
alogue system that conducts interviews related to
psychological distress conditions such as depres-
sion, anxiety, and post-traumatic stress disorder
(PTSD) (DeVault et al., 2014). SimSensei Kiosk
has two main functions – a virtual human called
Ellie (pictured in Figure 1), who converses with a
user in a spoken, semi-structured interview, and a
multimodal perception system which analyzes the
user’s behavior in real time to identify indicators
of psychological distress.
The system has been designed and devel-
oped over two years using a series of face-to-
face, Wizard-of-Oz, and automated system stud-
ies involving more than 350 human participants
(Scherer et al., 2013; DeVault et al., 2013; DeVault
et al., 2014). Agent design has been guided by
two overarching goals: (1) the agent should make
</bodyText>
<figureCaption confidence="0.947113">
Figure 1: Ellie, the virtual human interviewer in
SimSensei Kiosk.
</figureCaption>
<bodyText confidence="0.999844076923077">
the user feel comfortable talking and openly shar-
ing information, and at the same time (2) the agent
should create interactional situations that support
the automatic assessment of verbal and nonver-
bal behaviors correlated with psychological dis-
tress. During an interview, the agent presents a
set of questions which have been shown in user
testing to support these goals. Since the main in-
terview questions and their order are mostly fixed,
dialogue management concentrates on providing
appropriate verbal feedback behaviors to keep the
user engaged, maintain a natural and comfort-
able conversation flow, and elicit continuations
and elaborations from the user.
The agent is implemented using a modular ar-
chitecture (Hartholt et al., 2013). Dialogue pro-
cessing, which is the focus of this demonstration,
is supported by individual modules for speech
recognition, language understanding and dialogue
management (see Section 2). The agent’s lan-
guage and speech are executed by selecting from
pre-recorded audio clips. Additional agent mod-
ules include nonverbal behavior generation, which
matches appropriately timed body movements to
the agent’s speech; character animation in a vir-
tual 3D environment; and rendering in a game en-
</bodyText>
<page confidence="0.908939">
254
</page>
<bodyText confidence="0.951185">
Proceedings of the SIGDIAL 2014 Conference, pages 254–256,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
gine. The perception system analyzes audio and
video in real time to identify features such as head
position, gaze direction, smile intensity, and voice
quality. DeVault et al. (2014) provides details on
all the agent’s modules.
</bodyText>
<sectionHeader confidence="0.729683" genericHeader="method">
2 Overview of Dialogue Processing
</sectionHeader>
<subsectionHeader confidence="0.980237">
2.1 ASR and NLU components
</subsectionHeader>
<bodyText confidence="0.999948953488372">
Unlike many task-oriented dialogue domains, in-
terview dialogues between SimSensei Kiosk and
participants are naturally open-ended. People tend
to respond to interview stimuli such as “what’s
one of your most memorable experiences?” with
idiosyncratic stories and events from their lives.
The variability in the vocabulary and content of
participants’ answers to such questions is so large
that it makes the ASR task very challenging. Fur-
thermore, continuous ASR is employed to ensure
that participants feel comfortable interacting with
the system without being distracted by having to
use a push-to-talk microphone. The use of con-
tinuous ASR necessitates the development of spe-
cific policies for turn-taking (see Section 2.2). In
this demonstration, voice activity detection and
speech recognition are performed using Pocket-
Sphinx (Huggins-Daines et al., 2006).
Because of the open-ended participants’ re-
sponses, for NLU, we cannot simply construct a
small semantic ontology and expect to cover the
majority of meanings that will be expressed by
users. Thus, this is an application in which the
dialogue policy needs to be able to create a sense
of engagement, continuation, and empathy despite
relatively shallow and limited understanding of
user speech. SimSensei Kiosk currently uses 4
statistically trained utterance classifiers to capture
different aspects of user utterance meaning.
The first NLU classifier identifies generic di-
alogue act types, including statements, yes-no
questions, wh-questions, yes and no answers, and
several others. This classifier is trained using
the Switchboard DAMSL corpus (Jurafsky et al.,
1997) using a maximum entropy model.
The second NLU classifier assigns positive,
negative, or neutral valence to utterances, in or-
der to guide Ellie’s expression of empathy. We
use SentiWordNet 3.0 (Baccianella et al., 2010), a
lexical sentiment dictionary, to assign valence to
individual words spoken by users (as recognized
by the ASR); the valence assigned to an utterance
is based primarily on the mean valence scores of
</bodyText>
<table confidence="0.98525290625">
Opening Rapport Building Phase
Ellie What are some things you really like about LA?
User (top level question)
I love the weather, I love the palm trees, I love the
beaches, there’s a lot to do here.
Diagnostic Phase
Ellie Have you noticed any changes in your behavior or
User thoughts lately? (top level question)
Ellie Yes.
User Can you tell me about that? (continuation prompt)
Ellie I’m having a lot more nightmares now uh can’t
Ellie sleep have haven’t really been eating uh trying to
User eat... I have to force down food um just feeling
Ellie like an emotional wreck.
User I’m sorry to hear that. (empathy response)
Ellie What are you like when you don’t sleep well?
(follow-up question)
Irritable, emotional, it just adds to my overall
stress um [long pause]
What... (Ellie speaks after the participant’s long
pause)
Can’t concentrate uh I uh... (the participant starts
speaking while Ellie is speaking)
I’m sorry please continue. (Ellie realizes that she
has interrupted the participant and apologizes)
Positive Closing Phase
Ellie How would your best friend describe you? (top
User level question)
As caring, as fun because most of the time when
I’m around my best friends I’m happy and I’m fun
loving. I joke around with them a lot and uh I do
better when I’m around my friends.. .
</table>
<figureCaption confidence="0.99948">
Figure 2: Examples of Ellie’s interview phases.
</figureCaption>
<bodyText confidence="0.998878066666667">
the individual words in the utterance.
The third NLU classifier supports domain-
specific small talk by recognizing a handful of
specific anticipated responses to Ellie’s rapport-
building questions. For example, when Ellie asks
users where they are from, this classifier detects
the names of commonly mentioned cities and re-
gions using keyphrase spotting.
The fourth NLU classifier identifies domain-
specific dialogue acts, and supports Ellie’s follow-
up responses to specific questions, such as “how
close are you to your family?”. This maximum
entropy classifier is trained using face-to-face and
Wizard-of-Oz data to detect specific responses
such as assertions of closeness.
</bodyText>
<subsectionHeader confidence="0.991633">
2.2 Dialogue Management
</subsectionHeader>
<bodyText confidence="0.999971333333333">
Ellie currently uses about 100 fixed utterances in
total in the automated system. She employs 60 top
level interview questions (e.g., “do you travel a
</bodyText>
<page confidence="0.994958">
255
</page>
<bodyText confidence="0.999984395348837">
lot?”), plus some follow-up questions (e.g., “what
do you enjoy about traveling?”) and a range of
backchannels (e.g., “uh huh”), empathy responses
(e.g., “that’s great”, “I’m sorry”), and continua-
tion prompts (e.g., “tell me more about that”).
The dialogue policy is implemented using the
FLoReS dialogue manager (Morbini et al., 2012).
The policy groups interview questions into three
phases (opening rapport building, diagnostic, pos-
itive closing – ensuring that the participant leaves
with positive feelings). Questions are generally
asked in a fixed order, with some branching based
on responses to specific questions.
Rule-based subpolicies determine what Ellie’s
follow-up responses will be for each of her top-
level interview questions. The rules for follow-ups
are defined in relation to the four NLU classifiers
and the duration of user speech (measured in sec-
onds). These rules trigger continuation prompts
and empathy responses under specific conditions.
The turn-taking policy supports our design goal
to encourage users to openly share information
and to speak at length in response to Ellie’s open-
ended questions. In this domain, users often pause
before or during their responses to think about
their answers to Ellie’s personal questions. The
turn-taking policy is designed to provide ample
time for users to consider their responses, and to
let users take and keep the initiative as much as
possible. Ellie’s turn-taking decisions are based
on thresholds for user pause duration, i.e., how
much time the system should wait after the user
has stopped speaking before Ellie starts speaking.
These thresholds are tuned to the face-to-face and
Wizard-of-Oz data to minimize Ellie’s interrup-
tion rate, and are extended dynamically when El-
lie detects that she has interrupted the participant.
This is to take into account the fact that some peo-
ple tend to use longer pauses than others.
Examples of the three interview phases and of
Ellie’s subdialogue policies (top level and follow-
up questions, continuation prompts, empathy re-
sponses, and turn-taking) are given in Figure 2.
</bodyText>
<sectionHeader confidence="0.996888" genericHeader="method">
3 Demonstration Summary
</sectionHeader>
<bodyText confidence="0.999979454545455">
The demonstration will feature a live interac-
tion between Ellie and a participant, showing El-
lie’s real-time understanding and consequent pol-
icy actions. Live dialogues will highlight Ellie’s
strategies for questioning, follow-up continuation
prompts, displays of empathy, and turn-taking,
similar to the example in Figure 2. The demon-
stration will illustrate how these elements work to-
gether to enable Ellie to carry out extended inter-
views that also provide information relevant to the
automatic assessment of indicators of distress.
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999725">
The effort described here is supported by DARPA
under contract W911NF-04-D-0005 and the U.S.
Army. Any opinion, content or information pre-
sented does not necessarily reflect the position or
the policy of the United States Government, and
no official endorsement should be inferred.
</bodyText>
<sectionHeader confidence="0.99941" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999331736842106">
S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Sen-
tiWordNet 3.0: An enhanced lexical resource for
sentiment analysis and opinion mining. In Proceed-
ings of LREC.
D. DeVault, K. Georgila, R. Artstein, F. Morbini, D.
Traum, S. Scherer, A. Rizzo, and L.-P. Morency.
2013. Verbal indicators of psychological distress in
interactive dialogue with a virtual human. In Pro-
ceedings of SIGDIAL.
D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast,
A. Gainer, K. Georgila, J. Gratch, A. Hartholt, M.
Lhommet, G. Lucas, S. Marsella, F. Morbini, A.
Nazarian, S. Scherer, G. Stratou, A. Suri, D. Traum,
R. Wood, Y. Xu, A. Rizzo, and L.-P. Morency. 2014.
SimSensei Kiosk: A virtual human interviewer for
healthcare decision support. In Proceedings of AA-
MAS.
A. Hartholt, D. Traum, S. Marsella, A. Shapiro, G.
Stratou, A. Leuski, L.-P. Morency, and J. Gratch.
2013. All together now, introducing the virtual hu-
man toolkit. In Proceedings of IVA.
D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black,
M. Ravishankar, and A.I. Rudnicky. 2006. Pocket-
Sphinx: A free, real-time continuous speech recog-
nition system for hand-held devices. In Proceedings
of ICASSP.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL Shallow-Discourse-Function
Annotation Coders Manual, Draft 13.
F. Morbini, D. DeVault, K. Sagae, J. Gerten, A. Nazar-
ian, and D. Traum. 2012. FLoReS: A forward look-
ing reward seeking dialogue manager. In Proceed-
ings of IWSDS.
S. Scherer, G. Stratou, M. Mahmoud, J. Boberg,
J. Gratch, A. Rizzo, and L.-P. Morency. 2013. Au-
tomatic behavior descriptors for psychological dis-
order analysis. In Proceedings of IEEE Conference
on Automatic Face and Gesture Recognition.
</reference>
<page confidence="0.998404">
256
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.904764">
<title confidence="0.999412">A Demonstration of Dialogue Processing in SimSensei Kiosk</title>
<author confidence="0.992253">Fabrizio Morbini</author>
<author confidence="0.992253">David DeVault</author>
<author confidence="0.992253">Kallirroi</author>
<affiliation confidence="0.999797">USC Institute for Creative</affiliation>
<address confidence="0.99902">12015 Waterfront Dr., Playa Vista, CA</address>
<abstract confidence="0.99530347368421">This demonstration highlights the dialogue processing in SimSensei Kiosk, a virtual human dialogue system that conducts interviews related to psychological distress conditions such as depression, anxiety, and post-traumatic stress disorder (PTSD). The dialogue processing in Sim- Sensei Kiosk allows the system to conduct coherent spoken interviews of human users that are 15-25 minutes in length, and in which users feel comfortable talking and openly sharing information. We present the design of the individual dialogue components, and show examples of natural conversation flow between the system and users, including expressions of empathy, follow-up responses and continuation prompts, and turn-taking.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Baccianella</author>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="5514" citStr="Baccianella et al., 2010" startWordPosition="812" endWordPosition="815">understanding of user speech. SimSensei Kiosk currently uses 4 statistically trained utterance classifiers to capture different aspects of user utterance meaning. The first NLU classifier identifies generic dialogue act types, including statements, yes-no questions, wh-questions, yes and no answers, and several others. This classifier is trained using the Switchboard DAMSL corpus (Jurafsky et al., 1997) using a maximum entropy model. The second NLU classifier assigns positive, negative, or neutral valence to utterances, in order to guide Ellie’s expression of empathy. We use SentiWordNet 3.0 (Baccianella et al., 2010), a lexical sentiment dictionary, to assign valence to individual words spoken by users (as recognized by the ASR); the valence assigned to an utterance is based primarily on the mean valence scores of Opening Rapport Building Phase Ellie What are some things you really like about LA? User (top level question) I love the weather, I love the palm trees, I love the beaches, there’s a lot to do here. Diagnostic Phase Ellie Have you noticed any changes in your behavior or User thoughts lately? (top level question) Ellie Yes. User Can you tell me about that? (continuation prompt) Ellie I’m having a</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>S. Baccianella, A. Esuli, and F. Sebastiani. 2010. SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D DeVault</author>
<author>K Georgila</author>
<author>R Artstein</author>
<author>F Morbini</author>
<author>D Traum</author>
<author>S Scherer</author>
<author>A Rizzo</author>
<author>L-P Morency</author>
</authors>
<title>Verbal indicators of psychological distress in interactive dialogue with a virtual human.</title>
<date>2013</date>
<booktitle>In Proceedings of SIGDIAL.</booktitle>
<contexts>
<context position="1821" citStr="DeVault et al., 2013" startWordPosition="258" endWordPosition="261">ss conditions such as depression, anxiety, and post-traumatic stress disorder (PTSD) (DeVault et al., 2014). SimSensei Kiosk has two main functions – a virtual human called Ellie (pictured in Figure 1), who converses with a user in a spoken, semi-structured interview, and a multimodal perception system which analyzes the user’s behavior in real time to identify indicators of psychological distress. The system has been designed and developed over two years using a series of face-toface, Wizard-of-Oz, and automated system studies involving more than 350 human participants (Scherer et al., 2013; DeVault et al., 2013; DeVault et al., 2014). Agent design has been guided by two overarching goals: (1) the agent should make Figure 1: Ellie, the virtual human interviewer in SimSensei Kiosk. the user feel comfortable talking and openly sharing information, and at the same time (2) the agent should create interactional situations that support the automatic assessment of verbal and nonverbal behaviors correlated with psychological distress. During an interview, the agent presents a set of questions which have been shown in user testing to support these goals. Since the main interview questions and their order are</context>
</contexts>
<marker>DeVault, Georgila, Artstein, Morbini, Traum, Scherer, Rizzo, Morency, 2013</marker>
<rawString>D. DeVault, K. Georgila, R. Artstein, F. Morbini, D. Traum, S. Scherer, A. Rizzo, and L.-P. Morency. 2013. Verbal indicators of psychological distress in interactive dialogue with a virtual human. In Proceedings of SIGDIAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D DeVault</author>
<author>R Artstein</author>
<author>G Benn</author>
<author>T Dey</author>
<author>E Fast</author>
<author>A Gainer</author>
<author>K Georgila</author>
<author>J Gratch</author>
<author>A Hartholt</author>
<author>M Lhommet</author>
<author>G Lucas</author>
<author>S Marsella</author>
<author>F Morbini</author>
<author>A Nazarian</author>
<author>S Scherer</author>
<author>G Stratou</author>
<author>A Suri</author>
<author>D Traum</author>
<author>R Wood</author>
<author>Y Xu</author>
<author>A Rizzo</author>
<author>L-P Morency</author>
</authors>
<title>SimSensei Kiosk: A virtual human interviewer for healthcare decision support.</title>
<date>2014</date>
<booktitle>In Proceedings of AAMAS.</booktitle>
<contexts>
<context position="1308" citStr="DeVault et al., 2014" startWordPosition="176" endWordPosition="179"> 15-25 minutes in length, and in which users feel comfortable talking and openly sharing information. We present the design of the individual dialogue components, and show examples of natural conversation flow between the system and users, including expressions of empathy, follow-up responses and continuation prompts, and turn-taking. 1 Introduction This demonstration highlights the dialogue processing in SimSensei Kiosk, a virtual human dialogue system that conducts interviews related to psychological distress conditions such as depression, anxiety, and post-traumatic stress disorder (PTSD) (DeVault et al., 2014). SimSensei Kiosk has two main functions – a virtual human called Ellie (pictured in Figure 1), who converses with a user in a spoken, semi-structured interview, and a multimodal perception system which analyzes the user’s behavior in real time to identify indicators of psychological distress. The system has been designed and developed over two years using a series of face-toface, Wizard-of-Oz, and automated system studies involving more than 350 human participants (Scherer et al., 2013; DeVault et al., 2013; DeVault et al., 2014). Agent design has been guided by two overarching goals: (1) the</context>
<context position="3556" citStr="DeVault et al. (2014)" startWordPosition="520" endWordPosition="523">anguage and speech are executed by selecting from pre-recorded audio clips. Additional agent modules include nonverbal behavior generation, which matches appropriately timed body movements to the agent’s speech; character animation in a virtual 3D environment; and rendering in a game en254 Proceedings of the SIGDIAL 2014 Conference, pages 254–256, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics gine. The perception system analyzes audio and video in real time to identify features such as head position, gaze direction, smile intensity, and voice quality. DeVault et al. (2014) provides details on all the agent’s modules. 2 Overview of Dialogue Processing 2.1 ASR and NLU components Unlike many task-oriented dialogue domains, interview dialogues between SimSensei Kiosk and participants are naturally open-ended. People tend to respond to interview stimuli such as “what’s one of your most memorable experiences?” with idiosyncratic stories and events from their lives. The variability in the vocabulary and content of participants’ answers to such questions is so large that it makes the ASR task very challenging. Furthermore, continuous ASR is employed to ensure that part</context>
</contexts>
<marker>DeVault, Artstein, Benn, Dey, Fast, Gainer, Georgila, Gratch, Hartholt, Lhommet, Lucas, Marsella, Morbini, Nazarian, Scherer, Stratou, Suri, Traum, Wood, Xu, Rizzo, Morency, 2014</marker>
<rawString>D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast, A. Gainer, K. Georgila, J. Gratch, A. Hartholt, M. Lhommet, G. Lucas, S. Marsella, F. Morbini, A. Nazarian, S. Scherer, G. Stratou, A. Suri, D. Traum, R. Wood, Y. Xu, A. Rizzo, and L.-P. Morency. 2014. SimSensei Kiosk: A virtual human interviewer for healthcare decision support. In Proceedings of AAMAS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hartholt</author>
<author>D Traum</author>
<author>S Marsella</author>
<author>A Shapiro</author>
<author>G Stratou</author>
<author>A Leuski</author>
<author>L-P Morency</author>
<author>J Gratch</author>
</authors>
<title>All together now, introducing the virtual human toolkit.</title>
<date>2013</date>
<booktitle>In Proceedings of IVA.</booktitle>
<contexts>
<context position="2734" citStr="Hartholt et al., 2013" startWordPosition="400" endWordPosition="403">eractional situations that support the automatic assessment of verbal and nonverbal behaviors correlated with psychological distress. During an interview, the agent presents a set of questions which have been shown in user testing to support these goals. Since the main interview questions and their order are mostly fixed, dialogue management concentrates on providing appropriate verbal feedback behaviors to keep the user engaged, maintain a natural and comfortable conversation flow, and elicit continuations and elaborations from the user. The agent is implemented using a modular architecture (Hartholt et al., 2013). Dialogue processing, which is the focus of this demonstration, is supported by individual modules for speech recognition, language understanding and dialogue management (see Section 2). The agent’s language and speech are executed by selecting from pre-recorded audio clips. Additional agent modules include nonverbal behavior generation, which matches appropriately timed body movements to the agent’s speech; character animation in a virtual 3D environment; and rendering in a game en254 Proceedings of the SIGDIAL 2014 Conference, pages 254–256, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Ass</context>
</contexts>
<marker>Hartholt, Traum, Marsella, Shapiro, Stratou, Leuski, Morency, Gratch, 2013</marker>
<rawString>A. Hartholt, D. Traum, S. Marsella, A. Shapiro, G. Stratou, A. Leuski, L.-P. Morency, and J. Gratch. 2013. All together now, introducing the virtual human toolkit. In Proceedings of IVA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Huggins-Daines</author>
<author>M Kumar</author>
<author>A Chan</author>
<author>A W Black</author>
<author>M Ravishankar</author>
<author>A I Rudnicky</author>
</authors>
<title>PocketSphinx: A free, real-time continuous speech recognition system for hand-held devices.</title>
<date>2006</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<contexts>
<context position="4523" citStr="Huggins-Daines et al., 2006" startWordPosition="663" endWordPosition="666">with idiosyncratic stories and events from their lives. The variability in the vocabulary and content of participants’ answers to such questions is so large that it makes the ASR task very challenging. Furthermore, continuous ASR is employed to ensure that participants feel comfortable interacting with the system without being distracted by having to use a push-to-talk microphone. The use of continuous ASR necessitates the development of specific policies for turn-taking (see Section 2.2). In this demonstration, voice activity detection and speech recognition are performed using PocketSphinx (Huggins-Daines et al., 2006). Because of the open-ended participants’ responses, for NLU, we cannot simply construct a small semantic ontology and expect to cover the majority of meanings that will be expressed by users. Thus, this is an application in which the dialogue policy needs to be able to create a sense of engagement, continuation, and empathy despite relatively shallow and limited understanding of user speech. SimSensei Kiosk currently uses 4 statistically trained utterance classifiers to capture different aspects of user utterance meaning. The first NLU classifier identifies generic dialogue act types, includi</context>
</contexts>
<marker>Huggins-Daines, Kumar, Chan, Black, Ravishankar, Rudnicky, 2006</marker>
<rawString>D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black, M. Ravishankar, and A.I. Rudnicky. 2006. PocketSphinx: A free, real-time continuous speech recognition system for hand-held devices. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>E Shriberg</author>
<author>D Biasca</author>
</authors>
<date>1997</date>
<journal>Switchboard SWBD-DAMSL Shallow-Discourse-Function Annotation Coders Manual, Draft</journal>
<volume>13</volume>
<contexts>
<context position="5295" citStr="Jurafsky et al., 1997" startWordPosition="778" endWordPosition="781"> meanings that will be expressed by users. Thus, this is an application in which the dialogue policy needs to be able to create a sense of engagement, continuation, and empathy despite relatively shallow and limited understanding of user speech. SimSensei Kiosk currently uses 4 statistically trained utterance classifiers to capture different aspects of user utterance meaning. The first NLU classifier identifies generic dialogue act types, including statements, yes-no questions, wh-questions, yes and no answers, and several others. This classifier is trained using the Switchboard DAMSL corpus (Jurafsky et al., 1997) using a maximum entropy model. The second NLU classifier assigns positive, negative, or neutral valence to utterances, in order to guide Ellie’s expression of empathy. We use SentiWordNet 3.0 (Baccianella et al., 2010), a lexical sentiment dictionary, to assign valence to individual words spoken by users (as recognized by the ASR); the valence assigned to an utterance is based primarily on the mean valence scores of Opening Rapport Building Phase Ellie What are some things you really like about LA? User (top level question) I love the weather, I love the palm trees, I love the beaches, there’</context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switchboard SWBD-DAMSL Shallow-Discourse-Function Annotation Coders Manual, Draft 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Morbini</author>
<author>D DeVault</author>
<author>K Sagae</author>
<author>J Gerten</author>
<author>A Nazarian</author>
<author>D Traum</author>
</authors>
<title>FLoReS: A forward looking reward seeking dialogue manager.</title>
<date>2012</date>
<booktitle>In Proceedings of IWSDS.</booktitle>
<contexts>
<context position="8244" citStr="Morbini et al., 2012" startWordPosition="1255" endWordPosition="1258">ned using face-to-face and Wizard-of-Oz data to detect specific responses such as assertions of closeness. 2.2 Dialogue Management Ellie currently uses about 100 fixed utterances in total in the automated system. She employs 60 top level interview questions (e.g., “do you travel a 255 lot?”), plus some follow-up questions (e.g., “what do you enjoy about traveling?”) and a range of backchannels (e.g., “uh huh”), empathy responses (e.g., “that’s great”, “I’m sorry”), and continuation prompts (e.g., “tell me more about that”). The dialogue policy is implemented using the FLoReS dialogue manager (Morbini et al., 2012). The policy groups interview questions into three phases (opening rapport building, diagnostic, positive closing – ensuring that the participant leaves with positive feelings). Questions are generally asked in a fixed order, with some branching based on responses to specific questions. Rule-based subpolicies determine what Ellie’s follow-up responses will be for each of her toplevel interview questions. The rules for follow-ups are defined in relation to the four NLU classifiers and the duration of user speech (measured in seconds). These rules trigger continuation prompts and empathy respons</context>
</contexts>
<marker>Morbini, DeVault, Sagae, Gerten, Nazarian, Traum, 2012</marker>
<rawString>F. Morbini, D. DeVault, K. Sagae, J. Gerten, A. Nazarian, and D. Traum. 2012. FLoReS: A forward looking reward seeking dialogue manager. In Proceedings of IWSDS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Scherer</author>
<author>G Stratou</author>
<author>M Mahmoud</author>
<author>J Boberg</author>
<author>J Gratch</author>
<author>A Rizzo</author>
<author>L-P Morency</author>
</authors>
<title>Automatic behavior descriptors for psychological disorder analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of IEEE Conference on Automatic Face and Gesture Recognition.</booktitle>
<contexts>
<context position="1799" citStr="Scherer et al., 2013" startWordPosition="254" endWordPosition="257">o psychological distress conditions such as depression, anxiety, and post-traumatic stress disorder (PTSD) (DeVault et al., 2014). SimSensei Kiosk has two main functions – a virtual human called Ellie (pictured in Figure 1), who converses with a user in a spoken, semi-structured interview, and a multimodal perception system which analyzes the user’s behavior in real time to identify indicators of psychological distress. The system has been designed and developed over two years using a series of face-toface, Wizard-of-Oz, and automated system studies involving more than 350 human participants (Scherer et al., 2013; DeVault et al., 2013; DeVault et al., 2014). Agent design has been guided by two overarching goals: (1) the agent should make Figure 1: Ellie, the virtual human interviewer in SimSensei Kiosk. the user feel comfortable talking and openly sharing information, and at the same time (2) the agent should create interactional situations that support the automatic assessment of verbal and nonverbal behaviors correlated with psychological distress. During an interview, the agent presents a set of questions which have been shown in user testing to support these goals. Since the main interview questio</context>
</contexts>
<marker>Scherer, Stratou, Mahmoud, Boberg, Gratch, Rizzo, Morency, 2013</marker>
<rawString>S. Scherer, G. Stratou, M. Mahmoud, J. Boberg, J. Gratch, A. Rizzo, and L.-P. Morency. 2013. Automatic behavior descriptors for psychological disorder analysis. In Proceedings of IEEE Conference on Automatic Face and Gesture Recognition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000088">
<title confidence="0.909144">
Syntax and Semantics in Quality Estimation of Machine Translation
</title>
<author confidence="0.899154">
Rasoul Kaljahi†‡, Jennifer Foster†, Johann Roturier‡†NCLT, School of Computing, Dublin City University, Ireland
</author>
<email confidence="0.804031">
{rkaljahi, jfoster}@computing.dcu.ie
</email>
<author confidence="0.373323">
‡Symantec Research Labs, Dublin, Ireland
</author>
<email confidence="0.987431">
{johann roturier}@symantec.com
</email>
<sectionHeader confidence="0.993328" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99990952">
We employ syntactic and semantic infor-
mation in estimating the quality of ma-
chine translation from a new data set
which contains source text from English
customer support forums and target text
consisting of its machine translation into
French. These translations have been both
post-edited and evaluated by professional
translators. We find that quality estima-
tion using syntactic and semantic informa-
tion on this data set can hardly improve
over a baseline which uses only surface
features. However, the performance can
be improved when they are combined with
such surface features. We also introduce
a novel metric to measure translation ade-
quacy based on predicate-argument struc-
ture match using word alignments. While
word alignments can be reliably used,
the two main factors affecting the per-
formance of all semantic-based methods
seems to be the low quality of seman-
tic role labelling (especially on ill-formed
text) and the lack of nominal predicate an-
notation.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970651162791">
The problem of evaluating machine translation
output without reference translations is called
quality estimation (QE) and has recently been the
centre of attention (Bojar et al., 2014) following
the seminal work of Blatz et al. (2003). Most
QE studies have focused on surface and language-
model-based features of the source and target. The
quality of translation is however closely related to
the syntax and semantics of the languages, the for-
mer concerning fluency and the latter adequacy.
While there have been some attempts to utilize
syntax in this task, semantics has been paid less
attention. In this work, we aim to exploit both
syntax and semantics in QE, with a particular fo-
cus on the latter. We use shallow semantic analy-
sis obtained via semantic role labelling (SRL) and
employ this information in QE in various ways in-
cluding statistical learning using both tree kernels
and hand-crafted features. We also design a QE
metric which is based on the Predicate-Argument
structure Match (PAM) between the source and its
translation. The semantic-based system is then
combined with the syntax-based system to evalu-
ate the full power of structural linguistic informa-
tion. We also combine this system with a baseline
system consisting of effective surface features.
A second contribution of the paper is the release
of a new data set for QE.1 This data set comprises
a set of 4.5K sentences chosen from customer sup-
port forum text. The machine translation of the
sentences are not only evaluated in terms of ade-
quacy and fluency, but also manually post-edited
allowing various metrics of interest to be applied
to measure different aspects of quality. All exper-
iments are carried out on this data set.
The rest of the paper is organized as follows:
after reviewing the related work, the data is de-
scribed and the semantic role labelling approach
is explained. The baseline is then introduced, fol-
lowed by the experiments with tree kernels, hand-
crafted features, the PAM metric and finally the
combination of all methods. The paper ends with
a summary and suggestions for future work.
</bodyText>
<sectionHeader confidence="0.999852" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.996276166666667">
Syntax has been exploited in QE in various ways
including tree kernels (Hardmeier et al., 2012;
Kaljahi et al., 2013; Kaljahi et al., 2014b),
parse probabilities and syntactic label frequency
(Avramidis, 2012), parseability (Quirk, 2004) and
POS n-gram scores (Specia and Gim´enez, 2010).
</bodyText>
<footnote confidence="0.997193">
1The data will be made publicly available -see http://
www.computing.dcu.ie/mt/confidentmt.html
</footnote>
<page confidence="0.987732">
67
</page>
<note confidence="0.783002">
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67–77,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999918064935065">
Turning to the role of semantic knowledge in
QE and MT evaluation in general, Pighin and
M`arquez (2011) propose a method for ranking two
translation hypotheses that exploits the projection
of SRL from a sentence to its translation using
word alignments. They first project the SRL of a
source corpus to its parallel corpus and then build
two translation models: 1) translations of proposi-
tion labelling sequences in the source to its projec-
tion in the target and 2) translations of argument
role fillers in the source to their counterparts in
the target. The source SRL is then projected to
its machine translation and the above models are
forced to translate source proposition labelling se-
quences to the projected ones. Finally the confi-
dence scores of these translations and their reach-
ability are used to train a classifier which selects
the better of the two translation hypotheses with
an accuracy of 64%. Factors hindering their clas-
sifier are word alignment limitations and low SRL
recall due to the lack of a verb or the loss of a
predicate during translation.
In MT evaluation, where reference translations
are available, Gim´enez and M`arquez (2007) use
semantic roles in building several MT evaluation
metrics which measure the full or partial lexical
match between the fillers of same semantic roles in
the hypothesis and translation, or simply the role
label matches between them. They conclude that
these features can only be useful in combination
with other features and metrics reflecting different
aspects of the quality.
Lo and Wu (2011) introduce HMEANT, a man-
ual MT evaluation metric based on predicate-
argument structure matching which involves two
steps of human engagement: 1) semantic role an-
notation of the reference and machine translation,
2) evaluating the translation of predicates and ar-
guments. The metric calculates the F1 score of
the semantic frame match between the reference
and machine translation based on this evaluation.
To keep the costs reasonable, the first step is car-
ried out by amateur annotators who were mini-
mally trained with a simplified list of 10 thematic
roles. On a set of 40 examples, the metric is
meta-evaluated in terms of correlation with human
judgements of translation adequacy ranking, and a
correlation as high as that of HTER is reported.
Lo et al. (2012) propose MEANT, a variant of
HMEANT, which automatizes its manual steps
using 1) automatic SRL systems for (only) verb
predicates, 2) automatic alignment of predicates
and their arguments in the reference and ma-
chine translation based on their lexical similarity.
Once the predicates and arguments are aligned,
their similarities are measured using a variety of
methods such as cosine distance and even Me-
teor and BLEU. In computation of the final score,
the similarity scores replace the counts of correct
and partial translations used in HMEANT. This
metric outperforms several automatic metrics in-
cluding BLEU, Meteor and TER, but it signifi-
cantly under-performs HMEANT and HTER. Fur-
ther analysis shows that automatizing the second
step does not affect the performance of MEANT.
Therefore, it seems to be the lower accuracy of the
semantic role labelling that is responsible.
Bojar and Wu (2012) identify a set of flaws
with HMEANT and propose solutions for them.
The most important problems stem from the su-
perficial SRL annotation guidelines. These prob-
lems are exacerbated in MEANT due to the auto-
matic nature of the two steps. More recently, Lo
et al. (2014) extend MEANT to ranking transla-
tions without a reference by using phrase transla-
tion probabilities for aligning semantic role fillers
of the source and its translation.
</bodyText>
<sectionHeader confidence="0.997159" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.9997295">
We randomly select 4500 segments from a large
collection of Symantec English Norton forum
text.2 In order to be independent of any one MT
system, we translate these segments into French
with the following three systems and randomly
choose 1500 distinct segments from each.
</bodyText>
<listItem confidence="0.999877857142857">
• ACCEPT3: a phrase-based Moses system
trained on training sets of WMT12 releases
of Europarl and News Commentary plus
Symantec translation memories
• SYSTRAN: a proprietary rule-based system
augmented with domain-specific dictionaries
• Bing4: an online translation system
</listItem>
<bodyText confidence="0.991847666666667">
These translations are evaluated in two ways.
The first method involves light post-editing by
a professional human translator who is a native
</bodyText>
<footnote confidence="0.9981822">
2http://community.norton.com
3http://www.accept.unige.ch/Products/
D_4_1_Baseline_MT_systems.pdf
4http://www.bing.com/translator(on24-
Feb-2014)
</footnote>
<page confidence="0.998784">
68
</page>
<figure confidence="0.955189086956522">
5
4
3
2
1
All meaning
Most of meaning
Much of meaning
Little meaning
None of meaning
Fluency
Flawless Language
Good Language
Non-native Language
Disfluent Language
Incomprehensible
1-HTER HBLEU HMeteor Adq Flu
1-HTER - - - - -
HBLEU 0.9111 - - - -
HMeteor 0.9207 0.9314 - - -
Adq 0.6632 0.7049 0.6843 - -
Flu 0.6447 0.7213 0.6652 0.8824 -
Adequacy
</figure>
<tableCaption confidence="0.984153">
Table 2: Adequacy/fluency score interpretation
</tableCaption>
<bodyText confidence="0.999222787878788">
French speaker.5 Each sentence translation is then
scored against its post-edit using BLEU6(Papineni
et al., 2002), TER (Snover et al., 2006) and
METEOR (Denkowski and Lavie, 2011), which are
the most widely used MT evaluation metrics. Fol-
lowing Snover et al. (2006), we consider this way
of scoring MT output to be a variation of human-
targeted scoring, where no reference translation
is provided to the post-editor, so we call them
HBLEU, HTER and HMETEOR. The average scores
for the entire data set together with their standard
deviations are presented in Table 1.7
In the second method, we asked three profes-
sional translators, who are again native French
speakers, to assess the quality of MT output in
terms of adequacy and fluency in a 5-grade scale
(LDC, 2002). The interpretation of the scores is
given in Table 2. Each evaluator was given the
entire data set for evaluation. We therefore col-
lected three sets of scores and averaged them to
obtain the final scores. The averages of these
scores for the entire data set together with their
standard deviations are presented in Table 1. To
be easily comparable to human-targeted scores,
we scale these scores to the [0,1] range, i.e. ad-
equacy/fluency scores of 1 and 5 are mapped to 0
and 1 respectively and all the scores in between
are accordingly scaled.
The average Kappa inter-annotator agreement
for adequacy scores is 0.25 and for fluency scores
0.19. However, this measurement does not dif-
ferentiate between small and large differences in
agreement. In other words, the difference between
</bodyText>
<footnote confidence="0.567921153846154">
5The post-editing guidelines are based on the
TAUS/CNGL guidelines for achieving “good enough”
quality downloaded from https://evaluation.
taus.net/images/stories/guidelines/taus-
cngl-machine-translation-postediting-
guidelines.pdf.
6Version 13a of MTEval script was used at the segment
level which performs smoothing.
7Note that HTER scores have no upper limit and can be
higher than 1 when the number of errors is higher than the
segment length. In addition, the higher HTER indicates lower
translation quality. To be comparable to the other scores, we
cut-off them at 1 and convert to 1-HTER.
</footnote>
<tableCaption confidence="0.92064">
Table 3: Pearson r between pairs of metrics on the
entire 4.5K data set
</tableCaption>
<bodyText confidence="0.999456466666667">
scores of 5 and 4 is the same as the difference
between 5 and 2. To account for this, we use
weighted Kappa instead. Specifically, we consider
two scores of difference 1 to represent 75% agree-
ment instead of 100%. All the other differences
are considered to be a disagreement. The aver-
age weighted Kappa computed in this way is 0.65
for adequacy and 0.63 for fluency. Though the
weighting used is quite strict, the weighted Kappa
values are in the substantial agreement range.
Once we have both human-targeted and manual
evaluation scores together, it is interesting to know
how they are correlated. We calculate the Pearson
correlation coefficient r between each pair of the
five scores and present them in Table 3. HBLEU
has the highest correlation with both adequacy and
fluency scores among the human-targeted metrics.
HTER on the other hand has the lowest correla-
tion. Moreover, HBLEU is more correlated with
fluency than with adequacy which is the opposite
to HMeteor. This is expected according to the
definition of BLEU and Meteor. There is also
a high correlation between adequacy and fluency
scores. Although this could be related to the fact
that both scores are from the same evaluators, it
indicates that if either the fluency and adequacy of
the MT output is low or high, the other tends to be
the same.
The data is split into train, development and test
sets of 3000, 500 and 1000 sentences respectively.
</bodyText>
<sectionHeader confidence="0.973483" genericHeader="method">
4 Semantic Role Labelling
</sectionHeader>
<bodyText confidence="0.9999652">
The type of semantic information we use in this
work is the predicate-argument structure or se-
mantic role labelling of the sentence. This infor-
mation needs to be extracted from both sides of the
translation, i.e. English and French. Though the
SRL of English has been well-studied (M`arquez
et al., 2008) thanks to the existence of two major
hand-crafted resources, namely FrameNet (Baker
et al., 1998) and PropBank (Palmer et al., 2005),
French is one of the under-studied languages in
</bodyText>
<page confidence="0.997915">
69
</page>
<table confidence="0.954711">
1-HTER HBLEU HMeteor Adequacy Fluency
Average 0.6976 0.5517 0.7221 0.6230 0.4096
Standard Deviation 0.2446 0.2927 0.2129 0.2488 0.2780
</table>
<tableCaption confidence="0.999758">
Table 1: Average and standard deviation of the evaluation scores for the entire data set
</tableCaption>
<bodyText confidence="0.999658720930233">
this respect mainly due to a lack of such resources.
The only available gold standard resource is a
small set of 1000 sentences taken from Europarl
(Koehn, 2005) and manually annotated with Prop-
bank verb predicates (van der Plas et al., 2010).
van der Plas et al. (2011) attempt to tackle this
scarcity by automatically projecting SRL from the
English side of a large parallel corpus to its French
side. Our preliminary experiments (Kaljahi et al.,
2014a), however, show that SRL models trained
on the small manually annotated corpus have a
higher quality than ones trained on the much larger
projected corpus. We therefore use the 1K gold
standard set to train a French SRL model. For En-
glish, we use all the data provided in the CoNLL
2009 shared task (Hajiˇc et al., 2009).
We use LTH (Bj¨orkelund et al., 2009), a
dependency-based SRL system, for both the En-
glish and French data. This system was among
the best performing systems in the CoNLL 2009
shared task and is straightforward to use. It comes
with a set of features tuned for each shared task
language (English, German, Japanese, Spanish,
Catalan, Czech, Chinese). We compared the per-
formance of the English and Spanish feature sets
on French and chose the former due to its higher
performance (by 1 F1 point).
It should be noted that the English SRL data
come with gold standard syntactic annotation. On
the other hand, for our QE data set, such anno-
tation is not available. Our preliminary experi-
ments show that, since the SRL system heavily
relies on syntactic features, the performance con-
siderably drops when the syntactic annotation of
the test data is obtained using a different parser
than that of the training data. We therefore re-
place the parses of the training data with those ob-
tained automatically by first parsing the data us-
ing the Lorg PCFG-LA parser8 (Attia et al., 2010)
and then converting them to dependencies using
Stanford converter (de Marneffe and Manning,
2008). The POS tags are also replaced with those
output by the parser. For the same reason, we re-
</bodyText>
<footnote confidence="0.563399">
8https://github.com/CNGLdlab/LORG-
Release.
</footnote>
<bodyText confidence="0.999873916666667">
place the original POS tagging of the French 1K
data with those obtained by the MElt tagger (De-
nis and Sagot, 2012).
The English SRL achieves 77.77 and 67.02 la-
belled F1 points when trained only on the training
section of PropBank and tested on the WSJ and
Brown test sets respectively.9 The French SRL is
evaluated using 5-fold cross-validation on the 1K
data set and obtains an F1 average of 67.66. When
applied to the QE data set, these models identify
9133, 8875 and 8795 propositions on its source
side, post-edits and MT output respectively.
</bodyText>
<sectionHeader confidence="0.997227" genericHeader="method">
5 Baseline
</sectionHeader>
<bodyText confidence="0.9931312">
We compare the results of our experiments to a
baseline built using the 17 baseline features of the
WMT QE shared task (Bojar et al., 2014). These
features provide a strong baseline and have been
used in all three years of the shared task. We
use support vector regression implemented in the
SVMLight toolkit10 with Radial Basis Function
(RBF) kernel to build this baseline. To extract
these features, a parallel English-French corpus
is required to build a lexical translation table us-
ing GIZA++ (Och and Ney, 2003). We use the
Europarl English-French parallel corpus (Koehn,
2005) plus around 1M segments of Symantec
translation memory.
Table 4 shows the performance of this system
(WMT17) on the test set measured by Root Mean
Square Error (RMSE) and Pearson correlation co-
efficient (r). We only report the results on predict-
ing four of the metrics introduced above, omitting
HMeteor due to space constraints. C and γ pa-
rameters are tuned on the development set with re-
spect to r. The results show a significant differ-
ence between manual and human-targeted metric
prediction. The higher r for the former suggests
that the patterns of these scores are easier to learn.
The RMSE seems to follow the standard deviation
9Although the English SRL data are annotated for noun
predicates as well as verb predicates, since the French data
has only verb predicate annotations, we only consider verb
predicates for English.
</bodyText>
<footnote confidence="0.84283">
10http://svmlight.joachims.org/
</footnote>
<page confidence="0.993854">
70
</page>
<bodyText confidence="0.983931">
of the scores as the same ranking is seen in both.
</bodyText>
<sectionHeader confidence="0.982568" genericHeader="method">
6 Tree Kernels
</sectionHeader>
<bodyText confidence="0.999967422222222">
Tree kernels (Moschitti, 2006) have been success-
fully used in QE by Hardmeier et al. (2012) and
in our previous work (Kaljahi et al., 2013; Kal-
jahi et al., 2014b), where syntactic trees are em-
ployed. Tree kernels eliminate the burden of man-
ual feature engineering by efficiently utilizing all
subtrees of a tree. We employ both syntactic and
semantic information in learning quality scores,
using the SVMLight-TK11, a support vector ma-
chine (SVM) implementation of tree kernels.
We implement a syntactic tree kernel QE sys-
tem with constituency and dependency trees of
the source and target side, following our previous
work (Kaljahi et al., 2013; Kaljahi et al., 2014b).
The performance of this system (TKSyQE) is
shown in Table 4. Unlike our previous results,
where the syntax-based system significantly out-
performed the WMT17 baseline, TKSyQE can only
beat the baseline in HTER and fluency prediction,
with neither difference being statistically signifi-
cant and it is below the baseline for HBLEU and
adequacy prediction.12 It should be noted that in
our previous work, a WMT News data set was
used as the QE data set which, unlike our new data
set, is well-formed and in the same domain as the
parsers’ training data. The discrepancy between
our new and old results suggests that the perfor-
mance is strongly dependent on the data set.
Unlike syntactic parsing, semantic role la-
belling does not produce a tree to be directly used
in the tree kernel framework. There can be var-
ious ways to accomplish this goal. We first try
a method inspired by the PAS format introduced
by Moschitti et al. (2006). In this format, a fixed
number of nodes are gathered under a dummy root
node as slots of one predicate and 6 arguments of
a proposition (one tree per predicate). Each node
dominates an argument label or a dummy label for
the predicate, which in turn dominates the POS
tag of the argument or the predicate lemma. If a
proposition has more than 6 arguments they are
ignored, if it has fewer than 6 arguments, the extra
slots are attached to a dummy null label. Note that
these trees are derived from the dependency-based
SRL of both the source and target side (Figure
</bodyText>
<footnote confidence="0.761448">
11http://disi.unitn.it/moschitti/Tree-
Kernel.htm
12We use paired bootstrap resampling Koehn (2004) for
statistical significance testing.
</footnote>
<table confidence="0.999561470588236">
1-HTER HBLEU Adq Flu
RMSE
WMT17 0.2310 0.2696 0.2219 0.2469
TKSyQE 0.2267 0.2721 0.2258 0.2431
D-PAS 0.2489 0.2856 0.2423 0.2652
D-PST 0.2409 0.2815 0.2383 0.2606
C-PST 0.2400 0.2809 0.2410 0.2615
CD-PST 0.2394 0.2795 0.2373 0.2578
TKSSQE 0.2269 0.2722 0.2253 0.2425
Pearson r
WMT17 0.3661 0.3806 0.4710 0.4769
TKSyQE 0.3693 0.3559 0.4306 0.5013
D-PAS 0.1774 0.1843 0.2770 0.3252
D-PST 0.2136 0.2450 0.3169 0.3670
C-PST 0.2319 0.2541 0.2966 0.3616
CD-PST 0.2311 0.2714 0.3303 0.3923
TKSSQE 0.3682 0.3537 0.4351 0.5046
</table>
<tableCaption confidence="0.987139">
Table 4: RMSE and Pearson r of the 17 base-
</tableCaption>
<bodyText confidence="0.987408451612903">
line features (WMT17) and tree kernel systems;
TKSyQE: syntax-based tree kernels, D-PAS:
dependency-based PAS tree kernels of Moschitti
et al. (2006), D-PST, C-PST and CD-PST:
dependency-based, constituency-based proposi-
tion subtree kernels and their combination,
TKSSQE: syntactic-semantic tree kernels
1(a)). The results are shown in Table 4 (D-PAS).
The performance is statistically significantly lower
than the baseline.13
In order to encode more information in the trees,
we propose another format in which proposition
subtrees (PST) of the sentence are gathered un-
der a dummy root node. A dependency PST (Fig-
ure 1(b)) is formed by the predicate label under
the root dominating its lemma and all its argu-
ments roles. Each of these nodes in turn dominates
three nodes: the argument word form (the predi-
cate word form for the case of a predicate lemma),
its syntactic dependency relation to its head and its
POS tag. We preserve the order of arguments and
predicate in the sentence.14 This system is named
D-PST in Table 4. Tree kernels in this format sig-
nificantly outperform D-PAS. However, the per-
formance is still far lower than the baseline.
The above formats are based on dependency
trees. We try another PST format derived from
constituency trees. These PSTs (Figure 1(c)) are
the lowest common subtrees spanning the predi-
cate node and its argument nodes and are gath-
ered under a dummy root node. The argument role
</bodyText>
<footnote confidence="0.9670334">
13Note that the only lexical information in this format is
the predicate lemma. We tried replacing the POS tags with
argument word forms, which led to a slight degradation.
14This format is chosen among several other variations due
to its higher performance.
</footnote>
<page confidence="0.993567">
71
</page>
<figure confidence="0.996122">
(a) D-PAS (b) D-PST (c) C-PST (d) D-TKSSQE (e) C-TKSSQE
</figure>
<figureCaption confidence="0.999985">
Figure 1: Semantic tree kernel formats for the sentence: Can anyone help?
</figureCaption>
<bodyText confidence="0.994649906976744">
labels are concatenated with the syntactic non-
terminal category of the argument node. Predi-
cates are not marked. However, our dependency-
based SRL is required to be converted into a
constituency-based format. While constituency-
to-dependency conversion is straightforward us-
ing head-finding rules (Surdeanu et al., 2008),
the other way around is not. We therefore ap-
proximate the conversion using a heuristic we call
(D2C).15 As shown in Table 4, the system built us-
ing these PSTs C-PST improves over D-PST for
human-targeted metric prediction, but not man-
ual metric prediction. However, when they are
combined in CD-PST, we can see improvement
over the highest scores of both systems, except
for HTER prediction for Pearson r. The fluency
prediction improvement is statistically significant.
The other changes are not statistically significant.
An alternative approach to formulating seman-
tic tree kernels is to augment syntactic trees with
semantic information. We augment the trees in
TKSyQE with semantic role labels. We attach se-
mantic roles to dependency labels of the argument
nodes in the dependency trees as in Figure 1(d).
For constituency trees, we use the D2C heuristic
to elevate roles up the terminal nodes and attach
the labels to the syntactic non-terminal category
of the node as in Figure 1(e). The performance
of the resulting system, TKSSQE, is shown in Ta-
ble 4. It substantially outperforms its counterpart,
CD-PST, all differences being statistically signif-
icant. However, compared to the plain syntactic
tree kernels (TKSyQE), the changes are slight and
inconsistent, rendering the augmentation not use-
ful. We consider this system to be our syntactic-
15This heuristic (D2C) recursively elevates the argument
role already assigned to a terminal node (based on the
dependency-based argument position) to the parent node as
long as 1) the argument node is not a root node or is not
tagged as a POS (possessive), 2) the role is not an AM-NEG,
AM-MOD or AM-DIS adjunct, and 3) the argument does not
dominate its predicate’s node or another argument node of the
same proposition.
</bodyText>
<table confidence="0.999203555555556">
1-HTER HBLEU Adq Flu
RMSE
WMT17 0.2310 0.2696 0.2219 0.2469
HCSyQE 0.2435 0.2797 0.2334 0.2479
HCSeQE 0.2482 0.2868 0.2416 0.2612
Pearson r
WMT17 0.3661 0.3806 0.4710 0.4769
HCSyQE 0.2572 0.3080 0.3961 0.4696
HCSeQE 0.1794 0.1636 0.2972 0.3577
</table>
<tableCaption confidence="0.739510666666667">
Table 5: RMSE and Pearson r of the 17 baseline
features (WMT17) and hand-crafted features
semantic tree kernel system.
</tableCaption>
<sectionHeader confidence="0.997862" genericHeader="method">
7 Hand-crafted Features
</sectionHeader>
<bodyText confidence="0.999983777777778">
In our previous work (Kaljahi et al., 2014b), we
experiment with a set of hand-crafted syntactic
features extracted from both constituency and de-
pendency trees on a different data set. We apply
the same feature set on the new data set here. The
results are reported in Table 5. The performance of
this system (HCSyQE) is significantly lower than
the baseline. This is opposite to what we ob-
serve with the same feature set on a different data
set, again showing that the role of data is funda-
mental in understanding system performance. The
main difference between these two data sets is that
the former is extracted from a well-formed text in
the news domain, the same domain on which our
parsers and SRL system have been trained, while
the new data set does not necessarily contain well-
formed text nor is it from the same domain.
We design another set of feature types aiming
at capturing the semantics of the source and trans-
lation via predicate-argument structure. The fea-
ture types are listed in Table 6. Feature types
1 to 8 each contain two features, one extracted
from the source and the other from the transla-
tion. To compute argument span sizes (feature
types 4 and 5), we use the constituency conver-
sion of SRL obtained using the D2C heuristic in-
troduced in Section 6. The proposition label se-
</bodyText>
<page confidence="0.987195">
72
</page>
<figure confidence="0.924939909090909">
1 Number of propositions
2 Number of arguments
3 Average number of arguments per proposition
4 Sum of span sizes of arguments
5 Ratio of sum of span sizes of arguments to sentence
length
6 Proposition label sequences
7 Constituency label sequences of proposition elements
8 Dependency label sequences of proposition elements
9 Percentage of predicate/argument word alignment
mapping types
</figure>
<tableCaption confidence="0.98385">
Table 6: Semantic feature types
</tableCaption>
<bodyText confidence="0.999102619047619">
quence (feature type 6) is the concatenation of ar-
gument roles and predicate labels of the propo-
sition with their preserved order (e.g. A0-go.01-
A4). Similarly, constituency and dependency la-
bel sequences (feature types 4 and 5) are extracted
by replacing argument and predicate labels with
their constituency and dependency labels respec-
tively. Feature type 9 consists of three features
based on word alignment of source and target
sentences: number of non-aligned, one-to-many-
aligned and many-to-one-aligned predicates and
arguments. The word alignments are obtained us-
ing the grow-diag-final-and heuristic as
they performed slightly better than other types.16
As in the baseline system, we use SVMs to build
the QE systems using these hand-crafted features.
The nominal features are binarized to be usable by
SVM. However, the set of possible feature values
can be large, leading to a large number of binary
features. For example, there are more than 5000
unique proposition label sequences in our data.
Not only does this high dimensionality reduce the
efficiency of the system, it can also affect its per-
formance as these features are sparse. To tackle
this issue, we impose a frequency cutoff on these
features: we keep only frequent features using a
threshold set empirically on the development set.
Table 5 shows the performance of the system
(HCSeQE) built with these features. The semantic
features perform substantially lower than the syn-
tactic features and thus the baseline, especially in
predicting human-targeted scores. Since these fea-
tures are chosen from a comprehensive set of se-
mantic features, and as they should ideally capture
adequacy better than general features, a probable
reason for their low performance is the quality of
16It should be noted that a number of features in addition
to those presented here have been tried, e.g. the ratio and dif-
ference of the source and target values of numerical features.
However, through manual feature selection, we have removed
features which do not appear to contribute much.
the underlying syntactic and semantic analysis.
</bodyText>
<sectionHeader confidence="0.543256" genericHeader="method">
8 Predicate-Argument Match (PAM)
</sectionHeader>
<bodyText confidence="0.974588022222222">
Translation adequacy measures how much of the
source meaning is preserved in the translated text.
Predicate-argument structure or semantic role la-
belling expresses a substantial part of the meaning.
Therefore, the matching between the predicate-
argument structure of the source and its transla-
tion could be an important clue to the translation
adequacy, independent of the language pair used.
We attempt to exploit predicate-argument match
(PAM) to create a metric that measures the trans-
lation adequacy.
The algorithm to compute PAM score starts
by aligning the predicates and arguments of the
source side to its target side using word align-
ments.17 It then treats the problem as one of SRL
scoring, similar to the scoring scheme used in the
CoNLL 2009 shared task (Hajiˇc et al., 2009). As-
suming the source side SRL as a reference, it com-
putes unlabelled precision and recall of the target
side SRL with respect to it:
UPrec = # aligned preds and their args
# target side preds and args
URec = # aligned preds and their args
# source side preds and args
Labelled precision and recall are calculated in
the same way except that they also require argu-
ment label agreement. UF1 and LF1 are the har-
monic means of unlabelled and labelled scores re-
spectively. Inspired by the observation that most
source sentences with no identified proposition are
short and can be assumed to be easier to translate,
and based on experiments on the dev set, we assign
a score of 1 to such sentences. When no proposi-
tion is identified in the target side while there is a
proposition in the source, we assign a score of 0.5.
We obtain word alignments using the Moses
toolkit (Hoang et al., 2009), which can gener-
ate alignments in both directions and combine
them using a number of heuristics. We try in-
tersection, union, source-to-target only, as well
as the grow-diag-final-and heuristic, but
only the source-to-target results are reported here
as they slightly outperform the others.
Table 7 shows the RMSE and Pearson r for
each of the unlabelled and labelled F1 against ade-
</bodyText>
<footnote confidence="0.999339333333333">
17We also tried lexical and phrase translation tables for this
purpose in addition to word alignments but they do not out-
perform word alignments.
</footnote>
<page confidence="0.99587">
73
</page>
<table confidence="0.998528857142857">
1-HTER HBLEU Adq Flu
RMSE
1 UF1 0.3175 0.3607 0.3108 0.4033
LF1 0.4247 0.3903 0.3839 0.3586
Pearson r
UF1 0.2328 0.2179 0.2698 0.2865
LF1 0.1784 0.1835 0.2225 0.2688
</table>
<tableCaption confidence="0.991188333333333">
Table 7: RMSE and Pearson r of PAM unlabelled
and labelled Fi scores as estimation of the MT
evaluation metrics
</tableCaption>
<table confidence="0.999909222222222">
1-HTER HBLEU Adq Flu
RMSE
PAM 0.2414 0.2833 0.2414 0.2661
HCSeQE 0.2482 0.2868 0.2416 0.2612
HCSeQEpam 0.2445 0.2822 0.2370 0.2575
Pearson r
PAM 0.2292 0.2195 0.2787 0.3210
HCSeQE 0.1794 0.1636 0.2972 0.3577
HCSeQEpam 0.2387 0.2368 0.3571 0.3908
</table>
<tableCaption confidence="0.919397">
Table 8: RMSE and Pearson r of PAM scores as
features, alone and combined (PAM)
</tableCaption>
<bodyText confidence="0.999697571428572">
quacy and also fluency scores on the test data set.18
According to the results, the unlabelled Fi (UFj)
is a closer estimation than the labelled one. Its
Pearson correlation scores are overall competitive
to the hand-crafted semantic features (HCSeQE in
Table 5): they are better for the automatic metric
cases but lower for manual ones. However, the
RMSE scores are considerably larger. Overall, the
performance is not comparable to the baseline and
other well performing systems. We investigate the
reasons behind this result in the next section.
Another way to employ the PAM scores in QE
is to use them in a statistical framework. We build
a SVM model using all 6 PAM scores The per-
formance of this system (PAM) on the test set is
shown in Table 8. The performance is consider-
ably higher than when the PAM scores are used
directly as estimations. Interestingly, compared to
the 47 semantic hand-crafted features (HCSeQE),
this small feature set performs better in predicting
human-targeted metrics.
We add these features to our set of hand-
crafted features in Section 7 to yield a new sys-
tem (HCSeQEpam in Table 8). All scores improve
compared to the stronger of the two components.
However, only the manual metric prediction im-
provements are statistically significant. The per-
formance is still not close to the baseline.
</bodyText>
<footnote confidence="0.904256666666667">
18Precision and recall scores were also tried. Precision
proved to be the weakest estimator, whereas recall scores
were highest for some settings.
</footnote>
<subsectionHeader confidence="0.999135">
8.1 Analyzing PAM
</subsectionHeader>
<bodyText confidence="0.99997878">
Ideally, PAM scores should capture the adequacy
of translation with a high accuracy. The results
are however far from ideal. There are two fac-
tors involved in the PAM scoring procedure, the
quality of which can affect its performance: 1)
predicate-argument structure of the source and
target side of the translation, 2) alignment of
predicate-argument structures of source and target.
The SRL systems for both English and French
are trained on edited newswire. On the other
hand, our data is neither from the same domain nor
edited. The problem is exacerbated on the trans-
lation target side, where our French SRL system
is trained on only a small data set and applied to
machine translation output. To discover the con-
tribution of each of these factors in the accuracy
of PAM, we carry out a manual analysis. We ran-
domly select 10% of the development set (50 sen-
tences) and count the number of problems of each
of these two categories.
We find only 8 cases in which a wrong word
alignment misleads PAM scoring. On the other
hand, there are 219 cases of SRL problems, in-
cluding predicate and argument identification and
labelling: 82 cases (37%) in the source and 138
cases (63%) in the target.
We additionally look for the cases where a
translation divergence causes predicate-argument
mismatch in the source and translation. For ex-
ample, without sacrificing is translated into sans
impact sur (without impact on), a case of transpo-
sition, where the source side verb predicate is left
unaligned thus affecting the PAM score. We find
only 9 such cases in the sample, which is similar
to the proportion of word alignment problems.
As mentioned in the previous section, PAM
scoring has to assign default values for cases in
which there is no predicate in the source or tar-
get. This can be another source of estimation error.
In order to verify its effect, we find such cases in
the development set and manually categorize them
based on the reason causing the sentence to be left
without predicates. There are 79 (16%) source and
96 (19%) target sentences for which the SRL sys-
tems do not identify any predicate, out of which
64 cases have both sides without any predicate.
Among such source sentences, 20 (25%) have no
predicate due to a predicate identification error of
the SRL system, 57 (72%) because of the sentence
structure (e.g. copula verbs which are not labelled
</bodyText>
<page confidence="0.997477">
74
</page>
<table confidence="0.999110307692308">
1-HTER HBLEU Adq Flu
RMSE
WMT17 0.2310 0.2696 0.2219 0.2469
SyQE 0.2255 0.2711 0.2248 0.2419
SeQE 0.2249 0.2710 0.2242 0.2404
SSQE 0.2246 0.2696 0.2230 0.2402
SSQE+WMT17 0.2225 0.2673 0.2202 0.2379
Pearson r
WMT17 0.3661 0.3806 0.4710 0.4769
SyQE 0.3824 0.3650 0.4393 0.5087
SeQE 0.3884 0.3648 0.4447 0.5182
SSQE 0.3920 0.3768 0.4538 0.5196
SSQE+WMT17 0.4144 0.3953 0.4771 0.5331
</table>
<tableCaption confidence="0.986996">
Table 9: RMSE and Pearson r of the 17 baseline
features (WMT17) and system combinations
</tableCaption>
<bodyText confidence="0.999956764705882">
as predicates in the SRL training data, titles, etc.),
and the remaining 2 due to spelling errors mislead-
ing the SRL system. Among the target side sen-
tences, most of the cases are due to the sentence
structure (65 or 68%) and only 14 (15%) cases are
caused by an SRL error. In 13 cases, no verb pred-
icate in the source is translated correctly. Among
the remaining cases, two are due to untranslated
spelling errors in the source and the other two due
to tokenization errors misleading the SRL system.
These numbers show that the main reason lead-
ing to the sentences without verbal predicates is
the sentence structure. This problem can be al-
leviated by employing nominal predicates in both
sides. While this is possible for the English side,
there is currently no French resource where nomi-
nal predicates have been annotated.
</bodyText>
<sectionHeader confidence="0.927488" genericHeader="method">
9 Combining Systems
</sectionHeader>
<bodyText confidence="0.999989219512195">
We now combine the systems we have built so
far (Table 9). We first combine syntax-based
and semantic-based systems individually. SyQE
is the combination of the syntactic tree kernel
system (TKSyQE) and the hand-crafted features
(HCSyQE). Likewise, SeQE is the combination
of the semantic tree kernel system (TKSSQE) and
the semantic hand-crafted features including PAM
features (HCSeQEpam). These two systems are
combined in SSQE but without syntactic tree ker-
nels (TKSyQE) to avoid redundancy with TKSSQE
as these are the augmented syntactic tree kernels.
We finally combine SSQE with the baseline.
SyQE significantly improves over its tree ker-
nel and hand-crafted components. It also outper-
forms the baseline in HTER and fluency predic-
tion, but is beaten by it in HBLEU and adequacy
prediction. None of these differences are statis-
tically significant however. SeQE also performs
better than the stronger of its components. Except
for adequacy prediction, the other improvements
are statistically significant. This system performs
slightly better than SyQE. Its comparison to the
baseline is the same as that of SyQE, except that
its superiority to the baseline in fluency prediction
is statistically significant.
The full syntactic-semantic system (SSQE) also
improves over its syntactic and semantic compo-
nents. However, the improvements are not statisti-
cally significant. Compared to the baseline, HTER
and fluency prediction perform better, the latter
being statistically significant. HBLEU prediction
is around the same as the baseline, but adequacy
prediction performance is lower, though not statis-
tically significantly.
Finally, when we combine the syntactic-
semantic system with the baseline system, the
combination continues to improve further. Com-
pared to the stronger component however, only the
HTER and fluency prediction improvements are
statistically significant.
</bodyText>
<sectionHeader confidence="0.994786" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999988730769231">
We introduced a new QE data set drawn from cus-
tomer support forum text, machine translated and
both post-edited and manually evaluated for ad-
equacy and fluency. We used syntactic and se-
mantic QE systems via both tree kernels and hand-
crafted features. We found it hard to improve over
a baseline, albeit strong, using such information
which is extracted by applying parsers and seman-
tic role labellers on out-of-domain and unedited
text. We also defined a metric for estimating the
translation adequacy based on predicate-argument
structure match between source and target. This
metric relies on automatic word alignments and
semantic role labelling. We find that word align-
ment and translation divergence only have minor
effects on the performance of this metric, whereas
the quality of semantic role labelling is the main
hindering factor. Another major issue affecting the
performance of PAM is the unavailability of nom-
inal predicate annotation.
Our PAM scoring method is based on only word
matches as there are no constituent SRL resources
available for French – perhaps constituent-based
arguments can make a more accurate comparison
between the source and target predicate-argument
structure possible.
</bodyText>
<page confidence="0.998381">
75
</page>
<sectionHeader confidence="0.998316" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995032">
This research has been supported by the Irish
Research Council Enterprise Partnership Scheme
(EPSPG/2011/102) and the computing infrastruc-
ture of the CNGL at DCU. We thank the reviewers
for their helpful comments.
</bodyText>
<sectionHeader confidence="0.99844" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999496098039215">
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English
and French. In Proceedings of the 1st Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages.
Eleftherios Avramidis. 2012. Quality estimation for
Machine Translation output using linguistic analysis
and decoding features. In Proceedings of the 7th
WMT.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley Framenet project. In Proceed-
ings of the 36th ACL.
Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning: Shared Task.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence
estimation for Machine Translation. In JHU/CLSP
Summer Workshop Final Report.
Ondˇrej Bojar and Dekai Wu. 2012. Towards a
predicate-argument evaluation for MT. In Proceed-
ings of the Sixth Workshop on Syntax, Semantics and
Structure in Statistical Translation.
Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs
Tamchyna. 2014. Findings of the 2014 workshop
on Statistical Machine Translation. In Proceedings
of the 9th WMT.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies
representation. In Proceedings of the COLING
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Pascal Denis and Benoit Sagot. 2012. Coupling an
annotated corpus and a lexicon for state-of-the-art
pos tagging. Lang. Resour. Eval., 46(4):721–736.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the 6th WMT.
Jes´us Gim´enez and Llufs M`arquez. 2007. Linguistic
features for automatic evaluation of heterogenous mt
systems. In Proceedings of the Second Workshop on
Statistical Machine Translation.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Martf, Llufs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1–18.
Christian Hardmeier, Joakim Nivre, and J¨org Tiede-
mann. 2012. Tree kernels for machine translation
quality estimation. In Proceedings of the Seventh
WMT.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT).
Rasoul Kaljahi, Jennifer Foster, Raphael Rubino, Jo-
hann Roturier, and Fred Hollowood. 2013. Parser
accuracy in quality estimation of machine transla-
tion: a tree kernel approach. In International Joint
Conference on Natural Language Processing (IJC-
NLP).
Rasoul Kaljahi, Jennifer Foster, and Johann Roturier.
2014a. Semantic role labelling with minimal re-
sources: Experiments with french. In Third Joint
Conference on Lexical and Computational Seman-
tics (*SEM).
Rasoul Kaljahi, Jennifer Foster, Raphael Rubino, and
Johann Roturier. 2014b. Quality estimation of
english-french machine translation: A detailed study
of the role of syntax. In International Conference on
Computational Linguistics (COLING).
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Conference Pro-
ceedings: the tenth Machine Translation Summit.
LDC. 2002. Linguistic data annotation specification:
Assessment of fluency and adequacy in chinese-
english translations. Technical report.
Chi-kiu Lo and Dekai Wu. 2011. Meant: An inex-
pensive, high-accuracy, semi-automatic metric for
evaluating translation utility via semantic frames. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully automatic semantic mt evaluation. In
Proceedings of the Seventh WMT.
</reference>
<page confidence="0.843083">
76
</page>
<reference confidence="0.999858933333333">
Chi-kiu Lo, Meriem Beloucif, Markus Saers, and
Dekai Wu. 2014. Xmeant: Better semantic mt eval-
uation without reference translations. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), June.
Llu´ıs M`arquez, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic role labeling: An introduction to the special
issue. Comput. Linguist., 34(2):145–159, June.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Tree kernel engineering for propo-
sition re-ranking. In Proceedings of Mining and
Learning with Graphs (MLG).
Alessandro Moschitti. 2006. Making tree kernels prac-
tical for natural language learning. In Proceedings
of EACL.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19–51.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
ACL, pages 311–318.
Daniele Pighin and Llu´ıs M`arquez. 2011. Automatic
projection of semantic structures: An application to
pairwise translation ranking. In Proceedings of the
Fifth Workshop on Syntax, Semantics and Structure
in Statistical Translation, pages 1–9.
Chris Quirk. 2004. Training a sentence-level machine
translation confidence measure. In Proceedings of
LREC.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA.
Lucia Specia and Jes´us Gim´enez. 2010. Combining
confidence estimation and reference-based metrics
for segment level MT evaluation. In Proceedings of
AMTA.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The
conll-2008 shared task on joint parsing of syntactic
and semantic dependencies. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning.
Lonneke van der Plas, Tanja Samardˇzi´c, and Paola
Merlo. 2010. Cross-lingual validity of propbank
in the manual annotation of french. In Proceedings
of the Fourth Linguistic Annotation Workshop, LAW
IV ’10.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies.
</reference>
<page confidence="0.999127">
77
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.337077">
<title confidence="0.999303">Syntax and Semantics in Quality Estimation of Machine Translation</title>
<author confidence="0.6759585">Jennifer Johann School of Computing</author>
<author confidence="0.6759585">Dublin City University</author>
<author confidence="0.6759585">Research Labs</author>
<author confidence="0.6759585">Dublin</author>
<abstract confidence="0.991787346153846">We employ syntactic and semantic information in estimating the quality of machine translation from a new data set which contains source text from English customer support forums and target text consisting of its machine translation into French. These translations have been both post-edited and evaluated by professional translators. We find that quality estimation using syntactic and semantic information on this data set can hardly improve over a baseline which uses only surface features. However, the performance can be improved when they are combined with such surface features. We also introduce a novel metric to measure translation adequacy based on predicate-argument structure match using word alignments. While word alignments can be reliably used, the two main factors affecting the performance of all semantic-based methods seems to be the low quality of semantic role labelling (especially on ill-formed text) and the lack of nominal predicate annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohammed Attia</author>
<author>Jennifer Foster</author>
<author>Deirdre Hogan</author>
<author>Joseph Le Roux</author>
<author>Lamia Tounsi</author>
<author>Josef van Genabith</author>
</authors>
<title>Handling unknown words in statistical latent-variable parsing models for Arabic, English and French.</title>
<date>2010</date>
<booktitle>In Proceedings of the 1st Workshop on Statistical Parsing of Morphologically Rich Languages.</booktitle>
<marker>Attia, Foster, Hogan, Le Roux, Tounsi, van Genabith, 2010</marker>
<rawString>Mohammed Attia, Jennifer Foster, Deirdre Hogan, Joseph Le Roux, Lamia Tounsi, and Josef van Genabith. 2010. Handling unknown words in statistical latent-variable parsing models for Arabic, English and French. In Proceedings of the 1st Workshop on Statistical Parsing of Morphologically Rich Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleftherios Avramidis</author>
</authors>
<title>Quality estimation for Machine Translation output using linguistic analysis and decoding features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 7th WMT.</booktitle>
<contexts>
<context position="3594" citStr="Avramidis, 2012" startWordPosition="565" endWordPosition="566">data set. The rest of the paper is organized as follows: after reviewing the related work, the data is described and the semantic role labelling approach is explained. The baseline is then introduced, followed by the experiments with tree kernels, handcrafted features, the PAM metric and finally the combination of all methods. The paper ends with a summary and suggestions for future work. 2 Related Work Syntax has been exploited in QE in various ways including tree kernels (Hardmeier et al., 2012; Kaljahi et al., 2013; Kaljahi et al., 2014b), parse probabilities and syntactic label frequency (Avramidis, 2012), parseability (Quirk, 2004) and POS n-gram scores (Specia and Gim´enez, 2010). 1The data will be made publicly available -see http:// www.computing.dcu.ie/mt/confidentmt.html 67 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67–77, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Turning to the role of semantic knowledge in QE and MT evaluation in general, Pighin and M`arquez (2011) propose a method for ranking two translation hypotheses that exploits the projection of SRL from a sentence to its transl</context>
</contexts>
<marker>Avramidis, 2012</marker>
<rawString>Eleftherios Avramidis. 2012. Quality estimation for Machine Translation output using linguistic analysis and decoding features. In Proceedings of the 7th WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley Framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th ACL.</booktitle>
<contexts>
<context position="12928" citStr="Baker et al., 1998" startWordPosition="2056" endWordPosition="2059">he fluency and adequacy of the MT output is low or high, the other tends to be the same. The data is split into train, development and test sets of 3000, 500 and 1000 sentences respectively. 4 Semantic Role Labelling The type of semantic information we use in this work is the predicate-argument structure or semantic role labelling of the sentence. This information needs to be extracted from both sides of the translation, i.e. English and French. Though the SRL of English has been well-studied (M`arquez et al., 2008) thanks to the existence of two major hand-crafted resources, namely FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), French is one of the under-studied languages in 69 1-HTER HBLEU HMeteor Adequacy Fluency Average 0.6976 0.5517 0.7221 0.6230 0.4096 Standard Deviation 0.2446 0.2927 0.2129 0.2488 0.2780 Table 1: Average and standard deviation of the evaluation scores for the entire data set this respect mainly due to a lack of such resources. The only available gold standard resource is a small set of 1000 sentences taken from Europarl (Koehn, 2005) and manually annotated with Propbank verb predicates (van der Plas et al., 2010). van der Plas et al. (2011) attempt to tackle</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley Framenet project. In Proceedings of the 36th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Love Hafdell</author>
<author>Pierre Nugues</author>
</authors>
<title>Multilingual semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task.</booktitle>
<marker>Bj¨orkelund, Hafdell, Nugues, 2009</marker>
<rawString>Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues. 2009. Multilingual semantic role labeling. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for Machine Translation.</title>
<date>2003</date>
<booktitle>In JHU/CLSP Summer Workshop Final Report.</booktitle>
<contexts>
<context position="1516" citStr="Blatz et al. (2003)" startWordPosition="219" endWordPosition="222">e a novel metric to measure translation adequacy based on predicate-argument structure match using word alignments. While word alignments can be reliably used, the two main factors affecting the performance of all semantic-based methods seems to be the low quality of semantic role labelling (especially on ill-formed text) and the lack of nominal predicate annotation. 1 Introduction The problem of evaluating machine translation output without reference translations is called quality estimation (QE) and has recently been the centre of attention (Bojar et al., 2014) following the seminal work of Blatz et al. (2003). Most QE studies have focused on surface and languagemodel-based features of the source and target. The quality of translation is however closely related to the syntax and semantics of the languages, the former concerning fluency and the latter adequacy. While there have been some attempts to utilize syntax in this task, semantics has been paid less attention. In this work, we aim to exploit both syntax and semantics in QE, with a particular focus on the latter. We use shallow semantic analysis obtained via semantic role labelling (SRL) and employ this information in QE in various ways includ</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2003</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2003. Confidence estimation for Machine Translation. In JHU/CLSP Summer Workshop Final Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Dekai Wu</author>
</authors>
<title>Towards a predicate-argument evaluation for MT.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation.</booktitle>
<contexts>
<context position="7186" citStr="Bojar and Wu (2012)" startWordPosition="1133" endWordPosition="1136">ates and arguments are aligned, their similarities are measured using a variety of methods such as cosine distance and even Meteor and BLEU. In computation of the final score, the similarity scores replace the counts of correct and partial translations used in HMEANT. This metric outperforms several automatic metrics including BLEU, Meteor and TER, but it significantly under-performs HMEANT and HTER. Further analysis shows that automatizing the second step does not affect the performance of MEANT. Therefore, it seems to be the lower accuracy of the semantic role labelling that is responsible. Bojar and Wu (2012) identify a set of flaws with HMEANT and propose solutions for them. The most important problems stem from the superficial SRL annotation guidelines. These problems are exacerbated in MEANT due to the automatic nature of the two steps. More recently, Lo et al. (2014) extend MEANT to ranking translations without a reference by using phrase translation probabilities for aligning semantic role fillers of the source and its translation. 3 Data We randomly select 4500 segments from a large collection of Symantec English Norton forum text.2 In order to be independent of any one MT system, we transla</context>
</contexts>
<marker>Bojar, Wu, 2012</marker>
<rawString>Ondˇrej Bojar and Dekai Wu. 2012. Towards a predicate-argument evaluation for MT. In Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Johannes Leveling</author>
<author>Christof Monz</author>
<author>Pavel Pecina</author>
<author>Matt Post</author>
</authors>
<title>Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs Tamchyna.</title>
<date>2014</date>
<booktitle>Findings of the 2014 workshop on Statistical Machine Translation. In Proceedings of the 9th WMT.</booktitle>
<contexts>
<context position="1466" citStr="Bojar et al., 2014" startWordPosition="210" endWordPosition="213">bined with such surface features. We also introduce a novel metric to measure translation adequacy based on predicate-argument structure match using word alignments. While word alignments can be reliably used, the two main factors affecting the performance of all semantic-based methods seems to be the low quality of semantic role labelling (especially on ill-formed text) and the lack of nominal predicate annotation. 1 Introduction The problem of evaluating machine translation output without reference translations is called quality estimation (QE) and has recently been the centre of attention (Bojar et al., 2014) following the seminal work of Blatz et al. (2003). Most QE studies have focused on surface and languagemodel-based features of the source and target. The quality of translation is however closely related to the syntax and semantics of the languages, the former concerning fluency and the latter adequacy. While there have been some attempts to utilize syntax in this task, semantics has been paid less attention. In this work, we aim to exploit both syntax and semantics in QE, with a particular focus on the latter. We use shallow semantic analysis obtained via semantic role labelling (SRL) and em</context>
<context position="16021" citStr="Bojar et al., 2014" startWordPosition="2582" endWordPosition="2585"> MElt tagger (Denis and Sagot, 2012). The English SRL achieves 77.77 and 67.02 labelled F1 points when trained only on the training section of PropBank and tested on the WSJ and Brown test sets respectively.9 The French SRL is evaluated using 5-fold cross-validation on the 1K data set and obtains an F1 average of 67.66. When applied to the QE data set, these models identify 9133, 8875 and 8795 propositions on its source side, post-edits and MT output respectively. 5 Baseline We compare the results of our experiments to a baseline built using the 17 baseline features of the WMT QE shared task (Bojar et al., 2014). These features provide a strong baseline and have been used in all three years of the shared task. We use support vector regression implemented in the SVMLight toolkit10 with Radial Basis Function (RBF) kernel to build this baseline. To extract these features, a parallel English-French corpus is required to build a lexical translation table using GIZA++ (Och and Ney, 2003). We use the Europarl English-French parallel corpus (Koehn, 2005) plus around 1M segments of Symantec translation memory. Table 4 shows the performance of this system (WMT17) on the test set measured by Root Mean Square Er</context>
</contexts>
<marker>Bojar, Buck, Federmann, Haddow, Koehn, Leveling, Monz, Pecina, Post, 2014</marker>
<rawString>Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs Tamchyna. 2014. Findings of the 2014 workshop on Statistical Machine Translation. In Proceedings of the 9th WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Proceedings of the COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation.</booktitle>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representation. In Proceedings of the COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Benoit Sagot</author>
</authors>
<title>Coupling an annotated corpus and a lexicon for state-of-the-art pos tagging.</title>
<date>2012</date>
<journal>Lang. Resour. Eval.,</journal>
<volume>46</volume>
<issue>4</issue>
<contexts>
<context position="15438" citStr="Denis and Sagot, 2012" startWordPosition="2479" endWordPosition="2483">the syntactic annotation of the test data is obtained using a different parser than that of the training data. We therefore replace the parses of the training data with those obtained automatically by first parsing the data using the Lorg PCFG-LA parser8 (Attia et al., 2010) and then converting them to dependencies using Stanford converter (de Marneffe and Manning, 2008). The POS tags are also replaced with those output by the parser. For the same reason, we re8https://github.com/CNGLdlab/LORGRelease. place the original POS tagging of the French 1K data with those obtained by the MElt tagger (Denis and Sagot, 2012). The English SRL achieves 77.77 and 67.02 labelled F1 points when trained only on the training section of PropBank and tested on the WSJ and Brown test sets respectively.9 The French SRL is evaluated using 5-fold cross-validation on the 1K data set and obtains an F1 average of 67.66. When applied to the QE data set, these models identify 9133, 8875 and 8795 propositions on its source side, post-edits and MT output respectively. 5 Baseline We compare the results of our experiments to a baseline built using the 17 baseline features of the WMT QE shared task (Bojar et al., 2014). These features </context>
</contexts>
<marker>Denis, Sagot, 2012</marker>
<rawString>Pascal Denis and Benoit Sagot. 2012. Coupling an annotated corpus and a lexicon for state-of-the-art pos tagging. Lang. Resour. Eval., 46(4):721–736.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the 6th WMT.</booktitle>
<contexts>
<context position="9042" citStr="Denkowski and Lavie, 2011" startWordPosition="1414" endWordPosition="1417">4http://www.bing.com/translator(on24- Feb-2014) 68 5 4 3 2 1 All meaning Most of meaning Much of meaning Little meaning None of meaning Fluency Flawless Language Good Language Non-native Language Disfluent Language Incomprehensible 1-HTER HBLEU HMeteor Adq Flu 1-HTER - - - - - HBLEU 0.9111 - - - - HMeteor 0.9207 0.9314 - - - Adq 0.6632 0.7049 0.6843 - - Flu 0.6447 0.7213 0.6652 0.8824 - Adequacy Table 2: Adequacy/fluency score interpretation French speaker.5 Each sentence translation is then scored against its post-edit using BLEU6(Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011), which are the most widely used MT evaluation metrics. Following Snover et al. (2006), we consider this way of scoring MT output to be a variation of humantargeted scoring, where no reference translation is provided to the post-editor, so we call them HBLEU, HTER and HMETEOR. The average scores for the entire data set together with their standard deviations are presented in Table 1.7 In the second method, we asked three professional translators, who are again native French speakers, to assess the quality of MT output in terms of adequacy and fluency in a 5-grade scale (LDC, 2002). The interpr</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the 6th WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llufs M`arquez</author>
</authors>
<title>Linguistic features for automatic evaluation of heterogenous mt systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation.</booktitle>
<marker>Gim´enez, M`arquez, 2007</marker>
<rawString>Jes´us Gim´enez and Llufs M`arquez. 2007. Linguistic features for automatic evaluation of heterogenous mt systems. In Proceedings of the Second Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Martf</author>
<author>Llufs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--18</pages>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Martf, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Martf, Llufs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
<author>Joakim Nivre</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Tree kernels for machine translation quality estimation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh WMT.</booktitle>
<contexts>
<context position="3479" citStr="Hardmeier et al., 2012" startWordPosition="547" endWordPosition="550">rious metrics of interest to be applied to measure different aspects of quality. All experiments are carried out on this data set. The rest of the paper is organized as follows: after reviewing the related work, the data is described and the semantic role labelling approach is explained. The baseline is then introduced, followed by the experiments with tree kernels, handcrafted features, the PAM metric and finally the combination of all methods. The paper ends with a summary and suggestions for future work. 2 Related Work Syntax has been exploited in QE in various ways including tree kernels (Hardmeier et al., 2012; Kaljahi et al., 2013; Kaljahi et al., 2014b), parse probabilities and syntactic label frequency (Avramidis, 2012), parseability (Quirk, 2004) and POS n-gram scores (Specia and Gim´enez, 2010). 1The data will be made publicly available -see http:// www.computing.dcu.ie/mt/confidentmt.html 67 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67–77, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Turning to the role of semantic knowledge in QE and MT evaluation in general, Pighin and M`arquez (2011) propos</context>
<context position="17491" citStr="Hardmeier et al. (2012)" startWordPosition="2823" endWordPosition="2826">e results show a significant difference between manual and human-targeted metric prediction. The higher r for the former suggests that the patterns of these scores are easier to learn. The RMSE seems to follow the standard deviation 9Although the English SRL data are annotated for noun predicates as well as verb predicates, since the French data has only verb predicate annotations, we only consider verb predicates for English. 10http://svmlight.joachims.org/ 70 of the scores as the same ranking is seen in both. 6 Tree Kernels Tree kernels (Moschitti, 2006) have been successfully used in QE by Hardmeier et al. (2012) and in our previous work (Kaljahi et al., 2013; Kaljahi et al., 2014b), where syntactic trees are employed. Tree kernels eliminate the burden of manual feature engineering by efficiently utilizing all subtrees of a tree. We employ both syntactic and semantic information in learning quality scores, using the SVMLight-TK11, a support vector machine (SVM) implementation of tree kernels. We implement a syntactic tree kernel QE system with constituency and dependency trees of the source and target side, following our previous work (Kaljahi et al., 2013; Kaljahi et al., 2014b). The performance of t</context>
</contexts>
<marker>Hardmeier, Nivre, Tiedemann, 2012</marker>
<rawString>Christian Hardmeier, Joakim Nivre, and J¨org Tiedemann. 2012. Tree kernels for machine translation quality estimation. In Proceedings of the Seventh WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
<author>Adam Lopez</author>
</authors>
<title>A unified framework for phrase-based, hierarchical, and syntax-based statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT).</booktitle>
<contexts>
<context position="30086" citStr="Hoang et al., 2009" startWordPosition="4889" endWordPosition="4892"> Labelled precision and recall are calculated in the same way except that they also require argument label agreement. UF1 and LF1 are the harmonic means of unlabelled and labelled scores respectively. Inspired by the observation that most source sentences with no identified proposition are short and can be assumed to be easier to translate, and based on experiments on the dev set, we assign a score of 1 to such sentences. When no proposition is identified in the target side while there is a proposition in the source, we assign a score of 0.5. We obtain word alignments using the Moses toolkit (Hoang et al., 2009), which can generate alignments in both directions and combine them using a number of heuristics. We try intersection, union, source-to-target only, as well as the grow-diag-final-and heuristic, but only the source-to-target results are reported here as they slightly outperform the others. Table 7 shows the RMSE and Pearson r for each of the unlabelled and labelled F1 against ade17We also tried lexical and phrase translation tables for this purpose in addition to word alignments but they do not outperform word alignments. 73 1-HTER HBLEU Adq Flu RMSE 1 UF1 0.3175 0.3607 0.3108 0.4033 LF1 0.424</context>
</contexts>
<marker>Hoang, Koehn, Lopez, 2009</marker>
<rawString>Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009. A unified framework for phrase-based, hierarchical, and syntax-based statistical machine translation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rasoul Kaljahi</author>
<author>Jennifer Foster</author>
<author>Raphael Rubino</author>
<author>Johann Roturier</author>
<author>Fred Hollowood</author>
</authors>
<title>Parser accuracy in quality estimation of machine translation: a tree kernel approach.</title>
<date>2013</date>
<booktitle>In International Joint Conference on Natural Language Processing (IJCNLP).</booktitle>
<contexts>
<context position="3501" citStr="Kaljahi et al., 2013" startWordPosition="551" endWordPosition="554">t to be applied to measure different aspects of quality. All experiments are carried out on this data set. The rest of the paper is organized as follows: after reviewing the related work, the data is described and the semantic role labelling approach is explained. The baseline is then introduced, followed by the experiments with tree kernels, handcrafted features, the PAM metric and finally the combination of all methods. The paper ends with a summary and suggestions for future work. 2 Related Work Syntax has been exploited in QE in various ways including tree kernels (Hardmeier et al., 2012; Kaljahi et al., 2013; Kaljahi et al., 2014b), parse probabilities and syntactic label frequency (Avramidis, 2012), parseability (Quirk, 2004) and POS n-gram scores (Specia and Gim´enez, 2010). 1The data will be made publicly available -see http:// www.computing.dcu.ie/mt/confidentmt.html 67 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67–77, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Turning to the role of semantic knowledge in QE and MT evaluation in general, Pighin and M`arquez (2011) propose a method for ranking</context>
<context position="17538" citStr="Kaljahi et al., 2013" startWordPosition="2832" endWordPosition="2835">nual and human-targeted metric prediction. The higher r for the former suggests that the patterns of these scores are easier to learn. The RMSE seems to follow the standard deviation 9Although the English SRL data are annotated for noun predicates as well as verb predicates, since the French data has only verb predicate annotations, we only consider verb predicates for English. 10http://svmlight.joachims.org/ 70 of the scores as the same ranking is seen in both. 6 Tree Kernels Tree kernels (Moschitti, 2006) have been successfully used in QE by Hardmeier et al. (2012) and in our previous work (Kaljahi et al., 2013; Kaljahi et al., 2014b), where syntactic trees are employed. Tree kernels eliminate the burden of manual feature engineering by efficiently utilizing all subtrees of a tree. We employ both syntactic and semantic information in learning quality scores, using the SVMLight-TK11, a support vector machine (SVM) implementation of tree kernels. We implement a syntactic tree kernel QE system with constituency and dependency trees of the source and target side, following our previous work (Kaljahi et al., 2013; Kaljahi et al., 2014b). The performance of this system (TKSyQE) is shown in Table 4. Unlike</context>
</contexts>
<marker>Kaljahi, Foster, Rubino, Roturier, Hollowood, 2013</marker>
<rawString>Rasoul Kaljahi, Jennifer Foster, Raphael Rubino, Johann Roturier, and Fred Hollowood. 2013. Parser accuracy in quality estimation of machine translation: a tree kernel approach. In International Joint Conference on Natural Language Processing (IJCNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rasoul Kaljahi</author>
<author>Jennifer Foster</author>
<author>Johann Roturier</author>
</authors>
<title>Semantic role labelling with minimal resources: Experiments with french.</title>
<date>2014</date>
<booktitle>In Third Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<contexts>
<context position="3523" citStr="Kaljahi et al., 2014" startWordPosition="555" endWordPosition="558">sure different aspects of quality. All experiments are carried out on this data set. The rest of the paper is organized as follows: after reviewing the related work, the data is described and the semantic role labelling approach is explained. The baseline is then introduced, followed by the experiments with tree kernels, handcrafted features, the PAM metric and finally the combination of all methods. The paper ends with a summary and suggestions for future work. 2 Related Work Syntax has been exploited in QE in various ways including tree kernels (Hardmeier et al., 2012; Kaljahi et al., 2013; Kaljahi et al., 2014b), parse probabilities and syntactic label frequency (Avramidis, 2012), parseability (Quirk, 2004) and POS n-gram scores (Specia and Gim´enez, 2010). 1The data will be made publicly available -see http:// www.computing.dcu.ie/mt/confidentmt.html 67 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67–77, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Turning to the role of semantic knowledge in QE and MT evaluation in general, Pighin and M`arquez (2011) propose a method for ranking two translation hypot</context>
<context position="13693" citStr="Kaljahi et al., 2014" startWordPosition="2181" endWordPosition="2184">17 0.7221 0.6230 0.4096 Standard Deviation 0.2446 0.2927 0.2129 0.2488 0.2780 Table 1: Average and standard deviation of the evaluation scores for the entire data set this respect mainly due to a lack of such resources. The only available gold standard resource is a small set of 1000 sentences taken from Europarl (Koehn, 2005) and manually annotated with Propbank verb predicates (van der Plas et al., 2010). van der Plas et al. (2011) attempt to tackle this scarcity by automatically projecting SRL from the English side of a large parallel corpus to its French side. Our preliminary experiments (Kaljahi et al., 2014a), however, show that SRL models trained on the small manually annotated corpus have a higher quality than ones trained on the much larger projected corpus. We therefore use the 1K gold standard set to train a French SRL model. For English, we use all the data provided in the CoNLL 2009 shared task (Hajiˇc et al., 2009). We use LTH (Bj¨orkelund et al., 2009), a dependency-based SRL system, for both the English and French data. This system was among the best performing systems in the CoNLL 2009 shared task and is straightforward to use. It comes with a set of features tuned for each shared tas</context>
<context position="17560" citStr="Kaljahi et al., 2014" startWordPosition="2836" endWordPosition="2840">d metric prediction. The higher r for the former suggests that the patterns of these scores are easier to learn. The RMSE seems to follow the standard deviation 9Although the English SRL data are annotated for noun predicates as well as verb predicates, since the French data has only verb predicate annotations, we only consider verb predicates for English. 10http://svmlight.joachims.org/ 70 of the scores as the same ranking is seen in both. 6 Tree Kernels Tree kernels (Moschitti, 2006) have been successfully used in QE by Hardmeier et al. (2012) and in our previous work (Kaljahi et al., 2013; Kaljahi et al., 2014b), where syntactic trees are employed. Tree kernels eliminate the burden of manual feature engineering by efficiently utilizing all subtrees of a tree. We employ both syntactic and semantic information in learning quality scores, using the SVMLight-TK11, a support vector machine (SVM) implementation of tree kernels. We implement a syntactic tree kernel QE system with constituency and dependency trees of the source and target side, following our previous work (Kaljahi et al., 2013; Kaljahi et al., 2014b). The performance of this system (TKSyQE) is shown in Table 4. Unlike our previous results,</context>
<context position="24603" citStr="Kaljahi et al., 2014" startWordPosition="3977" endWordPosition="3980">ed as a POS (possessive), 2) the role is not an AM-NEG, AM-MOD or AM-DIS adjunct, and 3) the argument does not dominate its predicate’s node or another argument node of the same proposition. 1-HTER HBLEU Adq Flu RMSE WMT17 0.2310 0.2696 0.2219 0.2469 HCSyQE 0.2435 0.2797 0.2334 0.2479 HCSeQE 0.2482 0.2868 0.2416 0.2612 Pearson r WMT17 0.3661 0.3806 0.4710 0.4769 HCSyQE 0.2572 0.3080 0.3961 0.4696 HCSeQE 0.1794 0.1636 0.2972 0.3577 Table 5: RMSE and Pearson r of the 17 baseline features (WMT17) and hand-crafted features semantic tree kernel system. 7 Hand-crafted Features In our previous work (Kaljahi et al., 2014b), we experiment with a set of hand-crafted syntactic features extracted from both constituency and dependency trees on a different data set. We apply the same feature set on the new data set here. The results are reported in Table 5. The performance of this system (HCSyQE) is significantly lower than the baseline. This is opposite to what we observe with the same feature set on a different data set, again showing that the role of data is fundamental in understanding system performance. The main difference between these two data sets is that the former is extracted from a well-formed text in </context>
</contexts>
<marker>Kaljahi, Foster, Roturier, 2014</marker>
<rawString>Rasoul Kaljahi, Jennifer Foster, and Johann Roturier. 2014a. Semantic role labelling with minimal resources: Experiments with french. In Third Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rasoul Kaljahi</author>
<author>Jennifer Foster</author>
<author>Raphael Rubino</author>
<author>Johann Roturier</author>
</authors>
<title>Quality estimation of english-french machine translation: A detailed study of the role of syntax.</title>
<date>2014</date>
<booktitle>In International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="3523" citStr="Kaljahi et al., 2014" startWordPosition="555" endWordPosition="558">sure different aspects of quality. All experiments are carried out on this data set. The rest of the paper is organized as follows: after reviewing the related work, the data is described and the semantic role labelling approach is explained. The baseline is then introduced, followed by the experiments with tree kernels, handcrafted features, the PAM metric and finally the combination of all methods. The paper ends with a summary and suggestions for future work. 2 Related Work Syntax has been exploited in QE in various ways including tree kernels (Hardmeier et al., 2012; Kaljahi et al., 2013; Kaljahi et al., 2014b), parse probabilities and syntactic label frequency (Avramidis, 2012), parseability (Quirk, 2004) and POS n-gram scores (Specia and Gim´enez, 2010). 1The data will be made publicly available -see http:// www.computing.dcu.ie/mt/confidentmt.html 67 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67–77, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Turning to the role of semantic knowledge in QE and MT evaluation in general, Pighin and M`arquez (2011) propose a method for ranking two translation hypot</context>
<context position="13693" citStr="Kaljahi et al., 2014" startWordPosition="2181" endWordPosition="2184">17 0.7221 0.6230 0.4096 Standard Deviation 0.2446 0.2927 0.2129 0.2488 0.2780 Table 1: Average and standard deviation of the evaluation scores for the entire data set this respect mainly due to a lack of such resources. The only available gold standard resource is a small set of 1000 sentences taken from Europarl (Koehn, 2005) and manually annotated with Propbank verb predicates (van der Plas et al., 2010). van der Plas et al. (2011) attempt to tackle this scarcity by automatically projecting SRL from the English side of a large parallel corpus to its French side. Our preliminary experiments (Kaljahi et al., 2014a), however, show that SRL models trained on the small manually annotated corpus have a higher quality than ones trained on the much larger projected corpus. We therefore use the 1K gold standard set to train a French SRL model. For English, we use all the data provided in the CoNLL 2009 shared task (Hajiˇc et al., 2009). We use LTH (Bj¨orkelund et al., 2009), a dependency-based SRL system, for both the English and French data. This system was among the best performing systems in the CoNLL 2009 shared task and is straightforward to use. It comes with a set of features tuned for each shared tas</context>
<context position="17560" citStr="Kaljahi et al., 2014" startWordPosition="2836" endWordPosition="2840">d metric prediction. The higher r for the former suggests that the patterns of these scores are easier to learn. The RMSE seems to follow the standard deviation 9Although the English SRL data are annotated for noun predicates as well as verb predicates, since the French data has only verb predicate annotations, we only consider verb predicates for English. 10http://svmlight.joachims.org/ 70 of the scores as the same ranking is seen in both. 6 Tree Kernels Tree kernels (Moschitti, 2006) have been successfully used in QE by Hardmeier et al. (2012) and in our previous work (Kaljahi et al., 2013; Kaljahi et al., 2014b), where syntactic trees are employed. Tree kernels eliminate the burden of manual feature engineering by efficiently utilizing all subtrees of a tree. We employ both syntactic and semantic information in learning quality scores, using the SVMLight-TK11, a support vector machine (SVM) implementation of tree kernels. We implement a syntactic tree kernel QE system with constituency and dependency trees of the source and target side, following our previous work (Kaljahi et al., 2013; Kaljahi et al., 2014b). The performance of this system (TKSyQE) is shown in Table 4. Unlike our previous results,</context>
<context position="24603" citStr="Kaljahi et al., 2014" startWordPosition="3977" endWordPosition="3980">ed as a POS (possessive), 2) the role is not an AM-NEG, AM-MOD or AM-DIS adjunct, and 3) the argument does not dominate its predicate’s node or another argument node of the same proposition. 1-HTER HBLEU Adq Flu RMSE WMT17 0.2310 0.2696 0.2219 0.2469 HCSyQE 0.2435 0.2797 0.2334 0.2479 HCSeQE 0.2482 0.2868 0.2416 0.2612 Pearson r WMT17 0.3661 0.3806 0.4710 0.4769 HCSyQE 0.2572 0.3080 0.3961 0.4696 HCSeQE 0.1794 0.1636 0.2972 0.3577 Table 5: RMSE and Pearson r of the 17 baseline features (WMT17) and hand-crafted features semantic tree kernel system. 7 Hand-crafted Features In our previous work (Kaljahi et al., 2014b), we experiment with a set of hand-crafted syntactic features extracted from both constituency and dependency trees on a different data set. We apply the same feature set on the new data set here. The results are reported in Table 5. The performance of this system (HCSyQE) is significantly lower than the baseline. This is opposite to what we observe with the same feature set on a different data set, again showing that the role of data is fundamental in understanding system performance. The main difference between these two data sets is that the former is extracted from a well-formed text in </context>
</contexts>
<marker>Kaljahi, Foster, Rubino, Roturier, 2014</marker>
<rawString>Rasoul Kaljahi, Jennifer Foster, Raphael Rubino, and Johann Roturier. 2014b. Quality estimation of english-french machine translation: A detailed study of the role of syntax. In International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="19664" citStr="Koehn (2004)" startWordPosition="3191" endWordPosition="3192">d under a dummy root node as slots of one predicate and 6 arguments of a proposition (one tree per predicate). Each node dominates an argument label or a dummy label for the predicate, which in turn dominates the POS tag of the argument or the predicate lemma. If a proposition has more than 6 arguments they are ignored, if it has fewer than 6 arguments, the extra slots are attached to a dummy null label. Note that these trees are derived from the dependency-based SRL of both the source and target side (Figure 11http://disi.unitn.it/moschitti/TreeKernel.htm 12We use paired bootstrap resampling Koehn (2004) for statistical significance testing. 1-HTER HBLEU Adq Flu RMSE WMT17 0.2310 0.2696 0.2219 0.2469 TKSyQE 0.2267 0.2721 0.2258 0.2431 D-PAS 0.2489 0.2856 0.2423 0.2652 D-PST 0.2409 0.2815 0.2383 0.2606 C-PST 0.2400 0.2809 0.2410 0.2615 CD-PST 0.2394 0.2795 0.2373 0.2578 TKSSQE 0.2269 0.2722 0.2253 0.2425 Pearson r WMT17 0.3661 0.3806 0.4710 0.4769 TKSyQE 0.3693 0.3559 0.4306 0.5013 D-PAS 0.1774 0.1843 0.2770 0.3252 D-PST 0.2136 0.2450 0.3169 0.3670 C-PST 0.2319 0.2541 0.2966 0.3616 CD-PST 0.2311 0.2714 0.3303 0.3923 TKSSQE 0.3682 0.3537 0.4351 0.5046 Table 4: RMSE and Pearson r of the 17 basel</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Conference Proceedings: the tenth Machine Translation Summit.</booktitle>
<contexts>
<context position="13401" citStr="Koehn, 2005" startWordPosition="2134" endWordPosition="2135">been well-studied (M`arquez et al., 2008) thanks to the existence of two major hand-crafted resources, namely FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), French is one of the under-studied languages in 69 1-HTER HBLEU HMeteor Adequacy Fluency Average 0.6976 0.5517 0.7221 0.6230 0.4096 Standard Deviation 0.2446 0.2927 0.2129 0.2488 0.2780 Table 1: Average and standard deviation of the evaluation scores for the entire data set this respect mainly due to a lack of such resources. The only available gold standard resource is a small set of 1000 sentences taken from Europarl (Koehn, 2005) and manually annotated with Propbank verb predicates (van der Plas et al., 2010). van der Plas et al. (2011) attempt to tackle this scarcity by automatically projecting SRL from the English side of a large parallel corpus to its French side. Our preliminary experiments (Kaljahi et al., 2014a), however, show that SRL models trained on the small manually annotated corpus have a higher quality than ones trained on the much larger projected corpus. We therefore use the 1K gold standard set to train a French SRL model. For English, we use all the data provided in the CoNLL 2009 shared task (Hajiˇc</context>
<context position="16464" citStr="Koehn, 2005" startWordPosition="2654" endWordPosition="2655">tput respectively. 5 Baseline We compare the results of our experiments to a baseline built using the 17 baseline features of the WMT QE shared task (Bojar et al., 2014). These features provide a strong baseline and have been used in all three years of the shared task. We use support vector regression implemented in the SVMLight toolkit10 with Radial Basis Function (RBF) kernel to build this baseline. To extract these features, a parallel English-French corpus is required to build a lexical translation table using GIZA++ (Och and Ney, 2003). We use the Europarl English-French parallel corpus (Koehn, 2005) plus around 1M segments of Symantec translation memory. Table 4 shows the performance of this system (WMT17) on the test set measured by Root Mean Square Error (RMSE) and Pearson correlation coefficient (r). We only report the results on predicting four of the metrics introduced above, omitting HMeteor due to space constraints. C and γ parameters are tuned on the development set with respect to r. The results show a significant difference between manual and human-targeted metric prediction. The higher r for the former suggests that the patterns of these scores are easier to learn. The RMSE se</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceedings: the tenth Machine Translation Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LDC</author>
</authors>
<title>Linguistic data annotation specification: Assessment of fluency and adequacy in chineseenglish translations.</title>
<date>2002</date>
<tech>Technical report.</tech>
<contexts>
<context position="9629" citStr="LDC, 2002" startWordPosition="1517" endWordPosition="1518">kowski and Lavie, 2011), which are the most widely used MT evaluation metrics. Following Snover et al. (2006), we consider this way of scoring MT output to be a variation of humantargeted scoring, where no reference translation is provided to the post-editor, so we call them HBLEU, HTER and HMETEOR. The average scores for the entire data set together with their standard deviations are presented in Table 1.7 In the second method, we asked three professional translators, who are again native French speakers, to assess the quality of MT output in terms of adequacy and fluency in a 5-grade scale (LDC, 2002). The interpretation of the scores is given in Table 2. Each evaluator was given the entire data set for evaluation. We therefore collected three sets of scores and averaged them to obtain the final scores. The averages of these scores for the entire data set together with their standard deviations are presented in Table 1. To be easily comparable to human-targeted scores, we scale these scores to the [0,1] range, i.e. adequacy/fluency scores of 1 and 5 are mapped to 0 and 1 respectively and all the scores in between are accordingly scaled. The average Kappa inter-annotator agreement for adequ</context>
</contexts>
<marker>LDC, 2002</marker>
<rawString>LDC. 2002. Linguistic data annotation specification: Assessment of fluency and adequacy in chineseenglish translations. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>Meant: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility via semantic frames.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies -</booktitle>
<volume>1</volume>
<contexts>
<context position="5525" citStr="Lo and Wu (2011)" startWordPosition="865" endWordPosition="868">sifier are word alignment limitations and low SRL recall due to the lack of a verb or the loss of a predicate during translation. In MT evaluation, where reference translations are available, Gim´enez and M`arquez (2007) use semantic roles in building several MT evaluation metrics which measure the full or partial lexical match between the fillers of same semantic roles in the hypothesis and translation, or simply the role label matches between them. They conclude that these features can only be useful in combination with other features and metrics reflecting different aspects of the quality. Lo and Wu (2011) introduce HMEANT, a manual MT evaluation metric based on predicateargument structure matching which involves two steps of human engagement: 1) semantic role annotation of the reference and machine translation, 2) evaluating the translation of predicates and arguments. The metric calculates the F1 score of the semantic frame match between the reference and machine translation based on this evaluation. To keep the costs reasonable, the first step is carried out by amateur annotators who were minimally trained with a simplified list of 10 thematic roles. On a set of 40 examples, the metric is me</context>
</contexts>
<marker>Lo, Wu, 2011</marker>
<rawString>Chi-kiu Lo and Dekai Wu. 2011. Meant: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility via semantic frames. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Anand Karthik Tumuluru</author>
<author>Dekai Wu</author>
</authors>
<title>Fully automatic semantic mt evaluation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh WMT.</booktitle>
<contexts>
<context position="6288" citStr="Lo et al. (2012)" startWordPosition="991" endWordPosition="994">antic role annotation of the reference and machine translation, 2) evaluating the translation of predicates and arguments. The metric calculates the F1 score of the semantic frame match between the reference and machine translation based on this evaluation. To keep the costs reasonable, the first step is carried out by amateur annotators who were minimally trained with a simplified list of 10 thematic roles. On a set of 40 examples, the metric is meta-evaluated in terms of correlation with human judgements of translation adequacy ranking, and a correlation as high as that of HTER is reported. Lo et al. (2012) propose MEANT, a variant of HMEANT, which automatizes its manual steps using 1) automatic SRL systems for (only) verb predicates, 2) automatic alignment of predicates and their arguments in the reference and machine translation based on their lexical similarity. Once the predicates and arguments are aligned, their similarities are measured using a variety of methods such as cosine distance and even Meteor and BLEU. In computation of the final score, the similarity scores replace the counts of correct and partial translations used in HMEANT. This metric outperforms several automatic metrics in</context>
</contexts>
<marker>Lo, Tumuluru, Wu, 2012</marker>
<rawString>Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. 2012. Fully automatic semantic mt evaluation. In Proceedings of the Seventh WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Meriem Beloucif</author>
<author>Markus Saers</author>
<author>Dekai Wu</author>
</authors>
<title>Xmeant: Better semantic mt evaluation without reference translations.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</booktitle>
<volume>2</volume>
<institution>Short Papers),</institution>
<contexts>
<context position="7453" citStr="Lo et al. (2014)" startWordPosition="1180" endWordPosition="1183">his metric outperforms several automatic metrics including BLEU, Meteor and TER, but it significantly under-performs HMEANT and HTER. Further analysis shows that automatizing the second step does not affect the performance of MEANT. Therefore, it seems to be the lower accuracy of the semantic role labelling that is responsible. Bojar and Wu (2012) identify a set of flaws with HMEANT and propose solutions for them. The most important problems stem from the superficial SRL annotation guidelines. These problems are exacerbated in MEANT due to the automatic nature of the two steps. More recently, Lo et al. (2014) extend MEANT to ranking translations without a reference by using phrase translation probabilities for aligning semantic role fillers of the source and its translation. 3 Data We randomly select 4500 segments from a large collection of Symantec English Norton forum text.2 In order to be independent of any one MT system, we translate these segments into French with the following three systems and randomly choose 1500 distinct segments from each. • ACCEPT3: a phrase-based Moses system trained on training sets of WMT12 releases of Europarl and News Commentary plus Symantec translation memories •</context>
</contexts>
<marker>Lo, Beloucif, Saers, Wu, 2014</marker>
<rawString>Chi-kiu Lo, Meriem Beloucif, Markus Saers, and Dekai Wu. 2014. Xmeant: Better semantic mt evaluation without reference translations. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs M`arquez</author>
<author>Xavier Carreras</author>
<author>Kenneth C Litkowski</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Semantic role labeling: An introduction to the special issue.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>2</issue>
<marker>M`arquez, Carreras, Litkowski, Stevenson, 2008</marker>
<rawString>Llu´ıs M`arquez, Xavier Carreras, Kenneth C. Litkowski, and Suzanne Stevenson. 2008. Semantic role labeling: An introduction to the special issue. Comput. Linguist., 34(2):145–159, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Tree kernel engineering for proposition re-ranking.</title>
<date>2006</date>
<booktitle>In Proceedings of Mining and Learning with Graphs (MLG).</booktitle>
<contexts>
<context position="18999" citStr="Moschitti et al. (2006)" startWordPosition="3077" endWordPosition="3080">the baseline for HBLEU and adequacy prediction.12 It should be noted that in our previous work, a WMT News data set was used as the QE data set which, unlike our new data set, is well-formed and in the same domain as the parsers’ training data. The discrepancy between our new and old results suggests that the performance is strongly dependent on the data set. Unlike syntactic parsing, semantic role labelling does not produce a tree to be directly used in the tree kernel framework. There can be various ways to accomplish this goal. We first try a method inspired by the PAS format introduced by Moschitti et al. (2006). In this format, a fixed number of nodes are gathered under a dummy root node as slots of one predicate and 6 arguments of a proposition (one tree per predicate). Each node dominates an argument label or a dummy label for the predicate, which in turn dominates the POS tag of the argument or the predicate lemma. If a proposition has more than 6 arguments they are ignored, if it has fewer than 6 arguments, the extra slots are attached to a dummy null label. Note that these trees are derived from the dependency-based SRL of both the source and target side (Figure 11http://disi.unitn.it/moschitti</context>
<context position="20412" citStr="Moschitti et al. (2006)" startWordPosition="3301" endWordPosition="3304">8 0.2431 D-PAS 0.2489 0.2856 0.2423 0.2652 D-PST 0.2409 0.2815 0.2383 0.2606 C-PST 0.2400 0.2809 0.2410 0.2615 CD-PST 0.2394 0.2795 0.2373 0.2578 TKSSQE 0.2269 0.2722 0.2253 0.2425 Pearson r WMT17 0.3661 0.3806 0.4710 0.4769 TKSyQE 0.3693 0.3559 0.4306 0.5013 D-PAS 0.1774 0.1843 0.2770 0.3252 D-PST 0.2136 0.2450 0.3169 0.3670 C-PST 0.2319 0.2541 0.2966 0.3616 CD-PST 0.2311 0.2714 0.3303 0.3923 TKSSQE 0.3682 0.3537 0.4351 0.5046 Table 4: RMSE and Pearson r of the 17 baseline features (WMT17) and tree kernel systems; TKSyQE: syntax-based tree kernels, D-PAS: dependency-based PAS tree kernels of Moschitti et al. (2006), D-PST, C-PST and CD-PST: dependency-based, constituency-based proposition subtree kernels and their combination, TKSSQE: syntactic-semantic tree kernels 1(a)). The results are shown in Table 4 (D-PAS). The performance is statistically significantly lower than the baseline.13 In order to encode more information in the trees, we propose another format in which proposition subtrees (PST) of the sentence are gathered under a dummy root node. A dependency PST (Figure 1(b)) is formed by the predicate label under the root dominating its lemma and all its arguments roles. Each of these nodes in turn</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2006</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2006. Tree kernel engineering for proposition re-ranking. In Proceedings of Mining and Learning with Graphs (MLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Making tree kernels practical for natural language learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="17430" citStr="Moschitti, 2006" startWordPosition="2813" endWordPosition="2814">are tuned on the development set with respect to r. The results show a significant difference between manual and human-targeted metric prediction. The higher r for the former suggests that the patterns of these scores are easier to learn. The RMSE seems to follow the standard deviation 9Although the English SRL data are annotated for noun predicates as well as verb predicates, since the French data has only verb predicate annotations, we only consider verb predicates for English. 10http://svmlight.joachims.org/ 70 of the scores as the same ranking is seen in both. 6 Tree Kernels Tree kernels (Moschitti, 2006) have been successfully used in QE by Hardmeier et al. (2012) and in our previous work (Kaljahi et al., 2013; Kaljahi et al., 2014b), where syntactic trees are employed. Tree kernels eliminate the burden of manual feature engineering by efficiently utilizing all subtrees of a tree. We employ both syntactic and semantic information in learning quality scores, using the SVMLight-TK11, a support vector machine (SVM) implementation of tree kernels. We implement a syntactic tree kernel QE system with constituency and dependency trees of the source and target side, following our previous work (Kalja</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Making tree kernels practical for natural language learning. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="16398" citStr="Och and Ney, 2003" startWordPosition="2643" endWordPosition="2646">133, 8875 and 8795 propositions on its source side, post-edits and MT output respectively. 5 Baseline We compare the results of our experiments to a baseline built using the 17 baseline features of the WMT QE shared task (Bojar et al., 2014). These features provide a strong baseline and have been used in all three years of the shared task. We use support vector regression implemented in the SVMLight toolkit10 with Radial Basis Function (RBF) kernel to build this baseline. To extract these features, a parallel English-French corpus is required to build a lexical translation table using GIZA++ (Och and Ney, 2003). We use the Europarl English-French parallel corpus (Koehn, 2005) plus around 1M segments of Symantec translation memory. Table 4 shows the performance of this system (WMT17) on the test set measured by Root Mean Square Error (RMSE) and Pearson correlation coefficient (r). We only report the results on predicting four of the metrics introduced above, omitting HMeteor due to space constraints. C and γ parameters are tuned on the development set with respect to r. The results show a significant difference between manual and human-targeted metric prediction. The higher r for the former suggests </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Comput. Linguist., 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="12963" citStr="Palmer et al., 2005" startWordPosition="2062" endWordPosition="2065">output is low or high, the other tends to be the same. The data is split into train, development and test sets of 3000, 500 and 1000 sentences respectively. 4 Semantic Role Labelling The type of semantic information we use in this work is the predicate-argument structure or semantic role labelling of the sentence. This information needs to be extracted from both sides of the translation, i.e. English and French. Though the SRL of English has been well-studied (M`arquez et al., 2008) thanks to the existence of two major hand-crafted resources, namely FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), French is one of the under-studied languages in 69 1-HTER HBLEU HMeteor Adequacy Fluency Average 0.6976 0.5517 0.7221 0.6230 0.4096 Standard Deviation 0.2446 0.2927 0.2129 0.2488 0.2780 Table 1: Average and standard deviation of the evaluation scores for the entire data set this respect mainly due to a lack of such resources. The only available gold standard resource is a small set of 1000 sentences taken from Europarl (Koehn, 2005) and manually annotated with Propbank verb predicates (van der Plas et al., 2010). van der Plas et al. (2011) attempt to tackle this scarcity by automatically pro</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="8976" citStr="Papineni et al., 2002" startWordPosition="1403" endWordPosition="1406">//www.accept.unige.ch/Products/ D_4_1_Baseline_MT_systems.pdf 4http://www.bing.com/translator(on24- Feb-2014) 68 5 4 3 2 1 All meaning Most of meaning Much of meaning Little meaning None of meaning Fluency Flawless Language Good Language Non-native Language Disfluent Language Incomprehensible 1-HTER HBLEU HMeteor Adq Flu 1-HTER - - - - - HBLEU 0.9111 - - - - HMeteor 0.9207 0.9314 - - - Adq 0.6632 0.7049 0.6843 - - Flu 0.6447 0.7213 0.6652 0.8824 - Adequacy Table 2: Adequacy/fluency score interpretation French speaker.5 Each sentence translation is then scored against its post-edit using BLEU6(Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011), which are the most widely used MT evaluation metrics. Following Snover et al. (2006), we consider this way of scoring MT output to be a variation of humantargeted scoring, where no reference translation is provided to the post-editor, so we call them HBLEU, HTER and HMETEOR. The average scores for the entire data set together with their standard deviations are presented in Table 1.7 In the second method, we asked three professional translators, who are again native French speakers, to assess the quality of MT output in terms o</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniele Pighin</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Automatic projection of semantic structures: An application to pairwise translation ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<pages>1--9</pages>
<marker>Pighin, M`arquez, 2011</marker>
<rawString>Daniele Pighin and Llu´ıs M`arquez. 2011. Automatic projection of semantic structures: An application to pairwise translation ranking. In Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
</authors>
<title>Training a sentence-level machine translation confidence measure.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="3622" citStr="Quirk, 2004" startWordPosition="568" endWordPosition="569">is organized as follows: after reviewing the related work, the data is described and the semantic role labelling approach is explained. The baseline is then introduced, followed by the experiments with tree kernels, handcrafted features, the PAM metric and finally the combination of all methods. The paper ends with a summary and suggestions for future work. 2 Related Work Syntax has been exploited in QE in various ways including tree kernels (Hardmeier et al., 2012; Kaljahi et al., 2013; Kaljahi et al., 2014b), parse probabilities and syntactic label frequency (Avramidis, 2012), parseability (Quirk, 2004) and POS n-gram scores (Specia and Gim´enez, 2010). 1The data will be made publicly available -see http:// www.computing.dcu.ie/mt/confidentmt.html 67 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67–77, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Turning to the role of semantic knowledge in QE and MT evaluation in general, Pighin and M`arquez (2011) propose a method for ranking two translation hypotheses that exploits the projection of SRL from a sentence to its translation using word alignments.</context>
</contexts>
<marker>Quirk, 2004</marker>
<rawString>Chris Quirk. 2004. Training a sentence-level machine translation confidence measure. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="9003" citStr="Snover et al., 2006" startWordPosition="1408" endWordPosition="1411">s/ D_4_1_Baseline_MT_systems.pdf 4http://www.bing.com/translator(on24- Feb-2014) 68 5 4 3 2 1 All meaning Most of meaning Much of meaning Little meaning None of meaning Fluency Flawless Language Good Language Non-native Language Disfluent Language Incomprehensible 1-HTER HBLEU HMeteor Adq Flu 1-HTER - - - - - HBLEU 0.9111 - - - - HMeteor 0.9207 0.9314 - - - Adq 0.6632 0.7049 0.6843 - - Flu 0.6447 0.7213 0.6652 0.8824 - Adequacy Table 2: Adequacy/fluency score interpretation French speaker.5 Each sentence translation is then scored against its post-edit using BLEU6(Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011), which are the most widely used MT evaluation metrics. Following Snover et al. (2006), we consider this way of scoring MT output to be a variation of humantargeted scoring, where no reference translation is provided to the post-editor, so we call them HBLEU, HTER and HMETEOR. The average scores for the entire data set together with their standard deviations are presented in Table 1.7 In the second method, we asked three professional translators, who are again native French speakers, to assess the quality of MT output in terms of adequacy and fluency in a</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Jes´us Gim´enez</author>
</authors>
<title>Combining confidence estimation and reference-based metrics for segment level MT evaluation.</title>
<date>2010</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<marker>Specia, Gim´enez, 2010</marker>
<rawString>Lucia Specia and Jes´us Gim´enez. 2010. Combining confidence estimation and reference-based metrics for segment level MT evaluation. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The conll-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning.</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´ıs M`arquez, and Joakim Nivre. 2008. The conll-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the Twelfth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke van der Plas</author>
<author>Tanja Samardˇzi´c</author>
<author>Paola Merlo</author>
</authors>
<title>Cross-lingual validity of propbank in the manual annotation of french.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth Linguistic Annotation Workshop, LAW IV ’10.</booktitle>
<marker>van der Plas, Samardˇzi´c, Merlo, 2010</marker>
<rawString>Lonneke van der Plas, Tanja Samardˇzi´c, and Paola Merlo. 2010. Cross-lingual validity of propbank in the manual annotation of french. In Proceedings of the Fourth Linguistic Annotation Workshop, LAW IV ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke van der Plas</author>
<author>Paola Merlo</author>
<author>James Henderson</author>
</authors>
<title>Scaling up automatic cross-lingual semantic role annotation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<marker>van der Plas, Merlo, Henderson, 2011</marker>
<rawString>Lonneke van der Plas, Paola Merlo, and James Henderson. 2011. Scaling up automatic cross-lingual semantic role annotation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
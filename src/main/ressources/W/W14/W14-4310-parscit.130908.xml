<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021981">
<title confidence="0.996918">
User Modeling by Using Bag-of-Behaviors for Building a Dialog System
Sensitive to the Interlocutor’s Internal State
</title>
<author confidence="0.990438">
Yuya Chiba, Takashi Nose, Akinori Ito Masashi Ito
</author>
<affiliation confidence="0.9338815">
Graduate School of Engineering, Faculty of Engineering
Tohoku University, Japan Tohoku Institute of Technology, Japan
</affiliation>
<sectionHeader confidence="0.977962" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999917954545455">
When using spoken dialog systems in ac-
tual environments, users sometimes aban-
don the dialog without making any in-
put utterance. To help these users before
they give up, the system should know why
they could not make an utterance. Thus,
we have examined a method to estimate
the state of a dialog user by capturing the
user’s non-verbal behavior even when the
user’s utterance is not observed. The pro-
posed method is based on vector quan-
tization of multi-modal features such as
non-verbal speech, feature points of the
face, and gaze. The histogram of the VQ
code is used as a feature for determining
the state. We call this feature “the Bag-
of-Behaviors.” According to the experi-
mental results, we prove that the proposed
method surpassed the results of conven-
tional approaches and discriminated the
target user’s states with an accuracy of
more than 70%.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954105263158">
Spoken dialog systems have an advantage of be-
ing a natural interface since speech commands are
less subject to the physical constraints imposed by
devices. On the other hand, if the system accepts
only a limited expression, the user need to learn
how to use the system. If the user is not familiar
with the system, he/she cannot even make an in-
put utterance. Not all users are motivated to con-
verse with the system in actual environments, and
sometimes a user will abandon the dialog with-
out making any input utterance. When the user
has difficulty to make the utterance, conventional
systems just repeat the prompt at fixed interval
(Yankelovich, 1996) or taking the initiative in the
dialog to complete the task (Chung, 2004; Bo-
hus and Rudnicky, 2009). However, we think that
the system has to cope with the user’s implicit re-
quests to help the user more adequately. To solve
this problem, Chiba and Ito (2012) proposed a
method to estimate two “user’s states” by captur-
ing their non-verbal cues. Here, the state A is
when the user does not know what to input, and
the state B is when the user is considering how to
answer the system’s prompt. These states have not
been distinguished by the conventional dialog sys-
tems so far, but should be handled differently.
The researchers of spoken dialog systems have
focused on the various internal states of users
such as emotion (Forbes-Riley and Litman, 2011a;
Metallinou et al., 2012), preference (Pargellis et
al., 2004) and familiarity with the system (Jokinen
and Kanto, 2004; Rosis et al., 2006) to build natu-
ral dialog system. In particular, the user’s “uncer-
tainty” is assumed to be the nearest user’s states
that we wish to study. Forbes-Riley and Litman
(2011b) and Pon-Barry et al. (2005) introduced a
framework for estimating the user’s uncertainty to
a tutor system.
The above-mentioned researches have a cer-
tain result by employing linguistic information
for the estimation, but it remains difficult to as-
sist a user who does not make any input utter-
ance. By contrast, the method by Chiba and Ito
(2012) estimated the target user’s state by only
using the user’s non-verbal information. In their
work, the user’s multi-modal behaviors were de-
fined empirically, and the labels of the behaviors
were annotated manually. Based on this result, the
present paper proposes the method that does not
use manually-defined labels nor manual annota-
tion. The multi-modal behaviors are determined
automatically using the vector quantization, and
the frequency distribution of the VQ code is used
for estimation of the user’s state. Because this ap-
proach expects to construct clusters of the speech
events or behaviors of the user, we called it as Bag-
of-Behaviors approach.
</bodyText>
<sectionHeader confidence="0.95365" genericHeader="method">
2 Data collection
</sectionHeader>
<bodyText confidence="0.99960025">
The experimental data (video clips) were the same
as those used in the experiment by Chiba et al.
(Chiba and Ito, 2012; Chiba et al., 2012). The
video clips contained the frontal image of the user
</bodyText>
<page confidence="0.979114">
74
</page>
<bodyText confidence="0.967996363636364">
Proceedings of the SIGDIAL 2014 Conference, pages 74–78,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
and their speech, which were recorded with a web
camera and a lapel microphone, respectively. The
task of the dialog was a question-and-answer task
to ask users to answer common knowledge or
a number they remembered in advance, such as
“Please input your ID.” 16 users (14 males and 2
females) participated in the dialog collection.
Recorded clips were divided into sessions,
where one session included one interchange of the
system’s prompt and the user’s response. The total
number of sessions was 792. Then we employed
evaluators to label each video clip as either state A,
B or C, where state A and B were that described in
the previous section, and state C is the state where
the user had no problem answering the system. We
took the majority vote of the evaluators’ decisions
to determine the final label of a clip. Fleiss’ κ
among the evaluators was 0.22 (fair agreement).
Finally, we obtained 59, 195 and 538 sessions of
state A, B and C, respectively.
</bodyText>
<sectionHeader confidence="0.995496" genericHeader="method">
3 Discrimination method by using
</sectionHeader>
<subsectionHeader confidence="0.809882">
Bag-of-Behaviors
</subsectionHeader>
<bodyText confidence="0.999831310344828">
In the work of Chiba et al. (2013), the user’s
state was determined using the labels of the multi-
modal events such as fillers or face orientation,
which were estimated from the low-level acoustic
and visual features.
Here, inventory of multi-modal events was de-
termined empirically. There were, however, two
problems with this method. The first one was that
the optimality of the inventory was not guaran-
teed. The second one is that it was difficult to esti-
mate the events from the low-level features, which
made the final decision more difficult. Therefore,
we propose a new method for discriminating the
user’s state using automatically-determined events
obtained by the vector quantization.
First, a codebook of the low-level features
(which will be described in detail in the next
section) is created using k-means++ algorithm
(Arthur and Vassilvitskii, 2007). Let a low-level
feature vector at time t of session s of the training
data be x(s)
t . Then we perform the clustering of
the low-level feature vectors for all of t and s, and
create a codebook C = {c1, ... , cK1, where ck
denotes the k-th centroid of the codebook.
Then the input feature vectors are quantized
frame-by-frame using the codebook. When a ses-
sion for evaluation sE is given, we quantize the in-
put low-level feature vectors x(sE)
</bodyText>
<equation confidence="0.930463">
1 , ... , x(sE)
T into
q1, ... , qT, where
||x(sE)
t − cq||. (1)
Then we calculate the histogram Q0(sE) =
(Q1, ... , QK) where
δ(k, qt) (2)
{ δ(x, y) = 1 x = y0 x � y (3)
</equation>
<bodyText confidence="0.998152933333333">
Then Q(sE) = Q0(sE)/||Q0(sE) ||is used as
the feature of the discrimination. The similar fea-
tures based on the vector quantization were used
for image detection and scene analysis (Csurka
et al., 2004; Jiang et al., 2007; Natarajan et al.,
2012) and called “Bag-of-Features” or “Bag-of-
Keypoints.” In our research, each cluster of the
low-level features is expected to represent some
kind of user’s behavior. Therefore, we call the pro-
posed method the “Bag-of-Behaviors” approach.
After calculating the Bag-of-Behaviors, we em-
ploy an appropriate classifier to determine the
user’s state in the given session. In this research,
the support vector machine (SVM) is used as a
classifier.
</bodyText>
<sectionHeader confidence="0.770032" genericHeader="method">
4 The low-level features
</sectionHeader>
<bodyText confidence="0.999991294117647">
In this section, we describe the acoustic and visual
features employed as the low-level features.
The target user’s states are assumed to have sim-
ilar aspects to emotion. Collignon et al. (2008)
suggested that emotion has a multi-modality na-
ture. For example, W¨ollmer et al. (2013) showed
that the acoustic and visual features contributed to
discriminate arousal and expectation, respectively.
Several other researches also have reported that
recognition accuracy of emotion was improved by
combining multi-modal information (Lin et al.,
2012; Wang and Venetsanopoulos, 2012; Paul-
mann and Pell, 2011; Metallinou et al., 2012).
Therefore, we employed similar features as those
used in these previous works, such as the spectral
features and intonation of the speech, and facial
feature points, etc.
</bodyText>
<subsectionHeader confidence="0.99818">
4.1 Audio features
</subsectionHeader>
<bodyText confidence="0.999981727272727">
To represent spectral characteristics of the speech,
MFCC was employed as an acoustic feature. We
used a 39-dimension MFCC including the veloc-
ity and acceleration of the lower 12th-order coef-
ficients and log power. In addition, a differential
component of log F0 was used to represent the
prosodic feature of the speech, and zero cross (ZC)
was used to distinguish voiced and unvoiced seg-
ments. Therefore, total number of audio features
was 3. The basic conditions for extracting each
feature are shown in Table 1. Here, five frames
</bodyText>
<equation confidence="0.979494">
qt = arg min
q
∑T
t=1
Qk =
</equation>
<page confidence="0.960016">
75
</page>
<bodyText confidence="0.99850925">
(the current frame, the two previous frames and
two following frames) were used to calculate the
∆ and ∆∆ components of MFCC and ∆ compo-
nent of log F0.
</bodyText>
<subsectionHeader confidence="0.99555">
4.2 Face feature
</subsectionHeader>
<bodyText confidence="0.999963625">
Face feature (Chiba et al., 2013) was extracted by
the Constraint Local Model (CLM) (Saragih et al.,
2011) frame by frame. The coordinates of the
points relative to the center of the face were used
as the face features. The scale of the feature points
was normalized by the size of the facial region.
The number of feature points was 66 and the di-
mension of the feature was 132.
</bodyText>
<subsectionHeader confidence="0.997996">
4.3 Gaze feature
</subsectionHeader>
<bodyText confidence="0.999944466666667">
The evaluators of the dialogs declared that move-
ment of the user’s eyes seems to express their in-
ternal state. The present paper used the Haar-
like feature which has a fast calculation algo-
rithm using the integral image to represent the
brightness of the user’s eye regions. This feature
was extracted by applying filters comprehensively
changed the size and location to the image (eye
regions in our case). The eye regions were de-
tected by the facial feature points. Because this
feature had large dimensions, the principal com-
ponent analysis (PCA) was conducted to reduce
the dimensionality. Finally, gaze feature had 34 di-
mensions and the cumulative contribution rate was
about 95%.
</bodyText>
<subsectionHeader confidence="0.995225">
4.4 Feature synchronization
</subsectionHeader>
<bodyText confidence="0.9999402">
The audio features were calculated every 10 ms
(see Table 1) while the visual features were ex-
tracted every 33 ms. Therefore, the features were
synchronized by copying the visual features of the
previous frame in every 10 ms.
</bodyText>
<sectionHeader confidence="0.998195" genericHeader="method">
5 Discrimination examination
</sectionHeader>
<subsectionHeader confidence="0.9836695">
5.1 Conditions of the Bag-of-Behaviors
construction
</subsectionHeader>
<bodyText confidence="0.994835875">
We built the Bag-of-Behaviors under two condi-
tions described below.
Let x(s)
at , x(s)
ft and x(s)
et represent the audio fea-
ture, face feature and gaze feature of the session s
at time t, respectively.
</bodyText>
<tableCaption confidence="0.999376">
Table 1: Conditions of audio feature extraction
</tableCaption>
<table confidence="0.855920666666667">
MFCC log F0 ZC
Frame width 25.0 ms 17.0 ms 10.0 ms
Frame shift 10.0 ms 10.0 ms 10.0 ms
</table>
<tableCaption confidence="0.928673">
Table 2: Experimental conditions
</tableCaption>
<table confidence="0.9463516">
# of sessions State A(59), State B(195)
Codebook size K 4, 8, 16, 32, 64
Ka 4, 8, 16, 32, 64
Kf 4, 8, 16, 32, 64
Ke 4, 8, 16, 32, 64
</table>
<bodyText confidence="0.947625">
In Condition (1), the three features are com-
bined to single feature vector x(s)
</bodyText>
<equation confidence="0.880388">
t :
xt = (x(s)
(s) at , x(s)
ft , x(s)
et ) (4)
</equation>
<bodyText confidence="0.9991355">
Then, the low-level feature vectors x(s)
t are clus-
tered to construct one codebook C with size K.
When an input session sE is given, we calculate
the combined feature vector x(sE)
t , and generate
the Bag-of-Behaviors Q(sE). This method is a
kind of the feature-level fusion method.
In Condition (2), the three features are used sep-
arately. First, we generate three codebooks Ca, Cf
and Ce using the audio, face and gaze features, re-
spectively. Size of those codebooks were Ka, Kf
and Ke. When an input session sE is given,
we generate three Bag-of-Behaviors feature vec-
tors Qa(sE), Qf(sE) and Qe(sE) using the three
codebooks. Finally, we combine those features as
</bodyText>
<equation confidence="0.996005">
Q(sE) = (Qa(sE), Qf(sE), Qe(sE)). (5)
</equation>
<subsectionHeader confidence="0.991801">
5.2 Experimental condition
</subsectionHeader>
<bodyText confidence="0.999486866666667">
We employed the SVM with RBF-kernel as a clas-
sifier. The experimental conditions are summa-
rized in Table 2. The hyperparameters of the clas-
sifier were decided by grid-searching. Since the
session of state C and the other states (state A and
state B) were clearly distinguished by the duration
of the session, we used only the session of state
A and state B for the experiments. Hence, each
experiment was a two-class discrimination task.
As explained, the experimental data were un-
balanced. Since it is desirable that the system can
discriminate the user’s state without deviation, the
harmonic mean H of the accuracy of the two states
was used for measuring the performance. This is
calculated by
</bodyText>
<equation confidence="0.995986333333333">
2CACB
H = (6)
CA + CB,
</equation>
<bodyText confidence="0.99981825">
where CA and CB represent the discrimination ac-
curacy of state A and state B, respectively. The ex-
periments were conducted based on a 5-fold cross
validation.
</bodyText>
<page confidence="0.984421">
76
</page>
<figureCaption confidence="0.995403">
Figure 1: Discrimination results of condition (1)
Figure 2: Discrimination results of condition (2)
arranged in descending order
</figureCaption>
<subsectionHeader confidence="0.991437">
5.3 Experimental results
</subsectionHeader>
<bodyText confidence="0.999927875">
The results of condition (1) are shown in Figure
1. The figure shows the best H of each num-
ber of clusters. In condition (1), the best result
(H = 70.0%) was obtained when the number of
clusters K was 64. Figure 2 shows the results of
condition (2). In this figure, the results are shown
in descending order of the harmonic mean for all
combination of codebook size of the three code-
books (there were 53 = 125 conditions). The best
H = 70.7% was obtained when Ka = 8, Kf = 8
and Ke = 64.
The best results of the tested methods are sum-
marized in Table 3. Here, “Baseline + NN” in
the table denotes the result in Chiba et al. (2013),
where the visual events and acoustic events were
annotated manually, and the manual labels were
</bodyText>
<tableCaption confidence="0.999822">
Table 3: Comparison of estimation methods
</tableCaption>
<table confidence="0.997632">
State A State B Harm.
Baseline + NN 52.5 65.1 58.2
Baseline + Gaze + NN 64.5 59.5 61.9
Condition (1) + RBF-SVM 67.9 72.3 70.0
Condition (2) + RBF-SVM 67.7 73.8 70.7
Condition (2) + MKL-SVM 68.0 76.4 72.0
</table>
<bodyText confidence="0.999648805555555">
used as input for a neural network for the classi-
fication. The gaze feature was not used in “Base-
line + NN.” We added the result when including
the gaze feature, shown as “Baseline + Gaze +
NN.” As shown in Table 3, the performance of the
method proposed in this paper surpassed the base-
line methods. Therefore, the proposed method
could not only automatically determine the inven-
tory of the audio-visual events, but also achieved
better discrimination accuracy. One of the reasons
of the improvement is VQ can construct the clus-
ters in proper quantities.
Comparing the two conditions of feature combi-
nation, H of condition (2) (denoted as “Condition
(2) + RBF-SVM”) was slightly higher than that of
condition (1) (denoted as “Condition (1) + RBF-
SVM”). This result was similar to Split-VQ (Pari-
wal and Atal, 1991) where a single feature vec-
tor split into subvectors and the input vector was
quantized subvector by subvector.
We conducted additional experiments for con-
dition (2) by using SVM with combined kernel
trained by Multiple Kernel Learning (MKL) (Son-
nenburg et al., 2006). The combined kernel is rep-
resented as a linear combination of several sub-
kernels. The distinct kernel was employed for
the speech, face feature and gaze feature, respec-
tively. This paper used the RBF-kernel having the
same width as the sub-kernels.The best result was
shown as “Condition (2) + MKL-SVM” in Table 3.
As shown in the table, the MKL-SVM showed the
highest performance of 72.0 %. The weights of the
audio, face and gaze feature were 0.246, 0.005 and
0.749, respectively. This result suggested that the
contribution of the face feature was weaker than
the other features.
</bodyText>
<sectionHeader confidence="0.996145" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999275">
In this paper, we proposed a method to estimate
the state of the user of the dialog system by us-
ing non-verbal features. We proposed the Bag-
of-Behaviors approach, in which the user’s mult-
modal behavior was first classified by vector quan-
tization, and then the histogram of the VQ code
was used as a feature of the discrimination. We
verified that the method could discriminate the tar-
get user’s state with an accuracy of 70% or more.
One of the disadvantages of the current frame-
work is that it requires to observe the session until
just before the user’s input utterance. This prob-
lem makes it difficult to apply this method to an
actual system, because the system has to be able
to evaluate the user’s state successively in order to
help the user at an appropriate timing. Therefore,
we will examine a sequential estimation method
by using the Bag-of-Behaviors in a future work.
</bodyText>
<figure confidence="0.99157295">
State A(CO)
State B(C� )
Harm.(rp
4 8 16 32 64
Number of clusters K
90
80
70
60
50
40
30
75
Harm.(H)
70
65
60
55
500 20 40 60 80 100 120
Order of H
</figure>
<page confidence="0.988574">
77
</page>
<sectionHeader confidence="0.863491" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.98947187628866">
David Arthur and Sergei Vassilvitskii. 2007. k-means++:
The advantages of careful seeding. In Proc. the 18th
annual ACM-SIAM symposium on Discrete algorithms,
pages 1027–1035.
Dan Bohus and Alexander I. Rudnicky. 2009. The raven-
claw dialog management framework: Architecture and
systems. Computer Speech &amp; Language, 23(3):332–361.
Yuya Chiba and Akinori Ito. 2012. Estimating a
user’s internal state before the first input utterance.
Advances in Human-Computer Interaction, 2012:11,
DOI:10.1155/2012/865362, 2012.
Yuya Chiba, Masashi Ito, and Akinori Ito. 2012. Effect of
linguistic contents on human estimation of internal state
of dialog system users. In Proc. Feedback Behaviors in
Dialog, pages 11–14.
Yuya Chiba, Masashi Ito, and Akinori Ito. 2013. Estima-
tion of user’s state during a dialog turn with sequential
multi-modal features. In HCI International 2013-Posters’
Extended Abstracts, pages 572–576.
Grace Chung. 2004. Developing a flexible spoken dialog
system using simulation. In Proc. the 42nd Annual Meet-
ing on Association for Computational Linguistics, pages
63–70.
Olivier Collignon, Simon Girard, Frederic Gosselin, Syl-
vain Roy, Dave Saint-Amour, Maryse Lassonde, and Lep-
ore Franco. 2008. Audio-visual integration of emotion
expression. Brain research, 1242:126–135.
Gabriella Csurka, Christopher Dance, Lixin Fan,
Jutta Willamowski, and C´edric Bray. 2004. Visual
categorization with bags of keypoints. In Proc. workshop
on statistical learning in computer vision, ECCV, pages
1–2.
Kate Forbes-Riley and Diane Litman. 2011a. Benefits and
challenges of real-time uncertainty detection and adapta-
tion in a spoken dialogue computer tutor. Speech Commu-
nication, 53:1115–1136.
Kate Forbes-Riley and Diane Litman. 2011b. Designing
and evaluating a wizarded uncertainty-adaptive spoken di-
alogue tutoring system. Computer Speech &amp; Language,
25(1):105–126.
Yu-Gang Jiang, Chong-Wah Ngo, and Jun Yang. 2007. To-
wards optimal bag-of-features for object categorization
and semantic video retrieval. In Proc. of the 6th ACM
international conference on Image and video retrieval,
pages 494–501.
Kristiina Jokinen and Kari Kanto. 2004. User expertise mod-
elling and adaptivity in a speech-based e-mail system. In
Proc. the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 88–95.
Jen-Chun Lin, Chung-Hsien Wu, and Wen-Li Wei. 2012.
Error weighted semi-coupled hidden markov model for
audio-visual emotion recognition. IEEE Trans. Multime-
dia, 14(1):142–156.
Angeliki Metallinou, Martin W¨ollmer, Athanasios Kat-
samanis, Florian Eyben, Bj¨orn Schuller, and
Shrikanth Narayanan. 2012. Context-sensitive learning
for enhanced audiovisual emotion classification. IEEE
Trans. Affective Computing, 3(2):184–198.
Pradeep Natarajan, Shuang Wu, Shiv Vitaladevuni, Xiao-
dan Zhuang, Stavros Tsakalidis, and Unsang Park, Ro-
hit Prasad, and Premkumar Natarajan. 2012. Multimodal
feature fusion for robust event detection in web videos.
In Proc. Computer Vision and Pattern Recognition, pages
1298–1305.
Andrew Pargellis, Hong-Kwang Jeff Kuo, and Chin-Hui Lee.
2004. An automatic dialogue generation platform for per-
sonalized dialogue applications. Speech Communication,
42:329–351.
Kuldip Paliwal and Bishnu Atal. 1993. Efficient vector quan-
tization of lpc parameters at 24 bits/frame. In IEEE Trans.
Speech and Audio Processing, 1(1):3–14.
Silke Paulmann and Marc Pell. 2011. Is there an advantage
for recognizing multi-modal emotional stimuli? Motiva-
tion and Emotion, 35(2):192–201.
Heather Pon-Barry, Karl Schultz, Elizabeth Owen Bratt,
Brady Clark, and Stanley Peters. 2005. Responding to
student uncertainty in spoken tutorial dialogue systems.
Int. J. Artif. Intell. Edu., 16:171–194.
Fiorella Rosis, Nicole Novielli, Valeria Carofiglio, Addo-
lorata Cavalluzzi, and Berardina Carolis. 2006. User
modeling and adaptation in health promotion dialogs
with an animated character. J. Biomedical Informatics,
39:514–531.
Jason Saragih, Simon Lucey, and Jeffrey Cohn. 2011. De-
formable model fitting by regularized landmark mean-
shift. Int. J. Computer Vision, 91(2):200–215.
Yongjin Wang and Anastasios Venetsanopoulos. 2012. Ker-
nel cross-modal factor analysis for information fusion
with application to bimodal emotion recognition. IEEE
Trans. Multimedia, 14(3):597–607.
Martin W¨ollmer, Moritz Kaiser, Florian Eyben,
Bj¨orn Schuller, and Gerhard Rigoll. 2013. Lstm-
modeling of continuous emotions in an audiovisual affect
recognition framework. Image and Vision Computing,
31(2):153–163.
Nicole Yankelovich. 1996. How do users know what to say?
Interactions, 3(6):32–43.
</reference>
<page confidence="0.998823">
78
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.524337">
<title confidence="0.9950985">User Modeling by Using Bag-of-Behaviors for Building a Dialog Sensitive to the Interlocutor’s Internal State</title>
<author confidence="0.981045">Yuya Chiba</author>
<author confidence="0.981045">Takashi Nose</author>
<author confidence="0.981045">Akinori Ito Masashi Ito</author>
<affiliation confidence="0.771134">Graduate School of Engineering, Faculty of Engineering Tohoku University, Japan Tohoku Institute of Technology, Japan</affiliation>
<abstract confidence="0.999350695652174">When using spoken dialog systems in actual environments, users sometimes abandon the dialog without making any input utterance. To help these users before they give up, the system should know why they could not make an utterance. Thus, we have examined a method to estimate the state of a dialog user by capturing the user’s non-verbal behavior even when the user’s utterance is not observed. The proposed method is based on vector quantization of multi-modal features such as non-verbal speech, feature points of the face, and gaze. The histogram of the VQ code is used as a feature for determining the state. We call this feature “the Bagof-Behaviors.” According to the experimental results, we prove that the proposed method surpassed the results of conventional approaches and discriminated the target user’s states with an accuracy of more than 70%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Arthur</author>
<author>Sergei Vassilvitskii</author>
</authors>
<title>k-means++: The advantages of careful seeding.</title>
<date>2007</date>
<booktitle>In Proc. the 18th annual ACM-SIAM symposium on Discrete algorithms,</booktitle>
<pages>1027--1035</pages>
<contexts>
<context position="6120" citStr="Arthur and Vassilvitskii, 2007" startWordPosition="1007" endWordPosition="1010"> of multi-modal events was determined empirically. There were, however, two problems with this method. The first one was that the optimality of the inventory was not guaranteed. The second one is that it was difficult to estimate the events from the low-level features, which made the final decision more difficult. Therefore, we propose a new method for discriminating the user’s state using automatically-determined events obtained by the vector quantization. First, a codebook of the low-level features (which will be described in detail in the next section) is created using k-means++ algorithm (Arthur and Vassilvitskii, 2007). Let a low-level feature vector at time t of session s of the training data be x(s) t . Then we perform the clustering of the low-level feature vectors for all of t and s, and create a codebook C = {c1, ... , cK1, where ck denotes the k-th centroid of the codebook. Then the input feature vectors are quantized frame-by-frame using the codebook. When a session for evaluation sE is given, we quantize the input low-level feature vectors x(sE) 1 , ... , x(sE) T into q1, ... , qT, where ||x(sE) t − cq||. (1) Then we calculate the histogram Q0(sE) = (Q1, ... , QK) where δ(k, qt) (2) { δ(x, y) = 1 x </context>
</contexts>
<marker>Arthur, Vassilvitskii, 2007</marker>
<rawString>David Arthur and Sergei Vassilvitskii. 2007. k-means++: The advantages of careful seeding. In Proc. the 18th annual ACM-SIAM symposium on Discrete algorithms, pages 1027–1035.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Bohus</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>The ravenclaw dialog management framework: Architecture and systems.</title>
<date>2009</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1916" citStr="Bohus and Rudnicky, 2009" startWordPosition="313" endWordPosition="317">traints imposed by devices. On the other hand, if the system accepts only a limited expression, the user need to learn how to use the system. If the user is not familiar with the system, he/she cannot even make an input utterance. Not all users are motivated to converse with the system in actual environments, and sometimes a user will abandon the dialog without making any input utterance. When the user has difficulty to make the utterance, conventional systems just repeat the prompt at fixed interval (Yankelovich, 1996) or taking the initiative in the dialog to complete the task (Chung, 2004; Bohus and Rudnicky, 2009). However, we think that the system has to cope with the user’s implicit requests to help the user more adequately. To solve this problem, Chiba and Ito (2012) proposed a method to estimate two “user’s states” by capturing their non-verbal cues. Here, the state A is when the user does not know what to input, and the state B is when the user is considering how to answer the system’s prompt. These states have not been distinguished by the conventional dialog systems so far, but should be handled differently. The researchers of spoken dialog systems have focused on the various internal states of </context>
</contexts>
<marker>Bohus, Rudnicky, 2009</marker>
<rawString>Dan Bohus and Alexander I. Rudnicky. 2009. The ravenclaw dialog management framework: Architecture and systems. Computer Speech &amp; Language, 23(3):332–361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuya Chiba</author>
<author>Akinori Ito</author>
</authors>
<title>Estimating a user’s internal state before the first input utterance.</title>
<date>2012</date>
<booktitle>Advances in Human-Computer Interaction, 2012:11, DOI:10.1155/2012/865362,</booktitle>
<contexts>
<context position="2075" citStr="Chiba and Ito (2012)" startWordPosition="343" endWordPosition="346">iliar with the system, he/she cannot even make an input utterance. Not all users are motivated to converse with the system in actual environments, and sometimes a user will abandon the dialog without making any input utterance. When the user has difficulty to make the utterance, conventional systems just repeat the prompt at fixed interval (Yankelovich, 1996) or taking the initiative in the dialog to complete the task (Chung, 2004; Bohus and Rudnicky, 2009). However, we think that the system has to cope with the user’s implicit requests to help the user more adequately. To solve this problem, Chiba and Ito (2012) proposed a method to estimate two “user’s states” by capturing their non-verbal cues. Here, the state A is when the user does not know what to input, and the state B is when the user is considering how to answer the system’s prompt. These states have not been distinguished by the conventional dialog systems so far, but should be handled differently. The researchers of spoken dialog systems have focused on the various internal states of users such as emotion (Forbes-Riley and Litman, 2011a; Metallinou et al., 2012), preference (Pargellis et al., 2004) and familiarity with the system (Jokinen a</context>
<context position="4019" citStr="Chiba and Ito, 2012" startWordPosition="664" endWordPosition="667">f the behaviors were annotated manually. Based on this result, the present paper proposes the method that does not use manually-defined labels nor manual annotation. The multi-modal behaviors are determined automatically using the vector quantization, and the frequency distribution of the VQ code is used for estimation of the user’s state. Because this approach expects to construct clusters of the speech events or behaviors of the user, we called it as Bagof-Behaviors approach. 2 Data collection The experimental data (video clips) were the same as those used in the experiment by Chiba et al. (Chiba and Ito, 2012; Chiba et al., 2012). The video clips contained the frontal image of the user 74 Proceedings of the SIGDIAL 2014 Conference, pages 74–78, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics and their speech, which were recorded with a web camera and a lapel microphone, respectively. The task of the dialog was a question-and-answer task to ask users to answer common knowledge or a number they remembered in advance, such as “Please input your ID.” 16 users (14 males and 2 females) participated in the dialog collection. Recorded clips were divided into session</context>
</contexts>
<marker>Chiba, Ito, 2012</marker>
<rawString>Yuya Chiba and Akinori Ito. 2012. Estimating a user’s internal state before the first input utterance. Advances in Human-Computer Interaction, 2012:11, DOI:10.1155/2012/865362, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuya Chiba</author>
<author>Masashi Ito</author>
<author>Akinori Ito</author>
</authors>
<title>Effect of linguistic contents on human estimation of internal state of dialog system users.</title>
<date>2012</date>
<booktitle>In Proc. Feedback Behaviors in Dialog,</booktitle>
<pages>11--14</pages>
<contexts>
<context position="4040" citStr="Chiba et al., 2012" startWordPosition="668" endWordPosition="671">annotated manually. Based on this result, the present paper proposes the method that does not use manually-defined labels nor manual annotation. The multi-modal behaviors are determined automatically using the vector quantization, and the frequency distribution of the VQ code is used for estimation of the user’s state. Because this approach expects to construct clusters of the speech events or behaviors of the user, we called it as Bagof-Behaviors approach. 2 Data collection The experimental data (video clips) were the same as those used in the experiment by Chiba et al. (Chiba and Ito, 2012; Chiba et al., 2012). The video clips contained the frontal image of the user 74 Proceedings of the SIGDIAL 2014 Conference, pages 74–78, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics and their speech, which were recorded with a web camera and a lapel microphone, respectively. The task of the dialog was a question-and-answer task to ask users to answer common knowledge or a number they remembered in advance, such as “Please input your ID.” 16 users (14 males and 2 females) participated in the dialog collection. Recorded clips were divided into sessions, where one session </context>
</contexts>
<marker>Chiba, Ito, Ito, 2012</marker>
<rawString>Yuya Chiba, Masashi Ito, and Akinori Ito. 2012. Effect of linguistic contents on human estimation of internal state of dialog system users. In Proc. Feedback Behaviors in Dialog, pages 11–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuya Chiba</author>
<author>Masashi Ito</author>
<author>Akinori Ito</author>
</authors>
<title>Estimation of user’s state during a dialog turn with sequential multi-modal features.</title>
<date>2013</date>
<booktitle>In HCI International 2013-Posters’ Extended Abstracts,</booktitle>
<pages>572--576</pages>
<contexts>
<context position="5291" citStr="Chiba et al. (2013)" startWordPosition="878" endWordPosition="881">system’s prompt and the user’s response. The total number of sessions was 792. Then we employed evaluators to label each video clip as either state A, B or C, where state A and B were that described in the previous section, and state C is the state where the user had no problem answering the system. We took the majority vote of the evaluators’ decisions to determine the final label of a clip. Fleiss’ κ among the evaluators was 0.22 (fair agreement). Finally, we obtained 59, 195 and 538 sessions of state A, B and C, respectively. 3 Discrimination method by using Bag-of-Behaviors In the work of Chiba et al. (2013), the user’s state was determined using the labels of the multimodal events such as fillers or face orientation, which were estimated from the low-level acoustic and visual features. Here, inventory of multi-modal events was determined empirically. There were, however, two problems with this method. The first one was that the optimality of the inventory was not guaranteed. The second one is that it was difficult to estimate the events from the low-level features, which made the final decision more difficult. Therefore, we propose a new method for discriminating the user’s state using automatic</context>
<context position="9027" citStr="Chiba et al., 2013" startWordPosition="1505" endWordPosition="1508">and acceleration of the lower 12th-order coefficients and log power. In addition, a differential component of log F0 was used to represent the prosodic feature of the speech, and zero cross (ZC) was used to distinguish voiced and unvoiced segments. Therefore, total number of audio features was 3. The basic conditions for extracting each feature are shown in Table 1. Here, five frames qt = arg min q ∑T t=1 Qk = 75 (the current frame, the two previous frames and two following frames) were used to calculate the ∆ and ∆∆ components of MFCC and ∆ component of log F0. 4.2 Face feature Face feature (Chiba et al., 2013) was extracted by the Constraint Local Model (CLM) (Saragih et al., 2011) frame by frame. The coordinates of the points relative to the center of the face were used as the face features. The scale of the feature points was normalized by the size of the facial region. The number of feature points was 66 and the dimension of the feature was 132. 4.3 Gaze feature The evaluators of the dialogs declared that movement of the user’s eyes seems to express their internal state. The present paper used the Haarlike feature which has a fast calculation algorithm using the integral image to represent the b</context>
<context position="13440" citStr="Chiba et al. (2013)" startWordPosition="2289" endWordPosition="2292"> of condition (1) are shown in Figure 1. The figure shows the best H of each number of clusters. In condition (1), the best result (H = 70.0%) was obtained when the number of clusters K was 64. Figure 2 shows the results of condition (2). In this figure, the results are shown in descending order of the harmonic mean for all combination of codebook size of the three codebooks (there were 53 = 125 conditions). The best H = 70.7% was obtained when Ka = 8, Kf = 8 and Ke = 64. The best results of the tested methods are summarized in Table 3. Here, “Baseline + NN” in the table denotes the result in Chiba et al. (2013), where the visual events and acoustic events were annotated manually, and the manual labels were Table 3: Comparison of estimation methods State A State B Harm. Baseline + NN 52.5 65.1 58.2 Baseline + Gaze + NN 64.5 59.5 61.9 Condition (1) + RBF-SVM 67.9 72.3 70.0 Condition (2) + RBF-SVM 67.7 73.8 70.7 Condition (2) + MKL-SVM 68.0 76.4 72.0 used as input for a neural network for the classification. The gaze feature was not used in “Baseline + NN.” We added the result when including the gaze feature, shown as “Baseline + Gaze + NN.” As shown in Table 3, the performance of the method proposed i</context>
</contexts>
<marker>Chiba, Ito, Ito, 2013</marker>
<rawString>Yuya Chiba, Masashi Ito, and Akinori Ito. 2013. Estimation of user’s state during a dialog turn with sequential multi-modal features. In HCI International 2013-Posters’ Extended Abstracts, pages 572–576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Chung</author>
</authors>
<title>Developing a flexible spoken dialog system using simulation.</title>
<date>2004</date>
<booktitle>In Proc. the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>63--70</pages>
<contexts>
<context position="1889" citStr="Chung, 2004" startWordPosition="311" endWordPosition="312">physical constraints imposed by devices. On the other hand, if the system accepts only a limited expression, the user need to learn how to use the system. If the user is not familiar with the system, he/she cannot even make an input utterance. Not all users are motivated to converse with the system in actual environments, and sometimes a user will abandon the dialog without making any input utterance. When the user has difficulty to make the utterance, conventional systems just repeat the prompt at fixed interval (Yankelovich, 1996) or taking the initiative in the dialog to complete the task (Chung, 2004; Bohus and Rudnicky, 2009). However, we think that the system has to cope with the user’s implicit requests to help the user more adequately. To solve this problem, Chiba and Ito (2012) proposed a method to estimate two “user’s states” by capturing their non-verbal cues. Here, the state A is when the user does not know what to input, and the state B is when the user is considering how to answer the system’s prompt. These states have not been distinguished by the conventional dialog systems so far, but should be handled differently. The researchers of spoken dialog systems have focused on the </context>
</contexts>
<marker>Chung, 2004</marker>
<rawString>Grace Chung. 2004. Developing a flexible spoken dialog system using simulation. In Proc. the 42nd Annual Meeting on Association for Computational Linguistics, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Collignon</author>
<author>Simon Girard</author>
<author>Frederic Gosselin</author>
<author>Sylvain Roy</author>
<author>Dave Saint-Amour</author>
<author>Maryse Lassonde</author>
<author>Lepore Franco</author>
</authors>
<title>Audio-visual integration of emotion expression.</title>
<date>2008</date>
<journal>Brain research,</journal>
<pages>1242--126</pages>
<contexts>
<context position="7638" citStr="Collignon et al. (2008)" startWordPosition="1277" endWordPosition="1280">ag-ofKeypoints.” In our research, each cluster of the low-level features is expected to represent some kind of user’s behavior. Therefore, we call the proposed method the “Bag-of-Behaviors” approach. After calculating the Bag-of-Behaviors, we employ an appropriate classifier to determine the user’s state in the given session. In this research, the support vector machine (SVM) is used as a classifier. 4 The low-level features In this section, we describe the acoustic and visual features employed as the low-level features. The target user’s states are assumed to have similar aspects to emotion. Collignon et al. (2008) suggested that emotion has a multi-modality nature. For example, W¨ollmer et al. (2013) showed that the acoustic and visual features contributed to discriminate arousal and expectation, respectively. Several other researches also have reported that recognition accuracy of emotion was improved by combining multi-modal information (Lin et al., 2012; Wang and Venetsanopoulos, 2012; Paulmann and Pell, 2011; Metallinou et al., 2012). Therefore, we employed similar features as those used in these previous works, such as the spectral features and intonation of the speech, and facial feature points, </context>
</contexts>
<marker>Collignon, Girard, Gosselin, Roy, Saint-Amour, Lassonde, Franco, 2008</marker>
<rawString>Olivier Collignon, Simon Girard, Frederic Gosselin, Sylvain Roy, Dave Saint-Amour, Maryse Lassonde, and Lepore Franco. 2008. Audio-visual integration of emotion expression. Brain research, 1242:126–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriella Csurka</author>
<author>Christopher Dance</author>
<author>Lixin Fan</author>
<author>Jutta Willamowski</author>
<author>C´edric Bray</author>
</authors>
<title>Visual categorization with bags of keypoints.</title>
<date>2004</date>
<booktitle>In Proc. workshop on statistical learning in computer vision, ECCV,</booktitle>
<pages>1--2</pages>
<contexts>
<context position="6935" citStr="Csurka et al., 2004" startWordPosition="1167" endWordPosition="1170"> = {c1, ... , cK1, where ck denotes the k-th centroid of the codebook. Then the input feature vectors are quantized frame-by-frame using the codebook. When a session for evaluation sE is given, we quantize the input low-level feature vectors x(sE) 1 , ... , x(sE) T into q1, ... , qT, where ||x(sE) t − cq||. (1) Then we calculate the histogram Q0(sE) = (Q1, ... , QK) where δ(k, qt) (2) { δ(x, y) = 1 x = y0 x � y (3) Then Q(sE) = Q0(sE)/||Q0(sE) ||is used as the feature of the discrimination. The similar features based on the vector quantization were used for image detection and scene analysis (Csurka et al., 2004; Jiang et al., 2007; Natarajan et al., 2012) and called “Bag-of-Features” or “Bag-ofKeypoints.” In our research, each cluster of the low-level features is expected to represent some kind of user’s behavior. Therefore, we call the proposed method the “Bag-of-Behaviors” approach. After calculating the Bag-of-Behaviors, we employ an appropriate classifier to determine the user’s state in the given session. In this research, the support vector machine (SVM) is used as a classifier. 4 The low-level features In this section, we describe the acoustic and visual features employed as the low-level fea</context>
</contexts>
<marker>Csurka, Dance, Fan, Willamowski, Bray, 2004</marker>
<rawString>Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta Willamowski, and C´edric Bray. 2004. Visual categorization with bags of keypoints. In Proc. workshop on statistical learning in computer vision, ECCV, pages 1–2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Forbes-Riley</author>
<author>Diane Litman</author>
</authors>
<title>Benefits and challenges of real-time uncertainty detection and adaptation in a spoken dialogue computer tutor.</title>
<date>2011</date>
<journal>Speech Communication,</journal>
<pages>53--1115</pages>
<contexts>
<context position="2568" citStr="Forbes-Riley and Litman, 2011" startWordPosition="428" endWordPosition="431">t the system has to cope with the user’s implicit requests to help the user more adequately. To solve this problem, Chiba and Ito (2012) proposed a method to estimate two “user’s states” by capturing their non-verbal cues. Here, the state A is when the user does not know what to input, and the state B is when the user is considering how to answer the system’s prompt. These states have not been distinguished by the conventional dialog systems so far, but should be handled differently. The researchers of spoken dialog systems have focused on the various internal states of users such as emotion (Forbes-Riley and Litman, 2011a; Metallinou et al., 2012), preference (Pargellis et al., 2004) and familiarity with the system (Jokinen and Kanto, 2004; Rosis et al., 2006) to build natural dialog system. In particular, the user’s “uncertainty” is assumed to be the nearest user’s states that we wish to study. Forbes-Riley and Litman (2011b) and Pon-Barry et al. (2005) introduced a framework for estimating the user’s uncertainty to a tutor system. The above-mentioned researches have a certain result by employing linguistic information for the estimation, but it remains difficult to assist a user who does not make any input </context>
</contexts>
<marker>Forbes-Riley, Litman, 2011</marker>
<rawString>Kate Forbes-Riley and Diane Litman. 2011a. Benefits and challenges of real-time uncertainty detection and adaptation in a spoken dialogue computer tutor. Speech Communication, 53:1115–1136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Forbes-Riley</author>
<author>Diane Litman</author>
</authors>
<title>Designing and evaluating a wizarded uncertainty-adaptive spoken dialogue tutoring system.</title>
<date>2011</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="2568" citStr="Forbes-Riley and Litman, 2011" startWordPosition="428" endWordPosition="431">t the system has to cope with the user’s implicit requests to help the user more adequately. To solve this problem, Chiba and Ito (2012) proposed a method to estimate two “user’s states” by capturing their non-verbal cues. Here, the state A is when the user does not know what to input, and the state B is when the user is considering how to answer the system’s prompt. These states have not been distinguished by the conventional dialog systems so far, but should be handled differently. The researchers of spoken dialog systems have focused on the various internal states of users such as emotion (Forbes-Riley and Litman, 2011a; Metallinou et al., 2012), preference (Pargellis et al., 2004) and familiarity with the system (Jokinen and Kanto, 2004; Rosis et al., 2006) to build natural dialog system. In particular, the user’s “uncertainty” is assumed to be the nearest user’s states that we wish to study. Forbes-Riley and Litman (2011b) and Pon-Barry et al. (2005) introduced a framework for estimating the user’s uncertainty to a tutor system. The above-mentioned researches have a certain result by employing linguistic information for the estimation, but it remains difficult to assist a user who does not make any input </context>
</contexts>
<marker>Forbes-Riley, Litman, 2011</marker>
<rawString>Kate Forbes-Riley and Diane Litman. 2011b. Designing and evaluating a wizarded uncertainty-adaptive spoken dialogue tutoring system. Computer Speech &amp; Language, 25(1):105–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Gang Jiang</author>
<author>Chong-Wah Ngo</author>
<author>Jun Yang</author>
</authors>
<title>Towards optimal bag-of-features for object categorization and semantic video retrieval.</title>
<date>2007</date>
<booktitle>In Proc. of the 6th ACM international conference on Image and video retrieval,</booktitle>
<pages>494--501</pages>
<contexts>
<context position="6955" citStr="Jiang et al., 2007" startWordPosition="1171" endWordPosition="1174">ere ck denotes the k-th centroid of the codebook. Then the input feature vectors are quantized frame-by-frame using the codebook. When a session for evaluation sE is given, we quantize the input low-level feature vectors x(sE) 1 , ... , x(sE) T into q1, ... , qT, where ||x(sE) t − cq||. (1) Then we calculate the histogram Q0(sE) = (Q1, ... , QK) where δ(k, qt) (2) { δ(x, y) = 1 x = y0 x � y (3) Then Q(sE) = Q0(sE)/||Q0(sE) ||is used as the feature of the discrimination. The similar features based on the vector quantization were used for image detection and scene analysis (Csurka et al., 2004; Jiang et al., 2007; Natarajan et al., 2012) and called “Bag-of-Features” or “Bag-ofKeypoints.” In our research, each cluster of the low-level features is expected to represent some kind of user’s behavior. Therefore, we call the proposed method the “Bag-of-Behaviors” approach. After calculating the Bag-of-Behaviors, we employ an appropriate classifier to determine the user’s state in the given session. In this research, the support vector machine (SVM) is used as a classifier. 4 The low-level features In this section, we describe the acoustic and visual features employed as the low-level features. The target us</context>
</contexts>
<marker>Jiang, Ngo, Yang, 2007</marker>
<rawString>Yu-Gang Jiang, Chong-Wah Ngo, and Jun Yang. 2007. Towards optimal bag-of-features for object categorization and semantic video retrieval. In Proc. of the 6th ACM international conference on Image and video retrieval, pages 494–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristiina Jokinen</author>
<author>Kari Kanto</author>
</authors>
<title>User expertise modelling and adaptivity in a speech-based e-mail system.</title>
<date>2004</date>
<booktitle>In Proc. the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="2689" citStr="Jokinen and Kanto, 2004" startWordPosition="446" endWordPosition="449">to (2012) proposed a method to estimate two “user’s states” by capturing their non-verbal cues. Here, the state A is when the user does not know what to input, and the state B is when the user is considering how to answer the system’s prompt. These states have not been distinguished by the conventional dialog systems so far, but should be handled differently. The researchers of spoken dialog systems have focused on the various internal states of users such as emotion (Forbes-Riley and Litman, 2011a; Metallinou et al., 2012), preference (Pargellis et al., 2004) and familiarity with the system (Jokinen and Kanto, 2004; Rosis et al., 2006) to build natural dialog system. In particular, the user’s “uncertainty” is assumed to be the nearest user’s states that we wish to study. Forbes-Riley and Litman (2011b) and Pon-Barry et al. (2005) introduced a framework for estimating the user’s uncertainty to a tutor system. The above-mentioned researches have a certain result by employing linguistic information for the estimation, but it remains difficult to assist a user who does not make any input utterance. By contrast, the method by Chiba and Ito (2012) estimated the target user’s state by only using the user’s non</context>
</contexts>
<marker>Jokinen, Kanto, 2004</marker>
<rawString>Kristiina Jokinen and Kari Kanto. 2004. User expertise modelling and adaptivity in a speech-based e-mail system. In Proc. the 42nd Annual Meeting on Association for Computational Linguistics, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jen-Chun Lin</author>
<author>Chung-Hsien Wu</author>
<author>Wen-Li Wei</author>
</authors>
<title>Error weighted semi-coupled hidden markov model for audio-visual emotion recognition.</title>
<date>2012</date>
<journal>IEEE Trans. Multimedia,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="7987" citStr="Lin et al., 2012" startWordPosition="1326" endWordPosition="1329">t vector machine (SVM) is used as a classifier. 4 The low-level features In this section, we describe the acoustic and visual features employed as the low-level features. The target user’s states are assumed to have similar aspects to emotion. Collignon et al. (2008) suggested that emotion has a multi-modality nature. For example, W¨ollmer et al. (2013) showed that the acoustic and visual features contributed to discriminate arousal and expectation, respectively. Several other researches also have reported that recognition accuracy of emotion was improved by combining multi-modal information (Lin et al., 2012; Wang and Venetsanopoulos, 2012; Paulmann and Pell, 2011; Metallinou et al., 2012). Therefore, we employed similar features as those used in these previous works, such as the spectral features and intonation of the speech, and facial feature points, etc. 4.1 Audio features To represent spectral characteristics of the speech, MFCC was employed as an acoustic feature. We used a 39-dimension MFCC including the velocity and acceleration of the lower 12th-order coefficients and log power. In addition, a differential component of log F0 was used to represent the prosodic feature of the speech, and </context>
</contexts>
<marker>Lin, Wu, Wei, 2012</marker>
<rawString>Jen-Chun Lin, Chung-Hsien Wu, and Wen-Li Wei. 2012. Error weighted semi-coupled hidden markov model for audio-visual emotion recognition. IEEE Trans. Multimedia, 14(1):142–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Metallinou</author>
<author>Martin W¨ollmer</author>
<author>Athanasios Katsamanis</author>
<author>Florian Eyben</author>
<author>Bj¨orn Schuller</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>Context-sensitive learning for enhanced audiovisual emotion classification.</title>
<date>2012</date>
<journal>IEEE Trans. Affective Computing,</journal>
<volume>3</volume>
<issue>2</issue>
<marker>Metallinou, W¨ollmer, Katsamanis, Eyben, Schuller, Narayanan, 2012</marker>
<rawString>Angeliki Metallinou, Martin W¨ollmer, Athanasios Katsamanis, Florian Eyben, Bj¨orn Schuller, and Shrikanth Narayanan. 2012. Context-sensitive learning for enhanced audiovisual emotion classification. IEEE Trans. Affective Computing, 3(2):184–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pradeep Natarajan</author>
<author>Shuang Wu</author>
<author>Shiv Vitaladevuni</author>
<author>Xiaodan Zhuang</author>
<author>Stavros Tsakalidis</author>
<author>Unsang Park</author>
<author>Rohit Prasad</author>
<author>Premkumar Natarajan</author>
</authors>
<title>Multimodal feature fusion for robust event detection in web videos.</title>
<date>2012</date>
<booktitle>In Proc. Computer Vision and Pattern Recognition,</booktitle>
<pages>1298--1305</pages>
<contexts>
<context position="6980" citStr="Natarajan et al., 2012" startWordPosition="1175" endWordPosition="1178">-th centroid of the codebook. Then the input feature vectors are quantized frame-by-frame using the codebook. When a session for evaluation sE is given, we quantize the input low-level feature vectors x(sE) 1 , ... , x(sE) T into q1, ... , qT, where ||x(sE) t − cq||. (1) Then we calculate the histogram Q0(sE) = (Q1, ... , QK) where δ(k, qt) (2) { δ(x, y) = 1 x = y0 x � y (3) Then Q(sE) = Q0(sE)/||Q0(sE) ||is used as the feature of the discrimination. The similar features based on the vector quantization were used for image detection and scene analysis (Csurka et al., 2004; Jiang et al., 2007; Natarajan et al., 2012) and called “Bag-of-Features” or “Bag-ofKeypoints.” In our research, each cluster of the low-level features is expected to represent some kind of user’s behavior. Therefore, we call the proposed method the “Bag-of-Behaviors” approach. After calculating the Bag-of-Behaviors, we employ an appropriate classifier to determine the user’s state in the given session. In this research, the support vector machine (SVM) is used as a classifier. 4 The low-level features In this section, we describe the acoustic and visual features employed as the low-level features. The target user’s states are assumed t</context>
</contexts>
<marker>Natarajan, Wu, Vitaladevuni, Zhuang, Tsakalidis, Park, Prasad, Natarajan, 2012</marker>
<rawString>Pradeep Natarajan, Shuang Wu, Shiv Vitaladevuni, Xiaodan Zhuang, Stavros Tsakalidis, and Unsang Park, Rohit Prasad, and Premkumar Natarajan. 2012. Multimodal feature fusion for robust event detection in web videos. In Proc. Computer Vision and Pattern Recognition, pages 1298–1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Pargellis</author>
<author>Hong-Kwang Jeff Kuo</author>
<author>Chin-Hui Lee</author>
</authors>
<title>An automatic dialogue generation platform for personalized dialogue applications. Speech Communication,</title>
<date>2004</date>
<pages>42--329</pages>
<contexts>
<context position="2632" citStr="Pargellis et al., 2004" startWordPosition="437" endWordPosition="440"> user more adequately. To solve this problem, Chiba and Ito (2012) proposed a method to estimate two “user’s states” by capturing their non-verbal cues. Here, the state A is when the user does not know what to input, and the state B is when the user is considering how to answer the system’s prompt. These states have not been distinguished by the conventional dialog systems so far, but should be handled differently. The researchers of spoken dialog systems have focused on the various internal states of users such as emotion (Forbes-Riley and Litman, 2011a; Metallinou et al., 2012), preference (Pargellis et al., 2004) and familiarity with the system (Jokinen and Kanto, 2004; Rosis et al., 2006) to build natural dialog system. In particular, the user’s “uncertainty” is assumed to be the nearest user’s states that we wish to study. Forbes-Riley and Litman (2011b) and Pon-Barry et al. (2005) introduced a framework for estimating the user’s uncertainty to a tutor system. The above-mentioned researches have a certain result by employing linguistic information for the estimation, but it remains difficult to assist a user who does not make any input utterance. By contrast, the method by Chiba and Ito (2012) estim</context>
</contexts>
<marker>Pargellis, Kuo, Lee, 2004</marker>
<rawString>Andrew Pargellis, Hong-Kwang Jeff Kuo, and Chin-Hui Lee. 2004. An automatic dialogue generation platform for personalized dialogue applications. Speech Communication, 42:329–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuldip Paliwal</author>
<author>Bishnu Atal</author>
</authors>
<title>Efficient vector quantization of lpc parameters at 24 bits/frame.</title>
<date>1993</date>
<booktitle>In IEEE Trans. Speech and Audio Processing,</booktitle>
<pages>1--1</pages>
<marker>Paliwal, Atal, 1993</marker>
<rawString>Kuldip Paliwal and Bishnu Atal. 1993. Efficient vector quantization of lpc parameters at 24 bits/frame. In IEEE Trans. Speech and Audio Processing, 1(1):3–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silke Paulmann</author>
<author>Marc Pell</author>
</authors>
<title>Is there an advantage for recognizing multi-modal emotional stimuli? Motivation and Emotion,</title>
<date>2011</date>
<volume>35</volume>
<issue>2</issue>
<contexts>
<context position="8044" citStr="Paulmann and Pell, 2011" startWordPosition="1334" endWordPosition="1338">The low-level features In this section, we describe the acoustic and visual features employed as the low-level features. The target user’s states are assumed to have similar aspects to emotion. Collignon et al. (2008) suggested that emotion has a multi-modality nature. For example, W¨ollmer et al. (2013) showed that the acoustic and visual features contributed to discriminate arousal and expectation, respectively. Several other researches also have reported that recognition accuracy of emotion was improved by combining multi-modal information (Lin et al., 2012; Wang and Venetsanopoulos, 2012; Paulmann and Pell, 2011; Metallinou et al., 2012). Therefore, we employed similar features as those used in these previous works, such as the spectral features and intonation of the speech, and facial feature points, etc. 4.1 Audio features To represent spectral characteristics of the speech, MFCC was employed as an acoustic feature. We used a 39-dimension MFCC including the velocity and acceleration of the lower 12th-order coefficients and log power. In addition, a differential component of log F0 was used to represent the prosodic feature of the speech, and zero cross (ZC) was used to distinguish voiced and unvoic</context>
</contexts>
<marker>Paulmann, Pell, 2011</marker>
<rawString>Silke Paulmann and Marc Pell. 2011. Is there an advantage for recognizing multi-modal emotional stimuli? Motivation and Emotion, 35(2):192–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heather Pon-Barry</author>
<author>Karl Schultz</author>
<author>Elizabeth Owen Bratt</author>
<author>Brady Clark</author>
<author>Stanley Peters</author>
</authors>
<title>Responding to student uncertainty in spoken tutorial dialogue systems.</title>
<date>2005</date>
<journal>Int. J. Artif. Intell. Edu.,</journal>
<pages>16--171</pages>
<contexts>
<context position="2908" citStr="Pon-Barry et al. (2005)" startWordPosition="484" endWordPosition="487">er the system’s prompt. These states have not been distinguished by the conventional dialog systems so far, but should be handled differently. The researchers of spoken dialog systems have focused on the various internal states of users such as emotion (Forbes-Riley and Litman, 2011a; Metallinou et al., 2012), preference (Pargellis et al., 2004) and familiarity with the system (Jokinen and Kanto, 2004; Rosis et al., 2006) to build natural dialog system. In particular, the user’s “uncertainty” is assumed to be the nearest user’s states that we wish to study. Forbes-Riley and Litman (2011b) and Pon-Barry et al. (2005) introduced a framework for estimating the user’s uncertainty to a tutor system. The above-mentioned researches have a certain result by employing linguistic information for the estimation, but it remains difficult to assist a user who does not make any input utterance. By contrast, the method by Chiba and Ito (2012) estimated the target user’s state by only using the user’s non-verbal information. In their work, the user’s multi-modal behaviors were defined empirically, and the labels of the behaviors were annotated manually. Based on this result, the present paper proposes the method that do</context>
</contexts>
<marker>Pon-Barry, Schultz, Bratt, Clark, Peters, 2005</marker>
<rawString>Heather Pon-Barry, Karl Schultz, Elizabeth Owen Bratt, Brady Clark, and Stanley Peters. 2005. Responding to student uncertainty in spoken tutorial dialogue systems. Int. J. Artif. Intell. Edu., 16:171–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fiorella Rosis</author>
<author>Nicole Novielli</author>
</authors>
<title>Valeria Carofiglio, Addolorata Cavalluzzi, and Berardina Carolis.</title>
<date>2006</date>
<journal>J. Biomedical Informatics,</journal>
<pages>39--514</pages>
<marker>Rosis, Novielli, 2006</marker>
<rawString>Fiorella Rosis, Nicole Novielli, Valeria Carofiglio, Addolorata Cavalluzzi, and Berardina Carolis. 2006. User modeling and adaptation in health promotion dialogs with an animated character. J. Biomedical Informatics, 39:514–531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Saragih</author>
<author>Simon Lucey</author>
<author>Jeffrey Cohn</author>
</authors>
<title>Deformable model fitting by regularized landmark meanshift.</title>
<date>2011</date>
<journal>Int. J. Computer Vision,</journal>
<volume>91</volume>
<issue>2</issue>
<contexts>
<context position="9100" citStr="Saragih et al., 2011" startWordPosition="1517" endWordPosition="1520"> addition, a differential component of log F0 was used to represent the prosodic feature of the speech, and zero cross (ZC) was used to distinguish voiced and unvoiced segments. Therefore, total number of audio features was 3. The basic conditions for extracting each feature are shown in Table 1. Here, five frames qt = arg min q ∑T t=1 Qk = 75 (the current frame, the two previous frames and two following frames) were used to calculate the ∆ and ∆∆ components of MFCC and ∆ component of log F0. 4.2 Face feature Face feature (Chiba et al., 2013) was extracted by the Constraint Local Model (CLM) (Saragih et al., 2011) frame by frame. The coordinates of the points relative to the center of the face were used as the face features. The scale of the feature points was normalized by the size of the facial region. The number of feature points was 66 and the dimension of the feature was 132. 4.3 Gaze feature The evaluators of the dialogs declared that movement of the user’s eyes seems to express their internal state. The present paper used the Haarlike feature which has a fast calculation algorithm using the integral image to represent the brightness of the user’s eye regions. This feature was extracted by applyi</context>
</contexts>
<marker>Saragih, Lucey, Cohn, 2011</marker>
<rawString>Jason Saragih, Simon Lucey, and Jeffrey Cohn. 2011. Deformable model fitting by regularized landmark meanshift. Int. J. Computer Vision, 91(2):200–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongjin Wang</author>
<author>Anastasios Venetsanopoulos</author>
</authors>
<title>Kernel cross-modal factor analysis for information fusion with application to bimodal emotion recognition.</title>
<date>2012</date>
<journal>IEEE Trans. Multimedia,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="8019" citStr="Wang and Venetsanopoulos, 2012" startWordPosition="1330" endWordPosition="1333">SVM) is used as a classifier. 4 The low-level features In this section, we describe the acoustic and visual features employed as the low-level features. The target user’s states are assumed to have similar aspects to emotion. Collignon et al. (2008) suggested that emotion has a multi-modality nature. For example, W¨ollmer et al. (2013) showed that the acoustic and visual features contributed to discriminate arousal and expectation, respectively. Several other researches also have reported that recognition accuracy of emotion was improved by combining multi-modal information (Lin et al., 2012; Wang and Venetsanopoulos, 2012; Paulmann and Pell, 2011; Metallinou et al., 2012). Therefore, we employed similar features as those used in these previous works, such as the spectral features and intonation of the speech, and facial feature points, etc. 4.1 Audio features To represent spectral characteristics of the speech, MFCC was employed as an acoustic feature. We used a 39-dimension MFCC including the velocity and acceleration of the lower 12th-order coefficients and log power. In addition, a differential component of log F0 was used to represent the prosodic feature of the speech, and zero cross (ZC) was used to dist</context>
</contexts>
<marker>Wang, Venetsanopoulos, 2012</marker>
<rawString>Yongjin Wang and Anastasios Venetsanopoulos. 2012. Kernel cross-modal factor analysis for information fusion with application to bimodal emotion recognition. IEEE Trans. Multimedia, 14(3):597–607.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin W¨ollmer</author>
<author>Moritz Kaiser</author>
<author>Florian Eyben</author>
<author>Bj¨orn Schuller</author>
<author>Gerhard Rigoll</author>
</authors>
<title>Lstmmodeling of continuous emotions in an audiovisual affect recognition framework.</title>
<date>2013</date>
<journal>Image and Vision Computing,</journal>
<volume>31</volume>
<issue>2</issue>
<marker>W¨ollmer, Kaiser, Eyben, Schuller, Rigoll, 2013</marker>
<rawString>Martin W¨ollmer, Moritz Kaiser, Florian Eyben, Bj¨orn Schuller, and Gerhard Rigoll. 2013. Lstmmodeling of continuous emotions in an audiovisual affect recognition framework. Image and Vision Computing, 31(2):153–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicole Yankelovich</author>
</authors>
<title>How do users know what to say?</title>
<date>1996</date>
<journal>Interactions,</journal>
<volume>3</volume>
<issue>6</issue>
<contexts>
<context position="1816" citStr="Yankelovich, 1996" startWordPosition="298" endWordPosition="299">tage of being a natural interface since speech commands are less subject to the physical constraints imposed by devices. On the other hand, if the system accepts only a limited expression, the user need to learn how to use the system. If the user is not familiar with the system, he/she cannot even make an input utterance. Not all users are motivated to converse with the system in actual environments, and sometimes a user will abandon the dialog without making any input utterance. When the user has difficulty to make the utterance, conventional systems just repeat the prompt at fixed interval (Yankelovich, 1996) or taking the initiative in the dialog to complete the task (Chung, 2004; Bohus and Rudnicky, 2009). However, we think that the system has to cope with the user’s implicit requests to help the user more adequately. To solve this problem, Chiba and Ito (2012) proposed a method to estimate two “user’s states” by capturing their non-verbal cues. Here, the state A is when the user does not know what to input, and the state B is when the user is considering how to answer the system’s prompt. These states have not been distinguished by the conventional dialog systems so far, but should be handled d</context>
</contexts>
<marker>Yankelovich, 1996</marker>
<rawString>Nicole Yankelovich. 1996. How do users know what to say? Interactions, 3(6):32–43.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
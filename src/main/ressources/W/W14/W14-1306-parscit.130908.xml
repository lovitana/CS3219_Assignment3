<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004162">
<title confidence="0.9993595">
Aspect Term Extraction for Sentiment Analysis: New Datasets, New
Evaluation Measures and an Improved Unsupervised Method
</title>
<author confidence="0.99514">
John Pavlopoulos and Ion Androutsopoulos
</author>
<affiliation confidence="0.995592">
Dept. of Informatics, Athens University of Economics and Business, Greece
</affiliation>
<email confidence="0.577127">
http://nlp.cs.aueb.gr/
</email>
<sectionHeader confidence="0.992951" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966166666667">
Given a set of texts discussing a particular
entity (e.g., customer reviews of a smart-
phone), aspect based sentiment analysis
(ABSA) identifies prominent aspects of the
entity (e.g., battery, screen) and an aver-
age sentiment score per aspect. We fo-
cus on aspect term extraction (ATE), one
of the core processing stages of ABSA that
extracts terms naming aspects. We make
publicly available three new ATE datasets,
arguing that they are better than previously
available ones. We also introduce new
evaluation measures for ATE, again argu-
ing that they are better than previously
used ones. Finally, we show how a pop-
ular unsupervised ATE method can be im-
proved by using continuous space vector
representations of words and phrases.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.979458632653061">
Before buying a product or service, consumers of-
ten search the Web for expert reviews, but increas-
ingly also for opinions of other consumers, ex-
pressed in blogs, social networks etc. Many useful
opinions are expressed in text-only form (e.g., in
tweets). It is then desirable to extract aspects (e.g.,
screen, battery) from the texts that discuss a par-
ticular entity (e.g., a smartphone), i.e., figure out
what is being discussed, and also estimate aspect
sentiment scores, i.e., how positive or negative
the (usually average) sentiment for each aspect is.
These two goals are jointly known as Aspect Based
Sentiment Analysis (ABSA) (Liu, 2012).
In this paper, we consider free text customer re-
views of products and services; ABSA, however,
is also applicable to texts about other kinds of
entities (e.g., politicians, organizations). We as-
sume that a search engine retrieves customer re-
views about a particular target entity (product or
Figure 1: Automatically extracted prominent as-
pects (shown as clusters of aspect terms) and aver-
age aspect sentiment scores of a target entity.
service), that multiple reviews written by different
customers are retrieved for each target entity, and
that the ultimate goal is to produce a table like the
one of Fig. 1, which presents the most prominent
aspects and average aspect sentiment scores of the
target entity. Most ABSA systems in effect perform
all or some of the following three subtasks:
Aspect term extraction: Starting from texts
about a particular target entity or entities of the
same type as the target entity (e.g., laptop re-
views), this stage extracts and possibly ranks by
importance aspect terms, i.e., terms naming as-
pects (e.g., ‘battery’, ‘screen’) of the target en-
tity, including multi-word terms (e.g., ‘hard disk’)
(Liu, 2012; Long et al., 2010; Snyder and Barzi-
lay, 2007; Yu et al., 2011). At the end of this stage,
each aspect term is taken to be the name of a dif-
ferent aspect, but aspect terms may subsequently
be clustered during aspect aggregation; see below.
Aspect term sentiment estimation: This stage
estimates the polarity and possibly also the inten-
sity (e.g., strongly negative, mildly positive) of the
opinions for each aspect term of the target entity,
usually averaged over several texts. Classifying
texts by sentiment polarity is a popular research
topic (Liu, 2012; Pang and Lee, 2005; Tsytsarau
and Palpanas, 2012). The goal, however, in this
</bodyText>
<page confidence="0.992044">
44
</page>
<note confidence="0.9914045">
Proceedings of the 5th Workshop on Language Anal sis for Social Media (LASM) @ EACL 2014, pages 44–52,
Gothenburg, Sweden, April 26-30 2014. (c 2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999561433962264">
ABSA subtask is to estimate the (usually average)
polarity and intensity of the opinions about partic-
ular aspect terms of the target entity.
Aspect aggregation: Some systems group aspect
terms that are synonyms or near-synonyms (e.g.,
‘price’, ‘cost’) or, more generally, cluster aspect
terms to obtain aspects of a coarser granularity
(e.g., ‘chicken’, ‘steak’, and ‘fish’ may all be re-
placed by ‘food’) (Liu, 2012; Long et al., 2010;
Zhai et al., 2010; Zhai et al., 2011). A polar-
ity (and intensity) score can then be computed for
each coarser aspect (e.g., ‘food’) by combining
(e.g., averaging) the polarity scores of the aspect
terms that belong in the coarser aspect.
In this paper, we focus on aspect term extrac-
tion (ATE). Our contribution is threefold. Firstly,
we argue (Section 2) that previous ATE datasets are
not entirely satisfactory, mostly because they con-
tain reviews from a particular domain only (e.g.,
consumer electronics), or they contain reviews for
very few target entities, or they do not contain an-
notations for aspect terms. We constructed and
make publicly available three new ATE datasets
with customer reviews for a much larger number
of target entities from three domains (restaurants,
laptops, hotels), with gold annotations of all the
aspect term occurrences; we also measured inter-
annotator agreement, unlike previous datasets.
Secondly, we argue (Section 3) that commonly
used evaluation measures are also not entirely sat-
isfactory. For example, when precision, recall,
and F-measure are computed over distinct as-
pect terms (types), equal weight is assigned to
more and less frequent aspect terms, whereas fre-
quently discussed aspect terms are more impor-
tant; and when precision, recall, and F-measure
are computed over aspect term occurrences (to-
kens), methods that identify very few, but very fre-
quent aspect terms may appear to perform much
better than they actually do. We propose weighted
variants of precision and recall, which take into ac-
count the rankings of the distinct aspect terms that
are obtained when the distinct aspect terms are or-
dered by their true and predicted frequencies. We
also compute the average weighted precision over
several weighted recall levels.
Thirdly, we show (Section 4) how the popular
unsupervised ATE method of Hu and Liu (2004),
can be extended with continuous space word vec-
tors (Mikolov et al., 2013a; Mikolov et al., 2013b;
Mikolov et al., 2013c). Using our datasets and
evaluation measures, we demonstrate (Section 5)
that the extended method performs better.
</bodyText>
<sectionHeader confidence="0.994997" genericHeader="introduction">
2 Datasets
</sectionHeader>
<bodyText confidence="0.9996285">
We first discuss previous datasets that have been
used for ATE, and we then introduce our own.
</bodyText>
<subsectionHeader confidence="0.996036">
2.1 Previous datasets
</subsectionHeader>
<bodyText confidence="0.9999708">
So far, ATE methods have been evaluated mainly
on customer reviews, often from the consumer
electronics domain (Hu and Liu, 2004; Popescu
and Etzioni, 2005; Ding et al., 2008).
The most commonly used dataset is that of Hu
and Liu (2004), which contains reviews of only
five particular electronic products (e.g., Nikon
Coolpix 4300). Each sentence is annotated with
aspect terms, but inter-annotator agreement has
not been reported.1 All the sentences appear to
have been selected to express clear positive or neg-
ative opinions. There are no sentences express-
ing conflicting opinions about aspect terms (e.g.,
“The screen is clear but small”), nor are there
any sentences that do not express opinions about
their aspect terms (e.g., “It has a 4.8-inch screen”).
Hence, the dataset is not entirely representative of
product reviews. By contrast, our datasets, dis-
cussed below, contain reviews from three domains,
including sentences that express conflicting or no
opinions about aspect terms, they concern many
more target entities (not just five), and we have
also measured inter-annotator agreement.
The dataset of Ganu et al. (2009), on which
one of our datasets is based, is also popular. In
the original dataset, each sentence is tagged with
coarse aspects (‘food’, ‘service’, ‘price’, ‘ambi-
ence’, ‘anecdotes’, or ‘miscellaneous’). For exam-
ple, “The restaurant was expensive, but the menu
was great” would be tagged with the coarse as-
pects ‘price’ and ‘food’. The coarse aspects, how-
ever, are not necessarily terms occurring in the
sentence, and it is unclear how they were obtained.
By contrast, we asked human annotators to mark
the explicit aspect terms of each sentence, leaving
the task of clustering the terms to produce coarser
aspects for an aspect aggregation stage.
The ‘Concept-Level Sentiment Analysis Chal-
lenge’ of ESWC 2014 uses the dataset of Blitzer
et al. (2007), which contains customer reviews of
</bodyText>
<footnote confidence="0.9829585">
1Each aspect term occurrence is also annotated with a sen-
timent score. We do not discuss these scores here, since we
focus on ATE. The same comment applies to the dataset of
Ganu et al. (2009) and our datasets.
</footnote>
<page confidence="0.999538">
45
</page>
<bodyText confidence="0.9999684">
DVDs, books, kitchen appliances, and electronic
products, with an overall sentiment score for each
review. One of the challenge’s tasks requires sys-
tems to extract the aspects of each sentence and a
sentiment score (positive or negative) per aspect.2
The aspects are intended to be concepts from on-
tologies, not simply aspect terms. The ontologies
to be used, however, are not fully specified and no
training dataset with sentences and gold aspects is
currently available.
Overall, the previous datasets are not entirely
satisfactory, because they contain reviews from
a particular domain only, or reviews for very
few target entities, or their sentences are not en-
tirely representative of customer reviews, or they
do not contain annotations for aspect terms, or
no inter-annotator agreement has been reported.
To address these issues, we provide three new
ATE datasets, which contain customer reviews of
restaurants, hotels, and laptops, respectively.3
</bodyText>
<subsectionHeader confidence="0.99951">
2.2 Our datasets
</subsectionHeader>
<bodyText confidence="0.999966375">
The restaurants dataset contains 3,710 English
sentences from the reviews of Ganu et al. (2009).4
We asked human annotators to tag the aspect terms
of each sentence. In “The dessert was divine”,
for example, the annotators would tag the aspect
term ‘dessert’. In a sentence like “The restaurant
was expensive, but the menu was great”, the an-
notators were instructed to tag only the explicitly
mentioned aspect term ‘menu’. The sentence also
refers to the prices, and a possibility would be to
add ‘price’ as an implicit aspect term, but we do
not consider implicit aspect terms in this paper.
We used nine annotators for the restaurant re-
views. Each sentence was processed by a single
annotator, and each annotator processed approxi-
mately the same number of sentences. Among the
3,710 restaurant sentences, 1,248 contain exactly
one aspect term, 872 more than one, and 1,590 no
aspect terms. There are 593 distinct multi-word
aspect terms and 452 distinct single-word aspect
terms. Removing aspect terms that occur only
once leaves 67 distinct multi-word and 195 dis-
tinct single-word aspect terms.
The hotels dataset contains 3,600 English sen-
</bodyText>
<footnote confidence="0.9518585">
2Seehttp://2014.eswc-conferences.org/.
3Our datasets are available upon request. The datasets
of the ABSA task of SemEval 2014 (http://alt.qcri.
org/semeval2014/task4/) are based on our datasets.
4The original dataset of Ganu et al. contains 3,400 sen-
tences, but some of the sentences had not been properly split.
</footnote>
<bodyText confidence="0.999613454545455">
tences from online customer reviews of 30 hotels.
We used three annotators. Among the 3,600 hotel
sentences, 1,326 contain exactly one aspect term,
652 more than one, and 1,622 none. There are 199
distinct multi-word aspect terms and 262 distinct
single-word aspect terms, of which 24 and 120,
respectively, were tagged more than once.
The laptops dataset contains 3,085 English sen-
tences of 394 online customer reviews. A single
annotator (one of the authors) was used. Among
the 3,085 laptop sentences, 909 contain exactly
one aspect term, 416 more than one, and 1,760
none. There are 350 distinct multi-word and 289
distinct single-word aspect terms, of which 67 and
137, respectively, were tagged more than once.
To measure inter-annotator agreement, we used
a sample of 75 restaurant, 75 laptop, and 100 hotel
sentences. Each sentence was processed by two
(for restaurants and laptops) or three (for hotels)
annotators, other than the annotators used previ-
ously. For each sentence si, the inter-annotator
agreement was measured as the Dice coefficient
</bodyText>
<equation confidence="0.9616405">
Di = 2 �|Ai∩Bi|, where Ai, Bi are the sets of
|Ai|+|Bi|
</equation>
<bodyText confidence="0.999164888888889">
aspect term occurrences tagged by the two anno-
tators, respectively, and |S |denotes the cardinal-
ity of a set S; for hotels, we use the mean pair-
wise Di of the three annotators.5 The overall inter-
annotator agreement D was taken to be the aver-
age Di of the sentences of each sample. We, thus,
obtained D = 0.72, 0.70, 0.69, for restaurants, ho-
tels, and laptops, respectively, which indicate rea-
sonably high inter-annotator agreement.
</bodyText>
<subsectionHeader confidence="0.999314">
2.3 Single and multi-word aspect terms
</subsectionHeader>
<bodyText confidence="0.99984675">
ABSA systems use ATE methods ultimately to ob-
tain the m most prominent (frequently discussed)
distinct aspect terms of the target entity, for dif-
ferent values of m.6 In a system like the one of
Fig. 1, for example, if we ignore aspect aggrega-
tion, each row will report the average sentiment
score of a single frequent distinct aspect term, and
m will be the number of rows, which may depend
on the display size or user preferences.
Figure 2 shows the percentage of distinct multi-
word aspect terms among the m most frequent dis-
tinct aspect terms, for different values of m, in
</bodyText>
<footnote confidence="0.977448666666667">
5Cohen’s Kappa cannot be used here, because the annota-
tors may tag any word sequence of any sentence, which leads
to a very large set of categories. A similar problem was re-
ported by Kobayashi et al. (2007).
6A more general definition of prominence might also con-
sider the average sentiment score of each distinct aspect term.
</footnote>
<page confidence="0.99941">
46
</page>
<bodyText confidence="0.999264833333333">
our three datasets and the electronics dataset of Hu
and Liu (2004). There are many more single-word
distinct aspect terms than multi-word distinct as-
pect terms, especially in the restaurant and hotel
reviews. In the electronics and laptops datasets,
the percentage of multi-word distinct aspect terms
(e.g., ‘hard disk’) is higher, but most of the dis-
tinct aspect terms are still single-word, especially
for small values of m. By contrast, many ATE
methods (Hu and Liu, 2004; Popescu and Etzioni,
2005; Wei et al., 2010) devote much of their pro-
cessing to identifying multi-word aspect terms.
</bodyText>
<figureCaption confidence="0.8674035">
Figure 2: Percentage of (distinct) multi-word as-
pect terms among the most frequent aspect terms.
</figureCaption>
<sectionHeader confidence="0.992616" genericHeader="method">
3 Evaluation measures
</sectionHeader>
<bodyText confidence="0.999955">
We now discuss previous ATE evaluation mea-
sures, also introducing our own.
</bodyText>
<subsectionHeader confidence="0.999396">
3.1 Precision, Recall, F-measure
</subsectionHeader>
<bodyText confidence="0.994513857142857">
ATE methods are usually evaluated using preci-
sion, recall, and F-measure (Hu and Liu, 2004;
Popescu and Etzioni, 2005; Kim and Hovy, 2006;
Wei et al., 2010; Moghaddam and Ester, 2010;
Bagheri et al., 2013), but it is often unclear if these
measures are applied to distinct aspect terms (no
duplicates) or aspect term occurrences.
In the former case, each method is expected to
return a set A of distinct aspect terms, to be com-
pared to the set G of distinct aspect terms the hu-
man annotators identified in the texts. TP (true
positives) is |AnG|, FP (false positives) is |A\G|,
FN (false negatives) is |G \ A|, and precision (P),
recall (R), F = .P. Pare defined as usually:
</bodyText>
<equation confidence="0.965308666666667">
P
= TP = TP ( )
TP + FP , R TP + FN 1
</equation>
<bodyText confidence="0.999977923076923">
This way, however, precision, recall, and F-
measure assign the same importance to all the dis-
tinct aspect terms, whereas missing, for example, a
more frequent (more frequently discussed) distinct
aspect term should probably be penalized more
heavily than missing a less frequent one.
When precision, recall, and F-measure are ap-
plied to aspect term occurrences (Liu et al., 2005),
TP is the number of aspect term occurrences
tagged (each term occurrence) both by the method
being evaluated and the human annotators, FP is
the number of aspect term occurrences tagged by
the method but not the human annotators, and FN
is the number of aspect term occurrences tagged
by the human annotators but not the method. The
three measures are then defined as above. They
now assign more importance to frequently occur-
ring distinct aspect terms, but they can produce
misleadingly high scores when only a few, but
very frequent distinct aspect terms are handled
correctly. Furthermore, the occurrence-based def-
initions do not take into account that missing sev-
eral aspect term occurrences or wrongly tagging
expressions as aspect term occurrences may not
actually matter, as long as the m most frequent
distinct aspect terms can be correctly reported.
</bodyText>
<subsectionHeader confidence="0.9956">
3.2 Weighted precision, recall, AWP
</subsectionHeader>
<bodyText confidence="0.999968608695652">
What the previous definitions of precision and re-
call miss is that in practice ABSA systems use
ATE methods ultimately to obtain the m most fre-
quent distinct aspect terms, for a range of m val-
ues. Let Am and Gm be the lists that contain the
m most frequent distinct aspect terms, ordered by
their predicted and true frequencies, respectively;
the predicted and true frequencies are computed
by examining how frequently the ATE method or
the human annotators, respectively, tagged occur-
rences of each distinct aspect term. Differences
between the predicted and true frequencies do not
matter, as long as Am = Gm, for every m. Not
including in Am a term of Gm should be penal-
ized more or less heavily, depending on whether
the term’s true frequency was high or low, respec-
tively. Furthermore, including in Am a term not in
Gm should be penalized more or less heavily, de-
pending on whether the term was placed towards
the beginning or the end of Am, i.e., depending on
the prominence that was assigned to the term.
To address the issues discussed above, we in-
troduce weighted variants of precision and recall.
</bodyText>
<page confidence="0.996069">
47
</page>
<bodyText confidence="0.996042625">
For each ATE method, we now compute a single
list A = ⟨a1, ... , aJAJ⟩ of distinct aspect terms
identified by the method, ordered by decreasing
predicted frequency. For every m value (number
of most frequent distinct aspect terms to show),
the method is treated as having returned the sub-
list Am with the first m elements of A. Similarly,
we now take G = ⟨g1, ... , gJGJ ⟩ to be the list of
the distinct aspect terms that the human annotators
tagged, ordered by decreasing true frequency.7 We
define weighted precision (WPm) and weighted
recall (WRm) as in Eq. 2–3. The notation 1{r-}
denotes 1 if condition r. holds, and 0 otherwise.
By r(ai) we denote the ranking of the returned
term ai in G, i.e., if ai = gj, then r(ai) = j; if
ai E� G, then r(ai) is an arbitrary positive integer.
</bodyText>
<equation confidence="0.99363725">
WPm = ∑m 1 i 1{ai E G} ∑m (2)
1
i=1 i
m 1 1{ai E G}
(3)
∑i=1 r(ai)
WRm = GI 1
∑JGIj=1 j
</equation>
<bodyText confidence="0.99992675">
WRm counts how many terms of G (gold dis-
tinct aspect terms) the method returned in Am,
but weighting each term by its inverse ranking
r(ai), i.e., assigning more importance to terms the
</bodyText>
<equation confidence="0.72327">
1
</equation>
<bodyText confidence="0.996011">
human annotators tagged more frequently. The
denominator of Eq. 3 sums the weights of all
the terms of G; in unweighted recall applied to
distinct aspect terms, where all the terms of G
have the same weight, the denominator would be
|G |= TP + FN (Eq. 1). WPm counts how
many gold aspect terms the method returned in
Am, but weighting each returned term ai by its
inverse ranking 1i in Am, to reward methods that
return more gold aspect terms towards the begin-
ning of Am. The denominator of Eq. 2 sums the
weights of all the terms of Am; in unweighted pre-
cision applied to distinct aspect terms, the denom-
inator would be |Am |= TP + FN (Eq. 1).
We plot weighted precision-recall curves by
computing WPm, WRm pairs for different values
of m, as in Fig. 3 below.8 The higher the curve
of a method, the better the method. We also com-
pute the average (interpolated) weighted precision
</bodyText>
<footnote confidence="0.9871398">
7In our experiments, we exclude from G aspect terms
tagged by the annotators only once.
8With supervised methods, we perform a 10-fold cross-
validation for each m, and we macro-average WPm, WRm
over the folds. We provide our datasets partitioned in folds.
</footnote>
<bodyText confidence="0.700327">
(AWP) of each method over 11 recall levels:
</bodyText>
<equation confidence="0.930128">
WPint(r) = max
mE{1,...,JAJ}, WRm &gt; r
</equation>
<bodyText confidence="0.999615666666667">
AWP is similar to average (interpolated) precision
(AP), which is used to summarize the tradeoff be-
tween (unweighted) precision and recall.
</bodyText>
<subsectionHeader confidence="0.998867">
3.3 Other related measures
</subsectionHeader>
<bodyText confidence="0.999478">
Yu at al. (2011) used nDCG@m (J¨arvelin and
Kek¨al¨ainen, 2002; Sakai, 2004; Manning et al.,
2008), defined below, to evaluate each list of m
distinct aspect terms returned by an ATE method.
</bodyText>
<equation confidence="0.996838">
1
nDCG@m = Z
</equation>
<bodyText confidence="0.992796933333334">
Z is a normalization factor to ensure that a perfect
ranking gets nDCG@m = 1, and t(i) is a reward
function for a term placed at position i of the re-
turned list. In the work of Yu et al., t(i) = 1 if the
term at position i is not important (as judged by
a human), t(i) = 2 if the term is ‘ordinary’, and
t(i) = 3 if it is important. The logarithm is used to
reduce the reward for distinct aspect terms placed
at lower positions of the returned list.
The nDCG@m measure is well known in rank-
ing systems (e.g., search engines) and it is similar
to our weighted precision (WPm). The denomina-
tor or Eq. 2 corresponds to the normalization fac-
tor Z of nDCG@m; the 1i factor of in the numer-
ator of Eq. 2 corresponds to the 1
</bodyText>
<equation confidence="0.936875">
log2(1+i) degra-
</equation>
<bodyText confidence="0.999734714285714">
dation factor of nDCG@m; and the 1{ai E G}
factor of Eq. 2 is a binary reward function, corre-
sponding to the 2t(i) − 1 factor of nDCG@m.
The main difference from nDCG@m is that
WPm uses a degradation factor 1i that is inversely
proportional to the ranking of the returned term
ai in the returned list Am, whereas nDCG@m
</bodyText>
<subsectionHeader confidence="0.50471">
uses a logarithmic factor 1
</subsectionHeader>
<bodyText confidence="0.989948545454546">
log2(1+i), which reduces
less sharply the reward for distinct aspect terms
returned at lower positions in Am. We believe
that the degradation factor of WPm is more ap-
propriate for ABSA, because most users would in
practice wish to view sentiment scores for only a
few (e.g., m = 10) frequent distinct aspect terms,
whereas in search engines users are more likely to
examine more of the highly-ranked returned items.
It is possible, however, to use a logarithmic degra-
dation factor in WPm, as in nDCG@m.
</bodyText>
<equation confidence="0.981581">
WPint(r)
1
AWP =
∑
11
rE{0,0.1,...,1}
WPm
∑m
i=1
2t(i) − 1
lo92(1 + i)
</equation>
<page confidence="0.983873">
48
</page>
<bodyText confidence="0.999874074074074">
Another difference is that we use a binary re-
ward factor 1{ai E G} in WPm, instead of the
2t�i� − 1 factor of nDCG@m that has three pos-
sibly values in the work of Yu at al. (2011). We
use a binary reward factor, because preliminary
experiments we conducted indicated that multi-
ple relevance levels (e.g., not an aspect term, as-
pect term but unimportant, important aspect term)
confused the annotators and led to lower inter-
annotator agreement. The nDCG@m measure
can also be used with a binary reward factor; the
possible values t(i) would be 0 and 1.
With a binary reward factor, nDCG@m in ef-
fect measures the ratio of correct (distinct) aspect
terms to the terms returned, assigning more weight
to correct aspect terms placed closer the top of the
returned list, like WPm. The nDCG@m mea-
sure, however, does not provide any indication
of how many of the gold distinct aspect terms
have been returned. By contrast, we also mea-
sure weighted recall (Eq. 3), which examines how
many of the (distinct) gold aspect terms have been
returned in Am, also assigning more weight to the
gold aspect terms the human annotators tagged
more frequently. We also compute the average
weighted precision (AWP), which is a combina-
tion of WPm and WRm, for a range of m values.
</bodyText>
<sectionHeader confidence="0.977918" genericHeader="method">
4 Aspect term extraction methods
</sectionHeader>
<bodyText confidence="0.9999929">
We implemented and evaluated four ATE meth-
ods: (i) a popular baseline (dubbed FREQ) that re-
turns the most frequent distinct nouns and noun
phrases, (ii) the well-known method of Hu and Liu
(2004), which adds to the baseline pruning mech-
anisms and steps that detect more aspect terms
(dubbed H&amp;L), (iii) an extension of the previous
method (dubbed H&amp;L+W2V), with an extra prun-
ing step we devised that uses the recently pop-
ular continuous space word vectors (Mikolov et
al., 2013c), and (iv) a similar extension of FREQ
(dubbed FREQ+W2V). All four methods are unsu-
pervised, which is particularly important for ABSA
systems intended to be used across domains with
minimal changes. They return directly a list A of
distinct aspect terms ordered by decreasing pre-
dicted frequency, rather than tagging aspect term
occurrences, which would require computing the
A list from the tagged occurrences before apply-
ing our evaluation measures (Section 3.2).
</bodyText>
<subsectionHeader confidence="0.989861">
4.1 The FREQ baseline
</subsectionHeader>
<bodyText confidence="0.999799857142857">
The FREQ baseline returns the most frequent (dis-
tinct) nouns and noun phrases of the reviews in
each dataset (restaurants, hotels, laptops), ordered
by decreasing sentence frequency (how many sen-
tences contain the noun or noun phrase).9 This is a
reasonably effective and popular baseline (Hu and
Liu, 2004; Wei et al., 2010; Liu, 2012).
</bodyText>
<subsectionHeader confidence="0.974354">
4.2 The H&amp;L method
</subsectionHeader>
<bodyText confidence="0.999892944444445">
The method of Hu and Liu (2004), dubbed H&amp;L,
first extracts all the distinct nouns and noun
phrases from the reviews of each dataset (lines 3–
6 of Algorithm 1) and considers them candidate
distinct aspect terms.10 It then forms longer can-
didate distinct aspect terms by concatenating pairs
and triples of candidate aspect terms occurring in
the same sentence, in the order they appear in the
sentence (lines 7–11). For example, if ‘battery
life’ and ‘screen’ occur in the same sentence (in
this order), then ‘battery life screen’ will also be-
come a candidate distinct aspect term.
The resulting candidate distinct aspect terms
are ordered by decreasing p-support (lines 12–15).
The p-support of a candidate distinct aspect term t
is the number of sentences that contain t, exclud-
ing sentences that contain another candidate dis-
tinct aspect term t′ that subsumes t. For example,
if both ‘battery life’ and ‘battery’ are candidate
distinct aspect terms, a sentence like “The battery
life was good” is counted in the p-support of ‘bat-
tery life’, but not in the p-support of ‘battery’.
The method then tries to correct itself by prun-
ing wrong candidate distinct aspect terms and de-
tecting additional candidates. Firstly, it discards
multi-word distinct aspect terms that appear in
‘non-compact’ form in more than one sentences
(lines 16–23). A multi-word term t appears in non-
compact form in a sentence if there are more than
three other words (not words of t) between any
two of the words of t in the sentence. For exam-
ple, the candidate distinct aspect term ‘battery life
screen’ appears in non-compact form in “battery
life is way better than screen”. Secondly, if the
p-support of a candidate distinct aspect term t is
smaller than 3 and t is subsumed by another can-
</bodyText>
<footnote confidence="0.9986555">
9We use the default POS tagger of NLTK, and the chun-
ker of NLTK trained on the Treebank corpus; see http:
//nltk.org/. We convert all words to lower-case.
10Some details of the work of Hu and Liu (2004) were not
entirely clear to us. The discussion here and our implementa-
tion reflect our understanding.
</footnote>
<page confidence="0.999477">
49
</page>
<bodyText confidence="0.982681466666667">
didate distinct aspect term t′, then t is discarded
(lines 21–23).
Subsequently, a set of ‘opinion adjectives’ is
formed; for each sentence and each candidate dis-
tinct aspect term t that occurs in the sentence, the
closest to t adjective of the sentence (if there is
one) is added to the set of opinion adjectives (lines
25-27). The sentences are then re-scanned; if a
sentence does not contain any candidate aspect
term, but contains an opinion adjective, then the
nearest noun to the opinion adjective is added to
the candidate distinct aspect terms (lines 28–31).
The remaining candidate distinct aspect terms are
returned, ordered by decreasing p-support.
Algorithm 1 The method of Hu and Liu
</bodyText>
<listItem confidence="0.991443675675676">
Require: sentences: a list of sentences
1: terms = new Set(String)
2: psupport = new Map(String, int)
3: for s in sentences do
4: nouns = POSTagger(s).getNouns()
5: nps = Chunker(s).getNPChunks()
6: terms.add(nouns U nps)
7: for s in sentences do
8: for t1, t2 in terms s.t. t1, t2 in s n
s.index(t1)&lt;s.index(t2) do
9: terms.add(t1 + ” ” + t2)
10: for t1, t2, t3 in s.t. t1, t2,t3 in s n
s.index(t1)&lt;s.index(t2)&lt;s.index(t3) do
11: terms.add(t1 + ” ” + t2 + ” ” + t3)
12: for s in sentences do
13: for t: t in terms n t in s do
14: if -3 t’: t’ in terms n t’ in s n t in t’ then
15: psupport[term] += 1
16: nonCompact = new Map(String, int)
17: for t in terms do
18: for s in sentences do
19: if maxPairDistance(t.words())&gt;3 then
20: nonCompact[t] += 1
21: for t in terms do
22: if nonCompact[t]&gt;1 V (3 t’: t’ in terms n t in t’ n
psupport[t]&lt;3) then
23: terms.remove(t)
24: adjs = new Set(String)
25: for s in sentences do
26: if 3 t: t in terms n t in s then
27: adjs.add(POSTagger(s).getNearestAdj(t))
28: for s in sentences do
29: if -3 t: t in terms n t in s n 3 a: a in adjs n a in s
then
30: t = POSTagger(s).getNearestNoun(adjs)
31: terms.add(t)
32: return psupport.keysSortedByValue()
</listItem>
<subsectionHeader confidence="0.998158">
4.3 The H&amp;L+W2V method
</subsectionHeader>
<bodyText confidence="0.9984844">
We extended H&amp;L by including an additional
pruning step that uses continuous vector space
representations of words (Mikolov et al., 2013a;
Mikolov et al., 2013b; Mikolov et al., 2013c).
The vector representations of the words are pro-
</bodyText>
<table confidence="0.999552">
Centroid Closest Wikipedia words
Com. lang. only, however, so, way, because
Restaurants meal, meals, breakfast, wingstreet,
snacks
Hotels restaurant, guests, residence, bed, ho-
tels
Laptops gameport, hardware, hd floppy, pcs, ap-
ple macintosh
</table>
<tableCaption confidence="0.9551265">
Table 1: Wikipedia words closest to the common
language and domain centroids.
</tableCaption>
<bodyText confidence="0.999466">
duced by using a neural network language model,
whose inputs are the vectors of the words occur-
ring in each sentence, treated as latent variables to
be learned. We used the English Wikipedia to train
the language model and obtain word vectors, with
200 features per vector. Vectors for short phrases,
in our case candidate multi-word aspect terms, are
produced in a similar manner.11
Our additional pruning stage is invoked imme-
diately immediately after line 6 of Algorithm 1. It
uses the ten most frequent candidate distinct as-
pect terms that are available up to that point (fre-
quency taken to be the number of sentences that
contain each candidate) and computes the centroid
of their vectors, dubbed the domain centroid. Sim-
ilarly, it computes the centroid of the 20 most fre-
quent words of the Brown Corpus (news category),
excluding stop-words and words shorter than three
characters; this is the common language centroid.
Any candidate distinct aspect term whose vector is
closer to the common language centroid than the
domain centroid is discarded, the intuition being
that the candidate names a very general concept,
rather than a domain-specific aspect.12 We use co-
sine similarity to compute distances. Vectors ob-
tained from Wikipedia are used in all cases.
To showcase the insight of our pruning step,
Table 1 shows the five words from the English
Wikipedia whose vectors are closest to the com-
mon language centroid and the three domain cen-
troids. The words closest to the common language
centroid are common words, whereas words clos-
est to the domain centroids name domain-specific
concepts that are more likely to be aspect terms.
</bodyText>
<footnote confidence="0.517546375">
11We use WORD2VEC, available at https://code.
google.com/p/word2vec/, with a continuous bag of
words model, default parameters, the first billion characters
of the English Wikipedia, and the pre-processing of http:
//mattmahoney.net/dc/textdata.html.
12WORD2VEC does not produce vectors for phrases longer
than two words; thus, our pruning mechanism never discards
candidate aspect terms of more than two words.
</footnote>
<page confidence="0.994158">
50
</page>
<figureCaption confidence="0.999462">
Figure 3: Weighted precision – weighted recall curves for the three datasets.
</figureCaption>
<subsectionHeader confidence="0.98976">
4.4 The FREQ+W2V method
</subsectionHeader>
<bodyText confidence="0.999991142857143">
As with H&amp;L+W2V, we extended FREQ by adding
our pruning step that uses the continuous space
word (and phrase) vectors. Again, we produced
one common language and three domain cen-
troids, as before. Candidate distinct aspect terms
whose vector was closer to the common language
centroid than the domain centroid were discarded.
</bodyText>
<sectionHeader confidence="0.998669" genericHeader="evaluation">
5 Experimental results
</sectionHeader>
<bodyText confidence="0.999939714285714">
Table 2 shows the AWP scores of the methods.
All four methods perform better on the restaurants
dataset. At the other extreme, the laptops dataset
seems to be the most difficult one; this is due to the
fact that it contains many frequent nouns and noun
phrases that are not aspect terms; it also contains
more multi-word aspect terms (Fig. 2).
H&amp;L performs much better than FREQ in all
three domains, and our additional pruning (W2V)
improves H&amp;L in all three domains. By contrast
FREQ benefits from W2V only in the restaurant re-
views (but to a smaller degree than H&amp;L), it bene-
fits only marginally in the hotel reviews, and in the
laptop reviews FREQ+W2V performs worse than
FREQ. A possible explanation is that the list of
candidate (distinct) aspect terms that FREQ pro-
duces already misses many aspect terms in the ho-
tel and laptop datasets; hence, W2V, which can
only prune aspect terms, cannot improve the re-
sults much, and in the case of laptops W2V has a
negative effect, because it prunes several correct
candidate aspect terms. All differences between
AWP scores on the same dataset are statistically
significant; we use stratified approximate random-
ization, which indicates p G 0.01 in all cases.13
Figure 3 shows the weighted precision and
weighted recall curves of the four methods. In
the restaurants dataset, our pruning improves
</bodyText>
<footnote confidence="0.969847">
13See http://masanjin.net/sigtest.pdf.
</footnote>
<table confidence="0.9996524">
Method Restaurants Hotels Laptops
FREQ 43.40 30.11 9.09
FREQ+W2V 45.17 30.54 7.18
H&amp;L 52.23 49.73 34.34
H&amp;L+W2V 66.80 53.37 38.93
</table>
<tableCaption confidence="0.999808">
Table 2: Average weighted precision results (%).
</tableCaption>
<bodyText confidence="0.999957882352941">
the weighted precision of both H&amp;L and FREQ;
by contrast it does not improve weighted re-
call, since it can only prune candidate as-
pect terms. The maximum weighted precision
of FREQ+W2V is almost as good as that of
H&amp;L+W2V, but H&amp;L+W2V (and H&amp;L) reach
much higher weighted recall scores. In the hotel
reviews, W2V again improves the weighted pre-
cision of both H&amp;L and FREQ, but to a smaller
extent; again W2V does not improve weighted re-
call; also, H&amp;L and H&amp;L+W2V again reach higher
weighted recall scores. In the laptop reviews,
W2V marginally improves the weighted precision
of H&amp;L, but it lowers the weighted precision of
FREQ; again H&amp;L and H&amp;L+W2V reach higher
weighted recall scores. Overall, Fig. 3 confirms
that H&amp;L+W2V is the best method.
</bodyText>
<sectionHeader confidence="0.999551" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999949461538462">
We constructed and made publicly available three
new ATE datasets from three domains. We also
introduced weighted variants of precision, recall,
and average precision, arguing that they are more
appropriate for ATE. Finally, we discussed how
a popular unsupervised ATE method can be im-
proved by adding a new pruning mechanism that
uses continuous space vector representations of
words and phrases. Using our datasets and eval-
uation measures, we showed that the improved
method performs clearly better than the origi-
nal one, also outperforming a simpler frequency-
based baseline with or without our pruning.
</bodyText>
<page confidence="0.99792">
51
</page>
<sectionHeader confidence="0.998345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999596032608696">
A. Bagheri, M. Saraee, and F. Jong. 2013. An unsuper-
vised aspect detection model for sentiment analysis
of reviews. In Proceedings of NLDB, volume 7934,
pages 140–151.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-
phies, Bollywood, boom-boxes and blenders: Do-
main adaptation for sentiment classification. In Pro-
ceedings ofACL, pages 440–447, Prague, Czech Re-
public.
X. Ding, B. Liu, and P. S. Yu. 2008. A holistic lexicon-
based approach to opinion mining. In Proceedings
of WSDM, pages 231–240, Palo Alto, CA, USA.
G. Ganu, N. Elhadad, and A. Marian. 2009. Beyond
the stars: Improving rating predictions using review
text content. In Proceedings of WebDB, Providence,
RI, USA.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of KDD, pages
168–177, Seattle, WA, USA.
Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20(4):422–
446.
S.-M. Kim and E. Hovy. 2006. Extracting opinions,
opinion holders, and topics expressed in online news
media text. In Proceedings of SST, pages 1–8, Syd-
ney, Australia.
N. Kobayashi, K. Inui, and Y. Matsumoto. 2007. Ex-
tracting aspect-evaluation and aspect-of relations in
opinion mining. In Proceedings of EMNLP-CoNLL,
pages 1065–1074, Prague, Czech Republic.
B. Liu, M. Hu, and J. Cheng. 2005. Opinion ob-
server: analyzing and comparing opinions on the
web. In Proceedings of WWW, pages 342–351,
Chiba, Japan.
B. Liu. 2012. Sentiment Analysis and Opinion Mining.
Synthesis Lectures on Human Language Technolo-
gies. Morgan &amp; Claypool.
C. Long, J. Zhang, and X. Zhut. 2010. A review se-
lection approach for accurate feature rating estima-
tion. In Proceedings of COLING, pages 766–774,
Beijing, China.
C. D. Manning, P. Raghavan, and H. Sch¨utze. 2008.
Introduction to Information Retrieval. Cambridge
University Press.
T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a.
Efficient estimation of word representations in vec-
tor space. In Proceedings of Workshop at ICLR.
T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and
J. Dean. 2013b. Distributed representations of
words and phrases and their compositionality. In
Proceedings of NIPS.
T. Mikolov, W.-T. Yih, and G. Zweig. 2013c. Linguis-
tic regularities in continuous space word representa-
tions. In Proceedings of NAACL HLT.
S. Moghaddam and M. Ester. 2010. Opinion digger:
an unsupervised opinion miner from unstructured
product reviews. In Proceedings of CIKM, pages
1825–1828, Toronto, ON, Canada.
B. Pang and L. Lee. 2005. Seeing stars: exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings ofACL,
pages 115–124, Ann Arbor, MI, USA.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of HLT-EMNLP, pages 339–346, Van-
couver, Canada.
T. Sakai. 2004. Ranking the NTCIR systems based
on multigrade relevance. In Proceedings of AIRS,
pages 251–262, Beijing, China.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the good grief algorithm. In Proceedings
of NAACL, pages 300–307, Rochester, NY, USA.
M. Tsytsarau and T. Palpanas. 2012. Survey on min-
ing subjective data on the web. Data Mining and
Knowledge Discovery, 24(3):478–514.
C.-P. Wei, Y.-M. Chen, C.-S. Yang, and C. C Yang.
2010. Understanding what concerns consumers:
a semantic approach to product feature extraction
from consumer reviews. Information Systems and
E-Business Management, 8(2):149–167.
J. Yu, Z. Zha, M. Wang, and T. Chua. 2011. As-
pect ranking: identifying important product aspects
from online consumer reviews. In Proceedings of
NAACL, pages 1496–1505, Portland, OR, USA.
Z. Zhai, B. Liu, H. Xu, and P. Jia. 2010. Group-
ing product features using semi-supervised learning
with soft-constraints. In Proceedings of COLING,
pages 1272–1280, Beijing, China.
Z. Zhai, B. Liu, H. Xu, and P. Jia. 2011. Clustering
product features for opinion mining. In Proceedings
of WSDM, pages 347–354, Hong Kong, China.
</reference>
<page confidence="0.998842">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.517627">
<title confidence="0.997547">Aspect Term Extraction for Sentiment Analysis: New Datasets, Evaluation Measures and an Improved Unsupervised Method</title>
<author confidence="0.524786">Pavlopoulos Androutsopoulos</author>
<affiliation confidence="0.993497">Dept. of Informatics, Athens University of Economics and Business,</affiliation>
<web confidence="0.996637">http://nlp.cs.aueb.gr/</web>
<abstract confidence="0.999663789473684">Given a set of texts discussing a particular entity (e.g., customer reviews of a smartphone), aspect based sentiment analysis identifies prominent aspects of the entity (e.g., battery, screen) and an average sentiment score per aspect. We foon aspect term extraction one the core processing stages of extracts terms naming aspects. We make available three new arguing that they are better than previously available ones. We also introduce new measures for again arguing that they are better than previously used ones. Finally, we show how a popunsupervised can be improved by using continuous space vector representations of words and phrases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bagheri</author>
<author>M Saraee</author>
<author>F Jong</author>
</authors>
<title>An unsupervised aspect detection model for sentiment analysis of reviews.</title>
<date>2013</date>
<booktitle>In Proceedings of NLDB,</booktitle>
<volume>7934</volume>
<pages>140--151</pages>
<contexts>
<context position="14382" citStr="Bagheri et al., 2013" startWordPosition="2314" endWordPosition="2317">for small values of m. By contrast, many ATE methods (Hu and Liu, 2004; Popescu and Etzioni, 2005; Wei et al., 2010) devote much of their processing to identifying multi-word aspect terms. Figure 2: Percentage of (distinct) multi-word aspect terms among the most frequent aspect terms. 3 Evaluation measures We now discuss previous ATE evaluation measures, also introducing our own. 3.1 Precision, Recall, F-measure ATE methods are usually evaluated using precision, recall, and F-measure (Hu and Liu, 2004; Popescu and Etzioni, 2005; Kim and Hovy, 2006; Wei et al., 2010; Moghaddam and Ester, 2010; Bagheri et al., 2013), but it is often unclear if these measures are applied to distinct aspect terms (no duplicates) or aspect term occurrences. In the former case, each method is expected to return a set A of distinct aspect terms, to be compared to the set G of distinct aspect terms the human annotators identified in the texts. TP (true positives) is |AnG|, FP (false positives) is |A\G|, FN (false negatives) is |G \ A|, and precision (P), recall (R), F = .P. Pare defined as usually: P = TP = TP ( ) TP + FP , R TP + FN 1 This way, however, precision, recall, and Fmeasure assign the same importance to all the dis</context>
</contexts>
<marker>Bagheri, Saraee, Jong, 2013</marker>
<rawString>A. Bagheri, M. Saraee, and F. Jong. 2013. An unsupervised aspect detection model for sentiment analysis of reviews. In Proceedings of NLDB, volume 7934, pages 140–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>M Dredze</author>
<author>F Pereira</author>
</authors>
<title>Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>440--447</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="8186" citStr="Blitzer et al. (2007)" startWordPosition="1302" endWordPosition="1305">‘food’, ‘service’, ‘price’, ‘ambience’, ‘anecdotes’, or ‘miscellaneous’). For example, “The restaurant was expensive, but the menu was great” would be tagged with the coarse aspects ‘price’ and ‘food’. The coarse aspects, however, are not necessarily terms occurring in the sentence, and it is unclear how they were obtained. By contrast, we asked human annotators to mark the explicit aspect terms of each sentence, leaving the task of clustering the terms to produce coarser aspects for an aspect aggregation stage. The ‘Concept-Level Sentiment Analysis Challenge’ of ESWC 2014 uses the dataset of Blitzer et al. (2007), which contains customer reviews of 1Each aspect term occurrence is also annotated with a sentiment score. We do not discuss these scores here, since we focus on ATE. The same comment applies to the dataset of Ganu et al. (2009) and our datasets. 45 DVDs, books, kitchen appliances, and electronic products, with an overall sentiment score for each review. One of the challenge’s tasks requires systems to extract the aspects of each sentence and a sentiment score (positive or negative) per aspect.2 The aspects are intended to be concepts from ontologies, not simply aspect terms. The ontologies t</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings ofACL, pages 440–447, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Ding</author>
<author>B Liu</author>
<author>P S Yu</author>
</authors>
<title>A holistic lexiconbased approach to opinion mining.</title>
<date>2008</date>
<booktitle>In Proceedings of WSDM,</booktitle>
<pages>231--240</pages>
<location>Palo Alto, CA, USA.</location>
<contexts>
<context position="6479" citStr="Ding et al., 2008" startWordPosition="1031" endWordPosition="1034"> show (Section 4) how the popular unsupervised ATE method of Hu and Liu (2004), can be extended with continuous space word vectors (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). Using our datasets and evaluation measures, we demonstrate (Section 5) that the extended method performs better. 2 Datasets We first discuss previous datasets that have been used for ATE, and we then introduce our own. 2.1 Previous datasets So far, ATE methods have been evaluated mainly on customer reviews, often from the consumer electronics domain (Hu and Liu, 2004; Popescu and Etzioni, 2005; Ding et al., 2008). The most commonly used dataset is that of Hu and Liu (2004), which contains reviews of only five particular electronic products (e.g., Nikon Coolpix 4300). Each sentence is annotated with aspect terms, but inter-annotator agreement has not been reported.1 All the sentences appear to have been selected to express clear positive or negative opinions. There are no sentences expressing conflicting opinions about aspect terms (e.g., “The screen is clear but small”), nor are there any sentences that do not express opinions about their aspect terms (e.g., “It has a 4.8-inch screen”). Hence, the dat</context>
</contexts>
<marker>Ding, Liu, Yu, 2008</marker>
<rawString>X. Ding, B. Liu, and P. S. Yu. 2008. A holistic lexiconbased approach to opinion mining. In Proceedings of WSDM, pages 231–240, Palo Alto, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ganu</author>
<author>N Elhadad</author>
<author>A Marian</author>
</authors>
<title>Beyond the stars: Improving rating predictions using review text content.</title>
<date>2009</date>
<booktitle>In Proceedings of WebDB,</booktitle>
<location>Providence, RI, USA.</location>
<contexts>
<context position="7437" citStr="Ganu et al. (2009)" startWordPosition="1180" endWordPosition="1183">ve opinions. There are no sentences expressing conflicting opinions about aspect terms (e.g., “The screen is clear but small”), nor are there any sentences that do not express opinions about their aspect terms (e.g., “It has a 4.8-inch screen”). Hence, the dataset is not entirely representative of product reviews. By contrast, our datasets, discussed below, contain reviews from three domains, including sentences that express conflicting or no opinions about aspect terms, they concern many more target entities (not just five), and we have also measured inter-annotator agreement. The dataset of Ganu et al. (2009), on which one of our datasets is based, is also popular. In the original dataset, each sentence is tagged with coarse aspects (‘food’, ‘service’, ‘price’, ‘ambience’, ‘anecdotes’, or ‘miscellaneous’). For example, “The restaurant was expensive, but the menu was great” would be tagged with the coarse aspects ‘price’ and ‘food’. The coarse aspects, however, are not necessarily terms occurring in the sentence, and it is unclear how they were obtained. By contrast, we asked human annotators to mark the explicit aspect terms of each sentence, leaving the task of clustering the terms to produce coa</context>
<context position="9504" citStr="Ganu et al. (2009)" startWordPosition="1511" endWordPosition="1514">is currently available. Overall, the previous datasets are not entirely satisfactory, because they contain reviews from a particular domain only, or reviews for very few target entities, or their sentences are not entirely representative of customer reviews, or they do not contain annotations for aspect terms, or no inter-annotator agreement has been reported. To address these issues, we provide three new ATE datasets, which contain customer reviews of restaurants, hotels, and laptops, respectively.3 2.2 Our datasets The restaurants dataset contains 3,710 English sentences from the reviews of Ganu et al. (2009).4 We asked human annotators to tag the aspect terms of each sentence. In “The dessert was divine”, for example, the annotators would tag the aspect term ‘dessert’. In a sentence like “The restaurant was expensive, but the menu was great”, the annotators were instructed to tag only the explicitly mentioned aspect term ‘menu’. The sentence also refers to the prices, and a possibility would be to add ‘price’ as an implicit aspect term, but we do not consider implicit aspect terms in this paper. We used nine annotators for the restaurant reviews. Each sentence was processed by a single annotator,</context>
</contexts>
<marker>Ganu, Elhadad, Marian, 2009</marker>
<rawString>G. Ganu, N. Elhadad, and A. Marian. 2009. Beyond the stars: Improving rating predictions using review text content. In Proceedings of WebDB, Providence, RI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of KDD,</booktitle>
<pages>168--177</pages>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="5939" citStr="Hu and Liu (2004)" startWordPosition="943" endWordPosition="946">ortant; and when precision, recall, and F-measure are computed over aspect term occurrences (tokens), methods that identify very few, but very frequent aspect terms may appear to perform much better than they actually do. We propose weighted variants of precision and recall, which take into account the rankings of the distinct aspect terms that are obtained when the distinct aspect terms are ordered by their true and predicted frequencies. We also compute the average weighted precision over several weighted recall levels. Thirdly, we show (Section 4) how the popular unsupervised ATE method of Hu and Liu (2004), can be extended with continuous space word vectors (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). Using our datasets and evaluation measures, we demonstrate (Section 5) that the extended method performs better. 2 Datasets We first discuss previous datasets that have been used for ATE, and we then introduce our own. 2.1 Previous datasets So far, ATE methods have been evaluated mainly on customer reviews, often from the consumer electronics domain (Hu and Liu, 2004; Popescu and Etzioni, 2005; Ding et al., 2008). The most commonly used dataset is that of Hu and Liu (2004</context>
<context position="13423" citStr="Hu and Liu (2004)" startWordPosition="2161" endWordPosition="2164">e number of rows, which may depend on the display size or user preferences. Figure 2 shows the percentage of distinct multiword aspect terms among the m most frequent distinct aspect terms, for different values of m, in 5Cohen’s Kappa cannot be used here, because the annotators may tag any word sequence of any sentence, which leads to a very large set of categories. A similar problem was reported by Kobayashi et al. (2007). 6A more general definition of prominence might also consider the average sentiment score of each distinct aspect term. 46 our three datasets and the electronics dataset of Hu and Liu (2004). There are many more single-word distinct aspect terms than multi-word distinct aspect terms, especially in the restaurant and hotel reviews. In the electronics and laptops datasets, the percentage of multi-word distinct aspect terms (e.g., ‘hard disk’) is higher, but most of the distinct aspect terms are still single-word, especially for small values of m. By contrast, many ATE methods (Hu and Liu, 2004; Popescu and Etzioni, 2005; Wei et al., 2010) devote much of their processing to identifying multi-word aspect terms. Figure 2: Percentage of (distinct) multi-word aspect terms among the most</context>
<context position="23070" citStr="Hu and Liu (2004)" startWordPosition="3883" endWordPosition="3886"> aspect terms have been returned. By contrast, we also measure weighted recall (Eq. 3), which examines how many of the (distinct) gold aspect terms have been returned in Am, also assigning more weight to the gold aspect terms the human annotators tagged more frequently. We also compute the average weighted precision (AWP), which is a combination of WPm and WRm, for a range of m values. 4 Aspect term extraction methods We implemented and evaluated four ATE methods: (i) a popular baseline (dubbed FREQ) that returns the most frequent distinct nouns and noun phrases, (ii) the well-known method of Hu and Liu (2004), which adds to the baseline pruning mechanisms and steps that detect more aspect terms (dubbed H&amp;L), (iii) an extension of the previous method (dubbed H&amp;L+W2V), with an extra pruning step we devised that uses the recently popular continuous space word vectors (Mikolov et al., 2013c), and (iv) a similar extension of FREQ (dubbed FREQ+W2V). All four methods are unsupervised, which is particularly important for ABSA systems intended to be used across domains with minimal changes. They return directly a list A of distinct aspect terms ordered by decreasing predicted frequency, rather than tagging</context>
<context position="26168" citStr="Hu and Liu (2004)" startWordPosition="4403" endWordPosition="4406">term t appears in noncompact form in a sentence if there are more than three other words (not words of t) between any two of the words of t in the sentence. For example, the candidate distinct aspect term ‘battery life screen’ appears in non-compact form in “battery life is way better than screen”. Secondly, if the p-support of a candidate distinct aspect term t is smaller than 3 and t is subsumed by another can9We use the default POS tagger of NLTK, and the chunker of NLTK trained on the Treebank corpus; see http: //nltk.org/. We convert all words to lower-case. 10Some details of the work of Hu and Liu (2004) were not entirely clear to us. The discussion here and our implementation reflect our understanding. 49 didate distinct aspect term t′, then t is discarded (lines 21–23). Subsequently, a set of ‘opinion adjectives’ is formed; for each sentence and each candidate distinct aspect term t that occurs in the sentence, the closest to t adjective of the sentence (if there is one) is added to the set of opinion adjectives (lines 25-27). The sentences are then re-scanned; if a sentence does not contain any candidate aspect term, but contains an opinion adjective, then the nearest noun to the opinion a</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proceedings of KDD, pages 168–177, Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalervo J¨arvelin</author>
<author>Jaana Kek¨al¨ainen</author>
</authors>
<title>Cumulated gain-based evaluation of IR techniques.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>4</issue>
<pages>446</pages>
<marker>J¨arvelin, Kek¨al¨ainen, 2002</marker>
<rawString>Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems, 20(4):422– 446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-M Kim</author>
<author>E Hovy</author>
</authors>
<title>Extracting opinions, opinion holders, and topics expressed in online news media text.</title>
<date>2006</date>
<booktitle>In Proceedings of SST,</booktitle>
<pages>1--8</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="14314" citStr="Kim and Hovy, 2006" startWordPosition="2302" endWordPosition="2305">t of the distinct aspect terms are still single-word, especially for small values of m. By contrast, many ATE methods (Hu and Liu, 2004; Popescu and Etzioni, 2005; Wei et al., 2010) devote much of their processing to identifying multi-word aspect terms. Figure 2: Percentage of (distinct) multi-word aspect terms among the most frequent aspect terms. 3 Evaluation measures We now discuss previous ATE evaluation measures, also introducing our own. 3.1 Precision, Recall, F-measure ATE methods are usually evaluated using precision, recall, and F-measure (Hu and Liu, 2004; Popescu and Etzioni, 2005; Kim and Hovy, 2006; Wei et al., 2010; Moghaddam and Ester, 2010; Bagheri et al., 2013), but it is often unclear if these measures are applied to distinct aspect terms (no duplicates) or aspect term occurrences. In the former case, each method is expected to return a set A of distinct aspect terms, to be compared to the set G of distinct aspect terms the human annotators identified in the texts. TP (true positives) is |AnG|, FP (false positives) is |A\G|, FN (false negatives) is |G \ A|, and precision (P), recall (R), F = .P. Pare defined as usually: P = TP = TP ( ) TP + FP , R TP + FN 1 This way, however, preci</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>S.-M. Kim and E. Hovy. 2006. Extracting opinions, opinion holders, and topics expressed in online news media text. In Proceedings of SST, pages 1–8, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kobayashi</author>
<author>K Inui</author>
<author>Y Matsumoto</author>
</authors>
<title>Extracting aspect-evaluation and aspect-of relations in opinion mining.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>1065--1074</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="13232" citStr="Kobayashi et al. (2007)" startWordPosition="2129" endWordPosition="2132">of m.6 In a system like the one of Fig. 1, for example, if we ignore aspect aggregation, each row will report the average sentiment score of a single frequent distinct aspect term, and m will be the number of rows, which may depend on the display size or user preferences. Figure 2 shows the percentage of distinct multiword aspect terms among the m most frequent distinct aspect terms, for different values of m, in 5Cohen’s Kappa cannot be used here, because the annotators may tag any word sequence of any sentence, which leads to a very large set of categories. A similar problem was reported by Kobayashi et al. (2007). 6A more general definition of prominence might also consider the average sentiment score of each distinct aspect term. 46 our three datasets and the electronics dataset of Hu and Liu (2004). There are many more single-word distinct aspect terms than multi-word distinct aspect terms, especially in the restaurant and hotel reviews. In the electronics and laptops datasets, the percentage of multi-word distinct aspect terms (e.g., ‘hard disk’) is higher, but most of the distinct aspect terms are still single-word, especially for small values of m. By contrast, many ATE methods (Hu and Liu, 2004;</context>
</contexts>
<marker>Kobayashi, Inui, Matsumoto, 2007</marker>
<rawString>N. Kobayashi, K. Inui, and Y. Matsumoto. 2007. Extracting aspect-evaluation and aspect-of relations in opinion mining. In Proceedings of EMNLP-CoNLL, pages 1065–1074, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
<author>M Hu</author>
<author>J Cheng</author>
</authors>
<title>Opinion observer: analyzing and comparing opinions on the web.</title>
<date>2005</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>342--351</pages>
<location>Chiba, Japan.</location>
<contexts>
<context position="15268" citStr="Liu et al., 2005" startWordPosition="2476" endWordPosition="2479"> human annotators identified in the texts. TP (true positives) is |AnG|, FP (false positives) is |A\G|, FN (false negatives) is |G \ A|, and precision (P), recall (R), F = .P. Pare defined as usually: P = TP = TP ( ) TP + FP , R TP + FN 1 This way, however, precision, recall, and Fmeasure assign the same importance to all the distinct aspect terms, whereas missing, for example, a more frequent (more frequently discussed) distinct aspect term should probably be penalized more heavily than missing a less frequent one. When precision, recall, and F-measure are applied to aspect term occurrences (Liu et al., 2005), TP is the number of aspect term occurrences tagged (each term occurrence) both by the method being evaluated and the human annotators, FP is the number of aspect term occurrences tagged by the method but not the human annotators, and FN is the number of aspect term occurrences tagged by the human annotators but not the method. The three measures are then defined as above. They now assign more importance to frequently occurring distinct aspect terms, but they can produce misleadingly high scores when only a few, but very frequent distinct aspect terms are handled correctly. Furthermore, the o</context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>B. Liu, M. Hu, and J. Cheng. 2005. Opinion observer: analyzing and comparing opinions on the web. In Proceedings of WWW, pages 342–351, Chiba, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool.</publisher>
<contexts>
<context position="1657" citStr="Liu, 2012" startWordPosition="256" endWordPosition="257">, consumers often search the Web for expert reviews, but increasingly also for opinions of other consumers, expressed in blogs, social networks etc. Many useful opinions are expressed in text-only form (e.g., in tweets). It is then desirable to extract aspects (e.g., screen, battery) from the texts that discuss a particular entity (e.g., a smartphone), i.e., figure out what is being discussed, and also estimate aspect sentiment scores, i.e., how positive or negative the (usually average) sentiment for each aspect is. These two goals are jointly known as Aspect Based Sentiment Analysis (ABSA) (Liu, 2012). In this paper, we consider free text customer reviews of products and services; ABSA, however, is also applicable to texts about other kinds of entities (e.g., politicians, organizations). We assume that a search engine retrieves customer reviews about a particular target entity (product or Figure 1: Automatically extracted prominent aspects (shown as clusters of aspect terms) and average aspect sentiment scores of a target entity. service), that multiple reviews written by different customers are retrieved for each target entity, and that the ultimate goal is to produce a table like the one</context>
<context position="3360" citStr="Liu, 2012" startWordPosition="534" endWordPosition="535">ing multi-word terms (e.g., ‘hard disk’) (Liu, 2012; Long et al., 2010; Snyder and Barzilay, 2007; Yu et al., 2011). At the end of this stage, each aspect term is taken to be the name of a different aspect, but aspect terms may subsequently be clustered during aspect aggregation; see below. Aspect term sentiment estimation: This stage estimates the polarity and possibly also the intensity (e.g., strongly negative, mildly positive) of the opinions for each aspect term of the target entity, usually averaged over several texts. Classifying texts by sentiment polarity is a popular research topic (Liu, 2012; Pang and Lee, 2005; Tsytsarau and Palpanas, 2012). The goal, however, in this 44 Proceedings of the 5th Workshop on Language Anal sis for Social Media (LASM) @ EACL 2014, pages 44–52, Gothenburg, Sweden, April 26-30 2014. (c 2014 Association for Computational Linguistics ABSA subtask is to estimate the (usually average) polarity and intensity of the opinions about particular aspect terms of the target entity. Aspect aggregation: Some systems group aspect terms that are synonyms or near-synonyms (e.g., ‘price’, ‘cost’) or, more generally, cluster aspect terms to obtain aspects of a coarser gr</context>
<context position="24178" citStr="Liu, 2012" startWordPosition="4064" endWordPosition="4065">ectly a list A of distinct aspect terms ordered by decreasing predicted frequency, rather than tagging aspect term occurrences, which would require computing the A list from the tagged occurrences before applying our evaluation measures (Section 3.2). 4.1 The FREQ baseline The FREQ baseline returns the most frequent (distinct) nouns and noun phrases of the reviews in each dataset (restaurants, hotels, laptops), ordered by decreasing sentence frequency (how many sentences contain the noun or noun phrase).9 This is a reasonably effective and popular baseline (Hu and Liu, 2004; Wei et al., 2010; Liu, 2012). 4.2 The H&amp;L method The method of Hu and Liu (2004), dubbed H&amp;L, first extracts all the distinct nouns and noun phrases from the reviews of each dataset (lines 3– 6 of Algorithm 1) and considers them candidate distinct aspect terms.10 It then forms longer candidate distinct aspect terms by concatenating pairs and triples of candidate aspect terms occurring in the same sentence, in the order they appear in the sentence (lines 7–11). For example, if ‘battery life’ and ‘screen’ occur in the same sentence (in this order), then ‘battery life screen’ will also become a candidate distinct aspect ter</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>B. Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Long</author>
<author>J Zhang</author>
<author>X Zhut</author>
</authors>
<title>A review selection approach for accurate feature rating estimation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>766--774</pages>
<location>Beijing, China.</location>
<contexts>
<context position="2821" citStr="Long et al., 2010" startWordPosition="443" endWordPosition="446"> the ultimate goal is to produce a table like the one of Fig. 1, which presents the most prominent aspects and average aspect sentiment scores of the target entity. Most ABSA systems in effect perform all or some of the following three subtasks: Aspect term extraction: Starting from texts about a particular target entity or entities of the same type as the target entity (e.g., laptop reviews), this stage extracts and possibly ranks by importance aspect terms, i.e., terms naming aspects (e.g., ‘battery’, ‘screen’) of the target entity, including multi-word terms (e.g., ‘hard disk’) (Liu, 2012; Long et al., 2010; Snyder and Barzilay, 2007; Yu et al., 2011). At the end of this stage, each aspect term is taken to be the name of a different aspect, but aspect terms may subsequently be clustered during aspect aggregation; see below. Aspect term sentiment estimation: This stage estimates the polarity and possibly also the intensity (e.g., strongly negative, mildly positive) of the opinions for each aspect term of the target entity, usually averaged over several texts. Classifying texts by sentiment polarity is a popular research topic (Liu, 2012; Pang and Lee, 2005; Tsytsarau and Palpanas, 2012). The goal</context>
<context position="4068" citStr="Long et al., 2010" startWordPosition="644" endWordPosition="647">ngs of the 5th Workshop on Language Anal sis for Social Media (LASM) @ EACL 2014, pages 44–52, Gothenburg, Sweden, April 26-30 2014. (c 2014 Association for Computational Linguistics ABSA subtask is to estimate the (usually average) polarity and intensity of the opinions about particular aspect terms of the target entity. Aspect aggregation: Some systems group aspect terms that are synonyms or near-synonyms (e.g., ‘price’, ‘cost’) or, more generally, cluster aspect terms to obtain aspects of a coarser granularity (e.g., ‘chicken’, ‘steak’, and ‘fish’ may all be replaced by ‘food’) (Liu, 2012; Long et al., 2010; Zhai et al., 2010; Zhai et al., 2011). A polarity (and intensity) score can then be computed for each coarser aspect (e.g., ‘food’) by combining (e.g., averaging) the polarity scores of the aspect terms that belong in the coarser aspect. In this paper, we focus on aspect term extraction (ATE). Our contribution is threefold. Firstly, we argue (Section 2) that previous ATE datasets are not entirely satisfactory, mostly because they contain reviews from a particular domain only (e.g., consumer electronics), or they contain reviews for very few target entities, or they do not contain annotations</context>
</contexts>
<marker>Long, Zhang, Zhut, 2010</marker>
<rawString>C. Long, J. Zhang, and X. Zhut. 2010. A review selection approach for accurate feature rating estimation. In Proceedings of COLING, pages 766–774, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>P Raghavan</author>
<author>H Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>C. D. Manning, P. Raghavan, and H. Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>K Chen</author>
<author>G Corrado</author>
<author>J Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at ICLR.</booktitle>
<contexts>
<context position="6013" citStr="Mikolov et al., 2013" startWordPosition="956" endWordPosition="959">ect term occurrences (tokens), methods that identify very few, but very frequent aspect terms may appear to perform much better than they actually do. We propose weighted variants of precision and recall, which take into account the rankings of the distinct aspect terms that are obtained when the distinct aspect terms are ordered by their true and predicted frequencies. We also compute the average weighted precision over several weighted recall levels. Thirdly, we show (Section 4) how the popular unsupervised ATE method of Hu and Liu (2004), can be extended with continuous space word vectors (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). Using our datasets and evaluation measures, we demonstrate (Section 5) that the extended method performs better. 2 Datasets We first discuss previous datasets that have been used for ATE, and we then introduce our own. 2.1 Previous datasets So far, ATE methods have been evaluated mainly on customer reviews, often from the consumer electronics domain (Hu and Liu, 2004; Popescu and Etzioni, 2005; Ding et al., 2008). The most commonly used dataset is that of Hu and Liu (2004), which contains reviews of only five particular electronic products (e.g</context>
<context position="23352" citStr="Mikolov et al., 2013" startWordPosition="3931" endWordPosition="3934">compute the average weighted precision (AWP), which is a combination of WPm and WRm, for a range of m values. 4 Aspect term extraction methods We implemented and evaluated four ATE methods: (i) a popular baseline (dubbed FREQ) that returns the most frequent distinct nouns and noun phrases, (ii) the well-known method of Hu and Liu (2004), which adds to the baseline pruning mechanisms and steps that detect more aspect terms (dubbed H&amp;L), (iii) an extension of the previous method (dubbed H&amp;L+W2V), with an extra pruning step we devised that uses the recently popular continuous space word vectors (Mikolov et al., 2013c), and (iv) a similar extension of FREQ (dubbed FREQ+W2V). All four methods are unsupervised, which is particularly important for ABSA systems intended to be used across domains with minimal changes. They return directly a list A of distinct aspect terms ordered by decreasing predicted frequency, rather than tagging aspect term occurrences, which would require computing the A list from the tagged occurrences before applying our evaluation measures (Section 3.2). 4.1 The FREQ baseline The FREQ baseline returns the most frequent (distinct) nouns and noun phrases of the reviews in each dataset (</context>
<context position="28322" citStr="Mikolov et al., 2013" startWordPosition="4797" endWordPosition="4800"> nonCompact[t] += 1 21: for t in terms do 22: if nonCompact[t]&gt;1 V (3 t’: t’ in terms n t in t’ n psupport[t]&lt;3) then 23: terms.remove(t) 24: adjs = new Set(String) 25: for s in sentences do 26: if 3 t: t in terms n t in s then 27: adjs.add(POSTagger(s).getNearestAdj(t)) 28: for s in sentences do 29: if -3 t: t in terms n t in s n 3 a: a in adjs n a in s then 30: t = POSTagger(s).getNearestNoun(adjs) 31: terms.add(t) 32: return psupport.keysSortedByValue() 4.3 The H&amp;L+W2V method We extended H&amp;L by including an additional pruning step that uses continuous vector space representations of words (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). The vector representations of the words are proCentroid Closest Wikipedia words Com. lang. only, however, so, way, because Restaurants meal, meals, breakfast, wingstreet, snacks Hotels restaurant, guests, residence, bed, hotels Laptops gameport, hardware, hd floppy, pcs, apple macintosh Table 1: Wikipedia words closest to the common language and domain centroids. duced by using a neural network language model, whose inputs are the vectors of the words occurring in each sentence, treated as latent variables to be learned. We used the English Wik</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of Workshop at ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>I Sutskever</author>
<author>K Chen</author>
<author>G Corrado</author>
<author>J Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="6013" citStr="Mikolov et al., 2013" startWordPosition="956" endWordPosition="959">ect term occurrences (tokens), methods that identify very few, but very frequent aspect terms may appear to perform much better than they actually do. We propose weighted variants of precision and recall, which take into account the rankings of the distinct aspect terms that are obtained when the distinct aspect terms are ordered by their true and predicted frequencies. We also compute the average weighted precision over several weighted recall levels. Thirdly, we show (Section 4) how the popular unsupervised ATE method of Hu and Liu (2004), can be extended with continuous space word vectors (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). Using our datasets and evaluation measures, we demonstrate (Section 5) that the extended method performs better. 2 Datasets We first discuss previous datasets that have been used for ATE, and we then introduce our own. 2.1 Previous datasets So far, ATE methods have been evaluated mainly on customer reviews, often from the consumer electronics domain (Hu and Liu, 2004; Popescu and Etzioni, 2005; Ding et al., 2008). The most commonly used dataset is that of Hu and Liu (2004), which contains reviews of only five particular electronic products (e.g</context>
<context position="23352" citStr="Mikolov et al., 2013" startWordPosition="3931" endWordPosition="3934">compute the average weighted precision (AWP), which is a combination of WPm and WRm, for a range of m values. 4 Aspect term extraction methods We implemented and evaluated four ATE methods: (i) a popular baseline (dubbed FREQ) that returns the most frequent distinct nouns and noun phrases, (ii) the well-known method of Hu and Liu (2004), which adds to the baseline pruning mechanisms and steps that detect more aspect terms (dubbed H&amp;L), (iii) an extension of the previous method (dubbed H&amp;L+W2V), with an extra pruning step we devised that uses the recently popular continuous space word vectors (Mikolov et al., 2013c), and (iv) a similar extension of FREQ (dubbed FREQ+W2V). All four methods are unsupervised, which is particularly important for ABSA systems intended to be used across domains with minimal changes. They return directly a list A of distinct aspect terms ordered by decreasing predicted frequency, rather than tagging aspect term occurrences, which would require computing the A list from the tagged occurrences before applying our evaluation measures (Section 3.2). 4.1 The FREQ baseline The FREQ baseline returns the most frequent (distinct) nouns and noun phrases of the reviews in each dataset (</context>
<context position="28322" citStr="Mikolov et al., 2013" startWordPosition="4797" endWordPosition="4800"> nonCompact[t] += 1 21: for t in terms do 22: if nonCompact[t]&gt;1 V (3 t’: t’ in terms n t in t’ n psupport[t]&lt;3) then 23: terms.remove(t) 24: adjs = new Set(String) 25: for s in sentences do 26: if 3 t: t in terms n t in s then 27: adjs.add(POSTagger(s).getNearestAdj(t)) 28: for s in sentences do 29: if -3 t: t in terms n t in s n 3 a: a in adjs n a in s then 30: t = POSTagger(s).getNearestNoun(adjs) 31: terms.add(t) 32: return psupport.keysSortedByValue() 4.3 The H&amp;L+W2V method We extended H&amp;L by including an additional pruning step that uses continuous vector space representations of words (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). The vector representations of the words are proCentroid Closest Wikipedia words Com. lang. only, however, so, way, because Restaurants meal, meals, breakfast, wingstreet, snacks Hotels restaurant, guests, residence, bed, hotels Laptops gameport, hardware, hd floppy, pcs, apple macintosh Table 1: Wikipedia words closest to the common language and domain centroids. duced by using a neural network language model, whose inputs are the vectors of the words occurring in each sentence, treated as latent variables to be learned. We used the English Wik</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>W-T Yih</author>
<author>G Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL HLT.</booktitle>
<contexts>
<context position="6013" citStr="Mikolov et al., 2013" startWordPosition="956" endWordPosition="959">ect term occurrences (tokens), methods that identify very few, but very frequent aspect terms may appear to perform much better than they actually do. We propose weighted variants of precision and recall, which take into account the rankings of the distinct aspect terms that are obtained when the distinct aspect terms are ordered by their true and predicted frequencies. We also compute the average weighted precision over several weighted recall levels. Thirdly, we show (Section 4) how the popular unsupervised ATE method of Hu and Liu (2004), can be extended with continuous space word vectors (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). Using our datasets and evaluation measures, we demonstrate (Section 5) that the extended method performs better. 2 Datasets We first discuss previous datasets that have been used for ATE, and we then introduce our own. 2.1 Previous datasets So far, ATE methods have been evaluated mainly on customer reviews, often from the consumer electronics domain (Hu and Liu, 2004; Popescu and Etzioni, 2005; Ding et al., 2008). The most commonly used dataset is that of Hu and Liu (2004), which contains reviews of only five particular electronic products (e.g</context>
<context position="23352" citStr="Mikolov et al., 2013" startWordPosition="3931" endWordPosition="3934">compute the average weighted precision (AWP), which is a combination of WPm and WRm, for a range of m values. 4 Aspect term extraction methods We implemented and evaluated four ATE methods: (i) a popular baseline (dubbed FREQ) that returns the most frequent distinct nouns and noun phrases, (ii) the well-known method of Hu and Liu (2004), which adds to the baseline pruning mechanisms and steps that detect more aspect terms (dubbed H&amp;L), (iii) an extension of the previous method (dubbed H&amp;L+W2V), with an extra pruning step we devised that uses the recently popular continuous space word vectors (Mikolov et al., 2013c), and (iv) a similar extension of FREQ (dubbed FREQ+W2V). All four methods are unsupervised, which is particularly important for ABSA systems intended to be used across domains with minimal changes. They return directly a list A of distinct aspect terms ordered by decreasing predicted frequency, rather than tagging aspect term occurrences, which would require computing the A list from the tagged occurrences before applying our evaluation measures (Section 3.2). 4.1 The FREQ baseline The FREQ baseline returns the most frequent (distinct) nouns and noun phrases of the reviews in each dataset (</context>
<context position="28322" citStr="Mikolov et al., 2013" startWordPosition="4797" endWordPosition="4800"> nonCompact[t] += 1 21: for t in terms do 22: if nonCompact[t]&gt;1 V (3 t’: t’ in terms n t in t’ n psupport[t]&lt;3) then 23: terms.remove(t) 24: adjs = new Set(String) 25: for s in sentences do 26: if 3 t: t in terms n t in s then 27: adjs.add(POSTagger(s).getNearestAdj(t)) 28: for s in sentences do 29: if -3 t: t in terms n t in s n 3 a: a in adjs n a in s then 30: t = POSTagger(s).getNearestNoun(adjs) 31: terms.add(t) 32: return psupport.keysSortedByValue() 4.3 The H&amp;L+W2V method We extended H&amp;L by including an additional pruning step that uses continuous vector space representations of words (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). The vector representations of the words are proCentroid Closest Wikipedia words Com. lang. only, however, so, way, because Restaurants meal, meals, breakfast, wingstreet, snacks Hotels restaurant, guests, residence, bed, hotels Laptops gameport, hardware, hd floppy, pcs, apple macintosh Table 1: Wikipedia words closest to the common language and domain centroids. duced by using a neural network language model, whose inputs are the vectors of the words occurring in each sentence, treated as latent variables to be learned. We used the English Wik</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>T. Mikolov, W.-T. Yih, and G. Zweig. 2013c. Linguistic regularities in continuous space word representations. In Proceedings of NAACL HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Moghaddam</author>
<author>M Ester</author>
</authors>
<title>Opinion digger: an unsupervised opinion miner from unstructured product reviews.</title>
<date>2010</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>1825--1828</pages>
<location>Toronto, ON,</location>
<contexts>
<context position="14359" citStr="Moghaddam and Ester, 2010" startWordPosition="2310" endWordPosition="2313">ll single-word, especially for small values of m. By contrast, many ATE methods (Hu and Liu, 2004; Popescu and Etzioni, 2005; Wei et al., 2010) devote much of their processing to identifying multi-word aspect terms. Figure 2: Percentage of (distinct) multi-word aspect terms among the most frequent aspect terms. 3 Evaluation measures We now discuss previous ATE evaluation measures, also introducing our own. 3.1 Precision, Recall, F-measure ATE methods are usually evaluated using precision, recall, and F-measure (Hu and Liu, 2004; Popescu and Etzioni, 2005; Kim and Hovy, 2006; Wei et al., 2010; Moghaddam and Ester, 2010; Bagheri et al., 2013), but it is often unclear if these measures are applied to distinct aspect terms (no duplicates) or aspect term occurrences. In the former case, each method is expected to return a set A of distinct aspect terms, to be compared to the set G of distinct aspect terms the human annotators identified in the texts. TP (true positives) is |AnG|, FP (false positives) is |A\G|, FN (false negatives) is |G \ A|, and precision (P), recall (R), F = .P. Pare defined as usually: P = TP = TP ( ) TP + FP , R TP + FN 1 This way, however, precision, recall, and Fmeasure assign the same im</context>
</contexts>
<marker>Moghaddam, Ester, 2010</marker>
<rawString>S. Moghaddam and M. Ester. 2010. Opinion digger: an unsupervised opinion miner from unstructured product reviews. In Proceedings of CIKM, pages 1825–1828, Toronto, ON, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>115--124</pages>
<location>Ann Arbor, MI, USA.</location>
<contexts>
<context position="3380" citStr="Pang and Lee, 2005" startWordPosition="536" endWordPosition="539">ord terms (e.g., ‘hard disk’) (Liu, 2012; Long et al., 2010; Snyder and Barzilay, 2007; Yu et al., 2011). At the end of this stage, each aspect term is taken to be the name of a different aspect, but aspect terms may subsequently be clustered during aspect aggregation; see below. Aspect term sentiment estimation: This stage estimates the polarity and possibly also the intensity (e.g., strongly negative, mildly positive) of the opinions for each aspect term of the target entity, usually averaged over several texts. Classifying texts by sentiment polarity is a popular research topic (Liu, 2012; Pang and Lee, 2005; Tsytsarau and Palpanas, 2012). The goal, however, in this 44 Proceedings of the 5th Workshop on Language Anal sis for Social Media (LASM) @ EACL 2014, pages 44–52, Gothenburg, Sweden, April 26-30 2014. (c 2014 Association for Computational Linguistics ABSA subtask is to estimate the (usually average) polarity and intensity of the opinions about particular aspect terms of the target entity. Aspect aggregation: Some systems group aspect terms that are synonyms or near-synonyms (e.g., ‘price’, ‘cost’) or, more generally, cluster aspect terms to obtain aspects of a coarser granularity (e.g., ‘ch</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>B. Pang and L. Lee. 2005. Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings ofACL, pages 115–124, Ann Arbor, MI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>339--346</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="6459" citStr="Popescu and Etzioni, 2005" startWordPosition="1027" endWordPosition="1030"> recall levels. Thirdly, we show (Section 4) how the popular unsupervised ATE method of Hu and Liu (2004), can be extended with continuous space word vectors (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). Using our datasets and evaluation measures, we demonstrate (Section 5) that the extended method performs better. 2 Datasets We first discuss previous datasets that have been used for ATE, and we then introduce our own. 2.1 Previous datasets So far, ATE methods have been evaluated mainly on customer reviews, often from the consumer electronics domain (Hu and Liu, 2004; Popescu and Etzioni, 2005; Ding et al., 2008). The most commonly used dataset is that of Hu and Liu (2004), which contains reviews of only five particular electronic products (e.g., Nikon Coolpix 4300). Each sentence is annotated with aspect terms, but inter-annotator agreement has not been reported.1 All the sentences appear to have been selected to express clear positive or negative opinions. There are no sentences expressing conflicting opinions about aspect terms (e.g., “The screen is clear but small”), nor are there any sentences that do not express opinions about their aspect terms (e.g., “It has a 4.8-inch scre</context>
<context position="13858" citStr="Popescu and Etzioni, 2005" startWordPosition="2230" endWordPosition="2233"> 6A more general definition of prominence might also consider the average sentiment score of each distinct aspect term. 46 our three datasets and the electronics dataset of Hu and Liu (2004). There are many more single-word distinct aspect terms than multi-word distinct aspect terms, especially in the restaurant and hotel reviews. In the electronics and laptops datasets, the percentage of multi-word distinct aspect terms (e.g., ‘hard disk’) is higher, but most of the distinct aspect terms are still single-word, especially for small values of m. By contrast, many ATE methods (Hu and Liu, 2004; Popescu and Etzioni, 2005; Wei et al., 2010) devote much of their processing to identifying multi-word aspect terms. Figure 2: Percentage of (distinct) multi-word aspect terms among the most frequent aspect terms. 3 Evaluation measures We now discuss previous ATE evaluation measures, also introducing our own. 3.1 Precision, Recall, F-measure ATE methods are usually evaluated using precision, recall, and F-measure (Hu and Liu, 2004; Popescu and Etzioni, 2005; Kim and Hovy, 2006; Wei et al., 2010; Moghaddam and Ester, 2010; Bagheri et al., 2013), but it is often unclear if these measures are applied to distinct aspect t</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of HLT-EMNLP, pages 339–346, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sakai</author>
</authors>
<title>Ranking the NTCIR systems based on multigrade relevance.</title>
<date>2004</date>
<booktitle>In Proceedings of AIRS,</booktitle>
<pages>251--262</pages>
<location>Beijing, China.</location>
<contexts>
<context position="19798" citStr="Sakai, 2004" startWordPosition="3284" endWordPosition="3285">nterpolated) weighted precision 7In our experiments, we exclude from G aspect terms tagged by the annotators only once. 8With supervised methods, we perform a 10-fold crossvalidation for each m, and we macro-average WPm, WRm over the folds. We provide our datasets partitioned in folds. (AWP) of each method over 11 recall levels: WPint(r) = max mE{1,...,JAJ}, WRm &gt; r AWP is similar to average (interpolated) precision (AP), which is used to summarize the tradeoff between (unweighted) precision and recall. 3.3 Other related measures Yu at al. (2011) used nDCG@m (J¨arvelin and Kek¨al¨ainen, 2002; Sakai, 2004; Manning et al., 2008), defined below, to evaluate each list of m distinct aspect terms returned by an ATE method. 1 nDCG@m = Z Z is a normalization factor to ensure that a perfect ranking gets nDCG@m = 1, and t(i) is a reward function for a term placed at position i of the returned list. In the work of Yu et al., t(i) = 1 if the term at position i is not important (as judged by a human), t(i) = 2 if the term is ‘ordinary’, and t(i) = 3 if it is important. The logarithm is used to reduce the reward for distinct aspect terms placed at lower positions of the returned list. The nDCG@m measure is</context>
</contexts>
<marker>Sakai, 2004</marker>
<rawString>T. Sakai. 2004. Ranking the NTCIR systems based on multigrade relevance. In Proceedings of AIRS, pages 251–262, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>R Barzilay</author>
</authors>
<title>Multiple aspect ranking using the good grief algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>300--307</pages>
<location>Rochester, NY, USA.</location>
<contexts>
<context position="2848" citStr="Snyder and Barzilay, 2007" startWordPosition="447" endWordPosition="451">is to produce a table like the one of Fig. 1, which presents the most prominent aspects and average aspect sentiment scores of the target entity. Most ABSA systems in effect perform all or some of the following three subtasks: Aspect term extraction: Starting from texts about a particular target entity or entities of the same type as the target entity (e.g., laptop reviews), this stage extracts and possibly ranks by importance aspect terms, i.e., terms naming aspects (e.g., ‘battery’, ‘screen’) of the target entity, including multi-word terms (e.g., ‘hard disk’) (Liu, 2012; Long et al., 2010; Snyder and Barzilay, 2007; Yu et al., 2011). At the end of this stage, each aspect term is taken to be the name of a different aspect, but aspect terms may subsequently be clustered during aspect aggregation; see below. Aspect term sentiment estimation: This stage estimates the polarity and possibly also the intensity (e.g., strongly negative, mildly positive) of the opinions for each aspect term of the target entity, usually averaged over several texts. Classifying texts by sentiment polarity is a popular research topic (Liu, 2012; Pang and Lee, 2005; Tsytsarau and Palpanas, 2012). The goal, however, in this 44 Proce</context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>B. Snyder and R. Barzilay. 2007. Multiple aspect ranking using the good grief algorithm. In Proceedings of NAACL, pages 300–307, Rochester, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tsytsarau</author>
<author>T Palpanas</author>
</authors>
<title>Survey on mining subjective data on the web. Data Mining and Knowledge Discovery,</title>
<date>2012</date>
<contexts>
<context position="3411" citStr="Tsytsarau and Palpanas, 2012" startWordPosition="540" endWordPosition="543">rd disk’) (Liu, 2012; Long et al., 2010; Snyder and Barzilay, 2007; Yu et al., 2011). At the end of this stage, each aspect term is taken to be the name of a different aspect, but aspect terms may subsequently be clustered during aspect aggregation; see below. Aspect term sentiment estimation: This stage estimates the polarity and possibly also the intensity (e.g., strongly negative, mildly positive) of the opinions for each aspect term of the target entity, usually averaged over several texts. Classifying texts by sentiment polarity is a popular research topic (Liu, 2012; Pang and Lee, 2005; Tsytsarau and Palpanas, 2012). The goal, however, in this 44 Proceedings of the 5th Workshop on Language Anal sis for Social Media (LASM) @ EACL 2014, pages 44–52, Gothenburg, Sweden, April 26-30 2014. (c 2014 Association for Computational Linguistics ABSA subtask is to estimate the (usually average) polarity and intensity of the opinions about particular aspect terms of the target entity. Aspect aggregation: Some systems group aspect terms that are synonyms or near-synonyms (e.g., ‘price’, ‘cost’) or, more generally, cluster aspect terms to obtain aspects of a coarser granularity (e.g., ‘chicken’, ‘steak’, and ‘fish’ may</context>
</contexts>
<marker>Tsytsarau, Palpanas, 2012</marker>
<rawString>M. Tsytsarau and T. Palpanas. 2012. Survey on mining subjective data on the web. Data Mining and Knowledge Discovery, 24(3):478–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-P Wei</author>
<author>Y-M Chen</author>
<author>C-S Yang</author>
<author>C C Yang</author>
</authors>
<title>Understanding what concerns consumers: a semantic approach to product feature extraction from consumer reviews. Information Systems and E-Business</title>
<date>2010</date>
<tech>Management, 8(2):149–167.</tech>
<contexts>
<context position="13877" citStr="Wei et al., 2010" startWordPosition="2234" endWordPosition="2237"> of prominence might also consider the average sentiment score of each distinct aspect term. 46 our three datasets and the electronics dataset of Hu and Liu (2004). There are many more single-word distinct aspect terms than multi-word distinct aspect terms, especially in the restaurant and hotel reviews. In the electronics and laptops datasets, the percentage of multi-word distinct aspect terms (e.g., ‘hard disk’) is higher, but most of the distinct aspect terms are still single-word, especially for small values of m. By contrast, many ATE methods (Hu and Liu, 2004; Popescu and Etzioni, 2005; Wei et al., 2010) devote much of their processing to identifying multi-word aspect terms. Figure 2: Percentage of (distinct) multi-word aspect terms among the most frequent aspect terms. 3 Evaluation measures We now discuss previous ATE evaluation measures, also introducing our own. 3.1 Precision, Recall, F-measure ATE methods are usually evaluated using precision, recall, and F-measure (Hu and Liu, 2004; Popescu and Etzioni, 2005; Kim and Hovy, 2006; Wei et al., 2010; Moghaddam and Ester, 2010; Bagheri et al., 2013), but it is often unclear if these measures are applied to distinct aspect terms (no duplicates</context>
<context position="24166" citStr="Wei et al., 2010" startWordPosition="4060" endWordPosition="4063">s. They return directly a list A of distinct aspect terms ordered by decreasing predicted frequency, rather than tagging aspect term occurrences, which would require computing the A list from the tagged occurrences before applying our evaluation measures (Section 3.2). 4.1 The FREQ baseline The FREQ baseline returns the most frequent (distinct) nouns and noun phrases of the reviews in each dataset (restaurants, hotels, laptops), ordered by decreasing sentence frequency (how many sentences contain the noun or noun phrase).9 This is a reasonably effective and popular baseline (Hu and Liu, 2004; Wei et al., 2010; Liu, 2012). 4.2 The H&amp;L method The method of Hu and Liu (2004), dubbed H&amp;L, first extracts all the distinct nouns and noun phrases from the reviews of each dataset (lines 3– 6 of Algorithm 1) and considers them candidate distinct aspect terms.10 It then forms longer candidate distinct aspect terms by concatenating pairs and triples of candidate aspect terms occurring in the same sentence, in the order they appear in the sentence (lines 7–11). For example, if ‘battery life’ and ‘screen’ occur in the same sentence (in this order), then ‘battery life screen’ will also become a candidate distinc</context>
</contexts>
<marker>Wei, Chen, Yang, Yang, 2010</marker>
<rawString>C.-P. Wei, Y.-M. Chen, C.-S. Yang, and C. C Yang. 2010. Understanding what concerns consumers: a semantic approach to product feature extraction from consumer reviews. Information Systems and E-Business Management, 8(2):149–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yu</author>
<author>Z Zha</author>
<author>M Wang</author>
<author>T Chua</author>
</authors>
<title>Aspect ranking: identifying important product aspects from online consumer reviews.</title>
<date>2011</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>1496--1505</pages>
<location>Portland, OR, USA.</location>
<contexts>
<context position="2866" citStr="Yu et al., 2011" startWordPosition="452" endWordPosition="455">the one of Fig. 1, which presents the most prominent aspects and average aspect sentiment scores of the target entity. Most ABSA systems in effect perform all or some of the following three subtasks: Aspect term extraction: Starting from texts about a particular target entity or entities of the same type as the target entity (e.g., laptop reviews), this stage extracts and possibly ranks by importance aspect terms, i.e., terms naming aspects (e.g., ‘battery’, ‘screen’) of the target entity, including multi-word terms (e.g., ‘hard disk’) (Liu, 2012; Long et al., 2010; Snyder and Barzilay, 2007; Yu et al., 2011). At the end of this stage, each aspect term is taken to be the name of a different aspect, but aspect terms may subsequently be clustered during aspect aggregation; see below. Aspect term sentiment estimation: This stage estimates the polarity and possibly also the intensity (e.g., strongly negative, mildly positive) of the opinions for each aspect term of the target entity, usually averaged over several texts. Classifying texts by sentiment polarity is a popular research topic (Liu, 2012; Pang and Lee, 2005; Tsytsarau and Palpanas, 2012). The goal, however, in this 44 Proceedings of the 5th </context>
</contexts>
<marker>Yu, Zha, Wang, Chua, 2011</marker>
<rawString>J. Yu, Z. Zha, M. Wang, and T. Chua. 2011. Aspect ranking: identifying important product aspects from online consumer reviews. In Proceedings of NAACL, pages 1496–1505, Portland, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhai</author>
<author>B Liu</author>
<author>H Xu</author>
<author>P Jia</author>
</authors>
<title>Grouping product features using semi-supervised learning with soft-constraints.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1272--1280</pages>
<location>Beijing, China.</location>
<contexts>
<context position="4087" citStr="Zhai et al., 2010" startWordPosition="648" endWordPosition="651">shop on Language Anal sis for Social Media (LASM) @ EACL 2014, pages 44–52, Gothenburg, Sweden, April 26-30 2014. (c 2014 Association for Computational Linguistics ABSA subtask is to estimate the (usually average) polarity and intensity of the opinions about particular aspect terms of the target entity. Aspect aggregation: Some systems group aspect terms that are synonyms or near-synonyms (e.g., ‘price’, ‘cost’) or, more generally, cluster aspect terms to obtain aspects of a coarser granularity (e.g., ‘chicken’, ‘steak’, and ‘fish’ may all be replaced by ‘food’) (Liu, 2012; Long et al., 2010; Zhai et al., 2010; Zhai et al., 2011). A polarity (and intensity) score can then be computed for each coarser aspect (e.g., ‘food’) by combining (e.g., averaging) the polarity scores of the aspect terms that belong in the coarser aspect. In this paper, we focus on aspect term extraction (ATE). Our contribution is threefold. Firstly, we argue (Section 2) that previous ATE datasets are not entirely satisfactory, mostly because they contain reviews from a particular domain only (e.g., consumer electronics), or they contain reviews for very few target entities, or they do not contain annotations for aspect terms. </context>
</contexts>
<marker>Zhai, Liu, Xu, Jia, 2010</marker>
<rawString>Z. Zhai, B. Liu, H. Xu, and P. Jia. 2010. Grouping product features using semi-supervised learning with soft-constraints. In Proceedings of COLING, pages 1272–1280, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhai</author>
<author>B Liu</author>
<author>H Xu</author>
<author>P Jia</author>
</authors>
<title>Clustering product features for opinion mining.</title>
<date>2011</date>
<booktitle>In Proceedings of WSDM,</booktitle>
<pages>347--354</pages>
<location>Hong Kong, China.</location>
<contexts>
<context position="4107" citStr="Zhai et al., 2011" startWordPosition="652" endWordPosition="655">al sis for Social Media (LASM) @ EACL 2014, pages 44–52, Gothenburg, Sweden, April 26-30 2014. (c 2014 Association for Computational Linguistics ABSA subtask is to estimate the (usually average) polarity and intensity of the opinions about particular aspect terms of the target entity. Aspect aggregation: Some systems group aspect terms that are synonyms or near-synonyms (e.g., ‘price’, ‘cost’) or, more generally, cluster aspect terms to obtain aspects of a coarser granularity (e.g., ‘chicken’, ‘steak’, and ‘fish’ may all be replaced by ‘food’) (Liu, 2012; Long et al., 2010; Zhai et al., 2010; Zhai et al., 2011). A polarity (and intensity) score can then be computed for each coarser aspect (e.g., ‘food’) by combining (e.g., averaging) the polarity scores of the aspect terms that belong in the coarser aspect. In this paper, we focus on aspect term extraction (ATE). Our contribution is threefold. Firstly, we argue (Section 2) that previous ATE datasets are not entirely satisfactory, mostly because they contain reviews from a particular domain only (e.g., consumer electronics), or they contain reviews for very few target entities, or they do not contain annotations for aspect terms. We constructed and m</context>
</contexts>
<marker>Zhai, Liu, Xu, Jia, 2011</marker>
<rawString>Z. Zhai, B. Liu, H. Xu, and P. Jia. 2011. Clustering product features for opinion mining. In Proceedings of WSDM, pages 347–354, Hong Kong, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003258">
<title confidence="0.992372">
Towards Tracking Political Sentiment through Microblog Data
</title>
<author confidence="0.997871">
Yu Wang
</author>
<affiliation confidence="0.989659">
Emory University
</affiliation>
<email confidence="0.991611">
yu.wang@emory.edu
</email>
<author confidence="0.970766">
Tom Clark
</author>
<affiliation confidence="0.970781">
Emory University
</affiliation>
<email confidence="0.984995">
tclark7@emory.edu
</email>
<author confidence="0.992817">
Jeffrey Staton
</author>
<affiliation confidence="0.986333">
Emory University
</affiliation>
<email confidence="0.991746">
jkstato@emory.edu
</email>
<author confidence="0.990308">
Eugene Agichtein
</author>
<affiliation confidence="0.978067">
Emory University
</affiliation>
<email confidence="0.999331">
eugene@mathcs.emory.edu
</email>
<sectionHeader confidence="0.993909" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953222222222">
People express and amplify political opin-
ions in Microblogs such as Twitter, espe-
cially when major political decisions are
made. Twitter provides a useful vehicle for
capturing and tracking popular opinion on
burning issues of the day. In this paper,
we focus on tracking the changes in polit-
ical sentiment related to the U.S. Supreme
Court (SCOTUS) and its decisions, fo-
cusing on the key dimensions on support,
emotional intensity, and polarity. Mea-
suring changes in these sentiment dimen-
sions could be useful for social and politi-
cal scientists, policy makers, and the pub-
lic. This preliminary work adapts existing
sentiment analysis techniques to these new
dimensions and the specifics of the cor-
pus (Twitter). We illustrate the promise
of our work with an important case study
of tracking sentiment change building up
to, and immediately following one recent
landmark Supreme Court decision. This
example illustrates how our work could
help answer fundamental research ques-
tions in political science about the nature
of Supreme Court power and its capacity
to influence public discourse.
</bodyText>
<sectionHeader confidence="0.931296" genericHeader="categories and subject descriptors">
1 Background and Motivation
</sectionHeader>
<bodyText confidence="0.999927723404256">
Political opinions are a popular topic in Mi-
croblogs. On June 26th, 2013, when the U.S.
Supreme Court announced the decision on the un-
constitutionality of the ”Defense of Marriage Act”
(DOMA), there were millions of Tweets about the
users’ opinions of the decision. In their Tweets,
people not only voice their opinions about the is-
sues at stake, expressing different dimensions of
sentiment, such as support or opposition to the de-
cision, or anger or happiness. Thus, simply ap-
plying traditional sentiment analysis scales such
as ”positive” vs. ”negative” classification would
not be sufficient to understand the public reaction
to political decisions.
Research on mass opinion and the Supreme
Court is valuable as it could shed light on the fun-
damental and related normative concerns about the
role of constitutional review in American gover-
nance, which emerge in a political system possess-
ing democratic institutions at cross-purposes. One
line of thought, beginning with Dahl (Dahl, 1957),
suggests that the Supreme Court of the United
States has a unique capacity among major institu-
tions of American government to leverage its legit-
imacy in order to change mass opinion regarding
salient policies. If the Dahl’s hypothesis is correct,
then the Supreme Court’s same-sex marriage deci-
sions should have resulted in a measurable change
in opinion. A primary finding about implication of
Dahl’s hypothesis is that the Court is polarizing,
creating more supportive opinions of the policies
it reviews among those who supported the pol-
icy before the decision and more negative opin-
ions among those who opposed the policy prior to
the decision (Franklin and Kosaki, 1989) (Johnson
and Martin, 1998).
We consider Twitter as important example of
social expression of opinion. Recent studies of
content on Twitter have revealed that 85% of Twit-
ter content is related to spreading and commenting
on headline news (Kwak et al., 2010); when users
talk about commercial brands in their Tweets,
about 20% of them have personal sentiment in-
volved (Jansen et al., 2009). These statistical evi-
dences imply that Twitter has became a portal for
public to express opinions. In the context of pol-
itics, Twitter content, together with Twitter users’
</bodyText>
<page confidence="0.984897">
88
</page>
<bodyText confidence="0.98941163768116">
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 88–93,
Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics
information, such as user’s profile and social net-
work, have shown reasonable power of detecting
user’s political leaning (Conover et al., 2011) and
predicting elections (Tumasjan et al., 2010). Al-
though promising, the effectiveness of using Twit-
ter content to measure public political opinions re-
mains unclear. Several studies show limited corre-
lation between sentiment on Twitter and political
polls in elections (Mejova et al., 2013) (O’Connor
et al., 2010). Our study mainly focuses on inves-
tigating sentiment on Twitter about U.S. Supreme
Court decisions.
We propose more fine-grained dimensions for
political sentiment analysis, such as supportive-
ness, emotional intensity and polarity, allowing
political science researchers, policy makers, and
the public to better comprehend the public reaction
to major political issues of the day. As we describe
below, these different dimensions of discourse on
Twitter allows examination of the multiple ways in
which discourse changes when the Supreme Court
makes a decision on a given issue of public policy.
Our dimensions also open the door to new avenues
of theorizing about the nature of public discourse
on policy debates.
Although general sentiment analysis has made
significant advances over the last decade (Pang et
al., 2002) (Pang and Lee, 2008) (Liu, 2012) (Wil-
son et al., 2009), and with the focus on certain
aspects, such as intensity (Wilson et al., 2004),
irony detection (Carvalho et al., 2009) and sar-
casm detection (Davidov et al., 2010), analyzing
Microblog content such as Twitter remains a chal-
lenging research topic (Reyes et al., 2012) (Vanin
et al., 2013) (Agarwal et al., 2011). Unlike previ-
ous work, we introduce and focus on sentiment di-
mensions particularly important for political anal-
ysis of Microblog text, and extend and adapt clas-
sification techniques accordingly. To make the
data and sentiment analysis results accessible for
researchers in other domain, we build a website to
visualize the sentiment dynamics over time and let
users download the data. Users could also define
their own topics of interest and perform deeper
analysis with keyword filtering and geolocation
filtering.
We present a case study in which our results
might be used to answer core questions in polit-
ical science about the nature of Supreme Court
influence on public opinion. Political scientists
have long been concerned with whether and how
Supreme Court decisions affect public opinion and
discourse about political topics (Hoekstra, 2003)
(Johnson and Martin, 1998) (Gibson et al., 2003).
Survey research on the subject has been limited in
two ways. Survey analysis, including panel de-
signs, rely on estimates near but never on the date
of particular decisions. In addition, all survey-
based research relies on estimates derived from an
instrument designed to elicit sentiment – survey
responses, useful as they are, do not reflect well
how public opinion is naturally expressed. Our
analysis allows for the examination of public opin-
ion as it is naturally expressed and in a way that is
precisely connected to the timing of decisions.
Next, we state the problem more formally, and
outline our approach and implementation.
</bodyText>
<sectionHeader confidence="0.57561" genericHeader="method">
2 Problem Statement and Approach
</sectionHeader>
<subsectionHeader confidence="0.983356">
2.1 Political Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.999277">
We propose three refinements to sentiment analy-
sis to quantify political opinions. Specifically, we
pose the following dimensions as particularly im-
portant for politics:
</bodyText>
<listItem confidence="0.999945666666667">
• Support: Whether a Tweet is Opposed, Neu-
tral, or Supportive regarding the topic.
• Emotional Intensity: Whether a Tweet is
emotionally Intense or Dispassionate.
• Sentiment Polarity: Whether a Tweet’s tone
is Angry, Neutral, or Pleased.
</listItem>
<subsectionHeader confidence="0.99716">
2.2 Approach
</subsectionHeader>
<bodyText confidence="0.999925384615385">
In this work, each of the proposed measures is
treated as a supervised classification problem. We
use multi-class classification algorithms to model
Support and Sentiment Polarity, and binary classi-
fication for Emotional Intensity and Sarcasm. Sec-
tion 3.2 describes the labels used to train the super-
vised classification models. Notice some classes
are more interesting than the others. For exam-
ple, the trends or ratio of opposed vs. supportive
Microblogs are more informative than the factual
ones. Particularly, we pay more attention to the
classes of opposed, supportive, intense, angry, and
pleased.
</bodyText>
<subsectionHeader confidence="0.998553">
2.3 Classifier Feature Groups
</subsectionHeader>
<bodyText confidence="0.997889333333333">
To classify the Microblog message into the classes
of interest, we develop 6 groups of features:
Popularity: Number of times the message has been
</bodyText>
<page confidence="0.998845">
89
</page>
<bodyText confidence="0.99664375">
posted or favored by users. As for a Tweet, this
feature means number of Retweets and favorites.
Capitalization and Punctuation.
N-gram of text: Unigram, bigram, and trigram of
the message text.
Sentiment score: The maximum, minimum, aver-
age and sum of sentiment score of terms and each
Part-of-Speech tags in the message text.
Counter factuality and temporal compression dic-
tionary: This feature counts the number of times
such words appear in the message text.
Political dictionary: Number of times a political-
related word appears in the message text.
We compute sentiment scores based on Senti-
WordNet1, a sentiment dictionary constructed on
WordNet.2 Political dictionary is built upon
political-related words in WordNet. As in this pa-
per, we construct a political dictionary with 56
words and phrases, such as “liberal”, “conserva-
tive”, and “freedom” etc.
</bodyText>
<sectionHeader confidence="0.961391" genericHeader="method">
3 Case Study: DOMA
</sectionHeader>
<bodyText confidence="0.999975285714286">
Our goal is to build and test classifiers that can dis-
tinguish political content between classes of inter-
est. Particularly, we focus on classifying Tweets
related to one of the most popular political topics,
“Defence of Marriage Act” or DOMA, as the tar-
get. The techniques can be easily generalized to
other political issues in Twitter.
</bodyText>
<subsectionHeader confidence="0.967149">
3.1 Dataset
</subsectionHeader>
<bodyText confidence="0.99976875">
In order to obtain relevant Tweets, we use Twit-
ter streaming API to track representative key-
words which include “DOMA”, “gay marriage”,
“Prop8”, etc. We track all matched Tweets gen-
erated from June 16th to June 29th, immedi-
ately prior and subsequent to the DOMA decision,
which results in more than 40 thousand Tweets per
day on average.
</bodyText>
<subsectionHeader confidence="0.999371">
3.2 Human Judgments
</subsectionHeader>
<bodyText confidence="0.999923375">
With more than 0.5 million potential DOMA rele-
vant Tweets collected, we randomly sampled 100
Tweets per day from June 16th to June 29th, and
1,400 Tweets were selected in total. Three re-
search assistants were trained and they showed
high agreement on assigning labels of relevance,
support, emotional intensity, and sentiment polar-
ity after training. Each Tweet in our samples was
</bodyText>
<footnote confidence="0.9999005">
1http://sentiwordnet.isti.cnr.it/
2http://wordnet.princeton.edu/
</footnote>
<bodyText confidence="0.997002444444445">
labeled by all three annotators. After the label-
ing, we first removed “irrelevant” Tweets (if the
Tweet was assigned “irrelevant” label by at least
one annotator), and then the tweets with no major
agreement among annotators on any of the senti-
ment dimensions were removed. As a result, 1,151
tweets with what we consider to be reliable labels
remained in our dataset (which we expect to share
with the research community).
</bodyText>
<subsectionHeader confidence="0.584272">
3.2.1 Annotator Agreement
</subsectionHeader>
<bodyText confidence="0.9825146">
The Fleiss’ Kappa agreement for each scale is re-
ported in Table 1 and shows that labelers have an
almost perfect agreement on relevance. Support,
emotional intensity, and sentiment polarity, show
either moderate or almost perfect agreement.
</bodyText>
<table confidence="0.9922238">
Measure Fleiss’ Kappa
Relevance 0.93
Support 0.84
Intensity 0.54
Polarity 0.49
</table>
<tableCaption confidence="0.99953">
Table 1: Agreement (Fleiss’ Kappa) of Human Labels.
</tableCaption>
<subsectionHeader confidence="0.996388">
3.3 Classification Performance Results
</subsectionHeader>
<bodyText confidence="0.999661722222222">
We reproduce the same feature types as previous
work and develop the political dictionary feature
for this particular task. We experimented with a
variety of automated classification algorithms, and
for this preliminary experiment report the perfor-
mance of Naive Bayes algorithm (simple, fast, and
shown to be surprisingly robust to classification
tasks with sparse and noisy training data). 10-fold
cross validation are performed to test the general-
izability of the classifiers. Table 2 reports the aver-
age precision, recall and accuracy for all measures.
Sarcasm is challenging to detect in part due to the
lack of positive instances. One goal in this study
is to build a model that captures trends among the
different classes. In Section 3.4, we will show that
the trends of different measures estimated by the
trained classifier align with the human annotated
ones over time.
</bodyText>
<sectionHeader confidence="0.831637" genericHeader="method">
3.4 Visualizing Sentiment Before and After
DOMA
</sectionHeader>
<bodyText confidence="0.998829833333333">
One natural application of the automated politi-
cal sentiment analysis proposed in this paper is
tracking public sentiment around landmark U.S.
Supreme Court decisions. To provide a more re-
liable estimate, we apply our trained classifier on
all relevant Tweets in our collection. More than
</bodyText>
<page confidence="0.996348">
90
</page>
<table confidence="0.999919555555555">
Value Prec. (%) Rec. (%) Accuracy(%)
Supportive (48%) 73 74
Neutral (45%) 76 67 68
Opposed (7%) 17 30
Intense (31%) 56 60 73
Dispassionate (69%) 81 79
Pleased (10%) 48 31
Neutral (79%) 84 78 69
Angry (11%) 24 45
</table>
<tableCaption confidence="0.999298">
Table 2: Performance of Classifiers on Each Class.
</tableCaption>
<bodyText confidence="0.7420218">
2.5 million Tweets are estimated in four proposed
measures. Figure 1 shows the distribution of on-
topic Tweet count over time. The Supreme Court
decision triggered a huge wave of Tweets, and the
volume went down quickly since then.
</bodyText>
<figure confidence="0.979117142857143">
400,000
300,000
200,000
100,000
0
16-Jun 19-Jun 22-Jun 25-Jun 28-Jun
Date
</figure>
<figureCaption confidence="0.999991">
Figure 1: Number of “Gay Marriage” Tweets Over Time.
</figureCaption>
<bodyText confidence="0.996131352941177">
Figures 2 and 3 visualize both the human la-
beled trends and the ones obtained by the classi-
fier for the classes “Supportive” and “Intense”. In
both figures, the peaks in the predicted labels gen-
erally align with the human-judged ones. We can
see the supportiveness and intensity are both rela-
tively high before the decision, and then they de-
cline gradually after the Supreme Court decision.
Figure 3 shows the volume of intensive Tweets
detected by our trained model has a burst on June
22rd, which is not captured by human labeled
data. To investigate this, we manually checked all
Tweets estimated as “intensive” on June 22rd. It
turns out most of the Tweets are indeed intensive.
The reason of the burst is that one Tweet was heav-
ily retweeted on that day. We do not disclose the
actual tweet due to its offensive content.
</bodyText>
<figureCaption confidence="0.999976666666667">
Figure 2: Percentage of “Supportive” Tweets Over Time.
Figure 4 plots the trends of “supportive” and
Figure 3: Percentage of “Intense” Tweets Over Time.
</figureCaption>
<figure confidence="0.997842181818182">
Supportive Opposed
0.016
0.014
0.012
0.01
0.008
0.006
0.004
0.002
0
16-Jun 19-Jun 22-Jun 25-Jun 28-Jun
</figure>
<figureCaption confidence="0.999284">
Figure 4: Comparison between “Supportive” and “Op-
posed” Trends.
</figureCaption>
<bodyText confidence="0.999828043478261">
“opposed” Tweets in different scales. According
to the Supreme Court decision, the “supportive”
group wins the debate. Interestingly, instead of
responding immediately, the “loser” group react
and start Tweeting 2 days after the decision. These
trends indicate that “winner” and “loser” in the de-
bate react differently in time and intensity dimen-
sions.
We believe that our estimates of sentiment can
be used in various ways by political scientists.
The “positivity bias” (Gibson and Caldeira, 2009)
model of Supreme Court opinion suggests that
the Court can move public opinion in the direc-
tion of its decisions. Our results possibly indicate
the opposite, the “polarizing” model suggested by
(Franklin and Kosaki, 1989) and (Johnson and
Martin, 1998), where more negative opinions are
observed after the decision (in Figure 4), at least
for a short period. By learning and visualize polit-
ical sentiments, we could crystalize the nature of
the decision that influences the degree to which the
Supreme Court can move opinion in the direction
of its decisions.
</bodyText>
<sectionHeader confidence="0.82527" genericHeader="method">
4 An Open Platform for Sharing and
</sectionHeader>
<subsectionHeader confidence="0.613475">
Analyzing Political Sentiments
</subsectionHeader>
<bodyText confidence="0.99963775">
Figure 5 shows a website3 that visualizes politi-
cal sentiments over time. The website shows sev-
eral popular U.S. Supreme Court cases, such as
“gay marriage”, “voting right act”, “tax cases”,
</bodyText>
<footnote confidence="0.868923">
3http://www.courtometer.com
</footnote>
<figure confidence="0.97054785">
16-Jun 19-Jun 22-Jun 25-Jun 28-Jun
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Human Labeled
Estimated
16-Jun 19-Jun 22-Jun 25-Jun 28-Jun
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Human Labeled
Estimated
Number of Tweets
% of Supportive Tweets
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
% of Opposed Tweets
91
Number of Tweets
Number of Tweets
</figure>
<bodyText confidence="0.999944368421052">
etc., and general topics, such as “Supreme Court”
and “Justices”. Each of the topics is represented
by a list of keywords developed by political sci-
ence experts. The keywords are also used to track
relevant Tweets through Twitter streaming API. To
let users go deeper in analyzing public opinions,
the website provides two types of real-time filter-
ing: keywords and location of Tweet authors. Af-
ter applying filters, a subset of matched Tweets are
generated as subtopics and their sentiments are vi-
sualized. The example filtering in Figure 5 shows
the process of creating subtopic “voting right act”
out of a general topic “Supreme Court” by using
keyword “VRA”. We can see that the volume of
negative Tweets of “voting right act” is higher than
the positive ones, compared to the overall senti-
ment of the general Supreme Court topic. Once an
interesting subtopic is found, users can download
the corresponding data and share with other users.
</bodyText>
<figure confidence="0.833627666666667">
Topic “Supreme Court”
Filtered by keyword: “VRA”
Subtopic “Voting Right Act”
</figure>
<figureCaption confidence="0.998721333333333">
Figure 5: We build a website that visualizes political sen-
timents over time and let users create “subtopics” by using
keyword and location filters.
</figureCaption>
<sectionHeader confidence="0.999105" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.998699037037037">
In this paper we considered the problem of polit-
ical sentiment analysis. We refined the notion of
sentiment, as applicable to the political domain,
and explored the features needed to perform auto-
mated classification to these dimensions, on a real
corpus of tweets about one U.S. Supreme Court
case. We showed that our existing classifier can
already be useful for exploratory political analy-
sis, by comparing the predicted sentiment trends to
those derived from manual human judgments, and
then applying the classifier on a large sample of
tweets – with the results providing additional ev-
idence for an important model of Supreme Court
opinion formation from political science.
This work provides an important step towards
robust sentiment analysis in the political domain,
and the data collected in our study is expected to
serve as a stepping stone for subsequent explo-
ration. In the future, we plan to refine and im-
prove the classification performance by exploring
additional features, in particular in the latent topic
space, and experimenting with other political sci-
ence topics.
ACKNOWLEDGMENTS The work of Yu Wang
and Eugene Agichtein was supported in part by
DARPA grants N11AP20012 and D11AP00269,
and by the Google Research Award.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997388">
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
Analysis of Twitter Data. In Proceedings of the
Workshop on Language in Social Media (LSM).
Paula Carvalho, Lu´ıs Sarmento, M´ario J. Silva, and
Eug´enio de Oliveira. 2009. Clues for detecting
irony in user-generated contents: oh...!! it’s ”so
easy” ;-). In Proceedings of the 1st international
CIKM workshop on Topic-sentiment analysis for
mass opinion.
M.D. Conover, B. Goncalves, J. Ratkiewicz, A. Flam-
mini, and F. Menczer. 2011. Predicting the Political
Alignment of Twitter Users In Proceedings of IEEE
third international conference on social computing
Robert Dahl. 1957. Decision-Making in a Democracy:
The Supreme Court as National Policy-Maker. Jour-
nal of Public Law.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised Recognition of Sarcastic Sentences
in Twitter and Amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL).
Charles H. Franklin, and Liane C. Kosaki. 1989. Re-
publican Schoolmaster: The U.S. Supreme Court,
Public Opinion, and Abortion. The American Po-
litical Science Review.
James L Gibson, and Gregory A Caldeira. 2009. Cit-
izens, courts, and confirmations: Positivity theory
and the judgments of the American people. Prince-
ton University Press.
James L Gibson, Gregory A Caldeira, and Lester Keny-
atta Spence. 2003. Measuring Attitudes toward the
</reference>
<page confidence="0.933063">
92
</page>
<reference confidence="0.999845676470588">
United States Supreme Court. American Journal of
Political Science.
Valerie Hoekstra. 2003. Public Reaction to Supreme
Court Decisions. Cambridge University Press.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Micro-blogging As Online Word
of Mouth Branding. in CHI ’09 Extended Abstracts
on Human Factors in Computing Systems.
Timothy R. Johnson, and Andrew D. Martin. 1998.
The Public’s Conditional Response to Supreme
Court Decisions. American Political Science Re-
view 92(2):299-309.
Haewoon Kwak, Changhyun Lee, Hosung Park, and
Sue Moon. 2010. What is Twitter, a Social Network
or a News Media?. in Proceedings of the 19th Inter-
national Conference on World Wide Web (WWW).
Yu-Ru Lin, Drew Margolin, Brian Keegan, and David
Lazer. 2013. Voices of Victory: A Computational
Focus Group Framework for Tracking Opinion Shift
in Real Time. In Proceedings of International World
Wide Web Conference (WWW).
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies.
Yelena Mejova, Padmini Srinivasan, and Bob Boynton.
2013. GOP Primary Season on Twitter: ”Popular”
Political Sentiment in Social Media. In Proceedings
of the Sixth ACM International Conference on Web
Search and Data Mining (WSDM).
B. O’Connor, R. Balasubramanyan, B. R. Routledge,
and N. A. Smith. 2010. From tweets to polls: Link-
ing text sentiment to public opinion time series. In
Proceedings of International AAAI Conference on
Weblogs and Social Media (ICWSM).
Bo Pang, and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Antonio Reyes, Paolo Rosso, and Tony Veale. 2012.
A multidimensional approach for detecting irony in
Twitter. Language Resources and Evaluation.
Swapna Somasundaran, Galileo Namata, Lise Getoor,
and Janyce Wiebe. 2009. Opinion Graphs for Po-
larity and Discourse Classification. TextGraphs-
4: Graph-based Methods for Natural Language Pro-
cessing.
Aline A. Vanin, Larissa A. Freitas, Re-nata Vieira, and
Marco Bochernitsan. 2013. Some clues on irony
detection in tweets. In Proceedings of International
World Wide Web Conference (WWW).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing Contextual Polarity: an explo-
ration offeatures for phrase-level sentiment analy-
sis. Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa.
2004. Just how mad are you? Finding strong and
weak opinion clauses. In Proceedings of Conference
on Artificial Intelligence (AAAI).
Andranik Tumasjan, Timm O. Sprenger, Philipp G.
Sandner, and Isabell M. Welpe. 2010. Predicting
Elections with Twitter: What 140 Characters Re-
veal about Political Sentiment. In Proceedings of
the Fourth International AAAI Conference on We-
blogs and Social Media (ICWSM).
</reference>
<page confidence="0.999168">
93
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.7855515">Towards Tracking Political Sentiment through Microblog Data Yu</title>
<author confidence="0.909284">Emory</author>
<email confidence="0.999278">yu.wang@emory.edu</email>
<author confidence="0.8645495">Tom Emory</author>
<email confidence="0.996925">tclark7@emory.edu</email>
<author confidence="0.919487">Jeffrey Emory</author>
<email confidence="0.999303">jkstato@emory.edu</email>
<author confidence="0.541859">Eugene</author>
<affiliation confidence="0.997549">Emory University</affiliation>
<email confidence="0.999441">eugene@mathcs.emory.edu</email>
<abstract confidence="0.98400847601476">People express and amplify political opinions in Microblogs such as Twitter, especially when major political decisions are made. Twitter provides a useful vehicle for capturing and tracking popular opinion on burning issues of the day. In this paper, we focus on tracking the changes in political sentiment related to the U.S. Supreme Court (SCOTUS) and its decisions, focusing on the key dimensions on support, emotional intensity, and polarity. Measuring changes in these sentiment dimensions could be useful for social and political scientists, policy makers, and the public. This preliminary work adapts existing sentiment analysis techniques to these new dimensions and the specifics of the corpus (Twitter). We illustrate the promise of our work with an important case study of tracking sentiment change building up to, and immediately following one recent landmark Supreme Court decision. This example illustrates how our work could help answer fundamental research questions in political science about the nature of Supreme Court power and its capacity to influence public discourse. 1 Background and Motivation Political opinions are a popular topic in Microblogs. On June 26th, 2013, when the U.S. Supreme Court announced the decision on the unconstitutionality of the ”Defense of Marriage Act” (DOMA), there were millions of Tweets about the users’ opinions of the decision. In their Tweets, people not only voice their opinions about the issues at stake, expressing different dimensions of sentiment, such as support or opposition to the decision, or anger or happiness. Thus, simply applying traditional sentiment analysis scales such as ”positive” vs. ”negative” classification would not be sufficient to understand the public reaction to political decisions. Research on mass opinion and the Supreme Court is valuable as it could shed light on the fundamental and related normative concerns about the role of constitutional review in American governance, which emerge in a political system possessing democratic institutions at cross-purposes. One line of thought, beginning with Dahl (Dahl, 1957), suggests that the Supreme Court of the United States has a unique capacity among major institutions of American government to leverage its legitimacy in order to change mass opinion regarding salient policies. If the Dahl’s hypothesis is correct, then the Supreme Court’s same-sex marriage decisions should have resulted in a measurable change in opinion. A primary finding about implication of Dahl’s hypothesis is that the Court is polarizing, creating more supportive opinions of the policies it reviews among those who supported the policy before the decision and more negative opinions among those who opposed the policy prior to the decision (Franklin and Kosaki, 1989) (Johnson and Martin, 1998). We consider Twitter as important example of social expression of opinion. Recent studies of content on Twitter have revealed that 85% of Twitter content is related to spreading and commenting on headline news (Kwak et al., 2010); when users talk about commercial brands in their Tweets, about 20% of them have personal sentiment involved (Jansen et al., 2009). These statistical evidences imply that Twitter has became a portal for public to express opinions. In the context of politics, Twitter content, together with Twitter users’ 88 of the Joint Workshop on Social Dynamics and Personal Attributes in Social pages Maryland USA, 27 June 2014. Association for Computational Linguistics information, such as user’s profile and social network, have shown reasonable power of detecting user’s political leaning (Conover et al., 2011) and predicting elections (Tumasjan et al., 2010). Although promising, the effectiveness of using Twitter content to measure public political opinions remains unclear. Several studies show limited correlation between sentiment on Twitter and political polls in elections (Mejova et al., 2013) (O’Connor et al., 2010). Our study mainly focuses on investigating sentiment on Twitter about U.S. Supreme Court decisions. We propose more fine-grained dimensions for political sentiment analysis, such as supportiveness, emotional intensity and polarity, allowing political science researchers, policy makers, and the public to better comprehend the public reaction to major political issues of the day. As we describe below, these different dimensions of discourse on Twitter allows examination of the multiple ways in which discourse changes when the Supreme Court makes a decision on a given issue of public policy. Our dimensions also open the door to new avenues of theorizing about the nature of public discourse on policy debates. Although general sentiment analysis has made significant advances over the last decade (Pang et al., 2002) (Pang and Lee, 2008) (Liu, 2012) (Wilson et al., 2009), and with the focus on certain aspects, such as intensity (Wilson et al., 2004), irony detection (Carvalho et al., 2009) and sarcasm detection (Davidov et al., 2010), analyzing Microblog content such as Twitter remains a challenging research topic (Reyes et al., 2012) (Vanin et al., 2013) (Agarwal et al., 2011). Unlike previous work, we introduce and focus on sentiment dimensions particularly important for political analysis of Microblog text, and extend and adapt classification techniques accordingly. To make the data and sentiment analysis results accessible for researchers in other domain, we build a website to visualize the sentiment dynamics over time and let users download the data. Users could also define their own topics of interest and perform deeper analysis with keyword filtering and geolocation filtering. We present a case study in which our results might be used to answer core questions in political science about the nature of Supreme Court influence on public opinion. Political scientists have long been concerned with whether and how Supreme Court decisions affect public opinion and discourse about political topics (Hoekstra, 2003) (Johnson and Martin, 1998) (Gibson et al., 2003). Survey research on the subject has been limited in two ways. Survey analysis, including panel designs, rely on estimates near but never on the date of particular decisions. In addition, all surveybased research relies on estimates derived from an instrument designed to elicit sentiment – survey responses, useful as they are, do not reflect well how public opinion is naturally expressed. Our analysis allows for the examination of public opinion as it is naturally expressed and in a way that is precisely connected to the timing of decisions. Next, we state the problem more formally, and outline our approach and implementation. 2 Problem Statement and Approach 2.1 Political Sentiment Classification We propose three refinements to sentiment analysis to quantify political opinions. Specifically, we pose the following dimensions as particularly important for politics: Support: Whether a Tweet is Neuor the topic. • Emotional Intensity: Whether a Tweet is • Sentiment Polarity: Whether a Tweet’s tone or 2.2 Approach In this work, each of the proposed measures is treated as a supervised classification problem. We use multi-class classification algorithms to model Support and Sentiment Polarity, and binary classification for Emotional Intensity and Sarcasm. Section 3.2 describes the labels used to train the supervised classification models. Notice some classes are more interesting than the others. For example, the trends or ratio of opposed vs. supportive Microblogs are more informative than the factual ones. Particularly, we pay more attention to the of and 2.3 Classifier Feature Groups To classify the Microblog message into the classes of interest, we develop 6 groups of features: Number of times the message has been 89 posted or favored by users. As for a Tweet, this feature means number of Retweets and favorites. and of Unigram, bigram, and trigram of the message text. The maximum, minimum, average and sum of sentiment score of terms and each Part-of-Speech tags in the message text. Counter factuality and temporal compression dic- This feature counts the number of times such words appear in the message text. Number of times a politicalrelated word appears in the message text. We compute sentiment scores based on Sentia sentiment dictionary constructed on Political dictionary is built upon political-related words in WordNet. As in this paper, we construct a political dictionary with 56 words and phrases, such as “liberal”, “conservative”, and “freedom” etc. 3 Case Study: DOMA Our goal is to build and test classifiers that can distinguish political content between classes of interest. Particularly, we focus on classifying Tweets related to one of the most popular political topics, “Defence of Marriage Act” or DOMA, as the target. The techniques can be easily generalized to other political issues in Twitter. 3.1 Dataset In order to obtain relevant Tweets, we use Twitter streaming API to track representative keywords which include “DOMA”, “gay marriage”, “Prop8”, etc. We track all matched Tweets generated from June 16th to June 29th, immediately prior and subsequent to the DOMA decision, which results in more than 40 thousand Tweets per day on average. 3.2 Human Judgments With more than 0.5 million potential DOMA relevant Tweets collected, we randomly sampled 100 Tweets per day from June 16th to June 29th, and 1,400 Tweets were selected in total. Three research assistants were trained and they showed high agreement on assigning labels of relevance, support, emotional intensity, and sentiment polarity after training. Each Tweet in our samples was labeled by all three annotators. After the labeling, we first removed “irrelevant” Tweets (if the Tweet was assigned “irrelevant” label by at least one annotator), and then the tweets with no major agreement among annotators on any of the sentiment dimensions were removed. As a result, 1,151 tweets with what we consider to be reliable labels remained in our dataset (which we expect to share with the research community). 3.2.1 Annotator Agreement The Fleiss’ Kappa agreement for each scale is reported in Table 1 and shows that labelers have an almost perfect agreement on relevance. Support, emotional intensity, and sentiment polarity, show either moderate or almost perfect agreement. Measure Fleiss’ Kappa Relevance 0.93 Support 0.84 Intensity 0.54 Polarity 0.49 1: (Fleiss’ Kappa) of Human Labels. 3.3 Classification Performance Results We reproduce the same feature types as previous work and develop the political dictionary feature for this particular task. We experimented with a variety of automated classification algorithms, and for this preliminary experiment report the performance of Naive Bayes algorithm (simple, fast, and shown to be surprisingly robust to classification tasks with sparse and noisy training data). 10-fold cross validation are performed to test the generalizability of the classifiers. Table 2 reports the average precision, recall and accuracy for all measures. Sarcasm is challenging to detect in part due to the lack of positive instances. One goal in this study is to build a model that captures trends among the different classes. In Section 3.4, we will show that the trends of different measures estimated by the trained classifier align with the human annotated ones over time. 3.4 Visualizing Sentiment Before and After DOMA One natural application of the automated political sentiment analysis proposed in this paper is tracking public sentiment around landmark U.S. Supreme Court decisions. To provide a more reliable estimate, we apply our trained classifier on all relevant Tweets in our collection. More than 90 Value Prec. (%) Rec. (%) Accuracy(%)</abstract>
<phone confidence="0.6155975">Supportive (48%) 73 74 Neutral (45%) 76 67 68 Opposed (7%) 17 30 Intense (31%) Dispassionate (69%) 56 60 73 81 79 Pleased (10%) 48 31 Neutral (79%) 84 78 69 Angry (11%) 24 45</phone>
<abstract confidence="0.995310166666667">2: of Classifiers on Each Class. 2.5 million Tweets are estimated in four proposed measures. Figure 1 shows the distribution of ontopic Tweet count over time. The Supreme Court decision triggered a huge wave of Tweets, and the volume went down quickly since then.</abstract>
<address confidence="0.93439675">400,000 300,000 200,000 100,000</address>
<abstract confidence="0.82159192948718">0 16-Jun 19-Jun 22-Jun 25-Jun 28-Jun Date 1: of “Gay Marriage” Tweets Over Time. Figures 2 and 3 visualize both the human labeled trends and the ones obtained by the classifier for the classes “Supportive” and “Intense”. In both figures, the peaks in the predicted labels generally align with the human-judged ones. We can see the supportiveness and intensity are both relatively high before the decision, and then they decline gradually after the Supreme Court decision. Figure 3 shows the volume of intensive Tweets detected by our trained model has a burst on June 22rd, which is not captured by human labeled data. To investigate this, we manually checked all Tweets estimated as “intensive” on June 22rd. It turns out most of the Tweets are indeed intensive. The reason of the burst is that one Tweet was heavily retweeted on that day. We do not disclose the actual tweet due to its offensive content. 2: of “Supportive” Tweets Over Time. Figure 4 plots the trends of “supportive” and 3: of “Intense” Tweets Over Time. Supportive Opposed 0.016 0.014 0.012 0.01 0.008 0.006 0.004 0.002 0 16-Jun 19-Jun 22-Jun 25-Jun 28-Jun 4: between “Supportive” and “Opposed” Trends. “opposed” Tweets in different scales. According to the Supreme Court decision, the “supportive” group wins the debate. Interestingly, instead of responding immediately, the “loser” group react and start Tweeting 2 days after the decision. These trends indicate that “winner” and “loser” in the debate react differently in time and intensity dimensions. We believe that our estimates of sentiment can be used in various ways by political scientists. The “positivity bias” (Gibson and Caldeira, 2009) model of Supreme Court opinion suggests that the Court can move public opinion in the direction of its decisions. Our results possibly indicate the opposite, the “polarizing” model suggested by (Franklin and Kosaki, 1989) and (Johnson and Martin, 1998), where more negative opinions are observed after the decision (in Figure 4), at least for a short period. By learning and visualize political sentiments, we could crystalize the nature of the decision that influences the degree to which the Supreme Court can move opinion in the direction of its decisions. 4 An Open Platform for Sharing and Analyzing Political Sentiments 5 shows a that visualizes political sentiments over time. The website shows several popular U.S. Supreme Court cases, such as “gay marriage”, “voting right act”, “tax cases”, 16-Jun 19-Jun 22-Jun 25-Jun 28-Jun 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Human Labeled Estimated 16-Jun 19-Jun 22-Jun 25-Jun 28-Jun 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Human Labeled Estimated Number of Tweets % of Supportive Tweets 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 % of Opposed Tweets 91 Number of Tweets Number of Tweets etc., and general topics, such as “Supreme Court” and “Justices”. Each of the topics is represented by a list of keywords developed by political science experts. The keywords are also used to track relevant Tweets through Twitter streaming API. To let users go deeper in analyzing public opinions, the website provides two types of real-time filtering: keywords and location of Tweet authors. After applying filters, a subset of matched Tweets are generated as subtopics and their sentiments are visualized. The example filtering in Figure 5 shows the process of creating subtopic “voting right act” out of a general topic “Supreme Court” by using keyword “VRA”. We can see that the volume of negative Tweets of “voting right act” is higher than the positive ones, compared to the overall sentiment of the general Supreme Court topic. Once an interesting subtopic is found, users can download the corresponding data and share with other users. Topic “Supreme Court” Filtered by keyword: “VRA” Subtopic “Voting Right Act” 5: build a website that visualizes political sentiments over time and let users create “subtopics” by using keyword and location filters. 5 Conclusions In this paper we considered the problem of political sentiment analysis. We refined the notion of sentiment, as applicable to the political domain, and explored the features needed to perform automated classification to these dimensions, on a real corpus of tweets about one U.S. Supreme Court case. We showed that our existing classifier can already be useful for exploratory political analysis, by comparing the predicted sentiment trends to those derived from manual human judgments, and then applying the classifier on a large sample of tweets – with the results providing additional evidence for an important model of Supreme Court opinion formation from political science. This work provides an important step towards robust sentiment analysis in the political domain, and the data collected in our study is expected to serve as a stepping stone for subsequent exploration. In the future, we plan to refine and improve the classification performance by exploring additional features, in particular in the latent topic space, and experimenting with other political science topics. work of Yu Wang</abstract>
<note confidence="0.780285888888889">and Eugene Agichtein was supported in part by DARPA grants N11AP20012 and D11AP00269, and by the Google Research Award. References Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ramand Rebecca Passonneau. 2011. of Twitter In Proceedings of the Workshop on Language in Social Media (LSM). Paula Carvalho, Lu´ıs Sarmento, M´ario J. Silva, and</note>
<abstract confidence="0.937975555555556">de Oliveira. 2009. for detecting irony in user-generated contents: oh...!! it’s ”so In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion. M.D. Conover, B. Goncalves, J. Ratkiewicz, A. Flamand F. Menczer. 2011. the Political of Twitter Users Proceedings of IEEE third international conference on social computing</abstract>
<note confidence="0.61743575">Dahl. 1957. in a Democracy: Supreme Court as National Journal of Public Law. Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.</note>
<title confidence="0.93801375">Semi-supervised Recognition of Sarcastic Sentences Twitter and In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL).</title>
<author confidence="0.882904">Re-</author>
<affiliation confidence="0.789812666666667">publican Schoolmaster: The U.S. Supreme Court, Opinion, and The American Political Science Review.</affiliation>
<note confidence="0.689997289473684">L Gibson, and Gregory A Caldeira. 2009. Citizens, courts, and confirmations: Positivity theory the judgments of the American Princeton University Press. James L Gibson, Gregory A Caldeira, and Lester Keny- Spence. 2003. Attitudes toward the 92 States Supreme American Journal of Political Science. Hoekstra. 2003. Reaction to Supreme Cambridge University Press. Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur 2009. As Online Word Mouth in CHI ’09 Extended Abstracts on Human Factors in Computing Systems. Timothy R. Johnson, and Andrew D. Martin. 1998. The Public’s Conditional Response to Supreme American Political Science Review 92(2):299-309. Haewoon Kwak, Changhyun Lee, Hosung Park, and Moon. 2010. is Twitter, a Social Network a News in Proceedings of the 19th International Conference on World Wide Web (WWW). Yu-Ru Lin, Drew Margolin, Brian Keegan, and David 2013. of Victory: A Computational Focus Group Framework for Tracking Opinion Shift Real In Proceedings of International World Wide Web Conference (WWW). Liu. 2012. analysis and opinion min- Synthesis Lectures on Human Language Technologies. Yelena Mejova, Padmini Srinivasan, and Bob Boynton. Primary Season on Twitter: ”Popular” Sentiment in Social In Proceedings of the Sixth ACM International Conference on Web Search and Data Mining (WSDM). B. O’Connor, R. Balasubramanyan, B. R. Routledge, N. A. Smith. 2010. tweets to polls: Linktext sentiment to public opinion time In Proceedings of International AAAI Conference on Weblogs and Social Media (ICWSM). Pang, and Lillian Lee. 2008. mining and Foundations and Trends in Information Retrieval. Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. up? sentiment classification using learning In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). Antonio Reyes, Paolo Rosso, and Tony Veale. 2012. A multidimensional approach for detecting irony in Language Resources and Evaluation. Swapna Somasundaran, Galileo Namata, Lise Getoor, Janyce Wiebe. 2009. Graphs for Poand Discourse TextGraphs- 4: Graph-based Methods for Natural Language Processing. Aline A. Vanin, Larissa A. Freitas, Re-nata Vieira, and Bochernitsan. 2013. clues on irony in In Proceedings of International World Wide Web Conference (WWW). Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. Contextual Polarity: an exploration offeatures for phrase-level sentiment analy- Computational Linguistics. Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. how mad are you? Finding strong and opinion In Proceedings of Conference on Artificial Intelligence (AAAI). Andranik Tumasjan, Timm O. Sprenger, Philipp G. and Isabell M. Welpe. 2010. Elections with Twitter: What 140 Characters Reabout Political In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media (ICWSM). 93</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Boyi Xie</author>
<author>Ilia Vovsha</author>
<author>Owen Rambow</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Sentiment Analysis of Twitter Data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Language in Social Media (LSM).</booktitle>
<contexts>
<context position="5465" citStr="Agarwal et al., 2011" startWordPosition="845" endWordPosition="848">ven issue of public policy. Our dimensions also open the door to new avenues of theorizing about the nature of public discourse on policy debates. Although general sentiment analysis has made significant advances over the last decade (Pang et al., 2002) (Pang and Lee, 2008) (Liu, 2012) (Wilson et al., 2009), and with the focus on certain aspects, such as intensity (Wilson et al., 2004), irony detection (Carvalho et al., 2009) and sarcasm detection (Davidov et al., 2010), analyzing Microblog content such as Twitter remains a challenging research topic (Reyes et al., 2012) (Vanin et al., 2013) (Agarwal et al., 2011). Unlike previous work, we introduce and focus on sentiment dimensions particularly important for political analysis of Microblog text, and extend and adapt classification techniques accordingly. To make the data and sentiment analysis results accessible for researchers in other domain, we build a website to visualize the sentiment dynamics over time and let users download the data. Users could also define their own topics of interest and perform deeper analysis with keyword filtering and geolocation filtering. We present a case study in which our results might be used to answer core questions</context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment Analysis of Twitter Data. In Proceedings of the Workshop on Language in Social Media (LSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paula Carvalho</author>
<author>Lu´ıs Sarmento</author>
<author>M´ario J Silva</author>
<author>Eug´enio de Oliveira</author>
</authors>
<title>Clues for detecting irony in user-generated contents: oh...!! it’s ”so easy” ;-).</title>
<date>2009</date>
<booktitle>In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion.</booktitle>
<marker>Carvalho, Sarmento, Silva, de Oliveira, 2009</marker>
<rawString>Paula Carvalho, Lu´ıs Sarmento, M´ario J. Silva, and Eug´enio de Oliveira. 2009. Clues for detecting irony in user-generated contents: oh...!! it’s ”so easy” ;-). In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Conover</author>
<author>B Goncalves</author>
<author>J Ratkiewicz</author>
<author>A Flammini</author>
<author>F Menczer</author>
</authors>
<title>Predicting the Political Alignment of Twitter Users In</title>
<date>2011</date>
<booktitle>Proceedings of IEEE third international conference on social computing</booktitle>
<contexts>
<context position="3959" citStr="Conover et al., 2011" startWordPosition="609" endWordPosition="612">mercial brands in their Tweets, about 20% of them have personal sentiment involved (Jansen et al., 2009). These statistical evidences imply that Twitter has became a portal for public to express opinions. In the context of politics, Twitter content, together with Twitter users’ 88 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 88–93, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics information, such as user’s profile and social network, have shown reasonable power of detecting user’s political leaning (Conover et al., 2011) and predicting elections (Tumasjan et al., 2010). Although promising, the effectiveness of using Twitter content to measure public political opinions remains unclear. Several studies show limited correlation between sentiment on Twitter and political polls in elections (Mejova et al., 2013) (O’Connor et al., 2010). Our study mainly focuses on investigating sentiment on Twitter about U.S. Supreme Court decisions. We propose more fine-grained dimensions for political sentiment analysis, such as supportiveness, emotional intensity and polarity, allowing political science researchers, policy make</context>
</contexts>
<marker>Conover, Goncalves, Ratkiewicz, Flammini, Menczer, 2011</marker>
<rawString>M.D. Conover, B. Goncalves, J. Ratkiewicz, A. Flammini, and F. Menczer. 2011. Predicting the Political Alignment of Twitter Users In Proceedings of IEEE third international conference on social computing</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dahl</author>
</authors>
<title>Decision-Making in a Democracy: The Supreme Court as National Policy-Maker.</title>
<date>1957</date>
<journal>Journal of Public Law.</journal>
<contexts>
<context position="2377" citStr="Dahl, 1957" startWordPosition="360" endWordPosition="361"> sentiment, such as support or opposition to the decision, or anger or happiness. Thus, simply applying traditional sentiment analysis scales such as ”positive” vs. ”negative” classification would not be sufficient to understand the public reaction to political decisions. Research on mass opinion and the Supreme Court is valuable as it could shed light on the fundamental and related normative concerns about the role of constitutional review in American governance, which emerge in a political system possessing democratic institutions at cross-purposes. One line of thought, beginning with Dahl (Dahl, 1957), suggests that the Supreme Court of the United States has a unique capacity among major institutions of American government to leverage its legitimacy in order to change mass opinion regarding salient policies. If the Dahl’s hypothesis is correct, then the Supreme Court’s same-sex marriage decisions should have resulted in a measurable change in opinion. A primary finding about implication of Dahl’s hypothesis is that the Court is polarizing, creating more supportive opinions of the policies it reviews among those who supported the policy before the decision and more negative opinions among t</context>
</contexts>
<marker>Dahl, 1957</marker>
<rawString>Robert Dahl. 1957. Decision-Making in a Democracy: The Supreme Court as National Policy-Maker. Journal of Public Law.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Semi-supervised Recognition of Sarcastic Sentences in Twitter and Amazon.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="5318" citStr="Davidov et al., 2010" startWordPosition="821" endWordPosition="824">mensions of discourse on Twitter allows examination of the multiple ways in which discourse changes when the Supreme Court makes a decision on a given issue of public policy. Our dimensions also open the door to new avenues of theorizing about the nature of public discourse on policy debates. Although general sentiment analysis has made significant advances over the last decade (Pang et al., 2002) (Pang and Lee, 2008) (Liu, 2012) (Wilson et al., 2009), and with the focus on certain aspects, such as intensity (Wilson et al., 2004), irony detection (Carvalho et al., 2009) and sarcasm detection (Davidov et al., 2010), analyzing Microblog content such as Twitter remains a challenging research topic (Reyes et al., 2012) (Vanin et al., 2013) (Agarwal et al., 2011). Unlike previous work, we introduce and focus on sentiment dimensions particularly important for political analysis of Microblog text, and extend and adapt classification techniques accordingly. To make the data and sentiment analysis results accessible for researchers in other domain, we build a website to visualize the sentiment dynamics over time and let users download the data. Users could also define their own topics of interest and perform de</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Semi-supervised Recognition of Sarcastic Sentences in Twitter and Amazon. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles H Franklin</author>
<author>Liane C Kosaki</author>
</authors>
<title>Republican Schoolmaster: The U.S. Supreme Court, Public Opinion, and Abortion. The American Political Science Review.</title>
<date>1989</date>
<contexts>
<context position="3054" citStr="Franklin and Kosaki, 1989" startWordPosition="467" endWordPosition="470">tes has a unique capacity among major institutions of American government to leverage its legitimacy in order to change mass opinion regarding salient policies. If the Dahl’s hypothesis is correct, then the Supreme Court’s same-sex marriage decisions should have resulted in a measurable change in opinion. A primary finding about implication of Dahl’s hypothesis is that the Court is polarizing, creating more supportive opinions of the policies it reviews among those who supported the policy before the decision and more negative opinions among those who opposed the policy prior to the decision (Franklin and Kosaki, 1989) (Johnson and Martin, 1998). We consider Twitter as important example of social expression of opinion. Recent studies of content on Twitter have revealed that 85% of Twitter content is related to spreading and commenting on headline news (Kwak et al., 2010); when users talk about commercial brands in their Tweets, about 20% of them have personal sentiment involved (Jansen et al., 2009). These statistical evidences imply that Twitter has became a portal for public to express opinions. In the context of politics, Twitter content, together with Twitter users’ 88 Proceedings of the Joint Workshop </context>
<context position="14878" citStr="Franklin and Kosaki, 1989" startWordPosition="2344" endWordPosition="2347">” group wins the debate. Interestingly, instead of responding immediately, the “loser” group react and start Tweeting 2 days after the decision. These trends indicate that “winner” and “loser” in the debate react differently in time and intensity dimensions. We believe that our estimates of sentiment can be used in various ways by political scientists. The “positivity bias” (Gibson and Caldeira, 2009) model of Supreme Court opinion suggests that the Court can move public opinion in the direction of its decisions. Our results possibly indicate the opposite, the “polarizing” model suggested by (Franklin and Kosaki, 1989) and (Johnson and Martin, 1998), where more negative opinions are observed after the decision (in Figure 4), at least for a short period. By learning and visualize political sentiments, we could crystalize the nature of the decision that influences the degree to which the Supreme Court can move opinion in the direction of its decisions. 4 An Open Platform for Sharing and Analyzing Political Sentiments Figure 5 shows a website3 that visualizes political sentiments over time. The website shows several popular U.S. Supreme Court cases, such as “gay marriage”, “voting right act”, “tax cases”, 3htt</context>
</contexts>
<marker>Franklin, Kosaki, 1989</marker>
<rawString>Charles H. Franklin, and Liane C. Kosaki. 1989. Republican Schoolmaster: The U.S. Supreme Court, Public Opinion, and Abortion. The American Political Science Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James L Gibson</author>
<author>Gregory A Caldeira</author>
</authors>
<title>Citizens, courts, and confirmations: Positivity theory and the judgments of the American people.</title>
<date>2009</date>
<publisher>Princeton University Press.</publisher>
<contexts>
<context position="14656" citStr="Gibson and Caldeira, 2009" startWordPosition="2309" endWordPosition="2312">01 0.008 0.006 0.004 0.002 0 16-Jun 19-Jun 22-Jun 25-Jun 28-Jun Figure 4: Comparison between “Supportive” and “Opposed” Trends. “opposed” Tweets in different scales. According to the Supreme Court decision, the “supportive” group wins the debate. Interestingly, instead of responding immediately, the “loser” group react and start Tweeting 2 days after the decision. These trends indicate that “winner” and “loser” in the debate react differently in time and intensity dimensions. We believe that our estimates of sentiment can be used in various ways by political scientists. The “positivity bias” (Gibson and Caldeira, 2009) model of Supreme Court opinion suggests that the Court can move public opinion in the direction of its decisions. Our results possibly indicate the opposite, the “polarizing” model suggested by (Franklin and Kosaki, 1989) and (Johnson and Martin, 1998), where more negative opinions are observed after the decision (in Figure 4), at least for a short period. By learning and visualize political sentiments, we could crystalize the nature of the decision that influences the degree to which the Supreme Court can move opinion in the direction of its decisions. 4 An Open Platform for Sharing and Anal</context>
</contexts>
<marker>Gibson, Caldeira, 2009</marker>
<rawString>James L Gibson, and Gregory A Caldeira. 2009. Citizens, courts, and confirmations: Positivity theory and the judgments of the American people. Princeton University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>James L Gibson</author>
<author>Gregory A Caldeira</author>
<author>Lester Keny-</author>
</authors>
<marker>Gibson, Caldeira, Keny-, </marker>
<rawString>James L Gibson, Gregory A Caldeira, and Lester Keny-</rawString>
</citation>
<citation valid="true">
<authors>
<author>atta Spence</author>
</authors>
<title>Measuring Attitudes toward the United States Supreme Court.</title>
<date>2003</date>
<journal>American Journal of Political Science.</journal>
<marker>Spence, 2003</marker>
<rawString>atta Spence. 2003. Measuring Attitudes toward the United States Supreme Court. American Journal of Political Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valerie Hoekstra</author>
</authors>
<title>Public Reaction to Supreme Court Decisions.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6316" citStr="Hoekstra, 2003" startWordPosition="979" endWordPosition="980">results accessible for researchers in other domain, we build a website to visualize the sentiment dynamics over time and let users download the data. Users could also define their own topics of interest and perform deeper analysis with keyword filtering and geolocation filtering. We present a case study in which our results might be used to answer core questions in political science about the nature of Supreme Court influence on public opinion. Political scientists have long been concerned with whether and how Supreme Court decisions affect public opinion and discourse about political topics (Hoekstra, 2003) (Johnson and Martin, 1998) (Gibson et al., 2003). Survey research on the subject has been limited in two ways. Survey analysis, including panel designs, rely on estimates near but never on the date of particular decisions. In addition, all surveybased research relies on estimates derived from an instrument designed to elicit sentiment – survey responses, useful as they are, do not reflect well how public opinion is naturally expressed. Our analysis allows for the examination of public opinion as it is naturally expressed and in a way that is precisely connected to the timing of decisions. Nex</context>
</contexts>
<marker>Hoekstra, 2003</marker>
<rawString>Valerie Hoekstra. 2003. Public Reaction to Supreme Court Decisions. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard J Jansen</author>
<author>Mimi Zhang</author>
<author>Kate Sobel</author>
<author>Abdur Chowdury</author>
</authors>
<date>2009</date>
<booktitle>Micro-blogging As Online Word of Mouth Branding. in CHI ’09 Extended Abstracts on Human Factors in Computing Systems.</booktitle>
<contexts>
<context position="3442" citStr="Jansen et al., 2009" startWordPosition="531" endWordPosition="534">g, creating more supportive opinions of the policies it reviews among those who supported the policy before the decision and more negative opinions among those who opposed the policy prior to the decision (Franklin and Kosaki, 1989) (Johnson and Martin, 1998). We consider Twitter as important example of social expression of opinion. Recent studies of content on Twitter have revealed that 85% of Twitter content is related to spreading and commenting on headline news (Kwak et al., 2010); when users talk about commercial brands in their Tweets, about 20% of them have personal sentiment involved (Jansen et al., 2009). These statistical evidences imply that Twitter has became a portal for public to express opinions. In the context of politics, Twitter content, together with Twitter users’ 88 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 88–93, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics information, such as user’s profile and social network, have shown reasonable power of detecting user’s political leaning (Conover et al., 2011) and predicting elections (Tumasjan et al., 2010). Although promising, the effectiv</context>
</contexts>
<marker>Jansen, Zhang, Sobel, Chowdury, 2009</marker>
<rawString>Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur Chowdury. 2009. Micro-blogging As Online Word of Mouth Branding. in CHI ’09 Extended Abstracts on Human Factors in Computing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy R Johnson</author>
<author>Andrew D Martin</author>
</authors>
<title>The Public’s Conditional Response to Supreme Court Decisions.</title>
<date>1998</date>
<journal>American Political Science Review</journal>
<pages>92--2</pages>
<contexts>
<context position="3081" citStr="Johnson and Martin, 1998" startWordPosition="471" endWordPosition="474">ong major institutions of American government to leverage its legitimacy in order to change mass opinion regarding salient policies. If the Dahl’s hypothesis is correct, then the Supreme Court’s same-sex marriage decisions should have resulted in a measurable change in opinion. A primary finding about implication of Dahl’s hypothesis is that the Court is polarizing, creating more supportive opinions of the policies it reviews among those who supported the policy before the decision and more negative opinions among those who opposed the policy prior to the decision (Franklin and Kosaki, 1989) (Johnson and Martin, 1998). We consider Twitter as important example of social expression of opinion. Recent studies of content on Twitter have revealed that 85% of Twitter content is related to spreading and commenting on headline news (Kwak et al., 2010); when users talk about commercial brands in their Tweets, about 20% of them have personal sentiment involved (Jansen et al., 2009). These statistical evidences imply that Twitter has became a portal for public to express opinions. In the context of politics, Twitter content, together with Twitter users’ 88 Proceedings of the Joint Workshop on Social Dynamics and Pers</context>
<context position="6343" citStr="Johnson and Martin, 1998" startWordPosition="981" endWordPosition="984">e for researchers in other domain, we build a website to visualize the sentiment dynamics over time and let users download the data. Users could also define their own topics of interest and perform deeper analysis with keyword filtering and geolocation filtering. We present a case study in which our results might be used to answer core questions in political science about the nature of Supreme Court influence on public opinion. Political scientists have long been concerned with whether and how Supreme Court decisions affect public opinion and discourse about political topics (Hoekstra, 2003) (Johnson and Martin, 1998) (Gibson et al., 2003). Survey research on the subject has been limited in two ways. Survey analysis, including panel designs, rely on estimates near but never on the date of particular decisions. In addition, all surveybased research relies on estimates derived from an instrument designed to elicit sentiment – survey responses, useful as they are, do not reflect well how public opinion is naturally expressed. Our analysis allows for the examination of public opinion as it is naturally expressed and in a way that is precisely connected to the timing of decisions. Next, we state the problem mor</context>
<context position="14909" citStr="Johnson and Martin, 1998" startWordPosition="2349" endWordPosition="2352">tingly, instead of responding immediately, the “loser” group react and start Tweeting 2 days after the decision. These trends indicate that “winner” and “loser” in the debate react differently in time and intensity dimensions. We believe that our estimates of sentiment can be used in various ways by political scientists. The “positivity bias” (Gibson and Caldeira, 2009) model of Supreme Court opinion suggests that the Court can move public opinion in the direction of its decisions. Our results possibly indicate the opposite, the “polarizing” model suggested by (Franklin and Kosaki, 1989) and (Johnson and Martin, 1998), where more negative opinions are observed after the decision (in Figure 4), at least for a short period. By learning and visualize political sentiments, we could crystalize the nature of the decision that influences the degree to which the Supreme Court can move opinion in the direction of its decisions. 4 An Open Platform for Sharing and Analyzing Political Sentiments Figure 5 shows a website3 that visualizes political sentiments over time. The website shows several popular U.S. Supreme Court cases, such as “gay marriage”, “voting right act”, “tax cases”, 3http://www.courtometer.com 16-Jun </context>
</contexts>
<marker>Johnson, Martin, 1998</marker>
<rawString>Timothy R. Johnson, and Andrew D. Martin. 1998. The Public’s Conditional Response to Supreme Court Decisions. American Political Science Review 92(2):299-309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haewoon Kwak</author>
<author>Changhyun Lee</author>
<author>Hosung Park</author>
<author>Sue Moon</author>
</authors>
<title>What is Twitter, a Social Network or a News Media?.</title>
<date>2010</date>
<booktitle>in Proceedings of the 19th International Conference on World Wide Web (WWW).</booktitle>
<contexts>
<context position="3311" citStr="Kwak et al., 2010" startWordPosition="509" endWordPosition="512">sulted in a measurable change in opinion. A primary finding about implication of Dahl’s hypothesis is that the Court is polarizing, creating more supportive opinions of the policies it reviews among those who supported the policy before the decision and more negative opinions among those who opposed the policy prior to the decision (Franklin and Kosaki, 1989) (Johnson and Martin, 1998). We consider Twitter as important example of social expression of opinion. Recent studies of content on Twitter have revealed that 85% of Twitter content is related to spreading and commenting on headline news (Kwak et al., 2010); when users talk about commercial brands in their Tweets, about 20% of them have personal sentiment involved (Jansen et al., 2009). These statistical evidences imply that Twitter has became a portal for public to express opinions. In the context of politics, Twitter content, together with Twitter users’ 88 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 88–93, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics information, such as user’s profile and social network, have shown reasonable power of detecting</context>
</contexts>
<marker>Kwak, Lee, Park, Moon, 2010</marker>
<rawString>Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue Moon. 2010. What is Twitter, a Social Network or a News Media?. in Proceedings of the 19th International Conference on World Wide Web (WWW).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Ru Lin</author>
<author>Drew Margolin</author>
<author>Brian Keegan</author>
<author>David Lazer</author>
</authors>
<title>Voices of Victory: A Computational Focus Group Framework for Tracking Opinion Shift in Real Time.</title>
<date>2013</date>
<booktitle>In Proceedings of International World Wide Web Conference (WWW).</booktitle>
<marker>Lin, Margolin, Keegan, Lazer, 2013</marker>
<rawString>Yu-Ru Lin, Drew Margolin, Brian Keegan, and David Lazer. 2013. Voices of Victory: A Computational Focus Group Framework for Tracking Opinion Shift in Real Time. In Proceedings of International World Wide Web Conference (WWW).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies.</title>
<date>2012</date>
<contexts>
<context position="5130" citStr="Liu, 2012" startWordPosition="790" endWordPosition="791">itical science researchers, policy makers, and the public to better comprehend the public reaction to major political issues of the day. As we describe below, these different dimensions of discourse on Twitter allows examination of the multiple ways in which discourse changes when the Supreme Court makes a decision on a given issue of public policy. Our dimensions also open the door to new avenues of theorizing about the nature of public discourse on policy debates. Although general sentiment analysis has made significant advances over the last decade (Pang et al., 2002) (Pang and Lee, 2008) (Liu, 2012) (Wilson et al., 2009), and with the focus on certain aspects, such as intensity (Wilson et al., 2004), irony detection (Carvalho et al., 2009) and sarcasm detection (Davidov et al., 2010), analyzing Microblog content such as Twitter remains a challenging research topic (Reyes et al., 2012) (Vanin et al., 2013) (Agarwal et al., 2011). Unlike previous work, we introduce and focus on sentiment dimensions particularly important for political analysis of Microblog text, and extend and adapt classification techniques accordingly. To make the data and sentiment analysis results accessible for resear</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yelena Mejova</author>
<author>Padmini Srinivasan</author>
<author>Bob Boynton</author>
</authors>
<title>GOP Primary Season on Twitter: ”Popular” Political Sentiment in Social Media.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth ACM International Conference on Web Search and Data Mining (WSDM).</booktitle>
<contexts>
<context position="4251" citStr="Mejova et al., 2013" startWordPosition="653" endWordPosition="656"> of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 88–93, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics information, such as user’s profile and social network, have shown reasonable power of detecting user’s political leaning (Conover et al., 2011) and predicting elections (Tumasjan et al., 2010). Although promising, the effectiveness of using Twitter content to measure public political opinions remains unclear. Several studies show limited correlation between sentiment on Twitter and political polls in elections (Mejova et al., 2013) (O’Connor et al., 2010). Our study mainly focuses on investigating sentiment on Twitter about U.S. Supreme Court decisions. We propose more fine-grained dimensions for political sentiment analysis, such as supportiveness, emotional intensity and polarity, allowing political science researchers, policy makers, and the public to better comprehend the public reaction to major political issues of the day. As we describe below, these different dimensions of discourse on Twitter allows examination of the multiple ways in which discourse changes when the Supreme Court makes a decision on a given iss</context>
</contexts>
<marker>Mejova, Srinivasan, Boynton, 2013</marker>
<rawString>Yelena Mejova, Padmini Srinivasan, and Bob Boynton. 2013. GOP Primary Season on Twitter: ”Popular” Political Sentiment in Social Media. In Proceedings of the Sixth ACM International Conference on Web Search and Data Mining (WSDM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B O’Connor</author>
<author>R Balasubramanyan</author>
<author>B R Routledge</author>
<author>N A Smith</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Proceedings of International AAAI Conference on Weblogs and Social Media (ICWSM).</booktitle>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>B. O’Connor, R. Balasubramanyan, B. R. Routledge, and N. A. Smith. 2010. From tweets to polls: Linking text sentiment to public opinion time series. In Proceedings of International AAAI Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval.</title>
<date>2008</date>
<contexts>
<context position="5118" citStr="Pang and Lee, 2008" startWordPosition="786" endWordPosition="789">olarity, allowing political science researchers, policy makers, and the public to better comprehend the public reaction to major political issues of the day. As we describe below, these different dimensions of discourse on Twitter allows examination of the multiple ways in which discourse changes when the Supreme Court makes a decision on a given issue of public policy. Our dimensions also open the door to new avenues of theorizing about the nature of public discourse on policy debates. Although general sentiment analysis has made significant advances over the last decade (Pang et al., 2002) (Pang and Lee, 2008) (Liu, 2012) (Wilson et al., 2009), and with the focus on certain aspects, such as intensity (Wilson et al., 2004), irony detection (Carvalho et al., 2009) and sarcasm detection (Davidov et al., 2010), analyzing Microblog content such as Twitter remains a challenging research topic (Reyes et al., 2012) (Vanin et al., 2013) (Agarwal et al., 2011). Unlike previous work, we introduce and focus on sentiment dimensions particularly important for political analysis of Microblog text, and extend and adapt classification techniques accordingly. To make the data and sentiment analysis results accessibl</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang, and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="5097" citStr="Pang et al., 2002" startWordPosition="782" endWordPosition="785">onal intensity and polarity, allowing political science researchers, policy makers, and the public to better comprehend the public reaction to major political issues of the day. As we describe below, these different dimensions of discourse on Twitter allows examination of the multiple ways in which discourse changes when the Supreme Court makes a decision on a given issue of public policy. Our dimensions also open the door to new avenues of theorizing about the nature of public discourse on policy debates. Although general sentiment analysis has made significant advances over the last decade (Pang et al., 2002) (Pang and Lee, 2008) (Liu, 2012) (Wilson et al., 2009), and with the focus on certain aspects, such as intensity (Wilson et al., 2004), irony detection (Carvalho et al., 2009) and sarcasm detection (Davidov et al., 2010), analyzing Microblog content such as Twitter remains a challenging research topic (Reyes et al., 2012) (Vanin et al., 2013) (Agarwal et al., 2011). Unlike previous work, we introduce and focus on sentiment dimensions particularly important for political analysis of Microblog text, and extend and adapt classification techniques accordingly. To make the data and sentiment analy</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Reyes</author>
<author>Paolo Rosso</author>
<author>Tony Veale</author>
</authors>
<title>A multidimensional approach for detecting irony in Twitter. Language Resources and Evaluation.</title>
<date>2012</date>
<contexts>
<context position="5421" citStr="Reyes et al., 2012" startWordPosition="837" endWordPosition="840">the Supreme Court makes a decision on a given issue of public policy. Our dimensions also open the door to new avenues of theorizing about the nature of public discourse on policy debates. Although general sentiment analysis has made significant advances over the last decade (Pang et al., 2002) (Pang and Lee, 2008) (Liu, 2012) (Wilson et al., 2009), and with the focus on certain aspects, such as intensity (Wilson et al., 2004), irony detection (Carvalho et al., 2009) and sarcasm detection (Davidov et al., 2010), analyzing Microblog content such as Twitter remains a challenging research topic (Reyes et al., 2012) (Vanin et al., 2013) (Agarwal et al., 2011). Unlike previous work, we introduce and focus on sentiment dimensions particularly important for political analysis of Microblog text, and extend and adapt classification techniques accordingly. To make the data and sentiment analysis results accessible for researchers in other domain, we build a website to visualize the sentiment dynamics over time and let users download the data. Users could also define their own topics of interest and perform deeper analysis with keyword filtering and geolocation filtering. We present a case study in which our re</context>
</contexts>
<marker>Reyes, Rosso, Veale, 2012</marker>
<rawString>Antonio Reyes, Paolo Rosso, and Tony Veale. 2012. A multidimensional approach for detecting irony in Twitter. Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Galileo Namata</author>
<author>Lise Getoor</author>
<author>Janyce Wiebe</author>
</authors>
<title>Opinion Graphs for Polarity and Discourse Classification. TextGraphs4: Graph-based Methods for Natural Language Processing.</title>
<date>2009</date>
<marker>Somasundaran, Namata, Getoor, Wiebe, 2009</marker>
<rawString>Swapna Somasundaran, Galileo Namata, Lise Getoor, and Janyce Wiebe. 2009. Opinion Graphs for Polarity and Discourse Classification. TextGraphs4: Graph-based Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aline A Vanin</author>
<author>Larissa A Freitas</author>
<author>Re-nata Vieira</author>
<author>Marco Bochernitsan</author>
</authors>
<title>Some clues on irony detection in tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of International World Wide Web Conference (WWW).</booktitle>
<contexts>
<context position="5442" citStr="Vanin et al., 2013" startWordPosition="841" endWordPosition="844">es a decision on a given issue of public policy. Our dimensions also open the door to new avenues of theorizing about the nature of public discourse on policy debates. Although general sentiment analysis has made significant advances over the last decade (Pang et al., 2002) (Pang and Lee, 2008) (Liu, 2012) (Wilson et al., 2009), and with the focus on certain aspects, such as intensity (Wilson et al., 2004), irony detection (Carvalho et al., 2009) and sarcasm detection (Davidov et al., 2010), analyzing Microblog content such as Twitter remains a challenging research topic (Reyes et al., 2012) (Vanin et al., 2013) (Agarwal et al., 2011). Unlike previous work, we introduce and focus on sentiment dimensions particularly important for political analysis of Microblog text, and extend and adapt classification techniques accordingly. To make the data and sentiment analysis results accessible for researchers in other domain, we build a website to visualize the sentiment dynamics over time and let users download the data. Users could also define their own topics of interest and perform deeper analysis with keyword filtering and geolocation filtering. We present a case study in which our results might be used t</context>
</contexts>
<marker>Vanin, Freitas, Vieira, Bochernitsan, 2013</marker>
<rawString>Aline A. Vanin, Larissa A. Freitas, Re-nata Vieira, and Marco Bochernitsan. 2013. Some clues on irony detection in tweets. In Proceedings of International World Wide Web Conference (WWW).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity: an exploration offeatures for phrase-level sentiment analysis. Computational Linguistics.</title>
<date>2009</date>
<contexts>
<context position="5152" citStr="Wilson et al., 2009" startWordPosition="792" endWordPosition="796">ce researchers, policy makers, and the public to better comprehend the public reaction to major political issues of the day. As we describe below, these different dimensions of discourse on Twitter allows examination of the multiple ways in which discourse changes when the Supreme Court makes a decision on a given issue of public policy. Our dimensions also open the door to new avenues of theorizing about the nature of public discourse on policy debates. Although general sentiment analysis has made significant advances over the last decade (Pang et al., 2002) (Pang and Lee, 2008) (Liu, 2012) (Wilson et al., 2009), and with the focus on certain aspects, such as intensity (Wilson et al., 2004), irony detection (Carvalho et al., 2009) and sarcasm detection (Davidov et al., 2010), analyzing Microblog content such as Twitter remains a challenging research topic (Reyes et al., 2012) (Vanin et al., 2013) (Agarwal et al., 2011). Unlike previous work, we introduce and focus on sentiment dimensions particularly important for political analysis of Microblog text, and extend and adapt classification techniques accordingly. To make the data and sentiment analysis results accessible for researchers in other domain,</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing Contextual Polarity: an exploration offeatures for phrase-level sentiment analysis. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Rebecca Hwa</author>
</authors>
<title>Just how mad are you? Finding strong and weak opinion clauses.</title>
<date>2004</date>
<booktitle>In Proceedings of Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="5232" citStr="Wilson et al., 2004" startWordPosition="807" endWordPosition="810">action to major political issues of the day. As we describe below, these different dimensions of discourse on Twitter allows examination of the multiple ways in which discourse changes when the Supreme Court makes a decision on a given issue of public policy. Our dimensions also open the door to new avenues of theorizing about the nature of public discourse on policy debates. Although general sentiment analysis has made significant advances over the last decade (Pang et al., 2002) (Pang and Lee, 2008) (Liu, 2012) (Wilson et al., 2009), and with the focus on certain aspects, such as intensity (Wilson et al., 2004), irony detection (Carvalho et al., 2009) and sarcasm detection (Davidov et al., 2010), analyzing Microblog content such as Twitter remains a challenging research topic (Reyes et al., 2012) (Vanin et al., 2013) (Agarwal et al., 2011). Unlike previous work, we introduce and focus on sentiment dimensions particularly important for political analysis of Microblog text, and extend and adapt classification techniques accordingly. To make the data and sentiment analysis results accessible for researchers in other domain, we build a website to visualize the sentiment dynamics over time and let users </context>
</contexts>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004. Just how mad are you? Finding strong and weak opinion clauses. In Proceedings of Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andranik Tumasjan</author>
<author>Timm O Sprenger</author>
<author>Philipp G Sandner</author>
<author>Isabell M Welpe</author>
</authors>
<title>Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media (ICWSM).</booktitle>
<contexts>
<context position="4008" citStr="Tumasjan et al., 2010" startWordPosition="616" endWordPosition="619">m have personal sentiment involved (Jansen et al., 2009). These statistical evidences imply that Twitter has became a portal for public to express opinions. In the context of politics, Twitter content, together with Twitter users’ 88 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 88–93, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics information, such as user’s profile and social network, have shown reasonable power of detecting user’s political leaning (Conover et al., 2011) and predicting elections (Tumasjan et al., 2010). Although promising, the effectiveness of using Twitter content to measure public political opinions remains unclear. Several studies show limited correlation between sentiment on Twitter and political polls in elections (Mejova et al., 2013) (O’Connor et al., 2010). Our study mainly focuses on investigating sentiment on Twitter about U.S. Supreme Court decisions. We propose more fine-grained dimensions for political sentiment analysis, such as supportiveness, emotional intensity and polarity, allowing political science researchers, policy makers, and the public to better comprehend the publi</context>
</contexts>
<marker>Tumasjan, Sprenger, Sandner, Welpe, 2010</marker>
<rawString>Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sandner, and Isabell M. Welpe. 2010. Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment. In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
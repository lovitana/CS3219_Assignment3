<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000433">
<title confidence="0.9988">
A Template-based Abstractive Meeting Summarization: Leveraging
Summary and Source Text Relationships
</title>
<author confidence="0.998537">
Tatsuro Oya, Yashar Mehdad, Giuseppe Carenini, Raymond Ng
</author>
<affiliation confidence="0.999737">
Department of Computer Science
University of British Columbia, Vancouver, Canada
</affiliation>
<email confidence="0.98466">
{toya, mehdad, carenini, rng}@cs.ubc.ca
</email>
<sectionHeader confidence="0.993291" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999911642857143">
In this paper, we present an automatic
abstractive summarization system of
meeting conversations. Our system ex-
tends a novel multi-sentence fusion algo-
rithm in order to generate abstract tem-
plates. It also leverages the relationship
between summaries and their source
meeting transcripts to select the best
templates for generating abstractive
summaries of meetings. Our manual and
automatic evaluation results demonstrate
the success of our system in achieving
higher scores both in readability and in-
formativeness.
</bodyText>
<sectionHeader confidence="0.998215" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999979352941176">
People spend a vast amount of time in meetings
and these meetings play a prominent role in their
lives. Consequently, study of automatic meeting
summarization has been attracting peoples’ atten-
tion as it can save a great deal of their time and
increase their productivity.
The most common approaches to automatic
meeting summarization have been extractive.
Since extractive approaches do not require natu-
ral language generation techniques, they are ar-
guably simpler to apply and have been extensive-
ly investigated. However, a user study conducted
by Murray et al. (2010) indicates that users pre-
fer abstractive summaries to extractive ones.
Thereafter, more attention has been paid to ab-
stractive meeting summarization systems (Me-
hdad et al. 2013; Murray et al. 2010; Wang and
Cardie 2013). However, the approaches intro-
duced in previous studies create summaries by
either heavily relying on annotated data or by
fusing human utterances which may contain
grammatical mistakes. In this paper, we address
these issues by introducing a novel summariza-
tion approach that can create readable summaries
with less need for annotated data. Our system
first acquires templates from human-authored
summaries using a clustering and multi-sentence
fusion algorithm. It then takes a meeting tran-
script to be summarized, segments the transcript
based on topics, and extracts important phrases
from it. Finally, our system selects templates by
referring to the relationship between human-
authored summaries and their sources and fills
the templates with the phrases to create summar-
ies.
The main contributions of this paper are: 1)
The successful adaptation of a word graph algo-
rithm to generate templates from human-
authored summaries; 2) The implementation of a
novel template selection algorithm that effective-
ly leverages the relationship between human-
authored summary sentences and their source
transcripts; and 3) A comprehensive testing of
our approach, comprising both automatic and
manual evaluations.
We instantiate our framework on the AMI
corpus (Carletta et al., 2005) and compare our
summaries with those created from a state-of-
the-art systems. The evaluation results demon-
strate that our system successfully creates in-
formative and readable summaries.
</bodyText>
<sectionHeader confidence="0.99988" genericHeader="introduction">
2. Related Work
</sectionHeader>
<bodyText confidence="0.999958181818182">
Several studies have been conducted on creating
automatic abstractive meeting summarization
systems. One of them includes the system pro-
posed by Mehdad et al., (2013). Their approach
first clusters human utterances into communities
(Murray et al., 2012) and then builds an entail-
ment graph over each of the latter in order to se-
lect the salient utterances. It then applies a se-
mantic word graph algorithm to them and creates
abstractive summaries. Their results show some
improvement in creating informative summaries.
</bodyText>
<page confidence="0.992214">
45
</page>
<note confidence="0.689365">
Proceedings of the 8th International Natural Language Generation Conference, pages 45–53,
Philadelphia, Pennsylvania, 19-21 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99997292">
However, since they create these summaries by
merging human utterances, their summaries are
still partially extractive.
Recently, there have been some studies on
creating abstract summaries of specific aspects of
meetings such as decisions, actions and problems
(Murray et al. 2010; Wang and Cardie, 2013).
These summaries are called the Focused Meeting
Summaries (Carenini et al., 2011).
The system introduced by Murray et al. first
classifies human utterances into specific aspects
of meetings, e.g. decisions, problem, and action,
and then maps them onto ontologies. It then se-
lects the most informative subsets from these on-
tologies and finally generates abstractive sum-
maries of them, utilizing a natural language gen-
eration tool, simpleNLG (Gatt and Reiter, 2009).
Although their approach is essentially focused
meeting summarization, after creating summaries
of specific aspects, they aggregate them into one
single summary covering the whole meeting.
Wang and Cardie introduced a template-based
focused abstractive meeting summarization sys-
tem. Their system first clusters human-authored
summary sentences and applies a Multiple-
Sequence Alignment algorithm to them to gener-
ate templates. Then, given a meeting transcript to
be summarized, it identifies a human utterance
cluster describing a specific aspect and extracts
all summary-worthy relation instances, i.e. indi-
cator-argument pairs, from it. Finally, the tem-
plates are filled with these relation instances and
ranked accordingly, to generate summaries of a
specific aspect of the meeting.
Although the two approaches above are both
successful in creating readable summaries, they
rely on much annotated information, such as dia-
log act and sentiment types, and also require the
accurate classification of human utterances that
contain much noise and much ill-structured
grammar.
Our approach is inspired by the works intro-
duced here but improves on their shortcomings.
Unlike those of Murray et al. (2010) and Wang
and Cardie (2013), our system relies less on an-
notated training data and does not require a clas-
sifier. In addition, our evaluation indicates that
our system can create summaries of the entire
conversations that are more informative and
readable than those of Mehdad et al.(2013).
</bodyText>
<sectionHeader confidence="0.993129" genericHeader="method">
3. Framework
</sectionHeader>
<bodyText confidence="0.999978375">
In order for summaries to be readable and in-
formative, they should be grammatically correct
and contain important information in meetings.
To this end, we have created our framework con-
sisting of the following two components: 1) An
off-line template generation module, which gen-
eralizes collected human-authored summaries
and creates templates from them; and 2) An on-
line summary generation module, which seg-
ments meeting transcripts based on the topics
discussed, extracts the important phrases from
these segments, and generate abstractive sum-
maries of them by filling the phrases into the ap-
propriate templates. Figure 1 depicts our frame-
work. In the following sections, we describe each
of the two components in detail.
</bodyText>
<figureCaption confidence="0.931945">
Figure 1: Our meeting summarization framework. Top: off-line Template generation module. Bottom: on-line
Summary Generation module.
</figureCaption>
<page confidence="0.998337">
46
</page>
<subsectionHeader confidence="0.995919">
3.1 Template Generation Module
</subsectionHeader>
<bodyText confidence="0.999910666666667">
Our template generation module attempts to sat-
isfy two possibly conflicting objectives. First,
templates should be quite specific such that they
accept only the relevant fillers. Second, our
module should generate generalized templates
that can be used in many situations. We assume
that the former is achieved by labeling phrases
with their hypernyms that are not too general and
the latter by merging related templates. Based on
these assumptions, we divide our module into the
three tasks: 1) Hypernym labeling; 2) Clustering;
and 3) Template fusion.
</bodyText>
<subsectionHeader confidence="0.9757">
3.1.1 Hypernym Labeling
</subsectionHeader>
<bodyText confidence="0.999824414634146">
Templates are derived from human-authored
meeting summaries in the training data. We first
collect sentences whose subjects are meeting par-
ticipant(s) and that contain active root verbs,
from the summaries. This is achieved by utilizing
meeting participant information provided in the
corpus and parsing sentences with the Stanford
Parser (Marneffe et al., 2006). The motivation
behind this process is to collect sentences that are
syntactically similar. We then identify all noun
phrases in these sentences using the Illinois
Chunker (Punyakanok and Roth, 2001). This
chunker extracts all noun phrases as well as part
of speech (POS) for all words. To add further in-
formation on each noun phrase, we label the right
most nouns (the head nouns) in each phrase with
their hypernyms using WordNet (Fellbaum,
1998). In WordNet, hypernyms are organized in-
to hierarchies ranging from the most abstract to
the most specific. For our work, we utilize the
fourth most abstract hypernyms in light of the
first goal discussed at the beginning of Section
3.1, i.e. not too general. For disambiguating the
sense of the nouns, we simply select the sense
that has the highest frequency in WordNet.
At this stage, all noun phrases in sentences
are tagged with their hypernyms defined in
WordNet, such as “artifact.n.01”, and “act.n.02”,
where n’s stands for nouns and the two digit
numbers represent their sense numbers. We treat
these hypernym-labeled sentences as templates
and the phrases as blanks.
In addition, we also create two additional
rules for tagging noun phrases: 1) Since the sub-
jects of all collected sentences are meeting par-
ticipant(s), we label all subject noun phrases as
“speaker”; and 2) If the noun phrases consist of
meeting specific terms such as “the meeting” or
“the group”, we do not convert them into blanks.
These two rules guarantee the creation of tem-
plates suitable for meetings.
</bodyText>
<figureCaption confidence="0.977979">
Figure 2: Some examples of hypernym labeling task
</figureCaption>
<subsectionHeader confidence="0.907735">
3.1.2 Clustering
</subsectionHeader>
<bodyText confidence="0.999840789473684">
Next, we cluster the templates into similar
groups. We utilize root verb information for this
process assuming that these verbs such as “dis-
cuss” and “suggest” that appear in summaries are
the most informative factors in describing meet-
ings. Therefore, after extracting root verbs in
summary sentences, we create fully connected
graphs where each node represents the root verbs
and each edge represents a score denoting how
similar the two word senses are. To measure the
similarity of two verbs, we first identify the verb
senses based on their frequency in WordNet and
compute the similarity score based on the short-
est path that connects the senses in the hypernym
taxonomy. We then convert the graph into a
similarity matrix and apply a Normalized Cuts
method (Shi and Malik, 2000) to cluster the root
verbs. Finally, all templates are organized into
the groups created by their root verbs.
</bodyText>
<figureCaption confidence="0.988344">
Figure 3: A word graph generated from related templates and the highest scored path (shown in bold)
</figureCaption>
<page confidence="0.994465">
47
</page>
<subsectionHeader confidence="0.680514">
3.1.3 Template Fusion
</subsectionHeader>
<bodyText confidence="0.999961428571429">
We further generalize the clustered templates by
applying a word graph algorithm. The algorithm
was originally proven to be effective in summa-
rizing a cluster of related sentences (Boudin and
Morin, 2013; Filippova, 2010; Mehdad et al.,
2013). We extend it so that it can be applied to
templates.
</bodyText>
<subsectionHeader confidence="0.844259">
Word Graph Construction
</subsectionHeader>
<bodyText confidence="0.998864066666666">
In our system, a word graph is a directed graph
with words or blanks serving as nodes and edges
representing adjacency relations.
Given a set of related templates in a group,
the graph is constructed by first creating a start
and end node, and then iteratively adding tem-
plates to it. When adding a new template, the al-
gorithm first checks each word in the template to
see if it can be mapped onto existing nodes in the
graph. The word is mapped onto a node if the
node consists of the same word and the same
POS tag, and no word from this template has
been mapped onto this node yet. Then, it checks
each blank in the template and maps it onto a
node if the node consists of the same hypernym-
labeled blank and no blank from this template
has been mapped onto this node yet.
When more than one node refer to the same
word or blank in the template, or when more than
one word or blank in the template can be mapped
to the same node in the graph, the algorithm
checks the neighboring nodes in the current
graph as well as the preceding and the subse-
quent words or blanks in the template. Then,
those word-node or blank-node pairs with higher
overlap in the context are selected for mapping.
Otherwise, a new node is created and added to
the graph. As a simplified illustration, we show a
word graph in Figure 3 obtained from the follow-
ing four templates.
</bodyText>
<listItem confidence="0.999601">
• After introducing [situation.n.01], [speaker] then dis-
cussed [content.n.05] .
• Before beginning [act.n.02] of [artifact.n.01], [speaker]
discussed [act.n.02] and [content.n.05] for [arti-
fact.n.01] .
• [speaker] discussed [content.n.05] of [artifact.n.01] and
[material.n.01] .
• [speaker] discussed [act.n.02] and [asset.n.01] in attract-
ing [living_thing.n.01] .
</listItem>
<subsectionHeader confidence="0.986885">
Path Selection
</subsectionHeader>
<bodyText confidence="0.999894272727273">
The word graph generates many paths connect-
ing its start and end nodes, not all of which are
readable and cannot be used as templates. Our
aim is to create concise and generalized tem-
plates. Therefore, we create the following rank-
ing strategy to be able to select the ideal paths.
First, to filter ungrammatical or complex tem-
plates, the algorithm prunes away the paths hav-
ing more than three blanks; having subordinate
clauses; containing no verb; having two consecu-
tive blanks; containing blanks which are not la-
beled by any hypernym; or whose length are
shorter than three words. Note that these rules,
which were defined based on close observation
of the results obtained from our development set,
greatly reduce the chance of selecting ill-
structured templates. Second, the remaining
paths are reranked by 1) A normalized path
weight and 2) A language model learned from
hypernym-labeled human-authored summaries in
our training data, each of which is described be-
low.
</bodyText>
<sectionHeader confidence="0.443367" genericHeader="method">
1) Normalized Path Weight
</sectionHeader>
<bodyText confidence="0.990142230769231">
We adapt Filippova (2010)’s approach to com-
pute the edge weight. The formula is shown as:
∑
where ei,j is an edge that connects the nodes i
and j in a graph, freq(i) is the number of words
and blanks in the templates that are mapped to
node i and diff(p,i,j) is the distance between the
offset positions of nodes i and j in path p. This
weight is defined so that the paths that are in-
formative and that contain salient (frequent)
words are selected. To calculate a path score,
W(p), all the edge weights on the path are
summed and normalized by its length.
</bodyText>
<sectionHeader confidence="0.721865" genericHeader="method">
2) Language Model
</sectionHeader>
<bodyText confidence="0.99994975">
Although the goal is to create concise templates,
these templates must be grammatically correct.
Hence, we train an n-gram language model using
all templates generated from the training data in
the hypernym labeling stage. Then for each path,
we compute a sum of negative log probabilities
of n-gram occurrences and normalize the score
by its length, which is represented as H(p).
The final score of each path is calculated as
follows:
where α and R are the coefficient factors which
are tuned using our development set. For each
</bodyText>
<page confidence="0.996266">
48
</page>
<bodyText confidence="0.9995126">
group of clusters, the top ten best scored paths
are selected as templates and added to its group.
As an illustration, the path shown in bold in
Figure 3 is the highest scored path obtained from
this path ranking strategy.
</bodyText>
<subsectionHeader confidence="0.997785">
3.2 Summary Generation Module
</subsectionHeader>
<bodyText confidence="0.9998504">
This section explains our summary generation
module consisting of four tasks: 1) Topic seg-
mentation; 2) Phrase and speaker extraction; 3)
Template selection and filling; and 4) Sentence
ranking.
</bodyText>
<subsubsectionHeader confidence="0.481866">
3.2.1 Topic Segmentation
</subsubsectionHeader>
<bodyText confidence="0.999985666666667">
It is important for a summary to cover all topics
discussed in the meeting. Therefore, given a
meeting transcript to be summarized, after re-
moving speech disfluencies such as “uh”, and
“ah”, we employ a topic segmenter, LCSeg (Gal-
ley et al., 2003) which create topic segments by
observing word repetitions.
One shortcoming of LCSeg is that it ignores
speaker information when segmenting transcripts.
Important topics are often discussed by one or
two speakers. Therefore, in order to take ad-
vantage of the speaker information, we extend
LCSeg by adding the following post-process
step: If a topic segment contains more than 25 ut-
terances, we subdivide the segment based on the
speakers. These subsegments are then compared
with one another using cosine similarity, and if
the similarity score is greater than that of the
threshold (0.05), they are merged. The two num-
bers, i.e. 25 and 0.05, were selected based on the
development set so that, when segmenting a tran-
script, the system can effectively take into ac-
count speaker information without creating too
many segments.
</bodyText>
<subsectionHeader confidence="0.818202">
3.2.2 Phrase And Speaker Extraction
</subsectionHeader>
<bodyText confidence="0.99971055">
All salient phrases are then extracted from each
topic segment in the same manner as performed
in the template generation module in Section 3.1,
by: 1) Extracting all noun phrases; and 2) Label-
ing each phrase with the hypernym of its head
noun. Furthermore, to be able to select salient
phrases, these phrases are subsequently scored
and ranked based on the sum of the frequency of
each word in the segment. Finally, to handle re-
dundancy, we remove phrases that are subsets of
others.
In addition, for each utterance in the meeting,
the transcript contains its speaker’s name. There-
fore, we extract the most dominant speakers’
name(s) for each topic segment and label them as
“speaker”. These phrases and this speaker infor-
mation will later be used in the template filling
process. Table 1 below shows an example of
dominant speakers and high scored phrases ex-
tracted from a topic segment.
</bodyText>
<table confidence="0.9996682">
Dominant speakers
Project Manager (speaker)
Industrial Designer (speaker)
High scored phrases (hypernyms)
the whole look (appearance.n.01)
the company logo (symbol.n.01)
the product (artifact.n.01)
the outside (region.n.01)
electronics (content.n.05)
the fashion (manner.n.01)
</table>
<tableCaption confidence="0.960798">
Table 1: Dominant speakers and high scored
phrases extracted from a topic segment
</tableCaption>
<subsectionHeader confidence="0.896966">
3.2.3 Template Selection and Filling
</subsectionHeader>
<bodyText confidence="0.999983666666667">
In terms of our training data, all human-authored
abstractive summary sentences have links to the
subsets of their source transcripts which support
and convey the information in the abstractive
sentences as illustrated in Figure 4. These subsets
are called communities. Since each community is
used to create one summary sentence, we hy-
pothesize that each community covers one spe-
cific topic.
Thus, to find the best templates for each topic
segment, we refer to our training data. In particu-
lar, we first find communities in the training set
that are similar to the topic segment and identify
the templates derived from the summary sen-
tences linked to these communities.
</bodyText>
<figureCaption confidence="0.9922855">
Figure 4: A link from an abstractive summary sentence to a subset of a meeting transcript that conveys or sup-
ports the information in the abstractive sentence
</figureCaption>
<page confidence="0.998401">
49
</page>
<bodyText confidence="0.999765666666667">
This process is done in two steps, by: 1) As-
sociating the communities in the training data
with the groups containing templates that were
created in our template generation module; and
2) Finding templates for each topic segment by
comparing the similarities between the segments
and all sets of communities associated with the
template groups. Below, we describe the two
steps in detail.
</bodyText>
<listItem confidence="0.965609333333333">
1) Recall that in the template generation
module in Section 3.1, we label human-authored
summary sentences in training data with hyper-
nyms and cluster them into similar groups. Thus,
as shown in Figure 5, we first associate all sets of
communities in the training data into these
groups by determining to which groups the
summary sentences linked by these communities
belong.
</listItem>
<figureCaption confidence="0.985539666666667">
Figure 5: An example demonstrating how each com-
munity in training data is associated with a group con-
taining templates
</figureCaption>
<bodyText confidence="0.545331666666667">
2) Next, for each topic segment, we compute
average cosine similarity between the segment
and all communities in all of the groups.
</bodyText>
<figureCaption confidence="0.998503333333333">
Figure 6: Computing the average cosine similarities
between a topic segment and all sets of com munities
in each group
</figureCaption>
<bodyText confidence="0.999982363636364">
At this stage, each community is already as-
sociated with a group that contains ranked tem-
plates. In addition, each segment has a list of av-
erage-scores that measures how similar the seg-
ment is to the communities in each group. Hence,
the templates used for each segment are decided
by selecting the ones from the groups with higher
scores.
Our system now contains for each segment a
set of phrases and ideal templates, both of which
are scored, as well as the most dominant speakers’
name(s). Thus, candidate sentences are generated
for each segment by: first, selecting speakers’
name(s), then selecting phrases and templates
based on their scores; and finally filling the tem-
plates with matching labels. Here, we limit the
maximum number of sentences created for each
topic segment to 30. This number is defined so
that the system can avoid generating sentences
consisting of low scored phrases and templates.
Finally, these candidate sentences are passed to
our sentence ranking module.
</bodyText>
<subsectionHeader confidence="0.851588">
3.2.4 Sentence Ranking
</subsectionHeader>
<bodyText confidence="0.999950142857143">
Our system will create many candidate sentenc-
es, and most of them will be redundant. Hence,
to be able to select the most fluent, informative
and appropriate sentences, we create a sentence
ranking model considering 1) Fluency, 2) Cover-
age, and 3) The characteristics of the meeting,
each of which are summarized below:
</bodyText>
<sectionHeader confidence="0.657365" genericHeader="method">
1) Fluency
</sectionHeader>
<bodyText confidence="0.996692777777778">
We estimate the fluency of the generated sen-
tences in the same manner as in Section 3.1.3.
That is, we train a language model on human-
authored abstract summaries from the training
portions of meeting data and then compute a
normalized sum of negative log probabilities of
n-gram occurrences in the sentence. The fluency
score is represented as H(s) in the equation be-
low.
</bodyText>
<listItem confidence="0.639392">
2) Coverage
</listItem>
<bodyText confidence="0.998613333333333">
To select sentences that cover important topics,
we give special rewards to the sentences that
contain the top five ranked phrases.
</bodyText>
<sectionHeader confidence="0.39496" genericHeader="method">
3) The Characteristics of the Meeting
</sectionHeader>
<bodyText confidence="0.9998646">
We also add three additional scoring rules that
are specific to the meeting summaries. In particu-
lar, these three rules are created based on phrases
often used in the opening and closing of meet-
ings in a development set: 1) If sentences derived
</bodyText>
<page confidence="0.990577">
50
</page>
<bodyText confidence="0.999766333333333">
from the first segment contain the words “open”
or “meeting”, they will be rewarded; 2) If sen-
tences derived from the last segment contain the
words “close” or “meeting”, the sentences will
again be rewarded; and 3) If sentences not de-
rived from the first or last segment contains the
words “open” or “close”, they will be penalized.
The final ranking score of the candidate sen-
tences is computed using the follow formula:
</bodyText>
<equation confidence="0.966814">
Score(s)=α (s)+Z βi
i =1 Ri (s)+Z i
i =1 i (s)
</equation>
<bodyText confidence="0.999981166666667">
where, Ri (s) is a binary that indicates whether
the top i ranked phrase exists in sentence s; Mi (s)
is also a binary that indicates whether the i th
meeting specific rule can be met for sentence s;
and α, β i and γ i are the coefficient factors to tune
the ranking score, all of which are tuned using
our development set.
Finally, the sentence ranked the highest in
each segment is selected as the summary sen-
tence, and the entire meeting summary is created
by collecting these sentences and sorting them by
the chronological order of the topic segments.
</bodyText>
<sectionHeader confidence="0.986643" genericHeader="evaluation">
4. Evaluation
</sectionHeader>
<bodyText confidence="0.9999294">
In this section, we describe an evaluation of our
system. First, we describe the corpus data. Next,
the results of the automatic and manual evalua-
tions of our system against various baseline ap-
proaches are discussed.
</bodyText>
<subsectionHeader confidence="0.990811">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.99998825">
For our meeting summarization experiments, we
use manually transcripted meeting records and
their human-authored summaries in the AMI
corpus. The corpus contains 139 meeting records
in which groups of four people play different
roles in a fictitious team. We reserved 20 meet-
ings for development and implemented a three-
fold cross-validation using the remaining data.
</bodyText>
<subsectionHeader confidence="0.997803">
4.2 Automatic Evaluation
</subsectionHeader>
<bodyText confidence="0.999240885714286">
We report the F1-measure of ROUGE-1,
ROUGE-2 and ROUGE-SU4 (Lin and Hovy,
2003) to assess the performance of our system.
The scores of automatically generated summaries
are calculated by comparing them with human-
authored ones.
For our baselines, we use the system intro-
duced by Mehdad et al. (2013) (FUSION), which
creates abstractive summaries from extracted
sentences and was proven to be effective in cre-
ating abstractive meeting summaries; and Tex-
tRank (Mihalcea and Tarau, 2004), a graph based
sentence ranker that is suitable for creating ex-
tractive summaries. Our system can create sum-
maries of any length by adjusting the number of
segments to be created by LCSeg. Thus, we cre-
ate summaries of three different lengths (10, 15,
and 20 topic segments) with the average number
of words being 100, 137, and 173, respectively.
These numbers generally corresponds to human-
authored summary length in the corpus which
varies from 82 to 200 words.
Table 2 shows the results of our system in
comparison with those of the two baselines. The
results show that our model significantly outper-
forms the two baselines. Compared with FU-
SION, our system with 20 segments achieves
about 3 % of improvement in all ROUGE scores.
This indicates that our system creates summaries
that are more lexically similar to human-authored
ones. Surprisingly, there was not a significant
change in our ROUGE scores over the three dif-
ferent summary lengths. This indicates that our
system can create summaries of any length with-
out losing its content.
</bodyText>
<table confidence="0.9997045">
Models Rouge-1 Rouge-2 Rouge-SU4
TextRank 21.7 2.5 6.5
FUSION 27.9 4.0 8.1
Our System 10 Seg. 28.4 6.7 10.1
Our System 15 Seg. 30.6 6.8 10.9
Our System 20 Seg. 31.5 6.7 11.4
</table>
<tableCaption confidence="0.994395">
Table 2: An evaluation of summarization performance
using the F1 measure of ROUGE-1 2, and SU4
</tableCaption>
<subsectionHeader confidence="0.998687">
4.3 Manual Evaluation
</subsectionHeader>
<bodyText confidence="0.99998075">
We also conduct manual evaluations utilizing a
crowdsourcing tool1. In this experiment, our sys-
tem with 15 segments is compared with FUSION,
human-authored summaries (ABS) and, human-
annotated extractive summaries (EXT).
After randomly selecting 10 meetings, 10 par-
ticipants were selected for each meeting and giv-
en instructions to browse the transcription of the
meeting so as to understand its gist. They were
then asked to read all different types of summar-
ies described above and rate each of them on a 1-
5 scale for the following three items: 1) The
summary’s overall quality, with “5” being the
best and “1” being the worst possible quality; 2)
The summary’s fluency, ignoring the capitaliza-
tion or punctuation, with “5” indicating no
grammatical mistakes and “1” indicating too
many; and 3) The summary’s informativeness,
with “5” indicating that the summary covers all
meeting content and “1” indicating that the
</bodyText>
<footnote confidence="0.981551">
1 http://www.crowdflower.com/
</footnote>
<page confidence="0.998441">
51
</page>
<bodyText confidence="0.997983727272727">
summary does not cover the content at all.
The results are described in Table 3. Overall,
58 people worldwide, who are among the most
reliable contributors accounting for 7 % of over-
all members and who maintain the highest levels
of accuracy on test questions provided in pervi-
ous crowd sourcing jobs, participated in this rat-
ing task. As to statistical significance, we use the
2-tail pairwise t-test to compare our system with
the other three approaches. The results are sum-
marized in Table 4.
</bodyText>
<table confidence="0.998007">
Models Quality Fluency Informativeness
Our System 3.52 3.69 3.54
ABS 3.96 4.03 3.87
EXT 3.02 3.16 3.30
FUSION 3.16 3.14 3.05
</table>
<tableCaption confidence="0.999982">
Table 3: Average rating scores.
Table 4: T-test results of manual evaluation
</tableCaption>
<bodyText confidence="0.99996625">
As expected, for all of the three items, ABS
received the highest of all ratings, while our sys-
tem received the second highest. The t-test re-
sults indicate that the difference in the rating data
is statistically significant for all cases except that
of informativeness between ours and the extrac-
tive summaries. This can be understood because
the extractive summaries were manually created
by an annotator and contain all of the important
information in the meetings.
From this observation, we can conclude that
users prefer our template-based summaries over
human-annotated extractive summaries and ab-
stractive summaries created from extracted sali-
ent sentences. Furthermore, it demonstrates that
our summaries are as informative as human-
annotated extractive ones.
Finally, we show in Figure 7 one of the sum-
maries created by our system in line-with a hu-
man-authored one.
</bodyText>
<sectionHeader confidence="0.991589" genericHeader="conclusions">
5. Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.988738980392157">
In this paper, we have demonstrated a robust ab-
stractive meeting summarization system. Our ap-
proach makes three main contributions. First, we
have proposed a novel approach for generating
templates leveraging a multi-sentence fusion al-
gorithm and lexico-semantic information. Sec-
ond, we have introduced an effective template
selection method, which utilize the relationship
between human-authored summaries and their
source transcripts. Finally, comprehensive evalu-
ation demonstrated that summaries created by
our system are preferred over human-annotated
extractive ones as well as those created from a
state-of-the-art meeting summarization system.
The current version of our system uses only
hypernym information in WordNet to label
phrases. Considering limited coverage in Word-
Net, future work includes extending our frame-
work by applying a more sophisticated labeling
task utilizing a richer knowledge base (e.g., YA-
GO). Also, we plan to apply our framework to
different multi-party conversational domains
such as chat logs and forum discussions.
Human-Authored Summary
The project manager opened the meeting and had the
team members introduce themselves and describe their
roles in the upcoming project. The project manager then
described the upcoming project. The team then discussed
their experiences with remote controls. They also
discussed the project budget and which features they
would like to see in the remote control they are to create.
The team discussed universal usage, how to find remotes
when misplaced, shapes and colors, ball shaped remotes,
marketing strategies, keyboards on remotes, and remote
sizes. team then discussed various features to consider in
making the remote.
Summary Created by Our System with 15 Segment
project manager summarized their role of the meeting .
user interface expert and project manager talks about a
universal remote . the group recommended using the
International Remote Control Association rather than a
remote control . project manager offered the ball
idea .user interface expert suggested few buttons . user
interface expert and industrial designer then asked a
member about a nice idea for The idea . project manager
went over a weak point . the group announced the one-
handed design . project manager and industrial designer
went over their remote control idea . project manager
instructed a member to research the ball function .
industrial designer went over stability point .industrial
designer went over definite points .
</bodyText>
<figureCaption confidence="0.976221">
Figure 7: A comparison between a human-authored
summary and a summary created by our system
</figureCaption>
<sectionHeader confidence="0.994508" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999694333333333">
We would like to thank all members in UBC
NLP group for their comments and UBC LCI
group and ICICS for financial support.
</bodyText>
<sectionHeader confidence="0.997912" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.930586666666667">
Florian Boudin and Emmanuel Morin. 2013.
Keyphrase Extraction for N-best Reranking in Mul-
ti-Sentence Compression. In Proceedings of the
</reference>
<figure confidence="0.998690923076923">
Models
Compared
Our System
vs. ABS
Our System
vs. FUSION
Our System
vs. EXT.
Quality Fluency Informativeness
(P-value) (P-value) (P-value)
0.000162 0.000437 0.00211
0.00142 0.0000135 0.000151
0.000124 0.0000509 0.0621
</figure>
<page confidence="0.968071">
52
</page>
<reference confidence="0.999616917647058">
2013 Conference of the Iorth American Chapter of
the Association for Computational Linguistics:
Human Language Technologies (IAACL-HLT
2013), 2013.
Giuseppe Carenini, Gabriel Murray, and Raymond Ng.
2011. Methods for Mining and Summarizing Text
Conversations. Morgan Claypool.
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guil-
lemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,
M. Kronenthal, G. Lathoud, M. Lincoln, A.
Lisowska, I. McCowan, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI meeting corpus: A pre-
announcement. In Proceeding of MLMI 2005, Ed-
inburgh, UK, pages 28–39.
Christiane Fellbaum 1998. WordIet, An Electronic
Lexical Database. The MIT Press. Cambridge, MA.
Katja Filippova. 2010. Multi-sentence compression:
finding shortest paths in word graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ’10, pages 322–
330, Stroudsburg, PA, USA. Association for Com-
putational Linguistic
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL 2003). 562–
569. Sapporo, Japan.
Albert Gatt and Ehud Reiter. 2009. SimpleNLG: a
Realisation Engine for Practical Applications. In
ENLG’09: Proceedings of the 12th European
Workshop on Iatural Language Generation, pages
90–93, Morristown, NJ, USA. Association for
Computational Linguistics.
Ravi Kondadadi, Blake Howald and Frank Schilder.
2013. A Statistical NLG Framework for Aggregat-
ed Planning and Realization. In Proceeding of the
Annual Conferene for the Association of Computa-
tional Linguistic (ACL 2013).
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses.
In Proceedings of the Fifth International Confer-
ence on Language Resources and Evaluation
(LREC&apos;06).
Yashar Mehdad, Giuseppe Carenini, Frank Tompa.
2013. Abstractive Meeting Summarization with En-
tailment and Fusion. In Proceedings of the 14th Eu-
ropean Iatural Language Generation (EILG -
SIGGEI 2013), Sofia, Bulgaria.
Rata Mihalcea and Paul Tarau 2004. TextRank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Iatural
Language Processing, July.
Gabriel Murray, Giuseppe Carenini, and Raymond T.
Ng. 2010. Generating and validating abstracts of
meeting conversations: a user study. In IILG 2010.
Gabriel Murray, Giuseppe Carenini and Raymond Ng.
2012. Using the Omega Index for Evaluating Ab-
stractive Community Detection, IAACL 2012,
Workshop on Evaluation Metrics and System Com-
parison for Automatic Summarization, Montreal,
Canada.
Vasin Punyakanok and Dan Roth. 2001. The Use of
Classifiers in Sequential Inference. IIPS (2001) pp.
995-1001.
Jianbo Shi and Jitendra Malik. 2000. Normalized Cuts
&amp; Image Segmentation. IEEE Trans. of PAMI, Aug
2000.
David C. Uthus and David W. Aha. 2011. Plans to-
ward automated chat summarization. In Proceed-
ings of the Workshop on Automatic Summarization
for Different Genres, Media, and Languages,
WASDGML’11, pages 1-7, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Lu Wang and Claire Cardie. 2013. Domain-
Independent Abstract Generation for Focused
Meeting Summarization. In ACL 2013.
Liang Zhou and Eduard Hovy. 2005. Digesting virtual
“geek” culture: The summarization of technical in-
ternet relay chats. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL’05), pages 298-305, Ann Arbor,
Michigan, June. Association for Computational
Linguistics.
</reference>
<page confidence="0.999334">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.697359">
<title confidence="0.9994595">A Template-based Abstractive Meeting Summarization: Summary and Source Text Relationships</title>
<author confidence="0.995759">Tatsuro Oya</author>
<author confidence="0.995759">Yashar Mehdad</author>
<author confidence="0.995759">Giuseppe Carenini</author>
<author confidence="0.995759">Raymond</author>
<affiliation confidence="0.9986085">Department of Computer University of British Columbia, Vancouver,</affiliation>
<email confidence="0.968839">toya@cs.ubc.ca</email>
<email confidence="0.968839">mehdad@cs.ubc.ca</email>
<email confidence="0.968839">carenini@cs.ubc.ca</email>
<email confidence="0.968839">rng@cs.ubc.ca</email>
<abstract confidence="0.981659533333333">In this paper, we present an automatic abstractive summarization system of meeting conversations. Our system extends a novel multi-sentence fusion algorithm in order to generate abstract templates. It also leverages the relationship between summaries and their source meeting transcripts to select the best templates for generating abstractive summaries of meetings. Our manual and automatic evaluation results demonstrate the success of our system in achieving higher scores both in readability and informativeness.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Florian Boudin</author>
<author>Emmanuel Morin</author>
</authors>
<title>Keyphrase Extraction for N-best Reranking in Multi-Sentence Compression.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the Iorth American Chapter of the Association for Computational Linguistics: Human Language Technologies (IAACL-HLT</booktitle>
<contexts>
<context position="10724" citStr="Boudin and Morin, 2013" startWordPosition="1638" endWordPosition="1641">ased on the shortest path that connects the senses in the hypernym taxonomy. We then convert the graph into a similarity matrix and apply a Normalized Cuts method (Shi and Malik, 2000) to cluster the root verbs. Finally, all templates are organized into the groups created by their root verbs. Figure 3: A word graph generated from related templates and the highest scored path (shown in bold) 47 3.1.3 Template Fusion We further generalize the clustered templates by applying a word graph algorithm. The algorithm was originally proven to be effective in summarizing a cluster of related sentences (Boudin and Morin, 2013; Filippova, 2010; Mehdad et al., 2013). We extend it so that it can be applied to templates. Word Graph Construction In our system, a word graph is a directed graph with words or blanks serving as nodes and edges representing adjacency relations. Given a set of related templates in a group, the graph is constructed by first creating a start and end node, and then iteratively adding templates to it. When adding a new template, the algorithm first checks each word in the template to see if it can be mapped onto existing nodes in the graph. The word is mapped onto a node if the node consists of </context>
</contexts>
<marker>Boudin, Morin, 2013</marker>
<rawString>Florian Boudin and Emmanuel Morin. 2013. Keyphrase Extraction for N-best Reranking in Multi-Sentence Compression. In Proceedings of the 2013 Conference of the Iorth American Chapter of the Association for Computational Linguistics: Human Language Technologies (IAACL-HLT 2013), 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Gabriel Murray</author>
<author>Raymond Ng</author>
</authors>
<title>Methods for Mining and Summarizing Text Conversations.</title>
<date>2011</date>
<publisher>Morgan Claypool.</publisher>
<contexts>
<context position="4181" citStr="Carenini et al., 2011" startWordPosition="613" endWordPosition="616"> improvement in creating informative summaries. 45 Proceedings of the 8th International Natural Language Generation Conference, pages 45–53, Philadelphia, Pennsylvania, 19-21 June 2014. c�2014 Association for Computational Linguistics However, since they create these summaries by merging human utterances, their summaries are still partially extractive. Recently, there have been some studies on creating abstract summaries of specific aspects of meetings such as decisions, actions and problems (Murray et al. 2010; Wang and Cardie, 2013). These summaries are called the Focused Meeting Summaries (Carenini et al., 2011). The system introduced by Murray et al. first classifies human utterances into specific aspects of meetings, e.g. decisions, problem, and action, and then maps them onto ontologies. It then selects the most informative subsets from these ontologies and finally generates abstractive summaries of them, utilizing a natural language generation tool, simpleNLG (Gatt and Reiter, 2009). Although their approach is essentially focused meeting summarization, after creating summaries of specific aspects, they aggregate them into one single summary covering the whole meeting. Wang and Cardie introduced a</context>
</contexts>
<marker>Carenini, Murray, Ng, 2011</marker>
<rawString>Giuseppe Carenini, Gabriel Murray, and Raymond Ng. 2011. Methods for Mining and Summarizing Text Conversations. Morgan Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>S Ashby</author>
<author>S Bourban</author>
<author>M Flynn</author>
<author>M Guillemot</author>
<author>T Hain</author>
<author>J Kadlec</author>
<author>V Karaiskos</author>
<author>W Kraaij</author>
<author>M Kronenthal</author>
<author>G Lathoud</author>
<author>M Lincoln</author>
<author>A Lisowska</author>
<author>I McCowan</author>
<author>W Post</author>
<author>D Reidsma</author>
<author>P Wellner</author>
</authors>
<title>The AMI meeting corpus: A preannouncement.</title>
<date>2005</date>
<booktitle>In Proceeding of MLMI 2005,</booktitle>
<pages>28--39</pages>
<location>Edinburgh, UK,</location>
<contexts>
<context position="2886" citStr="Carletta et al., 2005" startWordPosition="423" endWordPosition="426">the relationship between humanauthored summaries and their sources and fills the templates with the phrases to create summaries. The main contributions of this paper are: 1) The successful adaptation of a word graph algorithm to generate templates from humanauthored summaries; 2) The implementation of a novel template selection algorithm that effectively leverages the relationship between humanauthored summary sentences and their source transcripts; and 3) A comprehensive testing of our approach, comprising both automatic and manual evaluations. We instantiate our framework on the AMI corpus (Carletta et al., 2005) and compare our summaries with those created from a state-ofthe-art systems. The evaluation results demonstrate that our system successfully creates informative and readable summaries. 2. Related Work Several studies have been conducted on creating automatic abstractive meeting summarization systems. One of them includes the system proposed by Mehdad et al., (2013). Their approach first clusters human utterances into communities (Murray et al., 2012) and then builds an entailment graph over each of the latter in order to select the salient utterances. It then applies a semantic word graph alg</context>
</contexts>
<marker>Carletta, Ashby, Bourban, Flynn, Guillemot, Hain, Kadlec, Karaiskos, Kraaij, Kronenthal, Lathoud, Lincoln, Lisowska, McCowan, Post, Reidsma, Wellner, 2005</marker>
<rawString>J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, D. Reidsma, and P. Wellner. 2005. The AMI meeting corpus: A preannouncement. In Proceeding of MLMI 2005, Edinburgh, UK, pages 28–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordIet, An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8347" citStr="Fellbaum, 1998" startWordPosition="1249" endWordPosition="1250">summaries. This is achieved by utilizing meeting participant information provided in the corpus and parsing sentences with the Stanford Parser (Marneffe et al., 2006). The motivation behind this process is to collect sentences that are syntactically similar. We then identify all noun phrases in these sentences using the Illinois Chunker (Punyakanok and Roth, 2001). This chunker extracts all noun phrases as well as part of speech (POS) for all words. To add further information on each noun phrase, we label the right most nouns (the head nouns) in each phrase with their hypernyms using WordNet (Fellbaum, 1998). In WordNet, hypernyms are organized into hierarchies ranging from the most abstract to the most specific. For our work, we utilize the fourth most abstract hypernyms in light of the first goal discussed at the beginning of Section 3.1, i.e. not too general. For disambiguating the sense of the nouns, we simply select the sense that has the highest frequency in WordNet. At this stage, all noun phrases in sentences are tagged with their hypernyms defined in WordNet, such as “artifact.n.01”, and “act.n.02”, where n’s stands for nouns and the two digit numbers represent their sense numbers. We tr</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum 1998. WordIet, An Electronic Lexical Database. The MIT Press. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
</authors>
<title>Multi-sentence compression: finding shortest paths in word graphs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>322--330</pages>
<publisher>Association for Computational Linguistic</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10741" citStr="Filippova, 2010" startWordPosition="1642" endWordPosition="1643">h that connects the senses in the hypernym taxonomy. We then convert the graph into a similarity matrix and apply a Normalized Cuts method (Shi and Malik, 2000) to cluster the root verbs. Finally, all templates are organized into the groups created by their root verbs. Figure 3: A word graph generated from related templates and the highest scored path (shown in bold) 47 3.1.3 Template Fusion We further generalize the clustered templates by applying a word graph algorithm. The algorithm was originally proven to be effective in summarizing a cluster of related sentences (Boudin and Morin, 2013; Filippova, 2010; Mehdad et al., 2013). We extend it so that it can be applied to templates. Word Graph Construction In our system, a word graph is a directed graph with words or blanks serving as nodes and edges representing adjacency relations. Given a set of related templates in a group, the graph is constructed by first creating a start and end node, and then iteratively adding templates to it. When adding a new template, the algorithm first checks each word in the template to see if it can be mapped onto existing nodes in the graph. The word is mapped onto a node if the node consists of the same word and</context>
<context position="13600" citStr="Filippova (2010)" startWordPosition="2132" endWordPosition="2133">subordinate clauses; containing no verb; having two consecutive blanks; containing blanks which are not labeled by any hypernym; or whose length are shorter than three words. Note that these rules, which were defined based on close observation of the results obtained from our development set, greatly reduce the chance of selecting illstructured templates. Second, the remaining paths are reranked by 1) A normalized path weight and 2) A language model learned from hypernym-labeled human-authored summaries in our training data, each of which is described below. 1) Normalized Path Weight We adapt Filippova (2010)’s approach to compute the edge weight. The formula is shown as: ∑ where ei,j is an edge that connects the nodes i and j in a graph, freq(i) is the number of words and blanks in the templates that are mapped to node i and diff(p,i,j) is the distance between the offset positions of nodes i and j in path p. This weight is defined so that the paths that are informative and that contain salient (frequent) words are selected. To calculate a path score, W(p), all the edge weights on the path are summed and normalized by its length. 2) Language Model Although the goal is to create concise templates, </context>
</contexts>
<marker>Filippova, 2010</marker>
<rawString>Katja Filippova. 2010. Multi-sentence compression: finding shortest paths in word graphs. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 322– 330, Stroudsburg, PA, USA. Association for Computational Linguistic</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
</authors>
<title>Eric FoslerLussier and Hongyan Jing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL 2003). 562– 569.</booktitle>
<location>Sapporo, Japan.</location>
<marker>Galley, McKeown, 2003</marker>
<rawString>Michel Galley, Kathleen McKeown, Eric FoslerLussier and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL 2003). 562– 569. Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Ehud Reiter</author>
</authors>
<title>SimpleNLG: a Realisation Engine for Practical Applications. In</title>
<date>2009</date>
<booktitle>ENLG’09: Proceedings of the 12th European Workshop on Iatural Language Generation,</booktitle>
<pages>90--93</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4563" citStr="Gatt and Reiter, 2009" startWordPosition="672" endWordPosition="675">ome studies on creating abstract summaries of specific aspects of meetings such as decisions, actions and problems (Murray et al. 2010; Wang and Cardie, 2013). These summaries are called the Focused Meeting Summaries (Carenini et al., 2011). The system introduced by Murray et al. first classifies human utterances into specific aspects of meetings, e.g. decisions, problem, and action, and then maps them onto ontologies. It then selects the most informative subsets from these ontologies and finally generates abstractive summaries of them, utilizing a natural language generation tool, simpleNLG (Gatt and Reiter, 2009). Although their approach is essentially focused meeting summarization, after creating summaries of specific aspects, they aggregate them into one single summary covering the whole meeting. Wang and Cardie introduced a template-based focused abstractive meeting summarization system. Their system first clusters human-authored summary sentences and applies a MultipleSequence Alignment algorithm to them to generate templates. Then, given a meeting transcript to be summarized, it identifies a human utterance cluster describing a specific aspect and extracts all summary-worthy relation instances, i</context>
</contexts>
<marker>Gatt, Reiter, 2009</marker>
<rawString>Albert Gatt and Ehud Reiter. 2009. SimpleNLG: a Realisation Engine for Practical Applications. In ENLG’09: Proceedings of the 12th European Workshop on Iatural Language Generation, pages 90–93, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Kondadadi</author>
<author>Blake Howald</author>
<author>Frank Schilder</author>
</authors>
<title>A Statistical NLG Framework for Aggregated Planning and Realization.</title>
<date>2013</date>
<booktitle>In Proceeding of the Annual Conferene for the Association of Computational Linguistic (ACL</booktitle>
<marker>Kondadadi, Howald, Schilder, 2013</marker>
<rawString>Ravi Kondadadi, Blake Howald and Frank Schilder. 2013. A Statistical NLG Framework for Aggregated Planning and Realization. In Proceeding of the Annual Conferene for the Association of Computational Linguistic (ACL 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC&apos;06).</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC&apos;06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Giuseppe Carenini</author>
<author>Frank Tompa</author>
</authors>
<title>Abstractive Meeting Summarization with Entailment and Fusion.</title>
<date>2013</date>
<booktitle>In Proceedings of the 14th European Iatural Language Generation (EILG -SIGGEI 2013),</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="1568" citStr="Mehdad et al. 2013" startWordPosition="223" endWordPosition="227">utomatic meeting summarization has been attracting peoples’ attention as it can save a great deal of their time and increase their productivity. The most common approaches to automatic meeting summarization have been extractive. Since extractive approaches do not require natural language generation techniques, they are arguably simpler to apply and have been extensively investigated. However, a user study conducted by Murray et al. (2010) indicates that users prefer abstractive summaries to extractive ones. Thereafter, more attention has been paid to abstractive meeting summarization systems (Mehdad et al. 2013; Murray et al. 2010; Wang and Cardie 2013). However, the approaches introduced in previous studies create summaries by either heavily relying on annotated data or by fusing human utterances which may contain grammatical mistakes. In this paper, we address these issues by introducing a novel summarization approach that can create readable summaries with less need for annotated data. Our system first acquires templates from human-authored summaries using a clustering and multi-sentence fusion algorithm. It then takes a meeting transcript to be summarized, segments the transcript based on topics</context>
<context position="3254" citStr="Mehdad et al., (2013)" startWordPosition="478" endWordPosition="481">e relationship between humanauthored summary sentences and their source transcripts; and 3) A comprehensive testing of our approach, comprising both automatic and manual evaluations. We instantiate our framework on the AMI corpus (Carletta et al., 2005) and compare our summaries with those created from a state-ofthe-art systems. The evaluation results demonstrate that our system successfully creates informative and readable summaries. 2. Related Work Several studies have been conducted on creating automatic abstractive meeting summarization systems. One of them includes the system proposed by Mehdad et al., (2013). Their approach first clusters human utterances into communities (Murray et al., 2012) and then builds an entailment graph over each of the latter in order to select the salient utterances. It then applies a semantic word graph algorithm to them and creates abstractive summaries. Their results show some improvement in creating informative summaries. 45 Proceedings of the 8th International Natural Language Generation Conference, pages 45–53, Philadelphia, Pennsylvania, 19-21 June 2014. c�2014 Association for Computational Linguistics However, since they create these summaries by merging human </context>
<context position="10763" citStr="Mehdad et al., 2013" startWordPosition="1644" endWordPosition="1647">he senses in the hypernym taxonomy. We then convert the graph into a similarity matrix and apply a Normalized Cuts method (Shi and Malik, 2000) to cluster the root verbs. Finally, all templates are organized into the groups created by their root verbs. Figure 3: A word graph generated from related templates and the highest scored path (shown in bold) 47 3.1.3 Template Fusion We further generalize the clustered templates by applying a word graph algorithm. The algorithm was originally proven to be effective in summarizing a cluster of related sentences (Boudin and Morin, 2013; Filippova, 2010; Mehdad et al., 2013). We extend it so that it can be applied to templates. Word Graph Construction In our system, a word graph is a directed graph with words or blanks serving as nodes and edges representing adjacency relations. Given a set of related templates in a group, the graph is constructed by first creating a start and end node, and then iteratively adding templates to it. When adding a new template, the algorithm first checks each word in the template to see if it can be mapped onto existing nodes in the graph. The word is mapped onto a node if the node consists of the same word and the same POS tag, and</context>
<context position="23623" citStr="Mehdad et al. (2013)" startWordPosition="3796" endWordPosition="3799">d meeting records and their human-authored summaries in the AMI corpus. The corpus contains 139 meeting records in which groups of four people play different roles in a fictitious team. We reserved 20 meetings for development and implemented a threefold cross-validation using the remaining data. 4.2 Automatic Evaluation We report the F1-measure of ROUGE-1, ROUGE-2 and ROUGE-SU4 (Lin and Hovy, 2003) to assess the performance of our system. The scores of automatically generated summaries are calculated by comparing them with humanauthored ones. For our baselines, we use the system introduced by Mehdad et al. (2013) (FUSION), which creates abstractive summaries from extracted sentences and was proven to be effective in creating abstractive meeting summaries; and TextRank (Mihalcea and Tarau, 2004), a graph based sentence ranker that is suitable for creating extractive summaries. Our system can create summaries of any length by adjusting the number of segments to be created by LCSeg. Thus, we create summaries of three different lengths (10, 15, and 20 topic segments) with the average number of words being 100, 137, and 173, respectively. These numbers generally corresponds to humanauthored summary length </context>
</contexts>
<marker>Mehdad, Carenini, Tompa, 2013</marker>
<rawString>Yashar Mehdad, Giuseppe Carenini, Frank Tompa. 2013. Abstractive Meeting Summarization with Entailment and Fusion. In Proceedings of the 14th European Iatural Language Generation (EILG -SIGGEI 2013), Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rata Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>TextRank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Iatural Language Processing,</booktitle>
<contexts>
<context position="23808" citStr="Mihalcea and Tarau, 2004" startWordPosition="3823" endWordPosition="3826">team. We reserved 20 meetings for development and implemented a threefold cross-validation using the remaining data. 4.2 Automatic Evaluation We report the F1-measure of ROUGE-1, ROUGE-2 and ROUGE-SU4 (Lin and Hovy, 2003) to assess the performance of our system. The scores of automatically generated summaries are calculated by comparing them with humanauthored ones. For our baselines, we use the system introduced by Mehdad et al. (2013) (FUSION), which creates abstractive summaries from extracted sentences and was proven to be effective in creating abstractive meeting summaries; and TextRank (Mihalcea and Tarau, 2004), a graph based sentence ranker that is suitable for creating extractive summaries. Our system can create summaries of any length by adjusting the number of segments to be created by LCSeg. Thus, we create summaries of three different lengths (10, 15, and 20 topic segments) with the average number of words being 100, 137, and 173, respectively. These numbers generally corresponds to humanauthored summary length in the corpus which varies from 82 to 200 words. Table 2 shows the results of our system in comparison with those of the two baselines. The results show that our model significantly out</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rata Mihalcea and Paul Tarau 2004. TextRank: Bringing order into texts. In Proceedings of the 2004 Conference on Empirical Methods in Iatural Language Processing, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
</authors>
<title>Generating and validating abstracts of meeting conversations: a user study.</title>
<date>2010</date>
<booktitle>In IILG</booktitle>
<contexts>
<context position="1392" citStr="Murray et al. (2010)" startWordPosition="197" endWordPosition="200"> readability and informativeness. 1. Introduction People spend a vast amount of time in meetings and these meetings play a prominent role in their lives. Consequently, study of automatic meeting summarization has been attracting peoples’ attention as it can save a great deal of their time and increase their productivity. The most common approaches to automatic meeting summarization have been extractive. Since extractive approaches do not require natural language generation techniques, they are arguably simpler to apply and have been extensively investigated. However, a user study conducted by Murray et al. (2010) indicates that users prefer abstractive summaries to extractive ones. Thereafter, more attention has been paid to abstractive meeting summarization systems (Mehdad et al. 2013; Murray et al. 2010; Wang and Cardie 2013). However, the approaches introduced in previous studies create summaries by either heavily relying on annotated data or by fusing human utterances which may contain grammatical mistakes. In this paper, we address these issues by introducing a novel summarization approach that can create readable summaries with less need for annotated data. Our system first acquires templates fr</context>
<context position="4075" citStr="Murray et al. 2010" startWordPosition="597" endWordPosition="600">ies a semantic word graph algorithm to them and creates abstractive summaries. Their results show some improvement in creating informative summaries. 45 Proceedings of the 8th International Natural Language Generation Conference, pages 45–53, Philadelphia, Pennsylvania, 19-21 June 2014. c�2014 Association for Computational Linguistics However, since they create these summaries by merging human utterances, their summaries are still partially extractive. Recently, there have been some studies on creating abstract summaries of specific aspects of meetings such as decisions, actions and problems (Murray et al. 2010; Wang and Cardie, 2013). These summaries are called the Focused Meeting Summaries (Carenini et al., 2011). The system introduced by Murray et al. first classifies human utterances into specific aspects of meetings, e.g. decisions, problem, and action, and then maps them onto ontologies. It then selects the most informative subsets from these ontologies and finally generates abstractive summaries of them, utilizing a natural language generation tool, simpleNLG (Gatt and Reiter, 2009). Although their approach is essentially focused meeting summarization, after creating summaries of specific asp</context>
<context position="5764" citStr="Murray et al. (2010)" startWordPosition="847" endWordPosition="850">lation instances, i.e. indicator-argument pairs, from it. Finally, the templates are filled with these relation instances and ranked accordingly, to generate summaries of a specific aspect of the meeting. Although the two approaches above are both successful in creating readable summaries, they rely on much annotated information, such as dialog act and sentiment types, and also require the accurate classification of human utterances that contain much noise and much ill-structured grammar. Our approach is inspired by the works introduced here but improves on their shortcomings. Unlike those of Murray et al. (2010) and Wang and Cardie (2013), our system relies less on annotated training data and does not require a classifier. In addition, our evaluation indicates that our system can create summaries of the entire conversations that are more informative and readable than those of Mehdad et al.(2013). 3. Framework In order for summaries to be readable and informative, they should be grammatically correct and contain important information in meetings. To this end, we have created our framework consisting of the following two components: 1) An off-line template generation module, which generalizes collected</context>
</contexts>
<marker>Murray, Carenini, Ng, 2010</marker>
<rawString>Gabriel Murray, Giuseppe Carenini, and Raymond T. Ng. 2010. Generating and validating abstracts of meeting conversations: a user study. In IILG 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
</authors>
<title>Using the Omega Index for Evaluating Abstractive Community Detection,</title>
<date>2012</date>
<booktitle>IAACL 2012, Workshop on Evaluation Metrics and System Comparison for Automatic Summarization,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="3341" citStr="Murray et al., 2012" startWordPosition="490" endWordPosition="493"> 3) A comprehensive testing of our approach, comprising both automatic and manual evaluations. We instantiate our framework on the AMI corpus (Carletta et al., 2005) and compare our summaries with those created from a state-ofthe-art systems. The evaluation results demonstrate that our system successfully creates informative and readable summaries. 2. Related Work Several studies have been conducted on creating automatic abstractive meeting summarization systems. One of them includes the system proposed by Mehdad et al., (2013). Their approach first clusters human utterances into communities (Murray et al., 2012) and then builds an entailment graph over each of the latter in order to select the salient utterances. It then applies a semantic word graph algorithm to them and creates abstractive summaries. Their results show some improvement in creating informative summaries. 45 Proceedings of the 8th International Natural Language Generation Conference, pages 45–53, Philadelphia, Pennsylvania, 19-21 June 2014. c�2014 Association for Computational Linguistics However, since they create these summaries by merging human utterances, their summaries are still partially extractive. Recently, there have been s</context>
</contexts>
<marker>Murray, Carenini, Ng, 2012</marker>
<rawString>Gabriel Murray, Giuseppe Carenini and Raymond Ng. 2012. Using the Omega Index for Evaluating Abstractive Community Detection, IAACL 2012, Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
</authors>
<date>2001</date>
<booktitle>The Use of Classifiers in Sequential Inference. IIPS</booktitle>
<pages>995--1001</pages>
<contexts>
<context position="8098" citStr="Punyakanok and Roth, 2001" startWordPosition="1203" endWordPosition="1206"> 2) Clustering; and 3) Template fusion. 3.1.1 Hypernym Labeling Templates are derived from human-authored meeting summaries in the training data. We first collect sentences whose subjects are meeting participant(s) and that contain active root verbs, from the summaries. This is achieved by utilizing meeting participant information provided in the corpus and parsing sentences with the Stanford Parser (Marneffe et al., 2006). The motivation behind this process is to collect sentences that are syntactically similar. We then identify all noun phrases in these sentences using the Illinois Chunker (Punyakanok and Roth, 2001). This chunker extracts all noun phrases as well as part of speech (POS) for all words. To add further information on each noun phrase, we label the right most nouns (the head nouns) in each phrase with their hypernyms using WordNet (Fellbaum, 1998). In WordNet, hypernyms are organized into hierarchies ranging from the most abstract to the most specific. For our work, we utilize the fourth most abstract hypernyms in light of the first goal discussed at the beginning of Section 3.1, i.e. not too general. For disambiguating the sense of the nouns, we simply select the sense that has the highest </context>
</contexts>
<marker>Punyakanok, Roth, 2001</marker>
<rawString>Vasin Punyakanok and Dan Roth. 2001. The Use of Classifiers in Sequential Inference. IIPS (2001) pp. 995-1001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianbo Shi</author>
<author>Jitendra Malik</author>
</authors>
<title>Normalized Cuts &amp; Image Segmentation.</title>
<date>2000</date>
<journal>IEEE Trans. of PAMI,</journal>
<contexts>
<context position="10286" citStr="Shi and Malik, 2000" startWordPosition="1567" endWordPosition="1570">appear in summaries are the most informative factors in describing meetings. Therefore, after extracting root verbs in summary sentences, we create fully connected graphs where each node represents the root verbs and each edge represents a score denoting how similar the two word senses are. To measure the similarity of two verbs, we first identify the verb senses based on their frequency in WordNet and compute the similarity score based on the shortest path that connects the senses in the hypernym taxonomy. We then convert the graph into a similarity matrix and apply a Normalized Cuts method (Shi and Malik, 2000) to cluster the root verbs. Finally, all templates are organized into the groups created by their root verbs. Figure 3: A word graph generated from related templates and the highest scored path (shown in bold) 47 3.1.3 Template Fusion We further generalize the clustered templates by applying a word graph algorithm. The algorithm was originally proven to be effective in summarizing a cluster of related sentences (Boudin and Morin, 2013; Filippova, 2010; Mehdad et al., 2013). We extend it so that it can be applied to templates. Word Graph Construction In our system, a word graph is a directed gr</context>
</contexts>
<marker>Shi, Malik, 2000</marker>
<rawString>Jianbo Shi and Jitendra Malik. 2000. Normalized Cuts &amp; Image Segmentation. IEEE Trans. of PAMI, Aug 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David C Uthus</author>
<author>David W Aha</author>
</authors>
<title>Plans toward automated chat summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, WASDGML’11,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Uthus, Aha, 2011</marker>
<rawString>David C. Uthus and David W. Aha. 2011. Plans toward automated chat summarization. In Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, WASDGML’11, pages 1-7, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu Wang</author>
<author>Claire Cardie</author>
</authors>
<title>DomainIndependent Abstract Generation for Focused Meeting Summarization.</title>
<date>2013</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="1611" citStr="Wang and Cardie 2013" startWordPosition="232" endWordPosition="235">attracting peoples’ attention as it can save a great deal of their time and increase their productivity. The most common approaches to automatic meeting summarization have been extractive. Since extractive approaches do not require natural language generation techniques, they are arguably simpler to apply and have been extensively investigated. However, a user study conducted by Murray et al. (2010) indicates that users prefer abstractive summaries to extractive ones. Thereafter, more attention has been paid to abstractive meeting summarization systems (Mehdad et al. 2013; Murray et al. 2010; Wang and Cardie 2013). However, the approaches introduced in previous studies create summaries by either heavily relying on annotated data or by fusing human utterances which may contain grammatical mistakes. In this paper, we address these issues by introducing a novel summarization approach that can create readable summaries with less need for annotated data. Our system first acquires templates from human-authored summaries using a clustering and multi-sentence fusion algorithm. It then takes a meeting transcript to be summarized, segments the transcript based on topics, and extracts important phrases from it. F</context>
<context position="4099" citStr="Wang and Cardie, 2013" startWordPosition="601" endWordPosition="604">graph algorithm to them and creates abstractive summaries. Their results show some improvement in creating informative summaries. 45 Proceedings of the 8th International Natural Language Generation Conference, pages 45–53, Philadelphia, Pennsylvania, 19-21 June 2014. c�2014 Association for Computational Linguistics However, since they create these summaries by merging human utterances, their summaries are still partially extractive. Recently, there have been some studies on creating abstract summaries of specific aspects of meetings such as decisions, actions and problems (Murray et al. 2010; Wang and Cardie, 2013). These summaries are called the Focused Meeting Summaries (Carenini et al., 2011). The system introduced by Murray et al. first classifies human utterances into specific aspects of meetings, e.g. decisions, problem, and action, and then maps them onto ontologies. It then selects the most informative subsets from these ontologies and finally generates abstractive summaries of them, utilizing a natural language generation tool, simpleNLG (Gatt and Reiter, 2009). Although their approach is essentially focused meeting summarization, after creating summaries of specific aspects, they aggregate the</context>
<context position="5791" citStr="Wang and Cardie (2013)" startWordPosition="852" endWordPosition="855">dicator-argument pairs, from it. Finally, the templates are filled with these relation instances and ranked accordingly, to generate summaries of a specific aspect of the meeting. Although the two approaches above are both successful in creating readable summaries, they rely on much annotated information, such as dialog act and sentiment types, and also require the accurate classification of human utterances that contain much noise and much ill-structured grammar. Our approach is inspired by the works introduced here but improves on their shortcomings. Unlike those of Murray et al. (2010) and Wang and Cardie (2013), our system relies less on annotated training data and does not require a classifier. In addition, our evaluation indicates that our system can create summaries of the entire conversations that are more informative and readable than those of Mehdad et al.(2013). 3. Framework In order for summaries to be readable and informative, they should be grammatically correct and contain important information in meetings. To this end, we have created our framework consisting of the following two components: 1) An off-line template generation module, which generalizes collected human-authored summaries a</context>
</contexts>
<marker>Wang, Cardie, 2013</marker>
<rawString>Lu Wang and Claire Cardie. 2013. DomainIndependent Abstract Generation for Focused Meeting Summarization. In ACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Eduard Hovy</author>
</authors>
<title>Digesting virtual “geek” culture: The summarization of technical internet relay chats.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>298--305</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<marker>Zhou, Hovy, 2005</marker>
<rawString>Liang Zhou and Eduard Hovy. 2005. Digesting virtual “geek” culture: The summarization of technical internet relay chats. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 298-305, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
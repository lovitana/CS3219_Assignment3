<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005187">
<title confidence="0.985061">
Optimizing annotation efforts to build reliable annotated corpora
for training statistical models
</title>
<author confidence="0.938832">
Cyril Grouin1 Thomas Lavergne1,2 Aur´elie N´ev´eol1
</author>
<note confidence="0.426473">
1 LIMSI–CNRS, 91405 Orsay, France 2 Universit´e Paris Sud 11, 91400 Orsay, France
</note>
<email confidence="0.910441">
firstname.lastname@limsi.fr
</email>
<sectionHeader confidence="0.991217" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999473833333333">
Creating high-quality manual annotations on text corpus is time-consuming and often requires the
work of experts. In order to explore methods for optimizing annotation efforts, we study three key
time burdens of the annotation process: (i) multiple annotations, (ii) consensus annotations, and
(iii) careful annotations. Through a series of experiments using a corpus of clinical documents
annotated for personally identifiable information written in French, we address each of these
aspects and draw conclusions on how to make the most of an annotation effort.
</bodyText>
<sectionHeader confidence="0.999266" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.962297033333333">
Statistical and Machine Learning methods have become prevalent in Natural Language Processing (NLP)
over the past decades. These methods sucessfully address NLP tasks such as part-of-speech tagging or
named entity recognition by relying on large annotated text corpora. As a result, developping high-
quality annotated corpora representing natural language phenomena that can be processed by statistical
tools has become a major challenge for the scientific community. Several aspects of the annotation task
have been studied in order to ensure corpus quality and affordable cost. Inter-annotator agreement (IAA)
has been used as an indicator of annotation quality. Early work showed that the use of automatic pre-
annotation tools improved annotation consistency (Marcus et al., 1993). Careful and detailed annotation
guideline definition was also shown to have positive impact on IAA (Wilbur et al., 2006).
Efforts have investigated methods to reduce the human workload while annotating corpora. In par-
ticular, active learning (Settles et al., 2008) sucessfully selects portions of corpora that yield the most
benefit when annotated. Alternatively, (Dligach and Palmer, 2011) investigated the need for double an-
notation and found that double annotation could be limited to carefully selected portions of a corpus.
They produced an algorithm that automatically selects portions of a corpus for double annotation. Their
approach allowed to reduce the amount of work by limiting the portion of doubly annotated data and
maintained annotation quality to the standard of a fully doubly annotated corpus. The use of automatic
pre-annotations was shown to increase annotation consistency and result in producing quality annotation
with a time gain over annotating raw data (Fort and Sagot, 2010; N´ev´eol et al., 2011; Rosset et al., 2013).
With the increasing use of crowdsourcing for obtaining annotated data, (Fort et al., 2011) show that there
are ethic aspects to consider in addition to technical and monetary cost when using a microworking plat-
form for annotation. While selecting the adequate methods for computing IAA is important (Artstein
and Poesio, 2008) for interpreting the IAA for a particular task, annotator disagreement is inherent to
all annotation tasks. To address this situation (Rzhetsky et al., 2009) designed a method to estimate
annotation confidence based on annotator modeling. Overall, past work shows that creating high-quality
manual annotations is time-consuming and often requires the work of experts. The time burden is dis-
tributed between the sheer creation of the annotations, the act of producing multiple annotations for the
same data and the subsequent analysis of multiple annotations to resolve conflicts, viz. the creation of a
consensus. Research has addressed methods for reducing the time burden associated to these annotation
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
</bodyText>
<page confidence="0.962073">
54
</page>
<note confidence="0.343586">
LAW VIII - The 8th Linguistic Annotation Workshop, pages 54–58,
Dublin, Ireland, August 23-24 2014.
</note>
<bodyText confidence="0.998762090909091">
activities (for example, adequate annotation tools such as automatic pre-annotations can reduce the time
burden of annotation creation) with the final goal of producing the highest quality of annotations.
In contrast, our hypothesis in this work is that annotations are being developed for the purpose of train-
ing a machine learning model. Therefore, our experiments consist in training a named entity recognizer
on a training set comprising annotations of varying quality to study the impact of training annotation
quality on model performance. In order to explore methods for optimizing annotation efforts for the de-
velopment of training corpora, we revisit the three key time burdens of the annotation process on textual
corpora: (i) careful annotations, (ii) multiple annotations, and (iii) consensus annotations. Through a
series of experiments using a corpus of French clinical documents annotated for personally identifiable
information (PHI), we address each of these aspects and draw conclusions on how to make the most of
an annotation effort.
</bodyText>
<sectionHeader confidence="0.986488" genericHeader="introduction">
2 Material and methods
</sectionHeader>
<subsectionHeader confidence="0.979872">
2.1 Annotated corpus
</subsectionHeader>
<bodyText confidence="0.999997363636364">
Experiments were conducted with a corpus of clinical documents in French annotated for 10 categories of
PHI. The distribution of the categories over the corpus varies with some categories being more prevalent
than others. In addition, the performance of entity recognition for each type of PHI also varies (Grouin
and N´ev´eol, 2014). The datasets were split to obtain a training corpus (200 documents) and a test
corpus (100 documents). For all documents in the training corpus, three types of human annotations
are available: annotations performed independently by two human annotators and consensus annotations
obtained after adjudication to resolve conflicts between the two annotators. Inter-annotator agreement
on the training corpus was above 85% F-measure, which is considered high (Artstein and Poesio, 2008).
The distribution of annotations over all PHI categories on both corpora (train/dev) is: address
(188/100), zip code (197/97), date (1025/498), e-mail (119/57), hospital (448/208), identifier (135/76),
last name (1855/855), first name (1568/724), telephone (802/386) and city (450/217).
</bodyText>
<subsectionHeader confidence="0.998904">
2.2 Automatic Annotation Methods
</subsectionHeader>
<bodyText confidence="0.99589475">
We directly applied the MEDINA rule-based de-identification tool (Grouin, 2013) to obtain baseline
automatic annotations. We used the CRF toolkit Wapiti (Lavergne et al., 2010) to train a series of models
on the various sets of annotations available for the training corpus.
Features set For each CRF experiment, we used the following set of features with a l1 regularization:
</bodyText>
<listItem confidence="0.840118222222222">
• Lexical features: unigram and bigram of tokens;
• Morphological features: (i) the token case (all in upper/lower case, combination of both), (ii) the
token is a digit, (iii) the token is a punctuation mark, (iv) the token belongs to a specific list (first
name, last name, city), (v) the token was not identified in a dictionary of inflected forms, (vi) the
token is a trigger word for specific categories (hospital, last name);
• Syntactic features: (i) the part-of-speech (POS) tag of the token, as provided by the Tree Tagger
tool (Schmid, 1994), (ii) the syntactic chunk the token belongs to, from a home made chunker
based upon the previouses POS tags;
• External features: (i) we created 320 classes of tokens using Liang’s implementation (Liang, 2005)
of the Brown clustering algorithm (Brown et al., 1992), (ii) the position of the token within the
document (begining, middle, end).
Design of experiments The models were built to assess three annotation time-saving strategies:
1. Careful annotation: (i) AR=based on automatic annotations from the rule-based system,
(ii) ARnH2=intersection of automatic annotations from the rule-based system with annotations
from annotator 2. This model captures a situation where the human annotator would quickly revise
the automatic annotations by removing errors: some annotations would be missing (average recall),
but the annotations present in the set would be correct (very high precision), (iii) ARH2=automatic
annotations from the rule-based system, with replacement of the three most difficult categories by
</listItem>
<page confidence="0.994002">
55
</page>
<bodyText confidence="0.995217666666667">
annotations from annotator 2. This model captures a situation where the human annotator would
focus on revising targeted categories, and (iv) ARHC=automatic annotations from the rule-based
system, with replacement of the three most difficult categories by consensus annotations;
</bodyText>
<listItem confidence="0.9939896">
2. Double annotation: (i) H1=annotations from annotator 1, (ii) H2=annotations from annotator 2,
(iii) H12=first half of the annotations from annotator 1, second half from annotator 2, and
(iv) H21=first half of the annotations from annotator 2, second half from annotator 1;
3. Consensus annotation: (i) H1UH2=all annotations from annotator 1 and 2 (concatenation without
adjudication), and (ii) HC=consensus annotations (after adjudication between annotator 1 and 2).
</listItem>
<sectionHeader confidence="0.99923" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.74756225">
Table 1 presents an overview of the global performance of each annotation run (H12 and H21 achieved
similar results) across all PHI categories in terms of precision, recall and F1-measure (Manning and
Sch¨utze, 2000). Table 2 presents the detailed performance of each annotation run for individual PHI
categories in terms of F-measure.
</bodyText>
<table confidence="0.999944">
Baseline AR ARnH2 ARH2* ARHC H1* H12 H1UH2 H2 HC
Precision .820 .868 .920 .942 .943 .959 .962 .969 .974 .974
Recall .806 .796 .763 .854 .854 .927 .934 .935 .936 .942
F-measure .813 .830 .834 .896 .896 .943 .948 .951 .955 .958
</table>
<tableCaption confidence="0.910769">
Table 1: Overall performance for all automatic PHI detection. A star indicates statistically significant
difference in F-measure over the previous model (Wilcoxon test, p&lt;0.05)
</tableCaption>
<table confidence="0.999961818181818">
Category Baseline AR ARnH2 ARH2 ARHC H1 H12 H1UH2 H2 HC
Address .648 .560 .000 .800 .800 .716 .744 .789 .795 .791
Zip code .950 .958 .947 .964 .958 .974 .984 .974 .984 .990
Date .958 .968 .962 .963 .967 .965 .963 .963 .959 .970
E-mail .937 .927 .927 .927 .927 1.000 1.000 1.000 1.000 1.000
Hospital .201 .248 .039 .856 .868 .789 .809 .856 .861 .867
Identifier .000 .000 .000 .762 .797 .870 .892 .823 .836 .876
Last name .816 .810 .834 .832 .828 .953 .957 .954 .961 .963
First name .849 .858 .900 .901 .902 .960 .956 .961 .965 .960
Telephone 1.000 .980 .978 .983 .980 .987 .994 .999 .999 1.000
City .869 .874 .883 .887 .887 .948 .972 .962 .965 .972
</table>
<tableCaption confidence="0.998945">
Table 2: Performance per PHI category (F-measure)
</tableCaption>
<sectionHeader confidence="0.999576" genericHeader="method">
4 Discussion
</sectionHeader>
<subsectionHeader confidence="0.999875">
4.1 Model performance
</subsectionHeader>
<bodyText confidence="0.999979272727273">
Overall, the task of automatic PHI recognition has been well studied and the rule-based tool provides a
strong baseline with .813 F-measure on the test set. Table 1 shows that there are three different types
of models, in terms of performance: the lower-performing category corresponds to models with no hu-
man input. The next category corresponds to models with some human input, and the higher-performing
models correspond to models with the most human input. This reflects the expectation that model per-
formance increases with training corpus quality. However, it also shows that, within the two categories
that include human input, there is no statistical difference in model performance with respect to the type
of human input. We observed that the model trained on annotations from the H2 human annotator per-
formed better (F=0.955) than the model trained on annotations from the H1 annotator (F=0.943). This
observation reflects the agreement of the annotators with consensus annotations, where H2 had higher
agreement than H1 (Grouin and N´ev´eol, 2014). This is also true at the category level: H2 achieved
</bodyText>
<page confidence="0.992869">
56
</page>
<bodyText confidence="0.999462">
higher agreement with the consensus compared to H1 on categories “address” (F=0.985&gt;0.767) and
“hospital” (F=0.947&gt;0.806) but H2 had lower agreement with the consensus on the category “identifier”
(F=0.840&lt;0.933).
</bodyText>
<subsectionHeader confidence="0.933979">
4.2 Error Analysis
</subsectionHeader>
<bodyText confidence="0.99997152631579">
The performance of CRF models depends on the size of the training corpus and the level of diversity of
the mentions. Error analysis on our test data shows that a few specific mentions are not tagged in the test
corpus, even though they occur in the training corpus. For example, some hospital names occur in the
clinical narratives either as acronyms or as full forms (e.g. “GWH” for “George Washington Hospital”
in transfer patient from GWH). The acronyms are overall much less prevalent than the full forms and
also happen to be difficult to identify for human annotators (depending on the context, a given acronym
could refer to either a medical procedure, a physician or a hospital). We observed that the only hospital
acronym present in the test corpus was not annotated by any of the CRF models. Nevertheless, only five
occurrences of this acronym were found in the training corpus which is not enough for the CRF to learn.
Other errors occur in recognizing sequences of doctors’ names that appear without separators in sig-
natures lines at the end of documents (e.g. “Jane BROWN John DOE Mary SMITH”). In our test set
we observed that models trained on automatic annotations correctly predicted the beginning of such se-
quences and then produced erroneous predictions for the rest of the sequence (models AR, AR∩H2,
ARHC and ARH2). In contrast, models built on human annotations produced correct predictions on the
entire sequence (models H1, H12, H1∪H2, H2 and HC). Similarly, for last names containing a nobiliary
particle, the models trained on automatic annotations only identified part of the last name as a PHI. We
also observed that spelling errors (e.g. “Jhn DOE”) only resulted in correct predictions from the mod-
els trained on the human annotations. We did not find cases where the models built on the automatic
annotations performed better than the models built on the human annotations.
</bodyText>
<subsectionHeader confidence="0.999965">
4.3 Annotation strategy
</subsectionHeader>
<bodyText confidence="0.999683857142857">
Table 1 indicates that for the purpose of training a machine learning entity recognizer, all types of human
input are equivalent. In practice, this means that double annotations or consensus annotations are not
necessary. The high inter-annotator agreement on our dataset may be a contributing factor for this finding.
Indeed, (Esuli et al., 2013) found that with low inter-annotator agreement, models are biased towards
the annotation style of the annotator who produced the training data. Therefore, we believe that inter-
annotator should be established on a small dataset before annotators work independently. Table 2 shows
that using human annotations for selected categories results in strong improvement of the performance
over these categories (“address”, “hospital” and “identifier” categories in ARHC and ARH2 vs. AR) with
little impact on the performance of the model on other categories. Therefore, careful human annotations
are not necessarily needed for the entire corpus. Targeting “hard” categories for human annotations can
be a good time-saving strategy. While the difference between the models using some human input vs.
all human input is statistically significant, the performance gain is lower than between models without
human input and some human input. Using data with partial human input for training statistical models
can cut annotation cost.
</bodyText>
<sectionHeader confidence="0.974182" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999986333333333">
Herein we have shown that full double annotation of a corpus is not necessary for the purpose of training a
competitive CRF-based model. Our results suggest that a three-step annotation strategy can optimize the
annotation effort: (i) double annotate a small subset of the corpus to ensure human annotators understand
the guidelines; (ii) have annotators work independently on different sections of the corpus to obtain wide
coverage; and (iii) train a machine-learning based model on the human annotations and apply this model
on a new dataset.
In future work, we plan to re-iterate these experiments on a different type of entity recognition task
where inter-annotator agreement may be more difficult to achieve, and may vary more between categories
in order to investigate the influence of inter-annotator-agreement on our conclusions.
</bodyText>
<page confidence="0.998055">
57
</page>
<sectionHeader confidence="0.996533" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998969666666667">
This work was supported by the French National Agency for Research under grant CABeRneT1 ANR-
13-JS02-0009-01 and by the French National Agency for Medicines and Health Products Safety under
grant Vigi4MED2 ANSM-2013-S-060.
</bodyText>
<sectionHeader confidence="0.989586" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987975615384615">
Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational
Linguistics, 34(4):555–596.
Peter F Brown, Vincent J Della Pietra, Peter V de Souza, Jenifer C Lai, and Robert L Mercer. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18(4):467–79.
Dmitriy Dligach and Martha Palmer. 2011. Reducing the need for double annotation. In Proc of Linguistic
Annotation Workshop (LAW), pages 65–73. Association for Computational Linguistics.
Andrea Esuli, Diego Marcheggiani, and Fabrizio Sebastiani. 2013. An enhanced CRFs-based system for informa-
tion extraction from radiology reports. J Biomed Inform, 46(3):425–35, Jun.
Kar¨en Fort and Benoit Sagot. 2010. Influence of pre-annotation on POS-tagged corpus development. In Proc of
Linguistic Annotation Workshop (LAW), pages 56–63. Association for Computational Linguistics.
Kar¨en Fort, Gilles Adda, and Kevin Bretonnel Cohen. 2011. Amazon Mechanical Turk: Gold Mine or Coal Mine?
Computational Linguistics, pages 413–420.
Cyril Grouin and Aur´elie N´ev´eol. 2014. De-identification of clinical notes in french: towards a protocol for
reference corpus development. J Biomed Inform.
Cyril Grouin. 2013. Anonymisation de documents cliniques : performances et limites des m´ethodes symboliques
et par apprentissage statistique. Ph.D. thesis, University Pierre et Marie Curie, Paris, France.
Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon. 2010. Practical very large scale CRFs. In Proceedings the
48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504–513. Association for
Computational Linguistics.
Percy Liang. 2005. Semi-supervised learning for natural language. Master’s thesis, MIT.
Chiristopher D. Manning and Hinrich Sch¨utze. 2000. Foundations of Statistical Natural Language Processing.
MIT Press, Cambridge, Massachusetts.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of
English: the Penn TreeBank. Computational Linguistics, 19(2):313–330.
Aur´elie N´ev´eol, Rezarta Islamaj Do˘gan, and Zhiyong Lu. 2011. Semi-automatic semantic annotation of PubMed
queries: a study on quality, efficiency, satisfaction. J Biomed Inform, 44(2):310–8.
Sophie Rosset, Cyril Grouin, Thomas Lavergne, Mohamed Ben Jannet, J´er´emy Leixa, Olivier Galibert, and Pierre
Zweigenbaum. 2013. Automatic named entity pre-annotation for out-of-domain human annotation. In Proc of
Linguistic Annotation Workshop (LAW), pages 168–177. Association for Computational Linguistics.
Andrey Rzhetsky, Hagit Shatkay, and W John Wilbur. 2009. How to get the most out of your curation effort. PLoS
ComputBiol, 5(5):e1000391.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proc of International Confer-
ence on New Methods in Language.
Burr Settles, Mark Craven, and Lewis Friedland. 2008. Active learning with real annotation costs. In Proc of the
NIPS Workshop on Cost-Sensitive Learning.
W John Wilbur, Andrey Rzhetsky, and Hagit Shatkay. 2006. New directions in biomedical text annotation:
definitions, guidelines and corpus construction. BMC Bioinformatics, 25(7):356.
1CABeRneT: Compr´ehension Automatique de Textes Biom´edicaux pour la Recherche Translationnelle
2Vigi4MED: Vigilance dans les forums sur les M´edicaments
</reference>
<page confidence="0.999032">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.745163">
<title confidence="0.999306">Optimizing annotation efforts to build reliable annotated for training statistical models</title>
<author confidence="0.998801">Thomas Aur´elie</author>
<address confidence="0.752952">1LIMSI–CNRS, 91405 Orsay, France 2Universit´e Paris Sud 11, 91400 Orsay,</address>
<email confidence="0.998971">firstname.lastname@limsi.fr</email>
<abstract confidence="0.998827571428571">Creating high-quality manual annotations on text corpus is time-consuming and often requires the work of experts. In order to explore methods for optimizing annotation efforts, we study three key burdens of the annotation process: annotations, annotations, and annotations. Through a series of experiments using a corpus of clinical documents annotated for personally identifiable information written in French, we address each of these aspects and draw conclusions on how to make the most of an annotation effort.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="3004" citStr="Artstein and Poesio, 2008" startWordPosition="439" endWordPosition="442">tion quality to the standard of a fully doubly annotated corpus. The use of automatic pre-annotations was shown to increase annotation consistency and result in producing quality annotation with a time gain over annotating raw data (Fort and Sagot, 2010; N´ev´eol et al., 2011; Rosset et al., 2013). With the increasing use of crowdsourcing for obtaining annotated data, (Fort et al., 2011) show that there are ethic aspects to consider in addition to technical and monetary cost when using a microworking platform for annotation. While selecting the adequate methods for computing IAA is important (Artstein and Poesio, 2008) for interpreting the IAA for a particular task, annotator disagreement is inherent to all annotation tasks. To address this situation (Rzhetsky et al., 2009) designed a method to estimate annotation confidence based on annotator modeling. Overall, past work shows that creating high-quality manual annotations is time-consuming and often requires the work of experts. The time burden is distributed between the sheer creation of the annotations, the act of producing multiple annotations for the same data and the subsequent analysis of multiple annotations to resolve conflicts, viz. the creation o</context>
<context position="5940" citStr="Artstein and Poesio, 2008" startWordPosition="872" endWordPosition="875">ng more prevalent than others. In addition, the performance of entity recognition for each type of PHI also varies (Grouin and N´ev´eol, 2014). The datasets were split to obtain a training corpus (200 documents) and a test corpus (100 documents). For all documents in the training corpus, three types of human annotations are available: annotations performed independently by two human annotators and consensus annotations obtained after adjudication to resolve conflicts between the two annotators. Inter-annotator agreement on the training corpus was above 85% F-measure, which is considered high (Artstein and Poesio, 2008). The distribution of annotations over all PHI categories on both corpora (train/dev) is: address (188/100), zip code (197/97), date (1025/498), e-mail (119/57), hospital (448/208), identifier (135/76), last name (1855/855), first name (1568/724), telephone (802/386) and city (450/217). 2.2 Automatic Annotation Methods We directly applied the MEDINA rule-based de-identification tool (Grouin, 2013) to obtain baseline automatic annotations. We used the CRF toolkit Wapiti (Lavergne et al., 2010) to train a series of models on the various sets of annotations available for the training corpus. Feat</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V de Souza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<marker>Brown, Pietra, de Souza, Lai, Mercer, 1992</marker>
<rawString>Peter F Brown, Vincent J Della Pietra, Peter V de Souza, Jenifer C Lai, and Robert L Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Reducing the need for double annotation.</title>
<date>2011</date>
<booktitle>In Proc of Linguistic Annotation Workshop (LAW),</booktitle>
<pages>65--73</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2019" citStr="Dligach and Palmer, 2011" startWordPosition="285" endWordPosition="288">s quality and affordable cost. Inter-annotator agreement (IAA) has been used as an indicator of annotation quality. Early work showed that the use of automatic preannotation tools improved annotation consistency (Marcus et al., 1993). Careful and detailed annotation guideline definition was also shown to have positive impact on IAA (Wilbur et al., 2006). Efforts have investigated methods to reduce the human workload while annotating corpora. In particular, active learning (Settles et al., 2008) sucessfully selects portions of corpora that yield the most benefit when annotated. Alternatively, (Dligach and Palmer, 2011) investigated the need for double annotation and found that double annotation could be limited to carefully selected portions of a corpus. They produced an algorithm that automatically selects portions of a corpus for double annotation. Their approach allowed to reduce the amount of work by limiting the portion of doubly annotated data and maintained annotation quality to the standard of a fully doubly annotated corpus. The use of automatic pre-annotations was shown to increase annotation consistency and result in producing quality annotation with a time gain over annotating raw data (Fort and</context>
</contexts>
<marker>Dligach, Palmer, 2011</marker>
<rawString>Dmitriy Dligach and Martha Palmer. 2011. Reducing the need for double annotation. In Proc of Linguistic Annotation Workshop (LAW), pages 65–73. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Diego Marcheggiani</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>An enhanced CRFs-based system for information extraction from radiology reports.</title>
<date>2013</date>
<journal>J Biomed Inform,</journal>
<volume>46</volume>
<issue>3</issue>
<contexts>
<context position="14061" citStr="Esuli et al., 2013" startWordPosition="2158" endWordPosition="2161">rors (e.g. “Jhn DOE”) only resulted in correct predictions from the models trained on the human annotations. We did not find cases where the models built on the automatic annotations performed better than the models built on the human annotations. 4.3 Annotation strategy Table 1 indicates that for the purpose of training a machine learning entity recognizer, all types of human input are equivalent. In practice, this means that double annotations or consensus annotations are not necessary. The high inter-annotator agreement on our dataset may be a contributing factor for this finding. Indeed, (Esuli et al., 2013) found that with low inter-annotator agreement, models are biased towards the annotation style of the annotator who produced the training data. Therefore, we believe that interannotator should be established on a small dataset before annotators work independently. Table 2 shows that using human annotations for selected categories results in strong improvement of the performance over these categories (“address”, “hospital” and “identifier” categories in ARHC and ARH2 vs. AR) with little impact on the performance of the model on other categories. Therefore, careful human annotations are not nece</context>
</contexts>
<marker>Esuli, Marcheggiani, Sebastiani, 2013</marker>
<rawString>Andrea Esuli, Diego Marcheggiani, and Fabrizio Sebastiani. 2013. An enhanced CRFs-based system for information extraction from radiology reports. J Biomed Inform, 46(3):425–35, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kar¨en Fort</author>
<author>Benoit Sagot</author>
</authors>
<title>Influence of pre-annotation on POS-tagged corpus development.</title>
<date>2010</date>
<booktitle>In Proc of Linguistic Annotation Workshop (LAW),</booktitle>
<pages>56--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2631" citStr="Fort and Sagot, 2010" startWordPosition="380" endWordPosition="383">r, 2011) investigated the need for double annotation and found that double annotation could be limited to carefully selected portions of a corpus. They produced an algorithm that automatically selects portions of a corpus for double annotation. Their approach allowed to reduce the amount of work by limiting the portion of doubly annotated data and maintained annotation quality to the standard of a fully doubly annotated corpus. The use of automatic pre-annotations was shown to increase annotation consistency and result in producing quality annotation with a time gain over annotating raw data (Fort and Sagot, 2010; N´ev´eol et al., 2011; Rosset et al., 2013). With the increasing use of crowdsourcing for obtaining annotated data, (Fort et al., 2011) show that there are ethic aspects to consider in addition to technical and monetary cost when using a microworking platform for annotation. While selecting the adequate methods for computing IAA is important (Artstein and Poesio, 2008) for interpreting the IAA for a particular task, annotator disagreement is inherent to all annotation tasks. To address this situation (Rzhetsky et al., 2009) designed a method to estimate annotation confidence based on annotat</context>
</contexts>
<marker>Fort, Sagot, 2010</marker>
<rawString>Kar¨en Fort and Benoit Sagot. 2010. Influence of pre-annotation on POS-tagged corpus development. In Proc of Linguistic Annotation Workshop (LAW), pages 56–63. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kar¨en Fort</author>
<author>Gilles Adda</author>
<author>Kevin Bretonnel Cohen</author>
</authors>
<title>Amazon Mechanical Turk: Gold Mine or Coal Mine? Computational Linguistics,</title>
<date>2011</date>
<pages>413--420</pages>
<contexts>
<context position="2768" citStr="Fort et al., 2011" startWordPosition="402" endWordPosition="405">corpus. They produced an algorithm that automatically selects portions of a corpus for double annotation. Their approach allowed to reduce the amount of work by limiting the portion of doubly annotated data and maintained annotation quality to the standard of a fully doubly annotated corpus. The use of automatic pre-annotations was shown to increase annotation consistency and result in producing quality annotation with a time gain over annotating raw data (Fort and Sagot, 2010; N´ev´eol et al., 2011; Rosset et al., 2013). With the increasing use of crowdsourcing for obtaining annotated data, (Fort et al., 2011) show that there are ethic aspects to consider in addition to technical and monetary cost when using a microworking platform for annotation. While selecting the adequate methods for computing IAA is important (Artstein and Poesio, 2008) for interpreting the IAA for a particular task, annotator disagreement is inherent to all annotation tasks. To address this situation (Rzhetsky et al., 2009) designed a method to estimate annotation confidence based on annotator modeling. Overall, past work shows that creating high-quality manual annotations is time-consuming and often requires the work of expe</context>
</contexts>
<marker>Fort, Adda, Cohen, 2011</marker>
<rawString>Kar¨en Fort, Gilles Adda, and Kevin Bretonnel Cohen. 2011. Amazon Mechanical Turk: Gold Mine or Coal Mine? Computational Linguistics, pages 413–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Grouin</author>
<author>Aur´elie N´ev´eol</author>
</authors>
<title>De-identification of clinical notes in french: towards a protocol for reference corpus development.</title>
<date>2014</date>
<journal>J Biomed Inform.</journal>
<marker>Grouin, N´ev´eol, 2014</marker>
<rawString>Cyril Grouin and Aur´elie N´ev´eol. 2014. De-identification of clinical notes in french: towards a protocol for reference corpus development. J Biomed Inform.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Grouin</author>
</authors>
<title>Anonymisation de documents cliniques : performances et limites des m´ethodes symboliques et par apprentissage statistique.</title>
<date>2013</date>
<tech>Ph.D. thesis,</tech>
<institution>University Pierre et Marie Curie,</institution>
<location>Paris, France.</location>
<contexts>
<context position="6340" citStr="Grouin, 2013" startWordPosition="925" endWordPosition="926">notations obtained after adjudication to resolve conflicts between the two annotators. Inter-annotator agreement on the training corpus was above 85% F-measure, which is considered high (Artstein and Poesio, 2008). The distribution of annotations over all PHI categories on both corpora (train/dev) is: address (188/100), zip code (197/97), date (1025/498), e-mail (119/57), hospital (448/208), identifier (135/76), last name (1855/855), first name (1568/724), telephone (802/386) and city (450/217). 2.2 Automatic Annotation Methods We directly applied the MEDINA rule-based de-identification tool (Grouin, 2013) to obtain baseline automatic annotations. We used the CRF toolkit Wapiti (Lavergne et al., 2010) to train a series of models on the various sets of annotations available for the training corpus. Features set For each CRF experiment, we used the following set of features with a l1 regularization: • Lexical features: unigram and bigram of tokens; • Morphological features: (i) the token case (all in upper/lower case, combination of both), (ii) the token is a digit, (iii) the token is a punctuation mark, (iv) the token belongs to a specific list (first name, last name, city), (v) the token was no</context>
</contexts>
<marker>Grouin, 2013</marker>
<rawString>Cyril Grouin. 2013. Anonymisation de documents cliniques : performances et limites des m´ethodes symboliques et par apprentissage statistique. Ph.D. thesis, University Pierre et Marie Curie, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lavergne</author>
<author>Olivier Capp´e</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Practical very large scale CRFs.</title>
<date>2010</date>
<booktitle>In Proceedings the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>504--513</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Lavergne, Capp´e, Yvon, 2010</marker>
<rawString>Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon. 2010. Practical very large scale CRFs. In Proceedings the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504–513. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language.</title>
<date>2005</date>
<tech>Master’s thesis, MIT.</tech>
<contexts>
<context position="7398" citStr="Liang, 2005" startWordPosition="1101" endWordPosition="1102">the token is a digit, (iii) the token is a punctuation mark, (iv) the token belongs to a specific list (first name, last name, city), (v) the token was not identified in a dictionary of inflected forms, (vi) the token is a trigger word for specific categories (hospital, last name); • Syntactic features: (i) the part-of-speech (POS) tag of the token, as provided by the Tree Tagger tool (Schmid, 1994), (ii) the syntactic chunk the token belongs to, from a home made chunker based upon the previouses POS tags; • External features: (i) we created 320 classes of tokens using Liang’s implementation (Liang, 2005) of the Brown clustering algorithm (Brown et al., 1992), (ii) the position of the token within the document (begining, middle, end). Design of experiments The models were built to assess three annotation time-saving strategies: 1. Careful annotation: (i) AR=based on automatic annotations from the rule-based system, (ii) ARnH2=intersection of automatic annotations from the rule-based system with annotations from annotator 2. This model captures a situation where the human annotator would quickly revise the automatic annotations by removing errors: some annotations would be missing (average reca</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. Master’s thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chiristopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>2000</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Manning, Sch¨utze, 2000</marker>
<rawString>Chiristopher D. Manning and Hinrich Sch¨utze. 2000. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn TreeBank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1627" citStr="Marcus et al., 1993" startWordPosition="228" endWordPosition="231">tasks such as part-of-speech tagging or named entity recognition by relying on large annotated text corpora. As a result, developping highquality annotated corpora representing natural language phenomena that can be processed by statistical tools has become a major challenge for the scientific community. Several aspects of the annotation task have been studied in order to ensure corpus quality and affordable cost. Inter-annotator agreement (IAA) has been used as an indicator of annotation quality. Early work showed that the use of automatic preannotation tools improved annotation consistency (Marcus et al., 1993). Careful and detailed annotation guideline definition was also shown to have positive impact on IAA (Wilbur et al., 2006). Efforts have investigated methods to reduce the human workload while annotating corpora. In particular, active learning (Settles et al., 2008) sucessfully selects portions of corpora that yield the most benefit when annotated. Alternatively, (Dligach and Palmer, 2011) investigated the need for double annotation and found that double annotation could be limited to carefully selected portions of a corpus. They produced an algorithm that automatically selects portions of a c</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn TreeBank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aur´elie N´ev´eol</author>
<author>Rezarta Islamaj Do˘gan</author>
<author>Zhiyong Lu</author>
</authors>
<title>Semi-automatic semantic annotation of PubMed queries: a study on quality, efficiency, satisfaction.</title>
<date>2011</date>
<journal>J Biomed Inform,</journal>
<volume>44</volume>
<issue>2</issue>
<marker>N´ev´eol, Do˘gan, Lu, 2011</marker>
<rawString>Aur´elie N´ev´eol, Rezarta Islamaj Do˘gan, and Zhiyong Lu. 2011. Semi-automatic semantic annotation of PubMed queries: a study on quality, efficiency, satisfaction. J Biomed Inform, 44(2):310–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sophie Rosset</author>
<author>Cyril Grouin</author>
<author>Thomas Lavergne</author>
<author>Mohamed Ben Jannet</author>
<author>J´er´emy Leixa</author>
<author>Olivier Galibert</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Automatic named entity pre-annotation for out-of-domain human annotation.</title>
<date>2013</date>
<booktitle>In Proc of Linguistic Annotation Workshop (LAW),</booktitle>
<pages>168--177</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2676" citStr="Rosset et al., 2013" startWordPosition="388" endWordPosition="391">otation and found that double annotation could be limited to carefully selected portions of a corpus. They produced an algorithm that automatically selects portions of a corpus for double annotation. Their approach allowed to reduce the amount of work by limiting the portion of doubly annotated data and maintained annotation quality to the standard of a fully doubly annotated corpus. The use of automatic pre-annotations was shown to increase annotation consistency and result in producing quality annotation with a time gain over annotating raw data (Fort and Sagot, 2010; N´ev´eol et al., 2011; Rosset et al., 2013). With the increasing use of crowdsourcing for obtaining annotated data, (Fort et al., 2011) show that there are ethic aspects to consider in addition to technical and monetary cost when using a microworking platform for annotation. While selecting the adequate methods for computing IAA is important (Artstein and Poesio, 2008) for interpreting the IAA for a particular task, annotator disagreement is inherent to all annotation tasks. To address this situation (Rzhetsky et al., 2009) designed a method to estimate annotation confidence based on annotator modeling. Overall, past work shows that cr</context>
</contexts>
<marker>Rosset, Grouin, Lavergne, Jannet, Leixa, Galibert, Zweigenbaum, 2013</marker>
<rawString>Sophie Rosset, Cyril Grouin, Thomas Lavergne, Mohamed Ben Jannet, J´er´emy Leixa, Olivier Galibert, and Pierre Zweigenbaum. 2013. Automatic named entity pre-annotation for out-of-domain human annotation. In Proc of Linguistic Annotation Workshop (LAW), pages 168–177. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrey Rzhetsky</author>
<author>Hagit Shatkay</author>
<author>W John Wilbur</author>
</authors>
<title>How to get the most out of your curation effort. PLoS</title>
<date>2009</date>
<journal>ComputBiol,</journal>
<volume>5</volume>
<issue>5</issue>
<contexts>
<context position="3162" citStr="Rzhetsky et al., 2009" startWordPosition="463" endWordPosition="466">roducing quality annotation with a time gain over annotating raw data (Fort and Sagot, 2010; N´ev´eol et al., 2011; Rosset et al., 2013). With the increasing use of crowdsourcing for obtaining annotated data, (Fort et al., 2011) show that there are ethic aspects to consider in addition to technical and monetary cost when using a microworking platform for annotation. While selecting the adequate methods for computing IAA is important (Artstein and Poesio, 2008) for interpreting the IAA for a particular task, annotator disagreement is inherent to all annotation tasks. To address this situation (Rzhetsky et al., 2009) designed a method to estimate annotation confidence based on annotator modeling. Overall, past work shows that creating high-quality manual annotations is time-consuming and often requires the work of experts. The time burden is distributed between the sheer creation of the annotations, the act of producing multiple annotations for the same data and the subsequent analysis of multiple annotations to resolve conflicts, viz. the creation of a consensus. Research has addressed methods for reducing the time burden associated to these annotation This work is licenced under a Creative Commons Attri</context>
</contexts>
<marker>Rzhetsky, Shatkay, Wilbur, 2009</marker>
<rawString>Andrey Rzhetsky, Hagit Shatkay, and W John Wilbur. 2009. How to get the most out of your curation effort. PLoS ComputBiol, 5(5):e1000391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proc of International Conference on New Methods in Language.</booktitle>
<contexts>
<context position="7188" citStr="Schmid, 1994" startWordPosition="1067" endWordPosition="1068">e used the following set of features with a l1 regularization: • Lexical features: unigram and bigram of tokens; • Morphological features: (i) the token case (all in upper/lower case, combination of both), (ii) the token is a digit, (iii) the token is a punctuation mark, (iv) the token belongs to a specific list (first name, last name, city), (v) the token was not identified in a dictionary of inflected forms, (vi) the token is a trigger word for specific categories (hospital, last name); • Syntactic features: (i) the part-of-speech (POS) tag of the token, as provided by the Tree Tagger tool (Schmid, 1994), (ii) the syntactic chunk the token belongs to, from a home made chunker based upon the previouses POS tags; • External features: (i) we created 320 classes of tokens using Liang’s implementation (Liang, 2005) of the Brown clustering algorithm (Brown et al., 1992), (ii) the position of the token within the document (begining, middle, end). Design of experiments The models were built to assess three annotation time-saving strategies: 1. Careful annotation: (i) AR=based on automatic annotations from the rule-based system, (ii) ARnH2=intersection of automatic annotations from the rule-based syst</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proc of International Conference on New Methods in Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
<author>Mark Craven</author>
<author>Lewis Friedland</author>
</authors>
<title>Active learning with real annotation costs.</title>
<date>2008</date>
<booktitle>In Proc of the NIPS Workshop on Cost-Sensitive Learning.</booktitle>
<contexts>
<context position="1893" citStr="Settles et al., 2008" startWordPosition="268" endWordPosition="271"> challenge for the scientific community. Several aspects of the annotation task have been studied in order to ensure corpus quality and affordable cost. Inter-annotator agreement (IAA) has been used as an indicator of annotation quality. Early work showed that the use of automatic preannotation tools improved annotation consistency (Marcus et al., 1993). Careful and detailed annotation guideline definition was also shown to have positive impact on IAA (Wilbur et al., 2006). Efforts have investigated methods to reduce the human workload while annotating corpora. In particular, active learning (Settles et al., 2008) sucessfully selects portions of corpora that yield the most benefit when annotated. Alternatively, (Dligach and Palmer, 2011) investigated the need for double annotation and found that double annotation could be limited to carefully selected portions of a corpus. They produced an algorithm that automatically selects portions of a corpus for double annotation. Their approach allowed to reduce the amount of work by limiting the portion of doubly annotated data and maintained annotation quality to the standard of a fully doubly annotated corpus. The use of automatic pre-annotations was shown to </context>
</contexts>
<marker>Settles, Craven, Friedland, 2008</marker>
<rawString>Burr Settles, Mark Craven, and Lewis Friedland. 2008. Active learning with real annotation costs. In Proc of the NIPS Workshop on Cost-Sensitive Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W John Wilbur</author>
<author>Andrey Rzhetsky</author>
<author>Hagit Shatkay</author>
</authors>
<title>New directions in biomedical text annotation: definitions, guidelines and corpus construction.</title>
<date>2006</date>
<journal>BMC Bioinformatics,</journal>
<volume>25</volume>
<issue>7</issue>
<contexts>
<context position="1749" citStr="Wilbur et al., 2006" startWordPosition="247" endWordPosition="250">developping highquality annotated corpora representing natural language phenomena that can be processed by statistical tools has become a major challenge for the scientific community. Several aspects of the annotation task have been studied in order to ensure corpus quality and affordable cost. Inter-annotator agreement (IAA) has been used as an indicator of annotation quality. Early work showed that the use of automatic preannotation tools improved annotation consistency (Marcus et al., 1993). Careful and detailed annotation guideline definition was also shown to have positive impact on IAA (Wilbur et al., 2006). Efforts have investigated methods to reduce the human workload while annotating corpora. In particular, active learning (Settles et al., 2008) sucessfully selects portions of corpora that yield the most benefit when annotated. Alternatively, (Dligach and Palmer, 2011) investigated the need for double annotation and found that double annotation could be limited to carefully selected portions of a corpus. They produced an algorithm that automatically selects portions of a corpus for double annotation. Their approach allowed to reduce the amount of work by limiting the portion of doubly annotat</context>
</contexts>
<marker>Wilbur, Rzhetsky, Shatkay, 2006</marker>
<rawString>W John Wilbur, Andrey Rzhetsky, and Hagit Shatkay. 2006. New directions in biomedical text annotation: definitions, guidelines and corpus construction. BMC Bioinformatics, 25(7):356. 1CABeRneT: Compr´ehension Automatique de Textes Biom´edicaux pour la Recherche Translationnelle 2Vigi4MED: Vigilance dans les forums sur les M´edicaments</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9994595">
An Empirical Comparison of Features and Tuning
for Phrase-based Machine Translation
</title>
<author confidence="0.989989">
Spence Green, Daniel Cer, and Christopher D. Manning
</author>
<affiliation confidence="0.989324">
Computer Science Department, Stanford University
</affiliation>
<email confidence="0.996935">
{spenceg,danielcer,manning}@stanford.edu
</email>
<sectionHeader confidence="0.993861" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999894954545454">
Scalable discriminative training methods
are now broadly available for estimating
phrase-based, feature-rich translation mod-
els. However, the sparse feature sets typi-
cally appearing in research evaluations are
less attractive than standard dense features
such as language and translation model
probabilities: they often overfit, do not gen-
eralize, or require complex and slow fea-
ture extractors. This paper introduces ex-
tended features, which are more specific
than dense features yet more general than
lexicalized sparse features. Large-scale ex-
periments show that extended features yield
robust BLEU gains for both Arabic-English
(+1.05) and Chinese-English (+0.67) rel-
ative to a strong feature-rich baseline. We
also specialize the feature set to specific
data domains, identify an objective function
that is less prone to overfitting, and release
fast, scalable, and language-independent
tools for implementing the features.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998764">
Scalable discriminative algorithm design for ma-
chine translation (MT) has lately been a booming
enterprise. There are now algorithms for every taste:
probabilistic and distribution-free, online and batch,
regularized and unregularized. Technical differ-
ences aside, the papers that apply these algorithms
to phrase-based translation often share a curious
empirical characteristic: the algorithms support ex-
tra features, but the features do not significantly
improve translation. For example, Hopkins and
May (2011) showed that PRO with some simple ad
hoc features only exceeds the baseline on one of
three language pairs. Gimpel and Smith (2012b)
observed a similar result for both PRO and their
ramp-loss algorithm. Cherry and Foster (2012)
found that, at least in the batch case, many algo-
rithms produce similar results, and features only
significantly increased quality for one of three lan-
guage pairs. Only recently did Cherry (2013) and
Green et al. (2013b) identify certain features that
consistently reduce error.
These empirical results suggest that feature de-
sign and model fitting, the subjects of this paper,
warrant a closer look. We introduce an effective
extended feature set for phrase-based MT and iden-
tify a loss function that is less prone to overfitting.
Extended features share three attractive characteris-
tics with the standard Moses dense features (Koehn
et al., 2007): ease of implementation, language in-
dependence, and independence from ancillary cor-
pora like treebanks. In our experiments, they do
not overfit and can be extracted efficiently during
decoding. Because all feature weights are tuned
on the development set, the new feature templates
are amenable to feature augmentation (Daumé III,
2007), a simple domain adaptation technique that
we show works surprisingly well for MT.
Extended features are designed according to a
principle rather than a rule: they should fire less
than standard dense features, which are general, but
more than so-called sparse features, which are very
specific—they are usually lexicalized—and thus
prone to overfitting. This principle is motivated
by analysis, which shows how expressive models
can be a mixed blessing in the translation setting.
It is obvious that features allow the model to fit
the tuning data more tightly. For example, sparse
lexicalized features could reduce tuning error by
learning that the references prefer U.S. over United
States, a minor lexical distinction. Reference choice
should matter more than in the dense case, an issue
that we quantify. We also show that frequency cut-
offs, which are a crude but common form of feature
selection, are unnecessary and even detrimental
when features follow this principle.
We report large-scale translation quality experi-
ments relative to both dense and feature-rich base-
lines. Our best feature set, which includes domain
adaptation features, yields an average +1.05 BLEU
improvement for Arabic-English and +0.67 for
</bodyText>
<page confidence="0.993775">
466
</page>
<note confidence="0.715981">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 466–476,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99732">
Chinese-English. In addition to the extended fea-
ture set, we show that an online variant of expected
error (Och, 2003) is significantly faster to compute,
less prone to overfitting, and nearly as effective as a
pairwise loss. We release all software—feature ex-
tractors, and fast word clustering and data selection
packages—used in our experiments.1
</bodyText>
<sectionHeader confidence="0.959528" genericHeader="method">
2 Phrase-based Models and Learning
</sectionHeader>
<bodyText confidence="0.99927">
The log-linear approach to phrase-based translation
(Och and Ney, 2004) directly models the predictive
translation distribution
</bodyText>
<equation confidence="0.892687">
p(el f; w) = Z1 exp I wTφ(e, f)] (1)
</equation>
<bodyText confidence="0.999920705882353">
where e is the target string, f is the source string,
w E Rd is the vector of model parameters, φ(·) E
Rd is a feature map, and Z(f) is an appropriate
normalizing constant. Assume that there is also a
function ρ(e, f) E Rd that produces a recombina-
tion map for the features. That is, each coordinate
in ρ represents the state of the corresponding co-
ordinate in φ. For example, suppose that φj is the
log probability produced by the n-gram language
model (LM). Then ρj would be the appropriate LM
history. Recall that recombination collapses deriva-
tions with equivalent recombination maps during
search and thus affects learning. This issue signifi-
cantly influences feature design.
To learn w, we follow the online procedure of
Green et al. (2013b), who calculate gradient steps
with AdaGrad (Duchi et al., 2011) and perform fea-
ture selection via L1 regularization in the FOBOS
(Duchi and Singer, 2009) framework. This proce-
dure accommodates any loss function for which a
subgradient can be computed. Green et al. (2013b)
used a PRO objective (Hopkins and May, 2011)
with a logistic (surrogate) loss function. However,
later results showed overfitting (Green et al., 2013a),
and we found that their online variant of PRO tends
to produce short translations like its batch counter-
part (Nakov et al., 2013). Moreover, PRO requires
sampling, making it slow to compute.
To address these shortcomings, we explore an
online variant of expected error (Och, 2003, Eq.7).
Let Et = {eiJni=1 be a scored n-best list of trans-
lations at time step t for source input ft. Let G(e)
be a gold error metric that evaluates each candi-
date translation with respect to a set of one or more
</bodyText>
<footnote confidence="0.896494">
1http://nlp.stanford.edu/software/phrasal
</footnote>
<bodyText confidence="0.699939">
references. The smooth loss function is
</bodyText>
<equation confidence="0.982759888888889">
`t(wt−1) = Ep(eJft;wt−,)[G(e)]
1 � � �
Z exp wTφ(e&apos;, f) · G(e&apos;)
e&apos;EEt (2)
with normalization constant Z =
Ee,EEt exp (wTφ(e&apos;, f)). The gradient gt
for coordinate j is:
gt = E[G(e)φj(e, ft)]−
E[G(e)]E[φj(e, ft)] (3)
</equation>
<bodyText confidence="0.999243666666667">
To our knowledge, we are the first to experiment
with the online version of this loss.2 When G(e) is
sentence-level BLEU+1 (Lin and Och, 2004)—the
setting in our experiments—this loss is also known
as expected BLEU (Cherry and Foster, 2012). How-
ever, other metrics are possible.
</bodyText>
<sectionHeader confidence="0.996372" genericHeader="method">
3 Extended Phrase-based Features
</sectionHeader>
<bodyText confidence="0.9999605">
We divide our feature templates into five categories,
which are well-known sources of error in phrase-
based translation. The features are defined over
derivations d = {riJD i=1, which are ordered se-
quences of rules r from the translation model. De-
fine functions f(·) to be the source string of a rule
or derivation and e(·) to be the target string. Local
features can be extracted from individual rules and
do not declare any state in the recombination map,
thus for all local features i we have ρi = 0. Non-
local features are defined over partial derivations
and declare some state, either a real-valued param-
eter or an index indicating a categorical value like
an n-gram context.
For each language, the extended feature tem-
plates require unigram counts and a word-to-class
mapping ϕ : w H c for word w E V and class
c E C. These can be extracted from any monolin-
gual data; our experiments simply use both sides of
the unaligned parallel training data.
The features are language-independent, but we
will use Arabic-English as a running example.
</bodyText>
<subsectionHeader confidence="0.999069">
3.1 Lexical Choice
</subsectionHeader>
<bodyText confidence="0.990537666666667">
Lexical choice features make more specific distinc-
tions between target words than the dense transla-
tion model features (Koehn et al., 2003).
</bodyText>
<footnote confidence="0.976992">
2Gao and He (2013) used stochastic gradient descent and
expected BLEU to learn phrase table feature weights, but not
the full translation model w.
</footnote>
<page confidence="0.999115">
467
</page>
<bodyText confidence="0.996513833333333">
Lexicalized rule indicator (Liang et al., 2006a)
Some rules occur frequently enough that we can
learn rule-specific weights that augment the dense
translation model features. For example, our model
learns the following rule indicator features and
weights:
</bodyText>
<equation confidence="0.965473333333333">
H. AJ.ƒ @ ==&gt;. reasons -0.022
H. AJ.ƒ @ ==&gt;. reasons for 0.002
H. AJ.ƒ @ ==&gt;. the reasons for 0.016
</equation>
<bodyText confidence="0.97684269047619">
These translations are all correct depending on con-
text. When the plural noun H. AJ.ƒ @ ‘reasons’ appears
in a construct state (iDafa) the preposition for is
unrealized. Moreover, depending on the context,
the English translation might also require the deter-
miner the, which is also unrealized. The weights
�
reflect that H. AJ.ƒ@ ‘reasons’ often appears in con-
struct and boost insertion of necessary target terms.
To prevent overfitting, this template only fires an
indicator for rules that occur more than 50 times
in the parallel training data (this is different from
frequency filtering on the tuning data; see section
6.1). The feature is local.
Class-based rule indicator Word classes ab-
stract over lexical items. For each rule r, a pro-
totype that abstracts over many rules can be built
by concatenating {cp(w) : w E f(r)} with
{cp(w) : w E e(r)}. For example, suppose
that Arabic class 492 consists primarily of Arabic
present tense verbs and class 59 contains English
auxiliaries. Then the model might penalize a rule
prototype like 492&gt;59_59, which drops the verb.
This template fires an indicator for each rule proto-
type and is local.
Target unigram class (Ammar et al., 2013) Tar-
get lexical items with similar syntactic and semantic
properties may have very different frequencies in
the training data. These frequencies will influence
the dense features. For example, in one of our En-
glish class mappings the following words map to
the same class:
word class freq.
surface-to-surface
air-to-air
ground-to-air
The classes capture common linguistic attributes of
these words, which is the motivation for a full class-
based LM. Learning unigram weights directly is
surprisingly effective and does not require building
another LM. This template fires a separate indicator
for each class {cp(w) : w E e(r)} and is local.
</bodyText>
<subsectionHeader confidence="0.999722">
3.2 Word Alignments
</subsectionHeader>
<bodyText confidence="0.9992946">
Word alignment features allow the model to recog-
nize fine-grained phrase-internal information that
is largely opaque in the dense model.
Lexicalized alignments (Liang et al., 2006a)
Consider the internal alignments of the rule:
</bodyText>
<equation confidence="0.838845666666667">
sunday ,
ÐñK� 1
YgB@ 2
</equation>
<bodyText confidence="0.999577891891892">
Alignment 1 (ÐñK� &apos;day&apos; ==&gt;. ,) is incorrect and align-
ment 2 is correct. The dense translation model
features might assign this rule high probability if
alignment 1 is a common alignment error. Lexical-
ized alignment features allow the model to compen-
sate for these events. This feature fires an indicator
for each alignment in a rule—including multiword
cliques—and is local.
Class-based alignments Like the class-based
rule indicator, this feature template replaces each
lexical item with its word class, resulting in an align-
ment prototype. This feature fires an indicator for
each alignment in a rule after mapping lexical items
to classes. It is local.
Source class deletion Phrase extraction algo-
rithms often use a “grow” symmetrization step (Och
and Ney, 2003) to add alignment points. Sometimes
this procedure can produce a rule that deletes im-
portant source content words. This feature template
allows the model to penalize these rules by firing
an indicator for the class of each unaligned source
word. The feature is local.
Punctuation ratio Languages use different types
and ratios of punctuation (Salton, 1958). For ex-
ample, quotation marks are not commonly used in
Arabic, but they are conventional in English. Fur-
thermore, spurious alignments often contain punc-
tuation. To control these two phenomena, this fea-
ture template returns the ratio of target punctuation
tokens to source punctuation tokens for each deriva-
tion. Since the denominator is constant, this feature
can be computed incrementally as a derivation is
constructed. It is local.
Function word ratio Words can also be spuri-
ously aligned to non-punctuation, non-digit func-
tion words such as determiners and particles. Fur-
thermore, linguistic differences may account for
</bodyText>
<figure confidence="0.964162">
0 269
0 98
0 63
</figure>
<page confidence="0.999276">
468
</page>
<bodyText confidence="0.999878777777778">
differences in function word occurrences. For ex-
ample, English has a broad array of modal verbs
and auxiliaries not found in Arabic. This feature
template takes the 25 most frequent words in each
language (according to the unigram counts), and
computes the ratio between target and source func-
tion words for each derivation. As before the de-
nominator is constant, so the feature can be com-
puted efficiently. It is local.
</bodyText>
<subsectionHeader confidence="0.999611">
3.3 Phrase Boundaries
</subsectionHeader>
<bodyText confidence="0.999778363636364">
The LM and hierarchical reordering model are the
only dense features that cross phrase boundaries.
Target-class bigram boundary We have already
added target class unigrams. We find that both lexi-
calized and class-based bigrams cause overfitting,
therefore we restrict to bigrams that straddle phrase
boundaries. The feature template fires an indicator
for the concatenation of the word classes on either
side of each boundary. This feature is non-local
and its recombination state p is the word class at
the right edge of the partial derivation.
</bodyText>
<subsectionHeader confidence="0.996726">
3.4 Derivation Quality
</subsectionHeader>
<bodyText confidence="0.991291466666667">
To satisfy strong features like the LM, or hard con-
straints like the distortion limit, the phrase-based
model can build derivations from poor translation
rules. For example, a derivation consisting mostly
of unigram rules may miss idiomatic usage that
larger rules can capture. All of these feature tem-
plates are local.
Source dimension (Hopkins and May, 2011) An
indicator feature for the source dimension of the
rule: |f(r)|.
Target dimension (Hopkins and May, 2011) An
indicator for the target dimension: |e(r)|.
Rule shape (Hopkins and May, 2011) The
conjunction of source and target dimension:
|f(r)|_|e(r)|.
</bodyText>
<subsectionHeader confidence="0.928827">
3.5 Reordering
</subsectionHeader>
<bodyText confidence="0.998228571428571">
Lexicalized reordering models score the orientation
of a rule in an alignment grid. We use the same
baseline feature extractor as Moses, which has three
classes: monotone, swap, and discontinuous. We
also add the non-monotone class, which is a con-
junction of swap and discontinuous, for a total of
eight orientations.3
</bodyText>
<footnote confidence="0.9957825">
3Each class has “with-previous” and “with-next” special-
izations.
</footnote>
<table confidence="0.996372833333333">
Algorithm (implementation) #threads Time
Brown (wcluster) 1 1023.39
Clark (cluster_neyessen) 1 890.11
Och (mkcls) 1 199.04
PredictiveFull (this paper) 8 3.27
Predictive (this paper) 8 2.42
</table>
<tableCaption confidence="0.99883">
Table 1: Wallclock time (min.sec) to generate a
</tableCaption>
<bodyText confidence="0.988132333333333">
mapping from a vocabulary of 63k English words
(3.7M tokens) to 512 classes. All experiments were
run on the same server, which had eight physical
cores. Our Java implementation is multi-threaded;
the C++ baselines are single-threaded.
Lexicalized rule orientation (Liang et al.,
2006a) For each rule, the template fires an indi-
cator for the concatenation of the orientation class,
each element in f(r), and each element in e(r). To
prevent overfitting, this template only fires for rules
that occur more than 50 times in the training data.
The feature is non-local and its recombination state
p is the rule orientation.
Class-based rule orientation For each rule, the
template fires an indicator for the concatenation
of the orientation class, each element in {co(w) :
w ∈ f(r)}, and each element in {co(w) : w ∈
e(r)}. The feature is non-local and its recombina-
tion state p is the rule orientation.
Signed linear distortion The dense feature set
includes a simple reordering cost model. Assume
that [r] returns the index of the leftmost source index
in f(d) and [[r]] returns the rightmost index. Then
the linear distortion is:
</bodyText>
<equation confidence="0.993507333333333">
D
δ = [r1] + |[[ri−1]] + 1 − [ri] |(4)
i=2
</equation>
<bodyText confidence="0.9999775">
This score does not distinguish between left and
right distortion. To correct this issue, this feature
template fires an indicator for each signed com-
ponent in the sum, for each positive and negative
component. The feature is non-local and its recom-
bination state p is the signed distortion.
</bodyText>
<subsectionHeader confidence="0.987921">
3.6 Feature Dependencies
</subsectionHeader>
<bodyText confidence="0.996231833333333">
While unigram counts are trivial to compute, the
same is not necessarily true of the word-to-class
mapping co. Standard algorithms run in O(n2),
where n = |V |. Table 1 shows an evaluation of
standard implementations of several popular algo-
rithms: Brown et al. (1992) implemented by Liang
</bodyText>
<page confidence="0.998159">
469
</page>
<bodyText confidence="0.999755212121212">
(2005); Clark (2003) without the morphological
prior, which increases training time dramatically;
and the implementation of Och (1999) that comes
with the GIZA++ word aligner. The latter has
been used recently for MT features (Ammar et al.,
2013; Cherry, 2013; Yu et al., 2013). In a broad
survey, Christodoulopoulos et al. (2010) found that
for several downstream tasks, most word clustering
algorithms—including Brown and Clark—result in
similar task accuracy. For our large-scale setting,
the primary issue is then the time to estimate co.
For large corpora the existing implementations
may require days or weeks, making our feature set
less practical than the traditional dense MT features.
Consequently, we re-implemented the predictive
one-sided class model of Whittaker and Woodland
(2001) with the parallelized clustering algorithm of
Uszkoreit and Brants (2008) (Predictive), which
was originally developed for very large scale lan-
guage modeling. Our implementation uses multiple
threads on a single processor instead of MapReduce.
We also added two extensions that are useful for
translation features. First, we map all digits to 0.
This reduces sparsity while retaining useful patterns
such as 0000 (e.g., years) and 0th (e.g., ordinals).
Second, we mapped all words occurring fewer than
T times to an &lt;unk&gt; token. In our experiment,
these two changes reduce the vocabulary size by
71.1%. They also make the mapping co more ro-
bust to unseen events during translation decoding.
For a conservative comparison to the other three
algorithms, we include results without these two
extensions (PredictiveFull).4
</bodyText>
<sectionHeader confidence="0.98931" genericHeader="method">
4 Domain Adaptation Features
</sectionHeader>
<bodyText confidence="0.999962916666667">
Feature augmentation is a simple yet effective do-
main adaptation technique (Daumé III, 2007). Sup-
pose that the source data comes from M domains.
Then for each original feature Oi, we add M addi-
tional features, one for each domain. The original
feature Oi can be interpreted as a prior over the M
domains (Finkel and Manning, 2009, fn.2).
Most of the extended features are defined over
rules, so the critical issue is how to identify in-
domain rules. The trick is to know which training
sentence pairs are in-domain. Then we can annotate
all rules extracted from these instances with domain
</bodyText>
<footnote confidence="0.70914175">
4For the baselines the training settings are the suggested
defaults: Brown, default; Clark, 10 iterations, frequency cutoff
τ = 5; Och, 10 iterations. Our implementation: PredictiveFull,
30 iterations, τ = 0; Predictive, 30 iterations, τ = 5.
</footnote>
<bodyText confidence="0.999962315789473">
labels. The in-domain rule sets need not be disjoint
since some rules might be useful across domains.
This paper explores the following approach: we
choose one of the M domains as the default. Next,
we collect some source sentences for each of the
M − 1 remaining domains. Using these examples
we then identify in-domain sentence pairs in the bi-
text via data selection, in our case the feature decay
algorithm (Biçici and Yuret, 2011). Finally, our rule
extractor adds domain labels to all rules extracted
from each selected sentence pair. Crucially, these
labels do not influence which rules are extracted
or how they are scored. The resulting phrase table
contains the same rules, but with a few additional
annotations.
Our method assumes domain labels for each
source input to be decoded. Our experiments utilize
gold, document-level labels, but accurate sentence-
level domain classifiers exist (Wang et al., 2012).
</bodyText>
<subsectionHeader confidence="0.99964">
4.1 Augmentation of Extended Features
</subsectionHeader>
<bodyText confidence="0.999835444444444">
Irvine et al. (2013) showed that lexical selection is
the most quantifiable and perhaps most common
source of error in phrase-based domain adaptation.
Our development experiments seemed to confirm
this hypothesis as augmentation of the class-based
and non-lexical (e.g., Rule shape) features did not
reduce error. Therefore, we only augment the lex-
icalized features: rule indicators and orientations,
and word alignments.
</bodyText>
<subsectionHeader confidence="0.997791">
4.2 Domain-Specific Feature Templates
</subsectionHeader>
<bodyText confidence="0.999961272727273">
In-domain Rule Indicator (Durrani et al., 2013)
An indicator for each rule that matches the input do-
main. This template fires a generic in-domain indi-
cator and a domain-specific indicator (e.g., the fea-
tures might be indomain and indomain-nw).
The feature is local.
Adjacent Rule Indicator Indicators for adjacent
in-domain rules. This template also fires both
generic and domain-specific features. The feature
is non-local and the state is a boolean indicating if
the last rule in a partial derivation is in-domain.
</bodyText>
<sectionHeader confidence="0.999253" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999632833333333">
We evaluate and analyze our feature set under a vari-
ety of large-scale experimental conditions including
multiple domains and references. To our knowl-
edge, the only language pairs with sufficient re-
search resources to support this protocol are Arabic-
English (Ar-En) and Chinese-English (Zh-En). The
</bodyText>
<page confidence="0.991743">
470
</page>
<table confidence="0.78372525">
Bilingual Monolingual
#Seg. #Tok. #Tok.
Ar-En 6.6M 375M 990M
Zh-En 9.3M 538M
</table>
<tableCaption confidence="0.791448">
Table 2: Bilingual and monolingual training cor-
</tableCaption>
<bodyText confidence="0.993288846153846">
pora. The monolingual English data comes from
the AFP and Xinhua sections of English Gigaword
4 (LDC2009T13).
training corpora5 come from several Linguistic
Data Consortium (LDC) sources from 2012 and
earlier (Table 2). The test, development, and tuning
corpora6 come from the NIST OpenMT and Metric-
sMATR evaluations (Table 3). Extended features
benefit from more tuning data, so we concatenated
five NIST data sets to build one large tuning set.
Observe that all test data come from later epochs
than the tuning and development data.
From these data we built phrase-based MT sys-
tems with Phrasal (Green et al., 2014).7 We aligned
the parallel corpora with the Berkeley aligner
(Liang et al., 2006b) with standard settings and
symmetrized via the grow-diag heuristic. We cre-
ated separate English LMs for each language pair by
concatenating the monolingual Gigaword data with
the target-side of the respective bitexts. For each
corpus we estimated unfiltered 5-gram language
models with lmplz (Heafield et al., 2013).
For each condition we ran the learning algorithm
for 25 epochs8 and selected the model according
to the maximum uncased, corpus-level BLEU-4
(Papineni et al., 2002) score on the dev set.
</bodyText>
<subsectionHeader confidence="0.924654">
5.1 Results
</subsectionHeader>
<bodyText confidence="0.999268">
We evaluate the new feature set relative to two base-
lines. DENSE is the same baseline as Green et al.
</bodyText>
<footnote confidence="0.508172055555556">
5We tokenized the English with Stanford CoreNLP ac-
cording to the Penn Treebank standard (Marcus et al., 1993),
the Arabic with the Stanford Arabic segmenter (Monroe et
al., 2014) according to the Penn Arabic Treebank standard
(Maamouri et al., 2008), and the Chinese with the Stanford
Chinese segmenter (Chang et al., 2008) according to the Penn
Chinese Treebank standard (Xue et al., 2005).
6Data sources: tune, MT023568; dev, MT04; dev-dom,
domain adaptation dev set is MT04 and all wb and bn data
from LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En);
test2, Progress0809 which was revealed in the OpenMT 2012
evaluation; test3, MetricsMATR08-10.
7System settings: distortion limit of 5, cube pruning beam
size of 1200, maximum phrase length of 7.
8Other learning settings: 16 threads, mini-batch size of 20;
Ll regularization strength λ = 0.001; learning rate η0 = 0.02;
initialization of LM to 0.5, word penalty to -1.0, and all other
dense features to 0.2; initialization of extended features to 0.0.
</footnote>
<table confidence="0.99954525">
#Seg. #Ref. Domains
Ar-En Zh-En
tune 5,604 5,900 4 nw,wb,bn
dev 1,075 1,597 4 nw
dev-dom 2,203 2,317 1 nw,wb,bn
test1 1,313 820 4 nw,wb
test2 1,378 1,370 4 nw,wb
test3 628 613 1 nw,wb,bn
</table>
<tableCaption confidence="0.898151333333333">
Table 3: Development, test, and tuning data. Do-
main abbreviations: broadcast news (bn), newswire
(nw), and web (wb).
</tableCaption>
<bodyText confidence="0.969924333333333">
(2013b); these dense features are included in all of
the models that follow. SPARSE is their best feature-
rich model, which adds lexicalized rule indicators,
alignments, orientations, and source deletions with-
out bitext frequency filtering.
We do not perform a full ablation study. Both
the approximate search and the randomization of
the order of tuning instances make the contribu-
tions of each individual template differ from run to
run. Resource constraints prohibit multiple large-
scale runs for each incremental feature. Instead,
we divide the extended feature set into two parts,
and report large-scale results. EXT includes all ex-
tended features except for the the filtered lexicalized
feature templates. EXT+FiLT adds those filtered
lexicalized templates: rule indicators and orienta-
tions, and word alignments (section 3).
Table 4 shows translation quality results. The
new feature set significantly exceeds the baseline
DENSE model for both language pairs. An interest-
ing result is that the new extended features alone
match the strong SPARSE baseline. The class-based
features, which are more general, should clearly
be preferred to the sparse features when decoding
out-of-domain data (so long as word mappings are
trained for that data). The increased runtime per
iteration comes not from feature extraction but from
larger inner products as the model size increases.
Next, we add the domain features from section
4.2. We marked in-domain sentence pairs by con-
catenating the tuning data with additional bn and
wb monolingual in-domain data from several LDC
sources.9 The FDA selection size was set to 20
times the number of in-domain examples for each
genre. Newswire was selected as the default domain
since most of the bitext comes from that domain.
The bottom rows of Tables 4a and 4b compare
9Catalog: LDC2007T24, LDC2008T08, LDC2008T18,
LDC2012T16, LDC2013T01, LDC2013T05, LDC2013T14.
</bodyText>
<page confidence="0.9572">
471
</page>
<table confidence="0.992107642857143">
Model #features Epochs Min. /Epoch tune dev test1 test2 test3
DENSE (D) 18 24 3 49.52 50.25 47.98 43.41 27.56
D+SPARSE 48,597 24 8 56.51 52.98 49.55 45.40 29.02
D+EXT 62,931 16 11 57.83 54.33 49.66 45.66 29.15
D+EXT+FiLT 94,606 17 14 59.13 55.35 50.02 46.24 29.59
D+EXT+FiLT+Dom 123,353 22 18 59.97 29.20† 50.45 46.24 30.84
(a) Ar-En.
Model #features Epochs Min. /Epoch tune dev test1 test2 test3
DENSE (D) 18 17 3 32.82 34.96 26.61 26.72 10.19
D+SPARSE 55,024 17 8 38.91 36.68 27.86 28.41 10.98
D+EXT 67,936 16 13 40.96 37.19 28.27 28.40 10.72
D+EXT+FiLT 100,275 17 14 41.38 37.36 28.68 28.90 11.24
D+EXT+FiLT+Dom 126,014 17 14 41.70 17.20† 28.71 28.96 11.67
(b) Zh-En.
</table>
<tableCaption confidence="0.990749">
Table 4: Translation quality results (uncased BLEU-4 %). Per-epoch times are in minutes (Min.). Statistical
</tableCaption>
<bodyText confidence="0.994045347826087">
significance relative to D+SPARSE, the strongest baseline: bold (p &lt; 0.001) and bold-italic (p &lt; 0.05).
Significance is computed by the permutation test of Riezler and Maxwell (2005). †The dev score of
EXT+FiLT+Dom is the dev-dom data set from Table 3, so it is not comparable with the other rows.
EXT+FiLT+Dom to the baselines and other feature
sets. The gains relative to SPARSE are statistically
significant for all six test sets.
A crucial result is that with domain features accu-
racy relative to EXT+FiLT never decreases: a single
domain-adapted system is effective across domains.
Irvine et al. (2013) showed that when models from
multiple domains are interpolated, scoring errors
affecting lexical selection—the model could have
generated the correct target lexical item but did
not—increase significantly. We do not observe that
behavior, at least from the perspective of BLEU.
Table 5 separates out per-domain results. The
web data appears to be the hardest domain. That is
sensible given that broadcast news transcripts are
more similar to newswire, the default domain, than
web data. Moreover, inspection of the bitext sources
revealed very little web data, so our automatic data
selection is probably less effective. Accuracy on
newswire actually increases slightly.
</bodyText>
<sectionHeader confidence="0.993966" genericHeader="method">
6 Analysis
</sectionHeader>
<subsectionHeader confidence="0.986224">
6.1 Learning
</subsectionHeader>
<bodyText confidence="0.998889666666667">
Loss Function In a now classic empirical com-
parison of batch tuning algorithms, Cherry and Fos-
ter (2012) showed that PRO and expected BLEU
</bodyText>
<table confidence="0.998534714285714">
Ar-En test1 test2 bn test3 wb
nw wb nw wb nw
EF 59.78 39.55 51.69 38.80 30.39 37.59 20.58
EFD 60.21 40.38 51.76 38.77 31.63 38.18 22.37
Zh-En
EF 34.56 21.94 17.38 12.07 3.04 17.42 12.83
EFD 34.87 21.82 17.96 12.66 3.01 17.74 13.80
</table>
<tableCaption confidence="0.978489">
Table 5: Per-domain results (uncased BLEU-4 %).
</tableCaption>
<bodyText confidence="0.893807">
Here bold simply indicates the maximum in each
column. Model abbreviations: EF is EXT+FiLT and
EFD is EXT+FiLT+Dom.
yielded similar translation quality results. In con-
trast, Table 6a shows significant differences be-
tween these loss functions. First, expected BLEU
can be computed faster since it is linear in the n-
best list size, whereas exact computation of the PRO
objective is O(n2) (thus sampling is often used). It
also converges faster. Second, PRO tends to select
larger models.10 Finally, PRO seems to overfit on
the tuning set, since there are no gains on test1.
Feature Selection A common yet crude method
of feature selection is frequency cutoffs on the
</bodyText>
<footnote confidence="0.811983">
10PRO L1 regularization strength of λ = 0.01, above which
model size decreases but translation quality degrades.
</footnote>
<page confidence="0.981919">
472
</page>
<table confidence="0.988096285714286">
Loss #epochs Min./Epoch #feat. tune test1
EB 17 14 94,606 59.13 50.02
PRO 14 25 181,542 61.20 50.09
(a) PRO vs. expected BLEU (EB) for EXT+FiLT.
Feature Selection #features tune test1
Ll 94,606 59.13 50.02
Freq. cutoffs 23,617 56.84 49.79
(b) Feature selection for EXT+FiLT.
Model #refs tune test1
DENSE 4 49.52 47.98
DENSE 1 49.34 47.78
EXT+FiLT 4 59.13 50.02
EXT+FiLT 1 55.39 48.88
(c) Single- vs. multiple-reference tuning.
</table>
<tableCaption confidence="0.996893">
Table 6: Ar-En learning comparisons.
</tableCaption>
<bodyText confidence="0.998435">
tuning data. Only features that fire more than
some threshold are admitted into the feature set.
Table 6b shows that for our new feature set, Li
regularization—which simply requires setting a reg-
ularization strength parameter—is more effective
than frequency cutoffs.
References Few MT data sets supply multiple ref-
erences. Even when they do, those references are
but a sample from a larger pool of possible trans-
lations. This observation has motivated attempts
at generating lattices of translations for evaluation
(Dreyer and Marcu, 2012; Bojar et al., 2013). But
evaluation is only part of the problem. Table 6c
shows that the DENSE model, which has only a
few features to describe the data, is little affected
by the elimination of references. In contrast, the
feature-rich model degrades significantly. This may
account for the underperformance of features in
single-reference settings like WMT (Durrani et al.,
2013; Green et al., 2013a). The next section ex-
plores the impact of references further.
</bodyText>
<subsectionHeader confidence="0.998218">
6.2 Reference Variance
</subsectionHeader>
<bodyText confidence="0.999912">
We took the DENSE Ar-En output for the dev
data, which has four references, and computed the
sentence-level BLEU+1 with respect to each refer-
ence. Figure 1a shows a point for each of the 1,075
translations. The horizontal axis is the minimum
score with respect to any reference and the verti-
cal axis is the maximum (BLEU has a maximum
value of 1.0). Ideally, from the perspective of learn-
</bodyText>
<figure confidence="0.998169714285714">
0 25 50 75 100
Minimum
(a) Maximum vs. minimum BLEU+1 (%)
0 25 50 75 100
Maximum
(b) BLEU+1 (%) according to all four references vs.
maximum
</figure>
<figureCaption confidence="0.9946795">
Figure 1: Reference choice analysis for Ar-En
DENSE output on the dev set.
</figureCaption>
<bodyText confidence="0.991345277777778">
ing, the scores should cluster around the diagonal:
the references should yield similar scores. This is
hardly the case. The mean difference is M = 18.1
BLEU, with a standard deviation SD = 11.5.
Figure 1b shows the same data set, but with the
maximum on the horizontal axis and the multiple-
reference score on the vertical axis. Assuming
a constant brevity penalty, the maximum lower-
bounds the multiple-reference score since BLEU ag-
gregates n-grams across references. The multiple-
reference score is an “easier” target since the model
has more opportunities to match n-grams.
Consider again the single-reference condition
and one of the pathological cases at the top of Fig-
ure 1a. Suppose that the low-scoring reference is
observed in the single-reference condition. The
more expressive feature-rich model has a greater
capacity to fit that reference when, under another
</bodyText>
<figure confidence="0.951489272727273">
●
●
● ●
●
●
●
●
●
●
●
●
</figure>
<equation confidence="0.986391907563025">
● ● ● ● ●
● ●
● ● ● ●
● ● ●
● ● ●
● ● ● ●
● ● ●
● ●
● ● ● ● ●
● ●
●
●
● ● ● ● ● ●
● ● ●
● ● ● ● ● ● ● ● ● ●
● ●
● ● ● ● ●
● ●
● ● ● ●
● ● ● ● ● ●
● ● ●
●
● ● ●
● ● ● ●
● ● ●
● ● ● ●
● ● ● ● ● ● ● ● ●●
● ● ● ● ●
● ●
● ● ●
● ● ● ● ● ● ● ●
● ●
● ●
● ● ●
● ● ● ● ●
● ●
● ●
● ● ● ● ●
● ● ● ●
● ● ● ●
●
●
● ●
●
● ●
● ● ● ● ● ● ● ● ● ● ●
● ● ●
● ● ●
● ● ● ● ● ● ● ●
● ●
●
● ● ● ● ● ●
● ● ● ●
● ● ● ●
● ● ● ● ●
● ●
● ●
● ● ● ● ● ● ●
●
● ●
● ● ● ● ● ●
● ● ●
● ● ●
● ● ● ● ●●
●
● ● ● ●
● ● ● ● ● ●
● ● ● ●
● ●
● ● ● ●
● ● ● ● ●●
● ●
● ● ●
● ●
● ● ●
● ●
● ●
● ●
● ● ● ●
● ● ●
● ● ●
●
●
● ● ● ● ● ● ● ● ●
●
●
● ●
● ● ● ●
● ● ● ●
● ● ●
● ●
● ●
● ● ●
● ● ● ●
● ● ● ● ● ●
● ●
● ● ● ● ●
● ●
●
●
● ● ●
● ●
● ● ● ●
● ● ● ●
● ● ● ● ● ● ● ● ●
●
● ● ● ● ● ●
● ●
● ● ● ● ● ● ● ●
● ● ● ●
● ● ●● ●
● ● ●
● ● ●
● ● ● ● ●
● ● ●
● ● ● ●
● ● ● ●
● ●
● ● ● ●
</equation>
<figure confidence="0.836073178082192">
●
●
● ●
● ● ● ●
● ● ● ● ●
● ● ●
● ●
● ● ● ●
● ●
● ● ● ● ●
●
● ● ● ● ● ● ● ●
● ● ● ● ●
● ●
● ● ●
● ● ● ●●
● ● ●
● ●
● ●
● ● ● ● ● ●
● ● ● ● ●
● ● ●
●
● ● ●
● ● ●
● ●
● ●
● ● ● ●
●
● ● ● ●
●
●
●
●
●
●
● ●
●
● ●
● ●●
●
● ●
●
●
● ● ● ●
● ●
● ● ●
● ● ● ●
● ● ●
● ● ●
● ●
● ●
● ●
● ●
● ●
●
●
●
●
●
●
●
●
●
●
●
All References
100
25
75
50
0
●
</figure>
<equation confidence="0.980517282828283">
● ● ● ●
● ● ●
● ● ● ● ● ● ●
● ● ● ●●
●
●
●
● ● ● ● ●
● ● ● ● ●
● ● ● ● ●
● ●
● ● ● ● ● ●
● ● ● ● ● ● ● ● ●
● ●
● ● ● ●
●
● ● ● ● ●
● ● ● ● ● ●
● ● ● ● ● ● ● ● ●
● ● ● ●●
● ●
● ●
● ●
● ● ●
● ● ● ● ● ●
● ● ● ● ●
● ● ● ●
● ● ●
● ● ● ●
● ● ● ● ● ● ●
● ● ●
● ● ● ● ● ● ●
● ● ● ● ● ●
●
● ● ●
● ● ● ● ● ● ● ●
● ●
● ● ●
● ● ● ● ● ●
● ● ●
● ● ●●
● ● ● ● ● ● ● ● ● ●
●● ● ● ●
● ● ● ● ●
● ●
● ●
● ● ●
●
● ● ● ● ●
● ●
● ● ●
● ● ● ● ● ●
● ● ●
● ● ●
● ● ●
● ●
● ● ● ● ● ● ●
● ● ●
● ● ●
● ● ●
● ● ●
● ● ●
● ●
● ● ●
● ● ● ●
● ● ● ● ●
● ● ● ● ● ●
● ● ● ● ● ●
●
● ●
●
● ● ●
● ●
● ● ● ● ● ●
● ● ● ● ● ●
● ● ● ●
● ● ●
● ● ● ● ● ●
● ● ●
● ●
● ● ●
● ● ●
● ● ● ●●
● ● ●
● ●
● ●
● ● ● ●
● ● ● ●
● ●
● ● ● ● ● ●
● ● ● ● ● ●
● ●
● ● ●
● ● ● ●
● ● ● ●
● ● ● ● ●
● ● ● ●
● ●
● ● ●
</equation>
<figure confidence="0.991646897435898">
● ●
●
●
● ● ●
●
●
●
● ● ● ● ●
● ● ●
● ●
● ●
● ● ● ●
● ● ● ● ●
● ● ● ● ●
● ●
● ● ● ●
● ● ● ●
●
● ● ●●
● ● ● ●
● ●
● ● ● ●
● ● ●
● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
● ● ● ● ●
●
● ● ● ●
● ● ● ●
● ●
● ● ● ● ●
● ● ● ● ● ●
●
●
● ● ● ● ● ●
● ● ● ●
● ● ●
●
● ● ● ● ● ● ● ●
● ●
● ● ● ● ●
● ● ● ●
● ●
● ● ●
● ● ● ●
● ● ●
●
● ● ● ● ●
● ● ●
● ● ● ●
● ● ●
● ● ●
● ●
●
●
●
●
● ● ● ●
● ● ●
● ●
●
●
●
●
●
●
● ● ● ● ●
●
●
●
●
●
●
●
Maximum 100
75
50
25
0
</figure>
<bodyText confidence="0.995953181818182">
reference, it would have matched the translation
exactly and incurred a low loss.
Nakov et al. (2012) suggested extensions to
BLEU+1 that were subsequently found to improve
accuracy in the single-reference condition (Gimpel
and Smith, 2012a). Repeating the min/max calcula-
tions with the most effective extensions (according
to Gimpel and Smith (2012a)) we observe lower
variance (M = 17.32, SD = 10.68). These exten-
sions are very simple, so a more sophisticated noise
model is a promising future direction.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.998418333333333">
We review work on phrase-based discriminative fea-
ture sets that influence decoder search, and domain
adaptation with features.11
</bodyText>
<subsectionHeader confidence="0.977099">
7.1 Feature Sets
</subsectionHeader>
<bodyText confidence="0.975425641025641">
Variants of some extended features are scattered
throughout previous work: unfiltered lexicalized
rule indicators and alignments (Liang et al., 2006a);
rule shape (Hopkins and May, 2011); rule orien-
tation (Liang et al., 2006b; Cherry, 2013); target
unigram class (Ammar et al., 2013). We found
that other prior features did not improve translation:
higher-order target lexical n-grams (Liang et al.,
2006a; Watanabe et al., 2007; Gimpel and Smith,
2012b), higher-order target class n-grams (Ammar
et al., 2013), target word insertion (Watanabe et al.,
2007; Chiang et al., 2009), and many other unpub-
lished ideas transmitted through received wisdom.
To our knowledge, Yu et al. (2013) were the first
to experiment with non-local (derivation) features
for phrase-based MT. They added discriminative
rule features conditioned on target context. This is
a good idea that we plan to explore. However, they
do not mention if their non-local features declare
recombination state. Our empirical experience is
that non-local features are less effective when they
do not influence recombination.
Liang et al. (2006a) proposed replacing lexical
items with supervised part-of-speech (POS) tags to
reduce sparsity. This is a natural idea that lay dor-
mant until recently. Ammar et al. (2013) incorpo-
rated unigram and bigram target class features. Yu
et al. (2013) used word classes as backoff features to
reduce overfitting. Wuebker et al. (2013) replaced
all lexical items in the bitext and monolingual data
with classes, and estimated the dense feature set.
11Space limitations preclude discussion of re-ranking fea-
tures.
Then they added these dense class-based features
to the baseline lexicalized system. Finally, Cherry
(2013) experimented with class-based hierarchical
reordering features. However, his features used a
bespoke representation rather than the simple full
rule string that we use.
</bodyText>
<subsectionHeader confidence="0.97608">
7.2 Domain Adaptation with Features
</subsectionHeader>
<bodyText confidence="0.999949961538461">
Both Clark et al. (2012) and Wang et al. (2012) aug-
mented the baseline dense feature set with domain
labels. They each showed modest improvements
for several language pairs. However, neither incor-
porated a notion of a default prior domain.
Liu et al. (2012) investigated local adaption of
the log-linear scores by selecting comparable bitext
examples for a given source input. After selecting
a small local corpus, their algorithm then performs
several online update steps—starting from a glob-
ally tuned weight vector—prior to decoding the
input. The resulting model is effectively a locally
weighted, domain-adapted classifier.
Su et al. (2012) proposed domain adaptation
via monolingual source resources much as we use
in-domain monolingual corpora for data selection.
They labeled each bitext sentence with a topic using
a Hidden Topic Markov Model (HTMM) Gruber
et al. (2007). Source topic information was then
mixed into the translation model dense feature cal-
culations. This work follows Chiang et al. (2011),
who present a similar technique but using the same
gold NIST labels that we use. Hasler et al. (2012)
extended these ideas to a discriminative sparse fea-
ture set by augmenting both rule and unigram align-
ment features with HTMM topic information.
</bodyText>
<sectionHeader confidence="0.997663" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999958785714286">
This paper makes four major contributions. First,
we introduced extended features for phrase-based
MT that exceeded both dense and feature-rich base-
lines. Second, we specialized the features to source
domains, further extending the gains. Third, we
showed that online expected BLEU is faster and
more stable than online PRO for extended fea-
tures. Finally, we released fast, scalable, language-
independent tools for implementing the feature set.
Our work should help practitioners quickly estab-
lish higher baselines on the way to more targeted
linguistic features. However, our analysis showed
that reference choice may restrain otherwise justifi-
able enthusiasm for feature-rich MT.
</bodyText>
<page confidence="0.997047">
474
</page>
<bodyText confidence="0.997131555555555">
Acknowledgments We thank John DeNero for comments on
an earlier version of this work. The first author is supported by
a National Science Foundation Graduate Research Fellowship.
This work was supported by the Defense Advanced Research
Projects Agency (DARPA) Broad Operational Language Trans-
lation (BOLT) program through IBM. Any opinions, findings,
and conclusions or recommendations expressed in this mate-
rial are those of the author(s) and do not necessarily reflect the
view of DARPA or the US government.
</bodyText>
<sectionHeader confidence="0.999119" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999957318681319">
W. Ammar, V. Chahuneau, M. Denkowski, G. Hanne-
man, W. Ling, A. Matthews, et al. 2013. The CMU
machine translation systems at WMT 2013: Syntax,
synthetic translation options, and pseudo-references.
In WMT.
E. Biçici and D. Yuret. 2011. Instance selection for
machine translation using feature decay algorithms.
In WMT.
O. Bojar, M. Machácek, A. Tamchyna, and D. Zeman.
2013. Scratching the surface of possible translations.
In I. Habernal and V. Matousek, editors, Text, Speech,
and Dialogue, volume 8082 of Lecture Notes in Com-
puter Science, pages 465–474. Springer Berlin Hei-
delberg.
P-C. Chang, M. Galley, and C. D. Manning. 2008.
Optimizing Chinese word segmentation for machine
translation performance. In WMT.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In HLT-NAACL.
C. Cherry. 2013. Improved reordering for phrase-based
translation using sparse features. In HLT-NAACL.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
HLT-NAACL.
D. Chiang, S. DeNeefe, and M. Pust. 2011. Two easy
improvements to lexical weighting. In ACL.
C. Christodoulopoulos, S. Goldwater, and M. Steedman.
2010. Two decades of unsupervised POS induction:
How far have we come? In EMNLP.
J. H. Clark, A. Lavie, and C. Dyer. 2012. One system,
many domains: Open-domain statistical machine
translation via feature augmentation. In AMTA.
H. Daumé III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
M. Dreyer and D. Marcu. 2012. HyTER: Meaning-
equivalent semantics for translation evaluation. In
NAACL.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2899–2934.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121–2159.
N. Durrani, B. Haddow, K. Heafield, and P. Koehn.
2013. Edinburgh’s machine translation systems for
European language pairs. In WMT.
J. R. Finkel and C. D. Manning. 2009. Hierarchical
bayesian domain adaptation. In HLT-NAACL.
J. Gao and X. He. 2013. Training MRF-based phrase
translation models using gradient ascent. In NAACL.
K. Gimpel and N. A. Smith. 2012a. Addendum to
structured ramp loss minimization for machine trans-
lation. Technical report, Language Technologies In-
stitute, Carnegie Mellon University.
K. Gimpel and N. A. Smith. 2012b. Structured ramp
loss minimization for machine translation. In HLT-
NAACL.
S. Green, D. Cer, K. Reschke, R. Voigt, J. Bauer,
S. Wang, and others. 2013a. Feature-rich phrase-
based translation: Stanford University’s submission
to the WMT 2013 translation task. In WMT.
S. Green, S. Wang, D. Cer, and C. D. Manning. 2013b.
Fast and adaptive online training of feature-rich trans-
lation models. In ACL.
S. Green, D. Cer, and C. D. Manning. 2014. Phrasal: A
toolkit for new directions in statistical machine trans-
lation. In WMT.
A. Gruber, Y. Weiss, and M. Rosen-Zvi. 2007. Hidden
topic markov models. In AISTATS.
E. Hasler, B. Haddow, and P. Koehn. 2012. Sparse
lexicalised features and topic adaptation for SMT. In
IWSLT.
K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013. Scalable modified Kneser-Ney language
model estimation. In ACL, Short Papers.
M. Hopkins and J. May. 2011. Tuning as ranking. In
EMNLP.
A. Irvine, J. Morgan, M. Carpuat, H. Daumé III, and
D. Munteanu. 2013. Measuring machine translation
errors in new domains. TACL, 1.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
P. Liang, A. Bouchard-Côté, D. Klein, and B. Taskar.
2006a. An end-to-end discriminative approach to
machine translation. In ACL.
P. Liang, B. Taskar, and D. Klein. 2006b. Alignment
by agreement. In NAACL.
</reference>
<page confidence="0.987559">
475
</page>
<reference confidence="0.999899">
P. Liang. 2005. Semi-supervised learning for natural
language. Master’s thesis, Massachusetts Institute of
Technology.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In COLING.
L. Liu, H. Cao, T. Watanabe, T. Zhao, M. Yu, and
C. Zhu. 2012. Locally training the log-linear model
for SMT. In EMNLP-CoNLL.
M. Maamouri, A. Bies, and S. Kulick. 2008. Enhanc-
ing the Arabic Treebank: A collaborative effort to-
ward new annotation guidelines. In LREC.
M. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19:313–330.
W. Monroe, S. Green, and C. D. Manning. 2014. Word
segmentation of informal Arabic with domain adap-
tation. In ACL, Short Papers.
P. Nakov, F. Guzman, and S. Vogel. 2012. Optimizing
for sentence-level BLEU+1 yields short translations.
In COLING.
P. Nakov, F. Guzmán, and S. Vogel. 2013. A tale about
PRO and monsters. In ACL, Short Papers.
F. J. Och and H. Ney. 2003. A systematic compari-
son of various statistical alignment models. Compu-
tational Linguistics, 29(1):19–51.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417–449.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing in MT.
In ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization.
G. Salton. 1958. The use of punctuation patterns in ma-
chine translation. Mechanical Translation, 5(1):16–
24, July.
J. Su, H. Wu, H. Wang, Y. Chen, X. Shi, H. Dong, and
Q. Liu. 2012. Translation model adaptation for sta-
tistical machine translation with monolingual topic
information. In ACL.
J. Uszkoreit and T. Brants. 2008. Distributed word clus-
tering for large scale class-based language modeling
in machine translation. In ACL-HLT.
W. Wang, K. Macherey, W. Macherey, F. J. Och, and
P. Xu. 2012. Improved domain adaptation for statis-
tical machine translation. In AMTA.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In EMNLP-CoNLL.
E. W. D. Whittaker and P. C. Woodland. 2001. Effi-
cient class-based language modelling for very large
vocabularies. In ICASSP.
J. Wuebker, S. Peitz, F. Rietig, and H. Ney. 2013.
Improving statistical machine translation with word
class models. In EMNLP.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
11(2):207–238.
H. Yu, L. Huang, H. Mi, and K. Zhao. 2013. Max-
violation perceptron and forced decoding for scalable
MT training. In EMNLP.
</reference>
<page confidence="0.999107">
476
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.981935">
<title confidence="0.9986815">An Empirical Comparison of Features and for Phrase-based Machine Translation</title>
<author confidence="0.998066">Daniel Cer Green</author>
<author confidence="0.998066">D Manning</author>
<affiliation confidence="0.999925">Computer Science Department, Stanford University</affiliation>
<email confidence="0.999064">spenceg@stanford.edu</email>
<email confidence="0.999064">danielcer@stanford.edu</email>
<email confidence="0.999064">manning@stanford.edu</email>
<abstract confidence="0.999432086956522">Scalable discriminative training methods are now broadly available for estimating phrase-based, feature-rich translation models. However, the sparse feature sets typically appearing in research evaluations are less attractive than standard dense features such as language and translation model probabilities: they often overfit, do not generalize, or require complex and slow feaextractors. This paper introduces exwhich are more specific than dense features yet more general than lexicalized sparse features. Large-scale experiments show that extended features yield robust BLEU gains for both Arabic-English and Chinese-English relative to a strong feature-rich baseline. We also specialize the feature set to specific data domains, identify an objective function that is less prone to overfitting, and release fast, scalable, and language-independent tools for implementing the features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Ammar</author>
<author>V Chahuneau</author>
<author>M Denkowski</author>
<author>G Hanneman</author>
<author>W Ling</author>
<author>A Matthews</author>
</authors>
<title>The CMU machine translation systems at WMT 2013: Syntax, synthetic translation options, and pseudo-references.</title>
<date>2013</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="10031" citStr="Ammar et al., 2013" startWordPosition="1581" endWordPosition="1584">rom frequency filtering on the tuning data; see section 6.1). The feature is local. Class-based rule indicator Word classes abstract over lexical items. For each rule r, a prototype that abstracts over many rules can be built by concatenating {cp(w) : w E f(r)} with {cp(w) : w E e(r)}. For example, suppose that Arabic class 492 consists primarily of Arabic present tense verbs and class 59 contains English auxiliaries. Then the model might penalize a rule prototype like 492&gt;59_59, which drops the verb. This template fires an indicator for each rule prototype and is local. Target unigram class (Ammar et al., 2013) Target lexical items with similar syntactic and semantic properties may have very different frequencies in the training data. These frequencies will influence the dense features. For example, in one of our English class mappings the following words map to the same class: word class freq. surface-to-surface air-to-air ground-to-air The classes capture common linguistic attributes of these words, which is the motivation for a full classbased LM. Learning unigram weights directly is surprisingly effective and does not require building another LM. This template fires a separate indicator for each</context>
<context position="16999" citStr="Ammar et al., 2013" startWordPosition="2692" endWordPosition="2695">nd its recombination state p is the signed distortion. 3.6 Feature Dependencies While unigram counts are trivial to compute, the same is not necessarily true of the word-to-class mapping co. Standard algorithms run in O(n2), where n = |V |. Table 1 shows an evaluation of standard implementations of several popular algorithms: Brown et al. (1992) implemented by Liang 469 (2005); Clark (2003) without the morphological prior, which increases training time dramatically; and the implementation of Och (1999) that comes with the GIZA++ word aligner. The latter has been used recently for MT features (Ammar et al., 2013; Cherry, 2013; Yu et al., 2013). In a broad survey, Christodoulopoulos et al. (2010) found that for several downstream tasks, most word clustering algorithms—including Brown and Clark—result in similar task accuracy. For our large-scale setting, the primary issue is then the time to estimate co. For large corpora the existing implementations may require days or weeks, making our feature set less practical than the traditional dense MT features. Consequently, we re-implemented the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Us</context>
<context position="35712" citStr="Ammar et al., 2013" startWordPosition="6461" endWordPosition="6464">l and Smith (2012a)) we observe lower variance (M = 17.32, SD = 10.68). These extensions are very simple, so a more sophisticated noise model is a promising future direction. 7 Related Work We review work on phrase-based discriminative feature sets that influence decoder search, and domain adaptation with features.11 7.1 Feature Sets Variants of some extended features are scattered throughout previous work: unfiltered lexicalized rule indicators and alignments (Liang et al., 2006a); rule shape (Hopkins and May, 2011); rule orientation (Liang et al., 2006b; Cherry, 2013); target unigram class (Ammar et al., 2013). We found that other prior features did not improve translation: higher-order target lexical n-grams (Liang et al., 2006a; Watanabe et al., 2007; Gimpel and Smith, 2012b), higher-order target class n-grams (Ammar et al., 2013), target word insertion (Watanabe et al., 2007; Chiang et al., 2009), and many other unpublished ideas transmitted through received wisdom. To our knowledge, Yu et al. (2013) were the first to experiment with non-local (derivation) features for phrase-based MT. They added discriminative rule features conditioned on target context. This is a good idea that we plan to expl</context>
</contexts>
<marker>Ammar, Chahuneau, Denkowski, Hanneman, Ling, Matthews, 2013</marker>
<rawString>W. Ammar, V. Chahuneau, M. Denkowski, G. Hanneman, W. Ling, A. Matthews, et al. 2013. The CMU machine translation systems at WMT 2013: Syntax, synthetic translation options, and pseudo-references. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Biçici</author>
<author>D Yuret</author>
</authors>
<title>Instance selection for machine translation using feature decay algorithms.</title>
<date>2011</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="19665" citStr="Biçici and Yuret, 2011" startWordPosition="3117" endWordPosition="3120">sted defaults: Brown, default; Clark, 10 iterations, frequency cutoff τ = 5; Och, 10 iterations. Our implementation: PredictiveFull, 30 iterations, τ = 0; Predictive, 30 iterations, τ = 5. labels. The in-domain rule sets need not be disjoint since some rules might be useful across domains. This paper explores the following approach: we choose one of the M domains as the default. Next, we collect some source sentences for each of the M − 1 remaining domains. Using these examples we then identify in-domain sentence pairs in the bitext via data selection, in our case the feature decay algorithm (Biçici and Yuret, 2011). Finally, our rule extractor adds domain labels to all rules extracted from each selected sentence pair. Crucially, these labels do not influence which rules are extracted or how they are scored. The resulting phrase table contains the same rules, but with a few additional annotations. Our method assumes domain labels for each source input to be decoded. Our experiments utilize gold, document-level labels, but accurate sentencelevel domain classifiers exist (Wang et al., 2012). 4.1 Augmentation of Extended Features Irvine et al. (2013) showed that lexical selection is the most quantifiable an</context>
</contexts>
<marker>Biçici, Yuret, 2011</marker>
<rawString>E. Biçici and D. Yuret. 2011. Instance selection for machine translation using feature decay algorithms. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>M Machácek</author>
<author>A Tamchyna</author>
<author>D Zeman</author>
</authors>
<title>Scratching the surface of possible translations.</title>
<date>2013</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>8082</volume>
<pages>465--474</pages>
<editor>In I. Habernal and V. Matousek, editors, Text, Speech, and Dialogue,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="30444" citStr="Bojar et al., 2013" startWordPosition="4825" endWordPosition="4828">ple-reference tuning. Table 6: Ar-En learning comparisons. tuning data. Only features that fire more than some threshold are admitted into the feature set. Table 6b shows that for our new feature set, Li regularization—which simply requires setting a regularization strength parameter—is more effective than frequency cutoffs. References Few MT data sets supply multiple references. Even when they do, those references are but a sample from a larger pool of possible translations. This observation has motivated attempts at generating lattices of translations for evaluation (Dreyer and Marcu, 2012; Bojar et al., 2013). But evaluation is only part of the problem. Table 6c shows that the DENSE model, which has only a few features to describe the data, is little affected by the elimination of references. In contrast, the feature-rich model degrades significantly. This may account for the underperformance of features in single-reference settings like WMT (Durrani et al., 2013; Green et al., 2013a). The next section explores the impact of references further. 6.2 Reference Variance We took the DENSE Ar-En output for the dev data, which has four references, and computed the sentence-level BLEU+1 with respect to e</context>
</contexts>
<marker>Bojar, Machácek, Tamchyna, Zeman, 2013</marker>
<rawString>O. Bojar, M. Machácek, A. Tamchyna, and D. Zeman. 2013. Scratching the surface of possible translations. In I. Habernal and V. Matousek, editors, Text, Speech, and Dialogue, volume 8082 of Lecture Notes in Computer Science, pages 465–474. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P-C Chang</author>
<author>M Galley</author>
<author>C D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="23246" citStr="Chang et al., 2008" startWordPosition="3674" endWordPosition="3677">ach condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. DENSE is the same baseline as Green et al. 5We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). 6Data sources: tune, MT023568; dev, MT04; dev-dom, domain adaptation dev set is MT04 and all wb and bn data from LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En); test2, Progress0809 which was revealed in the OpenMT 2012 evaluation; test3, MetricsMATR08-10. 7System settings: distortion limit of 5, cube pruning beam size of 1200, maximum phrase length of 7. 8Other learning settings: 16 threads, mini-batch size of 20; Ll regularization strength λ = 0.001; learning rate η0 = 0.02; initialization of LM to 0.5, word penalty to -1.</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>P-C. Chang, M. Galley, and C. D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
<author>G Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="1920" citStr="Cherry and Foster (2012)" startWordPosition="263" endWordPosition="266">e now algorithms for every taste: probabilistic and distribution-free, online and batch, regularized and unregularized. Technical differences aside, the papers that apply these algorithms to phrase-based translation often share a curious empirical characteristic: the algorithms support extra features, but the features do not significantly improve translation. For example, Hopkins and May (2011) showed that PRO with some simple ad hoc features only exceeds the baseline on one of three language pairs. Gimpel and Smith (2012b) observed a similar result for both PRO and their ramp-loss algorithm. Cherry and Foster (2012) found that, at least in the batch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look. We introduce an effective extended feature set for phrase-based MT and identify a loss function that is less prone to overfitting. Extended features share three attractive characteristics with the s</context>
<context position="7062" citStr="Cherry and Foster, 2012" startWordPosition="1083" endWordPosition="1086"> metric that evaluates each candidate translation with respect to a set of one or more 1http://nlp.stanford.edu/software/phrasal references. The smooth loss function is `t(wt−1) = Ep(eJft;wt−,)[G(e)] 1 � � � Z exp wTφ(e&apos;, f) · G(e&apos;) e&apos;EEt (2) with normalization constant Z = Ee,EEt exp (wTφ(e&apos;, f)). The gradient gt for coordinate j is: gt = E[G(e)φj(e, ft)]− E[G(e)]E[φj(e, ft)] (3) To our knowledge, we are the first to experiment with the online version of this loss.2 When G(e) is sentence-level BLEU+1 (Lin and Och, 2004)—the setting in our experiments—this loss is also known as expected BLEU (Cherry and Foster, 2012). However, other metrics are possible. 3 Extended Phrase-based Features We divide our feature templates into five categories, which are well-known sources of error in phrasebased translation. The features are defined over derivations d = {riJD i=1, which are ordered sequences of rules r from the translation model. Define functions f(·) to be the source string of a rule or derivation and e(·) to be the target string. Local features can be extracted from individual rules and do not declare any state in the recombination map, thus for all local features i we have ρi = 0. Nonlocal features are def</context>
<context position="28324" citStr="Cherry and Foster (2012)" startWordPosition="4478" endWordPosition="4482">id not—increase significantly. We do not observe that behavior, at least from the perspective of BLEU. Table 5 separates out per-domain results. The web data appears to be the hardest domain. That is sensible given that broadcast news transcripts are more similar to newswire, the default domain, than web data. Moreover, inspection of the bitext sources revealed very little web data, so our automatic data selection is probably less effective. Accuracy on newswire actually increases slightly. 6 Analysis 6.1 Learning Loss Function In a now classic empirical comparison of batch tuning algorithms, Cherry and Foster (2012) showed that PRO and expected BLEU Ar-En test1 test2 bn test3 wb nw wb nw wb nw EF 59.78 39.55 51.69 38.80 30.39 37.59 20.58 EFD 60.21 40.38 51.76 38.77 31.63 38.18 22.37 Zh-En EF 34.56 21.94 17.38 12.07 3.04 17.42 12.83 EFD 34.87 21.82 17.96 12.66 3.01 17.74 13.80 Table 5: Per-domain results (uncased BLEU-4 %). Here bold simply indicates the maximum in each column. Model abbreviations: EF is EXT+FiLT and EFD is EXT+FiLT+Dom. yielded similar translation quality results. In contrast, Table 6a shows significant differences between these loss functions. First, expected BLEU can be computed faster</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>C. Cherry and G. Foster. 2012. Batch tuning strategies for statistical machine translation. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
</authors>
<title>Improved reordering for phrase-based translation using sparse features.</title>
<date>2013</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="2116" citStr="Cherry (2013)" startWordPosition="297" endWordPosition="298">nslation often share a curious empirical characteristic: the algorithms support extra features, but the features do not significantly improve translation. For example, Hopkins and May (2011) showed that PRO with some simple ad hoc features only exceeds the baseline on one of three language pairs. Gimpel and Smith (2012b) observed a similar result for both PRO and their ramp-loss algorithm. Cherry and Foster (2012) found that, at least in the batch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look. We introduce an effective extended feature set for phrase-based MT and identify a loss function that is less prone to overfitting. Extended features share three attractive characteristics with the standard Moses dense features (Koehn et al., 2007): ease of implementation, language independence, and independence from ancillary corpora like treebanks. In our experiments, they do not overfit an</context>
<context position="17013" citStr="Cherry, 2013" startWordPosition="2696" endWordPosition="2697"> state p is the signed distortion. 3.6 Feature Dependencies While unigram counts are trivial to compute, the same is not necessarily true of the word-to-class mapping co. Standard algorithms run in O(n2), where n = |V |. Table 1 shows an evaluation of standard implementations of several popular algorithms: Brown et al. (1992) implemented by Liang 469 (2005); Clark (2003) without the morphological prior, which increases training time dramatically; and the implementation of Och (1999) that comes with the GIZA++ word aligner. The latter has been used recently for MT features (Ammar et al., 2013; Cherry, 2013; Yu et al., 2013). In a broad survey, Christodoulopoulos et al. (2010) found that for several downstream tasks, most word clustering algorithms—including Brown and Clark—result in similar task accuracy. For our large-scale setting, the primary issue is then the time to estimate co. For large corpora the existing implementations may require days or weeks, making our feature set less practical than the traditional dense MT features. Consequently, we re-implemented the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Br</context>
<context position="35669" citStr="Cherry, 2013" startWordPosition="6456" endWordPosition="6457">ective extensions (according to Gimpel and Smith (2012a)) we observe lower variance (M = 17.32, SD = 10.68). These extensions are very simple, so a more sophisticated noise model is a promising future direction. 7 Related Work We review work on phrase-based discriminative feature sets that influence decoder search, and domain adaptation with features.11 7.1 Feature Sets Variants of some extended features are scattered throughout previous work: unfiltered lexicalized rule indicators and alignments (Liang et al., 2006a); rule shape (Hopkins and May, 2011); rule orientation (Liang et al., 2006b; Cherry, 2013); target unigram class (Ammar et al., 2013). We found that other prior features did not improve translation: higher-order target lexical n-grams (Liang et al., 2006a; Watanabe et al., 2007; Gimpel and Smith, 2012b), higher-order target class n-grams (Ammar et al., 2013), target word insertion (Watanabe et al., 2007; Chiang et al., 2009), and many other unpublished ideas transmitted through received wisdom. To our knowledge, Yu et al. (2013) were the first to experiment with non-local (derivation) features for phrase-based MT. They added discriminative rule features conditioned on target contex</context>
<context position="37147" citStr="Cherry (2013)" startWordPosition="6681" endWordPosition="6682">a) proposed replacing lexical items with supervised part-of-speech (POS) tags to reduce sparsity. This is a natural idea that lay dormant until recently. Ammar et al. (2013) incorporated unigram and bigram target class features. Yu et al. (2013) used word classes as backoff features to reduce overfitting. Wuebker et al. (2013) replaced all lexical items in the bitext and monolingual data with classes, and estimated the dense feature set. 11Space limitations preclude discussion of re-ranking features. Then they added these dense class-based features to the baseline lexicalized system. Finally, Cherry (2013) experimented with class-based hierarchical reordering features. However, his features used a bespoke representation rather than the simple full rule string that we use. 7.2 Domain Adaptation with Features Both Clark et al. (2012) and Wang et al. (2012) augmented the baseline dense feature set with domain labels. They each showed modest improvements for several language pairs. However, neither incorporated a notion of a default prior domain. Liu et al. (2012) investigated local adaption of the log-linear scores by selecting comparable bitext examples for a given source input. After selecting a</context>
</contexts>
<marker>Cherry, 2013</marker>
<rawString>C. Cherry. 2013. Improved reordering for phrase-based translation using sparse features. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>K Knight</author>
<author>W Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="36007" citStr="Chiang et al., 2009" startWordPosition="6506" endWordPosition="6509">ptation with features.11 7.1 Feature Sets Variants of some extended features are scattered throughout previous work: unfiltered lexicalized rule indicators and alignments (Liang et al., 2006a); rule shape (Hopkins and May, 2011); rule orientation (Liang et al., 2006b; Cherry, 2013); target unigram class (Ammar et al., 2013). We found that other prior features did not improve translation: higher-order target lexical n-grams (Liang et al., 2006a; Watanabe et al., 2007; Gimpel and Smith, 2012b), higher-order target class n-grams (Ammar et al., 2013), target word insertion (Watanabe et al., 2007; Chiang et al., 2009), and many other unpublished ideas transmitted through received wisdom. To our knowledge, Yu et al. (2013) were the first to experiment with non-local (derivation) features for phrase-based MT. They added discriminative rule features conditioned on target context. This is a good idea that we plan to explore. However, they do not mention if their non-local features declare recombination state. Our empirical experience is that non-local features are less effective when they do not influence recombination. Liang et al. (2006a) proposed replacing lexical items with supervised part-of-speech (POS) </context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new features for statistical machine translation. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>S DeNeefe</author>
<author>M Pust</author>
</authors>
<title>Two easy improvements to lexical weighting.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="38367" citStr="Chiang et al. (2011)" startWordPosition="6864" endWordPosition="6867"> small local corpus, their algorithm then performs several online update steps—starting from a globally tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classifier. Su et al. (2012) proposed domain adaptation via monolingual source resources much as we use in-domain monolingual corpora for data selection. They labeled each bitext sentence with a topic using a Hidden Topic Markov Model (HTMM) Gruber et al. (2007). Source topic information was then mixed into the translation model dense feature calculations. This work follows Chiang et al. (2011), who present a similar technique but using the same gold NIST labels that we use. Hasler et al. (2012) extended these ideas to a discriminative sparse feature set by augmenting both rule and unigram alignment features with HTMM topic information. 8 Conclusion This paper makes four major contributions. First, we introduced extended features for phrase-based MT that exceeded both dense and feature-rich baselines. Second, we specialized the features to source domains, further extending the gains. Third, we showed that online expected BLEU is faster and more stable than online PRO for extended fe</context>
</contexts>
<marker>Chiang, DeNeefe, Pust, 2011</marker>
<rawString>D. Chiang, S. DeNeefe, and M. Pust. 2011. Two easy improvements to lexical weighting. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Christodoulopoulos</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Two decades of unsupervised POS induction: How far have we come?</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="17084" citStr="Christodoulopoulos et al. (2010)" startWordPosition="2706" endWordPosition="2709">ndencies While unigram counts are trivial to compute, the same is not necessarily true of the word-to-class mapping co. Standard algorithms run in O(n2), where n = |V |. Table 1 shows an evaluation of standard implementations of several popular algorithms: Brown et al. (1992) implemented by Liang 469 (2005); Clark (2003) without the morphological prior, which increases training time dramatically; and the implementation of Och (1999) that comes with the GIZA++ word aligner. The latter has been used recently for MT features (Ammar et al., 2013; Cherry, 2013; Yu et al., 2013). In a broad survey, Christodoulopoulos et al. (2010) found that for several downstream tasks, most word clustering algorithms—including Brown and Clark—result in similar task accuracy. For our large-scale setting, the primary issue is then the time to estimate co. For large corpora the existing implementations may require days or weeks, making our feature set less practical than the traditional dense MT features. Consequently, we re-implemented the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Brants (2008) (Predictive), which was originally developed for very large</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2010</marker>
<rawString>C. Christodoulopoulos, S. Goldwater, and M. Steedman. 2010. Two decades of unsupervised POS induction: How far have we come? In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Clark</author>
<author>A Lavie</author>
<author>C Dyer</author>
</authors>
<title>One system, many domains: Open-domain statistical machine translation via feature augmentation.</title>
<date>2012</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="37377" citStr="Clark et al. (2012)" startWordPosition="6712" endWordPosition="6715">res. Yu et al. (2013) used word classes as backoff features to reduce overfitting. Wuebker et al. (2013) replaced all lexical items in the bitext and monolingual data with classes, and estimated the dense feature set. 11Space limitations preclude discussion of re-ranking features. Then they added these dense class-based features to the baseline lexicalized system. Finally, Cherry (2013) experimented with class-based hierarchical reordering features. However, his features used a bespoke representation rather than the simple full rule string that we use. 7.2 Domain Adaptation with Features Both Clark et al. (2012) and Wang et al. (2012) augmented the baseline dense feature set with domain labels. They each showed modest improvements for several language pairs. However, neither incorporated a notion of a default prior domain. Liu et al. (2012) investigated local adaption of the log-linear scores by selecting comparable bitext examples for a given source input. After selecting a small local corpus, their algorithm then performs several online update steps—starting from a globally tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classif</context>
</contexts>
<marker>Clark, Lavie, Dyer, 2012</marker>
<rawString>J. H. Clark, A. Lavie, and C. Dyer. 2012. One system, many domains: Open-domain statistical machine translation via feature augmentation. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daumé</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<marker>Daumé, 2007</marker>
<rawString>H. Daumé III. 2007. Frustratingly easy domain adaptation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dreyer</author>
<author>D Marcu</author>
</authors>
<title>HyTER: Meaningequivalent semantics for translation evaluation.</title>
<date>2012</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="30423" citStr="Dreyer and Marcu, 2012" startWordPosition="4821" endWordPosition="4824">88 (c) Single- vs. multiple-reference tuning. Table 6: Ar-En learning comparisons. tuning data. Only features that fire more than some threshold are admitted into the feature set. Table 6b shows that for our new feature set, Li regularization—which simply requires setting a regularization strength parameter—is more effective than frequency cutoffs. References Few MT data sets supply multiple references. Even when they do, those references are but a sample from a larger pool of possible translations. This observation has motivated attempts at generating lattices of translations for evaluation (Dreyer and Marcu, 2012; Bojar et al., 2013). But evaluation is only part of the problem. Table 6c shows that the DENSE model, which has only a few features to describe the data, is little affected by the elimination of references. In contrast, the feature-rich model degrades significantly. This may account for the underperformance of features in single-reference settings like WMT (Durrani et al., 2013; Green et al., 2013a). The next section explores the impact of references further. 6.2 Reference Variance We took the DENSE Ar-En output for the dev data, which has four references, and computed the sentence-level BLE</context>
</contexts>
<marker>Dreyer, Marcu, 2012</marker>
<rawString>M. Dreyer and D. Marcu. 2012. HyTER: Meaningequivalent semantics for translation evaluation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>Y Singer</author>
</authors>
<title>Efficient online and batch learning using forward backward splitting.</title>
<date>2009</date>
<journal>JMLR,</journal>
<pages>10--2899</pages>
<contexts>
<context position="5759" citStr="Duchi and Singer, 2009" startWordPosition="867" endWordPosition="870">t is, each coordinate in ρ represents the state of the corresponding coordinate in φ. For example, suppose that φj is the log probability produced by the n-gram language model (LM). Then ρj would be the appropriate LM history. Recall that recombination collapses derivations with equivalent recombination maps during search and thus affects learning. This issue significantly influences feature design. To learn w, we follow the online procedure of Green et al. (2013b), who calculate gradient steps with AdaGrad (Duchi et al., 2011) and perform feature selection via L1 regularization in the FOBOS (Duchi and Singer, 2009) framework. This procedure accommodates any loss function for which a subgradient can be computed. Green et al. (2013b) used a PRO objective (Hopkins and May, 2011) with a logistic (surrogate) loss function. However, later results showed overfitting (Green et al., 2013a), and we found that their online variant of PRO tends to produce short translations like its batch counterpart (Nakov et al., 2013). Moreover, PRO requires sampling, making it slow to compute. To address these shortcomings, we explore an online variant of expected error (Och, 2003, Eq.7). Let Et = {eiJni=1 be a scored n-best li</context>
</contexts>
<marker>Duchi, Singer, 2009</marker>
<rawString>J. Duchi and Y. Singer. 2009. Efficient online and batch learning using forward backward splitting. JMLR, 10:2899–2934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>E Hazan</author>
<author>Y Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>JMLR,</journal>
<pages>12--2121</pages>
<contexts>
<context position="5669" citStr="Duchi et al., 2011" startWordPosition="852" endWordPosition="855">s also a function ρ(e, f) E Rd that produces a recombination map for the features. That is, each coordinate in ρ represents the state of the corresponding coordinate in φ. For example, suppose that φj is the log probability produced by the n-gram language model (LM). Then ρj would be the appropriate LM history. Recall that recombination collapses derivations with equivalent recombination maps during search and thus affects learning. This issue significantly influences feature design. To learn w, we follow the online procedure of Green et al. (2013b), who calculate gradient steps with AdaGrad (Duchi et al., 2011) and perform feature selection via L1 regularization in the FOBOS (Duchi and Singer, 2009) framework. This procedure accommodates any loss function for which a subgradient can be computed. Green et al. (2013b) used a PRO objective (Hopkins and May, 2011) with a logistic (surrogate) loss function. However, later results showed overfitting (Green et al., 2013a), and we found that their online variant of PRO tends to produce short translations like its batch counterpart (Nakov et al., 2013). Moreover, PRO requires sampling, making it slow to compute. To address these shortcomings, we explore an o</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Durrani</author>
<author>B Haddow</author>
<author>K Heafield</author>
<author>P Koehn</author>
</authors>
<title>Edinburgh’s machine translation systems for European language pairs.</title>
<date>2013</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="20694" citStr="Durrani et al., 2013" startWordPosition="3268" endWordPosition="3271">s, but accurate sentencelevel domain classifiers exist (Wang et al., 2012). 4.1 Augmentation of Extended Features Irvine et al. (2013) showed that lexical selection is the most quantifiable and perhaps most common source of error in phrase-based domain adaptation. Our development experiments seemed to confirm this hypothesis as augmentation of the class-based and non-lexical (e.g., Rule shape) features did not reduce error. Therefore, we only augment the lexicalized features: rule indicators and orientations, and word alignments. 4.2 Domain-Specific Feature Templates In-domain Rule Indicator (Durrani et al., 2013) An indicator for each rule that matches the input domain. This template fires a generic in-domain indicator and a domain-specific indicator (e.g., the features might be indomain and indomain-nw). The feature is local. Adjacent Rule Indicator Indicators for adjacent in-domain rules. This template also fires both generic and domain-specific features. The feature is non-local and the state is a boolean indicating if the last rule in a partial derivation is in-domain. 5 Experiments We evaluate and analyze our feature set under a variety of large-scale experimental conditions including multiple do</context>
<context position="30805" citStr="Durrani et al., 2013" startWordPosition="4882" endWordPosition="4885">y multiple references. Even when they do, those references are but a sample from a larger pool of possible translations. This observation has motivated attempts at generating lattices of translations for evaluation (Dreyer and Marcu, 2012; Bojar et al., 2013). But evaluation is only part of the problem. Table 6c shows that the DENSE model, which has only a few features to describe the data, is little affected by the elimination of references. In contrast, the feature-rich model degrades significantly. This may account for the underperformance of features in single-reference settings like WMT (Durrani et al., 2013; Green et al., 2013a). The next section explores the impact of references further. 6.2 Reference Variance We took the DENSE Ar-En output for the dev data, which has four references, and computed the sentence-level BLEU+1 with respect to each reference. Figure 1a shows a point for each of the 1,075 translations. The horizontal axis is the minimum score with respect to any reference and the vertical axis is the maximum (BLEU has a maximum value of 1.0). Ideally, from the perspective of learn0 25 50 75 100 Minimum (a) Maximum vs. minimum BLEU+1 (%) 0 25 50 75 100 Maximum (b) BLEU+1 (%) according</context>
</contexts>
<marker>Durrani, Haddow, Heafield, Koehn, 2013</marker>
<rawString>N. Durrani, B. Haddow, K. Heafield, and P. Koehn. 2013. Edinburgh’s machine translation systems for European language pairs. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>C D Manning</author>
</authors>
<title>Hierarchical bayesian domain adaptation.</title>
<date>2009</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="18728" citStr="Finkel and Manning, 2009" startWordPosition="2961" endWordPosition="2964"> two changes reduce the vocabulary size by 71.1%. They also make the mapping co more robust to unseen events during translation decoding. For a conservative comparison to the other three algorithms, we include results without these two extensions (PredictiveFull).4 4 Domain Adaptation Features Feature augmentation is a simple yet effective domain adaptation technique (Daumé III, 2007). Suppose that the source data comes from M domains. Then for each original feature Oi, we add M additional features, one for each domain. The original feature Oi can be interpreted as a prior over the M domains (Finkel and Manning, 2009, fn.2). Most of the extended features are defined over rules, so the critical issue is how to identify indomain rules. The trick is to know which training sentence pairs are in-domain. Then we can annotate all rules extracted from these instances with domain 4For the baselines the training settings are the suggested defaults: Brown, default; Clark, 10 iterations, frequency cutoff τ = 5; Och, 10 iterations. Our implementation: PredictiveFull, 30 iterations, τ = 0; Predictive, 30 iterations, τ = 5. labels. The in-domain rule sets need not be disjoint since some rules might be useful across doma</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>J. R. Finkel and C. D. Manning. 2009. Hierarchical bayesian domain adaptation. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>X He</author>
</authors>
<title>Training MRF-based phrase translation models using gradient ascent.</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="8356" citStr="Gao and He (2013)" startWordPosition="1304" endWordPosition="1307">rameter or an index indicating a categorical value like an n-gram context. For each language, the extended feature templates require unigram counts and a word-to-class mapping ϕ : w H c for word w E V and class c E C. These can be extracted from any monolingual data; our experiments simply use both sides of the unaligned parallel training data. The features are language-independent, but we will use Arabic-English as a running example. 3.1 Lexical Choice Lexical choice features make more specific distinctions between target words than the dense translation model features (Koehn et al., 2003). 2Gao and He (2013) used stochastic gradient descent and expected BLEU to learn phrase table feature weights, but not the full translation model w. 467 Lexicalized rule indicator (Liang et al., 2006a) Some rules occur frequently enough that we can learn rule-specific weights that augment the dense translation model features. For example, our model learns the following rule indicator features and weights: H. AJ.ƒ @ ==&gt;. reasons -0.022 H. AJ.ƒ @ ==&gt;. reasons for 0.002 H. AJ.ƒ @ ==&gt;. the reasons for 0.016 These translations are all correct depending on context. When the plural noun H. AJ.ƒ @ ‘reasons’ appears in a </context>
</contexts>
<marker>Gao, He, 2013</marker>
<rawString>J. Gao and X. He. 2013. Training MRF-based phrase translation models using gradient ascent. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Addendum to structured ramp loss minimization for machine translation.</title>
<date>2012</date>
<tech>Technical report,</tech>
<institution>Language Technologies Institute, Carnegie Mellon University.</institution>
<contexts>
<context position="1823" citStr="Gimpel and Smith (2012" startWordPosition="248" endWordPosition="251">ve algorithm design for machine translation (MT) has lately been a booming enterprise. There are now algorithms for every taste: probabilistic and distribution-free, online and batch, regularized and unregularized. Technical differences aside, the papers that apply these algorithms to phrase-based translation often share a curious empirical characteristic: the algorithms support extra features, but the features do not significantly improve translation. For example, Hopkins and May (2011) showed that PRO with some simple ad hoc features only exceeds the baseline on one of three language pairs. Gimpel and Smith (2012b) observed a similar result for both PRO and their ramp-loss algorithm. Cherry and Foster (2012) found that, at least in the batch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look. We introduce an effective extended feature set for phrase-based MT and identify a loss function that </context>
<context position="35000" citStr="Gimpel and Smith, 2012" startWordPosition="6352" endWordPosition="6355"> ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● Maximum 100 75 50 25 0 reference, it would have matched the translation exactly and incurred a low loss. Nakov et al. (2012) suggested extensions to BLEU+1 that were subsequently found to improve accuracy in the single-reference condition (Gimpel and Smith, 2012a). Repeating the min/max calculations with the most effective extensions (according to Gimpel and Smith (2012a)) we observe lower variance (M = 17.32, SD = 10.68). These extensions are very simple, so a more sophisticated noise model is a promising future direction. 7 Related Work We review work on phrase-based discriminative feature sets that influence decoder search, and domain adaptation with features.11 7.1 Feature Sets Variants of some extended features are scattered throughout previous work: unfiltered lexicalized rule indicators and alignments (Liang et al., 2006a); rule shape (Hopkins</context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>K. Gimpel and N. A. Smith. 2012a. Addendum to structured ramp loss minimization for machine translation. Technical report, Language Technologies Institute, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Structured ramp loss minimization for machine translation.</title>
<date>2012</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="1823" citStr="Gimpel and Smith (2012" startWordPosition="248" endWordPosition="251">ve algorithm design for machine translation (MT) has lately been a booming enterprise. There are now algorithms for every taste: probabilistic and distribution-free, online and batch, regularized and unregularized. Technical differences aside, the papers that apply these algorithms to phrase-based translation often share a curious empirical characteristic: the algorithms support extra features, but the features do not significantly improve translation. For example, Hopkins and May (2011) showed that PRO with some simple ad hoc features only exceeds the baseline on one of three language pairs. Gimpel and Smith (2012b) observed a similar result for both PRO and their ramp-loss algorithm. Cherry and Foster (2012) found that, at least in the batch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look. We introduce an effective extended feature set for phrase-based MT and identify a loss function that </context>
<context position="35000" citStr="Gimpel and Smith, 2012" startWordPosition="6352" endWordPosition="6355"> ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● Maximum 100 75 50 25 0 reference, it would have matched the translation exactly and incurred a low loss. Nakov et al. (2012) suggested extensions to BLEU+1 that were subsequently found to improve accuracy in the single-reference condition (Gimpel and Smith, 2012a). Repeating the min/max calculations with the most effective extensions (according to Gimpel and Smith (2012a)) we observe lower variance (M = 17.32, SD = 10.68). These extensions are very simple, so a more sophisticated noise model is a promising future direction. 7 Related Work We review work on phrase-based discriminative feature sets that influence decoder search, and domain adaptation with features.11 7.1 Feature Sets Variants of some extended features are scattered throughout previous work: unfiltered lexicalized rule indicators and alignments (Liang et al., 2006a); rule shape (Hopkins</context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>K. Gimpel and N. A. Smith. 2012b. Structured ramp loss minimization for machine translation. In HLTNAACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Green</author>
<author>D Cer</author>
<author>K Reschke</author>
<author>R Voigt</author>
<author>J Bauer</author>
<author>S Wang</author>
</authors>
<title>and others. 2013a. Feature-rich phrasebased translation: Stanford University’s submission to the WMT 2013 translation task.</title>
<booktitle>In WMT.</booktitle>
<marker>Green, Cer, Reschke, Voigt, Bauer, Wang, </marker>
<rawString>S. Green, D. Cer, K. Reschke, R. Voigt, J. Bauer, S. Wang, and others. 2013a. Feature-rich phrasebased translation: Stanford University’s submission to the WMT 2013 translation task. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Green</author>
<author>S Wang</author>
<author>D Cer</author>
<author>C D Manning</author>
</authors>
<title>Fast and adaptive online training of feature-rich translation models.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2139" citStr="Green et al. (2013" startWordPosition="300" endWordPosition="303">re a curious empirical characteristic: the algorithms support extra features, but the features do not significantly improve translation. For example, Hopkins and May (2011) showed that PRO with some simple ad hoc features only exceeds the baseline on one of three language pairs. Gimpel and Smith (2012b) observed a similar result for both PRO and their ramp-loss algorithm. Cherry and Foster (2012) found that, at least in the batch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look. We introduce an effective extended feature set for phrase-based MT and identify a loss function that is less prone to overfitting. Extended features share three attractive characteristics with the standard Moses dense features (Koehn et al., 2007): ease of implementation, language independence, and independence from ancillary corpora like treebanks. In our experiments, they do not overfit and can be extracted effi</context>
<context position="5603" citStr="Green et al. (2013" startWordPosition="842" endWordPosition="845"> Z(f) is an appropriate normalizing constant. Assume that there is also a function ρ(e, f) E Rd that produces a recombination map for the features. That is, each coordinate in ρ represents the state of the corresponding coordinate in φ. For example, suppose that φj is the log probability produced by the n-gram language model (LM). Then ρj would be the appropriate LM history. Recall that recombination collapses derivations with equivalent recombination maps during search and thus affects learning. This issue significantly influences feature design. To learn w, we follow the online procedure of Green et al. (2013b), who calculate gradient steps with AdaGrad (Duchi et al., 2011) and perform feature selection via L1 regularization in the FOBOS (Duchi and Singer, 2009) framework. This procedure accommodates any loss function for which a subgradient can be computed. Green et al. (2013b) used a PRO objective (Hopkins and May, 2011) with a logistic (surrogate) loss function. However, later results showed overfitting (Green et al., 2013a), and we found that their online variant of PRO tends to produce short translations like its batch counterpart (Nakov et al., 2013). Moreover, PRO requires sampling, making </context>
<context position="30825" citStr="Green et al., 2013" startWordPosition="4886" endWordPosition="4889"> Even when they do, those references are but a sample from a larger pool of possible translations. This observation has motivated attempts at generating lattices of translations for evaluation (Dreyer and Marcu, 2012; Bojar et al., 2013). But evaluation is only part of the problem. Table 6c shows that the DENSE model, which has only a few features to describe the data, is little affected by the elimination of references. In contrast, the feature-rich model degrades significantly. This may account for the underperformance of features in single-reference settings like WMT (Durrani et al., 2013; Green et al., 2013a). The next section explores the impact of references further. 6.2 Reference Variance We took the DENSE Ar-En output for the dev data, which has four references, and computed the sentence-level BLEU+1 with respect to each reference. Figure 1a shows a point for each of the 1,075 translations. The horizontal axis is the minimum score with respect to any reference and the vertical axis is the maximum (BLEU has a maximum value of 1.0). Ideally, from the perspective of learn0 25 50 75 100 Minimum (a) Maximum vs. minimum BLEU+1 (%) 0 25 50 75 100 Maximum (b) BLEU+1 (%) according to all four referen</context>
</contexts>
<marker>Green, Wang, Cer, Manning, 2013</marker>
<rawString>S. Green, S. Wang, D. Cer, and C. D. Manning. 2013b. Fast and adaptive online training of feature-rich translation models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Green</author>
<author>D Cer</author>
<author>C D Manning</author>
</authors>
<title>Phrasal: A toolkit for new directions in statistical machine translation.</title>
<date>2014</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="22222" citStr="Green et al., 2014" startWordPosition="3509" endWordPosition="3512">orpora. The monolingual English data comes from the AFP and Xinhua sections of English Gigaword 4 (LDC2009T13). training corpora5 come from several Linguistic Data Consortium (LDC) sources from 2012 and earlier (Table 2). The test, development, and tuning corpora6 come from the NIST OpenMT and MetricsMATR evaluations (Table 3). Extended features benefit from more tuning data, so we concatenated five NIST data sets to build one large tuning set. Observe that all test data come from later epochs than the tuning and development data. From these data we built phrase-based MT systems with Phrasal (Green et al., 2014).7 We aligned the parallel corpora with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. We created separate English LMs for each language pair by concatenating the monolingual Gigaword data with the target-side of the respective bitexts. For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results W</context>
</contexts>
<marker>Green, Cer, Manning, 2014</marker>
<rawString>S. Green, D. Cer, and C. D. Manning. 2014. Phrasal: A toolkit for new directions in statistical machine translation. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gruber</author>
<author>Y Weiss</author>
<author>M Rosen-Zvi</author>
</authors>
<title>Hidden topic markov models.</title>
<date>2007</date>
<booktitle>In AISTATS.</booktitle>
<contexts>
<context position="38232" citStr="Gruber et al. (2007)" startWordPosition="6843" endWordPosition="6846">nvestigated local adaption of the log-linear scores by selecting comparable bitext examples for a given source input. After selecting a small local corpus, their algorithm then performs several online update steps—starting from a globally tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classifier. Su et al. (2012) proposed domain adaptation via monolingual source resources much as we use in-domain monolingual corpora for data selection. They labeled each bitext sentence with a topic using a Hidden Topic Markov Model (HTMM) Gruber et al. (2007). Source topic information was then mixed into the translation model dense feature calculations. This work follows Chiang et al. (2011), who present a similar technique but using the same gold NIST labels that we use. Hasler et al. (2012) extended these ideas to a discriminative sparse feature set by augmenting both rule and unigram alignment features with HTMM topic information. 8 Conclusion This paper makes four major contributions. First, we introduced extended features for phrase-based MT that exceeded both dense and feature-rich baselines. Second, we specialized the features to source dom</context>
</contexts>
<marker>Gruber, Weiss, Rosen-Zvi, 2007</marker>
<rawString>A. Gruber, Y. Weiss, and M. Rosen-Zvi. 2007. Hidden topic markov models. In AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hasler</author>
<author>B Haddow</author>
<author>P Koehn</author>
</authors>
<title>Sparse lexicalised features and topic adaptation for SMT.</title>
<date>2012</date>
<booktitle>In IWSLT.</booktitle>
<contexts>
<context position="38470" citStr="Hasler et al. (2012)" startWordPosition="6883" endWordPosition="6886"> tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classifier. Su et al. (2012) proposed domain adaptation via monolingual source resources much as we use in-domain monolingual corpora for data selection. They labeled each bitext sentence with a topic using a Hidden Topic Markov Model (HTMM) Gruber et al. (2007). Source topic information was then mixed into the translation model dense feature calculations. This work follows Chiang et al. (2011), who present a similar technique but using the same gold NIST labels that we use. Hasler et al. (2012) extended these ideas to a discriminative sparse feature set by augmenting both rule and unigram alignment features with HTMM topic information. 8 Conclusion This paper makes four major contributions. First, we introduced extended features for phrase-based MT that exceeded both dense and feature-rich baselines. Second, we specialized the features to source domains, further extending the gains. Third, we showed that online expected BLEU is faster and more stable than online PRO for extended features. Finally, we released fast, scalable, languageindependent tools for implementing the feature set</context>
</contexts>
<marker>Hasler, Haddow, Koehn, 2012</marker>
<rawString>E. Hasler, B. Haddow, and P. Koehn. 2012. Sparse lexicalised features and topic adaptation for SMT. In IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Heafield</author>
<author>I Pouzyrevsky</author>
<author>J H Clark</author>
<author>P Koehn</author>
</authors>
<title>Scalable modified Kneser-Ney language model estimation. In</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="22620" citStr="Heafield et al., 2013" startWordPosition="3569" endWordPosition="3572">ed five NIST data sets to build one large tuning set. Observe that all test data come from later epochs than the tuning and development data. From these data we built phrase-based MT systems with Phrasal (Green et al., 2014).7 We aligned the parallel corpora with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. We created separate English LMs for each language pair by concatenating the monolingual Gigaword data with the target-side of the respective bitexts. For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. DENSE is the same baseline as Green et al. 5We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segm</context>
</contexts>
<marker>Heafield, Pouzyrevsky, Clark, Koehn, 2013</marker>
<rawString>K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn. 2013. Scalable modified Kneser-Ney language model estimation. In ACL, Short Papers. M. Hopkins and J. May. 2011. Tuning as ranking. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Irvine</author>
<author>J Morgan</author>
<author>M Carpuat</author>
<author>H Daumé</author>
<author>D Munteanu</author>
</authors>
<title>Measuring machine translation errors in new domains.</title>
<date>2013</date>
<journal>TACL,</journal>
<volume>1</volume>
<contexts>
<context position="20207" citStr="Irvine et al. (2013)" startWordPosition="3200" endWordPosition="3203">ta selection, in our case the feature decay algorithm (Biçici and Yuret, 2011). Finally, our rule extractor adds domain labels to all rules extracted from each selected sentence pair. Crucially, these labels do not influence which rules are extracted or how they are scored. The resulting phrase table contains the same rules, but with a few additional annotations. Our method assumes domain labels for each source input to be decoded. Our experiments utilize gold, document-level labels, but accurate sentencelevel domain classifiers exist (Wang et al., 2012). 4.1 Augmentation of Extended Features Irvine et al. (2013) showed that lexical selection is the most quantifiable and perhaps most common source of error in phrase-based domain adaptation. Our development experiments seemed to confirm this hypothesis as augmentation of the class-based and non-lexical (e.g., Rule shape) features did not reduce error. Therefore, we only augment the lexicalized features: rule indicators and orientations, and word alignments. 4.2 Domain-Specific Feature Templates In-domain Rule Indicator (Durrani et al., 2013) An indicator for each rule that matches the input domain. This template fires a generic in-domain indicator and </context>
<context position="27524" citStr="Irvine et al. (2013)" startWordPosition="4357" endWordPosition="4360"> Statistical significance relative to D+SPARSE, the strongest baseline: bold (p &lt; 0.001) and bold-italic (p &lt; 0.05). Significance is computed by the permutation test of Riezler and Maxwell (2005). †The dev score of EXT+FiLT+Dom is the dev-dom data set from Table 3, so it is not comparable with the other rows. EXT+FiLT+Dom to the baselines and other feature sets. The gains relative to SPARSE are statistically significant for all six test sets. A crucial result is that with domain features accuracy relative to EXT+FiLT never decreases: a single domain-adapted system is effective across domains. Irvine et al. (2013) showed that when models from multiple domains are interpolated, scoring errors affecting lexical selection—the model could have generated the correct target lexical item but did not—increase significantly. We do not observe that behavior, at least from the perspective of BLEU. Table 5 separates out per-domain results. The web data appears to be the hardest domain. That is sensible given that broadcast news transcripts are more similar to newswire, the default domain, than web data. Moreover, inspection of the bitext sources revealed very little web data, so our automatic data selection is pro</context>
</contexts>
<marker>Irvine, Morgan, Carpuat, Daumé, Munteanu, 2013</marker>
<rawString>A. Irvine, J. Morgan, M. Carpuat, H. Daumé III, and D. Munteanu. 2013. Measuring machine translation errors in new domains. TACL, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="8336" citStr="Koehn et al., 2003" startWordPosition="1300" endWordPosition="1303">ither a real-valued parameter or an index indicating a categorical value like an n-gram context. For each language, the extended feature templates require unigram counts and a word-to-class mapping ϕ : w H c for word w E V and class c E C. These can be extracted from any monolingual data; our experiments simply use both sides of the unaligned parallel training data. The features are language-independent, but we will use Arabic-English as a running example. 3.1 Lexical Choice Lexical choice features make more specific distinctions between target words than the dense translation model features (Koehn et al., 2003). 2Gao and He (2013) used stochastic gradient descent and expected BLEU to learn phrase table feature weights, but not the full translation model w. 467 Lexicalized rule indicator (Liang et al., 2006a) Some rules occur frequently enough that we can learn rule-specific weights that augment the dense translation model features. For example, our model learns the following rule indicator features and weights: H. AJ.ƒ @ ==&gt;. reasons -0.022 H. AJ.ƒ @ ==&gt;. reasons for 0.002 H. AJ.ƒ @ ==&gt;. the reasons for 0.016 These translations are all correct depending on context. When the plural noun H. AJ.ƒ @ ‘re</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL, Demonstration Session.</booktitle>
<contexts>
<context position="2569" citStr="Koehn et al., 2007" startWordPosition="366" endWordPosition="369">atch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look. We introduce an effective extended feature set for phrase-based MT and identify a loss function that is less prone to overfitting. Extended features share three attractive characteristics with the standard Moses dense features (Koehn et al., 2007): ease of implementation, language independence, and independence from ancillary corpora like treebanks. In our experiments, they do not overfit and can be extracted efficiently during decoding. Because all feature weights are tuned on the development set, the new feature templates are amenable to feature augmentation (Daumé III, 2007), a simple domain adaptation technique that we show works surprisingly well for MT. Extended features are designed according to a principle rather than a rule: they should fire less than standard dense features, which are general, but more than so-called sparse f</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, et al. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>A Bouchard-Côté</author>
<author>D Klein</author>
<author>B Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="8535" citStr="Liang et al., 2006" startWordPosition="1332" endWordPosition="1335">: w H c for word w E V and class c E C. These can be extracted from any monolingual data; our experiments simply use both sides of the unaligned parallel training data. The features are language-independent, but we will use Arabic-English as a running example. 3.1 Lexical Choice Lexical choice features make more specific distinctions between target words than the dense translation model features (Koehn et al., 2003). 2Gao and He (2013) used stochastic gradient descent and expected BLEU to learn phrase table feature weights, but not the full translation model w. 467 Lexicalized rule indicator (Liang et al., 2006a) Some rules occur frequently enough that we can learn rule-specific weights that augment the dense translation model features. For example, our model learns the following rule indicator features and weights: H. AJ.ƒ @ ==&gt;. reasons -0.022 H. AJ.ƒ @ ==&gt;. reasons for 0.002 H. AJ.ƒ @ ==&gt;. the reasons for 0.016 These translations are all correct depending on context. When the plural noun H. AJ.ƒ @ ‘reasons’ appears in a construct state (iDafa) the preposition for is unrealized. Moreover, depending on the context, the English translation might also require the determiner the, which is also unreali</context>
<context position="10870" citStr="Liang et al., 2006" startWordPosition="1710" endWordPosition="1713">mappings the following words map to the same class: word class freq. surface-to-surface air-to-air ground-to-air The classes capture common linguistic attributes of these words, which is the motivation for a full classbased LM. Learning unigram weights directly is surprisingly effective and does not require building another LM. This template fires a separate indicator for each class {cp(w) : w E e(r)} and is local. 3.2 Word Alignments Word alignment features allow the model to recognize fine-grained phrase-internal information that is largely opaque in the dense model. Lexicalized alignments (Liang et al., 2006a) Consider the internal alignments of the rule: sunday , ÐñK� 1 YgB@ 2 Alignment 1 (ÐñK� &apos;day&apos; ==&gt;. ,) is incorrect and alignment 2 is correct. The dense translation model features might assign this rule high probability if alignment 1 is a common alignment error. Lexicalized alignment features allow the model to compensate for these events. This feature fires an indicator for each alignment in a rule—including multiword cliques—and is local. Class-based alignments Like the class-based rule indicator, this feature template replaces each lexical item with its word class, resulting in an alignm</context>
<context position="15255" citStr="Liang et al., 2006" startWordPosition="2399" endWordPosition="2402">a total of eight orientations.3 3Each class has “with-previous” and “with-next” specializations. Algorithm (implementation) #threads Time Brown (wcluster) 1 1023.39 Clark (cluster_neyessen) 1 890.11 Och (mkcls) 1 199.04 PredictiveFull (this paper) 8 3.27 Predictive (this paper) 8 2.42 Table 1: Wallclock time (min.sec) to generate a mapping from a vocabulary of 63k English words (3.7M tokens) to 512 classes. All experiments were run on the same server, which had eight physical cores. Our Java implementation is multi-threaded; the C++ baselines are single-threaded. Lexicalized rule orientation (Liang et al., 2006a) For each rule, the template fires an indicator for the concatenation of the orientation class, each element in f(r), and each element in e(r). To prevent overfitting, this template only fires for rules that occur more than 50 times in the training data. The feature is non-local and its recombination state p is the rule orientation. Class-based rule orientation For each rule, the template fires an indicator for the concatenation of the orientation class, each element in {co(w) : w ∈ f(r)}, and each element in {co(w) : w ∈ e(r)}. The feature is non-local and its recombination state p is the r</context>
<context position="22302" citStr="Liang et al., 2006" startWordPosition="3522" endWordPosition="3525">glish Gigaword 4 (LDC2009T13). training corpora5 come from several Linguistic Data Consortium (LDC) sources from 2012 and earlier (Table 2). The test, development, and tuning corpora6 come from the NIST OpenMT and MetricsMATR evaluations (Table 3). Extended features benefit from more tuning data, so we concatenated five NIST data sets to build one large tuning set. Observe that all test data come from later epochs than the tuning and development data. From these data we built phrase-based MT systems with Phrasal (Green et al., 2014).7 We aligned the parallel corpora with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. We created separate English LMs for each language pair by concatenating the monolingual Gigaword data with the target-side of the respective bitexts. For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. DENSE is the same base</context>
<context position="35577" citStr="Liang et al., 2006" startWordPosition="6439" endWordPosition="6442">ference condition (Gimpel and Smith, 2012a). Repeating the min/max calculations with the most effective extensions (according to Gimpel and Smith (2012a)) we observe lower variance (M = 17.32, SD = 10.68). These extensions are very simple, so a more sophisticated noise model is a promising future direction. 7 Related Work We review work on phrase-based discriminative feature sets that influence decoder search, and domain adaptation with features.11 7.1 Feature Sets Variants of some extended features are scattered throughout previous work: unfiltered lexicalized rule indicators and alignments (Liang et al., 2006a); rule shape (Hopkins and May, 2011); rule orientation (Liang et al., 2006b; Cherry, 2013); target unigram class (Ammar et al., 2013). We found that other prior features did not improve translation: higher-order target lexical n-grams (Liang et al., 2006a; Watanabe et al., 2007; Gimpel and Smith, 2012b), higher-order target class n-grams (Ammar et al., 2013), target word insertion (Watanabe et al., 2007; Chiang et al., 2009), and many other unpublished ideas transmitted through received wisdom. To our knowledge, Yu et al. (2013) were the first to experiment with non-local (derivation) featur</context>
</contexts>
<marker>Liang, Bouchard-Côté, Klein, Taskar, 2006</marker>
<rawString>P. Liang, A. Bouchard-Côté, D. Klein, and B. Taskar. 2006a. An end-to-end discriminative approach to machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="8535" citStr="Liang et al., 2006" startWordPosition="1332" endWordPosition="1335">: w H c for word w E V and class c E C. These can be extracted from any monolingual data; our experiments simply use both sides of the unaligned parallel training data. The features are language-independent, but we will use Arabic-English as a running example. 3.1 Lexical Choice Lexical choice features make more specific distinctions between target words than the dense translation model features (Koehn et al., 2003). 2Gao and He (2013) used stochastic gradient descent and expected BLEU to learn phrase table feature weights, but not the full translation model w. 467 Lexicalized rule indicator (Liang et al., 2006a) Some rules occur frequently enough that we can learn rule-specific weights that augment the dense translation model features. For example, our model learns the following rule indicator features and weights: H. AJ.ƒ @ ==&gt;. reasons -0.022 H. AJ.ƒ @ ==&gt;. reasons for 0.002 H. AJ.ƒ @ ==&gt;. the reasons for 0.016 These translations are all correct depending on context. When the plural noun H. AJ.ƒ @ ‘reasons’ appears in a construct state (iDafa) the preposition for is unrealized. Moreover, depending on the context, the English translation might also require the determiner the, which is also unreali</context>
<context position="10870" citStr="Liang et al., 2006" startWordPosition="1710" endWordPosition="1713">mappings the following words map to the same class: word class freq. surface-to-surface air-to-air ground-to-air The classes capture common linguistic attributes of these words, which is the motivation for a full classbased LM. Learning unigram weights directly is surprisingly effective and does not require building another LM. This template fires a separate indicator for each class {cp(w) : w E e(r)} and is local. 3.2 Word Alignments Word alignment features allow the model to recognize fine-grained phrase-internal information that is largely opaque in the dense model. Lexicalized alignments (Liang et al., 2006a) Consider the internal alignments of the rule: sunday , ÐñK� 1 YgB@ 2 Alignment 1 (ÐñK� &apos;day&apos; ==&gt;. ,) is incorrect and alignment 2 is correct. The dense translation model features might assign this rule high probability if alignment 1 is a common alignment error. Lexicalized alignment features allow the model to compensate for these events. This feature fires an indicator for each alignment in a rule—including multiword cliques—and is local. Class-based alignments Like the class-based rule indicator, this feature template replaces each lexical item with its word class, resulting in an alignm</context>
<context position="15255" citStr="Liang et al., 2006" startWordPosition="2399" endWordPosition="2402">a total of eight orientations.3 3Each class has “with-previous” and “with-next” specializations. Algorithm (implementation) #threads Time Brown (wcluster) 1 1023.39 Clark (cluster_neyessen) 1 890.11 Och (mkcls) 1 199.04 PredictiveFull (this paper) 8 3.27 Predictive (this paper) 8 2.42 Table 1: Wallclock time (min.sec) to generate a mapping from a vocabulary of 63k English words (3.7M tokens) to 512 classes. All experiments were run on the same server, which had eight physical cores. Our Java implementation is multi-threaded; the C++ baselines are single-threaded. Lexicalized rule orientation (Liang et al., 2006a) For each rule, the template fires an indicator for the concatenation of the orientation class, each element in f(r), and each element in e(r). To prevent overfitting, this template only fires for rules that occur more than 50 times in the training data. The feature is non-local and its recombination state p is the rule orientation. Class-based rule orientation For each rule, the template fires an indicator for the concatenation of the orientation class, each element in {co(w) : w ∈ f(r)}, and each element in {co(w) : w ∈ e(r)}. The feature is non-local and its recombination state p is the r</context>
<context position="22302" citStr="Liang et al., 2006" startWordPosition="3522" endWordPosition="3525">glish Gigaword 4 (LDC2009T13). training corpora5 come from several Linguistic Data Consortium (LDC) sources from 2012 and earlier (Table 2). The test, development, and tuning corpora6 come from the NIST OpenMT and MetricsMATR evaluations (Table 3). Extended features benefit from more tuning data, so we concatenated five NIST data sets to build one large tuning set. Observe that all test data come from later epochs than the tuning and development data. From these data we built phrase-based MT systems with Phrasal (Green et al., 2014).7 We aligned the parallel corpora with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. We created separate English LMs for each language pair by concatenating the monolingual Gigaword data with the target-side of the respective bitexts. For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. DENSE is the same base</context>
<context position="35577" citStr="Liang et al., 2006" startWordPosition="6439" endWordPosition="6442">ference condition (Gimpel and Smith, 2012a). Repeating the min/max calculations with the most effective extensions (according to Gimpel and Smith (2012a)) we observe lower variance (M = 17.32, SD = 10.68). These extensions are very simple, so a more sophisticated noise model is a promising future direction. 7 Related Work We review work on phrase-based discriminative feature sets that influence decoder search, and domain adaptation with features.11 7.1 Feature Sets Variants of some extended features are scattered throughout previous work: unfiltered lexicalized rule indicators and alignments (Liang et al., 2006a); rule shape (Hopkins and May, 2011); rule orientation (Liang et al., 2006b; Cherry, 2013); target unigram class (Ammar et al., 2013). We found that other prior features did not improve translation: higher-order target lexical n-grams (Liang et al., 2006a; Watanabe et al., 2007; Gimpel and Smith, 2012b), higher-order target class n-grams (Ammar et al., 2013), target word insertion (Watanabe et al., 2007; Chiang et al., 2009), and many other unpublished ideas transmitted through received wisdom. To our knowledge, Yu et al. (2013) were the first to experiment with non-local (derivation) featur</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>P. Liang, B. Taskar, and D. Klein. 2006b. Alignment by agreement. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
</authors>
<title>Semi-supervised learning for natural language. Master’s thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology.</institution>
<marker>Liang, 2005</marker>
<rawString>P. Liang. 2005. Semi-supervised learning for natural language. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>F J Och</author>
</authors>
<title>ORANGE: a method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="6964" citStr="Lin and Och, 2004" startWordPosition="1068" endWordPosition="1071">red n-best list of translations at time step t for source input ft. Let G(e) be a gold error metric that evaluates each candidate translation with respect to a set of one or more 1http://nlp.stanford.edu/software/phrasal references. The smooth loss function is `t(wt−1) = Ep(eJft;wt−,)[G(e)] 1 � � � Z exp wTφ(e&apos;, f) · G(e&apos;) e&apos;EEt (2) with normalization constant Z = Ee,EEt exp (wTφ(e&apos;, f)). The gradient gt for coordinate j is: gt = E[G(e)φj(e, ft)]− E[G(e)]E[φj(e, ft)] (3) To our knowledge, we are the first to experiment with the online version of this loss.2 When G(e) is sentence-level BLEU+1 (Lin and Och, 2004)—the setting in our experiments—this loss is also known as expected BLEU (Cherry and Foster, 2012). However, other metrics are possible. 3 Extended Phrase-based Features We divide our feature templates into five categories, which are well-known sources of error in phrasebased translation. The features are defined over derivations d = {riJD i=1, which are ordered sequences of rules r from the translation model. Define functions f(·) to be the source string of a rule or derivation and e(·) to be the target string. Local features can be extracted from individual rules and do not declare any state</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for evaluating automatic evaluation metrics for machine translation. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Liu</author>
<author>H Cao</author>
<author>T Watanabe</author>
<author>T Zhao</author>
<author>M Yu</author>
<author>C Zhu</author>
</authors>
<title>Locally training the log-linear model for SMT. In EMNLP-CoNLL.</title>
<date>2012</date>
<contexts>
<context position="37610" citStr="Liu et al. (2012)" startWordPosition="6751" endWordPosition="6754">ns preclude discussion of re-ranking features. Then they added these dense class-based features to the baseline lexicalized system. Finally, Cherry (2013) experimented with class-based hierarchical reordering features. However, his features used a bespoke representation rather than the simple full rule string that we use. 7.2 Domain Adaptation with Features Both Clark et al. (2012) and Wang et al. (2012) augmented the baseline dense feature set with domain labels. They each showed modest improvements for several language pairs. However, neither incorporated a notion of a default prior domain. Liu et al. (2012) investigated local adaption of the log-linear scores by selecting comparable bitext examples for a given source input. After selecting a small local corpus, their algorithm then performs several online update steps—starting from a globally tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classifier. Su et al. (2012) proposed domain adaptation via monolingual source resources much as we use in-domain monolingual corpora for data selection. They labeled each bitext sentence with a topic using a Hidden Topic Markov Model (HTMM</context>
</contexts>
<marker>Liu, Cao, Watanabe, Zhao, Yu, Zhu, 2012</marker>
<rawString>L. Liu, H. Cao, T. Watanabe, T. Zhao, M. Yu, and C. Zhu. 2012. Locally training the log-linear model for SMT. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maamouri</author>
<author>A Bies</author>
<author>S Kulick</author>
</authors>
<title>Enhancing the Arabic Treebank: A collaborative effort toward new annotation guidelines.</title>
<date>2008</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="23172" citStr="Maamouri et al., 2008" startWordPosition="3662" endWordPosition="3665">d unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. DENSE is the same baseline as Green et al. 5We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). 6Data sources: tune, MT023568; dev, MT04; dev-dom, domain adaptation dev set is MT04 and all wb and bn data from LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En); test2, Progress0809 which was revealed in the OpenMT 2012 evaluation; test3, MetricsMATR08-10. 7System settings: distortion limit of 5, cube pruning beam size of 1200, maximum phrase length of 7. 8Other learning settings: 16 threads, mini-batch size of 20; Ll regularization strength λ = 0.001;</context>
</contexts>
<marker>Maamouri, Bies, Kulick, 2008</marker>
<rawString>M. Maamouri, A. Bies, and S. Kulick. 2008. Enhancing the Arabic Treebank: A collaborative effort toward new annotation guidelines. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="23032" citStr="Marcus et al., 1993" startWordPosition="3640" endWordPosition="3643">h language pair by concatenating the monolingual Gigaword data with the target-side of the respective bitexts. For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. DENSE is the same baseline as Green et al. 5We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). 6Data sources: tune, MT023568; dev, MT04; dev-dom, domain adaptation dev set is MT04 and all wb and bn data from LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En); test2, Progress0809 which was revealed in the OpenMT 2012 evaluation; test3, MetricsMATR08-10. 7System settings: distortion limit of 5, cube pruning beam </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. Marcus, M. A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Monroe</author>
<author>S Green</author>
<author>C D Manning</author>
</authors>
<title>Word segmentation of informal Arabic with domain adaptation.</title>
<date>2014</date>
<booktitle>In ACL, Short Papers.</booktitle>
<contexts>
<context position="23101" citStr="Monroe et al., 2014" startWordPosition="3651" endWordPosition="3654">he target-side of the respective bitexts. For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. DENSE is the same baseline as Green et al. 5We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). 6Data sources: tune, MT023568; dev, MT04; dev-dom, domain adaptation dev set is MT04 and all wb and bn data from LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En); test2, Progress0809 which was revealed in the OpenMT 2012 evaluation; test3, MetricsMATR08-10. 7System settings: distortion limit of 5, cube pruning beam size of 1200, maximum phrase length of 7. 8Other learning settings: 1</context>
</contexts>
<marker>Monroe, Green, Manning, 2014</marker>
<rawString>W. Monroe, S. Green, and C. D. Manning. 2014. Word segmentation of informal Arabic with domain adaptation. In ACL, Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nakov</author>
<author>F Guzman</author>
<author>S Vogel</author>
</authors>
<title>Optimizing for sentence-level BLEU+1 yields short translations.</title>
<date>2012</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="34862" citStr="Nakov et al. (2012)" startWordPosition="6333" endWordPosition="6336">● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● Maximum 100 75 50 25 0 reference, it would have matched the translation exactly and incurred a low loss. Nakov et al. (2012) suggested extensions to BLEU+1 that were subsequently found to improve accuracy in the single-reference condition (Gimpel and Smith, 2012a). Repeating the min/max calculations with the most effective extensions (according to Gimpel and Smith (2012a)) we observe lower variance (M = 17.32, SD = 10.68). These extensions are very simple, so a more sophisticated noise model is a promising future direction. 7 Related Work We review work on phrase-based discriminative feature sets that influence decoder search, and domain adaptation with features.11 7.1 Feature Sets Variants of some extended feature</context>
</contexts>
<marker>Nakov, Guzman, Vogel, 2012</marker>
<rawString>P. Nakov, F. Guzman, and S. Vogel. 2012. Optimizing for sentence-level BLEU+1 yields short translations. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nakov</author>
<author>F Guzmán</author>
<author>S Vogel</author>
</authors>
<title>A tale about PRO and monsters.</title>
<date>2013</date>
<booktitle>In ACL, Short Papers.</booktitle>
<contexts>
<context position="6161" citStr="Nakov et al., 2013" startWordPosition="932" endWordPosition="935">learn w, we follow the online procedure of Green et al. (2013b), who calculate gradient steps with AdaGrad (Duchi et al., 2011) and perform feature selection via L1 regularization in the FOBOS (Duchi and Singer, 2009) framework. This procedure accommodates any loss function for which a subgradient can be computed. Green et al. (2013b) used a PRO objective (Hopkins and May, 2011) with a logistic (surrogate) loss function. However, later results showed overfitting (Green et al., 2013a), and we found that their online variant of PRO tends to produce short translations like its batch counterpart (Nakov et al., 2013). Moreover, PRO requires sampling, making it slow to compute. To address these shortcomings, we explore an online variant of expected error (Och, 2003, Eq.7). Let Et = {eiJni=1 be a scored n-best list of translations at time step t for source input ft. Let G(e) be a gold error metric that evaluates each candidate translation with respect to a set of one or more 1http://nlp.stanford.edu/software/phrasal references. The smooth loss function is `t(wt−1) = Ep(eJft;wt−,)[G(e)] 1 � � � Z exp wTφ(e&apos;, f) · G(e&apos;) e&apos;EEt (2) with normalization constant Z = Ee,EEt exp (wTφ(e&apos;, f)). The gradient gt for coo</context>
</contexts>
<marker>Nakov, Guzmán, Vogel, 2013</marker>
<rawString>P. Nakov, F. Guzmán, and S. Vogel. 2013. A tale about PRO and monsters. In ACL, Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="11708" citStr="Och and Ney, 2003" startWordPosition="1845" endWordPosition="1848">y if alignment 1 is a common alignment error. Lexicalized alignment features allow the model to compensate for these events. This feature fires an indicator for each alignment in a rule—including multiword cliques—and is local. Class-based alignments Like the class-based rule indicator, this feature template replaces each lexical item with its word class, resulting in an alignment prototype. This feature fires an indicator for each alignment in a rule after mapping lexical items to classes. It is local. Source class deletion Phrase extraction algorithms often use a “grow” symmetrization step (Och and Ney, 2003) to add alignment points. Sometimes this procedure can produce a rule that deletes important source content words. This feature template allows the model to penalize these rules by firing an indicator for the class of each unaligned source word. The feature is local. Punctuation ratio Languages use different types and ratios of punctuation (Salton, 1958). For example, quotation marks are not commonly used in Arabic, but they are conventional in English. Furthermore, spurious alignments often contain punctuation. To control these two phenomena, this feature template returns the ratio of target </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="4764" citStr="Och and Ney, 2004" startWordPosition="696" endWordPosition="699"> the Ninth Workshop on Statistical Machine Translation, pages 466–476, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics Chinese-English. In addition to the extended feature set, we show that an online variant of expected error (Och, 2003) is significantly faster to compute, less prone to overfitting, and nearly as effective as a pairwise loss. We release all software—feature extractors, and fast word clustering and data selection packages—used in our experiments.1 2 Phrase-based Models and Learning The log-linear approach to phrase-based translation (Och and Ney, 2004) directly models the predictive translation distribution p(el f; w) = Z1 exp I wTφ(e, f)] (1) where e is the target string, f is the source string, w E Rd is the vector of model parameters, φ(·) E Rd is a feature map, and Z(f) is an appropriate normalizing constant. Assume that there is also a function ρ(e, f) E Rd that produces a recombination map for the features. That is, each coordinate in ρ represents the state of the corresponding coordinate in φ. For example, suppose that φj is the log probability produced by the n-gram language model (LM). Then ρj would be the appropriate LM history. R</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. J. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4427" citStr="Och, 2003" startWordPosition="649" endWordPosition="650">ecessary and even detrimental when features follow this principle. We report large-scale translation quality experiments relative to both dense and feature-rich baselines. Our best feature set, which includes domain adaptation features, yields an average +1.05 BLEU improvement for Arabic-English and +0.67 for 466 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 466–476, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics Chinese-English. In addition to the extended feature set, we show that an online variant of expected error (Och, 2003) is significantly faster to compute, less prone to overfitting, and nearly as effective as a pairwise loss. We release all software—feature extractors, and fast word clustering and data selection packages—used in our experiments.1 2 Phrase-based Models and Learning The log-linear approach to phrase-based translation (Och and Ney, 2004) directly models the predictive translation distribution p(el f; w) = Z1 exp I wTφ(e, f)] (1) where e is the target string, f is the source string, w E Rd is the vector of model parameters, φ(·) E Rd is a feature map, and Z(f) is an appropriate normalizing consta</context>
<context position="6311" citStr="Och, 2003" startWordPosition="957" endWordPosition="958">a L1 regularization in the FOBOS (Duchi and Singer, 2009) framework. This procedure accommodates any loss function for which a subgradient can be computed. Green et al. (2013b) used a PRO objective (Hopkins and May, 2011) with a logistic (surrogate) loss function. However, later results showed overfitting (Green et al., 2013a), and we found that their online variant of PRO tends to produce short translations like its batch counterpart (Nakov et al., 2013). Moreover, PRO requires sampling, making it slow to compute. To address these shortcomings, we explore an online variant of expected error (Och, 2003, Eq.7). Let Et = {eiJni=1 be a scored n-best list of translations at time step t for source input ft. Let G(e) be a gold error metric that evaluates each candidate translation with respect to a set of one or more 1http://nlp.stanford.edu/software/phrasal references. The smooth loss function is `t(wt−1) = Ep(eJft;wt−,)[G(e)] 1 � � � Z exp wTφ(e&apos;, f) · G(e&apos;) e&apos;EEt (2) with normalization constant Z = Ee,EEt exp (wTφ(e&apos;, f)). The gradient gt for coordinate j is: gt = E[G(e)φj(e, ft)]− E[G(e)]E[φj(e, ft)] (3) To our knowledge, we are the first to experiment with the online version of this loss.2 W</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="22786" citStr="Papineni et al., 2002" startWordPosition="3595" endWordPosition="3598"> phrase-based MT systems with Phrasal (Green et al., 2014).7 We aligned the parallel corpora with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. We created separate English LMs for each language pair by concatenating the monolingual Gigaword data with the target-side of the respective bitexts. For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. DENSE is the same baseline as Green et al. 5We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). 6Data sources: tune, MT023568; dev, MT04; dev-dom, domain adaptation de</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>J T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing in MT.</title>
<date>2005</date>
<booktitle>In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="27099" citStr="Riezler and Maxwell (2005)" startWordPosition="4287" endWordPosition="4290">Model #features Epochs Min. /Epoch tune dev test1 test2 test3 DENSE (D) 18 17 3 32.82 34.96 26.61 26.72 10.19 D+SPARSE 55,024 17 8 38.91 36.68 27.86 28.41 10.98 D+EXT 67,936 16 13 40.96 37.19 28.27 28.40 10.72 D+EXT+FiLT 100,275 17 14 41.38 37.36 28.68 28.90 11.24 D+EXT+FiLT+Dom 126,014 17 14 41.70 17.20† 28.71 28.96 11.67 (b) Zh-En. Table 4: Translation quality results (uncased BLEU-4 %). Per-epoch times are in minutes (Min.). Statistical significance relative to D+SPARSE, the strongest baseline: bold (p &lt; 0.001) and bold-italic (p &lt; 0.05). Significance is computed by the permutation test of Riezler and Maxwell (2005). †The dev score of EXT+FiLT+Dom is the dev-dom data set from Table 3, so it is not comparable with the other rows. EXT+FiLT+Dom to the baselines and other feature sets. The gains relative to SPARSE are statistically significant for all six test sets. A crucial result is that with domain features accuracy relative to EXT+FiLT never decreases: a single domain-adapted system is effective across domains. Irvine et al. (2013) showed that when models from multiple domains are interpolated, scoring errors affecting lexical selection—the model could have generated the correct target lexical item but </context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>S. Riezler and J. T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing in MT. In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>The use of punctuation patterns in machine translation.</title>
<date>1958</date>
<journal>Mechanical Translation,</journal>
<volume>5</volume>
<issue>1</issue>
<pages>24</pages>
<contexts>
<context position="12064" citStr="Salton, 1958" startWordPosition="1903" endWordPosition="1904">ulting in an alignment prototype. This feature fires an indicator for each alignment in a rule after mapping lexical items to classes. It is local. Source class deletion Phrase extraction algorithms often use a “grow” symmetrization step (Och and Ney, 2003) to add alignment points. Sometimes this procedure can produce a rule that deletes important source content words. This feature template allows the model to penalize these rules by firing an indicator for the class of each unaligned source word. The feature is local. Punctuation ratio Languages use different types and ratios of punctuation (Salton, 1958). For example, quotation marks are not commonly used in Arabic, but they are conventional in English. Furthermore, spurious alignments often contain punctuation. To control these two phenomena, this feature template returns the ratio of target punctuation tokens to source punctuation tokens for each derivation. Since the denominator is constant, this feature can be computed incrementally as a derivation is constructed. It is local. Function word ratio Words can also be spuriously aligned to non-punctuation, non-digit function words such as determiners and particles. Furthermore, linguistic dif</context>
</contexts>
<marker>Salton, 1958</marker>
<rawString>G. Salton. 1958. The use of punctuation patterns in machine translation. Mechanical Translation, 5(1):16– 24, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Su</author>
<author>H Wu</author>
<author>H Wang</author>
<author>Y Chen</author>
<author>X Shi</author>
<author>H Dong</author>
<author>Q Liu</author>
</authors>
<title>Translation model adaptation for statistical machine translation with monolingual topic information.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="37998" citStr="Su et al. (2012)" startWordPosition="6807" endWordPosition="6810">Wang et al. (2012) augmented the baseline dense feature set with domain labels. They each showed modest improvements for several language pairs. However, neither incorporated a notion of a default prior domain. Liu et al. (2012) investigated local adaption of the log-linear scores by selecting comparable bitext examples for a given source input. After selecting a small local corpus, their algorithm then performs several online update steps—starting from a globally tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classifier. Su et al. (2012) proposed domain adaptation via monolingual source resources much as we use in-domain monolingual corpora for data selection. They labeled each bitext sentence with a topic using a Hidden Topic Markov Model (HTMM) Gruber et al. (2007). Source topic information was then mixed into the translation model dense feature calculations. This work follows Chiang et al. (2011), who present a similar technique but using the same gold NIST labels that we use. Hasler et al. (2012) extended these ideas to a discriminative sparse feature set by augmenting both rule and unigram alignment features with HTMM to</context>
</contexts>
<marker>Su, Wu, Wang, Chen, Shi, Dong, Liu, 2012</marker>
<rawString>J. Su, H. Wu, H. Wang, Y. Chen, X. Shi, H. Dong, and Q. Liu. 2012. Translation model adaptation for statistical machine translation with monolingual topic information. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Uszkoreit</author>
<author>T Brants</author>
</authors>
<title>Distributed word clustering for large scale class-based language modeling in machine translation.</title>
<date>2008</date>
<booktitle>In ACL-HLT.</booktitle>
<contexts>
<context position="17624" citStr="Uszkoreit and Brants (2008)" startWordPosition="2783" endWordPosition="2786">13; Cherry, 2013; Yu et al., 2013). In a broad survey, Christodoulopoulos et al. (2010) found that for several downstream tasks, most word clustering algorithms—including Brown and Clark—result in similar task accuracy. For our large-scale setting, the primary issue is then the time to estimate co. For large corpora the existing implementations may require days or weeks, making our feature set less practical than the traditional dense MT features. Consequently, we re-implemented the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Brants (2008) (Predictive), which was originally developed for very large scale language modeling. Our implementation uses multiple threads on a single processor instead of MapReduce. We also added two extensions that are useful for translation features. First, we map all digits to 0. This reduces sparsity while retaining useful patterns such as 0000 (e.g., years) and 0th (e.g., ordinals). Second, we mapped all words occurring fewer than T times to an &lt;unk&gt; token. In our experiment, these two changes reduce the vocabulary size by 71.1%. They also make the mapping co more robust to unseen events during tran</context>
</contexts>
<marker>Uszkoreit, Brants, 2008</marker>
<rawString>J. Uszkoreit and T. Brants. 2008. Distributed word clustering for large scale class-based language modeling in machine translation. In ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>K Macherey</author>
<author>W Macherey</author>
<author>F J Och</author>
<author>P Xu</author>
</authors>
<title>Improved domain adaptation for statistical machine translation.</title>
<date>2012</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="20147" citStr="Wang et al., 2012" startWordPosition="3191" endWordPosition="3194">hen identify in-domain sentence pairs in the bitext via data selection, in our case the feature decay algorithm (Biçici and Yuret, 2011). Finally, our rule extractor adds domain labels to all rules extracted from each selected sentence pair. Crucially, these labels do not influence which rules are extracted or how they are scored. The resulting phrase table contains the same rules, but with a few additional annotations. Our method assumes domain labels for each source input to be decoded. Our experiments utilize gold, document-level labels, but accurate sentencelevel domain classifiers exist (Wang et al., 2012). 4.1 Augmentation of Extended Features Irvine et al. (2013) showed that lexical selection is the most quantifiable and perhaps most common source of error in phrase-based domain adaptation. Our development experiments seemed to confirm this hypothesis as augmentation of the class-based and non-lexical (e.g., Rule shape) features did not reduce error. Therefore, we only augment the lexicalized features: rule indicators and orientations, and word alignments. 4.2 Domain-Specific Feature Templates In-domain Rule Indicator (Durrani et al., 2013) An indicator for each rule that matches the input do</context>
<context position="37400" citStr="Wang et al. (2012)" startWordPosition="6717" endWordPosition="6720">ed word classes as backoff features to reduce overfitting. Wuebker et al. (2013) replaced all lexical items in the bitext and monolingual data with classes, and estimated the dense feature set. 11Space limitations preclude discussion of re-ranking features. Then they added these dense class-based features to the baseline lexicalized system. Finally, Cherry (2013) experimented with class-based hierarchical reordering features. However, his features used a bespoke representation rather than the simple full rule string that we use. 7.2 Domain Adaptation with Features Both Clark et al. (2012) and Wang et al. (2012) augmented the baseline dense feature set with domain labels. They each showed modest improvements for several language pairs. However, neither incorporated a notion of a default prior domain. Liu et al. (2012) investigated local adaption of the log-linear scores by selecting comparable bitext examples for a given source input. After selecting a small local corpus, their algorithm then performs several online update steps—starting from a globally tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classifier. Su et al. (2012) p</context>
</contexts>
<marker>Wang, Macherey, Macherey, Och, Xu, 2012</marker>
<rawString>W. Wang, K. Macherey, W. Macherey, F. J. Och, and P. Xu. 2012. Improved domain adaptation for statistical machine translation. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Watanabe</author>
<author>J Suzuki</author>
<author>H Tsukada</author>
<author>H Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="35857" citStr="Watanabe et al., 2007" startWordPosition="6483" endWordPosition="6486">s a promising future direction. 7 Related Work We review work on phrase-based discriminative feature sets that influence decoder search, and domain adaptation with features.11 7.1 Feature Sets Variants of some extended features are scattered throughout previous work: unfiltered lexicalized rule indicators and alignments (Liang et al., 2006a); rule shape (Hopkins and May, 2011); rule orientation (Liang et al., 2006b; Cherry, 2013); target unigram class (Ammar et al., 2013). We found that other prior features did not improve translation: higher-order target lexical n-grams (Liang et al., 2006a; Watanabe et al., 2007; Gimpel and Smith, 2012b), higher-order target class n-grams (Ammar et al., 2013), target word insertion (Watanabe et al., 2007; Chiang et al., 2009), and many other unpublished ideas transmitted through received wisdom. To our knowledge, Yu et al. (2013) were the first to experiment with non-local (derivation) features for phrase-based MT. They added discriminative rule features conditioned on target context. This is a good idea that we plan to explore. However, they do not mention if their non-local features declare recombination state. Our empirical experience is that non-local features ar</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki. 2007. Online large-margin training for statistical machine translation. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W D Whittaker</author>
<author>P C Woodland</author>
</authors>
<title>Efficient class-based language modelling for very large vocabularies.</title>
<date>2001</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="17550" citStr="Whittaker and Woodland (2001)" startWordPosition="2773" endWordPosition="2776">aligner. The latter has been used recently for MT features (Ammar et al., 2013; Cherry, 2013; Yu et al., 2013). In a broad survey, Christodoulopoulos et al. (2010) found that for several downstream tasks, most word clustering algorithms—including Brown and Clark—result in similar task accuracy. For our large-scale setting, the primary issue is then the time to estimate co. For large corpora the existing implementations may require days or weeks, making our feature set less practical than the traditional dense MT features. Consequently, we re-implemented the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Brants (2008) (Predictive), which was originally developed for very large scale language modeling. Our implementation uses multiple threads on a single processor instead of MapReduce. We also added two extensions that are useful for translation features. First, we map all digits to 0. This reduces sparsity while retaining useful patterns such as 0000 (e.g., years) and 0th (e.g., ordinals). Second, we mapped all words occurring fewer than T times to an &lt;unk&gt; token. In our experiment, these two changes reduce the vocabulary size by 71.</context>
</contexts>
<marker>Whittaker, Woodland, 2001</marker>
<rawString>E. W. D. Whittaker and P. C. Woodland. 2001. Efficient class-based language modelling for very large vocabularies. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wuebker</author>
<author>S Peitz</author>
<author>F Rietig</author>
<author>H Ney</author>
</authors>
<title>Improving statistical machine translation with word class models.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="36862" citStr="Wuebker et al. (2013)" startWordPosition="6638" endWordPosition="6641">conditioned on target context. This is a good idea that we plan to explore. However, they do not mention if their non-local features declare recombination state. Our empirical experience is that non-local features are less effective when they do not influence recombination. Liang et al. (2006a) proposed replacing lexical items with supervised part-of-speech (POS) tags to reduce sparsity. This is a natural idea that lay dormant until recently. Ammar et al. (2013) incorporated unigram and bigram target class features. Yu et al. (2013) used word classes as backoff features to reduce overfitting. Wuebker et al. (2013) replaced all lexical items in the bitext and monolingual data with classes, and estimated the dense feature set. 11Space limitations preclude discussion of re-ranking features. Then they added these dense class-based features to the baseline lexicalized system. Finally, Cherry (2013) experimented with class-based hierarchical reordering features. However, his features used a bespoke representation rather than the simple full rule string that we use. 7.2 Domain Adaptation with Features Both Clark et al. (2012) and Wang et al. (2012) augmented the baseline dense feature set with domain labels. </context>
</contexts>
<marker>Wuebker, Peitz, Rietig, Ney, 2013</marker>
<rawString>J. Wuebker, S. Peitz, F. Rietig, and H. Ney. 2013. Improving statistical machine translation with word class models. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F Xia</author>
<author>F Chiou</author>
<author>M Palmer</author>
</authors>
<title>The Penn Chinese Treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="23313" citStr="Xue et al., 2005" startWordPosition="3685" endWordPosition="3688">ed the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. DENSE is the same baseline as Green et al. 5We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). 6Data sources: tune, MT023568; dev, MT04; dev-dom, domain adaptation dev set is MT04 and all wb and bn data from LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En); test2, Progress0809 which was revealed in the OpenMT 2012 evaluation; test3, MetricsMATR08-10. 7System settings: distortion limit of 5, cube pruning beam size of 1200, maximum phrase length of 7. 8Other learning settings: 16 threads, mini-batch size of 20; Ll regularization strength λ = 0.001; learning rate η0 = 0.02; initialization of LM to 0.5, word penalty to -1.0, and all other dense features to 0.2; initialization of extended </context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The Penn Chinese Treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>L Huang</author>
<author>H Mi</author>
<author>K Zhao</author>
</authors>
<title>Maxviolation perceptron and forced decoding for scalable MT training.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="17031" citStr="Yu et al., 2013" startWordPosition="2698" endWordPosition="2701">e signed distortion. 3.6 Feature Dependencies While unigram counts are trivial to compute, the same is not necessarily true of the word-to-class mapping co. Standard algorithms run in O(n2), where n = |V |. Table 1 shows an evaluation of standard implementations of several popular algorithms: Brown et al. (1992) implemented by Liang 469 (2005); Clark (2003) without the morphological prior, which increases training time dramatically; and the implementation of Och (1999) that comes with the GIZA++ word aligner. The latter has been used recently for MT features (Ammar et al., 2013; Cherry, 2013; Yu et al., 2013). In a broad survey, Christodoulopoulos et al. (2010) found that for several downstream tasks, most word clustering algorithms—including Brown and Clark—result in similar task accuracy. For our large-scale setting, the primary issue is then the time to estimate co. For large corpora the existing implementations may require days or weeks, making our feature set less practical than the traditional dense MT features. Consequently, we re-implemented the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Brants (2008) (Predi</context>
<context position="36113" citStr="Yu et al. (2013)" startWordPosition="6523" endWordPosition="6526"> work: unfiltered lexicalized rule indicators and alignments (Liang et al., 2006a); rule shape (Hopkins and May, 2011); rule orientation (Liang et al., 2006b; Cherry, 2013); target unigram class (Ammar et al., 2013). We found that other prior features did not improve translation: higher-order target lexical n-grams (Liang et al., 2006a; Watanabe et al., 2007; Gimpel and Smith, 2012b), higher-order target class n-grams (Ammar et al., 2013), target word insertion (Watanabe et al., 2007; Chiang et al., 2009), and many other unpublished ideas transmitted through received wisdom. To our knowledge, Yu et al. (2013) were the first to experiment with non-local (derivation) features for phrase-based MT. They added discriminative rule features conditioned on target context. This is a good idea that we plan to explore. However, they do not mention if their non-local features declare recombination state. Our empirical experience is that non-local features are less effective when they do not influence recombination. Liang et al. (2006a) proposed replacing lexical items with supervised part-of-speech (POS) tags to reduce sparsity. This is a natural idea that lay dormant until recently. Ammar et al. (2013) incor</context>
</contexts>
<marker>Yu, Huang, Mi, Zhao, 2013</marker>
<rawString>H. Yu, L. Huang, H. Mi, and K. Zhao. 2013. Maxviolation perceptron and forced decoding for scalable MT training. In EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
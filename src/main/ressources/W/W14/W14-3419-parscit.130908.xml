<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000191">
<title confidence="0.764725">
FFTM: A Fuzzy Feature Transformation Method for Medical Documents
</title>
<author confidence="0.928192">
Amir Karami, Aryya Gangopadhyay
</author>
<affiliation confidence="0.9643955">
Information Systems Department
University of Maryland Baltimore County
</affiliation>
<address confidence="0.963">
Baltimore, MD, 21250
</address>
<email confidence="0.999101">
amir3@umbc.edu,gangopad@umbc.edu
</email>
<sectionHeader confidence="0.994801" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999816565217391">
The vast array of medical text data repre-
sents a valuable resource that can be an-
alyzed to advance the state of the art in
medicine. Currently, text mining meth-
ods are being used to analyze medical re-
search and clinical text data. Some of the
main challenges in text analysis are high
dimensionality and noisy data. There is a
need to develop novel feature transforma-
tion methods that help reduce the dimen-
sionality of data and improve the perfor-
mance of machine learning algorithms. In
this paper we present a feature transfor-
mation method named FFTM. We illus-
trate the efficacy of our method using lo-
cal term weighting, global term weighting,
and Fuzzy clustering methods and show
that the quality of text analysis in medical
text documents can be improved. We com-
pare FFTM with Latent Dirichlet Alloca-
tion (LDA) by using two different datasets
and statistical tests show that FFTM out-
performs LDA.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982333333333">
The exponential growth of medical text data
makes it difficult to extract useful information in a
structured format. Some important features of text
data are sparsity and high dimensionality. This
means that while there may be a large number
of terms in most of the documents in a corpus,
any one document may contain a small percentage
of those terms (Aggarwal and Zhai, 2012). This
characteristic of medical text data makes feature
transformation an important step in text analysis.
Feature transformation is a pre-processing step in
many machine-learning methods that is used to
characterize text data in terms of a different num-
ber of attributes in lower dimensions. This tech-
nique has a direct impact on the quality of text
mining methods. Topic models such as LDA has
been used as one of popular feature transforma-
tion techniques (Ramage et al., 2010). However,
fuzzy clustering methods, particularly in combina-
tion with term weighting methods, have not been
explored much in medical text mining.
In this research, we propose a new method
called FFTM to extract features from free-text
data. The rest of the paper is organized in the fol-
lowing sections. In the section 2, we review re-
lated work. Section 3 contains details about our
method. Section 4 describes our experiments, per-
formance evaluation, and discussions of our re-
sults. Finally we present a summary, limitations,
and future work in the last section.
</bodyText>
<sectionHeader confidence="0.999791" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999919125">
Text analysis is an important topic in medical in-
formatics that is challenging due to high sparse
dimensionality data. Big dimension and diver-
sity of text datasets have been motivated medi-
cal researchers to use more feature transforma-
tion methods. Feature transformation methods en-
capsulate a text corpus in smaller dimensions by
merging the initial features. Topic model is one of
popular feature transformation methods. Among
topic models, LDA (Blei et al., 2003) has been
considered more due to its better performance
(Ghassemi et al., 2012; Lee et al., 2010).
One of methods that has not been fully con-
sidered in medical text mining is Fuzzy cluster-
ing. Although most of Fuzzy Clusterings work
in medical literature is based on image analysis
(Saha and Maulik, 2014; Cui et al., 2013; Beevi
and Sathik, 2012), a few work have been done
in medical text mining (Ben-Arieh and Gullipalli,
2012; Fenza et al., 2012) by using fuzzy cluster-
ing. The main difference between our method and
other document fuzzy clustering such as (Singh et
al., 2011) is that our method use fuzzy clustering
and word weighting as a pre-processing step for
</bodyText>
<page confidence="0.961105">
128
</page>
<bodyText confidence="0.913952230769231">
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 128–133,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
feature transformation before implementing any
classification and clustering algorithms; however,
other methods use fuzzy clustering as a final step
to cluster the documents. Our main contribution
is to improve the quality of input data to improve
the output of fuzzy clustering. Among fuzzy clus-
tering methods, Fuzzy C-means (Bezdek, 1981)
is the most popular one (Bataineh et al., 2011).
In this research, we propose a novel method that
combines local term weighting and global term
weighting with fuzzy clustering.
</bodyText>
<sectionHeader confidence="0.970664" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999825636363636">
In this section, we detail our Fuzzy Feature Trans-
formation Method (FFTM) and describe the steps.
We begin with a brief review of LDA.
LDA is a topic model that can extract hidden
topics from a collection of documents. It assumes
that each document is a mixture of topics. The out-
put of LDA are the topic distributions over docu-
ments and the word distributions over topics. In
this research, we use the topics distributions over
documents. LDA uses term frequency for local
term weighting.
Now we introduce FFTM concepts and nota-
tions. This model has three main steps includ-
ing Local Term Weighting (LTW), Global Term
Weighting (GTM), and Fuzzy Clustering (Algo-
rithm 1). In this algorithm, each step is the out
put of each step will be the input of the next step.
Step 1: The first step is to calculate LTW.
Among different LTW methods we use term fre-
quency as a popular method. Symbol fij defines
the number of times term i happens in document
j.We have n documents and m words.Let
</bodyText>
<equation confidence="0.9919524">
(
1 fij &gt; 0
b(fij) = (1)
0 fij = 0
pij = Pj fij
</equation>
<bodyText confidence="0.808637647058824">
fij (2)
The outputs of this step are b(fij), fij , and pij.
We use them as inputs for the second step.
Step 2: The next step is to calculate GTW. We
explore four GTW methods in this paper includ-
ing Entropy, Inverse Document Frequency (IDF),
Probabilistic Inverse Document Frequency (Pro-
bIDF), and Normal(Table 1).
IDF assigns higher weights to rare terms and
lower weights to common terms (Papineni, 2001).
ProbIDF is similar to IDF and assigns very low
Algorithm 1 FFTM algorithm
Functions:E():Entropy;I():IDF;PI():ProbIDF;
NO():Normal; FC():Fuzzy Clustering.
Input: Document Term Matrix
Output: Clustering membership value (µij)
for all documents and clusters.
</bodyText>
<listItem confidence="0.948418875">
1: Remove stop words
Step 1: Calculate LTW
2: fori = 1 to ndo
3: forj = 1 to mdo
4: Calculate fij, b(fij), pij
5: endfor
6: endfor
Step 2: Calculate GTW
7: fori = 1 to ndo
8: forj = 1 to mdo
9: Execute E(pij,n),I(fij,n),PI(b(fij),n),
NO(fij,n)
10: endfor
11: endfor
Step 3: Perform Fuzzy Clustering
12: Execute FC(E),FC(I),FC(PI),FC(NO)
</listItem>
<sectionHeader confidence="0.274399" genericHeader="method">
Table1: GTW Methods
</sectionHeader>
<table confidence="0.936657727272727">
Name Formula
Entropy P� pij lo92 (pij)
1 +
lo92 n
IDF
log2
Pnfij
ProbIDF log2 n−Pj b(fij)
Pj b(fij)
Normal 1
qP j f2 ij
</table>
<bodyText confidence="0.9996614">
negative weight for the terms happen in every doc-
ument (Kolda, 1998). In Entropy, it gives higher
weight for the terms happen less in few documents
(Dumais, 1992). Finally, Normal is used to correct
discrepancies in document lengths and also nor-
malize the document vectors. The outputs of this
step are the inputs of the last step.
Step 3: Fuzzy clustering is a soft clustering
technique that finds the degree of membership for
each data point in each cluster, as opposed to
assigning a data point only one cluster. Fuzzy
clustering is a synthesis between clustering and
fuzzy set theory. Among fuzzy clustering meth-
ods, Fuzzy C-means (FCM) is the most popular
one and its goal is to minimize an objective func-
</bodyText>
<page confidence="0.958379">
129
</page>
<bodyText confidence="0.488762">
tion by considering constraints:
</bodyText>
<equation confidence="0.9862643">
Min Jq(µ, V, X) =
subject to:
0 &lt; µij &lt; 1; (4)
i E {1, .., c} and j E {1, ..., n} (5)
Xc µij = 1 (6)
i=1
n
0 &lt; X µij &lt; n; (7)
j=1
Where:
</equation>
<bodyText confidence="0.94041225">
n= number of data
c= number of clusters
µij= membership value
q= fuzzifier, 1 &lt; q &lt; oc
</bodyText>
<equation confidence="0.926934333333334">
V = cluster center vector
Dij = d(xj, vi)= distance between xj and vi
By optimizing eq.3:
1
D 2 (8)
Pck=1( Dki
q−1
Pnj=1(µij)qxj Pn (9)
j=1(µij)q
</equation>
<bodyText confidence="0.997944666666667">
The iterations in the clustering algorithms con-
tinue till the the maximum changes in µij becomes
less than or equal to a pre-specified threshold. The
computational time complexity is O(n). We use
µij as the degree of clusters’ membership for each
document.
</bodyText>
<sectionHeader confidence="0.997372" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.998974555555556">
In this section, we evaluate FFTM against LDA
using two measures: document clustering inter-
nal metrics and document classification evalua-
tion metrics by using one available text datasets.
We use Weka1for classification evaluation, MAL-
LET2package with its default setting for imple-
menting LDA, Matlab fcm package3for imple-
menting FCM clustering, and CVAP Matlab pack-
age4for clustering validation.
</bodyText>
<footnote confidence="0.99998275">
1http://www.cs.waikato.ac.nz/ml/weka/
2http://mallet.cs.umass.edu/
3http://tinyurl.com/kl33w67
4http://tinyurl.com/kb5bwnm
</footnote>
<subsectionHeader confidence="0.949466">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.99999615">
We leverage two available datasets in this re-
search. Our first test dataset called Deidentified
Medical Text5 is an unlabeled corpus of 2434
nursing notes with 12,877 terms after removing
stop words. The second dataset 6 is a labeled cor-
pus of English scientific medical abstracts from
Springer website. It is included 41 medical jour-
nals ranging from Neurology to Radiology. In this
research, we use the first 10 journals including:
Arthroscopy, Federal health standard sheet, The
anesthetist, The surgeon, The gynecologist, The
dermatologist, The internist, The neurologist, The
Ophthalmology, The orthopedist, and The pathol-
ogist. In our experiments we select three subsets
from the above journals, the first two with 4012
terms and 171 documents, first five with 14189
terms and 1527 documents, and then all ten re-
spectively with 23870 terms and 3764 documents
to track the performance of FFTM and LDA by
increasing the number of documents and labels.
</bodyText>
<subsectionHeader confidence="0.994415">
4.2 Document Clustering
</subsectionHeader>
<bodyText confidence="0.999931666666667">
The first evaluation comparing FFTM with LDA is
document clustering by using the first dataset. In-
ternal and external validation are two major meth-
ods for clustering validation; however, compari-
son between these two major methods shows that
internal validation is more more precise (Rend´on
et al., 2011). We evaluate different number of fea-
tures (topics) and clusters by using two internal
clustering validation methods including Silhouette
index and Calinski-Harabasz index using K-means
with 500 iterations. Silhouette index shows that
how closely related are objects in a cluster and
how distinct a cluster from other other clusters.
The higher value means the better result.The Sil-
houette index (S) is defined as:
</bodyText>
<equation confidence="0.991138">
S(i) = (b(i) − a(i)) (10)
Max{a(i),b(i)}
</equation>
<bodyText confidence="0.869185142857143">
Where a(i) is the average dissimilarity of sam-
ple i with the same data in a cluster and b(i) is the
minimum average dissimilarity of sample i with
other data that are not in the same cluster.
Calinski-Harabasz index (CH) valuates the
cluster validity based on the average between- and
within-cluster sum of squares.It is defined as:
</bodyText>
<footnote confidence="0.992791">
5http://tinyurl.com/kfz2hm4
6http://tinyurl.com/m2c8se6
</footnote>
<equation confidence="0.998648125">
(µij)qD2ij (3)
Xc
i=1
n
X
j=1
µij =
vi =
</equation>
<page confidence="0.992542">
130
</page>
<figure confidence="0.977337590163934">
Silhouette Index
Silhouette Index
Silhouette Index
0.8
0.6
0.4
0.2
0
2 3 4 5 6 7 8
# Clusters
(a) 50 Features
0.8
0.6
0.4
0.2
0
2 3 4 5 6 7 8
# Clusters
(b) 100 Features
0.8
0.6
0.4
0.2
0
2 3 4 5 6 7 8
# Clusters
(c) 150 Features
Figure1: Clustering Validation with Silhouette Index
Figure2: Clustering Validation with Calinski-Harabasz Index
4.3 Document Classification
# Clusters
(a) 50 Features
# Clusters
(c) 150 Features
# Clusters
(b) 100 Features
FFTM(Entropy) FFTM(IDF) FFTM(ProbIDF) FFTM(Normal) LDA
Calinski-Harabasz Index
0.5
1.5
0
1
·104
2 3 4 5 6 7 8
Calinski-Harabasz Index
0.5
1.5
0
1
·104
·104
2 3 4 5 6 7 8
Calinski-Harabasz Index
0.5
1.5
0
1
2 3 4 5 6 7 8
trace(5B) np − 1 (11)
CH = np − k
trace(5W).
</figure>
<bodyText confidence="0.999814386363636">
Where (5B) is the between-cluster scatter ma-
trix, (5W) the internal scatter matrix, np is the
number of clustered samples, and k is the number
of clusters. Higher value indicates a better clus-
tering. We track the performance of both FFTM
and LDA using different number of clusters rang-
ing from 2 to 8 with different number of features
including 50, 100, and 150. Both Silhouette in-
dex and Calinski-Harabasz index show that FFTM
is the best method with all ranges of features and
clusters (Figures 1 and 2). The gap between FFTM
and LDA does not change a lot by using different
number of features and clusters. LDA has the low-
est performance and Normal has the best perfor-
mance among GTW methods in different ranges of
features and clusters. According to the paired dif-
ference test, the improvement of FFTM over LDA
is statistically significant with a p − value &lt; 0.05
using the two internal clustering validation meth-
ods.
The second evaluation measure is document clas-
sification by using the second datasest. We evalu-
ate different number of classes and features (top-
ics) with accuracy, F-measure, and ROC using
Random Forest. Accuracy is the portion of true re-
sults in a dataset. F-measure is another measure of
classification evaluation that considers both preci-
sion and recall. ROC curves plot False Positive on
the X axis vs. True Positive on the Y axis to find
the trade off between them; therefore, the closer to
the upper left indicates better performance. We
assume more documents and classes have more
topics;therefore, we choose 100 features for two
classes, 150 features for five classes, and 200 fea-
tures for ten classes. In addition, we use 10 cross
validation as test option.
This experiment shows that FFTM has the best
performance in different number of features and
labels (Table 2). LDA has the lowest performance
and the average performance of ProbIDF has the
best among GTW methods in all ranges of features
and clusters. According to the paired difference
test, the improvement of FFTM over LDA is sta-
tistically significant with a p − value &lt; 0.05.
</bodyText>
<page confidence="0.997075">
131
</page>
<tableCaption confidence="0.500605">
Table2: The Second Dataset Classification Performance
</tableCaption>
<table confidence="0.9998419375">
Method #Features # Labels Acc % F-Measure ROC
FFTM(Entropy) 100 2 96.49 0.959 0.982
FFTM(IDF) 100 2 98.24 0.982 0.996
FFTM(ProIDF) 100 2 97.66 0.977 0.987
FFTM(Normal) 100 2 92.39 0.912 0.971
LDA 100 2 90.06 0.9 0.969
FFTM(Entropy) 150 5 71.84 0.694 0.874
FFTM(IDF) 150 5 70.79 0.686 0.859
FFTM(ProIDF) 150 5 70.39 0.674 0.859
FFTM(Normal) 150 5 68.11 0.649 0.851
LDA 150 5 66.27 0.637 0.815
FFTM(Entropy) 200 10 51.06 0.501 0.828
FFTM(IDF) 200 10 51.73 0.506 0.826
FFTM(ProIDF) 200 10 53.72 0.525 0.836
FFTM(Normal) 200 10 50.05 0.485 0.815
LDA 200 10 47.68 0.459 0.792
</table>
<sectionHeader confidence="0.834802" genericHeader="conclusions">
5 Conclusion fuzzy set theory.
</sectionHeader>
<bodyText confidence="0.999984382352942">
The explosive growth of medical text data makes
text analysis as a key requirement to find patterns
in datasets;however, the typical high dimensional-
ity of such features motivates researchers to utilize
dimension reduction techniques such as LDA. Al-
though LDA has been considered more recently in
medical text analysis (Jimeno-Yepes et al., 2011),
fuzzy clustering methods such as FCM has not
been used in medical text clustering, but rather in
image processing. In the current study, we pro-
pose a method called FFTM to combine LTW and
GTM with Fuzzy clustering, and compare its per-
formance with that of LDA. We use different sets
of data including different number of features, dif-
ferent number of clusters, and different number of
classes.The findings of this study show that com-
bining FCM with LTW and GTW methods can
significantly improve medical documents analysis.
We conclude that different factors including num-
ber of features, number of clusters, and classes
can affect the outputs of machine learning algo-
rithms. In addition, the performance of FFTM is
improved by using GTW methods. This method
proposed in this paper may be applied to other
medical documents to improve text analysis out-
puts. One limitation of this paper is that we use
one clustering method, one classification method,
and two internal clustering validation methods for
evaluation. Our future direction is to explore more
machine learning algorithms and clustering vali-
dation methods for evaluation and also other fuzzy
clustering algorithms for feature transformation.
The main goal of future research is to present an
efficient and effective medical topic model using
</bodyText>
<sectionHeader confidence="0.998306" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998631617647059">
CharuC Aggarwal and ChengXiang Zhai. 2012. An
introduction to text mining. In Mining Text Data,
pages 1–10. Springer.
KMBataineh, MNaji, and MSaqer. 2011. A compar-
ison study between various fuzzy clustering algo-
rithms. Jordan Journal of Mechanical &amp; Industrial
Engineering, 5(4).
Zulaikha Beevi and Mohamed Sathik. 2012. A ro-
bust segmentation approach for noisy medical im-
ages using fuzzy clustering with spatial probability.
International Arab Journal of Information Technol-
ogy (IAJIT), 9(1).
David Ben-Arieh and DeepKumar Gullipalli. 2012.
Data envelopment analysis of clinics with sparse
data: Fuzzy clustering approach. Computers &amp; In-
dustrial Engineering, 63(1):13–21.
JamesC Bezdek. 1981. Pattern recognition with fuzzy
objective function algorithms. Kluwer Academic
Publishers.
DavidM Blei, AndrewY Ng, and MichaelI Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.
Wenchao Cui, YiWang, Yangyu Fan, Yan Feng, and
Tao Lei. 2013. Global and local fuzzy clustering
with spatial information for medical image segmen-
tation. In Signal and Information Processing (Chi-
naSIP), 2013 IEEE China Summit &amp; International
Conference on, pages 533–537. IEEE.
Susan Dumais. 1992. Enhancing performance in latent
semantic indexing (lsi) retrieval.
Giuseppe Fenza, Domenico Furno, and Vincenzo Loia.
2012. Hybrid approach for context-aware service
discovery in healthcare domain. Journal of Com-
puter and System Sciences, 78(4):1232–1247.
</reference>
<page confidence="0.974909">
132
</page>
<reference confidence="0.999693904761905">
Marzyeh Ghassemi, Tristan Naumann, Rohit Joshi, and
Anna Rumshisky. 2012. Topic models for mortality
modeling in intensive care units. In ICML Machine
Learning for Clinical Data Analysis Workshop.
Antonio Jimeno-Yepes, Bartłomiej Wilkowski,
JamesG Mork, Elizabeth VanLenten, DinaDemner
Fushman, and AlanR Aronson. 2011. A bottom-up
approach to medline indexing recommendations.
In AMIA Annual Symposium Proceedings, volume
2011, page 1583. American Medical Informatics
Association.
TamaraG Kolda. 1998. Limited-memory matrix meth-
ods with applications.
Sangno Lee, Jeff Baker, Jaeki Song, and JamesC
Wetherbe. 2010. An empirical comparison of
four text mining methods. In System Sciences
(HICSS), 2010 43rd Hawaii International Confer-
ence on, pages 1–10. IEEE.
Kishore Papineni. 2001. Why inverse document fre-
quency? In Proceedings of the second meeting of
the North American Chapter of the Association for
Computational Linguistics on Language technolo-
gies, pages 1–8. Association for Computational Lin-
guistics.
Daniel Ramage, SusanT Dumais, and DanielJ Liebling.
2010. Characterizing microblogs with topic models.
In ICWSM.
Er´endira Rend´on, Itzel Abundez, Alejandra Arizmendi,
and ElviaM Quiroz. 2011. Internal versus external
cluster validation indexes. International Journal of
computers and communications, 5(1):27–34.
Indrajit Saha and Ujjwal Maulik. 2014. Multiobjective
differential evolution-based fuzzy clustering for mr
brain image segmentation image segmentation. In
Advanced Computational Approaches to Biomedical
Engineering, pages 71–86. Springer.
VivekKumar Singh, Nisha Tiwari, and Shekhar Garg.
2011. Document clustering using k-means, heuris-
tic k-means and fuzzy c-means. In Computational
Intelligence and Communication Networks (CICN),
2011 International Conference on, pages 297–301.
IEEE.
</reference>
<page confidence="0.999133">
133
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.389074">
<title confidence="0.999774">FFTM: A Fuzzy Feature Transformation Method for Medical Documents</title>
<author confidence="0.983633">Amir Karami</author>
<author confidence="0.983633">Aryya</author>
<affiliation confidence="0.854539666666667">Information Systems University of Maryland Baltimore Baltimore, MD,</affiliation>
<email confidence="0.994416">amir3@umbc.edu,gangopad@umbc.edu</email>
<abstract confidence="0.98747475">The vast array of medical text data represents a valuable resource that can be analyzed to advance the state of the art in medicine. Currently, text mining methods are being used to analyze medical research and clinical text data. Some of the main challenges in text analysis are high dimensionality and noisy data. There is a need to develop novel feature transformation methods that help reduce the dimensionality of data and improve the performance of machine learning algorithms. In this paper we present a feature transformation method named FFTM. We illustrate the efficacy of our method using local term weighting, global term weighting, and Fuzzy clustering methods and show that the quality of text analysis in medical text documents can be improved. We compare FFTM with Latent Dirichlet Allocation (LDA) by using two different datasets and statistical tests show that FFTM outperforms LDA.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>CharuC Aggarwal</author>
<author>ChengXiang Zhai</author>
</authors>
<title>An introduction to text mining.</title>
<date>2012</date>
<booktitle>In Mining Text Data,</booktitle>
<pages>1--10</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1525" citStr="Aggarwal and Zhai, 2012" startWordPosition="244" endWordPosition="247">hods and show that the quality of text analysis in medical text documents can be improved. We compare FFTM with Latent Dirichlet Allocation (LDA) by using two different datasets and statistical tests show that FFTM outperforms LDA. 1 Introduction The exponential growth of medical text data makes it difficult to extract useful information in a structured format. Some important features of text data are sparsity and high dimensionality. This means that while there may be a large number of terms in most of the documents in a corpus, any one document may contain a small percentage of those terms (Aggarwal and Zhai, 2012). This characteristic of medical text data makes feature transformation an important step in text analysis. Feature transformation is a pre-processing step in many machine-learning methods that is used to characterize text data in terms of a different number of attributes in lower dimensions. This technique has a direct impact on the quality of text mining methods. Topic models such as LDA has been used as one of popular feature transformation techniques (Ramage et al., 2010). However, fuzzy clustering methods, particularly in combination with term weighting methods, have not been explored muc</context>
</contexts>
<marker>Aggarwal, Zhai, 2012</marker>
<rawString>CharuC Aggarwal and ChengXiang Zhai. 2012. An introduction to text mining. In Mining Text Data, pages 1–10. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MNaji KMBataineh</author>
<author>MSaqer</author>
</authors>
<title>A comparison study between various fuzzy clustering algorithms.</title>
<date>2011</date>
<journal>Jordan Journal of Mechanical &amp; Industrial Engineering,</journal>
<volume>5</volume>
<issue>4</issue>
<marker>KMBataineh, MSaqer, 2011</marker>
<rawString>KMBataineh, MNaji, and MSaqer. 2011. A comparison study between various fuzzy clustering algorithms. Jordan Journal of Mechanical &amp; Industrial Engineering, 5(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zulaikha Beevi</author>
<author>Mohamed Sathik</author>
</authors>
<title>A robust segmentation approach for noisy medical images using fuzzy clustering with spatial probability.</title>
<date>2012</date>
<journal>International Arab Journal of Information Technology (IAJIT),</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="3395" citStr="Beevi and Sathik, 2012" startWordPosition="551" endWordPosition="554">al researchers to use more feature transformation methods. Feature transformation methods encapsulate a text corpus in smaller dimensions by merging the initial features. Topic model is one of popular feature transformation methods. Among topic models, LDA (Blei et al., 2003) has been considered more due to its better performance (Ghassemi et al., 2012; Lee et al., 2010). One of methods that has not been fully considered in medical text mining is Fuzzy clustering. Although most of Fuzzy Clusterings work in medical literature is based on image analysis (Saha and Maulik, 2014; Cui et al., 2013; Beevi and Sathik, 2012), a few work have been done in medical text mining (Ben-Arieh and Gullipalli, 2012; Fenza et al., 2012) by using fuzzy clustering. The main difference between our method and other document fuzzy clustering such as (Singh et al., 2011) is that our method use fuzzy clustering and word weighting as a pre-processing step for 128 Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 128–133, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics feature transformation before implementing any classification and clustering </context>
</contexts>
<marker>Beevi, Sathik, 2012</marker>
<rawString>Zulaikha Beevi and Mohamed Sathik. 2012. A robust segmentation approach for noisy medical images using fuzzy clustering with spatial probability. International Arab Journal of Information Technology (IAJIT), 9(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ben-Arieh</author>
<author>DeepKumar Gullipalli</author>
</authors>
<title>Data envelopment analysis of clinics with sparse data: Fuzzy clustering approach.</title>
<date>2012</date>
<journal>Computers &amp; Industrial Engineering,</journal>
<volume>63</volume>
<issue>1</issue>
<contexts>
<context position="3477" citStr="Ben-Arieh and Gullipalli, 2012" startWordPosition="565" endWordPosition="568">mation methods encapsulate a text corpus in smaller dimensions by merging the initial features. Topic model is one of popular feature transformation methods. Among topic models, LDA (Blei et al., 2003) has been considered more due to its better performance (Ghassemi et al., 2012; Lee et al., 2010). One of methods that has not been fully considered in medical text mining is Fuzzy clustering. Although most of Fuzzy Clusterings work in medical literature is based on image analysis (Saha and Maulik, 2014; Cui et al., 2013; Beevi and Sathik, 2012), a few work have been done in medical text mining (Ben-Arieh and Gullipalli, 2012; Fenza et al., 2012) by using fuzzy clustering. The main difference between our method and other document fuzzy clustering such as (Singh et al., 2011) is that our method use fuzzy clustering and word weighting as a pre-processing step for 128 Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 128–133, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics feature transformation before implementing any classification and clustering algorithms; however, other methods use fuzzy clustering as a final step to cluster</context>
</contexts>
<marker>Ben-Arieh, Gullipalli, 2012</marker>
<rawString>David Ben-Arieh and DeepKumar Gullipalli. 2012. Data envelopment analysis of clinics with sparse data: Fuzzy clustering approach. Computers &amp; Industrial Engineering, 63(1):13–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JamesC Bezdek</author>
</authors>
<title>Pattern recognition with fuzzy objective function algorithms.</title>
<date>1981</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="4258" citStr="Bezdek, 1981" startWordPosition="684" endWordPosition="685">hod use fuzzy clustering and word weighting as a pre-processing step for 128 Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 128–133, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics feature transformation before implementing any classification and clustering algorithms; however, other methods use fuzzy clustering as a final step to cluster the documents. Our main contribution is to improve the quality of input data to improve the output of fuzzy clustering. Among fuzzy clustering methods, Fuzzy C-means (Bezdek, 1981) is the most popular one (Bataineh et al., 2011). In this research, we propose a novel method that combines local term weighting and global term weighting with fuzzy clustering. 3 Method In this section, we detail our Fuzzy Feature Transformation Method (FFTM) and describe the steps. We begin with a brief review of LDA. LDA is a topic model that can extract hidden topics from a collection of documents. It assumes that each document is a mixture of topics. The output of LDA are the topic distributions over documents and the word distributions over topics. In this research, we use the topics dis</context>
</contexts>
<marker>Bezdek, 1981</marker>
<rawString>JamesC Bezdek. 1981. Pattern recognition with fuzzy objective function algorithms. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DavidM Blei</author>
<author>AndrewY Ng</author>
<author>MichaelI Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="3048" citStr="Blei et al., 2003" startWordPosition="490" endWordPosition="493">, performance evaluation, and discussions of our results. Finally we present a summary, limitations, and future work in the last section. 2 Related Work Text analysis is an important topic in medical informatics that is challenging due to high sparse dimensionality data. Big dimension and diversity of text datasets have been motivated medical researchers to use more feature transformation methods. Feature transformation methods encapsulate a text corpus in smaller dimensions by merging the initial features. Topic model is one of popular feature transformation methods. Among topic models, LDA (Blei et al., 2003) has been considered more due to its better performance (Ghassemi et al., 2012; Lee et al., 2010). One of methods that has not been fully considered in medical text mining is Fuzzy clustering. Although most of Fuzzy Clusterings work in medical literature is based on image analysis (Saha and Maulik, 2014; Cui et al., 2013; Beevi and Sathik, 2012), a few work have been done in medical text mining (Ben-Arieh and Gullipalli, 2012; Fenza et al., 2012) by using fuzzy clustering. The main difference between our method and other document fuzzy clustering such as (Singh et al., 2011) is that our method</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>DavidM Blei, AndrewY Ng, and MichaelI Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenchao Cui</author>
<author>Yangyu Fan YiWang</author>
<author>Yan Feng</author>
<author>Tao Lei</author>
</authors>
<title>Global and local fuzzy clustering with spatial information for medical image segmentation.</title>
<date>2013</date>
<booktitle>In Signal and Information Processing (ChinaSIP), 2013 IEEE China Summit &amp; International Conference on,</booktitle>
<pages>533--537</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3370" citStr="Cui et al., 2013" startWordPosition="547" endWordPosition="550">en motivated medical researchers to use more feature transformation methods. Feature transformation methods encapsulate a text corpus in smaller dimensions by merging the initial features. Topic model is one of popular feature transformation methods. Among topic models, LDA (Blei et al., 2003) has been considered more due to its better performance (Ghassemi et al., 2012; Lee et al., 2010). One of methods that has not been fully considered in medical text mining is Fuzzy clustering. Although most of Fuzzy Clusterings work in medical literature is based on image analysis (Saha and Maulik, 2014; Cui et al., 2013; Beevi and Sathik, 2012), a few work have been done in medical text mining (Ben-Arieh and Gullipalli, 2012; Fenza et al., 2012) by using fuzzy clustering. The main difference between our method and other document fuzzy clustering such as (Singh et al., 2011) is that our method use fuzzy clustering and word weighting as a pre-processing step for 128 Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 128–133, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics feature transformation before implementing any class</context>
</contexts>
<marker>Cui, YiWang, Feng, Lei, 2013</marker>
<rawString>Wenchao Cui, YiWang, Yangyu Fan, Yan Feng, and Tao Lei. 2013. Global and local fuzzy clustering with spatial information for medical image segmentation. In Signal and Information Processing (ChinaSIP), 2013 IEEE China Summit &amp; International Conference on, pages 533–537. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Dumais</author>
</authors>
<title>Enhancing performance in latent semantic indexing (lsi) retrieval.</title>
<date>1992</date>
<contexts>
<context position="6778" citStr="Dumais, 1992" startWordPosition="1126" endWordPosition="1127">1: Calculate LTW 2: fori = 1 to ndo 3: forj = 1 to mdo 4: Calculate fij, b(fij), pij 5: endfor 6: endfor Step 2: Calculate GTW 7: fori = 1 to ndo 8: forj = 1 to mdo 9: Execute E(pij,n),I(fij,n),PI(b(fij),n), NO(fij,n) 10: endfor 11: endfor Step 3: Perform Fuzzy Clustering 12: Execute FC(E),FC(I),FC(PI),FC(NO) Table1: GTW Methods Name Formula Entropy P� pij lo92 (pij) 1 + lo92 n IDF log2 Pnfij ProbIDF log2 n−Pj b(fij) Pj b(fij) Normal 1 qP j f2 ij negative weight for the terms happen in every document (Kolda, 1998). In Entropy, it gives higher weight for the terms happen less in few documents (Dumais, 1992). Finally, Normal is used to correct discrepancies in document lengths and also normalize the document vectors. The outputs of this step are the inputs of the last step. Step 3: Fuzzy clustering is a soft clustering technique that finds the degree of membership for each data point in each cluster, as opposed to assigning a data point only one cluster. Fuzzy clustering is a synthesis between clustering and fuzzy set theory. Among fuzzy clustering methods, Fuzzy C-means (FCM) is the most popular one and its goal is to minimize an objective func129 tion by considering constraints: Min Jq(µ, V, X)</context>
</contexts>
<marker>Dumais, 1992</marker>
<rawString>Susan Dumais. 1992. Enhancing performance in latent semantic indexing (lsi) retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Fenza</author>
<author>Domenico Furno</author>
<author>Vincenzo Loia</author>
</authors>
<title>Hybrid approach for context-aware service discovery in healthcare domain.</title>
<date>2012</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>78</volume>
<issue>4</issue>
<contexts>
<context position="3498" citStr="Fenza et al., 2012" startWordPosition="569" endWordPosition="572">t corpus in smaller dimensions by merging the initial features. Topic model is one of popular feature transformation methods. Among topic models, LDA (Blei et al., 2003) has been considered more due to its better performance (Ghassemi et al., 2012; Lee et al., 2010). One of methods that has not been fully considered in medical text mining is Fuzzy clustering. Although most of Fuzzy Clusterings work in medical literature is based on image analysis (Saha and Maulik, 2014; Cui et al., 2013; Beevi and Sathik, 2012), a few work have been done in medical text mining (Ben-Arieh and Gullipalli, 2012; Fenza et al., 2012) by using fuzzy clustering. The main difference between our method and other document fuzzy clustering such as (Singh et al., 2011) is that our method use fuzzy clustering and word weighting as a pre-processing step for 128 Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 128–133, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics feature transformation before implementing any classification and clustering algorithms; however, other methods use fuzzy clustering as a final step to cluster the documents. Our m</context>
</contexts>
<marker>Fenza, Furno, Loia, 2012</marker>
<rawString>Giuseppe Fenza, Domenico Furno, and Vincenzo Loia. 2012. Hybrid approach for context-aware service discovery in healthcare domain. Journal of Computer and System Sciences, 78(4):1232–1247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marzyeh Ghassemi</author>
<author>Tristan Naumann</author>
<author>Rohit Joshi</author>
<author>Anna Rumshisky</author>
</authors>
<title>Topic models for mortality modeling in intensive care units.</title>
<date>2012</date>
<booktitle>In ICML Machine Learning for Clinical Data Analysis Workshop.</booktitle>
<contexts>
<context position="3126" citStr="Ghassemi et al., 2012" startWordPosition="503" endWordPosition="506">t a summary, limitations, and future work in the last section. 2 Related Work Text analysis is an important topic in medical informatics that is challenging due to high sparse dimensionality data. Big dimension and diversity of text datasets have been motivated medical researchers to use more feature transformation methods. Feature transformation methods encapsulate a text corpus in smaller dimensions by merging the initial features. Topic model is one of popular feature transformation methods. Among topic models, LDA (Blei et al., 2003) has been considered more due to its better performance (Ghassemi et al., 2012; Lee et al., 2010). One of methods that has not been fully considered in medical text mining is Fuzzy clustering. Although most of Fuzzy Clusterings work in medical literature is based on image analysis (Saha and Maulik, 2014; Cui et al., 2013; Beevi and Sathik, 2012), a few work have been done in medical text mining (Ben-Arieh and Gullipalli, 2012; Fenza et al., 2012) by using fuzzy clustering. The main difference between our method and other document fuzzy clustering such as (Singh et al., 2011) is that our method use fuzzy clustering and word weighting as a pre-processing step for 128 Proc</context>
</contexts>
<marker>Ghassemi, Naumann, Joshi, Rumshisky, 2012</marker>
<rawString>Marzyeh Ghassemi, Tristan Naumann, Rohit Joshi, and Anna Rumshisky. 2012. Topic models for mortality modeling in intensive care units. In ICML Machine Learning for Clinical Data Analysis Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Jimeno-Yepes</author>
<author>Bartłomiej Wilkowski</author>
<author>JamesG Mork</author>
<author>Elizabeth VanLenten</author>
<author>DinaDemner Fushman</author>
<author>AlanR Aronson</author>
</authors>
<title>A bottom-up approach to medline indexing recommendations.</title>
<date>2011</date>
<booktitle>In AMIA Annual Symposium Proceedings, volume 2011,</booktitle>
<pages>1583</pages>
<publisher>American Medical Informatics Association.</publisher>
<contexts>
<context position="14513" citStr="Jimeno-Yepes et al., 2011" startWordPosition="2425" endWordPosition="2428">.859 FFTM(Normal) 150 5 68.11 0.649 0.851 LDA 150 5 66.27 0.637 0.815 FFTM(Entropy) 200 10 51.06 0.501 0.828 FFTM(IDF) 200 10 51.73 0.506 0.826 FFTM(ProIDF) 200 10 53.72 0.525 0.836 FFTM(Normal) 200 10 50.05 0.485 0.815 LDA 200 10 47.68 0.459 0.792 5 Conclusion fuzzy set theory. The explosive growth of medical text data makes text analysis as a key requirement to find patterns in datasets;however, the typical high dimensionality of such features motivates researchers to utilize dimension reduction techniques such as LDA. Although LDA has been considered more recently in medical text analysis (Jimeno-Yepes et al., 2011), fuzzy clustering methods such as FCM has not been used in medical text clustering, but rather in image processing. In the current study, we propose a method called FFTM to combine LTW and GTM with Fuzzy clustering, and compare its performance with that of LDA. We use different sets of data including different number of features, different number of clusters, and different number of classes.The findings of this study show that combining FCM with LTW and GTW methods can significantly improve medical documents analysis. We conclude that different factors including number of features, number of </context>
</contexts>
<marker>Jimeno-Yepes, Wilkowski, Mork, VanLenten, Fushman, Aronson, 2011</marker>
<rawString>Antonio Jimeno-Yepes, Bartłomiej Wilkowski, JamesG Mork, Elizabeth VanLenten, DinaDemner Fushman, and AlanR Aronson. 2011. A bottom-up approach to medline indexing recommendations. In AMIA Annual Symposium Proceedings, volume 2011, page 1583. American Medical Informatics Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TamaraG Kolda</author>
</authors>
<title>Limited-memory matrix methods with applications.</title>
<date>1998</date>
<contexts>
<context position="6684" citStr="Kolda, 1998" startWordPosition="1110" endWordPosition="1111"> Clustering membership value (µij) for all documents and clusters. 1: Remove stop words Step 1: Calculate LTW 2: fori = 1 to ndo 3: forj = 1 to mdo 4: Calculate fij, b(fij), pij 5: endfor 6: endfor Step 2: Calculate GTW 7: fori = 1 to ndo 8: forj = 1 to mdo 9: Execute E(pij,n),I(fij,n),PI(b(fij),n), NO(fij,n) 10: endfor 11: endfor Step 3: Perform Fuzzy Clustering 12: Execute FC(E),FC(I),FC(PI),FC(NO) Table1: GTW Methods Name Formula Entropy P� pij lo92 (pij) 1 + lo92 n IDF log2 Pnfij ProbIDF log2 n−Pj b(fij) Pj b(fij) Normal 1 qP j f2 ij negative weight for the terms happen in every document (Kolda, 1998). In Entropy, it gives higher weight for the terms happen less in few documents (Dumais, 1992). Finally, Normal is used to correct discrepancies in document lengths and also normalize the document vectors. The outputs of this step are the inputs of the last step. Step 3: Fuzzy clustering is a soft clustering technique that finds the degree of membership for each data point in each cluster, as opposed to assigning a data point only one cluster. Fuzzy clustering is a synthesis between clustering and fuzzy set theory. Among fuzzy clustering methods, Fuzzy C-means (FCM) is the most popular one and</context>
</contexts>
<marker>Kolda, 1998</marker>
<rawString>TamaraG Kolda. 1998. Limited-memory matrix methods with applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sangno Lee</author>
<author>Jeff Baker</author>
<author>Jaeki Song</author>
<author>JamesC Wetherbe</author>
</authors>
<title>An empirical comparison of four text mining methods.</title>
<date>2010</date>
<booktitle>In System Sciences (HICSS), 2010 43rd Hawaii International Conference on,</booktitle>
<pages>1--10</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3145" citStr="Lee et al., 2010" startWordPosition="507" endWordPosition="510">s, and future work in the last section. 2 Related Work Text analysis is an important topic in medical informatics that is challenging due to high sparse dimensionality data. Big dimension and diversity of text datasets have been motivated medical researchers to use more feature transformation methods. Feature transformation methods encapsulate a text corpus in smaller dimensions by merging the initial features. Topic model is one of popular feature transformation methods. Among topic models, LDA (Blei et al., 2003) has been considered more due to its better performance (Ghassemi et al., 2012; Lee et al., 2010). One of methods that has not been fully considered in medical text mining is Fuzzy clustering. Although most of Fuzzy Clusterings work in medical literature is based on image analysis (Saha and Maulik, 2014; Cui et al., 2013; Beevi and Sathik, 2012), a few work have been done in medical text mining (Ben-Arieh and Gullipalli, 2012; Fenza et al., 2012) by using fuzzy clustering. The main difference between our method and other document fuzzy clustering such as (Singh et al., 2011) is that our method use fuzzy clustering and word weighting as a pre-processing step for 128 Proceedings of the 2014</context>
</contexts>
<marker>Lee, Baker, Song, Wetherbe, 2010</marker>
<rawString>Sangno Lee, Jeff Baker, Jaeki Song, and JamesC Wetherbe. 2010. An empirical comparison of four text mining methods. In System Sciences (HICSS), 2010 43rd Hawaii International Conference on, pages 1–10. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
</authors>
<title>Why inverse document frequency?</title>
<date>2001</date>
<booktitle>In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5881" citStr="Papineni, 2001" startWordPosition="977" endWordPosition="978">nt LTW methods we use term frequency as a popular method. Symbol fij defines the number of times term i happens in document j.We have n documents and m words.Let ( 1 fij &gt; 0 b(fij) = (1) 0 fij = 0 pij = Pj fij fij (2) The outputs of this step are b(fij), fij , and pij. We use them as inputs for the second step. Step 2: The next step is to calculate GTW. We explore four GTW methods in this paper including Entropy, Inverse Document Frequency (IDF), Probabilistic Inverse Document Frequency (ProbIDF), and Normal(Table 1). IDF assigns higher weights to rare terms and lower weights to common terms (Papineni, 2001). ProbIDF is similar to IDF and assigns very low Algorithm 1 FFTM algorithm Functions:E():Entropy;I():IDF;PI():ProbIDF; NO():Normal; FC():Fuzzy Clustering. Input: Document Term Matrix Output: Clustering membership value (µij) for all documents and clusters. 1: Remove stop words Step 1: Calculate LTW 2: fori = 1 to ndo 3: forj = 1 to mdo 4: Calculate fij, b(fij), pij 5: endfor 6: endfor Step 2: Calculate GTW 7: fori = 1 to ndo 8: forj = 1 to mdo 9: Execute E(pij,n),I(fij,n),PI(b(fij),n), NO(fij,n) 10: endfor 11: endfor Step 3: Perform Fuzzy Clustering 12: Execute FC(E),FC(I),FC(PI),FC(NO) Table</context>
</contexts>
<marker>Papineni, 2001</marker>
<rawString>Kishore Papineni. 2001. Why inverse document frequency? In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>SusanT Dumais</author>
<author>DanielJ Liebling</author>
</authors>
<title>Characterizing microblogs with topic models.</title>
<date>2010</date>
<booktitle>In ICWSM.</booktitle>
<contexts>
<context position="2005" citStr="Ramage et al., 2010" startWordPosition="322" endWordPosition="325">mber of terms in most of the documents in a corpus, any one document may contain a small percentage of those terms (Aggarwal and Zhai, 2012). This characteristic of medical text data makes feature transformation an important step in text analysis. Feature transformation is a pre-processing step in many machine-learning methods that is used to characterize text data in terms of a different number of attributes in lower dimensions. This technique has a direct impact on the quality of text mining methods. Topic models such as LDA has been used as one of popular feature transformation techniques (Ramage et al., 2010). However, fuzzy clustering methods, particularly in combination with term weighting methods, have not been explored much in medical text mining. In this research, we propose a new method called FFTM to extract features from free-text data. The rest of the paper is organized in the following sections. In the section 2, we review related work. Section 3 contains details about our method. Section 4 describes our experiments, performance evaluation, and discussions of our results. Finally we present a summary, limitations, and future work in the last section. 2 Related Work Text analysis is an im</context>
</contexts>
<marker>Ramage, Dumais, Liebling, 2010</marker>
<rawString>Daniel Ramage, SusanT Dumais, and DanielJ Liebling. 2010. Characterizing microblogs with topic models. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Er´endira Rend´on</author>
<author>Itzel Abundez</author>
<author>Alejandra Arizmendi</author>
<author>ElviaM Quiroz</author>
</authors>
<title>Internal versus external cluster validation indexes.</title>
<date>2011</date>
<journal>International Journal of computers and communications,</journal>
<pages>5--1</pages>
<marker>Rend´on, Abundez, Arizmendi, Quiroz, 2011</marker>
<rawString>Er´endira Rend´on, Itzel Abundez, Alejandra Arizmendi, and ElviaM Quiroz. 2011. Internal versus external cluster validation indexes. International Journal of computers and communications, 5(1):27–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Indrajit Saha</author>
<author>Ujjwal Maulik</author>
</authors>
<title>Multiobjective differential evolution-based fuzzy clustering for mr brain image segmentation image segmentation.</title>
<date>2014</date>
<booktitle>In Advanced Computational Approaches to Biomedical Engineering,</booktitle>
<pages>71--86</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3352" citStr="Saha and Maulik, 2014" startWordPosition="543" endWordPosition="546">f text datasets have been motivated medical researchers to use more feature transformation methods. Feature transformation methods encapsulate a text corpus in smaller dimensions by merging the initial features. Topic model is one of popular feature transformation methods. Among topic models, LDA (Blei et al., 2003) has been considered more due to its better performance (Ghassemi et al., 2012; Lee et al., 2010). One of methods that has not been fully considered in medical text mining is Fuzzy clustering. Although most of Fuzzy Clusterings work in medical literature is based on image analysis (Saha and Maulik, 2014; Cui et al., 2013; Beevi and Sathik, 2012), a few work have been done in medical text mining (Ben-Arieh and Gullipalli, 2012; Fenza et al., 2012) by using fuzzy clustering. The main difference between our method and other document fuzzy clustering such as (Singh et al., 2011) is that our method use fuzzy clustering and word weighting as a pre-processing step for 128 Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 128–133, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics feature transformation before impl</context>
</contexts>
<marker>Saha, Maulik, 2014</marker>
<rawString>Indrajit Saha and Ujjwal Maulik. 2014. Multiobjective differential evolution-based fuzzy clustering for mr brain image segmentation image segmentation. In Advanced Computational Approaches to Biomedical Engineering, pages 71–86. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>VivekKumar Singh</author>
<author>Nisha Tiwari</author>
<author>Shekhar Garg</author>
</authors>
<title>Document clustering using k-means, heuristic k-means and fuzzy c-means.</title>
<date>2011</date>
<booktitle>In Computational Intelligence and Communication Networks (CICN), 2011 International Conference on,</booktitle>
<pages>297--301</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3629" citStr="Singh et al., 2011" startWordPosition="591" endWordPosition="594"> topic models, LDA (Blei et al., 2003) has been considered more due to its better performance (Ghassemi et al., 2012; Lee et al., 2010). One of methods that has not been fully considered in medical text mining is Fuzzy clustering. Although most of Fuzzy Clusterings work in medical literature is based on image analysis (Saha and Maulik, 2014; Cui et al., 2013; Beevi and Sathik, 2012), a few work have been done in medical text mining (Ben-Arieh and Gullipalli, 2012; Fenza et al., 2012) by using fuzzy clustering. The main difference between our method and other document fuzzy clustering such as (Singh et al., 2011) is that our method use fuzzy clustering and word weighting as a pre-processing step for 128 Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 128–133, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics feature transformation before implementing any classification and clustering algorithms; however, other methods use fuzzy clustering as a final step to cluster the documents. Our main contribution is to improve the quality of input data to improve the output of fuzzy clustering. Among fuzzy clustering methods,</context>
</contexts>
<marker>Singh, Tiwari, Garg, 2011</marker>
<rawString>VivekKumar Singh, Nisha Tiwari, and Shekhar Garg. 2011. Document clustering using k-means, heuristic k-means and fuzzy c-means. In Computational Intelligence and Communication Networks (CICN), 2011 International Conference on, pages 297–301. IEEE.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
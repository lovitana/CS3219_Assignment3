<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.183374">
<title confidence="0.998441">
A Simple Baseline for Discriminating Similar Languages
</title>
<author confidence="0.99719">
Matthew Purver
</author>
<affiliation confidence="0.993667666666667">
Cognitive Science Research Group
School of Electronic Engineering and Computer Science
Queen Mary University of London
</affiliation>
<email confidence="0.997561">
m.purver@qmul.ac.uk
</email>
<sectionHeader confidence="0.99387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952">
This paper describes an approach to discriminating similar languages using word- and character-
based features, submitted as the Queen Mary University of London entry to the Discriminating
Similar Languages shared task. Our motivation was to investigate how well a simple, data-
driven, linguistically naive method could perform, in order to provide a baseline by which more
linguistically complex or knowledge-rich approaches can be judged. Using a standard supervised
classifier with word and character n-grams as features, we achieved over 90% accuracy in the test;
on fixing simple file handling and feature extraction bugs, this improved to over 95%, comparable
to the best submitted systems. Similar accuracy is achieved using only word unigram features.
</bodyText>
<sectionHeader confidence="0.969178" genericHeader="keywords">
1 Introduction and Approach
</sectionHeader>
<bodyText confidence="0.999853913043478">
Most approaches to written language detection use character or byte ngram features to capture charac-
teristic orthographic sequences – see e.g. (Cavnar and Trenkle, 1994) to (Lui et al., 2014) and many
in between, as well as implementations such as the widely used open-source Chromium Compact Lan-
guage Detector.1 Some approaches determine these characteristic features from linguistic properties of
the language (e.g. (Lins and Gonc¸alves, 2004)), while some determine them from data (e.g. (Cavnar and
Trenkle, 1994)). A wide range of approaches to modelling and classification can be used, ranging from
simple Naive Bayes models (Grefenstette, 1995) to more complex generative mixture models for tasks
with multilingual texts (Lui et al., 2014). Our interest in this task was to see how well a naive, entirely
data-driven baseline method would perform in the task of discriminating similar languages (DSL) as
posed by the DSL Shared Task (Zampieri et al., 2014).
Our approach was intended to capture two basic insights into variation between similar languages.
First, that closely related languages often use quite different words for the same concept: e.g. US English
elevator vs UK English lift; Croatian tjedan vs Serbian nedelja vs Bosnian sedmica. Second, that there
are often regular variations in the details of a word’s orthographic or phonological form: e.g. US English
color, favorite vs UK English colour, favourite; Croatian/Bosnian rijeka, htjeti vs Serbian reka, hteti.
The former insight can be approximated by use of word ngrams; the latter via character ngrams. While
such ngram features cannot capture similarity of meaning or non-sequential dependencies, they may do
a reasonable job of capturing similarity of sentential context (often taken to be an indicator of lexical
meaning) and sequential phenomena.
Together with simplicity in method, speed and simplicity of implementation was also an objective.
We therefore used only the training and development data available in the shared task — see (Tan et
al., 2014) — together with a standard freely available discriminative SVM classifier and common text
pre-processing methods.
</bodyText>
<footnote confidence="0.999462333333333">
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1https://code.google.com/p/chromium-compact-language-detector/
</footnote>
<page confidence="0.940135">
155
</page>
<note confidence="0.867173">
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 155–160,
Dublin, Ireland, August 23 2014.
</note>
<sectionHeader confidence="0.768264" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.998582">
Shared Task The Discriminating Similar Languages (DSL) Shared Task was established as part of the
2014 VarDial workshop.2 The task provided datasets for 13 different languages in 6 groups of closely
related languages, shown in Table 1. Data was divided into training, development and test sets: for each
language, 18,000 labelled training instances and 2,000 labelled development instances were provided;
an unlabelled and previously unseen test set containing 1,000 instances per language was then used for
evaluation by the organisers – see (Tan et al., 2014) for full details of the dataset, and (Zampieri et al.,
2014) for the task and evaluation.
</bodyText>
<figure confidence="0.988012833333333">
Group A Bosnian (bs), Croatian (hr), Serbian (sr)
Group B Indonesian (id), Malaysian (my)
Group C Czech (cz), Slovakian (sk)
Group D Brazilian Portuguese (pt-BR), European Portuguese (pt-PT)
Group E Peninsular Spain (es-ES), Argentine Spanish (es-AR)
Group F American English (en-US), British English (en-GB)
</figure>
<tableCaption confidence="0.979264">
Table 1: Languages and groups in the DSL Shared Task.
</tableCaption>
<bodyText confidence="0.99892536">
However, problems were discovered in labelling the languages in Group F, and an evaluation for
groups A-E was therefore performed separately; we discuss only this latter task and evaluation here.
Related Work Classification approaches based on character or byte sequences have shown success in
providing general models of language identification; see e.g. (Lui et al., 2014). In more specific exper-
iments into discriminating between pairs or triples of similar languages, many researchers have found
that word-based features can aid accuracy; but classification method and feature choice vary widely.
When distinguishing Malay from Indonesian, Ranaivo-Malanc¸on (2006) combines character n-gram
frequencies with heuristics based on number format and lists of words unique to each language. Ljubeˇsi´c
et al. (2007) use a character trigram-based probabilistic language model, again in combination with a
unique-word list, to distinguish between Croatian, Serbian and Slovenian, achieving high accuracies
(over 99%); Tiedemann and Ljubeˇsi´c (2012) extend this task to include Bosnian and improve perfor-
mance by using a Naive Bayes classifier with unigram word features to achieve accuracies over 95%.
Some research suggests that word-based features can even outperform character-based approaches.
For Brazilian vs European Portuguese, Zampieri and Gebre (2012) found that word unigrams gave very
similar performance to character n-gram features when used in a probabilistic language model; Zampieri
et al. (2013) then showed that word 1- or 2-grams outperformed character ngrams of any length from 1
to 5 (and that both outperformed features based purely on syntactic part-of-speech), when distinguishing
different varieties of Spanish. Lui and Cook (2013) likewise found that bag-of-words features generally
outperformed features based on syntax or character sequences when distinguishing between Canadian,
Australian and UK English. However, Zampieri (2013) found that in some cases (e.g French) character
n-grams might give benefits above simple word unigram features.
In this work, then our interest was to investigate whether these simple, knowledge-poor approaches can
generalise and apply across several language groups, using a single integrated approach to classification
incorporating character- and word-based features within one model; and to compare the utility of word
and character features.
</bodyText>
<sectionHeader confidence="0.990913" genericHeader="method">
3 Methods
</sectionHeader>
<listItem confidence="0.7970655">
Processing and training We tokenise the training texts from (Tan et al., 2014) based on transitions
between alphanumeric and non-alphanumeric characters, and remove URLs, email addresses, Twitter
usernames and emoticons. We then form feature vectors with entries for all observed word (token) uni-
grams, and character ngrams of lengths 1-3; feature values are counts (raw term frequencies) normalised
</listItem>
<footnote confidence="0.96143">
2http://corporavm.uni-koeln.de/vardial/
</footnote>
<page confidence="0.997727">
156
</page>
<bodyText confidence="0.999837055555555">
by the text length in tokens or characters respectively. We then train a single multi-class linear-kernel sup-
port vector machine using LIBLINEAR (Fan et al., 2008) with the language identifiers (en-US, en-UK,
hr, bs, sr etc.) as labels. SVMs are well-suited to high-dimensional feature spaces; and SVMs with
ngrams of these lengths have shown good performance in other language identification work (Baldwin
and Lui, 2010). Features were given numerical indices corresponding to the unique ngram type (i.e.
we used a feature dictionary with no hashing). No feature selection or frequency cutoff was used. No
part-of-speech tagging or grammatical analysis was attempted; no external language resources or tools
were used other than described above.
Development and testing Development and test set texts were tokenised and featurised using the same
process; feature indices were taken from the dictionary generated during training, with unseen ngram
types ignored. LIBLINEAR was then used to predict the most likely language identifier label.
By re-using a standard set of in-house utilities for tokenisation and featurisation,3 the code for train-
ing and parameter testing (see below) was written and tested for functionality in around 30 minutes.
Pre-processing, featurisation and vectorisation then took around 25 minutes over the training and devel-
opment sets, and writing out LIBLINEAR format files around 15 minutes, running on a MacBook Air
with 1.7GHz Intel Core i7 processor and 8Gb memory. Classifier training then takes around 1 minute,
depending on exact parameter settings. Testing on the development set or test set takes around 1 second
per language group.
</bodyText>
<sectionHeader confidence="0.998748" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9997975">
Development We used 10-fold cross-validation on the training set, and testing on the development set,
to choose a suitable SVM cost parameter (tradeoff between error and maximum margin criterion). We
cross-validated over the training set to check overall multi-class accuracy while varying the cost over
a range from 1 to 100 – see Table 2. We then trained on the full training set, and tested accuracy on
the development across each language group – see Table 3. Given reported problems with the group F
dataset (en-UK/en-US), we focussed on groups A-E.
</bodyText>
<table confidence="0.992897">
Cost: 1.0 3.0 10.0 30.0 50.0 100.0
Overall A-E 91.36 93.24 94.44 94.83 94.86 94.85
</table>
<tableCaption confidence="0.972021">
Table 2: 10-fold cross-validation accuracy on training set with varying SVM cost.
</tableCaption>
<table confidence="0.999966714285714">
Cost: 1.0 3.0 10.0 30.0 50.0 100.0
Group A bs/hr/sr 88.93 91.96 93.20 93.56 93.46 93.26
Group B id/my 97.11 97.72 98.14 98.28 98.31 98.42
Group C cz/sk 99.90 99.92 99.95 99.97 99.97 99.95
Group D pt-BR/pt-PT 89.83 91.99 93.52 94.12 94.00 94.05
Group E es-AR/es-ES 82.72 85.82 87.78 89.26 89.24 89.01
Overall A-E 91.34 93.28 94.34 94.86 94.81 94.73
</table>
<tableCaption confidence="0.999938">
Table 3: Accuracy on development set with varying SVM cost.
</tableCaption>
<bodyText confidence="0.999793428571429">
A cost parameter value of 30 to 50 appeared to perform best across all groups, so these two values
were used for separate runs in the shared task test. Note though that performance appears relatively
stable over a cost range of 10-100 (perhaps 30-100 for group E). The classifier performs worst for group
E (es-AR/es-ES), with only this language group failing to reach 90% accuracy. Group C (cz/sk)
performs best with almost perfect accuracy; this may be due to the existence of characters which are
highly discriminative on their own (e.g. oˆ is used in Slovak, but not in Czech, u˚ in Czech but not in
Slovak – although a few dozen examples appear labelled as Slovak in this dataset).
</bodyText>
<footnote confidence="0.988004">
3Tools with equivalent functionality are widely available e.g. as part of NLTK, http://www.nltk.org/.
</footnote>
<page confidence="0.996781">
157
</page>
<bodyText confidence="0.896109666666667">
Test – Shared Task A blind run on the test set was then performed and submitted as part of the shared
task. Overall accuracy was 90.61% (macro-averaged F-score 92.51%), placing us 5th amongst the task
entrants; results per group are shown in Table 4.
</bodyText>
<figure confidence="0.955616714285714">
Cost: 30.0
Group A bs/hr/sr 87.87
Group B id/my 93.50
Group C cz/sk 96.20
Group D pt-BR/pt-PT 90.45
Group E es-AR/es-ES 86.45
Overall A-E 90.61
</figure>
<tableCaption confidence="0.976893">
Table 4: Accuracy on test set as submitted for the shared task.
</tableCaption>
<bodyText confidence="0.9972908">
Corrected Test However, after submission of the test run, a bug was discovered in the code which
paired test sentences with predictions; predictions had been omitted for about 500 of the 11,000 test
texts (i.e. 4.5% of the data) due to an unfortunate combination of unpaired double-quote characters in the
test data with the use of a standard CSV-file handling library. After release of the gold-standard test set
labels, the classifier was therefore re-run, with resulting accuracies as shown in Table 5.
</bodyText>
<table confidence="0.994630857142857">
Cost: 1.0 3.0 10.0 30.0 50.0 100.0
Group A bs/hr/sr 87.97 90.70 92.40 92.90 93.00 93.17
Group B id/my 98.30 98.85 99.05 99.15 99.15 99.15
Group C cz/sk 99.90 99.95 99.95 99.95 99.95 99.95
Group D pt-BR/pt-PT 88.50 91.45 93.25 93.95 93.90 93.80
Group E es-AR/es-ES 83.65 86.70 88.45 89.35 89.45 89.45
Overall A-E 91.33 93.27 94.42 94.86 94.90 94.93
</table>
<tableCaption confidence="0.999466">
Table 5: True accuracy on test set after restoring omitted predictions.
</tableCaption>
<bodyText confidence="0.9996964">
Accuracies are very similar to those on the development set. Overall accuracy at the chosen cost
parameter range of 30-50 is 94.9%, slightly worse than the 1st and slightly better than the 2nd-placed
systems in the official test (95.71% and 94.68% respectively). Increasing the cost parameter setting
could perhaps give a very slight boost to performance. Again, group E performs worst, and Group C
best; per-group and overall accuracies are very similar to those achieved on the development set.
A second unintended feature of the feature generation code was subsequently discovered: character
n-grams were being extracted spanning word boundaries (including the whitespace characters separating
words). These were removed, leaving only the intended character n-grams within words, and accuracies
are shown in Table 6. Again, overall performance increases slightly, now to over 95%, although Group
A accuracy shows a slight decrease (0.1%). Group E accuracy improves by over 1% and is now over
</bodyText>
<table confidence="0.97606475">
90% at the chosen cost parameter.
Cost: 1.0 3.0 10.0 30.0 50.0 100.0
Group A bs/hr/sr 89.83 92.13 92.73 92.77 92.63 92.67
Group B id/my 98.55 99.15 99.25 99.35 99.35 99.35
Group C cz/sk 99.95 99.95 99.95 99.95 99.95 99.95
Group D pt-BR/pt-PT 91.00 93.25 94.80 95.15 95.10 95.15
Group E es-AR/es-ES 86.10 88.35 90.30 90.85 90.95 91.15
Overall A-E 92.79 94.35 95.16 95.35 95.33 95.37
</table>
<tableCaption confidence="0.999355">
Table 6: Accuracy on test set after removing spurious character n-grams.
</tableCaption>
<page confidence="0.99764">
158
</page>
<bodyText confidence="0.9954766">
Effect of features To investigate the utility of our chosen feature sets and their insights into lexical and
orthographic distinctions, we then compared the overall performance to that achieved when removing
certain features. Table 7 shows the accuracies achieved without word unigram features (i.e. using only
character ngrams of lengths 1-3); Table 8 shows accuracies without character ngram features (i.e. using
only word unigrams).
</bodyText>
<table confidence="0.994414466666667">
Cost: 1.0 3.0 10.0 30.0 50.0 100.0
Group A bs/hr/sr 81.43 85.40 88.03 89.77 90.10 90.70
Group B id/my 91.80 94.15 96.05 96.95 97.20 97.50
Group C cz/sk 99.90 99.95 99.95 99.95 99.95 99.95
Group D pt-BR/pt-PT 82.75 86.70 89.15 90.80 91.50 91.80
Group E es-AR/es-ES 77.80 80.95 83.50 85.20 85.10 85.65
Overall A-E 86.25 89.06 91.04 92.28 92.53 92.90
Table 7: Accuracy on test set without word unigrams.
Cost: 1.0 3.0 10.0 30.0 50.0 100.0
Group A bs/hr/sr 86.83 89.63 91.73 92.20 92.43 92.07
Group B id/my 97.70 98.65 98.85 99.10 99.15 99.10
Group C cz/sk 99.70 99.70 99.80 99.90 99.90 99.90
Group D pt-BR/pt-PT 86.65 90.55 92.55 93.25 93.40 93.20
Group E es-AR/es-ES 85.10 87.10 88.35 89.35 89.35 89.45
Overall A-E 90.80 92.81 94.02 94.53 94.63 94.50
</table>
<tableCaption confidence="0.999736">
Table 8: Accuracy on test set using only word unigrams.
</tableCaption>
<bodyText confidence="0.999916">
Neither system performs as well as the classifier with the full, combined feature set (Table 6). How-
ever, the system with only word unigrams does almost as well as the full system, losing a maximum of
2% performance at the extreme range of cost parameter values, and less than 1% at the chosen optimal
values. The system with only character ngrams, however, loses noticably more performance, with around
3% lost even at optimal cost values.
</bodyText>
<sectionHeader confidence="0.999567" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999991125">
A simple approach using ngram features and discriminative classification achieves competitive results
on the task of discriminating similar languages, and the availability of existing language processing and
machine learning tools makes setting up and training such a system easy and extremely quick. Simple
word unigram features perform well on their own, although combination with character n-gram features
improves performance; the choice of classifier parameters is important but seems to generalise well
across different languages. Future extensions of this work could include features which take into account
longer word or character sequences and/or more flexible characterisations and combinations of those
features, for example via the convolutional neural network approach of (Kalchbrenner et al., 2014).
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.98361575">
Timothy Baldwin and Marco Lui. 2010. Language identification: The long and the short of the matter. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, pages 229–237, Los Angeles, California, June. Association for Computational
Linguistics.
William B. Cavnar and John M. Trenkle. 1994. N-gram-based text categorization. In Proceedings of the 3rd
Symposium on Document Analysis and Information Retrieval, pages 161–175, Las Vegas.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library
for large linear classification. Journal of Machine Learning Research, 9:1871–1874.
</reference>
<page confidence="0.994237">
159
</page>
<reference confidence="0.99961909375">
Gregory Grefenstette. 1995. Comparing two language identification schemes. In Proceedings ofAnalisi Statistica
dei Dati Testuali (JADT), pages 263–268, Rome.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling
sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), pages 655–665, Baltimore, Maryland, June. Association for Computational Linguistics.
Rafael Dueire Lins and Paulo Gonc¸alves. 2004. Automatic language identification of written texts. In Proceedings
of the 2004 ACM Symposium on Applied Computing, SAC ’04, pages 1128–1133, New York, NY, USA. ACM.
Nikola Ljube&amp;quot;si´c, Nives Mikeli´c, and Damir Boras. 2007. Language identification: How to distinguish similar
languages? In Proceedings of the 29th International Conference on Information Technology Interfaces (ITI),
pages 541–546.
Marco Lui and Paul Cook. 2013. Classifying english documents by national dialect. In Proceedings of the
Australasian Language Technology Association Workshop 2013 (ALTA 2013), pages 5–15, Brisbane, Australia.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014. Automatic detection and language identification of multi-
lingual documents. Transactions of the Association for Computational Linguistics, 2:27–40.
Bali Ranaivo-Malanc¸on. 2006. Automatic identification of close languages - case study: Malay and Indonesian.
ECTI Transactions on Computer and Information Technology, 2(2):126–134, November.
Liling Tan, Marcos Zampieri, Nikola Ljube&amp;quot;si´c, and J¨org Tiedemann. 2014. Merging comparable data sources for
the discrimination of similar languages: The DSL Corpus Collection. In Proceedings of the 7th Workshop on
Building and Using Comparable Corpora (BUCC).
J¨org Tiedemann and Nikola Ljube&amp;quot;si´c. 2012. Efficient discrimination between closely related languages. In
Proceedings of COLING 2012, pages 2619–2634, Mumbai, India, December.
Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic identification of language varieties: The case
of Portuguese. In Proceedings of KONVENS2012, pages 233–237, Vienna, September.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and POS
distribution for the identification of Spanish varieties. In Proceedings of TALN2013, pages 580–587, Sables
d’Olonne, France.
Marcos Zampieri, Liling Tan, Nikola Ljube&amp;quot;si´c, and J¨org Tiedemann. 2014. A report on the DSL Shared Task
2014. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects
(VarDial).
Marcos Zampieri. 2013. Using bag-of-words to distinguish similar languages: How efficient are they? In
Proceedings of the 14th IEEE International Symposium on Computational Intelligence and Informatics (CINTI),
pages 37–41, Budapest, November.
</reference>
<page confidence="0.997481">
160
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938024">
<title confidence="0.999966">A Simple Baseline for Discriminating Similar Languages</title>
<author confidence="0.995072">Matthew</author>
<affiliation confidence="0.985929666666667">Cognitive Science Research School of Electronic Engineering and Computer Queen Mary University of</affiliation>
<email confidence="0.989155">m.purver@qmul.ac.uk</email>
<abstract confidence="0.999231444444444">This paper describes an approach to discriminating similar languages using wordand characterbased features, submitted as the Queen Mary University of London entry to the Discriminating Similar Languages shared task. Our motivation was to investigate how well a simple, datadriven, linguistically naive method could perform, in order to provide a baseline by which more linguistically complex or knowledge-rich approaches can be judged. Using a standard supervised classifier with word and character n-grams as features, we achieved over 90% accuracy in the test; on fixing simple file handling and feature extraction bugs, this improved to over 95%, comparable to the best submitted systems. Similar accuracy is achieved using only word unigram features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Marco Lui</author>
</authors>
<title>Language identification: The long and the short of the matter.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>229--237</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="7909" citStr="Baldwin and Lui, 2010" startWordPosition="1146" endWordPosition="1149">ectors with entries for all observed word (token) unigrams, and character ngrams of lengths 1-3; feature values are counts (raw term frequencies) normalised 2http://corporavm.uni-koeln.de/vardial/ 156 by the text length in tokens or characters respectively. We then train a single multi-class linear-kernel support vector machine using LIBLINEAR (Fan et al., 2008) with the language identifiers (en-US, en-UK, hr, bs, sr etc.) as labels. SVMs are well-suited to high-dimensional feature spaces; and SVMs with ngrams of these lengths have shown good performance in other language identification work (Baldwin and Lui, 2010). Features were given numerical indices corresponding to the unique ngram type (i.e. we used a feature dictionary with no hashing). No feature selection or frequency cutoff was used. No part-of-speech tagging or grammatical analysis was attempted; no external language resources or tools were used other than described above. Development and testing Development and test set texts were tokenised and featurised using the same process; feature indices were taken from the dictionary generated during training, with unseen ngram types ignored. LIBLINEAR was then used to predict the most likely languag</context>
</contexts>
<marker>Baldwin, Lui, 2010</marker>
<rawString>Timothy Baldwin and Marco Lui. 2010. Language identification: The long and the short of the matter. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 229–237, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Cavnar</author>
<author>John M Trenkle</author>
</authors>
<title>N-gram-based text categorization.</title>
<date>1994</date>
<booktitle>In Proceedings of the 3rd Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>161--175</pages>
<location>Las Vegas.</location>
<contexts>
<context position="1172" citStr="Cavnar and Trenkle, 1994" startWordPosition="165" endWordPosition="168">order to provide a baseline by which more linguistically complex or knowledge-rich approaches can be judged. Using a standard supervised classifier with word and character n-grams as features, we achieved over 90% accuracy in the test; on fixing simple file handling and feature extraction bugs, this improved to over 95%, comparable to the best submitted systems. Similar accuracy is achieved using only word unigram features. 1 Introduction and Approach Most approaches to written language detection use character or byte ngram features to capture characteristic orthographic sequences – see e.g. (Cavnar and Trenkle, 1994) to (Lui et al., 2014) and many in between, as well as implementations such as the widely used open-source Chromium Compact Language Detector.1 Some approaches determine these characteristic features from linguistic properties of the language (e.g. (Lins and Gonc¸alves, 2004)), while some determine them from data (e.g. (Cavnar and Trenkle, 1994)). A wide range of approaches to modelling and classification can be used, ranging from simple Naive Bayes models (Grefenstette, 1995) to more complex generative mixture models for tasks with multilingual texts (Lui et al., 2014). Our interest in this t</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>William B. Cavnar and John M. Trenkle. 1994. N-gram-based text categorization. In Proceedings of the 3rd Symposium on Document Analysis and Information Retrieval, pages 161–175, Las Vegas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="7651" citStr="Fan et al., 2008" startWordPosition="1107" endWordPosition="1110">3 Methods Processing and training We tokenise the training texts from (Tan et al., 2014) based on transitions between alphanumeric and non-alphanumeric characters, and remove URLs, email addresses, Twitter usernames and emoticons. We then form feature vectors with entries for all observed word (token) unigrams, and character ngrams of lengths 1-3; feature values are counts (raw term frequencies) normalised 2http://corporavm.uni-koeln.de/vardial/ 156 by the text length in tokens or characters respectively. We then train a single multi-class linear-kernel support vector machine using LIBLINEAR (Fan et al., 2008) with the language identifiers (en-US, en-UK, hr, bs, sr etc.) as labels. SVMs are well-suited to high-dimensional feature spaces; and SVMs with ngrams of these lengths have shown good performance in other language identification work (Baldwin and Lui, 2010). Features were given numerical indices corresponding to the unique ngram type (i.e. we used a feature dictionary with no hashing). No feature selection or frequency cutoff was used. No part-of-speech tagging or grammatical analysis was attempted; no external language resources or tools were used other than described above. Development and </context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Comparing two language identification schemes.</title>
<date>1995</date>
<booktitle>In Proceedings ofAnalisi Statistica dei Dati Testuali (JADT),</booktitle>
<pages>263--268</pages>
<location>Rome.</location>
<contexts>
<context position="1653" citStr="Grefenstette, 1995" startWordPosition="239" endWordPosition="240">uage detection use character or byte ngram features to capture characteristic orthographic sequences – see e.g. (Cavnar and Trenkle, 1994) to (Lui et al., 2014) and many in between, as well as implementations such as the widely used open-source Chromium Compact Language Detector.1 Some approaches determine these characteristic features from linguistic properties of the language (e.g. (Lins and Gonc¸alves, 2004)), while some determine them from data (e.g. (Cavnar and Trenkle, 1994)). A wide range of approaches to modelling and classification can be used, ranging from simple Naive Bayes models (Grefenstette, 1995) to more complex generative mixture models for tasks with multilingual texts (Lui et al., 2014). Our interest in this task was to see how well a naive, entirely data-driven baseline method would perform in the task of discriminating similar languages (DSL) as posed by the DSL Shared Task (Zampieri et al., 2014). Our approach was intended to capture two basic insights into variation between similar languages. First, that closely related languages often use quite different words for the same concept: e.g. US English elevator vs UK English lift; Croatian tjedan vs Serbian nedelja vs Bosnian sedmi</context>
</contexts>
<marker>Grefenstette, 1995</marker>
<rawString>Gregory Grefenstette. 1995. Comparing two language identification schemes. In Proceedings ofAnalisi Statistica dei Dati Testuali (JADT), pages 263–268, Rome.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>655--665</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 655–665, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rafael Dueire Lins</author>
<author>Paulo Gonc¸alves</author>
</authors>
<title>Automatic language identification of written texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 ACM Symposium on Applied Computing, SAC ’04,</booktitle>
<pages>1128--1133</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Lins, Gonc¸alves, 2004</marker>
<rawString>Rafael Dueire Lins and Paulo Gonc¸alves. 2004. Automatic language identification of written texts. In Proceedings of the 2004 ACM Symposium on Applied Computing, SAC ’04, pages 1128–1133, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikola Ljubesi´c</author>
<author>Nives Mikeli´c</author>
<author>Damir Boras</author>
</authors>
<title>Language identification: How to distinguish similar languages?</title>
<date>2007</date>
<booktitle>In Proceedings of the 29th International Conference on Information Technology Interfaces (ITI),</booktitle>
<pages>541--546</pages>
<marker>Ljubesi´c, Mikeli´c, Boras, 2007</marker>
<rawString>Nikola Ljube&amp;quot;si´c, Nives Mikeli´c, and Damir Boras. 2007. Language identification: How to distinguish similar languages? In Proceedings of the 29th International Conference on Information Technology Interfaces (ITI), pages 541–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Paul Cook</author>
</authors>
<title>Classifying english documents by national dialect.</title>
<date>2013</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop 2013</booktitle>
<pages>5--15</pages>
<location>ALTA</location>
<contexts>
<context position="6383" citStr="Lui and Cook (2013)" startWordPosition="930" endWordPosition="933">fier with unigram word features to achieve accuracies over 95%. Some research suggests that word-based features can even outperform character-based approaches. For Brazilian vs European Portuguese, Zampieri and Gebre (2012) found that word unigrams gave very similar performance to character n-gram features when used in a probabilistic language model; Zampieri et al. (2013) then showed that word 1- or 2-grams outperformed character ngrams of any length from 1 to 5 (and that both outperformed features based purely on syntactic part-of-speech), when distinguishing different varieties of Spanish. Lui and Cook (2013) likewise found that bag-of-words features generally outperformed features based on syntax or character sequences when distinguishing between Canadian, Australian and UK English. However, Zampieri (2013) found that in some cases (e.g French) character n-grams might give benefits above simple word unigram features. In this work, then our interest was to investigate whether these simple, knowledge-poor approaches can generalise and apply across several language groups, using a single integrated approach to classification incorporating character- and word-based features within one model; and to c</context>
</contexts>
<marker>Lui, Cook, 2013</marker>
<rawString>Marco Lui and Paul Cook. 2013. Classifying english documents by national dialect. In Proceedings of the Australasian Language Technology Association Workshop 2013 (ALTA 2013), pages 5–15, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Jey Han Lau</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic detection and language identification of multilingual documents.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--27</pages>
<contexts>
<context position="1194" citStr="Lui et al., 2014" startWordPosition="170" endWordPosition="173"> which more linguistically complex or knowledge-rich approaches can be judged. Using a standard supervised classifier with word and character n-grams as features, we achieved over 90% accuracy in the test; on fixing simple file handling and feature extraction bugs, this improved to over 95%, comparable to the best submitted systems. Similar accuracy is achieved using only word unigram features. 1 Introduction and Approach Most approaches to written language detection use character or byte ngram features to capture characteristic orthographic sequences – see e.g. (Cavnar and Trenkle, 1994) to (Lui et al., 2014) and many in between, as well as implementations such as the widely used open-source Chromium Compact Language Detector.1 Some approaches determine these characteristic features from linguistic properties of the language (e.g. (Lins and Gonc¸alves, 2004)), while some determine them from data (e.g. (Cavnar and Trenkle, 1994)). A wide range of approaches to modelling and classification can be used, ranging from simple Naive Bayes models (Grefenstette, 1995) to more complex generative mixture models for tasks with multilingual texts (Lui et al., 2014). Our interest in this task was to see how wel</context>
<context position="5002" citStr="Lui et al., 2014" startWordPosition="734" endWordPosition="737">(sk) Group D Brazilian Portuguese (pt-BR), European Portuguese (pt-PT) Group E Peninsular Spain (es-ES), Argentine Spanish (es-AR) Group F American English (en-US), British English (en-GB) Table 1: Languages and groups in the DSL Shared Task. However, problems were discovered in labelling the languages in Group F, and an evaluation for groups A-E was therefore performed separately; we discuss only this latter task and evaluation here. Related Work Classification approaches based on character or byte sequences have shown success in providing general models of language identification; see e.g. (Lui et al., 2014). In more specific experiments into discriminating between pairs or triples of similar languages, many researchers have found that word-based features can aid accuracy; but classification method and feature choice vary widely. When distinguishing Malay from Indonesian, Ranaivo-Malanc¸on (2006) combines character n-gram frequencies with heuristics based on number format and lists of words unique to each language. Ljubeˇsi´c et al. (2007) use a character trigram-based probabilistic language model, again in combination with a unique-word list, to distinguish between Croatian, Serbian and Slovenia</context>
</contexts>
<marker>Lui, Lau, Baldwin, 2014</marker>
<rawString>Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014. Automatic detection and language identification of multilingual documents. Transactions of the Association for Computational Linguistics, 2:27–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bali Ranaivo-Malanc¸on</author>
</authors>
<title>Automatic identification of close languages - case study: Malay and Indonesian.</title>
<date>2006</date>
<journal>ECTI Transactions on Computer and Information Technology,</journal>
<volume>2</volume>
<issue>2</issue>
<marker>Ranaivo-Malanc¸on, 2006</marker>
<rawString>Bali Ranaivo-Malanc¸on. 2006. Automatic identification of close languages - case study: Malay and Indonesian. ECTI Transactions on Computer and Information Technology, 2(2):126–134, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liling Tan</author>
<author>Marcos Zampieri</author>
<author>Nikola Ljubesi´c</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Merging comparable data sources for the discrimination of similar languages: The DSL Corpus Collection.</title>
<date>2014</date>
<booktitle>In Proceedings of the 7th Workshop on Building and Using Comparable Corpora (BUCC).</booktitle>
<marker>Tan, Zampieri, Ljubesi´c, Tiedemann, 2014</marker>
<rawString>Liling Tan, Marcos Zampieri, Nikola Ljube&amp;quot;si´c, and J¨org Tiedemann. 2014. Merging comparable data sources for the discrimination of similar languages: The DSL Corpus Collection. In Proceedings of the 7th Workshop on Building and Using Comparable Corpora (BUCC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
<author>Nikola Ljubesi´c</author>
</authors>
<title>Efficient discrimination between closely related languages.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>2619--2634</pages>
<location>Mumbai, India,</location>
<marker>Tiedemann, Ljubesi´c, 2012</marker>
<rawString>J¨org Tiedemann and Nikola Ljube&amp;quot;si´c. 2012. Efficient discrimination between closely related languages. In Proceedings of COLING 2012, pages 2619–2634, Mumbai, India, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
<author>Binyam Gebrekidan Gebre</author>
</authors>
<title>Automatic identification of language varieties: The case of Portuguese.</title>
<date>2012</date>
<booktitle>In Proceedings of KONVENS2012,</booktitle>
<pages>233--237</pages>
<location>Vienna,</location>
<contexts>
<context position="5987" citStr="Zampieri and Gebre (2012)" startWordPosition="870" endWordPosition="873">mber format and lists of words unique to each language. Ljubeˇsi´c et al. (2007) use a character trigram-based probabilistic language model, again in combination with a unique-word list, to distinguish between Croatian, Serbian and Slovenian, achieving high accuracies (over 99%); Tiedemann and Ljubeˇsi´c (2012) extend this task to include Bosnian and improve performance by using a Naive Bayes classifier with unigram word features to achieve accuracies over 95%. Some research suggests that word-based features can even outperform character-based approaches. For Brazilian vs European Portuguese, Zampieri and Gebre (2012) found that word unigrams gave very similar performance to character n-gram features when used in a probabilistic language model; Zampieri et al. (2013) then showed that word 1- or 2-grams outperformed character ngrams of any length from 1 to 5 (and that both outperformed features based purely on syntactic part-of-speech), when distinguishing different varieties of Spanish. Lui and Cook (2013) likewise found that bag-of-words features generally outperformed features based on syntax or character sequences when distinguishing between Canadian, Australian and UK English. However, Zampieri (2013) </context>
</contexts>
<marker>Zampieri, Gebre, 2012</marker>
<rawString>Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic identification of language varieties: The case of Portuguese. In Proceedings of KONVENS2012, pages 233–237, Vienna, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
<author>Binyam Gebrekidan Gebre</author>
<author>Sascha Diwersy</author>
</authors>
<title>N-gram language models and POS distribution for the identification of Spanish varieties.</title>
<date>2013</date>
<booktitle>In Proceedings of TALN2013,</booktitle>
<pages>580--587</pages>
<location>Sables d’Olonne, France.</location>
<contexts>
<context position="6139" citStr="Zampieri et al. (2013)" startWordPosition="893" endWordPosition="896">nation with a unique-word list, to distinguish between Croatian, Serbian and Slovenian, achieving high accuracies (over 99%); Tiedemann and Ljubeˇsi´c (2012) extend this task to include Bosnian and improve performance by using a Naive Bayes classifier with unigram word features to achieve accuracies over 95%. Some research suggests that word-based features can even outperform character-based approaches. For Brazilian vs European Portuguese, Zampieri and Gebre (2012) found that word unigrams gave very similar performance to character n-gram features when used in a probabilistic language model; Zampieri et al. (2013) then showed that word 1- or 2-grams outperformed character ngrams of any length from 1 to 5 (and that both outperformed features based purely on syntactic part-of-speech), when distinguishing different varieties of Spanish. Lui and Cook (2013) likewise found that bag-of-words features generally outperformed features based on syntax or character sequences when distinguishing between Canadian, Australian and UK English. However, Zampieri (2013) found that in some cases (e.g French) character n-grams might give benefits above simple word unigram features. In this work, then our interest was to i</context>
</contexts>
<marker>Zampieri, Gebre, Diwersy, 2013</marker>
<rawString>Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and POS distribution for the identification of Spanish varieties. In Proceedings of TALN2013, pages 580–587, Sables d’Olonne, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
<author>Liling Tan</author>
<author>Nikola Ljubesi´c</author>
<author>J¨org Tiedemann</author>
</authors>
<title>A report on the DSL Shared Task</title>
<date>2014</date>
<booktitle>In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial).</booktitle>
<marker>Zampieri, Tan, Ljubesi´c, Tiedemann, 2014</marker>
<rawString>Marcos Zampieri, Liling Tan, Nikola Ljube&amp;quot;si´c, and J¨org Tiedemann. 2014. A report on the DSL Shared Task 2014. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
</authors>
<title>Using bag-of-words to distinguish similar languages: How efficient are they?</title>
<date>2013</date>
<booktitle>In Proceedings of the 14th IEEE International Symposium on Computational Intelligence and Informatics (CINTI),</booktitle>
<pages>37--41</pages>
<location>Budapest,</location>
<contexts>
<context position="6586" citStr="Zampieri (2013)" startWordPosition="957" endWordPosition="958">and Gebre (2012) found that word unigrams gave very similar performance to character n-gram features when used in a probabilistic language model; Zampieri et al. (2013) then showed that word 1- or 2-grams outperformed character ngrams of any length from 1 to 5 (and that both outperformed features based purely on syntactic part-of-speech), when distinguishing different varieties of Spanish. Lui and Cook (2013) likewise found that bag-of-words features generally outperformed features based on syntax or character sequences when distinguishing between Canadian, Australian and UK English. However, Zampieri (2013) found that in some cases (e.g French) character n-grams might give benefits above simple word unigram features. In this work, then our interest was to investigate whether these simple, knowledge-poor approaches can generalise and apply across several language groups, using a single integrated approach to classification incorporating character- and word-based features within one model; and to compare the utility of word and character features. 3 Methods Processing and training We tokenise the training texts from (Tan et al., 2014) based on transitions between alphanumeric and non-alphanumeric </context>
</contexts>
<marker>Zampieri, 2013</marker>
<rawString>Marcos Zampieri. 2013. Using bag-of-words to distinguish similar languages: How efficient are they? In Proceedings of the 14th IEEE International Symposium on Computational Intelligence and Informatics (CINTI), pages 37–41, Budapest, November.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
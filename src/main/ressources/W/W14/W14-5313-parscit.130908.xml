<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000264">
<title confidence="0.979463">
Improved Sentence-Level Arabic Dialect Classification
</title>
<author confidence="0.901666">
Christoph Tillmann and Yaser Al-Onaizan Saab Mansour∗
</author>
<affiliation confidence="0.8244805">
IBM T.J. Watson Research Center Aachen University
Yorktown Heights, NY, USA Aachen, Germany
</affiliation>
<email confidence="0.972555">
Ictill,onaizanj@us.ibm.com mansour@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.993276" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999985888888889">
The paper presents work on improved sentence-level dialect classification of Egyptian Arabic
(ARZ) vs. Modern Standard Arabic (MSA). Our approach is based on binary feature functions
that can be implemented with a minimal amount of task-specific knowledge. We train a feature-
rich linear classifier based on a linear support-vector machine (linear SVM) approach. Our best
system achieves an accuracy of 89.1 % on the Arabic Online Commentary (AOC) dataset (Zaidan
and Callison-Burch, 2011) using 10-fold stratified cross validation: a 1.3 % absolute accuracy
improvement over the results published by (Zaidan and Callison-Burch, 2014). We also evaluate
the classifier on dialect data from an additional data source. Here, we find that features which
measure the informalness of a sentence actually decrease classification accuracy significantly.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99989508">
The standard form of written Arabic is Modern Standard Arabic (MSA) . It differs significantly from
various spoken varieties of Arabic (Zaidan and Callison-Burch, 2011; Zaidan and Callison-Burch, 2014;
Elfardy and Diab, 2013). Even though these dialects do not originally exist in written form, they are
present in social media texts. Recently a dataset of dialectal Arabic has been made available in the form
of the Arabic Online Commentary (AOC) set (Zaidan and Callison-Burch, 2011; Zaidan and Callison-
Burch, 2014). The data consists of reader commentary from the online versions of Arabic newspapers,
which have a high degree of dialect content. Data for the following dialects has been collected: Levan-
tine, Gulf, and Egyptian. The data had been obtained by a crowd-sourcing effort. In the current paper, we
present results for a binary classification task only, where we predict the dialect of Egyptian Arabic ARZ
vs. MSA sentences from the Al-Youm Al-Sabe’ newspaper online commentaries 1. Our ultimate goal
is to use the dialect classifier for building a dialect-aware Arabic-English statistical machine translation
(SMT) system. Our Arabic-English training data contains a significant amount of Egyptian dialect data
only, and we would like to adapt the components of our hierarchical phrase-based SMT system (Zhao
and Al-Onaizan, 2008) to that data.
Similar to (Elfardy and Diab, 2013), we present a sentence-level classifier that is trained in a supervised
manner. Our approach is based on an Arabic tokenizer, but we do not use a range of specialized tokenizers
or orthography normalizers. In contrast to the language-model (LM) based classifier used by (Zaidan and
Callison-Burch, 2014), we present a linear classifier approach that works best without the use of LM-
based features. Some improvements in terms of classification accuracy and 10-fold cross validation under
the same data conditions as (Zaidan and Callison-Burch, 2011; Elfardy and Diab, 2013) are presented.
In general, we aim at a smaller amount of domain specific feature engineering than previous related
approaches.
The paper is structured as follows. In Section 2, we present related work on language and dialect
identification. In Section 3, we discuss the linear classification model used in this paper. In Section 4, we
evaluate the classifier performance in terms of classification accuracy on two data sets and present some
</bodyText>
<footnote confidence="0.9717455">
∗Part of the work was done while the author was a student intern at the IBM T.J. Watson Research Center.
1We use the ISO 639-3 code ARZ for denoting Egyptian Arabic.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
</footnote>
<page confidence="0.925321">
110
</page>
<note confidence="0.985788">
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 110–119,
Dublin, Ireland, August 23 2014.
</note>
<bodyText confidence="0.959108">
error analysis. Finally, in Section 5, we discuss future work on improved dialect-level classification and
its application to system adaptation for machine translation.
</bodyText>
<sectionHeader confidence="0.999245" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99997252631579">
From a computational perpective, we can view dialect identification as a more fine-grained form of lan-
guage identification (ID). Previous work on language ID examined the use of character histograms (Cav-
nar and Trenkle, 1994; Dunning, 1994), and high accuracy prediction results have been reported even
for languages with a common character set. (Baldwin and Lui, 2010) present a range of document-level
language identification techniques on three different data sets. They use n-gram counting techniques and
different tokenization schemes that are adopted to those data sets. Their classification task deals with
several languages, and it becomes more difficult as the number of languages increases. They present an
SVM-based multiclass classification approach similar to the one presented in this paper which performs
well on one of their data sets. (Trieschnigg et al., 2012) generates n-gram features based on character or
word sequences to classify dialectal documents in a dutch-language fairy-tale collection. Their baseline
model uses N-gram based text classification techniques as popularised in the TextCat tool (Cavnar and
Trenkle, 1994). Following (Baldwin and Lui, 2010), the authors extend the usage of n-gram features with
nearest neighbour and nearest-prototype models together with appropriately chosen similarity metrics.
(Zampieri and Gebre, 2012) classify two varieties of the same language: European and Brazilian Por-
tuguese. They use word and character-based language model classification techniques similar to (Zaidan
and Callison-Burch, 2014). (Huang and Lee, 2008) present simple bag-of-word techniques to classify
varieties of Chinese from the Chinese Gigaword corpus. (Kruengkrai et al., 2005) extend the use of n-
gram features to using string kernels: they may take into account all possible sub-strings for comparison
purposes. The resulting kernel-based classifier is compared against the method in (Cavnar and Trenkle,
1994). (Lui and Cook, 2013) present a dialect classification approach to identify Australian, British, and
Canadian English. They present results where they draw training and test data from different sources.
The successful transfer of models from one text source to another is evidence that their classifier indeed
captures dialectal rather than stylistic or formal differences. Language identification of related languages
is also addressed in the DSL (Discriminating Similar Languages) task of the present Vardial workshop
at COLING 14 (Tan et al., 2014).
While most of the above work focuses on document-level language classification, recent work on
handling Arabic dialect data addresses the problem of sentence-level classification (Zaidan and Callison-
Burch, 2011; Zaidan and Callison-Burch, 2014; Elfardy and Diab, 2013; Zaidan and Callison-Burch,
2014). The work is based on the data collection effort by (Zaidan and Callison-Burch, 2014) which
crowdsources the annotation task to workers on Amazons Mechanical Turk. The classification results
by (Zaidan and Callison-Burch, 2014) are based on n-gram language-models, where the n-grams are
defined both on words and characters. The authors find that unigram word-based models perform best.
The word-based models are obtained after a minimal amount of preprocessing such as proper handling
of HTML entities and Arabic numbers. Classification accuracy is significantly reduced for shorter sen-
tences. (Elfardy and Diab, 2013) presents classifcation result based on various tokinization and ortho-
graphic normalization techniques as well as so-called meta features that estimate the informalness of the
data. Like our work, the authors focus on a binary dialect classification based on the ARZ-MSA portion
of the dataset in (Zaidan and Callison-Burch, 2011).
</bodyText>
<sectionHeader confidence="0.999313" genericHeader="method">
3 Classification Model
</sectionHeader>
<bodyText confidence="0.885483">
We use a linear model and compute a score s(tn1) for a tokenized input sentence consisting of n tokens
ti:
d n
</bodyText>
<equation confidence="0.9990945">
s(tn1) = ws · φs(ci,ti) (1)
s=1 i=1
</equation>
<bodyText confidence="0.9982005">
where φs(ci, ti) is a binary feature function which takes into account the context ci of token ti. w ∈ Rd
is a high-dimensional weight vector obtained during training. In our experiments, we classify a tokenized
</bodyText>
<page confidence="0.997781">
111
</page>
<table confidence="0.9976355">
Description MSA ARZ
# sentences # words # sentences # words
ARZ-MSA portion of AOC 13,512 334K 12,527 327K
DEV12 tune set 585 8.4K 634 9.3K
</table>
<tableCaption confidence="0.999053">
Table 1: We used the following dialect data: 1) the ARZ-MSA portion of the AOC data from commen-
</tableCaption>
<bodyText confidence="0.9848214">
taries of the Egyptian newspaper Al-Youm Al-Sabe’, and 2) the DEV12 tune set (1219 sentences) which
is the LDC2012E30 corpus BOLT Phase 1 dev-tune set. The DEV12 tune set was annotated by a native
speaker of Arabic.
sentence as being Egyptian dialect (ARZ) if s(tn1) &gt; 0. To train the weights w in Eq. 1, we use a linear
SVM approach (Hsieh et al., 2008; Fan et al., 2008). The trainer can easily handle a huge number of
instances and features. The training data is given as instance-label pairs (xi, yi) where i E 11, · · · , l} and
l is the number of training sentences. The xi are d-dimensional vectors of integer-valued features that
count how often a binary feature fired for a tokenized sentence tn1. yi E 1+1, −11 are the class labels
where a label of ‘+1’ represents Egyptian dialect. During training, we solve the following optimization
problem:
</bodyText>
<equation confidence="0.999287">
min 11w111 + C l max(0, 1 − yi w xi ) , (2)
w i=1
</equation>
<bodyText confidence="0.999918333333333">
i.e. we use L1 regularized L2-loss support vector classification. We set the penalty term C = 0.5. For
our experiments, we use the data set provided in (Zaidan and Callison-Burch, 2011) which also has been
used in the experiments in (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2014). We focus on the
binary classification between MSA and ARZ. Details on the data sources can be found in Table 1. We
present accuracy results in terms of 10-fold stratified cross-validation which are comparable to previously
published work.
</bodyText>
<subsectionHeader confidence="0.999699">
3.1 Tokenization and Dictionaries
</subsectionHeader>
<bodyText confidence="0.999989875">
The Arabic tokenizer used in the current paper is based on (Lee et al., 2003). It is a general purpose
tokenizer which has been optimized towards improving machine translation quality of SMT systems
rather than dialect classification. Together with the tokenized text, a maximum-entropy based tagger
provides the part-of-speech (PoS) tags for each token. In addition, we have explored a range of features
that are based on the output of the AIDA software package (Elfardy and Diab, 2012; Mona Diab et
al., 2009 2011). The AIDA software has been made available to the participants of the DARPA-funded
Broad Operational Language Translation (BOLT) project. AIDA is a system for dialect identification,
classification and glossing on the token and sentence level for written Arabic. AIDA aggregates several
components including dictionaries and language models in order to perform named entity recognition,
dialect identification classification, and MSA English linearized glossing of the input text. We created
a dictionary from AIDA resources that includes about 41000 ARZ tokens. In addition, we obtained a
second small dictionary of about 70 ARZ dialect tokens with the help of a native speaker of Arabic. The
list was created by training two IBM Model 1 lexicons, one on Egyptian Arabic data and another on
MSA data. We then inspected the ARZ lexicon entries with the highest cosine distance to their MSA
counterparts and kept the ones that are strong ARZ words. The tokens in both dictionaries are not ARZ
exclusive, but could occur in MSA as well.
</bodyText>
<subsectionHeader confidence="0.999609">
3.2 Feature Set
</subsectionHeader>
<bodyText confidence="0.9999185">
In our work, we employ a simple set of binary feature functions based on the tokenized Arabic sentence.
For example, we define a token bigram feature as follows:
</bodyText>
<equation confidence="0.997933333333333">
φBi(tk,tk−1) =
� 1 tk = ‘s� y&apos;’ and tk−1 = ‘y��’
0 otherwise .(3)
</equation>
<page confidence="0.986136">
112
</page>
<bodyText confidence="0.9938817">
Token unigram and trigram features are defined accordingly. We also define unigram, bigram, and tri-
gram features based on PoS tags. Currently, just PoS unigrams are used in the experiments. We define
dictionary-based features as follows:
ODict,(tk) = 1 tk = ‘��..,.sgl�’ and tk E Dicta (4)
Sl 0 otherwise
where we use the two dictionaries Dict1 and Dict2 as described in Section 3.1. The dictionaries are
handled as token sets and we generate separate features for each of them. We generate some features
based on the AIDA tool output. AIDA provides a dialect label for each input token tk as well as a single
dialect label at the sentence level. A sentence-level binary feature based on the AIDA sentence level
classification is defined as follows:
</bodyText>
<equation confidence="0.9642265">
n
f 1 AIDA(ti) is ARZ
=
AIDA(t1) l (5)
0 otherwise
= 1/n
</equation>
<bodyText confidence="0.972074769230769">
where
is the language-model log probability for the dialect class ARZ . We used a trigram
language model.
is defined accordingly. In addition, we have implemented a range of so-called
features similar to the ones defined in (Elfardy and Diab, 2013). For example, we define a feature
which is equal to the length of the longest consecutive sequence of exclamation marks in
the tokenized sentence
.Similarly, we define features that count the longest sequence of punctuation
marks, the number of tokens, the averaged character-length of a token in the sentence, and the percentage
of words with word-lengthening effects. These features do not directly model dialectalness of the data
but rather try to capture the degree of in-formalness. Contrary to (Elfardy and Diab, 2013) we find that
those features do not improve accuracy of our best model in the cross-validation experiments. On the
set, the use of the meta features results in a significan
</bodyText>
<equation confidence="0.991400777777778">
OLM(tn1)
· [log(pMSA(tn1))−log(pARZ(tn1))] ,
log(pARZ(tn1))
pMSA(·)
‘meta’
OExca(tn1)
tn1
DEV12
t drop in accuracy.
</equation>
<bodyText confidence="0.504362">
effects on training a SMT system.
</bodyText>
<figure confidence="0.5676391">
sentences from the
set over a time period of three months using our own annotator. The kappa
value of the corresponding confusion matrix is 0.93, indicating very
DEV12
high agreement as well.
4.2 Classification Experiments
Following previous work, we present dialect prediction results in terms of accuracy:
# sent correctly tagged
ACC =
O
</figure>
<bodyText confidence="0.995821428571428">
where
is the sentence-level classification of the AIDA tool. A word-level feature
is
defined accordingly. These features improve the classification accuracy of our best system significantly.
We have also experimented with some real-valued feature. For example, we derived a feature from
dialect-specific lan
AIDA(tn1)
OAIDA(tk)
guage model probabilities:
4 Experiments
In this section, we present experimental results. Firstly, Section 4.1 demonstrates that our data is anno-
tated consistently. In Section 4.2, we present dialect prediction results in terms of accuracy and F-score
on our two data sets. In Section 4.3, we perform some qualitative error analysis for our classifier. In
Section 4.4, we present some preliminary
4.1 Annotator Agreement
To confirm the consistent annotation of our data, we have measured some inter-annotator and intra-
annotator agreement on it. A native speaker of Arabic was asked to classify the ARZ-MSA portion
of the dialect data using the following three labels: ARZ, MSA, Other. We randomly sampled 250
sentences from the ARZ-MSA portion of the Zaidan data maintaining the original dialect distribution.
The confusion matrix is shown in Table 2. It corresponds to a kappa value of 0.84 (using the definition of
(Fleiss, 1971)), which indicates a very high agreement. In addition, we did re-annotate asub-set of 200
</bodyText>
<equation confidence="0.2752885">
(6)
# sent ,
</equation>
<page confidence="0.738785">
113
</page>
<table confidence="0.9992344">
Predicted Class (IBM)
ARZ MSA Other
Actual ARZ 125 4 1
Class MSA 14 105 1
(AOC) Other 0 0 0
</table>
<tableCaption confidence="0.9055085">
Table 2: Inter annotator agreement on 250 randomly selected AOC sentences from the data in Table 1.
An in-lab annotator’s dialect prediction is compared against the AOC data gold-standard dialect labels.
</tableCaption>
<bodyText confidence="0.990155">
where ‘# sent’ is the number of sentences. In addition, we present dialect prediction results in terms of
precision, recall, and F-score. They are defined as follows:
</bodyText>
<equation confidence="0.708919875">
# sent correctly tagged as ARZ
Prec = (7)
# sent tagged as ARZ
# sent correctly tagged as ARZ
Recall =
# ref sent tagged as ARZ
F= 2 · Prec · Recall
(Prec + Recall).
</equation>
<bodyText confidence="0.998946">
MSA prediction F-score is defined analogously. Experimental results are presented in Table 3, where we
present results for different sets of feature types and the two test sets in Table 1. In the top half of the
table, results are presented in terms of 10-fold cross validation on the ARZ-MSA portion of the AOC
data. In the bottom half, we present results on DEV12 tune set, where we use the entire dialect data in
Table 1 for training (about 26K sentences).
As our baseline we have re-implemented the language-model-perplexity based approach reported in
(Zaidan and Callison-Burch, 2011). We train language models on the dialect-labeled commentary train-
ing data for each of the dialect classes c E {MSA, ARZ}. During testing, we compute the language
model probability of a sentence s for each of the classes c. We assign a sentence to the class c with the
highest probability (or the lowest perplexity) . For the 10-fold cross validation experiments, 10 language
models are built and perplexities are computed on 10 different test sets. The resulting (averaged) ac-
curacy is 83.3 % for cross-validation and 82.2 % on the DEV12 tune set. In comparison, (Elfardy and
Diab, 2013) reports an accuracy of 80.4 % as perplexity-based baseline. We have carried out additional
experiments with a simple feature set that consists of only unigram token and bigram token features as
defined in Eq. 3. Such a system performs surprisingly well under both testing conditions: we achieved an
accuracy of 87.7 % on the AOC data and an accuracy of 83.4 % on the DEV12 test set. On the AOC set
using 10-fold cross validation, we achieve only a small improvement from using the dictionary features
defined in Eq. 4. The accuracy is improved from 87.7 % to 88.0 %. On the DEV12 set, we obtain a
much larger improvement from using these features. Furthermore, we have investigated the usefulness
of the AIDA-based features. The stand-alone sentence-level classification of the AIDA tool performs
quite poorly. On the DEV12 set, it achieves an accuracy of just 77.9 %. But using the AIDA assigned
sentence-level and token-level dialect labels based on the binary features defined in Eq. 5 improves ac-
curacy significantly, e.g. from 85.3 % to 87.8 % on the DEV12 set. In the current experiments, the
so-called meta features which are computed at the sentence level do not improve classification accuracy.
The meta features are only useful in classifying dialect data based on the in-formalness of the data, i.e.
the ARZ news commentaries tend to exhibit more in-formalness than the MSA commentaries. Finally,
the sentence-level perplexity feature defined in Eq. 6 did not improve accuracy as well (no results for this
feature are presented in Table 3).
</bodyText>
<subsectionHeader confidence="0.998324">
4.3 Classifier Analysis
</subsectionHeader>
<bodyText confidence="0.996251666666667">
In this section, we perform a simple error analysis of the classifier performance on some dialect data for
which the degree of dialectalness is known. The data comes from news sources that differ from the data
used to train the classifier. The classifier is evaluated on data from the DARPA-funded BOLT project.
</bodyText>
<page confidence="0.996798">
114
</page>
<table confidence="0.999799416666667">
Feature Types MSA ARZ
ACC [%] PREC REC F PREC REC F
10-fold language-model 83.3 86.7 90.2 88.4 89.0 85.0 86.9
AOC aida-sentence label
uni,bi
uni,bi,dict,pos
uni,bi,dict,pos,aida
uni,bi,dict,pos,aida,meta
81.0 84.2 78.0 81.0 78.0 84.3 81.0
87.7 86.6 90.2 88.4 89.0 85.0 86.9
88.0 86.9 90.4 88.6 89.2 85.3 87.2
89.1 87.5 92.2 89.8 91.1 85.7 88.3
88.8 87.4 91.7 89.5 90.6 85.7 88.1
DEV12 language-model 82.2 85.1 76.2 80.4 80.0 87.7 83.7
aida-sentence label
uni,bi
uni,bi,dict,pos
uni,bi,dict,pos,aida
uni,bi,dict,pos,aida,meta
77.9 80.9 70.8 75.5 75.8 84.5 79.9
83.4 81.1 85.1 83.1 85.6 81.7 83.6
85.3 83.5 87.5 85.5 88.0 84.1 86.0
87.8 83.4 93.0 88.0 92.8 83.0 87.6
68.3 61.8 90.8 73.5 85.0 48.3 61.6
</table>
<tableCaption confidence="0.9925305">
Table 3: Arabic Dialect Classification Results: predicting MSA vs. (ARZ) dialect in terms of 10-fold
cross-validation on the AOC data and on the DEV12 set using all the AOC data for training.
</tableCaption>
<table confidence="0.99957525">
Corpus #Sent #Sent [ARZ] %[ARZ]
ARZ web forum 299K 183K 61%
Broadcast 169K 18K 11%
Newswire 885K 29K 3%
</table>
<tableCaption confidence="0.8333405">
Table 4: Sub-corpora together with total number as well as percentage of sentences that are classified as
ARZ.
</tableCaption>
<bodyText confidence="0.902946111111111">
The BOLT data consists of several corpora collected from various resources. These resources include
newswire, web-logs, ARZ web forum data and others. Classification statistics are presented in Table 4,
where we report the number of sentences along with the percentage of those sentences classified as ARZ.
The distribution of the dialect labels in the classifier output appears to correspond to the expected origin
of the data. For example, the ARZ web forum data contains a majority of ARZ sentences, but quite a
few sentences are MSA such as greetings and quotations from Islamic resources (Quran, Hadith ...). The
broadcast conversation data is mainly MSA, but sometimes the speaker switches to dialectal usage for
a short phrase and then switches back to MSA. Lastly, the newswire data has a vast majority of MSA
sentences. Examining a small portion of newswire sentences classified as ARZ, the sentences labeled as
ARZ are mostly classification errors.
Example sentence classifications from the BOLT data are shown in Table 5. The first two text frag-
ments are taken from the Egyptian Arabic (ARZ) web forum data. In the first document fragment, the
user starts with MSA sentences, then switches to Egyptian (ARZ) dialect marked by the ARZ indicator
úÎË@ and using the prefix # u before a verb which is not allowed in MSA. The user then switches back
to MSA. The classifier is able to classify the Egyptian Arabic (ARZ) sentence correctly. In the second
�
document fragment, the user uses several Egyptian Arabic (ARZ) words. In the forth sentence no ARZ
words exist, and the classifier correctly classifies the sentence as MSA. The third text fragment shows
</bodyText>
<page confidence="0.993891">
115
</page>
<table confidence="0.999912947368421">
Predicted Arabic English
Dialect
MSA . XðXQË@ ð ¨ñ �“ñÖÏ@ �H@Q�¯ A�K@ i read the topic and the replies .
MSA . �èñÊg �èQº�¯ ¨ñ �“ñÖÏ@ the topic is great !
ARZ ��&amp;quot;��� # y J�UI pB@ ©Ó A K@ # ð i agree with the brother who said
MSA �¯� ÑêÓ �áK�YË@ Islam is significant in all
ék. Ag É¿ ú
ARZ ZCJ.Ë@ ú because they accept affliction with patience
ARZ Î« &amp;quot;QA“ S-, €A�JË@ c1L v�6� what Hamas did was a victory
ARZ PA’:;@ Q.Lr@ &amp;quot;e-, €AÔg è+ :ÊÔ« JUI ð encountered the occupation
MSA ¯� @ñ ®~¯ð €AÔg S� jwho and they were patient despite the siege
ARZ ÈC�Jk@ v�3 ú that ’s why Allah rewarded them
PA’k Ll« @� ð
Ñë+ �jA¿ U+ H. P_ e.%r c1L A6.
ARZ* G~ �HXA�¯ Y~¯ # ð tdk ... led
ARZ* é» S� -, ú transport experts blame
ARZ* a.rCË@# y É�®,J@ ñj, # ð i cannot remember what he told me
. ø
+ # È è+ ÈA�¯ AÓ Q» �Y�K ©J�¢-Lƒ@ B
</table>
<tableCaption confidence="0.894996">
Table 5: Automatic classification examples for the dialect classes ARZ and MSA. Arabic source and
English target sentences are given. Dialectal words are in bold. Incorrect predictions are marked by an
asterisk (*).
</tableCaption>
<bodyText confidence="0.999716166666667">
some sentences from the newswire corpus that are mis-classified. The first sentence contains the word
øX which corresponds to the letter ‘d’ in the abbreviation ‘tdk’ . The word is contained in one of our ARZ
dictionaries such that the binary AIDA-based feature in Eq. 5 fires and triggers a mis-classification. In
this context, the word is part of an abbreviation which is split in the Arabic text. In the other examples,
only a few of the binary features defined in Section 3.2 apply and features that correspond to Arabic
prefixes tend to support a classification as ARZ dialect.
</bodyText>
<subsectionHeader confidence="0.99758">
4.4 Preliminary Application for SMT
</subsectionHeader>
<bodyText confidence="0.99997125">
The dialect classification of Arabic data for SMT can be used in various ways. Examples include domain-
specific tuning, mixture modeling, and the use of so-called provenance features (Chiang et al., 2011)
among others . As a motivation for the future use of the dialect classifier in SMT, we classify the BOLT
bilingual training data into ARZ and MSA parts and examine the effect on the phrase table scores. Phrase
translation pairs demonstrating the use of the classified training data are shown in Table 6. The ARZ web
forum data is split into an ARZ part and an MSA part and two separate phrase probability tables are
trained on these two splits. The ARZ web forum data is highly ambiguous with respect to dialect and it
is difficult to obtain good dialect-dependent splits of the data. In the first example in the table, the word
éJ��K.QªË@ could mean ‘Arab’ in MSA, but in ARZ it could also mean ‘car’. The phrase table scores obtained
from the classifier-split training data correctly reflect this ambiguity. The phrase pair with ‘car’ has the
lowest translation score for the BOLT.ARZ phrase table, while it has a higher cost in the BOLT.MSA
phrase table. In the full phrase table (BOLT), ‘car’ is the fifth translation candidate with a score of 2.09.
</bodyText>
<page confidence="0.99564">
116
</page>
<table confidence="0.9997937">
f BOLT.ARZ BOLT.MSA
e cost e cost
the car 1.20 arab 0.80
�������I arab 1.25 the arab 1.32
��
the arab 1.70 Arabic 1.52
merci 1.53 marsa 1.99
úæ...QÓ marsa 1.63 thanks 2.01
�
mursi 1.91 morcy 2.13
</table>
<tableCaption confidence="0.97886">
Table 6: Phrase tables based on classified training data. BOLT.ARZ is trained on the ARZ portion of
</tableCaption>
<bodyText confidence="0.966906">
the ARZ web forums data, while BOLT.MSA is trained on the MSA part. The table includes Arabic
words and the top three phrase translation candidates, sorted (first is best) by the phrase model cost
(cost= −log(p(f|e)) ).
In the second example, the word L5&amp;quot;QÓ could function as a proper noun with its English translation
‘mursi’ or ‘marsa’, but only in ARZ it could also be translated as ‘thanks’ (‘merci’). In this case, the
classifier is unable to distinguish between the ARZ dialect and the MSA usage. We found out that the
word token ‘merci’ appears only 4 times in the training data, rendering its binary features unreliable
reliable. In general we note that the phrase tables build on the classified data become more domain-
specific, and it is left to future work to check whether improvements could carry over to the translation
quality.
</bodyText>
<sectionHeader confidence="0.99696" genericHeader="discussions">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999990476190476">
The ultimate goal is to use the ARZ vs. MSA dialect classifier for training an adapted SMT system.
We split the training data at the sentence level using our classifier and train dialect-specific systems
on each of these splits along with a general dialect-independent system. We will be using techniques
similar to (Koehn and Schroeder, 2007; Chiang et al., 2011; Sennrich, 2012; Chen et al., 2013) to adapt
the general SMT system to a target domain with a predominant dialect. Or, we will be adopting an
SMT system to a development or test set where we use the classifier to predict the dialect for each
sentence and use a dialect-specific SMT system on each of them individually. Our approach of using
just binary feature functions in connection with a sentence-level global linear model can be related to
work on PoS-tagging (Collins, 2002). (Collins, 2002) trains a linear model based on Viterbi decoding
and the perceptron algorithm. The gold-standard PoS tags are given at the word-level, but the training
uses a global representation at the sentence level. Similarly, we use linear SVMs (Hsieh et al., 2008)
to train a classification model at the sentence level without access to sentence length statistics, i.e. our
best performing classifier does not compute features like the percentage of punctuation, numbers, or
averaged word length as has been proposed previously (Elfardy and Diab, 2013). All of our features are
actually computed at the token level (with the exception of a single sentence-level AIDA-based feature).
An interesting direction for future work could be to train the dialect classifier at the sentence level, but
use it to compute token-level predictions for a more fine-grained analysis. Even though the token-level
prediction task corresponds to a word-level tag set of just size 2, Viterbi decoding techniques could be
used to introduce novel context-dependent features, e.g. dialect tag n-gram features. Such a token-level
predictions might be used for weighting each phrase pair in an SMT system using methods like the
instance-based adaptation approach in (Foster et al., 2010).
</bodyText>
<sectionHeader confidence="0.969869" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99153">
The current work has been funded through the Broad Operational Language Translation (BOLT) program
under the project number DARPA HR0011-12-C-0015.
</bodyText>
<page confidence="0.996897">
117
</page>
<sectionHeader confidence="0.995855" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999551382978723">
Timothy Baldwin and Marco Lui. 2010. Language Identification: The Long and the Short of the Matter. In Proc.
of HLT’10, pages 229–237, Los Angeles, California, June.
William Cavnar and John M. Trenkle. 1994. N-gram-based Text Categorization. In In Proceedings of SDAIR-94,
3rd Annual Symposium on Document Analysis and Information Retrieval, pages 161–175.
Boxing Chen, George Foster, and Roland Kuhn. 2013. Adaptation of reordering models for statistical machine
translation. In Proc. of HLT’13, pages 938–946, Atlanta, Georgia, June.
David Chiang, Steve DeNeefe, and Michael Pust. 2011. Two Easy Improvements to Lexical Weighting. In Proc.
of HLT’11, pages 455–460, Portland, Oregon, USA, June.
Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments
with Perceptron Algorithms. In Proceedings of EMNLP’02, pages 1–8, Philadelphia,PA, July.
Ted Dunning. 1994. Statistical Identification of Language. technical report mccs 94-273. Technical report, New
Mexico State University.
Heba Elfardy and Mona Diab. 2012. Aida: Automatic Identification and Glossing of Dialectal Arabic. In Pro-
ceedings of the 16th EAMT Conference (Project Papers), pages 83–83, Trento, Italy, May.
Heba Elfardy and Mona Diab. 2013. Sentence level dialect Identification in arabic. In Proc. of the ACL 2013
(Volume 2: Short Papers), pages 456–461, Sofia, Bulgaria, August.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: a Library
for Large Linear Classification. Machine Learning Journal, 9:1871–1874.
Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin,
76(5):378.
George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative Instance Weighting for Domain Adaptation
in Statistical Machine Translation. In Proc. of EMNLP’10, pages 451–459.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S.S. Keerthi, and S.Sundararajan. 2008. A Dual Coordinate Descent Method
for Large-scale linear SVM. In ICML, pages 919–926, Helsinki,Finland.
Chu-Ren Huang and Lung-Hao Lee. 2008. Contrastive Approach towards Text Source Classication based on
top-bag-of-word Similarity. In PACLIC 2008, pages 404–410, Cebu City, Philippines.
Philipp Koehn and Josh Schroeder. 2007. Experiments in Domain Adaptation for Statistical Machine Translation.
In Proceedings of the Second Workshop on Statistical Machine Translation (WMT07), pages 224–227.
Canasai Kruengkrai, Prapass Srichaivattana, Virach Sornlertlamvanich, and Hitoshi Isahara. 2005. Language
Identification based on string kernels. In In Proceedings of the 5th International Symposium on Communications
and Information Technologies (ISCIT-2005, pages 896–899.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Ossama Emam, and Hany Hassan. 2003. Language Model
Based Arabic Word Segmentation. In Proc. of the 41st Annual Conf. of the Association for Computational
Linguistics (ACL 03), pages 399–406, Sapporo, Japan, July.
Marco Lui and Paul Cook. 2013. Classifying English Documents by National Dialect. In Proc. Australasian
Language Technology Workshop, pages 5–15.
Mona Mona Diab, Heba Elfardy, and Yassine Benajiba. 2009–2011. AIDA Automatic Identification of Arabic Di-
alectal Text. a Tool for Dialect Identification &amp; Classification, Named Entity Recognition, English and Modern
Standard Arabic Glossing and Normalization.
Rico Sennrich. 2012. Perplexity minimization for translation model domain adaptation in statistical machine
translation. In Proc. of EACL’12, pages 539–549.
Liling Tan, Marcos Zampieri, Nicola Ljubeˇsi´c, and J¨org Tiedemann. 2014. Merging Comparable Data Sources
for the Discrimination of Similar Languages: The DSL Corpus Collection. In 7th Workshop on Building and
Using Comparable Corpora at LREC’14, Reykjavik, Iceland, September.
D. Trieschnigg, D. Hiemstra, M. Theune F. Jong, and T. Meder. 2012. An Exploration of Language Identication
Techniques for the Dutch Folktale Database. In Adaptation of Language Resources and Tools for Processing
Cultural Heritage Workshop (LREC 2012), Istanbul, Turkey, May.
</reference>
<page confidence="0.984575">
118
</page>
<reference confidence="0.998840285714286">
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowdsourcing translation: Professional quality from non-
professionals. In Proc. ofACL/HLT 11, pages 1220–1229, Portland, Oregon, USA, June.
Omar F. Zaidan and Chris Callison-Burch. 2014. Arabic Dialect Classification. CL, 40(1):171–202.
Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic Identication of Language Varieties: The case
of Portuguese. In Konvens 12, pages 233–237, Vienna, Austria.
Bing Zhao and Yaser Al-Onaizan. 2008. Generalizing Local and Non-Local word-reordering patterns for syntax-
based machine translation. In Proc. of EMNLP’08, pages 572–581, Honolulu, Hawaii, October.
</reference>
<page confidence="0.999091">
119
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.832950">
<title confidence="0.999503">Improved Sentence-Level Arabic Dialect Classification</title>
<author confidence="0.995005">Tillmann Al-Onaizan Saab</author>
<affiliation confidence="0.999986">IBM T.J. Watson Research Center Aachen University</affiliation>
<address confidence="0.877182">Yorktown Heights, NY, USA Aachen, Germany</address>
<email confidence="0.952253">Ictill,onaizanj@us.ibm.commansour@cs.rwth-aachen.de</email>
<abstract confidence="0.9993479">The paper presents work on improved sentence-level dialect classification of Egyptian Arabic (ARZ) vs. Modern Standard Arabic (MSA). Our approach is based on binary feature functions that can be implemented with a minimal amount of task-specific knowledge. We train a featurerich linear classifier based on a linear support-vector machine (linear SVM) approach. Our best achieves an accuracy of on the Arabic Online Commentary (AOC) dataset (Zaidan Callison-Burch, 2011) using stratified cross validation: a absolute accuracy improvement over the results published by (Zaidan and Callison-Burch, 2014). We also evaluate the classifier on dialect data from an additional data source. Here, we find that features which measure the informalness of a sentence actually decrease classification accuracy significantly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Marco Lui</author>
</authors>
<title>Language Identification: The Long and the Short of the Matter.</title>
<date>2010</date>
<booktitle>In Proc. of HLT’10,</booktitle>
<pages>229--237</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="4617" citStr="Baldwin and Lui, 2010" startWordPosition="684" endWordPosition="687">ieties and Dialects, pages 110–119, Dublin, Ireland, August 23 2014. error analysis. Finally, in Section 5, we discuss future work on improved dialect-level classification and its application to system adaptation for machine translation. 2 Related Work From a computational perpective, we can view dialect identification as a more fine-grained form of language identification (ID). Previous work on language ID examined the use of character histograms (Cavnar and Trenkle, 1994; Dunning, 1994), and high accuracy prediction results have been reported even for languages with a common character set. (Baldwin and Lui, 2010) present a range of document-level language identification techniques on three different data sets. They use n-gram counting techniques and different tokenization schemes that are adopted to those data sets. Their classification task deals with several languages, and it becomes more difficult as the number of languages increases. They present an SVM-based multiclass classification approach similar to the one presented in this paper which performs well on one of their data sets. (Trieschnigg et al., 2012) generates n-gram features based on character or word sequences to classify dialectal docum</context>
</contexts>
<marker>Baldwin, Lui, 2010</marker>
<rawString>Timothy Baldwin and Marco Lui. 2010. Language Identification: The Long and the Short of the Matter. In Proc. of HLT’10, pages 229–237, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Cavnar</author>
<author>John M Trenkle</author>
</authors>
<title>N-gram-based Text Categorization. In</title>
<date>1994</date>
<booktitle>In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>161--175</pages>
<contexts>
<context position="4472" citStr="Cavnar and Trenkle, 1994" startWordPosition="661" endWordPosition="665"> License details: http://creativecommons.org/licenses/by/4.0/ 110 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 110–119, Dublin, Ireland, August 23 2014. error analysis. Finally, in Section 5, we discuss future work on improved dialect-level classification and its application to system adaptation for machine translation. 2 Related Work From a computational perpective, we can view dialect identification as a more fine-grained form of language identification (ID). Previous work on language ID examined the use of character histograms (Cavnar and Trenkle, 1994; Dunning, 1994), and high accuracy prediction results have been reported even for languages with a common character set. (Baldwin and Lui, 2010) present a range of document-level language identification techniques on three different data sets. They use n-gram counting techniques and different tokenization schemes that are adopted to those data sets. Their classification task deals with several languages, and it becomes more difficult as the number of languages increases. They present an SVM-based multiclass classification approach similar to the one presented in this paper which performs well</context>
<context position="6202" citStr="Cavnar and Trenkle, 1994" startWordPosition="910" endWordPosition="913">y metrics. (Zampieri and Gebre, 2012) classify two varieties of the same language: European and Brazilian Portuguese. They use word and character-based language model classification techniques similar to (Zaidan and Callison-Burch, 2014). (Huang and Lee, 2008) present simple bag-of-word techniques to classify varieties of Chinese from the Chinese Gigaword corpus. (Kruengkrai et al., 2005) extend the use of ngram features to using string kernels: they may take into account all possible sub-strings for comparison purposes. The resulting kernel-based classifier is compared against the method in (Cavnar and Trenkle, 1994). (Lui and Cook, 2013) present a dialect classification approach to identify Australian, British, and Canadian English. They present results where they draw training and test data from different sources. The successful transfer of models from one text source to another is evidence that their classifier indeed captures dialectal rather than stylistic or formal differences. Language identification of related languages is also addressed in the DSL (Discriminating Similar Languages) task of the present Vardial workshop at COLING 14 (Tan et al., 2014). While most of the above work focuses on docume</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>William Cavnar and John M. Trenkle. 1994. N-gram-based Text Categorization. In In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval, pages 161–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Adaptation of reordering models for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proc. of HLT’13,</booktitle>
<pages>938--946</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="26460" citStr="Chen et al., 2013" startWordPosition="4298" endWordPosition="4301">note that the phrase tables build on the classified data become more domainspecific, and it is left to future work to check whether improvements could carry over to the translation quality. 5 Discussion and Future Work The ultimate goal is to use the ARZ vs. MSA dialect classifier for training an adapted SMT system. We split the training data at the sentence level using our classifier and train dialect-specific systems on each of these splits along with a general dialect-independent system. We will be using techniques similar to (Koehn and Schroeder, 2007; Chiang et al., 2011; Sennrich, 2012; Chen et al., 2013) to adapt the general SMT system to a target domain with a predominant dialect. Or, we will be adopting an SMT system to a development or test set where we use the classifier to predict the dialect for each sentence and use a dialect-specific SMT system on each of them individually. Our approach of using just binary feature functions in connection with a sentence-level global linear model can be related to work on PoS-tagging (Collins, 2002). (Collins, 2002) trains a linear model based on Viterbi decoding and the perceptron algorithm. The gold-standard PoS tags are given at the word-level, but</context>
</contexts>
<marker>Chen, Foster, Kuhn, 2013</marker>
<rawString>Boxing Chen, George Foster, and Roland Kuhn. 2013. Adaptation of reordering models for statistical machine translation. In Proc. of HLT’13, pages 938–946, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Steve DeNeefe</author>
<author>Michael Pust</author>
</authors>
<title>Two Easy Improvements to Lexical Weighting.</title>
<date>2011</date>
<booktitle>In Proc. of HLT’11,</booktitle>
<pages>455--460</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="23835" citStr="Chiang et al., 2011" startWordPosition="3844" endWordPosition="3847">ur ARZ dictionaries such that the binary AIDA-based feature in Eq. 5 fires and triggers a mis-classification. In this context, the word is part of an abbreviation which is split in the Arabic text. In the other examples, only a few of the binary features defined in Section 3.2 apply and features that correspond to Arabic prefixes tend to support a classification as ARZ dialect. 4.4 Preliminary Application for SMT The dialect classification of Arabic data for SMT can be used in various ways. Examples include domainspecific tuning, mixture modeling, and the use of so-called provenance features (Chiang et al., 2011) among others . As a motivation for the future use of the dialect classifier in SMT, we classify the BOLT bilingual training data into ARZ and MSA parts and examine the effect on the phrase table scores. Phrase translation pairs demonstrating the use of the classified training data are shown in Table 6. The ARZ web forum data is split into an ARZ part and an MSA part and two separate phrase probability tables are trained on these two splits. The ARZ web forum data is highly ambiguous with respect to dialect and it is difficult to obtain good dialect-dependent splits of the data. In the first e</context>
<context position="26424" citStr="Chiang et al., 2011" startWordPosition="4292" endWordPosition="4295">s unreliable reliable. In general we note that the phrase tables build on the classified data become more domainspecific, and it is left to future work to check whether improvements could carry over to the translation quality. 5 Discussion and Future Work The ultimate goal is to use the ARZ vs. MSA dialect classifier for training an adapted SMT system. We split the training data at the sentence level using our classifier and train dialect-specific systems on each of these splits along with a general dialect-independent system. We will be using techniques similar to (Koehn and Schroeder, 2007; Chiang et al., 2011; Sennrich, 2012; Chen et al., 2013) to adapt the general SMT system to a target domain with a predominant dialect. Or, we will be adopting an SMT system to a development or test set where we use the classifier to predict the dialect for each sentence and use a dialect-specific SMT system on each of them individually. Our approach of using just binary feature functions in connection with a sentence-level global linear model can be related to work on PoS-tagging (Collins, 2002). (Collins, 2002) trains a linear model based on Viterbi decoding and the perceptron algorithm. The gold-standard PoS t</context>
</contexts>
<marker>Chiang, DeNeefe, Pust, 2011</marker>
<rawString>David Chiang, Steve DeNeefe, and Michael Pust. 2011. Two Easy Improvements to Lexical Weighting. In Proc. of HLT’11, pages 455–460, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP’02,</booktitle>
<pages>1--8</pages>
<location>Philadelphia,PA,</location>
<contexts>
<context position="26905" citStr="Collins, 2002" startWordPosition="4376" endWordPosition="4377">ong with a general dialect-independent system. We will be using techniques similar to (Koehn and Schroeder, 2007; Chiang et al., 2011; Sennrich, 2012; Chen et al., 2013) to adapt the general SMT system to a target domain with a predominant dialect. Or, we will be adopting an SMT system to a development or test set where we use the classifier to predict the dialect for each sentence and use a dialect-specific SMT system on each of them individually. Our approach of using just binary feature functions in connection with a sentence-level global linear model can be related to work on PoS-tagging (Collins, 2002). (Collins, 2002) trains a linear model based on Viterbi decoding and the perceptron algorithm. The gold-standard PoS tags are given at the word-level, but the training uses a global representation at the sentence level. Similarly, we use linear SVMs (Hsieh et al., 2008) to train a classification model at the sentence level without access to sentence length statistics, i.e. our best performing classifier does not compute features like the percentage of punctuation, numbers, or averaged word length as has been proposed previously (Elfardy and Diab, 2013). All of our features are actually comput</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of EMNLP’02, pages 1–8, Philadelphia,PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Statistical Identification of Language. technical report mccs 94-273.</title>
<date>1994</date>
<tech>Technical report,</tech>
<institution>New Mexico State University.</institution>
<contexts>
<context position="4488" citStr="Dunning, 1994" startWordPosition="666" endWordPosition="667">reativecommons.org/licenses/by/4.0/ 110 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 110–119, Dublin, Ireland, August 23 2014. error analysis. Finally, in Section 5, we discuss future work on improved dialect-level classification and its application to system adaptation for machine translation. 2 Related Work From a computational perpective, we can view dialect identification as a more fine-grained form of language identification (ID). Previous work on language ID examined the use of character histograms (Cavnar and Trenkle, 1994; Dunning, 1994), and high accuracy prediction results have been reported even for languages with a common character set. (Baldwin and Lui, 2010) present a range of document-level language identification techniques on three different data sets. They use n-gram counting techniques and different tokenization schemes that are adopted to those data sets. Their classification task deals with several languages, and it becomes more difficult as the number of languages increases. They present an SVM-based multiclass classification approach similar to the one presented in this paper which performs well on one of their</context>
</contexts>
<marker>Dunning, 1994</marker>
<rawString>Ted Dunning. 1994. Statistical Identification of Language. technical report mccs 94-273. Technical report, New Mexico State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heba Elfardy</author>
<author>Mona Diab</author>
</authors>
<title>Aida: Automatic Identification and Glossing of Dialectal Arabic.</title>
<date>2012</date>
<booktitle>In Proceedings of the 16th EAMT Conference (Project Papers),</booktitle>
<pages>83--83</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="10586" citStr="Elfardy and Diab, 2012" startWordPosition="1623" endWordPosition="1626">ts in terms of 10-fold stratified cross-validation which are comparable to previously published work. 3.1 Tokenization and Dictionaries The Arabic tokenizer used in the current paper is based on (Lee et al., 2003). It is a general purpose tokenizer which has been optimized towards improving machine translation quality of SMT systems rather than dialect classification. Together with the tokenized text, a maximum-entropy based tagger provides the part-of-speech (PoS) tags for each token. In addition, we have explored a range of features that are based on the output of the AIDA software package (Elfardy and Diab, 2012; Mona Diab et al., 2009 2011). The AIDA software has been made available to the participants of the DARPA-funded Broad Operational Language Translation (BOLT) project. AIDA is a system for dialect identification, classification and glossing on the token and sentence level for written Arabic. AIDA aggregates several components including dictionaries and language models in order to perform named entity recognition, dialect identification classification, and MSA English linearized glossing of the input text. We created a dictionary from AIDA resources that includes about 41000 ARZ tokens. In add</context>
</contexts>
<marker>Elfardy, Diab, 2012</marker>
<rawString>Heba Elfardy and Mona Diab. 2012. Aida: Automatic Identification and Glossing of Dialectal Arabic. In Proceedings of the 16th EAMT Conference (Project Papers), pages 83–83, Trento, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heba Elfardy</author>
<author>Mona Diab</author>
</authors>
<title>Sentence level dialect Identification in arabic.</title>
<date>2013</date>
<booktitle>In Proc. of the ACL 2013 (Volume 2: Short Papers),</booktitle>
<pages>456--461</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1347" citStr="Elfardy and Diab, 2013" startWordPosition="182" endWordPosition="185"> and Callison-Burch, 2011) using 10-fold stratified cross validation: a 1.3 % absolute accuracy improvement over the results published by (Zaidan and Callison-Burch, 2014). We also evaluate the classifier on dialect data from an additional data source. Here, we find that features which measure the informalness of a sentence actually decrease classification accuracy significantly. 1 Introduction The standard form of written Arabic is Modern Standard Arabic (MSA) . It differs significantly from various spoken varieties of Arabic (Zaidan and Callison-Burch, 2011; Zaidan and Callison-Burch, 2014; Elfardy and Diab, 2013). Even though these dialects do not originally exist in written form, they are present in social media texts. Recently a dataset of dialectal Arabic has been made available in the form of the Arabic Online Commentary (AOC) set (Zaidan and Callison-Burch, 2011; Zaidan and CallisonBurch, 2014). The data consists of reader commentary from the online versions of Arabic newspapers, which have a high degree of dialect content. Data for the following dialects has been collected: Levantine, Gulf, and Egyptian. The data had been obtained by a crowd-sourcing effort. In the current paper, we present resu</context>
<context position="3090" citStr="Elfardy and Diab, 2013" startWordPosition="452" endWordPosition="455"> to that data. Similar to (Elfardy and Diab, 2013), we present a sentence-level classifier that is trained in a supervised manner. Our approach is based on an Arabic tokenizer, but we do not use a range of specialized tokenizers or orthography normalizers. In contrast to the language-model (LM) based classifier used by (Zaidan and Callison-Burch, 2014), we present a linear classifier approach that works best without the use of LMbased features. Some improvements in terms of classification accuracy and 10-fold cross validation under the same data conditions as (Zaidan and Callison-Burch, 2011; Elfardy and Diab, 2013) are presented. In general, we aim at a smaller amount of domain specific feature engineering than previous related approaches. The paper is structured as follows. In Section 2, we present related work on language and dialect identification. In Section 3, we discuss the linear classification model used in this paper. In Section 4, we evaluate the classifier performance in terms of classification accuracy on two data sets and present some ∗Part of the work was done while the author was a student intern at the IBM T.J. Watson Research Center. 1We use the ISO 639-3 code ARZ for denoting Egyptian </context>
<context position="7023" citStr="Elfardy and Diab, 2013" startWordPosition="1028" endWordPosition="1031">ces. The successful transfer of models from one text source to another is evidence that their classifier indeed captures dialectal rather than stylistic or formal differences. Language identification of related languages is also addressed in the DSL (Discriminating Similar Languages) task of the present Vardial workshop at COLING 14 (Tan et al., 2014). While most of the above work focuses on document-level language classification, recent work on handling Arabic dialect data addresses the problem of sentence-level classification (Zaidan and CallisonBurch, 2011; Zaidan and Callison-Burch, 2014; Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2014). The work is based on the data collection effort by (Zaidan and Callison-Burch, 2014) which crowdsources the annotation task to workers on Amazons Mechanical Turk. The classification results by (Zaidan and Callison-Burch, 2014) are based on n-gram language-models, where the n-grams are defined both on words and characters. The authors find that unigram word-based models perform best. The word-based models are obtained after a minimal amount of preprocessing such as proper handling of HTML entities and Arabic numbers. Classification accuracy is significantly r</context>
<context position="9791" citStr="Elfardy and Diab, 2013" startWordPosition="1499" endWordPosition="1502">the number of training sentences. The xi are d-dimensional vectors of integer-valued features that count how often a binary feature fired for a tokenized sentence tn1. yi E 1+1, −11 are the class labels where a label of ‘+1’ represents Egyptian dialect. During training, we solve the following optimization problem: min 11w111 + C l max(0, 1 − yi w xi ) , (2) w i=1 i.e. we use L1 regularized L2-loss support vector classification. We set the penalty term C = 0.5. For our experiments, we use the data set provided in (Zaidan and Callison-Burch, 2011) which also has been used in the experiments in (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2014). We focus on the binary classification between MSA and ARZ. Details on the data sources can be found in Table 1. We present accuracy results in terms of 10-fold stratified cross-validation which are comparable to previously published work. 3.1 Tokenization and Dictionaries The Arabic tokenizer used in the current paper is based on (Lee et al., 2003). It is a general purpose tokenizer which has been optimized towards improving machine translation quality of SMT systems rather than dialect classification. Together with the tokenized text, a maximum-entropy base</context>
<context position="12955" citStr="Elfardy and Diab, 2013" startWordPosition="2021" endWordPosition="2024">e generate separate features for each of them. We generate some features based on the AIDA tool output. AIDA provides a dialect label for each input token tk as well as a single dialect label at the sentence level. A sentence-level binary feature based on the AIDA sentence level classification is defined as follows: n f 1 AIDA(ti) is ARZ = AIDA(t1) l (5) 0 otherwise = 1/n where is the language-model log probability for the dialect class ARZ . We used a trigram language model. is defined accordingly. In addition, we have implemented a range of so-called features similar to the ones defined in (Elfardy and Diab, 2013). For example, we define a feature which is equal to the length of the longest consecutive sequence of exclamation marks in the tokenized sentence .Similarly, we define features that count the longest sequence of punctuation marks, the number of tokens, the averaged character-length of a token in the sentence, and the percentage of words with word-lengthening effects. These features do not directly model dialectalness of the data but rather try to capture the degree of in-formalness. Contrary to (Elfardy and Diab, 2013) we find that those features do not improve accuracy of our best model in t</context>
<context position="17316" citStr="Elfardy and Diab, 2013" startWordPosition="2731" endWordPosition="2734">n (Zaidan and Callison-Burch, 2011). We train language models on the dialect-labeled commentary training data for each of the dialect classes c E {MSA, ARZ}. During testing, we compute the language model probability of a sentence s for each of the classes c. We assign a sentence to the class c with the highest probability (or the lowest perplexity) . For the 10-fold cross validation experiments, 10 language models are built and perplexities are computed on 10 different test sets. The resulting (averaged) accuracy is 83.3 % for cross-validation and 82.2 % on the DEV12 tune set. In comparison, (Elfardy and Diab, 2013) reports an accuracy of 80.4 % as perplexity-based baseline. We have carried out additional experiments with a simple feature set that consists of only unigram token and bigram token features as defined in Eq. 3. Such a system performs surprisingly well under both testing conditions: we achieved an accuracy of 87.7 % on the AOC data and an accuracy of 83.4 % on the DEV12 test set. On the AOC set using 10-fold cross validation, we achieve only a small improvement from using the dictionary features defined in Eq. 4. The accuracy is improved from 87.7 % to 88.0 %. On the DEV12 set, we obtain a mu</context>
<context position="27464" citStr="Elfardy and Diab, 2013" startWordPosition="4460" endWordPosition="4463">near model can be related to work on PoS-tagging (Collins, 2002). (Collins, 2002) trains a linear model based on Viterbi decoding and the perceptron algorithm. The gold-standard PoS tags are given at the word-level, but the training uses a global representation at the sentence level. Similarly, we use linear SVMs (Hsieh et al., 2008) to train a classification model at the sentence level without access to sentence length statistics, i.e. our best performing classifier does not compute features like the percentage of punctuation, numbers, or averaged word length as has been proposed previously (Elfardy and Diab, 2013). All of our features are actually computed at the token level (with the exception of a single sentence-level AIDA-based feature). An interesting direction for future work could be to train the dialect classifier at the sentence level, but use it to compute token-level predictions for a more fine-grained analysis. Even though the token-level prediction task corresponds to a word-level tag set of just size 2, Viterbi decoding techniques could be used to introduce novel context-dependent features, e.g. dialect tag n-gram features. Such a token-level predictions might be used for weighting each p</context>
</contexts>
<marker>Elfardy, Diab, 2013</marker>
<rawString>Heba Elfardy and Mona Diab. 2013. Sentence level dialect Identification in arabic. In Proc. of the ACL 2013 (Volume 2: Short Papers), pages 456–461, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: a Library for Large Linear Classification.</title>
<date>2008</date>
<journal>Machine Learning Journal,</journal>
<pages>9--1871</pages>
<contexts>
<context position="9002" citStr="Fan et al., 2008" startWordPosition="1355" endWordPosition="1358">nized 111 Description MSA ARZ # sentences # words # sentences # words ARZ-MSA portion of AOC 13,512 334K 12,527 327K DEV12 tune set 585 8.4K 634 9.3K Table 1: We used the following dialect data: 1) the ARZ-MSA portion of the AOC data from commentaries of the Egyptian newspaper Al-Youm Al-Sabe’, and 2) the DEV12 tune set (1219 sentences) which is the LDC2012E30 corpus BOLT Phase 1 dev-tune set. The DEV12 tune set was annotated by a native speaker of Arabic. sentence as being Egyptian dialect (ARZ) if s(tn1) &gt; 0. To train the weights w in Eq. 1, we use a linear SVM approach (Hsieh et al., 2008; Fan et al., 2008). The trainer can easily handle a huge number of instances and features. The training data is given as instance-label pairs (xi, yi) where i E 11, · · · , l} and l is the number of training sentences. The xi are d-dimensional vectors of integer-valued features that count how often a binary feature fired for a tokenized sentence tn1. yi E 1+1, −11 are the class labels where a label of ‘+1’ represents Egyptian dialect. During training, we solve the following optimization problem: min 11w111 + C l max(0, 1 − yi w xi ) , (2) w i=1 i.e. we use L1 regularized L2-loss support vector classification. W</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: a Library for Large Linear Classification. Machine Learning Journal, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="15403" citStr="Fleiss, 1971" startWordPosition="2401" endWordPosition="2402">error analysis for our classifier. In Section 4.4, we present some preliminary 4.1 Annotator Agreement To confirm the consistent annotation of our data, we have measured some inter-annotator and intraannotator agreement on it. A native speaker of Arabic was asked to classify the ARZ-MSA portion of the dialect data using the following three labels: ARZ, MSA, Other. We randomly sampled 250 sentences from the ARZ-MSA portion of the Zaidan data maintaining the original dialect distribution. The confusion matrix is shown in Table 2. It corresponds to a kappa value of 0.84 (using the definition of (Fleiss, 1971)), which indicates a very high agreement. In addition, we did re-annotate asub-set of 200 (6) # sent , 113 Predicted Class (IBM) ARZ MSA Other Actual ARZ 125 4 1 Class MSA 14 105 1 (AOC) Other 0 0 0 Table 2: Inter annotator agreement on 250 randomly selected AOC sentences from the data in Table 1. An in-lab annotator’s dialect prediction is compared against the AOC data gold-standard dialect labels. where ‘# sent’ is the number of sentences. In addition, we present dialect prediction results in terms of precision, recall, and F-score. They are defined as follows: # sent correctly tagged as ARZ</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP’10,</booktitle>
<pages>451--459</pages>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation. In Proc. of EMNLP’10, pages 451–459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-J Hsieh</author>
<author>K-W Chang</author>
<author>C-J Lin</author>
<author>S S Keerthi</author>
<author>S Sundararajan</author>
</authors>
<title>A Dual Coordinate Descent Method for Large-scale linear SVM. In</title>
<date>2008</date>
<booktitle>ICML,</booktitle>
<pages>919--926</pages>
<contexts>
<context position="8983" citStr="Hsieh et al., 2008" startWordPosition="1351" endWordPosition="1354">, we classify a tokenized 111 Description MSA ARZ # sentences # words # sentences # words ARZ-MSA portion of AOC 13,512 334K 12,527 327K DEV12 tune set 585 8.4K 634 9.3K Table 1: We used the following dialect data: 1) the ARZ-MSA portion of the AOC data from commentaries of the Egyptian newspaper Al-Youm Al-Sabe’, and 2) the DEV12 tune set (1219 sentences) which is the LDC2012E30 corpus BOLT Phase 1 dev-tune set. The DEV12 tune set was annotated by a native speaker of Arabic. sentence as being Egyptian dialect (ARZ) if s(tn1) &gt; 0. To train the weights w in Eq. 1, we use a linear SVM approach (Hsieh et al., 2008; Fan et al., 2008). The trainer can easily handle a huge number of instances and features. The training data is given as instance-label pairs (xi, yi) where i E 11, · · · , l} and l is the number of training sentences. The xi are d-dimensional vectors of integer-valued features that count how often a binary feature fired for a tokenized sentence tn1. yi E 1+1, −11 are the class labels where a label of ‘+1’ represents Egyptian dialect. During training, we solve the following optimization problem: min 11w111 + C l max(0, 1 − yi w xi ) , (2) w i=1 i.e. we use L1 regularized L2-loss support vecto</context>
<context position="27176" citStr="Hsieh et al., 2008" startWordPosition="4417" endWordPosition="4420">opting an SMT system to a development or test set where we use the classifier to predict the dialect for each sentence and use a dialect-specific SMT system on each of them individually. Our approach of using just binary feature functions in connection with a sentence-level global linear model can be related to work on PoS-tagging (Collins, 2002). (Collins, 2002) trains a linear model based on Viterbi decoding and the perceptron algorithm. The gold-standard PoS tags are given at the word-level, but the training uses a global representation at the sentence level. Similarly, we use linear SVMs (Hsieh et al., 2008) to train a classification model at the sentence level without access to sentence length statistics, i.e. our best performing classifier does not compute features like the percentage of punctuation, numbers, or averaged word length as has been proposed previously (Elfardy and Diab, 2013). All of our features are actually computed at the token level (with the exception of a single sentence-level AIDA-based feature). An interesting direction for future work could be to train the dialect classifier at the sentence level, but use it to compute token-level predictions for a more fine-grained analys</context>
</contexts>
<marker>Hsieh, Chang, Lin, Keerthi, Sundararajan, 2008</marker>
<rawString>C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S.S. Keerthi, and S.Sundararajan. 2008. A Dual Coordinate Descent Method for Large-scale linear SVM. In ICML, pages 919–926, Helsinki,Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chu-Ren Huang</author>
<author>Lung-Hao Lee</author>
</authors>
<title>Contrastive Approach towards Text Source Classication based on top-bag-of-word Similarity. In PACLIC</title>
<date>2008</date>
<pages>404--410</pages>
<location>Cebu City, Philippines.</location>
<contexts>
<context position="5837" citStr="Huang and Lee, 2008" startWordPosition="856" endWordPosition="859">ents in a dutch-language fairy-tale collection. Their baseline model uses N-gram based text classification techniques as popularised in the TextCat tool (Cavnar and Trenkle, 1994). Following (Baldwin and Lui, 2010), the authors extend the usage of n-gram features with nearest neighbour and nearest-prototype models together with appropriately chosen similarity metrics. (Zampieri and Gebre, 2012) classify two varieties of the same language: European and Brazilian Portuguese. They use word and character-based language model classification techniques similar to (Zaidan and Callison-Burch, 2014). (Huang and Lee, 2008) present simple bag-of-word techniques to classify varieties of Chinese from the Chinese Gigaword corpus. (Kruengkrai et al., 2005) extend the use of ngram features to using string kernels: they may take into account all possible sub-strings for comparison purposes. The resulting kernel-based classifier is compared against the method in (Cavnar and Trenkle, 1994). (Lui and Cook, 2013) present a dialect classification approach to identify Australian, British, and Canadian English. They present results where they draw training and test data from different sources. The successful transfer of mode</context>
</contexts>
<marker>Huang, Lee, 2008</marker>
<rawString>Chu-Ren Huang and Lung-Hao Lee. 2008. Contrastive Approach towards Text Source Classication based on top-bag-of-word Similarity. In PACLIC 2008, pages 404–410, Cebu City, Philippines.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in Domain Adaptation for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation (WMT07),</booktitle>
<pages>224--227</pages>
<contexts>
<context position="26403" citStr="Koehn and Schroeder, 2007" startWordPosition="4288" endWordPosition="4291">endering its binary features unreliable reliable. In general we note that the phrase tables build on the classified data become more domainspecific, and it is left to future work to check whether improvements could carry over to the translation quality. 5 Discussion and Future Work The ultimate goal is to use the ARZ vs. MSA dialect classifier for training an adapted SMT system. We split the training data at the sentence level using our classifier and train dialect-specific systems on each of these splits along with a general dialect-independent system. We will be using techniques similar to (Koehn and Schroeder, 2007; Chiang et al., 2011; Sennrich, 2012; Chen et al., 2013) to adapt the general SMT system to a target domain with a predominant dialect. Or, we will be adopting an SMT system to a development or test set where we use the classifier to predict the dialect for each sentence and use a dialect-specific SMT system on each of them individually. Our approach of using just binary feature functions in connection with a sentence-level global linear model can be related to work on PoS-tagging (Collins, 2002). (Collins, 2002) trains a linear model based on Viterbi decoding and the perceptron algorithm. Th</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in Domain Adaptation for Statistical Machine Translation. In Proceedings of the Second Workshop on Statistical Machine Translation (WMT07), pages 224–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Prapass Srichaivattana</author>
<author>Virach Sornlertlamvanich</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Language Identification based on string kernels. In</title>
<date>2005</date>
<booktitle>In Proceedings of the 5th International Symposium on Communications and Information Technologies (ISCIT-2005,</booktitle>
<pages>896--899</pages>
<contexts>
<context position="5968" citStr="Kruengkrai et al., 2005" startWordPosition="874" endWordPosition="877">rised in the TextCat tool (Cavnar and Trenkle, 1994). Following (Baldwin and Lui, 2010), the authors extend the usage of n-gram features with nearest neighbour and nearest-prototype models together with appropriately chosen similarity metrics. (Zampieri and Gebre, 2012) classify two varieties of the same language: European and Brazilian Portuguese. They use word and character-based language model classification techniques similar to (Zaidan and Callison-Burch, 2014). (Huang and Lee, 2008) present simple bag-of-word techniques to classify varieties of Chinese from the Chinese Gigaword corpus. (Kruengkrai et al., 2005) extend the use of ngram features to using string kernels: they may take into account all possible sub-strings for comparison purposes. The resulting kernel-based classifier is compared against the method in (Cavnar and Trenkle, 1994). (Lui and Cook, 2013) present a dialect classification approach to identify Australian, British, and Canadian English. They present results where they draw training and test data from different sources. The successful transfer of models from one text source to another is evidence that their classifier indeed captures dialectal rather than stylistic or formal diff</context>
</contexts>
<marker>Kruengkrai, Srichaivattana, Sornlertlamvanich, Isahara, 2005</marker>
<rawString>Canasai Kruengkrai, Prapass Srichaivattana, Virach Sornlertlamvanich, and Hitoshi Isahara. 2005. Language Identification based on string kernels. In In Proceedings of the 5th International Symposium on Communications and Information Technologies (ISCIT-2005, pages 896–899.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Ossama Emam</author>
<author>Hany Hassan</author>
</authors>
<title>Language Model Based Arabic Word Segmentation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Conf. of the Association for Computational Linguistics (ACL 03),</booktitle>
<pages>399--406</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="10177" citStr="Lee et al., 2003" startWordPosition="1560" endWordPosition="1563">d L2-loss support vector classification. We set the penalty term C = 0.5. For our experiments, we use the data set provided in (Zaidan and Callison-Burch, 2011) which also has been used in the experiments in (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2014). We focus on the binary classification between MSA and ARZ. Details on the data sources can be found in Table 1. We present accuracy results in terms of 10-fold stratified cross-validation which are comparable to previously published work. 3.1 Tokenization and Dictionaries The Arabic tokenizer used in the current paper is based on (Lee et al., 2003). It is a general purpose tokenizer which has been optimized towards improving machine translation quality of SMT systems rather than dialect classification. Together with the tokenized text, a maximum-entropy based tagger provides the part-of-speech (PoS) tags for each token. In addition, we have explored a range of features that are based on the output of the AIDA software package (Elfardy and Diab, 2012; Mona Diab et al., 2009 2011). The AIDA software has been made available to the participants of the DARPA-funded Broad Operational Language Translation (BOLT) project. AIDA is a system for d</context>
</contexts>
<marker>Lee, Papineni, Roukos, Emam, Hassan, 2003</marker>
<rawString>Young-Suk Lee, Kishore Papineni, Salim Roukos, Ossama Emam, and Hany Hassan. 2003. Language Model Based Arabic Word Segmentation. In Proc. of the 41st Annual Conf. of the Association for Computational Linguistics (ACL 03), pages 399–406, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Paul Cook</author>
</authors>
<title>Classifying English Documents by National Dialect.</title>
<date>2013</date>
<booktitle>In Proc. Australasian Language Technology Workshop,</booktitle>
<pages>5--15</pages>
<contexts>
<context position="6224" citStr="Lui and Cook, 2013" startWordPosition="914" endWordPosition="917">re, 2012) classify two varieties of the same language: European and Brazilian Portuguese. They use word and character-based language model classification techniques similar to (Zaidan and Callison-Burch, 2014). (Huang and Lee, 2008) present simple bag-of-word techniques to classify varieties of Chinese from the Chinese Gigaword corpus. (Kruengkrai et al., 2005) extend the use of ngram features to using string kernels: they may take into account all possible sub-strings for comparison purposes. The resulting kernel-based classifier is compared against the method in (Cavnar and Trenkle, 1994). (Lui and Cook, 2013) present a dialect classification approach to identify Australian, British, and Canadian English. They present results where they draw training and test data from different sources. The successful transfer of models from one text source to another is evidence that their classifier indeed captures dialectal rather than stylistic or formal differences. Language identification of related languages is also addressed in the DSL (Discriminating Similar Languages) task of the present Vardial workshop at COLING 14 (Tan et al., 2014). While most of the above work focuses on document-level language clas</context>
</contexts>
<marker>Lui, Cook, 2013</marker>
<rawString>Marco Lui and Paul Cook. 2013. Classifying English Documents by National Dialect. In Proc. Australasian Language Technology Workshop, pages 5–15.</rawString>
</citation>
<citation valid="false">
<title>AIDA Automatic Identification of Arabic Dialectal Text. a Tool for Dialect Identification &amp; Classification, Named Entity Recognition, English and Modern Standard Arabic Glossing and Normalization.</title>
<marker></marker>
<rawString>Mona Mona Diab, Heba Elfardy, and Yassine Benajiba. 2009–2011. AIDA Automatic Identification of Arabic Dialectal Text. a Tool for Dialect Identification &amp; Classification, Named Entity Recognition, English and Modern Standard Arabic Glossing and Normalization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Perplexity minimization for translation model domain adaptation in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proc. of EACL’12,</booktitle>
<pages>539--549</pages>
<contexts>
<context position="26440" citStr="Sennrich, 2012" startWordPosition="4296" endWordPosition="4297">. In general we note that the phrase tables build on the classified data become more domainspecific, and it is left to future work to check whether improvements could carry over to the translation quality. 5 Discussion and Future Work The ultimate goal is to use the ARZ vs. MSA dialect classifier for training an adapted SMT system. We split the training data at the sentence level using our classifier and train dialect-specific systems on each of these splits along with a general dialect-independent system. We will be using techniques similar to (Koehn and Schroeder, 2007; Chiang et al., 2011; Sennrich, 2012; Chen et al., 2013) to adapt the general SMT system to a target domain with a predominant dialect. Or, we will be adopting an SMT system to a development or test set where we use the classifier to predict the dialect for each sentence and use a dialect-specific SMT system on each of them individually. Our approach of using just binary feature functions in connection with a sentence-level global linear model can be related to work on PoS-tagging (Collins, 2002). (Collins, 2002) trains a linear model based on Viterbi decoding and the perceptron algorithm. The gold-standard PoS tags are given at</context>
</contexts>
<marker>Sennrich, 2012</marker>
<rawString>Rico Sennrich. 2012. Perplexity minimization for translation model domain adaptation in statistical machine translation. In Proc. of EACL’12, pages 539–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liling Tan</author>
<author>Marcos Zampieri</author>
<author>Nicola Ljubeˇsi´c</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Merging Comparable Data Sources for the Discrimination of Similar Languages: The DSL Corpus Collection.</title>
<date>2014</date>
<booktitle>In 7th Workshop on Building and Using Comparable Corpora at LREC’14,</booktitle>
<location>Reykjavik, Iceland,</location>
<marker>Tan, Zampieri, Ljubeˇsi´c, Tiedemann, 2014</marker>
<rawString>Liling Tan, Marcos Zampieri, Nicola Ljubeˇsi´c, and J¨org Tiedemann. 2014. Merging Comparable Data Sources for the Discrimination of Similar Languages: The DSL Corpus Collection. In 7th Workshop on Building and Using Comparable Corpora at LREC’14, Reykjavik, Iceland, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Trieschnigg</author>
<author>D Hiemstra</author>
<author>M Theune F Jong</author>
<author>T Meder</author>
</authors>
<title>An Exploration of Language Identication Techniques for the Dutch Folktale Database.</title>
<date>2012</date>
<booktitle>In Adaptation of Language Resources and Tools for Processing Cultural Heritage Workshop (LREC 2012),</booktitle>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="5126" citStr="Trieschnigg et al., 2012" startWordPosition="759" endWordPosition="762">curacy prediction results have been reported even for languages with a common character set. (Baldwin and Lui, 2010) present a range of document-level language identification techniques on three different data sets. They use n-gram counting techniques and different tokenization schemes that are adopted to those data sets. Their classification task deals with several languages, and it becomes more difficult as the number of languages increases. They present an SVM-based multiclass classification approach similar to the one presented in this paper which performs well on one of their data sets. (Trieschnigg et al., 2012) generates n-gram features based on character or word sequences to classify dialectal documents in a dutch-language fairy-tale collection. Their baseline model uses N-gram based text classification techniques as popularised in the TextCat tool (Cavnar and Trenkle, 1994). Following (Baldwin and Lui, 2010), the authors extend the usage of n-gram features with nearest neighbour and nearest-prototype models together with appropriately chosen similarity metrics. (Zampieri and Gebre, 2012) classify two varieties of the same language: European and Brazilian Portuguese. They use word and character-bas</context>
</contexts>
<marker>Trieschnigg, Hiemstra, Jong, Meder, 2012</marker>
<rawString>D. Trieschnigg, D. Hiemstra, M. Theune F. Jong, and T. Meder. 2012. An Exploration of Language Identication Techniques for the Dutch Folktale Database. In Adaptation of Language Resources and Tools for Processing Cultural Heritage Workshop (LREC 2012), Istanbul, Turkey, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Crowdsourcing translation: Professional quality from nonprofessionals.</title>
<date>2011</date>
<journal>CL,</journal>
<booktitle>In Proc. ofACL/HLT 11,</booktitle>
<volume>40</volume>
<issue>1</issue>
<pages>1220--1229</pages>
<publisher>Omar</publisher>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="750" citStr="Zaidan and Callison-Burch, 2011" startWordPosition="97" endWordPosition="100">atson Research Center Aachen University Yorktown Heights, NY, USA Aachen, Germany Ictill,onaizanj@us.ibm.com mansour@cs.rwth-aachen.de Abstract The paper presents work on improved sentence-level dialect classification of Egyptian Arabic (ARZ) vs. Modern Standard Arabic (MSA). Our approach is based on binary feature functions that can be implemented with a minimal amount of task-specific knowledge. We train a featurerich linear classifier based on a linear support-vector machine (linear SVM) approach. Our best system achieves an accuracy of 89.1 % on the Arabic Online Commentary (AOC) dataset (Zaidan and Callison-Burch, 2011) using 10-fold stratified cross validation: a 1.3 % absolute accuracy improvement over the results published by (Zaidan and Callison-Burch, 2014). We also evaluate the classifier on dialect data from an additional data source. Here, we find that features which measure the informalness of a sentence actually decrease classification accuracy significantly. 1 Introduction The standard form of written Arabic is Modern Standard Arabic (MSA) . It differs significantly from various spoken varieties of Arabic (Zaidan and Callison-Burch, 2011; Zaidan and Callison-Burch, 2014; Elfardy and Diab, 2013). E</context>
<context position="3065" citStr="Zaidan and Callison-Burch, 2011" startWordPosition="448" endWordPosition="451">ystem (Zhao and Al-Onaizan, 2008) to that data. Similar to (Elfardy and Diab, 2013), we present a sentence-level classifier that is trained in a supervised manner. Our approach is based on an Arabic tokenizer, but we do not use a range of specialized tokenizers or orthography normalizers. In contrast to the language-model (LM) based classifier used by (Zaidan and Callison-Burch, 2014), we present a linear classifier approach that works best without the use of LMbased features. Some improvements in terms of classification accuracy and 10-fold cross validation under the same data conditions as (Zaidan and Callison-Burch, 2011; Elfardy and Diab, 2013) are presented. In general, we aim at a smaller amount of domain specific feature engineering than previous related approaches. The paper is structured as follows. In Section 2, we present related work on language and dialect identification. In Section 3, we discuss the linear classification model used in this paper. In Section 4, we evaluate the classifier performance in terms of classification accuracy on two data sets and present some ∗Part of the work was done while the author was a student intern at the IBM T.J. Watson Research Center. 1We use the ISO 639-3 code A</context>
<context position="8007" citStr="Zaidan and Callison-Burch, 2011" startWordPosition="1172" endWordPosition="1175">acters. The authors find that unigram word-based models perform best. The word-based models are obtained after a minimal amount of preprocessing such as proper handling of HTML entities and Arabic numbers. Classification accuracy is significantly reduced for shorter sentences. (Elfardy and Diab, 2013) presents classifcation result based on various tokinization and orthographic normalization techniques as well as so-called meta features that estimate the informalness of the data. Like our work, the authors focus on a binary dialect classification based on the ARZ-MSA portion of the dataset in (Zaidan and Callison-Burch, 2011). 3 Classification Model We use a linear model and compute a score s(tn1) for a tokenized input sentence consisting of n tokens ti: d n s(tn1) = ws · φs(ci,ti) (1) s=1 i=1 where φs(ci, ti) is a binary feature function which takes into account the context ci of token ti. w ∈ Rd is a high-dimensional weight vector obtained during training. In our experiments, we classify a tokenized 111 Description MSA ARZ # sentences # words # sentences # words ARZ-MSA portion of AOC 13,512 334K 12,527 327K DEV12 tune set 585 8.4K 634 9.3K Table 1: We used the following dialect data: 1) the ARZ-MSA portion of t</context>
<context position="9720" citStr="Zaidan and Callison-Burch, 2011" startWordPosition="1486" endWordPosition="1489">data is given as instance-label pairs (xi, yi) where i E 11, · · · , l} and l is the number of training sentences. The xi are d-dimensional vectors of integer-valued features that count how often a binary feature fired for a tokenized sentence tn1. yi E 1+1, −11 are the class labels where a label of ‘+1’ represents Egyptian dialect. During training, we solve the following optimization problem: min 11w111 + C l max(0, 1 − yi w xi ) , (2) w i=1 i.e. we use L1 regularized L2-loss support vector classification. We set the penalty term C = 0.5. For our experiments, we use the data set provided in (Zaidan and Callison-Burch, 2011) which also has been used in the experiments in (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2014). We focus on the binary classification between MSA and ARZ. Details on the data sources can be found in Table 1. We present accuracy results in terms of 10-fold stratified cross-validation which are comparable to previously published work. 3.1 Tokenization and Dictionaries The Arabic tokenizer used in the current paper is based on (Lee et al., 2003). It is a general purpose tokenizer which has been optimized towards improving machine translation quality of SMT systems rather than dialect c</context>
<context position="16728" citStr="Zaidan and Callison-Burch, 2011" startWordPosition="2631" endWordPosition="2634"> ARZ F= 2 · Prec · Recall (Prec + Recall). MSA prediction F-score is defined analogously. Experimental results are presented in Table 3, where we present results for different sets of feature types and the two test sets in Table 1. In the top half of the table, results are presented in terms of 10-fold cross validation on the ARZ-MSA portion of the AOC data. In the bottom half, we present results on DEV12 tune set, where we use the entire dialect data in Table 1 for training (about 26K sentences). As our baseline we have re-implemented the language-model-perplexity based approach reported in (Zaidan and Callison-Burch, 2011). We train language models on the dialect-labeled commentary training data for each of the dialect classes c E {MSA, ARZ}. During testing, we compute the language model probability of a sentence s for each of the classes c. We assign a sentence to the class c with the highest probability (or the lowest perplexity) . For the 10-fold cross validation experiments, 10 language models are built and perplexities are computed on 10 different test sets. The resulting (averaged) accuracy is 83.3 % for cross-validation and 82.2 % on the DEV12 tune set. In comparison, (Elfardy and Diab, 2013) reports an </context>
</contexts>
<marker>Zaidan, Callison-Burch, 2011</marker>
<rawString>Omar F. Zaidan and Chris Callison-Burch. 2011. Crowdsourcing translation: Professional quality from nonprofessionals. In Proc. ofACL/HLT 11, pages 1220–1229, Portland, Oregon, USA, June. Omar F. Zaidan and Chris Callison-Burch. 2014. Arabic Dialect Classification. CL, 40(1):171–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
<author>Binyam Gebrekidan Gebre</author>
</authors>
<title>Automatic Identication of Language Varieties: The case of Portuguese.</title>
<date>2012</date>
<booktitle>In Konvens 12,</booktitle>
<pages>233--237</pages>
<location>Vienna, Austria.</location>
<contexts>
<context position="5614" citStr="Zampieri and Gebre, 2012" startWordPosition="825" endWordPosition="828">ssification approach similar to the one presented in this paper which performs well on one of their data sets. (Trieschnigg et al., 2012) generates n-gram features based on character or word sequences to classify dialectal documents in a dutch-language fairy-tale collection. Their baseline model uses N-gram based text classification techniques as popularised in the TextCat tool (Cavnar and Trenkle, 1994). Following (Baldwin and Lui, 2010), the authors extend the usage of n-gram features with nearest neighbour and nearest-prototype models together with appropriately chosen similarity metrics. (Zampieri and Gebre, 2012) classify two varieties of the same language: European and Brazilian Portuguese. They use word and character-based language model classification techniques similar to (Zaidan and Callison-Burch, 2014). (Huang and Lee, 2008) present simple bag-of-word techniques to classify varieties of Chinese from the Chinese Gigaword corpus. (Kruengkrai et al., 2005) extend the use of ngram features to using string kernels: they may take into account all possible sub-strings for comparison purposes. The resulting kernel-based classifier is compared against the method in (Cavnar and Trenkle, 1994). (Lui and C</context>
</contexts>
<marker>Zampieri, Gebre, 2012</marker>
<rawString>Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic Identication of Language Varieties: The case of Portuguese. In Konvens 12, pages 233–237, Vienna, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Yaser Al-Onaizan</author>
</authors>
<title>Generalizing Local and Non-Local word-reordering patterns for syntaxbased machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP’08,</booktitle>
<pages>572--581</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="2467" citStr="Zhao and Al-Onaizan, 2008" startWordPosition="355" endWordPosition="358">and Egyptian. The data had been obtained by a crowd-sourcing effort. In the current paper, we present results for a binary classification task only, where we predict the dialect of Egyptian Arabic ARZ vs. MSA sentences from the Al-Youm Al-Sabe’ newspaper online commentaries 1. Our ultimate goal is to use the dialect classifier for building a dialect-aware Arabic-English statistical machine translation (SMT) system. Our Arabic-English training data contains a significant amount of Egyptian dialect data only, and we would like to adapt the components of our hierarchical phrase-based SMT system (Zhao and Al-Onaizan, 2008) to that data. Similar to (Elfardy and Diab, 2013), we present a sentence-level classifier that is trained in a supervised manner. Our approach is based on an Arabic tokenizer, but we do not use a range of specialized tokenizers or orthography normalizers. In contrast to the language-model (LM) based classifier used by (Zaidan and Callison-Burch, 2014), we present a linear classifier approach that works best without the use of LMbased features. Some improvements in terms of classification accuracy and 10-fold cross validation under the same data conditions as (Zaidan and Callison-Burch, 2011; </context>
</contexts>
<marker>Zhao, Al-Onaizan, 2008</marker>
<rawString>Bing Zhao and Yaser Al-Onaizan. 2008. Generalizing Local and Non-Local word-reordering patterns for syntaxbased machine translation. In Proc. of EMNLP’08, pages 572–581, Honolulu, Hawaii, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
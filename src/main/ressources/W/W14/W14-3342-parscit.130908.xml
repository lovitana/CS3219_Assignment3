<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.934414">
LIG System for Word Level QE task at WMT14
</title>
<author confidence="0.95171">
Ngoc-Quang Luong Laurent Besacier Benjamin Lecouteux
</author>
<affiliation confidence="0.821095">
LIG, Campus de Grenoble
</affiliation>
<address confidence="0.912513">
41, Rue des Math´ematiques,
UJF - BP53, F-38041 Grenoble Cedex 9, France
</address>
<email confidence="0.999238">
{ngoc-quang.luong,laurent.besacier,benjamin.lecouteux}@imag.fr
</email>
<sectionHeader confidence="0.997392" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999942578947368">
This paper describes our Word-level QE
system for WMT 2014 shared task on
Spanish - English pair. Compared to
WMT 2013, this year’s task is different
due to the lack of SMT setting information
and additional resources. We report
how we overcome this challenge to retain
most of the important features which
performed well last year in our system.
Novel features related to the availability of
multiple systems output (new point of this
year) are also proposed and experimented
along with baseline set. The system
is optimized by several ways: tuning
the classification threshold, combining
with WMT 2013 data, and refining
using Feature Selection strategy on our
development set, before dealing with the
test set for submission.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="keywords">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.999643">
1.1 Overview of task 2 in WMT14
</subsectionHeader>
<bodyText confidence="0.999564647058824">
This year WMT calls for methods which predict
the MT output quality at run-time, on both levels:
sentence (Task 1) and word (Task 2). Towards
a SMT system-independent and widely-applied
estimation, MT outputs are collected from
multiple translation means (machine and human),
therefore all SMT specific settings (and the
associated features that could have been extracted
from it) become unavailable. This initiative puts
more challenges on participants, yet motivates
number of SMT-unconventional approaches and
inspires the endeavors aiming at an “Evaluation
For All”.
We focus our effort on Task 2 (Word-level QE),
where, unlike in WMT2013, participants are
requested to generate prediction labels for words
in three variants:
</bodyText>
<listItem confidence="0.879522090909091">
• Binary: words are judged as Good (no
translation error), or Bad (need for editing).
• Level 1: the Good class is kept intact,
whereas Bad one is further divided into
subcategories: Accuracy issue (the word does
not accurately reflect the source text) and
Fluency issue (the word does not relate to the
form or content of the target text).
• Multi-class: more detailed judgement, where
the translation errors are further decomposed
into 16 labels based on MQM1 metric.
</listItem>
<sectionHeader confidence="0.975129" genericHeader="introduction">
1.2 Related work
</sectionHeader>
<bodyText confidence="0.998680846153846">
WMT 2013 witnessed several attempts dealing
with this evaluation type in its first launch. Han
et al. (2013); Luong et al. (2013) employed the
Conditional Random Fields (CRF) (Lafferty et al.,
2001) model as their Machine Learning method
to address the problem as a sequence labeling
task. Meanwhile, Bicici (2013) extended the
global learning model by dynamic training with
adaptive weight updates in the perceptron training
algorithm. As far as prediction indicators are
concerned, Bicici (2013) proposed seven word
feature types and found among them the “common
cover links” (the links that point from the leaf
node containing this word to other leaf nodes
in the same subtree of the syntactic tree) the
most outstanding. Han et al. (2013) focused
only on various n-gram combinations of target
words. Inheriting most of previously-recognized
features, Luong et al. (2013) integrated a number
of new indicators relying on graph topology,
pseudo reference, syntactic behavior (constituent
label, distance to the semantic tree root) and
polysemy characteristic. Optimization endeavors
were also made to enhance the baseline, including
classification threshold tuning, feature selection
and boosting technique (Luong et al., 2013).
</bodyText>
<footnote confidence="0.990266">
1http://www.qt21.eu/launchpad/content/training
</footnote>
<page confidence="0.975633">
335
</page>
<bodyText confidence="0.2570695">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 335–341,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.996908">
1.3 Paper outline
</subsectionHeader>
<bodyText confidence="0.987729833333333">
The rest of our paper is structured as follows:
in the next section, we describe 2014 provided
data for Task 2, and the additional data used
to train the system. Section 3 lists the entire
feature set, involving WMT 2013 set as well as
a new feature proposed for this year. Baseline
system experiments and methods for optimizing it
are furthered discussed in Section 4 and Section
5 respectively. Section 6 selects the most
outstanding system for submission. The last
section summarizes the approach and opens new
outlook.
</bodyText>
<sectionHeader confidence="0.952614" genericHeader="method">
2 Data and Supporting Resources
</sectionHeader>
<bodyText confidence="0.929991242424242">
For English - Spanish language pair in Task 2,
the organizers released two bilingual data sets:
the training and the test ones. The training
set contains 1.957 MT outputs, in which each
token is annotated with one appropriate label.
In the binary variant, the words are classified
into “OK” (no translation error) or “BAD” (edit
operators needed) label. Meanwhile, in the level
1 variant, they belong to “OK”, “Accuracy”
or “Fluency” (two latter ones are divided from
“BAD” label of the first subtask). In the last
variant, multi-class, beside “Accuracy” and
“Fluency” we have further 15 labels based on
MQM metric: Terminology, Mistranslation,
Omission, Addition, Untranslated, Style/register,
Capitalization, Spelling, Punctuation,
Typography, Morphology (word form),
Part of speech, Agreement, Word order,
Function words, Tense/aspect/mood, Grammar
and Unintelligible. The test set consists of 382
sentences where all the labels accompanying
words are hidden. For optimizing parameters of
the classifier, we extract last 200 sentences from
the training set to form a development (dev) set.
Besides, the Spanish - English corpus provided in
WMT 2013 (total of 1087 tuples) is also exploited
to enrich our WMT 2014 system. Unfortunately,
2013 data can only help us in the binary variant,
due to the discrepancy in training labels. Some
statistics about each set can be found in Table 1.
In addition, additional (MT-independent)
resources are used for the feature extraction,
including:
</bodyText>
<listItem confidence="0.999992">
• Spanish and English Word Language Models
(LM)
• Spanish and English POS Language Models
• Spanish - English 2013 MT system
</listItem>
<bodyText confidence="0.999898172413793">
On the contrary, no specific MT setting is provided
(e.g. the code to re-run Moses system like
WMT 2013), leading to the unavailability of some
crucial resources, such as the N-best list and
alignment information. Coping with this, we
firstly thought of using the Moses “Constrained
Decoding” option as a method to tie our (already
available) decoder’s output to the given target
translations (this feature is supported by the
latest version of Moses (Koehn et al., 2007) in
2013). Our hope was that, by doing so, both
N-best list and alignment information would be
generated during decoding. But the decoder
failed to output all translations (only 1/4 was
obtained) when the number of allowed unknown
words (-max-unknowns) was set as 0. Switching
to non zero value for this option did not help
either since, even if more outputs were generated,
alignment information was biased in that case
due to additional/missing words in the obtained
MT output. Ultimately, we decided to employ
GIZA++ toolkit (Och and Ney, 2003) to obtain
at least the alignment information (and associated
features) between source text and target MT
output. However, no N-best list were extracted
nor available as in last year system. Nevertheless,
we tried to extract some features equivalent to
last year N-best features (details can be found in
Section 3.2).
</bodyText>
<sectionHeader confidence="0.99154" genericHeader="method">
3 Feature Extraction
</sectionHeader>
<bodyText confidence="0.9999435">
In this section, we briefly list out all the
features used in WMT 2013 (Luong et al.,
2013) that were kept for this year, followed
by some proposed features taking advantage of
the provided resources and multiple translation
system outputs (for a same source sentence).
</bodyText>
<subsectionHeader confidence="0.976478">
3.1 WMT13 features
</subsectionHeader>
<listItem confidence="0.995928875">
• Source word features: all the source words
that align to the target one, represented in
BIO2 format.
• Source alignment context features: the
combinations of the target word and one
word before (left source context) or after
(right source context) the source word
aligned to it.
</listItem>
<footnote confidence="0.948949">
2http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
</footnote>
<page confidence="0.993438">
336
</page>
<table confidence="0.998423">
Statistics WMT2014 WMT2013
train dev test train dev test
#segments 1757 200 382 753 50 284
#words 40975 6436 9613 18435 1306 7827
%G (OK) : %B (BAD) 67 : 33 58 : 42 - 70 : 30 77 : 23 75 : 25
</table>
<tableCaption confidence="0.8543435">
Table 1: Statistics of corpora used in LIG’s system. We use the notion name+year to indicate the dataset.
For instance, train14 stands for the training set of WMT14
</tableCaption>
<listItem confidence="0.9404819">
• Target alignment context features: the
combinations of the source word and each
word in the window ±2 (two before, two
after) of the target word.
• Backoff Behaviour: a score assigned to the
word according to how many times the target
Language Model has to back-off in order to
assign a probability to the word sequence, as
described in (Raybaud et al., 2011).
• Part-Of-Speech (POS) features (using
TreeTagger3 toolkit): The target word’s POS;
the source POS (POS of all source words
aligned to it); bigram and trigram sequences
between its POS and the POS of previous
and following words.
• Binary lexical features that indicate whether
the word is a: stop word (based on the stop
word list for target language), punctuation
symbol, proper name or numerical.
• Language Model (LM) features: the “longest
target n-gram length” and “longest source
n-gram length”(length of the longest
sequence created by the current target
(source aligned) word and its previous ones
in the target (source) LM). For example,
with the target word wi: if the sequence
wi_2wi_1wi appears in the target LM but
the sequence wi_3wi_2wi_1wi does not, the
n-gram value for wi will be 3.
• The word’s constituent label and its depth in
the tree (or the distance between it and the
tree root) obtained from the constituent tree
as an output of the Berkeley parser (Petrov
and Klein, 2007) (trained over a Spanish
treebank: AnCora4).
• Occurrence in Google Translate hypothesis:
we check whether this target word appears in
the sentence generated by Google Translate
engine for the same source.
• Polysemy Count: the number of senses of
</listItem>
<bodyText confidence="0.897349928571429">
each word given its POS can be a reliable
indicator for judging if it is the translation
of a particular source word. Here, we
investigate the polysemy characteristic in
both target word and its aligned source word.
For source word (English), the number
of senses can be counted by applying a
Perl extension named Lingua::WordNet5,
which provides functions for manipulating
the WordNet database. For target word
(Spanish), we employ BabelNet6 - a
multilingual semantic network that works
similarly to WordNet but covers more
European languages, including Spanish.
</bodyText>
<subsectionHeader confidence="0.951309">
3.2 WMT14 additional features
</subsectionHeader>
<listItem confidence="0.97497035">
• POS’s LM based features: we exploit
the Spanish and English LMs of POS
tag (provided as additional resources for
this year’s QE tasks) for calculating the
maximum length of the sequences created
by the current target token’s POS and those
of previous ones. The same score for POS
of aligned source word(s) is also computed.
Besides, the back-off score for word’s POS
tag is also taken into consideration. Actually,
these feature types are listed in Section
3.1 for target word, and we proposed the
similar ones for POS tags. In summary, three
POS LM’s new features are built, including:
“longest target n-gram length”, “longest
source n-gram length” and back-off score for
POS tag.
• Word Occurrence in multiple translations:
one novel point in this year’s shared task
is that the targets come from multiple MT
</listItem>
<footnote confidence="0.9997205">
3http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/ 5http://search.cpan.org/dist/Lingua-Wordnet/Wordnet.pm
4http://clic.ub.edu/corpus/en/ancora 6http://babelnet.org
</footnote>
<page confidence="0.995443">
337
</page>
<bodyText confidence="0.999907857142857">
outputs (from systems or from humans) for
the same source sentences. Obviously, one
would have a “natural” intuition that: the
occurrence of a word in all (or almost)
systems implies a higher likelihood of being
a correct translation. Relying on this
observation, we add a new binary-value
feature, telling whether the current token
can be found in more than N% (in our
experiments, we choose N = 50) out
of all translations generated for the same
source sentence. Here, in order to make
the judgments more accurate, we propose
several additional references besides those
provided in the corpora, coming from: (1)
Google Translate system, (2) The baseline
SMT engine provided for WMT2013 English
- Spanish QE task. These two MT outputs
are added to the already available MT outputs
of a given source sentence, before calculating
the (above described) binary feature.
</bodyText>
<sectionHeader confidence="0.9940315" genericHeader="method">
4 Baseline Experiments and
Optimization Strategies
</sectionHeader>
<subsectionHeader confidence="0.998029">
4.1 Machine Learning Method
</subsectionHeader>
<bodyText confidence="0.999960363636364">
Motivated by the idea of addressing Word
Confidence Estimation (WCE) problem as
a sequence labeling process, we employ the
Conditional Random Fields (CRFs) for our
model training, with WAPITI toolkit (Lavergne
et al., 2010). Let X = (x1, x2,... , xN) be the
random variable over data sequence to be labeled,
Y = (y1,y2,...,yN) be the output sequence
obtained after the labeling task. Basically, CRF
computes the probability of the output sequence
Y given the input sequence X by:
</bodyText>
<equation confidence="0.9993246">
K
pB(Y |X) = ZB(X)exp �θkFk(X,Y )
k=1
(1)
T
</equation>
<bodyText confidence="0.987467">
where Fk(X,Y ) = Et=1 fk(yt−1, yt, xt);
{fk} (k = 1, K) is a set of feature functions;
{θk} (k = 1, K) are the associated parameter
values; and ZB(x) is the normalization function.
In the training phase, we set the maximum
number of iterations, the stop window size,
and stop epsilon value at 200;6 and 0.00005
respectively.
</bodyText>
<table confidence="0.9997509375">
System Label Pr(%) Rc(%) F(%)
BL(bin) OK 66.67 81.92 73.51
Bad 60.69 41.92 49.58
BL(L1) OK 63.86 82.83 72.12
Accuracy 22.14 14.89 17.80
Fluency 50.40 27.98 35.98
BL(mult) OK 63.32 87.56 73.49
Fluency 14.44 10.10 11.88
Mistranslation 9.95 5.69 7.24
Terminology 3.62 3.89 3.75
Unintelligible 52.97 16.56 25.23
Agreement 5.93 11.76 7.88
Untranslated 5.65 7.76 6.53
Punctuation 56.97 25.82 35.53
BL+WMT OK 68.62 82.69 75.01
13(bin) Bad 64.38 45.73 53.47
</table>
<tableCaption confidence="0.996616">
Table 2: Average Pr, Rc and F for labels
</tableCaption>
<bodyText confidence="0.95366775">
of all-feature binary and multi-class systems,
obtained on our WMT 2014 dev set (200
sentences). In BL(multi), classes with zero value
for Pr or Rc will not be reported
</bodyText>
<subsectionHeader confidence="0.913839">
4.2 Experimental Classifiers
</subsectionHeader>
<bodyText confidence="0.99904">
We experiment with the following classifiers:
</bodyText>
<listItem confidence="0.993004090909091">
• BL(bin): all features (WMT14+WMT13)
trained on train14 only, using binary labels
(“OK” and “BAD”)
• BL(L1): all features trained on train14 only,
using level 1 labels (“OK”, “Accuracy”, and
“Fluency”)
• BL(mult): all features trained on train14
only, using 16 labels
• BL+WMT13(bin): all features trained on
train14 + {train+dev+test}13, using binary
labels.
</listItem>
<bodyText confidence="0.991771777777778">
System quality in Precision (Pr), Recall (Rc) and
F score (F) are shown in Table 2. It can be
observed that promising results are found in binary
variant where both BL(bin) and BL+WMT(bin)
are able to reach at least 50% F score in detecting
errors (BAD class), meanwhile the performances
in “OK” class go far beyond (73.51% and 75.01%
respectively). Interestingly, the combination
with WMT13 data boosts the baseline prediction
capability in both labels: BL+WMT13(bin)
outperforms BL(bin) in 1.10% ( 3.89%) for OK
(BAD) label. Nevertheless, level 1 and multi-class
systems maintain only good score for “OK” class.
In addition, BL(mult) seems suffer seriously
from its class imbalance, as well as the lack of
training data for each, resulting in the inability
of prediction for several among them (not all are
reported in Table 2 ).
</bodyText>
<page confidence="0.996335">
338
</page>
<subsectionHeader confidence="0.992462">
4.3 Decision threshold tuning for binary task
</subsectionHeader>
<bodyText confidence="0.999057222222222">
In binary systems BL(bin) and
BL+WMT13(bin), we run the classification
task multiple times, corresponding to a decision
threshold increase from 0.300 to 0.975 (step
= 0.025). The values of Precision (Pr), Recall
(Rc) and F-score (F) for OK and BAD label are
tracked along this threshold variation, allowing
us to select the optimal threshold that yields the
highest Favg = F(OK)+F(BAD) . Figure 1 shows
</bodyText>
<page confidence="0.684385">
2
</page>
<figureCaption confidence="0.817946571428571">
that BL(bin) reaches the best performance at the
threshold value of 0.95, meanwhile the one for
BL+WMT13(bin) is 0.75. The latter threshold
(0.75) has been used for the primary system
submitted.
Figure 1: Decision threshold tuning on BL(bin)
and BL+WMT2013(bin)
</figureCaption>
<subsectionHeader confidence="0.997883">
4.4 Feature Selection
</subsectionHeader>
<bodyText confidence="0.996109">
In order to improve the preliminary scores
of all-feature systems, we conduct a feature
selection which is based on the hypothesis
that some features may convey “noise” rather
than “information” and might be the obstacles
weakening the other ones. In order to prevent
this drawback, we propose a method to filter the
best features based on the “Sequential Backward
Selection” algorithm7. We start from the full set of
N features, and in each step sequentially remove
the most useless one. To do that, all subsets of
(N-1) features are considered and the subset that
leads to the best performance gives us the weakest
feature (not involved in the considered set). This
procedure is also called “leave one out” in the
literature. Obviously, the discarded feature is not
considered in the following steps. We iterate the
</bodyText>
<footnote confidence="0.955445">
7http://research.cs.tamu.edu/prism/lectures/pr/pr l11.pdf
</footnote>
<bodyText confidence="0.806557333333333">
process until there is only one remaining feature in
the set, and use the following score for comparing
systems: Favg(all) = Favg(OK)+Favg(BAD) ,
</bodyText>
<page confidence="0.31529">
2
</page>
<bodyText confidence="0.980427375">
where Favg(OK) and Favg(BAD) are the
averaged F scores for OK and BAD label,
respectively, when threshold varies from 0.300 to
0.975. This strategy enables us to sort the features
in descending order of importance, as displayed
in Table 3. Figure 2 shows the evolution of
the performance as more and more features are
removed. The feature selection is done from the
BL+WMT2013(bin) system.
We observe in Table 3 four valuable features
which appear in top 10 in both WMT13
and WMT14 systems: Source POS, Occur in
Google Translate, Left source context and Right
target context. Among our proposed features,
“Occurrence in multiple systems” is the most
outstanding one with rank 3, “longest target POS
gram length” plays an average role with rank 12,
whereas “longest source POS gram length” is
much less beneficial with the last position in the
list. Figure 2 reveals that the optimal subset of
features is the top 18 in Table 3, after discarding 6
weakest ones. This set will be used to train again
the classifiers in all subtasks and compare to the
baseline ones.
</bodyText>
<figureCaption confidence="0.957493">
Figure 2: The evolution of the performance
</figureCaption>
<bodyText confidence="0.9701945">
as more and more features are removed (from
BL+WMT2013(bin) system)
</bodyText>
<sectionHeader confidence="0.998108" genericHeader="method">
5 Submissions
</sectionHeader>
<bodyText confidence="0.9890338">
After finishing the optimization process and
comparing systems, we select two most
out-standing ones (of each subtask) for the
submission of this year’s shared task. They are
the following:
</bodyText>
<listItem confidence="0.992717">
• Binary variant: BL+WMT13(bin) and
FS(bin) (feature selection from the same
corresponding system)
• Level 1 variant: BL(L1) and FS(L1) (feature
selection from the same corresponding
system)
</listItem>
<page confidence="0.993011">
339
</page>
<table confidence="0.999658076923077">
Rank WMT2014 WMT2013
1 Target POS Source POS
2 Longest target gram length Occur in Google Translate
3 Occurrence in multiple systems Nodes
4 Target word Target POS
5 Occur in Google Translate WPP any
6 Source POS Left source context
7 Numeric Right target context
8 Polysemy count (target) Numeric
9 Left source context Polysemy count(target)
10 Right Target context Punctuation
11 Constituent label Stop word
12 Longest target POS gram length Right source context
13 Punctuation Target word
14 Stop word Distance to root
15 Number of occurrences Backoff behaviour
16 Left target context Constituent label
17 Backoff behaviour Proper name
18 Polysemy count (source) Number of occurrences
19 Source Word Min
20 Proper Name Max
21 Distance to root Left target context
22 Longest source gram length Polysemy count (source)
23 Right source context Longest target gram length
24 Longest source POS gram length Longest source gram length
25 Source Word
</table>
<tableCaption confidence="0.760662666666667">
Table 3: The rank of each feature (in term of usefulness) in WMT2014 and WMT2013 systems. The
bold ones perform well in both cases. Note that feature sets are not exactly the same for 2013 and 2014
(see explanations in section 3).
</tableCaption>
<listItem confidence="0.9816075">
• Multi-class variant: BL(mult) and
FS(mult) (feature selection from the
</listItem>
<bodyText confidence="0.969704333333333">
same corresponding system)
The official results can be seen in Table 4. This
year, in order to appreciate the translation error
detection capability of WCE systems, the official
evaluation metric used for systems ranking is the
average F score for all but the “OK” class. For
the non-binary variant, this average is weighted
by the frequency of the class in the test data.
Nevertheless, we find the F scores for “OK” class
are also informative, since they reflect how good
our systems are in identifying correct translations.
Therefore, both scores are reported in Table 4.
</bodyText>
<sectionHeader confidence="0.994652" genericHeader="conclusions">
6 Conclusion and perspectives
</sectionHeader>
<bodyText confidence="0.9996186">
We presented our preparation for this year’s shared
task on QE at word level, for the English - Spanish
language pair. The lack of some information
on MT system internals was a challenge. We
made efforts to maintain most of well-performing
</bodyText>
<table confidence="0.992233285714286">
System F(“OK”) (%) Average F(%)
FS(bin) (primary) 74.0961 0.444735
FS(L1) 73.9856 0.317814
FS(mult) 76.6645 0.204953
BL+WMT2013(bin) 74.6503 0.441074
BL(L1) 74.0045 0.317894
BL(mult) 76.6645 0.204953
</table>
<tableCaption confidence="0.945979">
Table 4: The F scores for “OK” class and the
</tableCaption>
<bodyText confidence="0.99272875">
average F scores for the remaining classes (official
WMT14 metric) , obtained on test set.
2013 features, especially the source side ones,
and propose some novel features based on this
year’s corpus specificities, as well as combine
them with those of last year. Generally, our
results are not able to beat those in WMT13 for
the same language pair, yet still promising under
these constraints. As future work, we are thinking
of using more efficiently the existing references
(coming from provided translations and other
reliable systems) to obtain stronger indicators, as
</bodyText>
<page confidence="0.988729">
340
</page>
<bodyText confidence="0.974236">
well as examine other ML methods besides CRF.
</bodyText>
<sectionHeader confidence="0.997263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998689383333333">
Ergun Bicici. Referential translation machines
for quality estimation. In Proceedings of
the Eighth Workshop on Statistical Machine
Translation, pages 343–351, Sofia, Bulgaria,
August 2013. Association for Computational
Linguistics. URL http://www.aclweb.
org/anthology/W13-2242.
Aaron Li-Feng Han, Yi Lu, Derek F. Wong,
Lidia S. Chao, Liangye He, and Junwen Xing.
Quality estimation for machine translation
using the joint method of evaluation criteria
and statistical modeling. In Proceedings of
the Eighth Workshop on Statistical Machine
Translation, pages 365–372, Sofia, Bulgaria,
August 2013. Association for Computational
Linguistics. URL http://www.aclweb.
org/anthology/W13-2245.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen,
Christine Moran, Richard Zens, Chris Dyer,
Ondrej Bojar, Alexandra Constantin, and
Evan Herbst. Moses: Open source toolkit for
statistical machine translation. In Proceedings
of the 45th Annual Meeting of the Association
for Computational Linguistics, pages 177–180,
Prague, Czech Republic, June 2007.
John Lafferty, Andrew McCallum, and
Fernando Pereira. Conditional random
fields: Probabilistic models for segmenting
et labeling sequence data. In Proceedings of
ICML-01, pages 282–289, 2001.
Thomas Lavergne, Olivier Capp´e, and Franc¸ois
Yvon. Practical very large scale crfs. In
Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 504–513, 2010.
Ngoc Quang Luong, Laurent Besacier, and
Benjamin Lecouteux. Word confidence
estimation and its integration in sentence
quality estimation for machine translation.
In Proceedings of the fifth international
conference on knowledge and systems
engineering (KSE), Hanoi, Vietnam, October
2013.
Franz Josef Och and Hermann Ney. A systematic
comparison of various statistical alignment
models. Computational Linguistics, 29(1):
19–51, 2003.
Slav Petrov and Dan Klein. Improved inference
for unlexicalized parsing. In Proceedings of
NAACL HLT 2007, pages 404–411, Rochester,
NY, April 2007.
S. Raybaud, D. Langlois, and K. Smaili. ”this
sentence is wrong.” detecting errors in machine
- translated sentences. In Machine Translation,
pages 1–34, 2011.
Matthew Snover, Nitin Madnani, Bonnie Dorr,
and Richard Schwartz. Terp system description.
In MetricsMATR workshop at AMTA, 2008.
</reference>
<page confidence="0.998697">
341
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.167714">
<note confidence="0.3209068">LIG System for Word Level QE task at WMT14 Ngoc-Quang Luong Laurent Besacier Benjamin Lecouteux LIG, Campus de 41, Rue des UJF - BP53, F-38041 Grenoble Cedex 9,</note>
<abstract confidence="0.9995324">This paper describes our Word-level QE system for WMT 2014 shared task on Spanish - English pair. Compared to WMT 2013, this year’s task is different due to the lack of SMT setting information and additional resources. We how we overcome this challenge to retain most of the important features which performed well last year in our system. Novel features related to the availability of multiple systems output (new point of this year) are also proposed and experimented along with baseline set. The system is optimized by several ways: tuning the classification threshold, combining with WMT 2013 data, and refining Selection on our development set, before dealing with the test set for submission.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ergun Bicici</author>
</authors>
<title>Referential translation machines for quality estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>343--351</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2572" citStr="Bicici (2013)" startWordPosition="397" endWordPosition="398">acy issue (the word does not accurately reflect the source text) and Fluency issue (the word does not relate to the form or content of the target text). • Multi-class: more detailed judgement, where the translation errors are further decomposed into 16 labels based on MQM1 metric. 1.2 Related work WMT 2013 witnessed several attempts dealing with this evaluation type in its first launch. Han et al. (2013); Luong et al. (2013) employed the Conditional Random Fields (CRF) (Lafferty et al., 2001) model as their Machine Learning method to address the problem as a sequence labeling task. Meanwhile, Bicici (2013) extended the global learning model by dynamic training with adaptive weight updates in the perceptron training algorithm. As far as prediction indicators are concerned, Bicici (2013) proposed seven word feature types and found among them the “common cover links” (the links that point from the leaf node containing this word to other leaf nodes in the same subtree of the syntactic tree) the most outstanding. Han et al. (2013) focused only on various n-gram combinations of target words. Inheriting most of previously-recognized features, Luong et al. (2013) integrated a number of new indicators r</context>
</contexts>
<marker>Bicici, 2013</marker>
<rawString>Ergun Bicici. Referential translation machines for quality estimation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 343–351, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL http://www.aclweb. org/anthology/W13-2242.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Aaron Li-Feng Han</author>
<author>Yi Lu</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
<author>Liangye He</author>
<author>Junwen Xing</author>
</authors>
<title>Quality estimation for machine translation using the joint method of evaluation criteria and statistical modeling.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>365--372</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2366" citStr="Han et al. (2013)" startWordPosition="363" endWordPosition="366">ds in three variants: • Binary: words are judged as Good (no translation error), or Bad (need for editing). • Level 1: the Good class is kept intact, whereas Bad one is further divided into subcategories: Accuracy issue (the word does not accurately reflect the source text) and Fluency issue (the word does not relate to the form or content of the target text). • Multi-class: more detailed judgement, where the translation errors are further decomposed into 16 labels based on MQM1 metric. 1.2 Related work WMT 2013 witnessed several attempts dealing with this evaluation type in its first launch. Han et al. (2013); Luong et al. (2013) employed the Conditional Random Fields (CRF) (Lafferty et al., 2001) model as their Machine Learning method to address the problem as a sequence labeling task. Meanwhile, Bicici (2013) extended the global learning model by dynamic training with adaptive weight updates in the perceptron training algorithm. As far as prediction indicators are concerned, Bicici (2013) proposed seven word feature types and found among them the “common cover links” (the links that point from the leaf node containing this word to other leaf nodes in the same subtree of the syntactic tree) the m</context>
</contexts>
<marker>Han, Lu, Wong, Chao, He, Xing, 2013</marker>
<rawString>Aaron Li-Feng Han, Yi Lu, Derek F. Wong, Lidia S. Chao, Liangye He, and Junwen Xing. Quality estimation for machine translation using the joint method of evaluation criteria and statistical modeling. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 365–372, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL http://www.aclweb. org/anthology/W13-2245.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6373" citStr="Koehn et al., 2007" startWordPosition="974" endWordPosition="977">extraction, including: • Spanish and English Word Language Models (LM) • Spanish and English POS Language Models • Spanish - English 2013 MT system On the contrary, no specific MT setting is provided (e.g. the code to re-run Moses system like WMT 2013), leading to the unavailability of some crucial resources, such as the N-best list and alignment information. Coping with this, we firstly thought of using the Moses “Constrained Decoding” option as a method to tie our (already available) decoder’s output to the given target translations (this feature is supported by the latest version of Moses (Koehn et al., 2007) in 2013). Our hope was that, by doing so, both N-best list and alignment information would be generated during decoding. But the decoder failed to output all translations (only 1/4 was obtained) when the number of allowed unknown words (-max-unknowns) was set as 0. Switching to non zero value for this option did not help either since, even if more outputs were generated, alignment information was biased in that case due to additional/missing words in the obtained MT output. Ultimately, we decided to employ GIZA++ toolkit (Och and Ney, 2003) to obtain at least the alignment information (and as</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 177–180, Prague, Czech Republic, June 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting et labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML-01,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="2456" citStr="Lafferty et al., 2001" startWordPosition="377" endWordPosition="380">ad (need for editing). • Level 1: the Good class is kept intact, whereas Bad one is further divided into subcategories: Accuracy issue (the word does not accurately reflect the source text) and Fluency issue (the word does not relate to the form or content of the target text). • Multi-class: more detailed judgement, where the translation errors are further decomposed into 16 labels based on MQM1 metric. 1.2 Related work WMT 2013 witnessed several attempts dealing with this evaluation type in its first launch. Han et al. (2013); Luong et al. (2013) employed the Conditional Random Fields (CRF) (Lafferty et al., 2001) model as their Machine Learning method to address the problem as a sequence labeling task. Meanwhile, Bicici (2013) extended the global learning model by dynamic training with adaptive weight updates in the perceptron training algorithm. As far as prediction indicators are concerned, Bicici (2013) proposed seven word feature types and found among them the “common cover links” (the links that point from the leaf node containing this word to other leaf nodes in the same subtree of the syntactic tree) the most outstanding. Han et al. (2013) focused only on various n-gram combinations of target w</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. Conditional random fields: Probabilistic models for segmenting et labeling sequence data. In Proceedings of ICML-01, pages 282–289, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lavergne</author>
<author>Olivier Capp´e</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Practical very large scale crfs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>504--513</pages>
<marker>Lavergne, Capp´e, Yvon, 2010</marker>
<rawString>Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon. Practical very large scale crfs. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504–513, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ngoc Quang Luong</author>
<author>Laurent Besacier</author>
<author>Benjamin Lecouteux</author>
</authors>
<title>Word confidence estimation and its integration in sentence quality estimation for machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the fifth international conference on knowledge and systems engineering (KSE), Hanoi,</booktitle>
<location>Vietnam,</location>
<contexts>
<context position="2387" citStr="Luong et al. (2013)" startWordPosition="367" endWordPosition="370">s: • Binary: words are judged as Good (no translation error), or Bad (need for editing). • Level 1: the Good class is kept intact, whereas Bad one is further divided into subcategories: Accuracy issue (the word does not accurately reflect the source text) and Fluency issue (the word does not relate to the form or content of the target text). • Multi-class: more detailed judgement, where the translation errors are further decomposed into 16 labels based on MQM1 metric. 1.2 Related work WMT 2013 witnessed several attempts dealing with this evaluation type in its first launch. Han et al. (2013); Luong et al. (2013) employed the Conditional Random Fields (CRF) (Lafferty et al., 2001) model as their Machine Learning method to address the problem as a sequence labeling task. Meanwhile, Bicici (2013) extended the global learning model by dynamic training with adaptive weight updates in the perceptron training algorithm. As far as prediction indicators are concerned, Bicici (2013) proposed seven word feature types and found among them the “common cover links” (the links that point from the leaf node containing this word to other leaf nodes in the same subtree of the syntactic tree) the most outstanding. Han </context>
<context position="7350" citStr="Luong et al., 2013" startWordPosition="1135" endWordPosition="1138"> were generated, alignment information was biased in that case due to additional/missing words in the obtained MT output. Ultimately, we decided to employ GIZA++ toolkit (Och and Ney, 2003) to obtain at least the alignment information (and associated features) between source text and target MT output. However, no N-best list were extracted nor available as in last year system. Nevertheless, we tried to extract some features equivalent to last year N-best features (details can be found in Section 3.2). 3 Feature Extraction In this section, we briefly list out all the features used in WMT 2013 (Luong et al., 2013) that were kept for this year, followed by some proposed features taking advantage of the provided resources and multiple translation system outputs (for a same source sentence). 3.1 WMT13 features • Source word features: all the source words that align to the target one, represented in BIO2 format. • Source alignment context features: the combinations of the target word and one word before (left source context) or after (right source context) the source word aligned to it. 2http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/ 336 Statistics WMT2014 WMT2013 train dev test train dev test #segment</context>
</contexts>
<marker>Luong, Besacier, Lecouteux, 2013</marker>
<rawString>Ngoc Quang Luong, Laurent Besacier, and Benjamin Lecouteux. Word confidence estimation and its integration in sentence quality estimation for machine translation. In Proceedings of the fifth international conference on knowledge and systems engineering (KSE), Hanoi, Vietnam, October 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="6920" citStr="Och and Ney, 2003" startWordPosition="1064" endWordPosition="1067">eature is supported by the latest version of Moses (Koehn et al., 2007) in 2013). Our hope was that, by doing so, both N-best list and alignment information would be generated during decoding. But the decoder failed to output all translations (only 1/4 was obtained) when the number of allowed unknown words (-max-unknowns) was set as 0. Switching to non zero value for this option did not help either since, even if more outputs were generated, alignment information was biased in that case due to additional/missing words in the obtained MT output. Ultimately, we decided to employ GIZA++ toolkit (Och and Ney, 2003) to obtain at least the alignment information (and associated features) between source text and target MT output. However, no N-best list were extracted nor available as in last year system. Nevertheless, we tried to extract some features equivalent to last year N-best features (details can be found in Section 3.2). 3 Feature Extraction In this section, we briefly list out all the features used in WMT 2013 (Luong et al., 2013) that were kept for this year, followed by some proposed features taking advantage of the provided resources and multiple translation system outputs (for a same source se</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1): 19–51, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT</booktitle>
<pages>404--411</pages>
<location>Rochester, NY,</location>
<contexts>
<context position="9605" citStr="Petrov and Klein, 2007" startWordPosition="1517" endWordPosition="1520">er name or numerical. • Language Model (LM) features: the “longest target n-gram length” and “longest source n-gram length”(length of the longest sequence created by the current target (source aligned) word and its previous ones in the target (source) LM). For example, with the target word wi: if the sequence wi_2wi_1wi appears in the target LM but the sequence wi_3wi_2wi_1wi does not, the n-gram value for wi will be 3. • The word’s constituent label and its depth in the tree (or the distance between it and the tree root) obtained from the constituent tree as an output of the Berkeley parser (Petrov and Klein, 2007) (trained over a Spanish treebank: AnCora4). • Occurrence in Google Translate hypothesis: we check whether this target word appears in the sentence generated by Google Translate engine for the same source. • Polysemy Count: the number of senses of each word given its POS can be a reliable indicator for judging if it is the translation of a particular source word. Here, we investigate the polysemy characteristic in both target word and its aligned source word. For source word (English), the number of senses can be counted by applying a Perl extension named Lingua::WordNet5, which provides funct</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. Improved inference for unlexicalized parsing. In Proceedings of NAACL HLT 2007, pages 404–411, Rochester, NY, April 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Raybaud</author>
<author>D Langlois</author>
<author>K Smaili</author>
</authors>
<title>this sentence is wrong.” detecting errors in machine - translated sentences.</title>
<date>2011</date>
<booktitle>In Machine Translation,</booktitle>
<pages>1--34</pages>
<contexts>
<context position="8602" citStr="Raybaud et al., 2011" startWordPosition="1350" endWordPosition="1353">s 40975 6436 9613 18435 1306 7827 %G (OK) : %B (BAD) 67 : 33 58 : 42 - 70 : 30 77 : 23 75 : 25 Table 1: Statistics of corpora used in LIG’s system. We use the notion name+year to indicate the dataset. For instance, train14 stands for the training set of WMT14 • Target alignment context features: the combinations of the source word and each word in the window ±2 (two before, two after) of the target word. • Backoff Behaviour: a score assigned to the word according to how many times the target Language Model has to back-off in order to assign a probability to the word sequence, as described in (Raybaud et al., 2011). • Part-Of-Speech (POS) features (using TreeTagger3 toolkit): The target word’s POS; the source POS (POS of all source words aligned to it); bigram and trigram sequences between its POS and the POS of previous and following words. • Binary lexical features that indicate whether the word is a: stop word (based on the stop word list for target language), punctuation symbol, proper name or numerical. • Language Model (LM) features: the “longest target n-gram length” and “longest source n-gram length”(length of the longest sequence created by the current target (source aligned) word and its previ</context>
</contexts>
<marker>Raybaud, Langlois, Smaili, 2011</marker>
<rawString>S. Raybaud, D. Langlois, and K. Smaili. ”this sentence is wrong.” detecting errors in machine - translated sentences. In Machine Translation, pages 1–34, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Terp system description.</title>
<date>2008</date>
<booktitle>In MetricsMATR workshop at AMTA,</booktitle>
<marker>Snover, Madnani, Dorr, Schwartz, 2008</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. Terp system description. In MetricsMATR workshop at AMTA, 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001146">
<title confidence="0.979201">
Sentence-level Rewriting Detection
</title>
<author confidence="0.999371">
Fan Zhang Diane Litman
</author>
<affiliation confidence="0.999937">
University of Pittsburgh University of Pittsburgh
</affiliation>
<address confidence="0.728035">
Pittsburgh, PA, 15260 Pittsburgh, PA, 15260
</address>
<email confidence="0.998745">
zhangfan@cs.pitt.edu litman@cs.pitt.edu
</email>
<sectionHeader confidence="0.993884" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949823529412">
Writers usually need iterations of revisions
and edits during their writings. To bet-
ter understand the process of rewriting,
we need to know what has changed be-
tween the revisions. Prior work mainly fo-
cuses on detecting corrections within sen-
tences, which is at the level of words
or phrases. This paper proposes to de-
tect revision changes at the sentence level.
Looking at revisions at a higher level al-
lows us to have a different understanding
of the revision process. This paper also
proposes an approach to automatically de-
tect sentence revision changes. The pro-
posed approach shows high accuracy in an
evaluation using first and final draft essays
from an undergraduate writing class.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966911111111">
Rewriting is considered to be an important process
during writing. However, conducting successful
rewriting is not an easy task, especially for novice
writers. Instructors work hard on providing sug-
gestions for rewriting (Wells et al., 2013), but usu-
ally such advice is quite general. We need to un-
derstand the changes between revisions better to
provide more specific and helpful advice.
There has already been work on detecting cor-
rections in sentence revisions (Xue and Hwa,
2014; Swanson and Yamangil, 2012; Heilman
and Smith, 2010; Rozovskaya and Roth, 2010).
However, these works mainly focus on detecting
changes at the level of words or phrases. Ac-
cording to Faigley’s definition of revision change
(Faigley and Witte, 1981), these works could help
the identification of Surface Changes (changes
that do not add or remove information to the orig-
inal text). However, Text Changes (changes that
add or remove information) will be more difficult
to identify if we only look at revisions within sen-
tences. According to Hashemi and Schunn (2014),
when instructors were presented a comparison of
differences between papers derived from words,
they felt the information regarding changes be-
tween revisions was overwhelming.
This paper proposes to look at the changes be-
tween revisions at the level of sentences. Com-
paring to detecting changes at the word level, de-
tecting changes at the sentence level contains less
information, but still keeps enough information
to understand the authors’ intention behind their
modifications to the text. The sentence level edits
could then be grouped and classified into differ-
ent types of changes. The long-term goal of this
project is to allow us to be able to identify both
Text Changes and Surface Changes automatically.
Students, teachers, and researchers could then per-
form analysis on the different types of changes and
have a better understanding of the rewriting pro-
cess. As a preliminary work, this paper explores
steps toward this goal: First, automatically gener-
ate the description of changes based on four prim-
itives: Add, Delete, Modify, Keep; Second, merge
the primitives that come from the same purpose.
</bodyText>
<sectionHeader confidence="0.999621" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999896642857143">
Hashemi and Schunn (2014) presented a tool
to help professors summarize students’ changes
across papers before and after peer review. They
first split the original documents into sentences
and then built on the output of Compare Suite
(CompareSuite, 2014) to count and highlight
changes in different colors. Figure 1 shows a
screenshot of their work. As we can see, the mod-
ifications to the text are misinterpreted. Line 66
in the final draft should correspond to line 55 and
line 56 in the first draft, while line 67 and line 68
should be a split of line 57 in the first draft. How-
ever, line 67 is aligned to line 56 wrongly in their
work. This wrong alignment caused many mis-
</bodyText>
<page confidence="0.986231">
149
</page>
<bodyText confidence="0.968616780487805">
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications, pages 149–154,
Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics
recognized modifications. According to Hashemi,
the instructors who use the system think that the
overwhelming information of changes make the
system less useful. We hypothesize that since their
work is based on analysis at the word level, al-
though their approach might work for identifying
differences within one sentence, it makes mistakes
when sentence analysis is the primary concern.
Our work avoids the above problem by detect-
ing differences at the sentence level. Sentence
alignment is the first step of our method; fur-
ther inferences about revision changes are then
based on the alignments generated. We borrow
ideas from the research on sentence alignment for
monolingual corpora. Existing research usually
focuses on the alignment from the text to its sum-
marization or its simplification (Jing, 2002; Barzi-
lay and Elhadad, 2003; Bott and Saggion, 2011).
Barzilay and Elhadad (2003) treat sentence align-
ment as a classification task. The paragraphs are
clustered into groups, and a binary classifier is
trained to decide whether two sentences should be
aligned or not. Nelken (2006) further improves
the performance by using TF*IDF score instead of
word overlap and also utilizing global optimiza-
tion to take sentence order information into con-
sideration. We argue that summarization could
be considered as a special form of revision and
adapted Nelken’s approach to our approach.
Edit sequences are then inferred based on the
results of sentence alignment. Fragments of ed-
its that come from the same purpose will then be
merged. Related work to our method is sentence
clustering (Shen et al., 2011; Wang et al., 2009).
While sentence clustering is trying to find and
cluster sentences similar to each other, our work
is to find a cluster of sentences in one document
that is similar to one sentence in the other docu-
ment after merging.
</bodyText>
<sectionHeader confidence="0.958148" genericHeader="method">
3 Sentence-level changes across revisions
</sectionHeader>
<subsectionHeader confidence="0.955791">
3.1 Primitives for sentence-level changes
</subsectionHeader>
<bodyText confidence="0.991801111111111">
Previous work in educational revision analysis
(Faigley and Witte, 1981; Connor and Asenav-
age, 1994) categorized revision changes to be ei-
ther surface changes or text-based changes. With
both categories, six kinds of changes were defined
as shown in Table 1.
Different from Faigley’s definition, we define
only 4 primitives for our first step of edit sequence
generation: Add, Delete, Modify and Keep. This
</bodyText>
<table confidence="0.996814428571429">
Code Explanation
Addition Adding a word or phrase
Deletion Omitting a word or phrase
Substitutions exchange words with synonyms
Permutation rearrange of words or phrases
Distribution one segment divided into two
Consolidation combine two segments into one
</table>
<tableCaption confidence="0.999753">
Table 1: Code Definition by L.Faigley and S.Witte
</tableCaption>
<bodyText confidence="0.9998034">
definition is similar to Bronner’s work (Bronner
and Monz, 2012). We choose this definition be-
cause these 4 primitives only correspond to one
sentence at a time. Add, Delete, Modify indicates
that the writer has added/deleted/modified a sen-
tence. Keep means the original sentence is not
modified. We believe Permutation, Distribution
and Consolidation as defined by Faigley could be
described with these four primitives, which could
be recognized in the later merge step.
</bodyText>
<subsectionHeader confidence="0.999841">
3.2 Data and annotation
</subsectionHeader>
<bodyText confidence="0.999983517241379">
The corpus we choose consists of paired first and
final drafts of short papers written by undergradu-
ates in a course “Social Implications of Comput-
ing Technology”. Students are required to write
papers on one topic and then revise their own pa-
pers. The revisions are guided by other students’
feedback based on a grading rubric, using a web-
based peer review system. Students first submitted
their original paper into the system, and then were
randomly assigned to review and comment others’
work according to the writing rubric. The authors
would receive the others’ anonymous comments,
and then could choose to revise their work based
on others’ comments as well as their own insights
obtained by reviewing other papers.
The papers in the corpus contain two topics.
In the first topic, the students discussed the role
that Big Data played in Obama’s presidential cam-
paign. This topic contains 11 pairs of first and final
drafts of short papers. We name this C1. The other
topic, named C2, talks about intellectual property
and contains 10 pairs of paper drafts. The students
involved in these two topics are from the same
class. Students make more modifications to their
papers in C2. More details can be seen in Table 2.
Our revision change detection approach con-
tains three steps: sentence alignment, edit se-
quence generation and merge of edit sequences.
Thus we annotated for these three steps.
</bodyText>
<page confidence="0.948078">
150
</page>
<figure confidence="0.987568">
(a) first draft (b) final draft
(c) Revision detection using Hashemi’s approach
</figure>
<figureCaption confidence="0.9411805">
Figure 1: Fragments of a paper in corpus C2 discussing intellectual property, (c) is Hashemi’s work,
green for recognized modifications, blue for insertions and red for deletion
</figureCaption>
<bodyText confidence="0.9985589375">
For sentence alignment, each sentence in the fi-
nal draft is assigned the index of its aligned sen-
tence in the original draft. If a sentence is newly
added, it will be annotated as ADD. Sentence
alignment is not necessarily one-to-one. It can
also be one-to-many (Consolidation) and many-
to-one (Distribution). Table 3 shows a fragment
of the annotation for the text shown in Figure 1.
For edit sequences, the annotators do the anno-
tation based on the initial draft. For the same frag-
ment in Table 3, the annotated sequence is: Keep,
Modify, Delete, Modify, Add1.
For edit sequence merging, we further annotate
Consolidation and Distribution based on the edit
sequences. In our example, 66 consolidates 55 and
56, while 57 distributes to 67 and 68.
</bodyText>
<table confidence="0.654189">
pairs #D1 #D2 Avg1 Avg2
C1 11 761 791 22.5 22.7
C2 10 645 733 24.7 24.5
</table>
<bodyText confidence="0.9969236">
tators annotating on one pair of papers. The paper
contains 76 sentences, and the annotators only dis-
agree in one sentence. The kappa is 0.794 2, which
suggests that the annotation is reliable based on
our annotation scheme.
</bodyText>
<sectionHeader confidence="0.986115" genericHeader="method">
4 Automatic detection of revision
changes
</sectionHeader>
<bodyText confidence="0.998425125">
The detection of revision changes contains three
parts: sentence alignment, edit sequence genera-
tion and edit sequence merging. The first two parts
generate edit sequences detected at the sentence
level, while the third part groups edit sequences
and classifies them into different types of changes.
Currently the third step only covers the identifica-
tion of Consolidation and Distribution.
</bodyText>
<table confidence="0.963337">
Sentence Index (Final) 65 66 67 68
Sentence Index (First) 54 55,56 57 57
</table>
<tableCaption confidence="0.992599">
Table 3: An example of alignment annotation
Table 2: Detailed information of corpora. #D1 and
</tableCaption>
<bodyText confidence="0.945479230769231">
#D2 are the number of sentences in the first and
final draft, Avg1 and Avg2 are the average number
of words in one sentence in the first and final draft
As a preliminary work, we only have one anno-
tator doing all the annotations. But for the anno-
tation of sentence alignments, we have two anno-
166 consolidates 55, 56; while 57 distributes to 67, 68.
Notice that Consolidation is illustrated as Modify, Delete and
Distribution is illustrated as Modify, Add. As the annotators
annotate based on the first draft, Modify always appears be-
fore Add or Delete
Sentence alignment We adapted Nelken’s ap-
proach to our problem.
</bodyText>
<subsectionHeader confidence="0.471264">
Alignment based on sentence similarity
</subsectionHeader>
<bodyText confidence="0.993193">
The alignment task goes through three stages.
</bodyText>
<listItem confidence="0.776396333333333">
1. Data preparation: for each sentence in the an-
notated final draft, if it is not a new sentence, cre-
ate a sentence pair with its aligned sentence in the
</listItem>
<footnote confidence="0.99745725">
2We calculate the Kappa value following Macken’s idea
(Macken, 2010), where the aligned sentences are categorized
as direct-link, while new added sentences are categorized as
null-link (ADD).
</footnote>
<page confidence="0.9959">
151
</page>
<bodyText confidence="0.999695">
first draft. The pair is considered to be an aligned
pair. Also, randomly select another sentence from
the first draft to make a negative sentence pair.
Thus we ensure there are nearly equal numbers of
positive and negative cases in the training data.
</bodyText>
<listItem confidence="0.987975">
2. Training: according to the similarity met-
ric defined, calculate the similarity of the sentence
pairs. A logistic regression classifier predicting
whether a sentence pair is aligned or not is trained
with the similarity score as the feature. In addi-
tion to classification, the classifier is also used to
provide a similarity score for global alignment.
3. Alignment: for each pair of paper drafts, con-
struct sentence pairs using the Cartesian product
of sentences in the first draft and sentences in the
final. Logistic regression classifier is used to deter-
mine whether the sentence pair is aligned or not.
</listItem>
<bodyText confidence="0.9988678">
We added Levenshtein distance (LD) (Leven-
shtein, 1966) as another similarity metric in ad-
dition to Nelken’s metrics. Together three similar-
ity metrics were compared: Levenshtein Distance,
Word Overlap(WO), and TF*IDF.
</bodyText>
<subsectionHeader confidence="0.643804">
Global alignment
</subsectionHeader>
<bodyText confidence="0.985626837837838">
Sentences are likely to preserve the same or-
der between rewritings. Thus, sentence or-
dering should be an important feature in sen-
tence alignment. Nelken’s work modifies the
Needleman-Wunsch alignment (Needleman and
Wunsch, 1970) to find the sentence alignments and
goes in the following steps.
Step]: The logistic regression classifier previ-
ously trained assigns a probability value from 0 to
1 for each sentence pair s(i, j). Use this value as
the similarity score of sentence pair: sim(i, j).
Step2: Starting from the first pair of sen-
tences, find the best path to maximize the likeli-
hood between sentences according to the formula
s(i, j) = max{s(i − 1, j − 1) + sim(i, j), s(i −
1, j) + sim(i, j) , s(i, j − 1) + sim(i, j)}
Step3: Infer the sentence alignments by back
tracing the matrix s(i, j).
We found out that changing bolded parts in the
formula to s(i, j) = max{s(i − 1, j − 1) +
sim(i, j), s(i − 1, j) + insertcost , s(i, j − 1) +
deletecost} shows better performance in our prob-
lem. According to our experiment with C1, insert-
cost and deletecost are both set to 0.1 as they are
found to be the most effective during practice.
Edit sequence generation This step is an inter-
mediate step, which tries to generate the edit se-
quence based on the sentence alignment results
from the previous step. The edit sequences gen-
erated would later be grouped together and clas-
sified into different types. In our current work, a
rule-based method is proposed for this step.
Step]: The index of original document i and the
index of the modified document j both start from
0. If sentence i in the original document is aligned
to sentence j in the modified one, go to step 2, if
not go to step 3.
Step2: If the two sentences are exactly the same,
add Keep to the edit sequence, if not, add Modify.
Increase i and j by 1, go to step 1.
Step3: Check the predicted alignment index of
sentence j, if the predicted index is larger than sen-
tence i in the original document, add Delete and
increase i by 1, otherwise, mark as Add and in-
crease j by 1, go to step 1.
Edit sequence merging Distribution means
splitting one sentence into two or more sentences,
while Consolidation means merging two or more
sentences into one sentence. These two operations
can be derived with primitives Modify, Add and
Delete. They follow the following patterns:
Consolidation: Modify-Delete-Delete-...
Distribution: Modify-Add-Add-...
These sequences both start with Modify fol-
lowed with a repetitive number of Delete or Add.
A group of edit sequences can be merged if they
can be merged to a sentence close to the sentence
in the other draft. We applied a rule-based ap-
proach based on our observations.
We first scan through the sequence generated
above. Sequences with Modify-Add-... or Mod-
ify-Delete-... are extracted. For each sequence ex-
tracted, if there are n consecutive Add or Delete
following Modify, create n groups, Groupz(i &lt;
n) contains sentences from the modified sentence
to the next consecutive i sentences. For each
group, merge all the sentences, and use the clas-
sifier trained above to get the similarity score
Simgroupi between the merged sentence and the
original one. If there are multiple groups classi-
fied as aligned, choose group i that has the largest
Simgroupi, merge the basic edit operations into
Consolidation or Distribution. If none of the
groups are classified as aligned, do not merge.
</bodyText>
<sectionHeader confidence="0.996612" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.995605666666667">
Sentence alignment We use accuracy as the
evaluation metric. For each pair of drafts, we
count the number of sentences in the final draft
</bodyText>
<page confidence="0.996208">
152
</page>
<bodyText confidence="0.999574">
N1. For each sentence in the final draft, we count
the number of sentences that get the correct align-
ment as N2. The accuracy of the sentence align-
ment is N2N1. 3
We use Hashemi’s approach as the baseline.
Compare Suite colors the differences out, as
shown in Figure 1. We treat the green sentences
as Modify and aligned to the original sentence.
For our method, we tried four groups of set-
tings. Group 1 and group 2 perform leave-one-out
cross validation on C1 and C2 (test on one pair of
paper drafts and train on the others). Group 3 and
group 4 train on one corpus and test on the other.
</bodyText>
<table confidence="0.9996728">
Group LD WO TF*IDF Baseline
1 0.9811 0.9863 0.9931 0.9427
2 0.9649 0.9593 0.9667 0.9011
3 0.9727 0.9700 0.9727 0.9045
4 0.9860 0.9886 0.9798 0.9589
</table>
<tableCaption confidence="0.999826">
Table 4: Accuracy of our approach vs. baseline
</tableCaption>
<bodyText confidence="0.971761">
Table 4 shows that all our methods beat the
baseline 4. Among the three similarity metrics,
TF*IDF is the most predictive.
Edit sequence generation We use WER (Word
Error Rate) from speech recognition for evaluat-
ing the generated sequence by comparing the gen-
erated sequence to the gold standard.
WER is calculated based on edit distances be-
tween sequences. The ratio is calculated as:
</bodyText>
<sectionHeader confidence="0.396878" genericHeader="method">
WER = S+D+I
</sectionHeader>
<bodyText confidence="0.88711025">
N , where S means the number of
modifications, D means the number of deletes, I
means the number of inserts.
We apply our method on the gold standard of
sentence alignment. The generated edit sequence
is then compared with the gold standard edit se-
quence to calculate WER. Hashemi’s approach is
chosen as the baseline. The WER of our method is
0.035 on C1 and 0.017 on C2, comparing to 0.091
on C1 and 0.153 on C2 for the baseline, which
shows that our rule-based method has promise.
3Notice that we have the case that one sentence is aligned
to two sentences (i.e. Consolidation, as sentence 66 in Table
3). In our evaluation, an alignment is considered to be correct
only if the alignment covers all the sentences that should be
covered. For example, if Sentence 66 in Table 3 is aligned to
Sentence 55 in the first draft, it is counted as an error.
4For Groups 1 and 2, we calculate the accuracy of
Hashemi’s approach under a leave-one-out setting, each time
remove one pair of document and calculate the accuracy. A
significance test is also conducted, the worst metric LD in
Group 1 and WO in Group 2 both beat the baseline signifi-
cantly ( P1 = 0.025,P2 = 0.017) in two-tailed T-test.
Applying our method on the predicted alignment
on the first step gets 0.067 on C1 and 0.025 on C2,
which although degraded still beats the baseline.
Edit sequence merging There are only a limited
number of Consolidation and Distribution exam-
ples in our corpus. Together there are 9 Consolida-
tion and 5 Distribution operations. In our current
data, the number of sentences involved in these
operations is always 2. Our rule-based method
achieved 100% accuracy in the identification of
these operations. It needs further work to see if
this method would perform equally well in more
complicated corpora.
</bodyText>
<sectionHeader confidence="0.99956" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999916">
This paper presents a preliminary work in the ef-
fort of describing changes across revisions at a
higher level than words, motivated by a long term
goal to build educational applications to support
revision analysis for writing. Comparing to revi-
sion analysis based on words or phrases, our ap-
proach is able to capture higher level revision op-
erations. We also propose algorithms to detect re-
vision changes automatically. Experiments show
that our method has a reliable performance.
Currently we are investigating applying se-
quence merging on the automatic generated edit
sequences based on edit distances directly. Our
next plan is to develop a tool for comparing drafts,
and conduct user studies to have extrinsic evalua-
tions on whether our method would provide more
useful information to the user. We are also plan-
ning to do further analysis based on the revisions
detected, and ultimately be able to distinguish be-
tween surface changes and text-based changes.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999065363636364">
We would like to thank W. Wang, W. Luo, H. Xue,
and the ITSPOKE group for their helpful feedback
and all the anonymous reviewers for their sugges-
tions.
This research is supported by the Institute of
Education Sciences, U.S. Department of Educa-
tion, through Grant R305A120370 to the Univer-
sity of Pittsburgh. The opinions expressed are
those of the authors and do not necessarily repre-
sent the views of the Institute or the U.S. Depart-
ment of Education.
</bodyText>
<page confidence="0.998872">
153
</page>
<sectionHeader confidence="0.987392" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998076193181819">
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 25–
32. Association for Computational Linguistics.
Stefan Bott and Horacio Saggion. 2011. An un-
supervised alignment algorithm for text simplifica-
tion corpus construction. In Proceedings of the
Workshop on Monolingual Text-To-Text Generation,
pages 20–26. Association for Computational Lin-
guistics.
Amit Bronner and Christof Monz. 2012. User edits
classification using document revision histories. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 356–366. Association for Computa-
tional Linguistics.
CompareSuite. 2014. Compare suite, feature-rich
file and folder compare tool. http://www.
comparesuite.com.
Ulla Connor and Karen Asenavage. 1994. Peer re-
sponse groups in esl writing classes: How much im-
pact on revision? Journal of Second Language Writ-
ing, 3(3):257–276.
Lester Faigley and Stephen Witte. 1981. Analyzing
revision. College composition and communication,
pages 400–414.
Homa B. Hashemi and Christian D. Schunn. 2014.
A tool for summarizing students’ shanges across
drafts. In International Conference on Intelligent
Tutoring Systems(ITS).
Michael Heilman and Noah A Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 1011–1019.
Association for Computational Linguistics.
Hongyan Jing. 2002. Using hidden markov modeling
to decompose human-written summaries. Computa-
tional linguistics, 28(4):527–543.
Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet physics doklady, volume 10, page 707.
Lieve Macken. 2010. An annotation scheme and
gold standard for dutch-english word alignment.
In 7th conference on International Language Re-
sources and Evaluation (LREC 2010), pages 3369–
3374. European Language Resources Association
(ELRA).
Saul B Needleman and Christian D Wunsch. 1970.
A general method applicable to the search for simi-
larities in the amino acid sequence of two proteins.
Journal of molecular biology, 48(3):443–453.
Rani Nelken and Stuart M Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for mono-
lingual corpora. In EACL.
Alla Rozovskaya and Dan Roth. 2010. Annotating
esl errors: Challenges and rewards. In Proceedings
of the NAACL HLT 2010 fifth workshop on innova-
tive use of NLP for building educational applica-
tions, pages 28–36. Association for Computational
Linguistics.
Chao Shen, Tao Li, and Chris HQ Ding. 2011. Inte-
grating clustering and multi-document summariza-
tion by bi-mixture probabilistic latent semantic anal-
ysis (plsa) with sentence bases. In AAAI.
Ben Swanson and Elif Yamangil. 2012. Correction
detection and error type selection as an esl educa-
tional aid. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 357–361. Association for Computa-
tional Linguistics.
Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Multi-document summarization us-
ing sentence-based topic models. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
pages 297–300. Association for Computational Lin-
guistics.
Jaclyn M. Wells, Morgan Sousa, Mia Martini, and
Allen Brizee. 2013. Steps for revising your pa-
per. http://owl.english.purdue.edu/
owl/resource/561/05.
Huichao Xue and Rebecca Hwa. 2014. Improved cor-
rection detection in revised esl sentences. In Pro-
ceedings of The 52nd Annual Meeting of the Associ-
ation for Computational Linguistics(ACL).
</reference>
<page confidence="0.999772">
154
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.818461">
<title confidence="0.999973">Sentence-level Rewriting Detection</title>
<author confidence="0.999964">Fan Zhang Diane Litman</author>
<affiliation confidence="0.999992">University of Pittsburgh University of Pittsburgh</affiliation>
<address confidence="0.99895">Pittsburgh, PA, 15260 Pittsburgh, PA, 15260</address>
<email confidence="0.824075">zhangfan@cs.pitt.edulitman@cs.pitt.edu</email>
<abstract confidence="0.999616611111111">Writers usually need iterations of revisions and edits during their writings. To better understand the process of rewriting, we need to know what has changed between the revisions. Prior work mainly focuses on detecting corrections within sentences, which is at the level of words or phrases. This paper proposes to detect revision changes at the sentence level. Looking at revisions at a higher level allows us to have a different understanding of the revision process. This paper also proposes an approach to automatically detect sentence revision changes. The proposed approach shows high accuracy in an evaluation using first and final draft essays from an undergraduate writing class.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
</authors>
<title>Sentence alignment for monolingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 conference on Empirical methods in natural language processing,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4799" citStr="Barzilay and Elhadad, 2003" startWordPosition="761" endWordPosition="765"> analysis at the word level, although their approach might work for identifying differences within one sentence, it makes mistakes when sentence analysis is the primary concern. Our work avoids the above problem by detecting differences at the sentence level. Sentence alignment is the first step of our method; further inferences about revision changes are then based on the alignments generated. We borrow ideas from the research on sentence alignment for monolingual corpora. Existing research usually focuses on the alignment from the text to its summarization or its simplification (Jing, 2002; Barzilay and Elhadad, 2003; Bott and Saggion, 2011). Barzilay and Elhadad (2003) treat sentence alignment as a classification task. The paragraphs are clustered into groups, and a binary classifier is trained to decide whether two sentences should be aligned or not. Nelken (2006) further improves the performance by using TF*IDF score instead of word overlap and also utilizing global optimization to take sentence order information into consideration. We argue that summarization could be considered as a special form of revision and adapted Nelken’s approach to our approach. Edit sequences are then inferred based on the r</context>
</contexts>
<marker>Barzilay, Elhadad, 2003</marker>
<rawString>Regina Barzilay and Noemie Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of the 2003 conference on Empirical methods in natural language processing, pages 25– 32. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Bott</author>
<author>Horacio Saggion</author>
</authors>
<title>An unsupervised alignment algorithm for text simplification corpus construction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-To-Text Generation,</booktitle>
<pages>pages</pages>
<contexts>
<context position="4824" citStr="Bott and Saggion, 2011" startWordPosition="766" endWordPosition="769"> although their approach might work for identifying differences within one sentence, it makes mistakes when sentence analysis is the primary concern. Our work avoids the above problem by detecting differences at the sentence level. Sentence alignment is the first step of our method; further inferences about revision changes are then based on the alignments generated. We borrow ideas from the research on sentence alignment for monolingual corpora. Existing research usually focuses on the alignment from the text to its summarization or its simplification (Jing, 2002; Barzilay and Elhadad, 2003; Bott and Saggion, 2011). Barzilay and Elhadad (2003) treat sentence alignment as a classification task. The paragraphs are clustered into groups, and a binary classifier is trained to decide whether two sentences should be aligned or not. Nelken (2006) further improves the performance by using TF*IDF score instead of word overlap and also utilizing global optimization to take sentence order information into consideration. We argue that summarization could be considered as a special form of revision and adapted Nelken’s approach to our approach. Edit sequences are then inferred based on the results of sentence alignm</context>
</contexts>
<marker>Bott, Saggion, 2011</marker>
<rawString>Stefan Bott and Horacio Saggion. 2011. An unsupervised alignment algorithm for text simplification corpus construction. In Proceedings of the Workshop on Monolingual Text-To-Text Generation, pages 20–26. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bronner</author>
<author>Christof Monz</author>
</authors>
<title>User edits classification using document revision histories.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>356--366</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6669" citStr="Bronner and Monz, 2012" startWordPosition="1058" endWordPosition="1061">nges or text-based changes. With both categories, six kinds of changes were defined as shown in Table 1. Different from Faigley’s definition, we define only 4 primitives for our first step of edit sequence generation: Add, Delete, Modify and Keep. This Code Explanation Addition Adding a word or phrase Deletion Omitting a word or phrase Substitutions exchange words with synonyms Permutation rearrange of words or phrases Distribution one segment divided into two Consolidation combine two segments into one Table 1: Code Definition by L.Faigley and S.Witte definition is similar to Bronner’s work (Bronner and Monz, 2012). We choose this definition because these 4 primitives only correspond to one sentence at a time. Add, Delete, Modify indicates that the writer has added/deleted/modified a sentence. Keep means the original sentence is not modified. We believe Permutation, Distribution and Consolidation as defined by Faigley could be described with these four primitives, which could be recognized in the later merge step. 3.2 Data and annotation The corpus we choose consists of paired first and final drafts of short papers written by undergraduates in a course “Social Implications of Computing Technology”. Stud</context>
</contexts>
<marker>Bronner, Monz, 2012</marker>
<rawString>Amit Bronner and Christof Monz. 2012. User edits classification using document revision histories. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 356–366. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CompareSuite</author>
</authors>
<title>Compare suite, feature-rich file and folder compare tool.</title>
<date>2014</date>
<note>http://www. comparesuite.com.</note>
<contexts>
<context position="3334" citStr="CompareSuite, 2014" startWordPosition="523" endWordPosition="524"> perform analysis on the different types of changes and have a better understanding of the rewriting process. As a preliminary work, this paper explores steps toward this goal: First, automatically generate the description of changes based on four primitives: Add, Delete, Modify, Keep; Second, merge the primitives that come from the same purpose. 2 Related work Hashemi and Schunn (2014) presented a tool to help professors summarize students’ changes across papers before and after peer review. They first split the original documents into sentences and then built on the output of Compare Suite (CompareSuite, 2014) to count and highlight changes in different colors. Figure 1 shows a screenshot of their work. As we can see, the modifications to the text are misinterpreted. Line 66 in the final draft should correspond to line 55 and line 56 in the first draft, while line 67 and line 68 should be a split of line 57 in the first draft. However, line 67 is aligned to line 56 wrongly in their work. This wrong alignment caused many mis149 Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications, pages 149–154, Baltimore, Maryland USA, June 26, 2014. c�2014 Association fo</context>
</contexts>
<marker>CompareSuite, 2014</marker>
<rawString>CompareSuite. 2014. Compare suite, feature-rich file and folder compare tool. http://www. comparesuite.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulla Connor</author>
<author>Karen Asenavage</author>
</authors>
<title>Peer response groups in esl writing classes: How much impact on revision?</title>
<date>1994</date>
<journal>Journal of Second Language Writing,</journal>
<volume>3</volume>
<issue>3</issue>
<contexts>
<context position="5992" citStr="Connor and Asenavage, 1994" startWordPosition="953" endWordPosition="957">s are then inferred based on the results of sentence alignment. Fragments of edits that come from the same purpose will then be merged. Related work to our method is sentence clustering (Shen et al., 2011; Wang et al., 2009). While sentence clustering is trying to find and cluster sentences similar to each other, our work is to find a cluster of sentences in one document that is similar to one sentence in the other document after merging. 3 Sentence-level changes across revisions 3.1 Primitives for sentence-level changes Previous work in educational revision analysis (Faigley and Witte, 1981; Connor and Asenavage, 1994) categorized revision changes to be either surface changes or text-based changes. With both categories, six kinds of changes were defined as shown in Table 1. Different from Faigley’s definition, we define only 4 primitives for our first step of edit sequence generation: Add, Delete, Modify and Keep. This Code Explanation Addition Adding a word or phrase Deletion Omitting a word or phrase Substitutions exchange words with synonyms Permutation rearrange of words or phrases Distribution one segment divided into two Consolidation combine two segments into one Table 1: Code Definition by L.Faigley</context>
</contexts>
<marker>Connor, Asenavage, 1994</marker>
<rawString>Ulla Connor and Karen Asenavage. 1994. Peer response groups in esl writing classes: How much impact on revision? Journal of Second Language Writing, 3(3):257–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lester Faigley</author>
<author>Stephen Witte</author>
</authors>
<title>Analyzing revision. College composition and communication,</title>
<date>1981</date>
<pages>400--414</pages>
<contexts>
<context position="1638" citStr="Faigley and Witte, 1981" startWordPosition="251" endWordPosition="254">is not an easy task, especially for novice writers. Instructors work hard on providing suggestions for rewriting (Wells et al., 2013), but usually such advice is quite general. We need to understand the changes between revisions better to provide more specific and helpful advice. There has already been work on detecting corrections in sentence revisions (Xue and Hwa, 2014; Swanson and Yamangil, 2012; Heilman and Smith, 2010; Rozovskaya and Roth, 2010). However, these works mainly focus on detecting changes at the level of words or phrases. According to Faigley’s definition of revision change (Faigley and Witte, 1981), these works could help the identification of Surface Changes (changes that do not add or remove information to the original text). However, Text Changes (changes that add or remove information) will be more difficult to identify if we only look at revisions within sentences. According to Hashemi and Schunn (2014), when instructors were presented a comparison of differences between papers derived from words, they felt the information regarding changes between revisions was overwhelming. This paper proposes to look at the changes between revisions at the level of sentences. Comparing to detect</context>
<context position="5963" citStr="Faigley and Witte, 1981" startWordPosition="949" endWordPosition="952">r approach. Edit sequences are then inferred based on the results of sentence alignment. Fragments of edits that come from the same purpose will then be merged. Related work to our method is sentence clustering (Shen et al., 2011; Wang et al., 2009). While sentence clustering is trying to find and cluster sentences similar to each other, our work is to find a cluster of sentences in one document that is similar to one sentence in the other document after merging. 3 Sentence-level changes across revisions 3.1 Primitives for sentence-level changes Previous work in educational revision analysis (Faigley and Witte, 1981; Connor and Asenavage, 1994) categorized revision changes to be either surface changes or text-based changes. With both categories, six kinds of changes were defined as shown in Table 1. Different from Faigley’s definition, we define only 4 primitives for our first step of edit sequence generation: Add, Delete, Modify and Keep. This Code Explanation Addition Adding a word or phrase Deletion Omitting a word or phrase Substitutions exchange words with synonyms Permutation rearrange of words or phrases Distribution one segment divided into two Consolidation combine two segments into one Table 1:</context>
</contexts>
<marker>Faigley, Witte, 1981</marker>
<rawString>Lester Faigley and Stephen Witte. 1981. Analyzing revision. College composition and communication, pages 400–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Homa B Hashemi</author>
<author>Christian D Schunn</author>
</authors>
<title>A tool for summarizing students’ shanges across drafts.</title>
<date>2014</date>
<booktitle>In International Conference on Intelligent Tutoring Systems(ITS).</booktitle>
<contexts>
<context position="1954" citStr="Hashemi and Schunn (2014)" startWordPosition="303" endWordPosition="306">ecting corrections in sentence revisions (Xue and Hwa, 2014; Swanson and Yamangil, 2012; Heilman and Smith, 2010; Rozovskaya and Roth, 2010). However, these works mainly focus on detecting changes at the level of words or phrases. According to Faigley’s definition of revision change (Faigley and Witte, 1981), these works could help the identification of Surface Changes (changes that do not add or remove information to the original text). However, Text Changes (changes that add or remove information) will be more difficult to identify if we only look at revisions within sentences. According to Hashemi and Schunn (2014), when instructors were presented a comparison of differences between papers derived from words, they felt the information regarding changes between revisions was overwhelming. This paper proposes to look at the changes between revisions at the level of sentences. Comparing to detecting changes at the word level, detecting changes at the sentence level contains less information, but still keeps enough information to understand the authors’ intention behind their modifications to the text. The sentence level edits could then be grouped and classified into different types of changes. The long-te</context>
</contexts>
<marker>Hashemi, Schunn, 2014</marker>
<rawString>Homa B. Hashemi and Christian D. Schunn. 2014. A tool for summarizing students’ shanges across drafts. In International Conference on Intelligent Tutoring Systems(ITS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>1011--1019</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1441" citStr="Heilman and Smith, 2010" startWordPosition="220" endWordPosition="223"> using first and final draft essays from an undergraduate writing class. 1 Introduction Rewriting is considered to be an important process during writing. However, conducting successful rewriting is not an easy task, especially for novice writers. Instructors work hard on providing suggestions for rewriting (Wells et al., 2013), but usually such advice is quite general. We need to understand the changes between revisions better to provide more specific and helpful advice. There has already been work on detecting corrections in sentence revisions (Xue and Hwa, 2014; Swanson and Yamangil, 2012; Heilman and Smith, 2010; Rozovskaya and Roth, 2010). However, these works mainly focus on detecting changes at the level of words or phrases. According to Faigley’s definition of revision change (Faigley and Witte, 1981), these works could help the identification of Surface Changes (changes that do not add or remove information to the original text). However, Text Changes (changes that add or remove information) will be more difficult to identify if we only look at revisions within sentences. According to Hashemi and Schunn (2014), when instructors were presented a comparison of differences between papers derived fr</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A Smith. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 1011–1019. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Using hidden markov modeling to decompose human-written summaries.</title>
<date>2002</date>
<booktitle>Computational linguistics,</booktitle>
<pages>28--4</pages>
<contexts>
<context position="4771" citStr="Jing, 2002" startWordPosition="759" endWordPosition="760"> is based on analysis at the word level, although their approach might work for identifying differences within one sentence, it makes mistakes when sentence analysis is the primary concern. Our work avoids the above problem by detecting differences at the sentence level. Sentence alignment is the first step of our method; further inferences about revision changes are then based on the alignments generated. We borrow ideas from the research on sentence alignment for monolingual corpora. Existing research usually focuses on the alignment from the text to its summarization or its simplification (Jing, 2002; Barzilay and Elhadad, 2003; Bott and Saggion, 2011). Barzilay and Elhadad (2003) treat sentence alignment as a classification task. The paragraphs are clustered into groups, and a binary classifier is trained to decide whether two sentences should be aligned or not. Nelken (2006) further improves the performance by using TF*IDF score instead of word overlap and also utilizing global optimization to take sentence order information into consideration. We argue that summarization could be considered as a special form of revision and adapted Nelken’s approach to our approach. Edit sequences are </context>
</contexts>
<marker>Jing, 2002</marker>
<rawString>Hongyan Jing. 2002. Using hidden markov modeling to decompose human-written summaries. Computational linguistics, 28(4):527–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<booktitle>In Soviet physics doklady,</booktitle>
<volume>10</volume>
<pages>707</pages>
<contexts>
<context position="12374" citStr="Levenshtein, 1966" startWordPosition="2003" endWordPosition="2005">c defined, calculate the similarity of the sentence pairs. A logistic regression classifier predicting whether a sentence pair is aligned or not is trained with the similarity score as the feature. In addition to classification, the classifier is also used to provide a similarity score for global alignment. 3. Alignment: for each pair of paper drafts, construct sentence pairs using the Cartesian product of sentences in the first draft and sentences in the final. Logistic regression classifier is used to determine whether the sentence pair is aligned or not. We added Levenshtein distance (LD) (Levenshtein, 1966) as another similarity metric in addition to Nelken’s metrics. Together three similarity metrics were compared: Levenshtein Distance, Word Overlap(WO), and TF*IDF. Global alignment Sentences are likely to preserve the same order between rewritings. Thus, sentence ordering should be an important feature in sentence alignment. Nelken’s work modifies the Needleman-Wunsch alignment (Needleman and Wunsch, 1970) to find the sentence alignments and goes in the following steps. Step]: The logistic regression classifier previously trained assigns a probability value from 0 to 1 for each sentence pair s</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. In Soviet physics doklady, volume 10, page 707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lieve Macken</author>
</authors>
<title>An annotation scheme and gold standard for dutch-english word alignment.</title>
<date>2010</date>
<booktitle>In 7th conference on International Language Resources and Evaluation (LREC 2010),</booktitle>
<pages>3369--3374</pages>
<contexts>
<context position="11330" citStr="Macken, 2010" startWordPosition="1836" endWordPosition="1837">5, 56; while 57 distributes to 67, 68. Notice that Consolidation is illustrated as Modify, Delete and Distribution is illustrated as Modify, Add. As the annotators annotate based on the first draft, Modify always appears before Add or Delete Sentence alignment We adapted Nelken’s approach to our problem. Alignment based on sentence similarity The alignment task goes through three stages. 1. Data preparation: for each sentence in the annotated final draft, if it is not a new sentence, create a sentence pair with its aligned sentence in the 2We calculate the Kappa value following Macken’s idea (Macken, 2010), where the aligned sentences are categorized as direct-link, while new added sentences are categorized as null-link (ADD). 151 first draft. The pair is considered to be an aligned pair. Also, randomly select another sentence from the first draft to make a negative sentence pair. Thus we ensure there are nearly equal numbers of positive and negative cases in the training data. 2. Training: according to the similarity metric defined, calculate the similarity of the sentence pairs. A logistic regression classifier predicting whether a sentence pair is aligned or not is trained with the similarit</context>
</contexts>
<marker>Macken, 2010</marker>
<rawString>Lieve Macken. 2010. An annotation scheme and gold standard for dutch-english word alignment. In 7th conference on International Language Resources and Evaluation (LREC 2010), pages 3369– 3374. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saul B Needleman</author>
<author>Christian D Wunsch</author>
</authors>
<title>A general method applicable to the search for similarities in the amino acid sequence of two proteins.</title>
<date>1970</date>
<journal>Journal of molecular biology,</journal>
<volume>48</volume>
<issue>3</issue>
<contexts>
<context position="12783" citStr="Needleman and Wunsch, 1970" startWordPosition="2061" endWordPosition="2064">roduct of sentences in the first draft and sentences in the final. Logistic regression classifier is used to determine whether the sentence pair is aligned or not. We added Levenshtein distance (LD) (Levenshtein, 1966) as another similarity metric in addition to Nelken’s metrics. Together three similarity metrics were compared: Levenshtein Distance, Word Overlap(WO), and TF*IDF. Global alignment Sentences are likely to preserve the same order between rewritings. Thus, sentence ordering should be an important feature in sentence alignment. Nelken’s work modifies the Needleman-Wunsch alignment (Needleman and Wunsch, 1970) to find the sentence alignments and goes in the following steps. Step]: The logistic regression classifier previously trained assigns a probability value from 0 to 1 for each sentence pair s(i, j). Use this value as the similarity score of sentence pair: sim(i, j). Step2: Starting from the first pair of sentences, find the best path to maximize the likelihood between sentences according to the formula s(i, j) = max{s(i − 1, j − 1) + sim(i, j), s(i − 1, j) + sim(i, j) , s(i, j − 1) + sim(i, j)} Step3: Infer the sentence alignments by back tracing the matrix s(i, j). We found out that changing </context>
</contexts>
<marker>Needleman, Wunsch, 1970</marker>
<rawString>Saul B Needleman and Christian D Wunsch. 1970. A general method applicable to the search for similarities in the amino acid sequence of two proteins. Journal of molecular biology, 48(3):443–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rani Nelken</author>
<author>Stuart M Shieber</author>
</authors>
<title>Towards robust context-sensitive sentence alignment for monolingual corpora.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<marker>Nelken, Shieber, 2006</marker>
<rawString>Rani Nelken and Stuart M Shieber. 2006. Towards robust context-sensitive sentence alignment for monolingual corpora. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Dan Roth</author>
</authors>
<title>Annotating esl errors: Challenges and rewards.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT</booktitle>
<pages>28--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1469" citStr="Rozovskaya and Roth, 2010" startWordPosition="224" endWordPosition="227">aft essays from an undergraduate writing class. 1 Introduction Rewriting is considered to be an important process during writing. However, conducting successful rewriting is not an easy task, especially for novice writers. Instructors work hard on providing suggestions for rewriting (Wells et al., 2013), but usually such advice is quite general. We need to understand the changes between revisions better to provide more specific and helpful advice. There has already been work on detecting corrections in sentence revisions (Xue and Hwa, 2014; Swanson and Yamangil, 2012; Heilman and Smith, 2010; Rozovskaya and Roth, 2010). However, these works mainly focus on detecting changes at the level of words or phrases. According to Faigley’s definition of revision change (Faigley and Witte, 1981), these works could help the identification of Surface Changes (changes that do not add or remove information to the original text). However, Text Changes (changes that add or remove information) will be more difficult to identify if we only look at revisions within sentences. According to Hashemi and Schunn (2014), when instructors were presented a comparison of differences between papers derived from words, they felt the info</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>Alla Rozovskaya and Dan Roth. 2010. Annotating esl errors: Challenges and rewards. In Proceedings of the NAACL HLT 2010 fifth workshop on innovative use of NLP for building educational applications, pages 28–36. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Shen</author>
<author>Tao Li</author>
<author>Chris HQ Ding</author>
</authors>
<title>Integrating clustering and multi-document summarization by bi-mixture probabilistic latent semantic analysis (plsa) with sentence bases.</title>
<date>2011</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="5569" citStr="Shen et al., 2011" startWordPosition="886" endWordPosition="889"> binary classifier is trained to decide whether two sentences should be aligned or not. Nelken (2006) further improves the performance by using TF*IDF score instead of word overlap and also utilizing global optimization to take sentence order information into consideration. We argue that summarization could be considered as a special form of revision and adapted Nelken’s approach to our approach. Edit sequences are then inferred based on the results of sentence alignment. Fragments of edits that come from the same purpose will then be merged. Related work to our method is sentence clustering (Shen et al., 2011; Wang et al., 2009). While sentence clustering is trying to find and cluster sentences similar to each other, our work is to find a cluster of sentences in one document that is similar to one sentence in the other document after merging. 3 Sentence-level changes across revisions 3.1 Primitives for sentence-level changes Previous work in educational revision analysis (Faigley and Witte, 1981; Connor and Asenavage, 1994) categorized revision changes to be either surface changes or text-based changes. With both categories, six kinds of changes were defined as shown in Table 1. Different from Fai</context>
</contexts>
<marker>Shen, Li, Ding, 2011</marker>
<rawString>Chao Shen, Tao Li, and Chris HQ Ding. 2011. Integrating clustering and multi-document summarization by bi-mixture probabilistic latent semantic analysis (plsa) with sentence bases. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Swanson</author>
<author>Elif Yamangil</author>
</authors>
<title>Correction detection and error type selection as an esl educational aid.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>357--361</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1416" citStr="Swanson and Yamangil, 2012" startWordPosition="216" endWordPosition="219">gh accuracy in an evaluation using first and final draft essays from an undergraduate writing class. 1 Introduction Rewriting is considered to be an important process during writing. However, conducting successful rewriting is not an easy task, especially for novice writers. Instructors work hard on providing suggestions for rewriting (Wells et al., 2013), but usually such advice is quite general. We need to understand the changes between revisions better to provide more specific and helpful advice. There has already been work on detecting corrections in sentence revisions (Xue and Hwa, 2014; Swanson and Yamangil, 2012; Heilman and Smith, 2010; Rozovskaya and Roth, 2010). However, these works mainly focus on detecting changes at the level of words or phrases. According to Faigley’s definition of revision change (Faigley and Witte, 1981), these works could help the identification of Surface Changes (changes that do not add or remove information to the original text). However, Text Changes (changes that add or remove information) will be more difficult to identify if we only look at revisions within sentences. According to Hashemi and Schunn (2014), when instructors were presented a comparison of differences </context>
</contexts>
<marker>Swanson, Yamangil, 2012</marker>
<rawString>Ben Swanson and Elif Yamangil. 2012. Correction detection and error type selection as an esl educational aid. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 357–361. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dingding Wang</author>
<author>Shenghuo Zhu</author>
<author>Tao Li</author>
<author>Yihong Gong</author>
</authors>
<title>Multi-document summarization using sentence-based topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>297--300</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5589" citStr="Wang et al., 2009" startWordPosition="890" endWordPosition="893">is trained to decide whether two sentences should be aligned or not. Nelken (2006) further improves the performance by using TF*IDF score instead of word overlap and also utilizing global optimization to take sentence order information into consideration. We argue that summarization could be considered as a special form of revision and adapted Nelken’s approach to our approach. Edit sequences are then inferred based on the results of sentence alignment. Fragments of edits that come from the same purpose will then be merged. Related work to our method is sentence clustering (Shen et al., 2011; Wang et al., 2009). While sentence clustering is trying to find and cluster sentences similar to each other, our work is to find a cluster of sentences in one document that is similar to one sentence in the other document after merging. 3 Sentence-level changes across revisions 3.1 Primitives for sentence-level changes Previous work in educational revision analysis (Faigley and Witte, 1981; Connor and Asenavage, 1994) categorized revision changes to be either surface changes or text-based changes. With both categories, six kinds of changes were defined as shown in Table 1. Different from Faigley’s definition, w</context>
</contexts>
<marker>Wang, Zhu, Li, Gong, 2009</marker>
<rawString>Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong Gong. 2009. Multi-document summarization using sentence-based topic models. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 297–300. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaclyn M Wells</author>
<author>Morgan Sousa</author>
<author>Mia Martini</author>
<author>Allen Brizee</author>
</authors>
<title>Steps for revising your paper.</title>
<date>2013</date>
<note>http://owl.english.purdue.edu/ owl/resource/561/05.</note>
<contexts>
<context position="1147" citStr="Wells et al., 2013" startWordPosition="171" endWordPosition="174">sion changes at the sentence level. Looking at revisions at a higher level allows us to have a different understanding of the revision process. This paper also proposes an approach to automatically detect sentence revision changes. The proposed approach shows high accuracy in an evaluation using first and final draft essays from an undergraduate writing class. 1 Introduction Rewriting is considered to be an important process during writing. However, conducting successful rewriting is not an easy task, especially for novice writers. Instructors work hard on providing suggestions for rewriting (Wells et al., 2013), but usually such advice is quite general. We need to understand the changes between revisions better to provide more specific and helpful advice. There has already been work on detecting corrections in sentence revisions (Xue and Hwa, 2014; Swanson and Yamangil, 2012; Heilman and Smith, 2010; Rozovskaya and Roth, 2010). However, these works mainly focus on detecting changes at the level of words or phrases. According to Faigley’s definition of revision change (Faigley and Witte, 1981), these works could help the identification of Surface Changes (changes that do not add or remove information</context>
</contexts>
<marker>Wells, Sousa, Martini, Brizee, 2013</marker>
<rawString>Jaclyn M. Wells, Morgan Sousa, Mia Martini, and Allen Brizee. 2013. Steps for revising your paper. http://owl.english.purdue.edu/ owl/resource/561/05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huichao Xue</author>
<author>Rebecca Hwa</author>
</authors>
<title>Improved correction detection in revised esl sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of The 52nd Annual Meeting of the Association for Computational Linguistics(ACL).</booktitle>
<contexts>
<context position="1388" citStr="Xue and Hwa, 2014" startWordPosition="212" endWordPosition="215">d approach shows high accuracy in an evaluation using first and final draft essays from an undergraduate writing class. 1 Introduction Rewriting is considered to be an important process during writing. However, conducting successful rewriting is not an easy task, especially for novice writers. Instructors work hard on providing suggestions for rewriting (Wells et al., 2013), but usually such advice is quite general. We need to understand the changes between revisions better to provide more specific and helpful advice. There has already been work on detecting corrections in sentence revisions (Xue and Hwa, 2014; Swanson and Yamangil, 2012; Heilman and Smith, 2010; Rozovskaya and Roth, 2010). However, these works mainly focus on detecting changes at the level of words or phrases. According to Faigley’s definition of revision change (Faigley and Witte, 1981), these works could help the identification of Surface Changes (changes that do not add or remove information to the original text). However, Text Changes (changes that add or remove information) will be more difficult to identify if we only look at revisions within sentences. According to Hashemi and Schunn (2014), when instructors were presented </context>
</contexts>
<marker>Xue, Hwa, 2014</marker>
<rawString>Huichao Xue and Rebecca Hwa. 2014. Improved correction detection in revised esl sentences. In Proceedings of The 52nd Annual Meeting of the Association for Computational Linguistics(ACL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
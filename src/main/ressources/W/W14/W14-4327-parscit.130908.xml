<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000240">
<title confidence="0.9989135">
Reducing Sparsity Improves the Recognition of Implicit Discourse
Relations
</title>
<author confidence="0.99278">
Junyi Jessy Li
</author>
<affiliation confidence="0.998164">
University of Pennsylvania
</affiliation>
<email confidence="0.980085">
ljunyi@seas.upenn.edu
</email>
<author confidence="0.980381">
Ani Nenkova
</author>
<affiliation confidence="0.99684">
University of Pennsylvania
</affiliation>
<email confidence="0.991328">
nenkova@seas.upenn.edu
</email>
<sectionHeader confidence="0.997317" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996225">
The earliest work on automatic detec-
tion of implicit discourse relations relied
on lexical features. More recently, re-
searchers have demonstrated that syntactic
features are superior to lexical features for
the task. In this paper we re-examine the
two classes of state of the art representa-
tions: syntactic production rules and word
pair features. In particular, we focus on the
need to reduce sparsity in instance repre-
sentation, demonstrating that different rep-
resentation choices even for the same class
of features may exacerbate sparsity issues
and reduce performance. We present re-
sults that clearly reveal that lexicalization
of the syntactic features is necessary for
good performance. We introduce a novel,
less sparse, syntactic representation which
leads to improvement in discourse rela-
tion recognition. Finally, we demonstrate
that classifiers trained on different repre-
sentations, especially lexical ones, behave
rather differently and thus could likely be
combined in future systems.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.973330870370371">
Implicit discourse relations hold between adjacent
sentences in the same paragraph, and are not sig-
naled by any of the common explicit discourse
connectives such as because, however, meanwhile,
etc. Consider the two examples below, drawn from
the Penn Discourse Treebank (PDTB) (Prasad et
al., 2008), of a causal and a contrast relation, re-
spectively. The italic and bold fonts mark the ar-
guments of the relation, i.e the portions of the text
connected by the discourse relation.
Ex1: Mrs Yeargin is lying. [Implicit = BECAUSE] They
found students in an advanced class a year earlier who
said she gave them similar help.
Ex2: Back downtown, the execs squeezed in a few meetings at
the hotel before boarding the buses again. [Implicit = BUT]
This time, it was for dinner and dancing - a block away.
The task is undisputedly hard, partly because it
is hard to come up with intuitive feature represen-
tations for the problem. Lexical and syntactic fea-
tures form the basis of the most successful stud-
ies on supervised prediction of implicit discourse
relations in the PDTB. Lexical features were the
focus of the earliest work in discourse recogni-
tion, when cross product of words (word pairs)
in the two spans connected via a discourse re-
lation was studied. Later, grammatical produc-
tions were found to be more effective. Features
of other classes such as verbs, inquirer tags, posi-
tions were also studied, but they only marginally
improve upon syntactic features.
In this study, we compare the most commonly
used lexical and syntactic features. We show that
representations that minimize sparsity issues are
superior to their sparse counterparts, i.e. the bet-
ter representations are those for which informative
features occur in larger portions of the data. Not
surprisingly, lexical features are more sparse (oc-
curring in fewer instances in the dataset) than syn-
tactic features; the superiority of syntactic repre-
sentations may thus be partially explained by this
property.
More surprising findings come from a closer ex-
amination of instance representation approaches
in prior work. We first discuss how choices in
prior work have in fact exacerbated the sparsity
problem of lexical features. Then, we introduce
a new syntactically informed feature class, which
is less sparse than prior lexical and syntactic fea-
tures, and improves significantly the classification
of implicit discourse relations.
Given these findings, we address the question
if any lexical information at all should be pre-
served in discourse parsers. We find that purely
syntactic representations show lower recognition
</bodyText>
<page confidence="0.985126">
199
</page>
<note confidence="0.7310125">
Proceedings of the SIGDIAL 2014 Conference, pages 199–207,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999983272727273">
for most relations, indicating that lexical features,
albeit sparse, are necessary for the task. Lexical
features also account for a high percentage of the
most predictive features.
We further quantify the agreement of predic-
tions produced from classifiers using different in-
stance representations. We find that our novel syn-
tactic representation is better for implicit discourse
relation prediction than prior syntactic feature be-
cause it has higher overall accuracy and makes
correct predictions for instances for which the al-
ternative representations are also correct. Differ-
ent representation of lexical features however ap-
pear complementary to each other, with markedly
higher fraction of instances recognized correctly
by only one of the models.
Our work advances the state of the art in implicit
discourse recognition by clarifying the extent to
which sparsity issues influence predictions, by in-
troducing a strong syntactic representation and by
documenting the need for further more complex
integration of lexical information.
</bodyText>
<sectionHeader confidence="0.977886" genericHeader="introduction">
2 The Penn Discourse Treebank
</sectionHeader>
<bodyText confidence="0.995159265306123">
The Penn Discourse Treebank (PDTB) (Prasad et
al., 2008) contains annotations for five types of
discourse relations over the Penn Treebank corpus
(Marcus et al., 1993). Explicit relations are those
signaled by a discourse connective that occurs in
the text, such as “because”, “however”, “for ex-
ample”. Implicit relations are annotated between
adjacent sentences in the same paragraph. There
are no discourse connectives between the two sen-
tences, and the annotators were asked to insert a
connective while marking their senses. Some pairs
of sentences do not contain one of the explicit dis-
course connectives, but the insertion of a connec-
tive provides redundant information into the text.
For example, they may contain phrases such as
“the consequence of the act”. These are marked
Alternative Lexicalizations (AltLex). Entity rela-
tions (EntRel) are adjacent sentences that are only
related via the same entity or topic. Finally, sen-
tences where no discourse relations were identi-
fied were marked NoRel. In this work, we consider
AltLex to be part of the Implicit relations, and En-
tRel to be part of NoRel.
All connectives, either explicit or implicitly in-
serted, are associated with two arguments of the
minimal span of text conveying the semantic con-
tent between which the relation holds. This is il-
lustrated in the following example where the two
arguments are marked in bold and italic:
Ex: They stopped delivering junk mail. [Implicit = SO] Now
thousands of mailers go straight into the trash.
Relation senses in the PDTB are drawn from
a 3-level hierarchy. The top level relations are
Comparison (arg1 and arg2 holds a contrast rela-
tion), Contingency (arg1 and arg2 are causally re-
lated), Expansion (arg2 further describes arg1) and
Temporal (arg1 and arg2 are temporally related).
Some of the largest second-tier relations are under
Expansion, which include Conjunction (arg2 pro-
vides new information to arg1), Instantiation (arg2
exemplifies arg1) and Restatement (arg2 semanti-
cally repeats arg1).
In our experiments we use the four top level re-
lations as well as the above three subclasses of Ex-
pansion. All of these subclasses occur with fre-
quencies similar to those of the Contingency and
Comparison classes, with thousands of examples
in the PDTB.1 We show the distribution of the
classes below:
</bodyText>
<table confidence="0.69267975">
Temporal 1038 Comparison 2550
Contingency 4532 Instantiation 1483
Restatement 3271 Conjunction 3646
EntRel/NoRel 5464
</table>
<sectionHeader confidence="0.993021" genericHeader="method">
3 Experimental settings
</sectionHeader>
<bodyText confidence="0.999908588235294">
In our experiments we use only lexical and syntac-
tic features. This choice is motivated by the fact
that lexical features have been used most widely
for the task and that recent work has demon-
strated that syntactic features are the single best
type of representation. Adding additional features
only minimally improves performance (Lin et al.,
2009). By zeroing in only on these classes of fea-
tures we are able to discuss more clearly the im-
pact that different instance representation have on
sparsity and classifier performance.
We use gold-standard parses from the original
Penn Treebank for syntax features.
To ensure that our conclusions are based on
analysis of the most common relations, we train
binary SVM classifiers2 for the seven relations de-
scribed above. We adopt the standard practice in
</bodyText>
<footnote confidence="0.987271833333333">
1All other sub-classes of implicit relations are too small
for general practical applications. For example the Alterna-
tive class and Concession class have only 185 and 228 oc-
currences, respectively, in the 16,224 implicit relation anno-
tations of the PDTB.
2We use SVMLight (Joachims, 1999) with linear kernel.
</footnote>
<page confidence="0.996542">
200
</page>
<bodyText confidence="0.99997215">
prior work and downsampled the negative class so
the number of positive and negative samples are
equal in the training set.3
Our training set consists of PDTB sections 2-
19. The testing set consists of sections 20-24. Like
most studies, we do not include sections 0-1 in the
training set. We expanded the test set (sections 23
or 23-24) used in previous work (Lin et al., 2014;
Park and Cardie, 2012) to ensure the number of
examples of the smaller relations, particularly of
Temporal or Instantiation, are suitable for carrying
out reliable tests for statistical significance.
Some of the discourse relations are much larger
than others, so we report our results in term of F-
measure for each relation and average unweighted
accuracy. Significance tests over F scores were
carried out using a paired t-test. To do this, the
test set is randomly partitioned into ten groups. In
each group, the relation distribution was kept as
close as possible to the overall test set.
</bodyText>
<sectionHeader confidence="0.9820845" genericHeader="method">
4 Sparsity and pure lexical
representations
</sectionHeader>
<bodyText confidence="0.999917192307692">
By far the most common features used for rep-
resenting implicit discourse relations are lexical
(Sporleder and Lascarides, 2008; Pitler et al.,
2009; Lin et al., 2009; Hernault et al., 2010;
Park and Cardie, 2012). Early studies have sug-
gested that lexical features, word pairs (cross-
product of the words in the first and second ar-
gument) in particular, will be powerful predictors
of discourse relations (Marcu and Echihabi, 2002;
Blair-Goldensohn et al., 2007). The intuition be-
hind word pairs was that semantic relations be-
tween the lexical items, such as drought–famine,
child–adult, may in turn signal causal or contrast
discourse relations. Later it has been shown that
word pair features do not appear to capture such
semantic relationship between words (Pitler et al.,
2009) and that syntactic features lead to higher ac-
curacies (Lin et al., 2009; Zhou et al., 2010; Park
and Cardie, 2012). Recently, Biran and McKeown
(2013) aggregated word pair features with explicit
connectives and reported improvements over the
original word pairs as features.
In this section, we show that the representation
of lexical features play a direct role in feature spar-
sity and ultimately affects prediction performance.
The first two studies that specifically addressed
</bodyText>
<footnote confidence="0.7766555">
3We also did not include features that occurred less than
5 times in the training set.
</footnote>
<table confidence="0.973325">
# Features Avg. F Avg. Accuracy
word-pairs 92128 29.46 57.22
binary-lexical 12116 31.79 60.42
</table>
<tableCaption confidence="0.899957">
Table 1: F-scores and average accuracies of paired
and binary representations of words.
</tableCaption>
<bodyText confidence="0.999946088888889">
the problem of predicting implicit discourse re-
lations in the PDTB made use of very different
instance representations. Pitler et al. (2009) rep-
resent instances of discourse relations in a vec-
tor space defined by word pairs, i.e. the cross-
product of the words that appear in the two argu-
ments of the relation. There, features are of the
form (w1, w2) where w1 ∈ arg1 and w2 ∈ arg2.
If there are N words in the entire vocabulary, the
size of each instance would be N × N.
In contrast, Lin et al. (2009) represent instances
by tracking the occurrences of grammatical pro-
ductions in the syntactic parse of argument spans.
There are three indicator features associated with
each production: whether the production appears
in arg1, in arg2, and in both arguments. For a
grammar with N production rules, the size of the
vector representing an instance will be 3N. For
convenience we call this “binary representation”,
in contrast to the word-pair features in which the
cross product of words constitute the representa-
tion. Note that the cross-product approach has
been extended to a wide variety of features (Pitler
et al., 2009; Park and Cardie, 2012). In the ex-
periments that follow we will demonstrate that bi-
nary representations lead to less sparse features
and higher prediction accuracy.
Lin et al. (2009) found that their syntactic fea-
tures are more powerful than the word pair fea-
tures. Here we show that the advantage comes not
only from the inclusion of syntactic information
but also from the less sparse instance representa-
tion they used for syntactic features. In Table 1
we show the number of features for each repre-
sentation and the average F score and accuracy for
word pairs and words with binary representation
(binary-lexical). The results for each relation are
shown in Table 8 and discussed in Section 7.
Using binary representation for lexical informa-
tion outperforms word pairs. Thus, the difference
in how lexical information is represented accounts
for a considerable portion of the improvement re-
ported in Lin et al. (2009). Most notably, for the
Instantiation class, we see a 7.7% increase in F-
score. On average, the less sparse representation
</bodyText>
<page confidence="0.989424">
201
</page>
<bodyText confidence="0.99986025">
translates into 2.34% absolute improvement in F-
score and 3.2% absolute improvement in accuracy.
From this point on we adopt the binary represen-
tation for the features discussed.
</bodyText>
<sectionHeader confidence="0.944421" genericHeader="method">
5 Sparsity and syntactic features
</sectionHeader>
<bodyText confidence="0.999944688888889">
Grammatical production rules were first used for
discourse relation representation in Lin et al.
(2009). They were identified as the most suitable
representation, that lead to highest performance in
a couple of independent studies (Lin et al., 2009;
Park and Cardie, 2012). The comparison repre-
sentations covered a number of semantic classes
related to sentiment, polarity and verb information
and dependency representations of syntax.
Production rules correspond to tree chunks in
the constituency parse of a sentence, i.e. a node
in the syntactic parse tree with all of its children,
which in turn correspond to grammar rules ap-
plied in the derivation of the tree, such as S→NP
VP. This syntactic representation subsumes lexi-
cal representations because of the production rules
with part-of-speech on the left-hand side and a lex-
ical item on the right-hand side.
We propose that the sparsity of production rules
can be reduced even further by introducing a new
representation of the parse tree. Specifically, in-
stead of having full production rules where a sin-
gle feature records the parent and all its children,
all (parent,child) pairs in the constituency parse
tree are used. For example, the rule S→NP VP
will now become two features, S→NP and S→VP.
Note that the leaves of the tree, i.e. the part-of-
speech→word features are not changed. For ease
of reference we call this new representation “pro-
duction sticks”. In this section we show that F
scores and accuracies for implicit discourse rela-
tion prediction based on production sticks is sig-
nificantly higher than using full production rules.
First, Table 2 illustrates the contrast in sparsity
among the lexical, production rule and stick repre-
sentations. The table gives the rate of occurrence
of each feature class, which is defined as the av-
erage fraction of features with non-zero values in
the representation of instances in the entire train-
ing set. Specifically, let N be the total number of
features, mi be the number of features triggered in
instance i, then the rate of occurrence is m;N .
The table clearly shows that the number of fea-
tures in the three representations is comparable,
but they vary notably in their rate of occurrence.
</bodyText>
<table confidence="0.9998092">
# Features Rate of Occurrence
sticks 14,165 0.00623
prodrules 16,173 0.00374
binary-lexical 12,116 0.00276
word-pairs 92,128 0.00113
</table>
<tableCaption confidence="0.972286333333333">
Table 2: Number of features and rate of occur-
rence for binary lexical representation, production
rules and sticks.
</tableCaption>
<table confidence="0.9998982">
Avg. F Avg. Accuracy
sticks 34.73 64.89
prodrules 33.69 63.55
binary-lexical 31.79 60.42
word-pairs 29.46 57.22
</table>
<tableCaption confidence="0.864159">
Table 3: F-scores and average accuracies of pro-
duction rules and production sticks.
</tableCaption>
<bodyText confidence="0.999579444444445">
Sticks have almost twice the rate of occurrence of
that of full production rules. Both syntactic rep-
resentations have much larger rate of occurrence
than lexical features, and the rate of occurrence of
word pairs is more than twice smaller than that of
the binary lexical representation.
Next, in Table 3, we give binary classifica-
tion prediction results based on both full rules
and sticks. The first two rows of Table 3 com-
pare full production rules (prodrules) with produc-
tion sticks (sticks) using the binary representation.
They both outperform the binary lexical represen-
tation. Again our results confirm that the better
performance of production rule features is partly
because they are less sparse than lexical represen-
tations, with an average of 1.04% F-score increase.
Individually the F scores of 6 of the 7 relations are
improved as shown in Table 8.
</bodyText>
<sectionHeader confidence="0.963368" genericHeader="method">
6 How important are lexical features?
</sectionHeader>
<bodyText confidence="0.999732666666667">
Production rules or sticks include lexical items
with their part-of-speech tags. These are the sub-
set of features that contribute most to sparsity is-
sues. In this section we test if these lexical fea-
tures contribute to the performance or if they can
be removed without noticeable degradation due to
its intrinsic sparsity. It turns out that it is not ad-
visable to remove the lexical features entirely, as
performance decreases substantially if we do so.
</bodyText>
<subsectionHeader confidence="0.994979">
6.1 Classification without lexical items
</subsectionHeader>
<bodyText confidence="0.9994384">
We start our exploration of the influence of lexical
items on the accuracy of prediction by inspecting
the performances of the classifiers with production
rules and sticks, but without the lexical items and
their parts of speech. Table 4 lists the average F
</bodyText>
<page confidence="0.983291">
202
</page>
<table confidence="0.9997514">
Avg. F Avg. Accuracy
prodrules 33.69 63.55
sticks 34.73 64.89
prodrules-nolex 32.30 62.03
sticks-nolex 33.86 63.99
</table>
<tableCaption confidence="0.873335666666667">
Table 4: F-scores and average accuracies of pro-
duction rules and sticks, with (rows 1-2) and with-
out (rows 3-4) lexical items.
</tableCaption>
<table confidence="0.9999252">
# Features Rate of Occurrence
prodrules 16,173 0.00374
sticks 14,165 0.00623
prodrules-nolex 3470 0.00902
sticks-nolex 922 0.0619
</table>
<tableCaption confidence="0.904969666666667">
Table 5: Number of features and rate of occur-
rence for production rules and sticks, with (rows
1-2) and without (rows 3-4) lexical items.
</tableCaption>
<bodyText confidence="0.998002107142857">
scores and accuracies. Table 8 provides detailed
results for individual relations. Here prodrules-
nolex and sticks-nolex denote full production rules
without lexical items, and production sticks with-
out lexical items, respectively. In all but two re-
lations, lexical items contribute to better classifier
performance.
When lexical items are not included in the rep-
resentation, the number of features is reduced to
fewer than 30% of that in the original full produc-
tion rules. At the same time however, including
the lexical items in the representation improves
performance even more than introducing the less
sparse production stick representation. Production
sticks with lexical information also perform bet-
ter than the same representation without the POS-
word sticks.
The number of features and their rates of occur-
rences are listed in Table 5. It again confirms that
the less sparse stick representation leads to better
classifier performance. Not surprisingly, purely
syntactic features (without the lexical items) are
much less sparse than syntax features with lexical
items present. However the classifier performance
is worse without the lexical features. This contrast
highlights the importance of a reasonable tradeoff
between attempts to reduce sparsity and the need
to preserve lexical features.
</bodyText>
<subsectionHeader confidence="0.99878">
6.2 Feature selection
</subsectionHeader>
<bodyText confidence="0.9996912">
So far our discussion was based on the behavior
of models trained on a complete set of relatively
frequent syntactic and lexical features (occurring
more than five times in the training data). Feature
selection is a way to reasonably prune out the set
</bodyText>
<table confidence="0.999385">
Relation %-nonlex %-allfeats
Temporal 25.56 10.95
Comparison 25.40 15.51
Contingency 20.12 25.05
Conjunction 21.15 19.20
Instantiation 25.08 16.16
Restatement 22.16 17.35
Expansion 18.36 18.66
</table>
<bodyText confidence="0.969349333333333">
Table 6: Non-lexical features selected using fea-
ture selection. %-nonlex records the percentage of
non-lexical features among all features selected;
%-allfeats records the percentage of selected non-
lexical features among all non-lexical features.
and reduce sparsity issues in the model. In fact
feature selection has been used in the majority of
prior work (Pitler et al., 2009; Lin et al., 2009;
Park and Cardie, 2012).
Here we perform feature selection and exam-
ine the proportion of syntactic and lexical features
among the most informative features. We use the
x2 test of independence, computed on the follow-
ing contingency table for each feature Fi and for
each relation Rj:
</bodyText>
<equation confidence="0.8650805">
Fi ∧ Rj |Fi ∧ ¬Rj
¬Fi ∧ Rj |¬Fi ∧ ¬Rj
</equation>
<bodyText confidence="0.99693532">
Each cell in the above table records the num-
ber of training instances in which Fi and Rj are
present or absent. We set our level of confidence
top &lt; 0.1.
Table 6 lists the proportions of non-lexical items
among the most informative features selected (col-
umn 2). It also lists the percentage of selected non-
lexical items among all the 922 purely syntactic
features from production rule and production stick
representations (column 3). For all relations, at
most about a quarter of the most informative fea-
tures are non-lexical and they only take up 10%-
25% of all possible non-lexical features. The pre-
diction results using only these features are either
higher than or comparable to that without feature
selection (sticks-x2 in Table 8). These numbers
suggest that lexical terms play a significant role as
part of the syntactic representations.
In Table 8 we record the F scores and accura-
cies for each relation under each feature represen-
tation. The representations are sorted according to
descending F scores for each relation. Notice that
x2 feature selection on sticks is the best represen-
tation for the three smallest relations: Compari-
son, Instantiation and Temporal.
</bodyText>
<page confidence="0.997624">
203
</page>
<bodyText confidence="0.99969025">
This finding led us to look into the selected lex-
ical features for these three classes. We found that
these most prominent features in fact capture some
semantic information. We list the top ten most pre-
dictive lexical features for these three relations be-
low, with examples. Somewhat disturbingly, many
of them are style or domain specific to the Wall
Street Journal that PDTB was built on.
</bodyText>
<equation confidence="0.600333333333333">
Comparison a1a2 NN share a1a2 NNS cents a1a2 CC or
a1a2 CD million a1a2 QP $ a1a2 NP $ a2 RB n’t
a1a2 NN % a2 JJ year a2 IN of
</equation>
<bodyText confidence="0.960217865384616">
For Comparison (contrast), the top lexical fea-
tures are words that occur in both argument 1 and
argument 2. Contrast within the financial domain,
such as “share”, “cents” and numbers between ar-
guments are captured by these features. Consider
the following example:
Ex. Analyst estimate the value of the BellSouth proposal at
about $115 to $125 a share. [Implicit=AND] They value
McCaw’s bid at $112 to $118 a share .
Here the contrast clearly happens with the value
estimation for two different parties.
Instantiation a2 SINV “ a2 SINV , a2 SINV ” a2 SINV .
a1 DT some a2 S a2 VBZ says a1 NP , a2 NP , a1 DT a
For Instantiation (arg2 gives an example of
arg1), besides words such as “some” or “a” that
sometimes mark a set of events, many attribution
features are selected. it turns out many Instanti-
ation instances in the PDTB involve argument 2
being an inverted declarative sentence that signals
a quote as illustrate by the following example:
Ex. Unease is widespread among exchange members. [Im-
plicit=FOR EXAMPLE] “ I can’t think of any reason to
join Lloyd’s now, ” says Keith Whitten, a British business-
man and a Lloyd’s member since 1979.
Temporal a1 VBD plunged a2 VBZ is a2 RB later
a1 VBD was a2 VBD responded a1a2 PRP he
a1 WRB when a1 PRP he a1 VBZ is a2 VBP are
For Temporal, verbs like plunge and responded
are selected. Words such as plunged are quite do-
main specific to stock markets, but words such as
later and responded are likely more general indi-
cators of the relation.
The presence of pronouns was also a predictive
feature. Consider the following example:
Ex. A Yale law school graduate, he began his career in cor-
porate law and then put in years at Metromedia Inc. and the
William Morris talent agency. [Implicit=THEN] In 1976, he
joined CBS Sports to head business affairs and, five years
later, became its president.
Overall, it is fairly easy to see that certain se-
mantic information was captured by these fea-
tures, such as similar structures in a pair of sen-
tences holding a contrast relation, the use of verbs
in a Temporal relation. However, it is rather unset-
tling to also see that some of these characteristics
are largely style or domain specific. For exam-
ple, for an Instantiation in an educational scenario
where the tutor provides an example for a concept,
it is highly unlikely that attribution features will be
helpful. Therefore, part of the question of finding
a general class of features that carry over to other
styles or domains of text still remain unanswered.
</bodyText>
<sectionHeader confidence="0.98713" genericHeader="method">
7 Per-relation evaluation
</sectionHeader>
<bodyText confidence="0.999664166666666">
Table 8 lists the F-scores and accuracies of each
representation mentioned in this work for predict-
ing individual relation classes. For each relation,
the representations are ordered by decreasing F-
score. We tested the results for statistical signifi-
cance of the change in F-score. We compare all
the representations with the best and the worse
representations for the relation. A “Y” marks a
significance level of p ≤ 0.05 for the comparison
with the best or worst representation, a “T” marks
a significance level of p ≤ 0.1, which means a
tendency towards significance.
For all relations, production sticks, either with
or without feature selection, is the top represen-
tation. Sticks without lexical items also under-
perform those including the lexical items for 6 of
the 7 relations. Notably, production rules without
lexical items are among the three worst represen-
tations, outperforming only the pure lexical fea-
tures in some cases. This is a strong indication
that being both a sparse syntactic representation
and lacking lexical information, these features are
not favored in this task. Pure lexical features give
the worst or second to worst F scores, significantly
worse than the alternatives in most of the cases.
In Table 7 we list the binary classification re-
sults from prior work: feature selected word pairs
(Pitler et al., 2009), aggregated word pairs (Biran
and McKeown, 2013), production rules only (Park
and Cardie, 2012), and the best combination pos-
sible from a variety of features (Park and Cardie,
2012), all of which include production rules. We
aim to compare the relative gains in performance
with different representations. Note that the abso-
lute results from prior work are not exactly com-
parable to ours for two reasons — the training
</bodyText>
<page confidence="0.997349">
204
</page>
<table confidence="0.973124199999999">
Sys. Pitler et al. Biran-McKeown
Feat. wordpair-implicit aggregated wp
Comp. 20.96 (42.55) 24.38 (61.72)
Cont. 43.79 (61.92) 44.03 (66.78)
Expa. 63.84 (60.28) 66.48 (60.93)
Temp. 16.21 (61.98) 19.54 (68.09)
Sys. Park-Cardie Park-Cardie
Feat. prodrules best combination
Comp. 30.04 (75.84) 31.32 (74.66)
Cont. 47.80 (71.90) 49.82 (72.09)
Expa. 77.64 (69.60) 79.22 (69.14)
Temp. 20.96 (63.36) 26.57 (79.32)
Representation F (A) sig- sig-
best worst
Comparison
sticks-x2 27.78 (62.83) N/A Y
prodrules 27.65 (59.5) - Y
sticks 27.50 (60.73) - Y
sticks-nolex 27.01 (59.63) - Y
prodrules-nolex 26.40 (58.47) T Y
binary-lexical 24.73 (58.32) Y -
word-pairs 22.68 (45.03) Y N/A
Conjunction
sticks 27.55 (63.82) N/A T
sticks-x2 27.53 (64.06) - T
prodrules 27.02 (63.91) - -
sticks-nolex 26.56 (61.03) T -
binary-lexical 25.90 (61.77) Y -
prodrules-nolex 25.20 (62.83) T N/A
word-pairs 25.18 (74.51) T -
Contingency
sticks 48.90 (67.49) N/A Y
sticks-x2 48.55 (67.76) - Y
sticks-nolex 48.08 (67.69) - Y
prodrules 47.14 (65.61) T Y
prodrules-nolex 45.79 (63.99) Y Y
binary-lexical 44.17 (62.68) Y Y
word-pairs 40.57 (50.53) Y N/A
Expansion
sticks 56.48 (61.75) N/A Y
sticks-x2 56.30 (62.26) - Y
sticks-nolex 55.43 (60.56) - Y
prodrules 55.42 (61.05) - Y
binary-lexical 54.20 (59.26) Y -
word-pairs 53.65 (56.64) Y -
prodrules-nolex 53.53 (58.79) Y N/A
Instantiation
sticks-x2 30.34 (74.54) N/A Y
sticks 29.93 (73.80) - Y
prodrules 29.59 (72.20) - Y
sticks-nolex 28.22 (72.66) Y Y
prodrules-nolex 27.83 (70.72) Y Y
binary-lexical 27.29 (70.05) Y Y
word-pairs 20.22 (51.00) Y N/A
Restatement
sticks 35.74 (61.45) N/A Y
sticks-x2 34.93 (61.42) - Y
sticks-nolex 34.62 (61.08) T Y
prodrules 33.52 (58.54) T Y
prodrules-nolex 32.05 (56.84) Y -
binary-lexical 31.27 (57.41) Y T
word-pairs 29.81 (47.42) Y N/A
Temporal
sticks-x2 17.97 (66.67) N/A Y
sticks-nolex 17.08 (65.27) T Y
sticks 17.04 (65.22) T Y
prodrules 15.51 (64.04) Y -
prodrules-nolex 15.29 (62.56) Y -
binary-lexical 14.97 (61.92) Y -
word-pairs 14.10 (75.38) Y N/A
</table>
<tableCaption confidence="0.976909">
Table 8: F-score (accuracy) of each relation for
</tableCaption>
<bodyText confidence="0.987702633333333">
each feature representation. The representations
in each relation are sorted in descending order.
The column “sig-best” marks the significance test
result against the best representation, the col-
umn “sig-worst” marks the significance test re-
sult against the worst representation. “Y” denotes
p &lt; 0.05, “T” denotes p &lt; 0.1.
Table 7: F-score (accuracy) of prior systems. Note
that the absolute numbers are not exactly compa-
rable with ours because of the important reasons
explained in this section.
and testing sets are different; how Expansion, En-
tRel/NoRel and AltLex relations are treated differ-
ently in each work. The only meaningful indicator
here is the absolute size of improvement. The table
shows that our introduction of production sticks
led to improvements comparable to those reported
in prior work.
The aggregated word pair is a less sparse ver-
sion of the word pair features, where each pair
is converted into weights associated with an ex-
plicit connective. Just as the less sparse binary
lexical representation presented previously, the ag-
gregated word pairs also gave better performance.
None of the three lexical features, however, sur-
passes raw production rules, which again echoes
our finding that binary lexical features are not bet-
ter than the full production rules. Finally, we
note that a combination of features gives better F-
scores.
</bodyText>
<listItem confidence="0.404081">
8 Discussion: are the features
complementary?
</listItem>
<bodyText confidence="0.999481538461538">
So far we have discussed how different represen-
tations for lexical and syntactic features can af-
fect the classifier performances. We focused on
the dilemma of how to reduce sparsity while still
preserving the useful lexical features. An impor-
tant question remains as whether these representa-
tions are complementary, that is, how different is
the classifier behaving under different feature sets
and if it makes sense to combine the features.
We compare the classifier output on the test data
with two methods in Table 9: the Q-statistic and
the percentage of data which the two classifiers
disagree (Kuncheva and Whitaker, 2003).
</bodyText>
<page confidence="0.995347">
205
</page>
<bodyText confidence="0.998826">
Q-statistic is a measure of agreement between
two systems s1 and s2 formulated as follows:
</bodyText>
<equation confidence="0.9913485">
__ N11 N00 − N01 N10
Qs1,s2 N11N00 + N01N10
</equation>
<bodyText confidence="0.999982307692308">
Where N denotes the number of instances, a sub-
script 1 on the left means s1 is correct, and a sub-
script 1 on the right means s2 is correct.
There are several rather surprising findings.
Most notably, word pairs and binary lexical repre-
sentations give very different classification results
in each relation. Their predictions disagree on at
least 25% of the data. This finding drastically con-
trast the fact that they are both lexical features and
that they both make use of the argument annota-
tions in the PDTB. A comparison of the percent-
ages and their differences in F scores or accuracies
easily shows that it is not the case that binary lex-
ical models correctly predict instances word pairs
made mistakes on, but that they are disagreeing in
both ways. Thus, given the previous discussion
that lexical items are useful, it is possible the most
suitable representation would combine both views
of lexical distribution.
Even more surprisingly, the difference in classi-
fier behavior is not as big when we compare lex-
ical and syntactic representations. The disagree-
ment of production sticks with and without lexi-
cal features are the smallest, even though, as we
have shown previously, the majority of production
sticks are lexical features with part-of-speech tags.
If we compare binary lexical features with produc-
tion sticks, the disagreement becomes bigger, but
still not as big as word pairs vs. binary lexical.
Besides the differences in classification, the big-
ger picture of improving implicit discourse rela-
tion classification is finding a set of feature repre-
sentations that are able to complement each other
to improve the classification. A direct conclusion
here is that one should not limit the focus on fea-
tures in different categories (for example, lexical
or syntax), but also features in the same category
represented differently (for example, word pairs or
binary lexical).
</bodyText>
<sectionHeader confidence="0.997932" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999797666666667">
In this work we study implicit discourse relation
classification from the perspective of the interplay
between lexical and syntactic feature representa-
tion. We are particularly interested in the trade-
off between reducing sparsity and preserving lex-
ical features. We first emphasize the important
</bodyText>
<table confidence="0.997028848484849">
Rel. Q-stat Disagreement
word-pairs vs. binary-lexical
Comparison 0.65 33.55
Conjunction 0.71 28.47
Contingency 0.81 26.35
Expansion 0.69 29.38
Instantiation 0.75 31.33
Restatement 0.76 28.42
Temporal 0.25 25.34
binary-lexical vs. sticks
Comparison 0.78 25.49
Conjunction 0.78 24.67
Contingency 0.86 20.68
Expansion 0.80 24.28
Instantiation 0.83 20.75
Restatement 0.76 26.72
Temporal 0.86 20.61
sticks vs. prodrules
Comparison 0.88 19.77
Conjunction 0.89 18.43
Contingency 0.94 14.00
Expansion 0.88 19.18
Instantiation 0.90 16.34
Restatement 0.89 18.88
Temporal 0.90 17.94
sticks vs. sticks-nolex
Comparison 0.94 14.61
Conjunction 0.92 16.63
Contingency 0.97 10.16
Expansion 0.91 17.35
Instantiation 0.97 9.51
Restatement 0.97 11.26
Temporal 0.98 8.42
</table>
<tableCaption confidence="0.95334">
Table 9: Q statistic and disagreement of different
classes of representations
</tableCaption>
<bodyText confidence="0.9999139375">
role of sparsity for traditional word-pair represen-
tations and how a less sparse representation could
improve performance. Then we proposed a less
sparse feature representation for production rules,
the best feature category so far, that further im-
proves classification. We study the role of lexical
features and show the contrast between the spar-
sity problem they brought along and their domi-
nant presence in the highly ranked features. Also,
lexical features included in syntactic features that
are most informative to the classifiers are found to
be style or domain specific in certain relations. Fi-
nally, we compare the representations in terms of
classifier disagreement and showed that within the
same feature category different feature representa-
tion can also be complementary with each other.
</bodyText>
<sectionHeader confidence="0.99962" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9956396">
Or Biran and Kathleen McKeown. 2013. Aggregated
word pair features for implicit discourse relation dis-
ambiguation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL): Short Papers, pages 69–73.
</reference>
<page confidence="0.981476">
206
</page>
<reference confidence="0.99976490625">
Sasha Blair-Goldensohn, Kathleen McKeown, and
Owen Rambow. 2007. Building and refining
rhetorical-semantic relation models. In Human Lan-
guage Technologies: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL), pages 428–435.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2010. A semi-supervised approach to im-
prove classification of infrequent discourse relations
using feature vector extension. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 399–409.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in
kernel methods, pages 169–184.
Ludmila I. Kuncheva and Christopher J. Whitaker.
2003. Measures of diversity in classifier ensembles
and their relationship with the ensemble accuracy.
Machine Learning, 51(2):181–207, May.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 343–351.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20:151–184, 4.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics
(ACL), pages 368–375.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics - Special issue on using large
corpora, 19(2):313–330.
Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature
set optimization. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue (SIGDIAL), pages 108–112.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP (ACL-IJCNLP),
pages 683–691.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC).
Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetori-
cal relations: An assessment. Natural Language En-
gineering, 14(3):369–416, July.
Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian
Su, and Chew Lim Tan. 2010. Predicting discourse
connectives for implicit discourse relation recogni-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (COLING),
pages 1507–1514.
</reference>
<page confidence="0.997997">
207
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.774758">
<title confidence="0.99903">Reducing Sparsity Improves the Recognition of Implicit Discourse Relations</title>
<author confidence="0.9951">Junyi Jessy</author>
<affiliation confidence="0.998786">University of</affiliation>
<email confidence="0.998241">ljunyi@seas.upenn.edu</email>
<author confidence="0.791403">Ani</author>
<affiliation confidence="0.997522">University of</affiliation>
<email confidence="0.997364">nenkova@seas.upenn.edu</email>
<abstract confidence="0.99963328">The earliest work on automatic detection of implicit discourse relations relied on lexical features. More recently, researchers have demonstrated that syntactic features are superior to lexical features for the task. In this paper we re-examine the two classes of state of the art representations: syntactic production rules and word pair features. In particular, we focus on the need to reduce sparsity in instance representation, demonstrating that different representation choices even for the same class of features may exacerbate sparsity issues and reduce performance. We present results that clearly reveal that lexicalization of the syntactic features is necessary for good performance. We introduce a novel, less sparse, syntactic representation which leads to improvement in discourse relation recognition. Finally, we demonstrate that classifiers trained on different representations, especially lexical ones, behave rather differently and thus could likely be combined in future systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Kathleen McKeown</author>
</authors>
<title>Aggregated word pair features for implicit discourse relation disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL): Short Papers,</booktitle>
<pages>69--73</pages>
<contexts>
<context position="10545" citStr="Biran and McKeown (2013)" startWordPosition="1651" endWordPosition="1654">he first and second argument) in particular, will be powerful predictors of discourse relations (Marcu and Echihabi, 2002; Blair-Goldensohn et al., 2007). The intuition behind word pairs was that semantic relations between the lexical items, such as drought–famine, child–adult, may in turn signal causal or contrast discourse relations. Later it has been shown that word pair features do not appear to capture such semantic relationship between words (Pitler et al., 2009) and that syntactic features lead to higher accuracies (Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012). Recently, Biran and McKeown (2013) aggregated word pair features with explicit connectives and reported improvements over the original word pairs as features. In this section, we show that the representation of lexical features play a direct role in feature sparsity and ultimately affects prediction performance. The first two studies that specifically addressed 3We also did not include features that occurred less than 5 times in the training set. # Features Avg. F Avg. Accuracy word-pairs 92128 29.46 57.22 binary-lexical 12116 31.79 60.42 Table 1: F-scores and average accuracies of paired and binary representations of words. t</context>
<context position="26450" citStr="Biran and McKeown, 2013" startWordPosition="4269" endWordPosition="4272">f the 7 relations. Notably, production rules without lexical items are among the three worst representations, outperforming only the pure lexical features in some cases. This is a strong indication that being both a sparse syntactic representation and lacking lexical information, these features are not favored in this task. Pure lexical features give the worst or second to worst F scores, significantly worse than the alternatives in most of the cases. In Table 7 we list the binary classification results from prior work: feature selected word pairs (Pitler et al., 2009), aggregated word pairs (Biran and McKeown, 2013), production rules only (Park and Cardie, 2012), and the best combination possible from a variety of features (Park and Cardie, 2012), all of which include production rules. We aim to compare the relative gains in performance with different representations. Note that the absolute results from prior work are not exactly comparable to ours for two reasons — the training 204 Sys. Pitler et al. Biran-McKeown Feat. wordpair-implicit aggregated wp Comp. 20.96 (42.55) 24.38 (61.72) Cont. 43.79 (61.92) 44.03 (66.78) Expa. 63.84 (60.28) 66.48 (60.93) Temp. 16.21 (61.98) 19.54 (68.09) Sys. Park-Cardie P</context>
</contexts>
<marker>Biran, McKeown, 2013</marker>
<rawString>Or Biran and Kathleen McKeown. 2013. Aggregated word pair features for implicit discourse relation disambiguation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL): Short Papers, pages 69–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasha Blair-Goldensohn</author>
<author>Kathleen McKeown</author>
<author>Owen Rambow</author>
</authors>
<title>Building and refining rhetorical-semantic relation models.</title>
<date>2007</date>
<booktitle>In Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>428--435</pages>
<contexts>
<context position="10074" citStr="Blair-Goldensohn et al., 2007" startWordPosition="1574" endWordPosition="1577">partitioned into ten groups. In each group, the relation distribution was kept as close as possible to the overall test set. 4 Sparsity and pure lexical representations By far the most common features used for representing implicit discourse relations are lexical (Sporleder and Lascarides, 2008; Pitler et al., 2009; Lin et al., 2009; Hernault et al., 2010; Park and Cardie, 2012). Early studies have suggested that lexical features, word pairs (crossproduct of the words in the first and second argument) in particular, will be powerful predictors of discourse relations (Marcu and Echihabi, 2002; Blair-Goldensohn et al., 2007). The intuition behind word pairs was that semantic relations between the lexical items, such as drought–famine, child–adult, may in turn signal causal or contrast discourse relations. Later it has been shown that word pair features do not appear to capture such semantic relationship between words (Pitler et al., 2009) and that syntactic features lead to higher accuracies (Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012). Recently, Biran and McKeown (2013) aggregated word pair features with explicit connectives and reported improvements over the original word pairs as features. In t</context>
</contexts>
<marker>Blair-Goldensohn, McKeown, Rambow, 2007</marker>
<rawString>Sasha Blair-Goldensohn, Kathleen McKeown, and Owen Rambow. 2007. Building and refining rhetorical-semantic relation models. In Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 428–435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Hernault</author>
<author>Danushka Bollegala</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>A semi-supervised approach to improve classification of infrequent discourse relations using feature vector extension.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>399--409</pages>
<contexts>
<context position="9801" citStr="Hernault et al., 2010" startWordPosition="1531" endWordPosition="1534">e of the discourse relations are much larger than others, so we report our results in term of Fmeasure for each relation and average unweighted accuracy. Significance tests over F scores were carried out using a paired t-test. To do this, the test set is randomly partitioned into ten groups. In each group, the relation distribution was kept as close as possible to the overall test set. 4 Sparsity and pure lexical representations By far the most common features used for representing implicit discourse relations are lexical (Sporleder and Lascarides, 2008; Pitler et al., 2009; Lin et al., 2009; Hernault et al., 2010; Park and Cardie, 2012). Early studies have suggested that lexical features, word pairs (crossproduct of the words in the first and second argument) in particular, will be powerful predictors of discourse relations (Marcu and Echihabi, 2002; Blair-Goldensohn et al., 2007). The intuition behind word pairs was that semantic relations between the lexical items, such as drought–famine, child–adult, may in turn signal causal or contrast discourse relations. Later it has been shown that word pair features do not appear to capture such semantic relationship between words (Pitler et al., 2009) and th</context>
</contexts>
<marker>Hernault, Bollegala, Ishizuka, 2010</marker>
<rawString>Hugo Hernault, Danushka Bollegala, and Mitsuru Ishizuka. 2010. A semi-supervised approach to improve classification of infrequent discourse relations using feature vector extension. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 399–409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<date>1999</date>
<booktitle>In Advances in kernel methods,</booktitle>
<pages>169--184</pages>
<contexts>
<context position="8573" citStr="Joachims, 1999" startWordPosition="1328" endWordPosition="1329">entation have on sparsity and classifier performance. We use gold-standard parses from the original Penn Treebank for syntax features. To ensure that our conclusions are based on analysis of the most common relations, we train binary SVM classifiers2 for the seven relations described above. We adopt the standard practice in 1All other sub-classes of implicit relations are too small for general practical applications. For example the Alternative class and Concession class have only 185 and 228 occurrences, respectively, in the 16,224 implicit relation annotations of the PDTB. 2We use SVMLight (Joachims, 1999) with linear kernel. 200 prior work and downsampled the negative class so the number of positive and negative samples are equal in the training set.3 Our training set consists of PDTB sections 2- 19. The testing set consists of sections 20-24. Like most studies, we do not include sections 0-1 in the training set. We expanded the test set (sections 23 or 23-24) used in previous work (Lin et al., 2014; Park and Cardie, 2012) to ensure the number of examples of the smaller relations, particularly of Temporal or Instantiation, are suitable for carrying out reliable tests for statistical significan</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale support vector machine learning practical. In Advances in kernel methods, pages 169–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludmila I Kuncheva</author>
<author>Christopher J Whitaker</author>
</authors>
<title>Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy.</title>
<date>2003</date>
<booktitle>Machine Learning,</booktitle>
<volume>51</volume>
<issue>2</issue>
<contexts>
<context position="30912" citStr="Kuncheva and Whitaker, 2003" startWordPosition="4977" endWordPosition="4980">So far we have discussed how different representations for lexical and syntactic features can affect the classifier performances. We focused on the dilemma of how to reduce sparsity while still preserving the useful lexical features. An important question remains as whether these representations are complementary, that is, how different is the classifier behaving under different feature sets and if it makes sense to combine the features. We compare the classifier output on the test data with two methods in Table 9: the Q-statistic and the percentage of data which the two classifiers disagree (Kuncheva and Whitaker, 2003). 205 Q-statistic is a measure of agreement between two systems s1 and s2 formulated as follows: __ N11 N00 − N01 N10 Qs1,s2 N11N00 + N01N10 Where N denotes the number of instances, a subscript 1 on the left means s1 is correct, and a subscript 1 on the right means s2 is correct. There are several rather surprising findings. Most notably, word pairs and binary lexical representations give very different classification results in each relation. Their predictions disagree on at least 25% of the data. This finding drastically contrast the fact that they are both lexical features and that they bot</context>
</contexts>
<marker>Kuncheva, Whitaker, 2003</marker>
<rawString>Ludmila I. Kuncheva and Christopher J. Whitaker. 2003. Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy. Machine Learning, 51(2):181–207, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Min-Yen Kan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Recognizing implicit discourse relations in the Penn Discourse Treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>343--351</pages>
<contexts>
<context position="7831" citStr="Lin et al., 2009" startWordPosition="1207" endWordPosition="1210">e Contingency and Comparison classes, with thousands of examples in the PDTB.1 We show the distribution of the classes below: Temporal 1038 Comparison 2550 Contingency 4532 Instantiation 1483 Restatement 3271 Conjunction 3646 EntRel/NoRel 5464 3 Experimental settings In our experiments we use only lexical and syntactic features. This choice is motivated by the fact that lexical features have been used most widely for the task and that recent work has demonstrated that syntactic features are the single best type of representation. Adding additional features only minimally improves performance (Lin et al., 2009). By zeroing in only on these classes of features we are able to discuss more clearly the impact that different instance representation have on sparsity and classifier performance. We use gold-standard parses from the original Penn Treebank for syntax features. To ensure that our conclusions are based on analysis of the most common relations, we train binary SVM classifiers2 for the seven relations described above. We adopt the standard practice in 1All other sub-classes of implicit relations are too small for general practical applications. For example the Alternative class and Concession cla</context>
<context position="9778" citStr="Lin et al., 2009" startWordPosition="1527" endWordPosition="1530"> significance. Some of the discourse relations are much larger than others, so we report our results in term of Fmeasure for each relation and average unweighted accuracy. Significance tests over F scores were carried out using a paired t-test. To do this, the test set is randomly partitioned into ten groups. In each group, the relation distribution was kept as close as possible to the overall test set. 4 Sparsity and pure lexical representations By far the most common features used for representing implicit discourse relations are lexical (Sporleder and Lascarides, 2008; Pitler et al., 2009; Lin et al., 2009; Hernault et al., 2010; Park and Cardie, 2012). Early studies have suggested that lexical features, word pairs (crossproduct of the words in the first and second argument) in particular, will be powerful predictors of discourse relations (Marcu and Echihabi, 2002; Blair-Goldensohn et al., 2007). The intuition behind word pairs was that semantic relations between the lexical items, such as drought–famine, child–adult, may in turn signal causal or contrast discourse relations. Later it has been shown that word pair features do not appear to capture such semantic relationship between words (Pitl</context>
<context position="11645" citStr="Lin et al. (2009)" startWordPosition="1837" endWordPosition="1840">y-lexical 12116 31.79 60.42 Table 1: F-scores and average accuracies of paired and binary representations of words. the problem of predicting implicit discourse relations in the PDTB made use of very different instance representations. Pitler et al. (2009) represent instances of discourse relations in a vector space defined by word pairs, i.e. the crossproduct of the words that appear in the two arguments of the relation. There, features are of the form (w1, w2) where w1 ∈ arg1 and w2 ∈ arg2. If there are N words in the entire vocabulary, the size of each instance would be N × N. In contrast, Lin et al. (2009) represent instances by tracking the occurrences of grammatical productions in the syntactic parse of argument spans. There are three indicator features associated with each production: whether the production appears in arg1, in arg2, and in both arguments. For a grammar with N production rules, the size of the vector representing an instance will be 3N. For convenience we call this “binary representation”, in contrast to the word-pair features in which the cross product of words constitute the representation. Note that the cross-product approach has been extended to a wide variety of features</context>
<context position="13187" citStr="Lin et al. (2009)" startWordPosition="2087" endWordPosition="2090"> not only from the inclusion of syntactic information but also from the less sparse instance representation they used for syntactic features. In Table 1 we show the number of features for each representation and the average F score and accuracy for word pairs and words with binary representation (binary-lexical). The results for each relation are shown in Table 8 and discussed in Section 7. Using binary representation for lexical information outperforms word pairs. Thus, the difference in how lexical information is represented accounts for a considerable portion of the improvement reported in Lin et al. (2009). Most notably, for the Instantiation class, we see a 7.7% increase in Fscore. On average, the less sparse representation 201 translates into 2.34% absolute improvement in Fscore and 3.2% absolute improvement in accuracy. From this point on we adopt the binary representation for the features discussed. 5 Sparsity and syntactic features Grammatical production rules were first used for discourse relation representation in Lin et al. (2009). They were identified as the most suitable representation, that lead to highest performance in a couple of independent studies (Lin et al., 2009; Park and Car</context>
<context position="20507" citStr="Lin et al., 2009" startWordPosition="3245" endWordPosition="3248">asonably prune out the set Relation %-nonlex %-allfeats Temporal 25.56 10.95 Comparison 25.40 15.51 Contingency 20.12 25.05 Conjunction 21.15 19.20 Instantiation 25.08 16.16 Restatement 22.16 17.35 Expansion 18.36 18.66 Table 6: Non-lexical features selected using feature selection. %-nonlex records the percentage of non-lexical features among all features selected; %-allfeats records the percentage of selected nonlexical features among all non-lexical features. and reduce sparsity issues in the model. In fact feature selection has been used in the majority of prior work (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012). Here we perform feature selection and examine the proportion of syntactic and lexical features among the most informative features. We use the x2 test of independence, computed on the following contingency table for each feature Fi and for each relation Rj: Fi ∧ Rj |Fi ∧ ¬Rj ¬Fi ∧ Rj |¬Fi ∧ ¬Rj Each cell in the above table records the number of training instances in which Fi and Rj are present or absent. We set our level of confidence top &lt; 0.1. Table 6 lists the proportions of non-lexical items among the most informative features selected (column 2). It also lists th</context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the Penn Discourse Treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 343–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A PDTB-styled end-to-end discourse parser.</title>
<date>2014</date>
<journal>Natural Language Engineering,</journal>
<volume>20</volume>
<pages>4</pages>
<contexts>
<context position="8975" citStr="Lin et al., 2014" startWordPosition="1397" endWordPosition="1400">tical applications. For example the Alternative class and Concession class have only 185 and 228 occurrences, respectively, in the 16,224 implicit relation annotations of the PDTB. 2We use SVMLight (Joachims, 1999) with linear kernel. 200 prior work and downsampled the negative class so the number of positive and negative samples are equal in the training set.3 Our training set consists of PDTB sections 2- 19. The testing set consists of sections 20-24. Like most studies, we do not include sections 0-1 in the training set. We expanded the test set (sections 23 or 23-24) used in previous work (Lin et al., 2014; Park and Cardie, 2012) to ensure the number of examples of the smaller relations, particularly of Temporal or Instantiation, are suitable for carrying out reliable tests for statistical significance. Some of the discourse relations are much larger than others, so we report our results in term of Fmeasure for each relation and average unweighted accuracy. Significance tests over F scores were carried out using a paired t-test. To do this, the test set is randomly partitioned into ten groups. In each group, the relation distribution was kept as close as possible to the overall test set. 4 Spar</context>
</contexts>
<marker>Lin, Ng, Kan, 2014</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A PDTB-styled end-to-end discourse parser. Natural Language Engineering, 20:151–184, 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<pages>368--375</pages>
<contexts>
<context position="10042" citStr="Marcu and Echihabi, 2002" startWordPosition="1570" endWordPosition="1573"> the test set is randomly partitioned into ten groups. In each group, the relation distribution was kept as close as possible to the overall test set. 4 Sparsity and pure lexical representations By far the most common features used for representing implicit discourse relations are lexical (Sporleder and Lascarides, 2008; Pitler et al., 2009; Lin et al., 2009; Hernault et al., 2010; Park and Cardie, 2012). Early studies have suggested that lexical features, word pairs (crossproduct of the words in the first and second argument) in particular, will be powerful predictors of discourse relations (Marcu and Echihabi, 2002; Blair-Goldensohn et al., 2007). The intuition behind word pairs was that semantic relations between the lexical items, such as drought–famine, child–adult, may in turn signal causal or contrast discourse relations. Later it has been shown that word pair features do not appear to capture such semantic relationship between words (Pitler et al., 2009) and that syntactic features lead to higher accuracies (Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012). Recently, Biran and McKeown (2013) aggregated word pair features with explicit connectives and reported improvements over the origi</context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>Daniel Marcu and Abdessamad Echihabi. 2002. An unsupervised approach to recognizing discourse relations. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL), pages 368–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics - Special issue on using large corpora,</title>
<date>1993</date>
<contexts>
<context position="5197" citStr="Marcus et al., 1993" startWordPosition="787" endWordPosition="790">res however appear complementary to each other, with markedly higher fraction of instances recognized correctly by only one of the models. Our work advances the state of the art in implicit discourse recognition by clarifying the extent to which sparsity issues influence predictions, by introducing a strong syntactic representation and by documenting the need for further more complex integration of lexical information. 2 The Penn Discourse Treebank The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) contains annotations for five types of discourse relations over the Penn Treebank corpus (Marcus et al., 1993). Explicit relations are those signaled by a discourse connective that occurs in the text, such as “because”, “however”, “for example”. Implicit relations are annotated between adjacent sentences in the same paragraph. There are no discourse connectives between the two sentences, and the annotators were asked to insert a connective while marking their senses. Some pairs of sentences do not contain one of the explicit discourse connectives, but the insertion of a connective provides redundant information into the text. For example, they may contain phrases such as “the consequence of the act”. </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics - Special issue on using large corpora, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joonsuk Park</author>
<author>Claire Cardie</author>
</authors>
<title>Improving implicit discourse relation recognition through feature set optimization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),</booktitle>
<pages>108--112</pages>
<contexts>
<context position="8999" citStr="Park and Cardie, 2012" startWordPosition="1401" endWordPosition="1404">. For example the Alternative class and Concession class have only 185 and 228 occurrences, respectively, in the 16,224 implicit relation annotations of the PDTB. 2We use SVMLight (Joachims, 1999) with linear kernel. 200 prior work and downsampled the negative class so the number of positive and negative samples are equal in the training set.3 Our training set consists of PDTB sections 2- 19. The testing set consists of sections 20-24. Like most studies, we do not include sections 0-1 in the training set. We expanded the test set (sections 23 or 23-24) used in previous work (Lin et al., 2014; Park and Cardie, 2012) to ensure the number of examples of the smaller relations, particularly of Temporal or Instantiation, are suitable for carrying out reliable tests for statistical significance. Some of the discourse relations are much larger than others, so we report our results in term of Fmeasure for each relation and average unweighted accuracy. Significance tests over F scores were carried out using a paired t-test. To do this, the test set is randomly partitioned into ten groups. In each group, the relation distribution was kept as close as possible to the overall test set. 4 Sparsity and pure lexical re</context>
<context position="10509" citStr="Park and Cardie, 2012" startWordPosition="1646" endWordPosition="1649">rs (crossproduct of the words in the first and second argument) in particular, will be powerful predictors of discourse relations (Marcu and Echihabi, 2002; Blair-Goldensohn et al., 2007). The intuition behind word pairs was that semantic relations between the lexical items, such as drought–famine, child–adult, may in turn signal causal or contrast discourse relations. Later it has been shown that word pair features do not appear to capture such semantic relationship between words (Pitler et al., 2009) and that syntactic features lead to higher accuracies (Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012). Recently, Biran and McKeown (2013) aggregated word pair features with explicit connectives and reported improvements over the original word pairs as features. In this section, we show that the representation of lexical features play a direct role in feature sparsity and ultimately affects prediction performance. The first two studies that specifically addressed 3We also did not include features that occurred less than 5 times in the training set. # Features Avg. F Avg. Accuracy word-pairs 92128 29.46 57.22 binary-lexical 12116 31.79 60.42 Table 1: F-scores and average accuracies of paired an</context>
<context position="12290" citStr="Park and Cardie, 2012" startWordPosition="1939" endWordPosition="1942"> tracking the occurrences of grammatical productions in the syntactic parse of argument spans. There are three indicator features associated with each production: whether the production appears in arg1, in arg2, and in both arguments. For a grammar with N production rules, the size of the vector representing an instance will be 3N. For convenience we call this “binary representation”, in contrast to the word-pair features in which the cross product of words constitute the representation. Note that the cross-product approach has been extended to a wide variety of features (Pitler et al., 2009; Park and Cardie, 2012). In the experiments that follow we will demonstrate that binary representations lead to less sparse features and higher prediction accuracy. Lin et al. (2009) found that their syntactic features are more powerful than the word pair features. Here we show that the advantage comes not only from the inclusion of syntactic information but also from the less sparse instance representation they used for syntactic features. In Table 1 we show the number of features for each representation and the average F score and accuracy for word pairs and words with binary representation (binary-lexical). The r</context>
<context position="13797" citStr="Park and Cardie, 2012" startWordPosition="2183" endWordPosition="2186">t al. (2009). Most notably, for the Instantiation class, we see a 7.7% increase in Fscore. On average, the less sparse representation 201 translates into 2.34% absolute improvement in Fscore and 3.2% absolute improvement in accuracy. From this point on we adopt the binary representation for the features discussed. 5 Sparsity and syntactic features Grammatical production rules were first used for discourse relation representation in Lin et al. (2009). They were identified as the most suitable representation, that lead to highest performance in a couple of independent studies (Lin et al., 2009; Park and Cardie, 2012). The comparison representations covered a number of semantic classes related to sentiment, polarity and verb information and dependency representations of syntax. Production rules correspond to tree chunks in the constituency parse of a sentence, i.e. a node in the syntactic parse tree with all of its children, which in turn correspond to grammar rules applied in the derivation of the tree, such as S→NP VP. This syntactic representation subsumes lexical representations because of the production rules with part-of-speech on the left-hand side and a lexical item on the right-hand side. We propo</context>
<context position="20531" citStr="Park and Cardie, 2012" startWordPosition="3249" endWordPosition="3252"> the set Relation %-nonlex %-allfeats Temporal 25.56 10.95 Comparison 25.40 15.51 Contingency 20.12 25.05 Conjunction 21.15 19.20 Instantiation 25.08 16.16 Restatement 22.16 17.35 Expansion 18.36 18.66 Table 6: Non-lexical features selected using feature selection. %-nonlex records the percentage of non-lexical features among all features selected; %-allfeats records the percentage of selected nonlexical features among all non-lexical features. and reduce sparsity issues in the model. In fact feature selection has been used in the majority of prior work (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012). Here we perform feature selection and examine the proportion of syntactic and lexical features among the most informative features. We use the x2 test of independence, computed on the following contingency table for each feature Fi and for each relation Rj: Fi ∧ Rj |Fi ∧ ¬Rj ¬Fi ∧ Rj |¬Fi ∧ ¬Rj Each cell in the above table records the number of training instances in which Fi and Rj are present or absent. We set our level of confidence top &lt; 0.1. Table 6 lists the proportions of non-lexical items among the most informative features selected (column 2). It also lists the percentage of selected</context>
<context position="26497" citStr="Park and Cardie, 2012" startWordPosition="4276" endWordPosition="4279">out lexical items are among the three worst representations, outperforming only the pure lexical features in some cases. This is a strong indication that being both a sparse syntactic representation and lacking lexical information, these features are not favored in this task. Pure lexical features give the worst or second to worst F scores, significantly worse than the alternatives in most of the cases. In Table 7 we list the binary classification results from prior work: feature selected word pairs (Pitler et al., 2009), aggregated word pairs (Biran and McKeown, 2013), production rules only (Park and Cardie, 2012), and the best combination possible from a variety of features (Park and Cardie, 2012), all of which include production rules. We aim to compare the relative gains in performance with different representations. Note that the absolute results from prior work are not exactly comparable to ours for two reasons — the training 204 Sys. Pitler et al. Biran-McKeown Feat. wordpair-implicit aggregated wp Comp. 20.96 (42.55) 24.38 (61.72) Cont. 43.79 (61.92) 44.03 (66.78) Expa. 63.84 (60.28) 66.48 (60.93) Temp. 16.21 (61.98) 19.54 (68.09) Sys. Park-Cardie Park-Cardie Feat. prodrules best combination Com</context>
</contexts>
<marker>Park, Cardie, 2012</marker>
<rawString>Joonsuk Park and Claire Cardie. 2012. Improving implicit discourse relation recognition through feature set optimization. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 108–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP),</booktitle>
<pages>683--691</pages>
<contexts>
<context position="9760" citStr="Pitler et al., 2009" startWordPosition="1523" endWordPosition="1526">tests for statistical significance. Some of the discourse relations are much larger than others, so we report our results in term of Fmeasure for each relation and average unweighted accuracy. Significance tests over F scores were carried out using a paired t-test. To do this, the test set is randomly partitioned into ten groups. In each group, the relation distribution was kept as close as possible to the overall test set. 4 Sparsity and pure lexical representations By far the most common features used for representing implicit discourse relations are lexical (Sporleder and Lascarides, 2008; Pitler et al., 2009; Lin et al., 2009; Hernault et al., 2010; Park and Cardie, 2012). Early studies have suggested that lexical features, word pairs (crossproduct of the words in the first and second argument) in particular, will be powerful predictors of discourse relations (Marcu and Echihabi, 2002; Blair-Goldensohn et al., 2007). The intuition behind word pairs was that semantic relations between the lexical items, such as drought–famine, child–adult, may in turn signal causal or contrast discourse relations. Later it has been shown that word pair features do not appear to capture such semantic relationship b</context>
<context position="11284" citStr="Pitler et al. (2009)" startWordPosition="1764" endWordPosition="1767">ures. In this section, we show that the representation of lexical features play a direct role in feature sparsity and ultimately affects prediction performance. The first two studies that specifically addressed 3We also did not include features that occurred less than 5 times in the training set. # Features Avg. F Avg. Accuracy word-pairs 92128 29.46 57.22 binary-lexical 12116 31.79 60.42 Table 1: F-scores and average accuracies of paired and binary representations of words. the problem of predicting implicit discourse relations in the PDTB made use of very different instance representations. Pitler et al. (2009) represent instances of discourse relations in a vector space defined by word pairs, i.e. the crossproduct of the words that appear in the two arguments of the relation. There, features are of the form (w1, w2) where w1 ∈ arg1 and w2 ∈ arg2. If there are N words in the entire vocabulary, the size of each instance would be N × N. In contrast, Lin et al. (2009) represent instances by tracking the occurrences of grammatical productions in the syntactic parse of argument spans. There are three indicator features associated with each production: whether the production appears in arg1, in arg2, and </context>
<context position="20489" citStr="Pitler et al., 2009" startWordPosition="3241" endWordPosition="3244">ection is a way to reasonably prune out the set Relation %-nonlex %-allfeats Temporal 25.56 10.95 Comparison 25.40 15.51 Contingency 20.12 25.05 Conjunction 21.15 19.20 Instantiation 25.08 16.16 Restatement 22.16 17.35 Expansion 18.36 18.66 Table 6: Non-lexical features selected using feature selection. %-nonlex records the percentage of non-lexical features among all features selected; %-allfeats records the percentage of selected nonlexical features among all non-lexical features. and reduce sparsity issues in the model. In fact feature selection has been used in the majority of prior work (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012). Here we perform feature selection and examine the proportion of syntactic and lexical features among the most informative features. We use the x2 test of independence, computed on the following contingency table for each feature Fi and for each relation Rj: Fi ∧ Rj |Fi ∧ ¬Rj ¬Fi ∧ Rj |¬Fi ∧ ¬Rj Each cell in the above table records the number of training instances in which Fi and Rj are present or absent. We set our level of confidence top &lt; 0.1. Table 6 lists the proportions of non-lexical items among the most informative features selected (column 2)</context>
<context position="26401" citStr="Pitler et al., 2009" startWordPosition="4262" endWordPosition="4265">orm those including the lexical items for 6 of the 7 relations. Notably, production rules without lexical items are among the three worst representations, outperforming only the pure lexical features in some cases. This is a strong indication that being both a sparse syntactic representation and lacking lexical information, these features are not favored in this task. Pure lexical features give the worst or second to worst F scores, significantly worse than the alternatives in most of the cases. In Table 7 we list the binary classification results from prior work: feature selected word pairs (Pitler et al., 2009), aggregated word pairs (Biran and McKeown, 2013), production rules only (Park and Cardie, 2012), and the best combination possible from a variety of features (Park and Cardie, 2012), all of which include production rules. We aim to compare the relative gains in performance with different representations. Note that the absolute results from prior work are not exactly comparable to ours for two reasons — the training 204 Sys. Pitler et al. Biran-McKeown Feat. wordpair-implicit aggregated wp Comp. 20.96 (42.55) 24.38 (61.72) Cont. 43.79 (61.92) 44.03 (66.78) Expa. 63.84 (60.28) 66.48 (60.93) Tem</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP), pages 683–691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse TreeBank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="1524" citStr="Prasad et al., 2008" startWordPosition="215" endWordPosition="218"> We introduce a novel, less sparse, syntactic representation which leads to improvement in discourse relation recognition. Finally, we demonstrate that classifiers trained on different representations, especially lexical ones, behave rather differently and thus could likely be combined in future systems. 1 Introduction Implicit discourse relations hold between adjacent sentences in the same paragraph, and are not signaled by any of the common explicit discourse connectives such as because, however, meanwhile, etc. Consider the two examples below, drawn from the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), of a causal and a contrast relation, respectively. The italic and bold fonts mark the arguments of the relation, i.e the portions of the text connected by the discourse relation. Ex1: Mrs Yeargin is lying. [Implicit = BECAUSE] They found students in an advanced class a year earlier who said she gave them similar help. Ex2: Back downtown, the execs squeezed in a few meetings at the hotel before boarding the buses again. [Implicit = BUT] This time, it was for dinner and dancing - a block away. The task is undisputedly hard, partly because it is hard to come up with intuitive feature representa</context>
<context position="5086" citStr="Prasad et al., 2008" startWordPosition="770" endWordPosition="773">instances for which the alternative representations are also correct. Different representation of lexical features however appear complementary to each other, with markedly higher fraction of instances recognized correctly by only one of the models. Our work advances the state of the art in implicit discourse recognition by clarifying the extent to which sparsity issues influence predictions, by introducing a strong syntactic representation and by documenting the need for further more complex integration of lexical information. 2 The Penn Discourse Treebank The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) contains annotations for five types of discourse relations over the Penn Treebank corpus (Marcus et al., 1993). Explicit relations are those signaled by a discourse connective that occurs in the text, such as “because”, “however”, “for example”. Implicit relations are annotated between adjacent sentences in the same paragraph. There are no discourse connectives between the two sentences, and the annotators were asked to insert a connective while marking their senses. Some pairs of sentences do not contain one of the explicit discourse connectives, but the insertion of a connective provides re</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse TreeBank 2.0. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Alex Lascarides</author>
</authors>
<title>Using automatically labelled examples to classify rhetorical relations: An assessment.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="9739" citStr="Sporleder and Lascarides, 2008" startWordPosition="1519" endWordPosition="1522">table for carrying out reliable tests for statistical significance. Some of the discourse relations are much larger than others, so we report our results in term of Fmeasure for each relation and average unweighted accuracy. Significance tests over F scores were carried out using a paired t-test. To do this, the test set is randomly partitioned into ten groups. In each group, the relation distribution was kept as close as possible to the overall test set. 4 Sparsity and pure lexical representations By far the most common features used for representing implicit discourse relations are lexical (Sporleder and Lascarides, 2008; Pitler et al., 2009; Lin et al., 2009; Hernault et al., 2010; Park and Cardie, 2012). Early studies have suggested that lexical features, word pairs (crossproduct of the words in the first and second argument) in particular, will be powerful predictors of discourse relations (Marcu and Echihabi, 2002; Blair-Goldensohn et al., 2007). The intuition behind word pairs was that semantic relations between the lexical items, such as drought–famine, child–adult, may in turn signal causal or contrast discourse relations. Later it has been shown that word pair features do not appear to capture such se</context>
</contexts>
<marker>Sporleder, Lascarides, 2008</marker>
<rawString>Caroline Sporleder and Alex Lascarides. 2008. Using automatically labelled examples to classify rhetorical relations: An assessment. Natural Language Engineering, 14(3):369–416, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi-Min Zhou</author>
<author>Yu Xu</author>
<author>Zheng-Yu Niu</author>
<author>Man Lan</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Predicting discourse connectives for implicit discourse relation recognition.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1507--1514</pages>
<contexts>
<context position="10485" citStr="Zhou et al., 2010" startWordPosition="1642" endWordPosition="1645"> features, word pairs (crossproduct of the words in the first and second argument) in particular, will be powerful predictors of discourse relations (Marcu and Echihabi, 2002; Blair-Goldensohn et al., 2007). The intuition behind word pairs was that semantic relations between the lexical items, such as drought–famine, child–adult, may in turn signal causal or contrast discourse relations. Later it has been shown that word pair features do not appear to capture such semantic relationship between words (Pitler et al., 2009) and that syntactic features lead to higher accuracies (Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012). Recently, Biran and McKeown (2013) aggregated word pair features with explicit connectives and reported improvements over the original word pairs as features. In this section, we show that the representation of lexical features play a direct role in feature sparsity and ultimately affects prediction performance. The first two studies that specifically addressed 3We also did not include features that occurred less than 5 times in the training set. # Features Avg. F Avg. Accuracy word-pairs 92128 29.46 57.22 binary-lexical 12116 31.79 60.42 Table 1: F-scores and average</context>
</contexts>
<marker>Zhou, Xu, Niu, Lan, Su, Tan, 2010</marker>
<rawString>Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian Su, and Chew Lim Tan. 2010. Predicting discourse connectives for implicit discourse relation recognition. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING), pages 1507–1514.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
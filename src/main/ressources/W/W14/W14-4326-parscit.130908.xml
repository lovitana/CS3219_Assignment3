<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000073">
<title confidence="0.99524">
Knowledge Acquisition Strategies for Goal-Oriented Dialog Systems
</title>
<author confidence="0.994224">
Aasish Pappu Alexander I. Rudnicky
</author>
<affiliation confidence="0.9912715">
School of Computer Science
Carnegie Mellon University
</affiliation>
<email confidence="0.996421">
{aasish, air}@cs.cmu.edu
</email>
<sectionHeader confidence="0.993835" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999085">
Many goal-oriented dialog agents are ex-
pected to identify slot-value pairs in a
spoken query, then perform lookup in
a knowledge base to complete the task.
When the agent encounters unknown slot-
values, it may ask the user to repeat or re-
formulate the query. But a robust agent
can proactively seek new knowledge from
a user, to help reduce subsequent task fail-
ures. In this paper, we propose knowledge
acquisition strategies for a dialog agent
and show their effectiveness. The acquired
knowledge can be shown to subsequently
contribute to task completion.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967775">
Many spoken dialog agents are designed to per-
form specific tasks in a specified domain e.g., in-
formation about public events in a city. To carry
out its task, an agent parses an input utterance, fills
in slot-value pairs, then completes the task. Some-
times, information on these slot-value pairs may
not be available in its knowledge base. In such
cases, typically the agent categorizes utterances as
non-understanding errors. Ideally the incident is
recorded and the missing knowledge is incorpo-
rated into the system with a developer’s assistance
— a slow offline process.
There are other sources of knowledge: automat-
ically crawling the web, as done by NELL [Carl-
son et al., 2010], and community knowledge
bases such as Freebase [Bollacker et al., 2008].
These approaches provide globally popular slot-
values [Araki, 2012] and high-level semantic con-
texts [Pappu and Rudnicky, 2013]. Despite their
size, these knowledge bases may not contain in-
formation about the entities in a specific target
domain. However, users in the agent’s domain
can potentially provide specific information on
slot/values that are unavailable on the web, e.g.,
regarding a recent interest/hobby of the user’s
friend. Lasecki et al. [2013] have elicited natu-
ral language dialogs from humans to build NLU
models for the agent and Bigham et al. [2010]
have elicited answers to visual questions by in-
tegrating users into the system. One observation
from this work is that both users and non-users
can impart useful knowledge to system. In this
paper we propose spoken language strategies that
allow an agent to elicit new slot-value pairs from
its own user population to extend its knowledge
base. Open-domain knowledge may be elicited
through text-based questionnaires from non-users
of the system, but in a situated interaction scenario
spoken strategies may be more effective. We ad-
dress the following research questions:
</bodyText>
<listItem confidence="0.994865466666667">
1. Can an agent elicit reliable knowledge about
its domain from users? Particularly knowl-
edge it cannot locate elsewhere (e.g., on-line
knowledge bases). Is the collective knowl-
edge of the users sufficient to allow the agent
to augment its knowledge through interactive
means?
2. What strategies elicit useful knowledge from
users? Based on previous work in com-
mon sense knowledge acquisition [Von Ahn,
2006, Singh et al., 2002, Witbrock et al.,
2003], we devise spoken language strategies
that allow the system to solicit information by
presenting concrete situations and by asking
user-centric questions.
</listItem>
<bodyText confidence="0.999938111111111">
We address these questions in the context of the
EVENTSPEAK dialog system, an agent that provides
information about seminars and talks in an aca-
demic environment. This paper is organized as
follows. In Section 2, we discuss knowledge ac-
quisition strategies. In Section 3, we describe a
user study on these strategies. Then, we present
an evaluation on system acquired knowledge and
finally we make concluding remarks.
</bodyText>
<page confidence="0.992288">
194
</page>
<tableCaption confidence="0.774455333333333">
Proceedings of the SIGDIAL 2014 Conference, pages 194–198,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
Table 1: System initiated strategies used by the agent for knowledge acquisition in the EVENTSPEAK system.
</tableCaption>
<table confidence="0.999427625">
StrategyType Strategy Example Prompt
QUERYDRIVEN QUERYEVENT I know events on campus. What do you want to know?
QUERYPERSON I know some of the researchers on campus.Whom do you want to know about?
PERSONAL BUZZWORDS What are some of the popular phrases in your research?
FAMOUSPEOPLE Tell me some well-known people in your research area
SHOW&amp;ASK TWEET How would you describe this talk in a sentence, say a tweet.
KEYWORDS Give keywords for this talk in your own words.
PEOPLE Do you know anyone who might be interested in this talk?
</table>
<sectionHeader confidence="0.883755" genericHeader="method">
2 Knowledge Acquisition Strategies
</sectionHeader>
<bodyText confidence="0.999895595238095">
We posit three different circumstances that can
trigger knowledge acquisition behavior: (1) initi-
ated by expert users of the system [Holzapfel et al.,
2008, Spexard et al., 2006, L¨utkebohle et al., 2009,
Rudnicky et al., 2010], (2) triggered by “misun-
derstanding” of the user’s input [Chung et al.,
2003, Filisko and Seneff, 2005, Prasad et al., 2012,
Pappu et al., 2014], or (3) triggered by the system.
They are described below:
QUERYDRIVEN. The system prompts a user
with an open-ended question akin to “how-may-I-
help-you” to learn what “values” of a slot are of
interest to the user. This strategy does not ground
user about system’s knowledge limitations. How-
ever, it allows the system to acquire information
(slot-value pairs) from user’s input. The system
can choose to respond to the input or ignore the
input depending on its knowledge about the slot-
value pairs in the input. Table 1 shows strategies
of this kind i.e., QUERYEVENT and QUERYPERSON.
PERSONAL. The system asks a user about their
own interests and people who may share those in-
terests. This is an open-ended request as well, but
the system expects the response to be confined to
the user’s knowledge about specific entities in the
environment. BUZZWORDS and FAMOUSPEOPLE ex-
pects the user to provide values for the slots.
SHOW&amp;ASK. The system provides a descrip-
tion of an event and asks questions to ground
user’s responses in relation to that event. E.g.,
given the title and abstract of a technical talk,
the system asks the user questions about the talk.
TWEET strategy is expected to elicit a concise de-
scription of the event, which eventually may help
the agent to both summarize events for other users
and identify keywords for an event. KEYWORDS
strategy expects the user to explicitly supply key-
words for an event. PEOPLE strategy expects the
user to provide names of likely event participants.
We hypothesized that these strategies may allow
the agent to learn new slot-value pairs that may
help towards better task performance.
</bodyText>
<sectionHeader confidence="0.982562" genericHeader="method">
3 Knowledge Acquisition Study
</sectionHeader>
<bodyText confidence="0.999979833333334">
We conducted a user study to determine reliability
of the information acquired by the system. We per-
formed this study using the EVENTSPEAK1 dialog
system, which provides information about upcom-
ing talks and other events that might be of inter-
est, and about ongoing research on campus. The
system presents material on a screen and accepts
spoken input, in a context similar to a kiosk.
The study evaluated performance of the seven
strategies described above. For SHOW&amp;ASK strate-
gies, we had users respond regarding a specific
event. We used descriptions of research talks col-
lected from the university’s website. We used a
web-based interface for data collection; the inter-
face presented the prompt material and recorded
the subject’s voice response. Testvox2 was used
to setup the experiments and Wami3 for audio
recording.
</bodyText>
<subsectionHeader confidence="0.999344">
3.1 User Study Design
</subsectionHeader>
<bodyText confidence="0.990916625">
We recruited 40 researchers (graduate students)
from the School of Computer Science, at Carnegie
Mellon, representative of the user population for
the EVENTSPEAK dialog system. Each subject re-
sponded to prompts from the QUERYDRIVEN, PER-
SONAL and SHOW&amp;ASK strategies.
In the QUERYDRIVEN tasks, the QUERYEVENT
strategy, the system responds to the user’s query
with a list of talks. The user’s response is
recorded, then sent to an open-vocabulary speech
recognizer; the result is used as a query to a
database of talks. The results are then displayed on
the screen. The system applies the QUERYPERSON
strategy in a similar way. In the PERSONAL tasks,
the system applies the BUZZWORDS strategy to ask
the user about popular keyphrases in their research
</bodyText>
<footnote confidence="0.99998">
1http://www.speech.cs.cmu.edu/apappu/kacq
2https://bitbucket.org/happyalu/testvox/wiki/Home
3https://code.google.com/p/wami-recorder/
</footnote>
<page confidence="0.9944">
195
</page>
<figureCaption confidence="0.999447">
Figure 1: Time per Task for all strategies Figure 2: Time per Task vs Expertise
</figureCaption>
<figure confidence="0.997832888888889">
4
3
2
1
0
2.23 2.51
1.51 0.91 0.97
0. 71 0.69
4
tweet
people
keywords
3
2
1
0
Time in minutes
Time in minutes
</figure>
<bodyText confidence="0.993985555555556">
area. The system then asks about well-known re-
searchers (FAMOUSPEOPLE) in the user’s area.
In the SHOW&amp;ASK tasks, we use two seminar
descriptions per subject (in our pilot study, we
found that people provide more diverse responses
(in term of entities) in the SHOW&amp;ASK based on
the event abstract, compared to PERSONAL, QUERY-
DRIVEN). We used a set of 80 research talk an-
nouncements (consisting of a title, abstract and
other information). For each talk, the system used
all three strategies viz., TWEET, KEYWORDS and PEO-
PLE. For the TWEET tasks, subjects were asked to
provide a one sentence description. They were al-
lowed to give a non-technical/high-level descrip-
tion if they were unfamiliar with the topic. For
the PEOPLE task, subjects had to give names of col-
leagues who might be interested in the talk. For
the KEYWORDS task, subjects provided keywords,
either their own words or ones selected from the
abstract.
Since the material is highly technical, we were
interested whether the tasks are cognitively de-
manding for people who are less familiar with the
subject of a talk. Therefore, users were asked to
indicate their familiarity with a particular talk (re-
search area in general) using a scale of 1–4: 4 be-
ing more familiar and 1 being less familiar.
</bodyText>
<subsectionHeader confidence="0.998841">
3.2 Corpus Description
</subsectionHeader>
<bodyText confidence="0.9999091875">
This user study produced 64 minutes of audio data,
on average 1.6 minutes per subject. We tran-
scribed the speech then annotated the corpus for
people names, and for research interests. Table 2
shows the number of unique slot-values found in
the corpus. We observe that the number of unique
research interests produced during SHOW&amp;ASK is
higher than for other strategies. This confirms
our initial observations that this strategy elicits
diverse responses. The PERSONAL task produced
a relatively higher number of researcher names
(FAMOUSPEOPLE strategy) than other tasks. One ex-
planation might be that people may find it easier
to recall names in their own research area, as com-
pared to other areas. Overall, we identified 139
unique researcher names and 485 interests.
</bodyText>
<tableCaption confidence="0.9701">
Table 2: Corpus Statistics
</tableCaption>
<table confidence="0.999036285714286">
StrategyType Unique Unique
Researcher Research
Names Interests
QUERYDRIVEN 21 30
PERSONAL 77 107
SHOW&amp;ASK 76 390
Overall 139 485
</table>
<subsectionHeader confidence="0.999424">
3.3 Corpus Analysis
</subsectionHeader>
<bodyText confidence="0.999962166666667">
One of the objectives of this work is to determine
What strategies can the agent use to elicit knowl-
edge from users? Although, time-cost will vary
with task and domain, a usable strategy should, in
general, be less demanding. We analyzed the time-
per-task for each strategy, shown in Figure 1. We
found that the TWEET strategy is not only more de-
manding, it has higher variance than other tasks.
One explanation is that people would attempt to
summarize the entire abstract including technical
details, despite the instructions indicated that a
non-technical description was acceptable. We can
see a similar trend in Figure 2 that irrespective
of expertise-level, subjects take more time to give
one sentence descriptions. We also observe high
variance and higher time-per-task for QUERYPER-
SON; this is due to the system deliberately not re-
turning any results for this task. This was done to
</bodyText>
<page confidence="0.999564">
196
</page>
<tableCaption confidence="0.921275">
Table 3: Mean Precision for 200 researchers, broken down by the “source” strategy used to acquire their name
Note: Only 85 of 200 researchers had Google Scholar pages, GScholar Accuracy is computed for only those 85.
</tableCaption>
<table confidence="0.995921666666667">
Metric Description Text SHOW&amp;ASK PERSONAL QUERYDRIVEN mean
Mean Precision 89.5% 86.9% 93.6% 86.2% 90.5%
GScholarAcc. 78.3% 82.3% 86.1% 100% 80.0%
</table>
<bodyText confidence="0.993927363636364">
find out whether subjects would repeat the task on
failure. Ideally the system needs to only rarely use
this strategy to not lose user’s trust and solicit mul-
tiple values for a given slot (e.g., person name) as
opposed to requesting list of values as in FAMOUS-
PEOPLE and PEOPLE strategies. We find that PEOPLE,
KEYWORDS, FAMOUSPEOPLE and BUZZWORDS strate-
gies are efficient with a time-per-task of less than
one minute. As shown in Figure 2, subjects do not
take much time to speak a list of names or key-
words.
</bodyText>
<sectionHeader confidence="0.971137" genericHeader="method">
4 Evaluation of Acquired Knowledge
</sectionHeader>
<bodyText confidence="0.999976210526316">
To answer Can an agent elicit reliable knowl-
edge about its domain from users? we analyzed
the relevance of acquired knowledge. We have
two disjoint list of entities, (a) researchers and
(b) research interests; in addition we have speaker
names from the talk descriptions. Our goal is
to implicitly infer a list of interests for each re-
searcher without soliciting the user for the inter-
ests of every researcher exhaustively. To each re-
searcher in the list, we attribute list of interests that
were mentioned in the same context as researcher
was mentioned. We tag list of names acquired
from the FAMOUSPEOPLE strategy with list of key-
words acquired from the BUZZWORDS strategy —
both lists acquired from same user. We repeat this
process for each name mentioned in relation to a
talk in the SHOW&amp;ASK strategy. We tag keywords
mentioned in the KEYWORDS strategy to researchers
mentioned in the PEOPLE strategy.
</bodyText>
<subsectionHeader confidence="0.999094">
4.1 Analysis
</subsectionHeader>
<bodyText confidence="0.9963015">
We produced 200 entries for researchers and their
set of interests. We then had two annotators (se-
nior graduate students) mark whether the system-
predicted interests were relevant/accurate. The an-
notators were allowed to use information found on
researchers’ home pages and Google Scholar4 to
evaluate the system-predicted interests.
This can be seen as an information retrieval (IR)
problem, where researcher is “query” and interests
are “documents”. So, we use Mean Precision, a
</bodyText>
<page confidence="0.291445">
4scholar.google.com
</page>
<bodyText confidence="0.999417571428571">
common metric in IR, to evaluate retrieval. In our
case, the ground truth for relevant interests comes
from the annotators. The results are shown in Ta-
ble 3. Our approach has high precision, 90.5%,
for all 200 researchers. We see that irrespective
of the strategy used to acquire entities, precision
is good. We also compared our predicted inter-
ests with interests listed by researchers themselves
on Google Scholar. There are only 85 researchers
from our list with a Google Scholar page; for these
our accuracy is 80%, again good. Moreover, sig-
nificant knowledge is absent from the web (at least
in our domain) yet can be elicited from users fa-
miliar with the domain.
</bodyText>
<sectionHeader confidence="0.99386" genericHeader="evaluation">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999980611111111">
We describe a set of knowledge acquisition strate-
gies that allow a system to solicit novel informa-
tion from users in a situated environment. To in-
vestigate the usability of these strategies, we con-
ducted a user study in the domain of research talks.
We analyzed a corpus of system-acquired knowl-
edge and have made the material available5. Our
data show that users on average take less than a
minute to provide new information using the pro-
posed elicitation strategies. The reliability of ac-
quired knowledge in predicting relationships be-
tween researchers and interests is quite good, with
a mean precision of 90.5%. We note that the PER-
SONAL strategy, which tries to tap personal knowl-
edge, appears to be particularly effective. More
generally, automated elicitation appears to be a
promising technique for continuous learning in
spoken dialog systems.
</bodyText>
<sectionHeader confidence="0.999264" genericHeader="conclusions">
6 Appendix
</sectionHeader>
<subsectionHeader confidence="0.657229">
System Predicted Researcher-Interests 1
</subsectionHeader>
<bodyText confidence="0.835213333333333">
rich stern deep neural networks, speech recog-
nition, signal processing, neural networks, machine
learning, speech synthesis
</bodyText>
<footnote confidence="0.993314">
5www.speech.cs.cmu.edu/apappu/pubdl/eventspeak corpus.zip
</footnote>
<page confidence="0.989618">
197
</page>
<bodyText confidence="0.73218625">
System Predicted Researcher-Interests 2
kishore prahallad dialogue systems, prosody,
speech synthesis, text to speech, pronunciation mod-
eling, low resource languages
</bodyText>
<subsubsectionHeader confidence="0.296284">
System Predicted Researcher-Interests 3
</subsubsectionHeader>
<bodyText confidence="0.83205875">
carolyn rose crowdsourcing, meta discourse clas-
sification, statistical analysis, presentation skills in-
struction, man made system, education models, human
learning
</bodyText>
<subsectionHeader confidence="0.371429">
System Predicted Researcher-Interests 4
</subsectionHeader>
<bodyText confidence="0.823372444444444">
florian metze dialogue systems, speech recogni-
tion, nlp, prosody, speech synthesis, text to speech,
pronunciation modeling, low resource languages, au-
tomatic accent identification
System Predicted Researcher-Interests 5
madhavi ganapathiraju protein structure, contin-
uous graphical models, generative models, structural
biology, protein structure dynamics, molecular dy-
namics
</bodyText>
<subsectionHeader confidence="0.401256">
System Predicted Researcher-Interests 6
</subsectionHeader>
<bodyText confidence="0.9676025">
alexander hauptmann discriminatively trained
models, deep learning, computer vision, big data
</bodyText>
<subsectionHeader confidence="0.404589">
System Predicted Researcher-Interests 7
</subsectionHeader>
<bodyText confidence="0.992885">
jamie callan learning to rank, search, large scale
search, web search, click prediction, information re-
trieval, web mining, user activity, recommendation,
relevance, machine learning, web crawling, distributed
systems, structural similarity
</bodyText>
<subsubsectionHeader confidence="0.465407">
System Predicted Researcher-Interests 8
</subsubsectionHeader>
<bodyText confidence="0.997326">
lori levin natural language understanding, knowl-
edge reasoning, construction grammar, knowledge
bases, natural language processing
</bodyText>
<sectionHeader confidence="0.998441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998582297297297">
Masahiro Araki. Rapid development process of spoken dia-
logue systems using collaboratively constructed semantic
resources. In Proceedings of the SIGDIAL 2012 Confer-
ence, pages 70–73. ACL, 2012.
Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little,
Andrew Miller, Robert C Miller, Robin Miller, Aubrey
Tatarowicz, Brandyn White, Samual White, et al. Vizwiz:
nearly real-time answers to visual questions. In Proceed-
ings of the 23rd ACM Symposium on User Interface soft-
ware and technology, pages 333–342. ACM, 2010.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge,
and Jamie Taylor. Freebase: a collaboratively created
graph database for structuring human knowledge. Pro-
ceedings of the SIGMOD, pages 1247–1249, 2008.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Set-
tles, Estevam R Hruschka Jr., and Tom M Mitchell. To-
ward an Architecture for Never-Ending Language Learn-
ing. Artificial Intelligence, 2(4):1306–1313, 2010.
Grace Chung, Stephanie Seneff, and Chao Wang. Automatic
acquisition of names using speak and spell mode in spo-
ken dialogue systems. In Proceedings of the NAACL-HLT,
pages 32–39. ACL, 2003.
Edward Filisko and Stephanie Seneff. Developing city name
acquisition strategies in spoken dialogue systems via user
simulation. In 6th SIGdial Workshop on Discourse and
Dialogue, 2005.
Hartwig Holzapfel, Daniel Neubig, and Alex Waibel. A dia-
logue approach to learning object descriptions and seman-
tic categories. Robotics and Autonomous Systems, 56(11):
1004–1013, November 2008.
Walter Stephen Lasecki, Ece Kamar, and Dan Bohus. Con-
versations in the crowd: Collecting data for task-oriented
dialog learning. In First AAAI Conference on Human
Computation and Crowdsourcing, 2013.
Ingo L¨utkebohle, Julia Peltason, Lars Schillingmann,
Christof Elbrechter, Britta Wrede, Sven Wachsmuth, and
Robert Haschke. The Curious Robot: Structuring Inter-
active Robot Learning. In ICRA’09, pages 4156–4162.
IEEE, 2009.
Aasish Pappu and Alexander Rudnicky. Predicting tasks
in goal-oriented spoken dialog systems using semantic
knowledge bases. In Proceedings of the SIGDIAL, pages
242–250. ACL, 2013.
Aasish Pappu, Teruhisa Misu, and Rakesh Gupta. Investi-
gating critical speech recognition errors in spoken short
messages. In Proceedings of IWSDS, pages 39–49, 2014.
Rohit Prasad, Rohit Kumar, Sankaranarayanan Ananthakr-
ishnan, Wei Chen, Sanjika Hewavitharana, Matthew Roy,
Frederick Choi, Aaron Challenner, Enoch Kan, Arvind
Neelakantan, et al. Active error detection and resolu-
tion for speech-to-speech translation. In Proceedings of
IWSLT, 2012.
Alexander I Rudnicky, Aasish Pappu, Peng Li, and Matthew
Marge. Instruction Taking in the TeamTalk System. In
Proceedings of the AAAI Fall Symposium on Dialog with
Robots, pages 173–174, 2010.
Push Singh, Thomas Lin, Erik T Mueller, Grace Lim, Trav-
ell Perkins, and Wan Li Zhu. Open mind common
sense: Knowledge acquisition from the general public. In
CoopIS, DOA, and ODBASE, pages 1223–1237. Springer,
2002.
Thorsten Spexard, Shuyin Li, Britta Wrede, Jannik Fritsch,
Gerhard Sagerer, Olaf Booij, Zoran Zivkovic, Bas Ter-
wijn, and Ben Krose. BIRON, where are you? Enabling
a robot to learn new places in a real home environment by
integrating spoken dialog and visual localization. Integra-
tion The VLSI Journal, (section II):934–940, 2006.
Luis Von Ahn. Games with a purpose. Computer, 39(6):
92–94, 2006.
Michael Witbrock, David Baxter, Jon Curtis, Dave Schneider,
Robert Kahlert, Pierluigi Miraglia, Peter Wagner, Kathy
Panton, Gavin Matthews, and Amanda Vizedom. An inter-
active dialogue system for knowledge acquisition in cyc.
In Proceedings of the 18th IJCAI, pages 138–145, 2003.
</reference>
<page confidence="0.997699">
198
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961454">
<title confidence="0.999534">Knowledge Acquisition Strategies for Goal-Oriented Dialog Systems</title>
<author confidence="0.987681">Aasish Pappu Alexander I</author>
<affiliation confidence="0.9950975">School of Computer Carnegie Mellon</affiliation>
<abstract confidence="0.998827333333333">Many goal-oriented dialog agents are expected to identify slot-value pairs in a spoken query, then perform lookup in a knowledge base to complete the task. When the agent encounters unknown slotvalues, it may ask the user to repeat or reformulate the query. But a robust agent can proactively seek new knowledge from a user, to help reduce subsequent task failures. In this paper, we propose knowledge acquisition strategies for a dialog agent and show their effectiveness. The acquired knowledge can be shown to subsequently contribute to task completion.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Masahiro Araki</author>
</authors>
<title>Rapid development process of spoken dialogue systems using collaboratively constructed semantic resources.</title>
<date>2012</date>
<booktitle>In Proceedings of the SIGDIAL 2012 Conference,</booktitle>
<pages>70--73</pages>
<publisher>ACL,</publisher>
<contexts>
<context position="1583" citStr="Araki, 2012" startWordPosition="247" endWordPosition="248">lue pairs, then completes the task. Sometimes, information on these slot-value pairs may not be available in its knowledge base. In such cases, typically the agent categorizes utterances as non-understanding errors. Ideally the incident is recorded and the missing knowledge is incorporated into the system with a developer’s assistance — a slow offline process. There are other sources of knowledge: automatically crawling the web, as done by NELL [Carlson et al., 2010], and community knowledge bases such as Freebase [Bollacker et al., 2008]. These approaches provide globally popular slotvalues [Araki, 2012] and high-level semantic contexts [Pappu and Rudnicky, 2013]. Despite their size, these knowledge bases may not contain information about the entities in a specific target domain. However, users in the agent’s domain can potentially provide specific information on slot/values that are unavailable on the web, e.g., regarding a recent interest/hobby of the user’s friend. Lasecki et al. [2013] have elicited natural language dialogs from humans to build NLU models for the agent and Bigham et al. [2010] have elicited answers to visual questions by integrating users into the system. One observation</context>
</contexts>
<marker>Araki, 2012</marker>
<rawString>Masahiro Araki. Rapid development process of spoken dialogue systems using collaboratively constructed semantic resources. In Proceedings of the SIGDIAL 2012 Conference, pages 70–73. ACL, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey P Bigham</author>
<author>Chandrika Jayant</author>
<author>Hanjie Ji</author>
<author>Greg Little</author>
<author>Andrew Miller</author>
<author>Robert C Miller</author>
<author>Robin Miller</author>
<author>Aubrey Tatarowicz</author>
<author>Brandyn White</author>
<author>Samual White</author>
</authors>
<title>Vizwiz: nearly real-time answers to visual questions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd ACM Symposium on User Interface software and technology,</booktitle>
<pages>333--342</pages>
<publisher>ACM,</publisher>
<marker>Bigham, Jayant, Ji, Little, Miller, Miller, Miller, Tatarowicz, White, White, 2010</marker>
<rawString>Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23rd ACM Symposium on User Interface software and technology, pages 333–342. ACM, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>Proceedings of the SIGMOD,</booktitle>
<pages>1247--1249</pages>
<contexts>
<context position="1515" citStr="Bollacker et al., 2008" startWordPosition="236" endWordPosition="239">ty. To carry out its task, an agent parses an input utterance, fills in slot-value pairs, then completes the task. Sometimes, information on these slot-value pairs may not be available in its knowledge base. In such cases, typically the agent categorizes utterances as non-understanding errors. Ideally the incident is recorded and the missing knowledge is incorporated into the system with a developer’s assistance — a slow offline process. There are other sources of knowledge: automatically crawling the web, as done by NELL [Carlson et al., 2010], and community knowledge bases such as Freebase [Bollacker et al., 2008]. These approaches provide globally popular slotvalues [Araki, 2012] and high-level semantic contexts [Pappu and Rudnicky, 2013]. Despite their size, these knowledge bases may not contain information about the entities in a specific target domain. However, users in the agent’s domain can potentially provide specific information on slot/values that are unavailable on the web, e.g., regarding a recent interest/hobby of the user’s friend. Lasecki et al. [2013] have elicited natural language dialogs from humans to build NLU models for the agent and Bigham et al. [2010] have elicited answers to vi</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. Proceedings of the SIGMOD, pages 1247–1249, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an Architecture for Never-Ending Language Learning.</title>
<date>2010</date>
<journal>Artificial Intelligence,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="1442" citStr="Carlson et al., 2010" startWordPosition="224" endWordPosition="228">sks in a specified domain e.g., information about public events in a city. To carry out its task, an agent parses an input utterance, fills in slot-value pairs, then completes the task. Sometimes, information on these slot-value pairs may not be available in its knowledge base. In such cases, typically the agent categorizes utterances as non-understanding errors. Ideally the incident is recorded and the missing knowledge is incorporated into the system with a developer’s assistance — a slow offline process. There are other sources of knowledge: automatically crawling the web, as done by NELL [Carlson et al., 2010], and community knowledge bases such as Freebase [Bollacker et al., 2008]. These approaches provide globally popular slotvalues [Araki, 2012] and high-level semantic contexts [Pappu and Rudnicky, 2013]. Despite their size, these knowledge bases may not contain information about the entities in a specific target domain. However, users in the agent’s domain can potentially provide specific information on slot/values that are unavailable on the web, e.g., regarding a recent interest/hobby of the user’s friend. Lasecki et al. [2013] have elicited natural language dialogs from humans to build NLU </context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka Jr., and Tom M Mitchell. Toward an Architecture for Never-Ending Language Learning. Artificial Intelligence, 2(4):1306–1313, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Chung</author>
<author>Stephanie Seneff</author>
<author>Chao Wang</author>
</authors>
<title>Automatic acquisition of names using speak and spell mode in spoken dialogue systems.</title>
<date>2003</date>
<booktitle>In Proceedings of the NAACL-HLT,</booktitle>
<pages>32--39</pages>
<publisher>ACL,</publisher>
<contexts>
<context position="4814" citStr="Chung et al., 2003" startWordPosition="752" endWordPosition="755">n your research? FAMOUSPEOPLE Tell me some well-known people in your research area SHOW&amp;ASK TWEET How would you describe this talk in a sentence, say a tweet. KEYWORDS Give keywords for this talk in your own words. PEOPLE Do you know anyone who might be interested in this talk? 2 Knowledge Acquisition Strategies We posit three different circumstances that can trigger knowledge acquisition behavior: (1) initiated by expert users of the system [Holzapfel et al., 2008, Spexard et al., 2006, L¨utkebohle et al., 2009, Rudnicky et al., 2010], (2) triggered by “misunderstanding” of the user’s input [Chung et al., 2003, Filisko and Seneff, 2005, Prasad et al., 2012, Pappu et al., 2014], or (3) triggered by the system. They are described below: QUERYDRIVEN. The system prompts a user with an open-ended question akin to “how-may-Ihelp-you” to learn what “values” of a slot are of interest to the user. This strategy does not ground user about system’s knowledge limitations. However, it allows the system to acquire information (slot-value pairs) from user’s input. The system can choose to respond to the input or ignore the input depending on its knowledge about the slotvalue pairs in the input. Table 1 shows stra</context>
</contexts>
<marker>Chung, Seneff, Wang, 2003</marker>
<rawString>Grace Chung, Stephanie Seneff, and Chao Wang. Automatic acquisition of names using speak and spell mode in spoken dialogue systems. In Proceedings of the NAACL-HLT, pages 32–39. ACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Filisko</author>
<author>Stephanie Seneff</author>
</authors>
<title>Developing city name acquisition strategies in spoken dialogue systems via user simulation.</title>
<date>2005</date>
<booktitle>In 6th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<contexts>
<context position="4840" citStr="Filisko and Seneff, 2005" startWordPosition="756" endWordPosition="759">OUSPEOPLE Tell me some well-known people in your research area SHOW&amp;ASK TWEET How would you describe this talk in a sentence, say a tweet. KEYWORDS Give keywords for this talk in your own words. PEOPLE Do you know anyone who might be interested in this talk? 2 Knowledge Acquisition Strategies We posit three different circumstances that can trigger knowledge acquisition behavior: (1) initiated by expert users of the system [Holzapfel et al., 2008, Spexard et al., 2006, L¨utkebohle et al., 2009, Rudnicky et al., 2010], (2) triggered by “misunderstanding” of the user’s input [Chung et al., 2003, Filisko and Seneff, 2005, Prasad et al., 2012, Pappu et al., 2014], or (3) triggered by the system. They are described below: QUERYDRIVEN. The system prompts a user with an open-ended question akin to “how-may-Ihelp-you” to learn what “values” of a slot are of interest to the user. This strategy does not ground user about system’s knowledge limitations. However, it allows the system to acquire information (slot-value pairs) from user’s input. The system can choose to respond to the input or ignore the input depending on its knowledge about the slotvalue pairs in the input. Table 1 shows strategies of this kind i.e., </context>
</contexts>
<marker>Filisko, Seneff, 2005</marker>
<rawString>Edward Filisko and Stephanie Seneff. Developing city name acquisition strategies in spoken dialogue systems via user simulation. In 6th SIGdial Workshop on Discourse and Dialogue, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hartwig Holzapfel</author>
<author>Daniel Neubig</author>
<author>Alex Waibel</author>
</authors>
<title>A dialogue approach to learning object descriptions and semantic categories.</title>
<date>2008</date>
<journal>Robotics and Autonomous Systems,</journal>
<volume>56</volume>
<issue>11</issue>
<pages>1004--1013</pages>
<contexts>
<context position="4665" citStr="Holzapfel et al., 2008" startWordPosition="727" endWordPosition="730">t to know? QUERYPERSON I know some of the researchers on campus.Whom do you want to know about? PERSONAL BUZZWORDS What are some of the popular phrases in your research? FAMOUSPEOPLE Tell me some well-known people in your research area SHOW&amp;ASK TWEET How would you describe this talk in a sentence, say a tweet. KEYWORDS Give keywords for this talk in your own words. PEOPLE Do you know anyone who might be interested in this talk? 2 Knowledge Acquisition Strategies We posit three different circumstances that can trigger knowledge acquisition behavior: (1) initiated by expert users of the system [Holzapfel et al., 2008, Spexard et al., 2006, L¨utkebohle et al., 2009, Rudnicky et al., 2010], (2) triggered by “misunderstanding” of the user’s input [Chung et al., 2003, Filisko and Seneff, 2005, Prasad et al., 2012, Pappu et al., 2014], or (3) triggered by the system. They are described below: QUERYDRIVEN. The system prompts a user with an open-ended question akin to “how-may-Ihelp-you” to learn what “values” of a slot are of interest to the user. This strategy does not ground user about system’s knowledge limitations. However, it allows the system to acquire information (slot-value pairs) from user’s input. Th</context>
</contexts>
<marker>Holzapfel, Neubig, Waibel, 2008</marker>
<rawString>Hartwig Holzapfel, Daniel Neubig, and Alex Waibel. A dialogue approach to learning object descriptions and semantic categories. Robotics and Autonomous Systems, 56(11): 1004–1013, November 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Stephen Lasecki</author>
<author>Ece Kamar</author>
<author>Dan Bohus</author>
</authors>
<title>Conversations in the crowd: Collecting data for task-oriented dialog learning.</title>
<date>2013</date>
<booktitle>In First AAAI Conference on Human Computation and Crowdsourcing,</booktitle>
<marker>Lasecki, Kamar, Bohus, 2013</marker>
<rawString>Walter Stephen Lasecki, Ece Kamar, and Dan Bohus. Conversations in the crowd: Collecting data for task-oriented dialog learning. In First AAAI Conference on Human Computation and Crowdsourcing, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingo L¨utkebohle</author>
<author>Julia Peltason</author>
<author>Lars Schillingmann</author>
<author>Christof Elbrechter</author>
<author>Britta Wrede</author>
<author>Sven Wachsmuth</author>
<author>Robert Haschke</author>
</authors>
<title>The Curious Robot: Structuring Interactive Robot Learning. In</title>
<date>2009</date>
<booktitle>ICRA’09,</booktitle>
<pages>4156--4162</pages>
<publisher>IEEE,</publisher>
<marker>L¨utkebohle, Peltason, Schillingmann, Elbrechter, Wrede, Wachsmuth, Haschke, 2009</marker>
<rawString>Ingo L¨utkebohle, Julia Peltason, Lars Schillingmann, Christof Elbrechter, Britta Wrede, Sven Wachsmuth, and Robert Haschke. The Curious Robot: Structuring Interactive Robot Learning. In ICRA’09, pages 4156–4162. IEEE, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aasish Pappu</author>
<author>Alexander Rudnicky</author>
</authors>
<title>Predicting tasks in goal-oriented spoken dialog systems using semantic knowledge bases.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL,</booktitle>
<pages>242--250</pages>
<publisher>ACL,</publisher>
<contexts>
<context position="1643" citStr="Pappu and Rudnicky, 2013" startWordPosition="254" endWordPosition="257">information on these slot-value pairs may not be available in its knowledge base. In such cases, typically the agent categorizes utterances as non-understanding errors. Ideally the incident is recorded and the missing knowledge is incorporated into the system with a developer’s assistance — a slow offline process. There are other sources of knowledge: automatically crawling the web, as done by NELL [Carlson et al., 2010], and community knowledge bases such as Freebase [Bollacker et al., 2008]. These approaches provide globally popular slotvalues [Araki, 2012] and high-level semantic contexts [Pappu and Rudnicky, 2013]. Despite their size, these knowledge bases may not contain information about the entities in a specific target domain. However, users in the agent’s domain can potentially provide specific information on slot/values that are unavailable on the web, e.g., regarding a recent interest/hobby of the user’s friend. Lasecki et al. [2013] have elicited natural language dialogs from humans to build NLU models for the agent and Bigham et al. [2010] have elicited answers to visual questions by integrating users into the system. One observation from this work is that both users and non-users can impart </context>
</contexts>
<marker>Pappu, Rudnicky, 2013</marker>
<rawString>Aasish Pappu and Alexander Rudnicky. Predicting tasks in goal-oriented spoken dialog systems using semantic knowledge bases. In Proceedings of the SIGDIAL, pages 242–250. ACL, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aasish Pappu</author>
<author>Teruhisa Misu</author>
<author>Rakesh Gupta</author>
</authors>
<title>Investigating critical speech recognition errors in spoken short messages.</title>
<date>2014</date>
<booktitle>In Proceedings of IWSDS,</booktitle>
<pages>39--49</pages>
<contexts>
<context position="4881" citStr="Pappu et al., 2014" startWordPosition="764" endWordPosition="767">r research area SHOW&amp;ASK TWEET How would you describe this talk in a sentence, say a tweet. KEYWORDS Give keywords for this talk in your own words. PEOPLE Do you know anyone who might be interested in this talk? 2 Knowledge Acquisition Strategies We posit three different circumstances that can trigger knowledge acquisition behavior: (1) initiated by expert users of the system [Holzapfel et al., 2008, Spexard et al., 2006, L¨utkebohle et al., 2009, Rudnicky et al., 2010], (2) triggered by “misunderstanding” of the user’s input [Chung et al., 2003, Filisko and Seneff, 2005, Prasad et al., 2012, Pappu et al., 2014], or (3) triggered by the system. They are described below: QUERYDRIVEN. The system prompts a user with an open-ended question akin to “how-may-Ihelp-you” to learn what “values” of a slot are of interest to the user. This strategy does not ground user about system’s knowledge limitations. However, it allows the system to acquire information (slot-value pairs) from user’s input. The system can choose to respond to the input or ignore the input depending on its knowledge about the slotvalue pairs in the input. Table 1 shows strategies of this kind i.e., QUERYEVENT and QUERYPERSON. PERSONAL. The</context>
</contexts>
<marker>Pappu, Misu, Gupta, 2014</marker>
<rawString>Aasish Pappu, Teruhisa Misu, and Rakesh Gupta. Investigating critical speech recognition errors in spoken short messages. In Proceedings of IWSDS, pages 39–49, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit Prasad</author>
<author>Rohit Kumar</author>
<author>Sankaranarayanan Ananthakrishnan</author>
<author>Wei Chen</author>
<author>Sanjika Hewavitharana</author>
<author>Matthew Roy</author>
<author>Frederick Choi</author>
<author>Aaron Challenner</author>
<author>Enoch Kan</author>
<author>Arvind Neelakantan</author>
</authors>
<title>Active error detection and resolution for speech-to-speech translation.</title>
<date>2012</date>
<booktitle>In Proceedings of IWSLT,</booktitle>
<contexts>
<context position="4861" citStr="Prasad et al., 2012" startWordPosition="760" endWordPosition="763">l-known people in your research area SHOW&amp;ASK TWEET How would you describe this talk in a sentence, say a tweet. KEYWORDS Give keywords for this talk in your own words. PEOPLE Do you know anyone who might be interested in this talk? 2 Knowledge Acquisition Strategies We posit three different circumstances that can trigger knowledge acquisition behavior: (1) initiated by expert users of the system [Holzapfel et al., 2008, Spexard et al., 2006, L¨utkebohle et al., 2009, Rudnicky et al., 2010], (2) triggered by “misunderstanding” of the user’s input [Chung et al., 2003, Filisko and Seneff, 2005, Prasad et al., 2012, Pappu et al., 2014], or (3) triggered by the system. They are described below: QUERYDRIVEN. The system prompts a user with an open-ended question akin to “how-may-Ihelp-you” to learn what “values” of a slot are of interest to the user. This strategy does not ground user about system’s knowledge limitations. However, it allows the system to acquire information (slot-value pairs) from user’s input. The system can choose to respond to the input or ignore the input depending on its knowledge about the slotvalue pairs in the input. Table 1 shows strategies of this kind i.e., QUERYEVENT and QUERYP</context>
</contexts>
<marker>Prasad, Kumar, Ananthakrishnan, Chen, Hewavitharana, Roy, Choi, Challenner, Kan, Neelakantan, 2012</marker>
<rawString>Rohit Prasad, Rohit Kumar, Sankaranarayanan Ananthakrishnan, Wei Chen, Sanjika Hewavitharana, Matthew Roy, Frederick Choi, Aaron Challenner, Enoch Kan, Arvind Neelakantan, et al. Active error detection and resolution for speech-to-speech translation. In Proceedings of IWSLT, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander I Rudnicky</author>
<author>Aasish Pappu</author>
<author>Peng Li</author>
<author>Matthew Marge</author>
</authors>
<title>Instruction Taking in the TeamTalk System.</title>
<date>2010</date>
<booktitle>In Proceedings of the AAAI Fall Symposium on Dialog with Robots,</booktitle>
<pages>173--174</pages>
<contexts>
<context position="4736" citStr="Rudnicky et al., 2010" startWordPosition="739" endWordPosition="742">you want to know about? PERSONAL BUZZWORDS What are some of the popular phrases in your research? FAMOUSPEOPLE Tell me some well-known people in your research area SHOW&amp;ASK TWEET How would you describe this talk in a sentence, say a tweet. KEYWORDS Give keywords for this talk in your own words. PEOPLE Do you know anyone who might be interested in this talk? 2 Knowledge Acquisition Strategies We posit three different circumstances that can trigger knowledge acquisition behavior: (1) initiated by expert users of the system [Holzapfel et al., 2008, Spexard et al., 2006, L¨utkebohle et al., 2009, Rudnicky et al., 2010], (2) triggered by “misunderstanding” of the user’s input [Chung et al., 2003, Filisko and Seneff, 2005, Prasad et al., 2012, Pappu et al., 2014], or (3) triggered by the system. They are described below: QUERYDRIVEN. The system prompts a user with an open-ended question akin to “how-may-Ihelp-you” to learn what “values” of a slot are of interest to the user. This strategy does not ground user about system’s knowledge limitations. However, it allows the system to acquire information (slot-value pairs) from user’s input. The system can choose to respond to the input or ignore the input dependi</context>
</contexts>
<marker>Rudnicky, Pappu, Li, Marge, 2010</marker>
<rawString>Alexander I Rudnicky, Aasish Pappu, Peng Li, and Matthew Marge. Instruction Taking in the TeamTalk System. In Proceedings of the AAAI Fall Symposium on Dialog with Robots, pages 173–174, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Push Singh</author>
<author>Thomas Lin</author>
<author>Erik T Mueller</author>
<author>Grace Lim</author>
<author>Travell Perkins</author>
<author>Wan Li Zhu</author>
</authors>
<title>Open mind common sense: Knowledge acquisition from the general public.</title>
<date>2002</date>
<booktitle>In CoopIS, DOA, and ODBASE,</booktitle>
<pages>1223--1237</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="3087" citStr="Singh et al., 2002" startWordPosition="482" endWordPosition="485">d through text-based questionnaires from non-users of the system, but in a situated interaction scenario spoken strategies may be more effective. We address the following research questions: 1. Can an agent elicit reliable knowledge about its domain from users? Particularly knowledge it cannot locate elsewhere (e.g., on-line knowledge bases). Is the collective knowledge of the users sufficient to allow the agent to augment its knowledge through interactive means? 2. What strategies elicit useful knowledge from users? Based on previous work in common sense knowledge acquisition [Von Ahn, 2006, Singh et al., 2002, Witbrock et al., 2003], we devise spoken language strategies that allow the system to solicit information by presenting concrete situations and by asking user-centric questions. We address these questions in the context of the EVENTSPEAK dialog system, an agent that provides information about seminars and talks in an academic environment. This paper is organized as follows. In Section 2, we discuss knowledge acquisition strategies. In Section 3, we describe a user study on these strategies. Then, we present an evaluation on system acquired knowledge and finally we make concluding remarks. 19</context>
</contexts>
<marker>Singh, Lin, Mueller, Lim, Perkins, Zhu, 2002</marker>
<rawString>Push Singh, Thomas Lin, Erik T Mueller, Grace Lim, Travell Perkins, and Wan Li Zhu. Open mind common sense: Knowledge acquisition from the general public. In CoopIS, DOA, and ODBASE, pages 1223–1237. Springer, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>BIRON</author>
</authors>
<title>where are you? Enabling a robot to learn new places in a real home environment by integrating spoken dialog and visual localization. Integration The VLSI Journal, (section II):934–940,</title>
<date>2006</date>
<marker>BIRON, 2006</marker>
<rawString>Thorsten Spexard, Shuyin Li, Britta Wrede, Jannik Fritsch, Gerhard Sagerer, Olaf Booij, Zoran Zivkovic, Bas Terwijn, and Ben Krose. BIRON, where are you? Enabling a robot to learn new places in a real home environment by integrating spoken dialog and visual localization. Integration The VLSI Journal, (section II):934–940, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Von Ahn</author>
</authors>
<title>Games with a purpose.</title>
<date>2006</date>
<journal>Computer,</journal>
<volume>39</volume>
<issue>6</issue>
<pages>92--94</pages>
<marker>Von Ahn, 2006</marker>
<rawString>Luis Von Ahn. Games with a purpose. Computer, 39(6): 92–94, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Witbrock</author>
<author>David Baxter</author>
<author>Jon Curtis</author>
<author>Dave Schneider</author>
<author>Robert Kahlert</author>
<author>Pierluigi Miraglia</author>
<author>Peter Wagner</author>
<author>Kathy Panton</author>
<author>Gavin Matthews</author>
<author>Amanda Vizedom</author>
</authors>
<title>An interactive dialogue system for knowledge acquisition in cyc.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th IJCAI,</booktitle>
<pages>138--145</pages>
<contexts>
<context position="3110" citStr="Witbrock et al., 2003" startWordPosition="486" endWordPosition="489"> questionnaires from non-users of the system, but in a situated interaction scenario spoken strategies may be more effective. We address the following research questions: 1. Can an agent elicit reliable knowledge about its domain from users? Particularly knowledge it cannot locate elsewhere (e.g., on-line knowledge bases). Is the collective knowledge of the users sufficient to allow the agent to augment its knowledge through interactive means? 2. What strategies elicit useful knowledge from users? Based on previous work in common sense knowledge acquisition [Von Ahn, 2006, Singh et al., 2002, Witbrock et al., 2003], we devise spoken language strategies that allow the system to solicit information by presenting concrete situations and by asking user-centric questions. We address these questions in the context of the EVENTSPEAK dialog system, an agent that provides information about seminars and talks in an academic environment. This paper is organized as follows. In Section 2, we discuss knowledge acquisition strategies. In Section 3, we describe a user study on these strategies. Then, we present an evaluation on system acquired knowledge and finally we make concluding remarks. 194 Proceedings of the SI</context>
</contexts>
<marker>Witbrock, Baxter, Curtis, Schneider, Kahlert, Miraglia, Wagner, Panton, Matthews, Vizedom, 2003</marker>
<rawString>Michael Witbrock, David Baxter, Jon Curtis, Dave Schneider, Robert Kahlert, Pierluigi Miraglia, Peter Wagner, Kathy Panton, Gavin Matthews, and Amanda Vizedom. An interactive dialogue system for knowledge acquisition in cyc. In Proceedings of the 18th IJCAI, pages 138–145, 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
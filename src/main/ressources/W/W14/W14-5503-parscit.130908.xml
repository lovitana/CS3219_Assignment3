<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001451">
<title confidence="0.9838315">
Integrating Dictionaries into an Unsupervised Model for
Myanmar Word Segmentation
</title>
<author confidence="0.943562">
Ye Kyaw Thu
</author>
<affiliation confidence="0.777425333333333">
NICT
Keihanna Science City
Kyoto, Japan
</affiliation>
<email confidence="0.971739">
yekyawthu@nict.go.jp
</email>
<author confidence="0.978076">
Eiichiro Sumita
</author>
<affiliation confidence="0.831797333333333">
NICT
Keihanna Science City
Kyoto, Japan
</affiliation>
<email confidence="0.998406">
eiichiro.sumita@nict.go.jp
</email>
<author confidence="0.774535">
Andrew Finch
</author>
<affiliation confidence="0.645390333333333">
NICT
Keihanna Science City
Kyoto, Japan
</affiliation>
<email confidence="0.927539">
andrew.finch@nict.go.jp
</email>
<author confidence="0.994616">
Yoshinori Sagisaka
</author>
<affiliation confidence="0.911977333333333">
GITI/Speech Science Research Lab.
Waseda Univerity
Tokyo, Japan
</affiliation>
<email confidence="0.997468">
ysagisaka@gmail.com
</email>
<sectionHeader confidence="0.996641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999981">
This paper addresses the problem of word segmentation for low resource languages, with the
main focus being on Myanmar language. In our proposed method, we focus on exploiting lim-
ited amounts of dictionary resource, in an attempt to improve the segmentation quality of an
unsupervised word segmenter. Three models are proposed. In the first, a set of dictionaries
(separate dictionaries for different classes of words) are directly introduced into the generative
model. In the second, a language model was built from the dictionaries, and the n-gram model
was inserted into the generative model. This model was expected to model words that did not
occur in the training data. The third model was a combination of the previous two models. We
evaluated our approach on a corpus of manually annotated data. Our results show that the pro-
posed methods are able to improve over a fully unsupervised baseline system. The best of our
systems improved the F-score from 0.48 to 0.66. In addition to segmenting the data, one pro-
posed method is also able to partially label the segmented corpus with POS tags. We found that
these labels were approximately 66% accurate.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.94934115">
In many natural language processing applications, for example machine translation, parsing and tagging,
it is essential to have text that is segmented into sequences of tokens (these tokens usually represent
‘words’). In many languages, including the Myanmar language (alternatively called the Burmese lan-
guage), Japanese, and Chinese, words are not necessarily delimited by white space in running text. How-
ever, in some low-resource languages (Myanmar being one) broad-coverage word segmentation tools are
scarce, and there are two common approaches to dealing with this issue. The first is to apply unsuper-
vised word segmentation tools to a body of monolingual text in order to induce a segmentation. The
second is to use a dictionary of words in the language together with a set of heuristics to identify word
boundaries in text.
Myanmar language can be accurately segmented into a sequence of syllables using finite state au-
tomata (examples being (Berment, 2004; Thu et al., 2013a)). However, words composed of single or
multiple syllables are not usually separated by white space. Although spaces are sometimes used for
separating phrases for easier reading, it is not strictly necessary, and these spaces are rarely used in short
sentences. There are no clear rules for using spaces in Myanmar language, and thus spaces may (or
may not) be inserted between words, phrases, and even between a root word and its affixes. Myanmar
language is a resource-poor language and large corpora, lexical resources, and grammatical dictionaries
are not yet widely available. For this reason, using corpus-based machine learning techniques to develop
word segmentation tools is a challenging task.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
</bodyText>
<page confidence="0.946761">
20
</page>
<note confidence="0.9008985">
Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 20–27,
Dublin, Ireland, August 23-29 2014.
</note>
<sectionHeader confidence="0.999019" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999985393939394">
In this section, we will briefly introduce some proposed word segmentation methods with an emphasis
on the schemes that have been applied to Myanmar.
Many word segmentation methods have been proposed especially for the Thai, Khmer, Lao, Chi-
nese and Japanese languages. These methods can be roughly classified into dictionary-based (Sorn-
lertlamvanich, 1993; Srithirath and Seresangtakul, 2013) and statistical methods (Wu and Tseng, 1993;
Maosong et al., 1998; Papageorgiou and P., 1994; Mochihashi et al., 2009; Jyun-Shen et al., 1991). In
dictionary-based methods, only words that are stored in the dictionary can be identified and the perfor-
mance depends to a large degree upon the coverage of the dictionary. New words appear constantly and
thus, increasing size of the dictionary is a not a solution to the out of vocabulary word (OOV) problem.
On the other hand, although statistical approaches can identify unknown words by utilizing probabilistic
or cost-based scoring mechanisms, they also suffer from some drawbacks. The main issues are: they
require large amounts of data; the processing time required; and the difficulty in incorporating linguistic
knowledge effectively into the segmentation process (Teahan et al., 2000). For low-resource languages
such as Myanmar, there is no freely available corpus and dictionary based or rule based methods are
being used as a temporary solution.
If we only focus on Myanmar language word segmentation, as far as the authors are aware there have
been only two published methodologies, and one study. Both of the proposed methodologies operate
according using a process of syllable breaking followed by Maximum Matching; the differences in the
approaches come from the manner in which the segmentation boundary decision is made. In (Thet et al.,
2008) statistical information is used (based on bigram information), whereas (Htay and Murthy, 2008)
utilize a word list extracted from a monolingual Myanmar corpus.
In a related study (Thu et al., 2013a), various Myanmar word segmentation approaches including
character segmentation, syllable segmentation, human lexical/phrasal segmentation, unsupervised and
semi-supervised word segmentation, were investigated. They reported that the highest quality machine
translation was attained either without word segmentation using simply sequences of syllables, or by
a process of Maximum Matching with a monolingual dictionary. In this study the effectiveness of ap-
proaches unsupervised word segmentation using latticelm (with 3-gram to 7-gram language models) and
supervised word segmentation using KyTea was evaluated, however, none of the approaches was able to
match the performance of the simpler syllable/Maximum Matching techniques.
In (Pei et al., 2013) an unsupervised Bayesian word segmentation scheme was augmented by using a
dictionary of words. These words were obtained from segmenting the data using another unsupervised
word segmenter. The probability distribution over these words was calculated from occurrence counts,
and this distribution was interpolated into the base measure.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.999857">
3.1 Baseline Non-parametric Bayesian Segmentation Model
</subsectionHeader>
<bodyText confidence="0.99998325">
The baseline system, and the model that forms the basis for all of the models is a non-parametric Bayesian
unsupervised word segmenter similar to that proposed in (Goldwater et al., 2009). The major differences
being the sampling strategy and the base measure. The principles behind this segmenter are described
below.
Intuitively, the model has two basic components: a model for generating an outcome that has already
been generated at least once before, and a second model that assigns a probability to an outcome that
has not yet been produced. Ideally, to encourage the re-use of model parameters, the probability of
generating a novel segment should be considerably lower then the probability of generating a previously
observed segment. This is a characteristic of the Dirichlet process model we use and furthermore, the
model has a preference to generate new segments early on in the process, but is much less likely to do so
later on. In this way, as the cache becomes more and more reliable and complete, so the model prefers to
use it rather than generate novel segments. The probability distribution over these segments (including an
infinite number of unseen segments) can be learned directly from unlabeled data by Bayesian inference
of the hidden segmentation of the corpus.
The underlying stochastic process for the generation of a corpus composed of segments sk is usually
written in the following from:
</bodyText>
<page confidence="0.951181">
21
</page>
<equation confidence="0.9981115">
G|a,G0 - DP(α, G0)
sk|G - G (1)
</equation>
<bodyText confidence="0.994253">
G is a discrete probability distribution over the all segments according to a Dirichlet process prior
with base measure G0 and concentration parameter α. The concentration parameter α &gt; 0 controls the
variance of G; intuitively, the larger α is, the more similar G0 will be to G.
</bodyText>
<subsectionHeader confidence="0.833824">
3.1.1 The Base Measure
</subsectionHeader>
<bodyText confidence="0.999715">
For the base measure G0 that controls the generation of novel sequence-pairs, we use a spelling model
that assigns probability to new segments according to the following distribution:
</bodyText>
<equation confidence="0.999452">
G0(s) = p(|s|)p(s||s|)
A|�|
= |s|! e
−a|V |−|s |(2)
</equation>
<bodyText confidence="0.999748375">
where |s |is the number of tokens in the segment; |V  |and is the token set size; and A is the expected
length of the segments.
According to this model, the segment length is chosen from a Poisson distribution, and then the el-
ements of the segment itself is generated given the length. Note that this model is able to assign a
probability to arbitrary sequences of tokens drawn from the set of tokens V (in this paper V is the set
of all Myanmar syllables). The motivation for using a base measure of this form, is to overcome issues
with overfitting when training the model; other base measures are possible for example the enhancement
proposed in Section 3.4.
</bodyText>
<subsectionHeader confidence="0.927134">
3.1.2 The Generative Model
</subsectionHeader>
<bodyText confidence="0.998621333333333">
The generative model is given in Equation 3 below. The equation assignes a probability to the kth
segment sk in a derivation of the corpus, given all of the other segments in the history so far s−k. Here
-k is read as: “up to but not including k”.
</bodyText>
<equation confidence="0.98617">
N(sk) + αG0(sk) (3)
p(sk |s−k) = N + α
</equation>
<bodyText confidence="0.999755">
In this equation, N is the total number of segments generated so far, N(sk) is the number of times the
segment sk has occurred in the history. G0 and α are the base measure and concentration parameter as
before.
</bodyText>
<subsectionHeader confidence="0.792898">
3.1.3 Bayesian Inference
</subsectionHeader>
<bodyText confidence="0.999601533333333">
We used a blocked version of a Gibbs sampler for training. In (Goldwater et al., 2006) they report
issues with mixing in the sampler that were overcome using annealing. In (Mochihashi et al., 2009)
this issue was overcome by using a blocked sampler together with a dynamic programming approach.
Our algorithm is an extension of application the forward filtering backward sampling (FFBS) algorithm
(Scott, 2002) to the problem of word segmentation presented in (Mochihashi et al., 2009). We extend
their approach to handle the joint segmentation and alignment of character sequences. We refer the reader
to (Mochihashi et al., 2009) for a complete description of the FFBS process. In essence the process uses
a forward variable at each node in the segmentation graph to store the probability of reaching the node
from the source node of the graph. These forward variables are calculated efficiently in a single forward
pass through the graph, from source node to sink node (forward filtering). During backward sampling, a
single path through the segmentation graph is sampled in accordance with its probability. This sampling
process uses the forward variables calculated in the forward filtering step.
In each iteration of the training process, each entry in the training corpus was sampled without re-
placement; its segmentation was removed and the models were updated to reflect this. Then a new
segmentation for the sequence was chosen using the FFBS process, and the models were updated with
</bodyText>
<page confidence="0.98274">
22
</page>
<bodyText confidence="0.99899725">
the counts from this new segmentation. The two hyperparameters, the Dirichlet concentration parameter
α, and the Poisson rate parameter A were set by slice sampling using vague priors (a Gamma prior in the
case of α and the Jeffreys prior was used for A). The token set size V used in the base measure was set
to the number of types in the training corpus, and V = 3363.
</bodyText>
<subsectionHeader confidence="0.991106">
3.2 Dictionary Augmented Model
</subsectionHeader>
<bodyText confidence="0.999960565217391">
The dictionary augmented model is in essence the same model as proposed by (Thu et al., 2013b), but
a different dictionary was used. Their method integrates dictionary-based word segmentation (similar to
the maximum matching approaches used successfully in (Thet et al., 2008; Htay and Murthy, 2008; Thu
et al., 2013a) ) into a fully unsupervised Bayesian word segmentation scheme.
Dictionary-based word segmentation has the advantage of being able to exploit human knowledge
about the sequences of characters in the language that are used to form words. This approach is simple
and has proven to be a very effective technique in previous studies. Problems arise due to the coverage
of the dictionary. The dictionary may not be able to cover the running text well, for example in the case
of low-resource languages the dictionary might be small, or in the case of named entities, even though a
comprehensive dictionary of common words may exist, it is likely to fall far short of covering all of the
words that can occur in the language.
Unsupervised word segmentation techniques, have high coverage. They are able to learn how to
segment by discovering patterns in the text that recur. The weakness of these approaches is that they
have no explicit knowledge of how words are formed in the language, and the sequences they discover
from text may simply be sequences in text that frequently occur and may bear no relationship to actual
words in the language. As such these units, although they are useful in the context of the generative
model used to discover them, may not be appropriate for use in an application that might benefit from
these segments being words in the language. We believe that machine translation is one such application.
This method gives the unsupervised method a means of exploiting a dictionary of words in its training
process, by allowing the integrated method to use the dictionary to segment text when appropriate, and
otherwise use its unsupervised models to handle the segmentation. To do this a separate dictionary
generation process is integrated into the generative model of the unsupervised segmenter to create a
semi-supervised segmenter that segments using a single unified generative model.
</bodyText>
<subsectionHeader confidence="0.994736">
3.3 Dictionary Set Augmented Model
</subsectionHeader>
<bodyText confidence="0.9999735">
In this model, the a set of subsets of the dictionary were extracted based on the part-of-speech labels
contained in the dictionary (Lwin, 1993). This set of subsets was not a partition of the original dictionary
since some of the types in the dictionary were ambiguous causing some overlap of the subsets. In the
previous model, during the generative process a decision was made, with a certain probability learned
from the data, as to whether the segment would be generated from the unsupervised sub-model or the
dictionary sub-model. In this model, the decision to generate from the dictionary model is refined into a
number of decisions to generate from a number of subsets of the dictionary, each with its own probability.
These probabilities were re-estimated from the sampled segmentation of the corpus at the end of each
iteration of the training (in a similar manner to the dictionary augmented model). A diagram showing
the generative process is shown in Figure 1.
</bodyText>
<subsectionHeader confidence="0.758149">
3.4 Language Model Augmented Model
</subsectionHeader>
<bodyText confidence="0.999942083333333">
In (Theeramunkong and Usanavasin, 2001) dictionary approaches were deliberately avoided in order
to address issues with unknown words. Instead a decision tree model for segmentation was proposed.
Our approach although different in character (since a generative model is used), shares the insight that
knowledge of how words are constructed is key to segmentation when dictionary information is absent.
In this model we used the dictionary resource, but in a more indirect manner. We use a language model
to capture the notion of exactly what constitutes a segment. To do this words in the dictionary were first
segmented into syllables. Then, a language model was trained on this segmented dictionary. This model
will assign high probabilities to the words it has been trained on, and therefore in some sense is able
to capture the spirit of the dictionary-based methods described previously. However, it will also have
learned something about the way Myanmar words are composed from syllables, and can be expected to
assign a higher probability to unknown words that resemble the words it has been trained on, than to
sequences of syllables that are not consistent with its training data.
</bodyText>
<page confidence="0.99293">
23
</page>
<figure confidence="0.659789">
P(unsupervised) DP Model
</figure>
<figureCaption confidence="0.9850045">
Figure 1: Generative process with multiple dictionaries competing to generate the data alongside an
unsupervised Dirichlet process model.
</figureCaption>
<bodyText confidence="0.7618055">
This model can be naturally introduced directly into the Dirichlet process model as a component of
the base measure. Equation 2 decomposes into two terms:
</bodyText>
<listItem confidence="0.996474333333333">
1. A Poisson probability mass function: �|s|
|�|� e−�`
2. A uniform distribution over the vocabulary: |V ||s |.
</listItem>
<bodyText confidence="0.9999544">
The first term above models the choice of length for the segment. The second term models the choice
of syllables that comprise the segment is in essence a unigram language model that provides little infor-
mation about segments are constructed, and serves simply to discourage the formation of long segments
that would lead to overfitting. We directly replace the part of the base measure with our more informative
language model built from the dictionary.
</bodyText>
<sectionHeader confidence="0.99929" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.89246">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.999988666666667">
In the experimental section we aim to analyze two aspects of the performance of the proposed segmen-
tation approaches. Firstly their segmentation quality and secondly for those approaches that are capable
of partially labeling the corpus, the accuracy of the labeling.
</bodyText>
<subsectionHeader confidence="0.948527">
4.2 Corpora
</subsectionHeader>
<bodyText confidence="0.999947166666667">
For all our experiments we used a 160K-sentence subset of the Basic Travel Expression (BTEC) corpus
(Kikui et al., 2003), for the Myanmar language. This corpus was segmented using an accurate rule-based
approach into individual syllables, and this segmentation was used as the base segmentation in all our
experiments.
In addition a test corpus was made by sampling 150-sentences randomly from the corpus. This corpus
was then segmented by hand and the segments were annotated manually using the set of POS tags that
our system was capable of annotating, together with an ‘UNK’ tag to annotate segments that fell out of
this scope. The test sentences were included in the data used for the training of the Bayesian models.
These sentences were treated in the same manner the rest of the data in that they were initially syllable
segmented. At the end of the training, the test sentences together with the segmentation assigned by the
training, were extracted from the corpus, and their segmentation/labeling was evaluated with reference
to the human annotation.
</bodyText>
<subsectionHeader confidence="0.999512">
4.3 Segmentation Performance
</subsectionHeader>
<bodyText confidence="0.9999434">
We used the Edit Distance of the Word Separator (EDWS) to evaluate the segmentation performance of
the models. This technique yields precision, recall and F-score statistics from the edit operations required
to transform one segmented sequence into another by means of the insertion, deletion and substitution of
segmentation boundaries. In this measure the substitution operator corresponds to an identity substitution
and therefore indicates a correct segment boundary. Insertions correspond to segmentation boundaries in
</bodyText>
<page confidence="0.997647">
24
</page>
<table confidence="0.999725142857143">
Method Precision Recall F-score Sub Ins Del
Unsupervised 82.27 33.82 0.48 348 681 75
Maximum Matching 78.39 99.42 0.88 1023 6 282
Dictionary 89.67 57.34 0.70 590 439 68
Dictionary Set 89.46 51.99 0.66 535 494 63
Language Model (LM) 88.15 31.10 0.46 320 709 43
Dictionary Set + LM 91.27 49.76 0.64 512 517 49
</table>
<tableCaption confidence="0.999871">
Table 1: Segmentation Performance.
</tableCaption>
<bodyText confidence="0.65437375">
the segmented output that do not correspond to any segmentation boundary in the reference. Deletions
correspond to segmentation boundaries in the reference that do not correspond to any segmentation
boundary in the segmented output. Precision recall and F-score are calculated as follows using the
Chinese Word Segmentation Evaluation Tookit (Joy, 2004):
</bodyText>
<equation confidence="0.969011625">
#substitutions
precision =
#segment boundaries in output
#substitutions
recall =
#segment boundaries in reference
F-score = 2 * precision * recall
precision + recall
</equation>
<bodyText confidence="0.9998794">
Table 1 shows the segmentation performance all of the systems. In terms of precision we see an im-
provement when the additional models are added to the baseline unsupervised model. The maximum
matching strategy has the lowest precision but the highest recall, this is due to the over-generation of
segmentation boundaries in regions where no dictionary matches are possible. In these regions the ref-
erence segmentation boundaries are always annotated (since the model defaults to syllable segmentation
in these regions), but at the expense of precision. This is reflected in the relatively low numbers of inser-
tions (6), and the relatively high number of deletions (282). As expected the dictionary set approach gave
similar performance to the Dictionary approach. The language model approach produced a respectable
level of precision, but a low value for recall. When integrated into the dictionary-based models however,
it was able to increase precision.
</bodyText>
<subsectionHeader confidence="0.999892">
4.4 Labeling Accuracy
</subsectionHeader>
<bodyText confidence="0.999991">
We evaluated the accuracy of the labeling on two methods: the first method used only a set of dictio-
naries (as described in Section 3.3). This method was able to label with an accuracy of 64.53%. The
second method consisted of the same technique, but with the addition of the language model trained on
the syllable segmented dictionaries (as described in Section 3.4). We found that the dictionary-based
language model was able to improved the labeling accuracy 1.2% to 65.73%.
</bodyText>
<sectionHeader confidence="0.936796" genericHeader="evaluation">
5 Examples and Analysis
</sectionHeader>
<bodyText confidence="0.99948">
Figure 2 shows an example an unsupervised segmentation with a typical error. Frequent words are often
attached to neighboring words to form erroneous compound segments. In this example, taken from real
output of the segmenter, the word ‘De’ (this) has been attached to the word ‘SarOuk’ (book), and similar
the words in the phrase ‘KaBeMharLe’ (where is it) have all been segmented as a single segment (which
occurs frequently in the corpus).
In Figure 3, a typical segmentation from the maximum matcher is shown. In this example the word
‘BarThar’ (language) occurs in the dictionary but the word ‘JaPan’ (Japan) does not. The maximum
matcher defaults to segmenting the word for Japan into its component syllables, whereas the unsuper-
vised segmenter with dictionary has attempted an unsupervised segmentation on this part of the string.
The word ‘Japan’ occurs sufficiently frequently in the BTEC corpus that the segmenter has been able to
</bodyText>
<page confidence="0.991024">
25
</page>
<figure confidence="0.968152916666667">
this book
DeSarOuk
where is it.
mood�
KaBeMharLe
a
3
C
U
0D3?
II
YDCU
</figure>
<figureCaption confidence="0.99268">
Figure 2: An unsupervised segmentation.
</figureCaption>
<bodyText confidence="0.99448175">
learn the word during training and has thereby managed to successfully segment the word in the output.
The word for language was segmented by means of the embedded dictionary model.
Figure 4 shows an example of the partial labeling produced from the model that used a set of dictio-
naries in combination with a dictionary-based language model in the base measure. Due to the small
amount of resources available, substantial parts of the sequence are unable to be labeled (and are anno-
tated with the ‘U’ tag in the figure, indicating that they were segmented by the unsupervised component
of the model). The remainder of the words are annotated with POS tags corresponding to the dictionary
they were generated from.
</bodyText>
<figureCaption confidence="0.992555">
Figure 3: A segmentation from maximum matching.
</figureCaption>
<figure confidence="0.99971495">
Japan
ဂ&amp;quot;ပန%
Ja Pan
N/A
ဂ&amp;quot;
Ja
Unsupervised with dictionary
Maximum Matching
N/A
ပန%
Pan
language
ဘ&apos;သ&apos;
BarThar
language
ဘ&apos;သ&apos;
BarThar
I manager phonecall doing .
က&amp;quot;န$ eတ&apos;$/PRO မန$ eနဂ*&apos;/NS ဖuန$-ကiu eခ0/U eနတ&apos;ပ2/U ။/P
KyunDaw ManNayJar PhoneGoKhaw NayDarBar
</figure>
<figureCaption confidence="0.999909">
Figure 4: A partially labeled segmentation.
</figureCaption>
<sectionHeader confidence="0.998065" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998480888888889">
In this paper we have proposed and investigated the effectiveness of several methods intended to exploit
limited quantities of dictionary resources available for low resource languages. Our results show that
by integrating a dictionary directly into an unsupervised word segmenter we were able to improve both
precision and recall. We found that attempting to model word formation using a language model on
its own was ineffective compared with the approaches that directly used a dictionary. However, this
language model proved useful when used in conjunction with the direct dictionary-based models, where
it served to assist the modeling of words that were not in the dictionary. In future work we intend to
develop the dictionary set approach by extending it to introduce basic knowledge of the morphological
structure of the language directly into the model.
</bodyText>
<sectionHeader confidence="0.992505" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.737645">
Vincent Berment. 2004. Sylla and gmsword: applications to myanmar languages computerization. In Burma
Studies Conference.
</reference>
<page confidence="0.983304">
26
</page>
<reference confidence="0.999158490196078">
Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Contextual dependencies in unsupervised word
segmentation. In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and
the 44th annual meeting of the Association for Computational Linguistics, pages 673–680, Morristown, NJ,
USA. Association for Computational Linguistics.
Sharon Goldwater, Thomas L Griffiths, and Mark Johnson. 2009. A bayesian framework for word segmentation:
Exploring the effects of context. Cognition, 112(1):21–54.
Hla Hla Htay and Kavi Narayana Murthy. 2008. Myanmar word segmentation using syllable level longest match-
ing. In IJCNLP, pages 41–48.
Joy, 2004. Chinese Word Segmentation Evaluation Toolkit.
Chang Jyun-Shen, Chi-Dah Chen, and Shun-Der Chen. 1991. Chinese word segmentation through constraint
satisfaction and statistical optimization. In Proceedings of ROC Computational Linguistics Conference, pages
147–165.
G. Kikui, E. Sumita, T. Takezawa, and S. Yamamoto. 2003. Creating corpora for speech-to-speech translation. In
Proceedings of EUROSPEECH-03, pages 381–384.
San Lwin. 1993. Myanmar - English Dictionary. Department of the Myanmar Language Commission, Ministry
of Education, Union of Myanmar.
Sun Maosong, Shen Dayang, and Benjamin K. Tsou. 1998. Chinese word segmentation without using lexicon and
hand-crafted training data. In Proceedings of the 17th international conference on Computational linguistics -
Volume 2, COLING ’98, pages 1265–1271, Stroudsburg, PA, USA. Association for Computational Linguistics.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with
nested pitman-yor language modeling. In ACL-IJCNLP ’09: Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the
AFNLP: Volume 1, pages 100–108, Morristown, NJ, USA. Association for Computational Linguistics.
Papageorgiou and Constantine P. 1994. Japanese word segmentation by hidden markov model. In Proceedings of
the workshop on Human Language Technology, HLT ’94, pages 283–288, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Wenzhe Pei, Dongxu Han, and Baobao Chang. 2013. A refined hdp-based model for unsupervised chinese word
segmentation. In Maosong Sun, Min Zhang, Dekang Lin, and Haifeng Wang, editors, Chinese Computational
Linguistics and Natural Language Processing Based on Naturally Annotated Big Data, volume 8202 of Lecture
Notes in Computer Science, pages 44–51. Springer Berlin Heidelberg.
Steven L Scott. 2002. Bayesian methods for hidden markov models : Recursive computing in the 21st century.
Journal of the American Statistical Association, 97(457):337–351.
Virach Sornlertlamvanich. 1993. Word segmentation for thai in machine translation system. Machine Translation,
National Electronics and Computer Technology Center, Bangkok, pages 50–56.
A Srithirath and P. Seresangtakul. 2013. A hybrid approach to lao word segmentation using longest syllable level
matching with named entities recognition. In Electrical Engineering/Electronics, Computer, Telecommunica-
tions and Information Technology (ECTI-CON), 2013 10th International Conference on, pages 1–5, May.
W. J. Teahan, Rodger McNab, Yingying Wen, and Ian H. Witten. 2000. A compression-based algorithm for
chinese word segmentation. Comput. Linguist., 26(3):375–393, September.
Thanaruk Theeramunkong and Sasiporn Usanavasin. 2001. Non-dictionary-based thai word segmentation us-
ing decision trees. In In Proceedings of the First International Conference on Human Language Technology
Research.
Tun Thura Thet, Jin-Cheon Na, and Wunna Ko Ko. 2008. Word segmentation for the myanmar language. J.
Information Science, 34(5):688–704.
Ye Kyaw Thu, Andrew Finch, Yoshinori Sagisaka, and Eiichiro Sumita. 2013a. A study of myanmar word
segmentation schemes for statistical machine translation. Proceeding of the 11th International Conference on
Computer Applications, pages 167–179.
Ye Kyaw Thu, Andrew Finch, Eiichiro Sumita, and Yoshinori Sagisaka. 2013b. Unsupervised and semi-supervised
myanmar word segmentation approaches for statistical machine translation.
Zinmin Wu and Gwyneth Tseng. 1993. Chinese text segmentation for text retrieval: Achievements and problems.
Journal of the American Society for Information Science, 44(5):532–542.
</reference>
<page confidence="0.998805">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.007532">
<title confidence="0.9725385">Integrating Dictionaries into an Unsupervised Model Myanmar Word Segmentation</title>
<author confidence="0.919495">Ye Kyaw</author>
<affiliation confidence="0.749166">Keihanna Science Kyoto,</affiliation>
<email confidence="0.929473">yekyawthu@nict.go.jp</email>
<affiliation confidence="0.634073">Eiichiro Keihanna Science</affiliation>
<address confidence="0.414314">Kyoto,</address>
<email confidence="0.968482">eiichiro.sumita@nict.go.jp</email>
<author confidence="0.873827">Andrew</author>
<affiliation confidence="0.7370015">Keihanna Science Kyoto,</affiliation>
<email confidence="0.815685">andrew.finch@nict.go.jp</email>
<author confidence="0.691137">Yoshinori</author>
<affiliation confidence="0.9133565">GITI/Speech Science Research Waseda</affiliation>
<address confidence="0.587611">Tokyo,</address>
<email confidence="0.999887">ysagisaka@gmail.com</email>
<abstract confidence="0.999431071428571">This paper addresses the problem of word segmentation for low resource languages, with the main focus being on Myanmar language. In our proposed method, we focus on exploiting limited amounts of dictionary resource, in an attempt to improve the segmentation quality of an unsupervised word segmenter. Three models are proposed. In the first, a set of dictionaries (separate dictionaries for different classes of words) are directly introduced into the generative model. In the second, a language model was built from the dictionaries, and the n-gram model was inserted into the generative model. This model was expected to model words that did not occur in the training data. The third model was a combination of the previous two models. We evaluated our approach on a corpus of manually annotated data. Our results show that the proposed methods are able to improve over a fully unsupervised baseline system. The best of our systems improved the F-score from 0.48 to 0.66. In addition to segmenting the data, one proposed method is also able to partially label the segmented corpus with POS tags. We found that these labels were approximately 66% accurate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Vincent Berment</author>
</authors>
<title>Sylla and gmsword: applications to myanmar languages computerization.</title>
<date>2004</date>
<booktitle>In Burma Studies Conference.</booktitle>
<contexts>
<context position="2565" citStr="Berment, 2004" startWordPosition="391" endWordPosition="392">cessarily delimited by white space in running text. However, in some low-resource languages (Myanmar being one) broad-coverage word segmentation tools are scarce, and there are two common approaches to dealing with this issue. The first is to apply unsupervised word segmentation tools to a body of monolingual text in order to induce a segmentation. The second is to use a dictionary of words in the language together with a set of heuristics to identify word boundaries in text. Myanmar language can be accurately segmented into a sequence of syllables using finite state automata (examples being (Berment, 2004; Thu et al., 2013a)). However, words composed of single or multiple syllables are not usually separated by white space. Although spaces are sometimes used for separating phrases for easier reading, it is not strictly necessary, and these spaces are rarely used in short sentences. There are no clear rules for using spaces in Myanmar language, and thus spaces may (or may not) be inserted between words, phrases, and even between a root word and its affixes. Myanmar language is a resource-poor language and large corpora, lexical resources, and grammatical dictionaries are not yet widely available</context>
</contexts>
<marker>Berment, 2004</marker>
<rawString>Vincent Berment. 2004. Sylla and gmsword: applications to myanmar languages computerization. In Burma Studies Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>673--680</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10147" citStr="Goldwater et al., 2006" startWordPosition="1585" endWordPosition="1588">rative Model The generative model is given in Equation 3 below. The equation assignes a probability to the kth segment sk in a derivation of the corpus, given all of the other segments in the history so far s−k. Here -k is read as: “up to but not including k”. N(sk) + αG0(sk) (3) p(sk |s−k) = N + α In this equation, N is the total number of segments generated so far, N(sk) is the number of times the segment sk has occurred in the history. G0 and α are the base measure and concentration parameter as before. 3.1.3 Bayesian Inference We used a blocked version of a Gibbs sampler for training. In (Goldwater et al., 2006) they report issues with mixing in the sampler that were overcome using annealing. In (Mochihashi et al., 2009) this issue was overcome by using a blocked sampler together with a dynamic programming approach. Our algorithm is an extension of application the forward filtering backward sampling (FFBS) algorithm (Scott, 2002) to the problem of word segmentation presented in (Mochihashi et al., 2009). We extend their approach to handle the joint segmentation and alignment of character sequences. We refer the reader to (Mochihashi et al., 2009) for a complete description of the FFBS process. In ess</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 673–680, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<contexts>
<context position="7042" citStr="Goldwater et al., 2009" startWordPosition="1048" endWordPosition="1051">iques. In (Pei et al., 2013) an unsupervised Bayesian word segmentation scheme was augmented by using a dictionary of words. These words were obtained from segmenting the data using another unsupervised word segmenter. The probability distribution over these words was calculated from occurrence counts, and this distribution was interpolated into the base measure. 3 Methodology 3.1 Baseline Non-parametric Bayesian Segmentation Model The baseline system, and the model that forms the basis for all of the models is a non-parametric Bayesian unsupervised word segmenter similar to that proposed in (Goldwater et al., 2009). The major differences being the sampling strategy and the base measure. The principles behind this segmenter are described below. Intuitively, the model has two basic components: a model for generating an outcome that has already been generated at least once before, and a second model that assigns a probability to an outcome that has not yet been produced. Ideally, to encourage the re-use of model parameters, the probability of generating a novel segment should be considerably lower then the probability of generating a previously observed segment. This is a characteristic of the Dirichlet pr</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L Griffiths, and Mark Johnson. 2009. A bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hla Hla Htay</author>
<author>Kavi Narayana Murthy</author>
</authors>
<title>Myanmar word segmentation using syllable level longest matching.</title>
<date>2008</date>
<booktitle>In IJCNLP,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="5590" citStr="Htay and Murthy, 2008" startWordPosition="845" endWordPosition="848">mar, there is no freely available corpus and dictionary based or rule based methods are being used as a temporary solution. If we only focus on Myanmar language word segmentation, as far as the authors are aware there have been only two published methodologies, and one study. Both of the proposed methodologies operate according using a process of syllable breaking followed by Maximum Matching; the differences in the approaches come from the manner in which the segmentation boundary decision is made. In (Thet et al., 2008) statistical information is used (based on bigram information), whereas (Htay and Murthy, 2008) utilize a word list extracted from a monolingual Myanmar corpus. In a related study (Thu et al., 2013a), various Myanmar word segmentation approaches including character segmentation, syllable segmentation, human lexical/phrasal segmentation, unsupervised and semi-supervised word segmentation, were investigated. They reported that the highest quality machine translation was attained either without word segmentation using simply sequences of syllables, or by a process of Maximum Matching with a monolingual dictionary. In this study the effectiveness of approaches unsupervised word segmentation</context>
<context position="12259" citStr="Htay and Murthy, 2008" startWordPosition="1928" endWordPosition="1931">oncentration parameter α, and the Poisson rate parameter A were set by slice sampling using vague priors (a Gamma prior in the case of α and the Jeffreys prior was used for A). The token set size V used in the base measure was set to the number of types in the training corpus, and V = 3363. 3.2 Dictionary Augmented Model The dictionary augmented model is in essence the same model as proposed by (Thu et al., 2013b), but a different dictionary was used. Their method integrates dictionary-based word segmentation (similar to the maximum matching approaches used successfully in (Thet et al., 2008; Htay and Murthy, 2008; Thu et al., 2013a) ) into a fully unsupervised Bayesian word segmentation scheme. Dictionary-based word segmentation has the advantage of being able to exploit human knowledge about the sequences of characters in the language that are used to form words. This approach is simple and has proven to be a very effective technique in previous studies. Problems arise due to the coverage of the dictionary. The dictionary may not be able to cover the running text well, for example in the case of low-resource languages the dictionary might be small, or in the case of named entities, even though a comp</context>
</contexts>
<marker>Htay, Murthy, 2008</marker>
<rawString>Hla Hla Htay and Kavi Narayana Murthy. 2008. Myanmar word segmentation using syllable level longest matching. In IJCNLP, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joy</author>
</authors>
<title>Chinese Word Segmentation Evaluation Toolkit.</title>
<date>2004</date>
<contexts>
<context position="19936" citStr="Joy, 2004" startWordPosition="3155" endWordPosition="3156">aximum Matching 78.39 99.42 0.88 1023 6 282 Dictionary 89.67 57.34 0.70 590 439 68 Dictionary Set 89.46 51.99 0.66 535 494 63 Language Model (LM) 88.15 31.10 0.46 320 709 43 Dictionary Set + LM 91.27 49.76 0.64 512 517 49 Table 1: Segmentation Performance. the segmented output that do not correspond to any segmentation boundary in the reference. Deletions correspond to segmentation boundaries in the reference that do not correspond to any segmentation boundary in the segmented output. Precision recall and F-score are calculated as follows using the Chinese Word Segmentation Evaluation Tookit (Joy, 2004): #substitutions precision = #segment boundaries in output #substitutions recall = #segment boundaries in reference F-score = 2 * precision * recall precision + recall Table 1 shows the segmentation performance all of the systems. In terms of precision we see an improvement when the additional models are added to the baseline unsupervised model. The maximum matching strategy has the lowest precision but the highest recall, this is due to the over-generation of segmentation boundaries in regions where no dictionary matches are possible. In these regions the reference segmentation boundaries are</context>
</contexts>
<marker>Joy, 2004</marker>
<rawString>Joy, 2004. Chinese Word Segmentation Evaluation Toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Jyun-Shen</author>
<author>Chi-Dah Chen</author>
<author>Shun-Der Chen</author>
</authors>
<title>Chinese word segmentation through constraint satisfaction and statistical optimization.</title>
<date>1991</date>
<booktitle>In Proceedings of ROC Computational Linguistics Conference,</booktitle>
<pages>147--165</pages>
<marker>Jyun-Shen, Chen, Shun-Der Chen, 1991</marker>
<rawString>Chang Jyun-Shen, Chi-Dah Chen, and Shun-Der Chen. 1991. Chinese word segmentation through constraint satisfaction and statistical optimization. In Proceedings of ROC Computational Linguistics Conference, pages 147–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kikui</author>
<author>E Sumita</author>
<author>T Takezawa</author>
<author>S Yamamoto</author>
</authors>
<title>Creating corpora for speech-to-speech translation.</title>
<date>2003</date>
<booktitle>In Proceedings of EUROSPEECH-03,</booktitle>
<pages>381--384</pages>
<contexts>
<context position="17742" citStr="Kikui et al., 2003" startWordPosition="2813" endWordPosition="2816"> discourage the formation of long segments that would lead to overfitting. We directly replace the part of the base measure with our more informative language model built from the dictionary. 4 Experiments 4.1 Overview In the experimental section we aim to analyze two aspects of the performance of the proposed segmentation approaches. Firstly their segmentation quality and secondly for those approaches that are capable of partially labeling the corpus, the accuracy of the labeling. 4.2 Corpora For all our experiments we used a 160K-sentence subset of the Basic Travel Expression (BTEC) corpus (Kikui et al., 2003), for the Myanmar language. This corpus was segmented using an accurate rule-based approach into individual syllables, and this segmentation was used as the base segmentation in all our experiments. In addition a test corpus was made by sampling 150-sentences randomly from the corpus. This corpus was then segmented by hand and the segments were annotated manually using the set of POS tags that our system was capable of annotating, together with an ‘UNK’ tag to annotate segments that fell out of this scope. The test sentences were included in the data used for the training of the Bayesian model</context>
</contexts>
<marker>Kikui, Sumita, Takezawa, Yamamoto, 2003</marker>
<rawString>G. Kikui, E. Sumita, T. Takezawa, and S. Yamamoto. 2003. Creating corpora for speech-to-speech translation. In Proceedings of EUROSPEECH-03, pages 381–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>San Lwin</author>
</authors>
<title>Myanmar -</title>
<date>1993</date>
<institution>English Dictionary. Department of the Myanmar Language Commission, Ministry of Education, Union of Myanmar.</institution>
<contexts>
<context position="14371" citStr="Lwin, 1993" startWordPosition="2276" endWordPosition="2277">xploiting a dictionary of words in its training process, by allowing the integrated method to use the dictionary to segment text when appropriate, and otherwise use its unsupervised models to handle the segmentation. To do this a separate dictionary generation process is integrated into the generative model of the unsupervised segmenter to create a semi-supervised segmenter that segments using a single unified generative model. 3.3 Dictionary Set Augmented Model In this model, the a set of subsets of the dictionary were extracted based on the part-of-speech labels contained in the dictionary (Lwin, 1993). This set of subsets was not a partition of the original dictionary since some of the types in the dictionary were ambiguous causing some overlap of the subsets. In the previous model, during the generative process a decision was made, with a certain probability learned from the data, as to whether the segment would be generated from the unsupervised sub-model or the dictionary sub-model. In this model, the decision to generate from the dictionary model is refined into a number of decisions to generate from a number of subsets of the dictionary, each with its own probability. These probabilit</context>
</contexts>
<marker>Lwin, 1993</marker>
<rawString>San Lwin. 1993. Myanmar - English Dictionary. Department of the Myanmar Language Commission, Ministry of Education, Union of Myanmar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sun Maosong</author>
<author>Shen Dayang</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Chinese word segmentation without using lexicon and hand-crafted training data.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics -Volume 2, COLING ’98,</booktitle>
<pages>1265--1271</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4152" citStr="Maosong et al., 1998" startWordPosition="622" endWordPosition="625">hop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 20–27, Dublin, Ireland, August 23-29 2014. 2 Related Work In this section, we will briefly introduce some proposed word segmentation methods with an emphasis on the schemes that have been applied to Myanmar. Many word segmentation methods have been proposed especially for the Thai, Khmer, Lao, Chinese and Japanese languages. These methods can be roughly classified into dictionary-based (Sornlertlamvanich, 1993; Srithirath and Seresangtakul, 2013) and statistical methods (Wu and Tseng, 1993; Maosong et al., 1998; Papageorgiou and P., 1994; Mochihashi et al., 2009; Jyun-Shen et al., 1991). In dictionary-based methods, only words that are stored in the dictionary can be identified and the performance depends to a large degree upon the coverage of the dictionary. New words appear constantly and thus, increasing size of the dictionary is a not a solution to the out of vocabulary word (OOV) problem. On the other hand, although statistical approaches can identify unknown words by utilizing probabilistic or cost-based scoring mechanisms, they also suffer from some drawbacks. The main issues are: they requir</context>
</contexts>
<marker>Maosong, Dayang, Tsou, 1998</marker>
<rawString>Sun Maosong, Shen Dayang, and Benjamin K. Tsou. 1998. Chinese word segmentation without using lexicon and hand-crafted training data. In Proceedings of the 17th international conference on Computational linguistics -Volume 2, COLING ’98, pages 1265–1271, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested pitman-yor language modeling.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP ’09: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>1</volume>
<pages>100--108</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4204" citStr="Mochihashi et al., 2009" startWordPosition="630" endWordPosition="633">ational Conference on Computational Linguistics, pages 20–27, Dublin, Ireland, August 23-29 2014. 2 Related Work In this section, we will briefly introduce some proposed word segmentation methods with an emphasis on the schemes that have been applied to Myanmar. Many word segmentation methods have been proposed especially for the Thai, Khmer, Lao, Chinese and Japanese languages. These methods can be roughly classified into dictionary-based (Sornlertlamvanich, 1993; Srithirath and Seresangtakul, 2013) and statistical methods (Wu and Tseng, 1993; Maosong et al., 1998; Papageorgiou and P., 1994; Mochihashi et al., 2009; Jyun-Shen et al., 1991). In dictionary-based methods, only words that are stored in the dictionary can be identified and the performance depends to a large degree upon the coverage of the dictionary. New words appear constantly and thus, increasing size of the dictionary is a not a solution to the out of vocabulary word (OOV) problem. On the other hand, although statistical approaches can identify unknown words by utilizing probabilistic or cost-based scoring mechanisms, they also suffer from some drawbacks. The main issues are: they require large amounts of data; the processing time require</context>
<context position="10258" citStr="Mochihashi et al., 2009" startWordPosition="1603" endWordPosition="1606"> segment sk in a derivation of the corpus, given all of the other segments in the history so far s−k. Here -k is read as: “up to but not including k”. N(sk) + αG0(sk) (3) p(sk |s−k) = N + α In this equation, N is the total number of segments generated so far, N(sk) is the number of times the segment sk has occurred in the history. G0 and α are the base measure and concentration parameter as before. 3.1.3 Bayesian Inference We used a blocked version of a Gibbs sampler for training. In (Goldwater et al., 2006) they report issues with mixing in the sampler that were overcome using annealing. In (Mochihashi et al., 2009) this issue was overcome by using a blocked sampler together with a dynamic programming approach. Our algorithm is an extension of application the forward filtering backward sampling (FFBS) algorithm (Scott, 2002) to the problem of word segmentation presented in (Mochihashi et al., 2009). We extend their approach to handle the joint segmentation and alignment of character sequences. We refer the reader to (Mochihashi et al., 2009) for a complete description of the FFBS process. In essence the process uses a forward variable at each node in the segmentation graph to store the probability of rea</context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested pitman-yor language modeling. In ACL-IJCNLP ’09: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1, pages 100–108, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Papageorgiou</author>
<author>P Constantine</author>
</authors>
<title>Japanese word segmentation by hidden markov model.</title>
<date>1994</date>
<booktitle>In Proceedings of the workshop on Human Language Technology, HLT ’94,</booktitle>
<pages>283--288</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Papageorgiou, Constantine, 1994</marker>
<rawString>Papageorgiou and Constantine P. 1994. Japanese word segmentation by hidden markov model. In Proceedings of the workshop on Human Language Technology, HLT ’94, pages 283–288, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenzhe Pei</author>
<author>Dongxu Han</author>
<author>Baobao Chang</author>
</authors>
<title>A refined hdp-based model for unsupervised chinese word segmentation.</title>
<date>2013</date>
<booktitle>Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data,</booktitle>
<volume>8202</volume>
<pages>44--51</pages>
<editor>In Maosong Sun, Min Zhang, Dekang Lin, and Haifeng Wang, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="6447" citStr="Pei et al., 2013" startWordPosition="962" endWordPosition="965">on, unsupervised and semi-supervised word segmentation, were investigated. They reported that the highest quality machine translation was attained either without word segmentation using simply sequences of syllables, or by a process of Maximum Matching with a monolingual dictionary. In this study the effectiveness of approaches unsupervised word segmentation using latticelm (with 3-gram to 7-gram language models) and supervised word segmentation using KyTea was evaluated, however, none of the approaches was able to match the performance of the simpler syllable/Maximum Matching techniques. In (Pei et al., 2013) an unsupervised Bayesian word segmentation scheme was augmented by using a dictionary of words. These words were obtained from segmenting the data using another unsupervised word segmenter. The probability distribution over these words was calculated from occurrence counts, and this distribution was interpolated into the base measure. 3 Methodology 3.1 Baseline Non-parametric Bayesian Segmentation Model The baseline system, and the model that forms the basis for all of the models is a non-parametric Bayesian unsupervised word segmenter similar to that proposed in (Goldwater et al., 2009). The</context>
</contexts>
<marker>Pei, Han, Chang, 2013</marker>
<rawString>Wenzhe Pei, Dongxu Han, and Baobao Chang. 2013. A refined hdp-based model for unsupervised chinese word segmentation. In Maosong Sun, Min Zhang, Dekang Lin, and Haifeng Wang, editors, Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data, volume 8202 of Lecture Notes in Computer Science, pages 44–51. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven L Scott</author>
</authors>
<title>Bayesian methods for hidden markov models : Recursive computing in the 21st century.</title>
<date>2002</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>97</volume>
<issue>457</issue>
<contexts>
<context position="10471" citStr="Scott, 2002" startWordPosition="1636" endWordPosition="1637">mber of segments generated so far, N(sk) is the number of times the segment sk has occurred in the history. G0 and α are the base measure and concentration parameter as before. 3.1.3 Bayesian Inference We used a blocked version of a Gibbs sampler for training. In (Goldwater et al., 2006) they report issues with mixing in the sampler that were overcome using annealing. In (Mochihashi et al., 2009) this issue was overcome by using a blocked sampler together with a dynamic programming approach. Our algorithm is an extension of application the forward filtering backward sampling (FFBS) algorithm (Scott, 2002) to the problem of word segmentation presented in (Mochihashi et al., 2009). We extend their approach to handle the joint segmentation and alignment of character sequences. We refer the reader to (Mochihashi et al., 2009) for a complete description of the FFBS process. In essence the process uses a forward variable at each node in the segmentation graph to store the probability of reaching the node from the source node of the graph. These forward variables are calculated efficiently in a single forward pass through the graph, from source node to sink node (forward filtering). During backward s</context>
</contexts>
<marker>Scott, 2002</marker>
<rawString>Steven L Scott. 2002. Bayesian methods for hidden markov models : Recursive computing in the 21st century. Journal of the American Statistical Association, 97(457):337–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Virach Sornlertlamvanich</author>
</authors>
<title>Word segmentation for thai in machine translation system.</title>
<date>1993</date>
<booktitle>Machine Translation, National Electronics and Computer Technology</booktitle>
<pages>50--56</pages>
<location>Center, Bangkok,</location>
<contexts>
<context position="4049" citStr="Sornlertlamvanich, 1993" startWordPosition="608" endWordPosition="610">e organizers. License details: http://creativecommons.org/licenses/by/4.0/ 20 Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 20–27, Dublin, Ireland, August 23-29 2014. 2 Related Work In this section, we will briefly introduce some proposed word segmentation methods with an emphasis on the schemes that have been applied to Myanmar. Many word segmentation methods have been proposed especially for the Thai, Khmer, Lao, Chinese and Japanese languages. These methods can be roughly classified into dictionary-based (Sornlertlamvanich, 1993; Srithirath and Seresangtakul, 2013) and statistical methods (Wu and Tseng, 1993; Maosong et al., 1998; Papageorgiou and P., 1994; Mochihashi et al., 2009; Jyun-Shen et al., 1991). In dictionary-based methods, only words that are stored in the dictionary can be identified and the performance depends to a large degree upon the coverage of the dictionary. New words appear constantly and thus, increasing size of the dictionary is a not a solution to the out of vocabulary word (OOV) problem. On the other hand, although statistical approaches can identify unknown words by utilizing probabilistic o</context>
</contexts>
<marker>Sornlertlamvanich, 1993</marker>
<rawString>Virach Sornlertlamvanich. 1993. Word segmentation for thai in machine translation system. Machine Translation, National Electronics and Computer Technology Center, Bangkok, pages 50–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Srithirath</author>
<author>P Seresangtakul</author>
</authors>
<title>A hybrid approach to lao word segmentation using longest syllable level matching with named entities recognition.</title>
<date>2013</date>
<booktitle>In Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON), 2013 10th International Conference on,</booktitle>
<pages>1--5</pages>
<contexts>
<context position="4086" citStr="Srithirath and Seresangtakul, 2013" startWordPosition="611" endWordPosition="614">ails: http://creativecommons.org/licenses/by/4.0/ 20 Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 20–27, Dublin, Ireland, August 23-29 2014. 2 Related Work In this section, we will briefly introduce some proposed word segmentation methods with an emphasis on the schemes that have been applied to Myanmar. Many word segmentation methods have been proposed especially for the Thai, Khmer, Lao, Chinese and Japanese languages. These methods can be roughly classified into dictionary-based (Sornlertlamvanich, 1993; Srithirath and Seresangtakul, 2013) and statistical methods (Wu and Tseng, 1993; Maosong et al., 1998; Papageorgiou and P., 1994; Mochihashi et al., 2009; Jyun-Shen et al., 1991). In dictionary-based methods, only words that are stored in the dictionary can be identified and the performance depends to a large degree upon the coverage of the dictionary. New words appear constantly and thus, increasing size of the dictionary is a not a solution to the out of vocabulary word (OOV) problem. On the other hand, although statistical approaches can identify unknown words by utilizing probabilistic or cost-based scoring mechanisms, they</context>
</contexts>
<marker>Srithirath, Seresangtakul, 2013</marker>
<rawString>A Srithirath and P. Seresangtakul. 2013. A hybrid approach to lao word segmentation using longest syllable level matching with named entities recognition. In Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON), 2013 10th International Conference on, pages 1–5, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Teahan</author>
<author>Rodger McNab</author>
<author>Yingying Wen</author>
<author>Ian H Witten</author>
</authors>
<title>A compression-based algorithm for chinese word segmentation.</title>
<date>2000</date>
<journal>Comput. Linguist.,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="4927" citStr="Teahan et al., 2000" startWordPosition="741" endWordPosition="744">n be identified and the performance depends to a large degree upon the coverage of the dictionary. New words appear constantly and thus, increasing size of the dictionary is a not a solution to the out of vocabulary word (OOV) problem. On the other hand, although statistical approaches can identify unknown words by utilizing probabilistic or cost-based scoring mechanisms, they also suffer from some drawbacks. The main issues are: they require large amounts of data; the processing time required; and the difficulty in incorporating linguistic knowledge effectively into the segmentation process (Teahan et al., 2000). For low-resource languages such as Myanmar, there is no freely available corpus and dictionary based or rule based methods are being used as a temporary solution. If we only focus on Myanmar language word segmentation, as far as the authors are aware there have been only two published methodologies, and one study. Both of the proposed methodologies operate according using a process of syllable breaking followed by Maximum Matching; the differences in the approaches come from the manner in which the segmentation boundary decision is made. In (Thet et al., 2008) statistical information is used</context>
</contexts>
<marker>Teahan, McNab, Wen, Witten, 2000</marker>
<rawString>W. J. Teahan, Rodger McNab, Yingying Wen, and Ian H. Witten. 2000. A compression-based algorithm for chinese word segmentation. Comput. Linguist., 26(3):375–393, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thanaruk Theeramunkong</author>
<author>Sasiporn Usanavasin</author>
</authors>
<title>Non-dictionary-based thai word segmentation using decision trees. In</title>
<date>2001</date>
<booktitle>In Proceedings of the First International Conference on Human Language Technology Research.</booktitle>
<contexts>
<context position="15276" citStr="Theeramunkong and Usanavasin, 2001" startWordPosition="2421" endWordPosition="2424">d from the data, as to whether the segment would be generated from the unsupervised sub-model or the dictionary sub-model. In this model, the decision to generate from the dictionary model is refined into a number of decisions to generate from a number of subsets of the dictionary, each with its own probability. These probabilities were re-estimated from the sampled segmentation of the corpus at the end of each iteration of the training (in a similar manner to the dictionary augmented model). A diagram showing the generative process is shown in Figure 1. 3.4 Language Model Augmented Model In (Theeramunkong and Usanavasin, 2001) dictionary approaches were deliberately avoided in order to address issues with unknown words. Instead a decision tree model for segmentation was proposed. Our approach although different in character (since a generative model is used), shares the insight that knowledge of how words are constructed is key to segmentation when dictionary information is absent. In this model we used the dictionary resource, but in a more indirect manner. We use a language model to capture the notion of exactly what constitutes a segment. To do this words in the dictionary were first segmented into syllables. Th</context>
</contexts>
<marker>Theeramunkong, Usanavasin, 2001</marker>
<rawString>Thanaruk Theeramunkong and Sasiporn Usanavasin. 2001. Non-dictionary-based thai word segmentation using decision trees. In In Proceedings of the First International Conference on Human Language Technology Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tun Thura Thet</author>
<author>Jin-Cheon Na</author>
<author>Wunna Ko Ko</author>
</authors>
<title>Word segmentation for the myanmar language.</title>
<date>2008</date>
<journal>J. Information Science,</journal>
<volume>34</volume>
<issue>5</issue>
<contexts>
<context position="5495" citStr="Thet et al., 2008" startWordPosition="832" endWordPosition="835">nto the segmentation process (Teahan et al., 2000). For low-resource languages such as Myanmar, there is no freely available corpus and dictionary based or rule based methods are being used as a temporary solution. If we only focus on Myanmar language word segmentation, as far as the authors are aware there have been only two published methodologies, and one study. Both of the proposed methodologies operate according using a process of syllable breaking followed by Maximum Matching; the differences in the approaches come from the manner in which the segmentation boundary decision is made. In (Thet et al., 2008) statistical information is used (based on bigram information), whereas (Htay and Murthy, 2008) utilize a word list extracted from a monolingual Myanmar corpus. In a related study (Thu et al., 2013a), various Myanmar word segmentation approaches including character segmentation, syllable segmentation, human lexical/phrasal segmentation, unsupervised and semi-supervised word segmentation, were investigated. They reported that the highest quality machine translation was attained either without word segmentation using simply sequences of syllables, or by a process of Maximum Matching with a monol</context>
<context position="12236" citStr="Thet et al., 2008" startWordPosition="1924" endWordPosition="1927">rs, the Dirichlet concentration parameter α, and the Poisson rate parameter A were set by slice sampling using vague priors (a Gamma prior in the case of α and the Jeffreys prior was used for A). The token set size V used in the base measure was set to the number of types in the training corpus, and V = 3363. 3.2 Dictionary Augmented Model The dictionary augmented model is in essence the same model as proposed by (Thu et al., 2013b), but a different dictionary was used. Their method integrates dictionary-based word segmentation (similar to the maximum matching approaches used successfully in (Thet et al., 2008; Htay and Murthy, 2008; Thu et al., 2013a) ) into a fully unsupervised Bayesian word segmentation scheme. Dictionary-based word segmentation has the advantage of being able to exploit human knowledge about the sequences of characters in the language that are used to form words. This approach is simple and has proven to be a very effective technique in previous studies. Problems arise due to the coverage of the dictionary. The dictionary may not be able to cover the running text well, for example in the case of low-resource languages the dictionary might be small, or in the case of named entit</context>
</contexts>
<marker>Thet, Na, Ko, 2008</marker>
<rawString>Tun Thura Thet, Jin-Cheon Na, and Wunna Ko Ko. 2008. Word segmentation for the myanmar language. J. Information Science, 34(5):688–704.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye Kyaw Thu</author>
<author>Andrew Finch</author>
<author>Yoshinori Sagisaka</author>
<author>Eiichiro Sumita</author>
</authors>
<title>A study of myanmar word segmentation schemes for statistical machine translation.</title>
<date>2013</date>
<booktitle>Proceeding of the 11th International Conference on Computer Applications,</booktitle>
<pages>167--179</pages>
<contexts>
<context position="2583" citStr="Thu et al., 2013" startWordPosition="393" endWordPosition="396">ited by white space in running text. However, in some low-resource languages (Myanmar being one) broad-coverage word segmentation tools are scarce, and there are two common approaches to dealing with this issue. The first is to apply unsupervised word segmentation tools to a body of monolingual text in order to induce a segmentation. The second is to use a dictionary of words in the language together with a set of heuristics to identify word boundaries in text. Myanmar language can be accurately segmented into a sequence of syllables using finite state automata (examples being (Berment, 2004; Thu et al., 2013a)). However, words composed of single or multiple syllables are not usually separated by white space. Although spaces are sometimes used for separating phrases for easier reading, it is not strictly necessary, and these spaces are rarely used in short sentences. There are no clear rules for using spaces in Myanmar language, and thus spaces may (or may not) be inserted between words, phrases, and even between a root word and its affixes. Myanmar language is a resource-poor language and large corpora, lexical resources, and grammatical dictionaries are not yet widely available. For this reason,</context>
<context position="5692" citStr="Thu et al., 2013" startWordPosition="863" endWordPosition="866">orary solution. If we only focus on Myanmar language word segmentation, as far as the authors are aware there have been only two published methodologies, and one study. Both of the proposed methodologies operate according using a process of syllable breaking followed by Maximum Matching; the differences in the approaches come from the manner in which the segmentation boundary decision is made. In (Thet et al., 2008) statistical information is used (based on bigram information), whereas (Htay and Murthy, 2008) utilize a word list extracted from a monolingual Myanmar corpus. In a related study (Thu et al., 2013a), various Myanmar word segmentation approaches including character segmentation, syllable segmentation, human lexical/phrasal segmentation, unsupervised and semi-supervised word segmentation, were investigated. They reported that the highest quality machine translation was attained either without word segmentation using simply sequences of syllables, or by a process of Maximum Matching with a monolingual dictionary. In this study the effectiveness of approaches unsupervised word segmentation using latticelm (with 3-gram to 7-gram language models) and supervised word segmentation using KyTea </context>
<context position="12053" citStr="Thu et al., 2013" startWordPosition="1899" endWordPosition="1902">ect this. Then a new segmentation for the sequence was chosen using the FFBS process, and the models were updated with 22 the counts from this new segmentation. The two hyperparameters, the Dirichlet concentration parameter α, and the Poisson rate parameter A were set by slice sampling using vague priors (a Gamma prior in the case of α and the Jeffreys prior was used for A). The token set size V used in the base measure was set to the number of types in the training corpus, and V = 3363. 3.2 Dictionary Augmented Model The dictionary augmented model is in essence the same model as proposed by (Thu et al., 2013b), but a different dictionary was used. Their method integrates dictionary-based word segmentation (similar to the maximum matching approaches used successfully in (Thet et al., 2008; Htay and Murthy, 2008; Thu et al., 2013a) ) into a fully unsupervised Bayesian word segmentation scheme. Dictionary-based word segmentation has the advantage of being able to exploit human knowledge about the sequences of characters in the language that are used to form words. This approach is simple and has proven to be a very effective technique in previous studies. Problems arise due to the coverage of the di</context>
</contexts>
<marker>Thu, Finch, Sagisaka, Sumita, 2013</marker>
<rawString>Ye Kyaw Thu, Andrew Finch, Yoshinori Sagisaka, and Eiichiro Sumita. 2013a. A study of myanmar word segmentation schemes for statistical machine translation. Proceeding of the 11th International Conference on Computer Applications, pages 167–179.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ye Kyaw Thu</author>
<author>Andrew Finch</author>
</authors>
<title>Eiichiro Sumita, and Yoshinori Sagisaka. 2013b. Unsupervised and semi-supervised myanmar word segmentation approaches for statistical machine translation.</title>
<marker>Thu, Finch, </marker>
<rawString>Ye Kyaw Thu, Andrew Finch, Eiichiro Sumita, and Yoshinori Sagisaka. 2013b. Unsupervised and semi-supervised myanmar word segmentation approaches for statistical machine translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zinmin Wu</author>
<author>Gwyneth Tseng</author>
</authors>
<title>Chinese text segmentation for text retrieval: Achievements and problems.</title>
<date>1993</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>44</volume>
<issue>5</issue>
<contexts>
<context position="4130" citStr="Wu and Tseng, 1993" startWordPosition="618" endWordPosition="621">ngs of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 20–27, Dublin, Ireland, August 23-29 2014. 2 Related Work In this section, we will briefly introduce some proposed word segmentation methods with an emphasis on the schemes that have been applied to Myanmar. Many word segmentation methods have been proposed especially for the Thai, Khmer, Lao, Chinese and Japanese languages. These methods can be roughly classified into dictionary-based (Sornlertlamvanich, 1993; Srithirath and Seresangtakul, 2013) and statistical methods (Wu and Tseng, 1993; Maosong et al., 1998; Papageorgiou and P., 1994; Mochihashi et al., 2009; Jyun-Shen et al., 1991). In dictionary-based methods, only words that are stored in the dictionary can be identified and the performance depends to a large degree upon the coverage of the dictionary. New words appear constantly and thus, increasing size of the dictionary is a not a solution to the out of vocabulary word (OOV) problem. On the other hand, although statistical approaches can identify unknown words by utilizing probabilistic or cost-based scoring mechanisms, they also suffer from some drawbacks. The main i</context>
</contexts>
<marker>Wu, Tseng, 1993</marker>
<rawString>Zinmin Wu and Gwyneth Tseng. 1993. Chinese text segmentation for text retrieval: Achievements and problems. Journal of the American Society for Information Science, 44(5):532–542.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
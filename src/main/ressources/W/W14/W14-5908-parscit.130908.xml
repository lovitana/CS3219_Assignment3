<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.035532">
<title confidence="0.965878">
Verbal Behaviors and Persuasiveness in Online Multimedia Content
</title>
<author confidence="0.888779">
Moitreya Chatterjee, Sunghyun Park*, Han Suk Shim*,
Kenji Sagae and Louis-Philippe Morency
</author>
<affiliation confidence="0.896697">
USC Institute for Creative Technologies
</affiliation>
<address confidence="0.954393">
Los Angeles, CA 90094
</address>
<email confidence="0.9623325">
metro.smiles@gmail.com,
{ park, hshim, sagae, morency }@ict.usc.edu
</email>
<sectionHeader confidence="0.992416" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999916727272727">
Persuasive communication is an essential component of our daily lives, whether it is negotiat-
ing, reviewing a product, or campaigning for the acceptance of a point of view. With the rapid
expansion of social media websites such as YouTube, Vimeo and ExpoTV, it is becoming ev-
er more important and useful to understand persuasiveness in social multimedia content. In
this paper we present a novel analysis of verbal behavior, based on lexical usage and para-
verbal markers of hesitation, in the context of predicting persuasiveness in online multimedia
content. Toward the end goal of predicting perceived persuasion, this work also explores the
potential differences in verbal behavior of people expressing a positive opinion (e.g., a posi-
tive movie review) versus a negative one. The analysis is performed on a multimedia corpus
of 1,000 movie review videos annotated for persuasiveness. Our results show that verbal be-
havior can be a significant predictor of persuasiveness in such online multimedia content.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999869869565218">
A message that is “intended to shape, reinforce or change the responses of another or others” is cate-
gorized as persuasive communication (Miller, 1980), and it is particularly important for the role it
plays in creating social influence and altering other people’s opinions (Reardon, 1991; Zimbardo and
Leippe, 1991). For instance, a persuasive advertisement could be a potential profit churner.
The growth of social networking sites on the Internet has resulted in an explosion of online content
with the purpose of delivering persuasive messages. Websites such as YouTube, Vimeo and ExpoTV
are examples of online media in which these messages propagate mainly in the form of videos.
ExpoTV, in particular, is a repository of a large number of videos dedicated for product reviews in
which people try to convince others in favor of or against the use of various products. This raises an
interesting research problem as to what it is that makes certain speakers have a substantial impact on
others’ opinions while other speakers are ignored.
In this paper, we present a novel analysis of spoken persuasion in online multimedia content. Our
work is motivated by prior research findings in psychology indicating that verbal behavior is a prom-
ising indicator for persuasive communication (Chaiken and Eagly, 1979; Werner, 1982). Such prior
findings allow us to hypothesize that two primary types of verbal features will be predictive of per-
suasion: lexical features and paraverbal markers of hesitation. Additionally we explore the relation-
ship of the sentiment of the content and perceived persuasion, by hypothesizing that speakers’ exhibit
different verbal behavior when expressing a positive opinion versus a negative one and taking into
account these differences will improve prediction performance. We conduct several experiments in
order to validate these hypotheses using a multimedia corpus of 1,000 movie review videos obtained
from ExpoTV.com, which is a great source of online reviews. Our experiments followed by a detailed
analysis also reveal a set of predictive features which characterize persuasive online presentations.
In the following section, we present an overview of related work. Section 3 elaborates on our re-
</bodyText>
<note confidence="0.850531">
* Both authors contributed equally to this work.
</note>
<footnote confidence="0.8725215">
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
</footnote>
<page confidence="0.912346">
50
</page>
<note confidence="0.8866785">
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 50–58,
Dublin, Ireland, August 24 2014.
</note>
<bodyText confidence="0.9986615">
search hypotheses. In Section 4, we present our multimedia corpus. The details of the experiments
with computational descriptors and methodology are described in Section 5. We discuss the results
and findings in Section 6, and finally we conclude our paper and present some future directions of
research in Section 7.
</bodyText>
<sectionHeader confidence="0.999345" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99997175">
Content in the form of written text are omnipresent in our society. Starting from books, magazines
and newspapers to the now prevalent emails and blog posts, text-based content are an invaluable
component for effective communication. Prior research reports possibly greater persuasiveness in
written messages compared to visual or acoustic modalities in certain situations (Chaiken and Eagly,
1979; Werner, 1982). Past research has also revealed that for sophisticated messages, such as those
used in a martial setting, written messages are more persuasive (Chaiken and Eagly, 1979).
Although the importance of studying verbal behavior for determining persuasiveness has been un-
derscored in prior work in the field of communication sciences (O’Keefe, 2002) and this line of re-
search gives us useful pointers to the factors that contribute to persuasiveness in text or verbal com-
munication, they present no computational aspect, which is where we put our emphasis in the paper.
In the field of natural language processing, text classification based on bag-of-words has been a
long standing approach (Lewis and Gale, 1994; Mitchell, 1997; Dave et al., 2003). In fact, Young et
al. (2011) have explored lexical features in the specific context of predicting persuasion, but they fo-
cus their attention on studying persuasion in dialogue. Our work draws inspiration from such ap-
proaches but explores it in the specific context of predicting persuasiveness in online multimedia con-
tent using lexical and paraverbal features.
</bodyText>
<sectionHeader confidence="0.993196" genericHeader="method">
3 Research Hypotheses
</sectionHeader>
<bodyText confidence="0.992686454545455">
Motivated by prior works and theoretical background, we designed our experiments to validate three
hypotheses.
Since multiple prior works point to the usefulness of the text modality in persuasive communication
and also to the power of text classification with lexical features in various tasks, we explored the fea-
sibility of capturing the difference in verbal behavior between persuasive and unpersuasive expres-
sions of opinions in online social multimedia content (specifically, movie reviews). The following is
the hypothesis that we specifically tested with our experiments:
Hypothesis 1: Verbal behavior, as captured by lexical usage, is indicative of persuasiveness in online
social multimedia content, irrespective of whether the opinion expressed is positive or negative.
Paraverbal behaviors indicative of hesitation can constitute important information for predicting
persuasiveness. For instance, a speaker’s stuttering or breaking his/her speech with filled pauses (such
as um and uh) has influence on how other people perceive his/her persuasiveness. Although previous
work (DeVault et al, 2013) suggests paraverbal behavior may be indicative of depression, another
work on emotion prediction however, (Devillers et al., 2006) raised questions about its predictive
power when compared to using standard cues derived from lexical usage. This leads us to our second
hypothesis on paraverbal behaviors in the context of predicting persuasiveness:
Hypothesis 2: Paraverbal behaviors related to hesitation are indicative of persuasiveness in online
social multimedia content.
Past research highlights the importance of the knowledge of the affective state of a document
towards its perceived persuasiveness (Murphy, 2001). We therefore hypothesize the following:
Hypothesis 3: Knowledge of the sentiment polarity of a movie review improves classification of the
speaker’s perceived persuasiveness.
</bodyText>
<sectionHeader confidence="0.998343" genericHeader="method">
4 Dataset
</sectionHeader>
<bodyText confidence="0.9997882">
ExpoTV.com is a popular website housing videos of product reviews. Each product review has a vid-
eo of a speaker talking about a particular product as well as the speaker’s direct rating of the product
on an integral scale from 1 star (for most negative review) to 5 stars (for most positive review). This
direct rating is useful for the purpose of our study because this allows us to study perceived persua-
sion under different directions of persuasion (in favor of or against). For instance, the speaker in a 5-
</bodyText>
<page confidence="0.991374">
51
</page>
<bodyText confidence="0.999986">
star movie review video would most likely try to persuade his/her audience in favor of watching the
movie while the speaker in a 1-star movie review video would argue against watching the movie. We
therefore collected a total of 1,000 movie review videos that were either highly positive or negative.
The dataset consists of the following:
</bodyText>
<listItem confidence="0.835784">
• Positive Reviews: 500 movie review videos with 5-star rating (315 males and 185 females).
• Negative Reviews: 500 movie review videos with 1 or 2-star rating, consisting of 216 1-star
videos (151 males and 65 females) and 284 2-star videos (212 males and 72 females). We included
2-star videos due to the lack of enough 1-star videos on the website.
</listItem>
<bodyText confidence="0.904738333333333">
Each video in the corpus has a frontal view of one person talking about a particular movie, and the
average length of the videos is about 94 seconds. The corpus contains 372 unique speakers and 600
unique movie titles and is available to the community for purposes of academic research1.
</bodyText>
<subsectionHeader confidence="0.999723">
4.1 Evaluation of Persuasiveness
</subsectionHeader>
<bodyText confidence="0.999919647058824">
Amazon Mechanical Turk (AMT), which is a popular online crowdsourcing platform, was used to
obtain subjective evaluation of each speaker’s perceived persuasiveness, following a similar annota-
tion scheme as (Mohammadi et al., 2013). For each video in the corpus, we obtained 3 repeated eval-
uations on the level of persuasiveness of the speaker by asking the workers to give direct rating on
each speaker’s persuasiveness on a Likert scale from 1 (very unpersuasive) to 7 (very persuasive). A
total of 50 native English-speaking workers based in the United States participated in the evaluation
process online, and the task was evenly distributed among the 50 workers. To minimize gender influ-
ence, the task was distributed such that the workers only evaluated speakers of the same gender. The
correlation between the mean score of every movie and the individual ratings was found to be 0.7 on
the average (Pearson’s Correlation Coefficient).
Once the evaluation was complete, we used the mean persuasiveness score for each video as the
ground-truth measure of the speaker’s perceived persuasiveness. In this initial effort, we focused on
videos that were extremely persuasive or not persuasive at all. Hence, videos with a mean score of
equal to or greater than 5.5 were taken as persuasive while those with a mean score of equal to or less
than 2.5 were taken as unpersuasive. After this, we ended up with a total of 300 videos, specifically
157 videos of positive reviews (75 persuasive and 82 unpersuasive) and 143 videos of negative re-
views (62 persuasive and 81 unpersuasive).
</bodyText>
<subsectionHeader confidence="0.97336">
4.2 Transcriptions
</subsectionHeader>
<bodyText confidence="0.9999452">
Using AMT and 18 participants from the same worker pool for persuasiveness evaluation, we ob-
tained verbatim transcriptions of these filtered 300 videos, including transcriptions for filled pauses
and stutters. Each transcription was reviewed and edited by multiple in-house experienced transcribers
for accuracy. We do not use automatic speech recognition techniques in order to avoid noisy tran-
scriptions.
</bodyText>
<sectionHeader confidence="0.998951" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9997695">
In this section, we give details on the design of our computational descriptors followed by the experi-
mental methodology.
</bodyText>
<subsectionHeader confidence="0.994311">
5.1 Computational Descriptors
</subsectionHeader>
<bodyText confidence="0.992129">
In our experiments, our main focus was on devising computational descriptors for verbal behaviors in
terms of lexical usage and also in terms of paraverbal markers of hesitation that can capture indica-
tions of persuasiveness of the speaker.
Verbal (Lexical) Descriptors: As in many text classification tasks, we designed our verbal de-
scriptors based on the bag-of-words representation using term frequency of both unigrams and bi-
</bodyText>
<footnote confidence="0.938909">
1 Dataset available online: http://multicomp.ict.usc.edu/
</footnote>
<page confidence="0.997688">
52
</page>
<bodyText confidence="0.994426545454545">
grams. Using the 300 filtered videos (see Section 4.1) and without feature selection, the numbers of
unigrams reach around 4,500 and bigrams around 24,000. We did not proceed further with higher or-
der n-grams because empirical evidence has shown that trigrams and other higher order n-grams do
not always show improvement because they introduce problems related to the sparsity of features
(Dave et al., 2003).
Paraverbal Descriptors of Hesitation: From the verbatim transcriptions of our corpus, we observed
a set of frequent paraverbal cues that could potentially be associated with the level of persuasiveness.
The set of descriptors is inspired from the findings of DeVault et al. (2013), who explored a similar
set of generic paraverbal features in an interactive dialogue setting. However, we are interested
specifically in the ones that capture signs of hesitation. The following were the descriptors that were
used:
</bodyText>
<listItem confidence="0.974890166666667">
• Pause-Fillers: The verbal behaviors of reviewers are often characterized with various pause-
fillers, such as um or uh. In order to account for the varying length of each review, we normalized
the count of all instances of filled pauses by the number of words spoken in the video.
• Disfluency Markers: A prominent marker of disfluency in human speech is stuttering. To capture
this disfluency, we counted all instances of stuttering in each video and normalized them by the
number of words spoken in the video.
• Articulation Rate: Articulation rate is defined as the rate of speaking in which all pauses are
excluded from calculation (Dankovicova, 1999). This descriptor was computed by taking the ratio
of the number of spoken words in each video to the actual time spent speaking.
• Mean Span of Silence: Human speech is often interspersed with pauses. We therefore computed
this descriptor, by measuring the total duration of silence during speech, normalized by the total
length of the video.
</listItem>
<subsectionHeader confidence="0.983702">
5.2 Methodology
</subsectionHeader>
<bodyText confidence="0.999774037037037">
We processed all the videos in our dataset and automatically extracted the indicated lexical and
paraverbal features. The extracted features were then used for several classification experiments under
three different settings to test our hypotheses: only positive reviews, only negative reviews (called the
sentiment-dependent classifiers) and a combined set of positive and negative reviews (called the
sentiment-independent classifiers). For each such setting, we divided the set of samples (transcription
of movie reviews) into 5 balanced folds that were both speaker-independent and movie-independent.
In other words, in all our experiments, no 2 folds contained samples from the same speaker or movie
title. This was done to remove any form of bias in the classifier based on either the speaker or the
movie.
We then performed classification experiments using 5-fold cross-validation using the lexical fea-
tures (unigrams and bigrams) on this combined set of reviews (positive and negative reviews togeth-
er), each time leaving 1 fold for hold-out testing. Here, we note that for constructing the dictionary,
only data from the training set was used. On average across 5-fold cross-validation, the number of
unigrams was around 4,560 and bigrams around 23,701 for the combined set of movie reviews.
However, since such a feature design typically suffers from problems arising out of the sparsity of
the entries of the dictionary in the dataset, we employed a feature selection step. For feature selection
and analysis, we used Information Gain (IG), which is a measure of the number of bits of information
obtained for category prediction by knowing the presence of a term in a document. Prior evaluation of
feature-selection methods for text classification has revealed the superiority of IG as a metric over
other ones such as Mutual Information, Term Strength or a simple Document Frequency thresholding
for document classification tasks (Yang and Pedersen, 1997). This serves as an inspiring basis for us-
ing IG as a metric for feature selection.
The gain score G(t) obtained from IG is a non-zero positive value for features that are strongly in-
dicative of the extent of persuasiveness of the document, while ones that are not so informative have a
value of 0. We therefore select only those lexical features (unigrams and bigrams) which have an IG &gt;
0 based on the distribution obtained from the training set. This allows us to trim the dictionary signifi-
cantly and use only meaningful features for classification.
</bodyText>
<page confidence="0.997241">
53
</page>
<table confidence="0.999922">
Feature Group Sentiment Dependent Classifier Sentiment Independent
Classifier
Mean Positive Reviews Negative Reviews
Lexical Features
(Unigrams and 83.92% 81.74% 86.09% 76.73%
Bigrams)
• Unigrams 77.70% 74.78% 80.62% 73.77%
Only
• Bigrams 84.05% 81.64% 86.46% 75.81%
Only
Para-Linguistic 64.23% 65.22% 63.23% 63.04%
Features
Early Fusion 84.54% 82.61% 86.46% 78.56%
Majority Baseline 52.14% 50.43% 53.85% 51.09%
</table>
<tableCaption confidence="0.9377915">
Table 1: Accuracies for our experiments using a Naïve Bayes classifier. The scores in bold indi-
cate the dominance of the sentiment-dependent classifier under all circumstances.
</tableCaption>
<bodyText confidence="0.999979625">
This was then followed up by a 5-fold cross-validation using only the paraverbal features (no fea-
ture selection was used here since they were too few in number). The accuracy of classification based
on paraverbal features was then compared with that obtained by classification using only the lexical
descriptors and by a majority baseline classifier.
Furthermore, we also tried an early-fusion approach, where we simply use both lexical and para-
verbal features together. Such an approach to fusion seemed more promising here than a decision-
level fusion approach because of the few categories of features used (just lexical and paralinguistic, as
motivated by the findings of (Gunes and Piccardi, 2005)).
</bodyText>
<subsectionHeader confidence="0.973433">
5.3 Classification Model
</subsectionHeader>
<bodyText confidence="0.999953666666667">
For performing classification experiments we used the Naïve Bayes classifier. A well-known issue
with using the Naïve Bayes classifier is its incapability of handling new features, which is handled by
performing a conditional uniform smoothing (Puurula, 2012).
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.990579736842105">
Table 1 shows the results for our classification experiments, which confirm the predictive power of
lexical features.
Hypothesis 1: The lexical features (unigrams and bigrams) are predictive of persuasiveness. This is
manifested by the fact that they perform significantly better than a majority baseline, which is only
51.04% accurate on the combined set of positive and negative reviews, while the lexical features
achieved an accuracy of around 77% (Figure 1). Considering the positive and the negative reviews
individually, we note that the lexical features were accurate for nearly 82% of the test samples for the
positive reviews and for 86% of the test samples for the negative reviews, again outperforming a sim-
ple majority baseline classifier (Table 1).
An analysis of the features (Table 2) reveals that certain lexical features contribute to the predicta-
bility of the persuasiveness of a speaker. The presence of unigrams such as character or make or bi-
grams such as to make or this movie for instance, contributes to the predictability of persuasiveness of
the speaker, even though they are not emotionally salient terms. The high IG scores of such features
irrespective of the setting we conduct our experiments in (positive reviews only, negative reviews on-
ly or a combined set of positive and negative reviews), highlights their importance. Moreover, a (+)
sign for most of these unigrams or bigrams show that their presence contributes favorably to the
speaker being perceived as persuasive. On the other hand a (-) sign for an informative bigram such as
it says is indicative of lack of speaker’s persuasiveness. This can be explained by the context of the
usage of such features. For instance, the bigram it says in it says that the movie duration isÉ is a bi-
</bodyText>
<page confidence="0.998155">
54
</page>
<figureCaption confidence="0.988674333333333">
Figure 1: Bar graph visualization of the classification accuracies using different types of fea-
tures on the combined set of reviews (i.e. sentiment-independent classifier). ** indicates 2-
samples t-test results with p &lt; 0.01 and *** indicates p &lt; 0.001. The error bars show 1 SD.
</figureCaption>
<table confidence="0.999858545454546">
Feature Positive Reviews Negative Reviews Both Combined
Word IG Score Word IG Score Word IG Score
Unigrams The (+) 0.1183 Even (+) 0.11 Make (+) 0.1117
Make (+) 0.0816 Make (-) 0.1082 Just (+) 0.0728
Everything (+) 0.0806 Movie (+) 0.0969 Very (+) 0.0669
Just (+) 0.0806 Real (+) 0.0873 Character (+) 0.0573
Dollars (+) 0.0722 Not (+) 0.0867 Becomes (+) 0.0558
Character (+) 0.0685 Big (+) 0.0858 Even (+) 0.0524
Can (+) 0.0685 One (+) 0.0817 One (+) 0.051
Product (+) 0.0685 Avoid (+) 0.079 Yourself (+) 0.05
Famous (+) 0.0609 Feel (+) 0.079 You (+) 0.04571
Enjoy (+) 0.0566 Character (+) 0.0773 Lot (+) 0.0456
Bigrams There are (+) 0.1183 This movie (+) 0.1083 To make (+) 0.0905
This movie (+) 0.0816 Do not (+) 0.1032 A lot (+) 0.0617
I can’t (+) 0.0806 I think (+) 0.1032 This movie (+) 0.0578
To make (+) 0.0806 To make (+) 0.0989 Lot of (+) 0.0443
Good movie (+) 0.0722 Not even (+) 0.091 It says (-) 0.0417
Buy it (+) 0.0685 Don’t even (+) 0.091 You will (-) 0.0417
Really a (+) 0.0685 The story (+) 0.079 Twenty dollars (+) 0.0368
Definitely one (+) 0.0685 The film (+) 0.0672 The character (+) 0.0386
Best movies (+) 0.0609 At all (+) 0.0672 So many (+) 0.033
It’s awesome (+) 0.0566 It’s so (+) 0.0672 See it (+) 0.033
</table>
<tableCaption confidence="0.69704">
Table 2: Important unigrams and bigrams when they are used individually as lexical features.
(+) indicates that it increases persuasiveness while (-) indicates it contributes to the lack of per-
suasiveness.
</tableCaption>
<bodyText confidence="0.964279142857143">
gram that is uttered by the reviewers when they refer to the DVD cover of the movie to give some
more detailed information about it. This is identified as a sign of an unpersuasive reviewer. Such re-
sults confirm that the verbal behaviors, as captured by lexical usage, are extremely predictive of per-
suasiveness irrespective of whether the opinion expressed is positive or negative, which validates Hy-
pothesis 1.
Hypothesis 2: Moreover, our experiments show that while the designed paraverbal features that are
markers of hesitation can classify only about 63% of the speakers correctly (see Table 1), however
</bodyText>
<page confidence="0.997689">
55
</page>
<figureCaption confidence="0.996749">
Figure 2: Boxplots for the paralinguistic hesitation markers for a classifier trained on the para-
linguistic features only. * and *** indicate p &lt;= 0.05 and 0.001, respectively.
</figureCaption>
<bodyText confidence="0.99942828">
they are statistically significant features, in terms of their p-values (Figure 2). While classification
performance is lower than that obtained with purely lexical features, it is still far above a majority
baseline, and thus confirms our second hypothesis.
Additionally, it is interesting to note from Table 1 that, although a feature-level fusion of the lexical
features and paraverbal features gives us an improvement in classification performance, the difference
between the results obtained with fusion and those with lexical features alone was minor and was not
statistically significant.
Hypothesis 3: We also observe that a sentiment-dependent classifier trained individually on positive
reviews or on negative reviews outperforms one that is trained on a combined set of reviews. This is
supported by our empirical results in Table 1 which show that when classification is performed with
any of the lexical features, the accuracies are significantly higher for the classifier trained only on the
positive or only on the negative reviews (sentiment-dependent classifiers) than for the classifier
trained on the combined set of reviews (sentiment-independent classifiers). For instance, when
unigrams and bigrams were both used as our lexical features, we observed that for a sentiment-
dependent classifier the classification accuracy jumps to over 84% on average. This is significantly
better than the scenario where the classifier is not aware of the sentiment of the review. Figure 3
demonstrates this phenomenon.
We resort to feature analysis for an explanation of such an observation (Table 2). The analysis re-
veals that certain sentiment-based lexical features, i.e. emotionally salient terms, assume an important
role in magnifying the discriminative power of language use in persuasiveness prediction, when prior
knowledge about the speaker’s opinion is known. For instance, in the case of a classifier trained only
on the positive reviews, unigrams such as enjoy and famous and bigrams such as good movie or it’s
awesome become significant. In the context of persuading against watching the movie prominent sen-
timent-based unigrams are not and avoid while bigrams are do not, don’t even and at all. This pro-
vides empirical support for our third hypothesis.
</bodyText>
<sectionHeader confidence="0.992827" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999953857142857">
This work presents several interesting findings about perceived persuasiveness prediction in online
social multimedia content by analyzing the verbal behavior of the speaker, modeled using lexical fea-
tures and paraverbal features of hesitation. We conducted experiments and showed that verbal behav-
ior as captured by lexical descriptors is a strong indicator of persuasiveness, irrespective of whether
we persuade in favor of or against something. Much of this is due to the presence of certain unigrams
and bigrams that are either indicative of strong persuasiveness or of lack of persuasiveness. Our ex-
periments further reveal the superiority of classifying with lexical features as compared to with para-
</bodyText>
<page confidence="0.995761">
56
</page>
<figureCaption confidence="0.895499333333333">
Figure 3: Bar graph visualization of the classification accuracies of lexical features using a senti-
ment-dependent classifier (mean) and a sentiment independent one. ** indicates 2-sample t-test
results with p &lt; 0.01 and the error bars show 1 SD.
</figureCaption>
<bodyText confidence="0.9982955">
verbal features alone. Moreover we empirically validate the hypothesis that a sentiment-aware classi-
fier outperforms a sentiment-independent one. As future work, we intend to explore more paraverbal
features for persuasiveness prediction and also try more sophisticated prediction models which explic-
itly model the temporal dynamic.
</bodyText>
<sectionHeader confidence="0.9982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999397666666667">
This work was supported by the National Science Foundation under Grant IIS-1118018 and the U.S.
Army. The content does not necessarily reflect the position or the policy of the Government, and no
official endorsement should be inferred.
</bodyText>
<sectionHeader confidence="0.999226" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99916475">
Shelly Chaiken and Alice H. Eagly. 1979. Communication modality as a determinant of message persuasiveness
and message comprehensibility. Journal of Personality and Social Psychology, 37:1387-1397.
Jana Dankovicova. 1999. Articulation rate variation within the intonation phrase in Czech and English. 14th Int.
Congress of Phonetic Sciences, San Francisco, Vol. 1, pp. 269-272.
Kushal Dave, Steve Lawrence, and David M. Pennck. Mining the Peanut Gallery: Opinion Extraction and
semantic Classification of Product Reviews, 2003. 2003 Association for Computational Linguistics (ACL
’03).
David DeVault, Kallirroi Georgila, Ron Artstein, Fabrizio Morbini, David Traum, Stefan, Scherer, Albert (Skip)
Rizzo, and Louis-Philippe Morency. 2013. Verbal indicators of psychological distress in interactive dialogue
with a virtual human. SIGDIAL 2013 Conf, 2013 Association for Computational Linguistics (ACL ‘13).
Laurence Devillers and Laurence Vidrascu. 2006. Real-life emotions detection with lexical and paralinguistric
cues on Human-Human call center dialogs. Interspeech 2006.
Hatice Gunes, and Massimo Piccardi. 2005. Affect Recognition from face and body: Early fusion vs. Late
fusion. IEEE Int’l Conf. on Systems, Man and Cybernnetic.
Daniel J. O’Keefe. 2002. Persuasion: Theory and research. (2nd Edition). Sage Publications, Thousand Oaks,
CA.
David D. Lewis and William A. Gale. 1994. A Sequential Algorithm for Training Text Classifiers. Special
Interest Group in Information Retieval (SIGIR’94 ).
Gerald R. Miller (1980). On being persuaded: Some basic distinctions. In M. Roloff, &amp; G. R. Miller (Eds.),
Persuasion: New directions in theory and research, 11–28. Beverly Hills, CA: Sage.
</reference>
<page confidence="0.980422">
57
</page>
<reference confidence="0.999396631578947">
Tom M. Mitchell. 1997. Machine Learning. McGraw-Hill.
Gelareh Mohammadi, Sunghyun Park, Kenji Sagae, Alessandro Vinciarelli, and Lois-Phillippe Morency. 2013.
Who is persuasive? The role of perceived personality and Communication modality in social multimedia.
Int’l Conf. on Multimodal Interfaces (ICMI ’13).
P. Karen Murphy. 2001. What makes a text persuasive? Comparing students’ and experts’ conceptions of
persuasiveness. Int’l Journal of Education Research, 35 (2001) 675-698.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. Conf. on Empirical Methods in Natural Language Processing. (EMNLP ’02).
Antti Puurula. 2012. Combining Modifications to Multinomial Naive Bayes for Text Classification. Springer,
LNCS.
Kathleen Kelley Reardon. 1991. Persuasion in practice. Sage Publication, Inc.
Carol Werner. 1982. Intrusiveness and persuasive impact of three communication media. Journal of Applied
Social Psychology, 89:155-181.
Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. Int’l
Conf. on Machine Learning (ICML ‘97).
Joel Young, Craig Martell, Pranav Anand, Pedro Ortiz and Henry T. Gilbert IV. 2011. A Microtext Corpus for
Persuasion Detection in Dialog. Analyzing Microtext: AAAI Workshop (AAAI-Workshop ‘11).
Phillip G. Zimbardo and Michael R. Leippe. 1991. The psychology of attitude change and social influence.
McGrew-Hill New York.
</reference>
<page confidence="0.999261">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.743335">
<title confidence="0.995221">Verbal Behaviors and Persuasiveness in Online Multimedia Content</title>
<author confidence="0.998524">Moitreya Chatterjee</author>
<author confidence="0.998524">Sunghyun Park</author>
<author confidence="0.998524">Han Suk</author>
<affiliation confidence="0.9627645">Sagae USC Institute for Creative</affiliation>
<address confidence="0.999234">Los Angeles, CA 90094</address>
<email confidence="0.9199315">metro.smiles@gmail.com,{park,hshim,sagae,morency}@ict.usc.edu</email>
<abstract confidence="0.995208583333333">Persuasive communication is an essential component of our daily lives, whether it is negotiating, reviewing a product, or campaigning for the acceptance of a point of view. With the rapid expansion of social media websites such as YouTube, Vimeo and ExpoTV, it is becoming ever more important and useful to understand persuasiveness in social multimedia content. In this paper we present a novel analysis of verbal behavior, based on lexical usage and paraverbal markers of hesitation, in the context of predicting persuasiveness in online multimedia content. Toward the end goal of predicting perceived persuasion, this work also explores the potential differences in verbal behavior of people expressing a positive opinion (e.g., a positive movie review) versus a negative one. The analysis is performed on a multimedia corpus of 1,000 movie review videos annotated for persuasiveness. Our results show that verbal behavior can be a significant predictor of persuasiveness in such online multimedia content.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shelly Chaiken</author>
<author>Alice H Eagly</author>
</authors>
<title>Communication modality as a determinant of message persuasiveness and message comprehensibility.</title>
<date>1979</date>
<journal>Journal of Personality and Social Psychology,</journal>
<pages>37--1387</pages>
<contexts>
<context position="2629" citStr="Chaiken and Eagly, 1979" startWordPosition="401" endWordPosition="404">xpoTV, in particular, is a repository of a large number of videos dedicated for product reviews in which people try to convince others in favor of or against the use of various products. This raises an interesting research problem as to what it is that makes certain speakers have a substantial impact on others’ opinions while other speakers are ignored. In this paper, we present a novel analysis of spoken persuasion in online multimedia content. Our work is motivated by prior research findings in psychology indicating that verbal behavior is a promising indicator for persuasive communication (Chaiken and Eagly, 1979; Werner, 1982). Such prior findings allow us to hypothesize that two primary types of verbal features will be predictive of persuasion: lexical features and paraverbal markers of hesitation. Additionally we explore the relationship of the sentiment of the content and perceived persuasion, by hypothesizing that speakers’ exhibit different verbal behavior when expressing a positive opinion versus a negative one and taking into account these differences will improve prediction performance. We conduct several experiments in order to validate these hypotheses using a multimedia corpus of 1,000 mov</context>
<context position="4688" citStr="Chaiken and Eagly, 1979" startWordPosition="702" endWordPosition="705">utational descriptors and methodology are described in Section 5. We discuss the results and findings in Section 6, and finally we conclude our paper and present some future directions of research in Section 7. 2 Related Work Content in the form of written text are omnipresent in our society. Starting from books, magazines and newspapers to the now prevalent emails and blog posts, text-based content are an invaluable component for effective communication. Prior research reports possibly greater persuasiveness in written messages compared to visual or acoustic modalities in certain situations (Chaiken and Eagly, 1979; Werner, 1982). Past research has also revealed that for sophisticated messages, such as those used in a martial setting, written messages are more persuasive (Chaiken and Eagly, 1979). Although the importance of studying verbal behavior for determining persuasiveness has been underscored in prior work in the field of communication sciences (O’Keefe, 2002) and this line of research gives us useful pointers to the factors that contribute to persuasiveness in text or verbal communication, they present no computational aspect, which is where we put our emphasis in the paper. In the field of natu</context>
</contexts>
<marker>Chaiken, Eagly, 1979</marker>
<rawString>Shelly Chaiken and Alice H. Eagly. 1979. Communication modality as a determinant of message persuasiveness and message comprehensibility. Journal of Personality and Social Psychology, 37:1387-1397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jana Dankovicova</author>
</authors>
<title>Articulation rate variation within the intonation phrase</title>
<date>1999</date>
<booktitle>in Czech and English. 14th Int. Congress of Phonetic Sciences,</booktitle>
<volume>1</volume>
<pages>269--272</pages>
<location>San Francisco,</location>
<contexts>
<context position="13519" citStr="Dankovicova, 1999" startWordPosition="2078" endWordPosition="2079"> of reviewers are often characterized with various pausefillers, such as um or uh. In order to account for the varying length of each review, we normalized the count of all instances of filled pauses by the number of words spoken in the video. • Disfluency Markers: A prominent marker of disfluency in human speech is stuttering. To capture this disfluency, we counted all instances of stuttering in each video and normalized them by the number of words spoken in the video. • Articulation Rate: Articulation rate is defined as the rate of speaking in which all pauses are excluded from calculation (Dankovicova, 1999). This descriptor was computed by taking the ratio of the number of spoken words in each video to the actual time spent speaking. • Mean Span of Silence: Human speech is often interspersed with pauses. We therefore computed this descriptor, by measuring the total duration of silence during speech, normalized by the total length of the video. 5.2 Methodology We processed all the videos in our dataset and automatically extracted the indicated lexical and paraverbal features. The extracted features were then used for several classification experiments under three different settings to test our hy</context>
</contexts>
<marker>Dankovicova, 1999</marker>
<rawString>Jana Dankovicova. 1999. Articulation rate variation within the intonation phrase in Czech and English. 14th Int. Congress of Phonetic Sciences, San Francisco, Vol. 1, pp. 269-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennck</author>
</authors>
<title>Mining the Peanut Gallery: Opinion Extraction and semantic Classification of Product Reviews,</title>
<date>2003</date>
<journal>Association for Computational Linguistics (ACL</journal>
<volume>03</volume>
<contexts>
<context position="5446" citStr="Dave et al., 2003" startWordPosition="821" endWordPosition="824">are more persuasive (Chaiken and Eagly, 1979). Although the importance of studying verbal behavior for determining persuasiveness has been underscored in prior work in the field of communication sciences (O’Keefe, 2002) and this line of research gives us useful pointers to the factors that contribute to persuasiveness in text or verbal communication, they present no computational aspect, which is where we put our emphasis in the paper. In the field of natural language processing, text classification based on bag-of-words has been a long standing approach (Lewis and Gale, 1994; Mitchell, 1997; Dave et al., 2003). In fact, Young et al. (2011) have explored lexical features in the specific context of predicting persuasion, but they focus their attention on studying persuasion in dialogue. Our work draws inspiration from such approaches but explores it in the specific context of predicting persuasiveness in online multimedia content using lexical and paraverbal features. 3 Research Hypotheses Motivated by prior works and theoretical background, we designed our experiments to validate three hypotheses. Since multiple prior works point to the usefulness of the text modality in persuasive communication and</context>
<context position="12349" citStr="Dave et al., 2003" startWordPosition="1889" endWordPosition="1892"> text classification tasks, we designed our verbal descriptors based on the bag-of-words representation using term frequency of both unigrams and bi1 Dataset available online: http://multicomp.ict.usc.edu/ 52 grams. Using the 300 filtered videos (see Section 4.1) and without feature selection, the numbers of unigrams reach around 4,500 and bigrams around 24,000. We did not proceed further with higher order n-grams because empirical evidence has shown that trigrams and other higher order n-grams do not always show improvement because they introduce problems related to the sparsity of features (Dave et al., 2003). Paraverbal Descriptors of Hesitation: From the verbatim transcriptions of our corpus, we observed a set of frequent paraverbal cues that could potentially be associated with the level of persuasiveness. The set of descriptors is inspired from the findings of DeVault et al. (2013), who explored a similar set of generic paraverbal features in an interactive dialogue setting. However, we are interested specifically in the ones that capture signs of hesitation. The following were the descriptors that were used: • Pause-Fillers: The verbal behaviors of reviewers are often characterized with vario</context>
</contexts>
<marker>Dave, Lawrence, Pennck, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M. Pennck. Mining the Peanut Gallery: Opinion Extraction and semantic Classification of Product Reviews, 2003. 2003 Association for Computational Linguistics (ACL ’03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Kallirroi Georgila</author>
<author>Ron Artstein</author>
<author>Fabrizio Morbini</author>
<author>David Traum</author>
<author>Scherer Stefan</author>
<author>Albert Rizzo</author>
<author>Louis-Philippe Morency</author>
</authors>
<title>Verbal indicators of psychological distress in interactive dialogue with a virtual human.</title>
<date>2013</date>
<booktitle>SIGDIAL 2013 Conf, 2013 Association for Computational Linguistics (ACL ‘13).</booktitle>
<contexts>
<context position="6941" citStr="DeVault et al, 2013" startWordPosition="1037" endWordPosition="1040">eviews). The following is the hypothesis that we specifically tested with our experiments: Hypothesis 1: Verbal behavior, as captured by lexical usage, is indicative of persuasiveness in online social multimedia content, irrespective of whether the opinion expressed is positive or negative. Paraverbal behaviors indicative of hesitation can constitute important information for predicting persuasiveness. For instance, a speaker’s stuttering or breaking his/her speech with filled pauses (such as um and uh) has influence on how other people perceive his/her persuasiveness. Although previous work (DeVault et al, 2013) suggests paraverbal behavior may be indicative of depression, another work on emotion prediction however, (Devillers et al., 2006) raised questions about its predictive power when compared to using standard cues derived from lexical usage. This leads us to our second hypothesis on paraverbal behaviors in the context of predicting persuasiveness: Hypothesis 2: Paraverbal behaviors related to hesitation are indicative of persuasiveness in online social multimedia content. Past research highlights the importance of the knowledge of the affective state of a document towards its perceived persuasi</context>
<context position="12631" citStr="DeVault et al. (2013)" startWordPosition="1932" endWordPosition="1935">ture selection, the numbers of unigrams reach around 4,500 and bigrams around 24,000. We did not proceed further with higher order n-grams because empirical evidence has shown that trigrams and other higher order n-grams do not always show improvement because they introduce problems related to the sparsity of features (Dave et al., 2003). Paraverbal Descriptors of Hesitation: From the verbatim transcriptions of our corpus, we observed a set of frequent paraverbal cues that could potentially be associated with the level of persuasiveness. The set of descriptors is inspired from the findings of DeVault et al. (2013), who explored a similar set of generic paraverbal features in an interactive dialogue setting. However, we are interested specifically in the ones that capture signs of hesitation. The following were the descriptors that were used: • Pause-Fillers: The verbal behaviors of reviewers are often characterized with various pausefillers, such as um or uh. In order to account for the varying length of each review, we normalized the count of all instances of filled pauses by the number of words spoken in the video. • Disfluency Markers: A prominent marker of disfluency in human speech is stuttering. </context>
</contexts>
<marker>DeVault, Georgila, Artstein, Morbini, Traum, Stefan, Rizzo, Morency, 2013</marker>
<rawString>David DeVault, Kallirroi Georgila, Ron Artstein, Fabrizio Morbini, David Traum, Stefan, Scherer, Albert (Skip) Rizzo, and Louis-Philippe Morency. 2013. Verbal indicators of psychological distress in interactive dialogue with a virtual human. SIGDIAL 2013 Conf, 2013 Association for Computational Linguistics (ACL ‘13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurence Devillers</author>
<author>Laurence Vidrascu</author>
</authors>
<title>Real-life emotions detection with lexical and paralinguistric cues on Human-Human call center dialogs. Interspeech</title>
<date>2006</date>
<marker>Devillers, Vidrascu, 2006</marker>
<rawString>Laurence Devillers and Laurence Vidrascu. 2006. Real-life emotions detection with lexical and paralinguistric cues on Human-Human call center dialogs. Interspeech 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hatice Gunes</author>
<author>Massimo Piccardi</author>
</authors>
<title>Affect Recognition from face and body: Early fusion vs.</title>
<date>2005</date>
<booktitle>Late fusion. IEEE Int’l Conf. on Systems, Man and Cybernnetic.</booktitle>
<contexts>
<context position="17695" citStr="Gunes and Piccardi, 2005" startWordPosition="2723" endWordPosition="2726">rbal features (no feature selection was used here since they were too few in number). The accuracy of classification based on paraverbal features was then compared with that obtained by classification using only the lexical descriptors and by a majority baseline classifier. Furthermore, we also tried an early-fusion approach, where we simply use both lexical and paraverbal features together. Such an approach to fusion seemed more promising here than a decisionlevel fusion approach because of the few categories of features used (just lexical and paralinguistic, as motivated by the findings of (Gunes and Piccardi, 2005)). 5.3 Classification Model For performing classification experiments we used the Naïve Bayes classifier. A well-known issue with using the Naïve Bayes classifier is its incapability of handling new features, which is handled by performing a conditional uniform smoothing (Puurula, 2012). 6 Results and Discussion Table 1 shows the results for our classification experiments, which confirm the predictive power of lexical features. Hypothesis 1: The lexical features (unigrams and bigrams) are predictive of persuasiveness. This is manifested by the fact that they perform significantly better than a</context>
</contexts>
<marker>Gunes, Piccardi, 2005</marker>
<rawString>Hatice Gunes, and Massimo Piccardi. 2005. Affect Recognition from face and body: Early fusion vs. Late fusion. IEEE Int’l Conf. on Systems, Man and Cybernnetic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel J O’Keefe</author>
</authors>
<title>Persuasion: Theory and research. (2nd Edition). Sage Publications,</title>
<date>2002</date>
<location>Thousand Oaks, CA.</location>
<marker>O’Keefe, 2002</marker>
<rawString>Daniel J. O’Keefe. 2002. Persuasion: Theory and research. (2nd Edition). Sage Publications, Thousand Oaks, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A Sequential Algorithm for Training Text Classifiers.</title>
<date>1994</date>
<booktitle>Special Interest Group in Information Retieval (SIGIR’94 ).</booktitle>
<contexts>
<context position="5410" citStr="Lewis and Gale, 1994" startWordPosition="815" endWordPosition="818">n a martial setting, written messages are more persuasive (Chaiken and Eagly, 1979). Although the importance of studying verbal behavior for determining persuasiveness has been underscored in prior work in the field of communication sciences (O’Keefe, 2002) and this line of research gives us useful pointers to the factors that contribute to persuasiveness in text or verbal communication, they present no computational aspect, which is where we put our emphasis in the paper. In the field of natural language processing, text classification based on bag-of-words has been a long standing approach (Lewis and Gale, 1994; Mitchell, 1997; Dave et al., 2003). In fact, Young et al. (2011) have explored lexical features in the specific context of predicting persuasion, but they focus their attention on studying persuasion in dialogue. Our work draws inspiration from such approaches but explores it in the specific context of predicting persuasiveness in online multimedia content using lexical and paraverbal features. 3 Research Hypotheses Motivated by prior works and theoretical background, we designed our experiments to validate three hypotheses. Since multiple prior works point to the usefulness of the text moda</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D. Lewis and William A. Gale. 1994. A Sequential Algorithm for Training Text Classifiers. Special Interest Group in Information Retieval (SIGIR’94 ).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald R Miller</author>
</authors>
<title>On being persuaded: Some basic distinctions. In</title>
<date>1980</date>
<journal>Machine Learning. McGraw-Hill.</journal>
<pages>11--28</pages>
<location>Beverly Hills, CA:</location>
<contexts>
<context position="1471" citStr="Miller, 1980" startWordPosition="219" endWordPosition="220">ard the end goal of predicting perceived persuasion, this work also explores the potential differences in verbal behavior of people expressing a positive opinion (e.g., a positive movie review) versus a negative one. The analysis is performed on a multimedia corpus of 1,000 movie review videos annotated for persuasiveness. Our results show that verbal behavior can be a significant predictor of persuasiveness in such online multimedia content. 1 Introduction A message that is “intended to shape, reinforce or change the responses of another or others” is categorized as persuasive communication (Miller, 1980), and it is particularly important for the role it plays in creating social influence and altering other people’s opinions (Reardon, 1991; Zimbardo and Leippe, 1991). For instance, a persuasive advertisement could be a potential profit churner. The growth of social networking sites on the Internet has resulted in an explosion of online content with the purpose of delivering persuasive messages. Websites such as YouTube, Vimeo and ExpoTV are examples of online media in which these messages propagate mainly in the form of videos. ExpoTV, in particular, is a repository of a large number of videos</context>
</contexts>
<marker>Miller, 1980</marker>
<rawString>Gerald R. Miller (1980). On being persuaded: Some basic distinctions. In M. Roloff, &amp; G. R. Miller (Eds.), Persuasion: New directions in theory and research, 11–28. Beverly Hills, CA: Sage. Tom M. Mitchell. 1997. Machine Learning. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gelareh Mohammadi</author>
<author>Sunghyun Park</author>
<author>Kenji Sagae</author>
<author>Alessandro Vinciarelli</author>
<author>Lois-Phillippe Morency</author>
</authors>
<title>Who is persuasive? The role of perceived personality and Communication modality in social multimedia.</title>
<date>2013</date>
<booktitle>Int’l Conf. on Multimodal Interfaces (ICMI ’13).</booktitle>
<contexts>
<context position="9507" citStr="Mohammadi et al., 2013" startWordPosition="1444" endWordPosition="1447">d 2-star videos due to the lack of enough 1-star videos on the website. Each video in the corpus has a frontal view of one person talking about a particular movie, and the average length of the videos is about 94 seconds. The corpus contains 372 unique speakers and 600 unique movie titles and is available to the community for purposes of academic research1. 4.1 Evaluation of Persuasiveness Amazon Mechanical Turk (AMT), which is a popular online crowdsourcing platform, was used to obtain subjective evaluation of each speaker’s perceived persuasiveness, following a similar annotation scheme as (Mohammadi et al., 2013). For each video in the corpus, we obtained 3 repeated evaluations on the level of persuasiveness of the speaker by asking the workers to give direct rating on each speaker’s persuasiveness on a Likert scale from 1 (very unpersuasive) to 7 (very persuasive). A total of 50 native English-speaking workers based in the United States participated in the evaluation process online, and the task was evenly distributed among the 50 workers. To minimize gender influence, the task was distributed such that the workers only evaluated speakers of the same gender. The correlation between the mean score of </context>
</contexts>
<marker>Mohammadi, Park, Sagae, Vinciarelli, Morency, 2013</marker>
<rawString>Gelareh Mohammadi, Sunghyun Park, Kenji Sagae, Alessandro Vinciarelli, and Lois-Phillippe Morency. 2013. Who is persuasive? The role of perceived personality and Communication modality in social multimedia. Int’l Conf. on Multimodal Interfaces (ICMI ’13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Karen Murphy</author>
</authors>
<title>What makes a text persuasive? Comparing students’ and experts’ conceptions of persuasiveness.</title>
<date>2001</date>
<journal>Int’l Journal of Education Research,</journal>
<volume>35</volume>
<pages>675--698</pages>
<contexts>
<context position="7562" citStr="Murphy, 2001" startWordPosition="1126" endWordPosition="1127">s paraverbal behavior may be indicative of depression, another work on emotion prediction however, (Devillers et al., 2006) raised questions about its predictive power when compared to using standard cues derived from lexical usage. This leads us to our second hypothesis on paraverbal behaviors in the context of predicting persuasiveness: Hypothesis 2: Paraverbal behaviors related to hesitation are indicative of persuasiveness in online social multimedia content. Past research highlights the importance of the knowledge of the affective state of a document towards its perceived persuasiveness (Murphy, 2001). We therefore hypothesize the following: Hypothesis 3: Knowledge of the sentiment polarity of a movie review improves classification of the speaker’s perceived persuasiveness. 4 Dataset ExpoTV.com is a popular website housing videos of product reviews. Each product review has a video of a speaker talking about a particular product as well as the speaker’s direct rating of the product on an integral scale from 1 star (for most negative review) to 5 stars (for most positive review). This direct rating is useful for the purpose of our study because this allows us to study perceived persuasion un</context>
</contexts>
<marker>Murphy, 2001</marker>
<rawString>P. Karen Murphy. 2001. What makes a text persuasive? Comparing students’ and experts’ conceptions of persuasiveness. Int’l Journal of Education Research, 35 (2001) 675-698.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>Conf. on Empirical Methods in Natural Language Processing. (EMNLP ’02).</booktitle>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. Conf. on Empirical Methods in Natural Language Processing. (EMNLP ’02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti Puurula</author>
</authors>
<title>Combining Modifications to Multinomial Naive Bayes for Text Classification.</title>
<date>2012</date>
<publisher>Springer, LNCS.</publisher>
<contexts>
<context position="17982" citStr="Puurula, 2012" startWordPosition="2765" endWordPosition="2766">ied an early-fusion approach, where we simply use both lexical and paraverbal features together. Such an approach to fusion seemed more promising here than a decisionlevel fusion approach because of the few categories of features used (just lexical and paralinguistic, as motivated by the findings of (Gunes and Piccardi, 2005)). 5.3 Classification Model For performing classification experiments we used the Naïve Bayes classifier. A well-known issue with using the Naïve Bayes classifier is its incapability of handling new features, which is handled by performing a conditional uniform smoothing (Puurula, 2012). 6 Results and Discussion Table 1 shows the results for our classification experiments, which confirm the predictive power of lexical features. Hypothesis 1: The lexical features (unigrams and bigrams) are predictive of persuasiveness. This is manifested by the fact that they perform significantly better than a majority baseline, which is only 51.04% accurate on the combined set of positive and negative reviews, while the lexical features achieved an accuracy of around 77% (Figure 1). Considering the positive and the negative reviews individually, we note that the lexical features were accura</context>
</contexts>
<marker>Puurula, 2012</marker>
<rawString>Antti Puurula. 2012. Combining Modifications to Multinomial Naive Bayes for Text Classification. Springer, LNCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen Kelley Reardon</author>
</authors>
<title>Persuasion in practice.</title>
<date>1991</date>
<publisher>Sage Publication, Inc.</publisher>
<contexts>
<context position="1608" citStr="Reardon, 1991" startWordPosition="240" endWordPosition="241">ssing a positive opinion (e.g., a positive movie review) versus a negative one. The analysis is performed on a multimedia corpus of 1,000 movie review videos annotated for persuasiveness. Our results show that verbal behavior can be a significant predictor of persuasiveness in such online multimedia content. 1 Introduction A message that is “intended to shape, reinforce or change the responses of another or others” is categorized as persuasive communication (Miller, 1980), and it is particularly important for the role it plays in creating social influence and altering other people’s opinions (Reardon, 1991; Zimbardo and Leippe, 1991). For instance, a persuasive advertisement could be a potential profit churner. The growth of social networking sites on the Internet has resulted in an explosion of online content with the purpose of delivering persuasive messages. Websites such as YouTube, Vimeo and ExpoTV are examples of online media in which these messages propagate mainly in the form of videos. ExpoTV, in particular, is a repository of a large number of videos dedicated for product reviews in which people try to convince others in favor of or against the use of various products. This raises an </context>
</contexts>
<marker>Reardon, 1991</marker>
<rawString>Kathleen Kelley Reardon. 1991. Persuasion in practice. Sage Publication, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Werner</author>
</authors>
<title>Intrusiveness and persuasive impact of three communication media.</title>
<date>1982</date>
<journal>Journal of Applied Social Psychology,</journal>
<pages>89--155</pages>
<contexts>
<context position="2644" citStr="Werner, 1982" startWordPosition="405" endWordPosition="406">a repository of a large number of videos dedicated for product reviews in which people try to convince others in favor of or against the use of various products. This raises an interesting research problem as to what it is that makes certain speakers have a substantial impact on others’ opinions while other speakers are ignored. In this paper, we present a novel analysis of spoken persuasion in online multimedia content. Our work is motivated by prior research findings in psychology indicating that verbal behavior is a promising indicator for persuasive communication (Chaiken and Eagly, 1979; Werner, 1982). Such prior findings allow us to hypothesize that two primary types of verbal features will be predictive of persuasion: lexical features and paraverbal markers of hesitation. Additionally we explore the relationship of the sentiment of the content and perceived persuasion, by hypothesizing that speakers’ exhibit different verbal behavior when expressing a positive opinion versus a negative one and taking into account these differences will improve prediction performance. We conduct several experiments in order to validate these hypotheses using a multimedia corpus of 1,000 movie review video</context>
<context position="4703" citStr="Werner, 1982" startWordPosition="706" endWordPosition="707"> methodology are described in Section 5. We discuss the results and findings in Section 6, and finally we conclude our paper and present some future directions of research in Section 7. 2 Related Work Content in the form of written text are omnipresent in our society. Starting from books, magazines and newspapers to the now prevalent emails and blog posts, text-based content are an invaluable component for effective communication. Prior research reports possibly greater persuasiveness in written messages compared to visual or acoustic modalities in certain situations (Chaiken and Eagly, 1979; Werner, 1982). Past research has also revealed that for sophisticated messages, such as those used in a martial setting, written messages are more persuasive (Chaiken and Eagly, 1979). Although the importance of studying verbal behavior for determining persuasiveness has been underscored in prior work in the field of communication sciences (O’Keefe, 2002) and this line of research gives us useful pointers to the factors that contribute to persuasiveness in text or verbal communication, they present no computational aspect, which is where we put our emphasis in the paper. In the field of natural language pr</context>
</contexts>
<marker>Werner, 1982</marker>
<rawString>Carol Werner. 1982. Intrusiveness and persuasive impact of three communication media. Journal of Applied Social Psychology, 89:155-181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>Int’l Conf. on Machine Learning (ICML ‘97).</booktitle>
<contexts>
<context position="15848" citStr="Yang and Pedersen, 1997" startWordPosition="2436" endWordPosition="2439">lly suffers from problems arising out of the sparsity of the entries of the dictionary in the dataset, we employed a feature selection step. For feature selection and analysis, we used Information Gain (IG), which is a measure of the number of bits of information obtained for category prediction by knowing the presence of a term in a document. Prior evaluation of feature-selection methods for text classification has revealed the superiority of IG as a metric over other ones such as Mutual Information, Term Strength or a simple Document Frequency thresholding for document classification tasks (Yang and Pedersen, 1997). This serves as an inspiring basis for using IG as a metric for feature selection. The gain score G(t) obtained from IG is a non-zero positive value for features that are strongly indicative of the extent of persuasiveness of the document, while ones that are not so informative have a value of 0. We therefore select only those lexical features (unigrams and bigrams) which have an IG &gt; 0 based on the distribution obtained from the training set. This allows us to trim the dictionary significantly and use only meaningful features for classification. 53 Feature Group Sentiment Dependent Classifie</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. Int’l Conf. on Machine Learning (ICML ‘97).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Young</author>
<author>Craig Martell</author>
<author>Pranav Anand</author>
<author>Pedro Ortiz</author>
<author>Henry T Gilbert</author>
</authors>
<title>A Microtext Corpus for Persuasion Detection in Dialog. Analyzing Microtext: AAAI Workshop (AAAI-Workshop ‘11).</title>
<date>2011</date>
<contexts>
<context position="5476" citStr="Young et al. (2011)" startWordPosition="827" endWordPosition="830">and Eagly, 1979). Although the importance of studying verbal behavior for determining persuasiveness has been underscored in prior work in the field of communication sciences (O’Keefe, 2002) and this line of research gives us useful pointers to the factors that contribute to persuasiveness in text or verbal communication, they present no computational aspect, which is where we put our emphasis in the paper. In the field of natural language processing, text classification based on bag-of-words has been a long standing approach (Lewis and Gale, 1994; Mitchell, 1997; Dave et al., 2003). In fact, Young et al. (2011) have explored lexical features in the specific context of predicting persuasion, but they focus their attention on studying persuasion in dialogue. Our work draws inspiration from such approaches but explores it in the specific context of predicting persuasiveness in online multimedia content using lexical and paraverbal features. 3 Research Hypotheses Motivated by prior works and theoretical background, we designed our experiments to validate three hypotheses. Since multiple prior works point to the usefulness of the text modality in persuasive communication and also to the power of text cla</context>
</contexts>
<marker>Young, Martell, Anand, Ortiz, Gilbert, 2011</marker>
<rawString>Joel Young, Craig Martell, Pranav Anand, Pedro Ortiz and Henry T. Gilbert IV. 2011. A Microtext Corpus for Persuasion Detection in Dialog. Analyzing Microtext: AAAI Workshop (AAAI-Workshop ‘11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phillip G Zimbardo</author>
<author>Michael R Leippe</author>
</authors>
<title>The psychology of attitude change and social influence.</title>
<date>1991</date>
<publisher>McGrew-Hill</publisher>
<location>New York.</location>
<contexts>
<context position="1636" citStr="Zimbardo and Leippe, 1991" startWordPosition="242" endWordPosition="245">e opinion (e.g., a positive movie review) versus a negative one. The analysis is performed on a multimedia corpus of 1,000 movie review videos annotated for persuasiveness. Our results show that verbal behavior can be a significant predictor of persuasiveness in such online multimedia content. 1 Introduction A message that is “intended to shape, reinforce or change the responses of another or others” is categorized as persuasive communication (Miller, 1980), and it is particularly important for the role it plays in creating social influence and altering other people’s opinions (Reardon, 1991; Zimbardo and Leippe, 1991). For instance, a persuasive advertisement could be a potential profit churner. The growth of social networking sites on the Internet has resulted in an explosion of online content with the purpose of delivering persuasive messages. Websites such as YouTube, Vimeo and ExpoTV are examples of online media in which these messages propagate mainly in the form of videos. ExpoTV, in particular, is a repository of a large number of videos dedicated for product reviews in which people try to convince others in favor of or against the use of various products. This raises an interesting research problem</context>
</contexts>
<marker>Zimbardo, Leippe, 1991</marker>
<rawString>Phillip G. Zimbardo and Michael R. Leippe. 1991. The psychology of attitude change and social influence. McGrew-Hill New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021973">
<title confidence="0.945142">
The KIT-LIMSI Translation System for WMT 2014
</title>
<author confidence="0.799147">
*Quoc Khanh Do, †Teresa Herrmann, *†Jan Niehues,
*Alexandre Allauzen, *Franc¸ois Yvon and †Alex Waibel
</author>
<affiliation confidence="0.9893">
*LIMSI-CNRS, Orsay, France
†Karlsruhe Institute of Technology, Karlsruhe, Germany
</affiliation>
<email confidence="0.997892">
*surname@limsi.fr †firstname.surname@kit.edu
</email>
<sectionHeader confidence="0.993858" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999808">
This paper describes the joined submis-
sion of LIMSI and KIT to the Shared
Translation Task for the German-to-
English direction. The system consists
of a phrase-based translation system us-
ing a pre-reordering approach. The base-
line system already includes several mod-
els like conventional language models on
different word factors and a discriminative
word lexicon. This system is used to gen-
erate a k-best list. In a second step, the
list is reranked using SOUL language and
translation models (Le et al., 2011).
Originally, SOUL translation models were
applied to n-gram-based translation sys-
tems that use tuples as translation units
instead of phrase pairs. In this article,
we describe their integration into the KIT
phrase-based system. Experimental re-
sults show that their use can yield sig-
nificant improvements in terms of BLEU
score.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998127913043478">
This paper describes the KIT-LIMSI system for
the Shared Task of the ACL 2014 Ninth Work-
shop on Statistical Machine Translation. The sys-
tem participates in the German-to-English trans-
lation task. It consists of two main components.
First, a k-best list is generated using a phrase-
based machine translation system. This system
will be described in Section 2. Afterwards, the k-
best list is reranked using SOUL (Structured OUt-
put Layer) models. Thereby, a neural network lan-
guage model (Le et al., 2011), as well as several
translation models (Le et al., 2012a) are used. A
detailed description of these models can be found
in Section 3. While the translation system uses
phrase pairs, the SOUL translation model uses tu-
ples as described in the n-gram approach (Mari˜no
et al., 2006). We describe the integration of the
SOUL models into the translation system in Sec-
tion 3.2. Section 4 summarizes the experimen-
tal results and compares two different tuning al-
gorithms: Minimum Error Rate Training (Och,
2003) and k-best Batch Margin Infused Relaxed
Algorithm (Cherry and Foster, 2012).
</bodyText>
<sectionHeader confidence="0.950256" genericHeader="method">
2 Baseline system
</sectionHeader>
<bodyText confidence="0.999822193548387">
The KIT translation system is an in-house imple-
mentation of the phrase-based approach and in-
cludes a pre-ordering step. This system is fully
described in Vogel (2003).
To train translation models, the provided Eu-
roparl, NC and Common Crawl parallel corpora
are used. The target side of those parallel corpora,
the News Shuffle corpus and the GigaWord cor-
pus are used as monolingual training data for the
different language models. Optimization is done
with Minimum Error Rate Training as described
in Venugopal et al. (2005), using newstest2012
and newstest2013 as development and test data,
respectively.
Compound splitting (Koehn and Knight, 2003)
is performed on the source side (German) of the
corpus before training. Since the web-crawled
Common Crawl corpus is noisy, this corpus is
first filtered using an SVM classifier as described
in Mediani et al. (2011).
The word alignment is generated using the
GIZA++ Toolkit (Och and Ney, 2003). Phrase
extraction and scoring is done using the Moses
toolkit (Koehn et al., 2007). Phrase pair proba-
bilities are computed using modified Kneser-Ney
smoothing (Foster et al., 2006).
We apply short-range reorderings (Rottmann
and Vogel, 2007) and long-range reorder-
ings (Niehues and Kolss, 2009) based on part-of-
speech tags. The POS tags are generated using
the TreeTagger (Schmid, 1994). Rewriting rules
</bodyText>
<page confidence="0.978709">
84
</page>
<bodyText confidence="0.906674214285714">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
and t contains J target words (t1,..., tJ). In the
n-gram approach (Mari˜no et al., 2006; Crego et
al., 2011), this segmentation is a by-product of
source reordering, and ultimately derives from ini-
tial word and phrase alignments. In this frame-
work, the basic translation units are tuples, which
are analogous to phrase pairs, and represent a
matching u = (s, t) between a source phrase s
and a target phrase t.
Using the n-gram assumption, the joint proba-
bility of a segmented sentence pair using L tupels
decomposes as:
</bodyText>
<equation confidence="0.9994075">
P(s, t) = HL P(ui|ui−1,...,ui−n+1) (1)
i=1
</equation>
<bodyText confidence="0.999776925925926">
A first issue with this decomposition is that the
elementary units are bilingual pairs. Therefore,
the underlying vocabulary and hence the number
of parameters can be quite large, even for small
translation tasks. Due to data sparsity issues, such
models are bound to face severe estimation prob-
lems. Another problem with Equation (1) is that
the source and target sides play symmetric roles,
whereas the source side is known, and the tar-
get side must be predicted. To overcome some
of these issues, the n-gram probability in Equa-
tion (1) can be factored by first decomposing tu-
ples in two (source and target) parts, and then de-
composing the source and target parts at the word
level.
Let ski denote the kth word of source part of the
tuple si. Let us consider the example of Figure 1,
s111 corresponds to the source word nobel, si1 to
the source word paix, and similarly t211 is the tar-
get word peace. We finally define hn−1(tki ) as the
sequence of the n−1 words preceding tki in the tar-
get sentence, and hn−1(ski ) as the n−1 words pre-
ceding ski in the reordered source sentence: in Fig-
ure 1, h3(t211) thus refers to the three word context
receive the nobel associated with the target word
peace. Using these notations, Equation 1 can be
rewritten as:
</bodyText>
<equation confidence="0.9953015">
P(s, t) = HL r H|ti |P(tki |hn−1(tki ),hn−1(s1i+1))
i=1 k=1
P(sk i ))i (2)
i |hn−1(t1 i ),hn−1(sk
</equation>
<bodyText confidence="0.9996205">
This decomposition relies on the n-gram assump-
tion, this time at the word level. Therefore, this
</bodyText>
<equation confidence="0.8245275">
X H|si|
k=1
</equation>
<bodyText confidence="0.99992462962963">
based on POS sequences are learnt automatically
to perform source sentence reordering according
to the target language word order. The long-range
reordering rules are further applied to the training
corpus to create reordering lattices to extract the
phrases for the translation model. In addition,
a tree-based reordering model (Herrmann et al.,
2013) trained on syntactic parse trees (Rafferty
and Manning, 2008; Klein and Manning, 2003)
is applied to the source sentence. In addition
to these pre-reordering models, a lexicalized
reordering model (Koehn et al., 2005) is applied
during decoding.
Language models are trained with the SRILM
toolkit (Stolcke, 2002) using modified Kneser-Ney
smoothing (Chen and Goodman, 1996). The sys-
tem uses a 4-gram word-based language model
trained on all monolingual data and an additional
language model trained on automatically selected
data (Moore and Lewis, 2010). The system fur-
ther applies a language model based on 1000 auto-
matically learned word classes using the MKCLS
algorithm (Och, 1999). In addition, a bilingual
language model (Niehues et al., 2011) is used as
well as a discriminative word lexicon (DWL) us-
ing source context to guide the word choices in the
target sentence.
</bodyText>
<sectionHeader confidence="0.914108" genericHeader="method">
3 SOUL models for statistical machine
translation
</sectionHeader>
<bodyText confidence="0.999969">
Neural networks, working on top of conventional
n-gram back-off language models (BOLMs), have
been introduced in (Bengio et al., 2003; Schwenk,
2007) as a potential means to improve discrete
language models. The SOUL model (Le et al.,
2011) is a specific neural network architecture that
allows us to estimate n-gram models using large
vocabularies, thereby making the training of large
neural network models feasible both for target lan-
guage models and translation models (Le et al.,
2012a).
</bodyText>
<subsectionHeader confidence="0.997809">
3.1 SOUL translation models
</subsectionHeader>
<bodyText confidence="0.9993082">
While the integration of SOUL target language
models is straightforward, SOUL translation mod-
els rely on a specific decomposition of the joint
probability P(s, t) of a sentence pair, where s is a
sequence of I reordered source words (s1, ..., sI)1
</bodyText>
<footnote confidence="0.992260666666667">
1In the context of the n-gram translation model, (s, t) thus
denotes an aligned sentence pair, where the source words are
reordered.
</footnote>
<page confidence="0.989417">
85
</page>
<equation confidence="0.679927">
org : .... à recevoir le prix nobel de la paix
s : ....
t : ....
</equation>
<figure confidence="0.973679529411765">
t8: to
8: à
̅
s
9: recevoir
̅
s
t̅9: receive
t̅10: the
s̅10: le
s̅11: nobel de la paix
t̅11: nobel peace
t̅12: prize
s̅12: prix
....
....
u8 u9 u10 u11 u12
</figure>
<figureCaption confidence="0.9266915">
Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org)
French sentence appears at the top of the figure, just above the reordered source s and the target t. The
pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1, ..., uL. Each tuple ui contains a
source and a target phrase: si and ti.
</figureCaption>
<bodyText confidence="0.999546470588235">
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one
for each language; however, the moves of these
windows remain synchronized by the tuple seg-
mentation. Moreover, the context is not limited
to the current phrase, and continues to include
words in adjacent phrases. Equation (2) involves
two terms that will be further denoted as TrgSrc
and Src, respectively P(tki |hn−1(tki ), hn−1(s1i+1))
and P(ski |hn−1(t1i ), hn−1(ski )). It is worth notic-
ing that the joint probability of a sentence pair
can also be decomposed by considering the fol-
lowing two terms: P(sk|hn−1(ski), hn−1(t1i+1))
and P (tki |hn−1(s1i ), hn−1(tk i )). These two terms
will be further denoted by SrcTrg and Trg. There-
fore, adding SOUL translation models means that
4 scores are added to the phrase-based systems.
</bodyText>
<subsectionHeader confidence="0.982816">
3.2 Integration
</subsectionHeader>
<bodyText confidence="0.999977842105263">
During the training step, the SOUL translation
models are trained as described in (Le et al.,
2012a). The main changes concern the inference
step. Given the computational cost of computing
n-gram probabilities with neural network models,
a solution is to resort to a two-pass approach: the
first pass uses a conventional system to produce
a k-best list (the k most likely hypotheses); in
the second pass, probabilities are computed by the
SOUL models for each hypothesis and added as
new features. Then the k-best list is reordered ac-
cording to a combination of all features including
these new features. In the following experiments,
we use 10-gram SOUL models to rescore 300-
best lists. Since the phrase-based system described
in Section 2 uses source reordering, the decoder
was modified in order to generate k-best lists that
contain necessary word alignment information be-
tween the reordered source sentence and its asso-
ciated target hypothesis. The goal is to recover
the information that is illustrated in Figure 1 and
to apply the n-gram decomposition of a sentence
pair.
These (target and bilingual) neural network
models produce scores for each hypothesis in the
k-best list; these new features, along with the fea-
tures from the baseline system, are then provided
to a new phase which runs the traditional Mini-
mum Error Rate Training (MERT) (Och, 2003), or
a recently proposed k-best Batch Margin Infused
Relaxed Algorithm (KBMIRA) (Cherry and Fos-
ter, 2012) for tuning purpose. The SOUL mod-
els used for this year’s evaluation are similar to
those described in Allauzen et al. (2013) and Le
et al. (2012b). However, since compared to these
evaluations less parallel data is available for the
German-to-English task, we use smaller vocabu-
laries of about 100K words.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999972176470588">
We evaluated the SOUL models on the German-
to-English translation task using two systems to
generate the k-best lists. The first system used
all models of the baseline system except the DWL
model and the other one used all models.
Table 1 summarizes experimental results in
terms of BLEU scores when the tuning is per-
formed using KBMIRA. As described in Section
3, the probability of a phrase pair can be decom-
posed into products of words’ probabilities in 2
different ways: we can first estimate the probabil-
ity of words in the source phrase given the context,
and then the probability of the target phrase given
its associated source phrase and context words
(see Equation (2)); or inversely we can generate
the target side before the source side. The for-
mer proceeds by adding Src and TrgSrc scores as
</bodyText>
<page confidence="0.996939">
86
</page>
<table confidence="0.997833875">
Soul models No DWL DWL
Dev Test Dev Test
No 26.02 27.02 26.27 27.46
Target 26.30 27.42 26.43 27.85
Translation st 26.46 27.70 26.66 28.04
Translation ts 26.48 27.41 26.61 28.00
All Translation 26.50 27.86 26.70 28.08
All SOUL models 26.62 27.84 26.75 28.10
</table>
<tableCaption confidence="0.995351">
Table 1: Results using KBMIRA
</tableCaption>
<table confidence="0.99990575">
No DWL DWL
Soul models Dev Test Dev Test
No 26.02 27.02 26.27 27.46
Target 26.18 27.09 26.44 27.54
Translation st 26.36 27.59 26.66 27.80
Translation ts 26.44 27.69 26.63 27.94
All Translation 26.53 27.65 26.69 27.99
All SOUL models 26.47 27.68 26.66 28.01
</table>
<tableCaption confidence="0.999748">
Table 2: Results using MERT. Results in bold correpond to the submitted system.
</tableCaption>
<bodyText confidence="0.999951955555555">
2 new features into the k-best list, and the latter by
adding Trg and SrcTrg scores. These 2 methods
correspond respectively to the Translation ts and
Translation st lines in the Table 1. The 4 trans-
lation models may also be added simultaneously
(All Translations). The first line gives baseline
results without SOUL models, while the Target
line shows results in adding only SOUL language
model. The last line (All SOUL models) shows
the results for adding all neural network models
into the baseline systems.
As evident in Table 1, using the SOUL trans-
lation models yields generally better results than
using the SOUL target language model, yielding
about 0.2 BLEU point differences on dev and test
sets. We can therefore assume that the SOUL
translation models provide richer information that,
to some extent, covers that contained in the neural
network language model. Indeed, these 4 trans-
lation models take into account not only lexi-
cal probabilities of translating target words given
source words (or in the inverse order), but also the
probabilities of generating words in the target side
(Trg model) as does a language model, with the
same context length over both source and target
sides. It is therefore not surprising that adding the
SOUL language model along with all translation
models (the last line in the table) does not give sig-
nificant improvement compared to the other con-
figurations. The different ways of using the SOUL
translation models perform very similarly.
Table 2 summarizes the results using MERT in-
stead of KBMIRA. We can observe that using KB-
MIRA results in 0.1 to 0.2 BLEU point improve-
ments compared to MERT. Moreover, this impact
becomes more important when more features are
considered (the last line when all 5 neural net-
work models are added into the baseline systems).
In short, the use of neural network models yields
up to 0.6 BLEU improvement on the DWL sys-
tem, and a 0.8 BLEU gain on the system without
DWL. Unfortunately, the experiments with KB-
MIRA were carried out after the the submission
date. Therefore the submitted system corresponds
to the last line of table 2 indicated in bold.
</bodyText>
<sectionHeader confidence="0.993859" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999965888888889">
We presented a system with two main features: a
phrase-based translation system which uses pre-
reordering and the integration of SOUL target lan-
guage and translation models. Although the trans-
lation performance of the baseline system is al-
ready very competitive, the rescoring by SOUL
models improve the performance significantly. In
the rescoring step, we used a continuous language
model as well as four continuous translation mod-
</bodyText>
<page confidence="0.997513">
87
</page>
<bodyText confidence="0.999962038461539">
els. When combining the different SOUL models,
the translation models are observed to be more im-
portant in increasing the translation performance
than the language model. Moreover, we observe a
slight benefit to use KBMIRA instead of the stan-
dard MERT tuning algorithm. It is worth noticing
that using KBMIRA improves the performance
but also reduces the variance of the final results.
As future work, the integration of the SOUL
translation models could be improved in differ-
ent ways. For SOUL translation models, there
is a mismatch between translation units used dur-
ing the training step and those used by the de-
coder. The former are derived using the n-gram-
based approach, while the latter use the conven-
tional phrase extraction heuristic. We assume that
reducing this mismatch could improve the overall
performance. This can be achieved for instance
using forced decoding to infer a segmentation of
the training data into translation units. Then the
SOUL translation models can be trained using
this segmentation. For the SOUL target language
model, in these experiments we only used the En-
glish part of the parallel data for training. Results
may be improved by including all the monolingual
data.
</bodyText>
<sectionHeader confidence="0.985628" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9970245">
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n◦ 287658 as well as the French Ar-
maments Procurement Agency (DGA) under the
RAPID Rapmat project.
</bodyText>
<sectionHeader confidence="0.991478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998848027777778">
Alexandre Allauzen, Nicolas P´echeux, Quoc Khanh
Do, Marco Dinarelli, Thomas Lavergne, Aur´elien
Max, Hai-Son Le, and Franc¸ois Yvon. 2013.
Limsi@ wmt13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 60–
67.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
S.F. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
Proceedings of the 34th Annual Meeting on Associa-
tion for Computational Linguistics (ACL ’96), pages
310–318, Santa Cruz, California, USA.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427–436. Association for Computational Lin-
guistics.
Josep M. Crego, Franois Yvon, and Jos B. Mari˜no.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49–58.
George F. Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In EMNLP, pages 53–61.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL
2003.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of ACL 2007, Demonstration Ses-
sion, Prague, Czech Republic.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc¸ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP, pages 5524–5527.
Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon.
2012a. Continuous space translation models with
neural networks. pages 39–48, Montr´eal, Canada,
June. Association for Computational Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aur´elien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc¸ois Yvon. 2012b. Limsi@ wmt’12. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, pages 330–337. Association for
Computational Linguistics.
</reference>
<page confidence="0.994695">
88
</page>
<reference confidence="0.999840553846154">
Jos´e B. Mari˜no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrick Lambert, Jos´e A.R. Fonol-
losa, and Marta R. Costa-Juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527–549.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight Interna-
tional Workshop on Spoken Language Translation
(IWSLT).
R.C. Moore and W. Lewis. 2010. Intelligent selection
of language model training data. In Proceedings of
the ACL 2010 Conference Short Papers, pages 220–
224, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 1999. An Efficient Method for De-
termining Bilingual Word Classes. In EACL’99.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160–167. As-
sociation for Computational Linguistics.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI), Sk¨ovde, Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, United Kingdom.
Holger Schwenk. 2007. Continuous space lan-
guage models. Computer Speech and Language,
21(3):492–518, July.
Andreas Stolcke. 2002. SRILM – An Extensible Lan-
guage Modeling Toolkit. In International Confer-
ence on Spoken Language Processing, Denver, Col-
orado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluating Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, Michigan, USA.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
</reference>
<page confidence="0.999749">
89
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.445606">
<title confidence="0.68914">The KIT-LIMSI Translation System for WMT 2014</title>
<author confidence="0.7755905">Khanh Do</author>
<author confidence="0.7755905">Allauzen Herrmann</author>
<author confidence="0.7755905">Yvon</author>
<affiliation confidence="0.9770435">Orsay, Institute of Technology, Karlsruhe,</affiliation>
<abstract confidence="0.998295391304348">This paper describes the joined submission of LIMSI and KIT to the Shared Translation Task for the German-to- English direction. The system consists of a phrase-based translation system using a pre-reordering approach. The baseline system already includes several models like conventional language models on different word factors and a discriminative word lexicon. This system is used to gena list. In a second step, the is reranked using and translation models (Le et al., 2011). models were to translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield significant improvements in terms of BLEU score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandre Allauzen</author>
<author>Nicolas P´echeux</author>
<author>Quoc Khanh Do</author>
<author>Marco Dinarelli</author>
<author>Thomas Lavergne</author>
<author>Aur´elien Max</author>
<author>Hai-Son Le</author>
<author>Franc¸ois Yvon</author>
</authors>
<date>2013</date>
<booktitle>Limsi@ wmt13. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>60--67</pages>
<marker>Allauzen, P´echeux, Do, Dinarelli, Lavergne, Max, Le, Yvon, 2013</marker>
<rawString>Alexandre Allauzen, Nicolas P´echeux, Quoc Khanh Do, Marco Dinarelli, Thomas Lavergne, Aur´elien Max, Hai-Son Le, and Franc¸ois Yvon. 2013. Limsi@ wmt13. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 60– 67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="7206" citStr="Bengio et al., 2003" startWordPosition="1156" endWordPosition="1159"> monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 SOUL models for statistical machine translation Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. The SOUL model (Le et al., 2011) is a specific neural network architecture that allows us to estimate n-gram models using large vocabularies, thereby making the training of large neural network models feasible both for target language models and translation models (Le et al., 2012a). 3.1 SOUL translation models While the integration of SOUL target language models is straightforward, SOUL translation models rely on a specific decomposition of the joint probability P(s, t) of a sentence pair, where s is a sequence of I re</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics (ACL ’96),</booktitle>
<pages>310--318</pages>
<location>Santa Cruz, California, USA.</location>
<contexts>
<context position="6520" citStr="Chen and Goodman, 1996" startWordPosition="1046" endWordPosition="1049">target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 SOUL models for statistical machine translation Neural networks, working on top of conventional</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>S.F. Chen and J. Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics (ACL ’96), pages 310–318, Santa Cruz, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>427--436</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2220" citStr="Cherry and Foster, 2012" startWordPosition="343" endWordPosition="346">ral network language model (Le et al., 2011), as well as several translation models (Le et al., 2012a) are used. A detailed description of these models can be found in Section 3. While the translation system uses phrase pairs, the SOUL translation model uses tuples as described in the n-gram approach (Mari˜no et al., 2006). We describe the integration of the SOUL models into the translation system in Section 3.2. Section 4 summarizes the experimental results and compares two different tuning algorithms: Minimum Error Rate Training (Och, 2003) and k-best Batch Margin Infused Relaxed Algorithm (Cherry and Foster, 2012). 2 Baseline system The KIT translation system is an in-house implementation of the phrase-based approach and includes a pre-ordering step. This system is fully described in Vogel (2003). To train translation models, the provided Europarl, NC and Common Crawl parallel corpora are used. The target side of those parallel corpora, the News Shuffle corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and</context>
<context position="10870" citStr="Cherry and Foster, 2012" startWordPosition="1773" endWordPosition="1777">sary word alignment information between the reordered source sentence and its associated target hypothesis. The goal is to recover the information that is illustrated in Figure 1 and to apply the n-gram decomposition of a sentence pair. These (target and bilingual) neural network models produce scores for each hypothesis in the k-best list; these new features, along with the features from the baseline system, are then provided to a new phase which runs the traditional Minimum Error Rate Training (MERT) (Och, 2003), or a recently proposed k-best Batch Margin Infused Relaxed Algorithm (KBMIRA) (Cherry and Foster, 2012) for tuning purpose. The SOUL models used for this year’s evaluation are similar to those described in Allauzen et al. (2013) and Le et al. (2012b). However, since compared to these evaluations less parallel data is available for the German-to-English task, we use smaller vocabularies of about 100K words. 4 Results We evaluated the SOUL models on the Germanto-English translation task using two systems to generate the k-best lists. The first system used all models of the baseline system except the DWL model and the other one used all models. Table 1 summarizes experimental results in terms of B</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427–436. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Franois Yvon</author>
<author>Jos B Mari˜no</author>
</authors>
<date>2011</date>
<booktitle>N-code: an open-source Bilingual N-gram SMT Toolkit. Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>96--49</pages>
<marker>Crego, Yvon, Mari˜no, 2011</marker>
<rawString>Josep M. Crego, Franois Yvon, and Jos B. Mari˜no. 2011. N-code: an open-source Bilingual N-gram SMT Toolkit. Prague Bulletin of Mathematical Linguistics, 96:49–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George F Foster</author>
<author>Roland Kuhn</author>
<author>Howard Johnson</author>
</authors>
<title>Phrasetable smoothing for statistical machine translation.</title>
<date>2006</date>
<booktitle>In EMNLP,</booktitle>
<pages>53--61</pages>
<contexts>
<context position="3365" citStr="Foster et al., 2006" startWordPosition="524" endWordPosition="527">gopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules 84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics and t contains J target words (t1,..., tJ). In the n-gram approach (Mari˜no et al., 2006; Crego et al., 2011), this segmentation is a by-product of source reordering, and ultimately derives from ini</context>
</contexts>
<marker>Foster, Kuhn, Johnson, 2006</marker>
<rawString>George F. Foster, Roland Kuhn, and Howard Johnson. 2006. Phrasetable smoothing for statistical machine translation. In EMNLP, pages 53–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teresa Herrmann</author>
<author>Jan Niehues</author>
<author>Alex Waibel</author>
</authors>
<title>Combining Word Reordering Methods on different Linguistic Abstraction Levels for Statistical Machine Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Altanta, Georgia, USA,</location>
<contexts>
<context position="6146" citStr="Herrmann et al., 2013" startWordPosition="991" endWordPosition="994"> peace. Using these notations, Equation 1 can be rewritten as: P(s, t) = HL r H|ti |P(tki |hn−1(tki ),hn−1(s1i+1)) i=1 k=1 P(sk i ))i (2) i |hn−1(t1 i ),hn−1(sk This decomposition relies on the n-gram assumption, this time at the word level. Therefore, this X H|si| k=1 based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model </context>
</contexts>
<marker>Herrmann, Niehues, Waibel, 2013</marker>
<rawString>Teresa Herrmann, Jan Niehues, and Alex Waibel. 2013. Combining Word Reordering Methods on different Linguistic Abstraction Levels for Statistical Machine Translation. In Proceedings of the Seventh Workshop on Syntax, Semantics and Structure in Statistical Translation, Altanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="6233" citStr="Klein and Manning, 2003" startWordPosition="1004" endWordPosition="1007">(tki |hn−1(tki ),hn−1(s1i+1)) i=1 k=1 P(sk i ))i (2) i |hn−1(t1 i ),hn−1(sk This decomposition relies on the n-gram assumption, this time at the word level. Therefore, this X H|si| k=1 based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999).</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical Methods for Compound Splitting. In EACL,</title>
<date>2003</date>
<location>Budapest, Hungary.</location>
<contexts>
<context position="2889" citStr="Koehn and Knight, 2003" startWordPosition="447" endWordPosition="450"> is an in-house implementation of the phrase-based approach and includes a pre-ordering step. This system is fully described in Vogel (2003). To train translation models, the provided Europarl, NC and Common Crawl parallel corpora are used. The target side of those parallel corpora, the News Shuffle corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on </context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philipp Koehn and Kevin Knight. 2003. Empirical Methods for Compound Splitting. In EACL, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra B Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="6364" citStr="Koehn et al., 2005" startWordPosition="1024" endWordPosition="1027">e at the word level. Therefore, this X H|si| k=1 based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL 2007, Demonstration Session,</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="3267" citStr="Koehn et al., 2007" startWordPosition="510" endWordPosition="513">erent language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules 84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics and t contains J target words (t1,..., tJ). In the n-gram approach (Mari˜no et al., 2006; Crego et a</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of ACL 2007, Demonstration Session, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Ilya Oparin</author>
<author>Alexandre Allauzen</author>
<author>JeanLuc Gauvain</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Structured output layer neural network language model.</title>
<date>2011</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>5524--5527</pages>
<contexts>
<context position="795" citStr="Le et al., 2011" startWordPosition="113" endWordPosition="116">rlsruhe Institute of Technology, Karlsruhe, Germany *surname@limsi.fr †firstname.surname@kit.edu Abstract This paper describes the joined submission of LIMSI and KIT to the Shared Translation Task for the German-toEnglish direction. The system consists of a phrase-based translation system using a pre-reordering approach. The baseline system already includes several models like conventional language models on different word factors and a discriminative word lexicon. This system is used to generate a k-best list. In a second step, the list is reranked using SOUL language and translation models (Le et al., 2011). Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield significant improvements in terms of BLEU score. 1 Introduction This paper describes the KIT-LIMSI system for the Shared Task of the ACL 2014 Ninth Workshop on Statistical Machine Translation. The system participates in the German-to-English translation task. It consists of two main components. First, a k-best list is</context>
<context position="7313" citStr="Le et al., 2011" startWordPosition="1174" endWordPosition="1177">10). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 SOUL models for statistical machine translation Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. The SOUL model (Le et al., 2011) is a specific neural network architecture that allows us to estimate n-gram models using large vocabularies, thereby making the training of large neural network models feasible both for target language models and translation models (Le et al., 2012a). 3.1 SOUL translation models While the integration of SOUL target language models is straightforward, SOUL translation models rely on a specific decomposition of the joint probability P(s, t) of a sentence pair, where s is a sequence of I reordered source words (s1, ..., sI)1 1In the context of the n-gram translation model, (s, t) thus denotes an</context>
</contexts>
<marker>Le, Oparin, Allauzen, Gauvain, Yvon, 2011</marker>
<rawString>Hai-Son Le, Ilya Oparin, Alexandre Allauzen, JeanLuc Gauvain, and Franc¸ois Yvon. 2011. Structured output layer neural network language model. In Proceedings of ICASSP, pages 5524–5527.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous space translation models with neural networks.</title>
<date>2012</date>
<pages>39--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1696" citStr="Le et al., 2012" startWordPosition="258" endWordPosition="261">ficant improvements in terms of BLEU score. 1 Introduction This paper describes the KIT-LIMSI system for the Shared Task of the ACL 2014 Ninth Workshop on Statistical Machine Translation. The system participates in the German-to-English translation task. It consists of two main components. First, a k-best list is generated using a phrasebased machine translation system. This system will be described in Section 2. Afterwards, the kbest list is reranked using SOUL (Structured OUtput Layer) models. Thereby, a neural network language model (Le et al., 2011), as well as several translation models (Le et al., 2012a) are used. A detailed description of these models can be found in Section 3. While the translation system uses phrase pairs, the SOUL translation model uses tuples as described in the n-gram approach (Mari˜no et al., 2006). We describe the integration of the SOUL models into the translation system in Section 3.2. Section 4 summarizes the experimental results and compares two different tuning algorithms: Minimum Error Rate Training (Och, 2003) and k-best Batch Margin Infused Relaxed Algorithm (Cherry and Foster, 2012). 2 Baseline system The KIT translation system is an in-house implementation</context>
<context position="7562" citStr="Le et al., 2012" startWordPosition="1213" endWordPosition="1216">DWL) using source context to guide the word choices in the target sentence. 3 SOUL models for statistical machine translation Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. The SOUL model (Le et al., 2011) is a specific neural network architecture that allows us to estimate n-gram models using large vocabularies, thereby making the training of large neural network models feasible both for target language models and translation models (Le et al., 2012a). 3.1 SOUL translation models While the integration of SOUL target language models is straightforward, SOUL translation models rely on a specific decomposition of the joint probability P(s, t) of a sentence pair, where s is a sequence of I reordered source words (s1, ..., sI)1 1In the context of the n-gram translation model, (s, t) thus denotes an aligned sentence pair, where the source words are reordered. 85 org : .... à recevoir le prix nobel de la paix s : .... t : .... t8: to 8: à ̅ s 9: recevoir ̅ s t̅9: receive t̅10: the s̅10: le s̅11: nobel de la paix t̅11: nobel peace t̅12: prize s̅</context>
<context position="9500" citStr="Le et al., 2012" startWordPosition="1553" endWordPosition="1556">lves two terms that will be further denoted as TrgSrc and Src, respectively P(tki |hn−1(tki ), hn−1(s1i+1)) and P(ski |hn−1(t1i ), hn−1(ski )). It is worth noticing that the joint probability of a sentence pair can also be decomposed by considering the following two terms: P(sk|hn−1(ski), hn−1(t1i+1)) and P (tki |hn−1(s1i ), hn−1(tk i )). These two terms will be further denoted by SrcTrg and Trg. Therefore, adding SOUL translation models means that 4 scores are added to the phrase-based systems. 3.2 Integration During the training step, the SOUL translation models are trained as described in (Le et al., 2012a). The main changes concern the inference step. Given the computational cost of computing n-gram probabilities with neural network models, a solution is to resort to a two-pass approach: the first pass uses a conventional system to produce a k-best list (the k most likely hypotheses); in the second pass, probabilities are computed by the SOUL models for each hypothesis and added as new features. Then the k-best list is reordered according to a combination of all features including these new features. In the following experiments, we use 10-gram SOUL models to rescore 300- best lists. Since th</context>
<context position="11015" citStr="Le et al. (2012" startWordPosition="1801" endWordPosition="1804"> illustrated in Figure 1 and to apply the n-gram decomposition of a sentence pair. These (target and bilingual) neural network models produce scores for each hypothesis in the k-best list; these new features, along with the features from the baseline system, are then provided to a new phase which runs the traditional Minimum Error Rate Training (MERT) (Och, 2003), or a recently proposed k-best Batch Margin Infused Relaxed Algorithm (KBMIRA) (Cherry and Foster, 2012) for tuning purpose. The SOUL models used for this year’s evaluation are similar to those described in Allauzen et al. (2013) and Le et al. (2012b). However, since compared to these evaluations less parallel data is available for the German-to-English task, we use smaller vocabularies of about 100K words. 4 Results We evaluated the SOUL models on the Germanto-English translation task using two systems to generate the k-best lists. The first system used all models of the baseline system except the DWL model and the other one used all models. Table 1 summarizes experimental results in terms of BLEU scores when the tuning is performed using KBMIRA. As described in Section 3, the probability of a phrase pair can be decomposed into products</context>
</contexts>
<marker>Le, Allauzen, Yvon, 2012</marker>
<rawString>Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon. 2012a. Continuous space translation models with neural networks. pages 39–48, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hai-Son Le</author>
<author>Thomas Lavergne</author>
<author>Alexandre Allauzen</author>
<author>Marianna Apidianaki</author>
<author>Li Gong</author>
<author>Aur´elien Max</author>
<author>Artem Sokolov</author>
<author>Guillaume Wisniewski</author>
<author>Franc¸ois Yvon</author>
</authors>
<booktitle>2012b. Limsi@ wmt’12. In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>330--337</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Le, Lavergne, Allauzen, Apidianaki, Gong, Max, Sokolov, Wisniewski, Yvon, </marker>
<rawString>Hai-Son Le, Thomas Lavergne, Alexandre Allauzen, Marianna Apidianaki, Li Gong, Aur´elien Max, Artem Sokolov, Guillaume Wisniewski, and Franc¸ois Yvon. 2012b. Limsi@ wmt’12. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 330–337. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e B Mari˜no</author>
<author>Rafael E Banchs</author>
<author>Josep M Crego</author>
<author>Adri`a de Gispert</author>
<author>Patrick Lambert</author>
<author>Jos´e A R Fonollosa</author>
<author>Marta R Costa-Juss`a</author>
</authors>
<title>N-grambased machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Mari˜no, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-Juss`a, 2006</marker>
<rawString>Jos´e B. Mari˜no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrick Lambert, Jos´e A.R. Fonollosa, and Marta R. Costa-Juss`a. 2006. N-grambased machine translation. Computational Linguistics, 32(4):527–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Mediani</author>
<author>Eunah Cho</author>
<author>Jan Niehues</author>
<author>Teresa Herrmann</author>
<author>Alex Waibel</author>
</authors>
<title>The KIT English-French Translation systems for IWSLT</title>
<date>2011</date>
<booktitle>In Proceedings of the Eight International Workshop on Spoken Language Translation (IWSLT).</booktitle>
<contexts>
<context position="3105" citStr="Mediani et al. (2011)" startWordPosition="483" endWordPosition="486">llel corpora are used. The target side of those parallel corpora, the News Shuffle corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules 84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, Baltimore, Maryland USA, June </context>
</contexts>
<marker>Mediani, Cho, Niehues, Herrmann, Waibel, 2011</marker>
<rawString>Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa Herrmann, and Alex Waibel. 2011. The KIT English-French Translation systems for IWSLT 2011. In Proceedings of the Eight International Workshop on Spoken Language Translation (IWSLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>W Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>220--224</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6700" citStr="Moore and Lewis, 2010" startWordPosition="1074" endWordPosition="1077">. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 SOUL models for statistical machine translation Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. The SOUL model (Le </context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>R.C. Moore and W. Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the ACL 2010 Conference Short Papers, pages 220– 224, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Muntsin Kolss</author>
</authors>
<title>A POS-Based Model for Long-Range Reorderings in SMT.</title>
<date>2009</date>
<booktitle>In Fourth Workshop on Statistical Machine Translation (WMT</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="3479" citStr="Niehues and Kolss, 2009" startWordPosition="540" endWordPosition="543">splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules 84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics and t contains J target words (t1,..., tJ). In the n-gram approach (Mari˜no et al., 2006; Crego et al., 2011), this segmentation is a by-product of source reordering, and ultimately derives from initial word and phrase alignments. In this framework, the basic translation units are tuples, which are analogous to</context>
</contexts>
<marker>Niehues, Kolss, 2009</marker>
<rawString>Jan Niehues and Muntsin Kolss. 2009. A POS-Based Model for Long-Range Reorderings in SMT. In Fourth Workshop on Statistical Machine Translation (WMT 2009), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Teresa Herrmann</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Wider Context by Using Bilingual Language Models in Machine Translation.</title>
<date>2011</date>
<booktitle>In Sixth Workshop on Statistical Machine Translation (WMT 2011),</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="6896" citStr="Niehues et al., 2011" startWordPosition="1106" endWordPosition="1109">ition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 SOUL models for statistical machine translation Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. The SOUL model (Le et al., 2011) is a specific neural network architecture that allows us to estimate n-gram models using large vocabularies, thereby making the training of large neural network models feasible both </context>
</contexts>
<marker>Niehues, Herrmann, Vogel, Waibel, 2011</marker>
<rawString>Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex Waibel. 2011. Wider Context by Using Bilingual Language Models in Machine Translation. In Sixth Workshop on Statistical Machine Translation (WMT 2011), Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="3183" citStr="Och and Ney, 2003" startWordPosition="496" endWordPosition="499">e corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules 84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics and t contains J</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>An Efficient Method for Determining Bilingual Word Classes.</title>
<date>1999</date>
<booktitle>In EACL’99.</booktitle>
<contexts>
<context position="6832" citStr="Och, 1999" startWordPosition="1098" endWordPosition="1099">ning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 SOUL models for statistical machine translation Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. The SOUL model (Le et al., 2011) is a specific neural network architecture that allows us to estimate n-gram models using large vocabularies, thereby m</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>Franz Josef Och. 1999. An Efficient Method for Determining Bilingual Word Classes. In EACL’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2144" citStr="Och, 2003" startWordPosition="334" endWordPosition="335">ed using SOUL (Structured OUtput Layer) models. Thereby, a neural network language model (Le et al., 2011), as well as several translation models (Le et al., 2012a) are used. A detailed description of these models can be found in Section 3. While the translation system uses phrase pairs, the SOUL translation model uses tuples as described in the n-gram approach (Mari˜no et al., 2006). We describe the integration of the SOUL models into the translation system in Section 3.2. Section 4 summarizes the experimental results and compares two different tuning algorithms: Minimum Error Rate Training (Och, 2003) and k-best Batch Margin Infused Relaxed Algorithm (Cherry and Foster, 2012). 2 Baseline system The KIT translation system is an in-house implementation of the phrase-based approach and includes a pre-ordering step. This system is fully described in Vogel (2003). To train translation models, the provided Europarl, NC and Common Crawl parallel corpora are used. The target side of those parallel corpora, the News Shuffle corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Ven</context>
<context position="10765" citStr="Och, 2003" startWordPosition="1760" endWordPosition="1761">e reordering, the decoder was modified in order to generate k-best lists that contain necessary word alignment information between the reordered source sentence and its associated target hypothesis. The goal is to recover the information that is illustrated in Figure 1 and to apply the n-gram decomposition of a sentence pair. These (target and bilingual) neural network models produce scores for each hypothesis in the k-best list; these new features, along with the features from the baseline system, are then provided to a new phase which runs the traditional Minimum Error Rate Training (MERT) (Och, 2003), or a recently proposed k-best Batch Margin Infused Relaxed Algorithm (KBMIRA) (Cherry and Foster, 2012) for tuning purpose. The SOUL models used for this year’s evaluation are similar to those described in Allauzen et al. (2013) and Le et al. (2012b). However, since compared to these evaluations less parallel data is available for the German-to-English task, we use smaller vocabularies of about 100K words. 4 Results We evaluated the SOUL models on the Germanto-English translation task using two systems to generate the k-best lists. The first system used all models of the baseline system exce</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160–167. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna N Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Parsing German.</booktitle>
<contexts>
<context position="6207" citStr="Rafferty and Manning, 2008" startWordPosition="1000" endWordPosition="1003">n as: P(s, t) = HL r H|ti |P(tki |hn−1(tki ),hn−1(s1i+1)) i=1 k=1 P(sk i ))i (2) i |hn−1(t1 i ),hn−1(sk This decomposition relies on the n-gram assumption, this time at the word level. Therefore, this X H|si| k=1 based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MK</context>
</contexts>
<marker>Rafferty, Manning, 2008</marker>
<rawString>Anna N. Rafferty and Christopher D. Manning. 2008. Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines. In Proceedings of the Workshop on Parsing German.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kay Rottmann</author>
<author>Stephan Vogel</author>
</authors>
<title>Word Reordering in Statistical Machine Translation with a POS-Based Distortion Model.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Sk¨ovde,</booktitle>
<contexts>
<context position="3426" citStr="Rottmann and Vogel, 2007" startWordPosition="532" endWordPosition="535"> as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules 84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics and t contains J target words (t1,..., tJ). In the n-gram approach (Mari˜no et al., 2006; Crego et al., 2011), this segmentation is a by-product of source reordering, and ultimately derives from initial word and phrase alignments. In this framework, the basic</context>
</contexts>
<marker>Rottmann, Vogel, 2007</marker>
<rawString>Kay Rottmann and Stephan Vogel. 2007. Word Reordering in Statistical Machine Translation with a POS-Based Distortion Model. In Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Sk¨ovde, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, United Kingdom.</location>
<contexts>
<context position="3571" citStr="Schmid, 1994" startWordPosition="557" endWordPosition="558">ng. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules 84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics and t contains J target words (t1,..., tJ). In the n-gram approach (Mari˜no et al., 2006; Crego et al., 2011), this segmentation is a by-product of source reordering, and ultimately derives from initial word and phrase alignments. In this framework, the basic translation units are tuples, which are analogous to phrase pairs, and represent a matching u = (s, t) between a source phrase s and a target ph</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference on New Methods in Language Processing, Manchester, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="7222" citStr="Schwenk, 2007" startWordPosition="1160" endWordPosition="1161"> an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 SOUL models for statistical machine translation Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. The SOUL model (Le et al., 2011) is a specific neural network architecture that allows us to estimate n-gram models using large vocabularies, thereby making the training of large neural network models feasible both for target language models and translation models (Le et al., 2012a). 3.1 SOUL translation models While the integration of SOUL target language models is straightforward, SOUL translation models rely on a specific decomposition of the joint probability P(s, t) of a sentence pair, where s is a sequence of I reordered source w</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Holger Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21(3):492–518, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In International Conference on Spoken Language Processing,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="6459" citStr="Stolcke, 2002" startWordPosition="1040" endWordPosition="1041">perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 SOUL models for statistical machin</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In International Conference on Spoken Language Processing, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollman</author>
<author>Alex Waibel</author>
</authors>
<title>Training and Evaluating Error Minimization Rules for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Workshop on Data-drive Machine Translation and Beyond (WPT-05),</booktitle>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="2764" citStr="Venugopal et al. (2005)" startWordPosition="431" endWordPosition="434">03) and k-best Batch Margin Infused Relaxed Algorithm (Cherry and Foster, 2012). 2 Baseline system The KIT translation system is an in-house implementation of the phrase-based approach and includes a pre-ordering step. This system is fully described in Vogel (2003). To train translation models, the provided Europarl, NC and Common Crawl parallel corpora are used. The target side of those parallel corpora, the News Shuffle corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006</context>
</contexts>
<marker>Venugopal, Zollman, Waibel, 2005</marker>
<rawString>Ashish Venugopal, Andreas Zollman, and Alex Waibel. 2005. Training and Evaluating Error Minimization Rules for Statistical Machine Translation. In Workshop on Data-drive Machine Translation and Beyond (WPT-05), Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
</authors>
<title>SMT Decoder Dissected: Word Reordering.</title>
<date>2003</date>
<booktitle>In International Conference on Natural Language Processing and Knowledge Engineering,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="2406" citStr="Vogel (2003)" startWordPosition="375" endWordPosition="376">on system uses phrase pairs, the SOUL translation model uses tuples as described in the n-gram approach (Mari˜no et al., 2006). We describe the integration of the SOUL models into the translation system in Section 3.2. Section 4 summarizes the experimental results and compares two different tuning algorithms: Minimum Error Rate Training (Och, 2003) and k-best Batch Margin Infused Relaxed Algorithm (Cherry and Foster, 2012). 2 Baseline system The KIT translation system is an in-house implementation of the phrase-based approach and includes a pre-ordering step. This system is fully described in Vogel (2003). To train translation models, the provided Europarl, NC and Common Crawl parallel corpora are used. The target side of those parallel corpora, the News Shuffle corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is</context>
</contexts>
<marker>Vogel, 2003</marker>
<rawString>Stephan Vogel. 2003. SMT Decoder Dissected: Word Reordering. In International Conference on Natural Language Processing and Knowledge Engineering, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
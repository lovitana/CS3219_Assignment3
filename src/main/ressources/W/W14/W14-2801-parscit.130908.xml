<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000095">
<title confidence="0.99486">
Revisiting Word Neighborhoods for Speech Recognition
</title>
<author confidence="0.994988">
Preethi Jyothi* Karen Livescu
</author>
<affiliation confidence="0.997465">
Beckman Institute Toyota Technological Institute at Chicago
University of Illinois, Urbana, IL Chicago, IL
</affiliation>
<email confidence="0.997885">
pjyothi@illinois.edu klivescu@ttic.edu
</email>
<sectionHeader confidence="0.993864" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999860133333333">
Word neighborhoods have been suggested
but not thoroughly explored as an ex-
planatory variable for errors in automatic
speech recognition (ASR). We revisit the
definition of word neighborhoods, propose
new measures using a fine-grained artic-
ulatory representation of word pronuncia-
tions, and consider new neighbor weight-
ing functions. We analyze the signifi-
cance of our measures as predictors of er-
rors in an isolated-word ASR system and
a continuous-word ASR system. We find
that our measures are significantly better
predictors of ASR errors than previously
used neighborhood density measures.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999264909090909">
An important pursuit for both human and ma-
chine speech recognition research is to under-
stand the factors that affect word recognition ac-
curacy. In the substantial body of work on hu-
man word recognition, it has been shown that
it is harder to recognize words that have many
“similar” neighboring words than words with few
neighbors (Luce and Pisoni, 1998), and that fre-
quent words are recognized faster and more accu-
rately than are infrequent words (Marslen-Wilson,
1987; Luce and Pisoni, 1998; Vitevitch and Luce,
1999). In the ASR research community, prior
work has also investigated various factors that
benefit or disrupt recognition. Examples of such
factors include word frequency, speaking rate,
and prosodic factors (Fosler-Lussier and Morgan,
1999; Shinozaki and Furui, 2001; Hirschberg et
al., 2004; Goldwater et al., 2010). There has also
been prior work that uses word confusability mea-
sures to predict speech recognition errors (Fosler-
Lussier et al., 2005; Jyothi and Fosler-Lussier,
2009).
</bodyText>
<subsectionHeader confidence="0.417196">
∗Supported by a Beckman Postdoctoral Fellowship.
</subsectionHeader>
<bodyText confidence="0.999601636363637">
Word neighborhood measures have been stud-
ied more heavily for human word recognition than
as predictors of ASR errors. Although not stud-
ied specifically in prior work (Fosler-Lussier et al.,
2005; Jyothi and Fosler-Lussier, 2009), word con-
fusability measures used in predicting ASR errors
could be utilized to build word neighborhoods.
Goldwater et al. (2010) examine the behavior of
certain standard neighborhood density measures
as predictors of ASR errors. To our knowledge,
this is the only study that explicitly considers word
neighborhoods as a potential factor in ASR.
In this work, we investigate word neighborhood
measures as predictors of ASR errors. We pro-
pose new neighborhood measures that we find to
be more well-suited to ASR than standard neigh-
borhood density measures. We also propose a
new mechanism to incorporate frequency weight-
ing within the measures. Finally, we analyze the
measures as predictors of errors in an isolated-
word recognition system and a continuous-word
recognition system for conversational speech.
</bodyText>
<sectionHeader confidence="0.980628" genericHeader="introduction">
2 Related Work: Neighborhood Density
Measures
</sectionHeader>
<bodyText confidence="0.999985833333333">
In much of the prior work in the psycholinguistics
literature, the notion of word similarity is quanti-
fied by a simple one-phone-away rule: A word w&apos;
is a neighbor of word w if w and w&apos; differ by a sin-
gle phone, via a substitution, deletion, or insertion.
We refer to this density measure as “ND”.
</bodyText>
<equation confidence="0.98787">
ND = � AND(w, w&apos;)
W/
</equation>
<bodyText confidence="0.999520857142857">
where AND(w, w&apos;) = 1 if w and w&apos; differ by a
phone and 0 otherwise.
The frequencies of the neighbors are often ac-
counted for in the neighborhood density measure
by computing the sum of the raw (or log) frequen-
cies of a word’s neighbors (Luce and Pisoni, 1998;
Vitevitch and Luce, 1999); the word frequencies
</bodyText>
<page confidence="0.82362">
1
</page>
<note confidence="0.7891725">
Proceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 1–9,
Baltimore, Maryland USA, June 27 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.994676">
are derived from a large corpus. We refer to this
frequency-weighted measure as “wND”.
</bodyText>
<equation confidence="0.9814765">
�wND = DND(w, w0) · π(w0)
w,
</equation>
<bodyText confidence="0.999528428571429">
where π(w0) is the frequency of the word w0.1
Both ND and wND are popular measures for word
neighborhoods that we consider to be our base-
lines; Goldwater et al. (2010) also make use of
these two density measures.2
Neither of these measures account for the fre-
quency of the word itself. In continuous ASR,
which uses a language model, frequent words are
more likely to be recognized correctly (Fosler-
Lussier and Morgan, 1999). To account for this,
instead of using absolute frequencies of the neigh-
boring words, we use their relative frequencies to
define a third baseline density measure,“rwND”
(relative-wND):
</bodyText>
<equation confidence="0.99787">
DND(w,w0) · π(w0)
π(w)
</equation>
<bodyText confidence="0.994935">
Relative frequencies have appeared in prior
work (Luce, 1986; Luce and Pisoni, 1998; Scar-
borough, 2012). In fact, the measure used by Scar-
borough (2012) is the reciprocal of rwND.
</bodyText>
<sectionHeader confidence="0.975034" genericHeader="method">
3 Proposed Neighborhood Measures
</sectionHeader>
<bodyText confidence="0.999966571428571">
Our new neighborhood measures are defined in
terms of a distance function between a pair of
words, D, and a weighting function, β. The pro-
posed measures are not densities in the same sense
as ND, wND, rwND, but are scores that we may
expect to correlate with recognition errors. We de-
fine the neighborhood score for a word w as:
</bodyText>
<equation confidence="0.990644">
�score(w) = β(w, w0) · D(w, w0) (1)
w,6=w
</equation>
<bodyText confidence="0.997643">
Intuitively, β is an averaging function that weighs
the importance of each neighboring word. For ex-
ample, Yarkoni et al. (2008) use a neighborhood
measure that gives equal importance to the top
</bodyText>
<footnote confidence="0.995046454545454">
1Here we use raw rather than log frequencies. The base-
line density measures in this section perform better with raw
rather than log frequencies on our evaluation data. Our pro-
posed measures perform significantly better than the baseline
measures using both raw and log frequencies.
2Goldwater et al. (2010) also consider the number of ho-
mophones (words that share a pronunciation with the tar-
get word) and frequency-weighted homophones as additional
neighborhood measures. In our data there is insufficient ho-
mophony for these measures to be significant, so we do not
report on experiments using them.
</footnote>
<bodyText confidence="0.9567715">
20 closest neighbors and rejects the others. The
rest of the section presents multiple choices for D
and β which will define our various neighborhood
measures via Equation 1.
</bodyText>
<subsectionHeader confidence="0.999396">
3.1 Distance Functions
</subsectionHeader>
<bodyText confidence="0.9997884">
All of our distance functions are based on an edit
distance between a pair of words, i.e., the mini-
mum cost incurred in converting one word to the
other using substitutions, insertions and deletions
of the sub-word units in the word. In addition
to binary edit costs, we consider edit costs that
depend on sub-phonetic properties of the phones
rather than a uniform cost across all phones. Sec-
ond, instead of a single pronunciation for a word,
we consider a distribution over multiple pronun-
ciations. These distance functions can be easily
computed via finite-state transducer (FST) opera-
tions, as explained below (see also Figure 1).
Edit Distance (DED): This is the simplest edit
distance function that incurs an equal cost of 1 for
any substitution, insertion, or deletion. To com-
pute the distance between a pair of words, each
word w is represented as a finite state acceptor,
Fw, that accepts the pronunciations (phone se-
quences) of the word. We also introduce a memo-
ryless transducer, T, that maps an input phone to
any output phone, with arc weights equal to the
corresponding substitution costs (mapping to or
from epsilon indicates a deletion or an insertion).
The weight of the shortest path in the composed
FST, Fw o T o Fw,, gives the edit distance between
w and w0. When either w or w0 has more than
one pronunciation, DED is the minimum edit dis-
tance among all pairs of pronunciations. This edit
distance function has been previously proposed
as a measure of phonological similarity between
words (Hahn and Bailey, 2005). Similar distance
functions have also been used for neighborhood
density measures in visual word recognition stud-
ies (Yarkoni et al., 2008).
</bodyText>
<subsectionHeader confidence="0.850997">
Simple Articulatory Feature-based Edit Dis-
</subsectionHeader>
<bodyText confidence="0.989607333333333">
tance (DAF): The distance function DED pe-
nalizes an incorrect substitution equally regardless
of the phone identity; for example, the phone [p]
can be substituted with [b] or [aa] with equal cost
according to DED, although we know it is more
likely for [p] to be produced as [b] than as [aa]. To
account for this, we adopt a finer-grained repre-
sentation of the phone as a vector of discrete artic-
ulatory “features”. Our features are derived from
</bodyText>
<equation confidence="0.99379">
�rwND =
w,
2
k:e/3.364
m:p/3.464
k:k/0
k ah m k aa p
DAF : 0 1 2 3 0 0 0 0 1 2 3
</equation>
<figureCaption confidence="0.557001">
Figure 1: Distance functions implemented using finite-state machines.
</figureCaption>
<equation confidence="0.936484090909091">
k
kcl
gcl
DAFx : 0 1
k:e/3.364
m:p/3.464
k:k/0
k
kcl
4 5 0 0 0 0 gcl 1
m
</equation>
<page confidence="0.73331">
2
</page>
<figure confidence="0.9943049">
ah
ah
ax n
em
ax n
2
aa
pcl
ah
pcl
4 p 5
g
ax
g
ao
kcl
3
ah n
3
ah n
</figure>
<bodyText confidence="0.978300393442623">
the vocal tract variables of articulatory phonol-
ogy (Browman and Goldstein, 1992), including
the constriction degrees and locations of the lips,
tongue tip, tongue body, velum and glottis.We bor-
row a particular feature set from (Livescu, 2005).3
The substitution cost between two phones is de-
fined as the L1 distance between the articulatory
vectors corresponding to the phones. We set the
insertion and deletion costs to the mean substitu-
tion cost between the articulatory vectors for all
phone pairs. These new costs will appear as the arc
weights on the edit transducer T. This is shown
in Figure 1; apart from the difference in the arc
weights on T, DAF is the same as DED.
Extended Articulatory Feature-based Edit Dis-
tance (DAFx): The words in our dictionary are
associated with one or more canonical pronuncia-
tions written as sequences of phones. The distance
functions DED and DAF make use of this small set
of canonical pronunciations and do not capture the
various other ways in which a word can be pro-
nounced. An alternative, explored in some prior
work on pronunciation modeling (Deng and Sun,
1994; Richardson et al., 2003; Livescu and Glass,
2004; Mitra et al., 2011; Jyothi et al., 2011), is
to model the pronunciation of a word as multiple,
possibly asynchronous streams of fine-grained ar-
ticulatory features, again inspired by articulatory
phonology. Such a model can be implemented as
a dynamic Bayesian network (DBN) with multi-
ple variables representing the articulatory features
3The mapping of phones to their articulatory feature val-
ues is defined in Appendix B of Livescu (2005). This map-
ping includes a probability distribution over feature values
for certain phones; in these cases, we choose the articulatory
feature value with the highest probability.
in each time frame; please refer to (Livescu and
Glass, 2004; Livescu, 2005; Jyothi et al., 2011)
for more details. In this approach, deviations from
a dictionary pronunciation are the result of either
asynchrony between the articulatory streams (ac-
counting for effects such as nasalization, round-
ing, and epenthetic stops) or the substitution of one
articulatory feature value for another (accounting
for many reduction phenomena).
Jyothi et al. (2012) describe an approach to
encode such a DBN model of pronunciation as
an FST that outputs an articulatory feature tu-
ple for each frame of speech. We modify this
FST by mapping each articulatory feature tuple
to a valid phone as per the phone-to-articulatory-
feature mapping used for DAF (discarding arcs
whose labels do not correspond to a valid phone).
The resulting FSTs are used to define DAFx by
composing with the edit transducer T as in the
definition of DAF. For computational efficiency,
we prune these FSTs to retain only paths that are
within three times the weight of the shortest path.
The pruned FSTs have hundreds of arcs and ∼50
states on average. A schematic diagram is used to
illustrate the computation of DAFx in Figure 1.
</bodyText>
<subsectionHeader confidence="0.999916">
3.2 Weighting Functions
</subsectionHeader>
<bodyText confidence="0.994243444444444">
Our weighting functions can be appropriately de-
fined to discount the contributions of words that
are infrequent or are very far away. We note here
that unlike the density measures in Section 2, the
lower the distance-based score for a word (from
Equation 1), the more confusable it would be with
its neighbors. One approach, as pursued in Nosof-
sky (1986) and Bailey and Hahn (2001), is to use
score(w) = E,,,, g(D(w, w&apos;)) where g is an expo-
</bodyText>
<page confidence="0.947307">
3
</page>
<figure confidence="0.796707">
r1 r2
</figure>
<figureCaption confidence="0.8488864">
Figure 2: Let w1 and w2 be the two closest
words to w. The area of the shaded region shows
β(w, w2) where ri = Rw(wi) = i. In the
weighted case given in Equation 4, r1 = Rηw(w1),
r2 = Rηw(w2) and r2 − r1 = ηw(w2).
</figureCaption>
<bodyText confidence="0.989808066666667">
nentially decreasing function. This, however, has
the disadvantage of being very sensitive to the dis-
tance measure used: Slight changes in the distance
can alter the score significantly, even if the overall
ordering of the distances is preserved. We propose
an alternative approach that keeps the score as a
linear function of the distances as long as the or-
dering is fixed. For this, we introduce β(w, w0) in
Equation 1 and let it be a (possibly exponentially)
decreasing function of the rank of w0.
Formally, we define the rank of w0 with re-
spect to w, Rw(w0), as follows: Fix an ordering
of all N − 1 words in the vocabulary other than
w as (w1, w2, ... , wN−1) such that Δ(w, wi) &lt;
Δ(w, wi+1) for all i E {1, ... , N − 21. Then
</bodyText>
<equation confidence="0.788226">
Rw(w0) = j if w0 = wj in the above ordering.
</equation>
<bodyText confidence="0.639045">
We then define β in terms of a “decay” function σ:
</bodyText>
<equation confidence="0.99809475">
Rw (w0 )
β(w, w0) = IR
σ(r)dr (2)
w(w0)−1
</equation>
<bodyText confidence="0.986821705882353">
If σ is monotonically decreasing, Equation 2 en-
sures that neighbors with a higher rank (i.e., fur-
ther away) contribute less weight than neighbors
with a lower rank. For example, a measure
that gives equal weight to the k closest neigh-
bors (Yarkoni et al., 2008) corresponds to
Instead of a step function that gives equal weight
to all k neighbors, we define σ as an exponen-
tially decreasing function of rank: σ(r) = e−r.
Then, from Equation 2, we obtain β(w, w0) =
(e−1)e−Rw(w0). Figure 2 shows the exponentially
decreasing σ(r) and a sample β(w, w0).
We know from prior work that it is also impor-
tant to distinguish among the neighbors depending
on how frequently they appear in the language. To
account for this, we define a frequency-weighted
rank function, Rηw(w0):
</bodyText>
<equation confidence="0.992662">
ηw(wi) (3)
</equation>
<bodyText confidence="0.904986">
where ηw is a suitably defined frequency function
(see below). We now redefine β as:
</bodyText>
<equation confidence="0.9956795">
β(w, w0) =JR7W7(w0)−ηw(w0)
Rw η (w0) σ(r)dr (4)
</equation>
<bodyText confidence="0.999784666666667">
Note that when ηw(w0) = 1 for all w0, Equation 4
reduces to Equation 2. β(w, w0) is robust in that
it is invariant to the ordering used to define rank,
Rηw, i.e. words with the same distance from w can
be arbitrarily ordered. Also, multiple words at the
same distance contribute to β equally to a single
word at the same distance with a frequency that is
the sum of their frequencies.
We use three choices for ηw(w0):
</bodyText>
<listItem confidence="0.981158642857143">
1. The first choice is simply ηw(w0) = 1 for all
w0.
2. Let π(w0) be the unigram probability of w0. We
then define ηw(w0) = P · π(w0) where P is
a scaling parameter. One natural choice for
P is the perplexity of the unigram probability
distribution, π, i.e., 2− Ew π(w) log(π(w)). With
this choice of P, when π is a uniform distribu-
tion over all words in the vocabulary, we have
ηw(w0) = 1 for all w0, and Rηw(w0) = Rw(w0).
3. As defined above, ηw(w0) does not depend on
w. Our third choice for the frequency func-
tion considers the frequency of w0 relative to
w: ηw(w0) = π(w0)/π(w)
</listItem>
<bodyText confidence="0.998889454545455">
To summarize, Equation 1 gives the neighbor-
hood score for w in terms of β and Δ. We use
three choices for Δ as specified in Section 3. β
is defined by Equation 4 where Rηw is defined
by Equation 3 in terms of the frequency function
ηw. We use the three choices described above for
ηw. The resulting nine score functions are sum-
marized in Table 1. For completeness, we also
include the neighborhood density baseline mea-
sures and represent them using our notation with
a distance function defined as ΔND(w, w0) =
</bodyText>
<figure confidence="0.997621916666667">
1
σ(r) 0.8
0.6
0.4
0.2
0
�1 ifr&lt;k
σ(r) =
0 otherwise
Rηw(w0) = Rw(w0)
�
i=1
</figure>
<page confidence="0.936016">
4
</page>
<table confidence="0.999788045454546">
Measure σ(r) Δ(w, w0) ηw(w0)
ND 1 ΔND 1
wND π(w0)
rwND π(w�)
π(w)
ED e−r ΔED 1
wED π(w0) • P
rwED π(w�)
AF
wAF
rwAF
AFx
wAFx
rwAFx
π(w)
ΔAF 1
π(w0) • P
π(w�)
ΔAFx 1
π(w0) • P
π(w�)
π(w)
</table>
<tableCaption confidence="0.999565">
Table 1: Summary of neighborhood measures.
</tableCaption>
<equation confidence="0.809767666666667">
1(ΔED(w, w0) = 1) (i.e. ΔND(w, w0) = 1 if
ΔED(w, w0) = 1 and 0 otherwise) and σ = 1.
With σ = 1 and β(w, w0) = ηw(w0), the three
</equation>
<bodyText confidence="0.99961725">
choices of ηw give us ND, wND and rwND, as
shown in Table 1. The notation ΔND(w, w0) is
to highlight the inverse relationship of the density
measures with our distance-based measures.
</bodyText>
<sectionHeader confidence="0.999585" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999732142857143">
We provide an individual analysis of each neigh-
borhood measure as it relates to recognition error
rate. We also present a matrix of pairwise com-
parisons among all of the neighborhood measures
with respect to their ability to predict recognition
errors. We study the relationship between neigh-
borhood measures and ASR errors in two settings:
</bodyText>
<listItem confidence="0.930687789473684">
• Isolated-word ASR: Psycholinguistic stud-
ies typically use isolated words as stimuli to study
the influence of neighborhood measures on recog-
nition (e.g., see Goldwater et al. (2010) and ref-
erences therein). Motivated by this, we build an
ASR system that recognizes words in isolation
and analyze the relationship between its errors and
each neighborhood measure. Further details of
this analysis are described in Section 4.1.
• Continuous-word ASR: ASR systems typ-
ically deal with continuous speech. However,
the usefulness of neighborhood measures for
continuous-word ASR has received little atten-
tion, with the notable exception of Goldwater et
al. (2010). We further this line of investigation in
our second set of experiments by analyzing the re-
lationship between errors made by a continuous-
word ASR system and our new measures. These
are described in more detail in Section 4.2.
</listItem>
<subsectionHeader confidence="0.868733">
4.1 Isolated-Word ASR
</subsectionHeader>
<bodyText confidence="0.99833262745098">
Experimental Setup: We extract isolated words
from a subset of the Switchboard-I conversational
speech corpus (Godfrey et al., 1992) called the
Switchboard Transcription Project, STP (Green-
berg et al., 1996; STP, 1996), which is phonet-
ically labeled at a fine-grained level. Isolated
words were excised from continuous utterances in
sets 20–22 in the STP corpus. We use a total of
401 word tokens (247 unique words) derived from
the 3500 most frequent words in Switchboard-I,
excluding non-speech events and partial words.
These words make up the development and eval-
uation sets used in prior related work on pronun-
ciation modeling (Livescu and Glass, 2004; Jyothi
et al., 2011; Jyothi et al., 2012). We use the dictio-
nary that accompanies the Switchboard-I corpus
consisting of 30,241 words; —98% of these words
are associated with a single pronunciation.
The recognition system for this isolated word
dataset was built using the Kaldi toolkit (Povey
et al., 2011; Kal, 2011). We use an acous-
tic model that is trained on all of Switchboard-
I, excluding the sentences from which our 401-
word set was drawn. The ASR system uses stan-
dard mel frequency cepstral coefficients with their
first and second derivatives (deltas and double-
deltas) as acoustic features, with standard normal-
ization and adaptation techniques including cep-
stral mean and variance normalization and maxi-
mum likelihood linear regression. Linear discrim-
inant analysis (LDA) and maximum likelihood lin-
ear transform (MLLT) feature-space transforma-
tions were applied to reduce the feature-space di-
mensionality (Povey et al., 2011). The acous-
tic models are standard Gaussian mixture model-
Hidden Markov models (GMM-HMMs) for tied-
state triphones. The recognition vocabulary in-
cludes 3328 words, consisting of the 3500 most
frequent words from Switchboard excluding par-
tial and non-speech words.4 Since this is an
isolated-word task, the ASR system does not use
any language model.
Results and Discussion: In order to individu-
ally analyze each of the neighborhood measures,
4Large-vocabulary automatic recognition of isolated
words is a hard task due to the absence of constraints from
a language model. Using the entire Switchboard vocabulary
would greatly deteriorate the recognition performance on an
already hard task. Thus, we restrict the vocabulary to 1/10th
of the original size in order to obtain reasonable performance
from the isolated ASR system.
</bodyText>
<page confidence="0.916398">
5
</page>
<figure confidence="0.991716954545455">
ER
0.0 0.2 0.4 0.6 0.8 1.0 ER
0 10 20 30 40 50 60
0.0 0.2 0.4 0.6 0.8 1.0 ER
0.000 0.010 0.020
0.0 0.2 0.4 0.6 0.8 1.0
5 10 15
ND wND wAFx
(a) Neighborhood measures ND, wND and wAFx as predictors of isolated-word error rate (ER).
ND ED AF AFx wND wED wAF wAFx
ND
ED
AF
AFx
wND
wED
wAF
wAFx
null 5−6 8−5 5−7 1−7 8−8 2−8 3−10 6−11
0 0.0001 0.001 0.01 0.05 0.1 1
(b) Pairwise comparison of word neighborhood measures as predictors of errors from the isolated-word ASR system using
p-values. Many low p-values (darker cells) along a column implies the corresponding measure is a significant predictor of ER.
</figure>
<figureCaption confidence="0.999989">
Figure 3: Analysis of neighborhood measures with isolated word ASR.
</figureCaption>
<bodyText confidence="0.999978613636364">
following Goldwater et al. (2010), we use a logis-
tic regression model implemented using the glm
function in R (R Development Core Team, 2005).
The logistic regression model fits the log-odds of
a binary response variable with a linear combina-
tion of one or more predictor variables. For our
isolated-word task, the response variable takes a
value of either 1 or 0 corresponding to the pres-
ence or absence of an error, respectively; we will
refer to it as “ER”. We build a separate logis-
tic regression model for each neighborhood mea-
sure acting as the only predictor of ER. We use
restricted cubic splines, using the res (Harrell Jr.,
2012) function in R, to model non-linear predic-
tive relationships. In order to determine whether
a neighborhood measure is a significant predictor
of ER, we use a likelihood-ratio test (using the
anova function in R) that compares the fit of the
model including only that neighborhood measure
as a predictor against the fit of a baseline model in-
cluding only an intercept and no other predictors.
All of the neighborhood measures were found to
be significant predictors, with our measures wAF
and wAFx being most significant. The p-values
from this test are shown in a separate row under
the header “null” in Figure 3(b); here, 5−6 stands
for 5 × 10−6 and so forth. We note that the neigh-
borhood measures are significantly correlated with
ER as individual predictors, but classifiers built
with each individual measure as the only feature
are not good predictors of ASR errors. This is
unsurprising as we expect many other predictors
other than neighborhood measures, as outlined in
Goldwater et al. (2010), to influence ASR errors.
This paper focuses only on analyzing each neigh-
borhood measure as an individual predictor; joint
models will be explored as part of future work.
Figure 3(a) shows the relationship between er-
rors from the isolated ASR system and three
neighborhood measures: the best-performing
measure (wAFx) and the two standard density
measures (ND, wND). The feature values are ag-
gregated into roughly equal-sized bins and the
average error rate for each bin is plotted. The
</bodyText>
<page confidence="0.981792">
6
</page>
<figure confidence="0.9887911">
IWER
IWER
0.0 0.2 0.4 0.6
0.0 0.2 0.4 0.6
2 4 6 8 10
0.000 0.005 0.010 0.015 0.020
0.0 0.2 0.4 0.6 IWER
0 10 20 30 40
wND rwND rwAFx
(a) Neighborhood measures wND, rwND and rwAFx as predictors of IWER.
</figure>
<table confidence="0.943142714285715">
ND ED AF AFx wND wED wAF wAFx rwND rwED rwAF rwAFx
ND
ED
AF
AFx
wND
wED
wAF
wAFx
rwND
rwED
rwAF
rwAFx
null 0.09 0.72 0.08 0.04 0.18 0.14 0.002 0.03 0.001 0.02 2−5 2−5
</table>
<figure confidence="0.8654855">
0 0.0001 0.001 0.01 0.05 0.1 1
(b) Pairwise comparison of all word neighborhood measures as predictors of IWER from the continuous-word ASR system.
</figure>
<figureCaption confidence="0.999948">
Figure 4: Analysis of neighborhood measures with continuous-word ASR system.
</figureCaption>
<bodyText confidence="0.999990727272727">
solid line shows the probability of an error from
the corresponding logistic regression model and
the dashed lines show a 95% confidence interval.
The dotted line is the average error rate from the
entire data set of 401 words, 0.483. The plots
clearly show the inverse relationship between our
distance-based measure (wAFx) and the density
measures (ND and wND). The slope of the fitted
probabilities from the logistic regression model
for a measure is indicative of the usefulness of the
measure in predicting ER. All of the measures are
significant predictors having non-zero slope with a
slightly larger slope for wAFx than ND and wND.
ND and wND being significant predictors of errors
for isolated words is consistent with prior stud-
ies from human speech recognition. The proposed
measures, wAF and wAFx, stand out as the best
predictors of errors. We next analyze the differ-
ences between the measures more closely.
Figure 3(b) shows a pairwise comparison of the
word neighborhood measures. Each cell {i, j}
shows a p-value range from a likelihood-ratio test
that compares the fit of a logistic regression model
using only measure i as a predictor with the fit of a
model using both measures i and j as independent
predictors. Lower p-values (darker cells) indicate
that adding the measure in column j significantly
improves the ability of the model to predict ER, as
opposed to only using the measure along row i.5
We use such nested models to compare the model
fits using likelihood-ratio significance tests. It is
clear from Figure 3(b) that our measures wAF and
wAFx are the most significant predictors.
</bodyText>
<footnote confidence="0.8833168">
5The relative frequency-weighted measures (rwND,
rwED, rwAF, rwAFx) were omitted since (wND, wED, wAF,
wAFx) are significantly better predictors. This could be be-
cause the isolated-word system has no language model and is
thus unaffected by the target word frequency.
</footnote>
<page confidence="0.998189">
7
</page>
<subsectionHeader confidence="0.63777">
4.2 Continuous-word ASR
</subsectionHeader>
<bodyText confidence="0.994095315068493">
Experimental Setup: For the continuous-word
task, our evaluation data consists of full sentences
from Switchboard-I that were used to extract the
isolated words in Section 4.1. For our analysis, we
include all the words in the evaluation sentences
that are 3 or more phonemes long and occur 100
times or more in the training set. This gives us a
total of 1223 word tokens (459 word types).
The continuous-word ASR system uses an
acoustic model trained on all of Switchboard-
I excluding the above-mentioned evaluation sen-
tences. The acoustic models are GMM-HMMs for
tied-state triphones using MFCC + delta + double-
delta features with LDA and MLLT feature-space
transformations and speaker adaptation. They are
also trained discriminatively using boosted maxi-
mum mutual information training from the Kaldi
toolkit. We use the entire Switchboard vocabu-
lary of 30,241 words and a 3-gram language model
trained on all of the training sentences. The word
error rate on the evaluation sentences is 28.3%.6
Results and Discussion: Unlike the isolated-
word task, the continuous-word ASR system gives
word error rates over full utterances. Since we
need to measure the errors associated with the in-
dividual words, we use the individual word er-
ror rate (IWER) metric proposed by Goldwater et
al. (2010). The IWER for word wi is a·ini+deli+
subi where ini is the number of insertions adja-
cent to wi; deli or subi is 1 if wi is either deleted
or substituted, respectively. a is chosen such that
a · Ei ini = I where I is the total number of inser-
tions for the entire dataset.
As in the isolated-word task, we fit logistic re-
gression models to analyze the neighborhood mea-
sures as predictors of IWER. Figure 4(a) shows fit-
ted probabilities from a logistic regression model
for IWER built individually using each of the mea-
sures wND, rwND and rwAFx as predictors. The
number of frequency-weighted neighbors, wND
(as well as the number of neighbors, ND), was
not found to be a significant predictor of IWER.
This is consistent with the findings in Goldwater
et al. (2010) that show weak correlations between
6The training set includes other utterances from the same
speakers in the STP evaluation utterances. This allows for
an additional boost in performance from the speaker adapted
acoustic models during recognition. Ideally, the training and
evaluation sets should not contain utterances from the same
speakers. We allow for this to get word error rates that are
more comparable to state-of-the-art results on this corpus.
the number of frequency-weighted neighbors and
the probability of misrecognizing a word. How-
ever, we find that using the number of frequency-
weighted neighbors relative to the frequency of
the word (rwND) improves the correlation with
the probability of error (seen in Figure 4(a) as an
increase in slope). Using our proposed distance
measures with relative frequency weighting im-
proves the correlation even further.
Figure 4(b) shows a pairwise comparison of all
measures in Table 1; the interpretation is sim-
ilar to Figure 3(b). We observe that the rela-
tive frequency-weighted measures (rwND, rwED,
rwAF, rwAFx) are consistently better than their
unweighted (ND, ED, AF, AFx) and frequency-
weighted (wND, wED, wAF, wAFx) counterparts,
with rwAF and rwAFx being most significant.
This suggests that the relative frequency-weighted
measures are taking precedence in the continuous-
word task as significant predictors of IWER (un-
like in the isolated-word task) due to the presence
of a strong language model.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999986178571429">
In this work, we propose new word neighborhood
measures using distances between words that em-
ploy a fine-grained articulatory feature-based rep-
resentation of the word. We present a new rank-
based averaging method to aggregate the word dis-
tances into a single neighborhood score. We also
suggest multiple ways of incorporating frequency
weighting into this score. We analyze the signifi-
cance of our word neighborhood measures as pre-
dictors of errors from an isolated-word ASR sys-
tem and a continuous-word ASR system. In both
cases, our measures perform significantly better
than standard neighborhood density measures.
This work reopens the question of whether word
neighborhood measures are a useful variable for
ASR. There are many possible directions for fu-
ture work. Our measures could be refined fur-
ther, for example by exploring alternative distance
measures, different articulatory feature sets, dif-
ferent choices of a and q in the weighting func-
tion, or automatically learned costs and distances.
Also, our analysis currently looks at each neigh-
borhood measure as an individual predictor; we
could jointly analyze the measures to account for
possible correlations. Finally, it may be possible
to use neighborhood measures in ASR confidence
scoring or even directly in recognition as an addi-
tional feature in a discriminative model.
</bodyText>
<page confidence="0.996975">
8
</page>
<sectionHeader confidence="0.995844" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999801858585858">
T. M. Bailey and U. Hahn. 2001. Determinants
of wordlikeness: Phonotactics or lexical neigh-
borhoods? Journal of Memory and Language,
44(4):568–591.
C. P. Browman and L. Goldstein. 1992. Articulatory
phonology: An overview. Phonetica, 49(3-4):155–
180.
L. Deng and D.X. Sun. 1994. A statistical approach
to automatic speech recognition using the atomic
speech units constructed from overlapping articula-
tory features. The Journal of the Acoustical Society
ofAmerica, 95(5):2702–2719.
E. Fosler-Lussier and N. Morgan. 1999. Effects of
speaking rate and word frequency on pronunciations
in conversational speech. Speech Communication,
29(2):137–158.
E. Fosler-Lussier, I. Amdal, and H-K. J. Kuo. 2005. A
framework for predicting speech recognition errors.
Speech Communication, 46(2):153–170.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In Proc. of ICASSP.
S. Goldwater, D. Jurafsky, and C. D. Manning. 2010.
Which words are hard to recognize? Prosodic,
lexical, and disfluency factors that increase speech
recognition error rates. Speech Communication,
52(3):181–200.
S. Greenberg, J. Hollenback, and D. Ellis. 1996. In-
sights into spoken language gleaned from phonetic
transcription of the Switchboard corpus. In Proc. of
ICSLP.
U. Hahn and T. M. Bailey. 2005. What makes words
sound similar? Cognition, 97(3):227–267.
F. E. Harrell Jr. 2012. RMS: Regression Modeling
Strategies. R package version 3.5-0.
J. Hirschberg, D. Litman, and M. Swerts. 2004.
Prosodic and other cues to speech recognition fail-
ures. Speech Communication, 43(1):155–175.
P. Jyothi and E. Fosler-Lussier. 2009. A comparison of
audio-free speech recognition error prediction meth-
ods. In Proc. of Interspeech.
P. Jyothi, K. Livescu, and E. Fosler-Lussier. 2011.
Lexical access experiments with context-dependent
articulatory feature-based models. In Proc. of
ICASSP.
P. Jyothi, E. Fosler-Lussier, and K. Livescu. 2012. Dis-
criminatively learning factorized finite state pronun-
ciation models from dynamic Bayesian networks. In
Proc. of Interspeech.
2011. Kaldi. http://kaldi.sourceforge.
net/.
K. Livescu and J. Glass. 2004. Feature-based pronun-
ciation modeling with trainable asynchrony proba-
bilities. In Proc. of ICSLP.
K. Livescu. 2005. Feature-based Pronunciation Mod-
eling for Automatic Speech Recognition. PhD Dis-
sertation, MIT EECS department.
P. A. Luce and D. B. Pisoni. 1998. Recognizing spo-
ken words: The neighborhood activation model. Ear
and hearing, 19:1–36.
P. A. Luce. 1986. Neighborhoods of words in the men-
tal lexicon. Research on Speech Perception, (Tech-
nical Report No. 6.).
W. D. Marslen-Wilson. 1987. Functional parallelism
in spoken word-recognition. Cognition, 25(1):71–
102.
V. Mitra, H. Nam, C. Y. Espy-Wilson, E. Saltzman,
and L. Goldstein. 2011. Articulatory information
for noise robust speech recognition. IEEE Transac-
tions on Audio, Speech, and Language Processing,
19(7):1913–1924.
R. M. Nosofsky. 1986. Attention, similarity, and the
identification–categorization relationship. Journal
of Experimental Psychology: General, 115(1):39.
D. Povey, A. Ghoshal, et al. 2011. The Kaldi speech
recognition toolkit. Proc. of ASRU.
R Development Core Team. 2005. R: A language and
environment for statistical computing. R foundation
for Statistical Computing.
M. Richardson, J. Bilmes, and C. Diorio. 2003.
Hidden-articulator Markov models for speech recog-
nition. Speech Communication, 41(2-3):511–529.
R. A. Scarborough. 2012. Lexical confusability and
degree of coarticulation. In Proceedings of the An-
nual Meeting of the Berkeley Linguistics Society.
T. Shinozaki and S. Furui. 2001. Error analysis using
decision trees in spontaneous presentation speech
recognition. In Proc. of ASRU.
1996. The Switchboard Transcription Project.
http://www1.icsi.berkeley.edu/
Speech/stp/.
M. S. Vitevitch and P. A. Luce. 1999. Probabilis-
tic phonotactics and neighborhood activation in spo-
ken word recognition. Journal of Memory and Lan-
guage, 40(3):374–408.
T. Yarkoni, D. Balota, and M. Yap. 2008. Mov-
ing beyond Coltheart’s N: A new measure of ortho-
graphic similarity. Psychonomic Bulletin &amp; Review,
15(5):971–979.
</reference>
<page confidence="0.997027">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.780666">
<title confidence="0.998393">Revisiting Word Neighborhoods for Speech Recognition</title>
<author confidence="0.800335">Livescu</author>
<affiliation confidence="0.9906305">Beckman Institute Toyota Technological Institute at Chicago University of Illinois, Urbana, IL Chicago, IL</affiliation>
<email confidence="0.998205">pjyothi@illinois.eduklivescu@ttic.edu</email>
<abstract confidence="0.9995339375">Word neighborhoods have been suggested but not thoroughly explored as an explanatory variable for errors in automatic speech recognition (ASR). We revisit the definition of word neighborhoods, propose new measures using a fine-grained articulatory representation of word pronunciations, and consider new neighbor weighting functions. We analyze the significance of our measures as predictors of errors in an isolated-word ASR system and a continuous-word ASR system. We find that our measures are significantly better predictors of ASR errors than previously used neighborhood density measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T M Bailey</author>
<author>U Hahn</author>
</authors>
<title>Determinants of wordlikeness: Phonotactics or lexical neighborhoods?</title>
<date>2001</date>
<journal>Journal of Memory and Language,</journal>
<volume>44</volume>
<issue>4</issue>
<contexts>
<context position="11938" citStr="Bailey and Hahn (2001)" startWordPosition="1979" endWordPosition="1982"> only paths that are within three times the weight of the shortest path. The pruned FSTs have hundreds of arcs and ∼50 states on average. A schematic diagram is used to illustrate the computation of DAFx in Figure 1. 3.2 Weighting Functions Our weighting functions can be appropriately defined to discount the contributions of words that are infrequent or are very far away. We note here that unlike the density measures in Section 2, the lower the distance-based score for a word (from Equation 1), the more confusable it would be with its neighbors. One approach, as pursued in Nosofsky (1986) and Bailey and Hahn (2001), is to use score(w) = E,,,, g(D(w, w&apos;)) where g is an expo3 r1 r2 Figure 2: Let w1 and w2 be the two closest words to w. The area of the shaded region shows β(w, w2) where ri = Rw(wi) = i. In the weighted case given in Equation 4, r1 = Rηw(w1), r2 = Rηw(w2) and r2 − r1 = ηw(w2). nentially decreasing function. This, however, has the disadvantage of being very sensitive to the distance measure used: Slight changes in the distance can alter the score significantly, even if the overall ordering of the distances is preserved. We propose an alternative approach that keeps the score as a linear func</context>
</contexts>
<marker>Bailey, Hahn, 2001</marker>
<rawString>T. M. Bailey and U. Hahn. 2001. Determinants of wordlikeness: Phonotactics or lexical neighborhoods? Journal of Memory and Language, 44(4):568–591.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Browman</author>
<author>L Goldstein</author>
</authors>
<title>Articulatory phonology: An overview.</title>
<date>1992</date>
<journal>Phonetica,</journal>
<pages>49--3</pages>
<contexts>
<context position="8643" citStr="Browman and Goldstein, 1992" startWordPosition="1435" endWordPosition="1438">t according to DED, although we know it is more likely for [p] to be produced as [b] than as [aa]. To account for this, we adopt a finer-grained representation of the phone as a vector of discrete articulatory “features”. Our features are derived from �rwND = w, 2 k:e/3.364 m:p/3.464 k:k/0 k ah m k aa p DAF : 0 1 2 3 0 0 0 0 1 2 3 Figure 1: Distance functions implemented using finite-state machines. k kcl gcl DAFx : 0 1 k:e/3.364 m:p/3.464 k:k/0 k kcl 4 5 0 0 0 0 gcl 1 m 2 ah ah ax n em ax n 2 aa pcl ah pcl 4 p 5 g ax g ao kcl 3 ah n 3 ah n the vocal tract variables of articulatory phonology (Browman and Goldstein, 1992), including the constriction degrees and locations of the lips, tongue tip, tongue body, velum and glottis.We borrow a particular feature set from (Livescu, 2005).3 The substitution cost between two phones is defined as the L1 distance between the articulatory vectors corresponding to the phones. We set the insertion and deletion costs to the mean substitution cost between the articulatory vectors for all phone pairs. These new costs will appear as the arc weights on the edit transducer T. This is shown in Figure 1; apart from the difference in the arc weights on T, DAF is the same as DED. Ext</context>
</contexts>
<marker>Browman, Goldstein, 1992</marker>
<rawString>C. P. Browman and L. Goldstein. 1992. Articulatory phonology: An overview. Phonetica, 49(3-4):155– 180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Deng</author>
<author>D X Sun</author>
</authors>
<title>A statistical approach to automatic speech recognition using the atomic speech units constructed from overlapping articulatory features.</title>
<date>1994</date>
<journal>The Journal of the Acoustical Society ofAmerica,</journal>
<volume>95</volume>
<issue>5</issue>
<contexts>
<context position="9671" citStr="Deng and Sun, 1994" startWordPosition="1609" endWordPosition="1612">hone pairs. These new costs will appear as the arc weights on the edit transducer T. This is shown in Figure 1; apart from the difference in the arc weights on T, DAF is the same as DED. Extended Articulatory Feature-based Edit Distance (DAFx): The words in our dictionary are associated with one or more canonical pronunciations written as sequences of phones. The distance functions DED and DAF make use of this small set of canonical pronunciations and do not capture the various other ways in which a word can be pronounced. An alternative, explored in some prior work on pronunciation modeling (Deng and Sun, 1994; Richardson et al., 2003; Livescu and Glass, 2004; Mitra et al., 2011; Jyothi et al., 2011), is to model the pronunciation of a word as multiple, possibly asynchronous streams of fine-grained articulatory features, again inspired by articulatory phonology. Such a model can be implemented as a dynamic Bayesian network (DBN) with multiple variables representing the articulatory features 3The mapping of phones to their articulatory feature values is defined in Appendix B of Livescu (2005). This mapping includes a probability distribution over feature values for certain phones; in these cases, we</context>
</contexts>
<marker>Deng, Sun, 1994</marker>
<rawString>L. Deng and D.X. Sun. 1994. A statistical approach to automatic speech recognition using the atomic speech units constructed from overlapping articulatory features. The Journal of the Acoustical Society ofAmerica, 95(5):2702–2719.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Fosler-Lussier</author>
<author>N Morgan</author>
</authors>
<title>Effects of speaking rate and word frequency on pronunciations in conversational speech.</title>
<date>1999</date>
<journal>Speech Communication,</journal>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="1603" citStr="Fosler-Lussier and Morgan, 1999" startWordPosition="234" endWordPosition="237">cognition accuracy. In the substantial body of work on human word recognition, it has been shown that it is harder to recognize words that have many “similar” neighboring words than words with few neighbors (Luce and Pisoni, 1998), and that frequent words are recognized faster and more accurately than are infrequent words (Marslen-Wilson, 1987; Luce and Pisoni, 1998; Vitevitch and Luce, 1999). In the ASR research community, prior work has also investigated various factors that benefit or disrupt recognition. Examples of such factors include word frequency, speaking rate, and prosodic factors (Fosler-Lussier and Morgan, 1999; Shinozaki and Furui, 2001; Hirschberg et al., 2004; Goldwater et al., 2010). There has also been prior work that uses word confusability measures to predict speech recognition errors (FoslerLussier et al., 2005; Jyothi and Fosler-Lussier, 2009). ∗Supported by a Beckman Postdoctoral Fellowship. Word neighborhood measures have been studied more heavily for human word recognition than as predictors of ASR errors. Although not studied specifically in prior work (Fosler-Lussier et al., 2005; Jyothi and Fosler-Lussier, 2009), word confusability measures used in predicting ASR errors could be utili</context>
</contexts>
<marker>Fosler-Lussier, Morgan, 1999</marker>
<rawString>E. Fosler-Lussier and N. Morgan. 1999. Effects of speaking rate and word frequency on pronunciations in conversational speech. Speech Communication, 29(2):137–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Fosler-Lussier</author>
<author>I Amdal</author>
<author>H-K J Kuo</author>
</authors>
<title>A framework for predicting speech recognition errors.</title>
<date>2005</date>
<journal>Speech Communication,</journal>
<volume>46</volume>
<issue>2</issue>
<contexts>
<context position="2095" citStr="Fosler-Lussier et al., 2005" startWordPosition="309" endWordPosition="312">rupt recognition. Examples of such factors include word frequency, speaking rate, and prosodic factors (Fosler-Lussier and Morgan, 1999; Shinozaki and Furui, 2001; Hirschberg et al., 2004; Goldwater et al., 2010). There has also been prior work that uses word confusability measures to predict speech recognition errors (FoslerLussier et al., 2005; Jyothi and Fosler-Lussier, 2009). ∗Supported by a Beckman Postdoctoral Fellowship. Word neighborhood measures have been studied more heavily for human word recognition than as predictors of ASR errors. Although not studied specifically in prior work (Fosler-Lussier et al., 2005; Jyothi and Fosler-Lussier, 2009), word confusability measures used in predicting ASR errors could be utilized to build word neighborhoods. Goldwater et al. (2010) examine the behavior of certain standard neighborhood density measures as predictors of ASR errors. To our knowledge, this is the only study that explicitly considers word neighborhoods as a potential factor in ASR. In this work, we investigate word neighborhood measures as predictors of ASR errors. We propose new neighborhood measures that we find to be more well-suited to ASR than standard neighborhood density measures. We also p</context>
</contexts>
<marker>Fosler-Lussier, Amdal, Kuo, 2005</marker>
<rawString>E. Fosler-Lussier, I. Amdal, and H-K. J. Kuo. 2005. A framework for predicting speech recognition errors. Speech Communication, 46(2):153–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Godfrey</author>
<author>E C Holliman</author>
<author>J McDaniel</author>
</authors>
<title>SWITCHBOARD: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="17530" citStr="Godfrey et al., 1992" startWordPosition="3009" endWordPosition="3012">ion 4.1. • Continuous-word ASR: ASR systems typically deal with continuous speech. However, the usefulness of neighborhood measures for continuous-word ASR has received little attention, with the notable exception of Goldwater et al. (2010). We further this line of investigation in our second set of experiments by analyzing the relationship between errors made by a continuousword ASR system and our new measures. These are described in more detail in Section 4.2. 4.1 Isolated-Word ASR Experimental Setup: We extract isolated words from a subset of the Switchboard-I conversational speech corpus (Godfrey et al., 1992) called the Switchboard Transcription Project, STP (Greenberg et al., 1996; STP, 1996), which is phonetically labeled at a fine-grained level. Isolated words were excised from continuous utterances in sets 20–22 in the STP corpus. We use a total of 401 word tokens (247 unique words) derived from the 3500 most frequent words in Switchboard-I, excluding non-speech events and partial words. These words make up the development and evaluation sets used in prior related work on pronunciation modeling (Livescu and Glass, 2004; Jyothi et al., 2011; Jyothi et al., 2012). We use the dictionary that acco</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992. SWITCHBOARD: Telephone speech corpus for research and development. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Which words are hard to recognize? Prosodic, lexical, and disfluency factors that increase speech recognition error rates.</title>
<date>2010</date>
<journal>Speech Communication,</journal>
<volume>52</volume>
<issue>3</issue>
<contexts>
<context position="1680" citStr="Goldwater et al., 2010" startWordPosition="246" endWordPosition="249"> been shown that it is harder to recognize words that have many “similar” neighboring words than words with few neighbors (Luce and Pisoni, 1998), and that frequent words are recognized faster and more accurately than are infrequent words (Marslen-Wilson, 1987; Luce and Pisoni, 1998; Vitevitch and Luce, 1999). In the ASR research community, prior work has also investigated various factors that benefit or disrupt recognition. Examples of such factors include word frequency, speaking rate, and prosodic factors (Fosler-Lussier and Morgan, 1999; Shinozaki and Furui, 2001; Hirschberg et al., 2004; Goldwater et al., 2010). There has also been prior work that uses word confusability measures to predict speech recognition errors (FoslerLussier et al., 2005; Jyothi and Fosler-Lussier, 2009). ∗Supported by a Beckman Postdoctoral Fellowship. Word neighborhood measures have been studied more heavily for human word recognition than as predictors of ASR errors. Although not studied specifically in prior work (Fosler-Lussier et al., 2005; Jyothi and Fosler-Lussier, 2009), word confusability measures used in predicting ASR errors could be utilized to build word neighborhoods. Goldwater et al. (2010) examine the behavior</context>
<context position="4058" citStr="Goldwater et al. (2010)" startWordPosition="638" endWordPosition="641">eighborhood density measure by computing the sum of the raw (or log) frequencies of a word’s neighbors (Luce and Pisoni, 1998; Vitevitch and Luce, 1999); the word frequencies 1 Proceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 1–9, Baltimore, Maryland USA, June 27 2014. c�2014 Association for Computational Linguistics are derived from a large corpus. We refer to this frequency-weighted measure as “wND”. �wND = DND(w, w0) · π(w0) w, where π(w0) is the frequency of the word w0.1 Both ND and wND are popular measures for word neighborhoods that we consider to be our baselines; Goldwater et al. (2010) also make use of these two density measures.2 Neither of these measures account for the frequency of the word itself. In continuous ASR, which uses a language model, frequent words are more likely to be recognized correctly (FoslerLussier and Morgan, 1999). To account for this, instead of using absolute frequencies of the neighboring words, we use their relative frequencies to define a third baseline density measure,“rwND” (relative-wND): DND(w,w0) · π(w0) π(w) Relative frequencies have appeared in prior work (Luce, 1986; Luce and Pisoni, 1998; Scarborough, 2012). In fact, the measure used by</context>
<context position="5609" citStr="Goldwater et al. (2010)" startWordPosition="897" endWordPosition="900">elate with recognition errors. We define the neighborhood score for a word w as: �score(w) = β(w, w0) · D(w, w0) (1) w,6=w Intuitively, β is an averaging function that weighs the importance of each neighboring word. For example, Yarkoni et al. (2008) use a neighborhood measure that gives equal importance to the top 1Here we use raw rather than log frequencies. The baseline density measures in this section perform better with raw rather than log frequencies on our evaluation data. Our proposed measures perform significantly better than the baseline measures using both raw and log frequencies. 2Goldwater et al. (2010) also consider the number of homophones (words that share a pronunciation with the target word) and frequency-weighted homophones as additional neighborhood measures. In our data there is insufficient homophony for these measures to be significant, so we do not report on experiments using them. 20 closest neighbors and rejects the others. The rest of the section presents multiple choices for D and β which will define our various neighborhood measures via Equation 1. 3.1 Distance Functions All of our distance functions are based on an edit distance between a pair of words, i.e., the minimum cos</context>
<context position="16673" citStr="Goldwater et al. (2010)" startWordPosition="2876" endWordPosition="2879">t the inverse relationship of the density measures with our distance-based measures. 4 Experiments We provide an individual analysis of each neighborhood measure as it relates to recognition error rate. We also present a matrix of pairwise comparisons among all of the neighborhood measures with respect to their ability to predict recognition errors. We study the relationship between neighborhood measures and ASR errors in two settings: • Isolated-word ASR: Psycholinguistic studies typically use isolated words as stimuli to study the influence of neighborhood measures on recognition (e.g., see Goldwater et al. (2010) and references therein). Motivated by this, we build an ASR system that recognizes words in isolation and analyze the relationship between its errors and each neighborhood measure. Further details of this analysis are described in Section 4.1. • Continuous-word ASR: ASR systems typically deal with continuous speech. However, the usefulness of neighborhood measures for continuous-word ASR has received little attention, with the notable exception of Goldwater et al. (2010). We further this line of investigation in our second set of experiments by analyzing the relationship between errors made b</context>
<context position="20525" citStr="Goldwater et al. (2010)" startWordPosition="3502" endWordPosition="3505"> 0.010 0.020 0.0 0.2 0.4 0.6 0.8 1.0 5 10 15 ND wND wAFx (a) Neighborhood measures ND, wND and wAFx as predictors of isolated-word error rate (ER). ND ED AF AFx wND wED wAF wAFx ND ED AF AFx wND wED wAF wAFx null 5−6 8−5 5−7 1−7 8−8 2−8 3−10 6−11 0 0.0001 0.001 0.01 0.05 0.1 1 (b) Pairwise comparison of word neighborhood measures as predictors of errors from the isolated-word ASR system using p-values. Many low p-values (darker cells) along a column implies the corresponding measure is a significant predictor of ER. Figure 3: Analysis of neighborhood measures with isolated word ASR. following Goldwater et al. (2010), we use a logistic regression model implemented using the glm function in R (R Development Core Team, 2005). The logistic regression model fits the log-odds of a binary response variable with a linear combination of one or more predictor variables. For our isolated-word task, the response variable takes a value of either 1 or 0 corresponding to the presence or absence of an error, respectively; we will refer to it as “ER”. We build a separate logistic regression model for each neighborhood measure acting as the only predictor of ER. We use restricted cubic splines, using the res (Harrell Jr.,</context>
<context position="22132" citStr="Goldwater et al. (2010)" startWordPosition="3774" endWordPosition="3777">r predictors. All of the neighborhood measures were found to be significant predictors, with our measures wAF and wAFx being most significant. The p-values from this test are shown in a separate row under the header “null” in Figure 3(b); here, 5−6 stands for 5 × 10−6 and so forth. We note that the neighborhood measures are significantly correlated with ER as individual predictors, but classifiers built with each individual measure as the only feature are not good predictors of ASR errors. This is unsurprising as we expect many other predictors other than neighborhood measures, as outlined in Goldwater et al. (2010), to influence ASR errors. This paper focuses only on analyzing each neighborhood measure as an individual predictor; joint models will be explored as part of future work. Figure 3(a) shows the relationship between errors from the isolated ASR system and three neighborhood measures: the best-performing measure (wAFx) and the two standard density measures (ND, wND). The feature values are aggregated into roughly equal-sized bins and the average error rate for each bin is plotted. The 6 IWER IWER 0.0 0.2 0.4 0.6 0.0 0.2 0.4 0.6 2 4 6 8 10 0.000 0.005 0.010 0.015 0.020 0.0 0.2 0.4 0.6 IWER 0 10 2</context>
<context position="26412" citStr="Goldwater et al. (2010)" startWordPosition="4488" endWordPosition="4491">rmations and speaker adaptation. They are also trained discriminatively using boosted maximum mutual information training from the Kaldi toolkit. We use the entire Switchboard vocabulary of 30,241 words and a 3-gram language model trained on all of the training sentences. The word error rate on the evaluation sentences is 28.3%.6 Results and Discussion: Unlike the isolatedword task, the continuous-word ASR system gives word error rates over full utterances. Since we need to measure the errors associated with the individual words, we use the individual word error rate (IWER) metric proposed by Goldwater et al. (2010). The IWER for word wi is a·ini+deli+ subi where ini is the number of insertions adjacent to wi; deli or subi is 1 if wi is either deleted or substituted, respectively. a is chosen such that a · Ei ini = I where I is the total number of insertions for the entire dataset. As in the isolated-word task, we fit logistic regression models to analyze the neighborhood measures as predictors of IWER. Figure 4(a) shows fitted probabilities from a logistic regression model for IWER built individually using each of the measures wND, rwND and rwAFx as predictors. The number of frequency-weighted neighbors</context>
</contexts>
<marker>Goldwater, Jurafsky, Manning, 2010</marker>
<rawString>S. Goldwater, D. Jurafsky, and C. D. Manning. 2010. Which words are hard to recognize? Prosodic, lexical, and disfluency factors that increase speech recognition error rates. Speech Communication, 52(3):181–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Greenberg</author>
<author>J Hollenback</author>
<author>D Ellis</author>
</authors>
<title>Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus.</title>
<date>1996</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="17604" citStr="Greenberg et al., 1996" startWordPosition="3019" endWordPosition="3023">s speech. However, the usefulness of neighborhood measures for continuous-word ASR has received little attention, with the notable exception of Goldwater et al. (2010). We further this line of investigation in our second set of experiments by analyzing the relationship between errors made by a continuousword ASR system and our new measures. These are described in more detail in Section 4.2. 4.1 Isolated-Word ASR Experimental Setup: We extract isolated words from a subset of the Switchboard-I conversational speech corpus (Godfrey et al., 1992) called the Switchboard Transcription Project, STP (Greenberg et al., 1996; STP, 1996), which is phonetically labeled at a fine-grained level. Isolated words were excised from continuous utterances in sets 20–22 in the STP corpus. We use a total of 401 word tokens (247 unique words) derived from the 3500 most frequent words in Switchboard-I, excluding non-speech events and partial words. These words make up the development and evaluation sets used in prior related work on pronunciation modeling (Livescu and Glass, 2004; Jyothi et al., 2011; Jyothi et al., 2012). We use the dictionary that accompanies the Switchboard-I corpus consisting of 30,241 words; —98% of these</context>
</contexts>
<marker>Greenberg, Hollenback, Ellis, 1996</marker>
<rawString>S. Greenberg, J. Hollenback, and D. Ellis. 1996. Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hahn</author>
<author>T M Bailey</author>
</authors>
<title>What makes words sound similar?</title>
<date>2005</date>
<journal>Cognition,</journal>
<volume>97</volume>
<issue>3</issue>
<contexts>
<context position="7637" citStr="Hahn and Bailey, 2005" startWordPosition="1237" endWordPosition="1240">ions (phone sequences) of the word. We also introduce a memoryless transducer, T, that maps an input phone to any output phone, with arc weights equal to the corresponding substitution costs (mapping to or from epsilon indicates a deletion or an insertion). The weight of the shortest path in the composed FST, Fw o T o Fw,, gives the edit distance between w and w0. When either w or w0 has more than one pronunciation, DED is the minimum edit distance among all pairs of pronunciations. This edit distance function has been previously proposed as a measure of phonological similarity between words (Hahn and Bailey, 2005). Similar distance functions have also been used for neighborhood density measures in visual word recognition studies (Yarkoni et al., 2008). Simple Articulatory Feature-based Edit Distance (DAF): The distance function DED penalizes an incorrect substitution equally regardless of the phone identity; for example, the phone [p] can be substituted with [b] or [aa] with equal cost according to DED, although we know it is more likely for [p] to be produced as [b] than as [aa]. To account for this, we adopt a finer-grained representation of the phone as a vector of discrete articulatory “features”. </context>
</contexts>
<marker>Hahn, Bailey, 2005</marker>
<rawString>U. Hahn and T. M. Bailey. 2005. What makes words sound similar? Cognition, 97(3):227–267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F E Harrell Jr</author>
</authors>
<title>RMS: Regression Modeling Strategies. R package version 3.5-0.</title>
<date>2012</date>
<marker>Jr, 2012</marker>
<rawString>F. E. Harrell Jr. 2012. RMS: Regression Modeling Strategies. R package version 3.5-0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>D Litman</author>
<author>M Swerts</author>
</authors>
<title>Prosodic and other cues to speech recognition failures.</title>
<date>2004</date>
<journal>Speech Communication,</journal>
<volume>43</volume>
<issue>1</issue>
<contexts>
<context position="1655" citStr="Hirschberg et al., 2004" startWordPosition="242" endWordPosition="245"> word recognition, it has been shown that it is harder to recognize words that have many “similar” neighboring words than words with few neighbors (Luce and Pisoni, 1998), and that frequent words are recognized faster and more accurately than are infrequent words (Marslen-Wilson, 1987; Luce and Pisoni, 1998; Vitevitch and Luce, 1999). In the ASR research community, prior work has also investigated various factors that benefit or disrupt recognition. Examples of such factors include word frequency, speaking rate, and prosodic factors (Fosler-Lussier and Morgan, 1999; Shinozaki and Furui, 2001; Hirschberg et al., 2004; Goldwater et al., 2010). There has also been prior work that uses word confusability measures to predict speech recognition errors (FoslerLussier et al., 2005; Jyothi and Fosler-Lussier, 2009). ∗Supported by a Beckman Postdoctoral Fellowship. Word neighborhood measures have been studied more heavily for human word recognition than as predictors of ASR errors. Although not studied specifically in prior work (Fosler-Lussier et al., 2005; Jyothi and Fosler-Lussier, 2009), word confusability measures used in predicting ASR errors could be utilized to build word neighborhoods. Goldwater et al. (2</context>
</contexts>
<marker>Hirschberg, Litman, Swerts, 2004</marker>
<rawString>J. Hirschberg, D. Litman, and M. Swerts. 2004. Prosodic and other cues to speech recognition failures. Speech Communication, 43(1):155–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jyothi</author>
<author>E Fosler-Lussier</author>
</authors>
<title>A comparison of audio-free speech recognition error prediction methods.</title>
<date>2009</date>
<booktitle>In Proc. of Interspeech.</booktitle>
<contexts>
<context position="1849" citStr="Jyothi and Fosler-Lussier, 2009" startWordPosition="272" endWordPosition="275">uent words are recognized faster and more accurately than are infrequent words (Marslen-Wilson, 1987; Luce and Pisoni, 1998; Vitevitch and Luce, 1999). In the ASR research community, prior work has also investigated various factors that benefit or disrupt recognition. Examples of such factors include word frequency, speaking rate, and prosodic factors (Fosler-Lussier and Morgan, 1999; Shinozaki and Furui, 2001; Hirschberg et al., 2004; Goldwater et al., 2010). There has also been prior work that uses word confusability measures to predict speech recognition errors (FoslerLussier et al., 2005; Jyothi and Fosler-Lussier, 2009). ∗Supported by a Beckman Postdoctoral Fellowship. Word neighborhood measures have been studied more heavily for human word recognition than as predictors of ASR errors. Although not studied specifically in prior work (Fosler-Lussier et al., 2005; Jyothi and Fosler-Lussier, 2009), word confusability measures used in predicting ASR errors could be utilized to build word neighborhoods. Goldwater et al. (2010) examine the behavior of certain standard neighborhood density measures as predictors of ASR errors. To our knowledge, this is the only study that explicitly considers word neighborhoods as </context>
</contexts>
<marker>Jyothi, Fosler-Lussier, 2009</marker>
<rawString>P. Jyothi and E. Fosler-Lussier. 2009. A comparison of audio-free speech recognition error prediction methods. In Proc. of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jyothi</author>
<author>K Livescu</author>
<author>E Fosler-Lussier</author>
</authors>
<title>Lexical access experiments with context-dependent articulatory feature-based models.</title>
<date>2011</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="9763" citStr="Jyothi et al., 2011" startWordPosition="1625" endWordPosition="1628">is shown in Figure 1; apart from the difference in the arc weights on T, DAF is the same as DED. Extended Articulatory Feature-based Edit Distance (DAFx): The words in our dictionary are associated with one or more canonical pronunciations written as sequences of phones. The distance functions DED and DAF make use of this small set of canonical pronunciations and do not capture the various other ways in which a word can be pronounced. An alternative, explored in some prior work on pronunciation modeling (Deng and Sun, 1994; Richardson et al., 2003; Livescu and Glass, 2004; Mitra et al., 2011; Jyothi et al., 2011), is to model the pronunciation of a word as multiple, possibly asynchronous streams of fine-grained articulatory features, again inspired by articulatory phonology. Such a model can be implemented as a dynamic Bayesian network (DBN) with multiple variables representing the articulatory features 3The mapping of phones to their articulatory feature values is defined in Appendix B of Livescu (2005). This mapping includes a probability distribution over feature values for certain phones; in these cases, we choose the articulatory feature value with the highest probability. in each time frame; ple</context>
<context position="18075" citStr="Jyothi et al., 2011" startWordPosition="3098" endWordPosition="3101">t of the Switchboard-I conversational speech corpus (Godfrey et al., 1992) called the Switchboard Transcription Project, STP (Greenberg et al., 1996; STP, 1996), which is phonetically labeled at a fine-grained level. Isolated words were excised from continuous utterances in sets 20–22 in the STP corpus. We use a total of 401 word tokens (247 unique words) derived from the 3500 most frequent words in Switchboard-I, excluding non-speech events and partial words. These words make up the development and evaluation sets used in prior related work on pronunciation modeling (Livescu and Glass, 2004; Jyothi et al., 2011; Jyothi et al., 2012). We use the dictionary that accompanies the Switchboard-I corpus consisting of 30,241 words; —98% of these words are associated with a single pronunciation. The recognition system for this isolated word dataset was built using the Kaldi toolkit (Povey et al., 2011; Kal, 2011). We use an acoustic model that is trained on all of SwitchboardI, excluding the sentences from which our 401- word set was drawn. The ASR system uses standard mel frequency cepstral coefficients with their first and second derivatives (deltas and doubledeltas) as acoustic features, with standard nor</context>
</contexts>
<marker>Jyothi, Livescu, Fosler-Lussier, 2011</marker>
<rawString>P. Jyothi, K. Livescu, and E. Fosler-Lussier. 2011. Lexical access experiments with context-dependent articulatory feature-based models. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jyothi</author>
<author>E Fosler-Lussier</author>
<author>K Livescu</author>
</authors>
<title>Discriminatively learning factorized finite state pronunciation models from dynamic Bayesian networks.</title>
<date>2012</date>
<booktitle>In Proc. of Interspeech.</booktitle>
<contexts>
<context position="10793" citStr="Jyothi et al. (2012)" startWordPosition="1781" endWordPosition="1784">ing includes a probability distribution over feature values for certain phones; in these cases, we choose the articulatory feature value with the highest probability. in each time frame; please refer to (Livescu and Glass, 2004; Livescu, 2005; Jyothi et al., 2011) for more details. In this approach, deviations from a dictionary pronunciation are the result of either asynchrony between the articulatory streams (accounting for effects such as nasalization, rounding, and epenthetic stops) or the substitution of one articulatory feature value for another (accounting for many reduction phenomena). Jyothi et al. (2012) describe an approach to encode such a DBN model of pronunciation as an FST that outputs an articulatory feature tuple for each frame of speech. We modify this FST by mapping each articulatory feature tuple to a valid phone as per the phone-to-articulatoryfeature mapping used for DAF (discarding arcs whose labels do not correspond to a valid phone). The resulting FSTs are used to define DAFx by composing with the edit transducer T as in the definition of DAF. For computational efficiency, we prune these FSTs to retain only paths that are within three times the weight of the shortest path. The </context>
<context position="18097" citStr="Jyothi et al., 2012" startWordPosition="3102" endWordPosition="3105">I conversational speech corpus (Godfrey et al., 1992) called the Switchboard Transcription Project, STP (Greenberg et al., 1996; STP, 1996), which is phonetically labeled at a fine-grained level. Isolated words were excised from continuous utterances in sets 20–22 in the STP corpus. We use a total of 401 word tokens (247 unique words) derived from the 3500 most frequent words in Switchboard-I, excluding non-speech events and partial words. These words make up the development and evaluation sets used in prior related work on pronunciation modeling (Livescu and Glass, 2004; Jyothi et al., 2011; Jyothi et al., 2012). We use the dictionary that accompanies the Switchboard-I corpus consisting of 30,241 words; —98% of these words are associated with a single pronunciation. The recognition system for this isolated word dataset was built using the Kaldi toolkit (Povey et al., 2011; Kal, 2011). We use an acoustic model that is trained on all of SwitchboardI, excluding the sentences from which our 401- word set was drawn. The ASR system uses standard mel frequency cepstral coefficients with their first and second derivatives (deltas and doubledeltas) as acoustic features, with standard normalization and adaptat</context>
</contexts>
<marker>Jyothi, Fosler-Lussier, Livescu, 2012</marker>
<rawString>P. Jyothi, E. Fosler-Lussier, and K. Livescu. 2012. Discriminatively learning factorized finite state pronunciation models from dynamic Bayesian networks. In Proc. of Interspeech.</rawString>
</citation>
<citation valid="false">
<date>2011</date>
<note>Kaldi. http://kaldi.sourceforge. net/.</note>
<marker>2011</marker>
<rawString>2011. Kaldi. http://kaldi.sourceforge. net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Livescu</author>
<author>J Glass</author>
</authors>
<title>Feature-based pronunciation modeling with trainable asynchrony probabilities.</title>
<date>2004</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="9721" citStr="Livescu and Glass, 2004" startWordPosition="1617" endWordPosition="1620">e arc weights on the edit transducer T. This is shown in Figure 1; apart from the difference in the arc weights on T, DAF is the same as DED. Extended Articulatory Feature-based Edit Distance (DAFx): The words in our dictionary are associated with one or more canonical pronunciations written as sequences of phones. The distance functions DED and DAF make use of this small set of canonical pronunciations and do not capture the various other ways in which a word can be pronounced. An alternative, explored in some prior work on pronunciation modeling (Deng and Sun, 1994; Richardson et al., 2003; Livescu and Glass, 2004; Mitra et al., 2011; Jyothi et al., 2011), is to model the pronunciation of a word as multiple, possibly asynchronous streams of fine-grained articulatory features, again inspired by articulatory phonology. Such a model can be implemented as a dynamic Bayesian network (DBN) with multiple variables representing the articulatory features 3The mapping of phones to their articulatory feature values is defined in Appendix B of Livescu (2005). This mapping includes a probability distribution over feature values for certain phones; in these cases, we choose the articulatory feature value with the hi</context>
<context position="18054" citStr="Livescu and Glass, 2004" startWordPosition="3094" endWordPosition="3097">olated words from a subset of the Switchboard-I conversational speech corpus (Godfrey et al., 1992) called the Switchboard Transcription Project, STP (Greenberg et al., 1996; STP, 1996), which is phonetically labeled at a fine-grained level. Isolated words were excised from continuous utterances in sets 20–22 in the STP corpus. We use a total of 401 word tokens (247 unique words) derived from the 3500 most frequent words in Switchboard-I, excluding non-speech events and partial words. These words make up the development and evaluation sets used in prior related work on pronunciation modeling (Livescu and Glass, 2004; Jyothi et al., 2011; Jyothi et al., 2012). We use the dictionary that accompanies the Switchboard-I corpus consisting of 30,241 words; —98% of these words are associated with a single pronunciation. The recognition system for this isolated word dataset was built using the Kaldi toolkit (Povey et al., 2011; Kal, 2011). We use an acoustic model that is trained on all of SwitchboardI, excluding the sentences from which our 401- word set was drawn. The ASR system uses standard mel frequency cepstral coefficients with their first and second derivatives (deltas and doubledeltas) as acoustic featur</context>
</contexts>
<marker>Livescu, Glass, 2004</marker>
<rawString>K. Livescu and J. Glass. 2004. Feature-based pronunciation modeling with trainable asynchrony probabilities. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Livescu</author>
</authors>
<title>Feature-based Pronunciation Modeling for Automatic Speech Recognition.</title>
<date>2005</date>
<tech>PhD Dissertation, MIT EECS department.</tech>
<contexts>
<context position="8805" citStr="Livescu, 2005" startWordPosition="1462" endWordPosition="1463">a vector of discrete articulatory “features”. Our features are derived from �rwND = w, 2 k:e/3.364 m:p/3.464 k:k/0 k ah m k aa p DAF : 0 1 2 3 0 0 0 0 1 2 3 Figure 1: Distance functions implemented using finite-state machines. k kcl gcl DAFx : 0 1 k:e/3.364 m:p/3.464 k:k/0 k kcl 4 5 0 0 0 0 gcl 1 m 2 ah ah ax n em ax n 2 aa pcl ah pcl 4 p 5 g ax g ao kcl 3 ah n 3 ah n the vocal tract variables of articulatory phonology (Browman and Goldstein, 1992), including the constriction degrees and locations of the lips, tongue tip, tongue body, velum and glottis.We borrow a particular feature set from (Livescu, 2005).3 The substitution cost between two phones is defined as the L1 distance between the articulatory vectors corresponding to the phones. We set the insertion and deletion costs to the mean substitution cost between the articulatory vectors for all phone pairs. These new costs will appear as the arc weights on the edit transducer T. This is shown in Figure 1; apart from the difference in the arc weights on T, DAF is the same as DED. Extended Articulatory Feature-based Edit Distance (DAFx): The words in our dictionary are associated with one or more canonical pronunciations written as sequences o</context>
<context position="10162" citStr="Livescu (2005)" startWordPosition="1688" endWordPosition="1689">n which a word can be pronounced. An alternative, explored in some prior work on pronunciation modeling (Deng and Sun, 1994; Richardson et al., 2003; Livescu and Glass, 2004; Mitra et al., 2011; Jyothi et al., 2011), is to model the pronunciation of a word as multiple, possibly asynchronous streams of fine-grained articulatory features, again inspired by articulatory phonology. Such a model can be implemented as a dynamic Bayesian network (DBN) with multiple variables representing the articulatory features 3The mapping of phones to their articulatory feature values is defined in Appendix B of Livescu (2005). This mapping includes a probability distribution over feature values for certain phones; in these cases, we choose the articulatory feature value with the highest probability. in each time frame; please refer to (Livescu and Glass, 2004; Livescu, 2005; Jyothi et al., 2011) for more details. In this approach, deviations from a dictionary pronunciation are the result of either asynchrony between the articulatory streams (accounting for effects such as nasalization, rounding, and epenthetic stops) or the substitution of one articulatory feature value for another (accounting for many reduction p</context>
</contexts>
<marker>Livescu, 2005</marker>
<rawString>K. Livescu. 2005. Feature-based Pronunciation Modeling for Automatic Speech Recognition. PhD Dissertation, MIT EECS department.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Luce</author>
<author>D B Pisoni</author>
</authors>
<title>Recognizing spoken words: The neighborhood activation model. Ear and hearing,</title>
<date>1998</date>
<pages>19--1</pages>
<contexts>
<context position="1202" citStr="Luce and Pisoni, 1998" startWordPosition="175" endWordPosition="178">ze the significance of our measures as predictors of errors in an isolated-word ASR system and a continuous-word ASR system. We find that our measures are significantly better predictors of ASR errors than previously used neighborhood density measures. 1 Introduction An important pursuit for both human and machine speech recognition research is to understand the factors that affect word recognition accuracy. In the substantial body of work on human word recognition, it has been shown that it is harder to recognize words that have many “similar” neighboring words than words with few neighbors (Luce and Pisoni, 1998), and that frequent words are recognized faster and more accurately than are infrequent words (Marslen-Wilson, 1987; Luce and Pisoni, 1998; Vitevitch and Luce, 1999). In the ASR research community, prior work has also investigated various factors that benefit or disrupt recognition. Examples of such factors include word frequency, speaking rate, and prosodic factors (Fosler-Lussier and Morgan, 1999; Shinozaki and Furui, 2001; Hirschberg et al., 2004; Goldwater et al., 2010). There has also been prior work that uses word confusability measures to predict speech recognition errors (FoslerLussier</context>
<context position="3560" citStr="Luce and Pisoni, 1998" startWordPosition="555" endWordPosition="558">Related Work: Neighborhood Density Measures In much of the prior work in the psycholinguistics literature, the notion of word similarity is quantified by a simple one-phone-away rule: A word w&apos; is a neighbor of word w if w and w&apos; differ by a single phone, via a substitution, deletion, or insertion. We refer to this density measure as “ND”. ND = � AND(w, w&apos;) W/ where AND(w, w&apos;) = 1 if w and w&apos; differ by a phone and 0 otherwise. The frequencies of the neighbors are often accounted for in the neighborhood density measure by computing the sum of the raw (or log) frequencies of a word’s neighbors (Luce and Pisoni, 1998; Vitevitch and Luce, 1999); the word frequencies 1 Proceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 1–9, Baltimore, Maryland USA, June 27 2014. c�2014 Association for Computational Linguistics are derived from a large corpus. We refer to this frequency-weighted measure as “wND”. �wND = DND(w, w0) · π(w0) w, where π(w0) is the frequency of the word w0.1 Both ND and wND are popular measures for word neighborhoods that we consider to be our baselines; Goldwater et al. (2010) also make use of these two density measures.2 Neither of these measures account for the frequency of </context>
</contexts>
<marker>Luce, Pisoni, 1998</marker>
<rawString>P. A. Luce and D. B. Pisoni. 1998. Recognizing spoken words: The neighborhood activation model. Ear and hearing, 19:1–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Luce</author>
</authors>
<title>Neighborhoods of words in the mental lexicon. Research on Speech Perception,</title>
<date>1986</date>
<tech>(Technical Report No. 6.).</tech>
<contexts>
<context position="4585" citStr="Luce, 1986" startWordPosition="723" endWordPosition="724">or word neighborhoods that we consider to be our baselines; Goldwater et al. (2010) also make use of these two density measures.2 Neither of these measures account for the frequency of the word itself. In continuous ASR, which uses a language model, frequent words are more likely to be recognized correctly (FoslerLussier and Morgan, 1999). To account for this, instead of using absolute frequencies of the neighboring words, we use their relative frequencies to define a third baseline density measure,“rwND” (relative-wND): DND(w,w0) · π(w0) π(w) Relative frequencies have appeared in prior work (Luce, 1986; Luce and Pisoni, 1998; Scarborough, 2012). In fact, the measure used by Scarborough (2012) is the reciprocal of rwND. 3 Proposed Neighborhood Measures Our new neighborhood measures are defined in terms of a distance function between a pair of words, D, and a weighting function, β. The proposed measures are not densities in the same sense as ND, wND, rwND, but are scores that we may expect to correlate with recognition errors. We define the neighborhood score for a word w as: �score(w) = β(w, w0) · D(w, w0) (1) w,6=w Intuitively, β is an averaging function that weighs the importance of each n</context>
</contexts>
<marker>Luce, 1986</marker>
<rawString>P. A. Luce. 1986. Neighborhoods of words in the mental lexicon. Research on Speech Perception, (Technical Report No. 6.).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W D Marslen-Wilson</author>
</authors>
<title>Functional parallelism in spoken word-recognition.</title>
<date>1987</date>
<journal>Cognition,</journal>
<volume>25</volume>
<issue>1</issue>
<pages>102</pages>
<contexts>
<context position="1317" citStr="Marslen-Wilson, 1987" startWordPosition="195" endWordPosition="196">system. We find that our measures are significantly better predictors of ASR errors than previously used neighborhood density measures. 1 Introduction An important pursuit for both human and machine speech recognition research is to understand the factors that affect word recognition accuracy. In the substantial body of work on human word recognition, it has been shown that it is harder to recognize words that have many “similar” neighboring words than words with few neighbors (Luce and Pisoni, 1998), and that frequent words are recognized faster and more accurately than are infrequent words (Marslen-Wilson, 1987; Luce and Pisoni, 1998; Vitevitch and Luce, 1999). In the ASR research community, prior work has also investigated various factors that benefit or disrupt recognition. Examples of such factors include word frequency, speaking rate, and prosodic factors (Fosler-Lussier and Morgan, 1999; Shinozaki and Furui, 2001; Hirschberg et al., 2004; Goldwater et al., 2010). There has also been prior work that uses word confusability measures to predict speech recognition errors (FoslerLussier et al., 2005; Jyothi and Fosler-Lussier, 2009). ∗Supported by a Beckman Postdoctoral Fellowship. Word neighborhood</context>
</contexts>
<marker>Marslen-Wilson, 1987</marker>
<rawString>W. D. Marslen-Wilson. 1987. Functional parallelism in spoken word-recognition. Cognition, 25(1):71– 102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Mitra</author>
<author>H Nam</author>
<author>C Y Espy-Wilson</author>
<author>E Saltzman</author>
<author>L Goldstein</author>
</authors>
<title>Articulatory information for noise robust speech recognition.</title>
<date>2011</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>19</volume>
<issue>7</issue>
<contexts>
<context position="9741" citStr="Mitra et al., 2011" startWordPosition="1621" endWordPosition="1624"> transducer T. This is shown in Figure 1; apart from the difference in the arc weights on T, DAF is the same as DED. Extended Articulatory Feature-based Edit Distance (DAFx): The words in our dictionary are associated with one or more canonical pronunciations written as sequences of phones. The distance functions DED and DAF make use of this small set of canonical pronunciations and do not capture the various other ways in which a word can be pronounced. An alternative, explored in some prior work on pronunciation modeling (Deng and Sun, 1994; Richardson et al., 2003; Livescu and Glass, 2004; Mitra et al., 2011; Jyothi et al., 2011), is to model the pronunciation of a word as multiple, possibly asynchronous streams of fine-grained articulatory features, again inspired by articulatory phonology. Such a model can be implemented as a dynamic Bayesian network (DBN) with multiple variables representing the articulatory features 3The mapping of phones to their articulatory feature values is defined in Appendix B of Livescu (2005). This mapping includes a probability distribution over feature values for certain phones; in these cases, we choose the articulatory feature value with the highest probability. i</context>
</contexts>
<marker>Mitra, Nam, Espy-Wilson, Saltzman, Goldstein, 2011</marker>
<rawString>V. Mitra, H. Nam, C. Y. Espy-Wilson, E. Saltzman, and L. Goldstein. 2011. Articulatory information for noise robust speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 19(7):1913–1924.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Nosofsky</author>
</authors>
<title>Attention, similarity, and the identification–categorization relationship.</title>
<date>1986</date>
<journal>Journal of Experimental Psychology: General,</journal>
<volume>115</volume>
<issue>1</issue>
<contexts>
<context position="11911" citStr="Nosofsky (1986)" startWordPosition="1975" endWordPosition="1977">these FSTs to retain only paths that are within three times the weight of the shortest path. The pruned FSTs have hundreds of arcs and ∼50 states on average. A schematic diagram is used to illustrate the computation of DAFx in Figure 1. 3.2 Weighting Functions Our weighting functions can be appropriately defined to discount the contributions of words that are infrequent or are very far away. We note here that unlike the density measures in Section 2, the lower the distance-based score for a word (from Equation 1), the more confusable it would be with its neighbors. One approach, as pursued in Nosofsky (1986) and Bailey and Hahn (2001), is to use score(w) = E,,,, g(D(w, w&apos;)) where g is an expo3 r1 r2 Figure 2: Let w1 and w2 be the two closest words to w. The area of the shaded region shows β(w, w2) where ri = Rw(wi) = i. In the weighted case given in Equation 4, r1 = Rηw(w1), r2 = Rηw(w2) and r2 − r1 = ηw(w2). nentially decreasing function. This, however, has the disadvantage of being very sensitive to the distance measure used: Slight changes in the distance can alter the score significantly, even if the overall ordering of the distances is preserved. We propose an alternative approach that keeps</context>
</contexts>
<marker>Nosofsky, 1986</marker>
<rawString>R. M. Nosofsky. 1986. Attention, similarity, and the identification–categorization relationship. Journal of Experimental Psychology: General, 115(1):39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Povey</author>
<author>A Ghoshal</author>
</authors>
<title>The Kaldi speech recognition toolkit.</title>
<date>2011</date>
<booktitle>Proc. of ASRU.</booktitle>
<marker>Povey, Ghoshal, 2011</marker>
<rawString>D. Povey, A. Ghoshal, et al. 2011. The Kaldi speech recognition toolkit. Proc. of ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Development Core Team</author>
</authors>
<title>R: A language and environment for statistical computing. R foundation for Statistical Computing.</title>
<date>2005</date>
<contexts>
<context position="20633" citStr="Team, 2005" startWordPosition="3523" endWordPosition="3524">lated-word error rate (ER). ND ED AF AFx wND wED wAF wAFx ND ED AF AFx wND wED wAF wAFx null 5−6 8−5 5−7 1−7 8−8 2−8 3−10 6−11 0 0.0001 0.001 0.01 0.05 0.1 1 (b) Pairwise comparison of word neighborhood measures as predictors of errors from the isolated-word ASR system using p-values. Many low p-values (darker cells) along a column implies the corresponding measure is a significant predictor of ER. Figure 3: Analysis of neighborhood measures with isolated word ASR. following Goldwater et al. (2010), we use a logistic regression model implemented using the glm function in R (R Development Core Team, 2005). The logistic regression model fits the log-odds of a binary response variable with a linear combination of one or more predictor variables. For our isolated-word task, the response variable takes a value of either 1 or 0 corresponding to the presence or absence of an error, respectively; we will refer to it as “ER”. We build a separate logistic regression model for each neighborhood measure acting as the only predictor of ER. We use restricted cubic splines, using the res (Harrell Jr., 2012) function in R, to model non-linear predictive relationships. In order to determine whether a neighbor</context>
</contexts>
<marker>Team, 2005</marker>
<rawString>R Development Core Team. 2005. R: A language and environment for statistical computing. R foundation for Statistical Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>J Bilmes</author>
<author>C Diorio</author>
</authors>
<title>Hidden-articulator Markov models for speech recognition.</title>
<date>2003</date>
<journal>Speech Communication,</journal>
<pages>41--2</pages>
<contexts>
<context position="9696" citStr="Richardson et al., 2003" startWordPosition="1613" endWordPosition="1616">w costs will appear as the arc weights on the edit transducer T. This is shown in Figure 1; apart from the difference in the arc weights on T, DAF is the same as DED. Extended Articulatory Feature-based Edit Distance (DAFx): The words in our dictionary are associated with one or more canonical pronunciations written as sequences of phones. The distance functions DED and DAF make use of this small set of canonical pronunciations and do not capture the various other ways in which a word can be pronounced. An alternative, explored in some prior work on pronunciation modeling (Deng and Sun, 1994; Richardson et al., 2003; Livescu and Glass, 2004; Mitra et al., 2011; Jyothi et al., 2011), is to model the pronunciation of a word as multiple, possibly asynchronous streams of fine-grained articulatory features, again inspired by articulatory phonology. Such a model can be implemented as a dynamic Bayesian network (DBN) with multiple variables representing the articulatory features 3The mapping of phones to their articulatory feature values is defined in Appendix B of Livescu (2005). This mapping includes a probability distribution over feature values for certain phones; in these cases, we choose the articulatory </context>
</contexts>
<marker>Richardson, Bilmes, Diorio, 2003</marker>
<rawString>M. Richardson, J. Bilmes, and C. Diorio. 2003. Hidden-articulator Markov models for speech recognition. Speech Communication, 41(2-3):511–529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Scarborough</author>
</authors>
<title>Lexical confusability and degree of coarticulation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Annual Meeting of the Berkeley Linguistics Society.</booktitle>
<contexts>
<context position="4628" citStr="Scarborough, 2012" startWordPosition="729" endWordPosition="731">der to be our baselines; Goldwater et al. (2010) also make use of these two density measures.2 Neither of these measures account for the frequency of the word itself. In continuous ASR, which uses a language model, frequent words are more likely to be recognized correctly (FoslerLussier and Morgan, 1999). To account for this, instead of using absolute frequencies of the neighboring words, we use their relative frequencies to define a third baseline density measure,“rwND” (relative-wND): DND(w,w0) · π(w0) π(w) Relative frequencies have appeared in prior work (Luce, 1986; Luce and Pisoni, 1998; Scarborough, 2012). In fact, the measure used by Scarborough (2012) is the reciprocal of rwND. 3 Proposed Neighborhood Measures Our new neighborhood measures are defined in terms of a distance function between a pair of words, D, and a weighting function, β. The proposed measures are not densities in the same sense as ND, wND, rwND, but are scores that we may expect to correlate with recognition errors. We define the neighborhood score for a word w as: �score(w) = β(w, w0) · D(w, w0) (1) w,6=w Intuitively, β is an averaging function that weighs the importance of each neighboring word. For example, Yarkoni et al</context>
</contexts>
<marker>Scarborough, 2012</marker>
<rawString>R. A. Scarborough. 2012. Lexical confusability and degree of coarticulation. In Proceedings of the Annual Meeting of the Berkeley Linguistics Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Shinozaki</author>
<author>S Furui</author>
</authors>
<title>Error analysis using decision trees in spontaneous presentation speech recognition.</title>
<date>2001</date>
<booktitle>In Proc. of ASRU.</booktitle>
<contexts>
<context position="1630" citStr="Shinozaki and Furui, 2001" startWordPosition="238" endWordPosition="241">ntial body of work on human word recognition, it has been shown that it is harder to recognize words that have many “similar” neighboring words than words with few neighbors (Luce and Pisoni, 1998), and that frequent words are recognized faster and more accurately than are infrequent words (Marslen-Wilson, 1987; Luce and Pisoni, 1998; Vitevitch and Luce, 1999). In the ASR research community, prior work has also investigated various factors that benefit or disrupt recognition. Examples of such factors include word frequency, speaking rate, and prosodic factors (Fosler-Lussier and Morgan, 1999; Shinozaki and Furui, 2001; Hirschberg et al., 2004; Goldwater et al., 2010). There has also been prior work that uses word confusability measures to predict speech recognition errors (FoslerLussier et al., 2005; Jyothi and Fosler-Lussier, 2009). ∗Supported by a Beckman Postdoctoral Fellowship. Word neighborhood measures have been studied more heavily for human word recognition than as predictors of ASR errors. Although not studied specifically in prior work (Fosler-Lussier et al., 2005; Jyothi and Fosler-Lussier, 2009), word confusability measures used in predicting ASR errors could be utilized to build word neighborh</context>
</contexts>
<marker>Shinozaki, Furui, 2001</marker>
<rawString>T. Shinozaki and S. Furui. 2001. Error analysis using decision trees in spontaneous presentation speech recognition. In Proc. of ASRU.</rawString>
</citation>
<citation valid="true">
<title>The Switchboard Transcription Project.</title>
<date>1996</date>
<note>http://www1.icsi.berkeley.edu/ Speech/stp/.</note>
<marker>1996</marker>
<rawString>1996. The Switchboard Transcription Project. http://www1.icsi.berkeley.edu/ Speech/stp/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Vitevitch</author>
<author>P A Luce</author>
</authors>
<title>Probabilistic phonotactics and neighborhood activation in spoken word recognition.</title>
<date>1999</date>
<journal>Journal of Memory and Language,</journal>
<volume>40</volume>
<issue>3</issue>
<contexts>
<context position="1367" citStr="Vitevitch and Luce, 1999" startWordPosition="201" endWordPosition="204">cantly better predictors of ASR errors than previously used neighborhood density measures. 1 Introduction An important pursuit for both human and machine speech recognition research is to understand the factors that affect word recognition accuracy. In the substantial body of work on human word recognition, it has been shown that it is harder to recognize words that have many “similar” neighboring words than words with few neighbors (Luce and Pisoni, 1998), and that frequent words are recognized faster and more accurately than are infrequent words (Marslen-Wilson, 1987; Luce and Pisoni, 1998; Vitevitch and Luce, 1999). In the ASR research community, prior work has also investigated various factors that benefit or disrupt recognition. Examples of such factors include word frequency, speaking rate, and prosodic factors (Fosler-Lussier and Morgan, 1999; Shinozaki and Furui, 2001; Hirschberg et al., 2004; Goldwater et al., 2010). There has also been prior work that uses word confusability measures to predict speech recognition errors (FoslerLussier et al., 2005; Jyothi and Fosler-Lussier, 2009). ∗Supported by a Beckman Postdoctoral Fellowship. Word neighborhood measures have been studied more heavily for human</context>
<context position="3587" citStr="Vitevitch and Luce, 1999" startWordPosition="559" endWordPosition="562">ood Density Measures In much of the prior work in the psycholinguistics literature, the notion of word similarity is quantified by a simple one-phone-away rule: A word w&apos; is a neighbor of word w if w and w&apos; differ by a single phone, via a substitution, deletion, or insertion. We refer to this density measure as “ND”. ND = � AND(w, w&apos;) W/ where AND(w, w&apos;) = 1 if w and w&apos; differ by a phone and 0 otherwise. The frequencies of the neighbors are often accounted for in the neighborhood density measure by computing the sum of the raw (or log) frequencies of a word’s neighbors (Luce and Pisoni, 1998; Vitevitch and Luce, 1999); the word frequencies 1 Proceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 1–9, Baltimore, Maryland USA, June 27 2014. c�2014 Association for Computational Linguistics are derived from a large corpus. We refer to this frequency-weighted measure as “wND”. �wND = DND(w, w0) · π(w0) w, where π(w0) is the frequency of the word w0.1 Both ND and wND are popular measures for word neighborhoods that we consider to be our baselines; Goldwater et al. (2010) also make use of these two density measures.2 Neither of these measures account for the frequency of the word itself. In continu</context>
</contexts>
<marker>Vitevitch, Luce, 1999</marker>
<rawString>M. S. Vitevitch and P. A. Luce. 1999. Probabilistic phonotactics and neighborhood activation in spoken word recognition. Journal of Memory and Language, 40(3):374–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Yarkoni</author>
<author>D Balota</author>
<author>M Yap</author>
</authors>
<title>Moving beyond Coltheart’s N: A new measure of orthographic similarity.</title>
<date>2008</date>
<journal>Psychonomic Bulletin &amp; Review,</journal>
<volume>15</volume>
<issue>5</issue>
<contexts>
<context position="5236" citStr="Yarkoni et al. (2008)" startWordPosition="837" endWordPosition="840">orough, 2012). In fact, the measure used by Scarborough (2012) is the reciprocal of rwND. 3 Proposed Neighborhood Measures Our new neighborhood measures are defined in terms of a distance function between a pair of words, D, and a weighting function, β. The proposed measures are not densities in the same sense as ND, wND, rwND, but are scores that we may expect to correlate with recognition errors. We define the neighborhood score for a word w as: �score(w) = β(w, w0) · D(w, w0) (1) w,6=w Intuitively, β is an averaging function that weighs the importance of each neighboring word. For example, Yarkoni et al. (2008) use a neighborhood measure that gives equal importance to the top 1Here we use raw rather than log frequencies. The baseline density measures in this section perform better with raw rather than log frequencies on our evaluation data. Our proposed measures perform significantly better than the baseline measures using both raw and log frequencies. 2Goldwater et al. (2010) also consider the number of homophones (words that share a pronunciation with the target word) and frequency-weighted homophones as additional neighborhood measures. In our data there is insufficient homophony for these measur</context>
<context position="7777" citStr="Yarkoni et al., 2008" startWordPosition="1258" endWordPosition="1261">hts equal to the corresponding substitution costs (mapping to or from epsilon indicates a deletion or an insertion). The weight of the shortest path in the composed FST, Fw o T o Fw,, gives the edit distance between w and w0. When either w or w0 has more than one pronunciation, DED is the minimum edit distance among all pairs of pronunciations. This edit distance function has been previously proposed as a measure of phonological similarity between words (Hahn and Bailey, 2005). Similar distance functions have also been used for neighborhood density measures in visual word recognition studies (Yarkoni et al., 2008). Simple Articulatory Feature-based Edit Distance (DAF): The distance function DED penalizes an incorrect substitution equally regardless of the phone identity; for example, the phone [p] can be substituted with [b] or [aa] with equal cost according to DED, although we know it is more likely for [p] to be produced as [b] than as [aa]. To account for this, we adopt a finer-grained representation of the phone as a vector of discrete articulatory “features”. Our features are derived from �rwND = w, 2 k:e/3.364 m:p/3.464 k:k/0 k ah m k aa p DAF : 0 1 2 3 0 0 0 0 1 2 3 Figure 1: Distance functions </context>
<context position="13350" citStr="Yarkoni et al., 2008" startWordPosition="2260" endWordPosition="2263"> we define the rank of w0 with respect to w, Rw(w0), as follows: Fix an ordering of all N − 1 words in the vocabulary other than w as (w1, w2, ... , wN−1) such that Δ(w, wi) &lt; Δ(w, wi+1) for all i E {1, ... , N − 21. Then Rw(w0) = j if w0 = wj in the above ordering. We then define β in terms of a “decay” function σ: Rw (w0 ) β(w, w0) = IR σ(r)dr (2) w(w0)−1 If σ is monotonically decreasing, Equation 2 ensures that neighbors with a higher rank (i.e., further away) contribute less weight than neighbors with a lower rank. For example, a measure that gives equal weight to the k closest neighbors (Yarkoni et al., 2008) corresponds to Instead of a step function that gives equal weight to all k neighbors, we define σ as an exponentially decreasing function of rank: σ(r) = e−r. Then, from Equation 2, we obtain β(w, w0) = (e−1)e−Rw(w0). Figure 2 shows the exponentially decreasing σ(r) and a sample β(w, w0). We know from prior work that it is also important to distinguish among the neighbors depending on how frequently they appear in the language. To account for this, we define a frequency-weighted rank function, Rηw(w0): ηw(wi) (3) where ηw is a suitably defined frequency function (see below). We now redefine β</context>
</contexts>
<marker>Yarkoni, Balota, Yap, 2008</marker>
<rawString>T. Yarkoni, D. Balota, and M. Yap. 2008. Moving beyond Coltheart’s N: A new measure of orthographic similarity. Psychonomic Bulletin &amp; Review, 15(5):971–979.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
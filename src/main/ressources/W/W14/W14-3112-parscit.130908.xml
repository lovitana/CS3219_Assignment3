<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021992">
<title confidence="0.995949">
Concurrent Visualization of Relationships between Words and Topics in
Topic Models
</title>
<author confidence="0.999701">
Alison Smith∗, Jason Chuang†, Yuening Hu∗, Jordan Boyd-Graber∗, Leah Findlater∗
</author>
<affiliation confidence="0.9992865">
∗University of Maryland, College Park, MD
†University of Washington, Seattle, WA
</affiliation>
<email confidence="0.993647">
amsmit@cs.umd.edu, jcchuang@cs.washington.edu, ynhu@cs.umd.edu, jbg@umiacs.umd.edu, leahkf@umd.edu
</email>
<sectionHeader confidence="0.99378" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999157294117647">
Analysis tools based on topic models are
often used as a means to explore large
amounts of unstructured data. Users of-
ten reason about the correctness of a model
using relationships between words within
the topics or topics within the model. We
compute this useful contextual informa-
tion as term co-occurrence and topic co-
variance and overlay it on top of stan-
dard topic model output via an intuitive
interactive visualization. This is a work
in progress with the end goal to combine
the visual representation with interactions
and online learning, so the users can di-
rectly explore (a) why a model may not
align with their intuition and (b) modify
the model as needed.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996951724138">
Topic modeling is a popular technique for analyz-
ing large text corpora. A user is unlikely to have
the time required to understand and exploit the raw
results of topic modeling for analysis of a corpus.
Therefore, an interesting and intuitive visualiza-
tion is required for a topic model to provide added
value. A common topic modeling technique is La-
tent Dirichlet Allocation (LDA) (Blei et al., 2003),
which is an unsupervised algorithm for perform-
ing statistical topic modeling that uses a “bag of
words” approach. The resulting topic model repre-
sents the corpus as an unrelated set of topics where
each topic is a probability distribution over words.
Experienced users who have worked with a text
corpus for an extended period of time often think
of the thematic relationships in the corpus in terms
of higher-level statistics such as (a) inter-topic cor-
relations or (b) word correlations. However, stan-
dard topic models do not explicitly provide such
contextual information to the users.
Existing tools based on topic models, such
as Topical Guide (Gardner et al., 2010), Top-
icViz (Eisenstein et al., 2012), and the topic vi-
sualization of (Chaney and Blei, 2012) support
topic-based corpus browsing and understanding.
Visualizations of this type typically represent stan-
dard topic models as a sea of word clouds; the in-
dividual topics within the model are presented as
an unordered set of word clouds — or something
similar — of the top words for the topic1 where
word size is proportional to the probability of the
word for the topic. A primary issue with word
clouds is that they can hinder understanding (Har-
ris, 2011) due to the fact that they lack information
about the relationships between words. Addition-
ally, topic model visualizations that display topics
in a random layout can lead to a huge, inefficiently
organized search space, which is not always help-
ful in providing a quick corpus overview or assist-
ing the user to diagnose possible problems with
the model.
The authors of Correlated Topic Models (CTM)
(Lafferty and Blei, 2006) recognize the limitation
of existing topic models to directly model the cor-
relation between topics, and present an alterna-
tive algorithm, CTM, which models the correla-
tion between topics discovered for a corpus by us-
ing a more flexible distribution for the topic pro-
portions in the model. Topical n-gram models
(TNG) (Wang et al., 2007) discover phrases in
addition to topics. TNG is a probabilistic model
which assigns words and n-grams based on sur-
rounding context, instead of for all references in
the corpus. These models independently account
for the two limitations of statistical topic modeling
discussed in this paper by modifying the underly-
ing topic modeling algorithm. Our work aims to
provide a low-cost method for incorporating this
</bodyText>
<footnote confidence="0.969361">
1This varies, but typically is either the top 10 to 20 words
or the number of words which hold a specific portion of the
distribution weight.
</footnote>
<page confidence="0.981646">
79
</page>
<bodyText confidence="0.904677">
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 79–82,
Baltimore, Maryland, USA, June 27, 2014. @c 2014 Association for Computational Linguistics
information as well as visualizing it in an effec-
tive way. We compute summary statistics, term
co-occurrence and topic covariance, which can be
overlaid on top of any traditional topic model. As
a number of application-specific LDA implemen-
tations exist, we propose a meta-technique which
can be applied to any underlying algorithm.
We present a relationship-enriched visualiza-
tion to help users explore topic models through
word and topic correlations. We propose inter-
actions to support user understanding, validation,
and refinement of the models.
</bodyText>
<sectionHeader confidence="0.9122375" genericHeader="method">
2 Group-in-a-box Layout for Visualizing
a Relationship-Enriched Topic Model
</sectionHeader>
<bodyText confidence="0.999958875">
Existing topic model visualizations do not eas-
ily support displaying the relationships between
words in the topics and topics in the model. In-
stead, this requires a layout that supports intuitive
visualization of nested network graphs. A group-
in-a-box (GIB) layout (Rodrigues et al., 2011) is a
network graph visualization that is ideal for our
scenario as it is typically used for representing
clusters with emphasis on the edges within and
between clusters. The GIB layout visualizes sub-
graphs within a graph using a Treemap (Shneider-
man, 1998) space filling technique and layout al-
gorithms for optimizing the layout of sub-graphs
within the space, such that related sub-graphs are
placed together spatially. Figure 1 shows a sample
group-in-a-box visualization.
We use the GIB layout to visually separate top-
ics of the model as groups. We implement each
topic as a force-directed network graph (Fruchter-
man and Reingold, 1991) where the nodes of the
graph are the top words of the topic. An edge ex-
ists between two words in the network graph if
the value of the term co-occurrence for the word
pair is above a certain threshold,2 and the edge is
weighted by this value. Similarly, the edges be-
tween the topic clusters represent the topic covari-
ance metric. Finally, the GIB layout optimizes the
visualization such that related topic clusters are
placed together spatially. The result is a topic visu-
alization where related words are clustered within
the topics and related topics are clustered within
the overall layout.
</bodyText>
<footnote confidence="0.9615045">
2There are a variety of techniques for setting this thresh-
old; currently, we aim to display fewer, stronger relationships
to balance informativeness and complexity of the visualiza-
tion
</footnote>
<figureCaption confidence="0.90357075">
Figure 1: A sample GIB layout from (Rodrigues
et al., 2011). The layout visualizes clusters dis-
tributed in a treemap structure where the partitions
are based on the size of the clusters.
</figureCaption>
<sectionHeader confidence="0.998785" genericHeader="method">
3 Relationship Metrics
</sectionHeader>
<bodyText confidence="0.999991285714286">
We compute the term and topic relationship in-
formation required by the GIB layout as term
co-occurrence and topic covariance, respectively.
Term co-occurrence is a corpus-level statistic that
can be computed independently from the LDA al-
gorithm. The results of the LDA algorithm are re-
quired to compute the topic covariance.
</bodyText>
<subsectionHeader confidence="0.998253">
3.1 Corpus-Level Term Co-Occurrence
</subsectionHeader>
<bodyText confidence="0.9999295">
Prior work has shown that Pointwise Mutual
Information (PMI) is the most consistent scor-
ing method for evaluating topic model coher-
ence (Newman et al., 2010). PMI is a statistical
technique for measuring the association between
two observations. For our purposes, PMI is used
to measure the correlation between each term pair
within each topic on the document level3. The
PMI is calculated for every possible term pair in
the ingested data set using Equation 1. The visu-
alization uses only the PMI for the term pairs for
the top terms for each topic, which is a small sub-
set of the calculated PMI values. Computing the
PMI is trivial compared to the LDA calculation,
and computing the values for all pairs allows the
job to be run in parallel, as opposed to waiting for
the results of the LDA job to determine the top
term pairs.
</bodyText>
<equation confidence="0.9924855">
PMI(x, y) = log p(x, y) (1)
p(x)p(y)
</equation>
<bodyText confidence="0.9937745">
The PMI measure represents the probability of
observing x given y and vice-versa. PMI can be
</bodyText>
<footnote confidence="0.579215">
3We use document here, but the PMI can be computed at
various levels of granularity as required by the analyst intent.
</footnote>
<page confidence="0.996504">
80
</page>
<bodyText confidence="0.999948">
positive or negative, where 0 represents indepen-
dence, and PMI is at its maximum when x and y
are perfectly associated.
</bodyText>
<subsectionHeader confidence="0.998996">
3.2 Topic Covariance
</subsectionHeader>
<bodyText confidence="0.999754555555555">
To quantify the relationship between topics in the
model, we calculate the topic covariance metric
for each pair of topics. To do this, we use the
theta vector from the LDA output. The theta vec-
tor describes which topics are used for which doc-
uments in the model, where theta(d,i) represents
how much the ith topic is expressed in document
d. The equations for calculation the topic covari-
ance are shown below.
</bodyText>
<equation confidence="0.998396285714286">
γdi = Ej (θdj) (2)
θdi
γi = 1 D (γdi) (3)
d
σ(i, j) =
D
1 (γdi − γi)(γdj − γj)) (4)
</equation>
<bodyText confidence="0.57885">
d
</bodyText>
<sectionHeader confidence="0.994669" genericHeader="evaluation">
4 Visualization
</sectionHeader>
<bodyText confidence="0.999962083333333">
The visualization represents the individual topics
as network graphs where nodes represent terms
and edges represent frequent term co-occurrence,
and the layout of the topics represents topic co-
variance. The most connected topic is placed in
the center of the layout, and the least connected
topics are placed at the corners. Figure 2 shows
the visualization for a topic model generated for
a 1,000 document NSF dataset. As demonstrated
in Figure 3, a user can hover over a topic to see
the related topics4. In this example, the user has
hovered over the {visualization, visual, interac-
tive} topic, which is related to {user, interfaces},
{human, computer, interaction}, {design, tools},
and {digital, data, web} among others. Unlike
other topical similarity measures, such as cosine
similarity or a count of shared words, the topic co-
variance represents topics which are typically dis-
cussed together in the same documents, helping
the user to discover semantically similar topics.
On the topic level, the size of the node in the
topic network graph represents the probability of
the word given the topic. By mapping word proba-
bility to the area of the nodes instead of the height
</bodyText>
<footnote confidence="0.989976">
4we consider topics related if the topic co-occurrence is
above a certain pre-defined threshold.
</footnote>
<figureCaption confidence="0.972270333333333">
Figure 2: The visualization utilizes a group-in-a-
box-inspired layout to represent the topic model as
a nested network graph.
</figureCaption>
<bodyText confidence="0.99993024">
of words, the resulting visual encoding is not af-
fected by the length of the words, a well-known
issue with word cloud presentations that can visu-
ally bias longer terms. Furthermore, circles can
overlap without affecting a user’s ability to visu-
ally separate them, and lead to more compact and
less cluttered visual layout. Hovering over a word
node highlights the same word in other topics as
shown in Figure 4.
This visualization is an alternative interface
for Interactive Topic Modeling (ITM) (Hu et al.,
2013). ITM presents users with topics that can be
modified as appropriate. Our preliminary results
show that topics containing highly-weighted sub-
clusters may be candidates for splitting, whereas
positively correlated topics are likely to be good
topics, which do not need to be modified. In fu-
ture work, we intend to perform an evaluation to
show that this visualization enhances quality and
efficiency of the ITM process.
To support user interactions required by the
ITM algorithm, the visualization has an edit mode,
which is shown in Figure 5. Ongoing work in-
cludes developing appropriate visual operations to
support the following model-editing operations:
</bodyText>
<listItem confidence="0.991356">
1. Adding words to a topic
2. Removing words from a topic
3. Requiring two words to be linked within a
topic (must link)
4. Requiring two words to be forced into sepa-
rate topics (cannot link)
</listItem>
<sectionHeader confidence="0.978532" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9991445">
The visualization presented here provides a novel
way to explore topic models with incorporated
</bodyText>
<page confidence="0.998049">
81
</page>
<figureCaption confidence="0.926995583333333">
Figure 3: The user has hovered over the most-
central topic in the layout, which is the most con-
nected topic. The hovered topic is outlined, and
the topic name is highlighted in turquoise. The
topic names of the related topics are also high-
lighted.
Figure 4: The visualization where the user has
hovered over a word of interest. The same word
is highlighted turquoise in other topics.
Figure 5: The edit mode for the visualization.
From this mode, the user can add words, remove
words, or rename the topic.
</figureCaption>
<bodyText confidence="0.999887111111111">
term and topic correlation information. This is a
work in progress with the end goal to combine the
visual representation with interactive topic mod-
eling to allow users to explore (a) why a model
may not align with their intuition and (b) modify
the model as needed. We plan to deploy the tool
on real-world domain users to iteratively refine the
visualization and evaluate it in ecologically valid
settings.
</bodyText>
<sectionHeader confidence="0.999106" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999908677419355">
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet
allocation. Machine Learning Journal, 3:993–1022.
Allison June-Barlow Chaney and David M Blei. 2012. Visualizing topic mod-
els. In ICWSM.
Jacob Eisenstein, Duen Horng Chau, Aniket Kittur, and Eric Xing. 2012. Top-
icviz: interactive topic exploration in document collections. In CHI’12
Extended Abstracts, pages 2177–2182. ACM.
Thomas MJ Fruchterman and Edward M Reingold. 1991. Graph draw-
ing by force-directed placement. Software: Practice and experience,
21(11):1129–1164.
Matthew J Gardner, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric
Ringger, and Kevin Seppi. 2010. The topic browser: An interactive tool
for browsing topic models. In NIPS Workshop on Challenges of Data Vi-
sualization.
Jacon Harris. 2011. Word clouds considered harm-
ful. http://www.niemanlab.org/2011/10/
word-clouds-considered-harmful/.
Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff, and Alison Smith. 2013.
Interactive topic modeling. Machine Learning, pages 1–47.
JD Lafferty and MD Blei. 2006. Correlated topic models. In NIPS, Proceed-
ings of the 2005 conference, pages 147–155. Citeseer.
David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Au-
tomatic evaluation of topic coherence. In HLT, pages 100–108. ACL.
Eduarda Mendes Rodrigues, Natasa Milic-Frayling, Marc Smith, Ben Shnei-
derman, and Derek Hansen. 2011. Group-in-a-box layout for multi-
faceted analysis of communities. In ICSM, pages 354–361. IEEE.
Ben Shneiderman. 1998. Treemaps for space-constrained visualization of hi-
erarchies.
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007. Topical n-grams:
Phrase and topic discovery, with an application to information retrieval. In
ICDM, pages 697–702. IEEE.
</reference>
<page confidence="0.99913">
82
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.793397">
<title confidence="0.997585">Concurrent Visualization of Relationships between Words and Topics in Topic Models</title>
<author confidence="0.999976">Jason Yuening Jordan Leah</author>
<affiliation confidence="0.842786">of Maryland, College Park,</affiliation>
<address confidence="0.976766">of Washington, Seattle,</address>
<email confidence="0.999931">amsmit@cs.umd.edu,jcchuang@cs.washington.edu,ynhu@cs.umd.edu,jbg@umiacs.umd.edu,leahkf@umd.edu</email>
<abstract confidence="0.997475944444445">Analysis tools based on topic models are often used as a means to explore large amounts of unstructured data. Users often reason about the correctness of a model using relationships between words within the topics or topics within the model. We compute this useful contextual information as term co-occurrence and topic covariance and overlay it on top of standard topic model output via an intuitive interactive visualization. This is a work in progress with the end goal to combine the visual representation with interactions and online learning, so the users can directly explore (a) why a model may not align with their intuition and (b) modify the model as needed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Machine Learning Journal,</journal>
<pages>3--993</pages>
<contexts>
<context position="1438" citStr="Blei et al., 2003" startWordPosition="219" endWordPosition="222">goal to combine the visual representation with interactions and online learning, so the users can directly explore (a) why a model may not align with their intuition and (b) modify the model as needed. 1 Introduction Topic modeling is a popular technique for analyzing large text corpora. A user is unlikely to have the time required to understand and exploit the raw results of topic modeling for analysis of a corpus. Therefore, an interesting and intuitive visualization is required for a topic model to provide added value. A common topic modeling technique is Latent Dirichlet Allocation (LDA) (Blei et al., 2003), which is an unsupervised algorithm for performing statistical topic modeling that uses a “bag of words” approach. The resulting topic model represents the corpus as an unrelated set of topics where each topic is a probability distribution over words. Experienced users who have worked with a text corpus for an extended period of time often think of the thematic relationships in the corpus in terms of higher-level statistics such as (a) inter-topic correlations or (b) word correlations. However, standard topic models do not explicitly provide such contextual information to the users. Existing </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Machine Learning Journal, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allison June-Barlow Chaney</author>
<author>David M Blei</author>
</authors>
<title>Visualizing topic models.</title>
<date>2012</date>
<booktitle>In ICWSM.</booktitle>
<contexts>
<context position="2203" citStr="Chaney and Blei, 2012" startWordPosition="344" endWordPosition="347">epresents the corpus as an unrelated set of topics where each topic is a probability distribution over words. Experienced users who have worked with a text corpus for an extended period of time often think of the thematic relationships in the corpus in terms of higher-level statistics such as (a) inter-topic correlations or (b) word correlations. However, standard topic models do not explicitly provide such contextual information to the users. Existing tools based on topic models, such as Topical Guide (Gardner et al., 2010), TopicViz (Eisenstein et al., 2012), and the topic visualization of (Chaney and Blei, 2012) support topic-based corpus browsing and understanding. Visualizations of this type typically represent standard topic models as a sea of word clouds; the individual topics within the model are presented as an unordered set of word clouds — or something similar — of the top words for the topic1 where word size is proportional to the probability of the word for the topic. A primary issue with word clouds is that they can hinder understanding (Harris, 2011) due to the fact that they lack information about the relationships between words. Additionally, topic model visualizations that display topi</context>
</contexts>
<marker>Chaney, Blei, 2012</marker>
<rawString>Allison June-Barlow Chaney and David M Blei. 2012. Visualizing topic models. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
</authors>
<title>Duen Horng Chau, Aniket Kittur, and Eric Xing.</title>
<date>2012</date>
<booktitle>In CHI’12 Extended Abstracts,</booktitle>
<pages>2177--2182</pages>
<publisher>ACM.</publisher>
<marker>Eisenstein, 2012</marker>
<rawString>Jacob Eisenstein, Duen Horng Chau, Aniket Kittur, and Eric Xing. 2012. Topicviz: interactive topic exploration in document collections. In CHI’12 Extended Abstracts, pages 2177–2182. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas MJ Fruchterman</author>
<author>Edward M Reingold</author>
</authors>
<title>Graph drawing by force-directed placement. Software: Practice and experience,</title>
<date>1991</date>
<pages>21--11</pages>
<contexts>
<context position="5731" citStr="Fruchterman and Reingold, 1991" startWordPosition="903" endWordPosition="907">work graph visualization that is ideal for our scenario as it is typically used for representing clusters with emphasis on the edges within and between clusters. The GIB layout visualizes subgraphs within a graph using a Treemap (Shneiderman, 1998) space filling technique and layout algorithms for optimizing the layout of sub-graphs within the space, such that related sub-graphs are placed together spatially. Figure 1 shows a sample group-in-a-box visualization. We use the GIB layout to visually separate topics of the model as groups. We implement each topic as a force-directed network graph (Fruchterman and Reingold, 1991) where the nodes of the graph are the top words of the topic. An edge exists between two words in the network graph if the value of the term co-occurrence for the word pair is above a certain threshold,2 and the edge is weighted by this value. Similarly, the edges between the topic clusters represent the topic covariance metric. Finally, the GIB layout optimizes the visualization such that related topic clusters are placed together spatially. The result is a topic visualization where related words are clustered within the topics and related topics are clustered within the overall layout. 2Ther</context>
</contexts>
<marker>Fruchterman, Reingold, 1991</marker>
<rawString>Thomas MJ Fruchterman and Edward M Reingold. 1991. Graph drawing by force-directed placement. Software: Practice and experience, 21(11):1129–1164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew J Gardner</author>
<author>Joshua Lutes</author>
<author>Jeff Lund</author>
<author>Josh Hansen</author>
<author>Dan Walker</author>
<author>Eric Ringger</author>
<author>Kevin Seppi</author>
</authors>
<title>The topic browser: An interactive tool for browsing topic models.</title>
<date>2010</date>
<booktitle>In NIPS Workshop on Challenges of Data Visualization.</booktitle>
<contexts>
<context position="2111" citStr="Gardner et al., 2010" startWordPosition="328" endWordPosition="331">statistical topic modeling that uses a “bag of words” approach. The resulting topic model represents the corpus as an unrelated set of topics where each topic is a probability distribution over words. Experienced users who have worked with a text corpus for an extended period of time often think of the thematic relationships in the corpus in terms of higher-level statistics such as (a) inter-topic correlations or (b) word correlations. However, standard topic models do not explicitly provide such contextual information to the users. Existing tools based on topic models, such as Topical Guide (Gardner et al., 2010), TopicViz (Eisenstein et al., 2012), and the topic visualization of (Chaney and Blei, 2012) support topic-based corpus browsing and understanding. Visualizations of this type typically represent standard topic models as a sea of word clouds; the individual topics within the model are presented as an unordered set of word clouds — or something similar — of the top words for the topic1 where word size is proportional to the probability of the word for the topic. A primary issue with word clouds is that they can hinder understanding (Harris, 2011) due to the fact that they lack information about</context>
</contexts>
<marker>Gardner, Lutes, Lund, Hansen, Walker, Ringger, Seppi, 2010</marker>
<rawString>Matthew J Gardner, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger, and Kevin Seppi. 2010. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Challenges of Data Visualization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacon Harris</author>
</authors>
<title>Word clouds considered harmful.</title>
<date>2011</date>
<note>http://www.niemanlab.org/2011/10/ word-clouds-considered-harmful/.</note>
<contexts>
<context position="2662" citStr="Harris, 2011" startWordPosition="424" endWordPosition="426"> on topic models, such as Topical Guide (Gardner et al., 2010), TopicViz (Eisenstein et al., 2012), and the topic visualization of (Chaney and Blei, 2012) support topic-based corpus browsing and understanding. Visualizations of this type typically represent standard topic models as a sea of word clouds; the individual topics within the model are presented as an unordered set of word clouds — or something similar — of the top words for the topic1 where word size is proportional to the probability of the word for the topic. A primary issue with word clouds is that they can hinder understanding (Harris, 2011) due to the fact that they lack information about the relationships between words. Additionally, topic model visualizations that display topics in a random layout can lead to a huge, inefficiently organized search space, which is not always helpful in providing a quick corpus overview or assisting the user to diagnose possible problems with the model. The authors of Correlated Topic Models (CTM) (Lafferty and Blei, 2006) recognize the limitation of existing topic models to directly model the correlation between topics, and present an alternative algorithm, CTM, which models the correlation bet</context>
</contexts>
<marker>Harris, 2011</marker>
<rawString>Jacon Harris. 2011. Word clouds considered harmful. http://www.niemanlab.org/2011/10/ word-clouds-considered-harmful/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuening Hu</author>
<author>Jordan Boyd-Graber</author>
<author>Brianna Satinoff</author>
<author>Alison Smith</author>
</authors>
<title>Interactive topic modeling.</title>
<date>2013</date>
<booktitle>Machine Learning,</booktitle>
<pages>1--47</pages>
<contexts>
<context position="10735" citStr="Hu et al., 2013" startWordPosition="1751" endWordPosition="1754">tion utilizes a group-in-abox-inspired layout to represent the topic model as a nested network graph. of words, the resulting visual encoding is not affected by the length of the words, a well-known issue with word cloud presentations that can visually bias longer terms. Furthermore, circles can overlap without affecting a user’s ability to visually separate them, and lead to more compact and less cluttered visual layout. Hovering over a word node highlights the same word in other topics as shown in Figure 4. This visualization is an alternative interface for Interactive Topic Modeling (ITM) (Hu et al., 2013). ITM presents users with topics that can be modified as appropriate. Our preliminary results show that topics containing highly-weighted subclusters may be candidates for splitting, whereas positively correlated topics are likely to be good topics, which do not need to be modified. In future work, we intend to perform an evaluation to show that this visualization enhances quality and efficiency of the ITM process. To support user interactions required by the ITM algorithm, the visualization has an edit mode, which is shown in Figure 5. Ongoing work includes developing appropriate visual opera</context>
</contexts>
<marker>Hu, Boyd-Graber, Satinoff, Smith, 2013</marker>
<rawString>Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff, and Alison Smith. 2013. Interactive topic modeling. Machine Learning, pages 1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JD Lafferty</author>
<author>MD Blei</author>
</authors>
<title>Correlated topic models.</title>
<date>2006</date>
<booktitle>In NIPS, Proceedings of the 2005 conference,</booktitle>
<pages>147--155</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="3086" citStr="Lafferty and Blei, 2006" startWordPosition="492" endWordPosition="495">ar — of the top words for the topic1 where word size is proportional to the probability of the word for the topic. A primary issue with word clouds is that they can hinder understanding (Harris, 2011) due to the fact that they lack information about the relationships between words. Additionally, topic model visualizations that display topics in a random layout can lead to a huge, inefficiently organized search space, which is not always helpful in providing a quick corpus overview or assisting the user to diagnose possible problems with the model. The authors of Correlated Topic Models (CTM) (Lafferty and Blei, 2006) recognize the limitation of existing topic models to directly model the correlation between topics, and present an alternative algorithm, CTM, which models the correlation between topics discovered for a corpus by using a more flexible distribution for the topic proportions in the model. Topical n-gram models (TNG) (Wang et al., 2007) discover phrases in addition to topics. TNG is a probabilistic model which assigns words and n-grams based on surrounding context, instead of for all references in the corpus. These models independently account for the two limitations of statistical topic modeli</context>
</contexts>
<marker>Lafferty, Blei, 2006</marker>
<rawString>JD Lafferty and MD Blei. 2006. Correlated topic models. In NIPS, Proceedings of the 2005 conference, pages 147–155. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic evaluation of topic coherence.</title>
<date>2010</date>
<booktitle>In HLT,</booktitle>
<pages>100--108</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="7239" citStr="Newman et al., 2010" startWordPosition="1150" endWordPosition="1153"> treemap structure where the partitions are based on the size of the clusters. 3 Relationship Metrics We compute the term and topic relationship information required by the GIB layout as term co-occurrence and topic covariance, respectively. Term co-occurrence is a corpus-level statistic that can be computed independently from the LDA algorithm. The results of the LDA algorithm are required to compute the topic covariance. 3.1 Corpus-Level Term Co-Occurrence Prior work has shown that Pointwise Mutual Information (PMI) is the most consistent scoring method for evaluating topic model coherence (Newman et al., 2010). PMI is a statistical technique for measuring the association between two observations. For our purposes, PMI is used to measure the correlation between each term pair within each topic on the document level3. The PMI is calculated for every possible term pair in the ingested data set using Equation 1. The visualization uses only the PMI for the term pairs for the top terms for each topic, which is a small subset of the calculated PMI values. Computing the PMI is trivial compared to the LDA calculation, and computing the values for all pairs allows the job to be run in parallel, as opposed to</context>
</contexts>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic evaluation of topic coherence. In HLT, pages 100–108. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduarda Mendes Rodrigues</author>
<author>Natasa Milic-Frayling</author>
<author>Marc Smith</author>
<author>Ben Shneiderman</author>
<author>Derek Hansen</author>
</authors>
<title>Group-in-a-box layout for multifaceted analysis of communities.</title>
<date>2011</date>
<booktitle>In ICSM,</booktitle>
<pages>354--361</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5091" citStr="Rodrigues et al., 2011" startWordPosition="801" endWordPosition="804">be applied to any underlying algorithm. We present a relationship-enriched visualization to help users explore topic models through word and topic correlations. We propose interactions to support user understanding, validation, and refinement of the models. 2 Group-in-a-box Layout for Visualizing a Relationship-Enriched Topic Model Existing topic model visualizations do not easily support displaying the relationships between words in the topics and topics in the model. Instead, this requires a layout that supports intuitive visualization of nested network graphs. A groupin-a-box (GIB) layout (Rodrigues et al., 2011) is a network graph visualization that is ideal for our scenario as it is typically used for representing clusters with emphasis on the edges within and between clusters. The GIB layout visualizes subgraphs within a graph using a Treemap (Shneiderman, 1998) space filling technique and layout algorithms for optimizing the layout of sub-graphs within the space, such that related sub-graphs are placed together spatially. Figure 1 shows a sample group-in-a-box visualization. We use the GIB layout to visually separate topics of the model as groups. We implement each topic as a force-directed networ</context>
<context position="6570" citStr="Rodrigues et al., 2011" startWordPosition="1045" endWordPosition="1048"> is weighted by this value. Similarly, the edges between the topic clusters represent the topic covariance metric. Finally, the GIB layout optimizes the visualization such that related topic clusters are placed together spatially. The result is a topic visualization where related words are clustered within the topics and related topics are clustered within the overall layout. 2There are a variety of techniques for setting this threshold; currently, we aim to display fewer, stronger relationships to balance informativeness and complexity of the visualization Figure 1: A sample GIB layout from (Rodrigues et al., 2011). The layout visualizes clusters distributed in a treemap structure where the partitions are based on the size of the clusters. 3 Relationship Metrics We compute the term and topic relationship information required by the GIB layout as term co-occurrence and topic covariance, respectively. Term co-occurrence is a corpus-level statistic that can be computed independently from the LDA algorithm. The results of the LDA algorithm are required to compute the topic covariance. 3.1 Corpus-Level Term Co-Occurrence Prior work has shown that Pointwise Mutual Information (PMI) is the most consistent scor</context>
</contexts>
<marker>Rodrigues, Milic-Frayling, Smith, Shneiderman, Hansen, 2011</marker>
<rawString>Eduarda Mendes Rodrigues, Natasa Milic-Frayling, Marc Smith, Ben Shneiderman, and Derek Hansen. 2011. Group-in-a-box layout for multifaceted analysis of communities. In ICSM, pages 354–361. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Shneiderman</author>
</authors>
<title>Treemaps for space-constrained visualization of hierarchies.</title>
<date>1998</date>
<contexts>
<context position="5348" citStr="Shneiderman, 1998" startWordPosition="845" endWordPosition="847">oup-in-a-box Layout for Visualizing a Relationship-Enriched Topic Model Existing topic model visualizations do not easily support displaying the relationships between words in the topics and topics in the model. Instead, this requires a layout that supports intuitive visualization of nested network graphs. A groupin-a-box (GIB) layout (Rodrigues et al., 2011) is a network graph visualization that is ideal for our scenario as it is typically used for representing clusters with emphasis on the edges within and between clusters. The GIB layout visualizes subgraphs within a graph using a Treemap (Shneiderman, 1998) space filling technique and layout algorithms for optimizing the layout of sub-graphs within the space, such that related sub-graphs are placed together spatially. Figure 1 shows a sample group-in-a-box visualization. We use the GIB layout to visually separate topics of the model as groups. We implement each topic as a force-directed network graph (Fruchterman and Reingold, 1991) where the nodes of the graph are the top words of the topic. An edge exists between two words in the network graph if the value of the term co-occurrence for the word pair is above a certain threshold,2 and the edge </context>
</contexts>
<marker>Shneiderman, 1998</marker>
<rawString>Ben Shneiderman. 1998. Treemaps for space-constrained visualization of hierarchies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuerui Wang</author>
<author>Andrew McCallum</author>
<author>Xing Wei</author>
</authors>
<title>Topical n-grams: Phrase and topic discovery, with an application to information retrieval.</title>
<date>2007</date>
<booktitle>In ICDM,</booktitle>
<pages>697--702</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3423" citStr="Wang et al., 2007" startWordPosition="548" endWordPosition="551"> in a random layout can lead to a huge, inefficiently organized search space, which is not always helpful in providing a quick corpus overview or assisting the user to diagnose possible problems with the model. The authors of Correlated Topic Models (CTM) (Lafferty and Blei, 2006) recognize the limitation of existing topic models to directly model the correlation between topics, and present an alternative algorithm, CTM, which models the correlation between topics discovered for a corpus by using a more flexible distribution for the topic proportions in the model. Topical n-gram models (TNG) (Wang et al., 2007) discover phrases in addition to topics. TNG is a probabilistic model which assigns words and n-grams based on surrounding context, instead of for all references in the corpus. These models independently account for the two limitations of statistical topic modeling discussed in this paper by modifying the underlying topic modeling algorithm. Our work aims to provide a low-cost method for incorporating this 1This varies, but typically is either the top 10 to 20 words or the number of words which hold a specific portion of the distribution weight. 79 Proceedings of the Workshop on Interactive La</context>
</contexts>
<marker>Wang, McCallum, Wei, 2007</marker>
<rawString>Xuerui Wang, Andrew McCallum, and Xing Wei. 2007. Topical n-grams: Phrase and topic discovery, with an application to information retrieval. In ICDM, pages 697–702. IEEE.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
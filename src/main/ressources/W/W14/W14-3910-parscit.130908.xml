<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.990437">
Language Identification in Code-Switching Scenario
</title>
<author confidence="0.834853">
Naman Jain
</author>
<affiliation confidence="0.544823">
LTRC, IIIT-H, Hyderabad, India
</affiliation>
<email confidence="0.833498">
naman.jain@research.iiit.ac.in
</email>
<note confidence="0.6252145">
Riyaz Ahmad Bhat
LTRC, IIIT-H, Hyderabad, India
</note>
<email confidence="0.923576">
riyaz.bhat@research.iiit.ac.in
</email>
<sectionHeader confidence="0.996605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999867230769231">
This paper describes a CRF based token
level language identification system en-
try to Language Identification in Code-
Switched (CS) Data task of CodeSwitch
2014. Our system hinges on using con-
ditional posterior probabilities for the in-
dividual codes (words) in code-switched
data to solve the language identification
task. We also experiment with other lin-
guistically motivated language specific as
well as generic features to train the CRF
based sequence labeling algorithm achiev-
ing reasonable results.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947714285714">
This paper describes our participation in the Lan-
guage Identification in Code-Switched Data task
at CodeSwitch 2014 (Solorio et al., 2014). The
workshop focuses on NLP approaches for the
analysis and processing of mixed-language data
with a focus on intra sentential code-switching,
while the shared task focuses on the identifica-
tion of the language of each word in a code-
switched data, which is a prerequisite for ana-
lyzing/processing such data. Code-switching is a
sociolinguistics phenomenon, where multilingual
speakers switch back and forth between two or
more common languages or language-varieties,
in the context of a single written or spoken
conversation. Natural language analysis of code-
switched (henceforth CS) data for various NLP
tasks like Parsing, Machine Translation (MT), Au-
tomatic Speech Recognition (ASR), Information
Retrieval (IR) and Extraction (IE) and Semantic
Processing, is more complex than monolingual
data. Traditional NLP techniques perform miser-
ably when processing mixed language data. The
performance degrades at a rate proportional to the
amount and level of code-switching present in the
data. Therefore, in order to process such data,
a separate language identification component is
needed, to first identify the language of individual
words.
Language identification in code-switched data
can be thought of as a sub-task of a document
level language identification task. The latter aims
to identify the language a given document is writ-
ten in (Baldwin and Lui, 2010), while the former
addresses the same problem, however at the token
level. Although, both the problems have separate
goals, they can fundamentally be modeled with a
similar set of features and techniques. However,
language identification at the word level is more
challenging than a typical document level lan-
guage identification problem. The number of fea-
tures available at document level is much higher
than at word level. The available features for word
level identification are word morphology, syllable
structure and phonemic (letter) inventory of the
language(s). Since these features are related to the
structure of a word, letter based n-gram models
have been reported to give reasonably accurate and
comparable results (Dunning, 1994; Elfardy and
Diab, 2012; King and Abney, 2013; Nguyen and
Dogruoz, 2014; Lui et al., 2014). In this work, we
present a token level language identification sys-
tem which mainly hinges on the posterior prob-
abilities computed using n-gram based language
models.
The rest of the paper is organized as follows: In
Section 2, we discuss about the data of the shared
task. In Section 3, we discuss the methodology
we adapted to address the problem of language
identification, in detail. Experiments based on our
methodology are discussed in Section 4. In Sec-
tion 5, we present the results obtained, with a brief
discussion. Finally we conclude in Section 6 with
some future directions.
</bodyText>
<page confidence="0.990185">
87
</page>
<note confidence="0.324286">
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 87–93,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.987712" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.996866769230769">
The Language Identification in the Code-Switched
(CS) data shared task is meant for language
identification in 4 language pairs (henceforth
LP) namely, Nepali-English (N-E), Spanish-
English (S-E), Mandarin-English (M-E) and Mod-
ern Standard Arabic-Arabic dialects (MSA-A). So
as to get familiar with the training and testing data,
trial data sets consisting of 20 tweets each, corre-
sponding to all the language-pairs, were first re-
leased. Additional test data as “surprise genre” for
S-E, N-E and MSA-A were also released, which
comprised of data from Facebook, blogs and Ara-
bic commentaries.
</bodyText>
<subsectionHeader confidence="0.98707">
2.1 Tag Description
</subsectionHeader>
<bodyText confidence="0.999428714285714">
Each word in the training data is classified into
one of the 6 different classes which are, Lang1,
Lang2, Mixed, Other, Ambiguous and NE.
“Lang1” and “Lang2” tags correspond to words
specific to the languages in an LP. “Mixed” words
are those words that are partially in both the lan-
guages. “Ambiguous” words are the ones that
could belong to either of the language. All gib-
berish and unintelligible words and words that
do not belong to any of the languages fall under
“Other” category. “Named Entities” (NE) com-
prise of proper names that refer to people, places,
organizations, locations, movie titles and song ti-
tles etc.
</bodyText>
<subsectionHeader confidence="0.999865">
2.2 Data Format and Data Crawling
</subsectionHeader>
<bodyText confidence="0.9998376">
Due to Twitter policies, distributing the data di-
rectly is not possible in the shared task and thus the
trial, training and testing data are provided as char
offsets with label information along with tweetID1
and userID2. We use twitter3 python script to crawl
the tweets and our own python script to further to-
kenize and synchronize the tags in the data.
Since the data for “surprise genre” comes from
different social media sources, the ID format
varies from file to file but all the other details are
kept as is. In addition to the details, the tokens ref-
erenced by the offsets are provided unlike Twitter
data. (1) and (2) below, show the format of tweets
in train and test data respectively, while (3) shows
a typical tweet in the surprise genre data.
</bodyText>
<footnote confidence="0.99365075">
1Each tweet on Twitter has a unique tweetID
2Each user on Twitter carries a userID
3http://emnlp2014.org/workshops/CodeSwitch/scripts/
twitter.zip
</footnote>
<listItem confidence="0.9992885">
(1) TweetID UserID startIndex endIndex Tag
(2) TweetID UserID startIndex endIndex
(3) SocialMediaID UserID startIndex endIn-
dex Word
</listItem>
<subsectionHeader confidence="0.999057">
2.3 Data Statistics
</subsectionHeader>
<bodyText confidence="0.999993142857143">
The CS data is divided into two types of
tweets (henceforth posts)4 namely, Code-switched
posts and Monolingual posts. Table 1 shows the
original number of posts that are released for the
shared task for all LPs, along with their tag counts.
Due to the dynamic nature of social media, the
posts can be either deleted or updated and thus
different participants would have crawled different
number of posts. Thus, to come up with a compa-
rable platform for all the teams, the intersection of
data from all the users is used as final testing data
to report the results. Table 1 shows the number of
tweets or posts in testing data that are finally used
for the evaluation.
</bodyText>
<sectionHeader confidence="0.998328" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.998969">
We divided the language identification task into
a pipeline of 3 sub-tasks namely Pre-Processing,
Language Modeling, and Sequence labeling using
CRF5. The pipeline is followed for all the LPs with
some LP specific variations in selecting the most
relevant features to boost the results.
</bodyText>
<subsectionHeader confidence="0.999829">
3.1 Pre-Processing
</subsectionHeader>
<bodyText confidence="0.999910294117647">
In the pre-processing stage, we crawl the tweets
from Twitter given their offsets in the training data
and then tokenize and synchronize the words with
their tags as mentioned in Section 2.2. For each
LP we separate out the tokens into six classes to
use the data for Language Modeling and also to
manually analyze the language specific properties
to be used as features further in sequence labeling
. While synchronizing the words in a tweet with
their tags, we observed that some offsets do not
match with the words and this would lead to mis-
match of labels with tokens and thus degrade the
quality of training data.
To filter out the incorrect instances from the
training data, we frame pattern matching rules
which are specific to the languages present. But
this filtering is done only for the words present in
</bodyText>
<footnote confidence="0.98842">
4In case of twitter data, we have tweets but in case of sur-
prise genre data we have posts
5Conditional Random Field
</footnote>
<page confidence="0.997216">
88
</page>
<table confidence="0.999905285714286">
Language Pairs # Tweets Ambiguous Lang1 # Tokens NE Other
CodeSwitched Monolingual Lang2 Mixed
MSA-A dialects 774 5,065 1,066 79,134 16,291 15 14,112 8,699
Mandarin-English 521 478 0 12,114 2,431 12 1,847 1,025
Nepali-English 7,203 2,790 126 45,483 60,697 117 3982 35,651
Spanish-English 3,063 8,337 344 77,107 33,099 51 2,918 27,227
MSA-A dialects I 32 2,300 11 44,314 141 0 5,939 3,902
Mandarin-English 247 66 0 4,703 881 1 254 442
Nepali-English 2,665 209 0 12,286 17,216 60 1,071 9,635
Spanish-English 471 1,155 43 7,040 5,549 12 464 4,311
MSA-A dialects II 293 1,484 119 10,459 14,800 2 4,321 2,940
MSA-A dialects - - 110 2,687 6,930 3 1,097 1,190
Nepali-English 20 82 0 173 699 0 127 88
Spanish-English 22 27 1 636 306 1 38 120
</table>
<tableCaption confidence="0.997867">
Table 1: Data Statistics
</tableCaption>
<figure confidence="0.776995666666667">
Train
Test
Surprise
</figure>
<bodyText confidence="0.9990080625">
‘Lang1’ and ‘Lang2’ classes. There are two rea-
sons to consider these labels. First, ‘Lang1’ and
‘Lang2’ classes hold maximum share of words in
any LP as shown in Table 1, and thus have a higher
impact on the overall accuracy of the language
identification system. In addition to the above,
these categories correspond to the focus point of
the shared task. Second, for ‘Ambiguous’, ‘NE’
and ‘Other’ categories, it is difficult to find the
patterns according to their definitions. Although
rules can be framed for ‘Mixed’ category, since
their count is too less as compared to the other
categories (Table 1), it is of no use to train a sepa-
rate language model with very less number of in-
stances.
For Mandarin and Arabic data sets, any word
present in Roman script is excluded from the data.
Similarly for English and Nepali, if any word con-
tains characters other than Roman or numeral they
are excluded from the data. In addition to the
rule for English and Nepali, the additional alpha-
bets in Spanish are also included in the set of Ro-
man and numeral entries. Table 2 shows the num-
ber of words that remained in each of the lan-
guages/dialects, after the preprocessing.
One of the bonus points in the shared task is
that 3 out of 4 LPs share ‘English’ as their sec-
ond language. In order to increase the training size
for English, we merged all the English words into
a single file and thus reduced the number of lan-
guage models to be trained from 8 to 6, one for
each language (or dialect).
</bodyText>
<table confidence="0.999740428571429">
Data Size Average Token Length
10,380 8.14
105,014 3.83
12,874 4.99
53,953 8.93
35,620 4.26
32,737 3.96
</table>
<tableCaption confidence="0.984558">
Table 2: Data Statistics after Filtering
</tableCaption>
<subsectionHeader confidence="0.997446">
3.2 Language Modeling
</subsectionHeader>
<bodyText confidence="0.999990333333333">
In this stage, we train separate smoothed n-gram
based language models for each language in an LP.
We compute the conditional probability for each
word using these language models, which is then
used as a feature, among others for sequence la-
beling to finally predict the tags.
</bodyText>
<subsectionHeader confidence="0.754895">
3.2.1 N-gram Language Models
</subsectionHeader>
<bodyText confidence="0.999625333333333">
Given a word w, we compute the conditional prob-
ability corresponding to k6 classes c1, c2, ... , ck
as:
</bodyText>
<equation confidence="0.999328">
p(ci|w) = p(w|ci) ∗ p(ci) (1)
</equation>
<bodyText confidence="0.99991125">
The prior distribution p(c) of a class is es-
timated from the respective training sets shown
in Table 2. Each training set is used to train a
separate letter-based language model to estimate
the probability of word w. The language model
p(w) is implemented as an n-gram model using
the IRSTLM-Toolkit (Federico et al., 2008) with
Kneser-Ney smoothing. The language model is
</bodyText>
<footnote confidence="0.948528">
6In our case value of k is 2 as there are 2 languages in an
</footnote>
<figure confidence="0.9143815">
LP
Language
Arabic
English
Mandarin
MSA
Nepali
Spanish
</figure>
<page confidence="0.985444">
89
</page>
<equation confidence="0.868355333333333">
defined as:
p(li|li−1
i−k) (2)
</equation>
<bodyText confidence="0.999804">
where l is a letter and k is a parameter indicating
the amount of context used (e.g., k=4 means 5-
gram model).
</bodyText>
<subsectionHeader confidence="0.997407">
3.3 CRF based Sequence Labeling
</subsectionHeader>
<bodyText confidence="0.999305714285714">
After Language Modeling, we use CRF-based
(Conditional Random Fields (Lafferty et al.,
2001)) sequence labeling to predict the labels of
words in their surrounding context. The CRF algo-
rithm predicts the class of a word in its surround-
ing context taking into account other features not
explicitly represented in its structure.
</bodyText>
<subsectionHeader confidence="0.670307">
3.3.1 Feature Set
</subsectionHeader>
<bodyText confidence="0.999900342465754">
In order to train CRF models, we define a feature
set which is a hybrid combination of three sub-
types of features namely, Language Model Fea-
tures (LMF), Language Specific Features (LSF)
and Morphological Features (MF).
LMF: This sub-feature set consists of poste-
rior probability scores calculated using language
models for each language in an LP. Although
we trained language models only for ‘Lang1’
and ‘Lang2’ classes, we computed the probabil-
ity scores for all the words belonging to any of the
categories.
LSF: Each language carries some specific traits
that could assist in language identification. In
this sub-feature set we exploited some of the lan-
guage specific features exclusively based on the
description of the tags provided. The common fea-
tures for all the LPs are HAS NUM (Numeral is
present in the word), HAS PUNC (Punctuation is
present in the word), IS NUM (Word is a numeral),
IS PUNC (word is a punctuation or a collection of
punctuations), STARTS NUM (word starts with a
numeral) and STARTS PUNC (word starts with a
punctuation). All these features are used to gener-
ate variations to distinguish ‘Other’ class from rest
of the classes during prediction.
Two features exclusively used for the English
sharing LPs are HAS CAPITAL (capital letters are
present in the word) and IS ENGLISH (word be-
longs to English or not). HAS CAPITAL is used
to capture the capitalization property of the En-
glish writing system. This feature is expected to
help in the identification of ‘NEs’. IS ENGLISH is
used to indicate whether a word is an valid English
word or not, based on its presence in English dic-
tionaries. We used dictionaries available in PyEn-
chant7.
For the M-E LP, we are using ‘TYPE’8 as a
feature with possible values as ENGLISH, MAN-
DARIN, NUM, PUNC and OTHER. If all the
characters in the word are English alphabets EN-
GLISH is taken as the value and Mandarin oth-
erwise. Similar checks are used for NUM and
PUNC types. But if no case is satisfied, OTHER
is taken as the value.
We observed that the above features did not con-
tribute much to distinguish between any of the tags
in case of the MSA-A LP. Since this pair consists
of two different dialects of a language rather than
two different languages, the posterior probabilities
would be close to each other as compared to other
LPs. Thus we use the difference of these probabil-
ities as a feature in order to discriminate ambigu-
ous words or NEs that are spelled similarly.
MF: This sub-feature set comprises of the mor-
phological features corresponding to a word. We
automatically extracted these features using a
python script. The first feature of this set is a bi-
nary length variable (MORE/LESS) depending on
the length of the word with threshold value 4. The
other 8 features capture the prefix and suffix prop-
erties of a word, 4 for each type. In prefix type,
4, 3, 2 and 1 characters, if present, are taken from
the beginning of a word as 4 features. Similarly
for the suffix type, 1, 2, 3 and 4 characters, again
if present, are taken from the end of a word as 4
features. In both the cases if any value is miss-
ing, it is kept as NULL (LL). (4) below, shows
a typical example from English data with the MF
sub-feature set for the word ‘one’, where F1 rep-
resents the value of binary length variable, F2-F5
and F6-F9 represent the prefix and suffix features
respectively.
</bodyText>
<listItem confidence="0.462586">
(4) one
Word
</listItem>
<subsectionHeader confidence="0.924898">
3.3.2 Context Window
</subsectionHeader>
<bodyText confidence="0.998949">
Along with the above mentioned features, we
chose an optimal context template to train the CRF
</bodyText>
<footnote confidence="0.998909">
7PyEnchant is a spell checking library in Python
(http://pythonhosted.org/pyenchant/)
8Since it captures the properties of IS NUM and
IS PUNC, these features are not used again
</footnote>
<equation confidence="0.7706976">
n
p(w) =
i=1
o LL
F4 F5 F6
</equation>
<figure confidence="0.976311846153846">
on
LL
F2
Less
F1
one
F3
ne
F8
one
F7
e
F9
</figure>
<page confidence="0.992091">
90
</page>
<bodyText confidence="0.999970625">
models. We selected the window size to be 5, with
2 words before and after the target word. Furnish-
ing the training, testing and surprise genre data
with the features discussed in 3.3.1, we trained 4
CRF models on training data using feature tem-
plates based on the context decided. These mod-
els are used to finally predict the tags on the testing
and surprise genre data.
</bodyText>
<sectionHeader confidence="0.999121" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99997675">
The pipeline mentioned in Section 3 was used for
the language identification task for all the LPs.
We carried out a series of experiments with pre-
processing to clean the training data and also to
synchronize the testing data. We also did some
post-processing to handle language and tag spe-
cific cases.
In order to generate language model scores,
we trained 6 language models (one for each lan-
guage/dialect) on the filtered-out training data as
mentioned in Table 2. We experimented with dif-
ferent values of n-gram to select the optimal value
based on the F1-measure. Table 3 shows the opti-
mal order of n-gram, selected corresponding to the
highest value of F1-score. Using the optimal value
of n-gram, language models have been trained and
then posterior probabilities have been calculated
using equation (1).
Finally, we trained separate CRF models for
each LP, using the CRF++9 tool kit based on the
features described in Section 3.3.1 and the feature
template in Section 3.3.2. To empirically find the
relevance of features we also performed leave-one
out experiments so as to decide the optimal fea-
tures for the language identification task (more de-
tails in Section 4.1). Then, using these CRF mod-
els, tags were predicted on the testing and surprise
genre datasets.
</bodyText>
<subsectionHeader confidence="0.984916">
Language-Pair N-gram
</subsectionHeader>
<bodyText confidence="0.87672625">
MSA-A 5
M-E 5
N-E 6
S-E 5
</bodyText>
<tableCaption confidence="0.975022">
Table 3: Optimal Value of N-gram
</tableCaption>
<subsectionHeader confidence="0.998687">
4.1 Feature Ranking
</subsectionHeader>
<bodyText confidence="0.997167">
We expect that some features would be more im-
portant than others and would impact the task
</bodyText>
<footnote confidence="0.9108045">
9http://crfpp.googlecode.com/svn/trunk/doc/index.html?
source=navbar
</footnote>
<bodyText confidence="0.9999205">
of language identification irrespective of the lan-
guage pair. In order to identify such optimal fea-
tures for the language identification task, we rank
them based on their information gain scores.
</bodyText>
<subsubsectionHeader confidence="0.523821">
4.1.1 Information Gain
</subsubsectionHeader>
<bodyText confidence="0.999960111111111">
We used information gain to score features ac-
cording to their expected usefulness for the task at
hand. Information gain is an information theoretic
concept that measures the amount of knowledge
that is gained about a given class by having access
to a particular feature. If f is the occurrence an
individual feature and f¯ the non-occurrence of a
feature, information gain can be measured by the
following formula:
</bodyText>
<equation confidence="0.99402025">
�
G(x) = P(f) P(y|f)logP(y|f)
� (3)
¯f) logP(y |¯f)logP(y |¯f)
</equation>
<bodyText confidence="0.998521">
For each language pair, the importance of fea-
ture types are represented by the following order:
</bodyText>
<listItem confidence="0.964838666666667">
• MSA-A dialects: token &gt; word morphology
&gt; posterior probabilities &gt; others
• Mandarin-English: token &gt; posterior prob-
abilities &gt; word morphology &gt; language
type &gt; others
• Nepali-English: token &gt; posterior probabil-
ities &gt; word morphology &gt; dictionary &gt; oth-
ers
• Spanish-English: token &gt; posterior proba-
</listItem>
<bodyText confidence="0.996859529411765">
bilities &gt; word morphology &gt; others &gt; dic-
tionary
Apart from MSA-A dialects, top 3 features sug-
gested by information gain are token and its sur-
rounding context, posterior probabilities and word
morphology. For Arabic dialects word morphol-
ogy is more important than posterior probabilities.
It could be due to the fact that Arabic dialects share
a similar phonetic inventory and thus have similar
posterior probabilities. However, they differ sig-
nificantly in their morphological structure (Zaidan
and Callison-Burch, 2013).
We also carried out leave-one-out experiments
over all the features to ascertain their impact on the
classification performance. The results of these
experiments are shown in Table (5). Accuracies
are averaged over 5-fold cross-validation.
</bodyText>
<equation confidence="0.52829">
+ P(
</equation>
<page confidence="0.97134">
91
</page>
<table confidence="0.613480904761905">
Token Level
Language Pairs Ambiguous Lang1 Lang2 Mixed NE Other
R P F1 R P F1 R P F1 R P F1 R P F1 R P F1 Overall Accuracy
0.00 0.00 0.00 0.92 0.95 0.94 0.40 0.03 0.06 - - - 0.70 0.77 0.73 0.90 0.85 0.87 0.90
- - - 0.98 0.98 0.98 0.67 0.66 0.67 0.00 1.00 0.00 0.84 0.38 0.53 0.22 0.71 0.33 0.88
- - - 0.95 0.93 0.94 0.98 0.96 0.97 0.00 1.00 0.00 0.39 0.79 0.52 0.94 0.96 0.95 0.95
0.00 1.00 0.00 0.88 0.81 0.84 0.83 0.90 0.86 0.00 1.00 0.00 0.16 0.40 0.23 0.83 0.80 0.82 0.83
0.00 0.00 0.00 0.91 0.47 0.62 0.36 0.84 0.51 0.00 1.00 0.00 0.59 0.80 0.68 0.80 0.71 0.75 0.60
0.00 0.00 0.00 0.94 0.38 0.54 0.46 0.93 0.61 0.00 1.00 0.00 0.52 0.78 0.62 0.96 0.96 0.96 0.62
- - - 0.92 0.76 0.84 0.95 0.89 0.91 - - - 0.35 0.92 0.50 0.85 0.89 0.87 0.86
0.00 1.00 0.00 0.86 0.81 0.83 0.82 0.87 0.85 0.00 1.00 0.00 0.15 0.40 0.22 0.82 0.78 0.80 0.94
MSA-A I
M-E
N-E
S-E
MSA-A II
MSA-A
N-E
S-E
Test
Surprise
</table>
<tableCaption confidence="0.994285">
Table 4: Token Level Results
</tableCaption>
<table confidence="0.999947625">
Left Out Feature MSA-A M-E N-E S-E
Context 76.32 94.07 93.97 92.30
Morphology 79.29 93.67 93.98 93.51
Probability 79.24 89.16 93.86 93.28
Dictionary - 87.75 93.73 92.99
Language Type - 87.97 - -
Others 78.80 83.84 92.10 92.20
All Features 79.37 95.11 94.52 93.54
</table>
<tableCaption confidence="0.993354">
Table 5: Leave-one-out Experiments
</tableCaption>
<sectionHeader confidence="0.999737" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999977736842105">
Each language identification system is evaluated
against two data tracks namely, ‘Testing’ and ‘Sur-
prise Genre’ data as mentioned in Section 2. Sur-
prise genre data of Mandarin-English LP was not
provided, so no results are available. All the results
are provided on two levels, comment/post/tweet
and token level. Tables 4 and 6 show results of our
language identification system on both the levels
respectively.
In case of Tweets, systems are evaluated using
the following measures: Accuracy, Recall, Preci-
sion and F-Score. However at token level, sys-
tems are evaluated separately for each tag in an
LP using Recall, Precision and F1-Score as the
measures. Table 4 shows that the results for ‘Am-
biguous’ and ‘Mixed’ categories are either miss-
ing (due to absence of tokens in that category), or
have 0.00 F1-Score. One obvious reason could be
the sparsity of data for these categories.
</bodyText>
<sectionHeader confidence="0.998674" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9994018">
In this paper, we have described a CRF based to-
ken level language identification system that uses a
set of naive easily computable features guarantee-
ing reasonable accuracies over multiple language
pairs. Our analysis showed that the most important
</bodyText>
<table confidence="0.8677601">
Language Pairs Tweet Level
Accuracy Recall Precision F-score
MSA-A I 0.605 0.719 0.025 0.048
M-E 0.751 0.814 0.863 0.838
N-E 0.948 0.979 0.966 0.972
S-E 0.835 0.773 0.692 0.730
MSA-A II 0.469 0.823 0.213 0.338
MSA-A 0.457 0.833 0.128 0.222
N-E 0.735 0.900 0.419 0.571
S-E 0.830 0.765 0.689 0.725
</table>
<tableCaption confidence="0.995976">
Table 6: Comment/Post/Tweet Level Results
</tableCaption>
<bodyText confidence="0.999958166666667">
feature is the word structure which in our system
is captured by n-gram posterior probabilities and
word morphology. Our analysis of Arabic dialects
shows that word morphology plays an important
role in the identification of mixed codes of closely
related languages.
</bodyText>
<sectionHeader confidence="0.998932" genericHeader="acknowledgments">
7 Acknowledgement
</sectionHeader>
<bodyText confidence="0.999986333333333">
We would like to thank Himani Chaudhry for her
valuable comments and suggestions that helped us
to improve the quality of the paper.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999323866666667">
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the mat-
ter. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 229–237. Association for Computational Lin-
guistics.
Ted Dunning. 1994. Statistical identification of lan-
guage. Computing Research Laboratory, New Mex-
ico State University.
Heba Elfardy and Mona T Diab. 2012. Token level
identification of linguistic code switching. In COL-
ING (Posters), pages 287–296.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. Irstlm: an open source toolkit for han-
</reference>
<figure confidence="0.8796015">
Test
Surprise
</figure>
<page confidence="0.838669">
92
</page>
<reference confidence="0.9974931875">
dling large scale language models. In Interspeech,
pages 1618–1621.
Ben King and Steven P Abney. 2013. Labeling the
languages of words in mixed-language documents
using weakly supervised methods. In HLT-NAACL,
pages 1110–1119.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning, pages 282–289.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. volume 2, pages 27–40.
Dong Nguyen and A Seza Dogruoz. 2014. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Octobe, 2014, Doha,
Qatar.
Omar F Zaidan and Chris Callison-Burch. 2013. Ara-
bic dialect identification. Computational Linguis-
tics, 40(1):171–202.
</reference>
<page confidence="0.999169">
93
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.238177">
<title confidence="0.999943">Language Identification in Code-Switching Scenario</title>
<author confidence="0.998543">Naman Jain</author>
<affiliation confidence="0.535779">LTRC, IIIT-H, Hyderabad, India</affiliation>
<email confidence="0.901283">naman.jain@research.iiit.ac.in</email>
<author confidence="0.902246">Riyaz Ahmad</author>
<affiliation confidence="0.544014">LTRC, IIIT-H, Hyderabad, India</affiliation>
<email confidence="0.973747">riyaz.bhat@research.iiit.ac.in</email>
<abstract confidence="0.998567571428572">This paper describes a CRF based token level language identification system ento Identification in Code- (CS) Data of Our system hinges on using conditional posterior probabilities for the individual codes (words) in code-switched data to solve the language identification task. We also experiment with other linguistically motivated language specific as well as generic features to train the CRF based sequence labeling algorithm achieving reasonable results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Marco Lui</author>
</authors>
<title>Language identification: The long and the short of the matter.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>229--237</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2234" citStr="Baldwin and Lui, 2010" startWordPosition="322" endWordPosition="325">ntic Processing, is more complex than monolingual data. Traditional NLP techniques perform miserably when processing mixed language data. The performance degrades at a rate proportional to the amount and level of code-switching present in the data. Therefore, in order to process such data, a separate language identification component is needed, to first identify the language of individual words. Language identification in code-switched data can be thought of as a sub-task of a document level language identification task. The latter aims to identify the language a given document is written in (Baldwin and Lui, 2010), while the former addresses the same problem, however at the token level. Although, both the problems have separate goals, they can fundamentally be modeled with a similar set of features and techniques. However, language identification at the word level is more challenging than a typical document level language identification problem. The number of features available at document level is much higher than at word level. The available features for word level identification are word morphology, syllable structure and phonemic (letter) inventory of the language(s). Since these features are relat</context>
</contexts>
<marker>Baldwin, Lui, 2010</marker>
<rawString>Timothy Baldwin and Marco Lui. 2010. Language identification: The long and the short of the matter. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 229–237. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Statistical identification of language.</title>
<date>1994</date>
<institution>Computing Research Laboratory, New Mexico State University.</institution>
<contexts>
<context position="2976" citStr="Dunning, 1994" startWordPosition="437" endWordPosition="438">can fundamentally be modeled with a similar set of features and techniques. However, language identification at the word level is more challenging than a typical document level language identification problem. The number of features available at document level is much higher than at word level. The available features for word level identification are word morphology, syllable structure and phonemic (letter) inventory of the language(s). Since these features are related to the structure of a word, letter based n-gram models have been reported to give reasonably accurate and comparable results (Dunning, 1994; Elfardy and Diab, 2012; King and Abney, 2013; Nguyen and Dogruoz, 2014; Lui et al., 2014). In this work, we present a token level language identification system which mainly hinges on the posterior probabilities computed using n-gram based language models. The rest of the paper is organized as follows: In Section 2, we discuss about the data of the shared task. In Section 3, we discuss the methodology we adapted to address the problem of language identification, in detail. Experiments based on our methodology are discussed in Section 4. In Section 5, we present the results obtained, with a b</context>
</contexts>
<marker>Dunning, 1994</marker>
<rawString>Ted Dunning. 1994. Statistical identification of language. Computing Research Laboratory, New Mexico State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heba Elfardy</author>
<author>Mona T Diab</author>
</authors>
<title>Token level identification of linguistic code switching.</title>
<date>2012</date>
<booktitle>In COLING (Posters),</booktitle>
<pages>287--296</pages>
<contexts>
<context position="3000" citStr="Elfardy and Diab, 2012" startWordPosition="439" endWordPosition="442">ly be modeled with a similar set of features and techniques. However, language identification at the word level is more challenging than a typical document level language identification problem. The number of features available at document level is much higher than at word level. The available features for word level identification are word morphology, syllable structure and phonemic (letter) inventory of the language(s). Since these features are related to the structure of a word, letter based n-gram models have been reported to give reasonably accurate and comparable results (Dunning, 1994; Elfardy and Diab, 2012; King and Abney, 2013; Nguyen and Dogruoz, 2014; Lui et al., 2014). In this work, we present a token level language identification system which mainly hinges on the posterior probabilities computed using n-gram based language models. The rest of the paper is organized as follows: In Section 2, we discuss about the data of the shared task. In Section 3, we discuss the methodology we adapted to address the problem of language identification, in detail. Experiments based on our methodology are discussed in Section 4. In Section 5, we present the results obtained, with a brief discussion. Finally</context>
</contexts>
<marker>Elfardy, Diab, 2012</marker>
<rawString>Heba Elfardy and Mona T Diab. 2012. Token level identification of linguistic code switching. In COLING (Posters), pages 287–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
</authors>
<title>Irstlm: an open source toolkit for handling large scale language models.</title>
<date>2008</date>
<booktitle>In Interspeech,</booktitle>
<pages>1618--1621</pages>
<contexts>
<context position="11286" citStr="Federico et al., 2008" startWordPosition="1847" endWordPosition="1850">each word using these language models, which is then used as a feature, among others for sequence labeling to finally predict the tags. 3.2.1 N-gram Language Models Given a word w, we compute the conditional probability corresponding to k6 classes c1, c2, ... , ck as: p(ci|w) = p(w|ci) ∗ p(ci) (1) The prior distribution p(c) of a class is estimated from the respective training sets shown in Table 2. Each training set is used to train a separate letter-based language model to estimate the probability of word w. The language model p(w) is implemented as an n-gram model using the IRSTLM-Toolkit (Federico et al., 2008) with Kneser-Ney smoothing. The language model is 6In our case value of k is 2 as there are 2 languages in an LP Language Arabic English Mandarin MSA Nepali Spanish 89 defined as: p(li|li−1 i−k) (2) where l is a letter and k is a parameter indicating the amount of context used (e.g., k=4 means 5- gram model). 3.3 CRF based Sequence Labeling After Language Modeling, we use CRF-based (Conditional Random Fields (Lafferty et al., 2001)) sequence labeling to predict the labels of words in their surrounding context. The CRF algorithm predicts the class of a word in its surrounding context taking int</context>
</contexts>
<marker>Federico, Bertoldi, Cettolo, 2008</marker>
<rawString>Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. Irstlm: an open source toolkit for handling large scale language models. In Interspeech, pages 1618–1621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben King</author>
<author>Steven P Abney</author>
</authors>
<title>Labeling the languages of words in mixed-language documents using weakly supervised methods.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>1110--1119</pages>
<contexts>
<context position="3022" citStr="King and Abney, 2013" startWordPosition="443" endWordPosition="446">ilar set of features and techniques. However, language identification at the word level is more challenging than a typical document level language identification problem. The number of features available at document level is much higher than at word level. The available features for word level identification are word morphology, syllable structure and phonemic (letter) inventory of the language(s). Since these features are related to the structure of a word, letter based n-gram models have been reported to give reasonably accurate and comparable results (Dunning, 1994; Elfardy and Diab, 2012; King and Abney, 2013; Nguyen and Dogruoz, 2014; Lui et al., 2014). In this work, we present a token level language identification system which mainly hinges on the posterior probabilities computed using n-gram based language models. The rest of the paper is organized as follows: In Section 2, we discuss about the data of the shared task. In Section 3, we discuss the methodology we adapted to address the problem of language identification, in detail. Experiments based on our methodology are discussed in Section 4. In Section 5, we present the results obtained, with a brief discussion. Finally we conclude in Sectio</context>
</contexts>
<marker>King, Abney, 2013</marker>
<rawString>Ben King and Steven P Abney. 2013. Labeling the languages of words in mixed-language documents using weakly supervised methods. In HLT-NAACL, pages 1110–1119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="11721" citStr="Lafferty et al., 2001" startWordPosition="1923" endWordPosition="1926">in a separate letter-based language model to estimate the probability of word w. The language model p(w) is implemented as an n-gram model using the IRSTLM-Toolkit (Federico et al., 2008) with Kneser-Ney smoothing. The language model is 6In our case value of k is 2 as there are 2 languages in an LP Language Arabic English Mandarin MSA Nepali Spanish 89 defined as: p(li|li−1 i−k) (2) where l is a letter and k is a parameter indicating the amount of context used (e.g., k=4 means 5- gram model). 3.3 CRF based Sequence Labeling After Language Modeling, we use CRF-based (Conditional Random Fields (Lafferty et al., 2001)) sequence labeling to predict the labels of words in their surrounding context. The CRF algorithm predicts the class of a word in its surrounding context taking into account other features not explicitly represented in its structure. 3.3.1 Feature Set In order to train CRF models, we define a feature set which is a hybrid combination of three subtypes of features namely, Language Model Features (LMF), Language Specific Features (LSF) and Morphological Features (MF). LMF: This sub-feature set consists of posterior probability scores calculated using language models for each language in an LP. </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Jey Han Lau</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic detection and language identification of multilingual documents.</title>
<date>2014</date>
<volume>2</volume>
<pages>27--40</pages>
<contexts>
<context position="3067" citStr="Lui et al., 2014" startWordPosition="451" endWordPosition="454">nguage identification at the word level is more challenging than a typical document level language identification problem. The number of features available at document level is much higher than at word level. The available features for word level identification are word morphology, syllable structure and phonemic (letter) inventory of the language(s). Since these features are related to the structure of a word, letter based n-gram models have been reported to give reasonably accurate and comparable results (Dunning, 1994; Elfardy and Diab, 2012; King and Abney, 2013; Nguyen and Dogruoz, 2014; Lui et al., 2014). In this work, we present a token level language identification system which mainly hinges on the posterior probabilities computed using n-gram based language models. The rest of the paper is organized as follows: In Section 2, we discuss about the data of the shared task. In Section 3, we discuss the methodology we adapted to address the problem of language identification, in detail. Experiments based on our methodology are discussed in Section 4. In Section 5, we present the results obtained, with a brief discussion. Finally we conclude in Section 6 with some future directions. 87 Proceedin</context>
</contexts>
<marker>Lui, Lau, Baldwin, 2014</marker>
<rawString>Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014. Automatic detection and language identification of multilingual documents. volume 2, pages 27–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Nguyen</author>
<author>A Seza Dogruoz</author>
</authors>
<title>Word level language identification in online multilingual communication.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="3048" citStr="Nguyen and Dogruoz, 2014" startWordPosition="447" endWordPosition="450">nd techniques. However, language identification at the word level is more challenging than a typical document level language identification problem. The number of features available at document level is much higher than at word level. The available features for word level identification are word morphology, syllable structure and phonemic (letter) inventory of the language(s). Since these features are related to the structure of a word, letter based n-gram models have been reported to give reasonably accurate and comparable results (Dunning, 1994; Elfardy and Diab, 2012; King and Abney, 2013; Nguyen and Dogruoz, 2014; Lui et al., 2014). In this work, we present a token level language identification system which mainly hinges on the posterior probabilities computed using n-gram based language models. The rest of the paper is organized as follows: In Section 2, we discuss about the data of the shared task. In Section 3, we discuss the methodology we adapted to address the problem of language identification, in detail. Experiments based on our methodology are discussed in Section 4. In Section 5, we present the results obtained, with a brief discussion. Finally we conclude in Section 6 with some future direc</context>
</contexts>
<marker>Nguyen, Dogruoz, 2014</marker>
<rawString>Dong Nguyen and A Seza Dogruoz. 2014. Word level language identification in online multilingual communication. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Thamar Solorio</author>
<author>Elizabeth Blair</author>
<author>Suraj Maharjan</author>
<author>Steve Bethard</author>
<author>Mona Diab</author>
<author>Mahmoud Gonheim</author>
<author>Abdelati Hawwari</author>
<author>Fahad AlGhamdi</author>
<author>Julia Hirshberg</author>
<author>Alison Chang</author>
<author>Pascale Fung</author>
</authors>
<title>Overview for the first shared task on language identification in codeswitched data.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Workshop on Computational Approaches to Code-Switching. EMNLP 2014, Conference on Empirical Methods in Natural Language Processing, Octobe,</booktitle>
<location>Doha, Qatar.</location>
<contexts>
<context position="868" citStr="Solorio et al., 2014" startWordPosition="116" endWordPosition="119">ken level language identification system entry to Language Identification in CodeSwitched (CS) Data task of CodeSwitch 2014. Our system hinges on using conditional posterior probabilities for the individual codes (words) in code-switched data to solve the language identification task. We also experiment with other linguistically motivated language specific as well as generic features to train the CRF based sequence labeling algorithm achieving reasonable results. 1 Introduction This paper describes our participation in the Language Identification in Code-Switched Data task at CodeSwitch 2014 (Solorio et al., 2014). The workshop focuses on NLP approaches for the analysis and processing of mixed-language data with a focus on intra sentential code-switching, while the shared task focuses on the identification of the language of each word in a codeswitched data, which is a prerequisite for analyzing/processing such data. Code-switching is a sociolinguistics phenomenon, where multilingual speakers switch back and forth between two or more common languages or language-varieties, in the context of a single written or spoken conversation. Natural language analysis of codeswitched (henceforth CS) data for vario</context>
</contexts>
<marker>Solorio, Blair, Maharjan, Bethard, Diab, Gonheim, Hawwari, AlGhamdi, Hirshberg, Chang, Fung, 2014</marker>
<rawString>Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve Bethard, Mona Diab, Mahmoud Gonheim, Abdelati Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison Chang, and Pascale Fung. 2014. Overview for the first shared task on language identification in codeswitched data. In Proceedings of the First Workshop on Computational Approaches to Code-Switching. EMNLP 2014, Conference on Empirical Methods in Natural Language Processing, Octobe, 2014, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Arabic dialect identification.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="19235" citStr="Zaidan and Callison-Burch, 2013" startWordPosition="3200" endWordPosition="3203">en &gt; posterior probabilities &gt; word morphology &gt; dictionary &gt; others • Spanish-English: token &gt; posterior probabilities &gt; word morphology &gt; others &gt; dictionary Apart from MSA-A dialects, top 3 features suggested by information gain are token and its surrounding context, posterior probabilities and word morphology. For Arabic dialects word morphology is more important than posterior probabilities. It could be due to the fact that Arabic dialects share a similar phonetic inventory and thus have similar posterior probabilities. However, they differ significantly in their morphological structure (Zaidan and Callison-Burch, 2013). We also carried out leave-one-out experiments over all the features to ascertain their impact on the classification performance. The results of these experiments are shown in Table (5). Accuracies are averaged over 5-fold cross-validation. + P( 91 Token Level Language Pairs Ambiguous Lang1 Lang2 Mixed NE Other R P F1 R P F1 R P F1 R P F1 R P F1 R P F1 Overall Accuracy 0.00 0.00 0.00 0.92 0.95 0.94 0.40 0.03 0.06 - - - 0.70 0.77 0.73 0.90 0.85 0.87 0.90 - - - 0.98 0.98 0.98 0.67 0.66 0.67 0.00 1.00 0.00 0.84 0.38 0.53 0.22 0.71 0.33 0.88 - - - 0.95 0.93 0.94 0.98 0.96 0.97 0.00 1.00 0.00 0.39</context>
</contexts>
<marker>Zaidan, Callison-Burch, 2013</marker>
<rawString>Omar F Zaidan and Chris Callison-Burch. 2013. Arabic dialect identification. Computational Linguistics, 40(1):171–202.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
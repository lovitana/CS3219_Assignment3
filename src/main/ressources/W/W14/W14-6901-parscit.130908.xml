<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000613">
<title confidence="0.99682">
Corpus annotation methodology for citation classification in
scientific literature
</title>
<author confidence="0.752209">
Myriam Hernández Álvarez José Gómez Soriano
Escuela Politécnica Nacional Universidad de Alicante
</author>
<affiliation confidence="0.9372945">
Facultad de Ingeniería de Sistemas Dpto. de Lenguajes y Sistemas
Quito, Ecuador Informáticos
</affiliation>
<email confidence="0.7608795">
myriam.hernandez@epn.edu.ec Alicante, España
jmgomez@ua.es
</email>
<sectionHeader confidence="0.982991" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967454545455">
Since, at the moment, there is not a gold-
standard annotated corpus to allow
generation and testing of automatic
systems for classifying the purpose or
function of a citation referenced in an
article; it is necessary to build one, for this
objective. The development of this kind of
corpus is subject to two conditions: the
first one is to present a clear and
unambiguous classification scheme. The
second one is to secure an initial manual
process of labeling to reach a sufficient
inter-coder agreement among annotators
to validate the annotation scheme and to
be able to reproduce it even with coders
who do not know in depth the topic of the
analyzed articles. This paper proposes and
validates a methodology for corpus
annotation for citation classification in
scientific literature that facilitate
annotation and produces substantial inter-
annotator agreement.
</bodyText>
<sectionHeader confidence="0.945451" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999625121212121">
Not all citations have the same effect in a
citing article. The impact of a cited paper may
vary considerably. It could go from being a
criticism, or a starting point for a job or simply
an acknowledge of the work of other authors.
However, accepted methods available today
are variations of citation counting where all
citations are considered equal and are
evaluated with the same weight. Current
methods of measuring impact fall into one of
three techniques: simple count of citations
(more citations, more impact); co citation
which adds as a measure of similarity between
two works the number of common documents
that cited them; and the Google’s PageRank
that measure citation relevance using the
relevance and frequency of the citing
document. Not all citations are equal, so they
should not weigh equally in the impact
calculation. None of the above mentioned
counting methods takes into account whether
the citation context is positive or negative, the
purpose of the citing article, or if the citation
has or not have influence on it.
It becomes important to identify more
complete metrics that take into account the
content about cited work to assess its impact
and relevance. It is necessary the construction
of a new impact index enriched with
qualitative criteria regarding the citation. This
process requires a content analysis of the
context containing citations to obtain certain
important features such as intent or purpose of
</bodyText>
<page confidence="0.966025">
1
</page>
<bodyText confidence="0.994596558139535">
the citing author when made the reference.
Content analysis is a group of procedures to
recollect and organized information in
standard format to make inferences about its
characteristics and meaning using manual or
automatic methods (Ding, Zhang, Chambers,
Song, Wang, and Zhai, 2014). This analysis
could be automatic starting from a tagged
corpus to build a model.
Since, there is not a gold-standard annotated
corpus for citation analysis data, it is necessary
to work in the generation of one in order to
facilitate collaborative work and results
comparison among researches. Development
of a corpus starts from the definition of a
citation classification scheme that considers
function (purpose), polarity (disposition) and
influence of cited paper to produce a reliable
and reproducible data set that could be the
basis for future work in this area. This tagged
corpus will allow overcoming problems
currently present that make very difficult to
strengthen collaborative efforts in this field
(Hernández y Gómez, 2014). Present
problems are, for instance, the lack of a
standard classification scheme and of
sufficient public data available such that
researchers could test their systems and
compare results.
According to Arstein and Poesio (2008), a
corpus is reliable if annotators agree in the
assigned categories because it displays a
similar understanding of the classification
scheme. This criterion is a prerequisite to
demonstrate validity of a scheme. If there is no
consistency among the obtained results, the
representation may be inappropriate for the
data.
In our experiment, we pose a scheme to
classify citation functions and we defined an
annotation methodology to allow a greater
accuracy in the process to facilitate decision-
making and generate a greater inter-coder
agreement. The subject of this article focus in
the proposed annotation methodology which
could be applied to any scheme with the only
condition that the scheme is not ambiguous i.e.
its categories are clearly differentiated.
Method
We applied different citation classification
schemes according to citation function. The
condition for these schemes were that
categories were well distinguished.
In this process, we detected two sources or
error that affected results and did not allow
good agreement among coders. One had to do
with usage by the annotators of context of
different length, which lend them to obtain
discrepant results; the other was lack of clarity
in the analyzed articles that made difficult to
find enough sense in text to reach a unique
citation classification.
We corrected the first factor setting fixed
criteria for determining context length.
Hernández and Gómez, (2014) highlighted the
need for defining context in view of argument
detection, so the context include all sentences
around citation that are talking about it.
However, due to the complexity of this task,
we decided to replace argument detection by
fixing a context delimited by a complete
paragraph. The rationale for this decision was
that, by definition, a paragraph is a group of
related sentences about the same idea. We
assumed that author’s purpose when making a
citation could be found using cue words and
ontological concepts that are within the same
paragraph.
To avoid the second error source, we
proposed a new annotation methodology to
help coders to organize the ideas expressed in
the text, to take them to decide citation
function classification in an orderly way. With
the proposed annotation methodology, we
could achieve a minimized human effort with
a clear understanding of the structure of the
</bodyText>
<page confidence="0.962513">
2
</page>
<bodyText confidence="0.995721523255814">
presented ideas, so that we could generate a
natural association of functions and their
classes. As an additional advantage, the
methodology includes detection of patterns
formed by lexical values (cue words) and
ontological classes related to a function. We
could convert this information to regular
expressions to be the foundation for automatic
citation classification. Without regular
expressions, to annotate a corpus of sufficient
size we would require too much effort to detect
patterns statistically. In fact, in the initial
experiments, the original intention for pattern
use was to annotate them in conjunction with
function definition, so that, these patterns
included in the corpus, would facilitate model
detection in an automatic tagging process. We
changed this approach and decided to pre-
annotate first in an attempt to improve a very
low annotation agreement, and with this
change, we obtained a new and more effective
way to data-set annotation.
The proposed annotation methodology
consists of two phases. In the first, we perform
a pre-annotation process in which we define
patterns. These patterns help coders to
understand structure of sentences within
context and help them to define citation
function. In this step, the annotator detects a
sentence type, saving the original sentence
order to maintain relevant information related
with citation purpose. Zock, 2012, presented
patterns that link ontological and syntactic
categories to generate sentences maintaining
the author’s original intention. These
techniques allow associating between purpose
and an ontological pattern. We adapt this basic
idea to the solution or our problem and
develop concepts and notation to our method.
In the pre-annotation stage, coders identify
manually ontological and lexical patterns that
are near of citations within the content defined
as a paragraph. A pattern consists of a fixed
part and a variable part. The fixed part is
underlined and corresponds to cue words
related to a function. We label the variable part
as XML, according to ontological concepts as
cited work, author, theory, action, method,
used material, concept, task, result, quoted
text, assumption, person, experiment, positive
feature, negative feature, etc. We design the
group of tags so that we cover without
ambiguity the largest number of possibilities.
For instance, if we have the text: “This
feature set is based on Dong and Schäfer,
2011.” Pre-annotation result will be:
“&lt;material&gt;this feature set&lt;/material&gt; is
based on &lt;cited&gt;Dong and Schäfer,
2011&lt;/cited&gt;.” In addition, the pattern will be
“MATERIAL is based CITED”, where
MATERIAL and CITED are the variable part
and “is based” is the fixed part that
corresponds to cue words. In this case, it is
clear that citation function has to do with the
use other author’s material as a base for own
work.
Other sentences can be generated with this
pattern, for instance, “The algorithm is based
in the Vector Space Model – VSM (Salton et
al., 1975)”. This sentence pre-annotated is
“&lt;material&gt;The algorithm&lt;/material&gt; is
based in the Vector Space Model – VSM
&lt;cited&gt; (Salton et al., 1975)&lt;/cited&gt;”. The
pattern is the same that the one in the previous
example “MATERIAL is based CITED”, with
the equal function type than last example
because pattern is identical.
Fixed part is a skip-gram with 1 to 4 length.
Each group of words is a sequence. A skip-
gram, according to Guthrie, Allison, Liu,
Guthrie, and Wilks (2006), is a generalization
of an n-gram, where text leave not considered
spaces, while a skip-gram does consider
spaces between word sequences.
Examples of pre-annotation process
To understand better the pre-annotation
</bodyText>
<page confidence="0.993283">
3
</page>
<bodyText confidence="0.9868185">
scheme, we show three examples of how to
apply it to the context of scientific citations.
First, consider the following sentence
containing a citation:
“We compare our zone classifier to a
reimplementation of Teufel and Moens&apos;s NB
classifier and features on their original
Computational Linguistic corpus”.
After applying the ontological pattern
annotation scheme, we obtained the following
result:
“&lt;author&gt;We&lt;\author&gt; compare our
&lt;material&gt;zone classifier&lt;\material&gt; to a
reimplementation of &lt;cited&gt;Teufel and
Moens&lt;\cited&gt;&apos;s &lt;material&gt;NB
classifier&lt;\material&gt; and
&lt;material&gt;features&lt;material&gt; on their
original &lt;material&gt;Computational Linguistic
corpus&lt;\material&gt;”.
Its ontological pattern is:” AUTHOR
compare our MATERIAL to CITED
MATERIAL”
This pattern contains a skip-gram, which is
formed by two word sequences: “compare
our” and “to”. The idea behind the whole
sentence is that the authors compare their own
material with a cited material. The
classification of author’s sentiment is not part
of this work, but the pattern clearly reveals a
comparison between authors’ contribution
with other researchers’.
Let us take a second example out of the
literature to illustrate our method. Consider the
following paragraph containing a citation:
“Comprehension-based summarization, e.g.
Kintsch and Van Dijk (1978) and Brown et al.
(1983), is the most ambitious model of
automatic summarization, requiring a
complete understanding of the text. Due to the
failure of rule-based NLP and knowledge
representation, other less knowledge-intensive
methods now dominate”.
Annotating this paragraph, we have:
“Comprehension-based summarization, e.g.
&lt;cited1&gt;Kintsch and Van Dijk
(1978)&lt;/cited1&gt; and &lt;cited2&gt;Brown et al.
(1983)&lt;\cited2&gt;, is the most ambitious model
of automatic summarization, requiring a
complete understanding of the text. Due to the
failure of &lt;method&gt;rule-based
NLP&lt;\method&gt; and &lt;method&gt;knowledge
representation&lt;\method&gt;, other less
knowledge-intensive methods now dominate”.
Its ontological pattern is:
“CITED ambitious * .Due to * failure of
METHOD
CITED ambitious * .Due to * failure of
METHOD”.
The pattern contains a skip-gram having
three word sequences: “ambitious”, “.Due to”
and ”failure of”. The skip-grams are indicated
by a star symbol * in between the sequences.
The variable parts are two: &lt;cited&gt; and
&lt;method&gt;. The idea behind this pattern is that
the cited researchers were ambitious, but they
failed on the authors’ point of view. This
pattern clearly reveals authors’ negative
impression or a weakness regarding the cited
work.
Finally, we present a third example by
taking the following sentence containing a
citation:
“The baseline score shown in bold, is
obtained with no context window and is
comparable to the results reported by Athar
(2011)”.
Applying our annotation scheme, we
produce:
“The &lt;result&gt;baseline score&lt;\result&gt;,
shown in bold, is obtained with no context
window and is comparable to the
&lt;result&gt;results&lt;\result&gt; reported by
&lt;cited&gt;Athar (2011)&lt;\cited&gt;”.
Its ontological pattern is:
“RESULT is * comparable to RESULT
CITED”.
Again, the pattern contains a skip-gram
having two word sequences: “is” and
</bodyText>
<page confidence="0.995643">
4
</page>
<bodyText confidence="0.989082534883721">
“comparable to”. Additionally, it contains
three variable parts: &lt;result&gt; &lt;result&gt; &lt;cited&gt;.
From this pattern, we clear see that authors are
comparing their results with other
researchers’. Independently of the function
classification, the ontological patterns are
supposed to reveal the authors’ intention
concerning the cited work.
The application of this strategy allows
identifying punctual lexical entries and their
relation to semantic features. An ontological
pattern here is a structure that conveys authors’
purpose to cite. By using that, we expect not
only to obtain a good level of agreement
among the annotators, but also to minimize the
human effort needed for annotating papers and
populate a big corpus by converting the
patterns into regular expressions.
Experiment setup and results
Three annotators collaborated. The annotation
process comply three requirements in order to
achieve reliability and reproducibility
(Krippendorff, 2004). The annotators had a
profile that allow them a good understanding
of the scientific texts in computational
linguistics; they worked in an independent
way and they had a clear function
classification scheme with detail instructions.
To test annotation reliability, we measured
inter-annotator agreement in a small section of
the corpus; the same people must review this
sample. It is necessary to achieve a good rate
in this agreement because it certifies that the
process is reliable and reproducible and that
results may be generalized to the complete
process in which probably are going to work
new annotators and not only the ones that
coded the trial (Artstein y Poesio, 2008).
We analyzed 101 citations to classify them
according to their function without pre-
annotation and 101 different citations with
pre-annotation. We measured inter-annotator
agreement in each case.
</bodyText>
<sectionHeader confidence="0.673887" genericHeader="background">
Results and discussion
</sectionHeader>
<bodyText confidence="0.990579857142857">
We computed Fleiss, Krippendorff indexes
and Pairwise average using Geertzen, J. (2012)
software. Calculations were made for
processes without and with pre-annotation.
Pre-annotation applies the explained
methodology. We present results in Table 1
and 2.
</bodyText>
<figure confidence="0.756567944444444">
Results without applying pre-annotation
The experiment had 3 annotators, 101 cases,
and 1 variable with 303 decisions.
Fleiss Krippendorff Pairwise
avg.
A_obs = 0.554 D_obs = 0.446 % agr = 55.4
A_exp= 0.274 D_exp = 0.728 Kappa=0.405
Kappa = 0.386 Alpha = 0.388
Table 1: Results for inter-annotator agreement
without pre-annotation
Results applying pre-annotation
The experiment had 3 annotators, 101 cases,
and 1 variable with 303 decisions.
Fleiss Krippendorff Pairwise
avg.
A_obs=0.845 D_obs = 0.155 % agr= 84.5
A_esp=0.365 D_esp = 0.637 Kappa=0.756
Kappa=0.756 Alpha = 0.756
</figure>
<tableCaption confidence="0.475498">
Table 2: Results for inter-annotator agreement
with pre-annotation
</tableCaption>
<page confidence="0.911374">
5
</page>
<bodyText confidence="0.988652145454546">
Conclusions and future work
Results without pre-annotation presented low
inter-annotator agreement values. We could
explicate this, due to the complexity that have
the process for defining functions in a medium
granularity scheme with at least five functions.
We consider that a five-function scheme
allows differentiating citation functions. We
tested the methodology with a scheme with
this number of classes. Annotators read
carefully the articles but, without a pre-
annotation process, results were poor because
annotators had to take into account too many
details and even with a through reading, text
structure is difficult to appreciate.
There is a big improvement in inter-
annotator agreement using the proposed
methodology that includes a pre-annotation
process of a citation context with a fixed one-
paragraph length. The previous process of
extracting ontological concepts and cue words
allowed that annotator could see more clearly
sentence structure and facilitate decision
making about the citation function
classification. The result is a very significant
enhancement of inter-annotator agreement that
validates the use of the proposed methodology.
With the proposed annotation methodology
the agreement percentage, without a random
correction is 84.5% and Kappa index is 0,756.
According Landis and Koch (1977), a K =
0,756 corresponds to a substantial annotator
agreement, while the initial results, without
pre-annotation corresponded to a minimum
value which was not enough to keep on
working in the topic.
We plan to annotate a sufficient number of
articles using this methodology together with
a non-ambiguous and complete scheme of
annotation. The annotations generated,
ontological patterns and cue words will serve
to mine in an automatic way in a non-
annotated corpus. Thus, we will continue to
expand a basic corpus for the development of
research in citation function analysis.
Our intention is to make available to the
scientific community this dataset to facilitate
research in order to develop better systems to
evaluate the citation impact in scientific
literature. The purpose of these systems will be
to take into account new factors that can be
incorporated in the calculation of indexes to
better assess function, significance and
disposition of an author towards the scientific
work of another that was referenced.
</bodyText>
<sectionHeader confidence="0.984281" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9987895">
This research work has been partially funded
by the Spanish Government and the European
Commission through the project, ATTOS
(TIN2012-38536-C03- 03), LEGOLANG
(TIN2012-31224), SAM (FP7- 611312) and
FIRST (FP7-287607).
</bodyText>
<sectionHeader confidence="0.997782" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999223944444445">
Artstein, R., and Poesio, M. &amp;quot;Inter-coder agreement
for computational linguistics.&amp;quot; Computational
Linguistics 34.4 (2008): 555-596.
Ding, Y., Zhang, G., Chambers, T., Song, M.,
Wang, X., &amp; Zhai, C. (2014). Content‐based
citation analysis: The next generation of citation
analysis. Journal of the Association for
Information Science and Technology.
Geertzen, J. (2012). Inter-Rater Agreement with
multiple raters and variables. Retrieved
November 16, 2014, from
https://mlnl.net/jg/software/ira
Guthrie, D., Allison, B., Liu, W., Guthrie, L., &amp;
Wilks, Y. (2006). A closer look at skip-gram
modelling. In Proceedings of the 5th
international Conference on Language
Resources and Evaluation (LREC-2006) (pp. 1-
4).
</reference>
<page confidence="0.984228">
6
</page>
<reference confidence="0.999399">
Hernández, M., &amp; Gómez, J. M. (2014). Survey in
sentiment, polarity and function analysis of
citation. ACL 2014, 102.
Krippendorff, K. 2004. Reliability in content
analysis: Some common misconceptions and
recommendations. Human Communication
Research, 30(3):411–433.
Landis, J. R., &amp; Koch, G. G. (1977). The
measurement of observer agreement for
categorical data. Biometrics, 159-174.
Zock, M., &amp; Tesfaye, D. (2012, December).
Automatic index creation to support navigation
in lexical graphs encoding part_of relations. In
24th International Conference on Computational
Linguistics (p. 33).
</reference>
<page confidence="0.999518">
7
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.714351">
<title confidence="0.978575">Corpus annotation methodology for citation classification scientific literature</title>
<author confidence="0.982459">Myriam Hernández Álvarez José Gómez Soriano</author>
<affiliation confidence="0.968824333333333">Escuela Politécnica Nacional Universidad de Alicante Facultad de Ingeniería de Sistemas Dpto. de Lenguajes y Sistemas Quito, Ecuador Informáticos</affiliation>
<address confidence="0.967309">myriam.hernandez@epn.edu.ec Alicante, España</address>
<email confidence="0.9728">jmgomez@ua.es</email>
<abstract confidence="0.993688086956522">Since, at the moment, there is not a goldstandard annotated corpus to allow generation and testing of automatic systems for classifying the purpose or function of a citation referenced in an article; it is necessary to build one, for this objective. The development of this kind of corpus is subject to two conditions: the first one is to present a clear and unambiguous classification scheme. The second one is to secure an initial manual process of labeling to reach a sufficient inter-coder agreement among annotators to validate the annotation scheme and to be able to reproduce it even with coders who do not know in depth the topic of the analyzed articles. This paper proposes and validates a methodology for corpus annotation for citation classification in scientific literature that facilitate annotation and produces substantial interannotator agreement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Artstein</author>
<author>M Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.&amp;quot;</title>
<date>2008</date>
<journal>Computational Linguistics</journal>
<volume>34</volume>
<pages>555--596</pages>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Artstein, R., and Poesio, M. &amp;quot;Inter-coder agreement for computational linguistics.&amp;quot; Computational Linguistics 34.4 (2008): 555-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ding</author>
<author>G Zhang</author>
<author>T Chambers</author>
<author>M Song</author>
<author>X Wang</author>
<author>C Zhai</author>
</authors>
<title>Content‐based citation analysis: The next generation of citation analysis.</title>
<date>2014</date>
<journal>Journal of the Association for Information Science and Technology.</journal>
<contexts>
<context position="2950" citStr="Ding, Zhang, Chambers, Song, Wang, and Zhai, 2014" startWordPosition="449" endWordPosition="456"> complete metrics that take into account the content about cited work to assess its impact and relevance. It is necessary the construction of a new impact index enriched with qualitative criteria regarding the citation. This process requires a content analysis of the context containing citations to obtain certain important features such as intent or purpose of 1 the citing author when made the reference. Content analysis is a group of procedures to recollect and organized information in standard format to make inferences about its characteristics and meaning using manual or automatic methods (Ding, Zhang, Chambers, Song, Wang, and Zhai, 2014). This analysis could be automatic starting from a tagged corpus to build a model. Since, there is not a gold-standard annotated corpus for citation analysis data, it is necessary to work in the generation of one in order to facilitate collaborative work and results comparison among researches. Development of a corpus starts from the definition of a citation classification scheme that considers function (purpose), polarity (disposition) and influence of cited paper to produce a reliable and reproducible data set that could be the basis for future work in this area. This tagged corpus will all</context>
</contexts>
<marker>Ding, Zhang, Chambers, Song, Wang, Zhai, 2014</marker>
<rawString>Ding, Y., Zhang, G., Chambers, T., Song, M., Wang, X., &amp; Zhai, C. (2014). Content‐based citation analysis: The next generation of citation analysis. Journal of the Association for Information Science and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Geertzen</author>
</authors>
<title>Inter-Rater Agreement with multiple raters and variables. Retrieved</title>
<date>2012</date>
<marker>Geertzen, 2012</marker>
<rawString>Geertzen, J. (2012). Inter-Rater Agreement with multiple raters and variables. Retrieved</rawString>
</citation>
<citation valid="false">
<date>2014</date>
<note>from https://mlnl.net/jg/software/ira</note>
<contexts>
<context position="5353" citStr="(2014)" startWordPosition="826" endWordPosition="826">ording to citation function. The condition for these schemes were that categories were well distinguished. In this process, we detected two sources or error that affected results and did not allow good agreement among coders. One had to do with usage by the annotators of context of different length, which lend them to obtain discrepant results; the other was lack of clarity in the analyzed articles that made difficult to find enough sense in text to reach a unique citation classification. We corrected the first factor setting fixed criteria for determining context length. Hernández and Gómez, (2014) highlighted the need for defining context in view of argument detection, so the context include all sentences around citation that are talking about it. However, due to the complexity of this task, we decided to replace argument detection by fixing a context delimited by a complete paragraph. The rationale for this decision was that, by definition, a paragraph is a group of related sentences about the same idea. We assumed that author’s purpose when making a citation could be found using cue words and ontological concepts that are within the same paragraph. To avoid the second error source, w</context>
</contexts>
<marker>2014</marker>
<rawString>November 16, 2014, from https://mlnl.net/jg/software/ira</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Guthrie</author>
<author>B Allison</author>
<author>W Liu</author>
<author>L Guthrie</author>
<author>Y Wilks</author>
</authors>
<title>A closer look at skip-gram modelling.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th international Conference on Language Resources and Evaluation (LREC-2006)</booktitle>
<pages>1--4</pages>
<marker>Guthrie, Allison, Liu, Guthrie, Wilks, 2006</marker>
<rawString>Guthrie, D., Allison, B., Liu, W., Guthrie, L., &amp; Wilks, Y. (2006). A closer look at skip-gram modelling. In Proceedings of the 5th international Conference on Language Resources and Evaluation (LREC-2006) (pp. 1-4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hernández</author>
<author>J M Gómez</author>
</authors>
<title>Survey in sentiment, polarity and function analysis of citation. ACL</title>
<date>2014</date>
<pages>102</pages>
<contexts>
<context position="5353" citStr="Hernández and Gómez, (2014)" startWordPosition="823" endWordPosition="826">ification schemes according to citation function. The condition for these schemes were that categories were well distinguished. In this process, we detected two sources or error that affected results and did not allow good agreement among coders. One had to do with usage by the annotators of context of different length, which lend them to obtain discrepant results; the other was lack of clarity in the analyzed articles that made difficult to find enough sense in text to reach a unique citation classification. We corrected the first factor setting fixed criteria for determining context length. Hernández and Gómez, (2014) highlighted the need for defining context in view of argument detection, so the context include all sentences around citation that are talking about it. However, due to the complexity of this task, we decided to replace argument detection by fixing a context delimited by a complete paragraph. The rationale for this decision was that, by definition, a paragraph is a group of related sentences about the same idea. We assumed that author’s purpose when making a citation could be found using cue words and ontological concepts that are within the same paragraph. To avoid the second error source, w</context>
</contexts>
<marker>Hernández, Gómez, 2014</marker>
<rawString>Hernández, M., &amp; Gómez, J. M. (2014). Survey in sentiment, polarity and function analysis of citation. ACL 2014, 102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Krippendorff</author>
</authors>
<title>Reliability in content analysis: Some common misconceptions and recommendations.</title>
<date>2004</date>
<journal>Human Communication Research,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="14044" citStr="Krippendorff, 2004" startWordPosition="2125" endWordPosition="2126">The application of this strategy allows identifying punctual lexical entries and their relation to semantic features. An ontological pattern here is a structure that conveys authors’ purpose to cite. By using that, we expect not only to obtain a good level of agreement among the annotators, but also to minimize the human effort needed for annotating papers and populate a big corpus by converting the patterns into regular expressions. Experiment setup and results Three annotators collaborated. The annotation process comply three requirements in order to achieve reliability and reproducibility (Krippendorff, 2004). The annotators had a profile that allow them a good understanding of the scientific texts in computational linguistics; they worked in an independent way and they had a clear function classification scheme with detail instructions. To test annotation reliability, we measured inter-annotator agreement in a small section of the corpus; the same people must review this sample. It is necessary to achieve a good rate in this agreement because it certifies that the process is reliable and reproducible and that results may be generalized to the complete process in which probably are going to work n</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Krippendorff, K. 2004. Reliability in content analysis: Some common misconceptions and recommendations. Human Communication Research, 30(3):411–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<pages>159--174</pages>
<contexts>
<context position="17170" citStr="Landis and Koch (1977)" startWordPosition="2587" endWordPosition="2590">ing the proposed methodology that includes a pre-annotation process of a citation context with a fixed oneparagraph length. The previous process of extracting ontological concepts and cue words allowed that annotator could see more clearly sentence structure and facilitate decision making about the citation function classification. The result is a very significant enhancement of inter-annotator agreement that validates the use of the proposed methodology. With the proposed annotation methodology the agreement percentage, without a random correction is 84.5% and Kappa index is 0,756. According Landis and Koch (1977), a K = 0,756 corresponds to a substantial annotator agreement, while the initial results, without pre-annotation corresponded to a minimum value which was not enough to keep on working in the topic. We plan to annotate a sufficient number of articles using this methodology together with a non-ambiguous and complete scheme of annotation. The annotations generated, ontological patterns and cue words will serve to mine in an automatic way in a nonannotated corpus. Thus, we will continue to expand a basic corpus for the development of research in citation function analysis. Our intention is to ma</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>Landis, J. R., &amp; Koch, G. G. (1977). The measurement of observer agreement for categorical data. Biometrics, 159-174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zock</author>
<author>D Tesfaye</author>
</authors>
<title>Automatic index creation to support navigation in lexical graphs encoding part_of relations.</title>
<date>2012</date>
<booktitle>In 24th International Conference on Computational Linguistics (p. 33).</booktitle>
<marker>Zock, Tesfaye, 2012</marker>
<rawString>Zock, M., &amp; Tesfaye, D. (2012, December). Automatic index creation to support navigation in lexical graphs encoding part_of relations. In 24th International Conference on Computational Linguistics (p. 33).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002176">
<title confidence="0.990909">
Efficient Logical Inference for Semantic Processing
</title>
<author confidence="0.9966">
Ran Tian∗ Yusuke Miyao Takuya Matsuzaki
</author>
<affiliation confidence="0.996292">
National Institute of Informatics, Japan
</affiliation>
<email confidence="0.997863">
{tianran,yusuke,takuya-matsuzaki}@nii.ac.jp
</email>
<sectionHeader confidence="0.993408" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.985037692307692">
Dependency-based Compositional Se-
mantics (DCS) provides a precise and
expressive way to model semantics of
natural language queries on relational
databases, by simple dependency-like
trees. Recently abstract denotation is pro-
posed to enable generic logical inference
on DCS. In this paper, we discuss some
other possibilities to equip DCS with
logical inference, and we discuss further
on how logical inference can help textual
entailment recognition, or other semantic
precessing tasks.
</bodyText>
<sectionHeader confidence="0.997916" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.979672203389831">
Dependency-based Compositional Semantics
(DCS) was proposed as an interface for querying
relational databases by natural language. It
features DCS trees as semantic representation,
with a structure similar to dependency trees. In
its basic version, a node of a DCS tree indicates
a table in the database, and an edge indicates a
join relation. Both ends of an edge are labeled by
a field of the corresponding table (Liang et al.,
2011). However, when DCS is applied to logical
inference on unrestricted texts, it is unrealistic to
assume an explicit database, because we cannot
prepare a database for everything in the world.
For this reason, DCS trees are detached from any
specific relational database, in a way that each
node of a DCS tree indicates a content word in a
sentence (thus no fixed set of possible word labels
for a DCS tree node), and each edge indicates
∗Current affiliation of the first author: Graduate School
of Information Sciences, Tohoku University, Japan. Email
address: tianran@ecei.tohoku.ac.jp
a semantic relation between two words. Labels
on the two ends of an edge, initially indicating
fields of tables in a database, are considered
as semantic roles of the corresponding words.
Abstract denotation is proposed to capture the
meaning of this abstract version of DCS tree,
and a textual inference system based on abstract
denotation is built (Tian et al., 2014).
It is quite natural to apply DCS trees, a simple
and expressive semantic representation, to textual
inference; however the use of abstract denotations
to convey logical inference is somehow unusual.
There are two seemingly obvious way to equip
DCS with logical inference: (i) at the tree level, by
defining a set of logically sound transformations
of DCS trees; or (ii) at the logic level, by convert-
ing DCS trees to first order predicate logic (FOL)
formulas and then utilizing a theorem prover. For
(i), it may not be easy to enumerate all types of
logically sound transformations, but tree transfor-
mations can be seen as an approximation of logical
inference. For (ii), abstract denotation is more ef-
ficient than FOL formula, because abstract deno-
tation eliminates quantifiers and meanings of nat-
ural language texts can be represented by atomic
sentences.
To elaborate the above discussion and to pro-
vide more topics to the literature, in this paper we
discuss the following four questions: (§2) How
well can tree transformation approximate logical
inference? (§3) With rigorous inference on DCS
trees, where does logic contribute in the system
of Tian et al. (2014)? (§4) Does logical inference
have further potentials in Recognizing Textual En-
tailment (RTE) task? and (§5) How efficient is ab-
stract denotation compared to FOL formula? We
provide examples or experimental results to the
above questions.
</bodyText>
<page confidence="0.987769">
71
</page>
<note confidence="0.299602">
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 71–75,
Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.99740135">
T : blame H :
Debby death
storm
tropical
cause
she#1
Virginia
damage
damage
T : storm H :
enter
tropical
enter storm cause
Hurrican_Isabel#1
Virginia
cause
storm
loss
life
significant
</figure>
<figureCaption confidence="0.681773333333333">
Figure 1: DCS trees of T: Tropical storm Debby is
blamed for death and H: A storm has caused loss
of life
</figureCaption>
<bodyText confidence="0.9759216">
2 Tree transformation vs. logical
inference
In the tree transformation based approach to RTE,
it has been realized that some gaps between T and
H cannot be filled even by a large number of tree
transformation rules extracted from corpus (Bar-
Haim et al., 2007a). For example in Figure 1, it
is possible to extract the rule blamed for death →
cause loss of life, but not easy to extract tropical
storm Debby → storm, because “Debby” could be
an arbitrary name which may not even appear in
the corpus.
This kind of gaps was typically addressed by
approximate matching methods, for example by
counting common sub-graphs of T and H, or by
computing a cost of tree edits that convert T to
H. In the example of Figure 1, we would expect
that T is “similar enough” (i.e. has many common
sub-graphs) with H, or the cost to convert T into H
(e.g. by deleting the node Debby and then add the
node storm) is low. As for how similar is enough,
or how the cost is evaluated, we will need a statis-
tical model to train on RTE development set.
It was neglected that some combinations of tree
edits are logical (while some are not). The entail-
ment pair in Figure 1 can be easily treated by log-
ical inference, as long as the apposition tropical
storm = Debby is appropriately handled. In con-
trast to graph matching or tree edit models which
theoretically admit arbitrary tree transformation,
logical inference clearly discriminate sound trans-
formations from unsound ones. In this sense, there
would be no need to train on RTE data.
When coreference is considered, logically
sound tree transformations can be quite compli-
cated. The following is a modified example from
RTE2-dev:
T: Hurricane Isabel, which caused significant
damage, was a tropical storm when she entered
Virginia.
</bodyText>
<figureCaption confidence="0.8671905">
Figure 2: DCS trees with coreference
H: A storm entered Virginia, causing damage.
</figureCaption>
<bodyText confidence="0.997581692307692">
The corresponding DCS trees are shown in Fig-
ure 2. Though the DCS trees of T and H are
quite different, H can actually be proven from T.
Note the coreference between Hurricane Isabel
and she, suggesting us to copy the subtree of Hur-
ricane Isabel to she, in a tree edit approach. This
is not enough yet, because the head storm in T is
not placed at the subject of cause. The issue is in-
deed very logical: from “Hurricane Isabel = she”,
“Hurricane Isabel = storm”, “she = subject of en-
ter” and “Hurricane Isabel = subject of cause”,
we can imply that “storm = subject of enter = sub-
ject of cause”.
</bodyText>
<sectionHeader confidence="0.943153" genericHeader="method">
3 Alignment with logical clues
</sectionHeader>
<bodyText confidence="0.998538730769231">
Tian et al. (2014) proposed a way to generate on-
the-fly knowledge to fill knowledge gaps: if H is
not proven, compare DCS trees of T and H to
generate path alignments (e.g. blamed for death
∼ cause loss of life, as underscored in Figure 1);
evaluate the path alignments by a similarity score
function; and path alignments with a score greater
than a threshold (0.4) are accepted and converted
to inference rules.
The word vectors Tian et al. (2014) use to
calculate similarities are reported able to cap-
ture semantic compositions by simple additions
and subtractions (Mikolov et al., 2013). This is
also the case when used as knowledge resource
for RTE, for example the similarities between
blamed+death and cause+loss+life, or between
found+shot+dead and killed, are computed &gt;
0.4.
However, generally such kind of similarity is
very noisy. Tian et al. (2014) used some logical
clues to filter out irrelevant path alignments, which
helps to keep a high precision. To evaluate the
effect of such logical filters, we compare it with
some other alignment strategies, the performance
of which on RTE5-test data is shown in Table 1.
Each strategy is described in the following.
</bodyText>
<page confidence="0.990208">
72
</page>
<table confidence="0.957331">
Strategy Prec. Rec. Acc.
LogicClue + Inference 69.9 55.0 65.7
LexNoun + Inference 64.2 57.3 62.7
LexNoun + Coverage 57.1 75.0 59.3
NoFilter + Coverage 54.2 87.7 56.8
</table>
<figure confidence="0.9895674">
6
5
4
3
2
1
R2 = 0.24
100 1000 10000 100000 1000000
Weight of statements
Time (Sec.)
</figure>
<tableCaption confidence="0.7561555">
Table 1: Comparison of different alignment strate-
gies
</tableCaption>
<bodyText confidence="0.999245947368421">
LogicClue + Inference This is the system of
Tian et al. (2014)1, which use logical clues to filter
out irrelevant path alignments, and apply accepted
path alignments as inference rules.
LexNoun + Inference The same system as
above, except that we only align paths between
lexically aligned nouns. Two nouns are aligned
if and only if they are synonyms, hyponyms or
derivatively related in WordNet.
LexNoun + Coverage As above, paths between
lexically aligned nouns are aligned, and aligned
paths with similarity score &gt; 0.4 are accepted. If
all nodes in H can be covered by some accepted
path alignments, then output “Y”. This is very
similar to the system described in Bar-Haim et al.
(2007b).
NoFilter + Coverage Same as above, but all
paths alignments with similarity score &gt; 0.4 are
accepted.
</bodyText>
<sectionHeader confidence="0.939474" genericHeader="method">
4 How can logical inference help RTE?
</sectionHeader>
<bodyText confidence="0.920695736842105">
Logical inference is shown to be useful for RTE,
as Tian et al. (2014) demonstrates a system with
competitive results. However, despite the expec-
tation that all entailment matters can be explained
logically, our observation is that currently logical
inference only fills very limited short gaps from T
to H. The logical phenomena easily addressed by
Tian et al. (2014)’s framework, namely universal
quantifiers and negations, seems rare in PASCAL
RTE data. Most heavy lifting is done by distribu-
tional similarities between phrases, which may fail
in complicated sentences. An especially complex
example is:
T: Wal-Mart Stores Inc. said Tuesday that a Mas-
sachusetts judge had granted its motion to decer-
tify a class action lawsuit accusing the world’s
largest retailer of denying employees breaks.
H: Employee breaks had been denied by a motion
granted by a Massachusetts judge.
</bodyText>
<footnote confidence="0.982551">
1http://kmcs.nii.ac.jp/tianran/tifmo/
</footnote>
<figureCaption confidence="0.998033">
Figure 3: Time of forward-chaining (seconds) in
our system, plotted on weights of statements (log-
arithmic scale).
</figureCaption>
<table confidence="0.998862166666667">
Orig. 3 Sec. Orig. 5 Min. Red. 5 Min.
Proof found 8 16 82
Too many variables 5 24 3
Failed to find proof 0 1 3
Memory limit 0 2 0
Time out 86 57 13
</table>
<tableCaption confidence="0.999668">
Table 2: Proportion (%) of exit status of Prover9
</tableCaption>
<bodyText confidence="0.999817769230769">
The system of Tian et al. (2014) generated on-
the-fly knowledge to join several fragments in T
and wrongly proved H. In examples of such com-
plexity, distributional similarity is no longer reli-
able. However, it may be possible to build a pri-
ori logical models at the meta level, such as on
epistemic, intentional and reportive attitudes. The
models then can provide signals for semantic pars-
ing to connect the logic to natural language, such
as the words “grant”, “decertify”, and “accuse” in
the above example. We hope this approach can
bring new progress to RTE and other semantic pro-
cessing tasks.
</bodyText>
<sectionHeader confidence="0.813447" genericHeader="method">
5 Efficiency of abstract denotations
</sectionHeader>
<bodyText confidence="0.999991142857143">
To evaluate the efficiency of logical inference on
abstract denotations, we took 110 true entailment
pairs from RTE5 development set, which are also
pairs that can be proven with on-the-fly knowl-
edge. We plot the running time of Tian et al.
(2014)’s inference engine (single-threaded) on a
2.27GHz Xeon CPU, with respect to the weighted
sum of all statements2, as shown in Figure 3. The
graph shows all pairs can be proven in 6 seconds,
and proof time scales logarithmically on weight of
statements.
On the other hand, we converted statements on
abstract denotations into FOL formulas, and tried
to prove the same pairs using Prover9,3 a popu-
</bodyText>
<footnote confidence="0.99520325">
2If a statement is translated to FOL formula, the weight of
this statement equals to the weighted sum of all predicates in
the FOL formula, where an n-ary predicate is weighted as n.
3www.cs.unm.edu/˜mccune/prover9/
</footnote>
<page confidence="0.999041">
73
</page>
<bodyText confidence="0.999648722222222">
lar FOL theorem prover. As the result turns out
(Table 2), only 8% of the pairs can be proven in
3 seconds (the “Orig. 3 Sec.” column), and only
16% pairs can be proven in 5 minutes (the “Orig.
5 Min.” column), showing severe difficulties for
an FOL prover to handle textual inferences with
many (usually hundreds of) on-the-fly rules. As
such, we use Tian et al. (2014)’s inference engine
to pin down statements that are actually needed for
proving H (usually just 2 or 3 statements), and try
to prove H by Prover9 again, using only necessary
statements. Proven pairs in 5 minutes then jump
to 82% (the “Red. 5 Min.” column), showing that
a large number of on-the-fly rules may drastically
increase computation cost. Still, nearly 20% pairs
cannot be proven even in this setting, suggesting
that traditional FOL prover is not suited for tex-
tual inference.
</bodyText>
<sectionHeader confidence="0.991114" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999931310344828">
We have discussed the role that logical infer-
ence could play in RTE task, and the efficiency
of performing inference on abstract denotations.
Though currently logical inference contributes at
places that are somehow inconspicuous, there is
the possibility that with some meta level logical
models and the methodology of semantic parsing,
we can build systems that understand natural lan-
guage texts deeply: logic implies (in)consistency,
which is in turn used as signals to produce more
accurate semantic interpretation. And after all, as
there may be many possible variations of seman-
tic representations, it is good to have an efficient
inference framework that has the potential to con-
nect them. It would be exciting if we can combine
different types of structured data with natural lan-
guage in semantic processing tasks. Directions of
our future work are described below.
Improvement of similarity score To calculate
phrase similarities, Tian et al. (2014) use the co-
sine similarity of sums of word vectors, which ig-
nores syntactic information. We plan to add syn-
tactic information to words by some supertags,
and learn a vector space embedding for this struc-
ture.
Integration of FreeBase to RTE It would be
exciting if we can utilize the huge amount of Free-
Base data in RTE task. Using the framework of
abstract denotation, meanings of sentences can be
explained as relational database queries; to convert
it to FreeBase data queries is like relational to on-
tology schema matching. In order to make effec-
tive use of FreeBase data, we also need to recog-
nize entities and relations in natural language sen-
tences. Previous research on semantic parsing will
be very helpful for learning such mapping.
Winograd Schema Challenge (WSC) As the
RTE task, WSC (Levesque et al., 2012) also pro-
vides a test bed for textual inference systems. A
Winograd schema is a pair of similar sentences but
contain an ambiguity of pronouns that is resolved
in opposite ways. A complicated partial example
is:
Michael decided to freeze himself in
cryo-stasis even though his father was
against it, because he hopes to be un-
frozen in the future when there is a cure
available.
The logical interplay among decided, hopes,
even though, because, and the realization that he
is coreferent to Michael (but not his father) is in-
triguing. By working on the task, we hope to gain
further understanding on how knowledge can be
gathered and applied in natural language reason-
ing.
Acknowledgments This research was supported
by the Todai Robot Project at National Institute of
Informatics.
</bodyText>
<sectionHeader confidence="0.999257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99979415">
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007a. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI 2007.
Roy Bar-Haim, Ido Dagan, Iddo Greental, Idan Szpek-
tor, and Moshe Friedman. 2007b. Semantic in-
ference at the lexical-syntactic level for textual en-
tailment recognition. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing.
Hector Levesque, Ernest Davis, and Leora Morgen-
stern. 2012. The winograd schema challenge. In
Knowledge Representation and Reasoning Confer-
ence.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of ACL 2011.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL
2013.
</reference>
<page confidence="0.980238">
74
</page>
<reference confidence="0.989067">
Ran Tian, Yusuke Miyao, and Matsuzaki Takuya.
2014. Logical inference on dependency-based com-
positional semantics. In Proceedings of ACL 2014.
</reference>
<page confidence="0.999131">
75
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.982838">
<title confidence="0.99989">Efficient Logical Inference for Semantic Processing</title>
<author confidence="0.996067">Yusuke Miyao Takuya Matsuzaki</author>
<affiliation confidence="0.999866">National Institute of Informatics,</affiliation>
<abstract confidence="0.999010428571429">Dependency-based Compositional Semantics (DCS) provides a precise and expressive way to model semantics of natural language queries on relational databases, by simple dependency-like trees. Recently abstract denotation is proposed to enable generic logical inference on DCS. In this paper, we discuss some other possibilities to equip DCS with logical inference, and we discuss further on how logical inference can help textual entailment recognition, or other semantic precessing tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
</authors>
<title>Ido Dagan, Iddo Greental, and Eyal Shnarch.</title>
<date>2007</date>
<booktitle>In Proceedings of AAAI</booktitle>
<marker>Bar-Haim, 2007</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal Shnarch. 2007a. Semantic inference at the lexicalsyntactic level. In Proceedings of AAAI 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Iddo Greental</author>
<author>Idan Szpektor</author>
<author>Moshe Friedman</author>
</authors>
<title>Semantic inference at the lexical-syntactic level for textual entailment recognition.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing.</booktitle>
<contexts>
<context position="8584" citStr="Bar-Haim et al. (2007" startWordPosition="1424" endWordPosition="1427"> use logical clues to filter out irrelevant path alignments, and apply accepted path alignments as inference rules. LexNoun + Inference The same system as above, except that we only align paths between lexically aligned nouns. Two nouns are aligned if and only if they are synonyms, hyponyms or derivatively related in WordNet. LexNoun + Coverage As above, paths between lexically aligned nouns are aligned, and aligned paths with similarity score &gt; 0.4 are accepted. If all nodes in H can be covered by some accepted path alignments, then output “Y”. This is very similar to the system described in Bar-Haim et al. (2007b). NoFilter + Coverage Same as above, but all paths alignments with similarity score &gt; 0.4 are accepted. 4 How can logical inference help RTE? Logical inference is shown to be useful for RTE, as Tian et al. (2014) demonstrates a system with competitive results. However, despite the expectation that all entailment matters can be explained logically, our observation is that currently logical inference only fills very limited short gaps from T to H. The logical phenomena easily addressed by Tian et al. (2014)’s framework, namely universal quantifiers and negations, seems rare in PASCAL RTE data.</context>
</contexts>
<marker>Bar-Haim, Dagan, Greental, Szpektor, Friedman, 2007</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Iddo Greental, Idan Szpektor, and Moshe Friedman. 2007b. Semantic inference at the lexical-syntactic level for textual entailment recognition. In Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hector Levesque</author>
<author>Ernest Davis</author>
<author>Leora Morgenstern</author>
</authors>
<title>The winograd schema challenge.</title>
<date>2012</date>
<booktitle>In Knowledge Representation and Reasoning Conference.</booktitle>
<contexts>
<context position="14115" citStr="Levesque et al., 2012" startWordPosition="2347" endWordPosition="2350">s structure. Integration of FreeBase to RTE It would be exciting if we can utilize the huge amount of FreeBase data in RTE task. Using the framework of abstract denotation, meanings of sentences can be explained as relational database queries; to convert it to FreeBase data queries is like relational to ontology schema matching. In order to make effective use of FreeBase data, we also need to recognize entities and relations in natural language sentences. Previous research on semantic parsing will be very helpful for learning such mapping. Winograd Schema Challenge (WSC) As the RTE task, WSC (Levesque et al., 2012) also provides a test bed for textual inference systems. A Winograd schema is a pair of similar sentences but contain an ambiguity of pronouns that is resolved in opposite ways. A complicated partial example is: Michael decided to freeze himself in cryo-stasis even though his father was against it, because he hopes to be unfrozen in the future when there is a cure available. The logical interplay among decided, hopes, even though, because, and the realization that he is coreferent to Michael (but not his father) is intriguing. By working on the task, we hope to gain further understanding on ho</context>
</contexts>
<marker>Levesque, Davis, Morgenstern, 2012</marker>
<rawString>Hector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Knowledge Representation and Reasoning Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1124" citStr="Liang et al., 2011" startWordPosition="158" endWordPosition="161">lities to equip DCS with logical inference, and we discuss further on how logical inference can help textual entailment recognition, or other semantic precessing tasks. 1 Introduction Dependency-based Compositional Semantics (DCS) was proposed as an interface for querying relational databases by natural language. It features DCS trees as semantic representation, with a structure similar to dependency trees. In its basic version, a node of a DCS tree indicates a table in the database, and an edge indicates a join relation. Both ends of an edge are labeled by a field of the corresponding table (Liang et al., 2011). However, when DCS is applied to logical inference on unrestricted texts, it is unrealistic to assume an explicit database, because we cannot prepare a database for everything in the world. For this reason, DCS trees are detached from any specific relational database, in a way that each node of a DCS tree indicates a content word in a sentence (thus no fixed set of possible word labels for a DCS tree node), and each edge indicates ∗Current affiliation of the first author: Graduate School of Information Sciences, Tohoku University, Japan. Email address: tianran@ecei.tohoku.ac.jp a semantic rel</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Percy Liang, Michael Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of ACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="7000" citStr="Mikolov et al., 2013" startWordPosition="1158" endWordPosition="1161"> 3 Alignment with logical clues Tian et al. (2014) proposed a way to generate onthe-fly knowledge to fill knowledge gaps: if H is not proven, compare DCS trees of T and H to generate path alignments (e.g. blamed for death ∼ cause loss of life, as underscored in Figure 1); evaluate the path alignments by a similarity score function; and path alignments with a score greater than a threshold (0.4) are accepted and converted to inference rules. The word vectors Tian et al. (2014) use to calculate similarities are reported able to capture semantic compositions by simple additions and subtractions (Mikolov et al., 2013). This is also the case when used as knowledge resource for RTE, for example the similarities between blamed+death and cause+loss+life, or between found+shot+dead and killed, are computed &gt; 0.4. However, generally such kind of similarity is very noisy. Tian et al. (2014) used some logical clues to filter out irrelevant path alignments, which helps to keep a high precision. To evaluate the effect of such logical filters, we compare it with some other alignment strategies, the performance of which on RTE5-test data is shown in Table 1. Each strategy is described in the following. 72 Strategy Pre</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In Proceedings of NAACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ran Tian</author>
<author>Yusuke Miyao</author>
<author>Matsuzaki Takuya</author>
</authors>
<title>Logical inference on dependency-based compositional semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="2079" citStr="Tian et al., 2014" startWordPosition="312" endWordPosition="315"> sentence (thus no fixed set of possible word labels for a DCS tree node), and each edge indicates ∗Current affiliation of the first author: Graduate School of Information Sciences, Tohoku University, Japan. Email address: tianran@ecei.tohoku.ac.jp a semantic relation between two words. Labels on the two ends of an edge, initially indicating fields of tables in a database, are considered as semantic roles of the corresponding words. Abstract denotation is proposed to capture the meaning of this abstract version of DCS tree, and a textual inference system based on abstract denotation is built (Tian et al., 2014). It is quite natural to apply DCS trees, a simple and expressive semantic representation, to textual inference; however the use of abstract denotations to convey logical inference is somehow unusual. There are two seemingly obvious way to equip DCS with logical inference: (i) at the tree level, by defining a set of logically sound transformations of DCS trees; or (ii) at the logic level, by converting DCS trees to first order predicate logic (FOL) formulas and then utilizing a theorem prover. For (i), it may not be easy to enumerate all types of logically sound transformations, but tree trans</context>
<context position="6429" citStr="Tian et al. (2014)" startWordPosition="1061" endWordPosition="1064"> are shown in Figure 2. Though the DCS trees of T and H are quite different, H can actually be proven from T. Note the coreference between Hurricane Isabel and she, suggesting us to copy the subtree of Hurricane Isabel to she, in a tree edit approach. This is not enough yet, because the head storm in T is not placed at the subject of cause. The issue is indeed very logical: from “Hurricane Isabel = she”, “Hurricane Isabel = storm”, “she = subject of enter” and “Hurricane Isabel = subject of cause”, we can imply that “storm = subject of enter = subject of cause”. 3 Alignment with logical clues Tian et al. (2014) proposed a way to generate onthe-fly knowledge to fill knowledge gaps: if H is not proven, compare DCS trees of T and H to generate path alignments (e.g. blamed for death ∼ cause loss of life, as underscored in Figure 1); evaluate the path alignments by a similarity score function; and path alignments with a score greater than a threshold (0.4) are accepted and converted to inference rules. The word vectors Tian et al. (2014) use to calculate similarities are reported able to capture semantic compositions by simple additions and subtractions (Mikolov et al., 2013). This is also the case when </context>
<context position="7955" citStr="Tian et al. (2014)" startWordPosition="1320" endWordPosition="1323"> which helps to keep a high precision. To evaluate the effect of such logical filters, we compare it with some other alignment strategies, the performance of which on RTE5-test data is shown in Table 1. Each strategy is described in the following. 72 Strategy Prec. Rec. Acc. LogicClue + Inference 69.9 55.0 65.7 LexNoun + Inference 64.2 57.3 62.7 LexNoun + Coverage 57.1 75.0 59.3 NoFilter + Coverage 54.2 87.7 56.8 6 5 4 3 2 1 R2 = 0.24 100 1000 10000 100000 1000000 Weight of statements Time (Sec.) Table 1: Comparison of different alignment strategies LogicClue + Inference This is the system of Tian et al. (2014)1, which use logical clues to filter out irrelevant path alignments, and apply accepted path alignments as inference rules. LexNoun + Inference The same system as above, except that we only align paths between lexically aligned nouns. Two nouns are aligned if and only if they are synonyms, hyponyms or derivatively related in WordNet. LexNoun + Coverage As above, paths between lexically aligned nouns are aligned, and aligned paths with similarity score &gt; 0.4 are accepted. If all nodes in H can be covered by some accepted path alignments, then output “Y”. This is very similar to the system descr</context>
<context position="9988" citStr="Tian et al. (2014)" startWordPosition="1658" endWordPosition="1661">y that a Massachusetts judge had granted its motion to decertify a class action lawsuit accusing the world’s largest retailer of denying employees breaks. H: Employee breaks had been denied by a motion granted by a Massachusetts judge. 1http://kmcs.nii.ac.jp/tianran/tifmo/ Figure 3: Time of forward-chaining (seconds) in our system, plotted on weights of statements (logarithmic scale). Orig. 3 Sec. Orig. 5 Min. Red. 5 Min. Proof found 8 16 82 Too many variables 5 24 3 Failed to find proof 0 1 3 Memory limit 0 2 0 Time out 86 57 13 Table 2: Proportion (%) of exit status of Prover9 The system of Tian et al. (2014) generated onthe-fly knowledge to join several fragments in T and wrongly proved H. In examples of such complexity, distributional similarity is no longer reliable. However, it may be possible to build a priori logical models at the meta level, such as on epistemic, intentional and reportive attitudes. The models then can provide signals for semantic parsing to connect the logic to natural language, such as the words “grant”, “decertify”, and “accuse” in the above example. We hope this approach can bring new progress to RTE and other semantic processing tasks. 5 Efficiency of abstract denotati</context>
<context position="11823" citStr="Tian et al. (2014)" startWordPosition="1970" endWordPosition="1973"> pairs using Prover9,3 a popu2If a statement is translated to FOL formula, the weight of this statement equals to the weighted sum of all predicates in the FOL formula, where an n-ary predicate is weighted as n. 3www.cs.unm.edu/˜mccune/prover9/ 73 lar FOL theorem prover. As the result turns out (Table 2), only 8% of the pairs can be proven in 3 seconds (the “Orig. 3 Sec.” column), and only 16% pairs can be proven in 5 minutes (the “Orig. 5 Min.” column), showing severe difficulties for an FOL prover to handle textual inferences with many (usually hundreds of) on-the-fly rules. As such, we use Tian et al. (2014)’s inference engine to pin down statements that are actually needed for proving H (usually just 2 or 3 statements), and try to prove H by Prover9 again, using only necessary statements. Proven pairs in 5 minutes then jump to 82% (the “Red. 5 Min.” column), showing that a large number of on-the-fly rules may drastically increase computation cost. Still, nearly 20% pairs cannot be proven even in this setting, suggesting that traditional FOL prover is not suited for textual inference. 6 Conclusion and future work We have discussed the role that logical inference could play in RTE task, and the ef</context>
<context position="13297" citStr="Tian et al. (2014)" startWordPosition="2206" endWordPosition="2209"> can build systems that understand natural language texts deeply: logic implies (in)consistency, which is in turn used as signals to produce more accurate semantic interpretation. And after all, as there may be many possible variations of semantic representations, it is good to have an efficient inference framework that has the potential to connect them. It would be exciting if we can combine different types of structured data with natural language in semantic processing tasks. Directions of our future work are described below. Improvement of similarity score To calculate phrase similarities, Tian et al. (2014) use the cosine similarity of sums of word vectors, which ignores syntactic information. We plan to add syntactic information to words by some supertags, and learn a vector space embedding for this structure. Integration of FreeBase to RTE It would be exciting if we can utilize the huge amount of FreeBase data in RTE task. Using the framework of abstract denotation, meanings of sentences can be explained as relational database queries; to convert it to FreeBase data queries is like relational to ontology schema matching. In order to make effective use of FreeBase data, we also need to recogniz</context>
</contexts>
<marker>Tian, Miyao, Takuya, 2014</marker>
<rawString>Ran Tian, Yusuke Miyao, and Matsuzaki Takuya. 2014. Logical inference on dependency-based compositional semantics. In Proceedings of ACL 2014.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
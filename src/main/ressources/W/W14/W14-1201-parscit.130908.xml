<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000309">
<title confidence="0.9985545">
One Step Closer to Automatic Evaluation
of Text Simplification Systems
</title>
<author confidence="0.964797">
Sanja ˇStajner1 and Ruslan Mitkov1 and Horacio Saggion2
</author>
<affiliation confidence="0.93046">
1Research Group in Computational Linguistics, University of Wolverhampton, UK
2TALN Research Group, Universitat Pompeu Fabra, Spain
</affiliation>
<email confidence="0.997155">
S.Stajner@wlv.ac.uk, R.Mitkov@wlv.ac.uk, horacio.saggion@upf.edu
</email>
<sectionHeader confidence="0.993882" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999730944444445">
This study explores the possibility of re-
placing the costly and time-consuming
human evaluation of the grammaticality
and meaning preservation of the output
of text simplification (TS) systems with
some automatic measures. The focus is on
six widely used machine translation (MT)
evaluation metrics and their correlation
with human judgements of grammatical-
ity and meaning preservation in text snip-
pets. As the results show a significant cor-
relation between them, we go further and
try to classify simplified sentences into:
(1) those which are acceptable; (2) those
which need minimal post-editing; and (3)
those which should be discarded. The pre-
liminary results, reported in this paper, are
promising.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999792191489362">
Lexically and syntactically complex sentences can
be difficult to understand for non-native speak-
ers (Petersen and Ostendorf, 2007; Alu´ısio et
al., 2008b), and for people with language impair-
ments, e.g. people diagnosed with aphasia (Car-
roll et al., 1999; Devlin, 1999), autism spectrum
disorder (ˇStajner et al., 2012; Martos et al., 2012),
dyslexia (Rello, 2012), congenital deafness (Inui
et al., 2003), and intellectual disability (Feng,
2009). At the same time, long and complex sen-
tences are also a stumbling block for many NLP
tasks and applications such as parsing, machine
translation, information retrieval, and summarisa-
tion (Chandrasekar et al., 1996). This justifies the
need for Text Simplification (TS) systems which
would convert such sentences into their simpler
and easier-to-read variants, while at the same time
preserving the original meaning.
So far, TS systems have been developed for En-
glish (Siddharthan, 2006; Zhu et al., 2010; Wood-
send and Lapata, 2011a; Coster and Kauchak,
2011; Wubben et al., 2012), Spanish (Saggion et
al., 2011), and Portuguese (Alu´ısio et al., 2008a),
with recent attempts at Basque (Aranzabe et al.,
2012), Swedish (Rybing et al., 2010), Dutch
(Ruiter et al., 2010), and Italian (Barlacchi and
Tonelli, 2013).
Usually, TS systems are either evaluated for: (1)
the quality of the generated output, or (2) the effec-
tiveness/usefulness of such simplification on read-
ing speed and comprehension of the target popula-
tion. For the purpose of this study we focused only
on the former. The quality of the output generated
by TS systems is commonly evaluated by using
a combination of readability metrics (measuring
the degree of simplification) and human assess-
ment (measuring the grammaticality and meaning
preservation). Despite the noticeable similarity
between evaluation of the fluency and adequacy of
a machine translation (MT) output, and evaluation
of grammaticality and meaning preservation of a
TS system output, there have been no works ex-
ploring whether any of the MT evaluation metrics
are well correlated with the latter, and could thus
replace the time-consuming human assessment.
The contributions of the present work are the
following:
</bodyText>
<listItem confidence="0.996755363636364">
• It is the first study to explore the possibility of
replacing human assessment of the quality of
TS system output with automatic evaluation.
• It is the first study to investigate the correla-
tion of human assessment of TS system out-
put with MT evaluation metrics.
• It proposes a decision-making procedure for
the classification of simplified sentences into:
(1) those which are acceptable; (2) those
which need further post-editing; and (3) those
which should be discarded.
</listItem>
<page confidence="0.773013">
1
</page>
<note confidence="0.9974725">
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 1–10,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.997368" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999826173913043">
The output of the TS system proposed by Sid-
dharthan (2006) was rated for grammaticality and
meaning preservation by three human evaluators.
Similarly, Drndarevic et al. (2013) evaluated the
grammaticality and the meaning preservation of
automatically simplified Spanish sentences on a
Likert scale with the help of twenty-five human
annotators. Additionally, the authors used seven
readability metrics to assess the degree of simplifi-
cation. Woodsend and Lapata (2011b), and Glavaˇs
and ˇStajner (2013) used human annotators’ rat-
ings for evaluating simplification, meaning preser-
vation, and grammaticality, while additionally ap-
plying several readability metrics for evaluating
complexity reduction in entire texts.
Another set of studies approached TS as an MT
task translating from “original” to “simplified”
language, e.g. (Specia, 2010; Woodsend and Lap-
ata, 2011a; Zhu et al., 2010). In this case, the qual-
ity of the output generated by the system was eval-
uated using several standard MT evaluation met-
rics: BLEU (Papineni et al., 2002), NIST (Dod-
dington, 2002), and TERp (Snover et al., 2009).
</bodyText>
<sectionHeader confidence="0.998156" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.99998125">
All experiments were conducted on a freely avail-
able sentence-level dataset1, fully described in
(Glavaˇs and ˇStajner, 2013), and the two datasets
we derived from it. The original dataset and the
instructions for the human assessment are given in
the next two subsections. Section 3.3 explains how
we derived two additional datasets from the origi-
nal one, and to what end. Section 3.4 describes the
automatic MT evaluation metrics used as features
in correlation and classification experiments; Sec-
tion 3.5 presents the main goals of the study; and
Section 3.6 describes the conducted experiments.
</bodyText>
<subsectionHeader confidence="0.999181">
3.1 Original dataset
</subsectionHeader>
<bodyText confidence="0.9999925">
The dataset contains 280 pairs of original sen-
tences and their corresponding simplified versions
annotated by humans for grammaticality, meaning
preservation, and simplicity of the simplified ver-
sion. We used all sentence pairs, focusing only on
four out of eight available features: (1) the original
text, (2) the simplified text, (3) the grammaticality
score, and (4) the score for meaning preservation.2
</bodyText>
<footnote confidence="0.999593333333333">
1http://takelab.fer.hr/data/evsimplify/
2The other four features contain the pairID, groupID, the
method with which the simplification was obtained, and the
</footnote>
<table confidence="0.999135">
Category weighted κ Pearson MAE
Grammaticality 0.68 0.77 0.18
Meaning 0.53 0.67 0.37
Simplicity 0.54 0.60 0.28
</table>
<tableCaption confidence="0.999823">
Table 1: IAA from (Glavaˇs and ˇStajner, 2013)
</tableCaption>
<bodyText confidence="0.999862625">
The simplified versions of original sentences
were obtained by using four different simplifi-
cation methods: baseline, sentence-wise, event-
wise, and pronominal anaphora. The baseline re-
tains only the main clause of a sentence, and dis-
cards all subordinate clauses, based on the out-
put of the Stanford constituency parser (Klein and
Manning, 2003). Sentence-wise simplification
eliminates all those tokens in the original sentence
that do not belong to any of the extracted factual
event mentions, while the event-wise simplifica-
tion transforms each factual event mention into a
separate sentence of the output. The last simplifi-
cation scheme (pronominal anaphora) additionally
employs pronominal anaphora resolution on top of
the event-wise simplification scheme.3
</bodyText>
<subsectionHeader confidence="0.999725">
3.2 Human Assessment
</subsectionHeader>
<bodyText confidence="0.999981095238095">
Human assessors were asked to score the given
sentence pairs (or text snippets in the case of split
sentences) on a 1–3 scale based on three crite-
ria: Grammaticality (1 – ungrammatical, 2 – mi-
nor problems with grammaticality, 3 – grammati-
cal), Meaning (1 – meaning is seriously changed
or most of the relevant information lost, 2 – some
of the relevant information is lost but the meaning
of the remaining information is unchanged, 3 – all
relevant information is kept without any change in
meaning), and Simplicity (1 – a lot of irrelevant in-
formation is retained, 2 – some of irrelevant infor-
mation is retained, 3 – all irrelevant information is
eliminated). The inter-annotator agreement (IAA)
was calculated using weighted Kappa (weighted
κ), Pearson’s correlation (Pearson), and mean av-
erage error (MAE), and the obtained results are
presented in Table 1. A few examples of assigned
scores are given in Table 2, where G, M, and S
denote human scores for grammaticality, meaning
preservation and simplicity respectively.
</bodyText>
<footnote confidence="0.802058666666667">
score for simplicity, which are not relevant here.
3For more detailed explanation of simplification schemes
and the dataset see (Glavaˇs and ˇStajner, 2013).
</footnote>
<page confidence="0.996483">
2
</page>
<bodyText confidence="0.934928692307692">
Ex. Original Simplified G M S SM
(a) “It is understood the dead girl had “The dead girl had been living at 3 3 3 S
been living at her family home, in her family home, in a neighbouring
a neighbouring housing estate, and housing estate and was visiting her
was visiting her older sister at the older sister.”
time of the shooting.”
(b) “On Facebook, more than 10,000 “On Facebook, more than 10,000 2 3 3 S
people signed up to a page an- people signed to a page announcing
nouncing an opposition rally for an opposition rally for Saturday.”
Saturday.”
(c) “Joel Elliott, also 22, of North “Joel Elliott was charged on May 3 3 2 3 S
Road, Brighton, was charged on with murder. He appeared at Lewes
May 3 with murder. He appeared at Crown Court on May 8.”
Lewes Crown Court on May 8 but
did not enter a plea.”
(d) “For years the former Bosnia Serb “For years the former Bosnia Serb 1 2 3 S
army commander Ratko Mladic had army commander Ratko Mladic had
evaded capture and was one of the evaded but his time the run ended
world’s most wanted men, but his last year he was arrested near Bel-
time on the run finally ended last grade.”
year when he was arrested near
Belgrade.”
(e) “Police have examined the scene at “Police have examined the scene 3 1 3 P
a house at William Court in Bel- at William Court near Magherafelt.
laghy, near Magherafelt for clues to The incident has stunned the com-
the incident which has stunned the munity.”
community.”
(f) “Rastan, 25 km (15 miles) north “Rastan has slipped government 2 1 3 P
of Homs city, has slipped in and control several times. The uprising
out of government control several erupted in March 2011.”
times since the uprising against As-
sad erupted in March 2011.”
(g) “But opposition parties and inter- “But opposition parties and inter- 1 1 3 B
national observers said the vote was national observers said .”
marred by vote-rigging, including
alleged ballot-box stuffing and false
voter rolls.”
(h) “Foreign Affairs Secretary Albert “Foreign Affairs Secretary Albert 3 3 1 B
del Rosario was seeking a diplo- del Rosario was seeking a diplo-
matic solution with Chinese Ambas- matic solution with Chinese Ambas-
sador Ma Keqing, the TV network sador Ma Keqing, the TV network
said.” said.”
(h) “ On Wednesday, two video jour- “On Wednesday two video journal- 3 2 2 E
nalists working for the state-owned ists were briefly detained outside
RIA Novosti news agency were the Election Commission building.
briefly detained outside the Election Two video journalists worked for
Commission building where Putin the state-owned RIA Novosti news
was handing in his application to agency. Putin was handing in his
run.” application.”
Table 2: Human evaluation examples (G, M, and S correspond to the human scores for grammaticality,
meaning preservation and simplicity, and SM denotes the simplification method used: B – baseline, S –
sentence-wise, E – event-wise, and P – pronominal anaphora)
</bodyText>
<page confidence="0.965318">
3
</page>
<subsectionHeader confidence="0.992224">
3.3 Derived Datasets
</subsectionHeader>
<bodyText confidence="0.999943125">
The original dataset (Original) contains separate
scores for grammaticality (G), meaning preserva-
tion (M), and simplicity (S), each of them on a 1–3
scale. From this dataset we derived two additional
ones: Total3 and Total2.
The Total3 dataset contains three marks (OK –
use as it is, PE – post-editing required, and Dis
– discard) derived from G and M in the Original
dataset. Those simplified sentences which scored
‘3’ for both meaning preservation (M) and gram-
maticality (G) are placed in the OK class as they
do not need any kind of post-editing. A closer
look at the remaining sentences suggests that any
simplified sentence which got a score ‘2’ or ‘3’
for meaning preservation (M) could be easily post-
edited, i.e. it requires minimal changes which are
obvious from its comparison to the corresponding
original. For instance, in the sentence (b) in Ta-
ble 2 the only change that needs to be made is
adding the word “up” after “signed”. Those sen-
tences which scored ‘2’ for meaning need slightly
more, albeit simple modification. The simplified
text snippet (c) in Table 2 would need “but did
not enter a plea” added at the end of the last
sentence. The next sentence (d) in the same ta-
ble needs a few more changes, but still very mi-
nor ones: adding the word “capture” after “had
evaded”, adding the preposition “on” before “the
run”, and adding “when” after “last year”. There-
fore, we grouped all those sentences into one class
– PE (sentences which require a minimal post-
editing effort). Those sentences which scored ‘1’
for meaning need to either be left in their original
form or simplified from scratch. We thus classify
them as Dis. This newly created dataset (Total3)
allows us to investigate whether we could auto-
matically classify simplified sentences into those
three categories, taking into account both gram-
maticality and meaning preservation at the same
time.
The Total2 dataset contains only two marks (‘0’
and ‘1’) which correspond to the sentences which
should be discarded (‘0’) and those which should
be retained (‘1’), where ‘0’ corresponds to Dis in
Total3, and ‘1’ corresponds to the union of OK and
PE in Total3. The derivation procedure for both
datasets is presented in Table 3. We wanted to in-
vestigate whether the classification task would be
simpler (better performed) if there were only two
classes instead of three. In the case that such clas-
sification could be performed with satisfactory ac-
curacy, all sentences classified as ‘0’ would be left
in their original form or simplified with some dif-
ferent simplification strategy, while those classi-
fied as ‘1’ would be sent for a quick human post-
editing procedure.
</bodyText>
<table confidence="0.993337090909091">
Original Total3 Total2
G M
3 3 OK 1
2 3 PE 1
1 3 PE 1
3 2 PE 1
2 2 PE 1
1 2 PE 1
3 1 Dis 0
2 1 Dis 0
1 1 Dis 0
</table>
<tableCaption confidence="0.999421">
Table 3: Datasets
</tableCaption>
<bodyText confidence="0.999982193548387">
Here it is important to mention that we decided
not to use human scores for simplicity (S) for sev-
eral reasons. First, simplicity was defined as the
amount of irrelevant information which was elim-
inated. Therefore, we cannot expect that any of
the six MT evaluation metrics would have a sig-
nificant correlation with this score (except maybe
TERp and, in particular, one of its parts – ‘number
of deletions’. However, none of the two demon-
strated any significant correlation with the sim-
plicity score, and those results are thus not re-
ported in this paper). Second, the output sentences
with a low simplicity score are not as detrimental
for the TS system as those with a low grammat-
icality or meaning preservation score. The sen-
tences with a low simplicity score would simply
not help the target user read faster or understand
better, but would not do any harm either. Alter-
natively, if the target “user” is an MT or infor-
mation extraction (IE) system, or a parser for ex-
ample, such sentences would not lower the perfor-
mance of the system; they would just not improve
it. Low scores for G and M, however, would lead
to a worse performance for such NLP systems,
longer reading time, and a worse or erroneous un-
derstanding of the text. Third, the simplicity of
the output (or complexity reduction performed by
a TS system) could be evaluated separately, in a
fully automatic manner – using some readability
measures or average sentence length as features
(as in (Drndarevi´c et al., 2013; Glavaˇs and ˇStajner,
</bodyText>
<page confidence="0.984841">
4
</page>
<bodyText confidence="0.714124">
2013) for example).
</bodyText>
<subsectionHeader confidence="0.625468">
3.4 Features: MT Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999953522727273">
In all experiments, we focused on six commonly
used MT evaluation metrics. These are cosine
similarity (using the bag-of-words representation),
METEOR (Denkowski and Lavie, 2011), TERp
(Snover et al., 2009), TINE (Rios et al., 2011), and
two components of TINE: T-BLEU (which differs
from the standard BLEU (Papineni et al., 2002) by
using 3-grams, 2-grams, and 1-grams when there
are no 4-grams found, where the “original” BLEU
would give score ‘0’) and SRL (which is the com-
ponent of TINE based on semantic role labeling
using SENNA4). Although these two components
contribute equally to TINE (thus being linearly
correlated with TINE), we wanted to investigate
which one of them contributes more to the cor-
relation of TINE with human judgements. Given
their different natures, we expect T-BLEU to con-
tribute more to the correlation of TINE with hu-
man judgements of grammaticality, and SRL to
contribute more to the correlation of TINE with
human judgements of meaning preservation.
As we do not have the reference for the simpli-
fied sentence, all metrics are applied in a slightly
different way than in MT. Instead of evaluating the
translation hypothesis (output of the automatic TS
system in our case) with the corresponding ref-
erence translation (which would be a ‘gold stan-
dard’ simplified sentence), we apply the metrics
to the output of the automatic TS system com-
paring it with the corresponding original sentence.
Given that the simplified sentences in the used
dataset are usually shorter than the original ones
(due to the elimination of irrelevant content which
was the main focus of the TS system proposed by
Glavaˇs and ˇStajner (2013)), we expect low scores
of T-BLEU and METEOR which apply a brevity
penalty. However, our dataset does not contain any
kind of lexical simplification, but rather copies all
relevant information from the original sentence5.
Therefore, we expect the exact matches of word
forms and semantic role labels (which are compo-
nents of the MT evaluation metrics) to have a good
correlation to human judgements of grammatical-
ity and meaning preservation.
</bodyText>
<footnote confidence="0.9924168">
4http://ml.nec-labs.com/senna/
5The exceptions being changes of gerundive forms into
past tense, and anaphoric pronoun resolution in some simpli-
fication schemes. See Section 3.1 and (Glavaˇs and ˇStajner,
2013) for more details.
</footnote>
<subsectionHeader confidence="0.686894">
3.5 Goal
</subsectionHeader>
<bodyText confidence="0.99981025">
After we obtained the six automatic metrics (co-
sine, METEOR, TERp, TINE, T-BLEU, and
SRL), we performed two sets of experiments, try-
ing to answer two main questions:
</bodyText>
<listItem confidence="0.996582">
1. Are the chosen MT evaluation metrics cor-
related with the human judgements of gram-
maticality and meaning preservation of the
TS system output?
2. Could we automatically classify the simpli-
</listItem>
<bodyText confidence="0.939401086956522">
fied sentences into those which are: (1) cor-
rect, (2) require a minimal post-editing, (3)
incorrect and need to be discarded?
A positive answer to the first question would
mean that there is a possibility of finding an au-
tomatic metric (or a combination of several au-
tomatic metrics) which could successfully replace
the time consuming human evaluation. The search
for that “ideal” combination of automatic metrics
could be performed by using various classification
algorithms and carefully designed features. If we
manage to classify simplified sentences into the
three aforementioned categories with a satisfying
accuracy, the benefits would be two-fold. Firstly,
such a classification system could be used for an
automatic evaluation of TS systems and an easy
comparison of their performances. Secondly, it
could be used inside a TS system to mark those
sentences of low quality which need to be checked
further, or those sentences whose original mean-
ing changed significantly. The latter could then be
left in their original form or simplified using some
different technique.
</bodyText>
<subsectionHeader confidence="0.837197">
3.6 Experiments
</subsectionHeader>
<bodyText confidence="0.999979933333333">
The six experiments conducted in this study are
presented in Table 4. The first two experiments
had the aim of answering the first question (Sec-
tion 3.5) as to whether the chosen MT metrics cor-
relate with the human judgements of grammatical-
ity (G) and meaning preservation (M) of the TS
system output. The results were obtained in terms
of Pearson’s, Kendall’s and Spearman’s correla-
tion coefficients. The third and the fourth exper-
iments (Table 4) could be seen as the intermediate
experiments exploring the possibility of automatic
classification of simplified sentences according to
their grammaticality, and meaning preservation.
The main experiment was the fifth experiment, try-
ing to answer the second question (Section 3.5)
</bodyText>
<page confidence="0.92306">
5
</page>
<listItem confidence="0.924981555555556">
Exp. Description
1. Correlation of the six automatic MT metrics with the human scores for Grammaticality
2. Correlation of the six automatic MT metrics with the human scores for Meaning preservation
3. Classification of the simplified sentences into 3 classes (‘1’ – Bad, ‘2’ – Medium, and ‘3’ – Good) according to
their Grammaticality
4. Classification of the simplified sentences into 3 classes (‘1’ – Bad, ‘2’ – Medium, and ‘3’ – Good) according to
their Meaning preservation
5. Classification of the simplified sentences into 3 classes (OK, PE, Dis) according to their Total3 score
6. Classification of the simplified sentences into 2 classes (‘1‘ – Retain, ‘0’ – Discard) according to their Total2 score
</listItem>
<tableCaption confidence="0.995076">
Table 4: Experiments
</tableCaption>
<bodyText confidence="0.999891285714286">
as to whether we could automatically classify the
simplified sentences into those which are: (1) cor-
rect (OK), (2) require minimal post-editing (PE),
and (3) incorrect and need to be discarded (Dis).
The last experiment (Table 4) was conducted with
the aim of exploring whether the classification of
simplified sentences into only two classes – Retain
(for further post-editing) and Discard – would lead
to better results than the classification into three
classes (OK, PE, and Dis) in the fifth experiment.
All classification experiments were performed
in Weka workbench (Witten and Frank, 2005; Hall
et al., 2009), using seven classification algorithms
in a 10-fold cross-validation setup:
</bodyText>
<listItem confidence="0.99738925">
• NB – NaiveBayes (John and Langley, 1995),
• SMO – Weka implementation of Support
Vector Machines (Keerthi et al., 2001) with
normalisation (n) or with standardisation (s),
• Logistic (le Cessie and van Houwelingen,
1992),
• Lazy.IBk – K-nearest neighbours (Aha and
Kibler, 1991),
• JRip – a propositional rule learner (Cohen,
1995),
• J48 – Weka implementation of C4.5 (Quin-
lan, 1993).
</listItem>
<bodyText confidence="0.9640895">
As a baseline we use the classifier which assigns
the most frequent (majority) class to all instances.
</bodyText>
<sectionHeader confidence="0.999824" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999975714285714">
The results of the first two experiments (correla-
tion experiments in Table 4) are presented in Sec-
tion 4.1, while the results of the other four exper-
iments (classification experiments in Table 4) can
be found in Section 4.2. When interpreting the re-
sults of all experiments, it is important to keep in
mind that human agreements for meaning preser-
vation (M) and grammaticality (G) were accept-
able but far from perfect (Section 3.2), and thus
it would be unrealistic to expect the correlation
between the MT evaluation metrics and human
judgements or the agreement of the classification
system with human assessments to be higher than
the reported IAA agreement.
</bodyText>
<subsectionHeader confidence="0.997938">
4.1 Correlation of Automatic Metrics with
Human Judgements
</subsectionHeader>
<bodyText confidence="0.9978212">
The correlations of automatic metrics with hu-
man judgements of grammaticality and meaning
preservation are given in Tables 5 and 6 respec-
tively. Statistically significant correlations (at a
0.01 level of significance) are presented in bold.
</bodyText>
<table confidence="0.998147857142857">
Metric Pearson Kendall Spearman
cosine 0.097 0.092 0.115
METEOR 0.176 0.141 0.178
T-BLEU 0.226 0.185 0.234
SRL 0.097 0.076 0.095
TINE 0.175 0.145 0.181
TERp -0.208 -0.158 -0.198
</table>
<tableCaption confidence="0.997979">
Table 5: Correlation between automatic evaluation
metrics and human scores for grammaticality
</tableCaption>
<table confidence="0.999684">
Metric Pearson Kendall Spearman
cosine 0.293 0.262 0.334
METEOR 0.386 0.322 0.405
T-BLEU 0.442 0.382 0.475
SRL 0.348 0.285 0.356
TINE 0.427 0.385 0.447
TERp -0.414 -0.336 -0.416
</table>
<tableCaption confidence="0.6852748">
Table 6: Correlation between automatic evaluation
metrics and human scores for meaning preserva-
tion
It can be noted that human perception of gram-
maticality is positively correlated with three auto-
</tableCaption>
<page confidence="0.998708">
6
</page>
<table confidence="0.9998279">
Algorithm Grammaticality P Meaning F P Total3 F P Total2 F
P R F R R R
NB 0.53 0.46 0.48 0.54 0.54 0.54 0.54 0.53 0.53 0.74 0.69 0.71
SMO(n) 0.39 0.63 0.48 0.52 0.49 0.45 0.43 0.53 0.44 0.55 0.74 0.63
SMO(s) 0.39 0.63 0.48 0.57 0.56 0.55 0.57 0.55 0.51 0.60 0.73 0.63
Logistic 0.45 0.61 0.49 0.57 0.57 0.56 0.61 0.60 0.59 0.75 0.77 0.74
Lazy.IBk 0.57 0.58 0.57 0.50 0.50 0.50 0.54 0.54 0.54 0.73 0.73 0.73
JRip 0.41 0.59 0.48 0.53 0.50 0.48 0.57 0.56 0.55 0.72 0.75 0.73
J48 0.45 0.61 0.49 0.48 0.47 0.47 0.59 0.57 0.54 0.68 0.71 0.69
baseline 0.39 0.63 0.48 0.17 0.41 0.24 0.21 0.46 0.29 0.55 0.74 0.63
</table>
<tableCaption confidence="0.975376">
Table 7: Classification results (the best performances are shown in bold; baseline uses the majority class)
</tableCaption>
<table confidence="0.9993616">
Actual Grammaticality Meaning
Good Med. Bad Good Med. Bad
Good 127 21 23 50 31 7
Med. 29 19 10 24 73 16
Bad 24 9 10 9 31 31
</table>
<tableCaption confidence="0.790587">
Table 8: Confusion matrices for the best classifications according to Grammaticality (Lazy.IBk) and
Meaning (Logistic). The number of “severe” classification mistakes (classifying Good as Bad or vice
versa) are presented in bold.
</tableCaption>
<bodyText confidence="0.995912235294118">
matic measures – METEOR, T-BLEU, and TINE,
while it is negatively correlated with TERp (TERp
measures the number of edits necessary to perform
on the simplified sentence to transform it into its
original one, i.e. the higher the value of TERp,
the less similar the original and its corresponding
simplified sentence are. The other five MT metrics
measure the similarity between the original and its
corresponding simplified version, i.e. the higher
their value is, the more similar are the sentences
are). All the MT metrics appear to be even bet-
ter correlated with the human scores for meaning
preservation (Table 6), demonstrating six positive
and one (TERp) negative statistically significant
correlation with M. The correlation is the highest
for T-BLEU, TINE, and TERp, though closely fol-
lowed by all others.
</bodyText>
<subsectionHeader confidence="0.998595">
4.2 Sentence Classification
</subsectionHeader>
<bodyText confidence="0.999966314285714">
The results of the four classification experiments
(Section 3.6) are given in Table 7.
At first glance, the performance of the classifi-
cation algorithms seems similar for the first two
tasks (classification of the simplified sentences
according to their Grammaticality and Meaning
preservation). However, one needs to take into ac-
count that the baseline for the first task was much
much higher than for the second task (Table 7).
Furthermore, it can be noted that for the first task,
recall was significantly higher than precision for
most classification algorithms (all except NB and
Logistic), while for the second task they were very
similar in all cases. More importantly, a closer
look at the confusion matrices reveals that most of
the incorrectly classified sentences were assigned
to the nearest class (Medium into Bad or Good;
Bad into Medium; and Good into Medium6) in the
second task, while it was not the case in the first
task (Table 8).
Classification performed on the Total3 dataset
outperformed both previous classifications – that
based on Grammaticality and that based on Mean-
ing – on four different algorithms (NB, Logis-
tic, JRip, and J48). Classification conducted on
Total3 using Logistic outperformed all results of
classifications on either Grammaticality or Mean-
ing separately (Table 7). It reached a 0.61, 0.60,
and 0.59 score for the weighted precision (P), re-
call (R), and F-measure (F), respectively, thus out-
performing the baseline significantly. More im-
portantly, classification on the Total3 dataset led
to significantly fewer mis-classifications between
Good and Bad (Table 9) than the classification
based on Grammaticality, and slightly less than
</bodyText>
<footnote confidence="0.78996">
6Bad, Medium, and Good correspond to marks ‘1’, ‘2’,
and ‘3’ given by human evaluators.
</footnote>
<page confidence="0.998731">
7
</page>
<table confidence="0.9977014">
Actual OK Total3 Dis.
PE
OK 41 32 4
PE 17 85 12
Dis. 6 31 28
</table>
<tableCaption confidence="0.993123">
Table 9: Confusion matrix for the best classifica-
</tableCaption>
<bodyText confidence="0.847597666666667">
tion according to Total3 (Logistic). The number of
“severe” classification mistakes (classifying Good
as Bad or vice versa) are presented in bold.
</bodyText>
<table confidence="0.998218">
Actual Total2
Retain Discard
Retain 21 50
Discard 12 189
</table>
<tableCaption confidence="0.950269">
Table 10: Confusion matrix for the best classifi-
</tableCaption>
<bodyText confidence="0.998785241379311">
cation according to Total2 (Logistic). The num-
ber of “severe” classification mistakes (classifying
Retain as Discard or vice versa) are presented in
bold.
the classification based only on Meaning (Table 8).
Therefore, it seems that simplified sentences are
better classified into three classes giving a unique
score for both grammaticality and preservation of
meaning together.
The binary classification experiments based on
the Total2 led to results which significantly out-
performed the baseline in terms of precision and
F-measure (Table 7). However, they resulted in
a great number of sentences which should be re-
tained (Retain) being classified into those which
should be discarded (Discard) and vice versa (Ta-
ble 10). Therefore, it seems that it would be better
to opt for classification into three classes (Total3)
than for classification into two classes (Total2).
Additionally, we used CfsSubsetEval attribute
selection algorithm (Hall and Smith, 1998) in or-
der to identify the ‘best’ subset of features. The
‘best’ subsets of features for each of the four clas-
sification tasks returned by the algorithm are listed
in Table 11. However, the classification perfor-
mances achieved (P, R, and F) when using only
the ‘best’ features did not differ significantly from
those when using all initially selected features, and
thus are not presented in this paper.
</bodyText>
<sectionHeader confidence="0.990855" genericHeader="evaluation">
5 Limitations
</sectionHeader>
<bodyText confidence="0.875657">
The used dataset does not contain any kind of
lexical simplification (Glavaˇs and ˇStajner, 2013).
</bodyText>
<table confidence="0.9951464">
Classification ‘Best’ features
Meaning {TERp, T-BLEU, SRL, TINE}
Grammaticality {TERp, T-BLEU}
New3 {TERp, T-BLEU, SRL, TINE}
New2 {TERp, T-BLEU, SRL}
</table>
<tableCaption confidence="0.999624">
Table 11: The ‘best’ features (CfsSubsetEval)
</tableCaption>
<bodyText confidence="0.99977116">
Therefore, one should consider the limitation of
this TS system which performs only syntactic sim-
plification and content reduction. On the other
hand, the dataset used contains a significant con-
tent reduction in most of the sentences. If the same
experiments were conducted on a dataset which
performs only syntactic simplification, we would
expect much higher correlation of MT evaluation
metrics to human judgements, due to the lesser im-
pact of the brevity penalty in that case.
If we were to apply the same MT evaluation
metrics to a TS system which additionally per-
forms some kind of lexical simplification (either
a simple lexical substitution or paraphrasing), the
correlation results for T-BLEU and cosine similar-
ity would be lower (due to the lower number of
exact matches), but not for METEOR, TERp and
SRL (and thus TINE as well). As a similar prob-
lem is also present in the evaluation of MT sys-
tems where the obtained output could differ from
the reference translation (while still being equally
good), METEOR, TERp, and SRL in TINE ad-
ditionally use inexact matching. The first two use
the stem, synonym, and paraphrase matches, while
SRL uses ontologies and thesaurus.
</bodyText>
<sectionHeader confidence="0.998619" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999893230769231">
While the results reported are preliminary and
their universality needs to be validated on different
TS datasets, the experiments and results presented
can be regarded as a promising step towards an au-
tomatic assessment of grammaticality and mean-
ing preservation for the output of TS systems. In
addition and to the best of our knowledge, there
are no such datasets publicly available other than
the one used. Nevertheless, we hope that these re-
sults would initiate an interesting discussion in the
TS community and start a new direction of studies
towards automatic evaluation of text simplification
systems.
</bodyText>
<page confidence="0.997369">
8
</page>
<sectionHeader confidence="0.996521" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998712">
The research described in this paper was par-
tially funded by the European Commission un-
der the Seventh (FP7-2007-2013) Framework Pro-
gramme for Research and Technological Develop-
ment (FP7-ICT-2011.5.5 FIRST 287607).
</bodyText>
<sectionHeader confidence="0.998525" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999657778947369">
D. Aha and D. Kibler. 1991. Instance-based learning
algorithms. Machine Learning, 6:37–66.
S. M. Alu´ısio, L. Specia, T. A. S. Pardo, E. G. Maziero,
H. M. Caseli, and R. P. M. Fortes. 2008a. A cor-
pus analysis of simple account texts and the pro-
posal of simplification strategies: first steps towards
text simplification systems. In Proceedings of the
26th annual ACM international conference on De-
sign of communication, SIGDOC ’08, pages 15–22,
New York, NY, USA. ACM.
S. M. Alu´ısio, L. Specia, T. A.S. Pardo, E. G. Maziero,
and R. P.M. Fortes. 2008b. Towards brazilian por-
tuguese automatic text simplification systems. In
Proceedings of the eighth ACM symposium on Doc-
ument engineering, DocEng ’08, pages 240–248,
New York, NY, USA. ACM.
M. J. Aranzabe, A. D´ıaz De Ilarraza, and I. Gonz´alez.
2012. First Approach to Automatic Text Simplifica-
tion in Basque. In Proceedings of the first Natural
Language Processing for Improving Textual Acces-
sibility Workshop (NLP4ITA).
G. Barlacchi and S. Tonelli. 2013. ERNESTA: A sen-
tence simplification tool for childrens stories in ital-
ian. In Computational Linguistics and Intelligent
Text Processing.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-
vlin, and J. Tait. 1999. Simplifying text for
language-impaired readers. In Proceedings of the
9th Conference of the European Chapter of the ACL
(EACL’99), pages 269–270.
R. Chandrasekar, Christine Doran, and B. Srinivas.
1996. Motivations and methods for text simplifi-
cation. In In Proceedings of the Sixteenth Inter-
national Conference on Computational Linguistics
(COLING ’96, pages 1041–1044.
W. Cohen. 1995. Fast Effective Rule Induction. In
Proceedings of the Twelfth International Conference
on Machine Learning, pages 115–123.
W. Coster and D. Kauchak. 2011. Learning to Sim-
plify Sentences Using Wikipedia. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics, pages 1–9.
M. Denkowski and A. Lavie. 2011. Meteor 1.3: Au-
tomatic Metric for Reliable Optimization and Evalu-
ation of Machine Translation Systems. In Proceed-
ings of the EMNLP Workshop on Statistical Machine
Translation.
S. Devlin. 1999. Simplifying natural language textfor
aphasic readers. Ph.D. thesis, University of Sunder-
land, UK.
G. Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram coocurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, pages 138–145. Morgan Kaufmann Pub-
lishers Inc.
B. Drndarevi´c, S. ˇStajner, S. Bott, S. Bautista, and
H. Saggion. 2013. Automatic Text Simplication
in Spanish: A Comparative Evaluation of Com-
plementing Components. In Proceedings of the
12th International Conference on Intelligent Text
Processing and Computational Linguistics. Lecture
Notes in Computer Science. Samos, Greece, 24-30
March, 2013., pages 488–500.
L. Feng. 2009. Automatic readability assessment for
people with intellectual disabilities. In SIGACCESS
Access. Comput., number 93, pages 84–91. ACM,
New York, NY, USA, jan.
G. Glavaˇs and S. ˇStajner. 2013. Event-Centered Sim-
plication of News Stories. In Proceedings of the
Student Workshop held in conjunction with RANLP
2013, Hissar, Bulgaria, pages 71–78.
M. A. Hall and L. A. Smith. 1998. Practical feature
subset selection for machine learning. In C. Mc-
Donald, editor, Computer Science ’98 Proceedings
of the 21st Australasian Computer Science Confer-
ence ACSC’98, pages 181–191. Berlin: Springer.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The weka data min-
ing software: an update. SIGKDD Explor. Newsl.,
11:10–18, November.
K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura.
2003. Text simplification for reading assistance: a
project note. In Proceedings of the second inter-
national workshop on Paraphrasing - Volume 16,
PARAPHRASE ’03, pages 9–16, Stroudsburg, PA,
USA. Association for Computational Linguistics.
G. H. John and P. Langley. 1995. Estimating Contin-
uous Distributions in Bayesian Classifiers. In Pro-
ceedings of the Eleventh Conference on Uncertainty
in Artificial Intelligence, pages 338–345.
S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and
K. R. K. Murthy. 2001. Improvements to Platt’s
SMO Algorithm for SVM Classifier Design. Neural
Computation, 13(3):637–649.
</reference>
<page confidence="0.944386">
9
</page>
<reference confidence="0.997877977777778">
D. Klein and C.D. Manning. 2003. Accurate unlex-
icalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, volume 1, pages 423–430. Association for
Computational Linguistics.
S. le Cessie and J.C. van Houwelingen. 1992. Ridge
Estimators in Logistic Regression. Applied Statis-
tics, 41(1):191–201.
J. Martos, S. Freire, A. Gonz´alez, D. Gil, and M. Se-
bastian. 2012. D2.1: Functional requirements spec-
ifications and user preference survey. Technical re-
port, FIRST technical report.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL.
S. E. Petersen and M. Ostendorf. 2007. Text Sim-
plification for Language Learners: A Corpus Anal-
ysis. In Proceedings of Workshop on Speech and
Language Technology for Education.
R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Ma-
teo, CA.
L. Rello. 2012. Dyswebxia: a model to improve ac-
cessibility of the textual web for dyslexic users. In
SIGACCESS Access. Comput., number 102, pages
41–44. ACM, New York, NY, USA, January.
M. Rios, W. Aziz, and L. Specia. 2011. TINE: A met-
ric to assess MT adequacy. In Proceedings of the
Sixth Workshop on Statistical Machine Translation
(WMT-2011), Edinburgh, UK.
M. B. Ruiter, T. C. M. Rietveld, Cucchiarini C., Krah-
mer E. J., and H. Strik. 2010. Human Language
Technology and communicative disabilities: Re-
quirements and possibilities for the future. In Pro-
ceedings of the the seventh international conference
on Language Resources and Evaluation (LREC).
J. Rybing, C. Smithr, and A. Silvervarg. 2010. To-
wards a Rule Based System for Automatic Simpli-
fication of Texts. In The Third Swedish Language
Technology Conference.
H. Saggion, E. G´omez Martinez, E. Etayo, A. An-
ula, and L. Bourg. 2011. Text Simplification in
Simplext: Making Text More Accessible. Revista
de la Sociedad Espa˜nola para el Procesamiento del
Lenguaje Natural, 47:341–342.
A. Siddharthan. 2006. Syntactic simplification and
text cohesion. Research on Language &amp; Computa-
tion, 4(1):77–109.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009. Fluency, Adequacy, or HTER? Exploring Dif-
ferent Human Judgments with a Tunable MT Metric.
In Proceedings of the Fourth Workshop on Statisti-
cal Machine Translation at the 12th Meeting of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2009), Athens, Greece.
L. Specia. 2010. Translating from complex to simpli-
fied sentences. In Proceedings of the 9th interna-
tional conference on Computational Processing of
the Portuguese Language, pages 30–39, Berlin, Hei-
delberg.
S. ˇStajner, R. Evans, C. Orasan, and R. Mitkov. 2012.
What Can Readability Measures Really Tell Us
About Text Complexity? In Proceedings of the
LREC’12 Workshop: Natural Language Processing
for Improving Textual Accessibility (NLP4ITA), Is-
tanbul, Turkey.
I. H. Witten and E. Frank. 2005. Data mining: practi-
cal machine learning tools and techniques. Morgan
Kaufmann Publishers.
K. Woodsend and M. Lapata. 2011a. Learning to Sim-
plify Sentences with Quasi-Synchronous Grammar
and Integer Programming. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing (EMNLP).
K. Woodsend and M. Lapata. 2011b. WikiSimple:
Automatic Simplification of Wikipedia Articles. In
Proceedings of the 25th AAI Coference on Artificial
Intelligence.
S. Wubben, A. van den Bosch, and E. Krahmer. 2012.
Sentence simplification by monolingual machine
translation. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ’12, pages 1015–
1024, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Z. Zhu, D. Berndard, and I. Gurevych. 2010. A Mono-
lingual Tree-based Translation Model for Sentence
Simplification. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(Coling 2010), pages 1353–1361.
</reference>
<page confidence="0.997783">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.180029">
<title confidence="0.987443">One Step Closer to Automatic of Text Simplification Systems</title>
<affiliation confidence="0.673078">Group in Computational Linguistics, University of Wolverhampton,</affiliation>
<address confidence="0.370669">Research Group, Universitat Pompeu Fabra,</address>
<email confidence="0.997069">S.Stajner@wlv.ac.uk,R.Mitkov@wlv.ac.uk,horacio.saggion@upf.edu</email>
<abstract confidence="0.995444">This study explores the possibility of replacing the costly and time-consuming human evaluation of the grammaticality and meaning preservation of the output of text simplification (TS) systems with some automatic measures. The focus is on six widely used machine translation (MT) evaluation metrics and their correlation with human judgements of grammaticality and meaning preservation in text snippets. As the results show a significant correlation between them, we go further and try to classify simplified sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Aha</author>
<author>D Kibler</author>
</authors>
<title>Instance-based learning algorithms.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<pages>6--37</pages>
<contexts>
<context position="21905" citStr="Aha and Kibler, 1991" startWordPosition="3544" endWordPosition="3547"> (for further post-editing) and Discard – would lead to better results than the classification into three classes (OK, PE, and Dis) in the fifth experiment. All classification experiments were performed in Weka workbench (Witten and Frank, 2005; Hall et al., 2009), using seven classification algorithms in a 10-fold cross-validation setup: • NB – NaiveBayes (John and Langley, 1995), • SMO – Weka implementation of Support Vector Machines (Keerthi et al., 2001) with normalisation (n) or with standardisation (s), • Logistic (le Cessie and van Houwelingen, 1992), • Lazy.IBk – K-nearest neighbours (Aha and Kibler, 1991), • JRip – a propositional rule learner (Cohen, 1995), • J48 – Weka implementation of C4.5 (Quinlan, 1993). As a baseline we use the classifier which assigns the most frequent (majority) class to all instances. 4 Results and Discussion The results of the first two experiments (correlation experiments in Table 4) are presented in Section 4.1, while the results of the other four experiments (classification experiments in Table 4) can be found in Section 4.2. When interpreting the results of all experiments, it is important to keep in mind that human agreements for meaning preservation (M) and gr</context>
</contexts>
<marker>Aha, Kibler, 1991</marker>
<rawString>D. Aha and D. Kibler. 1991. Instance-based learning algorithms. Machine Learning, 6:37–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Alu´ısio</author>
<author>L Specia</author>
<author>T A S Pardo</author>
<author>E G Maziero</author>
<author>H M Caseli</author>
<author>R P M Fortes</author>
</authors>
<title>A corpus analysis of simple account texts and the proposal of simplification strategies: first steps towards text simplification systems.</title>
<date>2008</date>
<booktitle>In Proceedings of the 26th annual ACM international conference on Design of communication, SIGDOC ’08,</booktitle>
<pages>15--22</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Alu´ısio, Specia, Pardo, Maziero, Caseli, Fortes, 2008</marker>
<rawString>S. M. Alu´ısio, L. Specia, T. A. S. Pardo, E. G. Maziero, H. M. Caseli, and R. P. M. Fortes. 2008a. A corpus analysis of simple account texts and the proposal of simplification strategies: first steps towards text simplification systems. In Proceedings of the 26th annual ACM international conference on Design of communication, SIGDOC ’08, pages 15–22, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Alu´ısio</author>
<author>L Specia</author>
<author>T A S Pardo</author>
<author>E G Maziero</author>
<author>R P M Fortes</author>
</authors>
<title>Towards brazilian portuguese automatic text simplification systems.</title>
<date>2008</date>
<booktitle>In Proceedings of the eighth ACM symposium on Document engineering, DocEng ’08,</booktitle>
<pages>240--248</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Alu´ısio, Specia, Pardo, Maziero, Fortes, 2008</marker>
<rawString>S. M. Alu´ısio, L. Specia, T. A.S. Pardo, E. G. Maziero, and R. P.M. Fortes. 2008b. Towards brazilian portuguese automatic text simplification systems. In Proceedings of the eighth ACM symposium on Document engineering, DocEng ’08, pages 240–248, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Aranzabe</author>
<author>A D´ıaz De Ilarraza</author>
<author>I Gonz´alez</author>
</authors>
<title>First Approach to Automatic Text Simplification in Basque.</title>
<date>2012</date>
<booktitle>In Proceedings of the first Natural Language Processing for Improving Textual Accessibility Workshop (NLP4ITA).</booktitle>
<marker>Aranzabe, De Ilarraza, Gonz´alez, 2012</marker>
<rawString>M. J. Aranzabe, A. D´ıaz De Ilarraza, and I. Gonz´alez. 2012. First Approach to Automatic Text Simplification in Basque. In Proceedings of the first Natural Language Processing for Improving Textual Accessibility Workshop (NLP4ITA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Barlacchi</author>
<author>S Tonelli</author>
</authors>
<title>ERNESTA: A sentence simplification tool for childrens stories in italian.</title>
<date>2013</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing.</booktitle>
<contexts>
<context position="2314" citStr="Barlacchi and Tonelli, 2013" startWordPosition="338" endWordPosition="341">ion (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008a), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), Dutch (Ruiter et al., 2010), and Italian (Barlacchi and Tonelli, 2013). Usually, TS systems are either evaluated for: (1) the quality of the generated output, or (2) the effectiveness/usefulness of such simplification on reading speed and comprehension of the target population. For the purpose of this study we focused only on the former. The quality of the output generated by TS systems is commonly evaluated by using a combination of readability metrics (measuring the degree of simplification) and human assessment (measuring the grammaticality and meaning preservation). Despite the noticeable similarity between evaluation of the fluency and adequacy of a machine</context>
</contexts>
<marker>Barlacchi, Tonelli, 2013</marker>
<rawString>G. Barlacchi and S. Tonelli. 2013. ERNESTA: A sentence simplification tool for childrens stories in italian. In Computational Linguistics and Intelligent Text Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>G Minnen</author>
<author>D Pearce</author>
<author>Y Canning</author>
<author>S Devlin</author>
<author>J Tait</author>
</authors>
<title>Simplifying text for language-impaired readers.</title>
<date>1999</date>
<booktitle>In Proceedings of the 9th Conference of the European Chapter of the ACL (EACL’99),</booktitle>
<pages>269--270</pages>
<contexts>
<context position="1308" citStr="Carroll et al., 1999" startWordPosition="182" endWordPosition="186">ality and meaning preservation in text snippets. As the results show a significant correlation between them, we go further and try to classify simplified sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising. 1 Introduction Lexically and syntactically complex sentences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu´ısio et al., 2008b), and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999), autism spectrum disorder (ˇStajner et al., 2012; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the origina</context>
</contexts>
<marker>Carroll, Minnen, Pearce, Canning, Devlin, Tait, 1999</marker>
<rawString>J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin, and J. Tait. 1999. Simplifying text for language-impaired readers. In Proceedings of the 9th Conference of the European Chapter of the ACL (EACL’99), pages 269–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chandrasekar</author>
<author>Christine Doran</author>
<author>B Srinivas</author>
</authors>
<title>Motivations and methods for text simplification. In</title>
<date>1996</date>
<booktitle>In Proceedings of the Sixteenth International Conference on Computational Linguistics (COLING ’96,</booktitle>
<pages>1041--1044</pages>
<contexts>
<context position="1717" citStr="Chandrasekar et al., 1996" startWordPosition="244" endWordPosition="247">ntences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu´ısio et al., 2008b), and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999), autism spectrum disorder (ˇStajner et al., 2012; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008a), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), Dutch (Ruiter et al., 2010), and Italian (Barlacchi and Tonelli, 2013). U</context>
</contexts>
<marker>Chandrasekar, Doran, Srinivas, 1996</marker>
<rawString>R. Chandrasekar, Christine Doran, and B. Srinivas. 1996. Motivations and methods for text simplification. In In Proceedings of the Sixteenth International Conference on Computational Linguistics (COLING ’96, pages 1041–1044.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
</authors>
<title>Fast Effective Rule Induction.</title>
<date>1995</date>
<booktitle>In Proceedings of the Twelfth International Conference on Machine Learning,</booktitle>
<pages>115--123</pages>
<contexts>
<context position="21958" citStr="Cohen, 1995" startWordPosition="3555" endWordPosition="3556"> results than the classification into three classes (OK, PE, and Dis) in the fifth experiment. All classification experiments were performed in Weka workbench (Witten and Frank, 2005; Hall et al., 2009), using seven classification algorithms in a 10-fold cross-validation setup: • NB – NaiveBayes (John and Langley, 1995), • SMO – Weka implementation of Support Vector Machines (Keerthi et al., 2001) with normalisation (n) or with standardisation (s), • Logistic (le Cessie and van Houwelingen, 1992), • Lazy.IBk – K-nearest neighbours (Aha and Kibler, 1991), • JRip – a propositional rule learner (Cohen, 1995), • J48 – Weka implementation of C4.5 (Quinlan, 1993). As a baseline we use the classifier which assigns the most frequent (majority) class to all instances. 4 Results and Discussion The results of the first two experiments (correlation experiments in Table 4) are presented in Section 4.1, while the results of the other four experiments (classification experiments in Table 4) can be found in Section 4.2. When interpreting the results of all experiments, it is important to keep in mind that human agreements for meaning preservation (M) and grammaticality (G) were acceptable but far from perfect</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>W. Cohen. 1995. Fast Effective Rule Induction. In Proceedings of the Twelfth International Conference on Machine Learning, pages 115–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Coster</author>
<author>D Kauchak</author>
</authors>
<title>Learning to Simplify Sentences Using Wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="2060" citStr="Coster and Kauchak, 2011" startWordPosition="298" endWordPosition="301">ess (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008a), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), Dutch (Ruiter et al., 2010), and Italian (Barlacchi and Tonelli, 2013). Usually, TS systems are either evaluated for: (1) the quality of the generated output, or (2) the effectiveness/usefulness of such simplification on reading speed and comprehension of the target population. For the purpose of this study we focused only on the former. The quality of the output generated by TS systems is commonly evaluated by u</context>
</contexts>
<marker>Coster, Kauchak, 2011</marker>
<rawString>W. Coster and D. Kauchak. 2011. Learning to Simplify Sentences Using Wikipedia. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Denkowski</author>
<author>A Lavie</author>
</authors>
<title>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="15864" citStr="Denkowski and Lavie, 2011" startWordPosition="2581" endWordPosition="2584">a worse performance for such NLP systems, longer reading time, and a worse or erroneous understanding of the text. Third, the simplicity of the output (or complexity reduction performed by a TS system) could be evaluated separately, in a fully automatic manner – using some readability measures or average sentence length as features (as in (Drndarevi´c et al., 2013; Glavaˇs and ˇStajner, 4 2013) for example). 3.4 Features: MT Evaluation Metrics In all experiments, we focused on six commonly used MT evaluation metrics. These are cosine similarity (using the bag-of-words representation), METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011), and two components of TINE: T-BLEU (which differs from the standard BLEU (Papineni et al., 2002) by using 3-grams, 2-grams, and 1-grams when there are no 4-grams found, where the “original” BLEU would give score ‘0’) and SRL (which is the component of TINE based on semantic role labeling using SENNA4). Although these two components contribute equally to TINE (thus being linearly correlated with TINE), we wanted to investigate which one of them contributes more to the correlation of TINE with human judgements. Given their different natures</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>M. Denkowski and A. Lavie. 2011. Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. In Proceedings of the EMNLP Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Devlin</author>
</authors>
<title>Simplifying natural language textfor aphasic readers.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sunderland, UK.</institution>
<contexts>
<context position="1323" citStr="Devlin, 1999" startWordPosition="187" endWordPosition="188">ervation in text snippets. As the results show a significant correlation between them, we go further and try to classify simplified sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising. 1 Introduction Lexically and syntactically complex sentences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu´ısio et al., 2008b), and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999), autism spectrum disorder (ˇStajner et al., 2012; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So f</context>
</contexts>
<marker>Devlin, 1999</marker>
<rawString>S. Devlin. 1999. Simplifying natural language textfor aphasic readers. Ph.D. thesis, University of Sunderland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram coocurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research,</booktitle>
<pages>138--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="5036" citStr="Doddington, 2002" startWordPosition="757" endWordPosition="759">nd and Lapata (2011b), and Glavaˇs and ˇStajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from “original” to “simplified” language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TERp (Snover et al., 2009). 3 Methodology All experiments were conducted on a freely available sentence-level dataset1, fully described in (Glavaˇs and ˇStajner, 2013), and the two datasets we derived from it. The original dataset and the instructions for the human assessment are given in the next two subsections. Section 3.3 explains how we derived two additional datasets from the original one, and to what end. Section 3.4 describes the automatic MT evaluation metrics used as features in correlation and classification experiments; Section 3.5 presents the main goals of the study; and Se</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram coocurrence statistics. In Proceedings of the second international conference on Human Language Technology Research, pages 138–145. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Drndarevi´c</author>
<author>S ˇStajner</author>
<author>S Bott</author>
<author>S Bautista</author>
<author>H Saggion</author>
</authors>
<title>Automatic Text Simplication in Spanish: A Comparative Evaluation of Complementing Components.</title>
<date>2013</date>
<booktitle>In Proceedings of the 12th International Conference on Intelligent Text Processing and Computational Linguistics. Lecture Notes in Computer Science. Samos,</booktitle>
<pages>488--500</pages>
<marker>Drndarevi´c, ˇStajner, Bott, Bautista, Saggion, 2013</marker>
<rawString>B. Drndarevi´c, S. ˇStajner, S. Bott, S. Bautista, and H. Saggion. 2013. Automatic Text Simplication in Spanish: A Comparative Evaluation of Complementing Components. In Proceedings of the 12th International Conference on Intelligent Text Processing and Computational Linguistics. Lecture Notes in Computer Science. Samos, Greece, 24-30 March, 2013., pages 488–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Feng</author>
</authors>
<title>Automatic readability assessment for people with intellectual disabilities.</title>
<date>2009</date>
<booktitle>In SIGACCESS Access. Comput., number 93,</booktitle>
<pages>84--91</pages>
<publisher>ACM,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="1501" citStr="Feng, 2009" startWordPosition="212" endWordPosition="213">(2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising. 1 Introduction Lexically and syntactically complex sentences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu´ısio et al., 2008b), and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999), autism spectrum disorder (ˇStajner et al., 2012; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Wubben et al., 2012), Spanish (Saggion </context>
</contexts>
<marker>Feng, 2009</marker>
<rawString>L. Feng. 2009. Automatic readability assessment for people with intellectual disabilities. In SIGACCESS Access. Comput., number 93, pages 84–91. ACM, New York, NY, USA, jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Glavaˇs</author>
<author>S ˇStajner</author>
</authors>
<title>Event-Centered Simplication of News Stories.</title>
<date>2013</date>
<booktitle>In Proceedings of the Student Workshop held in conjunction with RANLP 2013, Hissar, Bulgaria,</booktitle>
<pages>71--78</pages>
<marker>Glavaˇs, ˇStajner, 2013</marker>
<rawString>G. Glavaˇs and S. ˇStajner. 2013. Event-Centered Simplication of News Stories. In Proceedings of the Student Workshop held in conjunction with RANLP 2013, Hissar, Bulgaria, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hall</author>
<author>L A Smith</author>
</authors>
<title>Practical feature subset selection for machine learning.</title>
<date>1998</date>
<booktitle>Computer Science ’98 Proceedings of the 21st Australasian Computer Science Conference ACSC’98,</booktitle>
<pages>181--191</pages>
<editor>In C. McDonald, editor,</editor>
<publisher>Springer.</publisher>
<location>Berlin:</location>
<contexts>
<context position="28752" citStr="Hall and Smith, 1998" startWordPosition="4658" endWordPosition="4661"> and preservation of meaning together. The binary classification experiments based on the Total2 led to results which significantly outperformed the baseline in terms of precision and F-measure (Table 7). However, they resulted in a great number of sentences which should be retained (Retain) being classified into those which should be discarded (Discard) and vice versa (Table 10). Therefore, it seems that it would be better to opt for classification into three classes (Total3) than for classification into two classes (Total2). Additionally, we used CfsSubsetEval attribute selection algorithm (Hall and Smith, 1998) in order to identify the ‘best’ subset of features. The ‘best’ subsets of features for each of the four classification tasks returned by the algorithm are listed in Table 11. However, the classification performances achieved (P, R, and F) when using only the ‘best’ features did not differ significantly from those when using all initially selected features, and thus are not presented in this paper. 5 Limitations The used dataset does not contain any kind of lexical simplification (Glavaˇs and ˇStajner, 2013). Classification ‘Best’ features Meaning {TERp, T-BLEU, SRL, TINE} Grammaticality {TERp</context>
</contexts>
<marker>Hall, Smith, 1998</marker>
<rawString>M. A. Hall and L. A. Smith. 1998. Practical feature subset selection for machine learning. In C. McDonald, editor, Computer Science ’98 Proceedings of the 21st Australasian Computer Science Conference ACSC’98, pages 181–191. Berlin: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hall</author>
<author>E Frank</author>
<author>G Holmes</author>
<author>B Pfahringer</author>
<author>P Reutemann</author>
<author>I H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<pages>11--10</pages>
<contexts>
<context position="21548" citStr="Hall et al., 2009" startWordPosition="3490" endWordPosition="3493">ts as to whether we could automatically classify the simplified sentences into those which are: (1) correct (OK), (2) require minimal post-editing (PE), and (3) incorrect and need to be discarded (Dis). The last experiment (Table 4) was conducted with the aim of exploring whether the classification of simplified sentences into only two classes – Retain (for further post-editing) and Discard – would lead to better results than the classification into three classes (OK, PE, and Dis) in the fifth experiment. All classification experiments were performed in Weka workbench (Witten and Frank, 2005; Hall et al., 2009), using seven classification algorithms in a 10-fold cross-validation setup: • NB – NaiveBayes (John and Langley, 1995), • SMO – Weka implementation of Support Vector Machines (Keerthi et al., 2001) with normalisation (n) or with standardisation (s), • Logistic (le Cessie and van Houwelingen, 1992), • Lazy.IBk – K-nearest neighbours (Aha and Kibler, 1991), • JRip – a propositional rule learner (Cohen, 1995), • J48 – Weka implementation of C4.5 (Quinlan, 1993). As a baseline we use the classifier which assigns the most frequent (majority) class to all instances. 4 Results and Discussion The res</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. 2009. The weka data mining software: an update. SIGKDD Explor. Newsl., 11:10–18, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Inui</author>
<author>A Fujita</author>
<author>T Takahashi</author>
<author>R Iida</author>
<author>T Iwakura</author>
</authors>
<title>Text simplification for reading assistance: a project note.</title>
<date>2003</date>
<booktitle>In Proceedings of the second international workshop on Paraphrasing - Volume 16, PARAPHRASE ’03,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1459" citStr="Inui et al., 2003" startWordPosition="205" endWordPosition="208"> sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising. 1 Introduction Lexically and syntactically complex sentences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu´ısio et al., 2008b), and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999), autism spectrum disorder (ˇStajner et al., 2012; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011a; Coster and Kauchak, 201</context>
</contexts>
<marker>Inui, Fujita, Takahashi, Iida, Iwakura, 2003</marker>
<rawString>K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura. 2003. Text simplification for reading assistance: a project note. In Proceedings of the second international workshop on Paraphrasing - Volume 16, PARAPHRASE ’03, pages 9–16, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H John</author>
<author>P Langley</author>
</authors>
<title>Estimating Continuous Distributions in Bayesian Classifiers.</title>
<date>1995</date>
<booktitle>In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>338--345</pages>
<contexts>
<context position="21667" citStr="John and Langley, 1995" startWordPosition="3507" endWordPosition="3510">(2) require minimal post-editing (PE), and (3) incorrect and need to be discarded (Dis). The last experiment (Table 4) was conducted with the aim of exploring whether the classification of simplified sentences into only two classes – Retain (for further post-editing) and Discard – would lead to better results than the classification into three classes (OK, PE, and Dis) in the fifth experiment. All classification experiments were performed in Weka workbench (Witten and Frank, 2005; Hall et al., 2009), using seven classification algorithms in a 10-fold cross-validation setup: • NB – NaiveBayes (John and Langley, 1995), • SMO – Weka implementation of Support Vector Machines (Keerthi et al., 2001) with normalisation (n) or with standardisation (s), • Logistic (le Cessie and van Houwelingen, 1992), • Lazy.IBk – K-nearest neighbours (Aha and Kibler, 1991), • JRip – a propositional rule learner (Cohen, 1995), • J48 – Weka implementation of C4.5 (Quinlan, 1993). As a baseline we use the classifier which assigns the most frequent (majority) class to all instances. 4 Results and Discussion The results of the first two experiments (correlation experiments in Table 4) are presented in Section 4.1, while the results </context>
</contexts>
<marker>John, Langley, 1995</marker>
<rawString>G. H. John and P. Langley. 1995. Estimating Continuous Distributions in Bayesian Classifiers. In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, pages 338–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Keerthi</author>
<author>S K Shevade</author>
<author>C Bhattacharyya</author>
<author>K R K Murthy</author>
</authors>
<date>2001</date>
<booktitle>Improvements to Platt’s SMO Algorithm for SVM Classifier Design. Neural Computation,</booktitle>
<volume>13</volume>
<issue>3</issue>
<contexts>
<context position="21746" citStr="Keerthi et al., 2001" startWordPosition="3520" endWordPosition="3523"> (Dis). The last experiment (Table 4) was conducted with the aim of exploring whether the classification of simplified sentences into only two classes – Retain (for further post-editing) and Discard – would lead to better results than the classification into three classes (OK, PE, and Dis) in the fifth experiment. All classification experiments were performed in Weka workbench (Witten and Frank, 2005; Hall et al., 2009), using seven classification algorithms in a 10-fold cross-validation setup: • NB – NaiveBayes (John and Langley, 1995), • SMO – Weka implementation of Support Vector Machines (Keerthi et al., 2001) with normalisation (n) or with standardisation (s), • Logistic (le Cessie and van Houwelingen, 1992), • Lazy.IBk – K-nearest neighbours (Aha and Kibler, 1991), • JRip – a propositional rule learner (Cohen, 1995), • J48 – Weka implementation of C4.5 (Quinlan, 1993). As a baseline we use the classifier which assigns the most frequent (majority) class to all instances. 4 Results and Discussion The results of the first two experiments (correlation experiments in Table 4) are presented in Section 4.1, while the results of the other four experiments (classification experiments in Table 4) can be fo</context>
</contexts>
<marker>Keerthi, Shevade, Bhattacharyya, Murthy, 2001</marker>
<rawString>S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and K. R. K. Murthy. 2001. Improvements to Platt’s SMO Algorithm for SVM Classifier Design. Neural Computation, 13(3):637–649.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6771" citStr="Klein and Manning, 2003" startWordPosition="1019" endWordPosition="1022">fy/ 2The other four features contain the pairID, groupID, the method with which the simplification was obtained, and the Category weighted κ Pearson MAE Grammaticality 0.68 0.77 0.18 Meaning 0.53 0.67 0.37 Simplicity 0.54 0.60 0.28 Table 1: IAA from (Glavaˇs and ˇStajner, 2013) The simplified versions of original sentences were obtained by using four different simplification methods: baseline, sentence-wise, eventwise, and pronominal anaphora. The baseline retains only the main clause of a sentence, and discards all subordinate clauses, based on the output of the Stanford constituency parser (Klein and Manning, 2003). Sentence-wise simplification eliminates all those tokens in the original sentence that do not belong to any of the extracted factual event mentions, while the event-wise simplification transforms each factual event mention into a separate sentence of the output. The last simplification scheme (pronominal anaphora) additionally employs pronominal anaphora resolution on top of the event-wise simplification scheme.3 3.2 Human Assessment Human assessors were asked to score the given sentence pairs (or text snippets in the case of split sentences) on a 1–3 scale based on three criteria: Grammatic</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C.D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S le Cessie</author>
<author>J C van Houwelingen</author>
</authors>
<title>Ridge Estimators in Logistic Regression.</title>
<date>1992</date>
<journal>Applied Statistics,</journal>
<volume>41</volume>
<issue>1</issue>
<marker>le Cessie, van Houwelingen, 1992</marker>
<rawString>S. le Cessie and J.C. van Houwelingen. 1992. Ridge Estimators in Logistic Regression. Applied Statistics, 41(1):191–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Martos</author>
<author>S Freire</author>
<author>A Gonz´alez</author>
<author>D Gil</author>
<author>M Sebastian</author>
</authors>
<title>D2.1: Functional requirements specifications and user preference survey.</title>
<date>2012</date>
<tech>Technical report, FIRST technical report.</tech>
<marker>Martos, Freire, Gonz´alez, Gil, Sebastian, 2012</marker>
<rawString>J. Martos, S. Freire, A. Gonz´alez, D. Gil, and M. Sebastian. 2012. D2.1: Functional requirements specifications and user preference survey. Technical report, FIRST technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5011" citStr="Papineni et al., 2002" startWordPosition="752" endWordPosition="755">gree of simplification. Woodsend and Lapata (2011b), and Glavaˇs and ˇStajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from “original” to “simplified” language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TERp (Snover et al., 2009). 3 Methodology All experiments were conducted on a freely available sentence-level dataset1, fully described in (Glavaˇs and ˇStajner, 2013), and the two datasets we derived from it. The original dataset and the instructions for the human assessment are given in the next two subsections. Section 3.3 explains how we derived two additional datasets from the original one, and to what end. Section 3.4 describes the automatic MT evaluation metrics used as features in correlation and classification experiments; Section 3.5 presents the main g</context>
<context position="16016" citStr="Papineni et al., 2002" startWordPosition="2607" endWordPosition="2610">omplexity reduction performed by a TS system) could be evaluated separately, in a fully automatic manner – using some readability measures or average sentence length as features (as in (Drndarevi´c et al., 2013; Glavaˇs and ˇStajner, 4 2013) for example). 3.4 Features: MT Evaluation Metrics In all experiments, we focused on six commonly used MT evaluation metrics. These are cosine similarity (using the bag-of-words representation), METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011), and two components of TINE: T-BLEU (which differs from the standard BLEU (Papineni et al., 2002) by using 3-grams, 2-grams, and 1-grams when there are no 4-grams found, where the “original” BLEU would give score ‘0’) and SRL (which is the component of TINE based on semantic role labeling using SENNA4). Although these two components contribute equally to TINE (thus being linearly correlated with TINE), we wanted to investigate which one of them contributes more to the correlation of TINE with human judgements. Given their different natures, we expect T-BLEU to contribute more to the correlation of TINE with human judgements of grammaticality, and SRL to contribute more to the correlation </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Petersen</author>
<author>M Ostendorf</author>
</authors>
<title>Text Simplification for Language Learners: A Corpus Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of Workshop on Speech and Language Technology for Education.</booktitle>
<contexts>
<context position="1183" citStr="Petersen and Ostendorf, 2007" startWordPosition="162" endWordPosition="165"> The focus is on six widely used machine translation (MT) evaluation metrics and their correlation with human judgements of grammaticality and meaning preservation in text snippets. As the results show a significant correlation between them, we go further and try to classify simplified sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising. 1 Introduction Lexically and syntactically complex sentences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu´ısio et al., 2008b), and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999), autism spectrum disorder (ˇStajner et al., 2012; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems whi</context>
</contexts>
<marker>Petersen, Ostendorf, 2007</marker>
<rawString>S. E. Petersen and M. Ostendorf. 2007. Text Simplification for Language Learners: A Corpus Analysis. In Proceedings of Workshop on Speech and Language Technology for Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="22011" citStr="Quinlan, 1993" startWordPosition="3564" endWordPosition="3566"> (OK, PE, and Dis) in the fifth experiment. All classification experiments were performed in Weka workbench (Witten and Frank, 2005; Hall et al., 2009), using seven classification algorithms in a 10-fold cross-validation setup: • NB – NaiveBayes (John and Langley, 1995), • SMO – Weka implementation of Support Vector Machines (Keerthi et al., 2001) with normalisation (n) or with standardisation (s), • Logistic (le Cessie and van Houwelingen, 1992), • Lazy.IBk – K-nearest neighbours (Aha and Kibler, 1991), • JRip – a propositional rule learner (Cohen, 1995), • J48 – Weka implementation of C4.5 (Quinlan, 1993). As a baseline we use the classifier which assigns the most frequent (majority) class to all instances. 4 Results and Discussion The results of the first two experiments (correlation experiments in Table 4) are presented in Section 4.1, while the results of the other four experiments (classification experiments in Table 4) can be found in Section 4.2. When interpreting the results of all experiments, it is important to keep in mind that human agreements for meaning preservation (M) and grammaticality (G) were acceptable but far from perfect (Section 3.2), and thus it would be unrealistic to e</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rello</author>
</authors>
<title>Dyswebxia: a model to improve accessibility of the textual web for dyslexic users.</title>
<date>2012</date>
<booktitle>In SIGACCESS Access. Comput., number 102,</booktitle>
<pages>41--44</pages>
<publisher>ACM,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="1418" citStr="Rello, 2012" startWordPosition="201" endWordPosition="202">ther and try to classify simplified sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising. 1 Introduction Lexically and syntactically complex sentences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu´ısio et al., 2008b), and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999), autism spectrum disorder (ˇStajner et al., 2012; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Woodsend a</context>
</contexts>
<marker>Rello, 2012</marker>
<rawString>L. Rello. 2012. Dyswebxia: a model to improve accessibility of the textual web for dyslexic users. In SIGACCESS Access. Comput., number 102, pages 41–44. ACM, New York, NY, USA, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rios</author>
<author>W Aziz</author>
<author>L Specia</author>
</authors>
<title>TINE: A metric to assess MT adequacy.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation (WMT-2011),</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="15918" citStr="Rios et al., 2011" startWordPosition="2591" endWordPosition="2594"> and a worse or erroneous understanding of the text. Third, the simplicity of the output (or complexity reduction performed by a TS system) could be evaluated separately, in a fully automatic manner – using some readability measures or average sentence length as features (as in (Drndarevi´c et al., 2013; Glavaˇs and ˇStajner, 4 2013) for example). 3.4 Features: MT Evaluation Metrics In all experiments, we focused on six commonly used MT evaluation metrics. These are cosine similarity (using the bag-of-words representation), METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011), and two components of TINE: T-BLEU (which differs from the standard BLEU (Papineni et al., 2002) by using 3-grams, 2-grams, and 1-grams when there are no 4-grams found, where the “original” BLEU would give score ‘0’) and SRL (which is the component of TINE based on semantic role labeling using SENNA4). Although these two components contribute equally to TINE (thus being linearly correlated with TINE), we wanted to investigate which one of them contributes more to the correlation of TINE with human judgements. Given their different natures, we expect T-BLEU to contribute more to the correlati</context>
</contexts>
<marker>Rios, Aziz, Specia, 2011</marker>
<rawString>M. Rios, W. Aziz, and L. Specia. 2011. TINE: A metric to assess MT adequacy. In Proceedings of the Sixth Workshop on Statistical Machine Translation (WMT-2011), Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M B Ruiter</author>
<author>T C M Rietveld</author>
<author>C Cucchiarini</author>
<author>E J Krahmer</author>
<author>H Strik</author>
</authors>
<title>Human Language Technology and communicative disabilities: Requirements and possibilities for the future.</title>
<date>2010</date>
<booktitle>In Proceedings of the the seventh international conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="2271" citStr="Ruiter et al., 2010" startWordPosition="332" endWordPosition="335">formation retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008a), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), Dutch (Ruiter et al., 2010), and Italian (Barlacchi and Tonelli, 2013). Usually, TS systems are either evaluated for: (1) the quality of the generated output, or (2) the effectiveness/usefulness of such simplification on reading speed and comprehension of the target population. For the purpose of this study we focused only on the former. The quality of the output generated by TS systems is commonly evaluated by using a combination of readability metrics (measuring the degree of simplification) and human assessment (measuring the grammaticality and meaning preservation). Despite the noticeable similarity between evaluati</context>
</contexts>
<marker>Ruiter, Rietveld, Cucchiarini, Krahmer, Strik, 2010</marker>
<rawString>M. B. Ruiter, T. C. M. Rietveld, Cucchiarini C., Krahmer E. J., and H. Strik. 2010. Human Language Technology and communicative disabilities: Requirements and possibilities for the future. In Proceedings of the the seventh international conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rybing</author>
<author>C Smithr</author>
<author>A Silvervarg</author>
</authors>
<title>Towards a Rule Based System for Automatic Simplification of Texts.</title>
<date>2010</date>
<booktitle>In The Third Swedish Language Technology Conference.</booktitle>
<contexts>
<context position="2242" citStr="Rybing et al., 2010" startWordPosition="327" endWordPosition="330">sing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008a), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), Dutch (Ruiter et al., 2010), and Italian (Barlacchi and Tonelli, 2013). Usually, TS systems are either evaluated for: (1) the quality of the generated output, or (2) the effectiveness/usefulness of such simplification on reading speed and comprehension of the target population. For the purpose of this study we focused only on the former. The quality of the output generated by TS systems is commonly evaluated by using a combination of readability metrics (measuring the degree of simplification) and human assessment (measuring the grammaticality and meaning preservation). Despite the noticeabl</context>
</contexts>
<marker>Rybing, Smithr, Silvervarg, 2010</marker>
<rawString>J. Rybing, C. Smithr, and A. Silvervarg. 2010. Towards a Rule Based System for Automatic Simplification of Texts. In The Third Swedish Language Technology Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>E G´omez Martinez</author>
<author>E Etayo</author>
<author>A Anula</author>
<author>L Bourg</author>
</authors>
<date>2011</date>
<booktitle>Text Simplification in Simplext: Making Text More Accessible. Revista de la Sociedad Espa˜nola para el Procesamiento del Lenguaje Natural,</booktitle>
<pages>47--341</pages>
<contexts>
<context position="2114" citStr="Saggion et al., 2011" startWordPosition="307" endWordPosition="310">g, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008a), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), Dutch (Ruiter et al., 2010), and Italian (Barlacchi and Tonelli, 2013). Usually, TS systems are either evaluated for: (1) the quality of the generated output, or (2) the effectiveness/usefulness of such simplification on reading speed and comprehension of the target population. For the purpose of this study we focused only on the former. The quality of the output generated by TS systems is commonly evaluated by using a combination of readability metrics (measuring t</context>
</contexts>
<marker>Saggion, Martinez, Etayo, Anula, Bourg, 2011</marker>
<rawString>H. Saggion, E. G´omez Martinez, E. Etayo, A. Anula, and L. Bourg. 2011. Text Simplification in Simplext: Making Text More Accessible. Revista de la Sociedad Espa˜nola para el Procesamiento del Lenguaje Natural, 47:341–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Siddharthan</author>
</authors>
<title>Syntactic simplification and text cohesion.</title>
<date>2006</date>
<journal>Research on Language &amp; Computation,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="1988" citStr="Siddharthan, 2006" startWordPosition="287" endWordPosition="288">2; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008a), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), Dutch (Ruiter et al., 2010), and Italian (Barlacchi and Tonelli, 2013). Usually, TS systems are either evaluated for: (1) the quality of the generated output, or (2) the effectiveness/usefulness of such simplification on reading speed and comprehension of the target population. For the purpose of this study we focused only on the former. The </context>
<context position="4029" citStr="Siddharthan (2006)" startWordPosition="608" endWordPosition="610"> study to investigate the correlation of human assessment of TS system output with MT evaluation metrics. • It proposes a decision-making procedure for the classification of simplified sentences into: (1) those which are acceptable; (2) those which need further post-editing; and (3) those which should be discarded. 1 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 1–10, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related Work The output of the TS system proposed by Siddharthan (2006) was rated for grammaticality and meaning preservation by three human evaluators. Similarly, Drndarevic et al. (2013) evaluated the grammaticality and the meaning preservation of automatically simplified Spanish sentences on a Likert scale with the help of twenty-five human annotators. Additionally, the authors used seven readability metrics to assess the degree of simplification. Woodsend and Lapata (2011b), and Glavaˇs and ˇStajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability met</context>
</contexts>
<marker>Siddharthan, 2006</marker>
<rawString>A. Siddharthan. 2006. Syntactic simplification and text cohesion. Research on Language &amp; Computation, 4(1):77–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>N Madnani</author>
<author>B Dorr</author>
<author>R Schwartz</author>
</authors>
<title>Fluency, Adequacy, or HTER? Exploring Different Human Judgments with a Tunable MT Metric.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation at the 12th Meeting of the European Chapter of the Association for Computational Linguistics (EACL-2009),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="5068" citStr="Snover et al., 2009" startWordPosition="762" endWordPosition="765">avaˇs and ˇStajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from “original” to “simplified” language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TERp (Snover et al., 2009). 3 Methodology All experiments were conducted on a freely available sentence-level dataset1, fully described in (Glavaˇs and ˇStajner, 2013), and the two datasets we derived from it. The original dataset and the instructions for the human assessment are given in the next two subsections. Section 3.3 explains how we derived two additional datasets from the original one, and to what end. Section 3.4 describes the automatic MT evaluation metrics used as features in correlation and classification experiments; Section 3.5 presents the main goals of the study; and Section 3.6 describes the conducte</context>
<context position="15892" citStr="Snover et al., 2009" startWordPosition="2586" endWordPosition="2589">ystems, longer reading time, and a worse or erroneous understanding of the text. Third, the simplicity of the output (or complexity reduction performed by a TS system) could be evaluated separately, in a fully automatic manner – using some readability measures or average sentence length as features (as in (Drndarevi´c et al., 2013; Glavaˇs and ˇStajner, 4 2013) for example). 3.4 Features: MT Evaluation Metrics In all experiments, we focused on six commonly used MT evaluation metrics. These are cosine similarity (using the bag-of-words representation), METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011), and two components of TINE: T-BLEU (which differs from the standard BLEU (Papineni et al., 2002) by using 3-grams, 2-grams, and 1-grams when there are no 4-grams found, where the “original” BLEU would give score ‘0’) and SRL (which is the component of TINE based on semantic role labeling using SENNA4). Although these two components contribute equally to TINE (thus being linearly correlated with TINE), we wanted to investigate which one of them contributes more to the correlation of TINE with human judgements. Given their different natures, we expect T-BLEU to contri</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009. Fluency, Adequacy, or HTER? Exploring Different Human Judgments with a Tunable MT Metric. In Proceedings of the Fourth Workshop on Statistical Machine Translation at the 12th Meeting of the European Chapter of the Association for Computational Linguistics (EACL-2009), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Specia</author>
</authors>
<title>Translating from complex to simplified sentences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 9th international conference on Computational Processing of the Portuguese Language,</booktitle>
<pages>30--39</pages>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="4810" citStr="Specia, 2010" startWordPosition="717" endWordPosition="718">vation of automatically simplified Spanish sentences on a Likert scale with the help of twenty-five human annotators. Additionally, the authors used seven readability metrics to assess the degree of simplification. Woodsend and Lapata (2011b), and Glavaˇs and ˇStajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from “original” to “simplified” language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TERp (Snover et al., 2009). 3 Methodology All experiments were conducted on a freely available sentence-level dataset1, fully described in (Glavaˇs and ˇStajner, 2013), and the two datasets we derived from it. The original dataset and the instructions for the human assessment are given in the next two subsections. Section 3.3 explains how we derived two additional d</context>
</contexts>
<marker>Specia, 2010</marker>
<rawString>L. Specia. 2010. Translating from complex to simplified sentences. In Proceedings of the 9th international conference on Computational Processing of the Portuguese Language, pages 30–39, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S ˇStajner</author>
<author>R Evans</author>
<author>C Orasan</author>
<author>R Mitkov</author>
</authors>
<title>What Can Readability Measures Really Tell Us About Text Complexity?</title>
<date>2012</date>
<booktitle>In Proceedings of the LREC’12 Workshop: Natural Language Processing for Improving Textual Accessibility (NLP4ITA),</booktitle>
<location>Istanbul, Turkey.</location>
<marker>ˇStajner, Evans, Orasan, Mitkov, 2012</marker>
<rawString>S. ˇStajner, R. Evans, C. Orasan, and R. Mitkov. 2012. What Can Readability Measures Really Tell Us About Text Complexity? In Proceedings of the LREC’12 Workshop: Natural Language Processing for Improving Textual Accessibility (NLP4ITA), Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Data mining: practical machine learning tools and techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="21528" citStr="Witten and Frank, 2005" startWordPosition="3486" endWordPosition="3489">score Table 4: Experiments as to whether we could automatically classify the simplified sentences into those which are: (1) correct (OK), (2) require minimal post-editing (PE), and (3) incorrect and need to be discarded (Dis). The last experiment (Table 4) was conducted with the aim of exploring whether the classification of simplified sentences into only two classes – Retain (for further post-editing) and Discard – would lead to better results than the classification into three classes (OK, PE, and Dis) in the fifth experiment. All classification experiments were performed in Weka workbench (Witten and Frank, 2005; Hall et al., 2009), using seven classification algorithms in a 10-fold cross-validation setup: • NB – NaiveBayes (John and Langley, 1995), • SMO – Weka implementation of Support Vector Machines (Keerthi et al., 2001) with normalisation (n) or with standardisation (s), • Logistic (le Cessie and van Houwelingen, 1992), • Lazy.IBk – K-nearest neighbours (Aha and Kibler, 1991), • JRip – a propositional rule learner (Cohen, 1995), • J48 – Weka implementation of C4.5 (Quinlan, 1993). As a baseline we use the classifier which assigns the most frequent (majority) class to all instances. 4 Results an</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>I. H. Witten and E. Frank. 2005. Data mining: practical machine learning tools and techniques. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Woodsend</author>
<author>M Lapata</author>
</authors>
<title>Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2033" citStr="Woodsend and Lapata, 2011" startWordPosition="293" endWordPosition="297">llo, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008a), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), Dutch (Ruiter et al., 2010), and Italian (Barlacchi and Tonelli, 2013). Usually, TS systems are either evaluated for: (1) the quality of the generated output, or (2) the effectiveness/usefulness of such simplification on reading speed and comprehension of the target population. For the purpose of this study we focused only on the former. The quality of the output generated by TS systems</context>
<context position="4438" citStr="Woodsend and Lapata (2011" startWordPosition="664" endWordPosition="667">rget Reader Populations (PITR) @ EACL 2014, pages 1–10, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related Work The output of the TS system proposed by Siddharthan (2006) was rated for grammaticality and meaning preservation by three human evaluators. Similarly, Drndarevic et al. (2013) evaluated the grammaticality and the meaning preservation of automatically simplified Spanish sentences on a Likert scale with the help of twenty-five human annotators. Additionally, the authors used seven readability metrics to assess the degree of simplification. Woodsend and Lapata (2011b), and Glavaˇs and ˇStajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from “original” to “simplified” language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), </context>
</contexts>
<marker>Woodsend, Lapata, 2011</marker>
<rawString>K. Woodsend and M. Lapata. 2011a. Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Woodsend</author>
<author>M Lapata</author>
</authors>
<title>WikiSimple: Automatic Simplification of Wikipedia Articles.</title>
<date>2011</date>
<booktitle>In Proceedings of the 25th AAI Coference on Artificial Intelligence.</booktitle>
<contexts>
<context position="2033" citStr="Woodsend and Lapata, 2011" startWordPosition="293" endWordPosition="297">llo, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008a), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), Dutch (Ruiter et al., 2010), and Italian (Barlacchi and Tonelli, 2013). Usually, TS systems are either evaluated for: (1) the quality of the generated output, or (2) the effectiveness/usefulness of such simplification on reading speed and comprehension of the target population. For the purpose of this study we focused only on the former. The quality of the output generated by TS systems</context>
<context position="4438" citStr="Woodsend and Lapata (2011" startWordPosition="664" endWordPosition="667">rget Reader Populations (PITR) @ EACL 2014, pages 1–10, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related Work The output of the TS system proposed by Siddharthan (2006) was rated for grammaticality and meaning preservation by three human evaluators. Similarly, Drndarevic et al. (2013) evaluated the grammaticality and the meaning preservation of automatically simplified Spanish sentences on a Likert scale with the help of twenty-five human annotators. Additionally, the authors used seven readability metrics to assess the degree of simplification. Woodsend and Lapata (2011b), and Glavaˇs and ˇStajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from “original” to “simplified” language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), </context>
</contexts>
<marker>Woodsend, Lapata, 2011</marker>
<rawString>K. Woodsend and M. Lapata. 2011b. WikiSimple: Automatic Simplification of Wikipedia Articles. In Proceedings of the 25th AAI Coference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wubben</author>
<author>A van den Bosch</author>
<author>E Krahmer</author>
</authors>
<title>Sentence simplification by monolingual machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>1015--1024</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Wubben, van den Bosch, Krahmer, 2012</marker>
<rawString>S. Wubben, A. van den Bosch, and E. Krahmer. 2012. Sentence simplification by monolingual machine translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 1015– 1024, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhu</author>
<author>D Berndard</author>
<author>I Gurevych</author>
</authors>
<title>A Monolingual Tree-based Translation Model for Sentence Simplification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1353--1361</pages>
<contexts>
<context position="2006" citStr="Zhu et al., 2010" startWordPosition="289" endWordPosition="292">012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008a), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), Dutch (Ruiter et al., 2010), and Italian (Barlacchi and Tonelli, 2013). Usually, TS systems are either evaluated for: (1) the quality of the generated output, or (2) the effectiveness/usefulness of such simplification on reading speed and comprehension of the target population. For the purpose of this study we focused only on the former. The quality of the out</context>
<context position="4857" citStr="Zhu et al., 2010" startWordPosition="724" endWordPosition="727"> sentences on a Likert scale with the help of twenty-five human annotators. Additionally, the authors used seven readability metrics to assess the degree of simplification. Woodsend and Lapata (2011b), and Glavaˇs and ˇStajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from “original” to “simplified” language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TERp (Snover et al., 2009). 3 Methodology All experiments were conducted on a freely available sentence-level dataset1, fully described in (Glavaˇs and ˇStajner, 2013), and the two datasets we derived from it. The original dataset and the instructions for the human assessment are given in the next two subsections. Section 3.3 explains how we derived two additional datasets from the original one, and to what end.</context>
</contexts>
<marker>Zhu, Berndard, Gurevych, 2010</marker>
<rawString>Z. Zhu, D. Berndard, and I. Gurevych. 2010. A Monolingual Tree-based Translation Model for Sentence Simplification. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353–1361.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
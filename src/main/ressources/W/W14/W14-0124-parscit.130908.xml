<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000366">
<title confidence="0.995863">
Graph Based Algorithm for Automatic Domain Segmentation of WordNet
</title>
<author confidence="0.978578">
Brijesh Bhatt Subhash Kunnath Pushpak Bhattacharyya
</author>
<affiliation confidence="0.919513333333333">
Center for Indian Language Technology
Indian Institute of Technology Bombay
Mumbai, India
</affiliation>
<email confidence="0.959987">
{ brijesh, subhash, pb } @cse.iitb.ac.in
</email>
<sectionHeader confidence="0.992139" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995352">
We present a graph based algorithm for
automatic domain segmentation of Word-
net. We pose the problem as a Markov
Random Field Classification problem and
show how existing graph based algorithms
for Image Processing can be used to solve
the problem. Our approach is unsuper-
vised and can be easily adopted for any
language. We conduct our experiments
for two domains, health and tourism. We
achieve F-Score more than .70 in both do-
mains. This work can be useful for many
critical problems like word sense disam-
biguation, domain specific ontology ex-
traction etc.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999816508196721">
Over the years, Wordnet has served as an impor-
tant lexical resource for many Natural Language
Processing (NLP) applications. Picking up a right
sense of a word from the fine grained sense repos-
itory of Wordnet is at the heart of many NLP prob-
lems. Many researchers have used Wordnet for
domain specific applications like word sense dis-
ambiguation (Magnini et al., 2002a; Khapra et
al., 2010), domain specific taxonomy/ontology ex-
traction (Cimiano and Vlker, 2005; Yanna and
Zili, 2009) etc. These applications rely on ‘One
sense per discourse’ (Gale et al., 1992) hypoth-
esis to identify domain specific sense of a word.
‘Dividing Wordnet’s lexical and conceptual space
into various domain specific subspace can signif-
icantly reduce search space and thus help many
domain specific applications’ (Xiaojuan and Fell-
baum, 2012).
With the purpose of categorizing Wordnet
senses for different domain specific applications,
Magnini and Cavagli (2000) constructed a domain
hierarchy of 164 domain labels and annotated
Wordnet synsets with one or more label from the
hierarchy. The categories were further refined by
linking domain labels to subject codes of Dewey
Decimal Classification system (Bentivogli et al.,
2004). Beginning with Wordnet 2.0, Domain cate-
gory pointers were introduced to link domain spe-
cific synsets across part of speech. However, the
manual determination of a set of domain labels
and assigning them to Wordnet synsets is a time
consuming task. Also, the senses of words evolves
over a period of time and accordingly Wordnet
synsets also undergo changes. This makes the
static assignment of domain label a costly exer-
cise.
With the intention to reduce manual labor of do-
main categorization and to facilitate use of Word-
net in domain specific applications, there has been
efforts to (semi) automatically assign domain la-
bels to Wordnet synsets. Most of these efforts
rely on Wordnet concept hierarchy and use la-
bel propagation schemes to propagate domain la-
bels through the hierarchy. However, the hetero-
geneous level of generality poses a key challenge
to such approaches. For example, ‘Under Animal
(subsumed by Life Form) we find out specific con-
cepts, such as Work Animal, Domestic Animal,
kept together with general classes such as Chor-
date, Fictional Animal, etc.’(Gangemi et al.,
2003). Another key challenge in assigning the do-
main labels is the quality of domain hierarchy and
semantic distance between domain labels (Xiao-
juan and Fellbaum, 2012).
In this paper, we present a corpus based ap-
proach for automatic domain segmentation of
Wordnet. The aim of our work is to provide a gen-
eral solution that can be used across languages to
construct domain specific conceptualization from
Wordnet. The proposed system works in two
steps,
</bodyText>
<listItem confidence="0.725896">
1. We construct domain specific conceptualiza-
</listItem>
<bodyText confidence="0.9819303">
tion from the corpus.
2. The domain specific conceptualization is then
disambiguated and linked to Wordnet synsets
to generate domain labels.
We pose Wordnet domain segmentation as an
image labeling problem and use existing tech-
niques in the field of image processing system to
solve Wordnet domain labeling problem. The pro-
posed method is completely unsupervised and re-
quires only Part Of Speech tagged corpus. Hence,
it can be easily adopted across languages. Our
method also does not require any predefined set
of domain category labels, however if such labels
are available it can be incorporated into system to
generate better labeling.
The remaining of the paper is organized as fol-
lows, section 2 describes related work. Section 3
describes the proposed graph based algorithm for
Wordnet domain labeling. Section 4 and 5 discuss
the experiments and conclusion.
</bodyText>
<sectionHeader confidence="0.999685" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99919575">
Two major attempts to categorize Wordnet synsets
are Wordnet Domain (Magnini and Cavagli, 2000)
and Wordnet Domain Category pointers. In this
section we first present a brief overview of these
efforts and then describe some efforts to automate
the task of domain labeling of Wordnet synsets.
We also mention the attempts made for other lan-
guages apart from English.
</bodyText>
<subsectionHeader confidence="0.975615">
2.1 Wordnet Domain Hierarchy and Domain
Category Pointers
</subsectionHeader>
<bodyText confidence="0.999574347826087">
Domain categorization of Wordnet synset has been
an active area of research for more than a decade
now. Magnini and Cavagli (2000) have developed
Wordnet Domain Hierarchy (WDH) by annotating
Wordnet1.6 using 250 Subject Field Codes (SFC).
They used semi-automated approach in which the
top level concepts are manually marked with SFC
and then the labels are automatically propagated
through the hierarchy. Finally, the labeling is
again evaluated and refined manually. The seman-
tic structure of WDH was further refined by Ben-
tivogli et al. (2004).
Starting from Wordnet 2.0, domain category
pointers were introduced in the Wordnet. ‘Un-
like the original Wordnet Domain, the domain cat-
egory pointers use Wordnet synsets as domain la-
bels and synsets across part of speech are linked
through domain pointers’ (Xiaojuan and Fell-
baum, 2012). However, ‘only 5% of Wordnet 3.0
synsets are linked to 438 domain categories and
out of these linked synsets only 30% synsets have
same label in both Wordnet Domain and Domain
Category’.
</bodyText>
<subsectionHeader confidence="0.998197">
2.2 Automated Approaches
</subsectionHeader>
<bodyText confidence="0.999701045454546">
Considering the growing size of Wordnet and
the amount of efforts required to construct do-
main categories, it is apparent to develop semi-
automated or automated methods for domain cat-
egorization of Wordnets. One of the earlier efforts
in this direction was by Buitelaar and Sacaleanu
(2001). They extracted domain specific terms us-
ing tf*idf measure and then disambiguated these
terms using GermaNet synsets. The disambigua-
tion was performed based on the assumption that
the hypernymy and hyponymy terms are more
likely to have same domain label. Magnini et al.
(2002b) have performed a comparative study of
corpus based and ontology based domain annota-
tion. They have used frequency of words in the
synonym set as a measure to identify domain of a
synset.
Gonzalez-Agirre et al. (2012) have proposed a
semi-automatic method to align the original Word-
net 1.6 based domains to Wordnet 3.0. They have
used domain labels already assigned to some top
level synsets and then propagated the domain label
across Wordnet hierarchy using UKB algorithm
(Agirre and Soroa, 2009). Their approach is based
on an assumption that ‘A synset directly related
to several synsets labeled with a particular domain
(i.e biology) would itself possibly be also related
somehow to that domain (i.e. biology)’(Gonzalez-
Agirre et al., 2012).
Fukumoto and Suzuki (2011) have adopted a
corpus based approach to assign domain labels
to Wordnet synsets. They first disambiguate the
corpus words with Wordnet senses and then use
Markov Random Walk based Page Rank Algo-
rithm to rank domain relevance of Wordnet senses.
Zhu et al. (2011) have proposed gloss based dis-
ambiguation technique for domain assignment to
Wordnet synset. They used existing domain labels
of Wordnet 3.0 and predicted domains based on
words in the gloss of the synsets.
There have also been efforts to adopt English
Wordnet domain labels for other languages. Lee
et al. (2009) have used English-Chinese Wordnet
mapping to domain tag Chinese Wordnet.
</bodyText>
<subsectionHeader confidence="0.992515">
2.3 Proposed Approach
</subsectionHeader>
<bodyText confidence="0.9919124">
Like Buitelaar and Sacaleanu (2001), Magnini et
al. (2002b) and Fukumoto and Suzuki (2011), we
also follow corpus based approach for Wordnet
Domain Labeling. Key points of difference among
these approaches can be summerized as follows,
</bodyText>
<listItem confidence="0.997147923076923">
1. Both Buitelaar and Sacaleanu (2001) and
Magnini et al. (2002b) used word frequency
to detect domain specificity of a term. They
do not consider the label of neighbor terms to
determine the label for a term.
2. Fukumoto and Suzuki (2011) have modeled
domain labeling as a Markov Random Walk
problem, but they run their algorithm on en-
tire Wordnet graph. This is costly in terms
of time and space required for the process-
ing. In addition to that, Wordnet hypernymy-
hyponymy graph may not be a true represen-
tative of domain specific conceptualization.
</listItem>
<bodyText confidence="0.999859583333333">
In contrast to the above mentioned approaches,
our approach is based on the hypothesis that, ‘Do-
main specificity of a term depends on the spatial
property of the term’. So it is important to con-
struct a domain specific conceptualization to iden-
tify domain of a term. The domain for a con-
cept/term depends not only on the occurence of
the term in the domain but also on the neighbors
of the concept/term. Hence, we follow two step
process in which first we construct a domain con-
ceptualization from the corpus and then we align
this conceptualization with Wordnet.
</bodyText>
<sectionHeader confidence="0.995501" genericHeader="method">
3 Algorithm
</sectionHeader>
<bodyText confidence="0.999891916666667">
The proposed algorithm carves out a domain spe-
cific subgraph from the Wordnet. For that, we
first construct concept graph from the corpus and
then associate concepts with Wordnet senses. Fig-
ure 1 shows the overall system architecture. As
shown in the figure 1 after preprocessing, the sim-
ilarity graph is constructed from the corpus. Using
a graph based algorithm similarity graph is con-
verted into domain conceptualization and then it is
linked with Wordnet synsets to assign domain la-
bels to Wordnet synsets. The detailed description
of each component is as follows.
</bodyText>
<subsectionHeader confidence="0.997202">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.9999516">
The text corpus is first POS tagged using Stanford
POS tagger 1 and Morph Analyzer 2. Then term
frequency of each term is calculated using weird-
ness measure (Ahmad et al., 1999). Context vector
for each term is constructed using Point Wise Mu-
tual Information (Church and Hanks, 1990) mea-
sure. We used a sentence as a boundary to cal-
culate context vector. Output of the preprocess-
ing step is a list of domain specific terms and their
context vector.
</bodyText>
<subsectionHeader confidence="0.999302">
3.2 Constructing Document Graph
</subsectionHeader>
<bodyText confidence="0.992842125">
Using the term list and context vector generated
from the preprocessing step, a graph G(V, E) is
constructed in which each vi E V is term and
each edge e(vi, vj) is semantic relatedness be-
tween terms vi and vj. Semantic relatedness be-
tween two terms vi and vj is calculated by taking
cosine of terms vectors of vi and vj, as shown in
fig 2.
</bodyText>
<figureCaption confidence="0.995518">
Figure 2: Cosine Similarity
</figureCaption>
<subsectionHeader confidence="0.9331385">
3.3 Constructing Domain Specific
Conceptualization
</subsectionHeader>
<bodyText confidence="0.997349571428572">
Algorithm 1 Graph Cut Based Energy Minimiza-
tion
Input: set of labels L, undirected graph G(V, E)
where, V is set of random variables, E is
penalty cost, f(vl) is cost of assigning label
l E L to v E V , A set of initial labeling {(v, l),
for all v E V and l E L} and Energy Function
</bodyText>
<figure confidence="0.990186875">
0
for vi and vj E V do
Source vi
Target vj
Perform Graph Cut
Re-assign labels
Calculate 0
Repeat until 0 is minimized
</figure>
<footnote confidence="0.89338">
end for
1http://nlp.stanford.edu/software/
tagger.shtml
2http://www.sussex.ac.uk/Users/johnca/
morph.html
</footnote>
<figureCaption confidence="0.999938">
Figure 1: System Architecture
</figureCaption>
<bodyText confidence="0.9998965">
This module takes document graphs as an input
and constructs a cohesive domain specific concep-
tual structure. In order to do this, we need to clas-
sify each node in the corpus graph into various do-
mains. Assignment of a domain label to a node
depends on two parameters,
</bodyText>
<listItem confidence="0.9987792">
• Term Cost: This measures how strongly a
term belongs to the domain. It is measured
by frequency of occurrence of a term within
domain. This is formulated as a cost function
as shown in equation 1.
</listItem>
<equation confidence="0.9177135">
11 tcost = Ei(Xi) (1)
iEV
</equation>
<bodyText confidence="0.999686125">
where, Xi is the label assigned to term i and
Ei is the cost of assigning label Xi to node i.
We use term frequency based measure to cal-
culate cost of assigning label to a term. A
term should be assigned to a domain in which
it occurs more frequently. Hence, high if in-
dicates less cost to assign the term to domain.
Thus,
</bodyText>
<equation confidence="0.997836">
Ei(Xi) = 1 − tfi (2)
</equation>
<bodyText confidence="0.998921">
where, tfi is the term frequency of the term i
in domain X.
</bodyText>
<listItem confidence="0.83809725">
• Edge Cost: This measures the cost of assign-
ing separate labels to the two adjacent nodes
of an edge. This is formulated as a cost func-
tion as shown in equation 3.
</listItem>
<equation confidence="0.907059">
11 ecost = Eij(xi,xj) (3)
(i,j)EE
</equation>
<bodyText confidence="0.9997847">
where Eij(xi, xj) = cost of assigning dif-
ferent label to neighboring nodes i and j.
Eij(xi, xj) is equal to semantic similarity be-
tween nodes xi and xj. Higher the similarity
between nodes xi and xj more is the penalty
to assign different labels to xi and xj.
This can be formulated as an energy minimiza-
tion over a Markov Random Field (Kleinberg and
Tardos, 2002). Finding optimal solution is equal
to minimizing equation 4.
</bodyText>
<equation confidence="0.538611333333333">
11 minimzeO = Ei(Xi) + 11 Eij(xi, xj)
iEV (i,j)EE
(4)
</equation>
<figureCaption confidence="0.998566">
Figure 3: Domain Labeling
</figureCaption>
<bodyText confidence="0.984735617021277">
Figure 3 shows an example configuration of the
concept graph with three nodes t1, t2 and t3 and
two domains d1 and d2. Edges from the nodes ti to
dj indicates value of cost function c(p, d) of equa-
tion 2. and edges between nodes ti and tj indi-
cates cost for assigning different labels to node ti
and tj. As can be seen in the figure to minimize θ
of equation 4, node t1 will be assign to domain d2
and node t3 will be assign to domain d1. Choice
is to be made for t2, since it has equal cost to be in
d1 or d2. If t2 is assigned label d1, then ecost of
equation 2 is 0.8, since label for node t1 and t2 will
be different. In the same way ecost will be 0.2 if
t2 is assigned d2. So to minimize θ, Final labeling
is t1 and t2 are assigned d2 and t3 is assigned d1.
In other words, to minimize the cost of assign-
ment θ we cut the edge (t2, t3). Thus the energy
minimization problem can be solved by perform-
ing ‘Min-Cut’ on graph. For two labels the prob-
lem is solvable in polynomial time. However, for
more than 2 labels, solving this optimization prob-
lem is NP hard (Kolmogorov and Zabih, 2002).
In the field of image processing, many prob-
lems, e.g. image forground-background detection,
image segmentation etc. are formulated as energy
minimization in Markov Random Filed. Some of
the graph-cut based algorithms to perform the task
are, α-expansion, α − β swap (Schmidt and Ala-
hari, 2011) and α swap β shrink algorithm . For
our experiment we use α swap β shrink algorithm
proposed in Schmidt and Alahari (2011). We are
briefly describing the basic idea of the algorithms
here. Readers are directed to Kolmogorov and
Zabih (2002) and Szeliski et al. (2008) for further
details.
For more than two labels (domains), a subop-
timal solution can be derived by iteratively per-
forming graph cut for a pair of labels. This prob-
lem is usually solved using iterative descent tech-
nique. As shown in algorithm 1, the algorithm
start with an initial assignment. In each iteration
the algorithm selects a pair of labels and performs
the graph cut. Based on the graph cut the labels
will be reassigned to the nodes. The energy func-
tion θ is calculated at the end of each iteration and
the value of θ is minimized after every iteration to
guarantee the convergence.
</bodyText>
<subsectionHeader confidence="0.9353495">
3.4 Split-Merge algorithm to Link concept to
Wordnet
</subsectionHeader>
<bodyText confidence="0.999906857142857">
This module takes domain specific concept graph
generated from previous step as an input and as-
signs wordnet sense to each term. A term can have
more than one sense in the Wordnet and two terms
can refer to same Wordnet synset. So the basic
approach for the disambiguation is ‘Split for Pol-
ysemy and Merge for Synonymy’.
</bodyText>
<figure confidence="0.563457941176471">
Algorithm 2 Link with Wordnet
G(V, E)
V := vertices arranged in breadth first order
E := set of edges
|V  |:= m |E |:= n
for vi E V do
create node v0i for each sense of vi
distribute edges across senses
end for
v0 := new sense vertex set; k := |v0|
for i := 0 -+ k do
if Edge set v0 i == 0 then
delete v0i
end if
for j:=0-+kdo
if Edge set v0i == Edge set v0j then
merge v0 i and v0j
</figure>
<tableCaption confidence="0.565533">
end if
end for
end for
</tableCaption>
<bodyText confidence="0.999821333333333">
As shown in Algorithm 2, the algorithm iter-
ates through the nodes of the concept graph in a
breadth first manner. For each vertex in the graph,
all possible senses are found from the Wordnet.
If a vertex v has n senses then new nodes v1,
v2, ..., vn are created. Then the sense nodes are
linked with each other using Wordnet semantic re-
lation, e.g. if two senses si and sj are hypernym-
hyponym in wordnet then and edge is created be-
tween them.
Figure 4 shows an example of vertex split. The
left side of the fig. 4 shows concept graph for
term node cancer. The term cancer has five dif-
ferent senses. Hence the algorithm creates five
nodes for the term, one for each Wordnet sense.
Then the edges are distributed across vertices
depending upon the participating sense. Node
sign is assigned to sense 1977832, and nodes
leukemia, lymphoma and Ailment are assigned to
sense 14239981. Other sense nodes do not have
neighbors in the domain. Hence, sense 14239981
becomes winner sense in Health domain and it is
tagged in the domain. Right side of the fig. 4
shows resulting wordnet sense graph.
Once new vertices are created for all vertices in
the graph, the vertices with no edge are deleted
and vertices for which the sense ids are same are
merged as synonymy. Thus, at the end of the pro-
cess we get a Wordnet sense graph specific to the
domain. We label each sense with the specific do-
</bodyText>
<figureCaption confidence="0.992567">
Figure 4: Sense Splitting
</figureCaption>
<table confidence="0.911355333333333">
Health Tourism Domain Precision Recall F-Score
#terms 25056 56325
#terms after thresholding 4567 5968
</table>
<tableCaption confidence="0.981973">
Table 1: Corpus Statistics
</tableCaption>
<table confidence="0.7550685">
Health 0.69 0.82 0.74
Tourism 0.65 0.80 0.71
</table>
<tableCaption confidence="0.9002525">
Table 2: Precision and Recall of domain labeling
main tag.
</tableCaption>
<sectionHeader confidence="0.997564" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999258461538461">
We have conducted our experiments on publicly
available Heath and Tourism Corpus 3 (Khapra et
al., 2010). As shown in Table 1 the total number
of unique terms after preprocessing and stop word
removal are 25056 in health domain and 56325 in
tourism domain. We applied further thresholding
and remove low frequency terms (Frequency less
than 10) to reduce the size of the graph.
For preprocessing we have used Stanford POS
tagger and morpha morph analyzer. We have
used Matlab UGM package 4 which is publicly
available for researchers. UGM package pro-
vides implementation of α-expand, α − β swap
</bodyText>
<footnote confidence="0.9649635">
3http://www.cfilt.iitb.ac.in/wsd/
annotated_corpus
4http://www.di.ens.fr/˜mschmidt/
Software/alphaBeta.html
</footnote>
<bodyText confidence="0.998700888888889">
and α-expansion-β-Shrink algorithms. The graph
based disambiguation algorithm is written using
JGraphT library 5.
The overall performance of the system is calcu-
lated against manually labeled domain tags. Ta-
ble 2 shows overall precision, recall and f-score
for both the domains.
As shown in Table 2 the recall value is found to
be higher than the precision in both the domains.
Reason for high value of recall is the initial labels
and high number of edges. Initial labels are as-
signed based on the term frequency, then based on
the labels of the neighboring nodes, node labels
are changed. We observe that in case of two do-
mains this leads to add more false positives. In or-
der to reduce recall value and increase precision,
we need to run experiments for more domains and
with higher edge weights.
</bodyText>
<footnote confidence="0.912597">
5http://jgrapht.org/
</footnote>
<sectionHeader confidence="0.994626" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999979571428572">
We have proposed a novel graph based approach
for automatic domain tagging of WordNet synsets.
We pose domain labeling as an energy minization
problem and show how the existing image label-
ing algorithms can be used for the task of Word-
Net domain tagging. Our approach is completely
unsupervised and can be easily adopted across lan-
guages. For our experiments we used term fre-
quency based assignment of initial labels, however
other existing label can be used to enhance the la-
beling. In future we aim to construct domain la-
bels for more domains and compare our system
with existing labeling. We are also aiming to test
our system for multiple languages.
</bodyText>
<sectionHeader confidence="0.992882" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.967253092783505">
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, EACL ’09, pages 33–41.
Khurshid Ahmad, Lee Gillam, Lena Tostevin, and
Ai Group. 1999. Weirdness indexing for logical
document extrapolation and retrieval (wilder). In
The Eighth Text REtrieval Conference.
Luisa Bentivogli, Pamela Forner, Bernardo Magnini,
and Emanuele Pianta. 2004. Revising the wordnet
domains hierarchy: semantics, coverage and balanc-
ing. In Proceedings of the Workshop on Multilingual
Linguistic Ressources, MLR ’04, pages 101–108.
Paul Buitelaar and Bogdan Sacaleanu. 2001. Rank-
ing and selecting synsets by domain relevance. In
proceedings NAACL wordnet workshop.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Comput. Linguist., 16(1):22–29.
Philipp Cimiano and Johanna Vlker. 2005. Text2onto
- a framework for ontology learning and data-driven
change discovery. In Proceedings of the 10th In-
ternational Conference on Applications of Natural
Language to Information Systems (NLDB), volume
3513 of Lecture Notes in Computer Science, pages
227–238.
Fumiyo Fukumoto and Yoshimi Suzuki. 2011. Iden-
tification of domain-specific senses in a machine-
readable dictionary. In ACL (Short Papers), pages
552–557.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural
Language, HLT ’91, pages 233–237.
Aldo Gangemi, Nicola Guarino, Claudio Masolo, and
Alessandro Oltramari. 2003. Sweetening wordnet
with dolce. AI Mag., 24(3):13–24.
Aitor Gonzalez-Agirre, Mauro Castillo, and German
Rigau. 2012. A proposal for improving wordnet do-
mains. In Proceedings of Language Resources and
Evaluation Conference, pages 3457–3462.
Mitesh M. Khapra, Anup Kulkarni, Saurabh Sohoney,
and Pushpak Bhattacharyya. 2010. All words do-
main adapted wsd: finding a middle ground between
supervision and unsupervision. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ’10, pages 1532–1541.
Jon Kleinberg and ´Eva Tardos. 2002. Approximation
algorithms for classification problems with pairwise
relationships: metric labeling and markov random
fields. J. ACM, 49(5):616–639.
Vladimir Kolmogorov and Ramin Zabih. 2002. What
energy functions can be minimized via graph cuts?
In Proceedings of the 7th European Conference on
Computer Vision-Part III, ECCV ’02, pages 65–81.
Lung-Hao Lee, Yu-Ting Yu, and Chu-Ren Huang.
2009. Chinese wordnet domains: Bootstrapping
chinese wordnet with semantic domain labels. In
Olivia Kwong, editor, PACLIC, pages 288–296.
Bernardo Magnini and Gabriela Cavagli. 2000. In-
tegrating subject field codes into wordnet. pages
1413–1418.
Bernardo Magnini, Giovanni Pezzulo, and Alfio
Gliozzo. 2002a. The role of domain information
in word sense disambiguation. Natural Language
Engineering, 8:359–373.
Bernardo Magnini, Carlo Strapparava, Giovanni Pez-
zulo, and Alfio Gliozzo. 2002b. Comparing
ontology-based and corpus-based domain annota-
tions in wordnet. In First International Global
WordNet Conference, Mysore, India.
Mark W. Schmidt and Karteek Alahari. 2011. General-
ized fast approximate energy minimization via graph
cuts: Alpha-expansion beta-shrink moves. CoRR,
abs/1108.5710.
Richard Szeliski, Ramin Zabih, Daniel Scharstein,
Olga Veksler, Vladimir Kolmogorov, Aseem Agar-
wala, Marshall Tappen, and Carsten Rother. 2008.
A comparative study of energy minimization meth-
ods for markov random fields with smoothness-
based priors. IEEE Trans. Pattern Anal. Mach. In-
tell., 30(6):1068–1080.
Ma Xiaojuan and Christiane Fellbaum. 2012. Re-
thinking wordnet’s domains. In Proceedings of 6th
International Global WordNet Conference, Matsue,
Japan, jan.
Wang Yanna and Zhou Zili. 2009. Domain on-
tology generation based on wordnet and internet.
In Proceedings of the International Conference on
Management and Service Science, 2009. MASS ’09,
pages 1 –5.
Chaoyong Zhu, Shumin Shi, and Haijun Zhang. 2011.
Gloss-based word domain assignment. In 7th Inter-
national Conference on Natural Language Process-
ing and Knowledge Engineering (NLP-KE), pages
150–155.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.799355">
<title confidence="0.998205">Graph Based Algorithm for Automatic Domain Segmentation of WordNet</title>
<author confidence="0.991375">Brijesh Bhatt Subhash Kunnath Pushpak</author>
<affiliation confidence="0.9982585">Center for Indian Language Indian Institute of Technology</affiliation>
<address confidence="0.879785">Mumbai,</address>
<email confidence="0.98982">subhash,pb</email>
<abstract confidence="0.9953939375">We present a graph based algorithm for automatic domain segmentation of Wordnet. We pose the problem as a Markov Random Field Classification problem and show how existing graph based algorithms for Image Processing can be used to solve the problem. Our approach is unsupervised and can be easily adopted for any language. We conduct our experiments for two domains, health and tourism. We achieve F-Score more than .70 in both domains. This work can be useful for many problems like sense disambiguation, domain specific ontology extraction etc.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing pagerank for word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09,</booktitle>
<pages>33--41</pages>
<contexts>
<context position="7056" citStr="Agirre and Soroa, 2009" startWordPosition="1122" endWordPosition="1125">sed on the assumption that the hypernymy and hyponymy terms are more likely to have same domain label. Magnini et al. (2002b) have performed a comparative study of corpus based and ontology based domain annotation. They have used frequency of words in the synonym set as a measure to identify domain of a synset. Gonzalez-Agirre et al. (2012) have proposed a semi-automatic method to align the original Wordnet 1.6 based domains to Wordnet 3.0. They have used domain labels already assigned to some top level synsets and then propagated the domain label across Wordnet hierarchy using UKB algorithm (Agirre and Soroa, 2009). Their approach is based on an assumption that ‘A synset directly related to several synsets labeled with a particular domain (i.e biology) would itself possibly be also related somehow to that domain (i.e. biology)’(GonzalezAgirre et al., 2012). Fukumoto and Suzuki (2011) have adopted a corpus based approach to assign domain labels to Wordnet synsets. They first disambiguate the corpus words with Wordnet senses and then use Markov Random Walk based Page Rank Algorithm to rank domain relevance of Wordnet senses. Zhu et al. (2011) have proposed gloss based disambiguation technique for domain a</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing pagerank for word sense disambiguation. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09, pages 33–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khurshid Ahmad</author>
<author>Lee Gillam</author>
<author>Lena Tostevin</author>
<author>Ai Group</author>
</authors>
<title>Weirdness indexing for logical document extrapolation and retrieval (wilder).</title>
<date>1999</date>
<booktitle>In The Eighth Text REtrieval Conference.</booktitle>
<contexts>
<context position="10128" citStr="Ahmad et al., 1999" startWordPosition="1631" endWordPosition="1634">n associate concepts with Wordnet senses. Figure 1 shows the overall system architecture. As shown in the figure 1 after preprocessing, the similarity graph is constructed from the corpus. Using a graph based algorithm similarity graph is converted into domain conceptualization and then it is linked with Wordnet synsets to assign domain labels to Wordnet synsets. The detailed description of each component is as follows. 3.1 Preprocessing The text corpus is first POS tagged using Stanford POS tagger 1 and Morph Analyzer 2. Then term frequency of each term is calculated using weirdness measure (Ahmad et al., 1999). Context vector for each term is constructed using Point Wise Mutual Information (Church and Hanks, 1990) measure. We used a sentence as a boundary to calculate context vector. Output of the preprocessing step is a list of domain specific terms and their context vector. 3.2 Constructing Document Graph Using the term list and context vector generated from the preprocessing step, a graph G(V, E) is constructed in which each vi E V is term and each edge e(vi, vj) is semantic relatedness between terms vi and vj. Semantic relatedness between two terms vi and vj is calculated by taking cosine of te</context>
</contexts>
<marker>Ahmad, Gillam, Tostevin, Group, 1999</marker>
<rawString>Khurshid Ahmad, Lee Gillam, Lena Tostevin, and Ai Group. 1999. Weirdness indexing for logical document extrapolation and retrieval (wilder). In The Eighth Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Pamela Forner</author>
<author>Bernardo Magnini</author>
<author>Emanuele Pianta</author>
</authors>
<title>Revising the wordnet domains hierarchy: semantics, coverage and balancing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Multilingual Linguistic Ressources, MLR ’04,</booktitle>
<pages>101--108</pages>
<contexts>
<context position="2045" citStr="Bentivogli et al., 2004" startWordPosition="315" endWordPosition="318">domain specific sense of a word. ‘Dividing Wordnet’s lexical and conceptual space into various domain specific subspace can significantly reduce search space and thus help many domain specific applications’ (Xiaojuan and Fellbaum, 2012). With the purpose of categorizing Wordnet senses for different domain specific applications, Magnini and Cavagli (2000) constructed a domain hierarchy of 164 domain labels and annotated Wordnet synsets with one or more label from the hierarchy. The categories were further refined by linking domain labels to subject codes of Dewey Decimal Classification system (Bentivogli et al., 2004). Beginning with Wordnet 2.0, Domain category pointers were introduced to link domain specific synsets across part of speech. However, the manual determination of a set of domain labels and assigning them to Wordnet synsets is a time consuming task. Also, the senses of words evolves over a period of time and accordingly Wordnet synsets also undergo changes. This makes the static assignment of domain label a costly exercise. With the intention to reduce manual labor of domain categorization and to facilitate use of Wordnet in domain specific applications, there has been efforts to (semi) automa</context>
<context position="5488" citStr="Bentivogli et al. (2004)" startWordPosition="867" endWordPosition="871">apart from English. 2.1 Wordnet Domain Hierarchy and Domain Category Pointers Domain categorization of Wordnet synset has been an active area of research for more than a decade now. Magnini and Cavagli (2000) have developed Wordnet Domain Hierarchy (WDH) by annotating Wordnet1.6 using 250 Subject Field Codes (SFC). They used semi-automated approach in which the top level concepts are manually marked with SFC and then the labels are automatically propagated through the hierarchy. Finally, the labeling is again evaluated and refined manually. The semantic structure of WDH was further refined by Bentivogli et al. (2004). Starting from Wordnet 2.0, domain category pointers were introduced in the Wordnet. ‘Unlike the original Wordnet Domain, the domain category pointers use Wordnet synsets as domain labels and synsets across part of speech are linked through domain pointers’ (Xiaojuan and Fellbaum, 2012). However, ‘only 5% of Wordnet 3.0 synsets are linked to 438 domain categories and out of these linked synsets only 30% synsets have same label in both Wordnet Domain and Domain Category’. 2.2 Automated Approaches Considering the growing size of Wordnet and the amount of efforts required to construct domain cat</context>
</contexts>
<marker>Bentivogli, Forner, Magnini, Pianta, 2004</marker>
<rawString>Luisa Bentivogli, Pamela Forner, Bernardo Magnini, and Emanuele Pianta. 2004. Revising the wordnet domains hierarchy: semantics, coverage and balancing. In Proceedings of the Workshop on Multilingual Linguistic Ressources, MLR ’04, pages 101–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Buitelaar</author>
<author>Bogdan Sacaleanu</author>
</authors>
<title>Ranking and selecting synsets by domain relevance.</title>
<date>2001</date>
<booktitle>In proceedings NAACL wordnet workshop.</booktitle>
<contexts>
<context position="6279" citStr="Buitelaar and Sacaleanu (2001)" startWordPosition="996" endWordPosition="999">et synsets as domain labels and synsets across part of speech are linked through domain pointers’ (Xiaojuan and Fellbaum, 2012). However, ‘only 5% of Wordnet 3.0 synsets are linked to 438 domain categories and out of these linked synsets only 30% synsets have same label in both Wordnet Domain and Domain Category’. 2.2 Automated Approaches Considering the growing size of Wordnet and the amount of efforts required to construct domain categories, it is apparent to develop semiautomated or automated methods for domain categorization of Wordnets. One of the earlier efforts in this direction was by Buitelaar and Sacaleanu (2001). They extracted domain specific terms using tf*idf measure and then disambiguated these terms using GermaNet synsets. The disambiguation was performed based on the assumption that the hypernymy and hyponymy terms are more likely to have same domain label. Magnini et al. (2002b) have performed a comparative study of corpus based and ontology based domain annotation. They have used frequency of words in the synonym set as a measure to identify domain of a synset. Gonzalez-Agirre et al. (2012) have proposed a semi-automatic method to align the original Wordnet 1.6 based domains to Wordnet 3.0. T</context>
<context position="8036" citStr="Buitelaar and Sacaleanu (2001)" startWordPosition="1277" endWordPosition="1280">sets. They first disambiguate the corpus words with Wordnet senses and then use Markov Random Walk based Page Rank Algorithm to rank domain relevance of Wordnet senses. Zhu et al. (2011) have proposed gloss based disambiguation technique for domain assignment to Wordnet synset. They used existing domain labels of Wordnet 3.0 and predicted domains based on words in the gloss of the synsets. There have also been efforts to adopt English Wordnet domain labels for other languages. Lee et al. (2009) have used English-Chinese Wordnet mapping to domain tag Chinese Wordnet. 2.3 Proposed Approach Like Buitelaar and Sacaleanu (2001), Magnini et al. (2002b) and Fukumoto and Suzuki (2011), we also follow corpus based approach for Wordnet Domain Labeling. Key points of difference among these approaches can be summerized as follows, 1. Both Buitelaar and Sacaleanu (2001) and Magnini et al. (2002b) used word frequency to detect domain specificity of a term. They do not consider the label of neighbor terms to determine the label for a term. 2. Fukumoto and Suzuki (2011) have modeled domain labeling as a Markov Random Walk problem, but they run their algorithm on entire Wordnet graph. This is costly in terms of time and space r</context>
</contexts>
<marker>Buitelaar, Sacaleanu, 2001</marker>
<rawString>Paul Buitelaar and Bogdan Sacaleanu. 2001. Ranking and selecting synsets by domain relevance. In proceedings NAACL wordnet workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Comput. Linguist.,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="10234" citStr="Church and Hanks, 1990" startWordPosition="1648" endWordPosition="1651">the figure 1 after preprocessing, the similarity graph is constructed from the corpus. Using a graph based algorithm similarity graph is converted into domain conceptualization and then it is linked with Wordnet synsets to assign domain labels to Wordnet synsets. The detailed description of each component is as follows. 3.1 Preprocessing The text corpus is first POS tagged using Stanford POS tagger 1 and Morph Analyzer 2. Then term frequency of each term is calculated using weirdness measure (Ahmad et al., 1999). Context vector for each term is constructed using Point Wise Mutual Information (Church and Hanks, 1990) measure. We used a sentence as a boundary to calculate context vector. Output of the preprocessing step is a list of domain specific terms and their context vector. 3.2 Constructing Document Graph Using the term list and context vector generated from the preprocessing step, a graph G(V, E) is constructed in which each vi E V is term and each edge e(vi, vj) is semantic relatedness between terms vi and vj. Semantic relatedness between two terms vi and vj is calculated by taking cosine of terms vectors of vi and vj, as shown in fig 2. Figure 2: Cosine Similarity 3.3 Constructing Domain Specific </context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Comput. Linguist., 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Johanna Vlker</author>
</authors>
<title>Text2onto - a framework for ontology learning and data-driven change discovery.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th International Conference on Applications of Natural Language to Information Systems (NLDB),</booktitle>
<volume>3513</volume>
<pages>227--238</pages>
<contexts>
<context position="1296" citStr="Cimiano and Vlker, 2005" startWordPosition="202" endWordPosition="205"> .70 in both domains. This work can be useful for many critical problems like word sense disambiguation, domain specific ontology extraction etc. 1 Introduction Over the years, Wordnet has served as an important lexical resource for many Natural Language Processing (NLP) applications. Picking up a right sense of a word from the fine grained sense repository of Wordnet is at the heart of many NLP problems. Many researchers have used Wordnet for domain specific applications like word sense disambiguation (Magnini et al., 2002a; Khapra et al., 2010), domain specific taxonomy/ontology extraction (Cimiano and Vlker, 2005; Yanna and Zili, 2009) etc. These applications rely on ‘One sense per discourse’ (Gale et al., 1992) hypothesis to identify domain specific sense of a word. ‘Dividing Wordnet’s lexical and conceptual space into various domain specific subspace can significantly reduce search space and thus help many domain specific applications’ (Xiaojuan and Fellbaum, 2012). With the purpose of categorizing Wordnet senses for different domain specific applications, Magnini and Cavagli (2000) constructed a domain hierarchy of 164 domain labels and annotated Wordnet synsets with one or more label from the hier</context>
</contexts>
<marker>Cimiano, Vlker, 2005</marker>
<rawString>Philipp Cimiano and Johanna Vlker. 2005. Text2onto - a framework for ontology learning and data-driven change discovery. In Proceedings of the 10th International Conference on Applications of Natural Language to Information Systems (NLDB), volume 3513 of Lecture Notes in Computer Science, pages 227–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fumiyo Fukumoto</author>
<author>Yoshimi Suzuki</author>
</authors>
<title>Identification of domain-specific senses in a machinereadable dictionary.</title>
<date>2011</date>
<booktitle>In ACL (Short Papers),</booktitle>
<pages>552--557</pages>
<contexts>
<context position="7330" citStr="Fukumoto and Suzuki (2011)" startWordPosition="1164" endWordPosition="1167">a measure to identify domain of a synset. Gonzalez-Agirre et al. (2012) have proposed a semi-automatic method to align the original Wordnet 1.6 based domains to Wordnet 3.0. They have used domain labels already assigned to some top level synsets and then propagated the domain label across Wordnet hierarchy using UKB algorithm (Agirre and Soroa, 2009). Their approach is based on an assumption that ‘A synset directly related to several synsets labeled with a particular domain (i.e biology) would itself possibly be also related somehow to that domain (i.e. biology)’(GonzalezAgirre et al., 2012). Fukumoto and Suzuki (2011) have adopted a corpus based approach to assign domain labels to Wordnet synsets. They first disambiguate the corpus words with Wordnet senses and then use Markov Random Walk based Page Rank Algorithm to rank domain relevance of Wordnet senses. Zhu et al. (2011) have proposed gloss based disambiguation technique for domain assignment to Wordnet synset. They used existing domain labels of Wordnet 3.0 and predicted domains based on words in the gloss of the synsets. There have also been efforts to adopt English Wordnet domain labels for other languages. Lee et al. (2009) have used English-Chines</context>
</contexts>
<marker>Fukumoto, Suzuki, 2011</marker>
<rawString>Fumiyo Fukumoto and Yoshimi Suzuki. 2011. Identification of domain-specific senses in a machinereadable dictionary. In ACL (Short Papers), pages 552–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language, HLT ’91,</booktitle>
<pages>233--237</pages>
<contexts>
<context position="1397" citStr="Gale et al., 1992" startWordPosition="219" endWordPosition="222">omain specific ontology extraction etc. 1 Introduction Over the years, Wordnet has served as an important lexical resource for many Natural Language Processing (NLP) applications. Picking up a right sense of a word from the fine grained sense repository of Wordnet is at the heart of many NLP problems. Many researchers have used Wordnet for domain specific applications like word sense disambiguation (Magnini et al., 2002a; Khapra et al., 2010), domain specific taxonomy/ontology extraction (Cimiano and Vlker, 2005; Yanna and Zili, 2009) etc. These applications rely on ‘One sense per discourse’ (Gale et al., 1992) hypothesis to identify domain specific sense of a word. ‘Dividing Wordnet’s lexical and conceptual space into various domain specific subspace can significantly reduce search space and thus help many domain specific applications’ (Xiaojuan and Fellbaum, 2012). With the purpose of categorizing Wordnet senses for different domain specific applications, Magnini and Cavagli (2000) constructed a domain hierarchy of 164 domain labels and annotated Wordnet synsets with one or more label from the hierarchy. The categories were further refined by linking domain labels to subject codes of Dewey Decimal</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. One sense per discourse. In Proceedings of the workshop on Speech and Natural Language, HLT ’91, pages 233–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aldo Gangemi</author>
<author>Nicola Guarino</author>
<author>Claudio Masolo</author>
<author>Alessandro Oltramari</author>
</authors>
<title>Sweetening wordnet with dolce.</title>
<date>2003</date>
<journal>AI Mag.,</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="3141" citStr="Gangemi et al., 2003" startWordPosition="494" endWordPosition="497">main categorization and to facilitate use of Wordnet in domain specific applications, there has been efforts to (semi) automatically assign domain labels to Wordnet synsets. Most of these efforts rely on Wordnet concept hierarchy and use label propagation schemes to propagate domain labels through the hierarchy. However, the heterogeneous level of generality poses a key challenge to such approaches. For example, ‘Under Animal (subsumed by Life Form) we find out specific concepts, such as Work Animal, Domestic Animal, kept together with general classes such as Chordate, Fictional Animal, etc.’(Gangemi et al., 2003). Another key challenge in assigning the domain labels is the quality of domain hierarchy and semantic distance between domain labels (Xiaojuan and Fellbaum, 2012). In this paper, we present a corpus based approach for automatic domain segmentation of Wordnet. The aim of our work is to provide a general solution that can be used across languages to construct domain specific conceptualization from Wordnet. The proposed system works in two steps, 1. We construct domain specific conceptualization from the corpus. 2. The domain specific conceptualization is then disambiguated and linked to Wordnet</context>
</contexts>
<marker>Gangemi, Guarino, Masolo, Oltramari, 2003</marker>
<rawString>Aldo Gangemi, Nicola Guarino, Claudio Masolo, and Alessandro Oltramari. 2003. Sweetening wordnet with dolce. AI Mag., 24(3):13–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aitor Gonzalez-Agirre</author>
<author>Mauro Castillo</author>
<author>German Rigau</author>
</authors>
<title>A proposal for improving wordnet domains.</title>
<date>2012</date>
<booktitle>In Proceedings of Language Resources and Evaluation Conference,</booktitle>
<pages>3457--3462</pages>
<contexts>
<context position="6775" citStr="Gonzalez-Agirre et al. (2012)" startWordPosition="1077" endWordPosition="1080">ted methods for domain categorization of Wordnets. One of the earlier efforts in this direction was by Buitelaar and Sacaleanu (2001). They extracted domain specific terms using tf*idf measure and then disambiguated these terms using GermaNet synsets. The disambiguation was performed based on the assumption that the hypernymy and hyponymy terms are more likely to have same domain label. Magnini et al. (2002b) have performed a comparative study of corpus based and ontology based domain annotation. They have used frequency of words in the synonym set as a measure to identify domain of a synset. Gonzalez-Agirre et al. (2012) have proposed a semi-automatic method to align the original Wordnet 1.6 based domains to Wordnet 3.0. They have used domain labels already assigned to some top level synsets and then propagated the domain label across Wordnet hierarchy using UKB algorithm (Agirre and Soroa, 2009). Their approach is based on an assumption that ‘A synset directly related to several synsets labeled with a particular domain (i.e biology) would itself possibly be also related somehow to that domain (i.e. biology)’(GonzalezAgirre et al., 2012). Fukumoto and Suzuki (2011) have adopted a corpus based approach to assi</context>
</contexts>
<marker>Gonzalez-Agirre, Castillo, Rigau, 2012</marker>
<rawString>Aitor Gonzalez-Agirre, Mauro Castillo, and German Rigau. 2012. A proposal for improving wordnet domains. In Proceedings of Language Resources and Evaluation Conference, pages 3457–3462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitesh M Khapra</author>
<author>Anup Kulkarni</author>
<author>Saurabh Sohoney</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>All words domain adapted wsd: finding a middle ground between supervision and unsupervision.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1532--1541</pages>
<contexts>
<context position="1225" citStr="Khapra et al., 2010" startWordPosition="193" endWordPosition="196">ts for two domains, health and tourism. We achieve F-Score more than .70 in both domains. This work can be useful for many critical problems like word sense disambiguation, domain specific ontology extraction etc. 1 Introduction Over the years, Wordnet has served as an important lexical resource for many Natural Language Processing (NLP) applications. Picking up a right sense of a word from the fine grained sense repository of Wordnet is at the heart of many NLP problems. Many researchers have used Wordnet for domain specific applications like word sense disambiguation (Magnini et al., 2002a; Khapra et al., 2010), domain specific taxonomy/ontology extraction (Cimiano and Vlker, 2005; Yanna and Zili, 2009) etc. These applications rely on ‘One sense per discourse’ (Gale et al., 1992) hypothesis to identify domain specific sense of a word. ‘Dividing Wordnet’s lexical and conceptual space into various domain specific subspace can significantly reduce search space and thus help many domain specific applications’ (Xiaojuan and Fellbaum, 2012). With the purpose of categorizing Wordnet senses for different domain specific applications, Magnini and Cavagli (2000) constructed a domain hierarchy of 164 domain la</context>
<context position="17779" citStr="Khapra et al., 2010" startWordPosition="3052" endWordPosition="3055">he graph, the vertices with no edge are deleted and vertices for which the sense ids are same are merged as synonymy. Thus, at the end of the process we get a Wordnet sense graph specific to the domain. We label each sense with the specific doFigure 4: Sense Splitting Health Tourism Domain Precision Recall F-Score #terms 25056 56325 #terms after thresholding 4567 5968 Table 1: Corpus Statistics Health 0.69 0.82 0.74 Tourism 0.65 0.80 0.71 Table 2: Precision and Recall of domain labeling main tag. 4 Experiments We have conducted our experiments on publicly available Heath and Tourism Corpus 3 (Khapra et al., 2010). As shown in Table 1 the total number of unique terms after preprocessing and stop word removal are 25056 in health domain and 56325 in tourism domain. We applied further thresholding and remove low frequency terms (Frequency less than 10) to reduce the size of the graph. For preprocessing we have used Stanford POS tagger and morpha morph analyzer. We have used Matlab UGM package 4 which is publicly available for researchers. UGM package provides implementation of α-expand, α − β swap 3http://www.cfilt.iitb.ac.in/wsd/ annotated_corpus 4http://www.di.ens.fr/˜mschmidt/ Software/alphaBeta.html a</context>
</contexts>
<marker>Khapra, Kulkarni, Sohoney, Bhattacharyya, 2010</marker>
<rawString>Mitesh M. Khapra, Anup Kulkarni, Saurabh Sohoney, and Pushpak Bhattacharyya. 2010. All words domain adapted wsd: finding a middle ground between supervision and unsupervision. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1532–1541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Kleinberg</author>
<author>´Eva Tardos</author>
</authors>
<title>Approximation algorithms for classification problems with pairwise relationships: metric labeling and markov random fields.</title>
<date>2002</date>
<journal>J. ACM,</journal>
<volume>49</volume>
<issue>5</issue>
<contexts>
<context position="12850" citStr="Kleinberg and Tardos, 2002" startWordPosition="2135" endWordPosition="2138">(2) where, tfi is the term frequency of the term i in domain X. • Edge Cost: This measures the cost of assigning separate labels to the two adjacent nodes of an edge. This is formulated as a cost function as shown in equation 3. 11 ecost = Eij(xi,xj) (3) (i,j)EE where Eij(xi, xj) = cost of assigning different label to neighboring nodes i and j. Eij(xi, xj) is equal to semantic similarity between nodes xi and xj. Higher the similarity between nodes xi and xj more is the penalty to assign different labels to xi and xj. This can be formulated as an energy minimization over a Markov Random Field (Kleinberg and Tardos, 2002). Finding optimal solution is equal to minimizing equation 4. 11 minimzeO = Ei(Xi) + 11 Eij(xi, xj) iEV (i,j)EE (4) Figure 3: Domain Labeling Figure 3 shows an example configuration of the concept graph with three nodes t1, t2 and t3 and two domains d1 and d2. Edges from the nodes ti to dj indicates value of cost function c(p, d) of equation 2. and edges between nodes ti and tj indicates cost for assigning different labels to node ti and tj. As can be seen in the figure to minimize θ of equation 4, node t1 will be assign to domain d2 and node t3 will be assign to domain d1. Choice is to be mad</context>
</contexts>
<marker>Kleinberg, Tardos, 2002</marker>
<rawString>Jon Kleinberg and ´Eva Tardos. 2002. Approximation algorithms for classification problems with pairwise relationships: metric labeling and markov random fields. J. ACM, 49(5):616–639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Kolmogorov</author>
<author>Ramin Zabih</author>
</authors>
<title>What energy functions can be minimized via graph cuts?</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th European Conference on Computer Vision-Part III, ECCV ’02,</booktitle>
<pages>65--81</pages>
<contexts>
<context position="14084" citStr="Kolmogorov and Zabih, 2002" startWordPosition="2381" endWordPosition="2384">2, since it has equal cost to be in d1 or d2. If t2 is assigned label d1, then ecost of equation 2 is 0.8, since label for node t1 and t2 will be different. In the same way ecost will be 0.2 if t2 is assigned d2. So to minimize θ, Final labeling is t1 and t2 are assigned d2 and t3 is assigned d1. In other words, to minimize the cost of assignment θ we cut the edge (t2, t3). Thus the energy minimization problem can be solved by performing ‘Min-Cut’ on graph. For two labels the problem is solvable in polynomial time. However, for more than 2 labels, solving this optimization problem is NP hard (Kolmogorov and Zabih, 2002). In the field of image processing, many problems, e.g. image forground-background detection, image segmentation etc. are formulated as energy minimization in Markov Random Filed. Some of the graph-cut based algorithms to perform the task are, α-expansion, α − β swap (Schmidt and Alahari, 2011) and α swap β shrink algorithm . For our experiment we use α swap β shrink algorithm proposed in Schmidt and Alahari (2011). We are briefly describing the basic idea of the algorithms here. Readers are directed to Kolmogorov and Zabih (2002) and Szeliski et al. (2008) for further details. For more than t</context>
</contexts>
<marker>Kolmogorov, Zabih, 2002</marker>
<rawString>Vladimir Kolmogorov and Ramin Zabih. 2002. What energy functions can be minimized via graph cuts? In Proceedings of the 7th European Conference on Computer Vision-Part III, ECCV ’02, pages 65–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lung-Hao Lee</author>
<author>Yu-Ting Yu</author>
<author>Chu-Ren Huang</author>
</authors>
<title>Chinese wordnet domains: Bootstrapping chinese wordnet with semantic domain labels.</title>
<date>2009</date>
<pages>288--296</pages>
<editor>In Olivia Kwong, editor, PACLIC,</editor>
<contexts>
<context position="7905" citStr="Lee et al. (2009)" startWordPosition="1259" endWordPosition="1262"> et al., 2012). Fukumoto and Suzuki (2011) have adopted a corpus based approach to assign domain labels to Wordnet synsets. They first disambiguate the corpus words with Wordnet senses and then use Markov Random Walk based Page Rank Algorithm to rank domain relevance of Wordnet senses. Zhu et al. (2011) have proposed gloss based disambiguation technique for domain assignment to Wordnet synset. They used existing domain labels of Wordnet 3.0 and predicted domains based on words in the gloss of the synsets. There have also been efforts to adopt English Wordnet domain labels for other languages. Lee et al. (2009) have used English-Chinese Wordnet mapping to domain tag Chinese Wordnet. 2.3 Proposed Approach Like Buitelaar and Sacaleanu (2001), Magnini et al. (2002b) and Fukumoto and Suzuki (2011), we also follow corpus based approach for Wordnet Domain Labeling. Key points of difference among these approaches can be summerized as follows, 1. Both Buitelaar and Sacaleanu (2001) and Magnini et al. (2002b) used word frequency to detect domain specificity of a term. They do not consider the label of neighbor terms to determine the label for a term. 2. Fukumoto and Suzuki (2011) have modeled domain labeling</context>
</contexts>
<marker>Lee, Yu, Huang, 2009</marker>
<rawString>Lung-Hao Lee, Yu-Ting Yu, and Chu-Ren Huang. 2009. Chinese wordnet domains: Bootstrapping chinese wordnet with semantic domain labels. In Olivia Kwong, editor, PACLIC, pages 288–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Gabriela Cavagli</author>
</authors>
<title>Integrating subject field codes into wordnet.</title>
<date>2000</date>
<pages>1413--1418</pages>
<contexts>
<context position="1777" citStr="Magnini and Cavagli (2000)" startWordPosition="274" endWordPosition="277">ike word sense disambiguation (Magnini et al., 2002a; Khapra et al., 2010), domain specific taxonomy/ontology extraction (Cimiano and Vlker, 2005; Yanna and Zili, 2009) etc. These applications rely on ‘One sense per discourse’ (Gale et al., 1992) hypothesis to identify domain specific sense of a word. ‘Dividing Wordnet’s lexical and conceptual space into various domain specific subspace can significantly reduce search space and thus help many domain specific applications’ (Xiaojuan and Fellbaum, 2012). With the purpose of categorizing Wordnet senses for different domain specific applications, Magnini and Cavagli (2000) constructed a domain hierarchy of 164 domain labels and annotated Wordnet synsets with one or more label from the hierarchy. The categories were further refined by linking domain labels to subject codes of Dewey Decimal Classification system (Bentivogli et al., 2004). Beginning with Wordnet 2.0, Domain category pointers were introduced to link domain specific synsets across part of speech. However, the manual determination of a set of domain labels and assigning them to Wordnet synsets is a time consuming task. Also, the senses of words evolves over a period of time and accordingly Wordnet sy</context>
<context position="4613" citStr="Magnini and Cavagli, 2000" startWordPosition="729" endWordPosition="732">nsupervised and requires only Part Of Speech tagged corpus. Hence, it can be easily adopted across languages. Our method also does not require any predefined set of domain category labels, however if such labels are available it can be incorporated into system to generate better labeling. The remaining of the paper is organized as follows, section 2 describes related work. Section 3 describes the proposed graph based algorithm for Wordnet domain labeling. Section 4 and 5 discuss the experiments and conclusion. 2 Related Work Two major attempts to categorize Wordnet synsets are Wordnet Domain (Magnini and Cavagli, 2000) and Wordnet Domain Category pointers. In this section we first present a brief overview of these efforts and then describe some efforts to automate the task of domain labeling of Wordnet synsets. We also mention the attempts made for other languages apart from English. 2.1 Wordnet Domain Hierarchy and Domain Category Pointers Domain categorization of Wordnet synset has been an active area of research for more than a decade now. Magnini and Cavagli (2000) have developed Wordnet Domain Hierarchy (WDH) by annotating Wordnet1.6 using 250 Subject Field Codes (SFC). They used semi-automated approac</context>
</contexts>
<marker>Magnini, Cavagli, 2000</marker>
<rawString>Bernardo Magnini and Gabriela Cavagli. 2000. Integrating subject field codes into wordnet. pages 1413–1418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Giovanni Pezzulo</author>
<author>Alfio Gliozzo</author>
</authors>
<title>The role of domain information in word sense disambiguation.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<pages>8--359</pages>
<contexts>
<context position="1202" citStr="Magnini et al., 2002" startWordPosition="189" endWordPosition="192">e conduct our experiments for two domains, health and tourism. We achieve F-Score more than .70 in both domains. This work can be useful for many critical problems like word sense disambiguation, domain specific ontology extraction etc. 1 Introduction Over the years, Wordnet has served as an important lexical resource for many Natural Language Processing (NLP) applications. Picking up a right sense of a word from the fine grained sense repository of Wordnet is at the heart of many NLP problems. Many researchers have used Wordnet for domain specific applications like word sense disambiguation (Magnini et al., 2002a; Khapra et al., 2010), domain specific taxonomy/ontology extraction (Cimiano and Vlker, 2005; Yanna and Zili, 2009) etc. These applications rely on ‘One sense per discourse’ (Gale et al., 1992) hypothesis to identify domain specific sense of a word. ‘Dividing Wordnet’s lexical and conceptual space into various domain specific subspace can significantly reduce search space and thus help many domain specific applications’ (Xiaojuan and Fellbaum, 2012). With the purpose of categorizing Wordnet senses for different domain specific applications, Magnini and Cavagli (2000) constructed a domain hie</context>
<context position="6556" citStr="Magnini et al. (2002" startWordPosition="1040" endWordPosition="1043">Domain and Domain Category’. 2.2 Automated Approaches Considering the growing size of Wordnet and the amount of efforts required to construct domain categories, it is apparent to develop semiautomated or automated methods for domain categorization of Wordnets. One of the earlier efforts in this direction was by Buitelaar and Sacaleanu (2001). They extracted domain specific terms using tf*idf measure and then disambiguated these terms using GermaNet synsets. The disambiguation was performed based on the assumption that the hypernymy and hyponymy terms are more likely to have same domain label. Magnini et al. (2002b) have performed a comparative study of corpus based and ontology based domain annotation. They have used frequency of words in the synonym set as a measure to identify domain of a synset. Gonzalez-Agirre et al. (2012) have proposed a semi-automatic method to align the original Wordnet 1.6 based domains to Wordnet 3.0. They have used domain labels already assigned to some top level synsets and then propagated the domain label across Wordnet hierarchy using UKB algorithm (Agirre and Soroa, 2009). Their approach is based on an assumption that ‘A synset directly related to several synsets labele</context>
<context position="8058" citStr="Magnini et al. (2002" startWordPosition="1281" endWordPosition="1284">e corpus words with Wordnet senses and then use Markov Random Walk based Page Rank Algorithm to rank domain relevance of Wordnet senses. Zhu et al. (2011) have proposed gloss based disambiguation technique for domain assignment to Wordnet synset. They used existing domain labels of Wordnet 3.0 and predicted domains based on words in the gloss of the synsets. There have also been efforts to adopt English Wordnet domain labels for other languages. Lee et al. (2009) have used English-Chinese Wordnet mapping to domain tag Chinese Wordnet. 2.3 Proposed Approach Like Buitelaar and Sacaleanu (2001), Magnini et al. (2002b) and Fukumoto and Suzuki (2011), we also follow corpus based approach for Wordnet Domain Labeling. Key points of difference among these approaches can be summerized as follows, 1. Both Buitelaar and Sacaleanu (2001) and Magnini et al. (2002b) used word frequency to detect domain specificity of a term. They do not consider the label of neighbor terms to determine the label for a term. 2. Fukumoto and Suzuki (2011) have modeled domain labeling as a Markov Random Walk problem, but they run their algorithm on entire Wordnet graph. This is costly in terms of time and space required for the proces</context>
</contexts>
<marker>Magnini, Pezzulo, Gliozzo, 2002</marker>
<rawString>Bernardo Magnini, Giovanni Pezzulo, and Alfio Gliozzo. 2002a. The role of domain information in word sense disambiguation. Natural Language Engineering, 8:359–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Carlo Strapparava</author>
<author>Giovanni Pezzulo</author>
<author>Alfio Gliozzo</author>
</authors>
<title>Comparing ontology-based and corpus-based domain annotations in wordnet.</title>
<date>2002</date>
<booktitle>In First International Global WordNet Conference,</booktitle>
<location>Mysore, India.</location>
<contexts>
<context position="1202" citStr="Magnini et al., 2002" startWordPosition="189" endWordPosition="192">e conduct our experiments for two domains, health and tourism. We achieve F-Score more than .70 in both domains. This work can be useful for many critical problems like word sense disambiguation, domain specific ontology extraction etc. 1 Introduction Over the years, Wordnet has served as an important lexical resource for many Natural Language Processing (NLP) applications. Picking up a right sense of a word from the fine grained sense repository of Wordnet is at the heart of many NLP problems. Many researchers have used Wordnet for domain specific applications like word sense disambiguation (Magnini et al., 2002a; Khapra et al., 2010), domain specific taxonomy/ontology extraction (Cimiano and Vlker, 2005; Yanna and Zili, 2009) etc. These applications rely on ‘One sense per discourse’ (Gale et al., 1992) hypothesis to identify domain specific sense of a word. ‘Dividing Wordnet’s lexical and conceptual space into various domain specific subspace can significantly reduce search space and thus help many domain specific applications’ (Xiaojuan and Fellbaum, 2012). With the purpose of categorizing Wordnet senses for different domain specific applications, Magnini and Cavagli (2000) constructed a domain hie</context>
<context position="6556" citStr="Magnini et al. (2002" startWordPosition="1040" endWordPosition="1043">Domain and Domain Category’. 2.2 Automated Approaches Considering the growing size of Wordnet and the amount of efforts required to construct domain categories, it is apparent to develop semiautomated or automated methods for domain categorization of Wordnets. One of the earlier efforts in this direction was by Buitelaar and Sacaleanu (2001). They extracted domain specific terms using tf*idf measure and then disambiguated these terms using GermaNet synsets. The disambiguation was performed based on the assumption that the hypernymy and hyponymy terms are more likely to have same domain label. Magnini et al. (2002b) have performed a comparative study of corpus based and ontology based domain annotation. They have used frequency of words in the synonym set as a measure to identify domain of a synset. Gonzalez-Agirre et al. (2012) have proposed a semi-automatic method to align the original Wordnet 1.6 based domains to Wordnet 3.0. They have used domain labels already assigned to some top level synsets and then propagated the domain label across Wordnet hierarchy using UKB algorithm (Agirre and Soroa, 2009). Their approach is based on an assumption that ‘A synset directly related to several synsets labele</context>
<context position="8058" citStr="Magnini et al. (2002" startWordPosition="1281" endWordPosition="1284">e corpus words with Wordnet senses and then use Markov Random Walk based Page Rank Algorithm to rank domain relevance of Wordnet senses. Zhu et al. (2011) have proposed gloss based disambiguation technique for domain assignment to Wordnet synset. They used existing domain labels of Wordnet 3.0 and predicted domains based on words in the gloss of the synsets. There have also been efforts to adopt English Wordnet domain labels for other languages. Lee et al. (2009) have used English-Chinese Wordnet mapping to domain tag Chinese Wordnet. 2.3 Proposed Approach Like Buitelaar and Sacaleanu (2001), Magnini et al. (2002b) and Fukumoto and Suzuki (2011), we also follow corpus based approach for Wordnet Domain Labeling. Key points of difference among these approaches can be summerized as follows, 1. Both Buitelaar and Sacaleanu (2001) and Magnini et al. (2002b) used word frequency to detect domain specificity of a term. They do not consider the label of neighbor terms to determine the label for a term. 2. Fukumoto and Suzuki (2011) have modeled domain labeling as a Markov Random Walk problem, but they run their algorithm on entire Wordnet graph. This is costly in terms of time and space required for the proces</context>
</contexts>
<marker>Magnini, Strapparava, Pezzulo, Gliozzo, 2002</marker>
<rawString>Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo, and Alfio Gliozzo. 2002b. Comparing ontology-based and corpus-based domain annotations in wordnet. In First International Global WordNet Conference, Mysore, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark W Schmidt</author>
<author>Karteek Alahari</author>
</authors>
<title>Generalized fast approximate energy minimization via graph cuts: Alpha-expansion beta-shrink moves.</title>
<date>2011</date>
<location>CoRR, abs/1108.5710.</location>
<contexts>
<context position="14379" citStr="Schmidt and Alahari, 2011" startWordPosition="2426" endWordPosition="2430">. In other words, to minimize the cost of assignment θ we cut the edge (t2, t3). Thus the energy minimization problem can be solved by performing ‘Min-Cut’ on graph. For two labels the problem is solvable in polynomial time. However, for more than 2 labels, solving this optimization problem is NP hard (Kolmogorov and Zabih, 2002). In the field of image processing, many problems, e.g. image forground-background detection, image segmentation etc. are formulated as energy minimization in Markov Random Filed. Some of the graph-cut based algorithms to perform the task are, α-expansion, α − β swap (Schmidt and Alahari, 2011) and α swap β shrink algorithm . For our experiment we use α swap β shrink algorithm proposed in Schmidt and Alahari (2011). We are briefly describing the basic idea of the algorithms here. Readers are directed to Kolmogorov and Zabih (2002) and Szeliski et al. (2008) for further details. For more than two labels (domains), a suboptimal solution can be derived by iteratively performing graph cut for a pair of labels. This problem is usually solved using iterative descent technique. As shown in algorithm 1, the algorithm start with an initial assignment. In each iteration the algorithm selects </context>
</contexts>
<marker>Schmidt, Alahari, 2011</marker>
<rawString>Mark W. Schmidt and Karteek Alahari. 2011. Generalized fast approximate energy minimization via graph cuts: Alpha-expansion beta-shrink moves. CoRR, abs/1108.5710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Szeliski</author>
<author>Ramin Zabih</author>
<author>Daniel Scharstein</author>
<author>Olga Veksler</author>
<author>Vladimir Kolmogorov</author>
<author>Aseem Agarwala</author>
<author>Marshall Tappen</author>
<author>Carsten Rother</author>
</authors>
<title>A comparative study of energy minimization methods for markov random fields with smoothnessbased priors.</title>
<date>2008</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>30</volume>
<issue>6</issue>
<contexts>
<context position="14647" citStr="Szeliski et al. (2008)" startWordPosition="2474" endWordPosition="2477">ptimization problem is NP hard (Kolmogorov and Zabih, 2002). In the field of image processing, many problems, e.g. image forground-background detection, image segmentation etc. are formulated as energy minimization in Markov Random Filed. Some of the graph-cut based algorithms to perform the task are, α-expansion, α − β swap (Schmidt and Alahari, 2011) and α swap β shrink algorithm . For our experiment we use α swap β shrink algorithm proposed in Schmidt and Alahari (2011). We are briefly describing the basic idea of the algorithms here. Readers are directed to Kolmogorov and Zabih (2002) and Szeliski et al. (2008) for further details. For more than two labels (domains), a suboptimal solution can be derived by iteratively performing graph cut for a pair of labels. This problem is usually solved using iterative descent technique. As shown in algorithm 1, the algorithm start with an initial assignment. In each iteration the algorithm selects a pair of labels and performs the graph cut. Based on the graph cut the labels will be reassigned to the nodes. The energy function θ is calculated at the end of each iteration and the value of θ is minimized after every iteration to guarantee the convergence. 3.4 Spl</context>
</contexts>
<marker>Szeliski, Zabih, Scharstein, Veksler, Kolmogorov, Agarwala, Tappen, Rother, 2008</marker>
<rawString>Richard Szeliski, Ramin Zabih, Daniel Scharstein, Olga Veksler, Vladimir Kolmogorov, Aseem Agarwala, Marshall Tappen, and Carsten Rother. 2008. A comparative study of energy minimization methods for markov random fields with smoothnessbased priors. IEEE Trans. Pattern Anal. Mach. Intell., 30(6):1068–1080.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ma Xiaojuan</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Rethinking wordnet’s domains.</title>
<date>2012</date>
<booktitle>In Proceedings of 6th International Global WordNet Conference,</booktitle>
<location>Matsue, Japan,</location>
<contexts>
<context position="1657" citStr="Xiaojuan and Fellbaum, 2012" startWordPosition="257" endWordPosition="261">ory of Wordnet is at the heart of many NLP problems. Many researchers have used Wordnet for domain specific applications like word sense disambiguation (Magnini et al., 2002a; Khapra et al., 2010), domain specific taxonomy/ontology extraction (Cimiano and Vlker, 2005; Yanna and Zili, 2009) etc. These applications rely on ‘One sense per discourse’ (Gale et al., 1992) hypothesis to identify domain specific sense of a word. ‘Dividing Wordnet’s lexical and conceptual space into various domain specific subspace can significantly reduce search space and thus help many domain specific applications’ (Xiaojuan and Fellbaum, 2012). With the purpose of categorizing Wordnet senses for different domain specific applications, Magnini and Cavagli (2000) constructed a domain hierarchy of 164 domain labels and annotated Wordnet synsets with one or more label from the hierarchy. The categories were further refined by linking domain labels to subject codes of Dewey Decimal Classification system (Bentivogli et al., 2004). Beginning with Wordnet 2.0, Domain category pointers were introduced to link domain specific synsets across part of speech. However, the manual determination of a set of domain labels and assigning them to Word</context>
<context position="3304" citStr="Xiaojuan and Fellbaum, 2012" startWordPosition="519" endWordPosition="523"> Wordnet synsets. Most of these efforts rely on Wordnet concept hierarchy and use label propagation schemes to propagate domain labels through the hierarchy. However, the heterogeneous level of generality poses a key challenge to such approaches. For example, ‘Under Animal (subsumed by Life Form) we find out specific concepts, such as Work Animal, Domestic Animal, kept together with general classes such as Chordate, Fictional Animal, etc.’(Gangemi et al., 2003). Another key challenge in assigning the domain labels is the quality of domain hierarchy and semantic distance between domain labels (Xiaojuan and Fellbaum, 2012). In this paper, we present a corpus based approach for automatic domain segmentation of Wordnet. The aim of our work is to provide a general solution that can be used across languages to construct domain specific conceptualization from Wordnet. The proposed system works in two steps, 1. We construct domain specific conceptualization from the corpus. 2. The domain specific conceptualization is then disambiguated and linked to Wordnet synsets to generate domain labels. We pose Wordnet domain segmentation as an image labeling problem and use existing techniques in the field of image processing s</context>
<context position="5776" citStr="Xiaojuan and Fellbaum, 2012" startWordPosition="913" endWordPosition="917">g 250 Subject Field Codes (SFC). They used semi-automated approach in which the top level concepts are manually marked with SFC and then the labels are automatically propagated through the hierarchy. Finally, the labeling is again evaluated and refined manually. The semantic structure of WDH was further refined by Bentivogli et al. (2004). Starting from Wordnet 2.0, domain category pointers were introduced in the Wordnet. ‘Unlike the original Wordnet Domain, the domain category pointers use Wordnet synsets as domain labels and synsets across part of speech are linked through domain pointers’ (Xiaojuan and Fellbaum, 2012). However, ‘only 5% of Wordnet 3.0 synsets are linked to 438 domain categories and out of these linked synsets only 30% synsets have same label in both Wordnet Domain and Domain Category’. 2.2 Automated Approaches Considering the growing size of Wordnet and the amount of efforts required to construct domain categories, it is apparent to develop semiautomated or automated methods for domain categorization of Wordnets. One of the earlier efforts in this direction was by Buitelaar and Sacaleanu (2001). They extracted domain specific terms using tf*idf measure and then disambiguated these terms us</context>
</contexts>
<marker>Xiaojuan, Fellbaum, 2012</marker>
<rawString>Ma Xiaojuan and Christiane Fellbaum. 2012. Rethinking wordnet’s domains. In Proceedings of 6th International Global WordNet Conference, Matsue, Japan, jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Yanna</author>
<author>Zhou Zili</author>
</authors>
<title>Domain ontology generation based on wordnet and internet.</title>
<date>2009</date>
<journal>MASS</journal>
<booktitle>In Proceedings of the International Conference on Management and Service Science,</booktitle>
<volume>09</volume>
<pages>1--5</pages>
<contexts>
<context position="1319" citStr="Yanna and Zili, 2009" startWordPosition="206" endWordPosition="209">s work can be useful for many critical problems like word sense disambiguation, domain specific ontology extraction etc. 1 Introduction Over the years, Wordnet has served as an important lexical resource for many Natural Language Processing (NLP) applications. Picking up a right sense of a word from the fine grained sense repository of Wordnet is at the heart of many NLP problems. Many researchers have used Wordnet for domain specific applications like word sense disambiguation (Magnini et al., 2002a; Khapra et al., 2010), domain specific taxonomy/ontology extraction (Cimiano and Vlker, 2005; Yanna and Zili, 2009) etc. These applications rely on ‘One sense per discourse’ (Gale et al., 1992) hypothesis to identify domain specific sense of a word. ‘Dividing Wordnet’s lexical and conceptual space into various domain specific subspace can significantly reduce search space and thus help many domain specific applications’ (Xiaojuan and Fellbaum, 2012). With the purpose of categorizing Wordnet senses for different domain specific applications, Magnini and Cavagli (2000) constructed a domain hierarchy of 164 domain labels and annotated Wordnet synsets with one or more label from the hierarchy. The categories w</context>
</contexts>
<marker>Yanna, Zili, 2009</marker>
<rawString>Wang Yanna and Zhou Zili. 2009. Domain ontology generation based on wordnet and internet. In Proceedings of the International Conference on Management and Service Science, 2009. MASS ’09, pages 1 –5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chaoyong Zhu</author>
<author>Shumin Shi</author>
<author>Haijun Zhang</author>
</authors>
<title>Gloss-based word domain assignment.</title>
<date>2011</date>
<booktitle>In 7th International Conference on Natural Language Processing and Knowledge Engineering (NLP-KE),</booktitle>
<pages>150--155</pages>
<contexts>
<context position="7592" citStr="Zhu et al. (2011)" startWordPosition="1208" endWordPosition="1211">omain label across Wordnet hierarchy using UKB algorithm (Agirre and Soroa, 2009). Their approach is based on an assumption that ‘A synset directly related to several synsets labeled with a particular domain (i.e biology) would itself possibly be also related somehow to that domain (i.e. biology)’(GonzalezAgirre et al., 2012). Fukumoto and Suzuki (2011) have adopted a corpus based approach to assign domain labels to Wordnet synsets. They first disambiguate the corpus words with Wordnet senses and then use Markov Random Walk based Page Rank Algorithm to rank domain relevance of Wordnet senses. Zhu et al. (2011) have proposed gloss based disambiguation technique for domain assignment to Wordnet synset. They used existing domain labels of Wordnet 3.0 and predicted domains based on words in the gloss of the synsets. There have also been efforts to adopt English Wordnet domain labels for other languages. Lee et al. (2009) have used English-Chinese Wordnet mapping to domain tag Chinese Wordnet. 2.3 Proposed Approach Like Buitelaar and Sacaleanu (2001), Magnini et al. (2002b) and Fukumoto and Suzuki (2011), we also follow corpus based approach for Wordnet Domain Labeling. Key points of difference among th</context>
</contexts>
<marker>Zhu, Shi, Zhang, 2011</marker>
<rawString>Chaoyong Zhu, Shumin Shi, and Haijun Zhang. 2011. Gloss-based word domain assignment. In 7th International Conference on Natural Language Processing and Knowledge Engineering (NLP-KE), pages 150–155.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
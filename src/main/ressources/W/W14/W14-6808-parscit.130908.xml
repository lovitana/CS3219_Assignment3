<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000192">
<title confidence="0.9968065">
Problematic Situation Analysis and Automatic Recognition for Chi-
nese Online Conversational System
</title>
<author confidence="0.9962425">
Yang Xiang†, Yaoyun Zhang, Xiaoqiang Zhou‡, Xiaolong Wang,
Yang Qin
</author>
<affiliation confidence="0.993475">
Key Laboratory of Network Oriented Intelligent Computation,
Harbin Institute of Technology Shenzhen Graduate School, China
</affiliation>
<email confidence="0.993296">
†windseedxy@gmail.com ‡xiaoqiang.jeseph@gmail.com
</email>
<sectionHeader confidence="0.993785" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991745">
Automatic problematic situation recogni-
tion (PSR) is important for an online
conversational system to constantly im-
prove its performance. A PSR module is
responsible of automatically identifying
users’ un-satisfactions and then sending
feedbacks to conversation managers. In
this paper, we collect dialogues from a
Chinese online chatbot, annotate the
problematic situations and propose a
framework to predict utterance-level
problematic situations by integrating in-
tent and sentiment factors. Different from
previous work, the research field is set as
open-domain in which very few domain
specific textual features could be used
and the method is easy to be adapted to
other domains. Experimental results
show that integrating both intent and sen-
timent factors gains the best performance.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999910701754386">
Automatic conversational systems are computer
programs that interact with human users based
on their knowledge bases. Developers of conver-
sational systems devote plenty of efforts and
time in collecting and verifying knowledge so as
to maximize the information needs of potential
users. However, problematic situations are inevi-
table due to several reasons (i.e. human verifiers
would make mistakes or omissions, or quality of
some answers couldn’t be judged without certain
contexts). So it is necessary to equip a conversa-
tional system with an automatic PSR module to
keep its performance constantly improved. The
program is responsible of monitoring whether the
dialogue or some utterances are problematic dur-
ing interactions and then providing feedbacks to
the dialogue managers.
Problematic situations reflect that a human us-
er is not satisfied with answers that a conversa-
tional system offers. From one perspective, some
of these un-satisfactions can be captured through
a human user’s dialogue acts. For example, if a
user repeats requesting the same question or fre-
quently changes topics, it is likely that the sys-
tem provides unsatisfactory answers (Chai et al.,
2006). From another perspective, some explicit
manners (i.e. sentiment-related expressions or
dissatisfied feelings) that reflect the change of a
user’s mentality would also indicate a problemat-
ic situation occurs. Some previous systems use
surveys to capture users’ satisfactions: they let
users to vote or evaluate whether the system has
perfectly help them complete certain tasks (Has-
tie et al., 2002; Higashinaka et al., 2010) so as to
collect users’ satisficing scores. However, for a
real-world conversational application, there are
very few users who are willing to provide this
kind of feedbacks.
The dialogue materials for this research come
from a Chinese online chatting robot—BIT,
which is developed for chatting and entertain-
ment. It also integrates real-time data query func-
tions about share price, weather report, post-code
and telephone area code lookup. In addition to
queries about real-time data, the corpus is totally
open-domain and the number of topics that a dia-
logue could be related is unlimited. We annotat-
ed problematic situation labels in the utterance
level (whether a question-answer pair is prob-
lematic/whether an answer is problematic) and
took a deeper analysis towards different cases.
Finally, we introduce the PSR framework. This
framework is simple but efficient: we mapped
the user intent and user sentiment categories to
two groups of representative features and pre-
dicted problematic situations with supervised
learners.
</bodyText>
<page confidence="0.997802">
43
</page>
<note confidence="0.9865325">
Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 43–51,
Wuhan, China, 20-21 October 2014
</note>
<bodyText confidence="0.976354482758621">
Our main contributions stem from the features,
domains and language: Unlike most previous
researchers who considered only user intent
(Chai et al., 2006) or took offline satisfaction
scores provided by users as user sentiment (Has-
tie et al., 2002; Higashinaka et al., 2010), our
method integrates intent and sentiment in an
online manner, which automatically identifies
these two factors and gives the managers real-
time feedbacks. The domain of the dialogue is
open which is different from (Hastie et al., 2002;
Chai et al., 2006). Another contribution is that
this is the first work that solves this issue on the
Chinese language, which has very different lan-
guage specific features and resources from Eng-
lish.
We experimented on the corpus through 10-
fold cross validation. In each individual fold, we
compare our method with two baselines and with
four popular classifiers. Results show that inte-
grating both user intent and user sentiment fac-
tors gains the best performance with an average
F1 of 0.62 (by SVM).
Following, we first introduce related work o
PSR from different perspectives. Introduction to
the corpus are arranged next. The feature selec-
tions and the recognition framework are pro-
posed in Section 4. Experiments, future work and
conclusions constitute the rest.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999291666666667">
Previous researches in this literature differed in
research grains, input features and research do-
mains.
</bodyText>
<subsectionHeader confidence="0.962514">
2.1 Dialog-level vs. Utterance-level
</subsectionHeader>
<bodyText confidence="0.999899289473684">
Most early work focused on the prediction of a
complete dialogue. Hastie et al.(2002) predicted
problematic dialogues from a series of DARPA
Communicator dialogues according to user satis-
faction rates, task completion predictors and
some interaction based features. Walker et al.
(2002) presented their prediction model on the
basis of information the system collected early in
the dialogue and in real time. Oulasvirta et al.
(2006) reported relations between users’ satisfac-
tion rates among the goal-level, concept-level,
task-level and command-level, and captured a
number of qualified user features. Möller et al.
(2008) evaluated performance of different mod-
els including linear regression models and classi-
fication trees on predicting dialog-level user sat-
isfaction in three spoken dialogue datasets.
Although the predictions of progress towards
dialogue completion might be used as a cue to
the dialogue manager, the results couldn’t reflect
in which position a dialogue began to become
problematic. Chai et al. (2006) proposed the def-
inition of user intent and incorporate a few
matching features to predict utterance-level prob-
lematic situations (whether an immediate answer
is satisfactory). Engelbrecht et al. (2009) employ
the Hidden Markov Model (HMM) to model the
whole dialogue into a sequence where each node
of the sequence corresponds to the quality of the
utterance. Higashinaka et al. (2010a; 2010b) also
use HMM to model the good/bad sequence and
testing the effects of turn-wise and overall rat-
ings. Similar spirit also exists in (Hara et al.,
2010). Support Vector Machines (SVM) are used
by Schmitt et al. (2011) for the quality prediction
on the CMU’s Let’s Go Bus Information system
(Raux et al., 2006) and ASR features are com-
pared in their experiments.
</bodyText>
<subsectionHeader confidence="0.941523">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.999954210526316">
There are many factors that could affect the
performance of judging whether a dialogue is
problematic or not, i.e. time attributes like the
total time of a dialogue and the time delays be-
tween utterances (Hastie et al., 2002; Walker et
al., 2002; Möller et al., 2008), dialogue acts that
may reflect user intents (Hastie et al., 2002) and
users’ satisfaction ratings toward the system’s
performance (Hastie et al., 2002). To avoid the
side effects by Automatic Speech Recognition
(ASR) and concentrate on the pure textual fea-
tures in dialogues, several researchers only study
the effect of dialogue acts and users’ satisfaction
ratings (Chai et al., 2006; Higashinaka et al.,
2010). However, it has also proved that users’
satisfaction ratings could not be always relied on
since different groups of users may have differ-
ent predictive powers (i.e. from novices to ex-
perts) (Möller et al., 2005).
</bodyText>
<subsectionHeader confidence="0.997958">
2.3 Research Domains
</subsectionHeader>
<bodyText confidence="0.999522090909091">
Another main difference among previous re-
searches is domain restriction. Specific domains
or tasks simplify the PSR task and features are
easy to be defined by employing domain experts.
However, this restriction limits the ability of fea-
ture adaption from certain domains/tasks to oth-
ers. In a way, domain-specific knowledge and
user surveys are not easy to be adapted. As far as
we know, most previous related work restricted
their researches on specific domains such as
travel plan making (Hastie et al., 2002), restrict-
</bodyText>
<page confidence="0.998427">
44
</page>
<bodyText confidence="0.9993618">
ed scenarios (Chai et al., 2006), bus schedule
information (Schmitt et al., 2011), music infor-
mation (Hara et al., 2010), animal discussion and
attentive listening (Higashinaka et al., 2010a;
Higashinaka et al., 2010b).
</bodyText>
<sectionHeader confidence="0.9742" genericHeader="method">
3 Problematic Situation Analysis
</sectionHeader>
<bodyText confidence="0.99992175">
This section will first introduce the characteris-
tics of the corpus we construct and then provide
definitions and examples for what we have
learned from the dialogues.
</bodyText>
<subsectionHeader confidence="0.999392">
3.1 Corpus Description
</subsectionHeader>
<bodyText confidence="0.984159631578948">
The corpus includes 479 dialogues with totally
3111 QA pairs. The dialogues are extracted from
log files of the BIT robot from May to June,
2013. Each dialogue has a specific session ID,
identifying that the dialogues are collected from
different users or on different time. Chatting (&gt;
2/3), stock real-time inquiries (&lt;1/6) and weather
report inquiries (&lt;1/7) account for the largest
proportion. The dialogues are almost original
which contains a number of curse words (alt-
hough we have removed some too dirty words),
facial expressions (by expressing moods through
several punctuations such as &amp;quot;:)&amp;quot;), boring state-
ments (i.e. I am boring uh) as well as duplicate
questions, indicating the irregular and informal
characteristics of the online chatting contexts.
The language of the corpus is Chinese, with very
few English utterances (&lt;1/100). The length of
dialogues ranges from 1 to 64 QA pairs1.
To collect users’ un-satisfactions, the develop-
ers place good/bad comment buttons for each
answer but it seems to be useless (seldom users
would click the buttons). Through observation,
we found that users’ behaviors, including the
type of both inquiries and responses, provide
important cues to determine whether a user is
satisfied with an answer or not. To show this re-
lation, two examples are listed in Table 1.
We asked two annotators to label whether
each answer is problematic or not. They gave
their judgment according to whether they consid-
ered the answer provided by the chatting system
was reasonable or not, but ignored whether it
was not precise (i.e. a factual answer that is out
of date). When labeling the current pair, they
were asked to refer to the above interactions so
as to take the context restrains into consideration.
By using Cohen’s kappa coefficient, the inter
1 In this paper, we will use the notion &amp;quot;utterance&amp;quot; and &amp;quot;QA
pair&amp;quot; exchangeablely.
agreement of the two annotators is ,
PQ ;z , and which is relatively high.
Most conflicts occur when the two annotators
have different under standings towards a ques-
tion or one of them couldn’t understand what a
question really means. The conflict labels are
resolved by a third annotator. Finally, 832 out of
3111 pairs (26.7%) are labeled as problematic,
indicating that the chatting system still has a lot
of room for improvement.
Type. Question Answer by BIT Explanation
by User
Sentiment-related 你会...? 这也不会,那也不 The second
(Can you 会(I can’t do question which
do...?) anything.) -- is a curse sen-
problematic tence implies
that the user is
你真是个 我是比特
废物 (You (I am BIT.) not satisfied
are really a with the robot’s
good-for- previous an-
nothing.) swer.
Intent-related 你儿子 你要闹哪样啊? Adjacent utter-
呢? (What’re you ances. Repeat
(Where’s going to do?) questions re-
your son?) --Problematic flect the unsat-
isfactory of the
你儿子 天若赐我辉煌,user the last answer.
呢? 我必比天猖狂 Probably due to
(Where’s (If the God fa- irrelevant an-
your son?) vours me, I would swers. On the
be crazier than left, the first
the God.[Chinese answer is ir-
network catch- relavant.
words]) -- Prob-
lematic
</bodyText>
<tableCaption confidence="0.607627">
Table 1. Examples of problematic situations in
BIT.
</tableCaption>
<subsectionHeader confidence="0.999926">
3.2 Corpus Deeper Analysis
</subsectionHeader>
<bodyText confidence="0.999990076923077">
According to observation, the style of sen-
tences raised by users could be roughly divided
into two groups: questions and state-
ments(corresponding to inquiries and responses
in the previous section). Questions are sentences
that send inquiries to the system, indicating that
users have some information needs. Contrarily,
statements are sentences that reflect no infor-
mation needs, but could express complains, ex-
clamations or some other affections. We’ve also
observed that a specific group of features is
much related to questions while another group is
more likely to co-occur with statements.
</bodyText>
<page confidence="0.998538">
45
</page>
<figureCaption confidence="0.932874777777778">
Category Utterance Explanation
Int. switch 1. rPM(China) The current question belongs to a different topic from the
last one. The beginning of a new dialogue (other than
greeting) is classified to switch.
retry 2. rP!PAK;#M M The current question has the same idea as the last one but
(People’s Republic of China) may be expressed in a different style.
continue 3. rPM-&apos;4 The current question belongs to the same topic as the last
(The capital of China) one. The example is a detailed question about the topic
―China‖.
clarify 4. rPM-&apos;417�RfiRT�? Negotiate with the system to refine or coarsen the last
(Which city is the capital of China?) question for a clearer intent.
Sen. greeting V--h-0(Morning)/ Usually a beginning or ending of a dialog. Intimate
3.�J! (Honey!) speeches are also categorized into greeting.
criticize/ �0�H}j!(You are so clever!) / � Criticism or response towards the last answer. Positive or
response iAXfT(You are right) negative criticisms frequently occur in the corpus, indicat-
ing users’ (un)satisfactions.
exclaim/ 0;bNO &amp;quot;1!(It’s so boring!) / ��� Exclaims or statements that the user delivers which are not
statement **. (I love someone.) aiming at the chatbot.
</figureCaption>
<bodyText confidence="0.906913125">
curse Dirty words. Explicit curse words that are inevitable in chatting dia-
logues. They sometimes show unsatisfactory, but some-
times occur dues to that the user has been ridiculed by the
robot.
order i^ci!(Tell me a joke!) Order the system to provide information or do something.
other .../ !!! Utterances other than the above such as punctuations or
symbols that might show speechless(...), exclaiming /
warning(!!!) or some facial expressions.
</bodyText>
<tableCaption confidence="0.948312">
Table 2: Examples and definitions for user intent (Int.) and user sentiment (Sen.).
</tableCaption>
<bodyText confidence="0.999630316666666">
Based on this intuition, we define two con-
cepts as:
User Intent – the action of a user when rais-
ing a question, indicating that the user is execut-
ing an inquiry to the system.
User Sentiment – the sentiment or affection
that a user expresses through his/her utterances,
including negative and non-negative.
The definition of user intent follows (Chai et
al., 2006). It mainly contains four lower-level
types: switch, continue, retry, and clarify. Switch
means to start a new topic or a new dialog. Con-
tinue, retry and clarify are restricted in the same
topic, with different dialogue acts. User senti-
ment is associated with the following cases:
greeting, criticize/response, exclaim/statement,
curse, order and other. Other contains punctua-
tions, facial expressions and special symbols that
are frequently used in Chinese daily chatting.
Examples with explanations for user intent and
sentiment are listed in Table 2.
The annotations towards the lower-level cate-
gories have more conflicts (with an average κ
about 0.5) than the problematic labels. The disa-
greements are solved after declaring some issues:
1) if intent and sentiment characteristics both
occur, label according to the type of the sentence
(question correlates with intent and statement
with sentiment) 2) Criticizes are towards the sys-
tem’s last response while curses are not.
Problematic situations that originate from the
following types are more direct and easier to un-
derstand: a) repeat the last question (retry,
4.95%-45.45%); b) change the topic (switch,
32.27%-32.17% with 6.97% at the beginning); c)
try to clarify what the user intended to ask (clari-
fy, 1.29%-50%) d) negative criticisms towards
the last answer (criticize, 13.79%-15.85%); e)
negative words toward the robot (curse, 6.59%-
11.7%). The percentages 4.95%-45.45% stand
for that retry accounts for 4.95% in all, and
among all the retry cases, 45.45% are problemat-
ic. We also have the polarity (negative or non-
negative) of each user provided utterance anno-
tated and find that nearly all the negative occur
in statements. The rest problematic situations
mostly come from the other type (8.61%-48.13%
with 36.19% facial expressions that the system is
not able to recognize), continue (8.01%-28.4%),
exclaim (10.83%-19.58%) and order (7.23%-
19.55%).
We also notice that in several cases, although
users hadn’t received satisfactory answers, they
didn’t mean to negotiate with the system any
more, indicating that many users are not patient
enough to provide cues. These cases bring about
difficulties for the prediction. Another special
case we notice is from the disagreements of an-
notators, that is, sentiment and intent characteris-
tics could co-occur (i.e. repeated curses). This
</bodyText>
<page confidence="0.996971">
46
</page>
<bodyText confidence="0.9562395">
inspires us to synthesize both user intent and user
sentiment attributes for an utterance.
</bodyText>
<sectionHeader confidence="0.7940335" genericHeader="method">
4 Recognition Framework for Problem-
atic Situations
</sectionHeader>
<bodyText confidence="0.99994875">
Based on a simple dependency analysis for a dia-
logue, we first map user intent and user senti-
ment into related feature groups, and then use the
features to predict problematic situations.
</bodyText>
<subsectionHeader confidence="0.997916">
4.1 Utterance Dependency Analysis
</subsectionHeader>
<bodyText confidence="0.948802875">
A dialogue could be modeled using a directed
graph constituted by the question sequence Q
and the answer sequence A. In the graph, a node
stands for an utterance (question/answer), and
edges are drawn from each Qi-1 to Qi, Qi to Ai, Ai-
1 to Ai and Ai-1 to Qi. The edges stand for depend-
encies or constrains between utterances (Figure
1).
</bodyText>
<figureCaption confidence="0.841925">
Figure 1. Dependencies or constrains in dia-
logues
</figureCaption>
<bodyText confidence="0.81776385">
In this work, the edges from Ai-1 to Qi and from
Qi-1 to Qi are the main dependency types we re-
search. Ai-1 to Qi shows the last answer affects the
current question in a dialog, always reflected by
user sentiment. Constrains between questions are
more related to user intent, i.e. the current ques-
tion would have a high similarity with the last
one if one attempts to retry an inquiry. The fol-
lowing example typically shows the two types of
constrains:
Qi-1: Who did you go with yesterday?
Ai-1: My advantage is that I am handsome.
Qi: Who did you go with yesterday?
Ai: If the God favours me, I would be crazier
than the God.
Qi+1: You are an idiot.
In the example, the retry case from Qi-1 to Qi
implies that Ai-1 should not be a good answer.
The negative curse Qi+1 indicates that Ai may be
problematic.
</bodyText>
<subsectionHeader confidence="0.99721">
4.2 Mapping to problematic situations
</subsectionHeader>
<bodyText confidence="0.999780645161291">
To avoid cascade errors brought about by low-
er-level classifications, we weaken the category
constrains by mapping the taxonomy to related
features. The four types for user intent could be
distinguished by features considering about simi-
larity between sentences, which descends from
retry, clarify to continue and switch. For the six
types in user sentiment, we define word features,
word polarity features and pattern features to
make the types distinguished.
In our proposed framework, the automatic
PSR problem is simplified into a one level binary
classification task in which utterances are mod-
elled with general features, user intent specific
features and sentiment specific features. General
features are textual and non-textual features that
have nothing to do with user intent or user sen-
timent, including: whether the answer is from the
system’s default response list to underdeveloped
knowledge, whether the question is a real-time
inquiry, the number of utterances before and fol-
low (especially to distinguish the beginning or
ending of a dialog), the similarity between the
question and its corresponding answer.
User intent specific features are those extract-
ed from the perspective of user intent, mainly
related to the similarity between two adjacent
questions. User sentiment specific features are
those extracted from the perspective of user sen-
timent, which focus on whether a user-raised ut-
terance contains any sentiment information.
</bodyText>
<subsectionHeader confidence="0.996929">
4.3 Intent Specific Feature Selection
</subsectionHeader>
<bodyText confidence="0.999739388888889">
Specifically, we tag whether the current ques-
tion is retry because retry always corresponds to
a very high similarity which is easy to be identi-
fied and many of them are related to problematic
situations. We also use the similarity between
two questions to distinguish the other types of
intents. Typical features are listed in Table 3(NE
stands for Name Entity).
The semantic similarity measure between
questions (labeled by * in Table 3) is based on a
Chinese semantic web, HowNet (Dong and Dong,
2006). The defined semantic similarity in
HowNet is a normalized real value ([0,1]) of the
shortest path connecting two words in the
HowNet Concept Relation Net. Suppose two
questions P and Q (word sequence size m and n,
respectively), the semantic similarity between
them is defined as:
</bodyText>
<figure confidence="0.9982383">
Qi-1
Qi Qi+1
Ai-1
Ai Ai+1
...
...
Q-A
Q-Q
A-Q
A-A
</figure>
<page confidence="0.998222">
47
</page>
<bodyText confidence="0.9986587">
where P,. and Q; are denoted as:
ssim(Pi,Qm) denotes the semantic similarity of
the ith word in Question P and the mth word in
Question Q. If two words are the same, the simi-
larity is set to 1.
The final similarity is defined as:
nsim(P,Q) is the normalized real value of the
number of words the two questions share. A1 and
A2 are the weighted parameters (set to be 0.5,0.5
in our experiment).
</bodyText>
<table confidence="0.998196764705882">
Feature Description
Exact match After removing punctua-
(Boolean) tions and stop words.
No. of NEs By analyzing results of
LTP.
NE similarity The match No. and con-
tents for NEs.
Ques. Similarity Weighted similarity
based on lexicon and se-
mantics*.
Ques. similarity Weighted similarity
without NEs based on lexicon and se-
Target word mantics*.
The target word in a
question.
Dependency Dependency pattern simi-
similarity larity.
</table>
<tableCaption confidence="0.999758">
Table 3. User intent specific features.
</tableCaption>
<bodyText confidence="0.9999505">
The target words, name entities and dependen-
cy trees are identified or generated by LTP (LTP,
Liu et al., 2011). Target words are defined as the
direct objects that the root verb governs in a de-
pendency parse tree in questioning sentences.
The dependency similarity is computed by count-
ing the number of common dependency relations
(normalized to [0,1]).
</bodyText>
<subsectionHeader confidence="0.999143">
4.4 Sentiment Specific Feature Selection
</subsectionHeader>
<bodyText confidence="0.9996446">
User sentiment is a good reflection of a user’s
current mood. The difficulty lie on that curse
sentences and negative criticisms are not easy to
be distinguished, especially for the Chinese lan-
guage where many sentences have no subjects at
all. A solution is that considering both the similar
key words between the last answer and the cur-
rent statement, and whether a second person pro-
noun (i.e. you/BIT) exists.
This work models the possible relations from
sentiments to problematic situations by defining
a series of sentiment related features. We employ
dictionary-based method (Zhao et al., 2010) to
judge the polarity of words in a sentence. Typical
features are shown in Table 4.
</bodyText>
<table confidence="0.999592466666666">
Feature Example
Key words 弱智(stupid), 次(weak)
Question word/ 为什么(why), 是什么
question mark (what), 是谁(who), ?
Target word 天气(weather), 人名
(person name)
Ending word 好吗(is it ok?), 吗(modal)
Sent. pattern 你好/真/太傻(you’re
quite/very/too stupid)
Part-of-Speech Adjectives, nouns
Polarity Polarity of a word
Person pronoun 你(you), 比特(the name
of the robot)
Dependency Subject-verb-object (SBV
and VOB by LTP)
</table>
<tableCaption confidence="0.999879">
Table 4. User sentiment specific features.
</tableCaption>
<bodyText confidence="0.999910291666667">
Cursing sentences or negative criticisms are
usually expressed in certain patterns which could
be captured through regular expressions after
removing adverbs and modals. Adjective and
noun words are good indicators for sentiment
which could be looked up in sentiment dictionar-
ies. We employ two general Chinese sentiment
dictionaries (NTUSD2 and HowNet) to determine
the polarity of a word (including both nouns and
adjectives for the consideration of both You’re a
fool and You’re foolish.). In addition, we tag the
sentence as negative if it only contains negative
words (key words) after removing useless com-
ponents. Real-time inquiries are special cases
that we should filter out through key words
matching.
There are also something special that we
should consider. Suppose there are three contin-
uous pairs: A-&gt;B-&gt;C: If the question in B con-
tains negative criticism information but A is a
real-time inquiry, we couldn’t directly judge A is
problematic. A typical example is that the answer
is closely related but is not precise (i.e. out of
date). Inquiry includes questions about weather,
</bodyText>
<footnote confidence="0.956412">
2 http://nlg18.csie.ntu.edu.tw:8080/lwku/pub1.html
</footnote>
<page confidence="0.999071">
48
</page>
<bodyText confidence="0.999890166666667">
stock, post code, telephone and identity code in
this system.
In addition to un-satisfactions for not achiev-
ing the desired answer, curse/criticism sentences
could also grow out from some other cases: (1)
the user has been ridiculed by the system thereby
becomes irritated; (2) the user just wants to ex-
press his/her feelings to the system through re-
peated statements. These cases are not directly
related to problematic situations, which, however,
haven’t been well recognized yet, hindering the
improvements of the learners.
</bodyText>
<subsectionHeader confidence="0.996206">
4.5 The Recognition Framework
</subsectionHeader>
<bodyText confidence="0.999584166666667">
We expected that the lower-level category in-
formation could be well modeled through fea-
tures and classifiers. General features, user intent
specific features and user sentiment specific fea-
tures are extracted for each QA pair. Intuitively,
the feature groups for user intent and user senti-
ment have relatively different emphasis and the
hybrid features should naturally increase the sys-
tem’s recall.
Suppose the sequences are Q and A, in which
Qi is to be determined (see Figure 1). The auto-
matic PSR model is described as the follows:
</bodyText>
<listItem confidence="0.99337685">
a) Pre-processing: tokenization, POS tagging,
parsing, removing stop words, and filtering
system specified inquires (weather, stock,
post code, telephone and identity code);
b) Extract sentiment specific features for Qi
based on Qi;
c) Extract intent specific features for Qi based
on Qi-1 and Qi;
d) Tag whether Qi is retry or not, tag whether
Qi is negative or not;
e) Determine problematic of Qi according to
sentiment (retry or not) and intent labels
(negative or not), specific features (Table 3
and 4) and general features (§4.2), as well as
the labels for Qi-1 (retry, negative, and prob-
lematic);
f) Post-processing: For the last QA pair in a
dialog, if a same pair exists before and is la-
beled as problematic, Qi is labeled problem-
atic.
</listItem>
<bodyText confidence="0.999946785714286">
The reason why we also take the labels of Qi-1
into account is based on the fact that the labels of
Qi-1 may help determine the current label. For
example, if the last intent indicates a retry and
the current question indicates a switch (a much
lower similarity with the last one), it is very like-
ly that the user has tried at least twice but hasn’t
received a satisfactory answer. In this case, the
previous retry could also increase the probability
of switch, which is helpful for the final determi-
nation.
Post processing mainly deals with the last ut-
terance in a dialogue which doesn’t have any
followings.
</bodyText>
<sectionHeader confidence="0.988695" genericHeader="method">
5 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.999963882352941">
To prove the effectiveness of our model, we
compare it with two baselines on four classical
classifiers through 10-fold cross-validation.
The baselines include the model with general
features (GF) and intent specified features (ISF),
the model with GF and sentiment specified fea-
tures (SSF). We name our hybrid model that with
hybrid features as GF+ISF+SSF. We report the
detailed performance gains of the GF+ISF+SSF
model compared with the two baselines with in-
tense experiments on the corpus. General fea-
tures (GF) only contains little useful information
towards our task and has very poor performance,
therefore we didn’t set it as a baseline. We test
the model with SVM, Naïve Bayes, Decision
Tree and CRF so as to find out an efficient and
stable learner for the task.
</bodyText>
<table confidence="0.976150722222222">
GF+SSF
Prec. Rec. F1
SVM 92.97 44.44 60.05
J48 85.03 22.94 35.85
NB 95.37 22.53 36.37
CRF 89.20 40.06 55.01
GF+ISF
Prec. Rec. F1
SVM 93.77 43.80 59.57
J48 88.76 21.72 34.67
NB 96.39 23.24 36.42
CRF 88.74 44.89 59.46
GF+ISF+SSF
Prec. Rec. F1
SVM 85.73 49.38 62.19
J48 79.15 24.89 37.75
NB 85.97 29.09 43.35
CRF 91.08 45.02 60.16
</table>
<tableCaption confidence="0.999475">
Table 5. Average performance by cross-
</tableCaption>
<bodyText confidence="0.9312835">
validation.
10-fold cross validations are performed on the
dataset. To specify, the corpus should be divided
in the unit of dialogues rather than utterances for
the sake of integrating sequential features (i.e.
the previous labels). LibSVM (Chang and Lin,
2011), Naïve Bayes and Decision Tree (J48)
were provided by the Weka toolkit (Hall, et al.,
</bodyText>
<page confidence="0.997689">
49
</page>
<table confidence="0.993718363636364">
fold Prec. Rec. F1 Percent. Best Learner im-in im-sen
1 89.21 49.01 63.27 26.69 intent CRF -0.03 +6.1
2 84.57 50.51 63.25 30.24 hybrid SVM +3.16 +1.74
3 92.41 42.07 57.82 29.51 hybrid CRF +0.12 +6.38
4 85.47 47.52 61.08 28.85 hybrid SVM +3.16 +3.16
5 84.82 54.92 66.67 28.41 hybrid SVM +3.61 +3.61
6 95.42 44.17 60.39 25.94 sentiment SVM -0.07 -0.07
7 78.45 55.69 65.14 26.90 hybrid SVM +3.12 +3.72
8 85.33 52.46 64.97 32.50 hybrid SVM +2.93 +3.42
9 83.03 51.31 63.43 26.90 hybrid SVM +5.47 +4.0
10 94.29 39.87 56.05 28.55 sentiment SVM +1.3 -0.2
</table>
<tableCaption confidence="0.951504">
Table 6. Detailed results in 10-fold cross validation.
</tableCaption>
<bodyText confidence="0.988384846153846">
&amp;quot;im-in&amp;quot; and &amp;quot;im-sen&amp;quot; stand for the improvements of the hybrid model than intent and sentiment spe-
cific models. &amp;quot;Percent.&amp;quot; stands for the proportion% of problematic utterances in this fold of data.
2009). CRF is provided by CRF++3, a C++ im-
plementation. Metrics of precision, recall and F1
are used for evaluation.
We list results for the average performance of
cross-validation in Table 5. From the data we
notice, all the four learning models perform well
in precision but a little poor in recall (no matter
for which model). And the case of Naïve Bayes
is especially obvious. According to analysis to-
wards the output, the performance of high preci-
sion and low recall mainly due to the following
reasons: Firstly, we select features empirically
which may generate strong rules: if some condi-
tion is satisfied, some conclusion is drawn. Sec-
ondly, there are still a number of situations that
we couldn’t resolve by training our models. For
example, not all retry result in problematic situa-
tions, and sometimes the users’ intents are hard
to understand. Finally, there are many negative
sentences that are not related to problematic situ-
ations which could confuse the learners.
We also notice that SVM and CRF have much
better results than J48 and Naïve Bayes, imply-
ing the effectiveness of the two classifiers. The
hybrid model outperforms the two baselines
mainly by recall, reflecting the reasonability of
considering both user intent and sentiment.
More evidence for the robustness of the hybrid
features and the learners can be recognized
through a detailed report of the cross validation
(Table 6). From the table we observe two im-
portant things: one is that SVM performs much
more stable than other classifiers, and CRF is not
so good as what we have expected, considering
there are sequential features; the other is that the
hybrid model outperforms other baselines in
most cases, and it also has comparative results in
</bodyText>
<footnote confidence="0.574487">
3 http://crfpp.googlecode.com/svn/trunk/doc/index.html
</footnote>
<bodyText confidence="0.994041125">
other cases (fold 1, 6, and 10).
What we have also noticed is that although
Naïve Bayes doesn’t achieve a better score in F1,
it always performs well in precision (Table 5). Its
characteristics of running fast, easy implemented
and with high precision enable the developers to
integrate the automatic recognizer in the system
and send back precise predictions in real time.
</bodyText>
<sectionHeader confidence="0.999753" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.9999963">
We left two problems for future work. Firstly,
although we have defined lower-level categories
for user sentiment and user intent, we failed to
well identify each of them. More representative
features (maybe word embedding or something
else) should be extracted to clearly identify their
boundaries. Secondly, there is much noise in the
original corpus which may affect the model per-
formance. An automatic sieve should be devel-
oped to deal with the noisy information.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999918909090909">
This paper analyses different problematic situ-
ations under the chatting context for the Chinese
language. Other than previous work, we propose
the problematic situation recognition model from
two perspectives—user sentiment and user intent,
and test the proposed model on a totally open-
domain corpus. Experiments verify that integrat-
ing both the two factors gains the best predicting
result. More representative features and more
efficient approaches will be developed for further
improvement.
</bodyText>
<sectionHeader confidence="0.999362" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.798959333333333">
This work is supported in part by the National
Natural Science Foundation of China (No. 612-
72383 and 61173075). And the foundations of
</reference>
<page confidence="0.960817">
50
</page>
<reference confidence="0.994155474358975">
Shenzhen(JC201005260118A, ZDSY20120613-
125401420, JCYJ20120613151940045, and
JC201005260175A).
Reference
Joyce Y. Chai, Chen Zhang, and Tylor Baldwin.
2006. Towards Conversational QA: Automatic
Identification of Problematic Situations and
User Intent. In Proceedings of COLING/ACL.
C. C. Chang and C. J. Lin. 2011. LIBSVM : A
Library for Support Vector Machines. ACM
Transactions on Intelligent Systems and Tech-
nology, 2(27)1-27.
Zhendong Dong, and Qiang Dong. 2006.
HowNet and the Computation of Meaning.
River Edge, NJ: World Scientific, 25-76.
Klaus-Peter Engelbrech, et al. 2009. Modeling
User Satisfaction with Hidden Markov Model.
In Proceedings of the SIGDIAL 2009 Confer-
ence. Association for Computational Linguis-
tics.
Sunao Hara, Norihide Kitaoka, and Kazuya
Takeda. 2010. Estimation Method of User Sat-
isfaction Using N-gram-based Dialogue Histo-
ry Model for Spoken Dialogue System. LREC.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-
hard Pfahringer, Peter Reutemann, Ian H. Wit-
ten. 2009. The WEKA Data Mining Software:
An Update; SIGKDD Explorations, Volume
11, Issue 1.
Helen Wright Hastie, Rashmi Prasad, Marilyn-
Walker. 2002. What&apos;s the Trouble: Automati-
cally Identifying Problematic Dialogues in
DARPA Communicator Dialogue Systems. In
Proceedings of ACL.
Ryuichiro Higashinaka, et al. 2010. Issues in
Predicting User Satisfaction Transitions in Di-
alogues: Individual Differences, Evaluation
Criteria, and Prediction Models. Spoken Dia-
logue Systems for Ambient Environments. 48-
60.
Ryuichiro Higashinaka, et al. 2010. Modeling
User Satisfaction Transitions in Dialogues
from Overall Ratings. In Proceedings of the
SIGDIAL 2010 Conference.
Ting Liu, Wanxiang Che, Zhenghua Li. 2011.
Language Technology Platform. Journal of
Chinese Information Processing. 25(6): 53-62.
Sebastian Möller, et al. 2005. Quality of Tele-
phone-based Spoken Dialogue Systems.
Sebastian Möller, Klaus-Peter Engelbrecht, and
Robert Schleicher. 2008. Predicting the Quali-
ty and Usability of Spoken Dialogue Services.
Speech Communication 50.8: 730-744.
A.Oulasvirta, S.Möller, S. Engelbrecht, et al.
2006. The Relationship of User Errors to Per-
ceived Usability of a Spoken Dialogue System.
In Proceedings of the 2nd ISCA/DEGA Tuto-
rial and Research Workshop on Perceptual
Quality of Systems, Berlin.
A. Raux, D. Bohus, B. Langner, A. W. Black,
and M. Eskenazi. 2006. Doing Research on a
Deployed Spoken Dialogue System: One Year
of Let’s Go! Experience. In Proceedings of the
International Conference on Speech and Lan-
guage Processing.
Alexander Schmitt, Benjamin Schatz, and Wolf-
gang Minker. 2011. Modeling and Predicting
Quality in Spoken Human Computer Interac-
tion. In Proceedings of the SIGDIAL 2011
Conference.
Marilyn A. Walker, et al. 2002. Automatically
Training a Problematic Dialogue Predictor for
a Spoken Dialogue System. Journal of Artifi-
cial Intelligence Research, Vol.16(1): 293-319.
Yanyan Zhao, Bing Qin, and Ting Liu. 2010.
Sentiment Analysis. Journal of Software,
21(8): 1834-1848.DARPA Communicator Di-
alogue Systems. In Proceedings of ACL.
</reference>
<page confidence="0.999122">
51
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.551880">
<title confidence="0.999488">Situation Analysis and Automatic Recognition for nese Online Conversational System</title>
<author confidence="0.987158">Yaoyun Zhang</author>
<author confidence="0.987158">Xiaoqiang Xiaolong Wang</author>
<author confidence="0.987158">Yang Qin</author>
<affiliation confidence="0.7928425">Key Laboratory of Network Oriented Intelligent Computation, Harbin Institute of Technology Shenzhen Graduate School, China</affiliation>
<abstract confidence="0.998147523809524">Automatic problematic situation recognition (PSR) is important for an online conversational system to constantly improve its performance. A PSR module is responsible of automatically identifying and then sending feedbacks to conversation managers. In this paper, we collect dialogues from a Chinese online chatbot, annotate the problematic situations and propose a framework to predict utterance-level problematic situations by integrating intent and sentiment factors. Different from previous work, the research field is set as open-domain in which very few domain specific textual features could be used and the method is easy to be adapted to other domains. Experimental results show that integrating both intent and sentiment factors gains the best performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This work is supported in part by the National Natural Science Foundation of China (No.</title>
<booktitle>612-72383 and 61173075). And the foundations of Shenzhen(JC201005260118A, ZDSY20120613-125401420, JCYJ20120613151940045, and JC201005260175A).</booktitle>
<marker></marker>
<rawString>This work is supported in part by the National Natural Science Foundation of China (No. 612-72383 and 61173075). And the foundations of Shenzhen(JC201005260118A, ZDSY20120613-125401420, JCYJ20120613151940045, and JC201005260175A).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joyce Y Chai</author>
<author>Chen Zhang</author>
<author>Tylor Baldwin</author>
</authors>
<title>Towards Conversational QA: Automatic Identification of Problematic Situations and User Intent.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL.</booktitle>
<contexts>
<context position="2330" citStr="Chai et al., 2006" startWordPosition="334" endWordPosition="337"> module to keep its performance constantly improved. The program is responsible of monitoring whether the dialogue or some utterances are problematic during interactions and then providing feedbacks to the dialogue managers. Problematic situations reflect that a human user is not satisfied with answers that a conversational system offers. From one perspective, some of these un-satisfactions can be captured through a human user’s dialogue acts. For example, if a user repeats requesting the same question or frequently changes topics, it is likely that the system provides unsatisfactory answers (Chai et al., 2006). From another perspective, some explicit manners (i.e. sentiment-related expressions or dissatisfied feelings) that reflect the change of a user’s mentality would also indicate a problematic situation occurs. Some previous systems use surveys to capture users’ satisfactions: they let users to vote or evaluate whether the system has perfectly help them complete certain tasks (Hastie et al., 2002; Higashinaka et al., 2010) so as to collect users’ satisficing scores. However, for a real-world conversational application, there are very few users who are willing to provide this kind of feedbacks. </context>
<context position="4074" citStr="Chai et al., 2006" startWordPosition="596" endWordPosition="599">ic/whether an answer is problematic) and took a deeper analysis towards different cases. Finally, we introduce the PSR framework. This framework is simple but efficient: we mapped the user intent and user sentiment categories to two groups of representative features and predicted problematic situations with supervised learners. 43 Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 43–51, Wuhan, China, 20-21 October 2014 Our main contributions stem from the features, domains and language: Unlike most previous researchers who considered only user intent (Chai et al., 2006) or took offline satisfaction scores provided by users as user sentiment (Hastie et al., 2002; Higashinaka et al., 2010), our method integrates intent and sentiment in an online manner, which automatically identifies these two factors and gives the managers realtime feedbacks. The domain of the dialogue is open which is different from (Hastie et al., 2002; Chai et al., 2006). Another contribution is that this is the first work that solves this issue on the Chinese language, which has very different language specific features and resources from English. We experimented on the corpus through 10-</context>
<context position="6385" citStr="Chai et al. (2006)" startWordPosition="953" endWordPosition="956">. Oulasvirta et al. (2006) reported relations between users’ satisfaction rates among the goal-level, concept-level, task-level and command-level, and captured a number of qualified user features. Möller et al. (2008) evaluated performance of different models including linear regression models and classification trees on predicting dialog-level user satisfaction in three spoken dialogue datasets. Although the predictions of progress towards dialogue completion might be used as a cue to the dialogue manager, the results couldn’t reflect in which position a dialogue began to become problematic. Chai et al. (2006) proposed the definition of user intent and incorporate a few matching features to predict utterance-level problematic situations (whether an immediate answer is satisfactory). Engelbrecht et al. (2009) employ the Hidden Markov Model (HMM) to model the whole dialogue into a sequence where each node of the sequence corresponds to the quality of the utterance. Higashinaka et al. (2010a; 2010b) also use HMM to model the good/bad sequence and testing the effects of turn-wise and overall ratings. Similar spirit also exists in (Hara et al., 2010). Support Vector Machines (SVM) are used by Schmitt et</context>
<context position="7808" citStr="Chai et al., 2006" startWordPosition="1186" endWordPosition="1189">ect the performance of judging whether a dialogue is problematic or not, i.e. time attributes like the total time of a dialogue and the time delays between utterances (Hastie et al., 2002; Walker et al., 2002; Möller et al., 2008), dialogue acts that may reflect user intents (Hastie et al., 2002) and users’ satisfaction ratings toward the system’s performance (Hastie et al., 2002). To avoid the side effects by Automatic Speech Recognition (ASR) and concentrate on the pure textual features in dialogues, several researchers only study the effect of dialogue acts and users’ satisfaction ratings (Chai et al., 2006; Higashinaka et al., 2010). However, it has also proved that users’ satisfaction ratings could not be always relied on since different groups of users may have different predictive powers (i.e. from novices to experts) (Möller et al., 2005). 2.3 Research Domains Another main difference among previous researches is domain restriction. Specific domains or tasks simplify the PSR task and features are easy to be defined by employing domain experts. However, this restriction limits the ability of feature adaption from certain domains/tasks to others. In a way, domain-specific knowledge and user su</context>
<context position="14844" citStr="Chai et al., 2006" startWordPosition="2322" endWordPosition="2325">ng. other .../ !!! Utterances other than the above such as punctuations or symbols that might show speechless(...), exclaiming / warning(!!!) or some facial expressions. Table 2: Examples and definitions for user intent (Int.) and user sentiment (Sen.). Based on this intuition, we define two concepts as: User Intent – the action of a user when raising a question, indicating that the user is executing an inquiry to the system. User Sentiment – the sentiment or affection that a user expresses through his/her utterances, including negative and non-negative. The definition of user intent follows (Chai et al., 2006). It mainly contains four lower-level types: switch, continue, retry, and clarify. Switch means to start a new topic or a new dialog. Continue, retry and clarify are restricted in the same topic, with different dialogue acts. User sentiment is associated with the following cases: greeting, criticize/response, exclaim/statement, curse, order and other. Other contains punctuations, facial expressions and special symbols that are frequently used in Chinese daily chatting. Examples with explanations for user intent and sentiment are listed in Table 2. The annotations towards the lower-level catego</context>
</contexts>
<marker>Chai, Zhang, Baldwin, 2006</marker>
<rawString>Joyce Y. Chai, Chen Zhang, and Tylor Baldwin. 2006. Towards Conversational QA: Automatic Identification of Problematic Situations and User Intent. In Proceedings of COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C Chang</author>
<author>C J Lin</author>
</authors>
<title>LIBSVM : A Library for Support Vector Machines.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology,</booktitle>
<pages>2--27</pages>
<contexts>
<context position="28536" citStr="Chang and Lin, 2011" startWordPosition="4539" endWordPosition="4542"> Prec. Rec. F1 SVM 92.97 44.44 60.05 J48 85.03 22.94 35.85 NB 95.37 22.53 36.37 CRF 89.20 40.06 55.01 GF+ISF Prec. Rec. F1 SVM 93.77 43.80 59.57 J48 88.76 21.72 34.67 NB 96.39 23.24 36.42 CRF 88.74 44.89 59.46 GF+ISF+SSF Prec. Rec. F1 SVM 85.73 49.38 62.19 J48 79.15 24.89 37.75 NB 85.97 29.09 43.35 CRF 91.08 45.02 60.16 Table 5. Average performance by crossvalidation. 10-fold cross validations are performed on the dataset. To specify, the corpus should be divided in the unit of dialogues rather than utterances for the sake of integrating sequential features (i.e. the previous labels). LibSVM (Chang and Lin, 2011), Naïve Bayes and Decision Tree (J48) were provided by the Weka toolkit (Hall, et al., 49 fold Prec. Rec. F1 Percent. Best Learner im-in im-sen 1 89.21 49.01 63.27 26.69 intent CRF -0.03 +6.1 2 84.57 50.51 63.25 30.24 hybrid SVM +3.16 +1.74 3 92.41 42.07 57.82 29.51 hybrid CRF +0.12 +6.38 4 85.47 47.52 61.08 28.85 hybrid SVM +3.16 +3.16 5 84.82 54.92 66.67 28.41 hybrid SVM +3.61 +3.61 6 95.42 44.17 60.39 25.94 sentiment SVM -0.07 -0.07 7 78.45 55.69 65.14 26.90 hybrid SVM +3.12 +3.72 8 85.33 52.46 64.97 32.50 hybrid SVM +2.93 +3.42 9 83.03 51.31 63.43 26.90 hybrid SVM +5.47 +4.0 10 94.29 39.87</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>C. C. Chang and C. J. Lin. 2011. LIBSVM : A Library for Support Vector Machines. ACM Transactions on Intelligent Systems and Technology, 2(27)1-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhendong Dong</author>
<author>Qiang Dong</author>
</authors>
<title>HowNet and the Computation of Meaning. River Edge, NJ: World Scientific,</title>
<date>2006</date>
<pages>25--76</pages>
<contexts>
<context position="20794" citStr="Dong and Dong, 2006" startWordPosition="3276" endWordPosition="3279"> focus on whether a user-raised utterance contains any sentiment information. 4.3 Intent Specific Feature Selection Specifically, we tag whether the current question is retry because retry always corresponds to a very high similarity which is easy to be identified and many of them are related to problematic situations. We also use the similarity between two questions to distinguish the other types of intents. Typical features are listed in Table 3(NE stands for Name Entity). The semantic similarity measure between questions (labeled by * in Table 3) is based on a Chinese semantic web, HowNet (Dong and Dong, 2006). The defined semantic similarity in HowNet is a normalized real value ([0,1]) of the shortest path connecting two words in the HowNet Concept Relation Net. Suppose two questions P and Q (word sequence size m and n, respectively), the semantic similarity between them is defined as: Qi-1 Qi Qi+1 Ai-1 Ai Ai+1 ... ... Q-A Q-Q A-Q A-A 47 where P,. and Q; are denoted as: ssim(Pi,Qm) denotes the semantic similarity of the ith word in Question P and the mth word in Question Q. If two words are the same, the similarity is set to 1. The final similarity is defined as: nsim(P,Q) is the normalized real v</context>
</contexts>
<marker>Dong, Dong, 2006</marker>
<rawString>Zhendong Dong, and Qiang Dong. 2006. HowNet and the Computation of Meaning. River Edge, NJ: World Scientific, 25-76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus-Peter Engelbrech</author>
</authors>
<title>Modeling User Satisfaction with Hidden Markov Model.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGDIAL</booktitle>
<marker>Engelbrech, 2009</marker>
<rawString>Klaus-Peter Engelbrech, et al. 2009. Modeling User Satisfaction with Hidden Markov Model. In Proceedings of the SIGDIAL 2009 Conference. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunao Hara</author>
<author>Norihide Kitaoka</author>
<author>Kazuya Takeda</author>
</authors>
<title>Estimation Method of User Satisfaction Using N-gram-based Dialogue History Model for Spoken Dialogue System.</title>
<date>2010</date>
<publisher>LREC.</publisher>
<contexts>
<context position="6931" citStr="Hara et al., 2010" startWordPosition="1041" endWordPosition="1044">hich position a dialogue began to become problematic. Chai et al. (2006) proposed the definition of user intent and incorporate a few matching features to predict utterance-level problematic situations (whether an immediate answer is satisfactory). Engelbrecht et al. (2009) employ the Hidden Markov Model (HMM) to model the whole dialogue into a sequence where each node of the sequence corresponds to the quality of the utterance. Higashinaka et al. (2010a; 2010b) also use HMM to model the good/bad sequence and testing the effects of turn-wise and overall ratings. Similar spirit also exists in (Hara et al., 2010). Support Vector Machines (SVM) are used by Schmitt et al. (2011) for the quality prediction on the CMU’s Let’s Go Bus Information system (Raux et al., 2006) and ASR features are compared in their experiments. 2.2 Features There are many factors that could affect the performance of judging whether a dialogue is problematic or not, i.e. time attributes like the total time of a dialogue and the time delays between utterances (Hastie et al., 2002; Walker et al., 2002; Möller et al., 2008), dialogue acts that may reflect user intents (Hastie et al., 2002) and users’ satisfaction ratings toward the</context>
<context position="8717" citStr="Hara et al., 2010" startWordPosition="1334" endWordPosition="1337">ng previous researches is domain restriction. Specific domains or tasks simplify the PSR task and features are easy to be defined by employing domain experts. However, this restriction limits the ability of feature adaption from certain domains/tasks to others. In a way, domain-specific knowledge and user surveys are not easy to be adapted. As far as we know, most previous related work restricted their researches on specific domains such as travel plan making (Hastie et al., 2002), restrict44 ed scenarios (Chai et al., 2006), bus schedule information (Schmitt et al., 2011), music information (Hara et al., 2010), animal discussion and attentive listening (Higashinaka et al., 2010a; Higashinaka et al., 2010b). 3 Problematic Situation Analysis This section will first introduce the characteristics of the corpus we construct and then provide definitions and examples for what we have learned from the dialogues. 3.1 Corpus Description The corpus includes 479 dialogues with totally 3111 QA pairs. The dialogues are extracted from log files of the BIT robot from May to June, 2013. Each dialogue has a specific session ID, identifying that the dialogues are collected from different users or on different time. C</context>
</contexts>
<marker>Hara, Kitaoka, Takeda, 2010</marker>
<rawString>Sunao Hara, Norihide Kitaoka, and Kazuya Takeda. 2010. Estimation Method of User Satisfaction Using N-gram-based Dialogue History Model for Spoken Dialogue System. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update;</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, Ian H. Witten. 2009. The WEKA Data Mining Software: An Update; SIGKDD Explorations, Volume 11, Issue 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Wright Hastie</author>
<author>Rashmi Prasad</author>
<author>MarilynWalker</author>
</authors>
<title>What&apos;s the Trouble: Automatically Identifying Problematic Dialogues</title>
<date>2002</date>
<note>in</note>
<contexts>
<context position="2728" citStr="Hastie et al., 2002" startWordPosition="392" endWordPosition="396"> be captured through a human user’s dialogue acts. For example, if a user repeats requesting the same question or frequently changes topics, it is likely that the system provides unsatisfactory answers (Chai et al., 2006). From another perspective, some explicit manners (i.e. sentiment-related expressions or dissatisfied feelings) that reflect the change of a user’s mentality would also indicate a problematic situation occurs. Some previous systems use surveys to capture users’ satisfactions: they let users to vote or evaluate whether the system has perfectly help them complete certain tasks (Hastie et al., 2002; Higashinaka et al., 2010) so as to collect users’ satisficing scores. However, for a real-world conversational application, there are very few users who are willing to provide this kind of feedbacks. The dialogue materials for this research come from a Chinese online chatting robot—BIT, which is developed for chatting and entertainment. It also integrates real-time data query functions about share price, weather report, post-code and telephone area code lookup. In addition to queries about real-time data, the corpus is totally open-domain and the number of topics that a dialogue could be rel</context>
<context position="4167" citStr="Hastie et al., 2002" startWordPosition="611" endWordPosition="615">ally, we introduce the PSR framework. This framework is simple but efficient: we mapped the user intent and user sentiment categories to two groups of representative features and predicted problematic situations with supervised learners. 43 Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 43–51, Wuhan, China, 20-21 October 2014 Our main contributions stem from the features, domains and language: Unlike most previous researchers who considered only user intent (Chai et al., 2006) or took offline satisfaction scores provided by users as user sentiment (Hastie et al., 2002; Higashinaka et al., 2010), our method integrates intent and sentiment in an online manner, which automatically identifies these two factors and gives the managers realtime feedbacks. The domain of the dialogue is open which is different from (Hastie et al., 2002; Chai et al., 2006). Another contribution is that this is the first work that solves this issue on the Chinese language, which has very different language specific features and resources from English. We experimented on the corpus through 10- fold cross validation. In each individual fold, we compare our method with two baselines and</context>
<context position="7378" citStr="Hastie et al., 2002" startWordPosition="1118" endWordPosition="1121">t al. (2010a; 2010b) also use HMM to model the good/bad sequence and testing the effects of turn-wise and overall ratings. Similar spirit also exists in (Hara et al., 2010). Support Vector Machines (SVM) are used by Schmitt et al. (2011) for the quality prediction on the CMU’s Let’s Go Bus Information system (Raux et al., 2006) and ASR features are compared in their experiments. 2.2 Features There are many factors that could affect the performance of judging whether a dialogue is problematic or not, i.e. time attributes like the total time of a dialogue and the time delays between utterances (Hastie et al., 2002; Walker et al., 2002; Möller et al., 2008), dialogue acts that may reflect user intents (Hastie et al., 2002) and users’ satisfaction ratings toward the system’s performance (Hastie et al., 2002). To avoid the side effects by Automatic Speech Recognition (ASR) and concentrate on the pure textual features in dialogues, several researchers only study the effect of dialogue acts and users’ satisfaction ratings (Chai et al., 2006; Higashinaka et al., 2010). However, it has also proved that users’ satisfaction ratings could not be always relied on since different groups of users may have different</context>
</contexts>
<marker>Hastie, Prasad, MarilynWalker, 2002</marker>
<rawString>Helen Wright Hastie, Rashmi Prasad, MarilynWalker. 2002. What&apos;s the Trouble: Automatically Identifying Problematic Dialogues in</rawString>
</citation>
<citation valid="false">
<title>DARPA Communicator Dialogue Systems.</title>
<booktitle>In Proceedings of ACL.</booktitle>
<marker></marker>
<rawString>DARPA Communicator Dialogue Systems. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryuichiro Higashinaka</author>
</authors>
<title>Issues in Predicting User Satisfaction Transitions in Dialogues: Individual Differences, Evaluation Criteria, and Prediction Models. Spoken Dialogue Systems for Ambient Environments.</title>
<date>2010</date>
<pages>48--60</pages>
<marker>Higashinaka, 2010</marker>
<rawString>Ryuichiro Higashinaka, et al. 2010. Issues in Predicting User Satisfaction Transitions in Dialogues: Individual Differences, Evaluation Criteria, and Prediction Models. Spoken Dialogue Systems for Ambient Environments. 48-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryuichiro Higashinaka</author>
</authors>
<title>Modeling User Satisfaction Transitions in Dialogues from Overall Ratings.</title>
<date>2010</date>
<booktitle>In Proceedings of the SIGDIAL 2010 Conference.</booktitle>
<marker>Higashinaka, 2010</marker>
<rawString>Ryuichiro Higashinaka, et al. 2010. Modeling User Satisfaction Transitions in Dialogues from Overall Ratings. In Proceedings of the SIGDIAL 2010 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting Liu</author>
<author>Wanxiang Che</author>
<author>Zhenghua Li</author>
</authors>
<title>Language Technology Platform.</title>
<date>2011</date>
<journal>Journal of Chinese Information Processing.</journal>
<volume>25</volume>
<issue>6</issue>
<pages>53--62</pages>
<contexts>
<context position="22099" citStr="Liu et al., 2011" startWordPosition="3501" endWordPosition="3504">rs (set to be 0.5,0.5 in our experiment). Feature Description Exact match After removing punctua(Boolean) tions and stop words. No. of NEs By analyzing results of LTP. NE similarity The match No. and contents for NEs. Ques. Similarity Weighted similarity based on lexicon and semantics*. Ques. similarity Weighted similarity without NEs based on lexicon and seTarget word mantics*. The target word in a question. Dependency Dependency pattern simisimilarity larity. Table 3. User intent specific features. The target words, name entities and dependency trees are identified or generated by LTP (LTP, Liu et al., 2011). Target words are defined as the direct objects that the root verb governs in a dependency parse tree in questioning sentences. The dependency similarity is computed by counting the number of common dependency relations (normalized to [0,1]). 4.4 Sentiment Specific Feature Selection User sentiment is a good reflection of a user’s current mood. The difficulty lie on that curse sentences and negative criticisms are not easy to be distinguished, especially for the Chinese language where many sentences have no subjects at all. A solution is that considering both the similar key words between the </context>
</contexts>
<marker>Liu, Che, Li, 2011</marker>
<rawString>Ting Liu, Wanxiang Che, Zhenghua Li. 2011. Language Technology Platform. Journal of Chinese Information Processing. 25(6): 53-62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Möller</author>
</authors>
<title>Quality of Telephone-based Spoken Dialogue Systems.</title>
<date>2005</date>
<marker>Möller, 2005</marker>
<rawString>Sebastian Möller, et al. 2005. Quality of Telephone-based Spoken Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Möller</author>
<author>Klaus-Peter Engelbrecht</author>
<author>Robert Schleicher</author>
</authors>
<title>Predicting the Quality and Usability of Spoken Dialogue Services.</title>
<date>2008</date>
<journal>Speech Communication</journal>
<volume>50</volume>
<pages>730--744</pages>
<contexts>
<context position="5984" citStr="Möller et al. (2008)" startWordPosition="893" endWordPosition="896"> early work focused on the prediction of a complete dialogue. Hastie et al.(2002) predicted problematic dialogues from a series of DARPA Communicator dialogues according to user satisfaction rates, task completion predictors and some interaction based features. Walker et al. (2002) presented their prediction model on the basis of information the system collected early in the dialogue and in real time. Oulasvirta et al. (2006) reported relations between users’ satisfaction rates among the goal-level, concept-level, task-level and command-level, and captured a number of qualified user features. Möller et al. (2008) evaluated performance of different models including linear regression models and classification trees on predicting dialog-level user satisfaction in three spoken dialogue datasets. Although the predictions of progress towards dialogue completion might be used as a cue to the dialogue manager, the results couldn’t reflect in which position a dialogue began to become problematic. Chai et al. (2006) proposed the definition of user intent and incorporate a few matching features to predict utterance-level problematic situations (whether an immediate answer is satisfactory). Engelbrecht et al. (20</context>
<context position="7421" citStr="Möller et al., 2008" startWordPosition="1126" endWordPosition="1129"> the good/bad sequence and testing the effects of turn-wise and overall ratings. Similar spirit also exists in (Hara et al., 2010). Support Vector Machines (SVM) are used by Schmitt et al. (2011) for the quality prediction on the CMU’s Let’s Go Bus Information system (Raux et al., 2006) and ASR features are compared in their experiments. 2.2 Features There are many factors that could affect the performance of judging whether a dialogue is problematic or not, i.e. time attributes like the total time of a dialogue and the time delays between utterances (Hastie et al., 2002; Walker et al., 2002; Möller et al., 2008), dialogue acts that may reflect user intents (Hastie et al., 2002) and users’ satisfaction ratings toward the system’s performance (Hastie et al., 2002). To avoid the side effects by Automatic Speech Recognition (ASR) and concentrate on the pure textual features in dialogues, several researchers only study the effect of dialogue acts and users’ satisfaction ratings (Chai et al., 2006; Higashinaka et al., 2010). However, it has also proved that users’ satisfaction ratings could not be always relied on since different groups of users may have different predictive powers (i.e. from novices to ex</context>
</contexts>
<marker>Möller, Engelbrecht, Schleicher, 2008</marker>
<rawString>Sebastian Möller, Klaus-Peter Engelbrecht, and Robert Schleicher. 2008. Predicting the Quality and Usability of Spoken Dialogue Services. Speech Communication 50.8: 730-744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Möller A Oulasvirta</author>
<author>S Engelbrecht</author>
</authors>
<title>The Relationship of User Errors to Perceived Usability of a Spoken Dialogue System.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2nd ISCA/DEGA Tutorial and Research Workshop on Perceptual Quality of Systems,</booktitle>
<location>Berlin.</location>
<marker>Oulasvirta, Engelbrecht, 2006</marker>
<rawString>A.Oulasvirta, S.Möller, S. Engelbrecht, et al. 2006. The Relationship of User Errors to Perceived Usability of a Spoken Dialogue System. In Proceedings of the 2nd ISCA/DEGA Tutorial and Research Workshop on Perceptual Quality of Systems, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
<author>D Bohus</author>
<author>B Langner</author>
<author>A W Black</author>
<author>M Eskenazi</author>
</authors>
<title>Doing Research on a Deployed Spoken Dialogue System: One Year of Let’s Go! Experience.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Speech and Language Processing.</booktitle>
<contexts>
<context position="7088" citStr="Raux et al., 2006" startWordPosition="1068" endWordPosition="1071">edict utterance-level problematic situations (whether an immediate answer is satisfactory). Engelbrecht et al. (2009) employ the Hidden Markov Model (HMM) to model the whole dialogue into a sequence where each node of the sequence corresponds to the quality of the utterance. Higashinaka et al. (2010a; 2010b) also use HMM to model the good/bad sequence and testing the effects of turn-wise and overall ratings. Similar spirit also exists in (Hara et al., 2010). Support Vector Machines (SVM) are used by Schmitt et al. (2011) for the quality prediction on the CMU’s Let’s Go Bus Information system (Raux et al., 2006) and ASR features are compared in their experiments. 2.2 Features There are many factors that could affect the performance of judging whether a dialogue is problematic or not, i.e. time attributes like the total time of a dialogue and the time delays between utterances (Hastie et al., 2002; Walker et al., 2002; Möller et al., 2008), dialogue acts that may reflect user intents (Hastie et al., 2002) and users’ satisfaction ratings toward the system’s performance (Hastie et al., 2002). To avoid the side effects by Automatic Speech Recognition (ASR) and concentrate on the pure textual features in </context>
</contexts>
<marker>Raux, Bohus, Langner, Black, Eskenazi, 2006</marker>
<rawString>A. Raux, D. Bohus, B. Langner, A. W. Black, and M. Eskenazi. 2006. Doing Research on a Deployed Spoken Dialogue System: One Year of Let’s Go! Experience. In Proceedings of the International Conference on Speech and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Schmitt</author>
<author>Benjamin Schatz</author>
<author>Wolfgang Minker</author>
</authors>
<title>Modeling and Predicting Quality in Spoken Human Computer Interaction.</title>
<date>2011</date>
<booktitle>In Proceedings of the SIGDIAL 2011 Conference.</booktitle>
<contexts>
<context position="6996" citStr="Schmitt et al. (2011)" startWordPosition="1052" endWordPosition="1055">al. (2006) proposed the definition of user intent and incorporate a few matching features to predict utterance-level problematic situations (whether an immediate answer is satisfactory). Engelbrecht et al. (2009) employ the Hidden Markov Model (HMM) to model the whole dialogue into a sequence where each node of the sequence corresponds to the quality of the utterance. Higashinaka et al. (2010a; 2010b) also use HMM to model the good/bad sequence and testing the effects of turn-wise and overall ratings. Similar spirit also exists in (Hara et al., 2010). Support Vector Machines (SVM) are used by Schmitt et al. (2011) for the quality prediction on the CMU’s Let’s Go Bus Information system (Raux et al., 2006) and ASR features are compared in their experiments. 2.2 Features There are many factors that could affect the performance of judging whether a dialogue is problematic or not, i.e. time attributes like the total time of a dialogue and the time delays between utterances (Hastie et al., 2002; Walker et al., 2002; Möller et al., 2008), dialogue acts that may reflect user intents (Hastie et al., 2002) and users’ satisfaction ratings toward the system’s performance (Hastie et al., 2002). To avoid the side ef</context>
<context position="8678" citStr="Schmitt et al., 2011" startWordPosition="1327" endWordPosition="1330">search Domains Another main difference among previous researches is domain restriction. Specific domains or tasks simplify the PSR task and features are easy to be defined by employing domain experts. However, this restriction limits the ability of feature adaption from certain domains/tasks to others. In a way, domain-specific knowledge and user surveys are not easy to be adapted. As far as we know, most previous related work restricted their researches on specific domains such as travel plan making (Hastie et al., 2002), restrict44 ed scenarios (Chai et al., 2006), bus schedule information (Schmitt et al., 2011), music information (Hara et al., 2010), animal discussion and attentive listening (Higashinaka et al., 2010a; Higashinaka et al., 2010b). 3 Problematic Situation Analysis This section will first introduce the characteristics of the corpus we construct and then provide definitions and examples for what we have learned from the dialogues. 3.1 Corpus Description The corpus includes 479 dialogues with totally 3111 QA pairs. The dialogues are extracted from log files of the BIT robot from May to June, 2013. Each dialogue has a specific session ID, identifying that the dialogues are collected from </context>
</contexts>
<marker>Schmitt, Schatz, Minker, 2011</marker>
<rawString>Alexander Schmitt, Benjamin Schatz, and Wolfgang Minker. 2011. Modeling and Predicting Quality in Spoken Human Computer Interaction. In Proceedings of the SIGDIAL 2011 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
</authors>
<title>Automatically Training a Problematic Dialogue Predictor for a Spoken Dialogue System.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>293--319</pages>
<marker>Walker, 2002</marker>
<rawString>Marilyn A. Walker, et al. 2002. Automatically Training a Problematic Dialogue Predictor for a Spoken Dialogue System. Journal of Artificial Intelligence Research, Vol.16(1): 293-319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanyan Zhao</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
</authors>
<title>Sentiment Analysis.</title>
<date>2010</date>
<journal>Journal of Software,</journal>
<booktitle>In Proceedings of ACL.</booktitle>
<volume>21</volume>
<issue>8</issue>
<pages>1834--1848</pages>
<contexts>
<context position="22984" citStr="Zhao et al., 2010" startWordPosition="3642" endWordPosition="3645">Feature Selection User sentiment is a good reflection of a user’s current mood. The difficulty lie on that curse sentences and negative criticisms are not easy to be distinguished, especially for the Chinese language where many sentences have no subjects at all. A solution is that considering both the similar key words between the last answer and the current statement, and whether a second person pronoun (i.e. you/BIT) exists. This work models the possible relations from sentiments to problematic situations by defining a series of sentiment related features. We employ dictionary-based method (Zhao et al., 2010) to judge the polarity of words in a sentence. Typical features are shown in Table 4. Feature Example Key words 弱智(stupid), 次(weak) Question word/ 为什么(why), 是什么 question mark (what), 是谁(who), ? Target word 天气(weather), 人名 (person name) Ending word 好吗(is it ok?), 吗(modal) Sent. pattern 你好/真/太傻(you’re quite/very/too stupid) Part-of-Speech Adjectives, nouns Polarity Polarity of a word Person pronoun 你(you), 比特(the name of the robot) Dependency Subject-verb-object (SBV and VOB by LTP) Table 4. User sentiment specific features. Cursing sentences or negative criticisms are usually expressed in certa</context>
</contexts>
<marker>Zhao, Qin, Liu, 2010</marker>
<rawString>Yanyan Zhao, Bing Qin, and Ting Liu. 2010. Sentiment Analysis. Journal of Software, 21(8): 1834-1848.DARPA Communicator Dialogue Systems. In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
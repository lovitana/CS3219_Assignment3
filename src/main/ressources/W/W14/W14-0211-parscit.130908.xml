<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.134765">
<title confidence="0.996635">
Conversational Strategies for Robustly Managing Dialog in Public Spaces
</title>
<author confidence="0.98602">
Aasish Pappu Ming Sun Seshadri Sridharan Alexander I. Rudnicky
</author>
<affiliation confidence="0.833071666666667">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh PA, USA
</affiliation>
<email confidence="0.999564">
{aasish,mings,seshadrs,air}@cs.cmu.edu
</email>
<sectionHeader confidence="0.994818" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902">
Open environments present an attention
management challenge for conversational
systems. We describe a kiosk system
(based on Ravenclaw–Olympus) that uses
simple auditory and visual information to
interpret human presence and manage the
system’s attention. The system robustly
differentiates intended interactions from
unintended ones at an accuracy of 93%
and provides similar task completion rates
in both a quiet room and a public space.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999768307692308">
Dialog systems designers try to minimize disrup-
tive influences by introducing physical and be-
havioral constraints to create predictable environ-
ments. This includes using a closed-talking mi-
crophone or limiting interaction to one user at a
time. But such constraints are difficult to apply
in public environments such as kiosks (Bohus and
Horvitz, 2010; Foster et al., 2012; Nakashima et
al., 2014), in-car assistants (Kun et al., 2007; Hof-
mann et al., 2013; Misu et al., 2013) or on mo-
bile robots (Haasch et al., 2004; Sabanovic et al.,
2006; Kollar et al., 2012). To implement dialog
systems that operate in public spaces, we have to
relax some of these constraints and deal with addi-
tional challenges. For example, the system needs
to select the correct interlocutor, who may be only
one of several possible ones in the vicinity, then
determine whether they are initiating the process
of engaging with the system.
In this paper we focus on the problems of
identifying a potential interlocutor in the environ-
ment, engaging them in conversation and provid-
ing suitable channel-maintenance cues (Bruce et
al., 2002; Fukuda et al., 2002; Al Moubayed and
Skantze, 2011). We address these problems in the
context of a simple application, a kiosk agent that
</bodyText>
<figureCaption confidence="0.9544785">
Figure 1: Ravenclaw–Olympus augmented with
multimodal input and output functions.
</figureCaption>
<bodyText confidence="0.9996241">
accepts tasks such as taking a message to a named
recipient. To evaluate the effectiveness of our ap-
proach we compared the system’s ability to man-
age conversations in a quiet room and in a public
area.
The remainder of this paper is organized as fol-
lows: we first describe the system architecture,
then present the evaluation setup and the results,
then review related work and finally conclude with
an analysis of the study.
</bodyText>
<sectionHeader confidence="0.930047" genericHeader="introduction">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.980187833333333">
Figure 1 shows the architecture; it incorporates
Ravenclaw/Olympus (Bohus et al., 2007) stan-
dard components (in white), new components (in
black) and modified ones (shaded). In the system
pipeline, the Audio Server receives audio from a
microphone, endpoints it and sends it to the ASR
</bodyText>
<page confidence="0.996211">
63
</page>
<note confidence="0.865158">
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 63–67,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.995855">
Figure 2: Face states; some are animations.
</figureCaption>
<listItem confidence="0.9725288">
engine (PocketSphinx); the decoding is passed to
NLU (Phoenix parser). ICE (Input Confidence Es-
timation) (Helios) assigns confidence scores for
the input concepts. Based on user’s input and
the context, the Dialog Manager (DM) determines
what to do next, perhaps using data from the
Domain Reasoner (DR). An Interaction Manager
(IM) initiates a spoken response using Natural
Language Generation (NLG) and Text-to-Speech
(TTS) component.
</listItem>
<bodyText confidence="0.999439384615385">
Three components were added: (1) Multimodal
Capture acquires audio and human position data
using a Kinect device 1. (2) Awareness deter-
mines whether there is a potential interlocutor in
the vicinity and their current position, using skele-
tal and azimuth information. (3) Talking Head
that conveys the system’s state (as shown in Fig-
ure 2): whether it’s active (conversing and hint-
ing) or idle (asleep and doze) and whether fo-
cused concepts are grounded (conversing and non-
understanding); certain state representations (e.g.,
conversing) are coordinated with the TTS compo-
nent.
</bodyText>
<sectionHeader confidence="0.998091" genericHeader="background">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999110142857143">
A robust system should be able to function as well
in a difficult situation as in a controlled one. We
compare the system’s performance in two environ-
ments, public and quiet, and evaluate the (a) sys-
tem’s awareness of intended users, and its (b) end-
to-end performance.
The same twenty subjects participated in both
</bodyText>
<footnote confidence="0.73933525">
1See http://www.microsoft.com/en-us/
Kinectforwindows/develop/. Three sources are
tapped: the beam-formed audio, the sound source azimuth
and skeleton coordinates. Video data are not used.
</footnote>
<bodyText confidence="0.999581454545455">
experiments: a mix of American, Indian, Chinese
and Hispanic with different fluency levels of En-
glish. None of them had previously interacted with
this system prior to this study.
The subjects were told that they would interact
with a virtual agent displayed on a screen. Their
task for the awareness experiment was to make the
agent aware that they wished to interact. For the
end-to-end system performance, the task was to
instruct the agent to send a message to a named
recipient.
</bodyText>
<subsectionHeader confidence="0.997392">
3.1 Situated Awareness
</subsectionHeader>
<bodyText confidence="0.999936657894737">
We define situated awareness as correctly engag-
ing the intended interlocutor (i.e., verbally ac-
knowledge the user’s presence) under two con-
ditions. When the user is positioned (i) inside
the visual range of the Kinect at LOC-0 in Fig-
ure 3(a); and (ii) outside the visual range of the
Kinect at LOC-1 in Figure 3(a). We used the effec-
tive range of the camera’s documented horizontal
field of view (57°); hereafter referred as its cone-
of-awareness.
We conducted the awareness experiment in a
public space, a lounge at a hub connecting mul-
tiple corridors. The area has tables and seating,
self-serve coffee, a microwave oven, etc. The ex-
periment was conducted during regular hours, be-
tween 10am to 6pm on weekdays. During these
times we observed occupants discussing projects,
preparing food, making coffee, etc. No direct at-
tempt was made to influence their behavior and we
believe that they made no attempt to accommo-
date our activities. Accordingly, the natural sound
level in the room varied in unpredictable ways. To
supplement naturally-occurring sounds, we played
audio of a conversation between two humans, an
extract from the SwitchBoard corpus (Graff et al.,
2001). It was played using a loudspeaker placed at
LOC-2 in Figure 3(a). The locations (0, 1, and 2)
are all 1.5m from the Kinect, which we deemed to
be a comfortable distance for the subjects. LOC-
1 and LOC-2 are 70° to the left and right of the
Kinect, outside its cone.
To detect the presence of an intended user, we
build an awareness model that uses three sensory
streams viz., voice activity, skeleton, and sound
source azimuth. This model relies on the co-
incidence of azimuth angle and the skeleton angle
(along with voice activity) to determine the pres-
ence of an intended user. We compare the pro-
</bodyText>
<page confidence="0.998261">
64
</page>
<figureCaption confidence="0.9968535">
Figure 3: (a) Plan of Public Space (lounge);(b) Plan of Quiet Room (lab). Dark circled markers indicate
locations (LOC-0, LOC-1, LOC-2), discussed in the text.
</figureCaption>
<table confidence="0.998597">
Condition Voice +Skeleton +Azimuth
Outside 28% − 93%
the cone
Inside − 25% 93%
the cone
</table>
<tableCaption confidence="0.999035">
Table 1: Accuracy for the Awareness Detection
</tableCaption>
<bodyText confidence="0.992358701754386">
posed model with two baselines: (1) conventional
voice-activity-detection (VAD): once speech is de-
tected the system responds as if a conversation is
initiated and (2) based on skeleton plus VAD: once
the skeleton appears in front of the Kinect and a
voice is heard, the system engages in conversation.
Table 1 shows the combination of sensory
streams we used under two conditions. For the
outside-the-cone condition, the participants stand
in LOC-1 as shown in Figure 3(a) and follow the
instructions from the agent. Initially, the sub-
ject’s skeleton is invisible to the agent; however
the subject is audible to the agent. Therefore, in
certain combinations of sensors (e.g., voice +
skeleton model and voice + skeleton
+ azimuth model) the system attempts to guide
them to move in front of it, i.e. to LOC-0, an
ideal position for interacting with the system. For
inside-the-cone condition, subjects stand at LOC-
0 where the agent can sense their skeleton.
When user stands at LOC-1 i.e., outside-
the-cone voice + skeleton model and
voice + skeleton + azimuth models
are functionally the same since the source of
distraction has no skeleton in the cone. When
user stands at LOC-0, i.e., inside-the-cone voice
alone is the same as voice + skeleton
model since the agent always sees a skeleton in
front of it. Therefore, this variant was not used.
We treated awareness detection as a binary de-
cision. An utterance is classified either as “in-
tended” or “unintended”. We manually labeled the
utterances whether they were directed at the sys-
tem (“intended”), “unintended” otherwise. Accu-
racy on “intended” speech is reported in the Ta-
ble 1. Within each condition, the order of the ex-
periments with different awareness strategies was
randomized.
We observe that the voice + skeleton +
azimuth model proves to be robust in the pub-
lic space. Its performance is significantly better,
t(38) = 8.1, p ≈ 0.001, compared to the other
baselines in both conditions. This result agrees
with previous research (Haasch et al., 2004; Bohus
and Horvitz, 2009) showing that a fusion of multi-
modal features improves performance over a uni-
modal approach. Our result indicates that a sim-
ple heuristic approach, using minimal visual and
audio features, provides usable attention manage-
ment in open environments. This approach helped
the system handle a complex interaction scenario
such as out-of-cone speech directed to the sys-
tem. If the speaker is out of range but is producing
possibly system-directed utterances, system urges
them to step to the front. We believe it can be ex-
tended to other complex cases by introducing ad-
ditional logic.
</bodyText>
<subsectionHeader confidence="0.999431">
3.2 End-to-End System Performance
</subsectionHeader>
<bodyText confidence="0.99970575">
To investigate the effect of the environment, we
compare the system’s performance in public space
and quiet room. The average noise level in the
quiet room is about 47dB(A) with computers as
</bodyText>
<page confidence="0.998821">
65
</page>
<table confidence="0.9992072">
Metric Public Quiet
Space Room
Success Ratio 15/20 16/20
Avg # Turns 14.2 16.4
Concept Acc 67% 68%
</table>
<tableCaption confidence="0.998445">
Table 2: Public Space vs Quiet Room Performance
</tableCaption>
<bodyText confidence="0.999736074074074">
the primary source of noise. The background
sound level in the public space was 46dB; other
natural sources ranged up to 57dB. The audio dis-
tractor measured 57dB. The same ASR acoustic
models and processing parameters were used in
both environments. The participant stood at LOC-
0 in Figure 3(a) during the public space experi-
ment and Figure 3(b) during the quiet room ex-
periment. In both experiments, LOC-0 is 1.5m
away from the system. We used the voice +
skeleton + azimuth model to discriminate
user speech from distractions in the environment.
We gave each participant a randomized series
of message-sending tasks, e.g. “send a message
to (person) who is in room (number)”. Subjects
had a maximum of 3 minutes to complete; each
task required 7 turns. The number of tasks com-
pleted (over the group) is reported in terms of
task “success-ratio”. Table 2 shows the success-
ratio of the task, the average number of turns
needed to complete the task, and the system’s per-
utterance concept accuracy (Boros et al., 1996).
There were no statistically significant differences
between quiet room and public space, (t(38) &lt;
2, p &gt; 0.5, on any metric). We conclude that
the channel maintenance technique we tested was
equally effective in both environments.
</bodyText>
<sectionHeader confidence="0.999932" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999982184210526">
The problem of deploying social agents in public
spaces has been of enduring interest; (Bohus and
Horvitz, 2010) list engagement as a challenge for
a physically situated agent in open-world interac-
tions. But the problem was noted earlier and solu-
tions were proposed; e.g a “push-to-talk” protocol
to signal the onset of intended user speech (Stent
et al., 1999). (Sharp et al., 1997; Hieronymus et
al., 2006) described the use of attention phrase as
a required prefix to each user input. Although ex-
plicit actions are effective, they need to be learned
by users. This may not be practical for systems in
public areas engaged by casual users.
A more robust approach involves fusing sev-
eral sources of information such as audio, gaze
and pose(Horvitz et al., 2003; Bohus and Horvitz,
2009) (Hosoya et al., 2009; Nakano and Ishii,
2010). Previous works have shown that fusion
of different sensory information can improve at-
tention management. The drawback of such ap-
proaches is in the complexity of the sensor equip-
ment. Our work attempts to create the rele-
vant capabilities using a simple sensing device
and relying on explicitly modeled conversational
strategies. Others are also using the Microsoft
Kinect device for research in dialog. For example,
(Skantze and Al Moubayed, 2012) and (Foster et
al., 2012) presented a multiparty interaction sys-
tems that use Kinect for face tracking and skeleton
tracking combined with speech recognition.
In our current work, we show that situational
awareness can be integrated into an existing dia-
log framework, Ravenclaw–Olympus, that was not
originally designed with this functionality in mind.
The source code of the framework presented in
this work is publicly available for download 1 and
the acoustic models that have been adapted to the
Kinect audio channel 2
</bodyText>
<sectionHeader confidence="0.992717" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999937904761905">
We found that a conventional spoken dialog sys-
tem can be adapted to a public space with mini-
mal modifications to accommodate additional in-
formation sources. Investigating the effectiveness
of different awareness strategies, we found that a
simple heuristic approach that uses a combination
of sensory streams viz., voice, skeleton and az-
imuth, can reliably identify the likely interlocutor.
End-to-end system performance in a public space
is similar to that observed in a quiet room, indi-
cating that, at least under the conditions we cre-
ated, usable performance can be achieved. This
is a useful finding. We believe that on this level,
channel maintenance is a matter of articulating a
model that specifies appropriate behavior in dif-
ferent states defined by a small number of dis-
crete features (presence, absence, coincidence).
We conjecture that such a framework is likely to
be extensible to more complex situations, for ex-
ample ones involving multiple humans in the en-
vironment.
</bodyText>
<footnote confidence="0.998812833333333">
1http://trac.speech.cs.cmu.edu/repos/
olympus/tags/KinectOly2.0/
2http://trac.speech.cs.cmu.edu/repos/
olympus/tags/KinectOly2.0/Resources/
DecoderConfig/AcousticModels/Semi—
Kinect.cd—semi—5000/
</footnote>
<page confidence="0.985302">
66
</page>
<sectionHeader confidence="0.835138" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991921026315789">
[Al Moubayed and Skantze2011] S. Al Moubayed and
G. Skantze. 2011. Turn-taking control using gaze in
multiparty human-computer dialogue: Effects of 2d and
3d displays. In Proceedings of AVSP, Florence, Italy,
pages 99–102.
[Bohus and Horvitz2009] D. Bohus and E. Horvitz. 2009.
Dialog in the open world: platform and applications. In
Proceedings of the 2009 international conference on Mul-
timodal interfaces, pages 31–38. ACM.
[Bohus and Horvitz2010] D. Bohus and E. Horvitz. 2010. On
the challenges and opportunities of physically situated dia-
log. In 2010 AAAI Fall Symposium on Dialog with Robots.
AAAI.
[Bohus et al.2007] D. Bohus, A. Raux, T.K. Harris, M. Eske-
nazi, and A.I. Rudnicky. 2007. Olympus: an open-source
framework for conversational spoken language interface
research. In Proceedings of the workshop on bridging the
gap: Academic and industrial research in dialog technolo-
gies, pages 32–39. Association for Computational Lin-
guistics.
[Boros et al.1996] M. Boros, W. Eckert, F. Gallwitz, G. Gorz,
G. Hanrieder, and H. Niemann. 1996. Towards under-
standing spontaneous speech: Word accuracy vs. concept
accuracy. In Spoken Language, 1996. ICSLP 96. Pro-
ceedings., Fourth International Conference on, volume 2,
pages 1009–1012. IEEE.
[Bruce et al.2002] A. Bruce, I. Nourbakhsh, and R. Simmons.
2002. The role of expressiveness and attention in human-
robot interaction. In Proceedings of 2002 IEEE Interna-
tional Conference on Robotics and Automation, volume 4,
pages 4138–4142. IEEE.
[Foster et al.2012] M.E. Foster, A. Gaschler, M. Giuliani,
A. Isard, M. Pateraki, and R.P.A. Petrick. 2012. “two
people walk into a bar”: Dynamic multi-party social inter-
action with a robot agent. In Proc. of the 14th ACM Inter-
national Conference on Multimodal Interaction ICMI.
[Fukuda et al.2002] T. Fukuda, J. Taguri, F. Arai,
M. Nakashima, D. Tachibana, and Y. Hasegawa.
2002. Facial expression of robot face for human-robot
mutual communication. In Proceedings of 2002 IEEE
International Conference on Robotics and Automation,
volume 1, pages 46–51. IEEE.
[Graff et al.2001] D. Graff, K. Walker, and D. Miller. 2001.
Switchboard cellular part 1 transcribed audio. In Linguis-
tic Data Consortium, Philadelphia.
[Haasch et al.2004] A. Haasch, S. Hohenner, S. H¨uwel,
M. Kleinehagenbrock, S. Lang, I. Toptsis, GA Fink,
J. Fritsch, B. Wrede, and G. Sagerer. 2004. Biron–the
bielefeld robot companion. In Proc. Int. Workshop on Ad-
vances in Service Robotics, pages 27–32. Stuttgart, Ger-
many: Fraunhofer IRB Verlag.
[Hieronymus et al.2006] J. Hieronymus, G. Aist, and
J. Dowding. 2006. Open microphone speech under-
standing: correct discrimination of in domain speech. In
Proceedings of 2006 IEEE international conference on
acoustics, speech, and signal processing, volume 1. IEEE.
[Hofmann et al.2013] H. Hofmann, U. Ehrlich, A. Berton,
A. Mahr, R. Math, and C. M¨uller. 2013. Evaluation of
speech dialog strategies for internet applications in the car.
In Proceedings of the SIGDIAL 2013 Conference, pages
233–241, Metz, France, August. Association for Compu-
tational Linguistics.
[Horvitz et al.2003] E. Horvitz, C. Kadie, T. Paek, and
D. Hovel. 2003. Models of attention in computing and
communication: from principles to application. In Com-
munications of the ACM, volume 46, pages 52–59.
[Hosoya et al.2009] K. Hosoya, T. Ogawa, and T. Kobayashi.
2009. Robot auditory system using head-mounted square
microphone array. In Intelligent Robots and Systems,
2009. IROS 2009. IEEE/RSJ International Conference on,
pages 2736–2741. IEEE.
[Kollar et al.2012] T. Kollar, A. Vedantham, C. Sobel,
C. Chang, V. Perera, and M. Veloso. 2012. A multi-modal
approach for natural human-robot interaction. In Proceed-
ings of 2012 International Conference on Social Robots.
[Kun et al.2007] A. Kun, T. Paek, and Z. Medenica. 2007.
The effect of speech interface accuracy on driving perfor-
mance. In INTERSPEECH, pages 1326–1329.
[Misu et al.2013] T. Misu, A. Raux, I. Lane, J. Devassy, and
R. Gupta. 2013. Situated multi-modal dialog system in
vehicles. In Proceedings of the 6th Workshop on Eye Gaze
in Intelligent Human Machine Interaction: Gaze in Multi-
modal Interaction, pages 25–28. ACM.
[Nakano and Ishii2010] Y. Nakano and R. Ishii. 2010. Es-
timating user’s engagement from eye-gaze behaviors in
human-agent conversations. In Proceedings of the 15th
international conference on Intelligent user interfaces,
pages 139–148. ACM.
[Nakashima et al.2014] Taichi Nakashima, Kazunori Ko-
matani, and Satoshi Sato. 2014. Integration of multiple
sound source localization results for speaker identifica-
tion in multiparty dialogue system. In Natural Interaction
with Robots, Knowbots and Smartphones, pages 153–165.
Springer New York.
[Sabanovic et al.2006] S. Sabanovic, M.P. Michalowski, and
R. Simmons. 2006. Robots in the wild: Observing
human-robot social interaction outside the lab. In Ad-
vanced Motion Control, 2006. 9th IEEE International
Workshop on, pages 596–601. IEEE.
[Sharp et al.1997] R.D. Sharp, E. Bocchieri, C. Castillo,
S. Parthasarathy, C. Rath, M. Riley, and J. Rowland. 1997.
The watson speech recognition engine. In Proceedings of
1997 IEEE international conference on acoustics, speech,
and signal processing, volume 5, pages 4065–4068. IEEE.
[Skantze and Al Moubayed2012] G. Skantze and
S. Al Moubayed. 2012. Iristk: a statechart-based
toolkit for multi-party face-to-face interaction. In Proc.
of the 14th ACM International Conference on Multimodal
Interaction ICMI.
[Stent et al.1999] A. Stent, J. Dowding, J. Gawron, E. Bratt,
and R. Moore. 1999. The commandtalk spoken dialogue
system. In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Computa-
tional Linguistics, pages 183–190. ACL.
</reference>
<page confidence="0.999506">
67
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.453322">
<title confidence="0.999584">Conversational Strategies for Robustly Managing Dialog in Public Spaces</title>
<author confidence="0.994867">Aasish Pappu Ming Sun Seshadri Sridharan Alexander I Rudnicky</author>
<affiliation confidence="0.794344">Language Technologies Carnegie Mellon Pittsburgh PA,</affiliation>
<abstract confidence="0.999627083333333">Open environments present an attention management challenge for conversational systems. We describe a kiosk system (based on Ravenclaw–Olympus) that uses simple auditory and visual information to interpret human presence and manage the system’s attention. The system robustly differentiates intended interactions from unintended ones at an accuracy of 93% and provides similar task completion rates in both a quiet room and a public space.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Al Moubayed</author>
<author>G Skantze</author>
</authors>
<title>Turn-taking control using gaze in multiparty human-computer dialogue: Effects of 2d and 3d displays.</title>
<date>2011</date>
<booktitle>In Proceedings of AVSP,</booktitle>
<pages>99--102</pages>
<location>Florence, Italy,</location>
<marker>[Al Moubayed and Skantze2011]</marker>
<rawString>S. Al Moubayed and G. Skantze. 2011. Turn-taking control using gaze in multiparty human-computer dialogue: Effects of 2d and 3d displays. In Proceedings of AVSP, Florence, Italy, pages 99–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>E Horvitz</author>
</authors>
<title>Dialog in the open world: platform and applications.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 international conference on Multimodal interfaces,</booktitle>
<pages>31--38</pages>
<publisher>ACM.</publisher>
<marker>[Bohus and Horvitz2009]</marker>
<rawString>D. Bohus and E. Horvitz. 2009. Dialog in the open world: platform and applications. In Proceedings of the 2009 international conference on Multimodal interfaces, pages 31–38. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>E Horvitz</author>
</authors>
<title>On the challenges and opportunities of physically situated dialog.</title>
<date>2010</date>
<booktitle>In 2010 AAAI Fall Symposium on Dialog with Robots.</booktitle>
<publisher>AAAI.</publisher>
<marker>[Bohus and Horvitz2010]</marker>
<rawString>D. Bohus and E. Horvitz. 2010. On the challenges and opportunities of physically situated dialog. In 2010 AAAI Fall Symposium on Dialog with Robots. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>A Raux</author>
<author>T K Harris</author>
<author>M Eskenazi</author>
<author>A I Rudnicky</author>
</authors>
<title>Olympus: an open-source framework for conversational spoken language interface research.</title>
<date>2007</date>
<booktitle>In Proceedings of the workshop</booktitle>
<pages>32--39</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>[Bohus et al.2007]</marker>
<rawString>D. Bohus, A. Raux, T.K. Harris, M. Eskenazi, and A.I. Rudnicky. 2007. Olympus: an open-source framework for conversational spoken language interface research. In Proceedings of the workshop on bridging the gap: Academic and industrial research in dialog technologies, pages 32–39. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Boros</author>
<author>W Eckert</author>
<author>F Gallwitz</author>
<author>G Gorz</author>
<author>G Hanrieder</author>
<author>H Niemann</author>
</authors>
<title>Towards understanding spontaneous speech: Word accuracy vs. concept accuracy.</title>
<date>1996</date>
<booktitle>In Spoken Language,</booktitle>
<volume>2</volume>
<pages>1009--1012</pages>
<publisher>IEEE.</publisher>
<marker>[Boros et al.1996]</marker>
<rawString>M. Boros, W. Eckert, F. Gallwitz, G. Gorz, G. Hanrieder, and H. Niemann. 1996. Towards understanding spontaneous speech: Word accuracy vs. concept accuracy. In Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on, volume 2, pages 1009–1012. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bruce</author>
<author>I Nourbakhsh</author>
<author>R Simmons</author>
</authors>
<title>The role of expressiveness and attention in humanrobot interaction.</title>
<date>2002</date>
<booktitle>In Proceedings of 2002 IEEE International Conference on Robotics and Automation,</booktitle>
<volume>4</volume>
<pages>4138--4142</pages>
<publisher>IEEE.</publisher>
<marker>[Bruce et al.2002]</marker>
<rawString>A. Bruce, I. Nourbakhsh, and R. Simmons. 2002. The role of expressiveness and attention in humanrobot interaction. In Proceedings of 2002 IEEE International Conference on Robotics and Automation, volume 4, pages 4138–4142. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
<author>A Gaschler</author>
<author>M Giuliani</author>
<author>A Isard</author>
<author>M Pateraki</author>
<author>R P A Petrick</author>
</authors>
<title>two people walk into a bar”: Dynamic multi-party social interaction with a robot agent.</title>
<date>2012</date>
<booktitle>In Proc. of the 14th ACM International Conference on Multimodal Interaction ICMI.</booktitle>
<marker>[Foster et al.2012]</marker>
<rawString>M.E. Foster, A. Gaschler, M. Giuliani, A. Isard, M. Pateraki, and R.P.A. Petrick. 2012. “two people walk into a bar”: Dynamic multi-party social interaction with a robot agent. In Proc. of the 14th ACM International Conference on Multimodal Interaction ICMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fukuda</author>
<author>J Taguri</author>
<author>F Arai</author>
<author>M Nakashima</author>
<author>D Tachibana</author>
<author>Y Hasegawa</author>
</authors>
<title>Facial expression of robot face for human-robot mutual communication.</title>
<date>2002</date>
<booktitle>In Proceedings of 2002 IEEE International Conference on Robotics and Automation,</booktitle>
<volume>1</volume>
<pages>46--51</pages>
<publisher>IEEE.</publisher>
<marker>[Fukuda et al.2002]</marker>
<rawString>T. Fukuda, J. Taguri, F. Arai, M. Nakashima, D. Tachibana, and Y. Hasegawa. 2002. Facial expression of robot face for human-robot mutual communication. In Proceedings of 2002 IEEE International Conference on Robotics and Automation, volume 1, pages 46–51. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
<author>K Walker</author>
<author>D Miller</author>
</authors>
<title>Switchboard cellular part 1 transcribed audio.</title>
<date>2001</date>
<booktitle>In Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<marker>[Graff et al.2001]</marker>
<rawString>D. Graff, K. Walker, and D. Miller. 2001. Switchboard cellular part 1 transcribed audio. In Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haasch</author>
<author>S Hohenner</author>
<author>S H¨uwel</author>
<author>M Kleinehagenbrock</author>
<author>S Lang</author>
<author>I Toptsis</author>
<author>GA Fink</author>
<author>J Fritsch</author>
<author>B Wrede</author>
<author>G Sagerer</author>
</authors>
<title>Biron–the bielefeld robot companion.</title>
<date>2004</date>
<booktitle>In Proc. Int. Workshop on Advances in Service Robotics,</booktitle>
<pages>27--32</pages>
<publisher>Fraunhofer IRB Verlag.</publisher>
<location>Stuttgart, Germany:</location>
<marker>[Haasch et al.2004]</marker>
<rawString>A. Haasch, S. Hohenner, S. H¨uwel, M. Kleinehagenbrock, S. Lang, I. Toptsis, GA Fink, J. Fritsch, B. Wrede, and G. Sagerer. 2004. Biron–the bielefeld robot companion. In Proc. Int. Workshop on Advances in Service Robotics, pages 27–32. Stuttgart, Germany: Fraunhofer IRB Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hieronymus</author>
<author>G Aist</author>
<author>J Dowding</author>
</authors>
<title>Open microphone speech understanding: correct discrimination of in domain speech.</title>
<date>2006</date>
<booktitle>In Proceedings of 2006 IEEE international conference on acoustics, speech, and signal processing,</booktitle>
<volume>1</volume>
<publisher>IEEE.</publisher>
<marker>[Hieronymus et al.2006]</marker>
<rawString>J. Hieronymus, G. Aist, and J. Dowding. 2006. Open microphone speech understanding: correct discrimination of in domain speech. In Proceedings of 2006 IEEE international conference on acoustics, speech, and signal processing, volume 1. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hofmann</author>
<author>U Ehrlich</author>
<author>A Berton</author>
<author>A Mahr</author>
<author>R Math</author>
<author>C M¨uller</author>
</authors>
<title>Evaluation of speech dialog strategies for internet applications in the car.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>233--241</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Metz, France,</location>
<marker>[Hofmann et al.2013]</marker>
<rawString>H. Hofmann, U. Ehrlich, A. Berton, A. Mahr, R. Math, and C. M¨uller. 2013. Evaluation of speech dialog strategies for internet applications in the car. In Proceedings of the SIGDIAL 2013 Conference, pages 233–241, Metz, France, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Horvitz</author>
<author>C Kadie</author>
<author>T Paek</author>
<author>D Hovel</author>
</authors>
<title>Models of attention in computing and communication: from principles to application.</title>
<date>2003</date>
<journal>In Communications of the ACM,</journal>
<volume>46</volume>
<pages>52--59</pages>
<marker>[Horvitz et al.2003]</marker>
<rawString>E. Horvitz, C. Kadie, T. Paek, and D. Hovel. 2003. Models of attention in computing and communication: from principles to application. In Communications of the ACM, volume 46, pages 52–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hosoya</author>
<author>T Ogawa</author>
<author>T Kobayashi</author>
</authors>
<title>Robot auditory system using head-mounted square microphone array.</title>
<date>2009</date>
<booktitle>In Intelligent Robots and Systems,</booktitle>
<pages>2736--2741</pages>
<publisher>IEEE.</publisher>
<marker>[Hosoya et al.2009]</marker>
<rawString>K. Hosoya, T. Ogawa, and T. Kobayashi. 2009. Robot auditory system using head-mounted square microphone array. In Intelligent Robots and Systems, 2009. IROS 2009. IEEE/RSJ International Conference on, pages 2736–2741. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kollar</author>
<author>A Vedantham</author>
<author>C Sobel</author>
<author>C Chang</author>
<author>V Perera</author>
<author>M Veloso</author>
</authors>
<title>A multi-modal approach for natural human-robot interaction.</title>
<date>2012</date>
<booktitle>In Proceedings of 2012 International Conference on Social Robots.</booktitle>
<marker>[Kollar et al.2012]</marker>
<rawString>T. Kollar, A. Vedantham, C. Sobel, C. Chang, V. Perera, and M. Veloso. 2012. A multi-modal approach for natural human-robot interaction. In Proceedings of 2012 International Conference on Social Robots.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kun</author>
<author>T Paek</author>
<author>Z Medenica</author>
</authors>
<title>The effect of speech interface accuracy on driving performance.</title>
<date>2007</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1326--1329</pages>
<marker>[Kun et al.2007]</marker>
<rawString>A. Kun, T. Paek, and Z. Medenica. 2007. The effect of speech interface accuracy on driving performance. In INTERSPEECH, pages 1326–1329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Misu</author>
<author>A Raux</author>
<author>I Lane</author>
<author>J Devassy</author>
<author>R Gupta</author>
</authors>
<title>Situated multi-modal dialog system in vehicles.</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th Workshop on Eye Gaze in Intelligent Human Machine Interaction: Gaze in Multimodal Interaction,</booktitle>
<pages>25--28</pages>
<publisher>ACM.</publisher>
<marker>[Misu et al.2013]</marker>
<rawString>T. Misu, A. Raux, I. Lane, J. Devassy, and R. Gupta. 2013. Situated multi-modal dialog system in vehicles. In Proceedings of the 6th Workshop on Eye Gaze in Intelligent Human Machine Interaction: Gaze in Multimodal Interaction, pages 25–28. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Nakano</author>
<author>R Ishii</author>
</authors>
<title>Estimating user’s engagement from eye-gaze behaviors in human-agent conversations.</title>
<date>2010</date>
<booktitle>In Proceedings of the 15th international conference on Intelligent user interfaces,</booktitle>
<pages>139--148</pages>
<publisher>ACM.</publisher>
<marker>[Nakano and Ishii2010]</marker>
<rawString>Y. Nakano and R. Ishii. 2010. Estimating user’s engagement from eye-gaze behaviors in human-agent conversations. In Proceedings of the 15th international conference on Intelligent user interfaces, pages 139–148. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taichi Nakashima</author>
<author>Kazunori Komatani</author>
<author>Satoshi Sato</author>
</authors>
<title>Integration of multiple sound source localization results for speaker identification in multiparty dialogue system.</title>
<date>2014</date>
<booktitle>In Natural Interaction with Robots, Knowbots and Smartphones,</booktitle>
<pages>153--165</pages>
<publisher>Springer</publisher>
<location>New York.</location>
<marker>[Nakashima et al.2014]</marker>
<rawString>Taichi Nakashima, Kazunori Komatani, and Satoshi Sato. 2014. Integration of multiple sound source localization results for speaker identification in multiparty dialogue system. In Natural Interaction with Robots, Knowbots and Smartphones, pages 153–165. Springer New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sabanovic</author>
<author>M P Michalowski</author>
<author>R Simmons</author>
</authors>
<title>Robots in the wild: Observing human-robot social interaction outside the lab.</title>
<date>2006</date>
<booktitle>In Advanced Motion Control,</booktitle>
<pages>596--601</pages>
<publisher>IEEE.</publisher>
<marker>[Sabanovic et al.2006]</marker>
<rawString>S. Sabanovic, M.P. Michalowski, and R. Simmons. 2006. Robots in the wild: Observing human-robot social interaction outside the lab. In Advanced Motion Control, 2006. 9th IEEE International Workshop on, pages 596–601. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R D Sharp</author>
<author>E Bocchieri</author>
<author>C Castillo</author>
<author>S Parthasarathy</author>
<author>C Rath</author>
<author>M Riley</author>
<author>J Rowland</author>
</authors>
<title>The watson speech recognition engine.</title>
<date>1997</date>
<booktitle>In Proceedings of 1997 IEEE international conference on acoustics, speech, and signal processing,</booktitle>
<volume>5</volume>
<pages>4065--4068</pages>
<publisher>IEEE.</publisher>
<marker>[Sharp et al.1997]</marker>
<rawString>R.D. Sharp, E. Bocchieri, C. Castillo, S. Parthasarathy, C. Rath, M. Riley, and J. Rowland. 1997. The watson speech recognition engine. In Proceedings of 1997 IEEE international conference on acoustics, speech, and signal processing, volume 5, pages 4065–4068. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Skantze</author>
<author>S Al Moubayed</author>
</authors>
<title>Iristk: a statechart-based toolkit for multi-party face-to-face interaction.</title>
<date>2012</date>
<booktitle>In Proc. of the 14th ACM International Conference on Multimodal Interaction ICMI.</booktitle>
<marker>[Skantze and Al Moubayed2012]</marker>
<rawString>G. Skantze and S. Al Moubayed. 2012. Iristk: a statechart-based toolkit for multi-party face-to-face interaction. In Proc. of the 14th ACM International Conference on Multimodal Interaction ICMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stent</author>
<author>J Dowding</author>
<author>J Gawron</author>
<author>E Bratt</author>
<author>R Moore</author>
</authors>
<title>The commandtalk spoken dialogue system.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>183--190</pages>
<publisher>ACL.</publisher>
<marker>[Stent et al.1999]</marker>
<rawString>A. Stent, J. Dowding, J. Gawron, E. Bratt, and R. Moore. 1999. The commandtalk spoken dialogue system. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 183–190. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
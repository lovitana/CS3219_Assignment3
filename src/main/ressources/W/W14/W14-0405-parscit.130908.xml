<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000076">
<title confidence="0.904685">
{bs,hr,sr}WaC – Web corpora of Bosnian, Croatian and Serbian
</title>
<author confidence="0.99162">
Nikola Ljubeˇsi´c
</author>
<affiliation confidence="0.994769">
University of Zagreb
</affiliation>
<address confidence="0.802477">
Ivana Luˇci´ca 3, 10000 Zagreb, Croatia
</address>
<email confidence="0.979423">
nljubesi@ffzg.hr
</email>
<author confidence="0.994095">
Filip Klubiˇcka
</author>
<affiliation confidence="0.995423">
University of Zagreb
</affiliation>
<address confidence="0.792666">
Ivana Luˇci´ca 3, 10000 Zagreb, Croatia
</address>
<email confidence="0.964694">
fklubick@ffzg.hr
</email>
<sectionHeader confidence="0.992698" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99905744">
In this paper we present the construction
process of top-level-domain web corpora
of Bosnian, Croatian and Serbian. For
constructing the corpora we use the Spi-
derLing crawler with its associated tools
adapted for simultaneous crawling and
processing of text written in two scripts,
Latin and Cyrillic. In addition to the mod-
ified collection process we focus on two
sources of noise in the resulting corpora:
1. they contain documents written in the
other, closely related languages that can
not be identified with standard language
identification methods and 2. as most web
corpora, they partially contain low-quality
data not suitable for the specific research
and application objectives. We approach
both problems by using language mod-
eling on the crawled data only, omitting
the need for manually validated language
samples for training. On the task of dis-
criminating between closely related lan-
guages we outperform the state-of-the-art
Blacklist classifier reducing its error to a
fourth.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999771959183674">
Building web corpora for various NLP tasks has
become quite a standard approach, especially if
funding is limited and / or there is need for large
amounts of textual data.
Although off-the-shelf solutions for compiling
web corpora have emerged recently, there are still
specific challenges that have to be addressed in
most corpus construction processes. One such
challenge that we face while constructing the cor-
pora described in this paper is simultaneous us-
age of two scripts on two out of three top-level
domains (TLDs) crawled.
Additionally, there are still many open ques-
tions and possibilities for improvement in the
process of collecting data as well as data post-
processing. We address two of the latter kind –
discrimination between similar, neighboring lan-
guages that are used on all selected TLDs, and
the question of text quality in corpora collected in
such a fully automated fashion.
In the paper we present the process of building
web corpora of Bosnian, Croatian and Serbian by
crawling the .ba, .hr and .rs TLDs. The three
languages belong to the South Slavic language
branch and are very similar to each other. The
biggest differences between Croatian and Serbian
are the proto-Slavic vowel jat (Croatian ˇcovjek
vs. Serbian ˇcovek), way of handling proper nouns
(Croatian New York vs. Serbian Nju Jork), specific
syntactic constructions (Croatian ho´cu raditi vs.
Serbian ho´cu da radim) and a series of lexical dif-
ferences (Croatian mrkva vs. Serbian ˇsargarepa).
Bosnian is mostly seen as a mixture of those two
and allows, beside its own lexical specificities, so-
lutions from one or both languages.1
This paper is structured as follows: in Section
2 we give an overview of related work regarding
existing (web) corpora of the languages in ques-
tion, language identification and web text quality
estimation. Section 3 shows the process of col-
lecting the three TLD corpora with emphasis on
the problem of collecting data written in various
scripts, while in Section 4 we describe the linguis-
tic annotation layers added to the corpora. Section
5 depicts our approach to discriminating between
very similar languages while in Section 6 we de-
scribe our approach to identifying documents of
low text quality, and both approaches use recently
crawled data only.
</bodyText>
<footnote confidence="0.959168">
1A more thorough comparison of the three lan-
guages is available at http://en.wikipedia.org/
wiki/Comparison—of—standard—Bosnian,
—Croatian—and—Serbian
</footnote>
<page confidence="0.981371">
29
</page>
<bodyText confidence="0.3288955">
Felix Bildhauer &amp; Roland Schäfer (eds.), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 29–35,
Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.998461" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999890833333333">
The only two South Slavic languages for which
web corpora were previously built are Croatian
and Slovene (Ljubeˇsi´c and Erjavec, 2011). The
Croatian corpus presented in this paper is actually
an extension of the existing corpus, representing
its second version. hrWaC v1.0 was, until now,
the biggest available corpus of Croatian.
For Bosnian, almost no corpora are available
except the SETimes corpus2, which is a 10-
languages parallel corpus with its Bosnian side
consisting of 2.2 million words, and The Oslo
Corpus of Bosnian Texts3, which is a 1.5 mil-
lion words corpus consisting of different genres of
texts that were published in the 1990s.
For the Serbian language, until now, the largest
corpus was the SrpKor corpus4, consisting of 118
million words that are annotated with part-of-
speech information (16 tags) and lemmatized. The
corpus is available for search through an interface
for non-commercial purposes.
Until now, no large freely downloadable cor-
pora of Bosnian and Serbian were available, and
this was one of the strongest motivations for our
work.
Multiple pipelines for building web corpora
were described in many papers in the last decade
(Baroni et al., 2009; Ljubeˇsi´c and Erjavec, 2011;
Sch¨afer and Bildhauer, 2012), but, to the best of
our knowledge, only one pipeline is freely avail-
able as a complete, ready-to-use tool: the Brno
pipeline (Suchomel and Pomik´alek, 2012), con-
sisting of the SpiderLing crawler5, the Chared en-
coding detector6, the jusText content extractor7
and the Onion near-deduplicator8. Although we
have our own pipeline set up (this is the pipeline
the first versions of hrWaC and slWaC were built
with), we decided to compile these versions of
web corpora with the Brno pipeline for two rea-
sons: 1. to inspect the pipeline’s capabilities, and
2. to extend the Croatian web corpus as much as
possible by using a different crawler.
Although language identification is seen as a
</bodyText>
<footnote confidence="0.999102222222222">
2http://nlp.ffzg.hr/resources/corpora/
setimes/
3http://www.tekstlab.uio.no/Bosnian/
Corpus.html
4http://tinyurl.com/mocnzna
5http://nlp.fi.muni.cz/trac/spiderling
6https://code.google.com/p/chared/
7http://code.google.com/p/justext/
8http://code.google.com/p/onion/
</footnote>
<bodyText confidence="0.999989214285715">
solved problem by many, the recently growing in-
terest for it indicates the opposite. Recently, re-
searchers focused on improving off-the-shelf tools
for identifying many languages (Lui and Bald-
win, 2012), discriminating between similar lan-
guages where standard tools fail (Tiedemann and
Ljubeˇsi´c, 2012), identifying documents written in
multiple languages and identifying the languages
in such multilingual documents (Lui et al., 2014).
Text quality in automatically constructed web
corpora is quite an underresearched topic, with the
exception of boilerplate removal / content extrac-
tion approaches that deal with this problem implic-
itly (Baroni et al., 2008; Kohlsch¨utter et al., 2010),
but quite drastically, by removing all content that
does not conform to the criteria set. A recent ap-
proach to assessing text quality in web corpora in
an unsupervised manner (Sch¨afer et al., 2013) cal-
culates the weighted mean and standard deviation
of n most frequent words in a corpus sample and
measures how much a specific document deviates
from the estimated means. This approach is in its
basic idea quite similar to ours because it assumes
that most of the documents in the corpus contain
content of good quality. The main difference in
our approach is that we do not constrain ourselves
to most frequent words as features, but use char-
acter and word n-grams of all available text.
</bodyText>
<sectionHeader confidence="0.964234" genericHeader="method">
3 Corpus construction
</sectionHeader>
<bodyText confidence="0.99133945">
For constructing the corpora we used the Spi-
derLing crawler9 along with its associated tools
for encoding guessing, content extraction, lan-
guage identification and near-duplicate removal
(Suchomel and Pomik´alek, 2012). Seed URLs
for Bosnian and Serbian were obtained via the
Google Search API queried with bigrams of mid-
frequency terms. Those terms were obtained from
corpora that were built with focused crawls of
newspaper sites as part of our previous research
(Tiedemann and Ljubeˇsi´c, 2012). For Croatian
seed URLs, we used the home pages of web do-
mains obtained during the construction of the first
version of the hrWaC corpus. The number of seed
URLs was 8,388 for bsWaC, 11,427 for srWaC
and 14,396 for hrWaC. Each TLD was crawled for
21 days with 16 cores used for document process-
ing.
Because Serbian – which is frequently used on
the Serbian and Bosnian TLDs – uses two scripts
</bodyText>
<footnote confidence="0.942277">
9http://nlp.fi.muni.cz/trac/spiderling
</footnote>
<page confidence="0.997605">
30
</page>
<bodyText confidence="0.999467096774194">
– Latin and Cyrillic – we had to adjust the stan-
dard corpus construction process to cope with both
scripts. This was done by 1. building new two-
script models for encoding guessing with Chared,
2. defining stop-words used in content extraction
in both scripts and 3. transforming extracted text
from Cyrillic to Latin with serbian.py10 before
performing language identification and duplicate
removal. We kept all content of the final corpora in
the Latin script to simplify further processing, es-
pecially because linguistic annotation layers were
added with models developed for Croatian which
uses the Latin script exclusively. The information
about the amount of Cyrillic text in each document
is still preserved as an attribute of the &lt;doc&gt; el-
ement. Overall the percentage of documents writ-
ten &gt;90% in the Cyrillic script was 3.2% on the
Bosnian TLD and 16.7% on the Serbian TLD.
Near-duplicate identification was performed
both on the document and the paragraph level.
The document-level near-duplicates were removed
from the corpus cutting its size in half, while
paragraph-level near-duplicates were labeled by
the neardupe binary attribute in the &lt;p&gt; el-
ement enabling the corpus users to decide what
level of near-duplicate removal suits their needs.
The resulting size of the three corpora (in mil-
lions of tokens) after each of the three duplicate re-
moval stages is given in Table 1. Separate numbers
are shown for the new crawl of the Croatian TLD
and the final corpus consisting of both crawls.
</bodyText>
<table confidence="0.9866766">
PHYS DOCN PARN
bsWaC 1.0 722 429 288
hrWaC new 1,779 1,134 700
hrWaC 2.0 2,686 1,910 1,340
srWaC 1.0 1,554 894 557
</table>
<tableCaption confidence="0.993442">
Table 1: Size of the corpora in Mtokens after phys-
</tableCaption>
<bodyText confidence="0.82">
ical duplicate (PHY), document near-duplicate
(DOCN) and paragraph near-duplicate removal
(PARN)
At this point of the corpus construction process
the &lt;doc&gt; element contained the following at-
tributes:
</bodyText>
<listItem confidence="0.9842401">
• domain – the domain the document is pub-
lished on (e.g. zkvh.org.rs)
• url – the URL of the document
10http://klaus.e175.net/code/serbian.py
• crawl_date – date the document was
crawled
• cyrillic_num – number of Cyrillic let-
ters in the document
• cyrillic_perc – percentage of letters
that are Cyrillic
</listItem>
<sectionHeader confidence="0.820398" genericHeader="method">
4 Corpus annotation
</sectionHeader>
<bodyText confidence="0.999877894736842">
We annotated all three corpora on the level of
lemmas, morphosyntactic description (675 tags)
and dependency syntax (15 tags). Lemmatiza-
tion was performed with the CST’s Lemmatiser11
(Jongejan and Dalianis, 2009), morphosyntactic
tagging with HunPos12 (Hal´acsy et al., 2007) and
dependency syntax with mate-tools13 (Bohnet,
2010). All models were trained on the Croa-
tian 90k-token annotated corpus SETimes.HR14
(Agi´c and Ljubeˇsi´c, 2014) that we recently ex-
panded with 50k additional tokens from vari-
ous newspaper domains (at this point we call
it simply SETimes.HR+). Although the anno-
tated training corpora are Croatian, previous re-
search (Agi´c et al., 2013a; Agi´c et al., 2013b) has
shown that on this level of tagging accuracy on
in-domain test sets (lemma ≈96%, morphosyntac-
tic description (MSD) ≈87%, labeled attachment
score (LAS) ≈73%), annotating Serbian text with
models trained on Croatian data produced perfor-
mance loss of only up to 3% on all three levels
of annotation, while on out-of-domain test sets
(lemma ≈92%, MSD ≈81%, LAS ≈65%) there
was no loss in accuracy.
We nevertheless performed an intervention in
the SETimes.HR+ corpus before training the mod-
els used for annotating the Bosnian and the Ser-
bian TLD corpora. Namely, on the morphosyn-
tactic level the tagsets of Croatian and Serbian
are identical, except for one subset of tags for
the future tense which is present in Serbian and
not present in Croatian. This is because Croatian
uses the complex, analytic future tense consisting
of the infinitive of the main verb and the present
tense of the auxiliary verb have (radit ´cemo) while
Serbian uses both the analytic and the synthetic
form where the two words are conflated into one
(radi´cemo).
</bodyText>
<footnote confidence="0.9750134">
11https://github.com/kuhumcst/cstlemma
12https://code.google.com/p/hunpos/
13https://code.google.com/p/mate-tools/
14http://nlp.ffzg.hr/resources/corpora/
setimes-hr/
</footnote>
<page confidence="0.999768">
31
</page>
<bodyText confidence="0.9999788125">
To enable models to correctly handle both the
analytic and synthetic form of the future tense,
we simply repeated the sentences containing the
analytic form that we automatically transformed
to the synthetic one. By annotating the bsWaC
and srWaC corpora with the models trained on
the modified SETimes.HR+ corpus, we annotated
610k word forms in srWaC and 115k word forms
in bsWaC with the synthetic future tense. Manual
inspection showed that most of the tokens actually
do represent the future tense, proving that the in-
tervention was well worth it.
The lemmatization and morphosyntactic anno-
tation of all three corpora took just a few hours
while the full dependency parsing procedure on 40
server grade cores took 25 days.
</bodyText>
<sectionHeader confidence="0.988176" genericHeader="method">
5 Language identification
</sectionHeader>
<bodyText confidence="0.99997262962963">
Because each of the three languages of interest is
used to some extent on each of the three TLDs and,
additionally, these languages are very similar, dis-
criminating between them presented both a neces-
sity and a challenge.
In previous work on discriminating between
closely related languages, the Blacklist (BL) clas-
sifier (Tiedemann and Ljubeˇsi´c, 2012) has shown
to be, on a newspaper-based test set, 100% accu-
rate in discriminating between Croatian and Ser-
bian, and 97% accurate on all three languages of
interest.
Our aim at this stage was twofold: 1. to put the
existing BL classifier on a realistic test on (noisy)
web data and 2. to propose an alternative, simple,
data-intense, but noise-resistant method which can
be used for discriminating between closely related
languages or language varieties that are predomi-
nantly used on specific sections of the Web.
Our method (LM1) uses the whole content of
each of the three TLD web corpora (so large
amounts of automatically collected, noisy data) to
build unigram-level language models. Its advan-
tage over the BL classifier is that it does not re-
quire any clean, manually prepared samples for
training. The probability estimate for each word w
given the TLD, using add-one smoothing is this:
</bodyText>
<equation confidence="0.999395333333333">
c(w,TLD) + 1
Pˆ (w|TLD) = (1)
Ewi∈V (c(wi, TLD) + 1)
</equation>
<bodyText confidence="0.961359888888889">
where c(w, TLD) is the number of times word w
occurred on the specific TLD and V is the vocab-
ulary defined over all TLDs.
We perform classification on each document as
a maximum-a-posteriori (MAP) decision, i.e. we
choose the language of the corresponding TLD
(l ∈ TLD) that produces maximum probability
with respect to words occurring in the document
(w1...wn):
</bodyText>
<equation confidence="0.987274333333333">
�
lmap = arg max Pˆ(wi|l) (2)
l∈TLD i=1..n
</equation>
<bodyText confidence="0.999722631578947">
We should note here that our approach is identi-
cal to using the Naive Bayes classifier without the
a priori probability for each class, i.e. language.
Speaking in loose terms, what we do is that for
each document of each TLD, we identify, on the
word level, to which TLD data collection the doc-
ument corresponds best.
Because Bosnian is mostly a mixture of Croat-
ian and Serbian and actually represents a contin-
uum between those two languages, we decided
to compare the BL and the LM1 classifier on a
much more straight-forward task of discriminat-
ing between Croatian and Serbian. The results of
classifying each document with both classifiers are
given in Table 2. They show that both classifiers
agree on around 75% of decisions and that around
0.4 percent of documents from hrWaC are identi-
fied as Serbian and 1.5 percent of document from
srWaC as Croatian.
</bodyText>
<table confidence="0.998394333333333">
BL LM1 agreement
hrWaC 0.42% 0.3% 73.15%
srWaC 1.93 % 1.28% 80.53%
</table>
<tableCaption confidence="0.9043555">
Table 2: Percentage of documents identified by
each classifier as belonging to the other language
</tableCaption>
<bodyText confidence="0.999972133333333">
We compared the classifiers by manually in-
specting 100 random documents per corpus where
the two classifiers were not in agreement. The re-
sults of this tool-oriented evaluation are presented
in Table 3 showing that the LM1 classifier pro-
duced the correct answer in overall 4 times more
cases than the BL classifier.
If we assume that the decisions where the two
classifiers agree are correct (and manual inspec-
tion of data samples points in that direction) we
can conclude that our simple, data-intense, noise-
resistant LM1 method cuts the BL classification
error to a fourth. We consider a more thorough
evaluation of the two classifiers, probably by pool-
ing and annotating documents that were identified
</bodyText>
<page confidence="0.998313">
32
</page>
<table confidence="0.980593333333333">
BL LM1 NA
hrWaC 18% 62% 20%
srWaC 10% 48% 42%
</table>
<tableCaption confidence="0.994163">
Table 3: Percentage of correct decisions of each
</tableCaption>
<bodyText confidence="0.98584105">
classifier on documents where the classifiers dis-
agreed (NA represents documents that are a mix-
ture of both languages)
as belonging to the other TLD language by some
classifier, as future work.
Due to the significant reduction in error by the
LM1 classifier, we annotated each document in the
hrWaC and srWaC corpora with the LM1 binary
hr-sr language identifier while on bsWaC we used
the LM1 ternary bs-hr-sr classifier. This decision
is based on the fact that discriminating between all
three languages is very hard even for humans and
that for most users the hr-sr discrimination on the
two corpora will be informative enough. In each
document we encoded the normalized distribution
of log-probabilities for the considered languages,
enabling the corpus user to redefine his own lan-
guage criterion.
The percentage of documents from each corpus
being identified as a specific language is given in
</bodyText>
<tableCaption confidence="0.845082">
Table 4.
</tableCaption>
<table confidence="0.997973">
bs hr sr
bsWaC 78.0% 16.5% 5.5%
hrWaC - 99.7% 0.3%
srWaC - 1.3% 98.7%
</table>
<tableCaption confidence="0.997533">
Table 4: Distribution of identified languages
</tableCaption>
<bodyText confidence="0.848580666666667">
throughout the three corpora
Additional attributes added to the &lt;doc&gt; ele-
ment during language identification are these:
</bodyText>
<listItem confidence="0.818051428571429">
• lang – language code of the language iden-
tified by maximum-a-posteriori
• langdistr – normalized distri-
bution of log probabilities of lan-
guages taken under consideration (e.g.
bs:-0.324Jhr:-0.329Jsr:-0.347
for a document from bsWaC)
</listItem>
<sectionHeader confidence="0.939152" genericHeader="method">
6 Identifying text of low quality
</sectionHeader>
<bodyText confidence="0.999985647058824">
Finally, we tackled the problem of identifying doc-
uments of low text quality in an unsupervised
manner by assuming that most of the content of
each web corpus is of good quality and that low
quality content can be identified as data points
of lowest probability regarding language models
built on the whole data collection. We pragmati-
cally define low quality content as content not de-
sirable for a significant number of research or ap-
plication objectives.
For each TLD we calculated character n-gram
and word n-gram language models in the same
manner as in the previous section (Equation 1) for
language identification. We scored each TLD doc-
ument with each language model that was built on
that TLD. To get a probability estimate which does
not depend on the document length, we calculated
probabilities of subsequences of identical length
and computed the average of those.
We manually inspected documents with low
probability regarding character n-gram models
from level 1 to level 15 and word n-gram mod-
els from level 1 to level 5. Word n-gram mod-
els proved to be much less appropriate for cap-
turing low quality documents by lowest probabil-
ity scores than character n-gram models. Among
character n-gram models, 3-gram models were
able to identify documents with noise on the token
level while 12-gram models assigned low proba-
bilities to documents with noise above the token
level.
The most frequent types of potential noise
found in lowest scored documents in all three cor-
pora are the following:
</bodyText>
<listItem confidence="0.6644095625">
• 3-gram models
– non-standard usage of uppercase, lower-
case and punctuation
– URL-s
– uppercase want ads
– formulas
• 12-gram models
– words split into multiple words (due to
soft hyphen usage or HTML tags inside
words)
– enumerated and bulleted lists
– uppercase want ads
– non-standard text (slang, no uppercased
words, emoticons)
– dialects
– lyric, epic, historical texts
</listItem>
<page confidence="0.998358">
33
</page>
<bodyText confidence="0.999936763157894">
The character 3-gram method has additionally
proven to be a very good estimate of text quality on
the lexical level by strongly correlating (0.74) with
the knowledge-heavy method of calculating lexi-
cal overlap of each document with a morphologi-
cal dictionary which is available for Croatian15.
An interesting finding is that word-level models
perform much worse for this task than character-
level models. We hypothesize that this is due to
feature space sparsity on the word level which is
much lower on the character level.
We decided to postpone any final decisions (like
discretizing these two variables and defining one
or two categorical ones) and therefore encoded
both log-probabilities as attributes in each doc-
ument element in the corpus leaving to the fi-
nal users to define their own cut-off criteria. To
make that decision easier, for each document and
each character n-gram method we computed the
percentage of documents in the corpus that have
an equal or lower result of that character n-gram
method. This makes removing a specific percent-
age of documents with lowest scores regarding a
method much easier.
We also computed one very simple estimate of
text quality – the percentage of characters that are
diacritics. Namely, for some tasks, like lexicon en-
richment, working on non-diacritized text is not an
option. Additionally, it is to expect that lower us-
age of diacritics points to less standard language
usage. The distribution of this text quality esti-
mate in the hrWaC corpus (all three corpora fol-
low the same pattern) is depicted in Figure 1 show-
ing that the estimate is rather normally distributed
with a small peak at value zero representing non-
diacritized documents.
In each &lt;doc&gt; element we finally encoded 5
attributes regarding text quality:
</bodyText>
<listItem confidence="0.8142449">
• 3graph – average log-probability on 100-
character sequences regarding the character
3-gram model trained on the whole TLD cor-
pus
• 3graph_cumul – percentage of documents
with equal or lower 3graph attribute value
• 12graph – same as 3graph, but computed
with the character 12-gram model
• 12graph_cumul – like 3graph_cumul,
but for the 12graph attribute
</listItem>
<figure confidence="0.822689333333333">
15http://bit.ly/1mRjMrP
0.00 0.02 0.04 0.06 0.08 0.10
Percentage of diacritics
</figure>
<figureCaption confidence="0.964663">
Figure 1: Distribution of the percentage of charac-
ters of a document being diacritics
</figureCaption>
<listItem confidence="0.7041045">
• diacr_perc – percentage of non-
whitespace characters that are diacritics
</listItem>
<bodyText confidence="0.999928857142857">
We plan to perform extrinsic evaluation of the
three estimates of text quality on various NLP
tasks such as language modeling for statistical
machine translation, morphological lexicon induc-
tion, distributional lexicon induction of closely re-
lated languages and multi-word expression extrac-
tion.
</bodyText>
<sectionHeader confidence="0.998513" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.998484476190476">
In this paper we described the process of con-
structing three TLD corpora of Bosnian, Croatian
and Serbian.
After presenting the construction and annota-
tion process of the largest existing corpora for
each of the three languages, we focused on the
issue that all three languages are to some extent
used on all three TLDs. We presented a method
for discriminating between similar languages that
is based on unigram language modeling on the
crawled data only, which exploits the fact that the
majority of the data published on each TLD is
written in the language corresponding to that TLD.
We reduced the error of a state-of-the-art classifier
to a fourth on documents where the two classifiers
disagree on.
We dealt with the problem of identifying low
quality content as well, again using language mod-
eling on crawled data only, showing that document
probability regarding a character 3-gram model is
a very good estimate of lexical quality, while low
</bodyText>
<figure confidence="0.915245">
Frequency
0 50000 100000 150000
</figure>
<page confidence="0.995737">
34
</page>
<bodyText confidence="0.9999195">
character 12-gram probabilities identify low qual-
ity documents beyond the word boundary.
We encoded a total of 12 attributes in the docu-
ment element and the paragraph-near-duplicate in-
formation in the paragraph element enabling each
user to search for and define his own criteria.
We plan on experimenting with those attributes
on various tasks, from language modeling for sta-
tistical machine translation, to extracting various
linguistic knowledge from those corpora.
</bodyText>
<sectionHeader confidence="0.951968" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.998156">
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme FP7/2007-2013 un-
der grant agreement no. PIAP-GA-2012-324414
(project Abu-MaTran).
</bodyText>
<sectionHeader confidence="0.994183" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982297455555556">
[Agi´c and Ljube&amp;quot;si´c2014] &amp;quot;Zeljko Agi´c and Nikola
Ljube&amp;quot;si´c. 2014. The SETimes.HR linguistically
annotated corpus of Croatian. In Proceedings of
LREC 2014.
[Agi´c et al.2013a] &amp;quot;Zeljko Agi´c, Nikola Ljube&amp;quot;si´c, and
Danijela Merkler. 2013a. Lemmatization and mor-
phosyntactic tagging of Croatian and Serbian. In
Proceedings of the 4th Biennial International Work-
shop on Balto-Slavic Natural Language Processing,
pages 48–57, Sofia, Bulgaria, August. Association
for Computational Linguistics.
[Agi´c et al.2013b] &amp;quot;Zeljko Agi´c, Danijela Merkler, and
Da&amp;quot;sa Berovi´c. 2013b. Parsing Croatian and Serbian
by using Croatian dependency treebanks. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL
2013).
[Baroni et al.2008] Marco Baroni, Francis Chantree,
Adam Kilgarriff, and Serge Sharoff. 2008.
Cleaneval: a competition for cleaning web pages.
In Proceedings of the Sixth International Language
Resources and Evaluation (LREC’08), Marrakech,
Morocco. European Language Resources Associa-
tion (ELRA).
[Baroni et al.2009] Marco Baroni, Silvia Bernardini,
Adriano Ferraresi, and Eros Zanchetta. 2009. The
WaCky wide web: a collection of very large linguis-
tically processed web-crawled corpora. Language
Resources and Evaluation, pages 209–226.
[Bohnet2010] Bernd Bohnet. 2010. Very high accuracy
and fast dependency parsing is not a contradiction.
In The 23rd International Conference on Computa-
tional Linguistics (COLING 2010).
[Hal´acsy et al.2007] P´eter Hal´acsy, Andr´as Kornai, and
Csaba Oravecz. 2007. HunPos: an open source
trigram tagger. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, ACL ’07, pages 209–212,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
[Jongejan and Dalianis2009] Bart Jongejan and Her-
cules Dalianis. 2009. Automatic training of lemma-
tization rules that handle morphological changes in
pre-, in- and suffixes alike. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
145–153.
[Kohlsch¨utter et al.2010] Christian Kohlsch¨utter, Peter
Fankhauser, and Wolfgang Nejdl. 2010. Boilerplate
detection using shallow text features. In Brian D.
Davison, Torsten Suel, Nick Craswell, and Bing Liu,
editors, WSDM, pages 441–450. ACM.
[Ljube&amp;quot;si´c and Erjavec2011] Nikola Ljube&amp;quot;si´c and
Toma&amp;quot;z Erjavec. 2011. hrWaC and slWac: Com-
piling Web Corpora for Croatian and Slovene. In
Text, Speech and Dialogue - 14th International
Conference, TSD 2011, Pilsen, Czech Republic,
Lecture Notes in Computer Science, pages 395–402.
Springer.
[Lui and Baldwin2012] Marco Lui and Timothy Bald-
win. 2012. langid.py: An off-the-shelf language
identification tool. In ACL (System Demonstra-
tions), pages 25–30.
[Lui et al.2014] Marco Lui, Jey Han Lau, and Timothy
Baldwin. 2014. Automatic detection and language
identification of multilingual documents. Transac-
tions of the Association for Computational Linguis-
tics.
[Sch¨afer and Bildhauer2012] Roland Sch¨afer and Felix
Bildhauer. 2012. Building large corpora from the
web using a new efficient tool chain. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC’12), Istan-
bul, Turkey. European Language Resources Associ-
ation (ELRA).
[Sch¨afer et al.2013] Roland Sch¨afer, Adrien Barbaresi,
and Felix Bildhauer. 2013. The good, the bad, and
the hazy: Design decisions in web corpus construc-
tion. In Proceedings of the 8th Web as Corpus Work-
shop (WAC8).
[Suchomel and Pomik´alek2012] Vit Suchomel and Jan
Pomik´alek. 2012. Efficient web crawling for large
text corpora. In Serge Sharoff Adam Kilgarriff, edi-
tor, Proceedings of the seventh Web as Corpus Work-
shop (WAC7), pages 39–43, Lyon.
[Tiedemann and Ljube&amp;quot;si´c2012] J¨org Tiedemann and
Nikola Ljube&amp;quot;si´c. 2012. Efficient discrimination be-
tween closely related languages. In Proceedings of
COLING 2012, pages 2619–2634, Mumbai, India.
</reference>
<page confidence="0.999326">
35
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.511803">
<title confidence="0.997781">Web corpora of Bosnian, Croatian and Serbian</title>
<author confidence="0.991826">Nikola</author>
<affiliation confidence="0.8928895">University of Ivana Luˇci´ca 3, 10000 Zagreb,</affiliation>
<email confidence="0.969488">nljubesi@ffzg.hr</email>
<author confidence="0.912288">Filip</author>
<affiliation confidence="0.9262225">University of Ivana Luˇci´ca 3, 10000 Zagreb,</affiliation>
<email confidence="0.99703">fklubick@ffzg.hr</email>
<abstract confidence="0.994773730769231">In this paper we present the construction process of top-level-domain web corpora of Bosnian, Croatian and Serbian. For constructing the corpora we use the SpiderLing crawler with its associated tools adapted for simultaneous crawling and processing of text written in two scripts, Latin and Cyrillic. In addition to the modified collection process we focus on two sources of noise in the resulting corpora: 1. they contain documents written in the other, closely related languages that can not be identified with standard language identification methods and 2. as most web corpora, they partially contain low-quality data not suitable for the specific research and application objectives. We approach both problems by using language modeling on the crawled data only, omitting the need for manually validated language samples for training. On the task of discriminating between closely related languages we outperform the state-of-the-art Blacklist classifier reducing its error to a fourth.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Zeljko Agi´c</author>
<author>Nikola Ljubesi´c</author>
</authors>
<title>The SETimes.HR linguistically annotated corpus of Croatian.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC</booktitle>
<marker>[Agi´c and Ljube&amp;quot;si´c2014]</marker>
<rawString>&amp;quot;Zeljko Agi´c and Nikola Ljube&amp;quot;si´c. 2014. The SETimes.HR linguistically annotated corpus of Croatian. In Proceedings of LREC 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zeljko Agi´c</author>
<author>Nikola Ljubesi´c</author>
<author>Danijela Merkler</author>
</authors>
<title>Lemmatization and morphosyntactic tagging of Croatian and Serbian.</title>
<date>2013</date>
<booktitle>In Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing,</booktitle>
<pages>48--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>[Agi´c et al.2013a]</marker>
<rawString>&amp;quot;Zeljko Agi´c, Nikola Ljube&amp;quot;si´c, and Danijela Merkler. 2013a. Lemmatization and morphosyntactic tagging of Croatian and Serbian. In Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 48–57, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zeljko Agi´c</author>
<author>Danijela Merkler</author>
<author>Dasa Berovi´c</author>
</authors>
<title>Parsing Croatian and Serbian by using Croatian dependency treebanks.</title>
<date>2013</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL</booktitle>
<marker>[Agi´c et al.2013b]</marker>
<rawString>&amp;quot;Zeljko Agi´c, Danijela Merkler, and Da&amp;quot;sa Berovi´c. 2013b. Parsing Croatian and Serbian by using Croatian dependency treebanks. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Francis Chantree</author>
<author>Adam Kilgarriff</author>
<author>Serge Sharoff</author>
</authors>
<title>Cleaneval: a competition for cleaning web pages.</title>
<date>2008</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech,</location>
<marker>[Baroni et al.2008]</marker>
<rawString>Marco Baroni, Francis Chantree, Adam Kilgarriff, and Serge Sharoff. 2008. Cleaneval: a competition for cleaning web pages. In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), Marrakech, Morocco. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky wide web: a collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>209--226</pages>
<marker>[Baroni et al.2009]</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky wide web: a collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation, pages 209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Very high accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In The 23rd International Conference on Computational Linguistics (COLING</booktitle>
<marker>[Bohnet2010]</marker>
<rawString>Bernd Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In The 23rd International Conference on Computational Linguistics (COLING 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P´eter Hal´acsy</author>
<author>Andr´as Kornai</author>
<author>Csaba Oravecz</author>
</authors>
<title>HunPos: an open source trigram tagger.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>209--212</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>[Hal´acsy et al.2007]</marker>
<rawString>P´eter Hal´acsy, Andr´as Kornai, and Csaba Oravecz. 2007. HunPos: an open source trigram tagger. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 209–212, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Jongejan</author>
<author>Hercules Dalianis</author>
</authors>
<title>Automatic training of lemmatization rules that handle morphological changes in pre-, in- and suffixes alike.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>145--153</pages>
<marker>[Jongejan and Dalianis2009]</marker>
<rawString>Bart Jongejan and Hercules Dalianis. 2009. Automatic training of lemmatization rules that handle morphological changes in pre-, in- and suffixes alike. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 145–153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Kohlsch¨utter</author>
<author>Peter Fankhauser</author>
<author>Wolfgang Nejdl</author>
</authors>
<title>Boilerplate detection using shallow text features.</title>
<date>2010</date>
<pages>441--450</pages>
<editor>In Brian D. Davison, Torsten Suel, Nick Craswell, and Bing Liu, editors, WSDM,</editor>
<publisher>ACM.</publisher>
<marker>[Kohlsch¨utter et al.2010]</marker>
<rawString>Christian Kohlsch¨utter, Peter Fankhauser, and Wolfgang Nejdl. 2010. Boilerplate detection using shallow text features. In Brian D. Davison, Torsten Suel, Nick Craswell, and Bing Liu, editors, WSDM, pages 441–450. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikola Ljubesi´c</author>
<author>Tomaz Erjavec</author>
</authors>
<title>hrWaC and slWac: Compiling Web Corpora for Croatian and Slovene.</title>
<date>2011</date>
<booktitle>In Text, Speech and Dialogue - 14th International Conference, TSD 2011, Pilsen, Czech Republic, Lecture Notes in Computer Science,</booktitle>
<pages>395--402</pages>
<publisher>Springer.</publisher>
<marker>[Ljube&amp;quot;si´c and Erjavec2011]</marker>
<rawString>Nikola Ljube&amp;quot;si´c and Toma&amp;quot;z Erjavec. 2011. hrWaC and slWac: Compiling Web Corpora for Croatian and Slovene. In Text, Speech and Dialogue - 14th International Conference, TSD 2011, Pilsen, Czech Republic, Lecture Notes in Computer Science, pages 395–402. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>langid.py: An off-the-shelf language identification tool.</title>
<date>2012</date>
<booktitle>In ACL (System Demonstrations),</booktitle>
<pages>25--30</pages>
<marker>[Lui and Baldwin2012]</marker>
<rawString>Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In ACL (System Demonstrations), pages 25–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Jey Han Lau</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic detection and language identification of multilingual documents.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics.</journal>
<marker>[Lui et al.2014]</marker>
<rawString>Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014. Automatic detection and language identification of multilingual documents. Transactions of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Sch¨afer</author>
<author>Felix Bildhauer</author>
</authors>
<title>Building large corpora from the web using a new efficient tool chain.</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<location>Istanbul,</location>
<marker>[Sch¨afer and Bildhauer2012]</marker>
<rawString>Roland Sch¨afer and Felix Bildhauer. 2012. Building large corpora from the web using a new efficient tool chain. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Sch¨afer</author>
<author>Adrien Barbaresi</author>
<author>Felix Bildhauer</author>
</authors>
<title>The good, the bad, and the hazy: Design decisions in web corpus construction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8th Web as Corpus Workshop (WAC8).</booktitle>
<marker>[Sch¨afer et al.2013]</marker>
<rawString>Roland Sch¨afer, Adrien Barbaresi, and Felix Bildhauer. 2013. The good, the bad, and the hazy: Design decisions in web corpus construction. In Proceedings of the 8th Web as Corpus Workshop (WAC8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vit Suchomel</author>
<author>Jan Pomik´alek</author>
</authors>
<title>Efficient web crawling for large text corpora.</title>
<date>2012</date>
<booktitle>In Serge Sharoff Adam Kilgarriff, editor, Proceedings of the seventh Web as Corpus Workshop (WAC7),</booktitle>
<pages>39--43</pages>
<location>Lyon.</location>
<marker>[Suchomel and Pomik´alek2012]</marker>
<rawString>Vit Suchomel and Jan Pomik´alek. 2012. Efficient web crawling for large text corpora. In Serge Sharoff Adam Kilgarriff, editor, Proceedings of the seventh Web as Corpus Workshop (WAC7), pages 39–43, Lyon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
<author>Nikola Ljubesi´c</author>
</authors>
<title>Efficient discrimination between closely related languages.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>2619--2634</pages>
<location>Mumbai, India.</location>
<marker>[Tiedemann and Ljube&amp;quot;si´c2012]</marker>
<rawString>J¨org Tiedemann and Nikola Ljube&amp;quot;si´c. 2012. Efficient discrimination between closely related languages. In Proceedings of COLING 2012, pages 2619–2634, Mumbai, India.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
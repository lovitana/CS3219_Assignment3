<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9401605">
Code Mixing: A Challenge for Language Identification in the Language of
Social Media
</title>
<author confidence="0.998054">
Utsab Barman, Amitava Das†, Joachim Wagner and Jennifer Foster
</author>
<affiliation confidence="0.99917925">
CNGL Centre for Global Intelligent Content, National Centre for Language Technology
School of Computing, Dublin City University, Dublin, Ireland
†Department of Computer Science and Engineering
University of North Texas, Denton, Texas, USA
</affiliation>
<email confidence="0.9884755">
{ubarman,jwagner,jfoster}@computing.dcu.ie
amitava.das@unt.edu
</email>
<sectionHeader confidence="0.993858" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999900653846154">
In social media communication, multilin-
gual speakers often switch between lan-
guages, and, in such an environment, au-
tomatic language identification becomes
both a necessary and challenging task.
In this paper, we describe our work in
progress on the problem of automatic
language identification for the language
of social media. We describe a new
dataset that we are in the process of cre-
ating, which contains Facebook posts and
comments that exhibit code mixing be-
tween Bengali, English and Hindi. We
also present some preliminary word-level
language identification experiments using
this dataset. Different techniques are
employed, including a simple unsuper-
vised dictionary-based approach, super-
vised word-level classification with and
without contextual clues, and sequence la-
belling using Conditional Random Fields.
We find that the dictionary-based approach
is surpassed by supervised classification
and sequence labelling, and that it is im-
portant to take contextual clues into con-
sideration.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999853361702128">
Automatic processing and understanding of Social
Media Content (SMC) is currently attracting much
attention from the Natural Language Processing
research community. Although English is still by
far the most popular language in SMC, its domi-
nance is receding. Hong et al. (2011), for exam-
ple, applied an automatic language detection algo-
rithm to over 62 million tweets to identify the top
10 most popular languages on Twitter. They found
that only half of the tweets were in English. More-
over, mixing multiple languages together (code
mixing) is a popular trend in social media users
from language-dense areas (C´ardenas-Claros and
Isharyanti, 2009; Shafie and Nayan, 2013). In
a scenario where speakers switch between lan-
guages within a conversation, sentence or even
word, the task of automatic language identifica-
tion becomes increasingly important to facilitate
further processing.
Speakers whose first language uses a non-
Roman alphabet write using the Roman alphabet
for convenience (phonetic typing) which increases
the likelihood of code mixing with a Roman-
alphabet language. This can be especially ob-
served in South-East Asia and in the Indian sub-
continent. The following is a code mixing com-
ment taken from a Facebook group of Indian uni-
versity students:
Original: Yaar tu to, GOD hain. tui JU
te ki korchis? Hail u man!
Translation: Buddy you are GOD. What
are you doing in JU? Hail u man!
This comment is written in three languages: En-
glish, Hindi (italics), and Bengali (boldface). For
Bengali and Hindi, phonetic typing has been used.
We follow in the footsteps of recent work on
language identification for SMC (Hughes et al.,
2006; Baldwin and Lui, 2010; Bergsma et al.,
2012), focusing specifically on the problem of
word-level language identification for code mixing
SMC. Our corpus for this task is collected from
Facebook and contains instances of Bengali(BN)-
English(EN)-Hindi(HI) code mixing.
The paper is organized as follows: in Section 2,
we review related research in the area of code
mixing and language identification; in Section 3,
we describe our code mixing corpus, the data it-
</bodyText>
<page confidence="0.991983">
13
</page>
<note confidence="0.860787">
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 13–23,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999276666666667">
self and the annotation process; in Section 4, we
list the tools and resources which we use in our
language identification experiments, described in
Section 5. Finally, in Section 6, we conclude
and provide suggestions for future research on this
topic.
</bodyText>
<sectionHeader confidence="0.880305" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.999942351351352">
The problem of language identification has been
investigated for half a century (Gold, 1967) and
that of computational analysis of code switching
for several decades (Joshi, 1982), but there has
been less work on automatic language identifi-
cation for multilingual code-mixed texts. Before
turning to that topic, we first briefly survey studies
on the general characteristics of code mixing.
Code mixing is a normal, natural product of
bilingual and multilingual language use. Signif-
icant studies of the phenomenon can be found
in the linguistics literature (Milroy and Muysken,
1995; Alex, 2008; Auer, 2013). These works
mainly discuss the sociological and conversational
necessities behind code mixing as well as its lin-
guistic nature. Scholars distinguish between inter-
sentence, intra-sentence and intra-word code mix-
ing.
Several researchers have investigated the rea-
sons for and the types of code mixing. Initial stud-
ies on Chinese-English code mixing in Hong Kong
(Li, 2000) and Macao (San, 2009) indicated that
mainly linguistic motivations were triggering the
code mixing in those highly bilingual societies.
Hidayat (2012) showed that Facebook users tend
to mainly use inter-sentential switching over intra-
sentential, and report that 45% of the switching
was instigated by real lexical needs, 40% was used
for talking about a particular topic, and 5% for
content clarification. The predominance of inter-
sentential code mixing in social media text was
also noted in the study by San (2009), which com-
pared the mixing in blog posts to that in the spoken
language in Macao. Dewaele (2010) claims that
‘strong emotional arousal’ increases the frequency
of code mixing. Dey and Fung (2014) present
a speech corpus of English-Hindi code mixing in
student interviews and analyse the motivations for
code mixing and in what grammatical contexts
code mixing occurs.
Turning to the work on automatic analysis of
code mixing, there have been some studies on de-
tecting code mixing in speech (Solorio and Liu,
2008a; Weiner et al., 2012). Solorio and Liu
(2008b) try to predict the points inside a set of spo-
ken Spanish-English sentences where the speak-
ers switch between the two languages. Other
studies have looked at code mixing in differ-
ent types of short texts, such as information re-
trieval queries (Gottron and Lipka, 2010) and SMS
messages (Farrugia, 2004; Rosner and Farrugia,
2007). Yamaguchi and Tanaka-Ishii (2012) per-
form language identification using artificial mul-
tilingual data, created by randomly sampling text
segments from monolingual documents. King
and Abney (2013) used weakly semi-supervised
methods to perform word-level language identifi-
cation. A dataset of 30 languages has been used
in their work. They explore several language
identification approaches, including a Naive Bayes
classifier for individual word-level classification
and sequence labelling with Conditional Random
Fields trained with Generalized Expectation crite-
ria (Mann and McCallum, 2008; Mann and Mc-
Callum, 2010), which achieved the highest scores.
Another very recent work on this topic is (Nguyen
and Do˘gru¨oz, 2013). They report on language
identification experiments performed on Turkish
and Dutch forum data. Experiments have been
carried out using language models, dictionaries,
logistic regression classification and Conditional
Random Fields. They find that language models
are more robust than dictionaries and that contex-
tual information is helpful for the task.
</bodyText>
<sectionHeader confidence="0.98303" genericHeader="method">
3 Corpus Acquisition
</sectionHeader>
<bodyText confidence="0.999840823529412">
Taking into account the claim that code mixing is
frequent among speakers who are multilingual and
younger in age (C´ardenas-Claros and Isharyanti,
2009), we choose an Indian student community
between the 20-30 year age group as our data
source. India is a country with 30 spoken lan-
guages, among which 22 are official. code mix-
ing is very frequent in the Indian sub-continent
because languages change within very short geo-
distances and people generally have a basic knowl-
edge of their neighboring languages.
A Facebook group1 and 11 Facebook users
(known to the authors) were selected to obtain
publicly available posts and comments. The Face-
book graph API explorer was used for data collec-
tion. Since these Facebook users are from West
Bengal, the most dominant language is Bengali
</bodyText>
<footnote confidence="0.987405">
1https://www.facebook.com/jumatrimonial
</footnote>
<page confidence="0.999426">
14
</page>
<bodyText confidence="0.9996532">
(Native Language), followed by English and then
Hindi (National Language of India). The posts
and comments in Bengali and Hindi script were
discarded during data collection, resulting in 2335
posts and 9813 comments.
</bodyText>
<subsectionHeader confidence="0.999215">
3.1 Annotation
</subsectionHeader>
<bodyText confidence="0.999576575">
Four annotators took part in the annotation task.
Three were computer science students and the
other was one of the authors. The annotators are
proficient in all three languages of our corpus. A
simple annotation tool was developed which en-
abled these annotators to identify and distinguish
the different languages present in the content by
tagging them. Annotators were supplied with 4
basic tags (viz. sentence, fragment, inclusion and
wlcm (word-level code mixing)) to annotate differ-
ent levels of code mixing. Under each tag, six at-
tributes were provided, viz. English (en), Bengali
(bn), Hindi (hi), Mixed (mixd), Universal (univ)
and Undefined (undef). The attribute univ is as-
sociated with symbols, numbers, emoticons and
universal expressions (e.g. hahaha, lol). The at-
tribute undef is specified for a sentence or a word
for which no language tags can be attributed or
cannot be categorized as univ. In addition, anno-
tators were instructed to annotate named entities
separately. What follows are descriptions of each
of the annotation tags.
Sentence (sent): This tag refers to a sentence
and can be used to mark inter-sentential code mix-
ing. Annotators were instructed to identify a sen-
tence with its base language (e.g. en, bn, hi and
mixd) or with other types (e.g. univ, undef) as the
first task of annotation. Only the attribute mixd is
used to refer to a sentence which contains multi-
ple languages in the same proportion. A sentence
may contain any number of inclusions, fragments
and word-level code mixing. A sentence can be at-
tributed as univ if and only if it contains symbols,
numbers, emoticons, chat acronyms and no other
words (Hindi, English or Bengali). A sentence can
be attributed as undef if it is not a sentence marked
as univ and has words/tokens that can not be cate-
gorized as Hindi, English or Bengali. Some exam-
ples of sentence-level annotations are the follow-
ing:
</bodyText>
<listItem confidence="0.950331333333333">
1. English-Sentence:
[sent-lang=“en”] what a 6 hrs long...but re-
ally nice tennis.... [/sent]
2. Bengali-Sentence:
[sent-lang=“bn”] shubho nabo borsho.. :)
[/sent]
3. Hindi Sentence:
[sent-lang=“hi”] karwa sachh :( [/sent]
4. Mixed-Sentence:
[sent-lang=“mixd”] [frag-lang=“hi”] oye
hoye angreji me kahte hai ke [/frag]
[frag-lang=“en”] I love u.. !!! [/frag] [/sent]
5. Univ-Sentence:
[sent-lang=“univ”] hahahahahahah !!!!!
[/sent]
6. Undef-Sentence:
[sent-lang=“undef”] Hablando de una triple
amenaza. [/sent]
</listItem>
<bodyText confidence="0.994231">
Fragment (frag): This refers to a group of for-
eign words, grammatically related, in a sentence.
The presence of this tag in a sentence conveys that
intra-sentential code mixing has occurred within
the sentence boundary. Identification of fragments
(if present) in a sentence was the second task of
annotation. A sentence (sent) with attribute mixd
must contain multiple fragments (frag) with a spe-
cific language attribute. In the fourth example
above, the sentence contains a Hindi fragment oye
hoye angreji me kahte hai ke and an English
fragment I love u.. !!!, hence it is considered as a
mixd sentence. A fragment can have any number
of inclusions and word-level code mixing. In the
first example below, Jio is a popular Bengali word
appearing in the English fragment Jio.. good joke,
hence tagged as a Bengali inclusion. One can ar-
gue that the word Jio could be a separate Bengali
inclusion (i.e. can be tagged as a Bengali inclu-
sion outside the English fragment). But looking
at the syntactic pattern and the sense expressed by
the comment, the annotator kept it as a single unit.
In the second example below, an instance of word-
level code mixing, typer, has been found in an En-
glish fragment (where the root English word type
has the Bengali suffix r).
</bodyText>
<listItem confidence="0.843225666666667">
1. Fragment with Inclusion:
[sent-lang=“mixd”] [frag-lang=“en”] [incl-
lang=“bn”] Jio.. [/incl] good joke [/frag] [frag
lang=“bn”] ”amar Babin” [/frag] [/sent]
2. Fragment with Word-Level code mixing:
[sent-lang=“mixd”] [frag-lang=“en”] ” I will
find u and marry you ” [/frag] [frag-
lang=“bn”] [wlcm-type=“en-and-bn-suffix”]
typer [/wlcm] hoe glo to! :D [/frag] [/sent]
</listItem>
<page confidence="0.988664">
15
</page>
<bodyText confidence="0.998225181818182">
Inclusion (incl): An inclusion is a foreign word
or phrase in a sentence or in a fragment which
is assimilated or used very frequently in native
language. Identification of inclusions can be per-
formed after annotating a sentence and fragment
(if present in that sentence). An inclusion within a
sentence or fragment also denotes intra-sentential
code mixing. In the example below, seriously is an
English inclusion which is assimilated in today’s
colloquial Bengali and Hindi. The only tag that an
inclusion may contain is word-level code mixing.
</bodyText>
<subsectionHeader confidence="0.37907">
1. Sentence with Inclusion:
</subsectionHeader>
<bodyText confidence="0.980392166666666">
[sent-lang=“bn”] Na re [incl-lang=“en”] seri-
ously [/incl] ami khub kharap achi. [/sent]
Word-Level code mixing (wlcm): This is the
smallest unit of code mixing. This tag was in-
troduced to capture intra-word code mixing and
denotes cases where code mixing has occurred
within a single word. Identifying word-level code
mixing is the last task of annotation. Annotators
were told to mention the type of word-level code
mixing in the form of an attribute (Base Language
+ Second Language) format. Some examples are
provided below. In the first example below, the
root word class is English and e is an Bengali suf-
fix that has been added. In the third example be-
low, the opposite can be observed – the root word
Kando is Bengali, and an English suffix z has been
added. In the second example below, a named en-
tity suman is present with a Bengali suffix er.
</bodyText>
<listItem confidence="0.850375555555556">
1. Word-Level code mixing (EN-BN):
[wlcm-type=“en-and-bn-suffix”] classe
[/wlcm]
2. Word-Level code mixing (NE-BN):
[wlcm-type=“NE-and-bn-suffix”] sumaner
[/wlcm]
3. Word-Level code mixing (BN-EN):
[wlcm-type=“bn-and-en-suffix”] kandoz
[/wlcm]
</listItem>
<subsectionHeader confidence="0.425058">
3.1.1 Inter Annotator Agreement
</subsectionHeader>
<bodyText confidence="0.999085833333333">
We calculate word-level inter annotator agreement
(Cohen’s Kappa) on a subset of 100 comments
(randomly selected) between two annotators. Two
annotators are in agreement about a word if they
both annotate the word with the same attribute
(en, bn, hi, univ, undef), regardless of whether
the word is inside an inclusion, fragment or sen-
tence. Our observations that the word-level anno-
tation process is not a very ambiguous task and
that annotation instruction is also straightforward
are confirmed in a high inter-annotator agreement
(IAA) with a Kappa value of 0.884.
</bodyText>
<subsectionHeader confidence="0.999546">
3.2 Data Characteristics
</subsectionHeader>
<bodyText confidence="0.999903235294118">
Tag-level and word-level statistics of annotated
data that reveal the characteristics of our data set
are described in Table 1 and in Table 2 respec-
tively. More than 56% of total sentences and al-
most 40% of total tokens are in Bengali, which is
the dominant language of this corpus. English is
the second most dominant language covering al-
most 33% of total tokens and 35% of total sen-
tences. The amount of Hindi data is substantially
lower – nearly 1.75% of total tokens and 2% of to-
tal sentences. However, English inclusions (84%
of total inclusions) are more prominent than Hindi
or Bengali inclusions and there are a substantial
number of English fragments (almost 52% of total
fragments) present in our corpus. This means that
English is the main language involved in the code
mixing.
</bodyText>
<table confidence="0.961613">
Statistics of Different Tags
Tags En Bn Hi Mixd Univ Undef
sent 5,370 8,523 354 204 746 15
frag 288 213 40 0 6 0
incl 7,377 262 94 0 1,032 1
wlcm 477
Name Entity 3,602
Acronym 691
</table>
<tableCaption confidence="0.994031">
Table 1: Tag-level statistics
</tableCaption>
<table confidence="0.999775222222222">
Word-Level Tag Count
EN 66,298
BN 79,899
HI 3,440
WLCM 633
NE 5,233
ACRO 715
UNIV 39,291
UNDEF 61
</table>
<tableCaption confidence="0.995531">
Table 2: Word-level statistics
</tableCaption>
<subsectionHeader confidence="0.862087">
3.2.1 Code Mixing Types
</subsectionHeader>
<bodyText confidence="0.999773">
In our corpus, inter- and intra-sentential code mix-
ing are more prominent than word-level code mix-
ing, which is similar to the findings of (Hidayat,
2012) . Our corpus contains every type of code
mixing in English, Hindi and Bengali viz. in-
ter/intra sentential and word-level as described in
the previous section. Some examples of different
types of code mixing in our corpus are presented
below.
</bodyText>
<page confidence="0.969021">
16
</page>
<listItem confidence="0.812620133333333">
1. Inter-Sentential:
[sent-lang=“hi”] Itna izzat diye aapne mujhe
!!! [/sent]
[sent-lang=“en”] Tears ofjoy. :’( :’( [/sent]
2. Intra-Sentential:
[sent-lang=“bn”] [incl-lang=“en”] by d way
[/incl] ei [frag-lang=“en”] my craving arms
shall forever remain empty.. never hold u
close.. [/frag] line ta baddo [incl-lang=“en”]
cheezy [/incl] :P ;) [/sent]
3. Word-Level:
[sent-lang=“bn”] [incl-lang=“en”] 1st yr
[/incl] eo to ei [wlcm-type=“en+bnSuffix”]
tymer [/wlcm] modhye sobar jute jay ..
[/sent]
</listItem>
<subsectionHeader confidence="0.587113">
3.2.2 Ambiguous Words
</subsectionHeader>
<bodyText confidence="0.997257142857143">
Annotators were instructed to tag an English word
as English irrespective of any influence of word
borrowing or foreign inclusion but an inspection of
the annotations revealed that English words were
sometimes annotated as Bengali or Hindi. To un-
derstand this phenomenon we processed the list
of language (EN,BN and HI) word types (total
26,475) and observed the percentage of types that
were not always annotated with the one language
throughout the corpus. The results are presented in
Table 3. Almost 7% of total types are ambiguous
(i.e. tagged in different languages during annota-
tion). Among them, a substantial amount (5.58%)
are English/Bengali.
</bodyText>
<table confidence="0.999101375">
Label(s) Count Percentage
EN 9,109 34.40
BN 14,345 54.18
HI 1,039 3.92
EN or BN 1,479 5.58
EN or HI 61 0.23
BN or HI 277 1.04
EN or BN or HI 165 0.62
</table>
<tableCaption confidence="0.997908">
Table 3: Statistics of ambiguous and monolingual
word types
</tableCaption>
<bodyText confidence="0.999706655172414">
There are two reasons why this is happening:
Same Words Across Languages Some words
are the same (e.g. baba, maa, na, khali) in Hindi
and Bengali because both of the languages orig-
inated from a single language Sanskrit and share
a good amount of common vocabulary. It also
occurred in English-Hindi and English-Bengali as
a result of word borrowing. Most of these are
commonly used inclusions like clg, dept, ques-
tion, cigarette, and topic. Sometimes the anno-
tators were careful enough to tag such words as
English and sometimes these words were tagged
in the annotators’ native languages. During cross
checking of the annotated data the same error pat-
terns were observed for multiple annotators, i.e.
tagging commonly used foreign words into native
language. It only demonstrates that these English
words are highly assimilated in the conversational
vocabulary of Bengali and Hindi.
Phonetic Similarity of Spellings Due to pho-
netic typing some words share the same surface
form across two and sometimes across three lan-
guages. As an example, to is a word in the three
languages: it has occurred 1209 times as English,
715 times as Bengali and 55 times as Hindi in our
data. The meaning of these words (e.g. to, bolo,
die) are different in different languages. This phe-
nomenon is perhaps exacerbated by the trend to-
wards short and noisy spelling in SMC.
</bodyText>
<sectionHeader confidence="0.987386" genericHeader="method">
4 Tools and Resources
</sectionHeader>
<bodyText confidence="0.999289">
We have used the following resources and tools in
our experiment.
</bodyText>
<sectionHeader confidence="0.5725" genericHeader="method">
Dictionaries
</sectionHeader>
<listItem confidence="0.985021434782609">
1. British National Corpus (BNC): We com-
pile a word frequency list from the BNC (As-
ton and Burnard, 1998).
2. SEMEVAL 2013 Twitter Corpus (Se-
mevalTwitter): To cope with the language
of social media we use the SEMEVAL 2013
(Nakov et al., 2013) training data for the
Twitter sentiment analysis task. This data
comes from a popular social media site and
hence is likely to reflect the linguistic proper-
ties of SMC.
3. Lexical Normalization List (LexNorm-
List): Spelling variation is a well-known
phenomenon in SMC. We use a lexical nor-
malization dictionary created by Han et al.
(2012) to handle the different spelling vari-
ations in our data.
Machine Learning Toolkits
1. WEKA: We use the Weka toolkit (Hall et
al., 2009) for our experiments in decision tree
training.
2. MALLET: CRF learning is applied using the
MALLET toolkit (McCallum, 2002).
</listItem>
<page confidence="0.872364">
17
</page>
<listItem confidence="0.940940666666667">
3. Liblinear: We apply Support Vector Ma-
chine (SVM) learning with a linear kernel us-
ing the Liblinear package (Fan et al., 2008).
</listItem>
<note confidence="0.5282775">
NLP Tools For data tokenization we used the
CMU Tweet-Tokenizer (Owoputi et al., 2013).
</note>
<sectionHeader confidence="0.997011" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.994086642857143">
Since our training data is entirely labelled at the
word-level by human annotators, we address the
word-level language identification task in a fully
supervised way.
Out of the total data, 15% is set aside as a
blind test set, while the rest is employed in our ex-
periments through a 5-fold cross-validation setup.
There is a substantial amount of token overlap be-
tween the cross-validation data and the test set –
88% of total EN tokens, 86% of total Bengali to-
kens and 57% of total Hindi tokens of the test set
are present in the cross-validation data.2
We address the problem of word-level in three
different ways:
</bodyText>
<listItem confidence="0.9115025">
1. A simple heuristic-based approach which
uses a combination of our dictionaries to clas-
sify the language of a word
2. Word-level classification using supervised
machine learning with SVMs but no contex-
tual information
3. Word-level classification using supervised
machine learning with SVMs and sequence
labelling using CRFs, both employing con-
textual information
</listItem>
<bodyText confidence="0.999668518518519">
Named entities and instances of word-level
code mixing are excluded from evaluation. For
systems which do not take the context of a word
into account, i.e. the dictionary-based approach
(Section 5.1) and the SVM approach without con-
textual clues (Section 5.2), named entities and in-
stances of word-level code mixing can be safely
excluded from training. For systems which do
take context into account, the CRF system (Sec-
tion 5.3.1) and the SVM system with contextual
clues (Section 5.3.2), these are included in train-
ing, because to exclude them would result in un-
realistic contexts. This means that these systems
2We found 25 comments and 17 posts common between
the cross-validation data and the test set. The reason for this
is that users of social media often express themselves in a
concise way. Almost all of these common data consisted of 1
to 3 token(s). In most of the cases these tokens were emoti-
cons, symbols or universal expressions such as wow and lol.
As the percentage of these comments is low, we keep these
comments as they are.
can classify a word to be a named entity or an in-
stance of word-level code mixing. To avoid this,
we implement a post-processor which backs off in
these cases to a system which hasn’t seen named
entities or word-level code mixing in training (see
Section 5.3).
</bodyText>
<subsectionHeader confidence="0.899533">
5.1 Dictionary-Based Detection
</subsectionHeader>
<bodyText confidence="0.999744964285714">
We start with dictionary-based language detec-
tion. Generally a dictionary-based language de-
tector predicts the language of a word based on
its frequency in multiple language dictionaries. In
our data the Bengali and Hindi tokens are phoneti-
cally typed. As no such transliterated dictionary is,
to our knowledge, available for Bengali and Hindi,
we use the training set words as dictionaries. For
words that have multiple annotations in training
data (ambiguous words), we select the majority
tag based on frequency, e.g. the word to will al-
ways be tagged as English.
Our English dictionaries are those described
in Section 4 (BNC, LexNormList, SemEvalTwit-
ter) and the training set words. For LexNorm-
List, we have no frequency information, and so
we consider it as a simple word list. To pre-
dict the language of a word, dictionaries with nor-
malized frequency were considered first (BNC,
SemEvalTwitter, Training Data), if not found,
word list look-up was performed. The predicted
language is chosen based on the dominant lan-
guage(s) of the corpus if the word appears in mul-
tiple dictionaries with same frequency or if the
word does not appear in any dictionary or list.
A simple rule-based method is applied to pre-
dict universal expressions. A token is considered
as univ if any of the following conditions satisfies:
</bodyText>
<listItem confidence="0.989492555555556">
• All characters of the token are symbols or
numbers.
• The token contains certain repetitions identi-
fied by regular expressions.(e.g. hahaha).
• The token is a hash-tag or an URL or
mention-tags (e.g. @Sumit).
• Tokens (e.g. lol) identified by a word list
compiled from the relevant 4/5th of the train-
ing data.
</listItem>
<bodyText confidence="0.9997262">
Table 4 shows the results of dictionary-based
detection obtained from 5-fold cross-validation
averaging. We try different combinations and fre-
quency thresholds of the above dictionaries. We
find that using a normalized frequency is helpful
</bodyText>
<page confidence="0.998132">
18
</page>
<bodyText confidence="0.998702">
and that a combination of LexNormList and Train-
ing Data dictionaries is suited best for our data.
Hence, we consider this as our baseline language
identification system.
</bodyText>
<table confidence="0.9988885">
Dictionary Accuracy(%)
BNC 80.09
SemevalTwitter 77.61
LexNormList 79.86
Training Data 90.21
LexNormList+TrainingData (Baseline) 93.12
</table>
<tableCaption confidence="0.9909145">
Table 4: Average cross-validation accuracy of
dictionary-based detection
</tableCaption>
<subsectionHeader confidence="0.9636605">
5.2 Word-Level Classification without
Contextual Clues
</subsectionHeader>
<bodyText confidence="0.997629">
The following feature types are employed:
</bodyText>
<listItem confidence="0.999666666666667">
1. Char-n-grams (G): We start with a character
n-gram-based approach (Cavnar and Tren-
kle, 1994), which is most common and fol-
lowed by many language identification re-
searchers. Following the work of King and
Abney (2013), we select character n-grams
(n=1 to 5) and the word as the features in our
experiments.
2. Presence in Dictionaries (D): We use pres-
ence in a dictionary as a features for all avail-
able dictionaries in previous experiments.
3. Length of words (L): Instead of using the
</listItem>
<bodyText confidence="0.797083818181818">
raw length value as a feature, we follow our
previous work (Rubino et al., 2013; Wagner
et al., 2014) and create multiple features for
length using a decision tree (J48). We use
length as the only feature to train a decision
tree for each fold and use the nodes obtained
from the tree to create boolean features.
4. Capitalization (C): We use 3 boolean fea-
tures to encode capitalization information:
whether any letter in the word is capitalized,
whether all letters in the word are capitalized
and whether the first letter is capitalized.
We perform experiments with an SVM classifier
(linear kernel) for different combination of these
features.3 Parameter optimizations (C range 2-15
to 210) for SVM are performed for each feature
3According to (Hsu et al., 2010) the SVM linear kernel
with parameter C optimization is good enough when dealing
with a large number of features. Though an RBF kernel can
be more effective than a linear one, it is possible only after
proper optimization of C and γ parameters, which is compu-
tational expensive for such a large feature set.
</bodyText>
<table confidence="0.9998984">
Features Accuracy Features Accuracy
G 94.62 GD 94.67
GL 94.62 GDL 94.73
GC 94.64 GDC 94.72
GLC 94.64 GDLC 94.75
</table>
<tableCaption confidence="0.6224382">
Table 5: Average cross-validation accuracy for
SVM word-level classification (without context),
G = char-n-gram, L = binary length features, D
= presence in dictionaries and C = capitalization
features
</tableCaption>
<table confidence="0.98207975">
GDLC: 94.75%
GLC: 94.64% GDL: 94.73% GDC: 94.72%
GL: 94.62% GC: 94.64% GD: 94.67%
G: 94.62%
</table>
<figureCaption confidence="0.845064">
Figure 1: Average cross-validation accuracy for
SVM word-level classification (without context),
</figureCaption>
<bodyText confidence="0.982994">
G = char-n-gram, L = binary length features, D
= presence in dictionaries and C = capitalization
features: cube visualization
set and best cross-validation accuracy is found for
the GDLC-based run (94.75%) at C=1 (see Table 5
and Fig. 1).
We also investigate the use of a dictionary-to-
char-n-gram back-off model – the idea is to ap-
ply the char-n-gram model SVM-GDLC for those
words for which a majority-based decision is taken
during dictionary-based detection. However, it
does not outperform the SVM. Hence, we select
SVM-GDLC for the next steps of our experiments
as the best exemplar of our individual word-level
classifier (without contextual clues).
</bodyText>
<subsectionHeader confidence="0.9582265">
5.3 Language Identification with Contextual
Clues
</subsectionHeader>
<bodyText confidence="0.980573888888889">
Contextual clues can play a very important role in
word-level language identification. As an exam-
ple, a part of a comment is presented from cross-
validation fold 1 that contains the word die which
is wrongly classified by the SVM classifier. The
frequency of die in the training set of fold 1 is 6
for English, 31 for Bengali and 0 for Hindi.
Gold Data: ..../univ the/en movie/en
for/en which/en i/en can/en die/en for/en
</bodyText>
<page confidence="0.994259">
19
</page>
<table confidence="0.999798888888889">
Features Order-0 Order-1 Order-2
G 92.80 95.16 95.36
GD 93.42 95.59 95.98
GL 92.82 95.14 95.41
GDL 93.47 95.60 95.94
GC 92.07 94.60 95.05
GDC 93.47 95.62 95.98
GLC 92.36 94.53 95.02
GDLC 93.47 95.58 95.98
</table>
<tableCaption confidence="0.854886">
Table 6: Average cross-validation accuracy of
CRF-based predictions where G = char-n-gram, L
</tableCaption>
<bodyText confidence="0.619809666666667">
= length feature, D = single dictionary-based la-
bels (baseline system) and C = capitalization fea-
tures
</bodyText>
<equation confidence="0.260724">
/univ
SVM Output: ..../univ the/en
</equation>
<bodyText confidence="0.969246846153846">
movie/en for/en which/en i/en can/en
die/bn for/en /univ
We now investigate whether contextual informa-
tion can correct the mis-classified tags.
Although named entities and word-level code
mixing are excluded from evaluation, when deal-
ing with context it is important to consider named
entity and word-level code mixing during training
because these may contain some important infor-
mation. We include these tokens in the training
data for our context-based experiments, labelling
them as other. The presence of this new label may
affect the prediction for a language token during
classification and sequence labelling. To avoid this
situation, a 4-way (bn, hi, en, univ) backoff classi-
fier is trained separately on English, Hindi, Ben-
gali and universal tokens. During evaluation of
any context-based system we discard named en-
tity and word-level code mixing from the predic-
tion of that system. If any of the remaining tokens
is predicted as other we back off to the decision
of the 4-way classifier for that token. For the CRF
experiments (Section 5.3.1), the backoff classifier
is a CRF system, and, for the SVM experiments
(Section 5.3.2), the backoff classifier is an SVM
system.
</bodyText>
<subsectionHeader confidence="0.817204">
5.3.1 Conditional Random Fields (CRF)
</subsectionHeader>
<bodyText confidence="0.998586857142857">
As our goal is to apply contextual clues, we first
employ Conditional Random Fields (CRF), an ap-
proach which takes history into account in pre-
dicting the optimal sequence of labels. We em-
ploy a linear chain CRF with an increasing or-
der (Order-0, Order-1 and Order-2) with 200 it-
erations for different feature combinations (used
</bodyText>
<table confidence="0.988232">
GDLC: 95.98%
GLC: 95.02% GDL: 95.94% GDC: 95.98%
GL: 95.41% GC: 95.05% GD: 95.98%
G: 95.36%
Figure 2: CRF Order-2 results: cube visualisation
G = char-n-gram, L = binary length features, D
= presence in dictionaries and C = capitalization
features
Context Accuracy (%)
GDLC + P1 94.66
GDLC + P2 94.55
GDLC + N1 94.53
GDLC + N2 94.37
GDLC + P1N1 95.14
GDLC + P2N2 94.55
</table>
<tableCaption confidence="0.978753">
Table 7: Average cross-validation accuracy of
</tableCaption>
<bodyText confidence="0.96451875">
SVM (GDLC) context-based runs, where P-i =
previous i word(s) , N-i = next i word(s)
in SVM-based runs). However, we observe that
accuracy of CRF based runs decreases when bi-
narized length features (see Section 5.2 and dic-
tionary features (a feature for each dictionary) are
involved. Hence, we use the dictionary-based pre-
dictions of the baseline system to generate a single
dictionary feature for each token and only the raw
length value of a token instead of binarized length
features. The results are presented in Table 6 and
the second order results are visualized in Fig. 2.
As expected, the performance increases as the
order increases from zero to one and two. The use
of a single dictionary feature is also helpful. The
results for GDC, GDLC, and GD based runs are
almost similar (95.98%). However, we choose the
GDC system because it performed slightly better
(95.989%) than the GDLC (95.983%) and the GD
(95.983%) systems.
</bodyText>
<subsectionHeader confidence="0.964679">
5.3.2 SVM with Context
</subsectionHeader>
<bodyText confidence="0.999119">
We also add contextual clues to our SVM classi-
fier. To obtain contextual information we include
the previous and next two words as features in
the SVM-GDLC-based run.4 All possible com-
</bodyText>
<footnote confidence="0.9673865">
4We also experimented with extracting all GDLC features
for the context words but this did not help.
</footnote>
<page confidence="0.994714">
20
</page>
<bodyText confidence="0.9996774">
binations are considered during experiments (Ta-
ble 7). After C parameter optimization, the best
cross-validation accuracy is found for the PiNi
(one word previous and one word next) run with
C=0.125 (95.14%).
</bodyText>
<subsectionHeader confidence="0.99925">
5.4 Test Set Results
</subsectionHeader>
<bodyText confidence="0.9999506">
We apply our best dictionary-based system, our
best SVM system (with and without context) and
our best CRF system to the held-out test set. The
results are shown in Table 8. Our best result is
achieved using the CRF model (95.76%).
</bodyText>
<subsectionHeader confidence="0.950228">
5.5 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999904647058824">
Manual error analysis shows the limitations of
these systems. The word-level classifier without
contextual clues does not perform well with Hindi
data. The number of Hindi tokens is quite low.
Only 2.4% (4,658) of total tokens of the training
data are Hindi, out of which 55.36% are bilin-
gually ambiguous and 29.51% are tri-lingually
ambiguous tokens. Individual word-level systems
often fail to assign proper labels to ambiguous
words, but adding context information helps to
overcome this problem. Considering the previ-
ous example of die, both context-based SVM and
CRF systems classify it properly. Though the final
system CRF-GDC performs well, it also has some
limitations, failing to identify the language for the
tokens which appear very frequently in three lan-
guages (e.g. are, na, pic).
</bodyText>
<sectionHeader confidence="0.996154" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999858">
We have presented an initial study on automatic
language identification with Indian language code
mixing from social media communication. We
described our dataset of Bengali-Hindi-English
Facebook comments and we presented the results
of our word-level classification experiments on
this dataset. Our experimental results lead us to
conclude that character n-gram features are useful
for this task, contextual information is also impor-
tant and that information from dictionaries can be
effectively incorporated as features.
In the future we plan to apply the techniques
and feature sets that we used in these experiments
to other datasets. We have already started this by
applying variants of the systems presented here to
the Nepali-English and Spanish-English datasets
which were introduced as part of the 2014 code
mixing shared task (Solorio et al., 2014; Barman
et al., 2014).
We did not include word-level code mixing in
our experiments – in our future experiments we
will explore ways to identify and segment this type
of code mixing. It will be also important to find the
best way to handle inclusions since there is a fine
line between word borrowing and code mixing.
</bodyText>
<sectionHeader confidence="0.994734" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999608727272727">
This research is supported by the Science Founda-
tion Ireland (Grant 12/CE/I2267) as part of CNGL
(www.cngl.ie) at Dublin City University. The
authors wish to acknowledge the DJEI/DES/SFI/
HEA for the provision of computational facili-
ties and support. Our special thanks to Soumik
Mandal from Jadavpur University, India for co-
ordinating the annotation task. We also thank the
administrator of JUMatrimonial and the 11 Face-
book users who agreed that we can use their posts
for their support and permission.
</bodyText>
<sectionHeader confidence="0.998064" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998281133333333">
Beatrice Alex. 2008. Automatic detection of English
inclusions in mixed-lingual data with an application
to parsing. Ph.D. thesis, School of Informatics, The
University of Edinburgh, Edinburgh, UK.
Guy Aston and Lou Burnard. 1998. The BNC hand-
book: exploring the British National Corpus with
SARA. Capstone.
Peter Auer. 2013. Code-Switching in Conversation:
Language, Interaction and Identity. Routledge.
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the mat-
ter. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 229–237. Association for Computational Lin-
guistics.
Utsab Barman, Joachim Wagner, Grzegorz Chrupała,
and Jennifer Foster. 2014. DCU-UVT: Word-
level language classification with code-mixed data.
In Proceedings of the First Workshop on Com-
putational Approaches to Code-Switching. EMNLP
2014, Conference on Empirical Methods in Natural
Language Processing, Doha, Qatar. Association for
Computational Linguistics.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65–74. Associ-
ation for Computational Linguistics.
</reference>
<page confidence="0.996507">
21
</page>
<table confidence="0.999716142857143">
System Precision (%) Recall (%) Accuracy
(%)
EN BN HI UNIV EN BN HI UNIV
Baseline (Dictionary) 92.67 90.73 80.64 99.67 92.28 94.63 43.47 94.99 93.64
SVM-GDLC 92.49 94.89 80.31 99.34 96.23 94.28 44.92 97.07 95.21
SVM-P1N1 93.51 95.56 83.18 99.42 96.63 95.23 55.94 96.95 95.52
CRF-GDC 94.77 94.88 91.86 99.34 95.65 96.22 55.65 97.73 95.76
</table>
<tableCaption confidence="0.6388742">
Table 8: Test set results for Baseline (Dictionary), SVM-GDLC, SVM-P1N1 and CRF-GDC
MS C´ardenas-Claros and N Isharyanti. 2009. Code-
switching and code-mixing in internet chatting:
Between’yes,”ya,’and’si’-a case study. The Jalt Call
Journal, 5(3):67–78.
</tableCaption>
<reference confidence="0.998297707865168">
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Theo Pavlidis,
editor, Proceedings of SDAIR-94, Third Annual
Symposium on Document Analysis and Information
Retrieval, pages 161–175.
Jean-Marc Dewaele. 2010. Emotions in Multiple Lan-
guages. Palgrave Macmillan.
Anik Dey and Pascale Fung. 2014. A Hindi-
English code-switching corpus. In Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation (LREC’14), pages 2410–
2413, Reykjavik, Iceland. European Language Re-
sources Association (ELRA).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Paulseph-John Farrugia. 2004. TTS pre-processing is-
sues for mixed language support. In Proceedings
of CSAW’04, the second Computer Science Annual
Workshop, pages 36–41. Department of Computer
Science &amp; A.I., University of Malta.
E Mark Gold. 1967. Language identification in the
limit. Information and control, 10(5):447–474.
Thomas Gottron and Nedim Lipka. 2010. A compar-
ison of language identification approaches on short,
query-style texts. In Advances in Information Re-
trieval, pages 611–614. Springer.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDDExplor. Newsl., 11(1):10–18.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically constructing a normalisation dictio-
nary for microblogs. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 421–432. Association
for Computational Linguistics.
Taofik Hidayat. 2012. An analysis of code switch-
ing used by facebookers: a case study in a
social network site. Student essay for the
study programme “Pendidikan Bahasa Ing-
gris” (English Education) at STKIP Siliwangi
Bandung, Indonesia, http://publikasi.
stkipsiliwangi.ac.id/files/2012/
10/08220227-taofik-hidayat.pdf.
Lichan Hong, Gregorio Convertino, and Ed H. Chi.
2011. Language matters in twitter: A large scale
study. In Proceedings of the Fifth International
AAAI Conference on Weblogs and Social Media
(ICWSM-11), pages 518–521, Barcelona, Spain. As-
sociation for the Advancement of Artificial Intelli-
gence.
Chih-Wei Hsu, Chih-Chung Chang, and Chih-
Jen Lin. 2010. A practical guide to sup-
port vector classification. Technical re-
port. Department of Computer Science, Na-
tional Taiwan University, Taiwan, https:
//www.cs.sfu.ca/people/Faculty/
teaching/726/spring11/svmguide.pdf.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proc. of the 5th edition of the Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2006), pages 485–488, Genoa, Italy.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In J. Horeck´y, ed-
itor, Proceedings of the 9th conference on Compu-
tational linguistics - Volume 1 (COLING’82), pages
145–150. Academia Praha, North-Holland Publish-
ing Company.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1110–
1119, Atlanta, Georgia. Association for Computa-
tional Linguistics.
David C. S. Li. 2000. Cantonese-English code-
switching research in Hong Kong: a Y2K review.
World Englishes, 19(3):305–322.
Gideon S. Mann and Andrew McCallum. 2008.
Generalized expectation criteria for semi-supervised
learning of conditional random fields. In Proceed-
ings of ACL-08: HLT, pages 870–878, Columbus,
Ohio. Association for Computational Linguistics.
</reference>
<page confidence="0.972659">
22
</page>
<reference confidence="0.999714425742574">
Gideon S. Mann and Andrew McCallum. 2010.
Generalized expectation criteria for semi-supervised
learning with weakly labeled data. The Journal of
Machine Learning Research, 11:955–984.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Lesley Milroy and Pieter Muysken, editors. 1995. One
speaker, two languages: Cross-disciplinary per-
spectives on code-switching. Cambridge University
Press.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312–
320, Atlanta, Georgia, USA. Association for Com-
putational Linguistics.
Dong Nguyen and A. Seza Do˘gru¨oz. 2013. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2013), pages 857–862, Seattle,
Washington, USA. Association for Computational
Linguistics.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2013), pages 380–390, Atlanta, Geor-
gia. Association for Computational Linguistics.
Mike Rosner and Paulseph-John Farrugia. 2007. A
tagging algorithm for mixed language identifica-
tion in a noisy domain. In INTERSPEECH-2007,
8th Annual Conference of the International Speech
Communication Association, pages 190–193. ISCA
Archive.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392–397, Sofia, Bulgaria.
Association for Computational Linguistics.
Hong Ka San. 2009. Chinese-English code-switching
in blogs by Macao young people. Master’s the-
sis, The University of Edinburgh, Edinburgh, UK.
http://hdl.handle.net/1842/3626.
Latisha Asmaak Shafie and Surina Nayan. 2013.
Languages, code-switching practice and primary
functions of facebook among university students.
Study in English Language Teaching, 1(1):187–
199. http://www.scholink.org/ojs/
index.php/selt.
Thamar Solorio and Yang Liu. 2008a. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973–981. Association for
Computational Linguistics.
Thamar Solorio and Yang Liu. 2008b. Part-of-speech
tagging for English-Spanish code-switched text. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1051–
1060. Association for Computational Linguistics.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Doha, Qatar. Associ-
ation for Computational Linguistics.
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster, and
Lamia Tounsi. 2014. DCU: Aspect-based polarity
classification for SemEval task 4. In Proceedings
of the International Workshop on Semantic Evalu-
ation (SemEval-2014), pages 392–397, Dublin, Ire-
land. Association for Computational Linguistics.
Jochen Weiner, Ngoc Thang Vu, Dominic Telaar, Flo-
rian Metze, Tanja Schultz, Dau-Cheng Lyu, Eng-
Siong Chng, and Haizhou Li. 2012. Integration
of language identification into a recognition system
for spoken conversations containing code-switches.
In Proceedings of the 3rd Workshop on Spoken Lan-
guage Technologies for Under-resourced Languages
(SLTU’12), Cape Town, South Africa. International
Research Center MICA.
Hiroshi Yamaguchi and Kumiko Tanaka-Ishii. 2012.
Text segmentation by language using minimum de-
scription length. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 969–978.
Association for Computational Linguistics.
</reference>
<page confidence="0.998933">
23
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.381678">
<title confidence="0.902336">Code Mixing: A Challenge for Language Identification in the Language of Social Media</title>
<author confidence="0.573691">Amitava Joachim Wagner Barman</author>
<author confidence="0.573691">Jennifer</author>
<affiliation confidence="0.914334">CNGL Centre for Global Intelligent Content, National Centre for Language School of Computing, Dublin City University, Dublin, of Computer Science and University of North Texas, Denton, Texas,</affiliation>
<email confidence="0.999894">amitava.das@unt.edu</email>
<abstract confidence="0.991341481481482">In social media communication, multilingual speakers often switch between languages, and, in such an environment, automatic language identification becomes both a necessary and challenging task. In this paper, we describe our work in progress on the problem of automatic language identification for the language of social media. We describe a new dataset that we are in the process of creating, which contains Facebook posts and comments that exhibit code mixing between Bengali, English and Hindi. We also present some preliminary word-level language identification experiments using this dataset. Different techniques are employed, including a simple unsupervised dictionary-based approach, supervised word-level classification with and without contextual clues, and sequence labelling using Conditional Random Fields. We find that the dictionary-based approach is surpassed by supervised classification and sequence labelling, and that it is important to take contextual clues into consideration.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Beatrice Alex</author>
</authors>
<title>Automatic detection of English inclusions in mixed-lingual data with an application to parsing.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Informatics, The University of Edinburgh,</institution>
<location>Edinburgh, UK.</location>
<contexts>
<context position="4636" citStr="Alex, 2008" startWordPosition="703" endWordPosition="704">nd Related Work The problem of language identification has been investigated for half a century (Gold, 1967) and that of computational analysis of code switching for several decades (Joshi, 1982), but there has been less work on automatic language identification for multilingual code-mixed texts. Before turning to that topic, we first briefly survey studies on the general characteristics of code mixing. Code mixing is a normal, natural product of bilingual and multilingual language use. Significant studies of the phenomenon can be found in the linguistics literature (Milroy and Muysken, 1995; Alex, 2008; Auer, 2013). These works mainly discuss the sociological and conversational necessities behind code mixing as well as its linguistic nature. Scholars distinguish between intersentence, intra-sentence and intra-word code mixing. Several researchers have investigated the reasons for and the types of code mixing. Initial studies on Chinese-English code mixing in Hong Kong (Li, 2000) and Macao (San, 2009) indicated that mainly linguistic motivations were triggering the code mixing in those highly bilingual societies. Hidayat (2012) showed that Facebook users tend to mainly use inter-sentential s</context>
</contexts>
<marker>Alex, 2008</marker>
<rawString>Beatrice Alex. 2008. Automatic detection of English inclusions in mixed-lingual data with an application to parsing. Ph.D. thesis, School of Informatics, The University of Edinburgh, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy Aston</author>
<author>Lou Burnard</author>
</authors>
<date>1998</date>
<booktitle>The BNC handbook: exploring the British National Corpus with SARA.</booktitle>
<publisher>Capstone.</publisher>
<contexts>
<context position="19414" citStr="Aston and Burnard, 1998" startWordPosition="3062" endWordPosition="3066">ing some words share the same surface form across two and sometimes across three languages. As an example, to is a word in the three languages: it has occurred 1209 times as English, 715 times as Bengali and 55 times as Hindi in our data. The meaning of these words (e.g. to, bolo, die) are different in different languages. This phenomenon is perhaps exacerbated by the trend towards short and noisy spelling in SMC. 4 Tools and Resources We have used the following resources and tools in our experiment. Dictionaries 1. British National Corpus (BNC): We compile a word frequency list from the BNC (Aston and Burnard, 1998). 2. SEMEVAL 2013 Twitter Corpus (SemevalTwitter): To cope with the language of social media we use the SEMEVAL 2013 (Nakov et al., 2013) training data for the Twitter sentiment analysis task. This data comes from a popular social media site and hence is likely to reflect the linguistic properties of SMC. 3. Lexical Normalization List (LexNormList): Spelling variation is a well-known phenomenon in SMC. We use a lexical normalization dictionary created by Han et al. (2012) to handle the different spelling variations in our data. Machine Learning Toolkits 1. WEKA: We use the Weka toolkit (Hall e</context>
</contexts>
<marker>Aston, Burnard, 1998</marker>
<rawString>Guy Aston and Lou Burnard. 1998. The BNC handbook: exploring the British National Corpus with SARA. Capstone.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Auer</author>
</authors>
<title>Code-Switching in Conversation: Language, Interaction and Identity.</title>
<date>2013</date>
<publisher>Routledge.</publisher>
<contexts>
<context position="4649" citStr="Auer, 2013" startWordPosition="705" endWordPosition="706">ork The problem of language identification has been investigated for half a century (Gold, 1967) and that of computational analysis of code switching for several decades (Joshi, 1982), but there has been less work on automatic language identification for multilingual code-mixed texts. Before turning to that topic, we first briefly survey studies on the general characteristics of code mixing. Code mixing is a normal, natural product of bilingual and multilingual language use. Significant studies of the phenomenon can be found in the linguistics literature (Milroy and Muysken, 1995; Alex, 2008; Auer, 2013). These works mainly discuss the sociological and conversational necessities behind code mixing as well as its linguistic nature. Scholars distinguish between intersentence, intra-sentence and intra-word code mixing. Several researchers have investigated the reasons for and the types of code mixing. Initial studies on Chinese-English code mixing in Hong Kong (Li, 2000) and Macao (San, 2009) indicated that mainly linguistic motivations were triggering the code mixing in those highly bilingual societies. Hidayat (2012) showed that Facebook users tend to mainly use inter-sentential switching over</context>
</contexts>
<marker>Auer, 2013</marker>
<rawString>Peter Auer. 2013. Code-Switching in Conversation: Language, Interaction and Identity. Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Marco Lui</author>
</authors>
<title>Language identification: The long and the short of the matter.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>229--237</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3141" citStr="Baldwin and Lui, 2010" startWordPosition="472" endWordPosition="475">xing with a Romanalphabet language. This can be especially observed in South-East Asia and in the Indian subcontinent. The following is a code mixing comment taken from a Facebook group of Indian university students: Original: Yaar tu to, GOD hain. tui JU te ki korchis? Hail u man! Translation: Buddy you are GOD. What are you doing in JU? Hail u man! This comment is written in three languages: English, Hindi (italics), and Bengali (boldface). For Bengali and Hindi, phonetic typing has been used. We follow in the footsteps of recent work on language identification for SMC (Hughes et al., 2006; Baldwin and Lui, 2010; Bergsma et al., 2012), focusing specifically on the problem of word-level language identification for code mixing SMC. Our corpus for this task is collected from Facebook and contains instances of Bengali(BN)- English(EN)-Hindi(HI) code mixing. The paper is organized as follows: in Section 2, we review related research in the area of code mixing and language identification; in Section 3, we describe our code mixing corpus, the data it13 Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 13–23, October 25, 2014, Doha, Qatar. c�2014 Association for Computati</context>
</contexts>
<marker>Baldwin, Lui, 2010</marker>
<rawString>Timothy Baldwin and Marco Lui. 2010. Language identification: The long and the short of the matter. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 229–237. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Utsab Barman</author>
<author>Joachim Wagner</author>
<author>Grzegorz Chrupała</author>
<author>Jennifer Foster</author>
</authors>
<title>DCU-UVT: Wordlevel language classification with code-mixed data.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Workshop on Computational Approaches to Code-Switching. EMNLP 2014, Conference on Empirical Methods in Natural Language Processing,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar.</location>
<contexts>
<context position="34065" citStr="Barman et al., 2014" startWordPosition="5455" endWordPosition="5458">ication experiments on this dataset. Our experimental results lead us to conclude that character n-gram features are useful for this task, contextual information is also important and that information from dictionaries can be effectively incorporated as features. In the future we plan to apply the techniques and feature sets that we used in these experiments to other datasets. We have already started this by applying variants of the systems presented here to the Nepali-English and Spanish-English datasets which were introduced as part of the 2014 code mixing shared task (Solorio et al., 2014; Barman et al., 2014). We did not include word-level code mixing in our experiments – in our future experiments we will explore ways to identify and segment this type of code mixing. It will be also important to find the best way to handle inclusions since there is a fine line between word borrowing and code mixing. Acknowledgements This research is supported by the Science Foundation Ireland (Grant 12/CE/I2267) as part of CNGL (www.cngl.ie) at Dublin City University. The authors wish to acknowledge the DJEI/DES/SFI/ HEA for the provision of computational facilities and support. Our special thanks to Soumik Mandal</context>
</contexts>
<marker>Barman, Wagner, Chrupała, Foster, 2014</marker>
<rawString>Utsab Barman, Joachim Wagner, Grzegorz Chrupała, and Jennifer Foster. 2014. DCU-UVT: Wordlevel language classification with code-mixed data. In Proceedings of the First Workshop on Computational Approaches to Code-Switching. EMNLP 2014, Conference on Empirical Methods in Natural Language Processing, Doha, Qatar. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Paul McNamee</author>
<author>Mossaab Bagdouri</author>
<author>Clayton Fink</author>
<author>Theresa Wilson</author>
</authors>
<title>Language identification for creating language-specific Twitter collections.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Language in Social Media,</booktitle>
<pages>65--74</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3164" citStr="Bergsma et al., 2012" startWordPosition="476" endWordPosition="479">et language. This can be especially observed in South-East Asia and in the Indian subcontinent. The following is a code mixing comment taken from a Facebook group of Indian university students: Original: Yaar tu to, GOD hain. tui JU te ki korchis? Hail u man! Translation: Buddy you are GOD. What are you doing in JU? Hail u man! This comment is written in three languages: English, Hindi (italics), and Bengali (boldface). For Bengali and Hindi, phonetic typing has been used. We follow in the footsteps of recent work on language identification for SMC (Hughes et al., 2006; Baldwin and Lui, 2010; Bergsma et al., 2012), focusing specifically on the problem of word-level language identification for code mixing SMC. Our corpus for this task is collected from Facebook and contains instances of Bengali(BN)- English(EN)-Hindi(HI) code mixing. The paper is organized as follows: in Section 2, we review related research in the area of code mixing and language identification; in Section 3, we describe our code mixing corpus, the data it13 Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 13–23, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics self a</context>
</contexts>
<marker>Bergsma, McNamee, Bagdouri, Fink, Wilson, 2012</marker>
<rawString>Shane Bergsma, Paul McNamee, Mossaab Bagdouri, Clayton Fink, and Theresa Wilson. 2012. Language identification for creating language-specific Twitter collections. In Proceedings of the Second Workshop on Language in Social Media, pages 65–74. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Cavnar</author>
<author>John M Trenkle</author>
</authors>
<title>Ngram-based text categorization.</title>
<date>1994</date>
<booktitle>Proceedings of SDAIR-94, Third Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>161--175</pages>
<editor>In Theo Pavlidis, editor,</editor>
<contexts>
<context position="25151" citStr="Cavnar and Trenkle, 1994" startWordPosition="3996" endWordPosition="4000">We find that using a normalized frequency is helpful 18 and that a combination of LexNormList and Training Data dictionaries is suited best for our data. Hence, we consider this as our baseline language identification system. Dictionary Accuracy(%) BNC 80.09 SemevalTwitter 77.61 LexNormList 79.86 Training Data 90.21 LexNormList+TrainingData (Baseline) 93.12 Table 4: Average cross-validation accuracy of dictionary-based detection 5.2 Word-Level Classification without Contextual Clues The following feature types are employed: 1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers. Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature, we follow our previous work (Rubino et al., 2013; Wagner et al., 2014) and create multiple features for length using a decision tree (J48). We use length as the only feature t</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>William B. Cavnar and John M. Trenkle. 1994. Ngram-based text categorization. In Theo Pavlidis, editor, Proceedings of SDAIR-94, Third Annual Symposium on Document Analysis and Information Retrieval, pages 161–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Marc Dewaele</author>
</authors>
<title>Emotions in Multiple Languages.</title>
<date>2010</date>
<publisher>Palgrave Macmillan.</publisher>
<contexts>
<context position="5634" citStr="Dewaele (2010)" startWordPosition="860" endWordPosition="861"> and Macao (San, 2009) indicated that mainly linguistic motivations were triggering the code mixing in those highly bilingual societies. Hidayat (2012) showed that Facebook users tend to mainly use inter-sentential switching over intrasentential, and report that 45% of the switching was instigated by real lexical needs, 40% was used for talking about a particular topic, and 5% for content clarification. The predominance of intersentential code mixing in social media text was also noted in the study by San (2009), which compared the mixing in blog posts to that in the spoken language in Macao. Dewaele (2010) claims that ‘strong emotional arousal’ increases the frequency of code mixing. Dey and Fung (2014) present a speech corpus of English-Hindi code mixing in student interviews and analyse the motivations for code mixing and in what grammatical contexts code mixing occurs. Turning to the work on automatic analysis of code mixing, there have been some studies on detecting code mixing in speech (Solorio and Liu, 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other s</context>
</contexts>
<marker>Dewaele, 2010</marker>
<rawString>Jean-Marc Dewaele. 2010. Emotions in Multiple Languages. Palgrave Macmillan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anik Dey</author>
<author>Pascale Fung</author>
</authors>
<title>A HindiEnglish code-switching corpus.</title>
<date>2014</date>
<journal>Reykjavik, Iceland. European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14),</booktitle>
<pages>2410--2413</pages>
<contexts>
<context position="5733" citStr="Dey and Fung (2014)" startWordPosition="873" endWordPosition="876">ixing in those highly bilingual societies. Hidayat (2012) showed that Facebook users tend to mainly use inter-sentential switching over intrasentential, and report that 45% of the switching was instigated by real lexical needs, 40% was used for talking about a particular topic, and 5% for content clarification. The predominance of intersentential code mixing in social media text was also noted in the study by San (2009), which compared the mixing in blog posts to that in the spoken language in Macao. Dewaele (2010) claims that ‘strong emotional arousal’ increases the frequency of code mixing. Dey and Fung (2014) present a speech corpus of English-Hindi code mixing in student interviews and analyse the motivations for code mixing and in what grammatical contexts code mixing occurs. Turning to the work on automatic analysis of code mixing, there have been some studies on detecting code mixing in speech (Solorio and Liu, 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval </context>
</contexts>
<marker>Dey, Fung, 2014</marker>
<rawString>Anik Dey and Pascale Fung. 2014. A HindiEnglish code-switching corpus. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 2410– 2413, Reykjavik, Iceland. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="20283" citStr="Fan et al., 2008" startWordPosition="3210" endWordPosition="3213">is likely to reflect the linguistic properties of SMC. 3. Lexical Normalization List (LexNormList): Spelling variation is a well-known phenomenon in SMC. We use a lexical normalization dictionary created by Han et al. (2012) to handle the different spelling variations in our data. Machine Learning Toolkits 1. WEKA: We use the Weka toolkit (Hall et al., 2009) for our experiments in decision tree training. 2. MALLET: CRF learning is applied using the MALLET toolkit (McCallum, 2002). 17 3. Liblinear: We apply Support Vector Machine (SVM) learning with a linear kernel using the Liblinear package (Fan et al., 2008). NLP Tools For data tokenization we used the CMU Tweet-Tokenizer (Owoputi et al., 2013). 5 Experiments Since our training data is entirely labelled at the word-level by human annotators, we address the word-level language identification task in a fully supervised way. Out of the total data, 15% is set aside as a blind test set, while the rest is employed in our experiments through a 5-fold cross-validation setup. There is a substantial amount of token overlap between the cross-validation data and the test set – 88% of total EN tokens, 86% of total Bengali tokens and 57% of total Hindi tokens </context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paulseph-John Farrugia</author>
</authors>
<title>TTS pre-processing issues for mixed language support.</title>
<date>2004</date>
<booktitle>In Proceedings of CSAW’04, the second Computer Science Annual Workshop,</booktitle>
<pages>36--41</pages>
<institution>Department of Computer Science &amp; A.I., University of Malta.</institution>
<contexts>
<context position="6399" citStr="Farrugia, 2004" startWordPosition="986" endWordPosition="987">g in student interviews and analyse the motivations for code mixing and in what grammatical contexts code mixing occurs. Turning to the work on automatic analysis of code mixing, there have been some studies on detecting code mixing in speech (Solorio and Liu, 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval queries (Gottron and Lipka, 2010) and SMS messages (Farrugia, 2004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language identification. A dataset of 30 languages has been used in their work. They explore several language identification approaches, including a Naive Bayes classifier for individual word-level classification and sequence labelling with Conditional Random Fields trained with Generalized Expectation criteria (Man</context>
</contexts>
<marker>Farrugia, 2004</marker>
<rawString>Paulseph-John Farrugia. 2004. TTS pre-processing issues for mixed language support. In Proceedings of CSAW’04, the second Computer Science Annual Workshop, pages 36–41. Department of Computer Science &amp; A.I., University of Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Mark Gold</author>
</authors>
<date>1967</date>
<booktitle>Language identification in the limit. Information and control,</booktitle>
<pages>10--5</pages>
<contexts>
<context position="4134" citStr="Gold, 1967" startWordPosition="626" endWordPosition="627">Section 3, we describe our code mixing corpus, the data it13 Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 13–23, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics self and the annotation process; in Section 4, we list the tools and resources which we use in our language identification experiments, described in Section 5. Finally, in Section 6, we conclude and provide suggestions for future research on this topic. 2 Background and Related Work The problem of language identification has been investigated for half a century (Gold, 1967) and that of computational analysis of code switching for several decades (Joshi, 1982), but there has been less work on automatic language identification for multilingual code-mixed texts. Before turning to that topic, we first briefly survey studies on the general characteristics of code mixing. Code mixing is a normal, natural product of bilingual and multilingual language use. Significant studies of the phenomenon can be found in the linguistics literature (Milroy and Muysken, 1995; Alex, 2008; Auer, 2013). These works mainly discuss the sociological and conversational necessities behind c</context>
</contexts>
<marker>Gold, 1967</marker>
<rawString>E Mark Gold. 1967. Language identification in the limit. Information and control, 10(5):447–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Gottron</author>
<author>Nedim Lipka</author>
</authors>
<title>A comparison of language identification approaches on short, query-style texts.</title>
<date>2010</date>
<booktitle>In Advances in Information Retrieval,</booktitle>
<pages>611--614</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6366" citStr="Gottron and Lipka, 2010" startWordPosition="979" endWordPosition="982">a speech corpus of English-Hindi code mixing in student interviews and analyse the motivations for code mixing and in what grammatical contexts code mixing occurs. Turning to the work on automatic analysis of code mixing, there have been some studies on detecting code mixing in speech (Solorio and Liu, 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval queries (Gottron and Lipka, 2010) and SMS messages (Farrugia, 2004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language identification. A dataset of 30 languages has been used in their work. They explore several language identification approaches, including a Naive Bayes classifier for individual word-level classification and sequence labelling with Conditional Random Fields trained with Gene</context>
</contexts>
<marker>Gottron, Lipka, 2010</marker>
<rawString>Thomas Gottron and Nedim Lipka. 2010. A comparison of language identification approaches on short, query-style texts. In Advances in Information Retrieval, pages 611–614. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDDExplor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="20026" citStr="Hall et al., 2009" startWordPosition="3167" endWordPosition="3170"> 1998). 2. SEMEVAL 2013 Twitter Corpus (SemevalTwitter): To cope with the language of social media we use the SEMEVAL 2013 (Nakov et al., 2013) training data for the Twitter sentiment analysis task. This data comes from a popular social media site and hence is likely to reflect the linguistic properties of SMC. 3. Lexical Normalization List (LexNormList): Spelling variation is a well-known phenomenon in SMC. We use a lexical normalization dictionary created by Han et al. (2012) to handle the different spelling variations in our data. Machine Learning Toolkits 1. WEKA: We use the Weka toolkit (Hall et al., 2009) for our experiments in decision tree training. 2. MALLET: CRF learning is applied using the MALLET toolkit (McCallum, 2002). 17 3. Liblinear: We apply Support Vector Machine (SVM) learning with a linear kernel using the Liblinear package (Fan et al., 2008). NLP Tools For data tokenization we used the CMU Tweet-Tokenizer (Owoputi et al., 2013). 5 Experiments Since our training data is entirely labelled at the word-level by human annotators, we address the word-level language identification task in a fully supervised way. Out of the total data, 15% is set aside as a blind test set, while the re</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: An update. SIGKDDExplor. Newsl., 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatically constructing a normalisation dictionary for microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>421--432</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19890" citStr="Han et al. (2012)" startWordPosition="3143" endWordPosition="3146">ols in our experiment. Dictionaries 1. British National Corpus (BNC): We compile a word frequency list from the BNC (Aston and Burnard, 1998). 2. SEMEVAL 2013 Twitter Corpus (SemevalTwitter): To cope with the language of social media we use the SEMEVAL 2013 (Nakov et al., 2013) training data for the Twitter sentiment analysis task. This data comes from a popular social media site and hence is likely to reflect the linguistic properties of SMC. 3. Lexical Normalization List (LexNormList): Spelling variation is a well-known phenomenon in SMC. We use a lexical normalization dictionary created by Han et al. (2012) to handle the different spelling variations in our data. Machine Learning Toolkits 1. WEKA: We use the Weka toolkit (Hall et al., 2009) for our experiments in decision tree training. 2. MALLET: CRF learning is applied using the MALLET toolkit (McCallum, 2002). 17 3. Liblinear: We apply Support Vector Machine (SVM) learning with a linear kernel using the Liblinear package (Fan et al., 2008). NLP Tools For data tokenization we used the CMU Tweet-Tokenizer (Owoputi et al., 2013). 5 Experiments Since our training data is entirely labelled at the word-level by human annotators, we address the word</context>
</contexts>
<marker>Han, Cook, Baldwin, 2012</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically constructing a normalisation dictionary for microblogs. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 421–432. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taofik Hidayat</author>
</authors>
<title>An analysis of code switching used by facebookers: a case study in a social network site. Student essay for the study programme “Pendidikan Bahasa Inggris”</title>
<date>2012</date>
<booktitle>(English Education) at STKIP Siliwangi Bandung, Indonesia, http://publikasi. stkipsiliwangi.ac.id/files/2012/</booktitle>
<pages>10--08220227</pages>
<contexts>
<context position="5171" citStr="Hidayat (2012)" startWordPosition="782" endWordPosition="783">an be found in the linguistics literature (Milroy and Muysken, 1995; Alex, 2008; Auer, 2013). These works mainly discuss the sociological and conversational necessities behind code mixing as well as its linguistic nature. Scholars distinguish between intersentence, intra-sentence and intra-word code mixing. Several researchers have investigated the reasons for and the types of code mixing. Initial studies on Chinese-English code mixing in Hong Kong (Li, 2000) and Macao (San, 2009) indicated that mainly linguistic motivations were triggering the code mixing in those highly bilingual societies. Hidayat (2012) showed that Facebook users tend to mainly use inter-sentential switching over intrasentential, and report that 45% of the switching was instigated by real lexical needs, 40% was used for talking about a particular topic, and 5% for content clarification. The predominance of intersentential code mixing in social media text was also noted in the study by San (2009), which compared the mixing in blog posts to that in the spoken language in Macao. Dewaele (2010) claims that ‘strong emotional arousal’ increases the frequency of code mixing. Dey and Fung (2014) present a speech corpus of English-Hi</context>
<context position="16226" citStr="Hidayat, 2012" startWordPosition="2546" endWordPosition="2547">present in our corpus. This means that English is the main language involved in the code mixing. Statistics of Different Tags Tags En Bn Hi Mixd Univ Undef sent 5,370 8,523 354 204 746 15 frag 288 213 40 0 6 0 incl 7,377 262 94 0 1,032 1 wlcm 477 Name Entity 3,602 Acronym 691 Table 1: Tag-level statistics Word-Level Tag Count EN 66,298 BN 79,899 HI 3,440 WLCM 633 NE 5,233 ACRO 715 UNIV 39,291 UNDEF 61 Table 2: Word-level statistics 3.2.1 Code Mixing Types In our corpus, inter- and intra-sentential code mixing are more prominent than word-level code mixing, which is similar to the findings of (Hidayat, 2012) . Our corpus contains every type of code mixing in English, Hindi and Bengali viz. inter/intra sentential and word-level as described in the previous section. Some examples of different types of code mixing in our corpus are presented below. 16 1. Inter-Sentential: [sent-lang=“hi”] Itna izzat diye aapne mujhe !!! [/sent] [sent-lang=“en”] Tears ofjoy. :’( :’( [/sent] 2. Intra-Sentential: [sent-lang=“bn”] [incl-lang=“en”] by d way [/incl] ei [frag-lang=“en”] my craving arms shall forever remain empty.. never hold u close.. [/frag] line ta baddo [incl-lang=“en”] cheezy [/incl] :P ;) [/sent] 3. W</context>
</contexts>
<marker>Hidayat, 2012</marker>
<rawString>Taofik Hidayat. 2012. An analysis of code switching used by facebookers: a case study in a social network site. Student essay for the study programme “Pendidikan Bahasa Inggris” (English Education) at STKIP Siliwangi Bandung, Indonesia, http://publikasi. stkipsiliwangi.ac.id/files/2012/ 10/08220227-taofik-hidayat.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lichan Hong</author>
<author>Gregorio Convertino</author>
<author>Ed H Chi</author>
</authors>
<title>Language matters in twitter: A large scale study.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media (ICWSM-11),</booktitle>
<pages>518--521</pages>
<location>Barcelona,</location>
<contexts>
<context position="1751" citStr="Hong et al. (2011)" startWordPosition="244" endWordPosition="247">ry-based approach, supervised word-level classification with and without contextual clues, and sequence labelling using Conditional Random Fields. We find that the dictionary-based approach is surpassed by supervised classification and sequence labelling, and that it is important to take contextual clues into consideration. 1 Introduction Automatic processing and understanding of Social Media Content (SMC) is currently attracting much attention from the Natural Language Processing research community. Although English is still by far the most popular language in SMC, its dominance is receding. Hong et al. (2011), for example, applied an automatic language detection algorithm to over 62 million tweets to identify the top 10 most popular languages on Twitter. They found that only half of the tweets were in English. Moreover, mixing multiple languages together (code mixing) is a popular trend in social media users from language-dense areas (C´ardenas-Claros and Isharyanti, 2009; Shafie and Nayan, 2013). In a scenario where speakers switch between languages within a conversation, sentence or even word, the task of automatic language identification becomes increasingly important to facilitate further proc</context>
</contexts>
<marker>Hong, Convertino, Chi, 2011</marker>
<rawString>Lichan Hong, Gregorio Convertino, and Ed H. Chi. 2011. Language matters in twitter: A large scale study. In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media (ICWSM-11), pages 518–521, Barcelona, Spain. Association for the Advancement of Artificial Intelligence.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Chih-Wei Hsu</author>
</authors>
<title>Chih-Chung Chang, and Chih-</title>
<marker>Hsu, </marker>
<rawString>Chih-Wei Hsu, Chih-Chung Chang, and Chih-</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jen Lin</author>
</authors>
<title>A practical guide to support vector classification.</title>
<date>2010</date>
<tech>Technical report.</tech>
<institution>Department of Computer Science, National Taiwan University, Taiwan,</institution>
<note>https: //www.cs.sfu.ca/people/Faculty/ teaching/726/spring11/svmguide.pdf.</note>
<marker>Lin, 2010</marker>
<rawString>Jen Lin. 2010. A practical guide to support vector classification. Technical report. Department of Computer Science, National Taiwan University, Taiwan, https: //www.cs.sfu.ca/people/Faculty/ teaching/726/spring11/svmguide.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baden Hughes</author>
<author>Timothy Baldwin</author>
<author>Steven Bird</author>
<author>Jeremy Nicholson</author>
<author>Andrew MacKinlay</author>
</authors>
<title>Reconsidering language identification for written language resources.</title>
<date>2006</date>
<booktitle>In Proc. of the 5th edition of the International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>485--488</pages>
<location>Genoa, Italy.</location>
<contexts>
<context position="3118" citStr="Hughes et al., 2006" startWordPosition="468" endWordPosition="471">likelihood of code mixing with a Romanalphabet language. This can be especially observed in South-East Asia and in the Indian subcontinent. The following is a code mixing comment taken from a Facebook group of Indian university students: Original: Yaar tu to, GOD hain. tui JU te ki korchis? Hail u man! Translation: Buddy you are GOD. What are you doing in JU? Hail u man! This comment is written in three languages: English, Hindi (italics), and Bengali (boldface). For Bengali and Hindi, phonetic typing has been used. We follow in the footsteps of recent work on language identification for SMC (Hughes et al., 2006; Baldwin and Lui, 2010; Bergsma et al., 2012), focusing specifically on the problem of word-level language identification for code mixing SMC. Our corpus for this task is collected from Facebook and contains instances of Bengali(BN)- English(EN)-Hindi(HI) code mixing. The paper is organized as follows: in Section 2, we review related research in the area of code mixing and language identification; in Section 3, we describe our code mixing corpus, the data it13 Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 13–23, October 25, 2014, Doha, Qatar. c�2014 As</context>
</contexts>
<marker>Hughes, Baldwin, Bird, Nicholson, MacKinlay, 2006</marker>
<rawString>Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy Nicholson, and Andrew MacKinlay. 2006. Reconsidering language identification for written language resources. In Proc. of the 5th edition of the International Conference on Language Resources and Evaluation (LREC 2006), pages 485–488, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Processing of sentences with intra-sentential code-switching.</title>
<date>1982</date>
<booktitle>Proceedings of the 9th conference on Computational linguistics - Volume</booktitle>
<volume>1</volume>
<pages>145--150</pages>
<editor>In J. Horeck´y, editor,</editor>
<publisher>Academia Praha, North-Holland Publishing Company.</publisher>
<contexts>
<context position="4221" citStr="Joshi, 1982" startWordPosition="639" endWordPosition="640">Workshop on Computational Approaches to Code Switching, pages 13–23, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics self and the annotation process; in Section 4, we list the tools and resources which we use in our language identification experiments, described in Section 5. Finally, in Section 6, we conclude and provide suggestions for future research on this topic. 2 Background and Related Work The problem of language identification has been investigated for half a century (Gold, 1967) and that of computational analysis of code switching for several decades (Joshi, 1982), but there has been less work on automatic language identification for multilingual code-mixed texts. Before turning to that topic, we first briefly survey studies on the general characteristics of code mixing. Code mixing is a normal, natural product of bilingual and multilingual language use. Significant studies of the phenomenon can be found in the linguistics literature (Milroy and Muysken, 1995; Alex, 2008; Auer, 2013). These works mainly discuss the sociological and conversational necessities behind code mixing as well as its linguistic nature. Scholars distinguish between intersentence</context>
</contexts>
<marker>Joshi, 1982</marker>
<rawString>Aravind K. Joshi. 1982. Processing of sentences with intra-sentential code-switching. In J. Horeck´y, editor, Proceedings of the 9th conference on Computational linguistics - Volume 1 (COLING’82), pages 145–150. Academia Praha, North-Holland Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben King</author>
<author>Steven Abney</author>
</authors>
<title>Labeling the languages of words in mixed-language documents using weakly supervised methods.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1110--1119</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta,</location>
<contexts>
<context position="6623" citStr="King and Abney (2013)" startWordPosition="1014" endWordPosition="1017">ing code mixing in speech (Solorio and Liu, 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval queries (Gottron and Lipka, 2010) and SMS messages (Farrugia, 2004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language identification. A dataset of 30 languages has been used in their work. They explore several language identification approaches, including a Naive Bayes classifier for individual word-level classification and sequence labelling with Conditional Random Fields trained with Generalized Expectation criteria (Mann and McCallum, 2008; Mann and McCallum, 2010), which achieved the highest scores. Another very recent work on this topic is (Nguyen and Do˘gru¨oz, 2013). They report on language identification experiments performed on Turki</context>
<context position="25275" citStr="King and Abney (2013)" startWordPosition="4018" endWordPosition="4021">suited best for our data. Hence, we consider this as our baseline language identification system. Dictionary Accuracy(%) BNC 80.09 SemevalTwitter 77.61 LexNormList 79.86 Training Data 90.21 LexNormList+TrainingData (Baseline) 93.12 Table 4: Average cross-validation accuracy of dictionary-based detection 5.2 Word-Level Classification without Contextual Clues The following feature types are employed: 1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers. Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature, we follow our previous work (Rubino et al., 2013; Wagner et al., 2014) and create multiple features for length using a decision tree (J48). We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization</context>
</contexts>
<marker>King, Abney, 2013</marker>
<rawString>Ben King and Steven Abney. 2013. Labeling the languages of words in mixed-language documents using weakly supervised methods. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1110– 1119, Atlanta, Georgia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David C S Li</author>
</authors>
<title>Cantonese-English codeswitching research in Hong Kong: a Y2K review. World Englishes,</title>
<date>2000</date>
<contexts>
<context position="5020" citStr="Li, 2000" startWordPosition="761" endWordPosition="762">stics of code mixing. Code mixing is a normal, natural product of bilingual and multilingual language use. Significant studies of the phenomenon can be found in the linguistics literature (Milroy and Muysken, 1995; Alex, 2008; Auer, 2013). These works mainly discuss the sociological and conversational necessities behind code mixing as well as its linguistic nature. Scholars distinguish between intersentence, intra-sentence and intra-word code mixing. Several researchers have investigated the reasons for and the types of code mixing. Initial studies on Chinese-English code mixing in Hong Kong (Li, 2000) and Macao (San, 2009) indicated that mainly linguistic motivations were triggering the code mixing in those highly bilingual societies. Hidayat (2012) showed that Facebook users tend to mainly use inter-sentential switching over intrasentential, and report that 45% of the switching was instigated by real lexical needs, 40% was used for talking about a particular topic, and 5% for content clarification. The predominance of intersentential code mixing in social media text was also noted in the study by San (2009), which compared the mixing in blog posts to that in the spoken language in Macao. </context>
</contexts>
<marker>Li, 2000</marker>
<rawString>David C. S. Li. 2000. Cantonese-English codeswitching research in Hong Kong: a Y2K review. World Englishes, 19(3):305–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Generalized expectation criteria for semi-supervised learning of conditional random fields.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>870--878</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="7019" citStr="Mann and McCallum, 2008" startWordPosition="1067" endWordPosition="1070">004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language identification. A dataset of 30 languages has been used in their work. They explore several language identification approaches, including a Naive Bayes classifier for individual word-level classification and sequence labelling with Conditional Random Fields trained with Generalized Expectation criteria (Mann and McCallum, 2008; Mann and McCallum, 2010), which achieved the highest scores. Another very recent work on this topic is (Nguyen and Do˘gru¨oz, 2013). They report on language identification experiments performed on Turkish and Dutch forum data. Experiments have been carried out using language models, dictionaries, logistic regression classification and Conditional Random Fields. They find that language models are more robust than dictionaries and that contextual information is helpful for the task. 3 Corpus Acquisition Taking into account the claim that code mixing is frequent among speakers who are multiling</context>
</contexts>
<marker>Mann, McCallum, 2008</marker>
<rawString>Gideon S. Mann and Andrew McCallum. 2008. Generalized expectation criteria for semi-supervised learning of conditional random fields. In Proceedings of ACL-08: HLT, pages 870–878, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Generalized expectation criteria for semi-supervised learning with weakly labeled data.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--955</pages>
<contexts>
<context position="7045" citStr="Mann and McCallum, 2010" startWordPosition="1071" endWordPosition="1075"> 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language identification. A dataset of 30 languages has been used in their work. They explore several language identification approaches, including a Naive Bayes classifier for individual word-level classification and sequence labelling with Conditional Random Fields trained with Generalized Expectation criteria (Mann and McCallum, 2008; Mann and McCallum, 2010), which achieved the highest scores. Another very recent work on this topic is (Nguyen and Do˘gru¨oz, 2013). They report on language identification experiments performed on Turkish and Dutch forum data. Experiments have been carried out using language models, dictionaries, logistic regression classification and Conditional Random Fields. They find that language models are more robust than dictionaries and that contextual information is helpful for the task. 3 Corpus Acquisition Taking into account the claim that code mixing is frequent among speakers who are multilingual and younger in age (C´</context>
</contexts>
<marker>Mann, McCallum, 2010</marker>
<rawString>Gideon S. Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. The Journal of Machine Learning Research, 11:955–984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="20150" citStr="McCallum, 2002" startWordPosition="3188" endWordPosition="3189">ov et al., 2013) training data for the Twitter sentiment analysis task. This data comes from a popular social media site and hence is likely to reflect the linguistic properties of SMC. 3. Lexical Normalization List (LexNormList): Spelling variation is a well-known phenomenon in SMC. We use a lexical normalization dictionary created by Han et al. (2012) to handle the different spelling variations in our data. Machine Learning Toolkits 1. WEKA: We use the Weka toolkit (Hall et al., 2009) for our experiments in decision tree training. 2. MALLET: CRF learning is applied using the MALLET toolkit (McCallum, 2002). 17 3. Liblinear: We apply Support Vector Machine (SVM) learning with a linear kernel using the Liblinear package (Fan et al., 2008). NLP Tools For data tokenization we used the CMU Tweet-Tokenizer (Owoputi et al., 2013). 5 Experiments Since our training data is entirely labelled at the word-level by human annotators, we address the word-level language identification task in a fully supervised way. Out of the total data, 15% is set aside as a blind test set, while the rest is employed in our experiments through a 5-fold cross-validation setup. There is a substantial amount of token overlap be</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<title>One speaker, two languages: Cross-disciplinary perspectives on code-switching.</title>
<date>1995</date>
<editor>Lesley Milroy and Pieter Muysken, editors.</editor>
<publisher>Cambridge University Press.</publisher>
<marker>1995</marker>
<rawString>Lesley Milroy and Pieter Muysken, editors. 1995. One speaker, two languages: Cross-disciplinary perspectives on code-switching. Cambridge University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="19551" citStr="Nakov et al., 2013" startWordPosition="3087" endWordPosition="3090">: it has occurred 1209 times as English, 715 times as Bengali and 55 times as Hindi in our data. The meaning of these words (e.g. to, bolo, die) are different in different languages. This phenomenon is perhaps exacerbated by the trend towards short and noisy spelling in SMC. 4 Tools and Resources We have used the following resources and tools in our experiment. Dictionaries 1. British National Corpus (BNC): We compile a word frequency list from the BNC (Aston and Burnard, 1998). 2. SEMEVAL 2013 Twitter Corpus (SemevalTwitter): To cope with the language of social media we use the SEMEVAL 2013 (Nakov et al., 2013) training data for the Twitter sentiment analysis task. This data comes from a popular social media site and hence is likely to reflect the linguistic properties of SMC. 3. Lexical Normalization List (LexNormList): Spelling variation is a well-known phenomenon in SMC. We use a lexical normalization dictionary created by Han et al. (2012) to handle the different spelling variations in our data. Machine Learning Toolkits 1. WEKA: We use the Weka toolkit (Hall et al., 2009) for our experiments in decision tree training. 2. MALLET: CRF learning is applied using the MALLET toolkit (McCallum, 2002).</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 312– 320, Atlanta, Georgia, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Nguyen</author>
<author>A Seza Do˘gru¨oz</author>
</authors>
<title>Word level language identification in online multilingual communication.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013),</booktitle>
<pages>857--862</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA.</location>
<marker>Nguyen, Do˘gru¨oz, 2013</marker>
<rawString>Dong Nguyen and A. Seza Do˘gru¨oz. 2013. Word level language identification in online multilingual communication. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), pages 857–862, Seattle, Washington, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT</booktitle>
<pages>380--390</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta,</location>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013), pages 380–390, Atlanta, Georgia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Rosner</author>
<author>Paulseph-John Farrugia</author>
</authors>
<title>A tagging algorithm for mixed language identification in a noisy domain.</title>
<date>2007</date>
<booktitle>In INTERSPEECH-2007, 8th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>190--193</pages>
<publisher>ISCA Archive.</publisher>
<contexts>
<context position="6427" citStr="Rosner and Farrugia, 2007" startWordPosition="988" endWordPosition="991">erviews and analyse the motivations for code mixing and in what grammatical contexts code mixing occurs. Turning to the work on automatic analysis of code mixing, there have been some studies on detecting code mixing in speech (Solorio and Liu, 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval queries (Gottron and Lipka, 2010) and SMS messages (Farrugia, 2004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language identification. A dataset of 30 languages has been used in their work. They explore several language identification approaches, including a Naive Bayes classifier for individual word-level classification and sequence labelling with Conditional Random Fields trained with Generalized Expectation criteria (Mann and McCallum, 2008; Mann a</context>
</contexts>
<marker>Rosner, Farrugia, 2007</marker>
<rawString>Mike Rosner and Paulseph-John Farrugia. 2007. A tagging algorithm for mixed language identification in a noisy domain. In INTERSPEECH-2007, 8th Annual Conference of the International Speech Communication Association, pages 190–193. ISCA Archive.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Rubino</author>
<author>Joachim Wagner</author>
<author>Jennifer Foster</author>
<author>Johann Roturier</author>
<author>Rasoul Samad Zadeh Kaljahi</author>
<author>Fred Hollowood</author>
</authors>
<title>DCU-Symantec at the WMT 2013 quality estimation shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>392--397</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<contexts>
<context position="25624" citStr="Rubino et al., 2013" startWordPosition="4081" endWordPosition="4084">al Clues The following feature types are employed: 1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers. Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature, we follow our previous work (Rubino et al., 2013; Wagner et al., 2014) and create multiple features for length using a decision tree (J48). We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized. We perform experiments with an SVM classifier (linear kernel) for different combination of these features.3 Parameter optimizations (C rang</context>
</contexts>
<marker>Rubino, Wagner, Foster, Roturier, Kaljahi, Hollowood, 2013</marker>
<rawString>Raphael Rubino, Joachim Wagner, Jennifer Foster, Johann Roturier, Rasoul Samad Zadeh Kaljahi, and Fred Hollowood. 2013. DCU-Symantec at the WMT 2013 quality estimation shared task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 392–397, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Ka San</author>
</authors>
<title>Chinese-English code-switching in blogs by Macao young people. Master’s thesis,</title>
<date>2009</date>
<institution>The University of Edinburgh,</institution>
<location>Edinburgh, UK. http://hdl.handle.net/1842/3626.</location>
<contexts>
<context position="5042" citStr="San, 2009" startWordPosition="765" endWordPosition="766"> Code mixing is a normal, natural product of bilingual and multilingual language use. Significant studies of the phenomenon can be found in the linguistics literature (Milroy and Muysken, 1995; Alex, 2008; Auer, 2013). These works mainly discuss the sociological and conversational necessities behind code mixing as well as its linguistic nature. Scholars distinguish between intersentence, intra-sentence and intra-word code mixing. Several researchers have investigated the reasons for and the types of code mixing. Initial studies on Chinese-English code mixing in Hong Kong (Li, 2000) and Macao (San, 2009) indicated that mainly linguistic motivations were triggering the code mixing in those highly bilingual societies. Hidayat (2012) showed that Facebook users tend to mainly use inter-sentential switching over intrasentential, and report that 45% of the switching was instigated by real lexical needs, 40% was used for talking about a particular topic, and 5% for content clarification. The predominance of intersentential code mixing in social media text was also noted in the study by San (2009), which compared the mixing in blog posts to that in the spoken language in Macao. Dewaele (2010) claims </context>
</contexts>
<marker>San, 2009</marker>
<rawString>Hong Ka San. 2009. Chinese-English code-switching in blogs by Macao young people. Master’s thesis, The University of Edinburgh, Edinburgh, UK. http://hdl.handle.net/1842/3626.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Latisha Asmaak Shafie</author>
<author>Surina Nayan</author>
</authors>
<title>Languages, code-switching practice and primary functions of facebook among university students.</title>
<date>2013</date>
<journal>Study in English Language Teaching,</journal>
<volume>1</volume>
<issue>1</issue>
<note>http://www.scholink.org/ojs/ index.php/selt.</note>
<contexts>
<context position="2146" citStr="Shafie and Nayan, 2013" startWordPosition="307" endWordPosition="310">Media Content (SMC) is currently attracting much attention from the Natural Language Processing research community. Although English is still by far the most popular language in SMC, its dominance is receding. Hong et al. (2011), for example, applied an automatic language detection algorithm to over 62 million tweets to identify the top 10 most popular languages on Twitter. They found that only half of the tweets were in English. Moreover, mixing multiple languages together (code mixing) is a popular trend in social media users from language-dense areas (C´ardenas-Claros and Isharyanti, 2009; Shafie and Nayan, 2013). In a scenario where speakers switch between languages within a conversation, sentence or even word, the task of automatic language identification becomes increasingly important to facilitate further processing. Speakers whose first language uses a nonRoman alphabet write using the Roman alphabet for convenience (phonetic typing) which increases the likelihood of code mixing with a Romanalphabet language. This can be especially observed in South-East Asia and in the Indian subcontinent. The following is a code mixing comment taken from a Facebook group of Indian university students: Original:</context>
</contexts>
<marker>Shafie, Nayan, 2013</marker>
<rawString>Latisha Asmaak Shafie and Surina Nayan. 2013. Languages, code-switching practice and primary functions of facebook among university students. Study in English Language Teaching, 1(1):187– 199. http://www.scholink.org/ojs/ index.php/selt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thamar Solorio</author>
<author>Yang Liu</author>
</authors>
<title>Learning to predict code-switching points.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>973--981</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6050" citStr="Solorio and Liu, 2008" startWordPosition="925" endWordPosition="928">e predominance of intersentential code mixing in social media text was also noted in the study by San (2009), which compared the mixing in blog posts to that in the spoken language in Macao. Dewaele (2010) claims that ‘strong emotional arousal’ increases the frequency of code mixing. Dey and Fung (2014) present a speech corpus of English-Hindi code mixing in student interviews and analyse the motivations for code mixing and in what grammatical contexts code mixing occurs. Turning to the work on automatic analysis of code mixing, there have been some studies on detecting code mixing in speech (Solorio and Liu, 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval queries (Gottron and Lipka, 2010) and SMS messages (Farrugia, 2004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervise</context>
</contexts>
<marker>Solorio, Liu, 2008</marker>
<rawString>Thamar Solorio and Yang Liu. 2008a. Learning to predict code-switching points. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 973–981. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thamar Solorio</author>
<author>Yang Liu</author>
</authors>
<title>Part-of-speech tagging for English-Spanish code-switched text.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1051--1060</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="6050" citStr="Solorio and Liu, 2008" startWordPosition="925" endWordPosition="928">e predominance of intersentential code mixing in social media text was also noted in the study by San (2009), which compared the mixing in blog posts to that in the spoken language in Macao. Dewaele (2010) claims that ‘strong emotional arousal’ increases the frequency of code mixing. Dey and Fung (2014) present a speech corpus of English-Hindi code mixing in student interviews and analyse the motivations for code mixing and in what grammatical contexts code mixing occurs. Turning to the work on automatic analysis of code mixing, there have been some studies on detecting code mixing in speech (Solorio and Liu, 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval queries (Gottron and Lipka, 2010) and SMS messages (Farrugia, 2004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervise</context>
</contexts>
<marker>Solorio, Liu, 2008</marker>
<rawString>Thamar Solorio and Yang Liu. 2008b. Part-of-speech tagging for English-Spanish code-switched text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1051– 1060. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Thamar Solorio</author>
<author>Elizabeth Blair</author>
<author>Suraj Maharjan</author>
<author>Steve Bethard</author>
<author>Mona Diab</author>
<author>Mahmoud Gonheim</author>
<author>Abdelati Hawwari</author>
<author>Fahad AlGhamdi</author>
<author>Julia Hirshberg</author>
<author>Alison Chang</author>
<author>Pascale Fung</author>
</authors>
<title>Overview for the first shared task on language identification in codeswitched data.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Workshop on Computational Approaches to Code-Switching. EMNLP 2014, Conference on Empirical Methods in Natural Language Processing,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar.</location>
<contexts>
<context position="34043" citStr="Solorio et al., 2014" startWordPosition="5451" endWordPosition="5454">our word-level classification experiments on this dataset. Our experimental results lead us to conclude that character n-gram features are useful for this task, contextual information is also important and that information from dictionaries can be effectively incorporated as features. In the future we plan to apply the techniques and feature sets that we used in these experiments to other datasets. We have already started this by applying variants of the systems presented here to the Nepali-English and Spanish-English datasets which were introduced as part of the 2014 code mixing shared task (Solorio et al., 2014; Barman et al., 2014). We did not include word-level code mixing in our experiments – in our future experiments we will explore ways to identify and segment this type of code mixing. It will be also important to find the best way to handle inclusions since there is a fine line between word borrowing and code mixing. Acknowledgements This research is supported by the Science Foundation Ireland (Grant 12/CE/I2267) as part of CNGL (www.cngl.ie) at Dublin City University. The authors wish to acknowledge the DJEI/DES/SFI/ HEA for the provision of computational facilities and support. Our special t</context>
</contexts>
<marker>Solorio, Blair, Maharjan, Bethard, Diab, Gonheim, Hawwari, AlGhamdi, Hirshberg, Chang, Fung, 2014</marker>
<rawString>Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve Bethard, Mona Diab, Mahmoud Gonheim, Abdelati Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison Chang, and Pascale Fung. 2014. Overview for the first shared task on language identification in codeswitched data. In Proceedings of the First Workshop on Computational Approaches to Code-Switching. EMNLP 2014, Conference on Empirical Methods in Natural Language Processing, Doha, Qatar. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Wagner</author>
<author>Piyush Arora</author>
<author>Santiago Cortes</author>
<author>Utsab Barman</author>
<author>Dasha Bogdanova</author>
<author>Jennifer Foster</author>
<author>Lamia Tounsi</author>
</authors>
<title>DCU: Aspect-based polarity classification for SemEval task 4.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<pages>392--397</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Dublin, Ireland.</location>
<contexts>
<context position="25646" citStr="Wagner et al., 2014" startWordPosition="4085" endWordPosition="4088">g feature types are employed: 1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers. Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature, we follow our previous work (Rubino et al., 2013; Wagner et al., 2014) and create multiple features for length using a decision tree (J48). We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized. We perform experiments with an SVM classifier (linear kernel) for different combination of these features.3 Parameter optimizations (C range 2-15 to 210) for SVM</context>
</contexts>
<marker>Wagner, Arora, Cortes, Barman, Bogdanova, Foster, Tounsi, 2014</marker>
<rawString>Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab Barman, Dasha Bogdanova, Jennifer Foster, and Lamia Tounsi. 2014. DCU: Aspect-based polarity classification for SemEval task 4. In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2014), pages 392–397, Dublin, Ireland. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jochen Weiner</author>
</authors>
<title>Ngoc Thang Vu, Dominic Telaar, Florian Metze, Tanja Schultz, Dau-Cheng Lyu, EngSiong Chng, and Haizhou Li.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop on Spoken Language Technologies for Under-resourced Languages (SLTU’12),</booktitle>
<institution>Cape Town, South Africa. International Research Center MICA.</institution>
<marker>Weiner, 2012</marker>
<rawString>Jochen Weiner, Ngoc Thang Vu, Dominic Telaar, Florian Metze, Tanja Schultz, Dau-Cheng Lyu, EngSiong Chng, and Haizhou Li. 2012. Integration of language identification into a recognition system for spoken conversations containing code-switches. In Proceedings of the 3rd Workshop on Spoken Language Technologies for Under-resourced Languages (SLTU’12), Cape Town, South Africa. International Research Center MICA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Yamaguchi</author>
<author>Kumiko Tanaka-Ishii</author>
</authors>
<title>Text segmentation by language using minimum description length.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>969--978</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6462" citStr="Yamaguchi and Tanaka-Ishii (2012)" startWordPosition="992" endWordPosition="995">vations for code mixing and in what grammatical contexts code mixing occurs. Turning to the work on automatic analysis of code mixing, there have been some studies on detecting code mixing in speech (Solorio and Liu, 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval queries (Gottron and Lipka, 2010) and SMS messages (Farrugia, 2004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language identification. A dataset of 30 languages has been used in their work. They explore several language identification approaches, including a Naive Bayes classifier for individual word-level classification and sequence labelling with Conditional Random Fields trained with Generalized Expectation criteria (Mann and McCallum, 2008; Mann and McCallum, 2010), which achieved </context>
</contexts>
<marker>Yamaguchi, Tanaka-Ishii, 2012</marker>
<rawString>Hiroshi Yamaguchi and Kumiko Tanaka-Ishii. 2012. Text segmentation by language using minimum description length. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 969–978. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
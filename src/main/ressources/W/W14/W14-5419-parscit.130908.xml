<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.215403">
<title confidence="0.994302">
Multi-layered Image Representation for Image Interpretation
</title>
<author confidence="0.99897">
Marina Ivasic-Kos, Miran Pobar, Ivo Ipsic
</author>
<affiliation confidence="0.9984105">
Department of Informatics
University of Rijeka
</affiliation>
<address confidence="0.434547">
R. Matejcic 2, Rijeka, Croatia
</address>
<email confidence="0.987551">
{marina, mpobar, ivoi}@uniri.hr
</email>
<sectionHeader confidence="0.993762" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984428571428">
In order to bridge the semantic gap between the visual context of an image and semantic con-
cepts people would use to interpret it, we propose a multi-layered image representation model
considering different amounts of knowledge needed for the interpretation of the image at each
layer. Interpretation results on different semantic layers of Corel images related to outdoor
scenes are presented and compared. Obtained results show positive correlation of precision and
recall with the abstract level of classes used for image annotation, i.e. more generalized classes
have achieved better results.
</bodyText>
<sectionHeader confidence="0.999266" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998602">
Image captions and surrounding text can facilitate the retrieval of images if they exist, but the vast ma-
jority of images are not annotated with words. A number of methods have been developed in recent
years to automatically annotate images with words that users might intuitively use when searching for
them. This problem is challenging because different people will most likely interpret the same image
with different words on different levels of abstraction. Used words reflect their knowledge about the
context of the image, experience, cultural background, etc.
On the other hand, annotation methods deal with visual features such as color, texture and shape
that can be extracted from raw image data, so the major goal is to bridge the semantic gap between the
available features and the interpretation of the images in the way humans do. The idea is to define an
image representation model that will reflect the semantic levels of words used in image interpretation.
</bodyText>
<sectionHeader confidence="0.943274" genericHeader="method">
2 Multi-layered Image Representation
</sectionHeader>
<bodyText confidence="0.992222368421053">
Among the oldest models of image interpretation is Shatford’s (1986) model that suggests image
content classification into general, specific and abstract. Eakins and Graham (2000) have defined three
semantic layers of image interpretation considering the context of image search. The first layer corre-
sponds to the presence of certain combinations of low-level features, the second to the types of objects
and the third to descriptions of events, activities, locations or emotions that one can associate with the
image.
We propose an image representation model that follows the simplified hierarchical model of (Hare
et al,. 2006) that captures the layers between the two extremes, the &amp;quot;raw&amp;quot; data of the image and its full
semantics. Such image representation includes the visual content of an image and the concepts used to
interpret it on different layers of image representation, Fig.1. The initial layer of representation of an
image is the layer V0, representing the raw image. The image is usually segmented (layer V1) using
methods for automatic image segmentation or into a grid. The low-level features are then extracted
from the image segments (layer V2).
The next four layers, MI1 to MI4, are related to different levels of semantic interpretation. The se-
mantics includes elementary classes - EC into which image segments are classified, classes that de-
scribe the scene - SC, generalization classes - GC and derived classes – DC, organized in a hierarchy
as shown in Fig 1.
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
</bodyText>
<page confidence="0.972267">
115
</page>
<note confidence="0.8651595">
Proceedings of the 25th International Conference on Computational Linguistics, pages 115–117,
Dublin, Ireland, August 23-29 2014.
</note>
<figureCaption confidence="0.998956">
Fig. 1. Layers of image representation in relation to the knowledge level
</figureCaption>
<bodyText confidence="0.99691">
Elementary classes correspond to objects that can be recognised in an image, like sky, water and rock
for image in Fig. 1. Scene classes represent the context of the whole image, like seaside, and can be
either directly obtained as a result of global classification of image features or inferred from the ele-
mentary classes. Generalization classes are defined as a generalization of scene classes, like scenery,
natural scene and outdoor scene. Between generalisation classes the aggregation or generalization rela-
tion is defined. Derived classes include abstract concepts that can be associated with an image, like
specific place such as the island of Cres, or emotion e.g. solitude.
The amount of knowledge required for segmentation and extraction of features in layers V1 and V2
is low, while the amount of knowledge required for interpreting the image in the semantic layers MI1
to MI4 increases. Most automatic image annotation methods are generative or discriminative models
(Zhang, et al., 2012) and work with image interpretation at layers MI1 and MI2. For image interpreta-
tion at layers MI3 and MI4 knowledge representation models and a reasoning engine are needed.
</bodyText>
<sectionHeader confidence="0.99924" genericHeader="method">
3 Experiment
</sectionHeader>
<bodyText confidence="0.9986055">
Our goal was to compare image interpretation results on different semantic layers. We have used a
part of the Corel image database related to outdoor scenes. The data set consisted of 500 images seg-
mented with the n-cuts algorithm. For each image segment a 16D feature vector was computed
based on CIE L*a*b* colour model and geometric properties (size, position, height, width and shape
of the area) of image segments (Duygulu et al., 2002). The segments were labeled with one of the 28
keywords related to natural and artificial objects such as &apos;airplane&apos;, &apos;bird&apos;, ‘lion’, ‘train’ etc. and back-
ground objects like &apos;ground&apos;, &apos;sky&apos;, ‘water’ etc. The keywords correspond to the elementary classes.
Some image segments were too small and couldn’t be labeled manually and were excluded from data.
The final data set used for the experiment consists of 3960 segments. The data was divided into
training (3160) and testing (800) subsets by a 10-fold cross validation with 20% of the observations
for the holdout cross-validation.
For image classification into elementary classes Bayesian classifier was used according to the maxi-
</bodyText>
<equation confidence="0.8602405">
mum posterior probability ( ):
(1)
</equation>
<bodyText confidence="0.9824855">
The conditional probability of a feature vector for the given elementary classes
and the prior probability are estimated according to data in the train-
ing set. It is taken into account that the evidence factor is a scale factor that does not influence the
classification results and is not calculated.
</bodyText>
<page confidence="0.997725">
116
</page>
<bodyText confidence="0.99959765">
The results of the image-segments classification are compared with the ground truth and the precision
and recall measures are calculated. The achieved average precision for classification of elementary
classes is 32.6% and average recall is 27.5%.
To predict concepts on layers MI2 and higher we have used the knowledge representation scheme
based on fuzzy Petri nets with an integrated fuzzy inference engine (Ribaric and Pavesic, 2009). The
fuzzy knowledge base contains the following main components: fuzzy spatial and co-occurrence rela-
tionships between elementary classes, fuzzy aggregation relationships between elementary classes and
scene classes, and fuzzy generalization relationships between scene classes and generalization classes.
The knowledge chunks considering spatial and co-occurrence relationships as well as aggregation rela-
tionships are computed from the training set. The training set is also used to estimated the truth of
these relationships. The hierarchical and generalization relationships are defined according to expert
knowledge and so is their truth. There were 15 scene classes defined in the knowledge base such as
Scene Lion, Scene Shuttle and Seaside and 13 generalization classes on different levels of abstraction,
such as Wild Cats, Wildlife, Natural Scene, and Man-Made Objects.
The obtained results show positive correlation of precision and recall with the abstract level of se-
mantic concepts used for image interpretation. For scene classes achieved results are little bit higher
than for elementary classes, with precision of 37% and 31% for recall. For generalised classes the ob-
tained results are significantly better, with precision of 52% and recall of 42%.
In Table 1, some positive examples of a multilayered image interpretation following the proposed
model are shown.
</bodyText>
<table confidence="0.998053375">
Image example:
Multi-layered MI1 ‘shuttle&apos; &apos;train&apos;, &apos;tracks &apos;, &apos;sky&apos; - &apos;grass&apos;, &apos;tiger&apos; &apos;water&apos;, &apos;sand&apos;, &apos;sky’, &apos;road&apos;
image
interpretation
MI2 &apos;Scene Shuttle&apos;, &apos;Scene Train&apos;, &apos;Scene Tiger&apos;, &apos;Seaside&apos;,
MI3 &apos;Vehicle&apos;, &apos;Man-Made &apos;Vehicle&apos;, &apos;Man-Made &apos;Wildcat&apos;, &apos;Wildlife&apos;, &apos;Natural &apos;Natural Scenes&apos;, &apos;Outdoor
Object&apos;, &apos;Outdoor’ Object&apos;, &apos;Outdoor’ Scenes&apos;, &apos;Outdoor Scene&apos; Scene&apos;
MI4 ‘Space’ ‘Transport’ - ‘Vacation’
</table>
<tableCaption confidence="0.999886">
Table 1. Examples of multilayered image interpretation
</tableCaption>
<sectionHeader confidence="0.997505" genericHeader="method">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99998625">
The suggested model of image representation corresponds to the interpretation of images that are in-
herent to humans. It involves image interpretation at different semantic levels. For each semantic level
we tested interpretation accuracy on outdoor scenes and positive correlations of precision and recall
with respect to the abstract level of semantic concepts were obtained.
</bodyText>
<sectionHeader confidence="0.994081" genericHeader="method">
Reference
</sectionHeader>
<reference confidence="0.999039272727273">
P. Duygulu, K. Barnard, J. F. G. de Freitas, and D. A. Forsyth. 2002. Object recognition as machine
translation: Learning a lexicon for a fixed image vocabulary, ECCV 2002, UK, pp. 97–112.
J. Eakins and M. Graham. 2000. Content-based image retrieval. Technical Report JTAP-039, JISC,
Institute for Image Data Research, University of Northumbria, Newcastle.
J. S. Hare, P. H. Lewis, P. G. B. Enser and C. J. Sandom. 2006. Mind the Gap: Another look at the
problem of the semantic gap in image retrieval. Multimedia Content Analysis, Management and
Retrieval, USA.
S. Ribaric and N. Pavesic. 2009. Inference Procedures for Fuzzy Knowledge Representation Scheme,
Applied Artificial Intelligence, vol. 23, 2009, pp. 16-43.
D. Zhang, M. M. Islam and G. Lu. 2012. A review on automatic image annotation techniques. Pattern
Recognition, 45(1), 346-362.
</reference>
<page confidence="0.998147">
117
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968339">
<title confidence="0.999902">Multi-layered Image Representation for Image Interpretation</title>
<author confidence="0.999587">Marina Ivasic-Kos</author>
<author confidence="0.999587">Miran Pobar</author>
<author confidence="0.999587">Ivo Ipsic</author>
<affiliation confidence="0.999586">Department of University of Rijeka</affiliation>
<address confidence="0.989082">R. Matejcic 2, Rijeka, Croatia</address>
<email confidence="0.986129">marina@uniri.hr</email>
<email confidence="0.986129">mpobar@uniri.hr</email>
<email confidence="0.986129">ivoi@uniri.hr</email>
<abstract confidence="0.999203625">In order to bridge the semantic gap between the visual context of an image and semantic concepts people would use to interpret it, we propose a multi-layered image representation model considering different amounts of knowledge needed for the interpretation of the image at each layer. Interpretation results on different semantic layers of Corel images related to outdoor scenes are presented and compared. Obtained results show positive correlation of precision and recall with the abstract level of classes used for image annotation, i.e. more generalized classes have achieved better results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Duygulu</author>
<author>K Barnard</author>
<author>J F G de Freitas</author>
<author>D A Forsyth</author>
</authors>
<title>Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary,</title>
<date>2002</date>
<journal>ECCV</journal>
<pages>97--112</pages>
<publisher>UK,</publisher>
<marker>Duygulu, Barnard, de Freitas, Forsyth, 2002</marker>
<rawString>P. Duygulu, K. Barnard, J. F. G. de Freitas, and D. A. Forsyth. 2002. Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary, ECCV 2002, UK, pp. 97–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eakins</author>
<author>M Graham</author>
</authors>
<title>Content-based image retrieval.</title>
<date>2000</date>
<tech>Technical Report JTAP-039,</tech>
<institution>JISC, Institute for Image Data Research, University of Northumbria,</institution>
<location>Newcastle.</location>
<contexts>
<context position="2023" citStr="Eakins and Graham (2000)" startWordPosition="302" endWordPosition="305"> On the other hand, annotation methods deal with visual features such as color, texture and shape that can be extracted from raw image data, so the major goal is to bridge the semantic gap between the available features and the interpretation of the images in the way humans do. The idea is to define an image representation model that will reflect the semantic levels of words used in image interpretation. 2 Multi-layered Image Representation Among the oldest models of image interpretation is Shatford’s (1986) model that suggests image content classification into general, specific and abstract. Eakins and Graham (2000) have defined three semantic layers of image interpretation considering the context of image search. The first layer corresponds to the presence of certain combinations of low-level features, the second to the types of objects and the third to descriptions of events, activities, locations or emotions that one can associate with the image. We propose an image representation model that follows the simplified hierarchical model of (Hare et al,. 2006) that captures the layers between the two extremes, the &amp;quot;raw&amp;quot; data of the image and its full semantics. Such image representation includes the visual</context>
</contexts>
<marker>Eakins, Graham, 2000</marker>
<rawString>J. Eakins and M. Graham. 2000. Content-based image retrieval. Technical Report JTAP-039, JISC, Institute for Image Data Research, University of Northumbria, Newcastle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Hare</author>
<author>P H Lewis</author>
<author>P G B Enser</author>
<author>C J Sandom</author>
</authors>
<title>Mind the Gap: Another look at the problem of the semantic gap in image retrieval. Multimedia Content Analysis, Management and Retrieval,</title>
<date>2006</date>
<location>USA.</location>
<marker>Hare, Lewis, Enser, Sandom, 2006</marker>
<rawString>J. S. Hare, P. H. Lewis, P. G. B. Enser and C. J. Sandom. 2006. Mind the Gap: Another look at the problem of the semantic gap in image retrieval. Multimedia Content Analysis, Management and Retrieval, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ribaric</author>
<author>N Pavesic</author>
</authors>
<title>Inference Procedures for Fuzzy Knowledge Representation Scheme,</title>
<date>2009</date>
<journal>Applied Artificial Intelligence,</journal>
<volume>23</volume>
<pages>16--43</pages>
<contexts>
<context position="6839" citStr="Ribaric and Pavesic, 2009" startWordPosition="1059" endWordPosition="1062">ted according to data in the training set. It is taken into account that the evidence factor is a scale factor that does not influence the classification results and is not calculated. 116 The results of the image-segments classification are compared with the ground truth and the precision and recall measures are calculated. The achieved average precision for classification of elementary classes is 32.6% and average recall is 27.5%. To predict concepts on layers MI2 and higher we have used the knowledge representation scheme based on fuzzy Petri nets with an integrated fuzzy inference engine (Ribaric and Pavesic, 2009). The fuzzy knowledge base contains the following main components: fuzzy spatial and co-occurrence relationships between elementary classes, fuzzy aggregation relationships between elementary classes and scene classes, and fuzzy generalization relationships between scene classes and generalization classes. The knowledge chunks considering spatial and co-occurrence relationships as well as aggregation relationships are computed from the training set. The training set is also used to estimated the truth of these relationships. The hierarchical and generalization relationships are defined accordi</context>
</contexts>
<marker>Ribaric, Pavesic, 2009</marker>
<rawString>S. Ribaric and N. Pavesic. 2009. Inference Procedures for Fuzzy Knowledge Representation Scheme, Applied Artificial Intelligence, vol. 23, 2009, pp. 16-43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhang</author>
<author>M M Islam</author>
<author>G Lu</author>
</authors>
<title>A review on automatic image annotation techniques.</title>
<date>2012</date>
<journal>Pattern Recognition,</journal>
<volume>45</volume>
<issue>1</issue>
<pages>346--362</pages>
<contexts>
<context position="4751" citStr="Zhang, et al., 2012" startWordPosition="726" endWordPosition="729"> scene classes, like scenery, natural scene and outdoor scene. Between generalisation classes the aggregation or generalization relation is defined. Derived classes include abstract concepts that can be associated with an image, like specific place such as the island of Cres, or emotion e.g. solitude. The amount of knowledge required for segmentation and extraction of features in layers V1 and V2 is low, while the amount of knowledge required for interpreting the image in the semantic layers MI1 to MI4 increases. Most automatic image annotation methods are generative or discriminative models (Zhang, et al., 2012) and work with image interpretation at layers MI1 and MI2. For image interpretation at layers MI3 and MI4 knowledge representation models and a reasoning engine are needed. 3 Experiment Our goal was to compare image interpretation results on different semantic layers. We have used a part of the Corel image database related to outdoor scenes. The data set consisted of 500 images segmented with the n-cuts algorithm. For each image segment a 16D feature vector was computed based on CIE L*a*b* colour model and geometric properties (size, position, height, width and shape of the area) of image segm</context>
</contexts>
<marker>Zhang, Islam, Lu, 2012</marker>
<rawString>D. Zhang, M. M. Islam and G. Lu. 2012. A review on automatic image annotation techniques. Pattern Recognition, 45(1), 346-362.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
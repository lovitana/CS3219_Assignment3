<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001286">
<title confidence="0.998267">
A Hybrid Disambiguation Measure for Inaccurate Cultural Heritage Data
</title>
<author confidence="0.996585">
Julia Efremova&apos;, Bijan Ranjbar-Sahraei2, Toon Calders&apos;,3
</author>
<affiliation confidence="0.869318333333333">
&apos;Eindhoven University of Technology, The Netherlands
2Maastricht University, The Netherlands
3Universit´e Libre de Bruxelles, Belgium
</affiliation>
<email confidence="0.9519045">
i.efremova@tue.nl, b.ranjbarsahraei@maastrichtuniversity.nl,
toon.calders@ulb.ac.be
</email>
<sectionHeader confidence="0.99315" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999958363636364">
Cultural heritage data is always associ-
ated with inaccurate information and dif-
ferent types of ambiguities. For instance,
names of persons, occupations or places
mentioned in historical documents are not
standardized and contain numerous varia-
tions. This article examines in detail var-
ious existing similarity functions and pro-
poses a hybrid technique for the following
task: among the list of possible names, oc-
cupations and places extracted from his-
torical documents, identify those that are
variations of the same person name, oc-
cupation and place respectively. The per-
formance of our method is evaluated on
three manually constructed datasets and
one public dataset in terms of precision,
recall and F-measure. The results demon-
strate that the hybrid technique outper-
forms current methods and allows to sig-
nificantly improve the quality of cultural
heritage data.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951267857143">
Inaccurate information and lack of common iden-
tifiers are problems encountered when combining
information from heterogeneous sources. There
are a number of reasons that can cause inaccu-
rate information such as spelling variations, ab-
breviations, translation from one language into an-
other and modifying long names into shorter ones.
Inaccurate information often occurs in many do-
mains, for example, during information extraction
from the Web or when attributing a publication to
its proper author. Inaccurate information is very
typical in cultural heritage data as well. In his-
torical documents a real person could be men-
tioned many times, for instance in civil certificates
such as birth, marriage and death certificates or
in property transfer records and tax declarations.
The name of the same person, his occupation and
the place in such documents varies a lot. When
working with such information, researchers have
to identify which person references mentioned in
different historical documents belong to the same
person entity. This problem has been referred to in
literature in many different ways but is best known
as entity resolution (ER), record linkage or dupli-
cate detection (Lisbach and Meyer, 2013; Chris-
ten, 2012; Bhattacharya and Getoor, 2007). The
process of ER in historical documents is always
accompanied by inaccurate information as well.
As an example, there are more than 100 variants of
the first name Jan, such as Johan, Johannes, Janis,
Jean or the profession musician in historical doc-
uments can be spelled as musikant, muzikant or
even muzikant bij de tiende afd. The latter means
the musician in the 10th department.
The past few decades have seen a large re-
search interest in the problem of inaccurate infor-
mation. As a result, a large number of methods
for comparing string has been developed. These
standard methods are called string similarity func-
tions. Some of those well known techniques are
character-based, token-based or based on phonetic
functions, for instance Levenshtein Edit distance,
Jaro Winkler distance, Monge Elkan distance,
Smith Waterman distance, Soundex and Double
Metaphone. (Elmagarmid et al., 2007; Navarro,
2001; Winkler, 1995). Each of the mentioned sim-
ilarity functions perform optimally for a particular
dataset domain. For example, the phonetic func-
tion Soundex works great for encoding names by
sound as it pronounced in English, but neverthe-
less sometimes it is also used to encode names
in other European languages. However, only lit-
tle work has been done in studying combinations
of similarity functions, and in their simultaneous
use for achieving more reliable results. Bilenko
(2003) in his work computes names similarity with
</bodyText>
<page confidence="0.993539">
47
</page>
<note confidence="0.929488">
Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 47–55,
Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999699393939394">
affine gaps to train the Support Vector Machines
classifier. Ristard and Yianilos (1998) designed
a learnable Levenshtein distance for solving the
string similarity problem. Tejada et al. (2002)
learned weights of different types of string trans-
formations.
In this paper we explore various traditional
string similarity functions for solving data ambi-
guities and also design a supervised hybrid tech-
nique. We carry out our experiments on three man-
ually constructed datasets: Dutch names, occupa-
tions and places, and also on one publicly avail-
able dataset of restaurants. The clarified function,
that will allow us to recognize difficult ambiguities
in textual fields, later will be incorporated into the
overall ER process for a large historical database.
The main contributions of this paper is a practical
study of existing techniques and the design and the
extensive analysis of a hybrid technique that allow
us to achieve a significant improvement in results.
The remainder of this paper is structured as fol-
lows. In Section 2 we begin by presenting typical
ambiguities in real-life cultural heritage data. in
Section 3 we give an overview of standard string
similarity functions. We describe the general hy-
brid approach in Section 4. In Section 5 we de-
scribe the prediction models that we use in the
hybrid approach. In Section 6 we provide details
about carrying out the experiments. In Section 7
we present an evaluation of the results. Section
8 offers a discussion about applying the designed
approach to real-world data. Concluding remarks
are given in Section 9.
</bodyText>
<sectionHeader confidence="0.975599" genericHeader="introduction">
2 A Real-Life Cultural Heritage Data
</sectionHeader>
<bodyText confidence="0.996620785714286">
In this paper we use historical documents such as
birth, marriage and death certificated provided by
Brabants Historisch Informatie Centrum (BHIC) 1
to extract most common person names, occupa-
tions and places. Civil certificates are belonging
to North Brabant, a province of the Netherlands, in
the period 1700 - 1920. To study the name ambi-
guity we used a subset of data consisting of 10000
randomly selected different documents.Then for
each name we obtain its standardized code in the
database of Meertens Instituut2 which has a large
collection of Dutch names and last names and
their typical variations. In the same way, in the
database of The Historical Sample of the Nether-
</bodyText>
<footnote confidence="0.9989015">
1http://www.bhic.nl
2http://www.meertens.knaw.nl/nvb/
</footnote>
<bodyText confidence="0.999526428571429">
lands (HSN) 3, for each occupation and place ex-
tracted from civil certificates where possible, we
obtain its standardized code (van Leeuwen et al.,
2002; Mandemakers et al., 2013). Historians have
spent a number of years for creating a database of
names, occupations and places variations. Using
such data gives us a unique opportunity to explore
typical variations in different domains and to de-
sign a robust technique which is able to deal with
them automatically.
The resulting Name variations dataset contains
2170 distinct names that correspond to 1326 stan-
dardized forms. Table 1 shows a typical example
of the constructed dataset of name variations.
</bodyText>
<figure confidence="0.496794333333333">
ref id Name name id
1 Eustachius 1
2 Statius 1
3 Stefan 2
4 Stephan 2
5 Stephanus 2
</figure>
<tableCaption confidence="0.99794">
Table 1: An example of a name variation dataset
</tableCaption>
<bodyText confidence="0.999377">
The second dataset of Occupations contains
1401 occupation records which belong to 1098
standardized occupations.
The third dataset of Places contains 1196 lo-
cations records belonging to 617 standardized
places.
</bodyText>
<sectionHeader confidence="0.993134" genericHeader="method">
3 Traditional Similarity Functions
</sectionHeader>
<bodyText confidence="0.9996862">
There are three main different types of string sim-
ilarity functions that can be used for variation
tasks, namely character-based, phonetic-based and
token-based. Each of them we investigate in detail
below.
</bodyText>
<subsectionHeader confidence="0.998625">
3.1 Character-Based Similarity
</subsectionHeader>
<bodyText confidence="0.999208307692308">
Character-based similarities operate on character
sequences and their composition which makes
them suitable for identifying imprecise names and
spelling errors. They compute the similarity be-
tween two strings as the difference between their
common characters. In this paper, we will con-
sider the Levenshtein edit distance (LE), Jaro (J),
Jaro Winkler (JW), Smith Waterman (SW), Smith
Waterman with Gotohs backtracing (GH), Needle-
man Wunch (NW) and Monge Elkan (ME) string
similarities (Elmagarmid et al., 2007; Christen,
2012; Naumann and Herschel, 2010). All of
them return a number between 0 and 1 inclusively,
</bodyText>
<footnote confidence="0.927726">
3http://www.iisg.nl/hsn/data/occupations.html
</footnote>
<page confidence="0.998722">
48
</page>
<bodyText confidence="0.99998725">
where the highest value when two names are ex-
actly the same. Table 3 shows an example of com-
puted character-based similarities for three name
pair-variants.
</bodyText>
<subsectionHeader confidence="0.999086">
3.2 Phonetic-Based Similarity
</subsectionHeader>
<bodyText confidence="0.999215235294118">
Phonetic similarity functions analyze the sounds
of the names being compared instead of their
spelling differences. For example, the two names
Stefan and Stephan barely differ phonetically, but
nevertheless they have different spellings. Pho-
netic functions encode every name with phonetic
keys based on a set of rules. For instance, some
algorithms ignore all vowels and compare only
the groups of consonants, other algorithms ana-
lyze consonant combinations and thier sound that
describe a large number of sounds. In this pa-
per, we analyze 4 phonetic functions: Soundex
(SN), Double Metaphone (DM), IBMAlphaCode
(IA) and New York State Identification and Intelli-
gence System (NY) (Christen, 2006). The Table 2
shows an example of applied phonetic keys to en-
code imprecise names.
</bodyText>
<table confidence="0.9989355">
Name SN DM IA NYSIIS
Stefan S315 STFN 00182 STAFAN
Stephan S315 STFN 00182 STAFPAN
Stephanus S3152 STFNS 00182 STAFPAN
</table>
<tableCaption confidence="0.999713">
Table 2: An example of phonetic keys
</tableCaption>
<subsectionHeader confidence="0.997339">
3.3 Token-Based Similarity
</subsectionHeader>
<bodyText confidence="0.99993895">
Token-based functions divide two strings into sets
of tokens s1 and s2, then they compute the in-
tersection between two sets based on the num-
ber of equal tokens. Some token-based functions,
for instance Dice similarity (DS), Jaccard coeffi-
cient(JS) and Cosine similarity (CS) (Cohen et al.,
2003), consider as a token the whole word in a
string. In our case most of the person names, lo-
cations and places are quite different and there are
only few intersections between token-words avail-
able. Another approach, a q-gram (QG) tokeniza-
tion (McNamee and Mayfield, 2004), divides a
string into smaller tokens of size q. QG calculates
the similarity between two strings by counting the
number of q-grams in common and dividing by the
number of q-grams in the longer string. In this pa-
per we consider bigrams (q = 2). For example, the
name ’stefan’ contains the bigrams ’st’, ’te’, ’ef’,
’fa’,’an’. An example of applied QG and JS simi-
larities is shown in Table 3.
</bodyText>
<table confidence="0.9327765">
two names LE J JW SW GH NW ME QG JS
(Stefan, Stephan) 0.71 0.85 0.89 0.58 0.57 0.79 0.57 0.5 0
(Stefan, Stephanus) 0.56 0.80 0.86 0.58 0.57 0.61 0.57 0.38 0
(Stephan, Stephanus) 0.78 0.93 0.97 1 1 0.78 1 0.75 0
</table>
<tableCaption confidence="0.9556775">
Table 3: An example of character and token-based
similarities
</tableCaption>
<subsectionHeader confidence="0.999183">
3.4 Exploration of Standard Methods
</subsectionHeader>
<bodyText confidence="0.99998375">
The goal of this paper is to investigate in how far
the terms variation task can be addressed by using
standard methods and improve the results by ap-
plying a hybrid technique. Fig. 1 shows for each
string similarity function the distribution between
two non-matching pairs of records on the one hand
and two matching pairs of records on the other for
different measures. The more discriminative the
measure is, the larger is the separation between the
distributions. However, in this figure, each of sim-
ilarity functions is considered independently and
can be expected to only perform well in certain
situations. Therefore, the goal of this paper is to
design an appropriate hybrid technique, which al-
lows to achieve better performance results by us-
ing a combination of traditional measures.
</bodyText>
<sectionHeader confidence="0.98932" genericHeader="method">
4 General Hybrid Approach
</sectionHeader>
<bodyText confidence="0.999952125">
In this article we propose a new hybrid approach
which takes advantage of a number of existing
string similarities. Our method takes into account
the most relevant string similarity by obtaining a
ranking of each in terms of its importance for a
classification task. The outline of the algorithm
of the hybrid approach is shown below. The al-
gorithm uses training data B which is provided in
the form of matching and non-matching pairs of
terms. First, in steps 1 to 5 the algorithm calcu-
lates pairwise similarities between two terms by
every string function (sim1, sim2, ..., simK). In
steps 6 to 8 the algorithm computes for every simi
an importance rate using the Random Forest tech-
nique (Genuer et al., 2010; Breiman, 2001). In
subsection 4.2 we describe in more detail the pro-
cess of selecting the most important string similar-
ities. Then, in steps 10 to 22 the algorithm itera-
tively constructs the set of the similarity functions
T * which is a subset of Sim. It starts from an
empty set T* and at each iteration it adds to T*
the measure that has the highest importance rate
and after that it learns the classifier C. After every
iteration the algorithm evaluates the performance
</bodyText>
<page confidence="0.994883">
49
</page>
<figure confidence="0.982303">
(a) Levenshtein (b) Smith Waterman (c) Monge Elkan (d) Jaro Winkler
(e) Jaro (f) Gotoh (g) Needleman Wunch (h) Qgrams Distance
(i) Dice Similarity (j) Jaccard Similarity (k) Double Metaphone (l) Soundex
(m) IBMAlphaCode (n) NYSIISCode
</figure>
<figureCaption confidence="0.999996">
Figure 1: The distribution between two matching and two non-matching pairs of records for each string similarity function
</figureCaption>
<bodyText confidence="0.9981432">
in term of maximum F measure Fmeas on the
validation set R. The algorithm stops if Fmeas
doesn’t increase anymore or if the size T∗ reaches
the parameter q which can be set as a fraction of
the total number of string similarity functions.
</bodyText>
<subsectionHeader confidence="0.995484">
4.1 Pairwise Similarity Calculation
</subsectionHeader>
<bodyText confidence="0.999955818181818">
In order to solve the name ambiguity problem it is
necessary to compute the similarity score between
two records. Most of the standard string similarity
functions and standard classifiers require a pair-
wise records comparison. We convert each dataset
described in Section 2 into a dataset of variant
pairs using random combinations of records. Two
differently spelled terms are equal when their stan-
dardized codes are the same and different other-
wise. The example of term pair-variants dataset
on is shown in Table 4.
</bodyText>
<table confidence="0.959121">
Name1 Name2 class
Statius Eustachius 1
Statius Stefan 0
Stefan Stephanus 1
</table>
<tableCaption confidence="0.999759">
Table 4: An example of term pair-variants
</tableCaption>
<subsectionHeader confidence="0.98902">
4.2 Measure Selection
</subsectionHeader>
<bodyText confidence="0.999981909090909">
Using only the most important measures for solv-
ing the terms variation task can significantly re-
duce the computational cost. Therefore, we be-
fore learning the classifier we apply a selection
technique. Generally, there are two common tech-
niques that allow to reduce the number of dimen-
sions: filters and wrappers (Das, 2001). Typi-
cally filter-based approaches require only one sin-
gle scan, whereas wrapper-based ones iteratively
look for the set of features which are the most
suitable which leads to larger computational over-
</bodyText>
<page confidence="0.954416">
50
</page>
<table confidence="0.890746125">
Algorithm 1 Hybrid Disambiguation Measure
Input: Training set B = {b1, ..., bβ}
Validation set R = {r1, ..., rρ}
Set of similarity measures Sim = (sim1, ..., simK)
Maximum allowed number of similarity measures rl
L{C, B, T *} classifier C with learning algorithm L
which is trained on the training set B
Output: A hybrid measure Simhb based on classifier C
</table>
<listItem confidence="0.787956666666667">
1: for each b in B do
2: for each sim in Sim do
3: compute sim(b)
4: end for
5: end for
6: for each sim in Sim do
7: compute RFsim{B}
8: end for
9: T* ← ∅
</listItem>
<figure confidence="0.928328">
10: Fmeas1{R} ← 0
11: i ← 2
12: while |T* |≤ rl do
13: select simi that maximizes RF importance rate
14: Sim ← Sim − {simi}
15: T * ← T * ∪ {simi}
16: L{C, B, T *}
17: Calculate model performance Fmeasi{R}
18: if max(Fmeasi{R}) &gt; max(Fmeasi_1{R})
then
19: break
20: end if
21: i ← i + 1
22: end while
23: Simhb ← L{C, B, T*}
24: return Simhb corresponding to T*and C
</figure>
<bodyText confidence="0.999961666666667">
heads. For designing a hybrid approach we de-
cided to use Random Forest (RF) wrappers to
evaluate the weight of every similarity measures.
RF, according to many different sources is consid-
ered as one of the most reliable methods which is
able to deal with high-dimensional and noisy data
(Saeys et al., 2007). RF generates a forest of clas-
sification trees and then assign an importance rank
to each similarity function based on its usefulness
for the classification purpose. We use RF results
to perform a stepwise procedure and to construct
the set of measures T ∗.
</bodyText>
<subsectionHeader confidence="0.9872435">
4.3 Hybrid Score Computation and pairwise
Classification
</subsectionHeader>
<bodyText confidence="0.999905464285714">
We consider the problem of terms variations as
a prediction problem. There are many available
classification techniques that are suitable for a pre-
diction task. Many of them require a prior training
phase on a representative subset of data to make
a more efficient prediction on new data. After
that, pairs of references are classified into classes
Matched or non-Matched based on a threshold
value of the score function. The score func-
tion computes the final similarity score between
two terms based on results of single comparison
measures. For learning the score function we
use a training dataset B. We explore 2 robust
classifiers that could be applied to cultural her-
itage dataset domains. They are the Logistic Re-
gression (LG) and the Support Vector Machine
(SVM) (Hastie et al., 2003; Cristianini and Shawe-
Taylor, 2000). They are two of the most widely-
used classifiers that are suitable for the prediction
task (James et al., 2013). It is important to add
that we also carried out our experiments and ap-
plied three more classifiers, namely Linear Dis-
criminant Analysis, Quadratic Discriminant Anal-
ysis and k-nearest neighbors (Hastie et al., 2003;
Verma, 2012; Zezula et al., 2006). However re-
sults were not improved significantly on all of our
datasets, so we do not include those classifiers in
the designed hybrid approach.
</bodyText>
<sectionHeader confidence="0.996529" genericHeader="method">
5 The Prediction Models
</sectionHeader>
<bodyText confidence="0.99983025">
In this Section we will briefly describe models that
we incorporated into our hybrid approach to ad-
dress the problem of inaccurate cultural heritage
data.
</bodyText>
<subsectionHeader confidence="0.987065">
5.1 Logistic regression
</subsectionHeader>
<bodyText confidence="0.999972">
We apply a logistic regression as a predictive
model and calculate the score function as follows:
</bodyText>
<equation confidence="0.998094666666667">
Simhb(ai, aj) =
1
1 + e−z , (1)
</equation>
<bodyText confidence="0.964988625">
where z = w0 + w1 * sim1(ai, aj) + w2 *
sim2(ai, aj) + ··· + wn * simK(ai, aj) is an uti-
lization of a linear regression model with parame-
ters represented by w1 to wk. The parameters w0
to wn are learned in a training phase. The func-
tions sim1(ai, aj) to simK(ai, aj) represent sin-
gle similarity measures between two terms ai and
aj.
</bodyText>
<subsectionHeader confidence="0.999691">
5.2 Support Vector Machines
</subsectionHeader>
<bodyText confidence="0.999980454545454">
We apply and explore SVM as a predictive model.
The basic idea of SVM is that the training data is
mapped into a new high dimensional space where
it is possible to apply linear models to separate the
classes. A kernel function performs the mapping
of the training data into the new space. After that,
a separation between classes is done by maximiz-
ing a separation margin between cases belonging
to different classes. In our hybrid approach we use
the SVM classifier with a radial basis kernel func-
tion and train it on the training set B.
</bodyText>
<page confidence="0.998838">
51
</page>
<sectionHeader confidence="0.998898" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999760675">
Our experiments are conducted on four datasets.
Three datasets, namely names, occupations and
places variations are manually constructed from
Cultural Heritage Data. They are discussed in de-
tail in Section 2.
The fourth dataset is a public dataset called
Restaurant. It is a standard benchmark dataset
which is widely used in data matching studies
(Christen, 2012; Bilenko et al., 2003). It contains
information about 864 restaurant names and ad-
dresses where 112 records are duplicated. It was
obtained by integrating records from two sources:
Fodors and Zagats guidebooks. The Restaurant
dataset was taken from the SecondString toolkit4.
We carried out our experiments in accordance
to the algorithm described in Section 4. At first,
we convert each dataset into a dataset of variant
pairs using random combinations of records. Then
for each pair of records we compute string simi-
larity functions. We randomly divided all avail-
able data into two subsets, namely training and
test sets. To construct the set of string similari-
ties we use 70% of the training set to learn RF
importance rate and the other 30% of the train-
ing set to validate results under stepwise selec-
tion procedure as it was described in the algorithm
in Section 4. The resulting set of selected string
similarities for each dataset is shown in Table 5.
After constructing the set of string similarities we
learn the classifier on the complete training set and
then evaluate it on the test set. In order to assess
the performance of our results, we apply a 10-fold
cross-validation method. We randomly partition
the available dataset into 10 equal size subsets.
Then one subset was chosen as the validation data
for testing the classifier, and the remaining subsets
are used for training the classifier. Then the cross-
validation process is repeated 10 times, with each
of the 10 subsets used exactly once as the valida-
tion dataset.
</bodyText>
<table confidence="0.9973492">
Dataset
Names IA SN DM LE SW
Occupations JW J LE NW QG
Places QG JW LE SW J
Restaurants QG JW CS NW
</table>
<tableCaption confidence="0.956984">
Table 5: Selected string similarities during the
stepwise procedure
</tableCaption>
<footnote confidence="0.925047">
4http://secondstring.sourceforge.net/
</footnote>
<sectionHeader confidence="0.908588" genericHeader="evaluation">
7 Evaluation Results
</sectionHeader>
<bodyText confidence="0.999931043478261">
In order to evaluate the performance of standard
string similarity functions and the applied hybrid
approach, we compute the sets of True Positives
(TP), False Positives (FP) and False Negatives
(FN) as the correctly identified, incorrectly identi-
fied and incorrectly rejected matches, respectively.
Fig. 2 demonstrates the performance of standard
and hybrid approaches on four examined datasets.
The logistic regression as well as SVM clas-
sifiers which are used in the hybrid approach on
each of the dataset outperform standard string sim-
ilarities. The improvement in results is significant,
especially it is clearly seen on the dataset of occu-
pations. For a more detailed analysis, Fig. 3 shows
the evaluation of results in terms of F-measure
and the threshold value for all continuous meth-
ods. Moreover, Table 6 shows the maximum val-
ues of the F-measure for the five best performing
methods for each of the datasets. Two upper rows
of the table belong only to the hybrid approach.
SVM and logistic regression in the combination
with the RF selection technique both demonstrate
robustness on the multiple datasets domains.
</bodyText>
<table confidence="0.962691428571429">
Names Occupations Places Restaurants
Method Max.F Method Max.F Method Max.F Method Max.F
SVM 0.94 SVM 0.93 SVM 0.95 LG 0.95
LG 0.92 LG 0.86 LG 0.93 SVM 0.91
LE 0.89 J 0.82 QG 0.88 JW 0.87
J 0.87 LE 0.77 JW 0.87 QG 0.81
SW 0.86 JW 0.71 J 0.85 GH 0.76
</table>
<tableCaption confidence="0.996811">
Table 6: The maximum F-Measure values for the five best-
performing methods
</tableCaption>
<bodyText confidence="0.999915058823529">
In addition to analyzing the hybrid approach,
in this section we investigate in more detail func-
tions which demonstrate not the typical behavior
on the precision and recall plots. For instance, on
Fig. 2 for SW, GH and ME similarities the simul-
taneous growth of the precision is accomplished
by the growth in the recall on the interval (0, 0.3).
The same situation occurs for SW and GH simi-
larities on datasets of occupations and places. In
Table 7 for SW similarity on the dataset on names
for three levels of the threshold we show its perfor-
mance indicators, namely TP, FP and FN values
and calculated the precision and recall. With the
maximum similarity score SW incorrectly identify
as positive 99 pairs of names. With the slightly
decrease in the threshold value, the larger number
of pairs are identified correctly as the variation of
</bodyText>
<page confidence="0.990404">
52
</page>
<figure confidence="0.9723245">
(a) The dataset of names (b) The dataset of occupations
(c) The dataset of places (d) Public dataset of restaurants
</figure>
<figureCaption confidence="0.999827">
Figure 2: Evaluation of single string similarities and the hybrid approach in terms of precision and recall
</figureCaption>
<table confidence="0.99686675">
Threshold TP FP FN Precision Recall Name1 Name2 SW GH ME
0.96 809 99 3853 0.89 0.17 Peternella Peter 1 1 1
0.98 355 99 4307 0.78 0.08 Pauline Paul 1 1 1
1 200 99 4462 0.67 0.04 Henriette Henri 1 1 1
</table>
<tableCaption confidence="0.995589">
Table 7: Evaluation measures for SW similarity
</tableCaption>
<bodyText confidence="0.968922">
for 3 levels of the threshold
the same name. Therefore, to make it absolutely
clear, in Table 8 we gave an example of such pairs
of names that are included into 99 FP and cause
the simultaneous grows of precision and recall.
</bodyText>
<sectionHeader confidence="0.999458" genericHeader="discussions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.9990628">
The proposed hybrid approach shows very good
results in performing the pairwise terms com-
parison for completely different dataset domains.
Nevertheless, the bottleneck of the algorithm is
that it is expensive to apply it to real-world data
and compare all possible combinations of records.
Table 8: Example of FP pairs of names according
to the maximum value of SW, GH and ME func-
tions
There are various available techniques for re-
ducing the amount of candidate pairs to be com-
pared. Common techniques are partitioning data
into smaller subsets and comparing only records
with the same partition. Two widely used partition
approaches are blocking and windowing methods
(Naumann and Herschel, 2010; Bilenko et al.,
2003). The blocking technique assigns to each
record a special blocking key, for instance year,
place of the documents or the first 3 letters of
the last name. The windowing technique such as
</bodyText>
<page confidence="0.993249">
53
</page>
<figure confidence="0.9730815">
(a) The dataset of names (b) The dataset of occupations
(c) The dataset of places (d) Public dataset of restaurants
</figure>
<figureCaption confidence="0.999994">
Figure 3: Evaluation of single string similarities and the hybrid approach in terms of F-Measure and Threshold
</figureCaption>
<bodyText confidence="0.999767666666667">
Sorted Neighborhood method sorts data according
to some key, for instance year of the documents,
and then slides a window of fixed size across the
sorted data. Reducing the number of candidate
pairs may result that two references that refer to
the same entity appear in different partitions and
then they will never be compared. Therefore, our
next work will focus on searching the best parti-
tion method (or best hybrid methods) that allows
to reduce the number of potential candidate pairs
and keep all references referring to the same entity
within the same partition.
</bodyText>
<sectionHeader confidence="0.99742" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999525227272727">
In this paper we studied a number of tradi-
tional string similarities and proposed the hybrid
approach applied on different cultural heritage
dataset domains. It is obvious that dealing with
historical documents, where attributes information
is often imprecise, is not possible by using only
one string similarity. Therefore, we investigated
how to improve the performance by using a num-
ber of string similarities and applied supervised
learning technique.
As future step, the authors are working on in-
corporating the hybrid approach into overall entity
resolution process in a large genealogical database
(Efremova et al., 2014), which aims to discover
which of the person references mentioned in dif-
ferent historical documents refer to the same per-
son entity. The genealogical database contains a
collection of historical documents where names,
occupations and places are the essential attributes.
Therefore it is very important to find a robust and
reliable approach which is able to compare main
personal information in the noisy data.
</bodyText>
<sectionHeader confidence="0.995774" genericHeader="acknowledgments">
10 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999241">
The authors are grateful to the BHIC Center, in
particular to Rien Wols and Anton Schuttelaars for
the support in data gathering, data analysis and di-
rection.
</bodyText>
<page confidence="0.995694">
54
</page>
<note confidence="0.95069875">
Gareth James, Daniela Witten, Trevor Hastie, and
Robert Tibshirani. 2013. An introduction to sta-
tistical learning: with applications in R. Springer
Publishing Company, Incorporated.
</note>
<sectionHeader confidence="0.530094" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.750555333333333">
Indrajit Bhattacharya and Lise Getoor. 2007. Collec-
tive entity resolution in relational data. ACM Trans.
Knowl. Discov. Data, 1(1).
</bodyText>
<reference confidence="0.997934846153846">
Mikhail Bilenko and Raymond J. Mooney. 2003.
Adaptive duplicate detection using learnable string
similarity measures. In Proceedings of the Ninth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’03, pages
39–48. ACM.
Mikhail Bilenko, Raymond Mooney, William Cohen,
Pradeep Ravikumar, and Stephen Fienberg. 2003.
Adaptive name matching in information integration.
Intelligent Systems, 18(5):16–23.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5–32.
Peter Christen. 2006. A comparison of personal name
matching: techniques and practical issues. In Pro-
ceedings of the Workshop on Mining Complex Data
(MCD06), held at IEEE ICDM06, pages 290–294.
Peter Christen. 2012. Data matching. Springer Pub-
lishing Company, Incorporated.
William W. Cohen, Pradeep Ravikumar, and
Stephen E. Fienberg. 2003. A comparison of
string distance metrics for name-matching tasks. In
Proceedings of IJCAI-03 Workshop on Information
Integration, pages 73–78.
Nello Cristianini and John Shawe-Taylor. 2000. An
introduction to Support Vector Machines: and other
kernel-based learning methods. Cambridge Univer-
sity Press.
Sanmay Das. 2001. Filters, wrappers and a boosting-
based hybrid for feature selection. In Proceedings
of the Eighteenth International Conference on Ma-
chine Learning, ICML ’01, pages 74–81. Morgan
Kaufmann Publishers Inc.
Julia Efremova, Bijan Ranjbar-Sahraei, Frans A.
Oliehoek, Toon Calders, and Karl Tuyls. 2014.
A baseline method for genealogical entity resolu-
tion. In Proceedings of the Workshop on Population
Reconstruction, organized in the framework of the
LINKS project.
Ahmed K. Elmagarmid, Panagiotis G. Ipeirotis, and
Vassilios S. Verykios. 2007. Duplicate record de-
tection: A survey. Knowledge and Data Engineer-
ing, IEEE Transactions on, 19(1):1–16.
Robin Genuer, Jean-Michel Poggi, and Christine
Tuleau-Malot. 2010. Variable selection us-
ing random forests. Pattern Recognition Letters,
31(14):2225–2236.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2003. The elements of statistical learning.
Springer, corrected edition.
Bertrand Lisbach and Victoria Meyer. 2013. Linguistic
identity matching. Springer.
Kees Mandemakers, Sanne Muurling, Ineke Maas,
Bart Van de Putte, Richard L. Zijdeman, Paul Lam-
bert, Marco H.D. van Leeuwen, Frans van Pop-
pel, and Andrew Miles. 2013. HSN standard-
ized, HISCO-coded and classified occupational ti-
tles. IISG Amsterdam.
Paul McNamee and James Mayfield. 2004. Charac-
ter n-gram tokenization for european language text
retrieval. Information Retrieval, 7(1-2):73–97.
Felix Naumann and Melanie Herschel. 2010. An Intro-
duction to Duplicate Detection. Morgan and Clay-
pool Publishers.
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Comput. Surv., 33(1):31–88.
Eric Sven Ristad, Peter N. Yianilos, and Senior Mem-
ber. 1998. Learning string edit distance. IEEE
Transactions on Pattern Analysis and Machine In-
telligence, 20:522–532.
Yvan Saeys, I˜naki Inza, and Pedro Larra˜naga. 2007.
A review of feature selection techniques in bioin-
formatics. Bioinformatics, 23(19):2507–2517,
September.
Sheila Tejada, Craig A. Knoblock, and Steven Minton.
2002. Learning domain-independent string transfor-
mation weights for high accuracy object identifica-
tion. In Proceedings of the eighth ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, KDD ’02, pages 350–359. ACM.
Marco H. D. van Leeuwen, Ineke Maas, and Andrew
Miles. 2002. HISCO. Historical international stan-
dard classification of occupations. Leuven Univer-
sity Press.
J P Verma. 2012. Data Analysis in Management with
SPSS Software. Springer.
William E. Winkler. 1995. Matching and record link-
age. In Business Survey Methods, pages 355–384.
Wiley.
Pavel Zezula, Giuseppe Amato, Vlastislav Dohnal, and
Michal Batko. 2006. Similarity search: the metric
space approach. Springer.
</reference>
<page confidence="0.999065">
55
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.706100">
<title confidence="0.999902">A Hybrid Disambiguation Measure for Inaccurate Cultural Heritage Data</title>
<author confidence="0.998609">Bijan Toon</author>
<affiliation confidence="0.992383333333333">University of Technology, The University, The Libre de Bruxelles,</affiliation>
<email confidence="0.855596">i.efremova@tue.nl,toon.calders@ulb.ac.be</email>
<abstract confidence="0.999525565217391">Cultural heritage data is always associated with inaccurate information and different types of ambiguities. For instance, names of persons, occupations or places mentioned in historical documents are not standardized and contain numerous variations. This article examines in detail various existing similarity functions and proposes a hybrid technique for the following task: among the list of possible names, occupations and places extracted from historical documents, identify those that are variations of the same person name, occupation and place respectively. The performance of our method is evaluated on three manually constructed datasets and one public dataset in terms of precision, recall and F-measure. The results demonstrate that the hybrid technique outperforms current methods and allows to significantly improve the quality of cultural heritage data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mikhail Bilenko</author>
<author>Raymond J Mooney</author>
</authors>
<title>Adaptive duplicate detection using learnable string similarity measures.</title>
<date>2003</date>
<booktitle>In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’03,</booktitle>
<pages>39--48</pages>
<publisher>ACM.</publisher>
<marker>Bilenko, Mooney, 2003</marker>
<rawString>Mikhail Bilenko and Raymond J. Mooney. 2003. Adaptive duplicate detection using learnable string similarity measures. In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’03, pages 39–48. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Bilenko</author>
<author>Raymond Mooney</author>
<author>William Cohen</author>
<author>Pradeep Ravikumar</author>
<author>Stephen Fienberg</author>
</authors>
<title>Adaptive name matching in information integration.</title>
<date>2003</date>
<journal>Intelligent Systems,</journal>
<volume>18</volume>
<issue>5</issue>
<contexts>
<context position="19160" citStr="Bilenko et al., 2003" startWordPosition="3122" endWordPosition="3125"> classes is done by maximizing a separation margin between cases belonging to different classes. In our hybrid approach we use the SVM classifier with a radial basis kernel function and train it on the training set B. 51 6 Experiments Our experiments are conducted on four datasets. Three datasets, namely names, occupations and places variations are manually constructed from Cultural Heritage Data. They are discussed in detail in Section 2. The fourth dataset is a public dataset called Restaurant. It is a standard benchmark dataset which is widely used in data matching studies (Christen, 2012; Bilenko et al., 2003). It contains information about 864 restaurant names and addresses where 112 records are duplicated. It was obtained by integrating records from two sources: Fodors and Zagats guidebooks. The Restaurant dataset was taken from the SecondString toolkit4. We carried out our experiments in accordance to the algorithm described in Section 4. At first, we convert each dataset into a dataset of variant pairs using random combinations of records. Then for each pair of records we compute string similarity functions. We randomly divided all available data into two subsets, namely training and test sets.</context>
<context position="24622" citStr="Bilenko et al., 2003" startWordPosition="4048" endWordPosition="4051">or completely different dataset domains. Nevertheless, the bottleneck of the algorithm is that it is expensive to apply it to real-world data and compare all possible combinations of records. Table 8: Example of FP pairs of names according to the maximum value of SW, GH and ME functions There are various available techniques for reducing the amount of candidate pairs to be compared. Common techniques are partitioning data into smaller subsets and comparing only records with the same partition. Two widely used partition approaches are blocking and windowing methods (Naumann and Herschel, 2010; Bilenko et al., 2003). The blocking technique assigns to each record a special blocking key, for instance year, place of the documents or the first 3 letters of the last name. The windowing technique such as 53 (a) The dataset of names (b) The dataset of occupations (c) The dataset of places (d) Public dataset of restaurants Figure 3: Evaluation of single string similarities and the hybrid approach in terms of F-Measure and Threshold Sorted Neighborhood method sorts data according to some key, for instance year of the documents, and then slides a window of fixed size across the sorted data. Reducing the number of </context>
</contexts>
<marker>Bilenko, Mooney, Cohen, Ravikumar, Fienberg, 2003</marker>
<rawString>Mikhail Bilenko, Raymond Mooney, William Cohen, Pradeep Ravikumar, and Stephen Fienberg. 2003. Adaptive name matching in information integration. Intelligent Systems, 18(5):16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="12406" citStr="Breiman, 2001" startWordPosition="1952" endWordPosition="1953">es. Our method takes into account the most relevant string similarity by obtaining a ranking of each in terms of its importance for a classification task. The outline of the algorithm of the hybrid approach is shown below. The algorithm uses training data B which is provided in the form of matching and non-matching pairs of terms. First, in steps 1 to 5 the algorithm calculates pairwise similarities between two terms by every string function (sim1, sim2, ..., simK). In steps 6 to 8 the algorithm computes for every simi an importance rate using the Random Forest technique (Genuer et al., 2010; Breiman, 2001). In subsection 4.2 we describe in more detail the process of selecting the most important string similarities. Then, in steps 10 to 22 the algorithm iteratively constructs the set of the similarity functions T * which is a subset of Sim. It starts from an empty set T* and at each iteration it adds to T* the measure that has the highest importance rate and after that it learns the classifier C. After every iteration the algorithm evaluates the performance 49 (a) Levenshtein (b) Smith Waterman (c) Monge Elkan (d) Jaro Winkler (e) Jaro (f) Gotoh (g) Needleman Wunch (h) Qgrams Distance (i) Dice S</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random forests. Machine Learning, 45(1):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Christen</author>
</authors>
<title>A comparison of personal name matching: techniques and practical issues.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Mining Complex Data (MCD06), held at IEEE ICDM06,</booktitle>
<pages>290--294</pages>
<contexts>
<context position="9329" citStr="Christen, 2006" startWordPosition="1425" endWordPosition="1426">ead of their spelling differences. For example, the two names Stefan and Stephan barely differ phonetically, but nevertheless they have different spellings. Phonetic functions encode every name with phonetic keys based on a set of rules. For instance, some algorithms ignore all vowels and compare only the groups of consonants, other algorithms analyze consonant combinations and thier sound that describe a large number of sounds. In this paper, we analyze 4 phonetic functions: Soundex (SN), Double Metaphone (DM), IBMAlphaCode (IA) and New York State Identification and Intelligence System (NY) (Christen, 2006). The Table 2 shows an example of applied phonetic keys to encode imprecise names. Name SN DM IA NYSIIS Stefan S315 STFN 00182 STAFAN Stephan S315 STFN 00182 STAFPAN Stephanus S3152 STFNS 00182 STAFPAN Table 2: An example of phonetic keys 3.3 Token-Based Similarity Token-based functions divide two strings into sets of tokens s1 and s2, then they compute the intersection between two sets based on the number of equal tokens. Some token-based functions, for instance Dice similarity (DS), Jaccard coefficient(JS) and Cosine similarity (CS) (Cohen et al., 2003), consider as a token the whole word in</context>
</contexts>
<marker>Christen, 2006</marker>
<rawString>Peter Christen. 2006. A comparison of personal name matching: techniques and practical issues. In Proceedings of the Workshop on Mining Complex Data (MCD06), held at IEEE ICDM06, pages 290–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Christen</author>
</authors>
<title>Data matching.</title>
<date>2012</date>
<publisher>Springer Publishing Company, Incorporated.</publisher>
<contexts>
<context position="2465" citStr="Christen, 2012" startWordPosition="358" endWordPosition="360">d be mentioned many times, for instance in civil certificates such as birth, marriage and death certificates or in property transfer records and tax declarations. The name of the same person, his occupation and the place in such documents varies a lot. When working with such information, researchers have to identify which person references mentioned in different historical documents belong to the same person entity. This problem has been referred to in literature in many different ways but is best known as entity resolution (ER), record linkage or duplicate detection (Lisbach and Meyer, 2013; Christen, 2012; Bhattacharya and Getoor, 2007). The process of ER in historical documents is always accompanied by inaccurate information as well. As an example, there are more than 100 variants of the first name Jan, such as Johan, Johannes, Janis, Jean or the profession musician in historical documents can be spelled as musikant, muzikant or even muzikant bij de tiende afd. The latter means the musician in the 10th department. The past few decades have seen a large research interest in the problem of inaccurate information. As a result, a large number of methods for comparing string has been developed. Th</context>
<context position="8309" citStr="Christen, 2012" startWordPosition="1273" endWordPosition="1274">token-based. Each of them we investigate in detail below. 3.1 Character-Based Similarity Character-based similarities operate on character sequences and their composition which makes them suitable for identifying imprecise names and spelling errors. They compute the similarity between two strings as the difference between their common characters. In this paper, we will consider the Levenshtein edit distance (LE), Jaro (J), Jaro Winkler (JW), Smith Waterman (SW), Smith Waterman with Gotohs backtracing (GH), Needleman Wunch (NW) and Monge Elkan (ME) string similarities (Elmagarmid et al., 2007; Christen, 2012; Naumann and Herschel, 2010). All of them return a number between 0 and 1 inclusively, 3http://www.iisg.nl/hsn/data/occupations.html 48 where the highest value when two names are exactly the same. Table 3 shows an example of computed character-based similarities for three name pair-variants. 3.2 Phonetic-Based Similarity Phonetic similarity functions analyze the sounds of the names being compared instead of their spelling differences. For example, the two names Stefan and Stephan barely differ phonetically, but nevertheless they have different spellings. Phonetic functions encode every name w</context>
<context position="19137" citStr="Christen, 2012" startWordPosition="3120" endWordPosition="3121">paration between classes is done by maximizing a separation margin between cases belonging to different classes. In our hybrid approach we use the SVM classifier with a radial basis kernel function and train it on the training set B. 51 6 Experiments Our experiments are conducted on four datasets. Three datasets, namely names, occupations and places variations are manually constructed from Cultural Heritage Data. They are discussed in detail in Section 2. The fourth dataset is a public dataset called Restaurant. It is a standard benchmark dataset which is widely used in data matching studies (Christen, 2012; Bilenko et al., 2003). It contains information about 864 restaurant names and addresses where 112 records are duplicated. It was obtained by integrating records from two sources: Fodors and Zagats guidebooks. The Restaurant dataset was taken from the SecondString toolkit4. We carried out our experiments in accordance to the algorithm described in Section 4. At first, we convert each dataset into a dataset of variant pairs using random combinations of records. Then for each pair of records we compute string similarity functions. We randomly divided all available data into two subsets, namely </context>
</contexts>
<marker>Christen, 2012</marker>
<rawString>Peter Christen. 2012. Data matching. Springer Publishing Company, Incorporated.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Pradeep Ravikumar</author>
<author>Stephen E Fienberg</author>
</authors>
<title>A comparison of string distance metrics for name-matching tasks.</title>
<date>2003</date>
<booktitle>In Proceedings of IJCAI-03 Workshop on Information Integration,</booktitle>
<pages>73--78</pages>
<contexts>
<context position="9890" citStr="Cohen et al., 2003" startWordPosition="1516" endWordPosition="1519">entification and Intelligence System (NY) (Christen, 2006). The Table 2 shows an example of applied phonetic keys to encode imprecise names. Name SN DM IA NYSIIS Stefan S315 STFN 00182 STAFAN Stephan S315 STFN 00182 STAFPAN Stephanus S3152 STFNS 00182 STAFPAN Table 2: An example of phonetic keys 3.3 Token-Based Similarity Token-based functions divide two strings into sets of tokens s1 and s2, then they compute the intersection between two sets based on the number of equal tokens. Some token-based functions, for instance Dice similarity (DS), Jaccard coefficient(JS) and Cosine similarity (CS) (Cohen et al., 2003), consider as a token the whole word in a string. In our case most of the person names, locations and places are quite different and there are only few intersections between token-words available. Another approach, a q-gram (QG) tokenization (McNamee and Mayfield, 2004), divides a string into smaller tokens of size q. QG calculates the similarity between two strings by counting the number of q-grams in common and dividing by the number of q-grams in the longer string. In this paper we consider bigrams (q = 2). For example, the name ’stefan’ contains the bigrams ’st’, ’te’, ’ef’, ’fa’,’an’. An </context>
</contexts>
<marker>Cohen, Ravikumar, Fienberg, 2003</marker>
<rawString>William W. Cohen, Pradeep Ravikumar, and Stephen E. Fienberg. 2003. A comparison of string distance metrics for name-matching tasks. In Proceedings of IJCAI-03 Workshop on Information Integration, pages 73–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
</authors>
<title>An introduction to Support Vector Machines: and other kernel-based learning methods.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Nello Cristianini and John Shawe-Taylor. 2000. An introduction to Support Vector Machines: and other kernel-based learning methods. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanmay Das</author>
</authors>
<title>Filters, wrappers and a boostingbased hybrid for feature selection.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>74--81</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="14482" citStr="Das, 2001" startWordPosition="2297" endWordPosition="2298">ms are equal when their standardized codes are the same and different otherwise. The example of term pair-variants dataset on is shown in Table 4. Name1 Name2 class Statius Eustachius 1 Statius Stefan 0 Stefan Stephanus 1 Table 4: An example of term pair-variants 4.2 Measure Selection Using only the most important measures for solving the terms variation task can significantly reduce the computational cost. Therefore, we before learning the classifier we apply a selection technique. Generally, there are two common techniques that allow to reduce the number of dimensions: filters and wrappers (Das, 2001). Typically filter-based approaches require only one single scan, whereas wrapper-based ones iteratively look for the set of features which are the most suitable which leads to larger computational over50 Algorithm 1 Hybrid Disambiguation Measure Input: Training set B = {b1, ..., bβ} Validation set R = {r1, ..., rρ} Set of similarity measures Sim = (sim1, ..., simK) Maximum allowed number of similarity measures rl L{C, B, T *} classifier C with learning algorithm L which is trained on the training set B Output: A hybrid measure Simhb based on classifier C 1: for each b in B do 2: for each sim </context>
</contexts>
<marker>Das, 2001</marker>
<rawString>Sanmay Das. 2001. Filters, wrappers and a boostingbased hybrid for feature selection. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 74–81. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Efremova</author>
<author>Bijan Ranjbar-Sahraei</author>
<author>Frans A Oliehoek</author>
<author>Toon Calders</author>
<author>Karl Tuyls</author>
</authors>
<title>A baseline method for genealogical entity resolution.</title>
<date>2014</date>
<booktitle>In Proceedings of the Workshop on Population Reconstruction, organized in the framework of the LINKS</booktitle>
<pages>project.</pages>
<contexts>
<context position="26251" citStr="Efremova et al., 2014" startWordPosition="4310" endWordPosition="4313">s paper we studied a number of traditional string similarities and proposed the hybrid approach applied on different cultural heritage dataset domains. It is obvious that dealing with historical documents, where attributes information is often imprecise, is not possible by using only one string similarity. Therefore, we investigated how to improve the performance by using a number of string similarities and applied supervised learning technique. As future step, the authors are working on incorporating the hybrid approach into overall entity resolution process in a large genealogical database (Efremova et al., 2014), which aims to discover which of the person references mentioned in different historical documents refer to the same person entity. The genealogical database contains a collection of historical documents where names, occupations and places are the essential attributes. Therefore it is very important to find a robust and reliable approach which is able to compare main personal information in the noisy data. 10 Acknowledgments The authors are grateful to the BHIC Center, in particular to Rien Wols and Anton Schuttelaars for the support in data gathering, data analysis and direction. 54 Gareth J</context>
</contexts>
<marker>Efremova, Ranjbar-Sahraei, Oliehoek, Calders, Tuyls, 2014</marker>
<rawString>Julia Efremova, Bijan Ranjbar-Sahraei, Frans A. Oliehoek, Toon Calders, and Karl Tuyls. 2014. A baseline method for genealogical entity resolution. In Proceedings of the Workshop on Population Reconstruction, organized in the framework of the LINKS project.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed K Elmagarmid</author>
<author>Panagiotis G Ipeirotis</author>
<author>Vassilios S Verykios</author>
</authors>
<title>Duplicate record detection: A survey. Knowledge and Data Engineering,</title>
<date>2007</date>
<journal>IEEE Transactions on,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="3391" citStr="Elmagarmid et al., 2007" startWordPosition="503" endWordPosition="506">led as musikant, muzikant or even muzikant bij de tiende afd. The latter means the musician in the 10th department. The past few decades have seen a large research interest in the problem of inaccurate information. As a result, a large number of methods for comparing string has been developed. These standard methods are called string similarity functions. Some of those well known techniques are character-based, token-based or based on phonetic functions, for instance Levenshtein Edit distance, Jaro Winkler distance, Monge Elkan distance, Smith Waterman distance, Soundex and Double Metaphone. (Elmagarmid et al., 2007; Navarro, 2001; Winkler, 1995). Each of the mentioned similarity functions perform optimally for a particular dataset domain. For example, the phonetic function Soundex works great for encoding names by sound as it pronounced in English, but nevertheless sometimes it is also used to encode names in other European languages. However, only little work has been done in studying combinations of similarity functions, and in their simultaneous use for achieving more reliable results. Bilenko (2003) in his work computes names similarity with 47 Proceedings of the 8th Workshop on Language Technology </context>
<context position="8293" citStr="Elmagarmid et al., 2007" startWordPosition="1269" endWordPosition="1272">ased, phonetic-based and token-based. Each of them we investigate in detail below. 3.1 Character-Based Similarity Character-based similarities operate on character sequences and their composition which makes them suitable for identifying imprecise names and spelling errors. They compute the similarity between two strings as the difference between their common characters. In this paper, we will consider the Levenshtein edit distance (LE), Jaro (J), Jaro Winkler (JW), Smith Waterman (SW), Smith Waterman with Gotohs backtracing (GH), Needleman Wunch (NW) and Monge Elkan (ME) string similarities (Elmagarmid et al., 2007; Christen, 2012; Naumann and Herschel, 2010). All of them return a number between 0 and 1 inclusively, 3http://www.iisg.nl/hsn/data/occupations.html 48 where the highest value when two names are exactly the same. Table 3 shows an example of computed character-based similarities for three name pair-variants. 3.2 Phonetic-Based Similarity Phonetic similarity functions analyze the sounds of the names being compared instead of their spelling differences. For example, the two names Stefan and Stephan barely differ phonetically, but nevertheless they have different spellings. Phonetic functions enc</context>
</contexts>
<marker>Elmagarmid, Ipeirotis, Verykios, 2007</marker>
<rawString>Ahmed K. Elmagarmid, Panagiotis G. Ipeirotis, and Vassilios S. Verykios. 2007. Duplicate record detection: A survey. Knowledge and Data Engineering, IEEE Transactions on, 19(1):1–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Genuer</author>
<author>Jean-Michel Poggi</author>
<author>Christine Tuleau-Malot</author>
</authors>
<title>Variable selection using random forests.</title>
<date>2010</date>
<journal>Pattern Recognition Letters,</journal>
<volume>31</volume>
<issue>14</issue>
<contexts>
<context position="12390" citStr="Genuer et al., 2010" startWordPosition="1948" endWordPosition="1951">ing string similarities. Our method takes into account the most relevant string similarity by obtaining a ranking of each in terms of its importance for a classification task. The outline of the algorithm of the hybrid approach is shown below. The algorithm uses training data B which is provided in the form of matching and non-matching pairs of terms. First, in steps 1 to 5 the algorithm calculates pairwise similarities between two terms by every string function (sim1, sim2, ..., simK). In steps 6 to 8 the algorithm computes for every simi an importance rate using the Random Forest technique (Genuer et al., 2010; Breiman, 2001). In subsection 4.2 we describe in more detail the process of selecting the most important string similarities. Then, in steps 10 to 22 the algorithm iteratively constructs the set of the similarity functions T * which is a subset of Sim. It starts from an empty set T* and at each iteration it adds to T* the measure that has the highest importance rate and after that it learns the classifier C. After every iteration the algorithm evaluates the performance 49 (a) Levenshtein (b) Smith Waterman (c) Monge Elkan (d) Jaro Winkler (e) Jaro (f) Gotoh (g) Needleman Wunch (h) Qgrams Dis</context>
</contexts>
<marker>Genuer, Poggi, Tuleau-Malot, 2010</marker>
<rawString>Robin Genuer, Jean-Michel Poggi, and Christine Tuleau-Malot. 2010. Variable selection using random forests. Pattern Recognition Letters, 31(14):2225–2236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
<author>Jerome Friedman</author>
</authors>
<title>The elements of statistical learning.</title>
<date>2003</date>
<publisher>Springer,</publisher>
<note>corrected edition.</note>
<contexts>
<context position="16980" citStr="Hastie et al., 2003" startWordPosition="2744" endWordPosition="2747">m require a prior training phase on a representative subset of data to make a more efficient prediction on new data. After that, pairs of references are classified into classes Matched or non-Matched based on a threshold value of the score function. The score function computes the final similarity score between two terms based on results of single comparison measures. For learning the score function we use a training dataset B. We explore 2 robust classifiers that could be applied to cultural heritage dataset domains. They are the Logistic Regression (LG) and the Support Vector Machine (SVM) (Hastie et al., 2003; Cristianini and ShaweTaylor, 2000). They are two of the most widelyused classifiers that are suitable for the prediction task (James et al., 2013). It is important to add that we also carried out our experiments and applied three more classifiers, namely Linear Discriminant Analysis, Quadratic Discriminant Analysis and k-nearest neighbors (Hastie et al., 2003; Verma, 2012; Zezula et al., 2006). However results were not improved significantly on all of our datasets, so we do not include those classifiers in the designed hybrid approach. 5 The Prediction Models In this Section we will briefly </context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2003</marker>
<rawString>Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2003. The elements of statistical learning. Springer, corrected edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bertrand Lisbach</author>
<author>Victoria Meyer</author>
</authors>
<title>Linguistic identity matching.</title>
<date>2013</date>
<publisher>Springer.</publisher>
<contexts>
<context position="2449" citStr="Lisbach and Meyer, 2013" startWordPosition="354" endWordPosition="357">uments a real person could be mentioned many times, for instance in civil certificates such as birth, marriage and death certificates or in property transfer records and tax declarations. The name of the same person, his occupation and the place in such documents varies a lot. When working with such information, researchers have to identify which person references mentioned in different historical documents belong to the same person entity. This problem has been referred to in literature in many different ways but is best known as entity resolution (ER), record linkage or duplicate detection (Lisbach and Meyer, 2013; Christen, 2012; Bhattacharya and Getoor, 2007). The process of ER in historical documents is always accompanied by inaccurate information as well. As an example, there are more than 100 variants of the first name Jan, such as Johan, Johannes, Janis, Jean or the profession musician in historical documents can be spelled as musikant, muzikant or even muzikant bij de tiende afd. The latter means the musician in the 10th department. The past few decades have seen a large research interest in the problem of inaccurate information. As a result, a large number of methods for comparing string has be</context>
</contexts>
<marker>Lisbach, Meyer, 2013</marker>
<rawString>Bertrand Lisbach and Victoria Meyer. 2013. Linguistic identity matching. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees Mandemakers</author>
<author>Sanne Muurling</author>
<author>Ineke Maas</author>
<author>Bart Van de Putte</author>
<author>Richard L Zijdeman</author>
<author>Paul Lambert</author>
<author>Marco H D van Leeuwen</author>
<author>Frans van Poppel</author>
<author>Andrew Miles</author>
</authors>
<title>HSN standardized, HISCO-coded and classified occupational titles. IISG</title>
<date>2013</date>
<location>Amsterdam.</location>
<marker>Mandemakers, Muurling, Maas, Van de Putte, Zijdeman, Lambert, van Leeuwen, van Poppel, Miles, 2013</marker>
<rawString>Kees Mandemakers, Sanne Muurling, Ineke Maas, Bart Van de Putte, Richard L. Zijdeman, Paul Lambert, Marco H.D. van Leeuwen, Frans van Poppel, and Andrew Miles. 2013. HSN standardized, HISCO-coded and classified occupational titles. IISG Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>James Mayfield</author>
</authors>
<title>Character n-gram tokenization for european language text retrieval.</title>
<date>2004</date>
<journal>Information Retrieval,</journal>
<pages>7--1</pages>
<contexts>
<context position="10160" citStr="McNamee and Mayfield, 2004" startWordPosition="1562" endWordPosition="1565">ble 2: An example of phonetic keys 3.3 Token-Based Similarity Token-based functions divide two strings into sets of tokens s1 and s2, then they compute the intersection between two sets based on the number of equal tokens. Some token-based functions, for instance Dice similarity (DS), Jaccard coefficient(JS) and Cosine similarity (CS) (Cohen et al., 2003), consider as a token the whole word in a string. In our case most of the person names, locations and places are quite different and there are only few intersections between token-words available. Another approach, a q-gram (QG) tokenization (McNamee and Mayfield, 2004), divides a string into smaller tokens of size q. QG calculates the similarity between two strings by counting the number of q-grams in common and dividing by the number of q-grams in the longer string. In this paper we consider bigrams (q = 2). For example, the name ’stefan’ contains the bigrams ’st’, ’te’, ’ef’, ’fa’,’an’. An example of applied QG and JS similarities is shown in Table 3. two names LE J JW SW GH NW ME QG JS (Stefan, Stephan) 0.71 0.85 0.89 0.58 0.57 0.79 0.57 0.5 0 (Stefan, Stephanus) 0.56 0.80 0.86 0.58 0.57 0.61 0.57 0.38 0 (Stephan, Stephanus) 0.78 0.93 0.97 1 1 0.78 1 0.7</context>
</contexts>
<marker>McNamee, Mayfield, 2004</marker>
<rawString>Paul McNamee and James Mayfield. 2004. Character n-gram tokenization for european language text retrieval. Information Retrieval, 7(1-2):73–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Naumann</author>
<author>Melanie Herschel</author>
</authors>
<title>An Introduction to Duplicate Detection.</title>
<date>2010</date>
<publisher>Morgan and Claypool Publishers.</publisher>
<contexts>
<context position="8338" citStr="Naumann and Herschel, 2010" startWordPosition="1275" endWordPosition="1278">h of them we investigate in detail below. 3.1 Character-Based Similarity Character-based similarities operate on character sequences and their composition which makes them suitable for identifying imprecise names and spelling errors. They compute the similarity between two strings as the difference between their common characters. In this paper, we will consider the Levenshtein edit distance (LE), Jaro (J), Jaro Winkler (JW), Smith Waterman (SW), Smith Waterman with Gotohs backtracing (GH), Needleman Wunch (NW) and Monge Elkan (ME) string similarities (Elmagarmid et al., 2007; Christen, 2012; Naumann and Herschel, 2010). All of them return a number between 0 and 1 inclusively, 3http://www.iisg.nl/hsn/data/occupations.html 48 where the highest value when two names are exactly the same. Table 3 shows an example of computed character-based similarities for three name pair-variants. 3.2 Phonetic-Based Similarity Phonetic similarity functions analyze the sounds of the names being compared instead of their spelling differences. For example, the two names Stefan and Stephan barely differ phonetically, but nevertheless they have different spellings. Phonetic functions encode every name with phonetic keys based on a </context>
<context position="24599" citStr="Naumann and Herschel, 2010" startWordPosition="4044" endWordPosition="4047"> pairwise terms comparison for completely different dataset domains. Nevertheless, the bottleneck of the algorithm is that it is expensive to apply it to real-world data and compare all possible combinations of records. Table 8: Example of FP pairs of names according to the maximum value of SW, GH and ME functions There are various available techniques for reducing the amount of candidate pairs to be compared. Common techniques are partitioning data into smaller subsets and comparing only records with the same partition. Two widely used partition approaches are blocking and windowing methods (Naumann and Herschel, 2010; Bilenko et al., 2003). The blocking technique assigns to each record a special blocking key, for instance year, place of the documents or the first 3 letters of the last name. The windowing technique such as 53 (a) The dataset of names (b) The dataset of occupations (c) The dataset of places (d) Public dataset of restaurants Figure 3: Evaluation of single string similarities and the hybrid approach in terms of F-Measure and Threshold Sorted Neighborhood method sorts data according to some key, for instance year of the documents, and then slides a window of fixed size across the sorted data. </context>
</contexts>
<marker>Naumann, Herschel, 2010</marker>
<rawString>Felix Naumann and Melanie Herschel. 2010. An Introduction to Duplicate Detection. Morgan and Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Navarro</author>
</authors>
<title>A guided tour to approximate string matching.</title>
<date>2001</date>
<journal>ACM Comput. Surv.,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="3406" citStr="Navarro, 2001" startWordPosition="507" endWordPosition="508"> or even muzikant bij de tiende afd. The latter means the musician in the 10th department. The past few decades have seen a large research interest in the problem of inaccurate information. As a result, a large number of methods for comparing string has been developed. These standard methods are called string similarity functions. Some of those well known techniques are character-based, token-based or based on phonetic functions, for instance Levenshtein Edit distance, Jaro Winkler distance, Monge Elkan distance, Smith Waterman distance, Soundex and Double Metaphone. (Elmagarmid et al., 2007; Navarro, 2001; Winkler, 1995). Each of the mentioned similarity functions perform optimally for a particular dataset domain. For example, the phonetic function Soundex works great for encoding names by sound as it pronounced in English, but nevertheless sometimes it is also used to encode names in other European languages. However, only little work has been done in studying combinations of similarity functions, and in their simultaneous use for achieving more reliable results. Bilenko (2003) in his work computes names similarity with 47 Proceedings of the 8th Workshop on Language Technology for Cultural He</context>
</contexts>
<marker>Navarro, 2001</marker>
<rawString>Gonzalo Navarro. 2001. A guided tour to approximate string matching. ACM Comput. Surv., 33(1):31–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
<author>Peter N Yianilos</author>
<author>Senior Member</author>
</authors>
<title>Learning string edit distance.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>20--522</pages>
<marker>Ristad, Yianilos, Member, 1998</marker>
<rawString>Eric Sven Ristad, Peter N. Yianilos, and Senior Member. 1998. Learning string edit distance. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20:522–532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvan Saeys</author>
<author>I˜naki Inza</author>
<author>Pedro Larra˜naga</author>
</authors>
<title>A review of feature selection techniques in bioinformatics.</title>
<date>2007</date>
<journal>Bioinformatics,</journal>
<volume>23</volume>
<issue>19</issue>
<marker>Saeys, Inza, Larra˜naga, 2007</marker>
<rawString>Yvan Saeys, I˜naki Inza, and Pedro Larra˜naga. 2007. A review of feature selection techniques in bioinformatics. Bioinformatics, 23(19):2507–2517, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheila Tejada</author>
<author>Craig A Knoblock</author>
<author>Steven Minton</author>
</authors>
<title>Learning domain-independent string transformation weights for high accuracy object identification.</title>
<date>2002</date>
<booktitle>In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’02,</booktitle>
<pages>350--359</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4359" citStr="Tejada et al. (2002)" startWordPosition="648" endWordPosition="651">work has been done in studying combinations of similarity functions, and in their simultaneous use for achieving more reliable results. Bilenko (2003) in his work computes names similarity with 47 Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 47–55, Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics affine gaps to train the Support Vector Machines classifier. Ristard and Yianilos (1998) designed a learnable Levenshtein distance for solving the string similarity problem. Tejada et al. (2002) learned weights of different types of string transformations. In this paper we explore various traditional string similarity functions for solving data ambiguities and also design a supervised hybrid technique. We carry out our experiments on three manually constructed datasets: Dutch names, occupations and places, and also on one publicly available dataset of restaurants. The clarified function, that will allow us to recognize difficult ambiguities in textual fields, later will be incorporated into the overall ER process for a large historical database. The main contributions of this paper i</context>
</contexts>
<marker>Tejada, Knoblock, Minton, 2002</marker>
<rawString>Sheila Tejada, Craig A. Knoblock, and Steven Minton. 2002. Learning domain-independent string transformation weights for high accuracy object identification. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’02, pages 350–359. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco H D van Leeuwen</author>
<author>Ineke Maas</author>
<author>Andrew Miles</author>
</authors>
<title>HISCO. Historical international standard classification of occupations.</title>
<date>2002</date>
<publisher>Leuven University Press.</publisher>
<marker>van Leeuwen, Maas, Miles, 2002</marker>
<rawString>Marco H. D. van Leeuwen, Ineke Maas, and Andrew Miles. 2002. HISCO. Historical international standard classification of occupations. Leuven University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Verma</author>
</authors>
<date>2012</date>
<booktitle>Data Analysis in Management with SPSS</booktitle>
<publisher>Software. Springer.</publisher>
<contexts>
<context position="17356" citStr="Verma, 2012" startWordPosition="2807" endWordPosition="2808">the score function we use a training dataset B. We explore 2 robust classifiers that could be applied to cultural heritage dataset domains. They are the Logistic Regression (LG) and the Support Vector Machine (SVM) (Hastie et al., 2003; Cristianini and ShaweTaylor, 2000). They are two of the most widelyused classifiers that are suitable for the prediction task (James et al., 2013). It is important to add that we also carried out our experiments and applied three more classifiers, namely Linear Discriminant Analysis, Quadratic Discriminant Analysis and k-nearest neighbors (Hastie et al., 2003; Verma, 2012; Zezula et al., 2006). However results were not improved significantly on all of our datasets, so we do not include those classifiers in the designed hybrid approach. 5 The Prediction Models In this Section we will briefly describe models that we incorporated into our hybrid approach to address the problem of inaccurate cultural heritage data. 5.1 Logistic regression We apply a logistic regression as a predictive model and calculate the score function as follows: Simhb(ai, aj) = 1 1 + e−z , (1) where z = w0 + w1 * sim1(ai, aj) + w2 * sim2(ai, aj) + ··· + wn * simK(ai, aj) is an utilization of</context>
</contexts>
<marker>Verma, 2012</marker>
<rawString>J P Verma. 2012. Data Analysis in Management with SPSS Software. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William E Winkler</author>
</authors>
<title>Matching and record linkage.</title>
<date>1995</date>
<booktitle>In Business Survey Methods,</booktitle>
<pages>355--384</pages>
<publisher>Wiley.</publisher>
<contexts>
<context position="3422" citStr="Winkler, 1995" startWordPosition="509" endWordPosition="510">nt bij de tiende afd. The latter means the musician in the 10th department. The past few decades have seen a large research interest in the problem of inaccurate information. As a result, a large number of methods for comparing string has been developed. These standard methods are called string similarity functions. Some of those well known techniques are character-based, token-based or based on phonetic functions, for instance Levenshtein Edit distance, Jaro Winkler distance, Monge Elkan distance, Smith Waterman distance, Soundex and Double Metaphone. (Elmagarmid et al., 2007; Navarro, 2001; Winkler, 1995). Each of the mentioned similarity functions perform optimally for a particular dataset domain. For example, the phonetic function Soundex works great for encoding names by sound as it pronounced in English, but nevertheless sometimes it is also used to encode names in other European languages. However, only little work has been done in studying combinations of similarity functions, and in their simultaneous use for achieving more reliable results. Bilenko (2003) in his work computes names similarity with 47 Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social S</context>
</contexts>
<marker>Winkler, 1995</marker>
<rawString>William E. Winkler. 1995. Matching and record linkage. In Business Survey Methods, pages 355–384. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Zezula</author>
<author>Giuseppe Amato</author>
<author>Vlastislav Dohnal</author>
<author>Michal Batko</author>
</authors>
<title>Similarity search: the metric space approach.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="17378" citStr="Zezula et al., 2006" startWordPosition="2809" endWordPosition="2812">ction we use a training dataset B. We explore 2 robust classifiers that could be applied to cultural heritage dataset domains. They are the Logistic Regression (LG) and the Support Vector Machine (SVM) (Hastie et al., 2003; Cristianini and ShaweTaylor, 2000). They are two of the most widelyused classifiers that are suitable for the prediction task (James et al., 2013). It is important to add that we also carried out our experiments and applied three more classifiers, namely Linear Discriminant Analysis, Quadratic Discriminant Analysis and k-nearest neighbors (Hastie et al., 2003; Verma, 2012; Zezula et al., 2006). However results were not improved significantly on all of our datasets, so we do not include those classifiers in the designed hybrid approach. 5 The Prediction Models In this Section we will briefly describe models that we incorporated into our hybrid approach to address the problem of inaccurate cultural heritage data. 5.1 Logistic regression We apply a logistic regression as a predictive model and calculate the score function as follows: Simhb(ai, aj) = 1 1 + e−z , (1) where z = w0 + w1 * sim1(ai, aj) + w2 * sim2(ai, aj) + ··· + wn * simK(ai, aj) is an utilization of a linear regression m</context>
</contexts>
<marker>Zezula, Amato, Dohnal, Batko, 2006</marker>
<rawString>Pavel Zezula, Giuseppe Amato, Vlastislav Dohnal, and Michal Batko. 2006. Similarity search: the metric space approach. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
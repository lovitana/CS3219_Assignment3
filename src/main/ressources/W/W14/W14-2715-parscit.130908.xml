<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019363">
<title confidence="0.995106">
Collective Stance Classification of Posts in Online Debate Forums
</title>
<author confidence="0.983669">
Dhanya Sridhar Lise Getoor Marilyn Walker
</author>
<affiliation confidence="0.988124">
Computer Science Dept. Computer Science Dept. Computer Science Dept.
</affiliation>
<address confidence="0.837501">
UC Santa Cruz UC Santa Cruz UC Santa Cruz
</address>
<email confidence="0.997813">
dsridhar@soe.ucsc.edu getoor@soe.ucsc.edu maw@soe.ucsc.edu
</email>
<sectionHeader confidence="0.993871" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999939628571429">
Online debate sites are a large source of
informal and opinion-sharing dialogue on
current socio-political issues. Inferring
users’ stance (PRO or CON) towards dis-
cussion topics in domains such as politics
or news is an important problem, and is
of utility to researchers, government or-
ganizations, and companies. Predicting
users’ stance supports identification of so-
cial and political groups, building of better
recommender systems, and personaliza-
tion of users’ information preferences to
their ideological beliefs. In this paper, we
develop a novel collective classification
approach to stance classification, which
makes use of both structural and linguis-
tic features, and which collectively labels
the posts’ stance across a network of the
users’ posts. We identify both linguistic
features of the posts and features that cap-
ture the underlying relationships between
posts and users. We use probabilistic soft
logic (PSL) (Bach et al., 2013) to model
post stance by leveraging both these local
linguistic features as well as the observed
network structure of the posts to reason
over the dataset. We evaluate our approach
on 4FORUMS (Walker et al., 2012b), a col-
lection of discussions from an online de-
bate site on issues ranging from gun con-
trol to gay marriage. We show that our col-
lective classification model is able to eas-
ily incorporate rich, relational information
and outperforms a local model which uses
only linguistic information.
</bodyText>
<sectionHeader confidence="0.999326" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99959775">
Modeling user stance (PRO, CON) in discussion
topics in online social media debate is of inter-
est to researchers, corporations and governmental
organizations alike. Predicting a user’s stance to-
wards a given issue can support the identification
of social or political groups (Gawron et al., 2012;
Abu-Jbara et al., 2012; Anand et al., 2011; Qiu et
al., 2013; Hasan and Ng, 2013), help develop bet-
ter recommendation systems, or tailor users’ infor-
mation preferences to their ideologies and beliefs.
Stance classification problems consist of a collec-
tion of debate-style discussions by authors on dif-
ferent controversial, political topics.
While these may be spoken as in the Congres-
sional Debates corpus (Thomas et al., 2006; Bur-
foot, 2008), we focus on forum posts on social
media debate sites. Users on debate sites share
their opinions freely, using informal and social
language, providing a rich and much more chal-
lenging domain for stance prediction.
Social media debate sites contain online discus-
sions with posts from various authors, where each
post is either a response to another post or the root
of the discussion (Anand et al., 2011; Walker et
al., 2012a). Posts are linked to one another by ei-
ther rebuttal or agreement links and are labelled
for stance, either PRO or CON, depending on the
framing of the issue under discussion. Each post
reflects the stance and sentiment of its author. Au-
thors may participate in multiple discussions in the
same topic, and may discuss multiple topics. For
example consider the sample posts from the online
discussion forum 4forums.com shown in Fig.
1. Here, we see discussion topics, together with
sample quotes and responses, where the response
is a direct reply to the quote text. The annotations
for stance were gathered using Amazon’s Mechan-
ical Turk service with an interface that allowed an-
notators to see complete discussions. Quotes pro-
vide additional context that were used by human
annotators in a separate task for annotating agree-
ment and disagreement (Misra and Walker, 2013).
Responses can be labeled as either PRO or CON to-
ward the topic. For the example shown in Fig. 1,
</bodyText>
<page confidence="0.987812">
109
</page>
<note confidence="0.78375">
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 109–117,
Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.880139222222222">
Quote Q, Response R Stance Topic
Q: I thought I’d start a new thread for those newcomers who don’t want to be shocked by sick
minded nazi XXXX. Anyway... When are fetuses really alive, and how many fetuses are actually
aborted (murdered) before that time?
R: The heart starts beating 3 weeks after conception, and you can’t live without a beating heart, CON Abortion
but me personally, I think that as soon as the miracle starts, (egg and sperm combine) that is
when life begins. I know it’s more of a spiritual thing for me instead of a fact.:)
Q2: Most americans support a Federal Marriage Amendment. Defining Marriage as a union
between a man and a woman. Federal Marriage Amendment. This is the text of the Amend:
Marriage in the United States shall consist only of the union of a man and a woman. Neither
this constitution or the constitution of any state, nor state or federal law, shall be construed to
require that marital status or the legal incidents thereof be conferred upon unmarried couples or
groups.
R2: Debator, why does it bother you so much that some people are gay? Its a sexual prefference. PRO Gay
People like certain things when they have sex. Example: A man likes a women with small boobs. Mar-
Or, a man likes a women with nice legs. People like the way certain things feel (I’m not giving riage
te example for that one;) ). So why does it bother people that someone’s sexual prefference is
just a little kinkier than thiers?
</table>
<figureCaption confidence="0.9307655">
Figure 1: Sample Quote/Response Pair from 4forums.com with Mechanical Turk annotations for stance.
Both response posts are from the same author.
</figureCaption>
<bodyText confidence="0.99967528125">
both response posts are from the same author. We
describe the dataset further in Section 4.1.
We believe that models of post stance in on-
line debate should capture both the content and the
context of author posts. By jointly reasoning over
both the content of the post and its relationships
with other posts in the discussion, we perform col-
lective classification, as we further define in Sec-
tion 3 (Sen et al., 2008). Previous work has shown
that collective classification models often perform
better than content-only approaches. (Burfoot et
al., 2011; Hasan and Ng, 2013; Thomas et al.,
2006; Bansal et al., 2008; Walker et al., 2012c).
Here, we develop a collective classification ap-
proach for stance prediction which leverages the
sentiment conveyed in a post through its language,
and the reply links consisting of agreements or re-
buttals between posts in a discussion. We imple-
ment our approach using Probabilistic Soft Logic
(PSL) (Bach et al., 2013), a recently introduced
tool for collective inference in relational data. We
evaluate our model on data from the 4FORUMS
online debate site (Walker et al., 2012b).
Section 2 first presents an overview of our ap-
proach and then in Section 3.1 we describe the
PSL framework in more detail. Section 4 de-
scribes the evaluation data and our results show-
ing that the PSL model improves prediction of post
stance in the 4Forums dataset. In Section 5 we
describe related work, and compare with our pro-
posed approach. Section 6 summarizes our ap-
proach and results.
</bodyText>
<sectionHeader confidence="0.849503" genericHeader="method">
2 Overview of Approach
</sectionHeader>
<bodyText confidence="0.99975615625">
Given a set of topics {t1 ... tn}, where each topic
ti consists of a set of discussions {di1 ... dij}, we
model each discussion dk as a collection of posts
{pk0, . . . , pkm}, where each post pki is mapped to
its author ai.
A discussion di E D is a tree of posts, starting
with the initial post pi0. We distinguish between
posts that start a new thread (root) and others (non-
root). Each non-root post pij is the response to
some previous post pik, where k &lt; j, and we refer
to pik as the parent of pij. For a subset of the posts,
pij has been annotated with a real valued number
in the interval [−5, 5] that denotes whether the post
disagrees or agrees with its parent. Values &lt; 0 are
considered disagreement and values &gt; 1, as agree-
ment. We discard the posts where the annotations
are in the interval (0, 1) since those indicate high
annotator uncertainty about agreement.
Fig. 2 illustrates an example of three discussion
trees for two topics where author a2 participates
in multiple discussions of the same topic and a3
and a4 participate in multiple topics. An author
directly replies with a post to another author’s post
and either disagrees or agrees.
Each post pij in discussion di is also mapped to
{xij., ... , xijn, } linguistic features as described in
Section 3.2.1 as well as yij, the stance label (PRO,
CON) towards the discussion topic ti.
We say that aj participates in topic ti if there
exist any posts pj E di with author aj.
Using the tree structure and posts that have an-
notations for agreement or disagreement, we con-
</bodyText>
<page confidence="0.998219">
110
</page>
<figureCaption confidence="0.726619">
Figure 2: Example of 3 discussions in (a), (b) and (c). Dotted lines denote the ‘writes’ relation between
authors and posts and dashed lines denote the ‘disagrees’ relation between posts and between authors.
Authors can participate in multiple discussions of the same topic, shown by a2 in both (a) and (b).
</figureCaption>
<bodyText confidence="0.990966">
Moreover, authors may post in multiple topics, as shown by a3 and a4 in both (b) and (c), and may
interact with the same authors multiple times, as shown again in (b) and (c).
sider the network graph G of disagreement and
agreement between posts and between authors,
where the vertices are posts {p0, ... , pm} and au-
thors {a0, ... , an}. A disagreement edge exists
from post pu to pv if pu disagrees with pv.
A disagreement edge exists from aw to ay if any
of the posts {pw, ... , px} mapped to aw disagree
with any posts {py, ... pz} mapped to ay. We sim-
ilarly define agreement edges for both posts and
authors.
</bodyText>
<sectionHeader confidence="0.993683" genericHeader="method">
3 Collective Classification of Stance
</sectionHeader>
<bodyText confidence="0.999975857142857">
Given the discussion structure defined in the pre-
vious section, our task is to infer the stance of each
post. We make use of both linguistic features and
the relational structure in order to collectively or
jointly infer the stance labels. This corresponds to
a collective classification setting (Sen et al., 2008),
in which we are given a multi-relational network
and some partially observed labels, and we wish
to infer all of the unobserved labels, conditioned
on observed attributes and links. Collective clas-
sification refers to the combined classification of
a set of interdependent objects (posts, in our do-
main) using information given by both the local
features of the objects and the properties of the
objects’ neighbors in a network. For the stance
classification problem, we infer stance labels for
posts using both the correlation between a post and
its linguistic attributes {xij1, ... , xijN}, and the
labels and attributes of its neighbors in observed
network graph G. We use PSL, described below,
to perform collective classification.
</bodyText>
<subsectionHeader confidence="0.998562">
3.1 Probabilistic Soft Logic
</subsectionHeader>
<bodyText confidence="0.994222161290323">
Probabilistic soft logic (PSL) is a framework for
probabilistic modeling and collective reasoning in
relational domains (Kimmig et al., 2012; Bach et
al., 2013). PSL provides a declarative syntax and
uses first-order logic to define a templated undi-
rected graphical model over continuous random
variables. Like other statistical relational learn-
ing methods, dependencies in the domain are cap-
tured by constructing rules with weights that can
be learned from data.
But unlike other statistical relational learning
methods, PSL relaxes boolean truth values for
atoms in the domain to soft truth values in the in-
terval [0,1]. In this setting, finding the mostproba-
ble explanation (MPE), a joint assignment of truth
values to all random variable ground atoms, can be
done efficiently.
For example, a typical PSL rule looks like the
following:
P(A, B) n Q(B, C) → R(A, C)
where P, Q and R are predicates that represent
observed or unobserved attributes in the domain,
and A, B, and C are variables. For example, in
our 4FORUMS domain, we consider an observed
attribute such as writesPost(A, P) and infer an un-
observed attribute (or label) such as isProPost(P,
T). Instantiation of predicates with data is called
grounding (e.g. writesPost(A2, P7)), and each
ground predicate, often called ground atom, has a
soft truth value in the interval [0,1]. To build a PSL
model for stance classification, we represent posts
</bodyText>
<page confidence="0.901421">
111
</page>
<equation confidence="0.995135583333333">
isProPost(P, T) ∧ writesPost(A, P) → isProAuth(A, T)
¬ isProPost(P, T) ∧ writesPost(A, P) → ¬ isProAuth(A, T)
agreesPost(P, P2) ∧ isProPost(P, T) → isProPost(P2, T)
agreesPost(P, P2) ∧¬ isProPost(P, T) → ¬ isProPost(P2, T)
disagreesPost(P, P2) ∧ isProPost(P, T) → ¬ isProPost(P2, T)
disagreesPost(P, P2) ∧¬ isProPost(P, T) → isProPost(P2, T)
agreesAuth(A, A2) ∧ isProAuth(A, T) → isProAuth(A, T)
agreesAuth(A, A2) ∧¬ isProAuth(A, T) → ¬isProAuth(A2, T)
disagreesAuth(A, A2) ∧ isProAuth(A, T) → ¬ isProAuth(A2, T)
disagreesAuth(A, A2) ∧¬ isProAuth(A, T) → isProAuth(A2, T)
hasLabelPro(P, T) → isProPost(P, T)
¬ hasLabelPro(P, T) → ¬ isProPost(P, T)
</equation>
<tableCaption confidence="0.93732">
Table 1: Rules for PSL model, where P = post, T = Topic, and A = Author.
</tableCaption>
<bodyText confidence="0.999908826086957">
and authors as variables and specify predicates to
encode different interactions, such as writes, be-
tween them. Domain knowledge is captured by
writing rules with weights that govern the rela-
tive importance of the dependencies between pred-
icates. The groundings of all the rules result in
an undirected graphical model that represents the
joint probability distribution of assignments for all
unobserved atoms, conditioned on the observed
atoms.
Triangular norms, which are continuous relax-
ations of logical AND and OR, are used to com-
bine the atoms in the first-order clauses. As a
result of the soft truth values and the triangu-
lar norms, the underlying probabilistic model is
a hinge-loss Markov Random Field (HL-MRF).
Inference in HL-MRFs is a convex optimization,
which leads to a significant improvement in effi-
ciency over discrete probabilistic graphical mod-
els. Thus, PSL offers a very natural interface to
compactly represent stance classification as a col-
lective classification problem, along with methods
to reason about our domain.
</bodyText>
<subsectionHeader confidence="0.971909">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.999955333333333">
We extract both linguistic features that capture the
content of a post and features that capture multiple
relations from our dataset.
</bodyText>
<subsectionHeader confidence="0.771122">
3.2.1 Linguistic Features
</subsectionHeader>
<bodyText confidence="0.999974">
To capture the content of a post, on top of a bag-of-
words representation with unigrams and bigrams,
we also consider basic lengths, discourse cues,
repeated punctuation counts and counts of lex-
ical categories based on the Linguistic Inquiry
and Word Count tool (LIWC) (Pennebaker et al.,
2001). Basic length features capture the number
of sentences, words, and characters, along with
the average word and sentence lengths for each
post. The discourse cues feature captures fre-
quency counts for the first few words of the post,
which often contain discourse cues. To capture
the information in repeated punctuation like “!!”,
“??” or “?!” we include the frequency count of the
given punctuation patterns as a feature of each post
(Anand et al., 2011). LIWC counts capture senti-
ment by giving the degree to which the post uses
certain categories of subjective language.
</bodyText>
<subsubsectionHeader confidence="0.640196">
3.2.2 Relational Information
</subsubsectionHeader>
<bodyText confidence="0.999864666666667">
As our problem domain contains relations be-
tween both authors and posts, for our PSL model,
we consider the relations between authors, be-
tween posts and between authors and posts. As de-
scribed above, in PSL, we model these relations as
first-order predicates. In Section 3.3, we describe
how we populate the predicates with observations
from our data.
Author Information We observe that authors
participate in discussions by writing posts. For
a subset of authors, we have annotations for their
interactions with other authors as either disagree-
ment or agreement, as given by network graph
G. We encode this with the following predi-
cates: writesPost(A, P), disagreesAuth(A1, A2),
agreesAuth(A1, A2).
Post Information Posts are linked to the topic
of their given discussion, and to other posts in
their discussion through disagreement or agree-
ment. Additionally, we include a predicate for post
stance towards its topic as predicted by a classifier
</bodyText>
<page confidence="0.992744">
112
</page>
<bodyText confidence="0.9995336">
that only uses linguistic features, as described in
Section 3.3, as prior information. We capture these
relations with the following predicates: hasLabel-
Pro(P, T), hasTopic(P, T), disagreesPost(P1, P2),
agreesPost(P1, P2).
</bodyText>
<subsectionHeader confidence="0.847781">
3.2.3 Target attributes
</subsectionHeader>
<bodyText confidence="0.995775">
Our goal is to 1) predict the stance relation be-
tween a post and its topic, namely, PRO or CON and
2) predict the stance relation between an author
and a topic. In our PSL model, our target predi-
cates are isProPost(P, T) and isProAuth(A, T).
</bodyText>
<subsectionHeader confidence="0.930225">
3.3 PSL Model
</subsectionHeader>
<bodyText confidence="0.999503947368421">
We construct our collective stance classification
model in PSL using the predicates listed above.
For disagreement/agreement annotations in the in-
terval [-5, 5], we consider values [-5,0] as evidence
for the disagreesAuth relation and values [1, 5] as
evidence for the agreesAuth relation. We discard
observations with annotations in the interval [0,1]
because it indicates a very weak signal of agree-
ment, which is already rare on debate sites. We
populate disagreesPost and agreesPost in the same
way as described above.
For each relation, we populate the correspond-
ing predicate with all the instances that we observe
in data and we fix the truth value of each observa-
tion as 1. For all such predicates where we observe
instances in the data, we say that the predicate is
closed. For the relations isPostPro and isAuthPro
that we predict through inference, a truth value of
1 denotes a PRO stance and a truth value of 0 de-
notes a CON stance. We say that those predicates
are open, and the goal of inference is to jointly as-
sign truth values to groundings of those predicates.
We use our domain knowledge to describe rules
that relate these predicates to one another. We fol-
low our intuition that agreement between nodes
implies that they have the same stance, and dis-
agreement between nodes implies that they have
opposite stances. We relate post and author nodes
to each other by supposing that if a post is PRO
towards its topic, then its author will also be PRO
towards that topic.
We construct a classifier that takes as input the
linguistic features of the posts and outputs predic-
tions for stance label of each post. We then con-
sider the labels predicted by the local classifier as
a prior for the inference of the target attributes in
our PSL model. Table 1 shows the rules in our
PSL model.
</bodyText>
<table confidence="0.998661166666667">
Topic Authors Posts
Abortion 385 8114
Evolution 325 6186
Gun Control 319 3899
Gay Marriage 316 7025
Death Penalty 170 572
</table>
<tableCaption confidence="0.998843">
Table 2: Overview of topics in 4FORUMSdataset.
</tableCaption>
<sectionHeader confidence="0.989033" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.9999">
We first describe the dataset we use for evaluation
and then describe our evaluation method and re-
sults.
</bodyText>
<subsectionHeader confidence="0.922213">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999997472222222">
We evaluate our proposed approach on discus-
sions from https://www.4forums.com, an
online debate site on social and political issues.
The dataset is publicly available as part of the
Internet Argument Corpus, an annotated collec-
tion of 109,533 forum posts (Walker et al., 2012b;
Walker et al., 2012c). On 4FORUMS, a user ini-
tiates a discussion by posting a new question or
comment under a topic, or participate in an ongo-
ing discussion by replying to any of the posts in
the thread. The discussions were given to English
speaking Mechanical Turk annotators for a num-
ber of annotation tasks to get labels for the stances
of discussion participants towards the topic, and
scores for each post in a discussion indicating
whether it is in agreement or disagreement with
the preceding post.
The scores for agreement and disagreement
were on a 11 point scale [-5, 5] implemented using
a slider, and annotators were given quote/response
pairs to determine if the response text agreed
or disagreed with the quote text. We use the
mean score across the 5-7 annotators used in the
task. A more negative value indicates higher
inter-annotator confidence of disagreement, and a
more positive value indicates higher confidence of
agreement. The gold-standard annotation used for
the stance label of each post is given by the ma-
jority annotation among 3-8 Mechanical Turk an-
notators performed as a separate task, using en-
tire discussions to determine the stance of the au-
thors in the discussion towards the topic. We use
the stance of each post’s author to determine the
post’s stance. For our experiments, we use all
posts with annotations for stance, and about 90%
of these posts also have annotations for agree-
</bodyText>
<page confidence="0.998173">
113
</page>
<bodyText confidence="0.9984744">
ment/disagreement.
The discussions span many topics, and Table 2
gives a summary of the topics we consider in our
experiments and the distribution of posts across
these topics. Each post in a discussion comes as
a quote-response pair, where the quote is the text
that the post is in response to, and the response is
the post text. We refer to (Walker et al., 2012b) for
a full description of the corpus and the annotation
process.
</bodyText>
<subsectionHeader confidence="0.961728">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999994783783784">
In order to evaluate our methods, we split the
dataset into training and testing sets by randomly
selecting half the authors from each topic and their
posts for the training set and using the remaining
authors and their posts for the test set. This way,
we ensure that no two authors appear in both train-
ing and test sets for the same topic, since stance
is topic-dependent. We create 10 randomly sam-
pled train/test splits for evaluation. Each split con-
tains about 18,000 posts. For each train/test split,
we train a linear SVM for each topic, with the
L2-regularized-L1-loss SVM implemented in the
LibLINEAR package (Fan et al., 2008). We use
only the linguistic features from the posts, for each
topic in the training set. We refer to the baseline
model which only uses the the output of the SVM
as the LOCAL model. We output the predictions
from LOCAL model and get stance labels for posts
in both the training and test sets. We use the pre-
dictions as prior information for the true stance la-
bel in our PSL model, with the hasLabel predicate.
We use the gold standard stance annotation
(PRO, CON) for each post as ground truth for
weight learning and inference. A truth value of 1
for isPostPro and isAuthPro denotes a PRO stance
and a truth value of 0 denotes a CON stance. We
learn the weights of our PSL model (initially set to
1) for each of our training sets and perform infer-
ence on each of the test sets.
Table 3 shows averages for F1 score for the pos-
itive class (PRO), area under the precision-recall
curve (AUC-PR) for the negative class (CON) and
area under the ROC curve (AUROC) over the 10
train/test splits. For the PSL model, the measures
are computed for joint inference over all topics
in the test sets. For the per-topic linear SVMs
(LOCAL model), we compute the measures indi-
vidually for the predictions of each topic in the
test sets and take a weighted average over the
topics. Our PSL model outperforms the LOCAL
model, with statistically significant improvements
in the F1 score and AUC-PR for the negative class.
Moreover, our model completes weight learning
and inference on the order of seconds, boasting an
advantage in computational efficiency, while also
maintaining model interpretability.
Table 4 shows the weights learned by the PSL
model for the rules in one of the train/test splits
of the experiment. The first two rules relating
post stance and author stance are weighted more
heavily, in part because the writesPost predicate
has a grounding for each author-post pair and con-
tributes to lots of groundings of the rule. The rules
that capture the alternating disagreement stance
also have significant weight, while the rules denot-
ing agreement both between posts and between au-
thors are weighted least heavily since there are far
fewer instances of agreement than disagreement.
This matches our intuition of political debates.
We also explored variations of the PSL model
by removing the first two rules relating post stance
and author stance and found that the weight learn-
ing algorithm drove the weights of the other
rules close to 0, worsening the performance.
We also removed rules 3-10 that capture agree-
ment/disagreement from the model, and found that
the model performs poorly when disregarding the
links between nodes entirely. The PSL model
learns to weight the first and second rule very
highly, and does worse than when considering the
prior alone. Thus, the combination of the rules
gives the model its advantage, allowing the PSL
model to make use of a richer structure that has
multiple types of relations and more information.
</bodyText>
<sectionHeader confidence="0.999876" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999910642857143">
Over the last ten years, there has been significant
progress on modeling stance. Previous work cov-
ers three different debate settings: (1) congres-
sional debates (Thomas et al., 2006; Bansal et
al., 2008; Yessenalina et al., 2010; Balahur et al.,
2009); (2) company-internal discussion sites (Mu-
rakami and Raymond, 2010; Agrawal et al., 2003);
and (3) online social and political public forums
(Somasundaran and Wiebe, 2009; Somasundaran
and Wiebe, 2010; Wang and Ros´e, 2010; Biran
and Rambow, 2011; Walker et al., 2012c; Anand
et al., 2011). Debates in online public forums
(e.g. Fig. 1) differ from debates in congress and
on company discussion sites because the posts are
</bodyText>
<page confidence="0.995515">
114
</page>
<table confidence="0.991603">
Classifier F1 Score AUC-PR negative class AUROC
LOCAL 0.66 ± 0.015 0.44 ± 0.04 0.54 ± 0.02
PSL 0.74 ± 0.04 0.511 ± 0.04 0.59 ± 0.05
</table>
<tableCaption confidence="0.953796">
Table 3: Averages and standard deviations for F1 score for the positive class, area under PR curve for the
negative class, and area under ROC curve for post stance over 10 train/test splits.
</tableCaption>
<equation confidence="0.979054333333333">
isProPost(P, T) ∧ writesPost(A, P) → isProAuth(A, T) : 10.2
¬ isProPost(P, T) ∧ writesPost(A, P) → ¬ isProAuth(A, T) : 8.5
agreesPost(P, P2) ∧ isProPost(P, T) → isProPost(P2, T) : 0.003
agreesPost(P, P2) ∧¬ isProPost(P, T) → ¬ isProPost(P2, T) : 0.003
disagreesPost(P, P2) ∧ isProPost(P, T) → ¬ isProPost(P2, T) : 0.06
disagreesPost(P, P2) ∧¬ isProPost(P, T) → isProPost(P2, T) : 0.11
agreesAuth(A, A2) ∧ isProAuth(A, T) → isProAuth(A, T) : 0.001
agreesAuth(A, A2) ∧¬ isProAuth(A, T) → ¬isProAuth(A2, T) : 0.0
disagreesAuth(A, A2) ∧ isProAuth(A, T) → ¬ isProAuth(A2, T) : 0.23
disagreesAuth(A, A2) ∧¬ isProAuth(A, T) → isProAuth(A2, T) : 0.6
hasLabelPro(P, T) → isProPost(P, T) : 2.2
¬ hasLabelPro(P, T) → ¬ isProPost(P, T) : 4.8
</equation>
<tableCaption confidence="0.984933">
Table 4: Weights learned by the model for the PSL rules in train/test split 2 of experiments
</tableCaption>
<bodyText confidence="0.99616934375">
shorter and the language is more informal and so-
cial. We predict that this difference makes it more
difficult to achieve accuracies as high for 4FO-
RUMS discussions as can be achieved for the con-
gressional debates corpus.
Work by (Somasundaran and Wiebe, 2009) on
idealogical debates very similar to our own show
that identifying argumentation structure improves
performance; their best performance is approxi-
mately 64% accuracy over all topics. Research by
(Thomas et al., 2006; Bansal et al., 2008; Yesse-
nalina et al., 2010; Balahur et al., 2009) classifies
the speaker’s stance in a corpus of congressional
floor debates. This work combines graph-based
and text-classification approaches to achieve 75%
accuracy on Congressional debate siding over all
topics. Other work applies MaxCut to the re-
ply structure of company discussion forums (Mal-
ouf and Mullen, 2008; Murakami and Raymond,
2010; Agrawal et al., 2003). Murakami &amp; Ray-
mond (2010) show that rules for identifying agree-
ment, defined on the textual content of the post
improve performance.
More recent work has explicitly focused on the
benefits of collective classification in these set-
tings (Burfoot et al., 2011; Hasan and Ng, 2013;
Walker et al., 2012c), and has shown in each
case that collective classification improves perfor-
mance. The results reported here are the first to
apply the PSL collective classification framework
to the forums conversations from the IAC corpus
(Anand et al., 2011; Walker et al., 2012c).
</bodyText>
<sectionHeader confidence="0.998618" genericHeader="discussions">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999979666666667">
Here, we introduce a novel approach to classify
stance of posts from online debate forums with a
collective classification framework. We formally
construct a model, using PSL, to capture relational
information in the network of authors and posts
and our intuition that agreement or disagreement
between users correlates to their stance towards a
topic. Our initial results are promising, showing
that by incorporating more complex interactions
between authors and posts, we gain improvements
over a content-only approach. Our approach is
ideally suited to collective inference in social me-
dia. It easily extendable to use additional rela-
tional information, and richer behavioral and lin-
guistic information.
</bodyText>
<sectionHeader confidence="0.9983" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9892256">
Thanks to Pranav Anand for providing us with the
stance annotations for the 4forums dataset. This
work is supported by National Science Foundation
under Grant Nos. IIS1218488, CCF0937094 and
CISE-RI 1302668.
</bodyText>
<page confidence="0.998651">
115
</page>
<sectionHeader confidence="0.98985" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999455085714286">
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and
Dragomir Radev. 2012. Subgroup detection in ide-
ological discussions. In Association for Computa-
tional Linguistics (ACL), pages 399–409.
R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu.
2003. Mining newsgroups using networks arising
from social behavior. In International Conference
on World Wide Web (WWW), pages 529–535. ACM.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats Rule and Dogs Drool: Classifying
Stance in Online Debate. In ACL Workshop on Sen-
timent and Subjectivity.
Stephen H. Bach, Bert Huang, Ben London, and Lise
Getoor. 2013. Hinge-loss markov random fields:
Convex inference for structured prediction. In Un-
certainty in Artificial Intelligence (UAI).
A. Balahur, Z. Kozareva, and A. Montoyo. 2009. De-
termining the polarity and source of opinions ex-
pressed in political debates. Computational Linguis-
tics and Intelligent Text Processing, pages 468–480.
M. Bansal, C. Cardie, and L. Lee. 2008. The power
of negative thinking: Exploiting label disagreement
in the min-cut classification framework. COLING,
pages 13–16.
O. Biran and O. Rambow. 2011. Identifying justifi-
cations in written dialogs. In IEEE International
Conference on Semantic Computing (ICSC), pages
162–168.
Clinton Burfoot, Steven Bird, and Timothy Baldwin.
2011. Collective classification of congressional
floor-debate transcripts. In Association for Compu-
tational Linguistics (ACL), pages 1506–1515.
C. Burfoot. 2008. Using multiple sources of agree-
ment information for sentiment classification of po-
litical transcripts. In Australasian Language Tech-
nology Association Workshop, volume 6, pages 11–
18.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
J.M. Gawron, D. Gupta, K. Stephens, M.H. Tsou,
B. Spitzberg, and L. An. 2012. Using group mem-
bership markers for group identification in web logs.
In AAAI Conference on Weblogs and Social Media
(ICWSM).
Kazi Saidul Hasan and Vincent Ng. 2013. Stance clas-
sification of ideological debates: Data, models, fea-
tures, and constraints. International Joint Confer-
ence on Natural Language Processing.
Angelika Kimmig, Stephen H. Bach, Matthias
Broecheler, Bert Huang, and Lise Getoor. 2012.
A short introduction to probabilistic soft logic.
In NIPS Workshop on Probabilistic Programming:
Foundations and Applications.
R. Malouf and T. Mullen. 2008. Taking sides: User
classification for informal online political discourse.
Internet Research, 18(2):177–190.
Amita Misra and Marilyn A Walker. 2013. Topic in-
dependent identification of agreement and disagree-
ment in social media dialogue. In Conference of the
Special Interest Group on Discourse and Dialogue,
page 920.
A. Murakami and R. Raymond. 2010. Support or Op-
pose? Classifying Positions in Online Debates from
Reply Activities and Opinion Expressions. In Inter-
national Conference on Computational Linguistics
(ACL), pages 869–875.
J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001.
LIWC: Linguistic Inquiry and Word Count.
Minghui Qiu, Liu Yang, and Jing Jiang. 2013. Mod-
eling interaction features for debate side clustering.
In ACM International Conference on Information &amp;
Knowledge Management (CIKM), pages 873–878.
Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic,
Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad.
2008. Collective classification in network data. AI
Magazine, 29(3):93–106.
S. Somasundaran and J. Wiebe. 2009. Recogniz-
ing stances in online debates. In ACL and AFNLP,
pages 226–234.
S. Somasundaran and J. Wiebe. 2010. Recognizing
stances in ideological on-line debates. In NAACL
HLT 2010 Workshop on Computational Approaches
to Analysis and Generation of Emotion in Text,
pages 116–124.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from Con-
gressional floor-debate transcripts. In Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 327–335.
Marilyn Walker, Pranav Anand, Rob Abbott, Jean E.
Fox Tree, Craig Martell, and Joseph King. 2012a.
That’s your evidence?: Classifying stance in online
political debate. Decision Support Sciences.
Marilyn Walker, Pranav Anand, Robert Abbott, and
Jean E. Fox Tree. 2012b. A corpus for research
on deliberation and debate. In Language Resources
and Evaluation Conference, LREC2012.
Marilyn Walker, Pranav Anand, Robert Abbott, and
Richard Grant. 2012c. Stance classification using
dialogic properties of persuasion. In Meeting of the
North American Association for Computational Lin-
guistics. NAACL-HLT12.
</reference>
<page confidence="0.989691">
116
</page>
<reference confidence="0.999145545454545">
Y.C. Wang and C.P. Ros´e. 2010. Making conversa-
tional structure explicit: identification of initiation-
response pairs within online discussions. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 673–676.
A. Yessenalina, Y. Yue, and C. Cardie. 2010.
Multi-level structured models for document-level
sentiment classification. In Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1046–1056.
</reference>
<page confidence="0.998134">
117
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.989738">
<title confidence="0.999909">Collective Stance Classification of Posts in Online Debate Forums</title>
<author confidence="0.999188">Dhanya Sridhar Lise Getoor Marilyn Walker</author>
<affiliation confidence="0.999547">Computer Science Dept. Computer Science Dept. Computer Science Dept. UC Santa Cruz UC Santa Cruz UC Santa Cruz</affiliation>
<email confidence="0.999776">dsridhar@soe.ucsc.edugetoor@soe.ucsc.edumaw@soe.ucsc.edu</email>
<abstract confidence="0.999767527777778">Online debate sites are a large source of informal and opinion-sharing dialogue on current socio-political issues. Inferring stance towards discussion topics in domains such as politics or news is an important problem, and is of utility to researchers, government organizations, and companies. Predicting users’ stance supports identification of social and political groups, building of better recommender systems, and personalization of users’ information preferences to their ideological beliefs. In this paper, we develop a novel collective classification approach to stance classification, which makes use of both structural and linguistic features, and which collectively labels the posts’ stance across a network of the users’ posts. We identify both linguistic features of the posts and features that capture the underlying relationships between posts and users. We use probabilistic soft logic (PSL) (Bach et al., 2013) to model post stance by leveraging both these local linguistic features as well as the observed network structure of the posts to reason over the dataset. We evaluate our approach et al., 2012b), a collection of discussions from an online debate site on issues ranging from gun control to gay marriage. We show that our collective classification model is able to easily incorporate rich, relational information and outperforms a local model which uses only linguistic information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amjad Abu-Jbara</author>
<author>Mona Diab</author>
<author>Pradeep Dasigi</author>
<author>Dragomir Radev</author>
</authors>
<title>Subgroup detection in ideological discussions.</title>
<date>2012</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>399--409</pages>
<contexts>
<context position="2069" citStr="Abu-Jbara et al., 2012" startWordPosition="311" endWordPosition="314">), a collection of discussions from an online debate site on issues ranging from gun control to gay marriage. We show that our collective classification model is able to easily incorporate rich, relational information and outperforms a local model which uses only linguistic information. 1 Introduction Modeling user stance (PRO, CON) in discussion topics in online social media debate is of interest to researchers, corporations and governmental organizations alike. Predicting a user’s stance towards a given issue can support the identification of social or political groups (Gawron et al., 2012; Abu-Jbara et al., 2012; Anand et al., 2011; Qiu et al., 2013; Hasan and Ng, 2013), help develop better recommendation systems, or tailor users’ information preferences to their ideologies and beliefs. Stance classification problems consist of a collection of debate-style discussions by authors on different controversial, political topics. While these may be spoken as in the Congressional Debates corpus (Thomas et al., 2006; Burfoot, 2008), we focus on forum posts on social media debate sites. Users on debate sites share their opinions freely, using informal and social language, providing a rich and much more challe</context>
</contexts>
<marker>Abu-Jbara, Diab, Dasigi, Radev, 2012</marker>
<rawString>Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and Dragomir Radev. 2012. Subgroup detection in ideological discussions. In Association for Computational Linguistics (ACL), pages 399–409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Agrawal</author>
<author>S Rajagopalan</author>
<author>R Srikant</author>
<author>Y Xu</author>
</authors>
<title>Mining newsgroups using networks arising from social behavior.</title>
<date>2003</date>
<booktitle>In International Conference on World Wide Web (WWW),</booktitle>
<pages>529--535</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="24703" citStr="Agrawal et al., 2003" startWordPosition="4121" endWordPosition="4124">and second rule very highly, and does worse than when considering the prior alone. Thus, the combination of the rules gives the model its advantage, allowing the PSL model to make use of a richer structure that has multiple types of relations and more information. 5 Related Work Over the last ten years, there has been significant progress on modeling stance. Previous work covers three different debate settings: (1) congressional debates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Walker et al., 2012c; Anand et al., 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites because the posts are 114 Classifier F1 Score AUC-PR negative class AUROC LOCAL 0.66 ± 0.015 0.44 ± 0.04 0.54 ± 0.02 PSL 0.74 ± 0.04 0.511 ± 0.04 0.59 ± 0.05 Table 3: Averages and standard deviations for F1 score for the positive class, area under PR curve for the negative class, and are</context>
<context position="27102" citStr="Agrawal et al., 2003" startWordPosition="4518" endWordPosition="4521">r to our own show that identifying argumentation structure improves performance; their best performance is approximately 64% accuracy over all topics. Research by (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009) classifies the speaker’s stance in a corpus of congressional floor debates. This work combines graph-based and text-classification approaches to achieve 75% accuracy on Congressional debate siding over all topics. Other work applies MaxCut to the reply structure of company discussion forums (Malouf and Mullen, 2008; Murakami and Raymond, 2010; Agrawal et al., 2003). Murakami &amp; Raymond (2010) show that rules for identifying agreement, defined on the textual content of the post improve performance. More recent work has explicitly focused on the benefits of collective classification in these settings (Burfoot et al., 2011; Hasan and Ng, 2013; Walker et al., 2012c), and has shown in each case that collective classification improves performance. The results reported here are the first to apply the PSL collective classification framework to the forums conversations from the IAC corpus (Anand et al., 2011; Walker et al., 2012c). 6 Discussion and Future Work He</context>
</contexts>
<marker>Agrawal, Rajagopalan, Srikant, Xu, 2003</marker>
<rawString>R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu. 2003. Mining newsgroups using networks arising from social behavior. In International Conference on World Wide Web (WWW), pages 529–535. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pranav Anand</author>
<author>Marilyn Walker</author>
<author>Rob Abbott</author>
<author>Jean E Fox Tree</author>
<author>Robeson Bowmani</author>
<author>Michael Minor</author>
</authors>
<date>2011</date>
<booktitle>Cats Rule and Dogs Drool: Classifying Stance in Online Debate. In ACL Workshop on Sentiment and Subjectivity.</booktitle>
<contexts>
<context position="2089" citStr="Anand et al., 2011" startWordPosition="315" endWordPosition="318">ssions from an online debate site on issues ranging from gun control to gay marriage. We show that our collective classification model is able to easily incorporate rich, relational information and outperforms a local model which uses only linguistic information. 1 Introduction Modeling user stance (PRO, CON) in discussion topics in online social media debate is of interest to researchers, corporations and governmental organizations alike. Predicting a user’s stance towards a given issue can support the identification of social or political groups (Gawron et al., 2012; Abu-Jbara et al., 2012; Anand et al., 2011; Qiu et al., 2013; Hasan and Ng, 2013), help develop better recommendation systems, or tailor users’ information preferences to their ideologies and beliefs. Stance classification problems consist of a collection of debate-style discussions by authors on different controversial, political topics. While these may be spoken as in the Congressional Debates corpus (Thomas et al., 2006; Burfoot, 2008), we focus on forum posts on social media debate sites. Users on debate sites share their opinions freely, using informal and social language, providing a rich and much more challenging domain for sta</context>
<context position="14901" citStr="Anand et al., 2011" startWordPosition="2470" endWordPosition="2473">, discourse cues, repeated punctuation counts and counts of lexical categories based on the Linguistic Inquiry and Word Count tool (LIWC) (Pennebaker et al., 2001). Basic length features capture the number of sentences, words, and characters, along with the average word and sentence lengths for each post. The discourse cues feature captures frequency counts for the first few words of the post, which often contain discourse cues. To capture the information in repeated punctuation like “!!”, “??” or “?!” we include the frequency count of the given punctuation patterns as a feature of each post (Anand et al., 2011). LIWC counts capture sentiment by giving the degree to which the post uses certain categories of subjective language. 3.2.2 Relational Information As our problem domain contains relations between both authors and posts, for our PSL model, we consider the relations between authors, between posts and between authors and posts. As described above, in PSL, we model these relations as first-order predicates. In Section 3.3, we describe how we populate the predicates with observations from our data. Author Information We observe that authors participate in discussions by writing posts. For a subset</context>
<context position="24903" citStr="Anand et al., 2011" startWordPosition="4153" endWordPosition="4156">ure that has multiple types of relations and more information. 5 Related Work Over the last ten years, there has been significant progress on modeling stance. Previous work covers three different debate settings: (1) congressional debates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Walker et al., 2012c; Anand et al., 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites because the posts are 114 Classifier F1 Score AUC-PR negative class AUROC LOCAL 0.66 ± 0.015 0.44 ± 0.04 0.54 ± 0.02 PSL 0.74 ± 0.04 0.511 ± 0.04 0.59 ± 0.05 Table 3: Averages and standard deviations for F1 score for the positive class, area under PR curve for the negative class, and area under ROC curve for post stance over 10 train/test splits. isProPost(P, T) ∧ writesPost(A, P) → isProAuth(A, T) : 10.2 ¬ isProPost(P, T) ∧ writesPost(A, P) → ¬ isProAuth(A, T) : 8.5 agreesPost(P, P2</context>
<context position="27646" citStr="Anand et al., 2011" startWordPosition="4606" endWordPosition="4609">Malouf and Mullen, 2008; Murakami and Raymond, 2010; Agrawal et al., 2003). Murakami &amp; Raymond (2010) show that rules for identifying agreement, defined on the textual content of the post improve performance. More recent work has explicitly focused on the benefits of collective classification in these settings (Burfoot et al., 2011; Hasan and Ng, 2013; Walker et al., 2012c), and has shown in each case that collective classification improves performance. The results reported here are the first to apply the PSL collective classification framework to the forums conversations from the IAC corpus (Anand et al., 2011; Walker et al., 2012c). 6 Discussion and Future Work Here, we introduce a novel approach to classify stance of posts from online debate forums with a collective classification framework. We formally construct a model, using PSL, to capture relational information in the network of authors and posts and our intuition that agreement or disagreement between users correlates to their stance towards a topic. Our initial results are promising, showing that by incorporating more complex interactions between authors and posts, we gain improvements over a content-only approach. Our approach is ideally </context>
</contexts>
<marker>Anand, Walker, Abbott, Tree, Bowmani, Minor, 2011</marker>
<rawString>Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox Tree, Robeson Bowmani, and Michael Minor. 2011. Cats Rule and Dogs Drool: Classifying Stance in Online Debate. In ACL Workshop on Sentiment and Subjectivity.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen H Bach</author>
<author>Bert Huang</author>
<author>Ben London</author>
<author>Lise Getoor</author>
</authors>
<title>Hinge-loss markov random fields: Convex inference for structured prediction.</title>
<date>2013</date>
<booktitle>In Uncertainty in Artificial Intelligence (UAI).</booktitle>
<contexts>
<context position="1234" citStr="Bach et al., 2013" startWordPosition="175" endWordPosition="178">s’ stance supports identification of social and political groups, building of better recommender systems, and personalization of users’ information preferences to their ideological beliefs. In this paper, we develop a novel collective classification approach to stance classification, which makes use of both structural and linguistic features, and which collectively labels the posts’ stance across a network of the users’ posts. We identify both linguistic features of the posts and features that capture the underlying relationships between posts and users. We use probabilistic soft logic (PSL) (Bach et al., 2013) to model post stance by leveraging both these local linguistic features as well as the observed network structure of the posts to reason over the dataset. We evaluate our approach on 4FORUMS (Walker et al., 2012b), a collection of discussions from an online debate site on issues ranging from gun control to gay marriage. We show that our collective classification model is able to easily incorporate rich, relational information and outperforms a local model which uses only linguistic information. 1 Introduction Modeling user stance (PRO, CON) in discussion topics in online social media debate i</context>
<context position="6639" citStr="Bach et al., 2013" startWordPosition="1081" endWordPosition="1084">llective classification, as we further define in Section 3 (Sen et al., 2008). Previous work has shown that collective classification models often perform better than content-only approaches. (Burfoot et al., 2011; Hasan and Ng, 2013; Thomas et al., 2006; Bansal et al., 2008; Walker et al., 2012c). Here, we develop a collective classification approach for stance prediction which leverages the sentiment conveyed in a post through its language, and the reply links consisting of agreements or rebuttals between posts in a discussion. We implement our approach using Probabilistic Soft Logic (PSL) (Bach et al., 2013), a recently introduced tool for collective inference in relational data. We evaluate our model on data from the 4FORUMS online debate site (Walker et al., 2012b). Section 2 first presents an overview of our approach and then in Section 3.1 we describe the PSL framework in more detail. Section 4 describes the evaluation data and our results showing that the PSL model improves prediction of post stance in the 4Forums dataset. In Section 5 we describe related work, and compare with our proposed approach. Section 6 summarizes our approach and results. 2 Overview of Approach Given a set of topics </context>
<context position="10960" citStr="Bach et al., 2013" startWordPosition="1840" endWordPosition="1843">ng information given by both the local features of the objects and the properties of the objects’ neighbors in a network. For the stance classification problem, we infer stance labels for posts using both the correlation between a post and its linguistic attributes {xij1, ... , xijN}, and the labels and attributes of its neighbors in observed network graph G. We use PSL, described below, to perform collective classification. 3.1 Probabilistic Soft Logic Probabilistic soft logic (PSL) is a framework for probabilistic modeling and collective reasoning in relational domains (Kimmig et al., 2012; Bach et al., 2013). PSL provides a declarative syntax and uses first-order logic to define a templated undirected graphical model over continuous random variables. Like other statistical relational learning methods, dependencies in the domain are captured by constructing rules with weights that can be learned from data. But unlike other statistical relational learning methods, PSL relaxes boolean truth values for atoms in the domain to soft truth values in the interval [0,1]. In this setting, finding the mostprobable explanation (MPE), a joint assignment of truth values to all random variable ground atoms, can </context>
</contexts>
<marker>Bach, Huang, London, Getoor, 2013</marker>
<rawString>Stephen H. Bach, Bert Huang, Ben London, and Lise Getoor. 2013. Hinge-loss markov random fields: Convex inference for structured prediction. In Uncertainty in Artificial Intelligence (UAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Balahur</author>
<author>Z Kozareva</author>
<author>A Montoyo</author>
</authors>
<title>Determining the polarity and source of opinions expressed in political debates.</title>
<date>2009</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>468--480</pages>
<contexts>
<context position="24613" citStr="Balahur et al., 2009" startWordPosition="4108" endWordPosition="4111">n disregarding the links between nodes entirely. The PSL model learns to weight the first and second rule very highly, and does worse than when considering the prior alone. Thus, the combination of the rules gives the model its advantage, allowing the PSL model to make use of a richer structure that has multiple types of relations and more information. 5 Related Work Over the last ten years, there has been significant progress on modeling stance. Previous work covers three different debate settings: (1) congressional debates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Walker et al., 2012c; Anand et al., 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites because the posts are 114 Classifier F1 Score AUC-PR negative class AUROC LOCAL 0.66 ± 0.015 0.44 ± 0.04 0.54 ± 0.02 PSL 0.74 ± 0.04 0.511 ± 0.04 0.59 ± 0.05 Table 3: Averages and standard deviation</context>
<context position="26734" citStr="Balahur et al., 2009" startWordPosition="4463" endWordPosition="4466">arned by the model for the PSL rules in train/test split 2 of experiments shorter and the language is more informal and social. We predict that this difference makes it more difficult to achieve accuracies as high for 4FORUMS discussions as can be achieved for the congressional debates corpus. Work by (Somasundaran and Wiebe, 2009) on idealogical debates very similar to our own show that identifying argumentation structure improves performance; their best performance is approximately 64% accuracy over all topics. Research by (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009) classifies the speaker’s stance in a corpus of congressional floor debates. This work combines graph-based and text-classification approaches to achieve 75% accuracy on Congressional debate siding over all topics. Other work applies MaxCut to the reply structure of company discussion forums (Malouf and Mullen, 2008; Murakami and Raymond, 2010; Agrawal et al., 2003). Murakami &amp; Raymond (2010) show that rules for identifying agreement, defined on the textual content of the post improve performance. More recent work has explicitly focused on the benefits of collective classification in these set</context>
</contexts>
<marker>Balahur, Kozareva, Montoyo, 2009</marker>
<rawString>A. Balahur, Z. Kozareva, and A. Montoyo. 2009. Determining the polarity and source of opinions expressed in political debates. Computational Linguistics and Intelligent Text Processing, pages 468–480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bansal</author>
<author>C Cardie</author>
<author>L Lee</author>
</authors>
<title>The power of negative thinking: Exploiting label disagreement in the min-cut classification framework.</title>
<date>2008</date>
<journal>COLING,</journal>
<pages>13--16</pages>
<contexts>
<context position="6296" citStr="Bansal et al., 2008" startWordPosition="1026" endWordPosition="1029">or. both response posts are from the same author. We describe the dataset further in Section 4.1. We believe that models of post stance in online debate should capture both the content and the context of author posts. By jointly reasoning over both the content of the post and its relationships with other posts in the discussion, we perform collective classification, as we further define in Section 3 (Sen et al., 2008). Previous work has shown that collective classification models often perform better than content-only approaches. (Burfoot et al., 2011; Hasan and Ng, 2013; Thomas et al., 2006; Bansal et al., 2008; Walker et al., 2012c). Here, we develop a collective classification approach for stance prediction which leverages the sentiment conveyed in a post through its language, and the reply links consisting of agreements or rebuttals between posts in a discussion. We implement our approach using Probabilistic Soft Logic (PSL) (Bach et al., 2013), a recently introduced tool for collective inference in relational data. We evaluate our model on data from the 4FORUMS online debate site (Walker et al., 2012b). Section 2 first presents an overview of our approach and then in Section 3.1 we describe the </context>
<context position="24564" citStr="Bansal et al., 2008" startWordPosition="4100" endWordPosition="4103">l, and found that the model performs poorly when disregarding the links between nodes entirely. The PSL model learns to weight the first and second rule very highly, and does worse than when considering the prior alone. Thus, the combination of the rules gives the model its advantage, allowing the PSL model to make use of a richer structure that has multiple types of relations and more information. 5 Related Work Over the last ten years, there has been significant progress on modeling stance. Previous work covers three different debate settings: (1) congressional debates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Walker et al., 2012c; Anand et al., 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites because the posts are 114 Classifier F1 Score AUC-PR negative class AUROC LOCAL 0.66 ± 0.015 0.44 ± 0.04 0.54 ± 0.02 PSL 0.74 ± 0.04 0.511 ± 0.04 0.5</context>
<context position="26685" citStr="Bansal et al., 2008" startWordPosition="4454" endWordPosition="4457">) → ¬ isProPost(P, T) : 4.8 Table 4: Weights learned by the model for the PSL rules in train/test split 2 of experiments shorter and the language is more informal and social. We predict that this difference makes it more difficult to achieve accuracies as high for 4FORUMS discussions as can be achieved for the congressional debates corpus. Work by (Somasundaran and Wiebe, 2009) on idealogical debates very similar to our own show that identifying argumentation structure improves performance; their best performance is approximately 64% accuracy over all topics. Research by (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009) classifies the speaker’s stance in a corpus of congressional floor debates. This work combines graph-based and text-classification approaches to achieve 75% accuracy on Congressional debate siding over all topics. Other work applies MaxCut to the reply structure of company discussion forums (Malouf and Mullen, 2008; Murakami and Raymond, 2010; Agrawal et al., 2003). Murakami &amp; Raymond (2010) show that rules for identifying agreement, defined on the textual content of the post improve performance. More recent work has explicitly focused on the b</context>
</contexts>
<marker>Bansal, Cardie, Lee, 2008</marker>
<rawString>M. Bansal, C. Cardie, and L. Lee. 2008. The power of negative thinking: Exploiting label disagreement in the min-cut classification framework. COLING, pages 13–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Biran</author>
<author>O Rambow</author>
</authors>
<title>Identifying justifications in written dialogs.</title>
<date>2011</date>
<booktitle>In IEEE International Conference on Semantic Computing (ICSC),</booktitle>
<pages>162--168</pages>
<contexts>
<context position="24860" citStr="Biran and Rambow, 2011" startWordPosition="4145" endWordPosition="4148">g the PSL model to make use of a richer structure that has multiple types of relations and more information. 5 Related Work Over the last ten years, there has been significant progress on modeling stance. Previous work covers three different debate settings: (1) congressional debates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Walker et al., 2012c; Anand et al., 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites because the posts are 114 Classifier F1 Score AUC-PR negative class AUROC LOCAL 0.66 ± 0.015 0.44 ± 0.04 0.54 ± 0.02 PSL 0.74 ± 0.04 0.511 ± 0.04 0.59 ± 0.05 Table 3: Averages and standard deviations for F1 score for the positive class, area under PR curve for the negative class, and area under ROC curve for post stance over 10 train/test splits. isProPost(P, T) ∧ writesPost(A, P) → isProAuth(A, T) : 10.2 ¬ isProPost(P, T) ∧ writesPost(A, P)</context>
</contexts>
<marker>Biran, Rambow, 2011</marker>
<rawString>O. Biran and O. Rambow. 2011. Identifying justifications in written dialogs. In IEEE International Conference on Semantic Computing (ICSC), pages 162–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clinton Burfoot</author>
<author>Steven Bird</author>
<author>Timothy Baldwin</author>
</authors>
<title>Collective classification of congressional floor-debate transcripts.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>1506--1515</pages>
<contexts>
<context position="6234" citStr="Burfoot et al., 2011" startWordPosition="1014" endWordPosition="1017">otations for stance. Both response posts are from the same author. both response posts are from the same author. We describe the dataset further in Section 4.1. We believe that models of post stance in online debate should capture both the content and the context of author posts. By jointly reasoning over both the content of the post and its relationships with other posts in the discussion, we perform collective classification, as we further define in Section 3 (Sen et al., 2008). Previous work has shown that collective classification models often perform better than content-only approaches. (Burfoot et al., 2011; Hasan and Ng, 2013; Thomas et al., 2006; Bansal et al., 2008; Walker et al., 2012c). Here, we develop a collective classification approach for stance prediction which leverages the sentiment conveyed in a post through its language, and the reply links consisting of agreements or rebuttals between posts in a discussion. We implement our approach using Probabilistic Soft Logic (PSL) (Bach et al., 2013), a recently introduced tool for collective inference in relational data. We evaluate our model on data from the 4FORUMS online debate site (Walker et al., 2012b). Section 2 first presents an ove</context>
<context position="27361" citStr="Burfoot et al., 2011" startWordPosition="4560" endWordPosition="4563">fies the speaker’s stance in a corpus of congressional floor debates. This work combines graph-based and text-classification approaches to achieve 75% accuracy on Congressional debate siding over all topics. Other work applies MaxCut to the reply structure of company discussion forums (Malouf and Mullen, 2008; Murakami and Raymond, 2010; Agrawal et al., 2003). Murakami &amp; Raymond (2010) show that rules for identifying agreement, defined on the textual content of the post improve performance. More recent work has explicitly focused on the benefits of collective classification in these settings (Burfoot et al., 2011; Hasan and Ng, 2013; Walker et al., 2012c), and has shown in each case that collective classification improves performance. The results reported here are the first to apply the PSL collective classification framework to the forums conversations from the IAC corpus (Anand et al., 2011; Walker et al., 2012c). 6 Discussion and Future Work Here, we introduce a novel approach to classify stance of posts from online debate forums with a collective classification framework. We formally construct a model, using PSL, to capture relational information in the network of authors and posts and our intuiti</context>
</contexts>
<marker>Burfoot, Bird, Baldwin, 2011</marker>
<rawString>Clinton Burfoot, Steven Bird, and Timothy Baldwin. 2011. Collective classification of congressional floor-debate transcripts. In Association for Computational Linguistics (ACL), pages 1506–1515.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Burfoot</author>
</authors>
<title>Using multiple sources of agreement information for sentiment classification of political transcripts.</title>
<date>2008</date>
<booktitle>In Australasian Language Technology Association Workshop,</booktitle>
<volume>6</volume>
<pages>11--18</pages>
<contexts>
<context position="2489" citStr="Burfoot, 2008" startWordPosition="379" endWordPosition="381">s and governmental organizations alike. Predicting a user’s stance towards a given issue can support the identification of social or political groups (Gawron et al., 2012; Abu-Jbara et al., 2012; Anand et al., 2011; Qiu et al., 2013; Hasan and Ng, 2013), help develop better recommendation systems, or tailor users’ information preferences to their ideologies and beliefs. Stance classification problems consist of a collection of debate-style discussions by authors on different controversial, political topics. While these may be spoken as in the Congressional Debates corpus (Thomas et al., 2006; Burfoot, 2008), we focus on forum posts on social media debate sites. Users on debate sites share their opinions freely, using informal and social language, providing a rich and much more challenging domain for stance prediction. Social media debate sites contain online discussions with posts from various authors, where each post is either a response to another post or the root of the discussion (Anand et al., 2011; Walker et al., 2012a). Posts are linked to one another by either rebuttal or agreement links and are labelled for stance, either PRO or CON, depending on the framing of the issue under discussio</context>
</contexts>
<marker>Burfoot, 2008</marker>
<rawString>C. Burfoot. 2008. Using multiple sources of agreement information for sentiment classification of political transcripts. In Australasian Language Technology Association Workshop, volume 6, pages 11– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="21401" citStr="Fan et al., 2008" startWordPosition="3561" endWordPosition="3564">te our methods, we split the dataset into training and testing sets by randomly selecting half the authors from each topic and their posts for the training set and using the remaining authors and their posts for the test set. This way, we ensure that no two authors appear in both training and test sets for the same topic, since stance is topic-dependent. We create 10 randomly sampled train/test splits for evaluation. Each split contains about 18,000 posts. For each train/test split, we train a linear SVM for each topic, with the L2-regularized-L1-loss SVM implemented in the LibLINEAR package (Fan et al., 2008). We use only the linguistic features from the posts, for each topic in the training set. We refer to the baseline model which only uses the the output of the SVM as the LOCAL model. We output the predictions from LOCAL model and get stance labels for posts in both the training and test sets. We use the predictions as prior information for the true stance label in our PSL model, with the hasLabel predicate. We use the gold standard stance annotation (PRO, CON) for each post as ground truth for weight learning and inference. A truth value of 1 for isPostPro and isAuthPro denotes a PRO stance an</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Gawron</author>
<author>D Gupta</author>
<author>K Stephens</author>
<author>M H Tsou</author>
<author>B Spitzberg</author>
<author>L An</author>
</authors>
<title>Using group membership markers for group identification in web logs.</title>
<date>2012</date>
<booktitle>In AAAI Conference on Weblogs and Social Media (ICWSM).</booktitle>
<contexts>
<context position="2045" citStr="Gawron et al., 2012" startWordPosition="307" endWordPosition="310">(Walker et al., 2012b), a collection of discussions from an online debate site on issues ranging from gun control to gay marriage. We show that our collective classification model is able to easily incorporate rich, relational information and outperforms a local model which uses only linguistic information. 1 Introduction Modeling user stance (PRO, CON) in discussion topics in online social media debate is of interest to researchers, corporations and governmental organizations alike. Predicting a user’s stance towards a given issue can support the identification of social or political groups (Gawron et al., 2012; Abu-Jbara et al., 2012; Anand et al., 2011; Qiu et al., 2013; Hasan and Ng, 2013), help develop better recommendation systems, or tailor users’ information preferences to their ideologies and beliefs. Stance classification problems consist of a collection of debate-style discussions by authors on different controversial, political topics. While these may be spoken as in the Congressional Debates corpus (Thomas et al., 2006; Burfoot, 2008), we focus on forum posts on social media debate sites. Users on debate sites share their opinions freely, using informal and social language, providing a r</context>
</contexts>
<marker>Gawron, Gupta, Stephens, Tsou, Spitzberg, An, 2012</marker>
<rawString>J.M. Gawron, D. Gupta, K. Stephens, M.H. Tsou, B. Spitzberg, and L. An. 2012. Using group membership markers for group identification in web logs. In AAAI Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazi Saidul Hasan</author>
<author>Vincent Ng</author>
</authors>
<title>Stance classification of ideological debates: Data, models, features, and constraints.</title>
<date>2013</date>
<booktitle>International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="2128" citStr="Hasan and Ng, 2013" startWordPosition="323" endWordPosition="326">ssues ranging from gun control to gay marriage. We show that our collective classification model is able to easily incorporate rich, relational information and outperforms a local model which uses only linguistic information. 1 Introduction Modeling user stance (PRO, CON) in discussion topics in online social media debate is of interest to researchers, corporations and governmental organizations alike. Predicting a user’s stance towards a given issue can support the identification of social or political groups (Gawron et al., 2012; Abu-Jbara et al., 2012; Anand et al., 2011; Qiu et al., 2013; Hasan and Ng, 2013), help develop better recommendation systems, or tailor users’ information preferences to their ideologies and beliefs. Stance classification problems consist of a collection of debate-style discussions by authors on different controversial, political topics. While these may be spoken as in the Congressional Debates corpus (Thomas et al., 2006; Burfoot, 2008), we focus on forum posts on social media debate sites. Users on debate sites share their opinions freely, using informal and social language, providing a rich and much more challenging domain for stance prediction. Social media debate sit</context>
<context position="6254" citStr="Hasan and Ng, 2013" startWordPosition="1018" endWordPosition="1021">oth response posts are from the same author. both response posts are from the same author. We describe the dataset further in Section 4.1. We believe that models of post stance in online debate should capture both the content and the context of author posts. By jointly reasoning over both the content of the post and its relationships with other posts in the discussion, we perform collective classification, as we further define in Section 3 (Sen et al., 2008). Previous work has shown that collective classification models often perform better than content-only approaches. (Burfoot et al., 2011; Hasan and Ng, 2013; Thomas et al., 2006; Bansal et al., 2008; Walker et al., 2012c). Here, we develop a collective classification approach for stance prediction which leverages the sentiment conveyed in a post through its language, and the reply links consisting of agreements or rebuttals between posts in a discussion. We implement our approach using Probabilistic Soft Logic (PSL) (Bach et al., 2013), a recently introduced tool for collective inference in relational data. We evaluate our model on data from the 4FORUMS online debate site (Walker et al., 2012b). Section 2 first presents an overview of our approac</context>
<context position="27381" citStr="Hasan and Ng, 2013" startWordPosition="4564" endWordPosition="4567">nce in a corpus of congressional floor debates. This work combines graph-based and text-classification approaches to achieve 75% accuracy on Congressional debate siding over all topics. Other work applies MaxCut to the reply structure of company discussion forums (Malouf and Mullen, 2008; Murakami and Raymond, 2010; Agrawal et al., 2003). Murakami &amp; Raymond (2010) show that rules for identifying agreement, defined on the textual content of the post improve performance. More recent work has explicitly focused on the benefits of collective classification in these settings (Burfoot et al., 2011; Hasan and Ng, 2013; Walker et al., 2012c), and has shown in each case that collective classification improves performance. The results reported here are the first to apply the PSL collective classification framework to the forums conversations from the IAC corpus (Anand et al., 2011; Walker et al., 2012c). 6 Discussion and Future Work Here, we introduce a novel approach to classify stance of posts from online debate forums with a collective classification framework. We formally construct a model, using PSL, to capture relational information in the network of authors and posts and our intuition that agreement or</context>
</contexts>
<marker>Hasan, Ng, 2013</marker>
<rawString>Kazi Saidul Hasan and Vincent Ng. 2013. Stance classification of ideological debates: Data, models, features, and constraints. International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angelika Kimmig</author>
<author>Stephen H Bach</author>
<author>Matthias Broecheler</author>
<author>Bert Huang</author>
<author>Lise Getoor</author>
</authors>
<title>A short introduction to probabilistic soft logic.</title>
<date>2012</date>
<booktitle>In NIPS Workshop on Probabilistic Programming: Foundations and Applications.</booktitle>
<contexts>
<context position="10940" citStr="Kimmig et al., 2012" startWordPosition="1836" endWordPosition="1839">s, in our domain) using information given by both the local features of the objects and the properties of the objects’ neighbors in a network. For the stance classification problem, we infer stance labels for posts using both the correlation between a post and its linguistic attributes {xij1, ... , xijN}, and the labels and attributes of its neighbors in observed network graph G. We use PSL, described below, to perform collective classification. 3.1 Probabilistic Soft Logic Probabilistic soft logic (PSL) is a framework for probabilistic modeling and collective reasoning in relational domains (Kimmig et al., 2012; Bach et al., 2013). PSL provides a declarative syntax and uses first-order logic to define a templated undirected graphical model over continuous random variables. Like other statistical relational learning methods, dependencies in the domain are captured by constructing rules with weights that can be learned from data. But unlike other statistical relational learning methods, PSL relaxes boolean truth values for atoms in the domain to soft truth values in the interval [0,1]. In this setting, finding the mostprobable explanation (MPE), a joint assignment of truth values to all random variabl</context>
</contexts>
<marker>Kimmig, Bach, Broecheler, Huang, Getoor, 2012</marker>
<rawString>Angelika Kimmig, Stephen H. Bach, Matthias Broecheler, Bert Huang, and Lise Getoor. 2012. A short introduction to probabilistic soft logic. In NIPS Workshop on Probabilistic Programming: Foundations and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
<author>T Mullen</author>
</authors>
<title>Taking sides: User classification for informal online political discourse.</title>
<date>2008</date>
<journal>Internet Research,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="27051" citStr="Malouf and Mullen, 2008" startWordPosition="4509" endWordPosition="4513">n and Wiebe, 2009) on idealogical debates very similar to our own show that identifying argumentation structure improves performance; their best performance is approximately 64% accuracy over all topics. Research by (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009) classifies the speaker’s stance in a corpus of congressional floor debates. This work combines graph-based and text-classification approaches to achieve 75% accuracy on Congressional debate siding over all topics. Other work applies MaxCut to the reply structure of company discussion forums (Malouf and Mullen, 2008; Murakami and Raymond, 2010; Agrawal et al., 2003). Murakami &amp; Raymond (2010) show that rules for identifying agreement, defined on the textual content of the post improve performance. More recent work has explicitly focused on the benefits of collective classification in these settings (Burfoot et al., 2011; Hasan and Ng, 2013; Walker et al., 2012c), and has shown in each case that collective classification improves performance. The results reported here are the first to apply the PSL collective classification framework to the forums conversations from the IAC corpus (Anand et al., 2011; Wal</context>
</contexts>
<marker>Malouf, Mullen, 2008</marker>
<rawString>R. Malouf and T. Mullen. 2008. Taking sides: User classification for informal online political discourse. Internet Research, 18(2):177–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amita Misra</author>
<author>Marilyn A Walker</author>
</authors>
<title>Topic independent identification of agreement and disagreement in social media dialogue.</title>
<date>2013</date>
<booktitle>In Conference of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>920</pages>
<contexts>
<context position="3787" citStr="Misra and Walker, 2013" startWordPosition="593" endWordPosition="596">articipate in multiple discussions in the same topic, and may discuss multiple topics. For example consider the sample posts from the online discussion forum 4forums.com shown in Fig. 1. Here, we see discussion topics, together with sample quotes and responses, where the response is a direct reply to the quote text. The annotations for stance were gathered using Amazon’s Mechanical Turk service with an interface that allowed annotators to see complete discussions. Quotes provide additional context that were used by human annotators in a separate task for annotating agreement and disagreement (Misra and Walker, 2013). Responses can be labeled as either PRO or CON toward the topic. For the example shown in Fig. 1, 109 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 109–117, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics Quote Q, Response R Stance Topic Q: I thought I’d start a new thread for those newcomers who don’t want to be shocked by sick minded nazi XXXX. Anyway... When are fetuses really alive, and how many fetuses are actually aborted (murdered) before that time? R: The heart starts beating 3 weeks after co</context>
</contexts>
<marker>Misra, Walker, 2013</marker>
<rawString>Amita Misra and Marilyn A Walker. 2013. Topic independent identification of agreement and disagreement in social media dialogue. In Conference of the Special Interest Group on Discourse and Dialogue, page 920.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Murakami</author>
<author>R Raymond</author>
</authors>
<title>Support or Oppose? Classifying Positions in Online Debates from Reply Activities and Opinion Expressions.</title>
<date>2010</date>
<booktitle>In International Conference on Computational Linguistics (ACL),</booktitle>
<pages>869--875</pages>
<contexts>
<context position="24680" citStr="Murakami and Raymond, 2010" startWordPosition="4116" endWordPosition="4120"> learns to weight the first and second rule very highly, and does worse than when considering the prior alone. Thus, the combination of the rules gives the model its advantage, allowing the PSL model to make use of a richer structure that has multiple types of relations and more information. 5 Related Work Over the last ten years, there has been significant progress on modeling stance. Previous work covers three different debate settings: (1) congressional debates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Walker et al., 2012c; Anand et al., 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites because the posts are 114 Classifier F1 Score AUC-PR negative class AUROC LOCAL 0.66 ± 0.015 0.44 ± 0.04 0.54 ± 0.02 PSL 0.74 ± 0.04 0.511 ± 0.04 0.59 ± 0.05 Table 3: Averages and standard deviations for F1 score for the positive class, area under PR curve for the </context>
<context position="27079" citStr="Murakami and Raymond, 2010" startWordPosition="4514" endWordPosition="4517">alogical debates very similar to our own show that identifying argumentation structure improves performance; their best performance is approximately 64% accuracy over all topics. Research by (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009) classifies the speaker’s stance in a corpus of congressional floor debates. This work combines graph-based and text-classification approaches to achieve 75% accuracy on Congressional debate siding over all topics. Other work applies MaxCut to the reply structure of company discussion forums (Malouf and Mullen, 2008; Murakami and Raymond, 2010; Agrawal et al., 2003). Murakami &amp; Raymond (2010) show that rules for identifying agreement, defined on the textual content of the post improve performance. More recent work has explicitly focused on the benefits of collective classification in these settings (Burfoot et al., 2011; Hasan and Ng, 2013; Walker et al., 2012c), and has shown in each case that collective classification improves performance. The results reported here are the first to apply the PSL collective classification framework to the forums conversations from the IAC corpus (Anand et al., 2011; Walker et al., 2012c). 6 Discus</context>
<context position="27129" citStr="Murakami &amp; Raymond (2010)" startWordPosition="4522" endWordPosition="4526">identifying argumentation structure improves performance; their best performance is approximately 64% accuracy over all topics. Research by (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009) classifies the speaker’s stance in a corpus of congressional floor debates. This work combines graph-based and text-classification approaches to achieve 75% accuracy on Congressional debate siding over all topics. Other work applies MaxCut to the reply structure of company discussion forums (Malouf and Mullen, 2008; Murakami and Raymond, 2010; Agrawal et al., 2003). Murakami &amp; Raymond (2010) show that rules for identifying agreement, defined on the textual content of the post improve performance. More recent work has explicitly focused on the benefits of collective classification in these settings (Burfoot et al., 2011; Hasan and Ng, 2013; Walker et al., 2012c), and has shown in each case that collective classification improves performance. The results reported here are the first to apply the PSL collective classification framework to the forums conversations from the IAC corpus (Anand et al., 2011; Walker et al., 2012c). 6 Discussion and Future Work Here, we introduce a novel ap</context>
</contexts>
<marker>Murakami, Raymond, 2010</marker>
<rawString>A. Murakami and R. Raymond. 2010. Support or Oppose? Classifying Positions in Online Debates from Reply Activities and Opinion Expressions. In International Conference on Computational Linguistics (ACL), pages 869–875.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Pennebaker</author>
<author>L E Francis</author>
<author>R J Booth</author>
</authors>
<title>LIWC: Linguistic Inquiry and Word Count.</title>
<date>2001</date>
<contexts>
<context position="14445" citStr="Pennebaker et al., 2001" startWordPosition="2395" endWordPosition="2398"> a very natural interface to compactly represent stance classification as a collective classification problem, along with methods to reason about our domain. 3.2 Features We extract both linguistic features that capture the content of a post and features that capture multiple relations from our dataset. 3.2.1 Linguistic Features To capture the content of a post, on top of a bag-ofwords representation with unigrams and bigrams, we also consider basic lengths, discourse cues, repeated punctuation counts and counts of lexical categories based on the Linguistic Inquiry and Word Count tool (LIWC) (Pennebaker et al., 2001). Basic length features capture the number of sentences, words, and characters, along with the average word and sentence lengths for each post. The discourse cues feature captures frequency counts for the first few words of the post, which often contain discourse cues. To capture the information in repeated punctuation like “!!”, “??” or “?!” we include the frequency count of the given punctuation patterns as a feature of each post (Anand et al., 2011). LIWC counts capture sentiment by giving the degree to which the post uses certain categories of subjective language. 3.2.2 Relational Informat</context>
</contexts>
<marker>Pennebaker, Francis, Booth, 2001</marker>
<rawString>J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001. LIWC: Linguistic Inquiry and Word Count.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minghui Qiu</author>
<author>Liu Yang</author>
<author>Jing Jiang</author>
</authors>
<title>Modeling interaction features for debate side clustering.</title>
<date>2013</date>
<booktitle>In ACM International Conference on Information &amp; Knowledge Management (CIKM),</booktitle>
<pages>873--878</pages>
<contexts>
<context position="2107" citStr="Qiu et al., 2013" startWordPosition="319" endWordPosition="322">e debate site on issues ranging from gun control to gay marriage. We show that our collective classification model is able to easily incorporate rich, relational information and outperforms a local model which uses only linguistic information. 1 Introduction Modeling user stance (PRO, CON) in discussion topics in online social media debate is of interest to researchers, corporations and governmental organizations alike. Predicting a user’s stance towards a given issue can support the identification of social or political groups (Gawron et al., 2012; Abu-Jbara et al., 2012; Anand et al., 2011; Qiu et al., 2013; Hasan and Ng, 2013), help develop better recommendation systems, or tailor users’ information preferences to their ideologies and beliefs. Stance classification problems consist of a collection of debate-style discussions by authors on different controversial, political topics. While these may be spoken as in the Congressional Debates corpus (Thomas et al., 2006; Burfoot, 2008), we focus on forum posts on social media debate sites. Users on debate sites share their opinions freely, using informal and social language, providing a rich and much more challenging domain for stance prediction. So</context>
</contexts>
<marker>Qiu, Yang, Jiang, 2013</marker>
<rawString>Minghui Qiu, Liu Yang, and Jing Jiang. 2013. Modeling interaction features for debate side clustering. In ACM International Conference on Information &amp; Knowledge Management (CIKM), pages 873–878.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prithviraj Sen</author>
<author>Galileo Mark Namata</author>
<author>Mustafa Bilgic</author>
<author>Lise Getoor</author>
<author>Brian Gallagher</author>
<author>Tina Eliassi-Rad</author>
</authors>
<title>Collective classification in network data.</title>
<date>2008</date>
<journal>AI Magazine,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="6098" citStr="Sen et al., 2008" startWordPosition="996" endWordPosition="999">xual prefference is just a little kinkier than thiers? Figure 1: Sample Quote/Response Pair from 4forums.com with Mechanical Turk annotations for stance. Both response posts are from the same author. both response posts are from the same author. We describe the dataset further in Section 4.1. We believe that models of post stance in online debate should capture both the content and the context of author posts. By jointly reasoning over both the content of the post and its relationships with other posts in the discussion, we perform collective classification, as we further define in Section 3 (Sen et al., 2008). Previous work has shown that collective classification models often perform better than content-only approaches. (Burfoot et al., 2011; Hasan and Ng, 2013; Thomas et al., 2006; Bansal et al., 2008; Walker et al., 2012c). Here, we develop a collective classification approach for stance prediction which leverages the sentiment conveyed in a post through its language, and the reply links consisting of agreements or rebuttals between posts in a discussion. We implement our approach using Probabilistic Soft Logic (PSL) (Bach et al., 2013), a recently introduced tool for collective inference in re</context>
<context position="10033" citStr="Sen et al., 2008" startWordPosition="1695" endWordPosition="1698">ment edge exists from post pu to pv if pu disagrees with pv. A disagreement edge exists from aw to ay if any of the posts {pw, ... , px} mapped to aw disagree with any posts {py, ... pz} mapped to ay. We similarly define agreement edges for both posts and authors. 3 Collective Classification of Stance Given the discussion structure defined in the previous section, our task is to infer the stance of each post. We make use of both linguistic features and the relational structure in order to collectively or jointly infer the stance labels. This corresponds to a collective classification setting (Sen et al., 2008), in which we are given a multi-relational network and some partially observed labels, and we wish to infer all of the unobserved labels, conditioned on observed attributes and links. Collective classification refers to the combined classification of a set of interdependent objects (posts, in our domain) using information given by both the local features of the objects and the properties of the objects’ neighbors in a network. For the stance classification problem, we infer stance labels for posts using both the correlation between a post and its linguistic attributes {xij1, ... , xijN}, and t</context>
</contexts>
<marker>Sen, Namata, Bilgic, Getoor, Gallagher, Eliassi-Rad, 2008</marker>
<rawString>Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad. 2008. Collective classification in network data. AI Magazine, 29(3):93–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>J Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In ACL and AFNLP,</booktitle>
<pages>226--234</pages>
<contexts>
<context position="24784" citStr="Somasundaran and Wiebe, 2009" startWordPosition="4133" endWordPosition="4136">r alone. Thus, the combination of the rules gives the model its advantage, allowing the PSL model to make use of a richer structure that has multiple types of relations and more information. 5 Related Work Over the last ten years, there has been significant progress on modeling stance. Previous work covers three different debate settings: (1) congressional debates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Walker et al., 2012c; Anand et al., 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites because the posts are 114 Classifier F1 Score AUC-PR negative class AUROC LOCAL 0.66 ± 0.015 0.44 ± 0.04 0.54 ± 0.02 PSL 0.74 ± 0.04 0.511 ± 0.04 0.59 ± 0.05 Table 3: Averages and standard deviations for F1 score for the positive class, area under PR curve for the negative class, and area under ROC curve for post stance over 10 train/test splits. isProPost(P, T) ∧ wr</context>
<context position="26446" citStr="Somasundaran and Wiebe, 2009" startWordPosition="4418" endWordPosition="4421">Auth(A, A2) ∧¬ isProAuth(A, T) → ¬isProAuth(A2, T) : 0.0 disagreesAuth(A, A2) ∧ isProAuth(A, T) → ¬ isProAuth(A2, T) : 0.23 disagreesAuth(A, A2) ∧¬ isProAuth(A, T) → isProAuth(A2, T) : 0.6 hasLabelPro(P, T) → isProPost(P, T) : 2.2 ¬ hasLabelPro(P, T) → ¬ isProPost(P, T) : 4.8 Table 4: Weights learned by the model for the PSL rules in train/test split 2 of experiments shorter and the language is more informal and social. We predict that this difference makes it more difficult to achieve accuracies as high for 4FORUMS discussions as can be achieved for the congressional debates corpus. Work by (Somasundaran and Wiebe, 2009) on idealogical debates very similar to our own show that identifying argumentation structure improves performance; their best performance is approximately 64% accuracy over all topics. Research by (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009) classifies the speaker’s stance in a corpus of congressional floor debates. This work combines graph-based and text-classification approaches to achieve 75% accuracy on Congressional debate siding over all topics. Other work applies MaxCut to the reply structure of company discussion forums (Malouf and Mullen,</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>S. Somasundaran and J. Wiebe. 2009. Recognizing stances in online debates. In ACL and AFNLP, pages 226–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>J Wiebe</author>
</authors>
<title>Recognizing stances in ideological on-line debates.</title>
<date>2010</date>
<booktitle>In NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>116--124</pages>
<contexts>
<context position="24814" citStr="Somasundaran and Wiebe, 2010" startWordPosition="4137" endWordPosition="4140"> of the rules gives the model its advantage, allowing the PSL model to make use of a richer structure that has multiple types of relations and more information. 5 Related Work Over the last ten years, there has been significant progress on modeling stance. Previous work covers three different debate settings: (1) congressional debates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Walker et al., 2012c; Anand et al., 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites because the posts are 114 Classifier F1 Score AUC-PR negative class AUROC LOCAL 0.66 ± 0.015 0.44 ± 0.04 0.54 ± 0.02 PSL 0.74 ± 0.04 0.511 ± 0.04 0.59 ± 0.05 Table 3: Averages and standard deviations for F1 score for the positive class, area under PR curve for the negative class, and area under ROC curve for post stance over 10 train/test splits. isProPost(P, T) ∧ writesPost(A, P) → isProAuth(A, </context>
</contexts>
<marker>Somasundaran, Wiebe, 2010</marker>
<rawString>S. Somasundaran and J. Wiebe. 2010. Recognizing stances in ideological on-line debates. In NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 116–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thomas</author>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from Congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>327--335</pages>
<contexts>
<context position="2473" citStr="Thomas et al., 2006" startWordPosition="375" endWordPosition="378">earchers, corporations and governmental organizations alike. Predicting a user’s stance towards a given issue can support the identification of social or political groups (Gawron et al., 2012; Abu-Jbara et al., 2012; Anand et al., 2011; Qiu et al., 2013; Hasan and Ng, 2013), help develop better recommendation systems, or tailor users’ information preferences to their ideologies and beliefs. Stance classification problems consist of a collection of debate-style discussions by authors on different controversial, political topics. While these may be spoken as in the Congressional Debates corpus (Thomas et al., 2006; Burfoot, 2008), we focus on forum posts on social media debate sites. Users on debate sites share their opinions freely, using informal and social language, providing a rich and much more challenging domain for stance prediction. Social media debate sites contain online discussions with posts from various authors, where each post is either a response to another post or the root of the discussion (Anand et al., 2011; Walker et al., 2012a). Posts are linked to one another by either rebuttal or agreement links and are labelled for stance, either PRO or CON, depending on the framing of the issue</context>
<context position="6275" citStr="Thomas et al., 2006" startWordPosition="1022" endWordPosition="1025">re from the same author. both response posts are from the same author. We describe the dataset further in Section 4.1. We believe that models of post stance in online debate should capture both the content and the context of author posts. By jointly reasoning over both the content of the post and its relationships with other posts in the discussion, we perform collective classification, as we further define in Section 3 (Sen et al., 2008). Previous work has shown that collective classification models often perform better than content-only approaches. (Burfoot et al., 2011; Hasan and Ng, 2013; Thomas et al., 2006; Bansal et al., 2008; Walker et al., 2012c). Here, we develop a collective classification approach for stance prediction which leverages the sentiment conveyed in a post through its language, and the reply links consisting of agreements or rebuttals between posts in a discussion. We implement our approach using Probabilistic Soft Logic (PSL) (Bach et al., 2013), a recently introduced tool for collective inference in relational data. We evaluate our model on data from the 4FORUMS online debate site (Walker et al., 2012b). Section 2 first presents an overview of our approach and then in Section</context>
<context position="24543" citStr="Thomas et al., 2006" startWordPosition="4096" endWordPosition="4099">reement from the model, and found that the model performs poorly when disregarding the links between nodes entirely. The PSL model learns to weight the first and second rule very highly, and does worse than when considering the prior alone. Thus, the combination of the rules gives the model its advantage, allowing the PSL model to make use of a richer structure that has multiple types of relations and more information. 5 Related Work Over the last ten years, there has been significant progress on modeling stance. Previous work covers three different debate settings: (1) congressional debates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Walker et al., 2012c; Anand et al., 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites because the posts are 114 Classifier F1 Score AUC-PR negative class AUROC LOCAL 0.66 ± 0.015 0.44 ± 0.04 0.54 ± 0.02 PSL 0.74 ± </context>
<context position="26664" citStr="Thomas et al., 2006" startWordPosition="4450" endWordPosition="4453">.2 ¬ hasLabelPro(P, T) → ¬ isProPost(P, T) : 4.8 Table 4: Weights learned by the model for the PSL rules in train/test split 2 of experiments shorter and the language is more informal and social. We predict that this difference makes it more difficult to achieve accuracies as high for 4FORUMS discussions as can be achieved for the congressional debates corpus. Work by (Somasundaran and Wiebe, 2009) on idealogical debates very similar to our own show that identifying argumentation structure improves performance; their best performance is approximately 64% accuracy over all topics. Research by (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009) classifies the speaker’s stance in a corpus of congressional floor debates. This work combines graph-based and text-classification approaches to achieve 75% accuracy on Congressional debate siding over all topics. Other work applies MaxCut to the reply structure of company discussion forums (Malouf and Mullen, 2008; Murakami and Raymond, 2010; Agrawal et al., 2003). Murakami &amp; Raymond (2010) show that rules for identifying agreement, defined on the textual content of the post improve performance. More recent work has explic</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>M. Thomas, B. Pang, and L. Lee. 2006. Get out the vote: Determining support or opposition from Congressional floor-debate transcripts. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 327–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Pranav Anand</author>
<author>Rob Abbott</author>
<author>Jean E Fox Tree</author>
<author>Craig Martell</author>
<author>Joseph King</author>
</authors>
<title>That’s your evidence?: Classifying stance in online political debate. Decision Support Sciences.</title>
<date>2012</date>
<contexts>
<context position="1446" citStr="Walker et al., 2012" startWordPosition="211" endWordPosition="214">elop a novel collective classification approach to stance classification, which makes use of both structural and linguistic features, and which collectively labels the posts’ stance across a network of the users’ posts. We identify both linguistic features of the posts and features that capture the underlying relationships between posts and users. We use probabilistic soft logic (PSL) (Bach et al., 2013) to model post stance by leveraging both these local linguistic features as well as the observed network structure of the posts to reason over the dataset. We evaluate our approach on 4FORUMS (Walker et al., 2012b), a collection of discussions from an online debate site on issues ranging from gun control to gay marriage. We show that our collective classification model is able to easily incorporate rich, relational information and outperforms a local model which uses only linguistic information. 1 Introduction Modeling user stance (PRO, CON) in discussion topics in online social media debate is of interest to researchers, corporations and governmental organizations alike. Predicting a user’s stance towards a given issue can support the identification of social or political groups (Gawron et al., 2012;</context>
<context position="2914" citStr="Walker et al., 2012" startWordPosition="450" endWordPosition="453">lection of debate-style discussions by authors on different controversial, political topics. While these may be spoken as in the Congressional Debates corpus (Thomas et al., 2006; Burfoot, 2008), we focus on forum posts on social media debate sites. Users on debate sites share their opinions freely, using informal and social language, providing a rich and much more challenging domain for stance prediction. Social media debate sites contain online discussions with posts from various authors, where each post is either a response to another post or the root of the discussion (Anand et al., 2011; Walker et al., 2012a). Posts are linked to one another by either rebuttal or agreement links and are labelled for stance, either PRO or CON, depending on the framing of the issue under discussion. Each post reflects the stance and sentiment of its author. Authors may participate in multiple discussions in the same topic, and may discuss multiple topics. For example consider the sample posts from the online discussion forum 4forums.com shown in Fig. 1. Here, we see discussion topics, together with sample quotes and responses, where the response is a direct reply to the quote text. The annotations for stance were </context>
<context position="6317" citStr="Walker et al., 2012" startWordPosition="1030" endWordPosition="1033">ts are from the same author. We describe the dataset further in Section 4.1. We believe that models of post stance in online debate should capture both the content and the context of author posts. By jointly reasoning over both the content of the post and its relationships with other posts in the discussion, we perform collective classification, as we further define in Section 3 (Sen et al., 2008). Previous work has shown that collective classification models often perform better than content-only approaches. (Burfoot et al., 2011; Hasan and Ng, 2013; Thomas et al., 2006; Bansal et al., 2008; Walker et al., 2012c). Here, we develop a collective classification approach for stance prediction which leverages the sentiment conveyed in a post through its language, and the reply links consisting of agreements or rebuttals between posts in a discussion. We implement our approach using Probabilistic Soft Logic (PSL) (Bach et al., 2013), a recently introduced tool for collective inference in relational data. We evaluate our model on data from the 4FORUMS online debate site (Walker et al., 2012b). Section 2 first presents an overview of our approach and then in Section 3.1 we describe the PSL framework in more</context>
<context position="18893" citStr="Walker et al., 2012" startWordPosition="3133" endWordPosition="3136">ows the rules in our PSL model. Topic Authors Posts Abortion 385 8114 Evolution 325 6186 Gun Control 319 3899 Gay Marriage 316 7025 Death Penalty 170 572 Table 2: Overview of topics in 4FORUMSdataset. 4 Experimental Evaluation We first describe the dataset we use for evaluation and then describe our evaluation method and results. 4.1 Dataset We evaluate our proposed approach on discussions from https://www.4forums.com, an online debate site on social and political issues. The dataset is publicly available as part of the Internet Argument Corpus, an annotated collection of 109,533 forum posts (Walker et al., 2012b; Walker et al., 2012c). On 4FORUMS, a user initiates a discussion by posting a new question or comment under a topic, or participate in an ongoing discussion by replying to any of the posts in the thread. The discussions were given to English speaking Mechanical Turk annotators for a number of annotation tasks to get labels for the stances of discussion participants towards the topic, and scores for each post in a discussion indicating whether it is in agreement or disagreement with the preceding post. The scores for agreement and disagreement were on a 11 point scale [-5, 5] implemented usi</context>
<context position="20683" citStr="Walker et al., 2012" startWordPosition="3439" endWordPosition="3442"> of the authors in the discussion towards the topic. We use the stance of each post’s author to determine the post’s stance. For our experiments, we use all posts with annotations for stance, and about 90% of these posts also have annotations for agree113 ment/disagreement. The discussions span many topics, and Table 2 gives a summary of the topics we consider in our experiments and the distribution of posts across these topics. Each post in a discussion comes as a quote-response pair, where the quote is the text that the post is in response to, and the response is the post text. We refer to (Walker et al., 2012b) for a full description of the corpus and the annotation process. 4.2 Evaluation In order to evaluate our methods, we split the dataset into training and testing sets by randomly selecting half the authors from each topic and their posts for the training set and using the remaining authors and their posts for the test set. This way, we ensure that no two authors appear in both training and test sets for the same topic, since stance is topic-dependent. We create 10 randomly sampled train/test splits for evaluation. Each split contains about 18,000 posts. For each train/test split, we train a </context>
<context position="24881" citStr="Walker et al., 2012" startWordPosition="4149" endWordPosition="4152">use of a richer structure that has multiple types of relations and more information. 5 Related Work Over the last ten years, there has been significant progress on modeling stance. Previous work covers three different debate settings: (1) congressional debates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Walker et al., 2012c; Anand et al., 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites because the posts are 114 Classifier F1 Score AUC-PR negative class AUROC LOCAL 0.66 ± 0.015 0.44 ± 0.04 0.54 ± 0.02 PSL 0.74 ± 0.04 0.511 ± 0.04 0.59 ± 0.05 Table 3: Averages and standard deviations for F1 score for the positive class, area under PR curve for the negative class, and area under ROC curve for post stance over 10 train/test splits. isProPost(P, T) ∧ writesPost(A, P) → isProAuth(A, T) : 10.2 ¬ isProPost(P, T) ∧ writesPost(A, P) → ¬ isProAuth(A, T) </context>
<context position="27402" citStr="Walker et al., 2012" startWordPosition="4568" endWordPosition="4571">ongressional floor debates. This work combines graph-based and text-classification approaches to achieve 75% accuracy on Congressional debate siding over all topics. Other work applies MaxCut to the reply structure of company discussion forums (Malouf and Mullen, 2008; Murakami and Raymond, 2010; Agrawal et al., 2003). Murakami &amp; Raymond (2010) show that rules for identifying agreement, defined on the textual content of the post improve performance. More recent work has explicitly focused on the benefits of collective classification in these settings (Burfoot et al., 2011; Hasan and Ng, 2013; Walker et al., 2012c), and has shown in each case that collective classification improves performance. The results reported here are the first to apply the PSL collective classification framework to the forums conversations from the IAC corpus (Anand et al., 2011; Walker et al., 2012c). 6 Discussion and Future Work Here, we introduce a novel approach to classify stance of posts from online debate forums with a collective classification framework. We formally construct a model, using PSL, to capture relational information in the network of authors and posts and our intuition that agreement or disagreement between</context>
</contexts>
<marker>Walker, Anand, Abbott, Tree, Martell, King, 2012</marker>
<rawString>Marilyn Walker, Pranav Anand, Rob Abbott, Jean E. Fox Tree, Craig Martell, and Joseph King. 2012a. That’s your evidence?: Classifying stance in online political debate. Decision Support Sciences.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marilyn Walker</author>
<author>Pranav Anand</author>
<author>Robert Abbott</author>
<author>E Jean</author>
</authors>
<title>Fox Tree. 2012b. A corpus for research on deliberation and debate.</title>
<booktitle>In Language Resources and Evaluation Conference, LREC2012.</booktitle>
<marker>Walker, Anand, Abbott, Jean, </marker>
<rawString>Marilyn Walker, Pranav Anand, Robert Abbott, and Jean E. Fox Tree. 2012b. A corpus for research on deliberation and debate. In Language Resources and Evaluation Conference, LREC2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Pranav Anand</author>
<author>Robert Abbott</author>
<author>Richard Grant</author>
</authors>
<title>Stance classification using dialogic properties of persuasion.</title>
<date>2012</date>
<booktitle>In Meeting of the North American Association for Computational Linguistics. NAACL-HLT12.</booktitle>
<contexts>
<context position="1446" citStr="Walker et al., 2012" startWordPosition="211" endWordPosition="214">elop a novel collective classification approach to stance classification, which makes use of both structural and linguistic features, and which collectively labels the posts’ stance across a network of the users’ posts. We identify both linguistic features of the posts and features that capture the underlying relationships between posts and users. We use probabilistic soft logic (PSL) (Bach et al., 2013) to model post stance by leveraging both these local linguistic features as well as the observed network structure of the posts to reason over the dataset. We evaluate our approach on 4FORUMS (Walker et al., 2012b), a collection of discussions from an online debate site on issues ranging from gun control to gay marriage. We show that our collective classification model is able to easily incorporate rich, relational information and outperforms a local model which uses only linguistic information. 1 Introduction Modeling user stance (PRO, CON) in discussion topics in online social media debate is of interest to researchers, corporations and governmental organizations alike. Predicting a user’s stance towards a given issue can support the identification of social or political groups (Gawron et al., 2012;</context>
<context position="2914" citStr="Walker et al., 2012" startWordPosition="450" endWordPosition="453">lection of debate-style discussions by authors on different controversial, political topics. While these may be spoken as in the Congressional Debates corpus (Thomas et al., 2006; Burfoot, 2008), we focus on forum posts on social media debate sites. Users on debate sites share their opinions freely, using informal and social language, providing a rich and much more challenging domain for stance prediction. Social media debate sites contain online discussions with posts from various authors, where each post is either a response to another post or the root of the discussion (Anand et al., 2011; Walker et al., 2012a). Posts are linked to one another by either rebuttal or agreement links and are labelled for stance, either PRO or CON, depending on the framing of the issue under discussion. Each post reflects the stance and sentiment of its author. Authors may participate in multiple discussions in the same topic, and may discuss multiple topics. For example consider the sample posts from the online discussion forum 4forums.com shown in Fig. 1. Here, we see discussion topics, together with sample quotes and responses, where the response is a direct reply to the quote text. The annotations for stance were </context>
<context position="6317" citStr="Walker et al., 2012" startWordPosition="1030" endWordPosition="1033">ts are from the same author. We describe the dataset further in Section 4.1. We believe that models of post stance in online debate should capture both the content and the context of author posts. By jointly reasoning over both the content of the post and its relationships with other posts in the discussion, we perform collective classification, as we further define in Section 3 (Sen et al., 2008). Previous work has shown that collective classification models often perform better than content-only approaches. (Burfoot et al., 2011; Hasan and Ng, 2013; Thomas et al., 2006; Bansal et al., 2008; Walker et al., 2012c). Here, we develop a collective classification approach for stance prediction which leverages the sentiment conveyed in a post through its language, and the reply links consisting of agreements or rebuttals between posts in a discussion. We implement our approach using Probabilistic Soft Logic (PSL) (Bach et al., 2013), a recently introduced tool for collective inference in relational data. We evaluate our model on data from the 4FORUMS online debate site (Walker et al., 2012b). Section 2 first presents an overview of our approach and then in Section 3.1 we describe the PSL framework in more</context>
<context position="18893" citStr="Walker et al., 2012" startWordPosition="3133" endWordPosition="3136">ows the rules in our PSL model. Topic Authors Posts Abortion 385 8114 Evolution 325 6186 Gun Control 319 3899 Gay Marriage 316 7025 Death Penalty 170 572 Table 2: Overview of topics in 4FORUMSdataset. 4 Experimental Evaluation We first describe the dataset we use for evaluation and then describe our evaluation method and results. 4.1 Dataset We evaluate our proposed approach on discussions from https://www.4forums.com, an online debate site on social and political issues. The dataset is publicly available as part of the Internet Argument Corpus, an annotated collection of 109,533 forum posts (Walker et al., 2012b; Walker et al., 2012c). On 4FORUMS, a user initiates a discussion by posting a new question or comment under a topic, or participate in an ongoing discussion by replying to any of the posts in the thread. The discussions were given to English speaking Mechanical Turk annotators for a number of annotation tasks to get labels for the stances of discussion participants towards the topic, and scores for each post in a discussion indicating whether it is in agreement or disagreement with the preceding post. The scores for agreement and disagreement were on a 11 point scale [-5, 5] implemented usi</context>
<context position="20683" citStr="Walker et al., 2012" startWordPosition="3439" endWordPosition="3442"> of the authors in the discussion towards the topic. We use the stance of each post’s author to determine the post’s stance. For our experiments, we use all posts with annotations for stance, and about 90% of these posts also have annotations for agree113 ment/disagreement. The discussions span many topics, and Table 2 gives a summary of the topics we consider in our experiments and the distribution of posts across these topics. Each post in a discussion comes as a quote-response pair, where the quote is the text that the post is in response to, and the response is the post text. We refer to (Walker et al., 2012b) for a full description of the corpus and the annotation process. 4.2 Evaluation In order to evaluate our methods, we split the dataset into training and testing sets by randomly selecting half the authors from each topic and their posts for the training set and using the remaining authors and their posts for the test set. This way, we ensure that no two authors appear in both training and test sets for the same topic, since stance is topic-dependent. We create 10 randomly sampled train/test splits for evaluation. Each split contains about 18,000 posts. For each train/test split, we train a </context>
<context position="24881" citStr="Walker et al., 2012" startWordPosition="4149" endWordPosition="4152">use of a richer structure that has multiple types of relations and more information. 5 Related Work Over the last ten years, there has been significant progress on modeling stance. Previous work covers three different debate settings: (1) congressional debates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Walker et al., 2012c; Anand et al., 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites because the posts are 114 Classifier F1 Score AUC-PR negative class AUROC LOCAL 0.66 ± 0.015 0.44 ± 0.04 0.54 ± 0.02 PSL 0.74 ± 0.04 0.511 ± 0.04 0.59 ± 0.05 Table 3: Averages and standard deviations for F1 score for the positive class, area under PR curve for the negative class, and area under ROC curve for post stance over 10 train/test splits. isProPost(P, T) ∧ writesPost(A, P) → isProAuth(A, T) : 10.2 ¬ isProPost(P, T) ∧ writesPost(A, P) → ¬ isProAuth(A, T) </context>
<context position="27402" citStr="Walker et al., 2012" startWordPosition="4568" endWordPosition="4571">ongressional floor debates. This work combines graph-based and text-classification approaches to achieve 75% accuracy on Congressional debate siding over all topics. Other work applies MaxCut to the reply structure of company discussion forums (Malouf and Mullen, 2008; Murakami and Raymond, 2010; Agrawal et al., 2003). Murakami &amp; Raymond (2010) show that rules for identifying agreement, defined on the textual content of the post improve performance. More recent work has explicitly focused on the benefits of collective classification in these settings (Burfoot et al., 2011; Hasan and Ng, 2013; Walker et al., 2012c), and has shown in each case that collective classification improves performance. The results reported here are the first to apply the PSL collective classification framework to the forums conversations from the IAC corpus (Anand et al., 2011; Walker et al., 2012c). 6 Discussion and Future Work Here, we introduce a novel approach to classify stance of posts from online debate forums with a collective classification framework. We formally construct a model, using PSL, to capture relational information in the network of authors and posts and our intuition that agreement or disagreement between</context>
</contexts>
<marker>Walker, Anand, Abbott, Grant, 2012</marker>
<rawString>Marilyn Walker, Pranav Anand, Robert Abbott, and Richard Grant. 2012c. Stance classification using dialogic properties of persuasion. In Meeting of the North American Association for Computational Linguistics. NAACL-HLT12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Wang</author>
<author>C P Ros´e</author>
</authors>
<title>Making conversational structure explicit: identification of initiationresponse pairs within online discussions.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>673--676</pages>
<marker>Wang, Ros´e, 2010</marker>
<rawString>Y.C. Wang and C.P. Ros´e. 2010. Making conversational structure explicit: identification of initiationresponse pairs within online discussions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 673–676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yessenalina</author>
<author>Y Yue</author>
<author>C Cardie</author>
</authors>
<title>Multi-level structured models for document-level sentiment classification.</title>
<date>2010</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1046--1056</pages>
<contexts>
<context position="24590" citStr="Yessenalina et al., 2010" startWordPosition="4104" endWordPosition="4107"> model performs poorly when disregarding the links between nodes entirely. The PSL model learns to weight the first and second rule very highly, and does worse than when considering the prior alone. Thus, the combination of the rules gives the model its advantage, allowing the PSL model to make use of a richer structure that has multiple types of relations and more information. 5 Related Work Over the last ten years, there has been significant progress on modeling stance. Previous work covers three different debate settings: (1) congressional debates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011; Walker et al., 2012c; Anand et al., 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites because the posts are 114 Classifier F1 Score AUC-PR negative class AUROC LOCAL 0.66 ± 0.015 0.44 ± 0.04 0.54 ± 0.02 PSL 0.74 ± 0.04 0.511 ± 0.04 0.59 ± 0.05 Table 3: Averages</context>
<context position="26711" citStr="Yessenalina et al., 2010" startWordPosition="4458" endWordPosition="4462"> : 4.8 Table 4: Weights learned by the model for the PSL rules in train/test split 2 of experiments shorter and the language is more informal and social. We predict that this difference makes it more difficult to achieve accuracies as high for 4FORUMS discussions as can be achieved for the congressional debates corpus. Work by (Somasundaran and Wiebe, 2009) on idealogical debates very similar to our own show that identifying argumentation structure improves performance; their best performance is approximately 64% accuracy over all topics. Research by (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009) classifies the speaker’s stance in a corpus of congressional floor debates. This work combines graph-based and text-classification approaches to achieve 75% accuracy on Congressional debate siding over all topics. Other work applies MaxCut to the reply structure of company discussion forums (Malouf and Mullen, 2008; Murakami and Raymond, 2010; Agrawal et al., 2003). Murakami &amp; Raymond (2010) show that rules for identifying agreement, defined on the textual content of the post improve performance. More recent work has explicitly focused on the benefits of collective clas</context>
</contexts>
<marker>Yessenalina, Yue, Cardie, 2010</marker>
<rawString>A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multi-level structured models for document-level sentiment classification. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1046–1056.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
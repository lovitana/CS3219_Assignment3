<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005712">
<title confidence="0.997193">
Target-Centric Features for Translation Quality Estimation
</title>
<author confidence="0.98964">
Chris Hokamp and Iacer Calixto and Joachim Wagner and Jian Zhang
</author>
<affiliation confidence="0.995389">
CNGL Centre for Global Intelligent Content
Dublin City University
School of Computing
</affiliation>
<address confidence="0.658022">
Dublin, Ireland
</address>
<email confidence="0.998837">
{chokamp|icalixto|jwagner|zhangj}@computing.dcu.ie
</email>
<sectionHeader confidence="0.997388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999847">
We describe the DCU-MIXED and DCU-
SVR submissions to the WMT-14 Quality
Estimation task 1.1, predicting sentence-
level perceived post-editing effort. Fea-
ture design focuses on target-side features
as we hypothesise that the source side has
little effect on the quality of human trans-
lations, which are included in task 1.1
of this year’s WMT Quality Estimation
shared task. We experiment with features
of the QuEst framework, features of our
past work, and three novel feature sets.
Despite these efforts, our two systems per-
form poorly in the competition. Follow up
experiments indicate that the poor perfor-
mance is due to improperly optimised pa-
rameters.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999425222222222">
Translation quality estimation tries to predict the
quality of a translation given the source and target
text but no reference translations. Different from
previous years (Callison-Burch et al., 2012; Bo-
jar et al., 2013), the WMT 2014 Quality Estima-
tion shared task is MT system-independent, i. e. no
glass-box features are available and translations in
the training and test sets are produced by different
MT systems and also by human translators.
This paper describes the CNGL@DCU team
submission to task 1.1 of the WMT 2014 Quality
Estimation shared task.1 The task is to predict the
perceived post-editing effort given a source sen-
tence and its raw translation. Due to the inclusion
of human translation in the task, we focus our ef-
forts on target-side features as we expect that the
quality of a translation produced by a human trans-
lator is much less affected by features of the source
</bodyText>
<footnote confidence="0.937145">
1A CNGL system based on referential translation ma-
chines is submitted separately (Bic¸ici and Way, 2014).
</footnote>
<bodyText confidence="0.996244285714286">
than by extrinsic factors such as time pressure and
familiarity with the domain.
To build our quality estimation system, we use
and extend the QuEst framework for translation
quality estimation2 (Shah et al., 2013; Specia
et al., 2013). QuEst provides modules for fea-
ture extraction and machine learning. We modify
both the feature extraction framework and the ma-
chine learning components to add functionality to
QuEst.
The novel features we add to our systems are
(a) a language model on a combination of stop
words and POS tags, (b) inverse glass-box fea-
tures for translating the translation, and (c) ran-
dom indexing (Sahlgren, 2005) for measuring the
semantic similarity of source and target side across
languages. Furthermore, we integrated (d) source-
side pseudo-reference features (Soricut and Echi-
habi, 2010) and (e) error grammar features (Wag-
ner, 2012), which were used first in MT quality
estimation by (Rubino et al., 2012; Rubino et al.,
2013).
The remaining sections are organised as fol-
lows. Section 2 gives details on the features we
use. Section 3 describes how we set up our ex-
periments. Results are presented in Section 4 and
conclusions are drawn in Section 5 together with
pointers to future work.
</bodyText>
<sectionHeader confidence="0.998816" genericHeader="introduction">
2 Features
</sectionHeader>
<bodyText confidence="0.99982725">
This section describes the features we extract from
source and target sentences in order to train predic-
tion models and to make predictions in addition to
the baseline features provided for the task.
We focus on the target side as we assume that
the quality of the source side has little predictive
power for human translations, which are included
in task 1.1.
</bodyText>
<footnote confidence="0.981194">
2http://www.quest.dcs.shef.ac.uk/
</footnote>
<page confidence="0.977216">
329
</page>
<bodyText confidence="0.257566">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 329–334,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.998118">
2.1 QuEst Black-Box Features and Baseline
Features
</subsectionHeader>
<bodyText confidence="0.99995">
We use the QuEst framework to extract 47 ba-
sic black-box features from both source and tar-
get side, such as the ratio of the number of to-
kens, punctuation statistics, number if mismatched
brackets and quotes, language model perplexity,
n-gram frequency quartile statistics (n = 1, 2, 3),
and coarse-grained POS frequency ratios. 17 of
the 47 features are identical to the baseline fea-
tures from the shared task website, i.e. 30 fea-
tures are new. To train the language models and
to extract frequency information, we use the News
Commentary corpus (Bojar et al., 2013).
</bodyText>
<subsectionHeader confidence="0.995294">
2.2 POS and Stop Word Language Model
Features
</subsectionHeader>
<bodyText confidence="0.999966">
For all languages, we extract probability and per-
plexity features from language models trained on
POS tagged corpora. POS tagging is performed
using the IMS Tree Tagger (Schmid, 1994).
We also experiment with language models built
from a combination of stop words3 and POS tags.
Starting with a tokenised corpus, and its POS-
tagged counterpart, we create a new representation
of the corpus by replacing POS tags for stop words
with the literal stop word that occurred in the orig-
inal corpus, leaving non-stop word tags intact.4
The intuition behind the approach is that the com-
bined POS and stop word model should encode
the distributional tendencies of the most common
words in the language.
The log-probability and the perplexity of the
target side are used as features. The development
of these features was motivated by manual exam-
ination of the common error types in the train-
ing data. We noted that stop word errors (omis-
sion, mistranslation, mis-translation of idiom), are
prevalent in all language pairs, indicating that fea-
tures which focus on stop word usage could be
useful for predicting the quality of machine trans-
lation. We implement POS and stop word lan-
guage models inside the QuEst framework.
</bodyText>
<subsectionHeader confidence="0.999414">
2.3 Source-Side Pseudo-Reference Features
</subsectionHeader>
<bodyText confidence="0.996222">
We extract source-side pseudo-reference features
(Albrecht and Hwa, 2008; Soricut and Echihabi,
</bodyText>
<footnote confidence="0.9880346">
3We use the stop word lists from Apache Lucene (McCan-
dless et al., 2010).
4The News Commentary corpus from WMT13 was used
to build these models, same as for the black-box features
(Section 2.1).
</footnote>
<bodyText confidence="0.987398">
2010; Rubino et al., 2012), for English to German
quality prediction using a highly-tuned German to
English translation system (Li et al., 2014) work-
ing in the reverse direction. The MT system trans-
lates the German target side, the quality of which
is to be predicted, back into English, and we ex-
tract pseudo-reference features on the source side:
</bodyText>
<listItem confidence="0.825109428571429">
• BLEU score (Papineni et al., 2002) be-
tween back-translation and original source
sentence, and
• TER score (Snover et al., 2006).
For the 5th English to German test set item, for
example, the translation
(1) Und belasse sie dort eine Woche.
is translated back to English as
(2) and leave it therefor a week.
and compared to the original source sentence
(3) Leave for a week.
producing a BLEU score of 0.077 using the
Python interface to the cdec toolkit (Chahuneau et
al., 2012).
</listItem>
<subsectionHeader confidence="0.996407">
2.4 Inverse Glass-Box Features for
Translating the Translation
</subsectionHeader>
<bodyText confidence="0.999995368421053">
In the absence of direct glass-box features, we ob-
tain glass-box features from translating the raw
translation back to the source language using the
same MT system that we use for the source-side
pseudo-reference features. We extract features
from the following components of the Moses de-
coder: distortion model, language model, lexi-
cal reordering, lexical translation probability, op-
erational sequence model (Durrani et al., 2013),
phrase translation probability, and the decoder
score.
The intuition for this set of features is that back-
translating an incorrect translation will give low
system-internal scores, e. g. a low phrase transla-
tion score, and produce poor output with low lan-
guage model scores (garbage in, garbage out).
We are not aware of any previous work using
inverse glass-box features of translating the target
side to another language for quality estimation.
</bodyText>
<page confidence="0.984618">
330
</page>
<subsectionHeader confidence="0.9405015">
2.5 Semantic Similarity Using Random
Indexing
</subsectionHeader>
<bodyText confidence="0.999975235294117">
These features try to measure the semantic sim-
ilarity of source and target side of a translation
unit for quality estimation using random index-
ing (Sahlgren, 2005). We experiment with adding
the similarity score of the source and target ran-
dom vectors.
For each source and target pair in the English-
Spanish portion of the Europarl corpus (Koehn,
2005), we initialize a sparse random vector. We
then create token vectors for each source and tar-
get token by summing the vectors for all of the
segments where the token occurs. To extract the
similarity feature for new source and target pairs,
we map them into the vector space by taking the
centroid of the token vectors for the source side
and the target side, and computing their cosine
similarity.
</bodyText>
<subsectionHeader confidence="0.773484">
2.6 Error Grammar Parsing
</subsectionHeader>
<bodyText confidence="0.7676815">
We obtain features from monolingual parsing with
three grammars:
</bodyText>
<listItem confidence="0.981871777777778">
1. the vanilla grammar shipped with the Blipp
parser (Charniak, 2000; Charniak and John-
son, 2005) induced from the Penn-Treebank
(Marcus et al., 1994),
2. an error grammar induced from Penn-Tree-
bank trees distorted according to an error
model (Foster, 2007), and
3. a grammar induced from the union of the
above two treebanks.
</listItem>
<bodyText confidence="0.9954129">
Features include the log-ratios between the prob-
ability of the best parse obtained with each gram-
mar and structural differences measured with Par-
seval (Black et al., 1991) and leaf-ancestor (Samp-
son and Babarczy, 2003) metrics. These features
have been shown to be useful for judging the
grammaticality of sentences (Wagner et al., 2009;
Wagner, 2012) and have been used in MT quality
estimation before (Rubino et al., 2012; Rubino et
al., 2013).
</bodyText>
<sectionHeader confidence="0.9994" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9990055">
This section describes how we set up our experi-
ments.
</bodyText>
<subsectionHeader confidence="0.998111">
3.1 Cross-Validation
</subsectionHeader>
<bodyText confidence="0.999800083333333">
Decisions about parameters are made in 10-fold
cross-validation on the training data provided for
the task. As the datasets for task 1.1 include
three to four translations for each source segment,
we group segments by their source side and split
the data for cross-validation between segments to
ensure that a source segment does not occur in
both training and test data for any of the cross-
validation runs.
We implement these modifications to cross-
validation and randomisation in the QuEst frame-
work.
</bodyText>
<subsectionHeader confidence="0.997139">
3.2 Training
</subsectionHeader>
<bodyText confidence="0.999995125">
We use the QuEst framework to train our models.
Support vector regression (SVR) meta-parameters
are optimised using QuEst’s default settings, ex-
ploring RBF kernels with two possible values for
each of the three meta-parameters C, γ and E.5
The two final models are trained on the
full training set with the meta-parameters that
achieved the best average cross-validation score.
</bodyText>
<subsectionHeader confidence="0.991208">
3.3 Classifier Combination
</subsectionHeader>
<bodyText confidence="0.99998545">
We experiment with combining logistic regression
(LR) and support vector regression (SVR) by first
choosing the instances where LR classification is
confident and using the LR class label (1, 2, or
3) as predicted perceived post-editing effort, and
falling back to SVR for all other instances.
We employ several heuristics to decide whether
to use the output of LR or SVR. As the LR classi-
fier learns a decision function for each of the three
classes, we can exploit the scores of the classes to
measure the confidence of the LR classifier about
its decision. If the LR classifier is confident, we
use its prediction directly, otherwise we use the
SVR prediction.
For the cases where one of the three decision
functions for the LR classifier is positive, we select
the prediction directly, falling back to SVR when
the classifier is not confident about any of the three
classes. We implement the LR+SVR classifier
combination inside the QuEst framework.
</bodyText>
<sectionHeader confidence="0.999976" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9988538">
Table 1 shows cross-validation results for the 17
baseline features, the combination of all features
and target-side features only. We do not show
combinations of individual feature sets and base-
line features that do not improve over the base-
</bodyText>
<footnote confidence="0.9606725">
5We only discovered this limitation of the default config-
uration after the system submission, see Sections 4 and 5.
</footnote>
<page confidence="0.993573">
331
</page>
<table confidence="0.999513">
Features Classifier RMSE MAE
Basel.17 LR+SVR 0.75 0.62
ALL LR+SVR 0.74 0.59
ALL LR&gt; 0.5+SVR 0.75 0.58
Target LR+SVR 0.75 0.59
ALL LR&gt; 0.5+SVR-r 0.78 0.55
</table>
<tableCaption confidence="0.998196">
Table 1: Cross-validation results for English to
</tableCaption>
<bodyText confidence="0.984980875">
German. LR &gt; 0.5 indicates that we require the
LR decision function to be &gt; 0.5. SVR-r rounds
the output to the nearest natural number.
line. Several experiments, including those with the
semantic similarity feature sets, are thus omitted.
Furthermore, we only exemplify one language pair
(English to German), as the other language pairs
show similar patterns. The feature set target con-
tains the subset of the QuEst black-box features
(Section 2.1) which only examine the target side.
Our best results for English to German in the
cross-validation experiments are achieved by com-
bining a logistic regression (LR) classifier with
support vector regression (SVR). Furthermore,
performance on the cross-validation is slightly im-
proved for the mean absolute error (MAE) by
rounding SVR scores to the nearest integer. For
the root-mean-square error (RMSE), rounding has
the opposite effect.
Performing a more fine-grained grid search for
the meta-parameters C, γ and c after system sub-
mission, we were able to match the scores for
the baseline features published on the shared task
website.
</bodyText>
<subsectionHeader confidence="0.95818">
4.1 Parameters for the Final Models
</subsectionHeader>
<bodyText confidence="0.999902375">
The final two models for system submission are
trained on the full data set. We submit our best sys-
tem according to MAE in cross-validation com-
bining LR, SVR and rounding with all features
(ALL) as DCU-MIXED. For our second submis-
sion, we choose SVR on its own (system DCU-
SVR). For English-Spanish, we only submit DCU-
SVR.
</bodyText>
<sectionHeader confidence="0.998205" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999876642857143">
We identified improperly optimised parameters of
the SVR component as the cause, or at least as a
contributing factor, for the placement of our sys-
tems below the official baseline system. Other po-
tential factors may be an error in our experimen-
tal setup or over-fitting. Therefore, we plan to re-
peat the experiments with a more fine-grained grid
search for optimal parameters and/or will try an-
other machine learning toolkit.
Unfortunately, due to the above problems with
our system so far, we cannot draw conclusions
about the effectiveness of our novel feature sets.
A substantial gain is achieved on the MAE met-
ric with the rounding method, indicating that the
majority of prediction errors are below 0.5.6 Fu-
ture work should account for this effect. Two ideas
are: (a) round all predictions before evaluation
and (b) use more fine-grained gold values, e. g. the
(weighted) average over multiple annotations as in
the WMT 2012 quality estimation task (Callison-
Burch et al., 2012).
For the error grammar method, the next step
will be to adjust the error model to errors found in
translations. It may be possible to do this without a
time-consuming analysis of errors: Wagner (2012)
suggests to use parallel data of authentic errors and
corrections to build the error grammar, first pars-
ing the corrections and then guiding the error cre-
ation procedure with the edit operations inverse to
the corrections. Post-editing corpora can play this
role and have recently become available (Potet et
al., 2012).
Furthermore, future work should explore the
inverse glass-box feature idea with arbitrary tar-
get languages for the MT system. (There is no
requirement that the glass-box system translates
back to the original source language).
Finally, we would like to integrate referential
translation machines (Bic¸ici, 2013; Bic¸ici and
Way, 2014) into our system as they performed well
in the WMT quality estimation tasks this and last
year.
</bodyText>
<sectionHeader confidence="0.986961" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.983715">
This research is supported by the European Com-
mission under the 7th Framework Programme,
specifically its Marie Curie Programme 317471,
and by the Science Foundation Ireland (Grant
12/CE/I2267) as part of CNGL (www.cngl.ie) at
Dublin City University. We thank the anonymous
reviewers and Jennifer Foster for their comments
on earlier versions of this paper.
</bodyText>
<footnote confidence="0.9777455">
6The simultaneous increase on RMSE can be explained if
there is a sufficient number of errors above 0.5: After squar-
ing, these errors are still quite small, e. g. 0.36 for an error of
0.6, but after rounding, the square error becomes 1.0 or 4.0.
</footnote>
<page confidence="0.994689">
332
</page>
<sectionHeader confidence="0.984116" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996420612612612">
Joshua Albrecht and Rebecca Hwa. 2008. The role
of pseudo references in MT evaluation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 187–190, Columbus, Ohio, June.
Association for Computational Linguistics.
Ergun Bic¸ici and Andy Way. 2014. Referential trans-
lation machines for predicting translation quality. In
Proceedings of the Nineth Workshop on Statistical
Machine Translation, Baltimore, USA, June. Asso-
ciation for Computational Linguistics.
Ergun Bic¸ici. 2013. Referential translation machines
for quality estimation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation, pages
343–351, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Ezra Black, Steve Abney, Dan Flickinger, Claudia
Gdaniec, Robert Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Fred Jelinek, Judith Klavans,
Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991.
A procedure for quantitatively comparing the syn-
tactic coverage of English grammars. In E. Black,
editor, Proceedings of the HLT Workshop on Speech
and Natural Language, pages 306–311, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1–44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10–51, Montr´eal, Canada, June. Association for
Computational Linguistics.
Victor Chahuneau, Noah A. Smith, and Chris Dyer.
2012. pycdec: A python interface to cdec. Prague
Bull. Math. Linguistics, 98:51–62.
Eugene Charniak and Mark Johnson. 2005. Course-
to-fine n-best-parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the ACL (ACL-05), pages 173–180, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of the First Annual Meeting
of the North American Chapter of the Association
for Computational Linguistics (NAACL-00), pages
132–139, Seattle, WA.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh’s machine trans-
lation systems for European language pairs. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 114–121, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Jennifer Foster. 2007. Treebanks gone bad: Parser
evaluation and retraining using a treebank of un-
grammatical sentences. International Journal on
Document Analysis and Recognition, 10(3-4):129–
145.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5, pages 79–86.
Liangyou Li, Xiaofeng Wu, Santiago Cort´es Va´ıllo, Jun
Xie, Jia Xu, Andy Way, and Qun Liu. 2014. The
DCU-ICTCAS-Tsinghua MT system at WMT 2014
on German-English translation task. In Proceed-
ings of the Nineth Workshop on Statistical Machine
Translation, Baltimore, USA, June. Association for
Computational Linguistics.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings of
the 1994 ARPA Speech and Natural Language
Workshop, pages 114–119.
Michael McCandless, Erik Hatcher, and Otis Gospod-
netic. 2010. Lucene in Action, Second Edition:
Covers Apache Lucene 3.0. Manning Publications
Co., Greenwich, CT, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics (ACL02), pages 311–318, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Marion Potet, Emmanuelle Esperanc¸a-Rodier, Laurent
Besacier, and Herv´e Blanchon. 2012. Collection
of a large database of French-English SMT output
corrections. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC).
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and
Fred Hollowood. 2012. Dcu-symantec submis-
sion for the wmt 2012 quality estimation task. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 138–144, Montr´eal,
Canada, June. Association for Computational Lin-
guistics.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
</reference>
<page confidence="0.991398">
333
</page>
<reference confidence="0.998267425531915">
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392–397, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Magnus Sahlgren. 2005. An introduction to random
indexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International Con-
ference on Terminology and Knowledge Engineering
(TKE), volume 5, Copenhagen, Denmark.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(4):365–380.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, Manchester, United Kingdom.
Kashif Shah, Eleftherios Avramidis, Ergun Bic¸ici, and
Lucia Specia. 2013. QuEst - design, implemen-
tation and extensions of a framework for machine
translation quality estimation. The Prague Bulletin
of Mathematical Linguistics, 100.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, pages 223–231.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612–621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Lucia Specia, Kashif Shah, Jose G.C. de Souza, and
Trevor Cohn. 2013. QuEst - a translation quality es-
timation framework. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics: System Demonstrations, pages 79–84,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2009. Judging grammaticality: Experiments
in sentence classification. CALICO Journal (Special
Issue of the 2008 CALICO Workshop on Automatic
Analysis of Learner Language), 26(3):474–490.
Joachim Wagner. 2012. Detecting grammatical errors
with treebank-induced, probabilistic parsers. Ph.D.
thesis, Dublin City University, Dublin, Ireland.
</reference>
<page confidence="0.999071">
334
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.537436">
<title confidence="0.999864">Target-Centric Features for Translation Quality Estimation</title>
<author confidence="0.990742">Hokamp Calixto Wagner</author>
<affiliation confidence="0.89231125">CNGL Centre for Global Intelligent Dublin City School of Dublin,</affiliation>
<abstract confidence="0.993529555555556">We describe the DCU-MIXED and DCU- SVR submissions to the WMT-14 Quality Estimation task 1.1, predicting sentencelevel perceived post-editing effort. Feature design focuses on target-side features as we hypothesise that the source side has little effect on the quality of human translations, which are included in task 1.1 of this year’s WMT Quality Estimation shared task. We experiment with features of the QuEst framework, features of our past work, and three novel feature sets. Despite these efforts, our two systems perform poorly in the competition. Follow up experiments indicate that the poor performance is due to improperly optimised parameters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joshua Albrecht</author>
<author>Rebecca Hwa</author>
</authors>
<title>The role of pseudo references in MT evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>187--190</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="5748" citStr="Albrecht and Hwa, 2008" startWordPosition="915" endWordPosition="918">obability and the perplexity of the target side are used as features. The development of these features was motivated by manual examination of the common error types in the training data. We noted that stop word errors (omission, mistranslation, mis-translation of idiom), are prevalent in all language pairs, indicating that features which focus on stop word usage could be useful for predicting the quality of machine translation. We implement POS and stop word language models inside the QuEst framework. 2.3 Source-Side Pseudo-Reference Features We extract source-side pseudo-reference features (Albrecht and Hwa, 2008; Soricut and Echihabi, 3We use the stop word lists from Apache Lucene (McCandless et al., 2010). 4The News Commentary corpus from WMT13 was used to build these models, same as for the black-box features (Section 2.1). 2010; Rubino et al., 2012), for English to German quality prediction using a highly-tuned German to English translation system (Li et al., 2014) working in the reverse direction. The MT system translates the German target side, the quality of which is to be predicted, back into English, and we extract pseudo-reference features on the source side: • BLEU score (Papineni et al., 2</context>
</contexts>
<marker>Albrecht, Hwa, 2008</marker>
<rawString>Joshua Albrecht and Rebecca Hwa. 2008. The role of pseudo references in MT evaluation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 187–190, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Andy Way</author>
</authors>
<title>Referential translation machines for predicting translation quality.</title>
<date>2014</date>
<booktitle>In Proceedings of the Nineth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA,</location>
<marker>Bic¸ici, Way, 2014</marker>
<rawString>Ergun Bic¸ici and Andy Way. 2014. Referential translation machines for predicting translation quality. In Proceedings of the Nineth Workshop on Statistical Machine Translation, Baltimore, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
</authors>
<title>Referential translation machines for quality estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>343--351</pages>
<location>Sofia, Bulgaria,</location>
<marker>Bic¸ici, 2013</marker>
<rawString>Ergun Bic¸ici. 2013. Referential translation machines for quality estimation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 343–351, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>Proceedings of the HLT Workshop on Speech and Natural Language,</booktitle>
<pages>306--311</pages>
<editor>Ezra Black, Steve Abney, Dan Flickinger, Claudia Gdaniec, Robert Grishman, Philip Harrison, Donald Hindle, Robert Ingria, Fred Jelinek, Judith Klavans, Mark Liberman, Mitchell Marcus, Salim Roukos, Beatrice Santorini, and Tomek</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Morristown, NJ, USA.</location>
<marker>Strzalkowski, 1991</marker>
<rawString>Ezra Black, Steve Abney, Dan Flickinger, Claudia Gdaniec, Robert Grishman, Philip Harrison, Donald Hindle, Robert Ingria, Fred Jelinek, Judith Klavans, Mark Liberman, Mitchell Marcus, Salim Roukos, Beatrice Santorini, and Tomek Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. In E. Black, editor, Proceedings of the HLT Workshop on Speech and Natural Language, pages 306–311, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1176" citStr="Bojar et al., 2013" startWordPosition="169" endWordPosition="173">uman translations, which are included in task 1.1 of this year’s WMT Quality Estimation shared task. We experiment with features of the QuEst framework, features of our past work, and three novel feature sets. Despite these efforts, our two systems perform poorly in the competition. Follow up experiments indicate that the poor performance is due to improperly optimised parameters. 1 Introduction Translation quality estimation tries to predict the quality of a translation given the source and target text but no reference translations. Different from previous years (Callison-Burch et al., 2012; Bojar et al., 2013), the WMT 2014 Quality Estimation shared task is MT system-independent, i. e. no glass-box features are available and translations in the training and test sets are produced by different MT systems and also by human translators. This paper describes the CNGL@DCU team submission to task 1.1 of the WMT 2014 Quality Estimation shared task.1 The task is to predict the perceived post-editing effort given a source sentence and its raw translation. Due to the inclusion of human translation in the task, we focus our efforts on target-side features as we expect that the quality of a translation produce</context>
<context position="4376" citStr="Bojar et al., 2013" startWordPosition="693" endWordPosition="696"> 2.1 QuEst Black-Box Features and Baseline Features We use the QuEst framework to extract 47 basic black-box features from both source and target side, such as the ratio of the number of tokens, punctuation statistics, number if mismatched brackets and quotes, language model perplexity, n-gram frequency quartile statistics (n = 1, 2, 3), and coarse-grained POS frequency ratios. 17 of the 47 features are identical to the baseline features from the shared task website, i.e. 30 features are new. To train the language models and to extract frequency information, we use the News Commentary corpus (Bojar et al., 2013). 2.2 POS and Stop Word Language Model Features For all languages, we extract probability and perplexity features from language models trained on POS tagged corpora. POS tagging is performed using the IMS Tree Tagger (Schmid, 1994). We also experiment with language models built from a combination of stop words3 and POS tags. Starting with a tokenised corpus, and its POStagged counterpart, we create a new representation of the corpus by replacing POS tags for stop words with the literal stop word that occurred in the original corpus, leaving non-stop word tags intact.4 The intuition behind the </context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1155" citStr="Callison-Burch et al., 2012" startWordPosition="165" endWordPosition="168">le effect on the quality of human translations, which are included in task 1.1 of this year’s WMT Quality Estimation shared task. We experiment with features of the QuEst framework, features of our past work, and three novel feature sets. Despite these efforts, our two systems perform poorly in the competition. Follow up experiments indicate that the poor performance is due to improperly optimised parameters. 1 Introduction Translation quality estimation tries to predict the quality of a translation given the source and target text but no reference translations. Different from previous years (Callison-Burch et al., 2012; Bojar et al., 2013), the WMT 2014 Quality Estimation shared task is MT system-independent, i. e. no glass-box features are available and translations in the training and test sets are produced by different MT systems and also by human translators. This paper describes the CNGL@DCU team submission to task 1.1 of the WMT 2014 Quality Estimation shared task.1 The task is to predict the perceived post-editing effort given a source sentence and its raw translation. Due to the inclusion of human translation in the task, we focus our efforts on target-side features as we expect that the quality of </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
<author>Chris Dyer</author>
</authors>
<title>pycdec: A python interface to cdec.</title>
<date>2012</date>
<journal>Prague Bull. Math. Linguistics,</journal>
<pages>98--51</pages>
<contexts>
<context position="6795" citStr="Chahuneau et al., 2012" startWordPosition="1096" endWordPosition="1099">he German target side, the quality of which is to be predicted, back into English, and we extract pseudo-reference features on the source side: • BLEU score (Papineni et al., 2002) between back-translation and original source sentence, and • TER score (Snover et al., 2006). For the 5th English to German test set item, for example, the translation (1) Und belasse sie dort eine Woche. is translated back to English as (2) and leave it therefor a week. and compared to the original source sentence (3) Leave for a week. producing a BLEU score of 0.077 using the Python interface to the cdec toolkit (Chahuneau et al., 2012). 2.4 Inverse Glass-Box Features for Translating the Translation In the absence of direct glass-box features, we obtain glass-box features from translating the raw translation back to the source language using the same MT system that we use for the source-side pseudo-reference features. We extract features from the following components of the Moses decoder: distortion model, language model, lexical reordering, lexical translation probability, operational sequence model (Durrani et al., 2013), phrase translation probability, and the decoder score. The intuition for this set of features is that </context>
</contexts>
<marker>Chahuneau, Smith, Dyer, 2012</marker>
<rawString>Victor Chahuneau, Noah A. Smith, and Chris Dyer. 2012. pycdec: A python interface to cdec. Prague Bull. Math. Linguistics, 98:51–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Courseto-fine n-best-parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL (ACL-05),</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="8727" citStr="Charniak and Johnson, 2005" startWordPosition="1404" endWordPosition="1408">tion of the Europarl corpus (Koehn, 2005), we initialize a sparse random vector. We then create token vectors for each source and target token by summing the vectors for all of the segments where the token occurs. To extract the similarity feature for new source and target pairs, we map them into the vector space by taking the centroid of the token vectors for the source side and the target side, and computing their cosine similarity. 2.6 Error Grammar Parsing We obtain features from monolingual parsing with three grammars: 1. the vanilla grammar shipped with the Blipp parser (Charniak, 2000; Charniak and Johnson, 2005) induced from the Penn-Treebank (Marcus et al., 1994), 2. an error grammar induced from Penn-Treebank trees distorted according to an error model (Foster, 2007), and 3. a grammar induced from the union of the above two treebanks. Features include the log-ratios between the probability of the best parse obtained with each grammar and structural differences measured with Parseval (Black et al., 1991) and leaf-ancestor (Sampson and Babarczy, 2003) metrics. These features have been shown to be useful for judging the grammaticality of sentences (Wagner et al., 2009; Wagner, 2012) and have been used</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Courseto-fine n-best-parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the ACL (ACL-05), pages 173–180, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum entropy inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-00),</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="8698" citStr="Charniak, 2000" startWordPosition="1402" endWordPosition="1403">glishSpanish portion of the Europarl corpus (Koehn, 2005), we initialize a sparse random vector. We then create token vectors for each source and target token by summing the vectors for all of the segments where the token occurs. To extract the similarity feature for new source and target pairs, we map them into the vector space by taking the centroid of the token vectors for the source side and the target side, and computing their cosine similarity. 2.6 Error Grammar Parsing We obtain features from monolingual parsing with three grammars: 1. the vanilla grammar shipped with the Blipp parser (Charniak, 2000; Charniak and Johnson, 2005) induced from the Penn-Treebank (Marcus et al., 1994), 2. an error grammar induced from Penn-Treebank trees distorted according to an error model (Foster, 2007), and 3. a grammar induced from the union of the above two treebanks. Features include the log-ratios between the probability of the best parse obtained with each grammar and structural differences measured with Parseval (Black et al., 1991) and leaf-ancestor (Sampson and Babarczy, 2003) metrics. These features have been shown to be useful for judging the grammaticality of sentences (Wagner et al., 2009; Wag</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum entropy inspired parser. In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-00), pages 132–139, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Barry Haddow</author>
<author>Kenneth Heafield</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburgh’s machine translation systems for European language pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>114--121</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="7291" citStr="Durrani et al., 2013" startWordPosition="1169" endWordPosition="1172">(3) Leave for a week. producing a BLEU score of 0.077 using the Python interface to the cdec toolkit (Chahuneau et al., 2012). 2.4 Inverse Glass-Box Features for Translating the Translation In the absence of direct glass-box features, we obtain glass-box features from translating the raw translation back to the source language using the same MT system that we use for the source-side pseudo-reference features. We extract features from the following components of the Moses decoder: distortion model, language model, lexical reordering, lexical translation probability, operational sequence model (Durrani et al., 2013), phrase translation probability, and the decoder score. The intuition for this set of features is that backtranslating an incorrect translation will give low system-internal scores, e. g. a low phrase translation score, and produce poor output with low language model scores (garbage in, garbage out). We are not aware of any previous work using inverse glass-box features of translating the target side to another language for quality estimation. 330 2.5 Semantic Similarity Using Random Indexing These features try to measure the semantic similarity of source and target side of a translation unit</context>
</contexts>
<marker>Durrani, Haddow, Heafield, Koehn, 2013</marker>
<rawString>Nadir Durrani, Barry Haddow, Kenneth Heafield, and Philipp Koehn. 2013. Edinburgh’s machine translation systems for European language pairs. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 114–121, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
</authors>
<title>Treebanks gone bad: Parser evaluation and retraining using a treebank of ungrammatical sentences.</title>
<date>2007</date>
<journal>International Journal on Document Analysis and Recognition,</journal>
<pages>10--3</pages>
<contexts>
<context position="8887" citStr="Foster, 2007" startWordPosition="1432" endWordPosition="1433">of the segments where the token occurs. To extract the similarity feature for new source and target pairs, we map them into the vector space by taking the centroid of the token vectors for the source side and the target side, and computing their cosine similarity. 2.6 Error Grammar Parsing We obtain features from monolingual parsing with three grammars: 1. the vanilla grammar shipped with the Blipp parser (Charniak, 2000; Charniak and Johnson, 2005) induced from the Penn-Treebank (Marcus et al., 1994), 2. an error grammar induced from Penn-Treebank trees distorted according to an error model (Foster, 2007), and 3. a grammar induced from the union of the above two treebanks. Features include the log-ratios between the probability of the best parse obtained with each grammar and structural differences measured with Parseval (Black et al., 1991) and leaf-ancestor (Sampson and Babarczy, 2003) metrics. These features have been shown to be useful for judging the grammaticality of sentences (Wagner et al., 2009; Wagner, 2012) and have been used in MT quality estimation before (Rubino et al., 2012; Rubino et al., 2013). 3 Experimental Setup This section describes how we set up our experiments. 3.1 Cros</context>
</contexts>
<marker>Foster, 2007</marker>
<rawString>Jennifer Foster. 2007. Treebanks gone bad: Parser evaluation and retraining using a treebank of ungrammatical sentences. International Journal on Document Analysis and Recognition, 10(3-4):129– 145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT summit,</booktitle>
<volume>5</volume>
<pages>79--86</pages>
<contexts>
<context position="8141" citStr="Koehn, 2005" startWordPosition="1308" endWordPosition="1309">output with low language model scores (garbage in, garbage out). We are not aware of any previous work using inverse glass-box features of translating the target side to another language for quality estimation. 330 2.5 Semantic Similarity Using Random Indexing These features try to measure the semantic similarity of source and target side of a translation unit for quality estimation using random indexing (Sahlgren, 2005). We experiment with adding the similarity score of the source and target random vectors. For each source and target pair in the EnglishSpanish portion of the Europarl corpus (Koehn, 2005), we initialize a sparse random vector. We then create token vectors for each source and target token by summing the vectors for all of the segments where the token occurs. To extract the similarity feature for new source and target pairs, we map them into the vector space by taking the centroid of the token vectors for the source side and the target side, and computing their cosine similarity. 2.6 Error Grammar Parsing We obtain features from monolingual parsing with three grammars: 1. the vanilla grammar shipped with the Blipp parser (Charniak, 2000; Charniak and Johnson, 2005) induced from </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit, volume 5, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liangyou Li</author>
<author>Xiaofeng Wu</author>
<author>Santiago Cort´es Va´ıllo</author>
<author>Jun Xie</author>
<author>Jia Xu</author>
<author>Andy Way</author>
<author>Qun Liu</author>
</authors>
<title>The DCU-ICTCAS-Tsinghua MT system at WMT</title>
<date>2014</date>
<booktitle>In Proceedings of the Nineth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA,</location>
<marker>Li, Wu, Va´ıllo, Xie, Xu, Way, Liu, 2014</marker>
<rawString>Liangyou Li, Xiaofeng Wu, Santiago Cort´es Va´ıllo, Jun Xie, Jia Xu, Andy Way, and Qun Liu. 2014. The DCU-ICTCAS-Tsinghua MT system at WMT 2014 on German-English translation task. In Proceedings of the Nineth Workshop on Statistical Machine Translation, Baltimore, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proceedings of the 1994 ARPA Speech and Natural Language Workshop,</booktitle>
<pages>114--119</pages>
<contexts>
<context position="8780" citStr="Marcus et al., 1994" startWordPosition="1413" endWordPosition="1416">parse random vector. We then create token vectors for each source and target token by summing the vectors for all of the segments where the token occurs. To extract the similarity feature for new source and target pairs, we map them into the vector space by taking the centroid of the token vectors for the source side and the target side, and computing their cosine similarity. 2.6 Error Grammar Parsing We obtain features from monolingual parsing with three grammars: 1. the vanilla grammar shipped with the Blipp parser (Charniak, 2000; Charniak and Johnson, 2005) induced from the Penn-Treebank (Marcus et al., 1994), 2. an error grammar induced from Penn-Treebank trees distorted according to an error model (Foster, 2007), and 3. a grammar induced from the union of the above two treebanks. Features include the log-ratios between the probability of the best parse obtained with each grammar and structural differences measured with Parseval (Black et al., 1991) and leaf-ancestor (Sampson and Babarczy, 2003) metrics. These features have been shown to be useful for judging the grammaticality of sentences (Wagner et al., 2009; Wagner, 2012) and have been used in MT quality estimation before (Rubino et al., 2012</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of the 1994 ARPA Speech and Natural Language Workshop, pages 114–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael McCandless</author>
<author>Erik Hatcher</author>
<author>Otis Gospodnetic</author>
</authors>
<date>2010</date>
<booktitle>Lucene in Action, Second Edition: Covers Apache Lucene 3.0. Manning Publications Co.,</booktitle>
<location>Greenwich, CT, USA.</location>
<contexts>
<context position="5844" citStr="McCandless et al., 2010" startWordPosition="931" endWordPosition="935"> features was motivated by manual examination of the common error types in the training data. We noted that stop word errors (omission, mistranslation, mis-translation of idiom), are prevalent in all language pairs, indicating that features which focus on stop word usage could be useful for predicting the quality of machine translation. We implement POS and stop word language models inside the QuEst framework. 2.3 Source-Side Pseudo-Reference Features We extract source-side pseudo-reference features (Albrecht and Hwa, 2008; Soricut and Echihabi, 3We use the stop word lists from Apache Lucene (McCandless et al., 2010). 4The News Commentary corpus from WMT13 was used to build these models, same as for the black-box features (Section 2.1). 2010; Rubino et al., 2012), for English to German quality prediction using a highly-tuned German to English translation system (Li et al., 2014) working in the reverse direction. The MT system translates the German target side, the quality of which is to be predicted, back into English, and we extract pseudo-reference features on the source side: • BLEU score (Papineni et al., 2002) between back-translation and original source sentence, and • TER score (Snover et al., 2006</context>
</contexts>
<marker>McCandless, Hatcher, Gospodnetic, 2010</marker>
<rawString>Michael McCandless, Erik Hatcher, and Otis Gospodnetic. 2010. Lucene in Action, Second Edition: Covers Apache Lucene 3.0. Manning Publications Co., Greenwich, CT, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL02),</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6352" citStr="Papineni et al., 2002" startWordPosition="1018" endWordPosition="1021">echt and Hwa, 2008; Soricut and Echihabi, 3We use the stop word lists from Apache Lucene (McCandless et al., 2010). 4The News Commentary corpus from WMT13 was used to build these models, same as for the black-box features (Section 2.1). 2010; Rubino et al., 2012), for English to German quality prediction using a highly-tuned German to English translation system (Li et al., 2014) working in the reverse direction. The MT system translates the German target side, the quality of which is to be predicted, back into English, and we extract pseudo-reference features on the source side: • BLEU score (Papineni et al., 2002) between back-translation and original source sentence, and • TER score (Snover et al., 2006). For the 5th English to German test set item, for example, the translation (1) Und belasse sie dort eine Woche. is translated back to English as (2) and leave it therefor a week. and compared to the original source sentence (3) Leave for a week. producing a BLEU score of 0.077 using the Python interface to the cdec toolkit (Chahuneau et al., 2012). 2.4 Inverse Glass-Box Features for Translating the Translation In the absence of direct glass-box features, we obtain glass-box features from translating t</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL02), pages 311–318, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marion Potet</author>
<author>Emmanuelle Esperanc¸a-Rodier</author>
<author>Laurent Besacier</author>
<author>Herv´e Blanchon</author>
</authors>
<title>Collection of a large database of French-English SMT output corrections.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC).</booktitle>
<marker>Potet, Esperanc¸a-Rodier, Besacier, Blanchon, 2012</marker>
<rawString>Marion Potet, Emmanuelle Esperanc¸a-Rodier, Laurent Besacier, and Herv´e Blanchon. 2012. Collection of a large database of French-English SMT output corrections. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Rubino</author>
<author>Jennifer Foster</author>
<author>Joachim Wagner</author>
<author>Johann Roturier</author>
<author>Rasul Samad Zadeh Kaljahi</author>
<author>Fred Hollowood</author>
</authors>
<title>Dcu-symantec submission for the wmt 2012 quality estimation task.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>138--144</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="2886" citStr="Rubino et al., 2012" startWordPosition="451" endWordPosition="454">y both the feature extraction framework and the machine learning components to add functionality to QuEst. The novel features we add to our systems are (a) a language model on a combination of stop words and POS tags, (b) inverse glass-box features for translating the translation, and (c) random indexing (Sahlgren, 2005) for measuring the semantic similarity of source and target side across languages. Furthermore, we integrated (d) sourceside pseudo-reference features (Soricut and Echihabi, 2010) and (e) error grammar features (Wagner, 2012), which were used first in MT quality estimation by (Rubino et al., 2012; Rubino et al., 2013). The remaining sections are organised as follows. Section 2 gives details on the features we use. Section 3 describes how we set up our experiments. Results are presented in Section 4 and conclusions are drawn in Section 5 together with pointers to future work. 2 Features This section describes the features we extract from source and target sentences in order to train prediction models and to make predictions in addition to the baseline features provided for the task. We focus on the target side as we assume that the quality of the source side has little predictive power</context>
<context position="5993" citStr="Rubino et al., 2012" startWordPosition="957" endWordPosition="960">is-translation of idiom), are prevalent in all language pairs, indicating that features which focus on stop word usage could be useful for predicting the quality of machine translation. We implement POS and stop word language models inside the QuEst framework. 2.3 Source-Side Pseudo-Reference Features We extract source-side pseudo-reference features (Albrecht and Hwa, 2008; Soricut and Echihabi, 3We use the stop word lists from Apache Lucene (McCandless et al., 2010). 4The News Commentary corpus from WMT13 was used to build these models, same as for the black-box features (Section 2.1). 2010; Rubino et al., 2012), for English to German quality prediction using a highly-tuned German to English translation system (Li et al., 2014) working in the reverse direction. The MT system translates the German target side, the quality of which is to be predicted, back into English, and we extract pseudo-reference features on the source side: • BLEU score (Papineni et al., 2002) between back-translation and original source sentence, and • TER score (Snover et al., 2006). For the 5th English to German test set item, for example, the translation (1) Und belasse sie dort eine Woche. is translated back to English as (2</context>
<context position="9380" citStr="Rubino et al., 2012" startWordPosition="1512" endWordPosition="1515">arcus et al., 1994), 2. an error grammar induced from Penn-Treebank trees distorted according to an error model (Foster, 2007), and 3. a grammar induced from the union of the above two treebanks. Features include the log-ratios between the probability of the best parse obtained with each grammar and structural differences measured with Parseval (Black et al., 1991) and leaf-ancestor (Sampson and Babarczy, 2003) metrics. These features have been shown to be useful for judging the grammaticality of sentences (Wagner et al., 2009; Wagner, 2012) and have been used in MT quality estimation before (Rubino et al., 2012; Rubino et al., 2013). 3 Experimental Setup This section describes how we set up our experiments. 3.1 Cross-Validation Decisions about parameters are made in 10-fold cross-validation on the training data provided for the task. As the datasets for task 1.1 include three to four translations for each source segment, we group segments by their source side and split the data for cross-validation between segments to ensure that a source segment does not occur in both training and test data for any of the crossvalidation runs. We implement these modifications to crossvalidation and randomisation in</context>
</contexts>
<marker>Rubino, Foster, Wagner, Roturier, Kaljahi, Hollowood, 2012</marker>
<rawString>Raphael Rubino, Jennifer Foster, Joachim Wagner, Johann Roturier, Rasul Samad Zadeh Kaljahi, and Fred Hollowood. 2012. Dcu-symantec submission for the wmt 2012 quality estimation task. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 138–144, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Rubino</author>
<author>Joachim Wagner</author>
<author>Jennifer Foster</author>
<author>Johann Roturier</author>
<author>Rasoul Samad Zadeh Kaljahi</author>
<author>Fred Hollowood</author>
</authors>
<title>DCU-Symantec at the WMT 2013 quality estimation shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>392--397</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2908" citStr="Rubino et al., 2013" startWordPosition="455" endWordPosition="458">traction framework and the machine learning components to add functionality to QuEst. The novel features we add to our systems are (a) a language model on a combination of stop words and POS tags, (b) inverse glass-box features for translating the translation, and (c) random indexing (Sahlgren, 2005) for measuring the semantic similarity of source and target side across languages. Furthermore, we integrated (d) sourceside pseudo-reference features (Soricut and Echihabi, 2010) and (e) error grammar features (Wagner, 2012), which were used first in MT quality estimation by (Rubino et al., 2012; Rubino et al., 2013). The remaining sections are organised as follows. Section 2 gives details on the features we use. Section 3 describes how we set up our experiments. Results are presented in Section 4 and conclusions are drawn in Section 5 together with pointers to future work. 2 Features This section describes the features we extract from source and target sentences in order to train prediction models and to make predictions in addition to the baseline features provided for the task. We focus on the target side as we assume that the quality of the source side has little predictive power for human translation</context>
<context position="9402" citStr="Rubino et al., 2013" startWordPosition="1516" endWordPosition="1519">2. an error grammar induced from Penn-Treebank trees distorted according to an error model (Foster, 2007), and 3. a grammar induced from the union of the above two treebanks. Features include the log-ratios between the probability of the best parse obtained with each grammar and structural differences measured with Parseval (Black et al., 1991) and leaf-ancestor (Sampson and Babarczy, 2003) metrics. These features have been shown to be useful for judging the grammaticality of sentences (Wagner et al., 2009; Wagner, 2012) and have been used in MT quality estimation before (Rubino et al., 2012; Rubino et al., 2013). 3 Experimental Setup This section describes how we set up our experiments. 3.1 Cross-Validation Decisions about parameters are made in 10-fold cross-validation on the training data provided for the task. As the datasets for task 1.1 include three to four translations for each source segment, we group segments by their source side and split the data for cross-validation between segments to ensure that a source segment does not occur in both training and test data for any of the crossvalidation runs. We implement these modifications to crossvalidation and randomisation in the QuEst framework. </context>
</contexts>
<marker>Rubino, Wagner, Foster, Roturier, Kaljahi, Hollowood, 2013</marker>
<rawString>Raphael Rubino, Joachim Wagner, Jennifer Foster, Johann Roturier, Rasoul Samad Zadeh Kaljahi, and Fred Hollowood. 2013. DCU-Symantec at the WMT 2013 quality estimation shared task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 392–397, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>An introduction to random indexing.</title>
<date>2005</date>
<booktitle>In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering (TKE),</booktitle>
<volume>5</volume>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="2589" citStr="Sahlgren, 2005" startWordPosition="407" endWordPosition="408">ctors such as time pressure and familiarity with the domain. To build our quality estimation system, we use and extend the QuEst framework for translation quality estimation2 (Shah et al., 2013; Specia et al., 2013). QuEst provides modules for feature extraction and machine learning. We modify both the feature extraction framework and the machine learning components to add functionality to QuEst. The novel features we add to our systems are (a) a language model on a combination of stop words and POS tags, (b) inverse glass-box features for translating the translation, and (c) random indexing (Sahlgren, 2005) for measuring the semantic similarity of source and target side across languages. Furthermore, we integrated (d) sourceside pseudo-reference features (Soricut and Echihabi, 2010) and (e) error grammar features (Wagner, 2012), which were used first in MT quality estimation by (Rubino et al., 2012; Rubino et al., 2013). The remaining sections are organised as follows. Section 2 gives details on the features we use. Section 3 describes how we set up our experiments. Results are presented in Section 4 and conclusions are drawn in Section 5 together with pointers to future work. 2 Features This se</context>
<context position="7953" citStr="Sahlgren, 2005" startWordPosition="1276" endWordPosition="1277">er score. The intuition for this set of features is that backtranslating an incorrect translation will give low system-internal scores, e. g. a low phrase translation score, and produce poor output with low language model scores (garbage in, garbage out). We are not aware of any previous work using inverse glass-box features of translating the target side to another language for quality estimation. 330 2.5 Semantic Similarity Using Random Indexing These features try to measure the semantic similarity of source and target side of a translation unit for quality estimation using random indexing (Sahlgren, 2005). We experiment with adding the similarity score of the source and target random vectors. For each source and target pair in the EnglishSpanish portion of the Europarl corpus (Koehn, 2005), we initialize a sparse random vector. We then create token vectors for each source and target token by summing the vectors for all of the segments where the token occurs. To extract the similarity feature for new source and target pairs, we map them into the vector space by taking the centroid of the token vectors for the source side and the target side, and computing their cosine similarity. 2.6 Error Gram</context>
</contexts>
<marker>Sahlgren, 2005</marker>
<rawString>Magnus Sahlgren. 2005. An introduction to random indexing. In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering (TKE), volume 5, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Sampson</author>
<author>Anna Babarczy</author>
</authors>
<title>A test of the leaf-ancestor metric for parse accuracy.</title>
<date>2003</date>
<journal>Natural Language Engineering,</journal>
<volume>9</volume>
<issue>4</issue>
<contexts>
<context position="9175" citStr="Sampson and Babarczy, 2003" startWordPosition="1477" endWordPosition="1481"> Grammar Parsing We obtain features from monolingual parsing with three grammars: 1. the vanilla grammar shipped with the Blipp parser (Charniak, 2000; Charniak and Johnson, 2005) induced from the Penn-Treebank (Marcus et al., 1994), 2. an error grammar induced from Penn-Treebank trees distorted according to an error model (Foster, 2007), and 3. a grammar induced from the union of the above two treebanks. Features include the log-ratios between the probability of the best parse obtained with each grammar and structural differences measured with Parseval (Black et al., 1991) and leaf-ancestor (Sampson and Babarczy, 2003) metrics. These features have been shown to be useful for judging the grammaticality of sentences (Wagner et al., 2009; Wagner, 2012) and have been used in MT quality estimation before (Rubino et al., 2012; Rubino et al., 2013). 3 Experimental Setup This section describes how we set up our experiments. 3.1 Cross-Validation Decisions about parameters are made in 10-fold cross-validation on the training data provided for the task. As the datasets for task 1.1 include three to four translations for each source segment, we group segments by their source side and split the data for cross-validation</context>
</contexts>
<marker>Sampson, Babarczy, 2003</marker>
<rawString>Geoffrey Sampson and Anna Babarczy. 2003. A test of the leaf-ancestor metric for parse accuracy. Natural Language Engineering, 9(4):365–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, United Kingdom.</location>
<contexts>
<context position="4607" citStr="Schmid, 1994" startWordPosition="732" endWordPosition="733">ed brackets and quotes, language model perplexity, n-gram frequency quartile statistics (n = 1, 2, 3), and coarse-grained POS frequency ratios. 17 of the 47 features are identical to the baseline features from the shared task website, i.e. 30 features are new. To train the language models and to extract frequency information, we use the News Commentary corpus (Bojar et al., 2013). 2.2 POS and Stop Word Language Model Features For all languages, we extract probability and perplexity features from language models trained on POS tagged corpora. POS tagging is performed using the IMS Tree Tagger (Schmid, 1994). We also experiment with language models built from a combination of stop words3 and POS tags. Starting with a tokenised corpus, and its POStagged counterpart, we create a new representation of the corpus by replacing POS tags for stop words with the literal stop word that occurred in the original corpus, leaving non-stop word tags intact.4 The intuition behind the approach is that the combined POS and stop word model should encode the distributional tendencies of the most common words in the language. The log-probability and the perplexity of the target side are used as features. The develop</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, Manchester, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashif Shah</author>
</authors>
<title>Eleftherios Avramidis, Ergun Bic¸ici, and Lucia Specia.</title>
<date>2013</date>
<pages>100</pages>
<marker>Shah, 2013</marker>
<rawString>Kashif Shah, Eleftherios Avramidis, Ergun Bic¸ici, and Lucia Specia. 2013. QuEst - design, implementation and extensions of a framework for machine translation quality estimation. The Prague Bulletin of Mathematical Linguistics, 100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of association for machine translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="6445" citStr="Snover et al., 2006" startWordPosition="1033" endWordPosition="1036">dless et al., 2010). 4The News Commentary corpus from WMT13 was used to build these models, same as for the black-box features (Section 2.1). 2010; Rubino et al., 2012), for English to German quality prediction using a highly-tuned German to English translation system (Li et al., 2014) working in the reverse direction. The MT system translates the German target side, the quality of which is to be predicted, back into English, and we extract pseudo-reference features on the source side: • BLEU score (Papineni et al., 2002) between back-translation and original source sentence, and • TER score (Snover et al., 2006). For the 5th English to German test set item, for example, the translation (1) Und belasse sie dort eine Woche. is translated back to English as (2) and leave it therefor a week. and compared to the original source sentence (3) Leave for a week. producing a BLEU score of 0.077 using the Python interface to the cdec toolkit (Chahuneau et al., 2012). 2.4 Inverse Glass-Box Features for Translating the Translation In the absence of direct glass-box features, we obtain glass-box features from translating the raw translation back to the source language using the same MT system that we use for the s</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of association for machine translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>Trustrank: Inducing trust in automatic translations via ranking.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>612--621</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2768" citStr="Soricut and Echihabi, 2010" startWordPosition="429" endWordPosition="433">mation2 (Shah et al., 2013; Specia et al., 2013). QuEst provides modules for feature extraction and machine learning. We modify both the feature extraction framework and the machine learning components to add functionality to QuEst. The novel features we add to our systems are (a) a language model on a combination of stop words and POS tags, (b) inverse glass-box features for translating the translation, and (c) random indexing (Sahlgren, 2005) for measuring the semantic similarity of source and target side across languages. Furthermore, we integrated (d) sourceside pseudo-reference features (Soricut and Echihabi, 2010) and (e) error grammar features (Wagner, 2012), which were used first in MT quality estimation by (Rubino et al., 2012; Rubino et al., 2013). The remaining sections are organised as follows. Section 2 gives details on the features we use. Section 3 describes how we set up our experiments. Results are presented in Section 4 and conclusions are drawn in Section 5 together with pointers to future work. 2 Features This section describes the features we extract from source and target sentences in order to train prediction models and to make predictions in addition to the baseline features provided </context>
</contexts>
<marker>Soricut, Echihabi, 2010</marker>
<rawString>Radu Soricut and Abdessamad Echihabi. 2010. Trustrank: Inducing trust in automatic translations via ranking. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 612–621, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Kashif Shah</author>
<author>Jose G C de Souza</author>
<author>Trevor Cohn</author>
</authors>
<title>QuEst - a translation quality estimation framework.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>79--84</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Specia, Shah, de Souza, Cohn, 2013</marker>
<rawString>Lucia Specia, Kashif Shah, Jose G.C. de Souza, and Trevor Cohn. 2013. QuEst - a translation quality estimation framework. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 79–84, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Wagner</author>
<author>Jennifer Foster</author>
<author>Josef van Genabith</author>
</authors>
<title>Judging grammaticality: Experiments in sentence classification.</title>
<date>2009</date>
<journal>CALICO Journal (Special Issue of the</journal>
<booktitle>CALICO Workshop on Automatic Analysis of Learner Language),</booktitle>
<pages>26--3</pages>
<marker>Wagner, Foster, van Genabith, 2009</marker>
<rawString>Joachim Wagner, Jennifer Foster, and Josef van Genabith. 2009. Judging grammaticality: Experiments in sentence classification. CALICO Journal (Special Issue of the 2008 CALICO Workshop on Automatic Analysis of Learner Language), 26(3):474–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Wagner</author>
</authors>
<title>Detecting grammatical errors with treebank-induced, probabilistic parsers.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Dublin City University,</institution>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2814" citStr="Wagner, 2012" startWordPosition="439" endWordPosition="441">ides modules for feature extraction and machine learning. We modify both the feature extraction framework and the machine learning components to add functionality to QuEst. The novel features we add to our systems are (a) a language model on a combination of stop words and POS tags, (b) inverse glass-box features for translating the translation, and (c) random indexing (Sahlgren, 2005) for measuring the semantic similarity of source and target side across languages. Furthermore, we integrated (d) sourceside pseudo-reference features (Soricut and Echihabi, 2010) and (e) error grammar features (Wagner, 2012), which were used first in MT quality estimation by (Rubino et al., 2012; Rubino et al., 2013). The remaining sections are organised as follows. Section 2 gives details on the features we use. Section 3 describes how we set up our experiments. Results are presented in Section 4 and conclusions are drawn in Section 5 together with pointers to future work. 2 Features This section describes the features we extract from source and target sentences in order to train prediction models and to make predictions in addition to the baseline features provided for the task. We focus on the target side as w</context>
<context position="9308" citStr="Wagner, 2012" startWordPosition="1501" endWordPosition="1502">000; Charniak and Johnson, 2005) induced from the Penn-Treebank (Marcus et al., 1994), 2. an error grammar induced from Penn-Treebank trees distorted according to an error model (Foster, 2007), and 3. a grammar induced from the union of the above two treebanks. Features include the log-ratios between the probability of the best parse obtained with each grammar and structural differences measured with Parseval (Black et al., 1991) and leaf-ancestor (Sampson and Babarczy, 2003) metrics. These features have been shown to be useful for judging the grammaticality of sentences (Wagner et al., 2009; Wagner, 2012) and have been used in MT quality estimation before (Rubino et al., 2012; Rubino et al., 2013). 3 Experimental Setup This section describes how we set up our experiments. 3.1 Cross-Validation Decisions about parameters are made in 10-fold cross-validation on the training data provided for the task. As the datasets for task 1.1 include three to four translations for each source segment, we group segments by their source side and split the data for cross-validation between segments to ensure that a source segment does not occur in both training and test data for any of the crossvalidation runs. </context>
<context position="14607" citStr="Wagner (2012)" startWordPosition="2370" endWordPosition="2371"> substantial gain is achieved on the MAE metric with the rounding method, indicating that the majority of prediction errors are below 0.5.6 Future work should account for this effect. Two ideas are: (a) round all predictions before evaluation and (b) use more fine-grained gold values, e. g. the (weighted) average over multiple annotations as in the WMT 2012 quality estimation task (CallisonBurch et al., 2012). For the error grammar method, the next step will be to adjust the error model to errors found in translations. It may be possible to do this without a time-consuming analysis of errors: Wagner (2012) suggests to use parallel data of authentic errors and corrections to build the error grammar, first parsing the corrections and then guiding the error creation procedure with the edit operations inverse to the corrections. Post-editing corpora can play this role and have recently become available (Potet et al., 2012). Furthermore, future work should explore the inverse glass-box feature idea with arbitrary target languages for the MT system. (There is no requirement that the glass-box system translates back to the original source language). Finally, we would like to integrate referential tran</context>
</contexts>
<marker>Wagner, 2012</marker>
<rawString>Joachim Wagner. 2012. Detecting grammatical errors with treebank-induced, probabilistic parsers. Ph.D. thesis, Dublin City University, Dublin, Ireland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
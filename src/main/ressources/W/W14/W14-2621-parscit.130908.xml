<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014459">
<title confidence="0.739471333333333">
Sentiment Classification on Polarity Reviews:
An Empirical Study Using Rating-based Features
Dai Quoc Nguyen* and Dat Quoc Nguyen* and Thanh Vu† and
</title>
<author confidence="0.966766">
Son Bao Pham*
</author>
<affiliation confidence="0.989798">
* Faculty of Information Technology
University of Engineering and Technology
Vietnam National University, Hanoi
</affiliation>
<email confidence="0.827986">
{dainq, datnq, sonpb}@vnu.edu.vn
</email>
<affiliation confidence="0.7356685">
† Computing and Communications Department
The Open University, Milton Keynes, UK
</affiliation>
<email confidence="0.979195">
thanh.vu@open.ac.uk
</email>
<sectionHeader confidence="0.993239" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999847444444445">
We present a new feature type named
rating-based feature and evaluate the
contribution of this feature to the task
of document-level sentiment analy-
sis. We achieve state-of-the-art re-
sults on two publicly available stan-
dard polarity movie datasets: on the
dataset consisting of 2000 reviews pro-
duced by Pang and Lee (2004) we ob-
tain an accuracy of 91.6% while it
is 89.87% evaluated on the dataset of
50000 reviews created by Maas et al.
(2011). We also get a performance
at 93.24% on our own dataset consist-
ing of 233600 movie reviews, and we
aim to share this dataset for further re-
search in sentiment polarity analysis
task.
</bodyText>
<sectionHeader confidence="0.999001" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999896022222223">
This paper focuses on document-level sen-
timent classification on polarity reviews.
Specifically, the document-level sentiment
analysis is to identify either a positive or
negative opinion in a given opinionated re-
view (Pang and Lee, 2008; Liu, 2010). In
early work, Turney (2002) proposed an un-
supervised learning algorithm to classify re-
views by calculating the mutual information
between a given phrase and reference words
“excellent” and “poor”. Pang et al. (2002)
applied supervised learners of Naive Bayes,
Maximum Entropy, and Support Vector Ma-
chine (SVM) to determine sentiment polarity
over movie reviews. Pang and Lee (2004)
presented a minimum cut-based approach to
detect whether each review’ sentence is more
likely subjective or not. Then the sentiment of
the whole document review is determined by
employing a machine learning method on the
document’s most-subjective sentences.
Recently, most sentiment polarity clas-
sification systems (Whitelaw et al., 2005;
Kennedy and Inkpen, 2006; Martineau and
Finin, 2009; Maas et al., 2011; Tu et al., 2012;
Wang and Manning, 2012; Nguyen et al.,
2013) have obtained state-of-the-art results by
employing machine learning techniques using
combination of various features such as N-
grams, syntactic and semantic representations
as well as exploiting lexicon resources (Wil-
son et al., 2005; Ng et al., 2006; Baccianella
et al., 2010; Taboada et al., 2011).
In this paper, we firstly introduce a novel
rating-based feature for the sentiment polarity
classification task. Our rating-based feature
can be seen by that the scores – which users
employ to rate entities on review websites –
could bring useful information for improving
the performance of classifying polarity senti-
ment. For a review with no associated score,
we could predict a score for the review in the
use of a regression model learned from an ex-
ternal independent dataset of reviews and their
actual corresponding scores. We refer to the
</bodyText>
<page confidence="0.990675">
128
</page>
<bodyText confidence="0.9191015">
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 128–135,
Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics
predicted score as the rating-based feature for
learning sentiment categorization.
By combining the rating-based feature with
unigrams, bigrams and trigrams, we then
present the results from sentiment classifica-
tion experiments on the benchmark datasets
published by Pang and Lee (2004) and Maas
et al. (2011).
To sum up, the contributions of our study
are:
</bodyText>
<listItem confidence="0.9892482">
• Propose a novel rating-based feature and
describe regression models learned from
the external dataset to predict the feature
value for the reviews in the two experi-
mental datasets.
• Achieve state-of-the-art performances in
the use of the rating-based feature for the
sentiment polarity classification task on
the two datasets.
• Analyze comprehensively the profi-
ciency of the rating-based feature to the
accuracy performance.
• Report additional experimental results on
our own dataset containing 233600 re-
views.
</listItem>
<bodyText confidence="0.999909">
The paper is organized as follows: We pro-
vide some related works and describe our ap-
proach in section 2 and section 3, respectively.
We detail our experiments in section 4. Fi-
nally, section 5 presents concluding remarks.
</bodyText>
<sectionHeader confidence="0.999011" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.999947821428571">
Whitelaw et al. (2005) described an approach
using appraisal groups such as “extremely
boring”, or “not really very good” for senti-
ment analysis, in which a semi-automatically
constructed lexicon is used to return appraisal
attribute values for related terms. Kennedy
and Inkpen (2006) analyzed the effect of con-
textual valence shifters on sentiment classi-
fication of movie reviews. Martineau and
Finin (2009) weighted bag-of-words in em-
ploying a delta TF-IDF function for train-
ing SVMs to classify the reviews. Maas et
al. (2011) introduced a model to catch sen-
timent information and word meanings. Tu
et al. (2012) proposed an approach utiliz-
ing high-impact parse features for convolution
kernels in document-level sentiment recogni-
tion. Meanwhile, Wang and Manning (2012)
obtained a strong and robust performance
by identifying simple NB and SVM vari-
ants. Dahl et al. (2012) applied the restricted
Boltzmann machine to learn representations
capturing meaningful syntactic and semantic
properties of words. In addition, Nguyen et
al. (2013) constructed a two-stage sentiment
classifier applying reject option, where docu-
ments rejected at the first stage are forwarded
to be classified at the second stage.
</bodyText>
<sectionHeader confidence="0.989577" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.999979833333333">
We apply a supervised machine learning ap-
proach to handle the task of document-level
sentiment polarity classification. For machine
learning experiments, besides the N-gram fea-
tures, we employ a new rating-based feature
for training models.
</bodyText>
<subsectionHeader confidence="0.99631">
3.1 Rating-based Feature
</subsectionHeader>
<bodyText confidence="0.99996805">
Our proposed rating-based feature can be seen
by the fact that, on various review websites,
users’ reviews of entities such as products,
services, events and their properties ordinar-
ily associate to scores which the users utilize
to rate the entities: a positive review mostly
corresponds with a high score whereas a neg-
ative one strongly correlates to a low score.
Therefore, the rated score could bring useful
information to enhance the sentiment classifi-
cation performance.
We consider the rated score associated to
each document review as a feature named RbF
for learning classification model, in which
the rating-based feature RbF’s value of each
document review in training and test sets
is estimated based on a regression model
learned from an external independent dataset
of reviews along with their actual associated
scores.
</bodyText>
<page confidence="0.992392">
129
</page>
<subsectionHeader confidence="0.989603">
3.2 N-gram Features
</subsectionHeader>
<bodyText confidence="0.9999479375">
In most related works, unigrams are consid-
ered as the most basic features, in which each
document is represented as a collection of
unique unigram words where each word is
considered as an individual feature.
In addition, we take into account bigrams
and trigrams since a combination of unigram,
bigram and trigram features (N-grams) could
outperform a baseline performance based on
unigram features as pointed out in (Ng et al.,
2006; Martineau and Finin, 2009; Wang and
Manning, 2012).
We calculate the value of the N-gram fea-
ture ith by using term frequency - inverse doc-
umentfrequency (tf*idf) weighting scheme for
the document D as follows:
</bodyText>
<equation confidence="0.858468">
NgramzD = log(1 + tfzD) ∗ log |{D}|
dfz
</equation>
<bodyText confidence="0.99999">
where tfzD is the occurrence frequency of
the feature ith in document D, |{D} |is the
number of documents in the data corpus {D},
and dfz is the number of documents contain-
ing the feature ith. We then normalize N-gram
feature vector of the document D as follows:
</bodyText>
<equation confidence="0.994283333333333">
�δ∈{D} 11−−−−−→
−−−−−−−→ Ngramδ11
ηNgramD =
</equation>
<sectionHeader confidence="0.995837" genericHeader="method">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.561204">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999975666666667">
Benchmark datasets. We conducted exper-
imental evaluations on the polarity dataset
PL041 of 2000 movie reviews constructed by
Pang and Lee (2004). The dataset PL04 con-
sists of 1000 positive and 1000 negative doc-
ument reviews in which each review was split
into sentences with lowercase normalization.
In order to compare with other published re-
sults, we evaluate our method according to
10-fold cross-validation scheme on the dataset
PL04.
In addition, we carry out experiments on
a large dataset IMDB112 of 50000 movie re-
views produced by Maas et al. (2011). The
large dataset IMDB11 contains a training set
</bodyText>
<footnote confidence="0.999497">
1http://www.cs.cornell.edu/people/pabo/movie-review-data/
2http://ai.stanford.edu/—amaas/data/sentiment/
</footnote>
<bodyText confidence="0.9998325">
of 25000 labeled reviews and a test set of
25000 labeled reviews, where training and test
sets have 12500 positive reviews and 12500
negative reviews in each.
Machine learning algorithm. We utilize
SVM implementation in LIBSVM3 (Chang
and Lin, 2011) for learning classification
models in all our experiments on the two
benchmark datasets.
Preprocess. We did not apply stop-word
removal, stemming and lemmatization to the
dataset in any process in our system, because
such stop-words as negation words might in-
dicate sentiment orientation, and as pointed
out by Leopold and Kindermann (2002) stem-
ming and lemmatization processes could be
detrimental to accuracy.
In all experiments on PL04, we kept 30000
most frequent N-grams in the training set for
each cross-validation run over each polarity
class. After removing duplication, on an aver-
age, there are total 39950 N-gram features in-
cluding 10280 unigrams, 20505 bigrams and
9165 trigrams.
On the dataset IMDB11, it was 40000 most
frequent N-grams in each polarity class to be
selected for creating feature set of 53724 N-
grams consisting of 13038 unigrams, 26907
bigrams and 13779 trigrams.
RbF feature extraction procedure. We
aim to create an independent dataset for learn-
ing a regression model to predict the feature
RbF’s value for each document review in ex-
perimental datasets. Since Maas et al. (2011)
also provided 7091 IMDB movie titles4, we
used those movie titles to extract all user re-
views that their associated scores5 are not
equal to either 5 or 6 from the IMDB website.
</bodyText>
<footnote confidence="0.919758">
3http://www.csie.ntu.edu.tw/—cjlin/libsvm/. Using linear
kernel, default parameter settings.
4 http://www.imdb.com/. It is noted that the 7091 movie
titles are completely different from those that were used to
produce the datasets PL04 and IMDB11.
5 The score scale ranges from 1 to 10. As the reviews cor-
responding to rated scores 5 or 6 are likely to be ambiguous
for expressing positive or negative sentiments, we decide to
ignore those 5-6 score reviews. We also abandon user reviews
having no associated rated scores.
</footnote>
<table confidence="0.7681215">
|{Dj |* 11 NgramD11
* NgramD
</table>
<page confidence="0.879861">
130
</page>
<figureCaption confidence="0.999915">
Figure 1: The score distribution of SAR14.
</figureCaption>
<bodyText confidence="0.99999296">
Consequently, we created an independent
score-associated review dataset (SAR14)6 of
233600 movie reviews and their accompany-
ing actual scores. The external dataset SAR14
consists of 167378 user reviews connected to
scores valued from 7 to 10, and 66222 reviews
linked to 1-4 rated ones (as shown in Fig-
ure 1). Using SAR14, we employed Support
Vector Regression algorithm implemented in
SV Mlight package7 (Joachims, 1999) to learn
the regression model employing unigram fea-
tures. We then applied the learned model
to predict real score values of reviews in the
benchmark datasets, and referred to those val-
ues as the values of the feature RbF.
Although using N-gram features (consist-
ing of unigrams, bigrams and trigrams) may
give better results, we tend to use only uni-
grams for learning the regression model be-
cause of saving the training time on the large
size of SAR14. Furthermore, using unigram
features is good enough as presented in sec-
tion 4.4. To extract the RbF feature’s value
for each PL04’s movie review, the regres-
sion model was trained with 20000 most fre-
</bodyText>
<footnote confidence="0.99781275">
6The SAR14 data set is available to download at
https://sites.google.com/site/nquocdai/resources
7http://svmlight.joachims.org/. Using with default param-
eter settings.
</footnote>
<bodyText confidence="0.99966925">
quent unigrams whilst 35000 most frequent
unigrams were employed to learn regression
model to estimate the RbF feature for each re-
view in the dataset IMDB11.
</bodyText>
<subsectionHeader confidence="0.922725">
4.2 Results on PL04
</subsectionHeader>
<bodyText confidence="0.999967090909091">
Table 1 shows the accuracy results of our
method in comparison with other state-of-the-
art SVM-based performances on the dataset
PL04. Our method achieves a baseline accu-
racy of 87.6% which is higher than baselines
obtained by all other compared approaches.
The accuracy based on only RbF feature is
88.2% being higher than those published in
(Pang and Lee, 2004; Martineau and Finin,
2009; Nguyen et al., 2013). By exploiting
a combination of unigram and RbF features,
we gain a result at 89.8% which is compara-
ble with the highest performances reached by
(Whitelaw et al., 2005; Ng et al., 2006; Wang
and Manning, 2012). It is evident that rising
from 87.6% to 89.8% proves the effectiveness
of using RbF in sentiment polarity classifica-
tion.
Turning to the use of N-grams, we attain
an accuracy of 89.25% which is 1.65% higher
than the baseline result of 87.6%. This shows
the usefulness of adding bigram and trigram
</bodyText>
<page confidence="0.995493">
131
</page>
<table confidence="0.999886266666667">
Features PL04 IMDB11
Unigrams (baseline) 87.60 83.69
N-grams 89.25 88.67
RbF 88.20 89.14
Unigrams + RbF 89.80 84.71
N-grams + RbF 91.60 89.87
Pang and Lee (2004) 87.20 ——
Whitelaw et al. (2005) 90.20 ——
Ng et al. (2006) 90.50 ——
Martineau and Finin (2009) 88.10 ——
Maas et al. (2011) 88.90 88.89
Tu et al. (2012) 88.50 ——
Dahl et al. (2012) —— 89.23
Wang and Manning (2012) 89.45 91.22
Nguyen et al. (2013) 87.95 ——
</table>
<tableCaption confidence="0.999757">
Table 1: Accuracy results (in %).
</tableCaption>
<bodyText confidence="0.999892">
features to improve the accuracy. With 91.6%,
we reach a new state-of-the-art performance
by combining N-gram and RbF features. We
also note that our state-of-the-art accuracy is
1.1% impressively higher than the highest ac-
curacy published by Ng et al. (2006).
</bodyText>
<subsectionHeader confidence="0.993846">
4.3 Results on IMDB11
</subsectionHeader>
<bodyText confidence="0.999930166666667">
Table 1 also shows the performance results
of our approach on the dataset IMDB11. Al-
though our method gets a baseline accuracy of
83.69% which is lower than other baseline re-
sults of 88.23% and 88.29% reported by Maas
et al. (2011) and Wang and Manning (2012)
respectively, we achieve a noticeable accuracy
of 89.14% based on only RbF feature.
Furthermore, starting at the result of
88.67% with N-gram features, we obtain a
significant increase to 89.87% by employing
N-gram and RbF features. Particularly, we do
better than the performance at 89.23% pub-
lished by Dahl et al. (2012) with a 0.64% im-
provement in accuracy on 160 test cases.
From our experimental results in section
4.2 and 4.3, we conclude that there are signif-
icant gains in performance results by adding
bigrams and trigrams as well as RbF fea-
ture for sentiment polarity classification. Our
method combining N-grams and RbF fea-
ture outperforms most other published results
on the two benchmark datasets PL04 and
IMDB11.
</bodyText>
<subsectionHeader confidence="0.999412">
4.4 Effects of RbF to Accuracy
</subsectionHeader>
<bodyText confidence="0.999994916666667">
This section is to give a detail analysis about
the effects of using RbF feature to accuracy
results of our approach (as shown in Figure
2) using full combination of N-gram and RbF
features in which the RbF feature is predicted
by regression models learned on the dataset
SAR14 in varying number K of most frequent
unigrams from 5000 to 40000.
On the dataset PL04, the highest accuracy
obtained by using only the RbF feature is
88.90% at K’s value of 10000, which it is
equal to that published by Maas et al. (2011).
In most cases of using N-gram and RbF fea-
tures, we obtain state-of-the-art results which
are higher than 91%.
On the IMDB11 dataset, at K’s value of
5000, we achieve the lowest accuracy of
89.29% by using N-gram and RbF features,
which it is slightly higher than the accuracy of
89.23% given by Dahl et al. (2012). In cases
that K’s value is higher than 10000, accura-
cies using only RbF feature are around 89.1%,
while using the full combination returns re-
sults which are higher than 89.74%.
</bodyText>
<page confidence="0.995289">
132
</page>
<figureCaption confidence="0.9955385">
Figure 2: Effects of rating-based feature to our method’s performance. The horizontal presents
the number of unigram features selected for learning regression models.
</figureCaption>
<subsectionHeader confidence="0.969686">
4.5 Results on SAR14
</subsectionHeader>
<bodyText confidence="0.999696727272727">
As mentioned in section 4.1, our dataset
SAR14 contains 233600 movie reviews. We
label a review as ‘positive’ or ‘negative’ if
the review has a score ≥ 7 or ≤ 4 respec-
tively. Therefore, we create a very large
dataset of 167378 positive reviews and 66222
negative reviews. Due to the large size of the
dataset SAR14 and the training and classifi-
cation time, we employed LIBLINEAR8 (Fan
et al., 2008) for this experiment under 10 fold
cross validation scheme. We kept 50000 N-
</bodyText>
<footnote confidence="0.991236">
8Using L2-regularized logistic regression and
setting tolerance of termination criterion to 0.01.
http://www.csie.ntu.edu.tw/∼cjlin/liblinear/
</footnote>
<bodyText confidence="0.999651">
grams over each polarity class in the training
set for each cross-validation run. Finally, we
obtained an accuracy of 93.24% by using N-
gram features.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="evaluation">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999957">
In this paper, we conducted an experimen-
tal study on sentiment polarity classification.
We firstly described our new rating-based fea-
ture, in which the rating-based feature is es-
timated based on a regression model learned
from our external independent dataset SAR14
of 233600 movie reviews. We then exam-
ined the contribution of the rating-based fea-
ture and N-grams in a machine learning-based
</bodyText>
<page confidence="0.997136">
133
</page>
<bodyText confidence="0.999723714285714">
approach on two datasets PL04 and IMDB11.
Specifically, we reach state-of-the-art accu-
racies at 91.6% and 89.87% on the dataset
PL04 and IMDB11 respectively. Further-
more, by analyzing the effects of rating-based
feature to accuracy performance, we show
that the rating-based feature is very efficient to
sentiment classification on polarity reviews.
And adding bigram and trigram features also
enhances accuracy performance. Further-
more, we get an accuracy of 93.24% on the
dataset SAR14, and we also share this dataset
for further research in sentiment polarity anal-
ysis task.
</bodyText>
<sectionHeader confidence="0.976982" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.994994666666667">
This work is partially supported by the Re-
search Grant from Vietnam National Univer-
sity, Hanoi No. QG.14.04.
</bodyText>
<sectionHeader confidence="0.99826" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997337375">
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. Sentiwordnet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC’10), pages 2200–2204.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1–27:27.
George Dahl, Hugo Larochelle, and Ryan P. Adams.
2012. Training restricted boltzmann machines on
word observations. In Proceedings of the 29th Inter-
national Conference on Machine Learning (ICML-
12), pages 679–686.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871–1874.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Bernhard
Sch¨olkopf, Christopher J. C. Burges, and Alexan-
der J. Smola, editors, Advances in Kernel Methods:
Support Vector Machines, pages 169–184.
Alistair Kennedy and Diana Inkpen. 2006. Senti-
ment Classification of Movie Reviews Using Con-
textual Valence Shifters. Computational Intelli-
gence, 22(2):110–125.
Edda Leopold and J¨org Kindermann. 2002. Text cat-
egorization with support vector machines. how to
represent texts in input space? Mach. Learn., 46(1-
3):423–444.
Bing Liu. 2010. Sentiment analysis and subjectivity.
In Handbook of Natural Language Processing, Sec-
ond Edition, pages 1–38.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, Vol 1, pages 142–150.
Justin Martineau and Tim Finin. 2009. Delta tfidf: an
improved feature space for sentiment analysis. In
Proceedings of the Third Annual Conference on We-
blogs and Social Media, pages 258–261.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin.
2006. Examining the role of linguistic knowledge
sources in the automatic identification and classi-
fication of reviews. In Proceedings of the COL-
ING/ACL on Main conference poster sessions, pages
611–618.
Dai Quoc Nguyen, Dat Quoc Nguyen, and Son Bao
Pham. 2013. A Two-Stage Classifier for Sentiment
Analysis. In Proceedings of the 6th International
Joint Conference on Natural Language Processing,
pages 897–901.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL’04), pages 271–278.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1–135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing - Volume 10, pages 79–86.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Comput. Lin-
guist., 37(2):267–307, June.
Zhaopeng Tu, Yifan He, Jennifer Foster, Josef van Gen-
abith, Qun Liu, and Shouxun Lin. 2012. Identify-
ing high-impact sub-structures for convolution ker-
nels in document-level sentiment classification. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), ACL ’12, pages 338–343.
</reference>
<page confidence="0.986237">
134
</page>
<reference confidence="0.998950636363636">
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised clas-
sification of reviews. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, ACL ’02, pages 417–424.
Sida Wang and Christopher D. Manning. 2012. Base-
lines and bigrams: simple, good sentiment and topic
classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), ACL ’12, pages
90–94.
Casey Whitelaw, Navendu Garg, and Shlomo Arga-
mon. 2005. Using appraisal groups for sentiment
analysis. In Proceedings of the 14th ACM inter-
national conference on Information and knowledge
management, CIKM ’05, pages 625–631.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ’05, pages 347–354.
</reference>
<page confidence="0.998775">
135
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.386058">
<title confidence="0.875127333333333">Sentiment Classification on Polarity An Empirical Study Using Rating-based Features Quoc Quoc</title>
<author confidence="0.716769">Bao</author>
<affiliation confidence="0.997303">of Information University of Engineering and Vietnam National University,</affiliation>
<email confidence="0.787595">datnq,</email>
<title confidence="0.908201">and Communications</title>
<author confidence="0.936243">The Open University</author>
<author confidence="0.936243">Milton Keynes</author>
<email confidence="0.998432">thanh.vu@open.ac.uk</email>
<abstract confidence="0.996359263157895">We present a new feature type named feature evaluate the contribution of this feature to the task of document-level sentiment analysis. We achieve state-of-the-art results on two publicly available standard polarity movie datasets: on the dataset consisting of 2000 reviews produced by Pang and Lee (2004) we obtain an accuracy of 91.6% while it is 89.87% evaluated on the dataset of 50000 reviews created by Maas et al. (2011). We also get a performance at 93.24% on our own dataset consisting of 233600 movie reviews, and we aim to share this dataset for further research in sentiment polarity analysis task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<pages>2200--2204</pages>
<contexts>
<context position="2445" citStr="Baccianella et al., 2010" startWordPosition="372" endWordPosition="375">iment of the whole document review is determined by employing a machine learning method on the document’s most-subjective sentences. Recently, most sentiment polarity classification systems (Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012; Nguyen et al., 2013) have obtained state-of-the-art results by employing machine learning techniques using combination of various features such as Ngrams, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Baccianella et al., 2010; Taboada et al., 2011). In this paper, we firstly introduce a novel rating-based feature for the sentiment polarity classification task. Our rating-based feature can be seen by that the scores – which users employ to rate entities on review websites – could bring useful information for improving the performance of classifying polarity sentiment. For a review with no associated score, we could predict a score for the review in the use of a regression model learned from an external independent dataset of reviews and their actual corresponding scores. We refer to the 128 Proceedings of the 5th W</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), pages 2200–2204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<contexts>
<context position="8699" citStr="Chang and Lin, 2011" startWordPosition="1343" endWordPosition="1346">esults, we evaluate our method according to 10-fold cross-validation scheme on the dataset PL04. In addition, we carry out experiments on a large dataset IMDB112 of 50000 movie reviews produced by Maas et al. (2011). The large dataset IMDB11 contains a training set 1http://www.cs.cornell.edu/people/pabo/movie-review-data/ 2http://ai.stanford.edu/—amaas/data/sentiment/ of 25000 labeled reviews and a test set of 25000 labeled reviews, where training and test sets have 12500 positive reviews and 12500 negative reviews in each. Machine learning algorithm. We utilize SVM implementation in LIBSVM3 (Chang and Lin, 2011) for learning classification models in all our experiments on the two benchmark datasets. Preprocess. We did not apply stop-word removal, stemming and lemmatization to the dataset in any process in our system, because such stop-words as negation words might indicate sentiment orientation, and as pointed out by Leopold and Kindermann (2002) stemming and lemmatization processes could be detrimental to accuracy. In all experiments on PL04, we kept 30000 most frequent N-grams in the training set for each cross-validation run over each polarity class. After removing duplication, on an average, ther</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Dahl</author>
<author>Hugo Larochelle</author>
<author>Ryan P Adams</author>
</authors>
<title>Training restricted boltzmann machines on word observations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine Learning (ICML12),</booktitle>
<pages>679--686</pages>
<contexts>
<context position="5228" citStr="Dahl et al. (2012)" startWordPosition="804" endWordPosition="807"> Kennedy and Inkpen (2006) analyzed the effect of contextual valence shifters on sentiment classification of movie reviews. Martineau and Finin (2009) weighted bag-of-words in employing a delta TF-IDF function for training SVMs to classify the reviews. Maas et al. (2011) introduced a model to catch sentiment information and word meanings. Tu et al. (2012) proposed an approach utilizing high-impact parse features for convolution kernels in document-level sentiment recognition. Meanwhile, Wang and Manning (2012) obtained a strong and robust performance by identifying simple NB and SVM variants. Dahl et al. (2012) applied the restricted Boltzmann machine to learn representations capturing meaningful syntactic and semantic properties of words. In addition, Nguyen et al. (2013) constructed a two-stage sentiment classifier applying reject option, where documents rejected at the first stage are forwarded to be classified at the second stage. 3 Our Approach We apply a supervised machine learning approach to handle the task of document-level sentiment polarity classification. For machine learning experiments, besides the N-gram features, we employ a new rating-based feature for training models. 3.1 Rating-ba</context>
<context position="13267" citStr="Dahl et al. (2012)" startWordPosition="2093" endWordPosition="2096">ng from 87.6% to 89.8% proves the effectiveness of using RbF in sentiment polarity classification. Turning to the use of N-grams, we attain an accuracy of 89.25% which is 1.65% higher than the baseline result of 87.6%. This shows the usefulness of adding bigram and trigram 131 Features PL04 IMDB11 Unigrams (baseline) 87.60 83.69 N-grams 89.25 88.67 RbF 88.20 89.14 Unigrams + RbF 89.80 84.71 N-grams + RbF 91.60 89.87 Pang and Lee (2004) 87.20 —— Whitelaw et al. (2005) 90.20 —— Ng et al. (2006) 90.50 —— Martineau and Finin (2009) 88.10 —— Maas et al. (2011) 88.90 88.89 Tu et al. (2012) 88.50 —— Dahl et al. (2012) —— 89.23 Wang and Manning (2012) 89.45 91.22 Nguyen et al. (2013) 87.95 —— Table 1: Accuracy results (in %). features to improve the accuracy. With 91.6%, we reach a new state-of-the-art performance by combining N-gram and RbF features. We also note that our state-of-the-art accuracy is 1.1% impressively higher than the highest accuracy published by Ng et al. (2006). 4.3 Results on IMDB11 Table 1 also shows the performance results of our approach on the dataset IMDB11. Although our method gets a baseline accuracy of 83.69% which is lower than other baseline results of 88.23% and 88.29% report</context>
<context position="15508" citStr="Dahl et al. (2012)" startWordPosition="2481" endWordPosition="2484">re is predicted by regression models learned on the dataset SAR14 in varying number K of most frequent unigrams from 5000 to 40000. On the dataset PL04, the highest accuracy obtained by using only the RbF feature is 88.90% at K’s value of 10000, which it is equal to that published by Maas et al. (2011). In most cases of using N-gram and RbF features, we obtain state-of-the-art results which are higher than 91%. On the IMDB11 dataset, at K’s value of 5000, we achieve the lowest accuracy of 89.29% by using N-gram and RbF features, which it is slightly higher than the accuracy of 89.23% given by Dahl et al. (2012). In cases that K’s value is higher than 10000, accuracies using only RbF feature are around 89.1%, while using the full combination returns results which are higher than 89.74%. 132 Figure 2: Effects of rating-based feature to our method’s performance. The horizontal presents the number of unigram features selected for learning regression models. 4.5 Results on SAR14 As mentioned in section 4.1, our dataset SAR14 contains 233600 movie reviews. We label a review as ‘positive’ or ‘negative’ if the review has a score ≥ 7 or ≤ 4 respectively. Therefore, we create a very large dataset of 167378 po</context>
</contexts>
<marker>Dahl, Larochelle, Adams, 2012</marker>
<rawString>George Dahl, Hugo Larochelle, and Ryan P. Adams. 2012. Training restricted boltzmann machines on word observations. In Proceedings of the 29th International Conference on Machine Learning (ICML12), pages 679–686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="16277" citStr="Fan et al., 2008" startWordPosition="2611" endWordPosition="2614">ch are higher than 89.74%. 132 Figure 2: Effects of rating-based feature to our method’s performance. The horizontal presents the number of unigram features selected for learning regression models. 4.5 Results on SAR14 As mentioned in section 4.1, our dataset SAR14 contains 233600 movie reviews. We label a review as ‘positive’ or ‘negative’ if the review has a score ≥ 7 or ≤ 4 respectively. Therefore, we create a very large dataset of 167378 positive reviews and 66222 negative reviews. Due to the large size of the dataset SAR14 and the training and classification time, we employed LIBLINEAR8 (Fan et al., 2008) for this experiment under 10 fold cross validation scheme. We kept 50000 N8Using L2-regularized logistic regression and setting tolerance of termination criterion to 0.01. http://www.csie.ntu.edu.tw/∼cjlin/liblinear/ grams over each polarity class in the training set for each cross-validation run. Finally, we obtained an accuracy of 93.24% by using Ngram features. 5 Conclusion In this paper, we conducted an experimental study on sentiment polarity classification. We firstly described our new rating-based feature, in which the rating-based feature is estimated based on a regression model learn</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods: Support Vector Machines,</booktitle>
<pages>169--184</pages>
<editor>In Bernhard Sch¨olkopf, Christopher J. C. Burges, and Alexander J. Smola, editors,</editor>
<contexts>
<context position="11008" citStr="Joachims, 1999" startWordPosition="1715" endWordPosition="1716">timents, we decide to ignore those 5-6 score reviews. We also abandon user reviews having no associated rated scores. |{Dj |* 11 NgramD11 * NgramD 130 Figure 1: The score distribution of SAR14. Consequently, we created an independent score-associated review dataset (SAR14)6 of 233600 movie reviews and their accompanying actual scores. The external dataset SAR14 consists of 167378 user reviews connected to scores valued from 7 to 10, and 66222 reviews linked to 1-4 rated ones (as shown in Figure 1). Using SAR14, we employed Support Vector Regression algorithm implemented in SV Mlight package7 (Joachims, 1999) to learn the regression model employing unigram features. We then applied the learned model to predict real score values of reviews in the benchmark datasets, and referred to those values as the values of the feature RbF. Although using N-gram features (consisting of unigrams, bigrams and trigrams) may give better results, we tend to use only unigrams for learning the regression model because of saving the training time on the large size of SAR14. Furthermore, using unigram features is good enough as presented in section 4.4. To extract the RbF feature’s value for each PL04’s movie review, th</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale support vector machine learning practical. In Bernhard Sch¨olkopf, Christopher J. C. Burges, and Alexander J. Smola, editors, Advances in Kernel Methods: Support Vector Machines, pages 169–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Kennedy</author>
<author>Diana Inkpen</author>
</authors>
<title>Sentiment Classification of Movie Reviews Using Contextual Valence Shifters.</title>
<date>2006</date>
<journal>Computational Intelligence,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="2059" citStr="Kennedy and Inkpen, 2006" startWordPosition="311" endWordPosition="314">rmation between a given phrase and reference words “excellent” and “poor”. Pang et al. (2002) applied supervised learners of Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) to determine sentiment polarity over movie reviews. Pang and Lee (2004) presented a minimum cut-based approach to detect whether each review’ sentence is more likely subjective or not. Then the sentiment of the whole document review is determined by employing a machine learning method on the document’s most-subjective sentences. Recently, most sentiment polarity classification systems (Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012; Nguyen et al., 2013) have obtained state-of-the-art results by employing machine learning techniques using combination of various features such as Ngrams, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Baccianella et al., 2010; Taboada et al., 2011). In this paper, we firstly introduce a novel rating-based feature for the sentiment polarity classification task. Our rating-based feature can be seen by that the scores – which users employ</context>
<context position="4636" citStr="Kennedy and Inkpen (2006)" startWordPosition="710" endWordPosition="713">cy performance. • Report additional experimental results on our own dataset containing 233600 reviews. The paper is organized as follows: We provide some related works and describe our approach in section 2 and section 3, respectively. We detail our experiments in section 4. Finally, section 5 presents concluding remarks. 2 Related Works Whitelaw et al. (2005) described an approach using appraisal groups such as “extremely boring”, or “not really very good” for sentiment analysis, in which a semi-automatically constructed lexicon is used to return appraisal attribute values for related terms. Kennedy and Inkpen (2006) analyzed the effect of contextual valence shifters on sentiment classification of movie reviews. Martineau and Finin (2009) weighted bag-of-words in employing a delta TF-IDF function for training SVMs to classify the reviews. Maas et al. (2011) introduced a model to catch sentiment information and word meanings. Tu et al. (2012) proposed an approach utilizing high-impact parse features for convolution kernels in document-level sentiment recognition. Meanwhile, Wang and Manning (2012) obtained a strong and robust performance by identifying simple NB and SVM variants. Dahl et al. (2012) applied</context>
</contexts>
<marker>Kennedy, Inkpen, 2006</marker>
<rawString>Alistair Kennedy and Diana Inkpen. 2006. Sentiment Classification of Movie Reviews Using Contextual Valence Shifters. Computational Intelligence, 22(2):110–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edda Leopold</author>
<author>J¨org Kindermann</author>
</authors>
<title>Text categorization with support vector machines. how to represent texts in input space?</title>
<date>2002</date>
<pages>46--1</pages>
<location>Mach. Learn.,</location>
<contexts>
<context position="9040" citStr="Leopold and Kindermann (2002)" startWordPosition="1395" endWordPosition="1398">ai.stanford.edu/—amaas/data/sentiment/ of 25000 labeled reviews and a test set of 25000 labeled reviews, where training and test sets have 12500 positive reviews and 12500 negative reviews in each. Machine learning algorithm. We utilize SVM implementation in LIBSVM3 (Chang and Lin, 2011) for learning classification models in all our experiments on the two benchmark datasets. Preprocess. We did not apply stop-word removal, stemming and lemmatization to the dataset in any process in our system, because such stop-words as negation words might indicate sentiment orientation, and as pointed out by Leopold and Kindermann (2002) stemming and lemmatization processes could be detrimental to accuracy. In all experiments on PL04, we kept 30000 most frequent N-grams in the training set for each cross-validation run over each polarity class. After removing duplication, on an average, there are total 39950 N-gram features including 10280 unigrams, 20505 bigrams and 9165 trigrams. On the dataset IMDB11, it was 40000 most frequent N-grams in each polarity class to be selected for creating feature set of 53724 Ngrams consisting of 13038 unigrams, 26907 bigrams and 13779 trigrams. RbF feature extraction procedure. We aim to cre</context>
</contexts>
<marker>Leopold, Kindermann, 2002</marker>
<rawString>Edda Leopold and J¨org Kindermann. 2002. Text categorization with support vector machines. how to represent texts in input space? Mach. Learn., 46(1-3):423–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and subjectivity.</title>
<date>2010</date>
<booktitle>In Handbook of Natural Language Processing, Second Edition,</booktitle>
<pages>1--38</pages>
<contexts>
<context position="1310" citStr="Liu, 2010" startWordPosition="201" endWordPosition="202"> reviews produced by Pang and Lee (2004) we obtain an accuracy of 91.6% while it is 89.87% evaluated on the dataset of 50000 reviews created by Maas et al. (2011). We also get a performance at 93.24% on our own dataset consisting of 233600 movie reviews, and we aim to share this dataset for further research in sentiment polarity analysis task. 1 Introduction This paper focuses on document-level sentiment classification on polarity reviews. Specifically, the document-level sentiment analysis is to identify either a positive or negative opinion in a given opinionated review (Pang and Lee, 2008; Liu, 2010). In early work, Turney (2002) proposed an unsupervised learning algorithm to classify reviews by calculating the mutual information between a given phrase and reference words “excellent” and “poor”. Pang et al. (2002) applied supervised learners of Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) to determine sentiment polarity over movie reviews. Pang and Lee (2004) presented a minimum cut-based approach to detect whether each review’ sentence is more likely subjective or not. Then the sentiment of the whole document review is determined by employing a machine learning method o</context>
</contexts>
<marker>Liu, 2010</marker>
<rawString>Bing Liu. 2010. Sentiment analysis and subjectivity. In Handbook of Natural Language Processing, Second Edition, pages 1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<volume>1</volume>
<pages>142--150</pages>
<contexts>
<context position="862" citStr="Maas et al. (2011)" startWordPosition="127" endWordPosition="130">nam National University, Hanoi {dainq, datnq, sonpb}@vnu.edu.vn † Computing and Communications Department The Open University, Milton Keynes, UK thanh.vu@open.ac.uk Abstract We present a new feature type named rating-based feature and evaluate the contribution of this feature to the task of document-level sentiment analysis. We achieve state-of-the-art results on two publicly available standard polarity movie datasets: on the dataset consisting of 2000 reviews produced by Pang and Lee (2004) we obtain an accuracy of 91.6% while it is 89.87% evaluated on the dataset of 50000 reviews created by Maas et al. (2011). We also get a performance at 93.24% on our own dataset consisting of 233600 movie reviews, and we aim to share this dataset for further research in sentiment polarity analysis task. 1 Introduction This paper focuses on document-level sentiment classification on polarity reviews. Specifically, the document-level sentiment analysis is to identify either a positive or negative opinion in a given opinionated review (Pang and Lee, 2008; Liu, 2010). In early work, Turney (2002) proposed an unsupervised learning algorithm to classify reviews by calculating the mutual information between a given phr</context>
<context position="2105" citStr="Maas et al., 2011" startWordPosition="319" endWordPosition="322">xcellent” and “poor”. Pang et al. (2002) applied supervised learners of Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) to determine sentiment polarity over movie reviews. Pang and Lee (2004) presented a minimum cut-based approach to detect whether each review’ sentence is more likely subjective or not. Then the sentiment of the whole document review is determined by employing a machine learning method on the document’s most-subjective sentences. Recently, most sentiment polarity classification systems (Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012; Nguyen et al., 2013) have obtained state-of-the-art results by employing machine learning techniques using combination of various features such as Ngrams, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Baccianella et al., 2010; Taboada et al., 2011). In this paper, we firstly introduce a novel rating-based feature for the sentiment polarity classification task. Our rating-based feature can be seen by that the scores – which users employ to rate entities on review websites – could b</context>
<context position="3549" citStr="Maas et al. (2011)" startWordPosition="540" endWordPosition="543">independent dataset of reviews and their actual corresponding scores. We refer to the 128 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 128–135, Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics predicted score as the rating-based feature for learning sentiment categorization. By combining the rating-based feature with unigrams, bigrams and trigrams, we then present the results from sentiment classification experiments on the benchmark datasets published by Pang and Lee (2004) and Maas et al. (2011). To sum up, the contributions of our study are: • Propose a novel rating-based feature and describe regression models learned from the external dataset to predict the feature value for the reviews in the two experimental datasets. • Achieve state-of-the-art performances in the use of the rating-based feature for the sentiment polarity classification task on the two datasets. • Analyze comprehensively the proficiency of the rating-based feature to the accuracy performance. • Report additional experimental results on our own dataset containing 233600 reviews. The paper is organized as follows: </context>
<context position="4881" citStr="Maas et al. (2011)" startWordPosition="750" endWordPosition="753">periments in section 4. Finally, section 5 presents concluding remarks. 2 Related Works Whitelaw et al. (2005) described an approach using appraisal groups such as “extremely boring”, or “not really very good” for sentiment analysis, in which a semi-automatically constructed lexicon is used to return appraisal attribute values for related terms. Kennedy and Inkpen (2006) analyzed the effect of contextual valence shifters on sentiment classification of movie reviews. Martineau and Finin (2009) weighted bag-of-words in employing a delta TF-IDF function for training SVMs to classify the reviews. Maas et al. (2011) introduced a model to catch sentiment information and word meanings. Tu et al. (2012) proposed an approach utilizing high-impact parse features for convolution kernels in document-level sentiment recognition. Meanwhile, Wang and Manning (2012) obtained a strong and robust performance by identifying simple NB and SVM variants. Dahl et al. (2012) applied the restricted Boltzmann machine to learn representations capturing meaningful syntactic and semantic properties of words. In addition, Nguyen et al. (2013) constructed a two-stage sentiment classifier applying reject option, where documents re</context>
<context position="8294" citStr="Maas et al. (2011)" startWordPosition="1293" endWordPosition="1296">mδ11 ηNgramD = 4 Experimental Results 4.1 Experimental Setup Benchmark datasets. We conducted experimental evaluations on the polarity dataset PL041 of 2000 movie reviews constructed by Pang and Lee (2004). The dataset PL04 consists of 1000 positive and 1000 negative document reviews in which each review was split into sentences with lowercase normalization. In order to compare with other published results, we evaluate our method according to 10-fold cross-validation scheme on the dataset PL04. In addition, we carry out experiments on a large dataset IMDB112 of 50000 movie reviews produced by Maas et al. (2011). The large dataset IMDB11 contains a training set 1http://www.cs.cornell.edu/people/pabo/movie-review-data/ 2http://ai.stanford.edu/—amaas/data/sentiment/ of 25000 labeled reviews and a test set of 25000 labeled reviews, where training and test sets have 12500 positive reviews and 12500 negative reviews in each. Machine learning algorithm. We utilize SVM implementation in LIBSVM3 (Chang and Lin, 2011) for learning classification models in all our experiments on the two benchmark datasets. Preprocess. We did not apply stop-word removal, stemming and lemmatization to the dataset in any process </context>
<context position="9809" citStr="Maas et al. (2011)" startWordPosition="1520" endWordPosition="1523"> set for each cross-validation run over each polarity class. After removing duplication, on an average, there are total 39950 N-gram features including 10280 unigrams, 20505 bigrams and 9165 trigrams. On the dataset IMDB11, it was 40000 most frequent N-grams in each polarity class to be selected for creating feature set of 53724 Ngrams consisting of 13038 unigrams, 26907 bigrams and 13779 trigrams. RbF feature extraction procedure. We aim to create an independent dataset for learning a regression model to predict the feature RbF’s value for each document review in experimental datasets. Since Maas et al. (2011) also provided 7091 IMDB movie titles4, we used those movie titles to extract all user reviews that their associated scores5 are not equal to either 5 or 6 from the IMDB website. 3http://www.csie.ntu.edu.tw/—cjlin/libsvm/. Using linear kernel, default parameter settings. 4 http://www.imdb.com/. It is noted that the 7091 movie titles are completely different from those that were used to produce the datasets PL04 and IMDB11. 5 The score scale ranges from 1 to 10. As the reviews corresponding to rated scores 5 or 6 are likely to be ambiguous for expressing positive or negative sentiments, we deci</context>
<context position="13210" citStr="Maas et al. (2011)" startWordPosition="2081" endWordPosition="2084">., 2006; Wang and Manning, 2012). It is evident that rising from 87.6% to 89.8% proves the effectiveness of using RbF in sentiment polarity classification. Turning to the use of N-grams, we attain an accuracy of 89.25% which is 1.65% higher than the baseline result of 87.6%. This shows the usefulness of adding bigram and trigram 131 Features PL04 IMDB11 Unigrams (baseline) 87.60 83.69 N-grams 89.25 88.67 RbF 88.20 89.14 Unigrams + RbF 89.80 84.71 N-grams + RbF 91.60 89.87 Pang and Lee (2004) 87.20 —— Whitelaw et al. (2005) 90.20 —— Ng et al. (2006) 90.50 —— Martineau and Finin (2009) 88.10 —— Maas et al. (2011) 88.90 88.89 Tu et al. (2012) 88.50 —— Dahl et al. (2012) —— 89.23 Wang and Manning (2012) 89.45 91.22 Nguyen et al. (2013) 87.95 —— Table 1: Accuracy results (in %). features to improve the accuracy. With 91.6%, we reach a new state-of-the-art performance by combining N-gram and RbF features. We also note that our state-of-the-art accuracy is 1.1% impressively higher than the highest accuracy published by Ng et al. (2006). 4.3 Results on IMDB11 Table 1 also shows the performance results of our approach on the dataset IMDB11. Although our method gets a baseline accuracy of 83.69% which is lowe</context>
<context position="15193" citStr="Maas et al. (2011)" startWordPosition="2424" endWordPosition="2427"> other published results on the two benchmark datasets PL04 and IMDB11. 4.4 Effects of RbF to Accuracy This section is to give a detail analysis about the effects of using RbF feature to accuracy results of our approach (as shown in Figure 2) using full combination of N-gram and RbF features in which the RbF feature is predicted by regression models learned on the dataset SAR14 in varying number K of most frequent unigrams from 5000 to 40000. On the dataset PL04, the highest accuracy obtained by using only the RbF feature is 88.90% at K’s value of 10000, which it is equal to that published by Maas et al. (2011). In most cases of using N-gram and RbF features, we obtain state-of-the-art results which are higher than 91%. On the IMDB11 dataset, at K’s value of 5000, we achieve the lowest accuracy of 89.29% by using N-gram and RbF features, which it is slightly higher than the accuracy of 89.23% given by Dahl et al. (2012). In cases that K’s value is higher than 10000, accuracies using only RbF feature are around 89.1%, while using the full combination returns results which are higher than 89.74%. 132 Figure 2: Effects of rating-based feature to our method’s performance. The horizontal presents the num</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Vol 1, pages 142–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Martineau</author>
<author>Tim Finin</author>
</authors>
<title>Delta tfidf: an improved feature space for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Annual Conference on Weblogs and Social Media,</booktitle>
<pages>258--261</pages>
<contexts>
<context position="2086" citStr="Martineau and Finin, 2009" startWordPosition="315" endWordPosition="318">rase and reference words “excellent” and “poor”. Pang et al. (2002) applied supervised learners of Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) to determine sentiment polarity over movie reviews. Pang and Lee (2004) presented a minimum cut-based approach to detect whether each review’ sentence is more likely subjective or not. Then the sentiment of the whole document review is determined by employing a machine learning method on the document’s most-subjective sentences. Recently, most sentiment polarity classification systems (Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012; Nguyen et al., 2013) have obtained state-of-the-art results by employing machine learning techniques using combination of various features such as Ngrams, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Baccianella et al., 2010; Taboada et al., 2011). In this paper, we firstly introduce a novel rating-based feature for the sentiment polarity classification task. Our rating-based feature can be seen by that the scores – which users employ to rate entities on review</context>
<context position="4760" citStr="Martineau and Finin (2009)" startWordPosition="729" endWordPosition="732">ed as follows: We provide some related works and describe our approach in section 2 and section 3, respectively. We detail our experiments in section 4. Finally, section 5 presents concluding remarks. 2 Related Works Whitelaw et al. (2005) described an approach using appraisal groups such as “extremely boring”, or “not really very good” for sentiment analysis, in which a semi-automatically constructed lexicon is used to return appraisal attribute values for related terms. Kennedy and Inkpen (2006) analyzed the effect of contextual valence shifters on sentiment classification of movie reviews. Martineau and Finin (2009) weighted bag-of-words in employing a delta TF-IDF function for training SVMs to classify the reviews. Maas et al. (2011) introduced a model to catch sentiment information and word meanings. Tu et al. (2012) proposed an approach utilizing high-impact parse features for convolution kernels in document-level sentiment recognition. Meanwhile, Wang and Manning (2012) obtained a strong and robust performance by identifying simple NB and SVM variants. Dahl et al. (2012) applied the restricted Boltzmann machine to learn representations capturing meaningful syntactic and semantic properties of words. </context>
<context position="7159" citStr="Martineau and Finin, 2009" startWordPosition="1102" endWordPosition="1105">is estimated based on a regression model learned from an external independent dataset of reviews along with their actual associated scores. 129 3.2 N-gram Features In most related works, unigrams are considered as the most basic features, in which each document is represented as a collection of unique unigram words where each word is considered as an individual feature. In addition, we take into account bigrams and trigrams since a combination of unigram, bigram and trigram features (N-grams) could outperform a baseline performance based on unigram features as pointed out in (Ng et al., 2006; Martineau and Finin, 2009; Wang and Manning, 2012). We calculate the value of the N-gram feature ith by using term frequency - inverse documentfrequency (tf*idf) weighting scheme for the document D as follows: NgramzD = log(1 + tfzD) ∗ log |{D}| dfz where tfzD is the occurrence frequency of the feature ith in document D, |{D} |is the number of documents in the data corpus {D}, and dfz is the number of documents containing the feature ith. We then normalize N-gram feature vector of the document D as follows: �δ∈{D} 11−−−−−→ −−−−−−−→ Ngramδ11 ηNgramD = 4 Experimental Results 4.1 Experimental Setup Benchmark datasets. We</context>
<context position="12392" citStr="Martineau and Finin, 2009" startWordPosition="1937" endWordPosition="1940">ight.joachims.org/. Using with default parameter settings. quent unigrams whilst 35000 most frequent unigrams were employed to learn regression model to estimate the RbF feature for each review in the dataset IMDB11. 4.2 Results on PL04 Table 1 shows the accuracy results of our method in comparison with other state-of-theart SVM-based performances on the dataset PL04. Our method achieves a baseline accuracy of 87.6% which is higher than baselines obtained by all other compared approaches. The accuracy based on only RbF feature is 88.2% being higher than those published in (Pang and Lee, 2004; Martineau and Finin, 2009; Nguyen et al., 2013). By exploiting a combination of unigram and RbF features, we gain a result at 89.8% which is comparable with the highest performances reached by (Whitelaw et al., 2005; Ng et al., 2006; Wang and Manning, 2012). It is evident that rising from 87.6% to 89.8% proves the effectiveness of using RbF in sentiment polarity classification. Turning to the use of N-grams, we attain an accuracy of 89.25% which is 1.65% higher than the baseline result of 87.6%. This shows the usefulness of adding bigram and trigram 131 Features PL04 IMDB11 Unigrams (baseline) 87.60 83.69 N-grams 89.2</context>
</contexts>
<marker>Martineau, Finin, 2009</marker>
<rawString>Justin Martineau and Tim Finin. 2009. Delta tfidf: an improved feature space for sentiment analysis. In Proceedings of the Third Annual Conference on Weblogs and Social Media, pages 258–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Sajib Dasgupta</author>
<author>S M Niaz Arifin</author>
</authors>
<title>Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>611--618</pages>
<contexts>
<context position="2419" citStr="Ng et al., 2006" startWordPosition="368" endWordPosition="371">ot. Then the sentiment of the whole document review is determined by employing a machine learning method on the document’s most-subjective sentences. Recently, most sentiment polarity classification systems (Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012; Nguyen et al., 2013) have obtained state-of-the-art results by employing machine learning techniques using combination of various features such as Ngrams, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Baccianella et al., 2010; Taboada et al., 2011). In this paper, we firstly introduce a novel rating-based feature for the sentiment polarity classification task. Our rating-based feature can be seen by that the scores – which users employ to rate entities on review websites – could bring useful information for improving the performance of classifying polarity sentiment. For a review with no associated score, we could predict a score for the review in the use of a regression model learned from an external independent dataset of reviews and their actual corresponding scores. We refer to the 12</context>
<context position="7132" citStr="Ng et al., 2006" startWordPosition="1098" endWordPosition="1101">ng and test sets is estimated based on a regression model learned from an external independent dataset of reviews along with their actual associated scores. 129 3.2 N-gram Features In most related works, unigrams are considered as the most basic features, in which each document is represented as a collection of unique unigram words where each word is considered as an individual feature. In addition, we take into account bigrams and trigrams since a combination of unigram, bigram and trigram features (N-grams) could outperform a baseline performance based on unigram features as pointed out in (Ng et al., 2006; Martineau and Finin, 2009; Wang and Manning, 2012). We calculate the value of the N-gram feature ith by using term frequency - inverse documentfrequency (tf*idf) weighting scheme for the document D as follows: NgramzD = log(1 + tfzD) ∗ log |{D}| dfz where tfzD is the occurrence frequency of the feature ith in document D, |{D} |is the number of documents in the data corpus {D}, and dfz is the number of documents containing the feature ith. We then normalize N-gram feature vector of the document D as follows: �δ∈{D} 11−−−−−→ −−−−−−−→ Ngramδ11 ηNgramD = 4 Experimental Results 4.1 Experimental S</context>
<context position="12599" citStr="Ng et al., 2006" startWordPosition="1974" endWordPosition="1977">4.2 Results on PL04 Table 1 shows the accuracy results of our method in comparison with other state-of-theart SVM-based performances on the dataset PL04. Our method achieves a baseline accuracy of 87.6% which is higher than baselines obtained by all other compared approaches. The accuracy based on only RbF feature is 88.2% being higher than those published in (Pang and Lee, 2004; Martineau and Finin, 2009; Nguyen et al., 2013). By exploiting a combination of unigram and RbF features, we gain a result at 89.8% which is comparable with the highest performances reached by (Whitelaw et al., 2005; Ng et al., 2006; Wang and Manning, 2012). It is evident that rising from 87.6% to 89.8% proves the effectiveness of using RbF in sentiment polarity classification. Turning to the use of N-grams, we attain an accuracy of 89.25% which is 1.65% higher than the baseline result of 87.6%. This shows the usefulness of adding bigram and trigram 131 Features PL04 IMDB11 Unigrams (baseline) 87.60 83.69 N-grams 89.25 88.67 RbF 88.20 89.14 Unigrams + RbF 89.80 84.71 N-grams + RbF 91.60 89.87 Pang and Lee (2004) 87.20 —— Whitelaw et al. (2005) 90.20 —— Ng et al. (2006) 90.50 —— Martineau and Finin (2009) 88.10 —— Maas et</context>
</contexts>
<marker>Ng, Dasgupta, Arifin, 2006</marker>
<rawString>Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin. 2006. Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 611–618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dai Quoc Nguyen</author>
<author>Dat Quoc Nguyen</author>
<author>Son Bao Pham</author>
</authors>
<title>A Two-Stage Classifier for Sentiment Analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th International Joint Conference on Natural Language Processing,</booktitle>
<pages>897--901</pages>
<contexts>
<context position="2168" citStr="Nguyen et al., 2013" startWordPosition="331" endWordPosition="334">learners of Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) to determine sentiment polarity over movie reviews. Pang and Lee (2004) presented a minimum cut-based approach to detect whether each review’ sentence is more likely subjective or not. Then the sentiment of the whole document review is determined by employing a machine learning method on the document’s most-subjective sentences. Recently, most sentiment polarity classification systems (Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012; Nguyen et al., 2013) have obtained state-of-the-art results by employing machine learning techniques using combination of various features such as Ngrams, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Baccianella et al., 2010; Taboada et al., 2011). In this paper, we firstly introduce a novel rating-based feature for the sentiment polarity classification task. Our rating-based feature can be seen by that the scores – which users employ to rate entities on review websites – could bring useful information for improving the performance of classi</context>
<context position="5393" citStr="Nguyen et al. (2013)" startWordPosition="826" endWordPosition="829">-of-words in employing a delta TF-IDF function for training SVMs to classify the reviews. Maas et al. (2011) introduced a model to catch sentiment information and word meanings. Tu et al. (2012) proposed an approach utilizing high-impact parse features for convolution kernels in document-level sentiment recognition. Meanwhile, Wang and Manning (2012) obtained a strong and robust performance by identifying simple NB and SVM variants. Dahl et al. (2012) applied the restricted Boltzmann machine to learn representations capturing meaningful syntactic and semantic properties of words. In addition, Nguyen et al. (2013) constructed a two-stage sentiment classifier applying reject option, where documents rejected at the first stage are forwarded to be classified at the second stage. 3 Our Approach We apply a supervised machine learning approach to handle the task of document-level sentiment polarity classification. For machine learning experiments, besides the N-gram features, we employ a new rating-based feature for training models. 3.1 Rating-based Feature Our proposed rating-based feature can be seen by the fact that, on various review websites, users’ reviews of entities such as products, services, events</context>
<context position="12414" citStr="Nguyen et al., 2013" startWordPosition="1941" endWordPosition="1944">ith default parameter settings. quent unigrams whilst 35000 most frequent unigrams were employed to learn regression model to estimate the RbF feature for each review in the dataset IMDB11. 4.2 Results on PL04 Table 1 shows the accuracy results of our method in comparison with other state-of-theart SVM-based performances on the dataset PL04. Our method achieves a baseline accuracy of 87.6% which is higher than baselines obtained by all other compared approaches. The accuracy based on only RbF feature is 88.2% being higher than those published in (Pang and Lee, 2004; Martineau and Finin, 2009; Nguyen et al., 2013). By exploiting a combination of unigram and RbF features, we gain a result at 89.8% which is comparable with the highest performances reached by (Whitelaw et al., 2005; Ng et al., 2006; Wang and Manning, 2012). It is evident that rising from 87.6% to 89.8% proves the effectiveness of using RbF in sentiment polarity classification. Turning to the use of N-grams, we attain an accuracy of 89.25% which is 1.65% higher than the baseline result of 87.6%. This shows the usefulness of adding bigram and trigram 131 Features PL04 IMDB11 Unigrams (baseline) 87.60 83.69 N-grams 89.25 88.67 RbF 88.20 89.1</context>
</contexts>
<marker>Nguyen, Nguyen, Pham, 2013</marker>
<rawString>Dai Quoc Nguyen, Dat Quoc Nguyen, and Son Bao Pham. 2013. A Two-Stage Classifier for Sentiment Analysis. In Proceedings of the 6th International Joint Conference on Natural Language Processing, pages 897–901.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04),</booktitle>
<pages>271--278</pages>
<contexts>
<context position="740" citStr="Pang and Lee (2004)" startWordPosition="103" endWordPosition="106">c Nguyen* and Thanh Vu† and Son Bao Pham* * Faculty of Information Technology University of Engineering and Technology Vietnam National University, Hanoi {dainq, datnq, sonpb}@vnu.edu.vn † Computing and Communications Department The Open University, Milton Keynes, UK thanh.vu@open.ac.uk Abstract We present a new feature type named rating-based feature and evaluate the contribution of this feature to the task of document-level sentiment analysis. We achieve state-of-the-art results on two publicly available standard polarity movie datasets: on the dataset consisting of 2000 reviews produced by Pang and Lee (2004) we obtain an accuracy of 91.6% while it is 89.87% evaluated on the dataset of 50000 reviews created by Maas et al. (2011). We also get a performance at 93.24% on our own dataset consisting of 233600 movie reviews, and we aim to share this dataset for further research in sentiment polarity analysis task. 1 Introduction This paper focuses on document-level sentiment classification on polarity reviews. Specifically, the document-level sentiment analysis is to identify either a positive or negative opinion in a given opinionated review (Pang and Lee, 2008; Liu, 2010). In early work, Turney (2002)</context>
<context position="3526" citStr="Pang and Lee (2004)" startWordPosition="535" endWordPosition="538">earned from an external independent dataset of reviews and their actual corresponding scores. We refer to the 128 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 128–135, Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics predicted score as the rating-based feature for learning sentiment categorization. By combining the rating-based feature with unigrams, bigrams and trigrams, we then present the results from sentiment classification experiments on the benchmark datasets published by Pang and Lee (2004) and Maas et al. (2011). To sum up, the contributions of our study are: • Propose a novel rating-based feature and describe regression models learned from the external dataset to predict the feature value for the reviews in the two experimental datasets. • Achieve state-of-the-art performances in the use of the rating-based feature for the sentiment polarity classification task on the two datasets. • Analyze comprehensively the proficiency of the rating-based feature to the accuracy performance. • Report additional experimental results on our own dataset containing 233600 reviews. The paper is</context>
<context position="7881" citStr="Pang and Lee (2004)" startWordPosition="1224" endWordPosition="1227">nverse documentfrequency (tf*idf) weighting scheme for the document D as follows: NgramzD = log(1 + tfzD) ∗ log |{D}| dfz where tfzD is the occurrence frequency of the feature ith in document D, |{D} |is the number of documents in the data corpus {D}, and dfz is the number of documents containing the feature ith. We then normalize N-gram feature vector of the document D as follows: �δ∈{D} 11−−−−−→ −−−−−−−→ Ngramδ11 ηNgramD = 4 Experimental Results 4.1 Experimental Setup Benchmark datasets. We conducted experimental evaluations on the polarity dataset PL041 of 2000 movie reviews constructed by Pang and Lee (2004). The dataset PL04 consists of 1000 positive and 1000 negative document reviews in which each review was split into sentences with lowercase normalization. In order to compare with other published results, we evaluate our method according to 10-fold cross-validation scheme on the dataset PL04. In addition, we carry out experiments on a large dataset IMDB112 of 50000 movie reviews produced by Maas et al. (2011). The large dataset IMDB11 contains a training set 1http://www.cs.cornell.edu/people/pabo/movie-review-data/ 2http://ai.stanford.edu/—amaas/data/sentiment/ of 25000 labeled reviews and a </context>
<context position="12365" citStr="Pang and Lee, 2004" startWordPosition="1933" endWordPosition="1936">sources 7http://svmlight.joachims.org/. Using with default parameter settings. quent unigrams whilst 35000 most frequent unigrams were employed to learn regression model to estimate the RbF feature for each review in the dataset IMDB11. 4.2 Results on PL04 Table 1 shows the accuracy results of our method in comparison with other state-of-theart SVM-based performances on the dataset PL04. Our method achieves a baseline accuracy of 87.6% which is higher than baselines obtained by all other compared approaches. The accuracy based on only RbF feature is 88.2% being higher than those published in (Pang and Lee, 2004; Martineau and Finin, 2009; Nguyen et al., 2013). By exploiting a combination of unigram and RbF features, we gain a result at 89.8% which is comparable with the highest performances reached by (Whitelaw et al., 2005; Ng et al., 2006; Wang and Manning, 2012). It is evident that rising from 87.6% to 89.8% proves the effectiveness of using RbF in sentiment polarity classification. Turning to the use of N-grams, we attain an accuracy of 89.25% which is 1.65% higher than the baseline result of 87.6%. This shows the usefulness of adding bigram and trigram 131 Features PL04 IMDB11 Unigrams (baselin</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="1298" citStr="Pang and Lee, 2008" startWordPosition="197" endWordPosition="200">t consisting of 2000 reviews produced by Pang and Lee (2004) we obtain an accuracy of 91.6% while it is 89.87% evaluated on the dataset of 50000 reviews created by Maas et al. (2011). We also get a performance at 93.24% on our own dataset consisting of 233600 movie reviews, and we aim to share this dataset for further research in sentiment polarity analysis task. 1 Introduction This paper focuses on document-level sentiment classification on polarity reviews. Specifically, the document-level sentiment analysis is to identify either a positive or negative opinion in a given opinionated review (Pang and Lee, 2008; Liu, 2010). In early work, Turney (2002) proposed an unsupervised learning algorithm to classify reviews by calculating the mutual information between a given phrase and reference words “excellent” and “poor”. Pang et al. (2002) applied supervised learners of Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) to determine sentiment polarity over movie reviews. Pang and Lee (2004) presented a minimum cut-based approach to detect whether each review’ sentence is more likely subjective or not. Then the sentiment of the whole document review is determined by employing a machine learn</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume</booktitle>
<volume>10</volume>
<pages>79--86</pages>
<contexts>
<context position="1528" citStr="Pang et al. (2002)" startWordPosition="233" endWordPosition="236"> dataset consisting of 233600 movie reviews, and we aim to share this dataset for further research in sentiment polarity analysis task. 1 Introduction This paper focuses on document-level sentiment classification on polarity reviews. Specifically, the document-level sentiment analysis is to identify either a positive or negative opinion in a given opinionated review (Pang and Lee, 2008; Liu, 2010). In early work, Turney (2002) proposed an unsupervised learning algorithm to classify reviews by calculating the mutual information between a given phrase and reference words “excellent” and “poor”. Pang et al. (2002) applied supervised learners of Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) to determine sentiment polarity over movie reviews. Pang and Lee (2004) presented a minimum cut-based approach to detect whether each review’ sentence is more likely subjective or not. Then the sentiment of the whole document review is determined by employing a machine learning method on the document’s most-subjective sentences. Recently, most sentiment polarity classification systems (Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexiconbased methods for sentiment analysis.</title>
<date>2011</date>
<journal>Comput. Linguist.,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="2468" citStr="Taboada et al., 2011" startWordPosition="376" endWordPosition="379">t review is determined by employing a machine learning method on the document’s most-subjective sentences. Recently, most sentiment polarity classification systems (Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012; Nguyen et al., 2013) have obtained state-of-the-art results by employing machine learning techniques using combination of various features such as Ngrams, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Baccianella et al., 2010; Taboada et al., 2011). In this paper, we firstly introduce a novel rating-based feature for the sentiment polarity classification task. Our rating-based feature can be seen by that the scores – which users employ to rate entities on review websites – could bring useful information for improving the performance of classifying polarity sentiment. For a review with no associated score, we could predict a score for the review in the use of a regression model learned from an external independent dataset of reviews and their actual corresponding scores. We refer to the 128 Proceedings of the 5th Workshop on Computationa</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexiconbased methods for sentiment analysis. Comput. Linguist., 37(2):267–307, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhaopeng Tu</author>
<author>Yifan He</author>
<author>Jennifer Foster</author>
<author>Josef van Genabith</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Identifying high-impact sub-structures for convolution kernels in document-level sentiment classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL ’12,</booktitle>
<pages>338--343</pages>
<marker>Tu, He, Foster, van Genabith, Liu, Lin, 2012</marker>
<rawString>Zhaopeng Tu, Yifan He, Jennifer Foster, Josef van Genabith, Qun Liu, and Shouxun Lin. 2012. Identifying high-impact sub-structures for convolution kernels in document-level sentiment classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL ’12, pages 338–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="1340" citStr="Turney (2002)" startWordPosition="206" endWordPosition="207">nd Lee (2004) we obtain an accuracy of 91.6% while it is 89.87% evaluated on the dataset of 50000 reviews created by Maas et al. (2011). We also get a performance at 93.24% on our own dataset consisting of 233600 movie reviews, and we aim to share this dataset for further research in sentiment polarity analysis task. 1 Introduction This paper focuses on document-level sentiment classification on polarity reviews. Specifically, the document-level sentiment analysis is to identify either a positive or negative opinion in a given opinionated review (Pang and Lee, 2008; Liu, 2010). In early work, Turney (2002) proposed an unsupervised learning algorithm to classify reviews by calculating the mutual information between a given phrase and reference words “excellent” and “poor”. Pang et al. (2002) applied supervised learners of Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) to determine sentiment polarity over movie reviews. Pang and Lee (2004) presented a minimum cut-based approach to detect whether each review’ sentence is more likely subjective or not. Then the sentiment of the whole document review is determined by employing a machine learning method on the document’s most-subjecti</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Baselines and bigrams: simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL ’12,</booktitle>
<pages>90--94</pages>
<contexts>
<context position="2146" citStr="Wang and Manning, 2012" startWordPosition="327" endWordPosition="330">002) applied supervised learners of Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) to determine sentiment polarity over movie reviews. Pang and Lee (2004) presented a minimum cut-based approach to detect whether each review’ sentence is more likely subjective or not. Then the sentiment of the whole document review is determined by employing a machine learning method on the document’s most-subjective sentences. Recently, most sentiment polarity classification systems (Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012; Nguyen et al., 2013) have obtained state-of-the-art results by employing machine learning techniques using combination of various features such as Ngrams, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Baccianella et al., 2010; Taboada et al., 2011). In this paper, we firstly introduce a novel rating-based feature for the sentiment polarity classification task. Our rating-based feature can be seen by that the scores – which users employ to rate entities on review websites – could bring useful information for improving the</context>
<context position="5125" citStr="Wang and Manning (2012)" startWordPosition="786" endWordPosition="789">ich a semi-automatically constructed lexicon is used to return appraisal attribute values for related terms. Kennedy and Inkpen (2006) analyzed the effect of contextual valence shifters on sentiment classification of movie reviews. Martineau and Finin (2009) weighted bag-of-words in employing a delta TF-IDF function for training SVMs to classify the reviews. Maas et al. (2011) introduced a model to catch sentiment information and word meanings. Tu et al. (2012) proposed an approach utilizing high-impact parse features for convolution kernels in document-level sentiment recognition. Meanwhile, Wang and Manning (2012) obtained a strong and robust performance by identifying simple NB and SVM variants. Dahl et al. (2012) applied the restricted Boltzmann machine to learn representations capturing meaningful syntactic and semantic properties of words. In addition, Nguyen et al. (2013) constructed a two-stage sentiment classifier applying reject option, where documents rejected at the first stage are forwarded to be classified at the second stage. 3 Our Approach We apply a supervised machine learning approach to handle the task of document-level sentiment polarity classification. For machine learning experiment</context>
<context position="7184" citStr="Wang and Manning, 2012" startWordPosition="1106" endWordPosition="1109">ression model learned from an external independent dataset of reviews along with their actual associated scores. 129 3.2 N-gram Features In most related works, unigrams are considered as the most basic features, in which each document is represented as a collection of unique unigram words where each word is considered as an individual feature. In addition, we take into account bigrams and trigrams since a combination of unigram, bigram and trigram features (N-grams) could outperform a baseline performance based on unigram features as pointed out in (Ng et al., 2006; Martineau and Finin, 2009; Wang and Manning, 2012). We calculate the value of the N-gram feature ith by using term frequency - inverse documentfrequency (tf*idf) weighting scheme for the document D as follows: NgramzD = log(1 + tfzD) ∗ log |{D}| dfz where tfzD is the occurrence frequency of the feature ith in document D, |{D} |is the number of documents in the data corpus {D}, and dfz is the number of documents containing the feature ith. We then normalize N-gram feature vector of the document D as follows: �δ∈{D} 11−−−−−→ −−−−−−−→ Ngramδ11 ηNgramD = 4 Experimental Results 4.1 Experimental Setup Benchmark datasets. We conducted experimental e</context>
<context position="12624" citStr="Wang and Manning, 2012" startWordPosition="1978" endWordPosition="1981">04 Table 1 shows the accuracy results of our method in comparison with other state-of-theart SVM-based performances on the dataset PL04. Our method achieves a baseline accuracy of 87.6% which is higher than baselines obtained by all other compared approaches. The accuracy based on only RbF feature is 88.2% being higher than those published in (Pang and Lee, 2004; Martineau and Finin, 2009; Nguyen et al., 2013). By exploiting a combination of unigram and RbF features, we gain a result at 89.8% which is comparable with the highest performances reached by (Whitelaw et al., 2005; Ng et al., 2006; Wang and Manning, 2012). It is evident that rising from 87.6% to 89.8% proves the effectiveness of using RbF in sentiment polarity classification. Turning to the use of N-grams, we attain an accuracy of 89.25% which is 1.65% higher than the baseline result of 87.6%. This shows the usefulness of adding bigram and trigram 131 Features PL04 IMDB11 Unigrams (baseline) 87.60 83.69 N-grams 89.25 88.67 RbF 88.20 89.14 Unigrams + RbF 89.80 84.71 N-grams + RbF 91.60 89.87 Pang and Lee (2004) 87.20 —— Whitelaw et al. (2005) 90.20 —— Ng et al. (2006) 90.50 —— Martineau and Finin (2009) 88.10 —— Maas et al. (2011) 88.90 88.89 T</context>
<context position="13919" citStr="Wang and Manning (2012)" startWordPosition="2205" endWordPosition="2208">012) 89.45 91.22 Nguyen et al. (2013) 87.95 —— Table 1: Accuracy results (in %). features to improve the accuracy. With 91.6%, we reach a new state-of-the-art performance by combining N-gram and RbF features. We also note that our state-of-the-art accuracy is 1.1% impressively higher than the highest accuracy published by Ng et al. (2006). 4.3 Results on IMDB11 Table 1 also shows the performance results of our approach on the dataset IMDB11. Although our method gets a baseline accuracy of 83.69% which is lower than other baseline results of 88.23% and 88.29% reported by Maas et al. (2011) and Wang and Manning (2012) respectively, we achieve a noticeable accuracy of 89.14% based on only RbF feature. Furthermore, starting at the result of 88.67% with N-gram features, we obtain a significant increase to 89.87% by employing N-gram and RbF features. Particularly, we do better than the performance at 89.23% published by Dahl et al. (2012) with a 0.64% improvement in accuracy on 160 test cases. From our experimental results in section 4.2 and 4.3, we conclude that there are significant gains in performance results by adding bigrams and trigrams as well as RbF feature for sentiment polarity classification. Our m</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Sida Wang and Christopher D. Manning. 2012. Baselines and bigrams: simple, good sentiment and topic classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL ’12, pages 90–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Whitelaw</author>
<author>Navendu Garg</author>
<author>Shlomo Argamon</author>
</authors>
<title>Using appraisal groups for sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM international conference on Information and knowledge management, CIKM ’05,</booktitle>
<pages>625--631</pages>
<contexts>
<context position="2033" citStr="Whitelaw et al., 2005" startWordPosition="307" endWordPosition="310">ulating the mutual information between a given phrase and reference words “excellent” and “poor”. Pang et al. (2002) applied supervised learners of Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) to determine sentiment polarity over movie reviews. Pang and Lee (2004) presented a minimum cut-based approach to detect whether each review’ sentence is more likely subjective or not. Then the sentiment of the whole document review is determined by employing a machine learning method on the document’s most-subjective sentences. Recently, most sentiment polarity classification systems (Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012; Nguyen et al., 2013) have obtained state-of-the-art results by employing machine learning techniques using combination of various features such as Ngrams, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Baccianella et al., 2010; Taboada et al., 2011). In this paper, we firstly introduce a novel rating-based feature for the sentiment polarity classification task. Our rating-based feature can be seen by that the s</context>
<context position="4373" citStr="Whitelaw et al. (2005)" startWordPosition="671" endWordPosition="674">he two experimental datasets. • Achieve state-of-the-art performances in the use of the rating-based feature for the sentiment polarity classification task on the two datasets. • Analyze comprehensively the proficiency of the rating-based feature to the accuracy performance. • Report additional experimental results on our own dataset containing 233600 reviews. The paper is organized as follows: We provide some related works and describe our approach in section 2 and section 3, respectively. We detail our experiments in section 4. Finally, section 5 presents concluding remarks. 2 Related Works Whitelaw et al. (2005) described an approach using appraisal groups such as “extremely boring”, or “not really very good” for sentiment analysis, in which a semi-automatically constructed lexicon is used to return appraisal attribute values for related terms. Kennedy and Inkpen (2006) analyzed the effect of contextual valence shifters on sentiment classification of movie reviews. Martineau and Finin (2009) weighted bag-of-words in employing a delta TF-IDF function for training SVMs to classify the reviews. Maas et al. (2011) introduced a model to catch sentiment information and word meanings. Tu et al. (2012) propo</context>
<context position="12582" citStr="Whitelaw et al., 2005" startWordPosition="1970" endWordPosition="1973">in the dataset IMDB11. 4.2 Results on PL04 Table 1 shows the accuracy results of our method in comparison with other state-of-theart SVM-based performances on the dataset PL04. Our method achieves a baseline accuracy of 87.6% which is higher than baselines obtained by all other compared approaches. The accuracy based on only RbF feature is 88.2% being higher than those published in (Pang and Lee, 2004; Martineau and Finin, 2009; Nguyen et al., 2013). By exploiting a combination of unigram and RbF features, we gain a result at 89.8% which is comparable with the highest performances reached by (Whitelaw et al., 2005; Ng et al., 2006; Wang and Manning, 2012). It is evident that rising from 87.6% to 89.8% proves the effectiveness of using RbF in sentiment polarity classification. Turning to the use of N-grams, we attain an accuracy of 89.25% which is 1.65% higher than the baseline result of 87.6%. This shows the usefulness of adding bigram and trigram 131 Features PL04 IMDB11 Unigrams (baseline) 87.60 83.69 N-grams 89.25 88.67 RbF 88.20 89.14 Unigrams + RbF 89.80 84.71 N-grams + RbF 91.60 89.87 Pang and Lee (2004) 87.20 —— Whitelaw et al. (2005) 90.20 —— Ng et al. (2006) 90.50 —— Martineau and Finin (2009)</context>
</contexts>
<marker>Whitelaw, Garg, Argamon, 2005</marker>
<rawString>Casey Whitelaw, Navendu Garg, and Shlomo Argamon. 2005. Using appraisal groups for sentiment analysis. In Proceedings of the 14th ACM international conference on Information and knowledge management, CIKM ’05, pages 625–631.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="2402" citStr="Wilson et al., 2005" startWordPosition="363" endWordPosition="367">ikely subjective or not. Then the sentiment of the whole document review is determined by employing a machine learning method on the document’s most-subjective sentences. Recently, most sentiment polarity classification systems (Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012; Nguyen et al., 2013) have obtained state-of-the-art results by employing machine learning techniques using combination of various features such as Ngrams, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Baccianella et al., 2010; Taboada et al., 2011). In this paper, we firstly introduce a novel rating-based feature for the sentiment polarity classification task. Our rating-based feature can be seen by that the scores – which users employ to rate entities on review websites – could bring useful information for improving the performance of classifying polarity sentiment. For a review with no associated score, we could predict a score for the review in the use of a regression model learned from an external independent dataset of reviews and their actual corresponding scores. W</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 347–354.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.9920515">
Unsupervised method for the acquisition of general language paraphrases
for medical compounds
</title>
<note confidence="0.631792">
Natalia Grabar Thierry Hamon
CNRS UMR 8163 STL LIMSI-CNRS, BP133, Orsay
Universit´e Lille 3 Universit´e Paris 13
59653 Villeneuve d’Ascq, France Sorbonne Paris Cit´e, France
</note>
<email confidence="0.54477">
natalia.grabar@univ-lille3.fr hamon@limsi.fr
</email>
<sectionHeader confidence="0.984817" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998236">
Medical information is widespread in modern society (e.g. scientific research, medical blogs,
clinical documents, TV and radio broadcast, novels). Moreover, everybody’s life may be con-
cerned with medical problems. However, the medical field conveys very specific and often
opaque notions (e.g., myocardial infarction, cholecystectomy, abdominal strangulated hernia,
galactose urine), that are difficult to understand by lay people. We propose an automatic method
based on the morphological analysis of terms and on text mining for finding the paraphrases of
technical terms. Analysis of the results and their evaluation indicate that we can find correct
paraphrases for 343 terms. Depending on the semantics of the terms, error rate of the extractions
ranges between 0 and 59%. This kind of resources is useful for several Natural Language Pro-
cessing applications (i.e., information extraction, text simplification, question and answering).
</bodyText>
<sectionHeader confidence="0.988271" genericHeader="keywords">
1 Background
</sectionHeader>
<bodyText confidence="0.999892333333333">
Medical and health information is widespread in the modern society in light of pressing health concerns
and of maintaining of healthy lifestyles. Besides, it is also available through modern media: scientific
research, articles, medical blogs and fora, clinical documents, TV and radio broadcast, novels, discussion
fora, epidemiological alerts, etc. Still, availability of medical and health information does not guaran-
tee its easy and correct understanding by lay people. The medical field conveys indeed very technical
notions, such as in example (1).
</bodyText>
<listItem confidence="0.395131">
(1) myocardial infarction, cholecystectomy, erythredema polyneuropathy, acromegaly, galactosemia
</listItem>
<bodyText confidence="0.963187055555556">
Although technical, these notions are nevertheless important for patients (AMA, 1999; McCray, 2005;
Eysenbach, 2007; Oregon Evidence-based Practice Center, 2008). It has been shown that in several
situations such notions cannot be correctly understood by patients: the steps needed for the medication
preparing and use (Patel et al., 2002); the instructions on drugs from patient package inserts, and the
information delivered in informed consensus and health brochures: it appears that among the 2,600
patients recruited in two hospitals, 26% to 60% cannot manage information available in these sources
(Williams et al., 1995); health information in different languages (English, Spanish, French) provided in
websites created for patients require high reading level (Berland et al., 2001; Hargrave et al., 2003; Kusec,
2004) and remains difficult to manage by patients, which can be negative for the communication between
patients and medical professionals, and the healthcare process (Tran et al., 2009). This situation sets the
context of our work. Our objective is to propose method for the automatic acquisition of paraphrases for
technical medical notions. More particularly, we propose to concentrate on terms and their words that
show neoclassical compounding word formation (Booij, 2010; Iacobini, 1997; Amiot and Dal, 2005),
such as in the example (1). Such words often involve Latin and Greek roots or bases, which makes them
more difficult to understand, as such words must be decomposed first (see examples (2) and (3)). To our
knowledge, this kind of approach has not been applied for the acquisition of laymen paraphrases.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
</bodyText>
<page confidence="0.989423">
94
</page>
<note confidence="0.9675515">
Proceedings of the 4th International Workshop on Computational Terminology, pages 94–103,
Dublin, Ireland, August 23 2014.
</note>
<listItem confidence="0.968247">
(2) myocardial is formed with Latin myo (muscle) and Greek cardia (heart)
(3) cholecystectomy is formed with Greek chole (bile), Latin cystis (bladder), and Greek ectomy (surgical
removal)
</listItem>
<bodyText confidence="0.776821">
Our work is related to the following research topics:
</bodyText>
<listItem confidence="0.997660790697675">
• Readability. The readability studies the ease in which text can be understood. Two kinds of readabil-
ity measures are distinguished: classical and computational (Franc¸ois, 2011). Classical measures
are usually based on number of characters and/or syllables in words, sentences or documents and on
linear regression models (Flesch, 1948; Gunning, 1973; Dubay, 2004). Computational measures,
that are more recent, can involve vectorial models and a great variety of descriptors. These de-
scriptors, usually specific to the texts processed, are for instance: combination of classical measures
with medical terminologies (Kokkinakis and Toporowska Gronostaj, 2006); n-grams of characters
(Poprat et al., 2006); discursive descriptors (Goeuriot et al., 2007); lexicon (Miller et al., 2007);
morphological descriptors (Chmielik and Grabar, 2011); combination of various descriptors (Wang,
2006; Zeng-Treiler et al., 2007; Leroy et al., 2008; Franc¸ois and Fairon, 2013).
• Lexical simplification. The lexical simplification helps to make text easier to understand. Lexical
simplification of texts in English has been addressed during the SemEval 2012 challengea. Given
a short input text and a target word in English, and given several English substitutes for the tar-
get word that fit the context, the goal was to rank these substitutes according to how simple they
are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and
Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, mutual in-
formation and frequency of words (Jauhar and Specia, 2012); frequency in Wikipedia, word length,
n-grams of characters and of words, syntactic complexity of documents (Johannsen et al., 2012);
n-grams, frequency in Wikipedia, n-Google grams (Ligozat et al., 2012); WordNet and word fre-
quency (Amoia and Romanelli, 2012).
• Dedicated resources. The building of resources suitable for performing the simplification is an-
other related research topics. Such resources are mainly two-fold lexica in which specialized and
non-specialized vocabularies are aligned (in examples (4) to (6), the technical terms are followed
by their non-technical equivalents). The first initiative of the kind appeared with the collaborative
effort Consumer Health Vocabulary (Zeng and Tse, 2006) (examples in (4)). One of the meth-
ods was applied to the most frequently occurring medical queries aligned to the UMLS (Unified
Medical Language System) concepts (Lindberg et al., 1993). Another work exploited a small cor-
pus and several statistical association measures for building aligned lexicon with technical terms
from the UMLS and their lay equivalents (Elhadad and Sutaria, 2007). Similar work in other lan-
guages followed. In French, researchers proposed methods for the acquisition of syntactic variation
(Del´eger and Zweigenbaum, 2008; Cartoni and Del´eger, 2011) from comparable specialized and
non-specialized corpora, that led to the detection of verb/noun variations (examples in (5)) and a
larger set of syntactic variations (examples in (6)). Besides, research on the acquisition of termi-
nological variation (Hahn et al., 2001), synonymy (Fern´andez-Silva et al., 2011) and paraphrasing
(Max et al., 2012) is also relevant to outline the topics.
(4) {myocardial infarction, heart attack}, {abortion, termination of pregnancy}, {acrodynia, pink
disease}
(5) {consommation r´eguli`ere, consommer de fac¸on r´eguli`ere} (regular use), {gˆene a` la lecture,
ˆeche de lire} (reading difficulty), {´evolution de l’affection, la maladie ´evolue} (evolution of the
condition)
(6) {retard de cicatrisation, retarder la cicatrisation} (delay the healing), {apports caloriques, apport
en calories} (calorie supply), {calculer les doses, doses sont calcul´ees} (calculate the dose), {efficacit´e
est renforc´ee, renforcer son efficacit´e} (improve the efficiency)
</listItem>
<footnote confidence="0.49612">
ahttp://www.cs.york.ac.uk/semeval-2012/
emp
</footnote>
<page confidence="0.992728">
95
</page>
<bodyText confidence="0.999947583333333">
Our work is closely related to the building of resources dedicated to the lexical simplification. Our
objective is to propose method for paraphrasing the technical medical terms (i.e. medical compounds) in
expressions that are easier to understand by lay people. This aspect is seldom addressed: we can observe
that only some examples in (4) are concerned with the paraphrasing of technical and compound terms
(myocardial infarction, acrodynia). We work with the French data. Contrary to previous work, we do
not use comparable corpora with technical and non-technical texts. Instead, we exploit terms from an
existing medical terminology and corpora built from social media sources. We assume that this kind
of corpora may provide lay people equivalents for technical terms. We also rely on the morphological
analysis of technical terms. The expected result is to obtain pairs like {myocardial, heart muscle} or
{cholecystectomy, removal of gall bladder}. In the following, we start with the presentation of the
resources used (section 2), we present then the steps of the methodology (section 3). We describe and
discuss the obtained results (section 4) and conclude with some directions for future work (section 5).
</bodyText>
<sectionHeader confidence="0.999205" genericHeader="introduction">
2 Resources
</sectionHeader>
<subsectionHeader confidence="0.99929">
2.1 Medical terms
</subsectionHeader>
<bodyText confidence="0.999991545454545">
The material processed is issued from the French part of the UMLS. It provides syntactically simple terms
that contain one word only (acrodynia), and syntactically complex terms that contain more than one word
(myocardial infarction). Syntactically complex terms are segmented in words. Each term is associated
to semantic types. When a given word receives more than one semantic type, a manual post-processing
allows to disambiguate it: each word is assigned to one semantic type only. Among the semantic types
available, we consider the three most common in the medical practice to which the lay people are the
most exposed: Anatomy (616 words): describe human body anatomy (e.g. abdominopelvic); Disorders
(2,283 words): describe medical problems and their signs (e.g. infarction, diabetes); Procedures (1,271
words): describe procedures which may be performed by medical staff to detect or cure disorders (e.g.
cholecystectomy). In what follows, word and term can be exchangeable and mean either the graphical
unit provided by the segmentation, or the medical notion.
</bodyText>
<subsectionHeader confidence="0.992343">
2.2 Corpora
</subsectionHeader>
<bodyText confidence="0.7227435">
Wiki LesDiab DiabDoct HT Dos
Number of pages/threads 17,525 6,939 387,435 67,652 8,319
Number of articles/messages 17,525 1,438 22,431 12,588 1,124
Number of words 4,326,880 624,571 35,059,868 6,788,361 836,520
</bodyText>
<tableCaption confidence="0.993195">
Table 1: Size of the corpora exploited.
</tableCaption>
<bodyText confidence="0.991142">
We use several corpora collected from the social media sources (their sizes are indicated in Table 1):
</bodyText>
<listItem confidence="0.995773285714286">
1. Wiki contains French Wikipedia articles downloaded in February 2014, of which we keep those that
are categorized under the medical category Portail de la m´edecine;
2. LesDiab is collected from the discussion forum Les diab´etiquesb posted between June and July
2013. It is dedicated to diabetes;
3. DiabDoct is collected in June 2011 from the discussion forum Diab`ete of Doctissimoc
4. HT is collected in May 2013 from the discussion forum Hypertension of Doctissimod
5. Dos is collected in May 2013 from the discussion forum Douleurs de dos (backache) of Doctissimoe
</listItem>
<footnote confidence="0.99282775">
bhttp://www.lesdiabetiques.com/modules.php?name=Forums
chttp://forum.doctissimo.fr/sante/diabete/liste sujet-1.htm
dhttp://forum.doctissimo.fr/sante/hypertension-problemes-cardiaques/liste sujet-1.htm
ehttp://forum.doctissimo.fr/sante/douleur-dos/liste sujet-1.htm
</footnote>
<page confidence="0.995198">
96
</page>
<bodyText confidence="0.9991035">
The Wiki corpus contains encyclopaedic information on several medical notions from Wikipedia. Thanks
to the collaborative writing of the articles, these contain mostly correct information about the topics
concerned. Other corpora are collected from the dedicated fora (e.g. diabetes or backache). We assume
that people involved in these discussions may show low, middle or high degree of knowledge about the
disorders and related notions. We expect that all our corpora are written in a simple style and that they
contain paraphrases of technical terms. From Table 1, we can observe that the corpora vary in size.
</bodyText>
<sectionHeader confidence="0.974562" genericHeader="method">
3 Methodology for the automatic acquisition of paraphrases for medical compounds
</sectionHeader>
<bodyText confidence="0.999954833333333">
The methodology is designed for analyzing the neoclassical medical compounds and for searching their
non-technical paraphrases in corpora. In our approach, the paraphrases may occur alone, such as heart
muscle, without being accompanied by their technical compounds (myocarde). In this case, we need first
to acquire the knowledge needed for their automatic detection. We propose to rely on the morphological
analysis of terms. The method is composed of four main steps: the processing of terms, the processing of
corpora, the extraction of layman paraphrases for technical terms, and the evaluation of the extractions.
</bodyText>
<subsectionHeader confidence="0.998771">
3.1 The processing of medical terms
</subsectionHeader>
<bodyText confidence="0.996749">
To reach the morphological information on terms we apply three specific processing:
</bodyText>
<listItem confidence="0.6874716">
1. Morpho-syntactic tagging and lemmatization of terms. The terms are morpho-syntactically tagged
and lemmatized with TreeTagger for French (Schmid, 1994). The morpho-syntactic tagging is
done in context of the terms. If a given word receives more than one tag, the most frequent one is
kept. At this step, we obtain term lemmas with their part-of-speech tags, such as in example (7).
(7) myocardique/A (myocardial/A), chol´ecystectomie/N (cholecystectomy/N), polyneuropathie/N
(polyneuropathy/N), acrom´egalie/N (acromegaly/N), galactos´emie/N (galactosemia/N)
2. Morphological analysis. The lemmas are then morphologically analyzed with D´eriF (Namer,
2009). This tool performs the analysis of lemmas in order to detect their morphological structure,
to decompose them into their components (bases and affixes), and to semantically analyze their
structure. We give some examples of the morphological analysis in (8).
(8) myocardique/A: [[[myo N*] [carde N*] NOM] ique ADJ]
chol´ecystectomie/N: [[chol´ecysto N*] [ectomie N*] NOM]
polyneuropathie/N: [poly [[neur N*] [pathie N*] NOM] NOM]
acrom´egalie/N: [[acr N*] [m´egal N*] ie NOM]
galactos´emie/N: [[galactose NOM] [´em N*] ie NOM]
</listItem>
<bodyText confidence="0.999523125">
The computed bases and affixes are associated with syntactic categories (NOM, ADJ, V). When a
given base is suppletive (does not exist in modern French but was borrowed from Latin or Greek
languages), D´eriF assigns the most probable category (e.g. N* for nouns, A* for adjectives). For
instance, the analysis of myocardique/A indicates that this word contains the suppletive noun bases
myo N* (muscle) and carde N* (heart), and the affix -ique/ADJ. We can observe that some bases can
be decomposed further (e.g. galactose in galact (milk) and ose (sugars), cholecystectomy in chole (bile)
and cystis (bladder)). The words that contain more than one base are considered to be compounds
and are processed in the further steps of the method.
</bodyText>
<listItem confidence="0.9440212">
3. Association of morphological components with French words. The bases are “translated” with words
from modern French. We use for this resource built in previous work (Zweigenbaum and Grabar,
2003; Namer, 2003) (see some examples in (9)).
(9) myocardique/A: myo=muscle (muscle), carde=coeur (heart)
chol´ecystectomie/N: chol´ecysto=v´esicule biliaire (gall bladder), ectomie=ablation (removal)
</listItem>
<page confidence="0.994959">
97
</page>
<bodyText confidence="0.9429444">
polyneuropathie/N: poly=nombreux (several), neuro=nerf (nerve), pathie=maladie (disorder)
acrom´egalie/N: acr=extr´emit´e (extremity), m´egal=grandeur (size)
galactos´emie/N: galactose=galactose (galactose), ´em=sang (blood)
Some words can remain technical (e.g., galactose, v´esicule biliaire), while other components totally
lose their technical meaning (e.g. m´egal=grandeur (size), poly=nombreux (several)).
</bodyText>
<subsectionHeader confidence="0.999356">
3.2 The processing of corpora
</subsectionHeader>
<bodyText confidence="0.9999635">
The corpora are first segmented in words and sentences. Then, we also perform morpho-syntactic tagging
and lemmatization with TreeTagger for French.
</bodyText>
<subsectionHeader confidence="0.994028">
3.3 The extraction of layman paraphrases corresponding to technical terms
</subsectionHeader>
<bodyText confidence="0.999902285714286">
French words corresponding to the morphological decomposition of terms (examples in (9)) are projected
on corpora in order to extract sentences and their segments which can provide the layman paraphrases for
the corresponding technical terms. Sentences that contain the translated French words are extracted as
candidates for proposing the paraphrases. Additionally, the segments delimited by these words are also
extracted. We consider the co-occurrence of the words issued from the morphological decomposition in
a sliding graphical window of n words. In the experiments presented, the window size n is fixed to 10
words. Smaller or larger windows show less performance.
</bodyText>
<listItem confidence="0.532777">
(10) Les causes de tachycardie ventriculaire sont superposables a` celles des extrasystoles ventric-
</listItem>
<bodyText confidence="0.9823788">
ulaires: infarctus du myocarde, insuffisance cardiaque, hypertrophie du muscle du coeur et
prolapsus de la valve mitrale.
The sentence in (10) contains words muscle and coeur, underlined in the example, that correspond to
the morphological components of myocardique (see examples in (9)). For this reason, this sentence is
extracted, as well as the segment delimited by these two words muscle du coeur (heart muscle).
</bodyText>
<subsectionHeader confidence="0.982888">
3.4 The evaluation
</subsectionHeader>
<bodyText confidence="0.99813675">
The objective of the evaluation is to assess whether the proposed method is valid for the acquisition
of paraphrases for technical medical terms. The obtained results are evaluated manually by a com-
puter scientist with no training in biomedicine, but with background in computational linguistics and
morphology. We analyze the candidates for paraphrases from several points of view: Are the French
words corresponding to the components extracted correctly? Do these French words provide valid can-
didates for paraphrases? How easy are these paraphrases to be understood by laymen or by non-experts
in medicine? During the evaluation related to the second point (Do these French words provide valid
candidates for paraphrases?), we distinguish four situations:
</bodyText>
<listItem confidence="0.9964309">
1. the extraction is correct: e.g. myocardique paraphrased in muscle du coeur (heart muscle);
2. the extraction suffers from the incorrect morphological decomposition or from the wrong “trans-
lation” in French: e.g. p´erianal is “translated” in autour (around) and an (meaning year as it is). The
“translation” of this last word an is not correct and should be anus (anus) instead. Because of the
wrong “translation”, we collect a lot of incorrect segments like autour de 30 ans (around 30 years);
3. the extraction should be post-processed but contains the correct paraphrase: e.g. spondylarthrose,
“translated” in vert`ebre (vertebra) and arthrose (arthrosis), is paraphrased in arthrose que l’on ne voyait
pas sur la vert`ebre (arthrosis that was not seen on the vertebra), while the correct paraphrase from this
segment should be arthrose sur la vert`ebre (arthrosis on the vertebra);
4. the extraction is wrong and can provide no useful information.
</listItem>
<bodyText confidence="0.990417">
This evaluation allows to estimate precision of the results in three versions: strong precision Pstrong (only
the correct extractions are considered (extractions from 1)); weak precision Pweak (correct extractions
and extractions that need post-processing are considered (extractions from 1 and 3)); rate of incorrect
extractions %incorrect (the percentage of the incorrect extractions is computed (extractions from 4)).
</bodyText>
<page confidence="0.984845">
98
</page>
<sectionHeader confidence="0.999727" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.99426">
4.1 The morphological analysis of terms
</subsectionHeader>
<bodyText confidence="0.999997357142857">
We generate the morphological analysis for 218 single words from the anatomy semantic type, 1,789 dis-
order words and 1,023 procedure words: over 70% of words are morphologically analyzed. Among these
words, we observe compounds (myocardique) and words formed with affixes (e.g. r´eadaptation derived
from adaptation, derived in its turn from adapter). The remaining words may be simple (e.g. abc`es
(abscess), l`epre (leprosy), cicatrice (scar)) or contain bases and affixes that are not managed by D´erif (e.g.
pneumostrongylose (pneumostrongylosis), lagophtalmie (lagophthalmos), n´ecatorose (necatorosis)). Among the
generated decompositions by D´erif, we can find some cases with ambiguous decomposition that occur
when medical terms can be decomposed in several possible ways, among which only one is semantically
correct. For instance, posturographie (posturography) is decomposed into: [post [[uro N*] [graphie N*]
NOM] NOM], which may be glossed as control during the period which follows the therapy done on
the urinary system. From the formal point of view, such decomposition is very possible, although it is
weak semantically. For the term posturographie, the right decomposition is: [[posturo N*] [graphie
N*] NOM], which is related to the definition of the optimal body position when walking or sitting. As
indicated above, some terms (e.g. p´erianal) can be incorrectly “translated” in French.
</bodyText>
<subsectionHeader confidence="0.9906">
4.2 The preprocessing of corpora
</subsectionHeader>
<bodyText confidence="0.99999425">
Our main difficulty at this step is related to the processing of forum messages and to their segmentation
into sentences. In addition to possible and frequent spelling and grammatical errors, forum messages
have also a very specific punctuation, which may be missing or convey personal feelings and emotions.
This seriously impedes the possibility to provide the correct segmentation in sentences, and means that,
because of the missing punctuation, the mapping of decomposed terms with corpora may be done with
bigger text segments in which the semantic relations between the mapped components may be weak or
non-existent, and provide incorrect extractions. We plan to combine the current method with the syntactic
analysis in order to ensure that stronger syntactic and semantic relations exist between the components.
</bodyText>
<subsectionHeader confidence="0.980415">
4.3 The extraction of paraphrases and their evaluation
</subsectionHeader>
<bodyText confidence="0.999783625">
We present the results on extraction of sentences and paraphrases from the corpora processed. In Table
2, for the three semantic types of terms (anatomy ana., disorders dis., and procedures pro.) from each
corpus, we indicate the following information: the number of different sentences extracted (sentences),
yje number of different terms (uniq. terms), the number of correct paraphrases (correct), the number
of paraphrases that are possibly correct (pos. correct), the number of paraphrases which morphological
analysis and “translation” should be improved (morph. ana.), and the number of incorrect paraphrases
(incorrect). The last three lines indicate the precision values: strong precision (Pstrong), weak precision
(Pweak) and incorrect extractions (%incorrect).
</bodyText>
<table confidence="0.904100545454545">
Number of Wiki pro LesDiab DiabDoct ana HT pro Dos
ana dis ana dis pro ana dis pro dis ana dis pro
sentences 1238 4003 999 15 71 10 721 2901 564 246 1233 678 42 708 30
uniq. terms 93 382 154 7 30 5 35 204 48 29 133 42 13 44 13
correct 469 1571 364 3 32 4 227 1189 67 114 637 38 12 466 13
pos. correct 270 868 93 3 7 - 40 332 5 10 85 9 3 98 2
morph. ana. 41 155 323 1 2 6 100 3 394 22 - 591 2 1 12
incorrect 462 1424 220 8 30 - 354 1 98 100 511 40 25 135 3
Pstrong 38 39 36 20 45 40 32 40 12 46 52 6 29 66 43
Pweak 60 61 46 40 55 40 37 52 13 50 59 7 36 80 50
%incorrect 40 39 54 53 42 0 49 47 17 41 41 41 59 20 10
</table>
<tableCaption confidence="0.999433">
Table 2: Results on the paraphrases extracted and evaluated.
</tableCaption>
<page confidence="0.998676">
99
</page>
<bodyText confidence="0.9999745625">
From the data presented in Table 2, we can propose several observations: (1) the Wiki corpus, that is
not the largest in our dataset, provides the largest number of extractions (sentences and unique terms);
(2) among the three semantic types (anatomy, disorders and procedures), the number of paraphrases ex-
tracted for disorders is the largest in all corpora; (3) the largest set of paraphrases, that suffer from the
incorrect morphological decomposition or “translation”, is obtained for the procedure terms. According
to these observations, Pstrong ranges between 20 to 46% for anatomy, 39 and 66% for disorders, and 6 to
43 for procedures. The PTAJeak values, that takes into account the paraphrases that need post-processing,
show the increase by 0 to 28% by comparison with the Pstrong values. The %incorrect values indicate
that anatomy terms show the largest rate (40 to 59%) of incorrect paraphrases: it is possible that the
anatomy terms present the lowest rate of compositionality. The incorrect paraphrases are between 20
and 47 among the disorder terms, and between 0 to 54 among the procedure terms. The syntactic anal-
ysis may help to improve the current results. On the whole, the proposed method allows to extract the
paraphrases for 722 different terms from the corpora processed. Within the evaluated set of extractions,
these paraphrases are correct for 273 terms; while 343 terms are provided with correct paraphrases and
paraphrases that need to be post-processed. Most of the extracted paraphrases are noun phrases, and, at
a lesser extent, verb phrases. We present some examples of the correct paraphrases extracted:
</bodyText>
<listItem confidence="0.884200375">
- dorsalgie (dorsalgia): douleur dans le dos (pain in the back)
- my´elocyte (myelocyte): cellules dans la moelle osseuse (cells of the bone marrow)
- lombalgie (lombalgia): douleurs dans les reins (pain in kidney)
- gastralgie (gastralgia): douleurs a` l’estomac (stomach pain)
- desmorrhexie (desmorrhexia): rupture des ligaments (ligamentous rupture)
- h´epatite (hepatitis): inflammation du foie (liver inflammation)
We can find several types of paraphrases that suffer from incorrect decomposition or “translation”:
• syringomy´elie (syringomyelia) is currently “translated” in moelle (marrow or spinal cord) and canal (canal).
This term means a disorder in which a cyst or cavity forms within the spinal cord. We assume that
a more correct “translation” of this term should be: moelle (marrow or spinal cord) and cavit´e (cavity);
• sous-dural is “translated” in sous (sub) and dur (hard). The term is related to specific space in brain
that can be opened by the separation of the arachnoid mater from the dura mater. Concerning its
“translation”, we assume that dure-mire (dura mater) should be used instead of dur (hard). Besides,
the names of anatomical locations often remains difficult to understand. We assume that even when
terms are decomposed and “translated” correctly, the paraphrases for such terms may be not suitable
for laymen: other types of explanations (e.g. schemes or pictures) should be used instead;
• hyper´emie (hyperaemia) is “translated” in hyper and sang (blood). The term means the increase of
blood flow to different tissues in the body. This term is not fully compositional because the notion
of tissues is absent, while necessary for its understanding. The proposed extractions for this term
mainly come from corpora related to diabetes, in which hyper and hypo are often used in relation
with the hyperglycemia or hypoglycemia. This means that hyper should be “translated” with other
words, such as increase or elevated;
• h´et´erotopie is translated in autre (another) and endroit (place). The term means the displacement of an
organ from its normal position and that [an organ] is found in another place than the one expected.
</listItem>
<bodyText confidence="0.928713">
This term brings no correct candidates for paraphrases because: it is not fully compositional and its
“translation” provides very common words widely used in the corpora.
Among the incorrect extractions we can find: (1) more terms with non-compositional semantics (such
as ost´eodermie (osteoderm), causalgie (causalgia), ad´enoide (adenoid), or xanthochromie (xanthochromia)) for
which the extracted paraphrases capture only part of the meaning; and (2) extractions that must be con-
troled by the syntactic analysis (e.g. petite boule de peau qui a sortie entre l’ongle et... (small skinball
that appeared between the nail and...) for micronychie (micronychia)) to make them more grammatical. Para-
phrases extracted from the Wiki corpus cover larger range of medical terms, while those extracted from
</bodyText>
<page confidence="0.982347">
100
</page>
<bodyText confidence="0.999607">
fora dedicated to a given medical topics are redundant. On the whole, we can consider that the currently
proposed method allows extracting interesting candidates as the paraphrases of technical terms, that are
indeed much easier to understand than the technical terms by themselves.
If we compare the obtained results with those presented in previous work, we can observe that:
</bodyText>
<listItem confidence="0.918661625">
• we extract paraphrases for larger number of terms: 343 terms with correct and possibly correct
paraphrases (722 terms with paraphrases in total) in our work against a total of 65 and 82 in (Del´eger
and Zweigenbaum, 2008), 109 in (Cartoni and Del´eger, 2011), and 152 in (Elhadad and Sutaria,
2007). In our work, the terms may receive more than one paraphrase;
• the precision values we obtain are comparable with those indicated in previous work: 67% and 60%
in (Del´eger and Zweigenbaum, 2008), 66% in (Cartoni and Del´eger, 2011), and 58% in (Elhadad
and Sutaria, 2007);
• in the cited work, the content of the corpora is explored but no reference is done to the set of terms
</listItem>
<bodyText confidence="0.827358">
expected to be found. Because we work with a termset, we can compute the recall. If we consider the
terms that can be analyzed morphologically (3,030 terms), and for which we can find the paraphrases
with the proposed method, the recall value is close to 10% with the correct paraphrases (299 terms),
and to 24% with all the paraphrases extracted (722 terms). Yet, it is not sure that all of the terms, that
have been analyzed morphologically, can be provided with paraphrases in the corpora processed.
Besides, we should not forget that the nature of compounds and the decomposition of terms into com-
ponents also mean that specific semantic relations exist between these components (Namer and Zweigen-
baum, 2004; Booij, 2010). These are inherent to the syntactic constructions extracted. The characteristics
of these relations will be described and modeled in future work.
</bodyText>
<sectionHeader confidence="0.989264" genericHeader="conclusions">
5 Conclusions and Future work
</sectionHeader>
<bodyText confidence="0.9999958">
We propose to exploit social media texts in order to detect paraphrases for technical medical terms,
concentrating particularly on neoclassical compounds (e.g., myocardial, cholecystectomy, galactose,
acromegaly). The work is done in French. The method relies on the morphological analysis of terms,
on the “translation” of the components of terms in modern French words (e.g. {card, heart}), and on
the projection of these words on corpora. The method allows extracting correct and possibly correct
paraphrases for up to 343 technical terms. For covering larger set of terms, additional corpora must be
treated. The extracted paraphrases are easier to understand than the original technical terms. Moreover,
the semantic relations among the components, although non explicated, are conveyed by the paraphrases.
We can consider that the method proves to be efficient and promising for the creation of lexicon suit-
able for the simplification of medical texts. Besides, the purpose of the method is to cover neoclassical
compound terms that are usually non treated with automatic approaches, as they do not present clear
formal similarity with their paraphrases. One of the difficulties we have currently is related to the lack of
constrains on the extracted segments. In future work, we plan to apply the syntactic analysis for parsing
the extracted sentences. Another possibility is to compute the probability for a given paraphrase to be
correct, which can rely for instance on frequency of the extracted paraphrases, on their syntactic struc-
ture, etc. In order to make the extraction of paraphrases more exhaustive, we will apply the method to
other corpora and we will use additional resources (synonyms, associative resources) for performing the
approximate mapping of paraphrases. In future work, we will take into account syntactically complex
terms and not only simple words. The very objective of our work is to exploit and test the resource
created for the simplification of medical texts.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998736333333333">
The authors acknowledge the support of the Universit´e Paris 13 (project BQR Bonus Quality Research,
2011), the support of the MESHS Lille projet ´Emergent CoMeTe, and the support of the French Agence
Nationale de la Recherche (ANR) and the DGA, under the Tecsan grant ANR-11-TECS-012.
</bodyText>
<page confidence="0.998529">
101
</page>
<sectionHeader confidence="0.988813" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998317133333333">
AMA. 1999. Health literacy: report of the council on scientific affairs. Ad hoc committee on health literacy for
the council on scientific affairs, American Medical Association. JAMA, 281(6):552–7.
D Amiot and G Dal. 2005. Integrating combining forms into a lexeme-based morphology. In Mediterranean
Morphology Meeting (MMM5), pages 323–336.
M Amoia and M Romanelli. 2012. Sb: mmsystem - using decompositional semantics for lexical simplification.
In *SEM 2012, pages 482–486, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.
GK Berland, MN Elliott, LS Morales, JI Algazy, RL Kravitz, MS Broder, DE Kanouse, JA Munoz, JA Puyol,
M Lara, KE Watkins, H Yang, and EA McGlynn. 2001. Health information on the internet. accessibility,
quality, and readability in english ans spanish. JAMA, 285(20):2612–2621.
Geert Booij. 2010. Construction Morphology. Oxford University Press, Oxford.
B Cartoni and L Del´eger. 2011. Dcouverte de patrons paraphrastiques en corpus comparable: une approche base
sur les n-grammes. In TALN.
J Chmielik and N Grabar. 2011. D´etection de la sp´ecialisation scientifique et technique des documents
biom´edicaux grˆace aux informations morphologiques. TAL, 51(2):151–179.
L Del´eger and P Zweigenbaum. 2008. Paraphrase acquisition from comparable medical corpora of specialized
and lay texts. In AMIA 2008, pages 146–50.
William H. Dubay. 2004. The principles of readability. Impact Information. Available at
http://almacenplantillasweb.es/wp-content/uploads/2009/11/The-Principles-of-Readability.pdf.
N Elhadad and K Sutaria. 2007. Mining a lexicon of technical terms and lay equivalents. In BioNLP, pages 49–56.
Gunther Eysenbach. 2007. Poverty, human development, and the role of ehealth. J Med Internet Res, 9(4):e34.
S Fern´andez-Silva, J Freixa, and MT Cabr´e. 2011. A proposed method for analysing the dynamics of cognition
through term variation. Terminology, 17(1):49–73.
R Flesch. 1948. A new readability yardstick. Journal of Applied Psychology, 23:221–233.
T Franc¸ois and C Fairon. 2013. Les apports du TAL a` la lisibilit´e du franc¸ais langue ´etrang`ere. TAL, 54(1):171–
202.
T Franc¸ois. 2011. Les apports du traitements automatique du langage la lisibilit du franais langue trangre. Phd
thesis, Universit Catholique de Louvain, Louvain.
L Goeuriot, N Grabar, and B Daille. 2007. Caract´erisation des discours scientifique et vulgaris´e en franc¸ais,
japonais et russe. In TALN, pages 93–102.
R Gunning. 1973. The art of clear writing. McGraw Hill, New York, NY.
Udo Hahn, Martin Honeck, Michael Piotrowsky, and Stefan Schulz. 2001. Subword segmentation - leveling out
morphological variations for medical document retrieval. In AMIA, 229-33.
Darren Hargrave, Ute Bartels, Loretta Lau, Carlos Esquembre, and ´Eric Bouffet. 2003. ´evaluation de la qualit´e
de l’information m´edicale francophone accessible au public sur internet : application aux tumeurs c´er´ebrales de
l’enfant. Bulletin du Cancer, 90(7):650–5.
C Iacobini. 1997. Distinguishing derivational prefixes from initial combining forms. In First mediterranean
conference of morphology, Mytilene, Island of Lesbos, Greece, septembre.
SK Jauhar and L Specia. 2012. Uow-shef: Simplex – lexical simplicity ranking based on contextual and psycholin-
guistic features. In *SEM 2012, pages 477–481, Montr´eal, Canada, 7-8 June. Association for Computational
Linguistics.
A Johannsen, H Martinez, S Klerke, and A Søgaard. 2012. Emnlp@cph: Is frequency all there is to simplicity?
In *SEM 2012, pages 408–412, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.
D Kokkinakis and M Toporowska Gronostaj. 2006. Comparing lay and professional language in cardiovascular
disorders corpora. In Australia Pham T., James Cook University, editor, WSEAS Transactions on BIOLOGY
and BIOMEDICINE, pages 429–437.
</reference>
<page confidence="0.977897">
102
</page>
<reference confidence="0.999274727272727">
Sanja Kusec. 2004. Les sites web relatifs au diab`ete, sont-ils lisibles ? Dib`ete et soci´et´e, 49(3):46–48.
G Leroy, S Helmreich, J Cowie, T Miller, and W Zheng. 2008. Evaluating online health information: Beyond
readability formulas. In AMIA 2008, pages 394–8.
AL Ligozat, C Grouin, A Garcia-Fernandez, and D Bernhard. 2012. Annlor: A naive notation-system for lexical
outputs ranking. In *SEM 2012, pages 487–492.
DA Lindberg, BL Humphreys, and AT McCray. 1993. The unified medical language system. Methods Inf Med,
32(4):281–291.
Aur´elien Max, Houda Bouamor, and Anne Vilnat. 2012. Generalizing sub-sentential paraphrase acquisition across
original signal type of text pairs. In EMNLP, pages 721–31.
A McCray. 2005. Promoting health literacy. J of Am Med Infor Ass, 12:152–163.
T Miller, G Leroy, S Chatterjee, J Fan, and B Thoms. 2007. A classifier to evaluate language specificity of medical
documents. In HICSS, pages 134–140.
Fiammetta Namer and Pierre Zweigenbaum. 2004. Acquiring meaning for French medical terminology: contri-
bution of morphosemantics. In Annual Symposium of the American Medical Informatics Association (AMIA),
San-Francisco.
F Namer. 2003. Automatiser l’analyse morpho-s´emantique non affixale: le syst`eme D´eriF. Cahiers de Gram-
maire, 28:31–48.
F Namer. 2009. Morphologie, Lexique et TAL : l’analyseur D´eriF. TIC et Sciences cognitives. Hermes Sciences
Publishing, London.
Oregon Evidence-based Practice Center. 2008. Barriers and drivers of health information technology use for the
elderly, chronically ill, and underserved. Technical report, Agency for healthcare research and quality.
V Patel, T Branch, and J Arocha. 2002. Errors in interpreting quantities as procedures : The case of pharmaceutical
labels. International journal of medical informatics, 65(3):193–211.
M Poprat, K Mark´o, and U Hahn. 2006. A language classifier that automatically divides medical documents
for experts and health care consumers. In MIE 2006 - Proceedings of the XX International Congress of the
European Federation for Medical Informatics, pages 503–508, Maastricht.
H Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In ICNMLP, pages 44–49, Manchester,
UK.
R Sinha. 2012. Unt-simprank: Systems for lexical simplification ranking. In *SEM 2012, pages 493–496,
Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.
L Specia, SK Jauhar, and R Mihalcea. 2012. Semeval-2012 task 1: English lexical simplification. In *SEM 2012,
pages 347–355.
TM Tran, H Chekroud, P Thiery, and A Julienne. 2009. Internet et soins : un tiers invisible dans la relation
m´edecine/patient ? Ethica Clinica, 53:34–43.
Y Wang. 2006. Automatic recognition of text difficulty from consumers health information. In IEEE, editor,
Computer-Based Medical Systems, pages 131–136.
MV Williams, RM Parker, DW Baker, NS Parikh, K Pitkin, WC Coates, and JR Nurss. 1995. Inadequate func-
tional health literacy among patients at two public hospitals. JAMA, 274(21):1677–82.
QT Zeng and T Tse. 2006. Exploring and developing consumer health vocabularies. JAMIA, 13:24–29.
Q Zeng-Treiler, H Kim, S Goryachev, A Keselman, L Slaugther, and CA Smith. 2007. Text characteristics of
clinical reports and their implications for the readability of personal health records. In MEDINFO, pages 1117–
1121, Brisbane, Australia.
Pierre Zweigenbaum and Natalia Grabar. 2003. Corpus-based associations provide additional morphological
variants to medical terminologies. In AMIA.
</reference>
<page confidence="0.999289">
103
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.003054">
<title confidence="0.996352">Unsupervised method for the acquisition of general language paraphrases for medical compounds</title>
<author confidence="0.993204">Natalia Thierry</author>
<affiliation confidence="0.9437185">CNRS UMR 8163 LIMSI-CNRS, BP133, Universit´e Lille Universit´e Paris</affiliation>
<address confidence="0.958001">59653 Villeneuve d’Ascq, Sorbonne Paris Cit´e,</address>
<email confidence="0.703478">natalia.grabar@univ-lille3.frhamon@limsi.fr</email>
<abstract confidence="0.96233374609375">information is widespread in modern society research, medical blogs, clinical documents, TV and radio broadcast, novels). Moreover, everybody’s life may be concerned with medical problems. However, the medical field conveys very specific and often notions infarction, cholecystectomy, abdominal strangulated hernia, that are difficult to understand by lay people. We propose an automatic method based on the morphological analysis of terms and on text mining for finding the paraphrases of technical terms. Analysis of the results and their evaluation indicate that we can find correct paraphrases for 343 terms. Depending on the semantics of the terms, error rate of the extractions ranges between 0 and 59%. This kind of resources is useful for several Natural Language Proapplications information extraction, text simplification, question and answering). 1 Background Medical and health information is widespread in the modern society in light of pressing health concerns and of maintaining of healthy lifestyles. Besides, it is also available through modern media: scientific research, articles, medical blogs and fora, clinical documents, TV and radio broadcast, novels, discussion fora, epidemiological alerts, etc. Still, availability of medical and health information does not guarantee its easy and correct understanding by lay people. The medical field conveys indeed very technical notions, such as in example (1). infarction, cholecystectomy, erythredema polyneuropathy, acromegaly, galactosemia Although technical, these notions are nevertheless important for patients (AMA, 1999; McCray, 2005; Eysenbach, 2007; Oregon Evidence-based Practice Center, 2008). It has been shown that in several situations such notions cannot be correctly understood by patients: the steps needed for the medication preparing and use (Patel et al., 2002); the instructions on drugs from patient package inserts, and the information delivered in informed consensus and health brochures: it appears that among the 2,600 patients recruited in two hospitals, 26% to 60% cannot manage information available in these sources (Williams et al., 1995); health information in different languages (English, Spanish, French) provided in websites created for patients require high reading level (Berland et al., 2001; Hargrave et al., 2003; Kusec, 2004) and remains difficult to manage by patients, which can be negative for the communication between patients and medical professionals, and the healthcare process (Tran et al., 2009). This situation sets the context of our work. Our objective is to propose method for the automatic acquisition of paraphrases for technical medical notions. More particularly, we propose to concentrate on terms and their words that show neoclassical compounding word formation (Booij, 2010; Iacobini, 1997; Amiot and Dal, 2005), such as in the example (1). Such words often involve Latin and Greek roots or bases, which makes them more difficult to understand, as such words must be decomposed first (see examples (2) and (3)). To our knowledge, this kind of approach has not been applied for the acquisition of laymen paraphrases. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer added by the organizers. License details: 94 of the 4th International Workshop on Computational pages Dublin, Ireland, August 23 2014. myocardial formed with Latin Greek cholecystectomy formed with Greek Latin and Greek removal) Our work is related to the following research topics: • The readability studies the ease in which text can be understood. Two kinds of readabilmeasures are distinguished: classical and computational 2011). Classical measures are usually based on number of characters and/or syllables in words, sentences or documents and on linear regression models (Flesch, 1948; Gunning, 1973; Dubay, 2004). Computational measures, that are more recent, can involve vectorial models and a great variety of descriptors. These descriptors, usually specific to the texts processed, are for instance: combination of classical measures with medical terminologies (Kokkinakis and Toporowska Gronostaj, 2006); n-grams of characters (Poprat et al., 2006); discursive descriptors (Goeuriot et al., 2007); lexicon (Miller et al., 2007); morphological descriptors (Chmielik and Grabar, 2011); combination of various descriptors (Wang, Zeng-Treiler et al., 2007; Leroy et al., 2008; and Fairon, 2013). Lexical The lexical simplification helps to make text easier to understand. Lexical of texts in English has been addressed during the Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, mutual information and frequency of words (Jauhar and Specia, 2012); frequency in Wikipedia, word length, n-grams of characters and of words, syntactic complexity of documents (Johannsen et al., 2012); n-grams, frequency in Wikipedia, n-Google grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). Dedicated The building of resources suitable for performing the simplification is another related research topics. Such resources are mainly two-fold lexica in which specialized and non-specialized vocabularies are aligned (in examples (4) to (6), the technical terms are followed by their non-technical equivalents). The first initiative of the kind appeared with the collaborative effort Consumer Health Vocabulary (Zeng and Tse, 2006) (examples in (4)). One of the methods was applied to the most frequently occurring medical queries aligned to the UMLS (Unified Medical Language System) concepts (Lindberg et al., 1993). Another work exploited a small corpus and several statistical association measures for building aligned lexicon with technical terms from the UMLS and their lay equivalents (Elhadad and Sutaria, 2007). Similar work in other languages followed. In French, researchers proposed methods for the acquisition of syntactic variation (Del´eger and Zweigenbaum, 2008; Cartoni and Del´eger, 2011) from comparable specialized and non-specialized corpora, that led to the detection of verb/noun variations (examples in (5)) and a larger set of syntactic variations (examples in (6)). Besides, research on the acquisition of terminological variation (Hahn et al., 2001), synonymy (Fern´andez-Silva et al., 2011) and paraphrasing (Max et al., 2012) is also relevant to outline the topics. (4) of (5) de a` la de de maladie of the condition) (6) de la the les sont the son the emp 95 Our work is closely related to the building of resources dedicated to the lexical simplification. Our is to propose method for paraphrasing the technical medical terms compounds) in expressions that are easier to understand by lay people. This aspect is seldom addressed: we can observe that only some examples in (4) are concerned with the paraphrasing of technical and compound terms We work with the French data. Contrary to previous work, we do not use comparable corpora with technical and non-technical texts. Instead, we exploit terms from an existing medical terminology and corpora built from social media sources. We assume that this kind of corpora may provide lay people equivalents for technical terms. We also rely on the morphological of technical terms. The expected result is to obtain pairs like of gall In the following, we start with the presentation of the resources used (section 2), we present then the steps of the methodology (section 3). We describe and discuss the obtained results (section 4) and conclude with some directions for future work (section 5). 2 Resources 2.1 Medical terms The material processed is issued from the French part of the UMLS. It provides syntactically simple terms contain one word only and syntactically complex terms that contain more than one word Syntactically complex terms are segmented in words. Each term is associated to semantic types. When a given word receives more than one semantic type, a manual post-processing allows to disambiguate it: each word is assigned to one semantic type only. Among the semantic types available, we consider the three most common in the medical practice to which the lay people are the exposed: Anatomy (616 words): describe human body anatomy Disorders words): describe medical problems and their signs infarction, Procedures (1,271 describe procedures which may be performed by medical staff to detect or cure disorders In what follows, be exchangeable and mean either the graphical unit provided by the segmentation, or the medical notion. 2.2 Corpora Wiki LesDiab DiabDoct HT Dos Number of pages/threads 17,525 6,939 387,435 67,652 8,319 Number of articles/messages 17,525 1,438 22,431 12,588 1,124 Number of words 4,326,880 624,571 35,059,868 6,788,361 836,520 Table 1: Size of the corpora exploited. We use several corpora collected from the social media sources (their sizes are indicated in Table 1): Wiki French Wikipedia articles downloaded in February 2014, of which we keep those that categorized under the medical category de la LesDiab collected from the discussion forum posted between June and July 2013. It is dedicated to diabetes; DiabDoct collected in June 2011 from the discussion forum HT collected in May 2013 from the discussion forum Dos collected in May 2013 from the discussion forum de dos sujet-1.htm sujet-1.htm sujet-1.htm 96 contains encyclopaedic information on several medical notions from Wikipedia. Thanks to the collaborative writing of the articles, these contain mostly correct information about the topics Other corpora are collected from the dedicated fora or backache). We assume that people involved in these discussions may show low, middle or high degree of knowledge about the disorders and related notions. We expect that all our corpora are written in a simple style and that they contain paraphrases of technical terms. From Table 1, we can observe that the corpora vary in size. 3 Methodology for the automatic acquisition of paraphrases for medical compounds The methodology is designed for analyzing the neoclassical medical compounds and for searching their paraphrases in corpora. In our approach, the paraphrases may occur alone, such as without being accompanied by their technical compounds In this case, we need first to acquire the knowledge needed for their automatic detection. We propose to rely on the morphological analysis of terms. The method is composed of four main steps: the processing of terms, the processing of corpora, the extraction of layman paraphrases for technical terms, and the evaluation of the extractions. 3.1 The processing of medical terms To reach the morphological information on terms we apply three specific processing: tagging and lemmatization of terms. terms are morpho-syntactically tagged lemmatized with French (Schmid, 1994). The morpho-syntactic tagging is done in context of the terms. If a given word receives more than one tag, the most frequent one is kept. At this step, we obtain term lemmas with their part-of-speech tags, such as in example (7). analysis. lemmas are then morphologically analyzed with 2009). This tool performs the analysis of lemmas in order to detect their morphological structure, to decompose them into their components (bases and affixes), and to semantically analyze their structure. We give some examples of the morphological analysis in (8). N*] [carde N*] NOM] ique ADJ] N*] [ectomie N*] NOM] [[neur N*] [pathie N*] NOM] NOM] N*] [m´egal N*] ie NOM] NOM] [´em N*] ie NOM] computed bases and affixes are associated with syntactic categories ADJ, When a given base is suppletive (does not exist in modern French but was borrowed from Latin or Greek the most probable category N* nouns, adjectives). For the analysis of that this word contains the suppletive noun bases N* N* and the affix We can observe that some bases can decomposed further galactose The words that contain more than one base are considered to be compounds and are processed in the further steps of the method. of morphological components with French words. bases are “translated” with words from modern French. We use for this resource built in previous work (Zweigenbaum and Grabar, 2003; Namer, 2003) (see some examples in (9)). biliaire 97 words can remain technical while other components totally their technical meaning 3.2 The processing of corpora The corpora are first segmented in words and sentences. Then, we also perform morpho-syntactic tagging lemmatization with French. 3.3 The extraction of layman paraphrases corresponding to technical terms French words corresponding to the morphological decomposition of terms (examples in (9)) are projected on corpora in order to extract sentences and their segments which can provide the layman paraphrases for the corresponding technical terms. Sentences that contain the translated French words are extracted as candidates for proposing the paraphrases. Additionally, the segments delimited by these words are also extracted. We consider the co-occurrence of the words issued from the morphological decomposition in sliding graphical window of In the experiments presented, the window size fixed to 10 words. Smaller or larger windows show less performance. causes de tachycardie ventriculaire sont superposables a` celles des extrasystoles ventricinfarctus du myocarde, insuffisance cardiaque, hypertrophie du muscledu coeuret prolapsus de la valve mitrale. sentence in (10) contains words underlined in the example, that correspond to morphological components of examples in (9)). For this reason, this sentence is as well as the segment delimited by these two words du coeur 3.4 The evaluation The objective of the evaluation is to assess whether the proposed method is valid for the acquisition of paraphrases for technical medical terms. The obtained results are evaluated manually by a computer scientist with no training in biomedicine, but with background in computational linguistics and morphology. We analyze the candidates for paraphrases from several points of view: Are the French words corresponding to the components extracted correctly? Do these French words provide valid candidates for paraphrases? How easy are these paraphrases to be understood by laymen or by non-experts medicine? During the evaluation related to the second point these French words provide valid for we distinguish four situations: the extraction is correct: myocardique in du coeur 2. the extraction suffers from the incorrect morphological decomposition or from the wrong “transin French: p´erianal “translated” in year as it The of this last word not correct and should be Because of the “translation”, we collect a lot of incorrect segments like de 30 ans 30 the extraction should be post-processed but contains the correct paraphrase: in is paraphrased in que l’on ne voyait sur la vert`ebre that was not seen on the while the correct paraphrase from this should be sur la vert`ebre on the 4. the extraction is wrong and can provide no useful information. evaluation allows to estimate precision of the results in three versions: strong precision (only correct extractions are considered (extractions from 1)); weak precision (correct extractions and extractions that need post-processing are considered (extractions from 1 and 3)); rate of incorrect (the percentage of the incorrect extractions is computed (extractions from 4)). 98 4 Results and Discussion 4.1 The morphological analysis of terms We generate the morphological analysis for 218 single words from the anatomy semantic type, 1,789 disorder words and 1,023 procedure words: over 70% of words are morphologically analyzed. Among these we observe compounds and words formed with affixes r´eadaptation derived in its turn from The remaining words may be simple abc`es or contain bases and affixes that are not managed by Among the decompositions by we can find some cases with ambiguous decomposition that occur when medical terms can be decomposed in several possible ways, among which only one is semantically For instance, decomposed into: [[uro N*] [graphie N*] which may be glossed as during the period which follows the therapy done on urinary From the formal point of view, such decomposition is very possible, although it is semantically. For the term the right decomposition is: N*] [graphie which is related to the of the optimal body position when walking or As above, some terms can be incorrectly “translated” in French. 4.2 The preprocessing of corpora Our main difficulty at this step is related to the processing of forum messages and to their segmentation into sentences. In addition to possible and frequent spelling and grammatical errors, forum messages have also a very specific punctuation, which may be missing or convey personal feelings and emotions. This seriously impedes the possibility to provide the correct segmentation in sentences, and means that, because of the missing punctuation, the mapping of decomposed terms with corpora may be done with bigger text segments in which the semantic relations between the mapped components may be weak or non-existent, and provide incorrect extractions. We plan to combine the current method with the syntactic analysis in order to ensure that stronger syntactic and semantic relations exist between the components. 4.3 The extraction of paraphrases and their evaluation We present the results on extraction of sentences and paraphrases from the corpora processed. In Table for the three semantic types of terms (anatomy disorders and procedures from each we indicate the following information: the number of different sentences extracted number of different terms the number of correct paraphrases the number paraphrases that are possibly correct the number of paraphrases which morphological and “translation” should be improved and the number of incorrect paraphrases The last three lines indicate the precision values: strong precision weak precision and incorrect extractions Number of Wiki pro ana dis pro DiabDoct ana HT dis pro Dos ana dis ana dis pro ana dis pro sentences 1238 4003 999 15 71 10 721 2901 564 246 1233 678 42 708 30 uniq. terms 93 382 154 7 30 5 35 204 48 29 133 42 13 44 13 correct 469 1571 364 3 32 4 227 1189 67 114 637 38 12 466 13 pos. correct 270 868 93 3 7 - 40 332 5 10 85 9 3 98 2 morph. ana. 41 155 323 1 2 6 100 3 394 22 - 591 2 1 12</abstract>
<phone confidence="0.7773585">incorrect 462 1424 220 8 30 - 354 1 98 100 511 40 25 135 3 38 39 36 20 45 40 32 40 12 46 52 6 29 66 43 60 61 46 40 55 40 37 52 13 50 59 7 36 80 50 40 39 54 53 42 0 49 47 17 41 41 41 59 20 10</phone>
<abstract confidence="0.995412666666667">Table 2: Results on the paraphrases extracted and evaluated. 99 the data presented in Table 2, we can propose several observations: that is not the largest in our dataset, provides the largest number of extractions (sentences and unique terms); the three semantic types (anatomy, disorders and procedures), the number of paraphrases exfor disorders is the largest in all corpora; largest set of paraphrases, that suffer from the incorrect morphological decomposition or “translation”, is obtained for the procedure terms. According these observations, between 20 to 46% for anatomy, 39 and 66% for disorders, and 6 to for procedures. The that takes into account the paraphrases that need post-processing, the increase by 0 to 28% by comparison with the The values indicate that anatomy terms show the largest rate (40 to 59%) of incorrect paraphrases: it is possible that the anatomy terms present the lowest rate of compositionality. The incorrect paraphrases are between 20 and 47 among the disorder terms, and between 0 to 54 among the procedure terms. The syntactic analysis may help to improve the current results. On the whole, the proposed method allows to extract the paraphrases for 722 different terms from the corpora processed. Within the evaluated set of extractions, these paraphrases are correct for 273 terms; while 343 terms are provided with correct paraphrases and paraphrases that need to be post-processed. Most of the extracted paraphrases are noun phrases, and, at a lesser extent, verb phrases. We present some examples of the correct paraphrases extracted: dans le dos in the back) dans la moelle osseuse of the bone marrow) dans les reins in kidney) a` l’estomac pain) des ligaments rupture) du foie inflammation) We can find several types of paraphrases that suffer from incorrect decomposition or “translation”: syringomy´elie currently “translated” in or spinal cord) This term means a disorder in which a cyst or cavity forms within the spinal cord. We assume that more correct “translation” of this term should be: or spinal cord) sous-dural “translated” in The term is related to specific space in brain that can be opened by the separation of the arachnoid mater from the dura mater. Concerning its we assume that mater) be used instead of Besides, the names of anatomical locations often remains difficult to understand. We assume that even when terms are decomposed and “translated” correctly, the paraphrases for such terms may be not suitable laymen: other types of explanations or pictures) should be used instead; hyper´emie “translated” in The term means the increase of blood flow to different tissues in the body. This term is not fully compositional because the notion of tissues is absent, while necessary for its understanding. The proposed extractions for this term come from corpora related to diabetes, in which often used in relation the This means that be “translated” with other such as h´et´erotopie translated in The term means the displacement of an organ from its normal position and that [an organ] is found in another place than the one expected. This term brings no correct candidates for paraphrases because: it is not fully compositional and its “translation” provides very common words widely used in the corpora. the incorrect extractions we can find: terms with non-compositional semantics (such or for the extracted paraphrases capture only part of the meaning; and that must be conby the syntactic analysis petite boule de peau qui a sortie entre l’ongle et... skinball appeared between the nail and...) to make them more grammatical. Paraextracted from the cover larger range of medical terms, while those extracted from 100 fora dedicated to a given medical topics are redundant. On the whole, we can consider that the currently proposed method allows extracting interesting candidates as the paraphrases of technical terms, that are indeed much easier to understand than the technical terms by themselves. If we compare the obtained results with those presented in previous work, we can observe that: • we extract paraphrases for larger number of terms: 343 terms with correct and possibly correct paraphrases (722 terms with paraphrases in total) in our work against a total of 65 and 82 in (Del´eger and Zweigenbaum, 2008), 109 in (Cartoni and Del´eger, 2011), and 152 in (Elhadad and Sutaria, 2007). In our work, the terms may receive more than one paraphrase; • the precision values we obtain are comparable with those indicated in previous work: 67% and 60% in (Del´eger and Zweigenbaum, 2008), 66% in (Cartoni and Del´eger, 2011), and 58% in (Elhadad and Sutaria, 2007); • in the cited work, the content of the corpora is explored but no reference is done to the set of terms expected to be found. Because we work with a termset, we can compute the recall. If we consider the terms that can be analyzed morphologically (3,030 terms), and for which we can find the paraphrases with the proposed method, the recall value is close to 10% with the correct paraphrases (299 terms), and to 24% with all the paraphrases extracted (722 terms). Yet, it is not sure that all of the terms, that have been analyzed morphologically, can be provided with paraphrases in the corpora processed. Besides, we should not forget that the nature of compounds and the decomposition of terms into components also mean that specific semantic relations exist between these components (Namer and Zweigenbaum, 2004; Booij, 2010). These are inherent to the syntactic constructions extracted. The characteristics of these relations will be described and modeled in future work. 5 Conclusions and Future work We propose to exploit social media texts in order to detect paraphrases for technical medical terms, particularly on neoclassical compounds cholecystectomy, galactose, The work is done in French. The method relies on the morphological analysis of terms, the “translation” of the components of terms in modern French words and on the projection of these words on corpora. The method allows extracting correct and possibly correct paraphrases for up to 343 technical terms. For covering larger set of terms, additional corpora must be treated. The extracted paraphrases are easier to understand than the original technical terms. Moreover, the semantic relations among the components, although non explicated, are conveyed by the paraphrases. We can consider that the method proves to be efficient and promising for the creation of lexicon suitable for the simplification of medical texts. Besides, the purpose of the method is to cover neoclassical compound terms that are usually non treated with automatic approaches, as they do not present clear formal similarity with their paraphrases. One of the difficulties we have currently is related to the lack of constrains on the extracted segments. In future work, we plan to apply the syntactic analysis for parsing the extracted sentences. Another possibility is to compute the probability for a given paraphrase to be correct, which can rely for instance on frequency of the extracted paraphrases, on their syntactic structure, etc. In order to make the extraction of paraphrases more exhaustive, we will apply the method to other corpora and we will use additional resources (synonyms, associative resources) for performing the approximate mapping of paraphrases. In future work, we will take into account syntactically complex terms and not only simple words. The very objective of our work is to exploit and test the resource created for the simplification of medical texts.</abstract>
<note confidence="0.980879322916667">Acknowledgments The authors acknowledge the support of the Universit´e Paris 13 (project BQR Bonus Quality Research, 2011), the support of the MESHS Lille projet ´Emergent CoMeTe, and the support of the French Agence Nationale de la Recherche (ANR) and the DGA, under the Tecsan grant ANR-11-TECS-012. 101 References AMA. 1999. Health literacy: report of the council on scientific affairs. Ad hoc committee on health literacy for council on scientific affairs, American Medical Association. 281(6):552–7. Amiot and G Dal. 2005. Integrating combining forms into a lexeme-based morphology. In Meeting pages 323–336. M Amoia and M Romanelli. 2012. Sb: mmsystem using decompositional semantics for lexical simplification. pages 482–486, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics. GK Berland, MN Elliott, LS Morales, JI Algazy, RL Kravitz, MS Broder, DE Kanouse, JA Munoz, JA Puyol, M Lara, KE Watkins, H Yang, and EA McGlynn. 2001. Health information on the internet. accessibility, and readability in english ans spanish. 285(20):2612–2621. Booij. 2010. Oxford University Press, Oxford. B Cartoni and L Del´eger. 2011. Dcouverte de patrons paraphrastiques en corpus comparable: une approche base les n-grammes. In J Chmielik and N Grabar. 2011. D´etection de la sp´ecialisation scientifique et technique des documents grˆace aux informations morphologiques. 51(2):151–179. L Del´eger and P Zweigenbaum. 2008. Paraphrase acquisition from comparable medical corpora of specialized lay texts. In pages 146–50. H. Dubay. 2004. The principles of readability. Available at Elhadad and K Sutaria. 2007. Mining a lexicon of technical terms and lay equivalents. In pages 49–56. Eysenbach. 2007. Poverty, human development, and the role of ehealth. Med Internet 9(4):e34. S Fern´andez-Silva, J Freixa, and MT Cabr´e. 2011. A proposed method for analysing the dynamics of cognition term variation. 17(1):49–73. Flesch. 1948. A new readability yardstick. of Applied 23:221–233. and C Fairon. 2013. Les apports du TAL a` la lisibilit´e du langue ´etrang`ere. 54(1):171– 202. 2011. apports du traitements automatique du langage la lisibilit du franais langue Phd thesis, Universit Catholique de Louvain, Louvain. Goeuriot, N Grabar, and B Daille. 2007. Caract´erisation des discours scientifique et vulgaris´e en et russe. In pages 93–102. Gunning. 1973. art of clear McGraw Hill, New York, NY. Udo Hahn, Martin Honeck, Michael Piotrowsky, and Stefan Schulz. 2001. Subword segmentation leveling out variations for medical document retrieval. In 229-33. Darren Hargrave, Ute Bartels, Loretta Lau, Carlos Esquembre, and ´Eric Bouffet. 2003. ´evaluation de la qualit´e de l’information m´edicale francophone accessible au public sur internet : application aux tumeurs c´er´ebrales de du 90(7):650–5. Iacobini. 1997. Distinguishing derivational prefixes from initial combining forms. In mediterranean of Mytilene, Island of Lesbos, Greece, septembre. SK Jauhar and L Specia. 2012. Uow-shef: Simplex – lexical simplicity ranking based on contextual and psycholinfeatures. In pages 477–481, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics. A Johannsen, H Martinez, S Klerke, and A Søgaard. 2012. Emnlp@cph: Is frequency all there is to simplicity? pages 408–412, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics. D Kokkinakis and M Toporowska Gronostaj. 2006. Comparing lay and professional language in cardiovascular corpora. In Australia Pham T., James Cook University, editor, Transactions on BIOLOGY pages 429–437. 102 Kusec. 2004. Les sites web relatifs au diab`ete, sont-ils lisibles ? et 49(3):46–48. G Leroy, S Helmreich, J Cowie, T Miller, and W Zheng. 2008. Evaluating online health information: Beyond formulas. In pages 394–8. AL Ligozat, C Grouin, A Garcia-Fernandez, and D Bernhard. 2012. Annlor: A naive notation-system for lexical ranking. In pages 487–492. Lindberg, BL Humphreys, and AT McCray. 1993. The unified medical language system. Inf 32(4):281–291. Aur´elien Max, Houda Bouamor, and Anne Vilnat. 2012. Generalizing sub-sentential paraphrase acquisition across signal type of text pairs. In pages 721–31. McCray. 2005. Promoting health literacy. of Am Med Infor 12:152–163. T Miller, G Leroy, S Chatterjee, J Fan, and B Thoms. 2007. A classifier to evaluate language specificity of medical In pages 134–140. Fiammetta Namer and Pierre Zweigenbaum. 2004. Acquiring meaning for French medical terminology: contriof morphosemantics. In Symposium of the American Medical Informatics Association San-Francisco. Namer. 2003. Automatiser l’analyse morpho-s´emantique non affixale: le syst`eme D´eriF. de Gram- 28:31–48. Namer. 2009. Lexique et TAL : l’analyseur D´eriF. TIC et Sciences Hermes Sciences Publishing, London. Oregon Evidence-based Practice Center. 2008. Barriers and drivers of health information technology use for the elderly, chronically ill, and underserved. Technical report, Agency for healthcare research and quality. V Patel, T Branch, and J Arocha. 2002. Errors in interpreting quantities as procedures : The case of pharmaceutical journal of medical 65(3):193–211. M Poprat, K Mark´o, and U Hahn. 2006. A language classifier that automatically divides medical documents experts and health care consumers. In 2006 - Proceedings of the XX International Congress of the Federation for Medical pages 503–508, Maastricht. Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In pages 44–49, Manchester, UK. Sinha. 2012. Unt-simprank: Systems for lexical simplification ranking. In pages 493–496, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics. Specia, SK Jauhar, and R Mihalcea. 2012. Semeval-2012 task 1: English lexical simplification. In pages 347–355. TM Tran, H Chekroud, P Thiery, and A Julienne. 2009. Internet et soins : un tiers invisible dans la relation ? 53:34–43. Y Wang. 2006. Automatic recognition of text difficulty from consumers health information. In IEEE, editor, Medical pages 131–136. MV Williams, RM Parker, DW Baker, NS Parikh, K Pitkin, WC Coates, and JR Nurss. 1995. Inadequate funchealth literacy among patients at two public hospitals. 274(21):1677–82. Zeng and T Tse. 2006. Exploring and developing consumer health vocabularies. 13:24–29. Q Zeng-Treiler, H Kim, S Goryachev, A Keselman, L Slaugther, and CA Smith. 2007. Text characteristics of reports and their implications for the readability of personal health records. In pages 1117– 1121, Brisbane, Australia. Pierre Zweigenbaum and Natalia Grabar. 2003. Corpus-based associations provide additional morphological to medical terminologies. In 103</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>AMA</author>
</authors>
<title>Health literacy: report of the council on scientific affairs. Ad hoc committee on health literacy for the council on scientific affairs,</title>
<date>1999</date>
<journal>American Medical Association. JAMA,</journal>
<volume>281</volume>
<issue>6</issue>
<contexts>
<context position="2012" citStr="AMA, 1999" startWordPosition="275" endWordPosition="276">ealthy lifestyles. Besides, it is also available through modern media: scientific research, articles, medical blogs and fora, clinical documents, TV and radio broadcast, novels, discussion fora, epidemiological alerts, etc. Still, availability of medical and health information does not guarantee its easy and correct understanding by lay people. The medical field conveys indeed very technical notions, such as in example (1). (1) myocardial infarction, cholecystectomy, erythredema polyneuropathy, acromegaly, galactosemia Although technical, these notions are nevertheless important for patients (AMA, 1999; McCray, 2005; Eysenbach, 2007; Oregon Evidence-based Practice Center, 2008). It has been shown that in several situations such notions cannot be correctly understood by patients: the steps needed for the medication preparing and use (Patel et al., 2002); the instructions on drugs from patient package inserts, and the information delivered in informed consensus and health brochures: it appears that among the 2,600 patients recruited in two hospitals, 26% to 60% cannot manage information available in these sources (Williams et al., 1995); health information in different languages (English, Spa</context>
</contexts>
<marker>AMA, 1999</marker>
<rawString>AMA. 1999. Health literacy: report of the council on scientific affairs. Ad hoc committee on health literacy for the council on scientific affairs, American Medical Association. JAMA, 281(6):552–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Amiot</author>
<author>G Dal</author>
</authors>
<title>Integrating combining forms into a lexeme-based morphology.</title>
<date>2005</date>
<booktitle>In Mediterranean Morphology Meeting (MMM5),</booktitle>
<pages>323--336</pages>
<contexts>
<context position="3261" citStr="Amiot and Dal, 2005" startWordPosition="458" endWordPosition="461">sites created for patients require high reading level (Berland et al., 2001; Hargrave et al., 2003; Kusec, 2004) and remains difficult to manage by patients, which can be negative for the communication between patients and medical professionals, and the healthcare process (Tran et al., 2009). This situation sets the context of our work. Our objective is to propose method for the automatic acquisition of paraphrases for technical medical notions. More particularly, we propose to concentrate on terms and their words that show neoclassical compounding word formation (Booij, 2010; Iacobini, 1997; Amiot and Dal, 2005), such as in the example (1). Such words often involve Latin and Greek roots or bases, which makes them more difficult to understand, as such words must be decomposed first (see examples (2) and (3)). To our knowledge, this kind of approach has not been applied for the acquisition of laymen paraphrases. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 94 Proceedings of the 4th International Workshop on Computational Terminology, page</context>
</contexts>
<marker>Amiot, Dal, 2005</marker>
<rawString>D Amiot and G Dal. 2005. Integrating combining forms into a lexeme-based morphology. In Mediterranean Morphology Meeting (MMM5), pages 323–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Amoia</author>
<author>M Romanelli</author>
</authors>
<title>Sb: mmsystem - using decompositional semantics for lexical simplification.</title>
<date>2012</date>
<booktitle>In *SEM 2012,</booktitle>
<pages>482--486</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="6019" citStr="Amoia and Romanelli, 2012" startWordPosition="864" endWordPosition="867">utes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, mutual information and frequency of words (Jauhar and Specia, 2012); frequency in Wikipedia, word length, n-grams of characters and of words, syntactic complexity of documents (Johannsen et al., 2012); n-grams, frequency in Wikipedia, n-Google grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). • Dedicated resources. The building of resources suitable for performing the simplification is another related research topics. Such resources are mainly two-fold lexica in which specialized and non-specialized vocabularies are aligned (in examples (4) to (6), the technical terms are followed by their non-technical equivalents). The first initiative of the kind appeared with the collaborative effort Consumer Health Vocabulary (Zeng and Tse, 2006) (examples in (4)). One of the methods was applied to the most frequently occurring medical queries aligned to the UMLS (Unified Medical Language Sy</context>
</contexts>
<marker>Amoia, Romanelli, 2012</marker>
<rawString>M Amoia and M Romanelli. 2012. Sb: mmsystem - using decompositional semantics for lexical simplification. In *SEM 2012, pages 482–486, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GK Berland</author>
<author>MN Elliott</author>
<author>LS Morales</author>
<author>JI Algazy</author>
<author>RL Kravitz</author>
<author>MS Broder</author>
<author>JA Munoz DE Kanouse</author>
<author>JA Puyol</author>
<author>M Lara</author>
<author>KE Watkins</author>
<author>H Yang</author>
<author>EA McGlynn</author>
</authors>
<title>Health information on the internet. accessibility, quality, and readability in english ans spanish.</title>
<date>2001</date>
<journal>JAMA,</journal>
<volume>285</volume>
<issue>20</issue>
<marker>Berland, Elliott, Morales, Algazy, Kravitz, Broder, DE Kanouse, Puyol, Lara, Watkins, Yang, McGlynn, 2001</marker>
<rawString>GK Berland, MN Elliott, LS Morales, JI Algazy, RL Kravitz, MS Broder, DE Kanouse, JA Munoz, JA Puyol, M Lara, KE Watkins, H Yang, and EA McGlynn. 2001. Health information on the internet. accessibility, quality, and readability in english ans spanish. JAMA, 285(20):2612–2621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geert Booij</author>
</authors>
<title>Construction Morphology.</title>
<date>2010</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="3223" citStr="Booij, 2010" startWordPosition="454" endWordPosition="455">nish, French) provided in websites created for patients require high reading level (Berland et al., 2001; Hargrave et al., 2003; Kusec, 2004) and remains difficult to manage by patients, which can be negative for the communication between patients and medical professionals, and the healthcare process (Tran et al., 2009). This situation sets the context of our work. Our objective is to propose method for the automatic acquisition of paraphrases for technical medical notions. More particularly, we propose to concentrate on terms and their words that show neoclassical compounding word formation (Booij, 2010; Iacobini, 1997; Amiot and Dal, 2005), such as in the example (1). Such words often involve Latin and Greek roots or bases, which makes them more difficult to understand, as such words must be decomposed first (see examples (2) and (3)). To our knowledge, this kind of approach has not been applied for the acquisition of laymen paraphrases. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 94 Proceedings of the 4th International Works</context>
<context position="29612" citStr="Booij, 2010" startWordPosition="4444" endWordPosition="4445">n be analyzed morphologically (3,030 terms), and for which we can find the paraphrases with the proposed method, the recall value is close to 10% with the correct paraphrases (299 terms), and to 24% with all the paraphrases extracted (722 terms). Yet, it is not sure that all of the terms, that have been analyzed morphologically, can be provided with paraphrases in the corpora processed. Besides, we should not forget that the nature of compounds and the decomposition of terms into components also mean that specific semantic relations exist between these components (Namer and Zweigenbaum, 2004; Booij, 2010). These are inherent to the syntactic constructions extracted. The characteristics of these relations will be described and modeled in future work. 5 Conclusions and Future work We propose to exploit social media texts in order to detect paraphrases for technical medical terms, concentrating particularly on neoclassical compounds (e.g., myocardial, cholecystectomy, galactose, acromegaly). The work is done in French. The method relies on the morphological analysis of terms, on the “translation” of the components of terms in modern French words (e.g. {card, heart}), and on the projection of thes</context>
</contexts>
<marker>Booij, 2010</marker>
<rawString>Geert Booij. 2010. Construction Morphology. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Cartoni</author>
<author>L Del´eger</author>
</authors>
<title>Dcouverte de patrons paraphrastiques en corpus comparable: une approche base sur les n-grammes.</title>
<date>2011</date>
<booktitle>In TALN.</booktitle>
<marker>Cartoni, Del´eger, 2011</marker>
<rawString>B Cartoni and L Del´eger. 2011. Dcouverte de patrons paraphrastiques en corpus comparable: une approche base sur les n-grammes. In TALN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chmielik</author>
<author>N Grabar</author>
</authors>
<title>D´etection de la sp´ecialisation scientifique et technique des documents biom´edicaux grˆace aux informations morphologiques.</title>
<date>2011</date>
<journal>TAL,</journal>
<volume>51</volume>
<issue>2</issue>
<contexts>
<context position="4987" citStr="Chmielik and Grabar, 2011" startWordPosition="706" endWordPosition="709">sed on number of characters and/or syllables in words, sentences or documents and on linear regression models (Flesch, 1948; Gunning, 1973; Dubay, 2004). Computational measures, that are more recent, can involve vectorial models and a great variety of descriptors. These descriptors, usually specific to the texts processed, are for instance: combination of classical measures with medical terminologies (Kokkinakis and Toporowska Gronostaj, 2006); n-grams of characters (Poprat et al., 2006); discursive descriptors (Goeuriot et al., 2007); lexicon (Miller et al., 2007); morphological descriptors (Chmielik and Grabar, 2011); combination of various descriptors (Wang, 2006; Zeng-Treiler et al., 2007; Leroy et al., 2008; Franc¸ois and Fairon, 2013). • Lexical simplification. The lexical simplification helps to make text easier to understand. Lexical simplification of texts in English has been addressed during the SemEval 2012 challengea. Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted f</context>
</contexts>
<marker>Chmielik, Grabar, 2011</marker>
<rawString>J Chmielik and N Grabar. 2011. D´etection de la sp´ecialisation scientifique et technique des documents biom´edicaux grˆace aux informations morphologiques. TAL, 51(2):151–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Del´eger</author>
<author>P Zweigenbaum</author>
</authors>
<title>Paraphrase acquisition from comparable medical corpora of specialized and lay texts. In AMIA</title>
<date>2008</date>
<pages>146--50</pages>
<marker>Del´eger, Zweigenbaum, 2008</marker>
<rawString>L Del´eger and P Zweigenbaum. 2008. Paraphrase acquisition from comparable medical corpora of specialized and lay texts. In AMIA 2008, pages 146–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Dubay</author>
</authors>
<title>The principles of readability. Impact Information. Available at http://almacenplantillasweb.es/wp-content/uploads/2009/11/The-Principles-of-Readability.pdf.</title>
<date>2004</date>
<contexts>
<context position="4513" citStr="Dubay, 2004" startWordPosition="644" endWordPosition="645">4. (2) myocardial is formed with Latin myo (muscle) and Greek cardia (heart) (3) cholecystectomy is formed with Greek chole (bile), Latin cystis (bladder), and Greek ectomy (surgical removal) Our work is related to the following research topics: • Readability. The readability studies the ease in which text can be understood. Two kinds of readability measures are distinguished: classical and computational (Franc¸ois, 2011). Classical measures are usually based on number of characters and/or syllables in words, sentences or documents and on linear regression models (Flesch, 1948; Gunning, 1973; Dubay, 2004). Computational measures, that are more recent, can involve vectorial models and a great variety of descriptors. These descriptors, usually specific to the texts processed, are for instance: combination of classical measures with medical terminologies (Kokkinakis and Toporowska Gronostaj, 2006); n-grams of characters (Poprat et al., 2006); discursive descriptors (Goeuriot et al., 2007); lexicon (Miller et al., 2007); morphological descriptors (Chmielik and Grabar, 2011); combination of various descriptors (Wang, 2006; Zeng-Treiler et al., 2007; Leroy et al., 2008; Franc¸ois and Fairon, 2013). </context>
</contexts>
<marker>Dubay, 2004</marker>
<rawString>William H. Dubay. 2004. The principles of readability. Impact Information. Available at http://almacenplantillasweb.es/wp-content/uploads/2009/11/The-Principles-of-Readability.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Elhadad</author>
<author>K Sutaria</author>
</authors>
<title>Mining a lexicon of technical terms and lay equivalents.</title>
<date>2007</date>
<journal>J Med Internet Res,</journal>
<booktitle>In BioNLP,</booktitle>
<volume>9</volume>
<issue>4</issue>
<pages>49--56</pages>
<contexts>
<context position="6859" citStr="Elhadad and Sutaria, 2007" startWordPosition="988" endWordPosition="991">ocabularies are aligned (in examples (4) to (6), the technical terms are followed by their non-technical equivalents). The first initiative of the kind appeared with the collaborative effort Consumer Health Vocabulary (Zeng and Tse, 2006) (examples in (4)). One of the methods was applied to the most frequently occurring medical queries aligned to the UMLS (Unified Medical Language System) concepts (Lindberg et al., 1993). Another work exploited a small corpus and several statistical association measures for building aligned lexicon with technical terms from the UMLS and their lay equivalents (Elhadad and Sutaria, 2007). Similar work in other languages followed. In French, researchers proposed methods for the acquisition of syntactic variation (Del´eger and Zweigenbaum, 2008; Cartoni and Del´eger, 2011) from comparable specialized and non-specialized corpora, that led to the detection of verb/noun variations (examples in (5)) and a larger set of syntactic variations (examples in (6)). Besides, research on the acquisition of terminological variation (Hahn et al., 2001), synonymy (Fern´andez-Silva et al., 2011) and paraphrasing (Max et al., 2012) is also relevant to outline the topics. (4) {myocardial infarcti</context>
<context position="28506" citStr="Elhadad and Sutaria, 2007" startWordPosition="4253" endWordPosition="4256">nt. On the whole, we can consider that the currently proposed method allows extracting interesting candidates as the paraphrases of technical terms, that are indeed much easier to understand than the technical terms by themselves. If we compare the obtained results with those presented in previous work, we can observe that: • we extract paraphrases for larger number of terms: 343 terms with correct and possibly correct paraphrases (722 terms with paraphrases in total) in our work against a total of 65 and 82 in (Del´eger and Zweigenbaum, 2008), 109 in (Cartoni and Del´eger, 2011), and 152 in (Elhadad and Sutaria, 2007). In our work, the terms may receive more than one paraphrase; • the precision values we obtain are comparable with those indicated in previous work: 67% and 60% in (Del´eger and Zweigenbaum, 2008), 66% in (Cartoni and Del´eger, 2011), and 58% in (Elhadad and Sutaria, 2007); • in the cited work, the content of the corpora is explored but no reference is done to the set of terms expected to be found. Because we work with a termset, we can compute the recall. If we consider the terms that can be analyzed morphologically (3,030 terms), and for which we can find the paraphrases with the proposed m</context>
</contexts>
<marker>Elhadad, Sutaria, 2007</marker>
<rawString>N Elhadad and K Sutaria. 2007. Mining a lexicon of technical terms and lay equivalents. In BioNLP, pages 49–56. Gunther Eysenbach. 2007. Poverty, human development, and the role of ehealth. J Med Internet Res, 9(4):e34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Fern´andez-Silva</author>
<author>J Freixa</author>
<author>MT Cabr´e</author>
</authors>
<title>A proposed method for analysing the dynamics of cognition through term variation.</title>
<date>2011</date>
<journal>Terminology,</journal>
<volume>17</volume>
<issue>1</issue>
<pages>23--221</pages>
<marker>Fern´andez-Silva, Freixa, Cabr´e, 2011</marker>
<rawString>S Fern´andez-Silva, J Freixa, and MT Cabr´e. 2011. A proposed method for analysing the dynamics of cognition through term variation. Terminology, 17(1):49–73. R Flesch. 1948. A new readability yardstick. Journal of Applied Psychology, 23:221–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Franc¸ois</author>
<author>C Fairon</author>
</authors>
<title>Les apports du TAL a` la lisibilit´e du franc¸ais langue ´etrang`ere.</title>
<date>2013</date>
<journal>TAL,</journal>
<volume>54</volume>
<issue>1</issue>
<pages>202</pages>
<marker>Franc¸ois, Fairon, 2013</marker>
<rawString>T Franc¸ois and C Fairon. 2013. Les apports du TAL a` la lisibilit´e du franc¸ais langue ´etrang`ere. TAL, 54(1):171– 202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Franc¸ois</author>
</authors>
<title>Les apports du traitements automatique du langage la lisibilit du franais langue trangre. Phd thesis,</title>
<date>2011</date>
<institution>Universit Catholique de Louvain, Louvain.</institution>
<marker>Franc¸ois, 2011</marker>
<rawString>T Franc¸ois. 2011. Les apports du traitements automatique du langage la lisibilit du franais langue trangre. Phd thesis, Universit Catholique de Louvain, Louvain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Goeuriot</author>
<author>N Grabar</author>
<author>B Daille</author>
</authors>
<title>Caract´erisation des discours scientifique et vulgaris´e en franc¸ais, japonais et russe.</title>
<date>2007</date>
<booktitle>In TALN,</booktitle>
<pages>93--102</pages>
<contexts>
<context position="4901" citStr="Goeuriot et al., 2007" startWordPosition="695" endWordPosition="698">: classical and computational (Franc¸ois, 2011). Classical measures are usually based on number of characters and/or syllables in words, sentences or documents and on linear regression models (Flesch, 1948; Gunning, 1973; Dubay, 2004). Computational measures, that are more recent, can involve vectorial models and a great variety of descriptors. These descriptors, usually specific to the texts processed, are for instance: combination of classical measures with medical terminologies (Kokkinakis and Toporowska Gronostaj, 2006); n-grams of characters (Poprat et al., 2006); discursive descriptors (Goeuriot et al., 2007); lexicon (Miller et al., 2007); morphological descriptors (Chmielik and Grabar, 2011); combination of various descriptors (Wang, 2006; Zeng-Treiler et al., 2007; Leroy et al., 2008; Franc¸ois and Fairon, 2013). • Lexical simplification. The lexical simplification helps to make text easier to understand. Lexical simplification of texts in English has been addressed during the SemEval 2012 challengea. Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simpl</context>
</contexts>
<marker>Goeuriot, Grabar, Daille, 2007</marker>
<rawString>L Goeuriot, N Grabar, and B Daille. 2007. Caract´erisation des discours scientifique et vulgaris´e en franc¸ais, japonais et russe. In TALN, pages 93–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gunning</author>
</authors>
<title>The art of clear writing.</title>
<date>1973</date>
<publisher>McGraw Hill,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="4499" citStr="Gunning, 1973" startWordPosition="642" endWordPosition="643">, August 23 2014. (2) myocardial is formed with Latin myo (muscle) and Greek cardia (heart) (3) cholecystectomy is formed with Greek chole (bile), Latin cystis (bladder), and Greek ectomy (surgical removal) Our work is related to the following research topics: • Readability. The readability studies the ease in which text can be understood. Two kinds of readability measures are distinguished: classical and computational (Franc¸ois, 2011). Classical measures are usually based on number of characters and/or syllables in words, sentences or documents and on linear regression models (Flesch, 1948; Gunning, 1973; Dubay, 2004). Computational measures, that are more recent, can involve vectorial models and a great variety of descriptors. These descriptors, usually specific to the texts processed, are for instance: combination of classical measures with medical terminologies (Kokkinakis and Toporowska Gronostaj, 2006); n-grams of characters (Poprat et al., 2006); discursive descriptors (Goeuriot et al., 2007); lexicon (Miller et al., 2007); morphological descriptors (Chmielik and Grabar, 2011); combination of various descriptors (Wang, 2006; Zeng-Treiler et al., 2007; Leroy et al., 2008; Franc¸ois and F</context>
</contexts>
<marker>Gunning, 1973</marker>
<rawString>R Gunning. 1973. The art of clear writing. McGraw Hill, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udo Hahn</author>
<author>Martin Honeck</author>
<author>Michael Piotrowsky</author>
<author>Stefan Schulz</author>
</authors>
<title>Subword segmentation - leveling out morphological variations for medical document retrieval.</title>
<date>2001</date>
<booktitle>In AMIA,</booktitle>
<pages>229--33</pages>
<contexts>
<context position="7316" citStr="Hahn et al., 2001" startWordPosition="1054" endWordPosition="1057">nd several statistical association measures for building aligned lexicon with technical terms from the UMLS and their lay equivalents (Elhadad and Sutaria, 2007). Similar work in other languages followed. In French, researchers proposed methods for the acquisition of syntactic variation (Del´eger and Zweigenbaum, 2008; Cartoni and Del´eger, 2011) from comparable specialized and non-specialized corpora, that led to the detection of verb/noun variations (examples in (5)) and a larger set of syntactic variations (examples in (6)). Besides, research on the acquisition of terminological variation (Hahn et al., 2001), synonymy (Fern´andez-Silva et al., 2011) and paraphrasing (Max et al., 2012) is also relevant to outline the topics. (4) {myocardial infarction, heart attack}, {abortion, termination of pregnancy}, {acrodynia, pink disease} (5) {consommation r´eguli`ere, consommer de fac¸on r´eguli`ere} (regular use), {gˆene a` la lecture, ˆeche de lire} (reading difficulty), {´evolution de l’affection, la maladie ´evolue} (evolution of the condition) (6) {retard de cicatrisation, retarder la cicatrisation} (delay the healing), {apports caloriques, apport en calories} (calorie supply), {calculer les doses, d</context>
</contexts>
<marker>Hahn, Honeck, Piotrowsky, Schulz, 2001</marker>
<rawString>Udo Hahn, Martin Honeck, Michael Piotrowsky, and Stefan Schulz. 2001. Subword segmentation - leveling out morphological variations for medical document retrieval. In AMIA, 229-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darren Hargrave</author>
<author>Ute Bartels</author>
<author>Loretta Lau</author>
<author>Carlos Esquembre</author>
<author>´Eric Bouffet</author>
</authors>
<title>evaluation de la qualit´e de l’information m´edicale francophone accessible au public sur internet : application aux tumeurs c´er´ebrales de l’enfant.</title>
<date>2003</date>
<journal>Bulletin du Cancer,</journal>
<volume>90</volume>
<issue>7</issue>
<contexts>
<context position="2739" citStr="Hargrave et al., 2003" startWordPosition="380" endWordPosition="383">everal situations such notions cannot be correctly understood by patients: the steps needed for the medication preparing and use (Patel et al., 2002); the instructions on drugs from patient package inserts, and the information delivered in informed consensus and health brochures: it appears that among the 2,600 patients recruited in two hospitals, 26% to 60% cannot manage information available in these sources (Williams et al., 1995); health information in different languages (English, Spanish, French) provided in websites created for patients require high reading level (Berland et al., 2001; Hargrave et al., 2003; Kusec, 2004) and remains difficult to manage by patients, which can be negative for the communication between patients and medical professionals, and the healthcare process (Tran et al., 2009). This situation sets the context of our work. Our objective is to propose method for the automatic acquisition of paraphrases for technical medical notions. More particularly, we propose to concentrate on terms and their words that show neoclassical compounding word formation (Booij, 2010; Iacobini, 1997; Amiot and Dal, 2005), such as in the example (1). Such words often involve Latin and Greek roots o</context>
</contexts>
<marker>Hargrave, Bartels, Lau, Esquembre, Bouffet, 2003</marker>
<rawString>Darren Hargrave, Ute Bartels, Loretta Lau, Carlos Esquembre, and ´Eric Bouffet. 2003. ´evaluation de la qualit´e de l’information m´edicale francophone accessible au public sur internet : application aux tumeurs c´er´ebrales de l’enfant. Bulletin du Cancer, 90(7):650–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Iacobini</author>
</authors>
<title>Distinguishing derivational prefixes from initial combining forms.</title>
<date>1997</date>
<booktitle>In First mediterranean conference of morphology, Mytilene, Island of Lesbos, Greece,</booktitle>
<pages>septembre.</pages>
<contexts>
<context position="3239" citStr="Iacobini, 1997" startWordPosition="456" endWordPosition="457"> provided in websites created for patients require high reading level (Berland et al., 2001; Hargrave et al., 2003; Kusec, 2004) and remains difficult to manage by patients, which can be negative for the communication between patients and medical professionals, and the healthcare process (Tran et al., 2009). This situation sets the context of our work. Our objective is to propose method for the automatic acquisition of paraphrases for technical medical notions. More particularly, we propose to concentrate on terms and their words that show neoclassical compounding word formation (Booij, 2010; Iacobini, 1997; Amiot and Dal, 2005), such as in the example (1). Such words often involve Latin and Greek roots or bases, which makes them more difficult to understand, as such words must be decomposed first (see examples (2) and (3)). To our knowledge, this kind of approach has not been applied for the acquisition of laymen paraphrases. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 94 Proceedings of the 4th International Workshop on Computati</context>
</contexts>
<marker>Iacobini, 1997</marker>
<rawString>C Iacobini. 1997. Distinguishing derivational prefixes from initial combining forms. In First mediterranean conference of morphology, Mytilene, Island of Lesbos, Greece, septembre.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SK Jauhar</author>
<author>L Specia</author>
</authors>
<title>Uow-shef: Simplex – lexical simplicity ranking based on contextual and psycholinguistic features.</title>
<date>2012</date>
<booktitle>In *SEM 2012,</booktitle>
<pages>477--481</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="5758" citStr="Jauhar and Specia, 2012" startWordPosition="826" endWordPosition="829">ion. The lexical simplification helps to make text easier to understand. Lexical simplification of texts in English has been addressed during the SemEval 2012 challengea. Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, mutual information and frequency of words (Jauhar and Specia, 2012); frequency in Wikipedia, word length, n-grams of characters and of words, syntactic complexity of documents (Johannsen et al., 2012); n-grams, frequency in Wikipedia, n-Google grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). • Dedicated resources. The building of resources suitable for performing the simplification is another related research topics. Such resources are mainly two-fold lexica in which specialized and non-specialized vocabularies are aligned (in examples (4) to (6), the technical terms are followed by their non-technical equivalents). The fi</context>
</contexts>
<marker>Jauhar, Specia, 2012</marker>
<rawString>SK Jauhar and L Specia. 2012. Uow-shef: Simplex – lexical simplicity ranking based on contextual and psycholinguistic features. In *SEM 2012, pages 477–481, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Johannsen</author>
<author>H Martinez</author>
<author>S Klerke</author>
<author>A Søgaard</author>
</authors>
<title>Emnlp@cph: Is frequency all there is to simplicity?</title>
<date>2012</date>
<booktitle>In *SEM 2012,</booktitle>
<pages>408--412</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="5891" citStr="Johannsen et al., 2012" startWordPosition="845" endWordPosition="848"> during the SemEval 2012 challengea. Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, mutual information and frequency of words (Jauhar and Specia, 2012); frequency in Wikipedia, word length, n-grams of characters and of words, syntactic complexity of documents (Johannsen et al., 2012); n-grams, frequency in Wikipedia, n-Google grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). • Dedicated resources. The building of resources suitable for performing the simplification is another related research topics. Such resources are mainly two-fold lexica in which specialized and non-specialized vocabularies are aligned (in examples (4) to (6), the technical terms are followed by their non-technical equivalents). The first initiative of the kind appeared with the collaborative effort Consumer Health Vocabulary (Zeng and Tse, 2006) (examples in (4)). </context>
</contexts>
<marker>Johannsen, Martinez, Klerke, Søgaard, 2012</marker>
<rawString>A Johannsen, H Martinez, S Klerke, and A Søgaard. 2012. Emnlp@cph: Is frequency all there is to simplicity? In *SEM 2012, pages 408–412, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kokkinakis</author>
<author>M Toporowska Gronostaj</author>
</authors>
<title>Comparing lay and professional language in cardiovascular disorders corpora.</title>
<date>2006</date>
<booktitle>WSEAS Transactions on BIOLOGY and BIOMEDICINE,</booktitle>
<pages>429--437</pages>
<editor>In Australia Pham T., James Cook University, editor,</editor>
<marker>Kokkinakis, Gronostaj, 2006</marker>
<rawString>D Kokkinakis and M Toporowska Gronostaj. 2006. Comparing lay and professional language in cardiovascular disorders corpora. In Australia Pham T., James Cook University, editor, WSEAS Transactions on BIOLOGY and BIOMEDICINE, pages 429–437. Sanja Kusec. 2004. Les sites web relatifs au diab`ete, sont-ils lisibles ? Dib`ete et soci´et´e, 49(3):46–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leroy</author>
<author>S Helmreich</author>
<author>J Cowie</author>
<author>T Miller</author>
<author>W Zheng</author>
</authors>
<title>Evaluating online health information: Beyond readability formulas. In AMIA</title>
<date>2008</date>
<pages>394--8</pages>
<contexts>
<context position="5082" citStr="Leroy et al., 2008" startWordPosition="720" endWordPosition="723">models (Flesch, 1948; Gunning, 1973; Dubay, 2004). Computational measures, that are more recent, can involve vectorial models and a great variety of descriptors. These descriptors, usually specific to the texts processed, are for instance: combination of classical measures with medical terminologies (Kokkinakis and Toporowska Gronostaj, 2006); n-grams of characters (Poprat et al., 2006); discursive descriptors (Goeuriot et al., 2007); lexicon (Miller et al., 2007); morphological descriptors (Chmielik and Grabar, 2011); combination of various descriptors (Wang, 2006; Zeng-Treiler et al., 2007; Leroy et al., 2008; Franc¸ois and Fairon, 2013). • Lexical simplification. The lexical simplification helps to make text easier to understand. Lexical simplification of texts in English has been addressed during the SemEval 2012 challengea. Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of sy</context>
</contexts>
<marker>Leroy, Helmreich, Cowie, Miller, Zheng, 2008</marker>
<rawString>G Leroy, S Helmreich, J Cowie, T Miller, and W Zheng. 2008. Evaluating online health information: Beyond readability formulas. In AMIA 2008, pages 394–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>AL Ligozat</author>
<author>C Grouin</author>
<author>A Garcia-Fernandez</author>
<author>D Bernhard</author>
</authors>
<title>Annlor: A naive notation-system for lexical outputs ranking.</title>
<date>2012</date>
<booktitle>In *SEM 2012,</booktitle>
<pages>487--492</pages>
<contexts>
<context position="5963" citStr="Ligozat et al., 2012" startWordPosition="855" endWordPosition="858"> word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, mutual information and frequency of words (Jauhar and Specia, 2012); frequency in Wikipedia, word length, n-grams of characters and of words, syntactic complexity of documents (Johannsen et al., 2012); n-grams, frequency in Wikipedia, n-Google grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). • Dedicated resources. The building of resources suitable for performing the simplification is another related research topics. Such resources are mainly two-fold lexica in which specialized and non-specialized vocabularies are aligned (in examples (4) to (6), the technical terms are followed by their non-technical equivalents). The first initiative of the kind appeared with the collaborative effort Consumer Health Vocabulary (Zeng and Tse, 2006) (examples in (4)). One of the methods was applied to the most frequently occurring medical </context>
</contexts>
<marker>Ligozat, Grouin, Garcia-Fernandez, Bernhard, 2012</marker>
<rawString>AL Ligozat, C Grouin, A Garcia-Fernandez, and D Bernhard. 2012. Annlor: A naive notation-system for lexical outputs ranking. In *SEM 2012, pages 487–492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DA Lindberg</author>
<author>BL Humphreys</author>
<author>AT McCray</author>
</authors>
<title>The unified medical language system.</title>
<date>1993</date>
<journal>Methods Inf Med,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="6657" citStr="Lindberg et al., 1993" startWordPosition="958" endWordPosition="961">sources. The building of resources suitable for performing the simplification is another related research topics. Such resources are mainly two-fold lexica in which specialized and non-specialized vocabularies are aligned (in examples (4) to (6), the technical terms are followed by their non-technical equivalents). The first initiative of the kind appeared with the collaborative effort Consumer Health Vocabulary (Zeng and Tse, 2006) (examples in (4)). One of the methods was applied to the most frequently occurring medical queries aligned to the UMLS (Unified Medical Language System) concepts (Lindberg et al., 1993). Another work exploited a small corpus and several statistical association measures for building aligned lexicon with technical terms from the UMLS and their lay equivalents (Elhadad and Sutaria, 2007). Similar work in other languages followed. In French, researchers proposed methods for the acquisition of syntactic variation (Del´eger and Zweigenbaum, 2008; Cartoni and Del´eger, 2011) from comparable specialized and non-specialized corpora, that led to the detection of verb/noun variations (examples in (5)) and a larger set of syntactic variations (examples in (6)). Besides, research on the </context>
</contexts>
<marker>Lindberg, Humphreys, McCray, 1993</marker>
<rawString>DA Lindberg, BL Humphreys, and AT McCray. 1993. The unified medical language system. Methods Inf Med, 32(4):281–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aur´elien Max</author>
<author>Houda Bouamor</author>
<author>Anne Vilnat</author>
</authors>
<title>Generalizing sub-sentential paraphrase acquisition across original signal type of text pairs.</title>
<date>2012</date>
<journal>J of Am Med Infor Ass,</journal>
<booktitle>In EMNLP,</booktitle>
<pages>721--31</pages>
<publisher>A</publisher>
<contexts>
<context position="7394" citStr="Max et al., 2012" startWordPosition="1065" endWordPosition="1068">echnical terms from the UMLS and their lay equivalents (Elhadad and Sutaria, 2007). Similar work in other languages followed. In French, researchers proposed methods for the acquisition of syntactic variation (Del´eger and Zweigenbaum, 2008; Cartoni and Del´eger, 2011) from comparable specialized and non-specialized corpora, that led to the detection of verb/noun variations (examples in (5)) and a larger set of syntactic variations (examples in (6)). Besides, research on the acquisition of terminological variation (Hahn et al., 2001), synonymy (Fern´andez-Silva et al., 2011) and paraphrasing (Max et al., 2012) is also relevant to outline the topics. (4) {myocardial infarction, heart attack}, {abortion, termination of pregnancy}, {acrodynia, pink disease} (5) {consommation r´eguli`ere, consommer de fac¸on r´eguli`ere} (regular use), {gˆene a` la lecture, ˆeche de lire} (reading difficulty), {´evolution de l’affection, la maladie ´evolue} (evolution of the condition) (6) {retard de cicatrisation, retarder la cicatrisation} (delay the healing), {apports caloriques, apport en calories} (calorie supply), {calculer les doses, doses sont calcul´ees} (calculate the dose), {efficacit´e est renforc´ee, renfo</context>
</contexts>
<marker>Max, Bouamor, Vilnat, 2012</marker>
<rawString>Aur´elien Max, Houda Bouamor, and Anne Vilnat. 2012. Generalizing sub-sentential paraphrase acquisition across original signal type of text pairs. In EMNLP, pages 721–31. A McCray. 2005. Promoting health literacy. J of Am Med Infor Ass, 12:152–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Miller</author>
<author>G Leroy</author>
<author>S Chatterjee</author>
<author>J Fan</author>
<author>B Thoms</author>
</authors>
<title>A classifier to evaluate language specificity of medical documents.</title>
<date>2007</date>
<booktitle>In HICSS,</booktitle>
<pages>134--140</pages>
<contexts>
<context position="4932" citStr="Miller et al., 2007" startWordPosition="700" endWordPosition="703">anc¸ois, 2011). Classical measures are usually based on number of characters and/or syllables in words, sentences or documents and on linear regression models (Flesch, 1948; Gunning, 1973; Dubay, 2004). Computational measures, that are more recent, can involve vectorial models and a great variety of descriptors. These descriptors, usually specific to the texts processed, are for instance: combination of classical measures with medical terminologies (Kokkinakis and Toporowska Gronostaj, 2006); n-grams of characters (Poprat et al., 2006); discursive descriptors (Goeuriot et al., 2007); lexicon (Miller et al., 2007); morphological descriptors (Chmielik and Grabar, 2011); combination of various descriptors (Wang, 2006; Zeng-Treiler et al., 2007; Leroy et al., 2008; Franc¸ois and Fairon, 2013). • Lexical simplification. The lexical simplification helps to make text easier to understand. Lexical simplification of texts in English has been addressed during the SemEval 2012 challengea. Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012</context>
</contexts>
<marker>Miller, Leroy, Chatterjee, Fan, Thoms, 2007</marker>
<rawString>T Miller, G Leroy, S Chatterjee, J Fan, and B Thoms. 2007. A classifier to evaluate language specificity of medical documents. In HICSS, pages 134–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fiammetta Namer</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Acquiring meaning for French medical terminology: contribution of morphosemantics.</title>
<date>2004</date>
<booktitle>In Annual Symposium of the American Medical Informatics Association (AMIA), San-Francisco.</booktitle>
<contexts>
<context position="29598" citStr="Namer and Zweigenbaum, 2004" startWordPosition="4439" endWordPosition="4443">we consider the terms that can be analyzed morphologically (3,030 terms), and for which we can find the paraphrases with the proposed method, the recall value is close to 10% with the correct paraphrases (299 terms), and to 24% with all the paraphrases extracted (722 terms). Yet, it is not sure that all of the terms, that have been analyzed morphologically, can be provided with paraphrases in the corpora processed. Besides, we should not forget that the nature of compounds and the decomposition of terms into components also mean that specific semantic relations exist between these components (Namer and Zweigenbaum, 2004; Booij, 2010). These are inherent to the syntactic constructions extracted. The characteristics of these relations will be described and modeled in future work. 5 Conclusions and Future work We propose to exploit social media texts in order to detect paraphrases for technical medical terms, concentrating particularly on neoclassical compounds (e.g., myocardial, cholecystectomy, galactose, acromegaly). The work is done in French. The method relies on the morphological analysis of terms, on the “translation” of the components of terms in modern French words (e.g. {card, heart}), and on the proj</context>
</contexts>
<marker>Namer, Zweigenbaum, 2004</marker>
<rawString>Fiammetta Namer and Pierre Zweigenbaum. 2004. Acquiring meaning for French medical terminology: contribution of morphosemantics. In Annual Symposium of the American Medical Informatics Association (AMIA), San-Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Namer</author>
</authors>
<title>Automatiser l’analyse morpho-s´emantique non affixale: le syst`eme D´eriF. Cahiers de Grammaire,</title>
<date>2003</date>
<pages>28--31</pages>
<contexts>
<context position="15189" citStr="Namer, 2003" startWordPosition="2194" endWordPosition="2195">at this word contains the suppletive noun bases myo N* (muscle) and carde N* (heart), and the affix -ique/ADJ. We can observe that some bases can be decomposed further (e.g. galactose in galact (milk) and ose (sugars), cholecystectomy in chole (bile) and cystis (bladder)). The words that contain more than one base are considered to be compounds and are processed in the further steps of the method. 3. Association of morphological components with French words. The bases are “translated” with words from modern French. We use for this resource built in previous work (Zweigenbaum and Grabar, 2003; Namer, 2003) (see some examples in (9)). (9) myocardique/A: myo=muscle (muscle), carde=coeur (heart) chol´ecystectomie/N: chol´ecysto=v´esicule biliaire (gall bladder), ectomie=ablation (removal) 97 polyneuropathie/N: poly=nombreux (several), neuro=nerf (nerve), pathie=maladie (disorder) acrom´egalie/N: acr=extr´emit´e (extremity), m´egal=grandeur (size) galactos´emie/N: galactose=galactose (galactose), ´em=sang (blood) Some words can remain technical (e.g., galactose, v´esicule biliaire), while other components totally lose their technical meaning (e.g. m´egal=grandeur (size), poly=nombreux (several)). 3</context>
</contexts>
<marker>Namer, 2003</marker>
<rawString>F Namer. 2003. Automatiser l’analyse morpho-s´emantique non affixale: le syst`eme D´eriF. Cahiers de Grammaire, 28:31–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Namer</author>
</authors>
<date>2009</date>
<booktitle>Morphologie, Lexique et TAL : l’analyseur D´eriF. TIC et Sciences cognitives. Hermes Sciences Publishing,</booktitle>
<location>London.</location>
<contexts>
<context position="13710" citStr="Namer, 2009" startWordPosition="1966" endWordPosition="1967">f terms. The terms are morpho-syntactically tagged and lemmatized with TreeTagger for French (Schmid, 1994). The morpho-syntactic tagging is done in context of the terms. If a given word receives more than one tag, the most frequent one is kept. At this step, we obtain term lemmas with their part-of-speech tags, such as in example (7). (7) myocardique/A (myocardial/A), chol´ecystectomie/N (cholecystectomy/N), polyneuropathie/N (polyneuropathy/N), acrom´egalie/N (acromegaly/N), galactos´emie/N (galactosemia/N) 2. Morphological analysis. The lemmas are then morphologically analyzed with D´eriF (Namer, 2009). This tool performs the analysis of lemmas in order to detect their morphological structure, to decompose them into their components (bases and affixes), and to semantically analyze their structure. We give some examples of the morphological analysis in (8). (8) myocardique/A: [[[myo N*] [carde N*] NOM] ique ADJ] chol´ecystectomie/N: [[chol´ecysto N*] [ectomie N*] NOM] polyneuropathie/N: [poly [[neur N*] [pathie N*] NOM] NOM] acrom´egalie/N: [[acr N*] [m´egal N*] ie NOM] galactos´emie/N: [[galactose NOM] [´em N*] ie NOM] The computed bases and affixes are associated with syntactic categories </context>
</contexts>
<marker>Namer, 2009</marker>
<rawString>F Namer. 2009. Morphologie, Lexique et TAL : l’analyseur D´eriF. TIC et Sciences cognitives. Hermes Sciences Publishing, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oregon Evidence-based Practice Center</author>
</authors>
<title>Barriers and drivers of health information technology use for the elderly, chronically ill, and underserved. Technical report, Agency for healthcare research and quality.</title>
<date>2008</date>
<contexts>
<context position="2089" citStr="Center, 2008" startWordPosition="284" endWordPosition="285">ientific research, articles, medical blogs and fora, clinical documents, TV and radio broadcast, novels, discussion fora, epidemiological alerts, etc. Still, availability of medical and health information does not guarantee its easy and correct understanding by lay people. The medical field conveys indeed very technical notions, such as in example (1). (1) myocardial infarction, cholecystectomy, erythredema polyneuropathy, acromegaly, galactosemia Although technical, these notions are nevertheless important for patients (AMA, 1999; McCray, 2005; Eysenbach, 2007; Oregon Evidence-based Practice Center, 2008). It has been shown that in several situations such notions cannot be correctly understood by patients: the steps needed for the medication preparing and use (Patel et al., 2002); the instructions on drugs from patient package inserts, and the information delivered in informed consensus and health brochures: it appears that among the 2,600 patients recruited in two hospitals, 26% to 60% cannot manage information available in these sources (Williams et al., 1995); health information in different languages (English, Spanish, French) provided in websites created for patients require high reading </context>
</contexts>
<marker>Center, 2008</marker>
<rawString>Oregon Evidence-based Practice Center. 2008. Barriers and drivers of health information technology use for the elderly, chronically ill, and underserved. Technical report, Agency for healthcare research and quality.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Patel</author>
<author>T Branch</author>
<author>J Arocha</author>
</authors>
<title>Errors in interpreting quantities as procedures : The case of pharmaceutical labels.</title>
<date>2002</date>
<booktitle>International journal of medical informatics,</booktitle>
<pages>65--3</pages>
<contexts>
<context position="2267" citStr="Patel et al., 2002" startWordPosition="311" endWordPosition="314">f medical and health information does not guarantee its easy and correct understanding by lay people. The medical field conveys indeed very technical notions, such as in example (1). (1) myocardial infarction, cholecystectomy, erythredema polyneuropathy, acromegaly, galactosemia Although technical, these notions are nevertheless important for patients (AMA, 1999; McCray, 2005; Eysenbach, 2007; Oregon Evidence-based Practice Center, 2008). It has been shown that in several situations such notions cannot be correctly understood by patients: the steps needed for the medication preparing and use (Patel et al., 2002); the instructions on drugs from patient package inserts, and the information delivered in informed consensus and health brochures: it appears that among the 2,600 patients recruited in two hospitals, 26% to 60% cannot manage information available in these sources (Williams et al., 1995); health information in different languages (English, Spanish, French) provided in websites created for patients require high reading level (Berland et al., 2001; Hargrave et al., 2003; Kusec, 2004) and remains difficult to manage by patients, which can be negative for the communication between patients and med</context>
</contexts>
<marker>Patel, Branch, Arocha, 2002</marker>
<rawString>V Patel, T Branch, and J Arocha. 2002. Errors in interpreting quantities as procedures : The case of pharmaceutical labels. International journal of medical informatics, 65(3):193–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poprat</author>
<author>K Mark´o</author>
<author>U Hahn</author>
</authors>
<title>A language classifier that automatically divides medical documents for experts and health care consumers.</title>
<date>2006</date>
<booktitle>In MIE 2006 - Proceedings of the XX International Congress of the European Federation for Medical Informatics,</booktitle>
<pages>503--508</pages>
<location>Maastricht.</location>
<marker>Poprat, Mark´o, Hahn, 2006</marker>
<rawString>M Poprat, K Mark´o, and U Hahn. 2006. A language classifier that automatically divides medical documents for experts and health care consumers. In MIE 2006 - Proceedings of the XX International Congress of the European Federation for Medical Informatics, pages 503–508, Maastricht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In ICNMLP,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="13205" citStr="Schmid, 1994" startWordPosition="1901" endWordPosition="1902">this case, we need first to acquire the knowledge needed for their automatic detection. We propose to rely on the morphological analysis of terms. The method is composed of four main steps: the processing of terms, the processing of corpora, the extraction of layman paraphrases for technical terms, and the evaluation of the extractions. 3.1 The processing of medical terms To reach the morphological information on terms we apply three specific processing: 1. Morpho-syntactic tagging and lemmatization of terms. The terms are morpho-syntactically tagged and lemmatized with TreeTagger for French (Schmid, 1994). The morpho-syntactic tagging is done in context of the terms. If a given word receives more than one tag, the most frequent one is kept. At this step, we obtain term lemmas with their part-of-speech tags, such as in example (7). (7) myocardique/A (myocardial/A), chol´ecystectomie/N (cholecystectomy/N), polyneuropathie/N (polyneuropathy/N), acrom´egalie/N (acromegaly/N), galactos´emie/N (galactosemia/N) 2. Morphological analysis. The lemmas are then morphologically analyzed with D´eriF (Namer, 2009). This tool performs the analysis of lemmas in order to detect their morphological structure, t</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>H Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In ICNMLP, pages 44–49, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sinha</author>
</authors>
<title>Unt-simprank: Systems for lexical simplification ranking.</title>
<date>2012</date>
<booktitle>In *SEM 2012,</booktitle>
<pages>493--496</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="5655" citStr="Sinha, 2012" startWordPosition="812" endWordPosition="813">reiler et al., 2007; Leroy et al., 2008; Franc¸ois and Fairon, 2013). • Lexical simplification. The lexical simplification helps to make text easier to understand. Lexical simplification of texts in English has been addressed during the SemEval 2012 challengea. Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, mutual information and frequency of words (Jauhar and Specia, 2012); frequency in Wikipedia, word length, n-grams of characters and of words, syntactic complexity of documents (Johannsen et al., 2012); n-grams, frequency in Wikipedia, n-Google grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). • Dedicated resources. The building of resources suitable for performing the simplification is another related research topics. Such resources are mainly two-fold lexica in which specialized and non-specialized vocabularies are aligne</context>
</contexts>
<marker>Sinha, 2012</marker>
<rawString>R Sinha. 2012. Unt-simprank: Systems for lexical simplification ranking. In *SEM 2012, pages 493–496, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Specia</author>
<author>SK Jauhar</author>
<author>R Mihalcea</author>
</authors>
<date>2012</date>
<booktitle>Semeval-2012 task 1: English lexical simplification. In *SEM 2012,</booktitle>
<pages>347--355</pages>
<contexts>
<context position="5533" citStr="Specia et al., 2012" startWordPosition="793" endWordPosition="796">iller et al., 2007); morphological descriptors (Chmielik and Grabar, 2011); combination of various descriptors (Wang, 2006; Zeng-Treiler et al., 2007; Leroy et al., 2008; Franc¸ois and Fairon, 2013). • Lexical simplification. The lexical simplification helps to make text easier to understand. Lexical simplification of texts in English has been addressed during the SemEval 2012 challengea. Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, mutual information and frequency of words (Jauhar and Specia, 2012); frequency in Wikipedia, word length, n-grams of characters and of words, syntactic complexity of documents (Johannsen et al., 2012); n-grams, frequency in Wikipedia, n-Google grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). • Dedicated resources. The building of resources suitable for performing the simplification is another related r</context>
</contexts>
<marker>Specia, Jauhar, Mihalcea, 2012</marker>
<rawString>L Specia, SK Jauhar, and R Mihalcea. 2012. Semeval-2012 task 1: English lexical simplification. In *SEM 2012, pages 347–355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TM Tran</author>
<author>H Chekroud</author>
<author>P Thiery</author>
<author>A Julienne</author>
</authors>
<title>Internet et soins : un tiers invisible dans la relation m´edecine/patient ? Ethica Clinica,</title>
<date>2009</date>
<pages>53--34</pages>
<contexts>
<context position="2933" citStr="Tran et al., 2009" startWordPosition="409" endWordPosition="412"> inserts, and the information delivered in informed consensus and health brochures: it appears that among the 2,600 patients recruited in two hospitals, 26% to 60% cannot manage information available in these sources (Williams et al., 1995); health information in different languages (English, Spanish, French) provided in websites created for patients require high reading level (Berland et al., 2001; Hargrave et al., 2003; Kusec, 2004) and remains difficult to manage by patients, which can be negative for the communication between patients and medical professionals, and the healthcare process (Tran et al., 2009). This situation sets the context of our work. Our objective is to propose method for the automatic acquisition of paraphrases for technical medical notions. More particularly, we propose to concentrate on terms and their words that show neoclassical compounding word formation (Booij, 2010; Iacobini, 1997; Amiot and Dal, 2005), such as in the example (1). Such words often involve Latin and Greek roots or bases, which makes them more difficult to understand, as such words must be decomposed first (see examples (2) and (3)). To our knowledge, this kind of approach has not been applied for the ac</context>
</contexts>
<marker>Tran, Chekroud, Thiery, Julienne, 2009</marker>
<rawString>TM Tran, H Chekroud, P Thiery, and A Julienne. 2009. Internet et soins : un tiers invisible dans la relation m´edecine/patient ? Ethica Clinica, 53:34–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
</authors>
<title>Automatic recognition of text difficulty from consumers health information.</title>
<date>2006</date>
<booktitle>In IEEE, editor, Computer-Based Medical Systems,</booktitle>
<pages>131--136</pages>
<contexts>
<context position="5035" citStr="Wang, 2006" startWordPosition="714" endWordPosition="715"> or documents and on linear regression models (Flesch, 1948; Gunning, 1973; Dubay, 2004). Computational measures, that are more recent, can involve vectorial models and a great variety of descriptors. These descriptors, usually specific to the texts processed, are for instance: combination of classical measures with medical terminologies (Kokkinakis and Toporowska Gronostaj, 2006); n-grams of characters (Poprat et al., 2006); discursive descriptors (Goeuriot et al., 2007); lexicon (Miller et al., 2007); morphological descriptors (Chmielik and Grabar, 2011); combination of various descriptors (Wang, 2006; Zeng-Treiler et al., 2007; Leroy et al., 2008; Franc¸ois and Fairon, 2013). • Lexical simplification. The lexical simplification helps to make text easier to understand. Lexical simplification of texts in English has been addressed during the SemEval 2012 challengea. Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and Wikipedia, Google n-grams, W</context>
</contexts>
<marker>Wang, 2006</marker>
<rawString>Y Wang. 2006. Automatic recognition of text difficulty from consumers health information. In IEEE, editor, Computer-Based Medical Systems, pages 131–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MV Williams</author>
<author>RM Parker</author>
<author>DW Baker</author>
<author>NS Parikh</author>
<author>K Pitkin</author>
<author>WC Coates</author>
<author>JR Nurss</author>
</authors>
<title>Inadequate functional health literacy among patients at two public hospitals.</title>
<date>1995</date>
<journal>JAMA, 274(21):1677–82. QT Zeng</journal>
<pages>13--24</pages>
<contexts>
<context position="2555" citStr="Williams et al., 1995" startWordPosition="354" endWordPosition="357">ugh technical, these notions are nevertheless important for patients (AMA, 1999; McCray, 2005; Eysenbach, 2007; Oregon Evidence-based Practice Center, 2008). It has been shown that in several situations such notions cannot be correctly understood by patients: the steps needed for the medication preparing and use (Patel et al., 2002); the instructions on drugs from patient package inserts, and the information delivered in informed consensus and health brochures: it appears that among the 2,600 patients recruited in two hospitals, 26% to 60% cannot manage information available in these sources (Williams et al., 1995); health information in different languages (English, Spanish, French) provided in websites created for patients require high reading level (Berland et al., 2001; Hargrave et al., 2003; Kusec, 2004) and remains difficult to manage by patients, which can be negative for the communication between patients and medical professionals, and the healthcare process (Tran et al., 2009). This situation sets the context of our work. Our objective is to propose method for the automatic acquisition of paraphrases for technical medical notions. More particularly, we propose to concentrate on terms and their </context>
</contexts>
<marker>Williams, Parker, Baker, Parikh, Pitkin, Coates, Nurss, 1995</marker>
<rawString>MV Williams, RM Parker, DW Baker, NS Parikh, K Pitkin, WC Coates, and JR Nurss. 1995. Inadequate functional health literacy among patients at two public hospitals. JAMA, 274(21):1677–82. QT Zeng and T Tse. 2006. Exploring and developing consumer health vocabularies. JAMIA, 13:24–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Zeng-Treiler</author>
<author>H Kim</author>
<author>S Goryachev</author>
<author>A Keselman</author>
<author>L Slaugther</author>
<author>CA Smith</author>
</authors>
<title>Text characteristics of clinical reports and their implications for the readability of personal health records.</title>
<date>2007</date>
<booktitle>In MEDINFO,</booktitle>
<pages>1117--1121</pages>
<location>Brisbane, Australia.</location>
<contexts>
<context position="5062" citStr="Zeng-Treiler et al., 2007" startWordPosition="716" endWordPosition="719">s and on linear regression models (Flesch, 1948; Gunning, 1973; Dubay, 2004). Computational measures, that are more recent, can involve vectorial models and a great variety of descriptors. These descriptors, usually specific to the texts processed, are for instance: combination of classical measures with medical terminologies (Kokkinakis and Toporowska Gronostaj, 2006); n-grams of characters (Poprat et al., 2006); discursive descriptors (Goeuriot et al., 2007); lexicon (Miller et al., 2007); morphological descriptors (Chmielik and Grabar, 2011); combination of various descriptors (Wang, 2006; Zeng-Treiler et al., 2007; Leroy et al., 2008; Franc¸ois and Fairon, 2013). • Lexical simplification. The lexical simplification helps to make text easier to understand. Lexical simplification of texts in English has been addressed during the SemEval 2012 challengea. Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word </context>
</contexts>
<marker>Zeng-Treiler, Kim, Goryachev, Keselman, Slaugther, Smith, 2007</marker>
<rawString>Q Zeng-Treiler, H Kim, S Goryachev, A Keselman, L Slaugther, and CA Smith. 2007. Text characteristics of clinical reports and their implications for the readability of personal health records. In MEDINFO, pages 1117– 1121, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Zweigenbaum</author>
<author>Natalia Grabar</author>
</authors>
<title>Corpus-based associations provide additional morphological variants to medical terminologies.</title>
<date>2003</date>
<booktitle>In AMIA.</booktitle>
<contexts>
<context position="15175" citStr="Zweigenbaum and Grabar, 2003" startWordPosition="2190" endWordPosition="2193"> of myocardique/A indicates that this word contains the suppletive noun bases myo N* (muscle) and carde N* (heart), and the affix -ique/ADJ. We can observe that some bases can be decomposed further (e.g. galactose in galact (milk) and ose (sugars), cholecystectomy in chole (bile) and cystis (bladder)). The words that contain more than one base are considered to be compounds and are processed in the further steps of the method. 3. Association of morphological components with French words. The bases are “translated” with words from modern French. We use for this resource built in previous work (Zweigenbaum and Grabar, 2003; Namer, 2003) (see some examples in (9)). (9) myocardique/A: myo=muscle (muscle), carde=coeur (heart) chol´ecystectomie/N: chol´ecysto=v´esicule biliaire (gall bladder), ectomie=ablation (removal) 97 polyneuropathie/N: poly=nombreux (several), neuro=nerf (nerve), pathie=maladie (disorder) acrom´egalie/N: acr=extr´emit´e (extremity), m´egal=grandeur (size) galactos´emie/N: galactose=galactose (galactose), ´em=sang (blood) Some words can remain technical (e.g., galactose, v´esicule biliaire), while other components totally lose their technical meaning (e.g. m´egal=grandeur (size), poly=nombreux</context>
</contexts>
<marker>Zweigenbaum, Grabar, 2003</marker>
<rawString>Pierre Zweigenbaum and Natalia Grabar. 2003. Corpus-based associations provide additional morphological variants to medical terminologies. In AMIA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
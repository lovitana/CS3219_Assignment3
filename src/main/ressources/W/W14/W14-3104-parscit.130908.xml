<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003577">
<title confidence="0.994866">
Active Learning with Constrained Topic Model
</title>
<author confidence="0.998923">
Yi Yang
</author>
<affiliation confidence="0.993552">
Northwestern University
</affiliation>
<email confidence="0.981272">
yiyang@u.northwestern.edu
</email>
<author confidence="0.989285">
Doug Downey
</author>
<affiliation confidence="0.991746">
Northwestern University
</affiliation>
<email confidence="0.993">
ddowney@eecs.northwestern.edu
</email>
<note confidence="0.7958895">
Shimei Pan
IBM T. J. Watson Research Center
</note>
<email confidence="0.861854">
shimei@us.ibm.com
</email>
<author confidence="0.998258">
Kunpeng Zhang
</author>
<affiliation confidence="0.999666">
University of Illinois at Chicago
</affiliation>
<email confidence="0.995479">
kzhang6@uic.edu
</email>
<sectionHeader confidence="0.997337" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999147769230769">
Latent Dirichlet Allocation (LDA) is a topic
modeling tool that automatically discovers
topics from a large collection of documents.
It is one of the most popular text analysis
tools currently in use. In practice however,
the topics discovered by LDA do not al-
ways make sense to end users. In this ex-
tended abstract, we propose an active learn-
ing framework that interactively and itera-
tively acquires user feedback to improve the
quality of learned topics. We conduct exper-
iments to demonstrate its effectiveness with
simulated user input on a benchmark dataset.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955333333333">
Statistical topic models such as Latent Dirichlet Al-
location (LDA) (Blei et al., 2003) provide powerful
tools for uncovering hidden thematic patterns in text
and are useful for representing and summarizing the
contents of large document collections. However,
when using topic models in practice, users often face
one critical problem: topics discovered by the model
do not always make sense. A topic may contain the-
matically unrelated words. Moreover, two thematic
related words may appear in different topics. This
is mainly because the objective function optimized
by LDA may not reflect human judgments of topic
quality (Boyd-Graber et al., 2009).
Potentially, we can solve these problems by incor-
porating additional user guidance or domain knowl-
edge in topic modeling. With standard LDA how-
ever, it is impossible for users to interact with the
model and provide feedback. (Hu et al., 2011) pro-
posed an interactive topic modeling framework that
allows users to add word must-links. However, it
has several limitations. Since the vocabulary size of
a large document collection can be very large, users
may need to annotate a large number of word con-
straints for this method to be effective. Thus, this
process can be very tedious. More importantly, it
cannot handle polysemes. For example, the word
“pound” can refer to either a currency or a unit of
mass. If a user adds a must-link between “pound”
and another financial term, then he/she cannot add
a must-link between “pound” and any measurement
terms. Since word must-links are added without
context, there is no way to disambiguate them. As a
result, word constraints frequently are not as effec-
tive as document constraints.
Active learning (Settles, 2010) provides a use-
ful framework which allows users to iteratively give
feedback to the model to improve its quality. In gen-
eral, with the same amount of human labeling, ac-
tive learning often results in a better model than that
learned by an off-line method.
In this extended abstract, we propose an active
learning framework for LDA. It is based on a new
constrained topic modeling framework which is ca-
pable of handling pairwise document constraints.
We present several design choices and the pros and
cons of each choice. We also conduct simulated ex-
periments to demonstrate the effectiveness of the ap-
proach.
</bodyText>
<sectionHeader confidence="0.876501" genericHeader="method">
2 Active Learning With Constrained Topic
Modeling
</sectionHeader>
<bodyText confidence="0.999815">
In this section, we first summarize our work on con-
strained topic modeling. Then, we introduce an
active topic learning framework that employs con-
strained topic modeling.
In LDA, a document’s topic distribution 9 is
drawn from a Dirichlet distribution with prior a.
A simple and commonly used Dirichlet distribution
uses a symmetric a prior. However, (Wallach et al.,
2009) has shown that an asymmetric Dirichlet prior
over the document-topic distributions Wand a sym-
metric Dirichlet prior over the topic-word distribu-
tions � yield significant improvements in model per-
formance. Our constrained topic model uses asym-
metric priors to encode constraints.
To incorporate user feedback, we focus on two
</bodyText>
<page confidence="0.990806">
130
</page>
<note confidence="0.3702465">
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 30–33,
Baltimore, Maryland, USA, June 27, 2014. @c 2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999791">
Figure 1: Diagram illustrating the topic model active learning framework.
</figureCaption>
<bodyText confidence="0.998830083333333">
types of document constraints. A must-link be-
tween two documents indicates that they belong to
the same topics, while a cannot-link indicates that
they belong to different topics.
Previously, we proposed a constrained LDA
framework called cLDA,1 which is capable of incor-
porating pairwise document constraints. Given pair-
wise document constraints, the topic distribution of
a document cannot be assumed to be independently
sampled. More specifically, we denote the collection
of documents as D = {d1, d2, ..., dN}. We also de-
note Mi E D as the set of documents sharing must-
links with document di, and Ci E D as the set of
documents sharing cannot-links with document di.
~θi is the topic distribution of di, and α~ is the global
document-topic hyper-parameter shared by all doc-
uments.
Given the documents in Mi, we introduce an aux-
iliary variable ~αmi :
where T controls the concentration parameters. The
larger the value of T is, the closer ~θi is to the average
of ~θj’s.
Given the documents in Ci, we introduce another
auxiliary variable:
</bodyText>
<equation confidence="0.9250665">
~αiC = T * arg~θi max min KL (~θi, θj), (2)
j ECi
</equation>
<bodyText confidence="0.999228636363636">
where KL( ~θi, ~θj) is the KL-divergence between two
distributions ~θi and ~θj. This means we choose a vec-
tor that is maximally far away from Ci, in terms of
KL divergence to its nearest neighbor in Ci.
In such a way, we force documents sharing must-
links to have similar topic distributions while docu-
ments sharing cannot-links to have dissimilar topic
distributions. Note that it also encodes constraint as
soft preference rather than hard constraint. We use
Collapsed Gibbs Sampling for LDA inference. Dur-
ing Gibbs Sampling, instead of always drawing ~θi
</bodyText>
<listItem confidence="0.310877">
1currently in submission.
from Dirichlet(~α), we draw ~θi based on the fol-
lowing distribution:
</listItem>
<equation confidence="0.822073">
~θi ∼ Dir(η~α+ηm~αim +ηC ~αiC) = Dir(~αi). (3)
</equation>
<bodyText confidence="0.998207777777778">
Here, ηg, ηm and ηC are the weights to control the
trade-off among the three terms. In our experiment,
we choose T = 100, ηg = ηm = ηC = 1.
Our evaluation has shown that cLDA is effective
in improving topic model quality. For example, it
achieved a significant topic classification error re-
duction on the 20 Newsgroup dataset. Also, top-
ics learned by cLDA are more coherent than those
learned by standard LDA.
</bodyText>
<subsectionHeader confidence="0.981269">
2.1 Active Learning with User Interaction
</subsectionHeader>
<bodyText confidence="0.999737107142857">
In this subsection, we present an active learning
framework to iteratively acquire constraints from
users. As shown in Figure 1, given a document col-
lection, the framework first runs standard LDA with
a burnin component. Since it uses a Gibbs sampler
(Griffiths and Steyvers, 2004) to infer topic samples
for each word token, it usually takes hundreds of it-
erations for the sampler to converge to a stable state.
Based on the results of the burnt-in model, the sys-
tem generates a target document and a set of anchor
documents for a user to annotate. Target document is
a document on which the active learner solicits user
feedback, and anchor documents are representatives
of a topic model’s latent topics. If a large portion of
the word tokens in a document belongs to topic i, we
say the document is an anchor document for topic i.
A user judges the content of the target and the
anchor documents and then informs the system
whether the target document is similar to any of the
anchor documents. The user interface is designed
so that the user can drag the target document near
an anchor document if she considers both to be the
same topic. Currently, one target document can be
must-linked to only one anchor document. Since
it is possbile to have multiple topics in one docu-
ment, in the future, we will allow user to add must
links between one target and mulitple anchor doc-
uments. After adding one or more must-links, the
</bodyText>
<equation confidence="0.99330225">
~αim = T * 1
|
~θj, (1)
Mi |jEmi
</equation>
<page confidence="0.996202">
31
</page>
<bodyText confidence="0.999875923076923">
system automatically adds cannot-links between the
target document and the rest anchor documents.
Given this input, the system adds them to a con-
straint pool. It then uses cLDA to incorporate these
constraints and generates an updated topic model.
Based on the new topic model, the system chooses a
new target document and several new anchor docu-
ments for the user to annotate. This process contin-
ues until the user is satisfied with the resulting topic
model.
How to choose the target and anchor documents
are the key questions that we consider in the next
subsections.
</bodyText>
<subsectionHeader confidence="0.998679">
2.2 Target Document Selection
</subsectionHeader>
<bodyText confidence="0.995738578947368">
A target document is defined as a document on
which the active learner solicits user feedback. We
have investigated several strategies for selecting a
target document.
Random: The active learner randomly selects a doc-
ument from the corpus. Although this strategy is
the simplest, it may not be efficient since the model
may have enough information about the document
already.
MaxEntropy: The entropy of a document d is com-
puted as Hd = − EK iJ 9dk log 9dk, where K is the
number of topics, and 9 is model’s document-topic
distribution. Therefore, the system will select a doc-
ument about which it is most confused. A uniform
9 implies that the model has no topic information
about the document and thus assigns equal probabil-
ity to all topics.
MinLikelihood: The likelihood of a document d is
computed as Ld = (EN EK k�1 φki9dk)/N, where
</bodyText>
<equation confidence="0.732021">
i�1
</equation>
<bodyText confidence="0.999933428571429">
N is the number of tokens in d, and φ is model’s
topic-word distribution. Since the overall likeli-
hood of the input documents is the objective func-
tion LDA aims to maximize, using this criteria, the
system will choose a document that is most difficult
for which the current model achieves the lowest ob-
jective score.
</bodyText>
<subsectionHeader confidence="0.999242">
2.3 Anchor Documents Selection
</subsectionHeader>
<bodyText confidence="0.9954733">
Given a target document d, the active learner then
generates one or more anchor documents based on
the target document’s topic distribution 9d. It filters
out topics with trivial value in 9d and extracts an an-
chor topic set Ta,,,, which only contains topics with
non-trivial value in 9d. A trivial 9di means that the
mass of ith component in 9d is neglectable, which
indicates that the model rarely assign topic i to doc-
ument d. For each topic t in Ta,,,,, the active learner
selects an anchor document who has minimum Eu-
clidean distance with an ideal anchor 9&apos;t. In the ideal
anchor 9&apos;t, all the components are zero except the
value of the tth component is 1. For example, if a
target document d’s 9d is {0.5, 0.3, 0.03, 0.02, 0.151
in a K = 5 topic model, the active learner would
generate Ta,,,, = {0, 1, 41 and for each t in Ta,,,,, an
anchor document.
However, it is possible that some topics learned
by LDA are only “background” topics which have
significant non-trivial probabilities over many doc-
uments (Song et al., 2009). Since background top-
ics are often uninteresting ones, we use a weighted
anchor topic selection method to filter them. A
weighted kth component of 9&apos;&apos; for document d is
dk
defined as follows: 9&apos;&apos;dk
= 9dk/ EDi�0 9ik. There-
fore, instead of keeping the topics with non-trivial
values, we keep those whose weighted values are
non-trivial.
</bodyText>
<sectionHeader confidence="0.998187" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999941">
In this section, we evaluate our active learning
framework. Topic models are often evaluated us-
ing perplexity on held-out test data. However, re-
cent work (Boyd-Graber et al., 2009; Chuang et al.,
2013) has shown that human judgment sometimes
is contrary to the perplexity measure. Following
(Mimno et al., 2011), we employ Topic Coherence,
a metric which was shown to be highly consistent
with human judgment, to measure a topic model’s
quality. It relies upon word co-occurrence statistics
within documents, and does not depend on external
resources or human labeling.
We followed (Basu et al., 2004) to create a Mix3
sub-dataset from the 20 Newsgroups data2, which
consists of two newsgroups with similar topics
(rec.sport.hockey, rec.sport.baseball) and one with
a distinctive topic (sci.space). We use this dataset
to evaluate the effectiveness of the proposed frame-
work.
</bodyText>
<subsectionHeader confidence="0.999495">
3.1 Simulated Experiments
</subsectionHeader>
<bodyText confidence="0.999723076923077">
We first burn-in LDA for 500 iterations. Then for
each additional iteration, the active learner generates
one query which consists of one target document and
one or more anchor documents. We simulate user
feedback using the documents’ ground truth labels.
If a target document has the same label as one of
the anchor documents, we add a must-link between
them. We also add cannot-links between the target
document and the rest of the anchor documents. All
these constraints are added into a constraint pool.
We also augment the constraint pool with derived
constraints. For example, due to transitivity, if there
is a must-link between (a, b) and (b, c), then we add
</bodyText>
<footnote confidence="0.9987045">
2Available at http://people.csail.mit.edu/
jrennie/20Newsgroups
</footnote>
<page confidence="0.994313">
32
</page>
<table confidence="0.885267857142857">
Topic Words
1 writes, like, think, good, know, better, even, people, run, hit
2 space, nasa, system, gov, launch, orbit, moon, earth, access, data
3 game, play, hockey, season, league, fun, wing, cup, shot, score
1 baseball, hit, won, shot, hitter, base, pitching, cub, ball, yankee
2 space, nasa, system, gov, launch, obit, moon, earth, mission, shuttle
3 hockey, nhl, playoff, star, wing, cup, king, detroit, ranger
</table>
<tableCaption confidence="0.999744">
Table 1: Ten most probable words of each topic before (above) and after active learning (below).
</tableCaption>
<bodyText confidence="0.9427695">
a must link between (a, c). We simulate the process
for 100 iterations to acquire constraints. After that,
we keep cLDA running for 400 more iterations with
the acquired constraints until it converges.
</bodyText>
<figureCaption confidence="0.801476">
Figure 2: Topic coherence with different number of
iterations.
</figureCaption>
<bodyText confidence="0.984413090909091">
Figure 2 shows the topic coherence scores for dif-
ferent target document selection strategies. This re-
sult indicates 1). MaxEntropy has the best topic co-
herence score. 2). All active learning strategies out-
perform standard LDA, and the results are statisti-
cally significant at p = 0.05. With standard LDA,
500 more iterations without any constraints does not
improve the topic coherence. However, by active
learning with cLDA for 500 iterations, the topic co-
herences are significantly improved.
Using MaxEntropy target document selection
method, we demonstrate the improvement of the
most probable topic keywords before and after ac-
tive learning. Table 1 shows that before active learn-
ing, topic 1’s most probable words are incoherent
and thus it is difficult to determine the meaning of
the topic. After active learning, in contrast, topic 1’s
most probable words become more consistent with
a “baseball” topic. This example suggests that the
active learning framework that interactively and it-
eratively acquires pairwise document constraints is
effective in improving the topic model’s quality.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99995">
We presented a novel active learning framework for
LDA that employs constrained topic modeling to
actively incorporate user feedback encoded as pair-
wise document constraints. With simulated user in-
put, our preliminary results demonstrate the effec-
tiveness of the framework on a benchmark dataset.
In the future, we will perform a formal user study
in which real users will interact with the system to
iteratively refine topic models.
</bodyText>
<sectionHeader confidence="0.99894" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.993466">
This work was supported in part by DARPA contract
D11AP00268.
</bodyText>
<sectionHeader confidence="0.999488" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999952848484848">
Sugato Basu, A. Banjeree, ER. Mooney, Arindam Baner-
jee, and Raymond J. Mooney. 2004. Active semi-
supervision for pairwise constrained clustering. In
SDM, pages 333–344.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
Jordan Boyd-Graber, Jonathan Chang, Sean Gerrish,
Chong Wang, and David Blei. 2009. Reading tea
leaves: How humans interpret topic models. In NIPS.
Jason Chuang, Sonal Gupta, Christopher D. Manning,
and Jeffrey Heer. 2013. Topic model diagnostics:
Assessing domain relevance via topical alignment. In
ICML.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228–5235.
Yuening Hu, Jordan Boyd-Graber, and Brianna Satinoff.
2011. Interactive topic modeling. In ACL, pages 248–
257.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
EMNLP, pages 262–272.
Burr Settles. 2010. Active learning literature survey.
Technical report, University of Wisconsin Madison.
Yangqiu Song, Shimei Pan, Shixia Liu, Michelle X.
Zhou, and Weihong Qian. 2009. Topic and keyword
re-ranking for lda-based topic modeling. In CIKM,
pages 1757–1760.
Hanna M. Wallach, David M. Mimno, and Andrew Mc-
Callum. 2009. Rethinking lda: Why priors matter. In
NIPS, pages 1973–1981.
</reference>
<page confidence="0.99938">
33
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.253042">
<title confidence="0.999569">Active Learning with Constrained Topic Model</title>
<author confidence="0.723342">Yi Yang Northwestern</author>
<email confidence="0.998842">yiyang@u.northwestern.edu</email>
<author confidence="0.785387">Doug Downey Northwestern</author>
<email confidence="0.999034">ddowney@eecs.northwestern.edu</email>
<author confidence="0.999789">Shimei Pan</author>
<affiliation confidence="0.998239">IBM T. J. Watson Research Center</affiliation>
<email confidence="0.999453">shimei@us.ibm.com</email>
<author confidence="0.998439">Kunpeng Zhang</author>
<affiliation confidence="0.998831">University of Illinois at</affiliation>
<email confidence="0.988683">kzhang6@uic.edu</email>
<abstract confidence="0.998813357142857">Latent Dirichlet Allocation (LDA) is a topic modeling tool that automatically discovers topics from a large collection of documents. It is one of the most popular text analysis tools currently in use. In practice however, the topics discovered by LDA do not always make sense to end users. In this extended abstract, we propose an active learning framework that interactively and iteratively acquires user feedback to improve the quality of learned topics. We conduct experiments to demonstrate its effectiveness with simulated user input on a benchmark dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arindam Banerjee Mooney</author>
<author>Raymond J Mooney</author>
</authors>
<title>Active semisupervision for pairwise constrained clustering.</title>
<date>2004</date>
<booktitle>In SDM,</booktitle>
<pages>333--344</pages>
<marker>Mooney, Mooney, 2004</marker>
<rawString>Sugato Basu, A. Banjeree, ER. Mooney, Arindam Banerjee, and Raymond J. Mooney. 2004. Active semisupervision for pairwise constrained clustering. In SDM, pages 333–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="968" citStr="Blei et al., 2003" startWordPosition="138" endWordPosition="141">odeling tool that automatically discovers topics from a large collection of documents. It is one of the most popular text analysis tools currently in use. In practice however, the topics discovered by LDA do not always make sense to end users. In this extended abstract, we propose an active learning framework that interactively and iteratively acquires user feedback to improve the quality of learned topics. We conduct experiments to demonstrate its effectiveness with simulated user input on a benchmark dataset. 1 Introduction Statistical topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) provide powerful tools for uncovering hidden thematic patterns in text and are useful for representing and summarizing the contents of large document collections. However, when using topic models in practice, users often face one critical problem: topics discovered by the model do not always make sense. A topic may contain thematically unrelated words. Moreover, two thematic related words may appear in different topics. This is mainly because the objective function optimized by LDA may not reflect human judgments of topic quality (Boyd-Graber et al., 2009). Potentially, we can solve these pro</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>Jonathan Chang</author>
<author>Sean Gerrish</author>
<author>Chong Wang</author>
<author>David Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1531" citStr="Boyd-Graber et al., 2009" startWordPosition="224" endWordPosition="227"> such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) provide powerful tools for uncovering hidden thematic patterns in text and are useful for representing and summarizing the contents of large document collections. However, when using topic models in practice, users often face one critical problem: topics discovered by the model do not always make sense. A topic may contain thematically unrelated words. Moreover, two thematic related words may appear in different topics. This is mainly because the objective function optimized by LDA may not reflect human judgments of topic quality (Boyd-Graber et al., 2009). Potentially, we can solve these problems by incorporating additional user guidance or domain knowledge in topic modeling. With standard LDA however, it is impossible for users to interact with the model and provide feedback. (Hu et al., 2011) proposed an interactive topic modeling framework that allows users to add word must-links. However, it has several limitations. Since the vocabulary size of a large document collection can be very large, users may need to annotate a large number of word constraints for this method to be effective. Thus, this process can be very tedious. More importantly</context>
<context position="11282" citStr="Boyd-Graber et al., 2009" startWordPosition="1881" endWordPosition="1884"> topics which have significant non-trivial probabilities over many documents (Song et al., 2009). Since background topics are often uninteresting ones, we use a weighted anchor topic selection method to filter them. A weighted kth component of 9&apos;&apos; for document d is dk defined as follows: 9&apos;&apos;dk = 9dk/ EDi�0 9ik. Therefore, instead of keeping the topics with non-trivial values, we keep those whose weighted values are non-trivial. 3 Evaluation In this section, we evaluate our active learning framework. Topic models are often evaluated using perplexity on held-out test data. However, recent work (Boyd-Graber et al., 2009; Chuang et al., 2013) has shown that human judgment sometimes is contrary to the perplexity measure. Following (Mimno et al., 2011), we employ Topic Coherence, a metric which was shown to be highly consistent with human judgment, to measure a topic model’s quality. It relies upon word co-occurrence statistics within documents, and does not depend on external resources or human labeling. We followed (Basu et al., 2004) to create a Mix3 sub-dataset from the 20 Newsgroups data2, which consists of two newsgroups with similar topics (rec.sport.hockey, rec.sport.baseball) and one with a distinctive</context>
</contexts>
<marker>Boyd-Graber, Chang, Gerrish, Wang, Blei, 2009</marker>
<rawString>Jordan Boyd-Graber, Jonathan Chang, Sean Gerrish, Chong Wang, and David Blei. 2009. Reading tea leaves: How humans interpret topic models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Chuang</author>
<author>Sonal Gupta</author>
<author>Christopher D Manning</author>
<author>Jeffrey Heer</author>
</authors>
<title>Topic model diagnostics: Assessing domain relevance via topical alignment.</title>
<date>2013</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="11304" citStr="Chuang et al., 2013" startWordPosition="1885" endWordPosition="1888">cant non-trivial probabilities over many documents (Song et al., 2009). Since background topics are often uninteresting ones, we use a weighted anchor topic selection method to filter them. A weighted kth component of 9&apos;&apos; for document d is dk defined as follows: 9&apos;&apos;dk = 9dk/ EDi�0 9ik. Therefore, instead of keeping the topics with non-trivial values, we keep those whose weighted values are non-trivial. 3 Evaluation In this section, we evaluate our active learning framework. Topic models are often evaluated using perplexity on held-out test data. However, recent work (Boyd-Graber et al., 2009; Chuang et al., 2013) has shown that human judgment sometimes is contrary to the perplexity measure. Following (Mimno et al., 2011), we employ Topic Coherence, a metric which was shown to be highly consistent with human judgment, to measure a topic model’s quality. It relies upon word co-occurrence statistics within documents, and does not depend on external resources or human labeling. We followed (Basu et al., 2004) to create a Mix3 sub-dataset from the 20 Newsgroups data2, which consists of two newsgroups with similar topics (rec.sport.hockey, rec.sport.baseball) and one with a distinctive topic (sci.space). We</context>
</contexts>
<marker>Chuang, Gupta, Manning, Heer, 2013</marker>
<rawString>Jason Chuang, Sonal Gupta, Christopher D. Manning, and Jeffrey Heer. 2013. Topic model diagnostics: Assessing domain relevance via topical alignment. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="6752" citStr="Griffiths and Steyvers, 2004" startWordPosition="1084" endWordPosition="1087">e choose T = 100, ηg = ηm = ηC = 1. Our evaluation has shown that cLDA is effective in improving topic model quality. For example, it achieved a significant topic classification error reduction on the 20 Newsgroup dataset. Also, topics learned by cLDA are more coherent than those learned by standard LDA. 2.1 Active Learning with User Interaction In this subsection, we present an active learning framework to iteratively acquire constraints from users. As shown in Figure 1, given a document collection, the framework first runs standard LDA with a burnin component. Since it uses a Gibbs sampler (Griffiths and Steyvers, 2004) to infer topic samples for each word token, it usually takes hundreds of iterations for the sampler to converge to a stable state. Based on the results of the burnt-in model, the system generates a target document and a set of anchor documents for a user to annotate. Target document is a document on which the active learner solicits user feedback, and anchor documents are representatives of a topic model’s latent topics. If a large portion of the word tokens in a document belongs to topic i, we say the document is an anchor document for topic i. A user judges the content of the target and the</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(Suppl. 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuening Hu</author>
<author>Jordan Boyd-Graber</author>
<author>Brianna Satinoff</author>
</authors>
<title>Interactive topic modeling.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>248--257</pages>
<contexts>
<context position="1775" citStr="Hu et al., 2011" startWordPosition="265" endWordPosition="268">odels in practice, users often face one critical problem: topics discovered by the model do not always make sense. A topic may contain thematically unrelated words. Moreover, two thematic related words may appear in different topics. This is mainly because the objective function optimized by LDA may not reflect human judgments of topic quality (Boyd-Graber et al., 2009). Potentially, we can solve these problems by incorporating additional user guidance or domain knowledge in topic modeling. With standard LDA however, it is impossible for users to interact with the model and provide feedback. (Hu et al., 2011) proposed an interactive topic modeling framework that allows users to add word must-links. However, it has several limitations. Since the vocabulary size of a large document collection can be very large, users may need to annotate a large number of word constraints for this method to be effective. Thus, this process can be very tedious. More importantly, it cannot handle polysemes. For example, the word “pound” can refer to either a currency or a unit of mass. If a user adds a must-link between “pound” and another financial term, then he/she cannot add a must-link between “pound” and any meas</context>
</contexts>
<marker>Hu, Boyd-Graber, Satinoff, 2011</marker>
<rawString>Yuening Hu, Jordan Boyd-Graber, and Brianna Satinoff. 2011. Interactive topic modeling. In ACL, pages 248– 257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna M Wallach</author>
<author>Edmund Talley</author>
<author>Miriam Leenders</author>
<author>Andrew McCallum</author>
</authors>
<title>Optimizing semantic coherence in topic models.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>262--272</pages>
<contexts>
<context position="11414" citStr="Mimno et al., 2011" startWordPosition="1902" endWordPosition="1905">eresting ones, we use a weighted anchor topic selection method to filter them. A weighted kth component of 9&apos;&apos; for document d is dk defined as follows: 9&apos;&apos;dk = 9dk/ EDi�0 9ik. Therefore, instead of keeping the topics with non-trivial values, we keep those whose weighted values are non-trivial. 3 Evaluation In this section, we evaluate our active learning framework. Topic models are often evaluated using perplexity on held-out test data. However, recent work (Boyd-Graber et al., 2009; Chuang et al., 2013) has shown that human judgment sometimes is contrary to the perplexity measure. Following (Mimno et al., 2011), we employ Topic Coherence, a metric which was shown to be highly consistent with human judgment, to measure a topic model’s quality. It relies upon word co-occurrence statistics within documents, and does not depend on external resources or human labeling. We followed (Basu et al., 2004) to create a Mix3 sub-dataset from the 20 Newsgroups data2, which consists of two newsgroups with similar topics (rec.sport.hockey, rec.sport.baseball) and one with a distinctive topic (sci.space). We use this dataset to evaluate the effectiveness of the proposed framework. 3.1 Simulated Experiments We first </context>
</contexts>
<marker>Mimno, Wallach, Talley, Leenders, McCallum, 2011</marker>
<rawString>David Mimno, Hanna M. Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. Optimizing semantic coherence in topic models. In EMNLP, pages 262–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Active learning literature survey.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>University of Wisconsin Madison.</institution>
<contexts>
<context position="2595" citStr="Settles, 2010" startWordPosition="402" endWordPosition="403">sers may need to annotate a large number of word constraints for this method to be effective. Thus, this process can be very tedious. More importantly, it cannot handle polysemes. For example, the word “pound” can refer to either a currency or a unit of mass. If a user adds a must-link between “pound” and another financial term, then he/she cannot add a must-link between “pound” and any measurement terms. Since word must-links are added without context, there is no way to disambiguate them. As a result, word constraints frequently are not as effective as document constraints. Active learning (Settles, 2010) provides a useful framework which allows users to iteratively give feedback to the model to improve its quality. In general, with the same amount of human labeling, active learning often results in a better model than that learned by an off-line method. In this extended abstract, we propose an active learning framework for LDA. It is based on a new constrained topic modeling framework which is capable of handling pairwise document constraints. We present several design choices and the pros and cons of each choice. We also conduct simulated experiments to demonstrate the effectiveness of the a</context>
</contexts>
<marker>Settles, 2010</marker>
<rawString>Burr Settles. 2010. Active learning literature survey. Technical report, University of Wisconsin Madison.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangqiu Song</author>
<author>Shimei Pan</author>
<author>Shixia Liu</author>
<author>Michelle X Zhou</author>
<author>Weihong Qian</author>
</authors>
<title>Topic and keyword re-ranking for lda-based topic modeling.</title>
<date>2009</date>
<booktitle>In CIKM,</booktitle>
<pages>1757--1760</pages>
<contexts>
<context position="10754" citStr="Song et al., 2009" startWordPosition="1794" endWordPosition="1797">i to document d. For each topic t in Ta,,,,, the active learner selects an anchor document who has minimum Euclidean distance with an ideal anchor 9&apos;t. In the ideal anchor 9&apos;t, all the components are zero except the value of the tth component is 1. For example, if a target document d’s 9d is {0.5, 0.3, 0.03, 0.02, 0.151 in a K = 5 topic model, the active learner would generate Ta,,,, = {0, 1, 41 and for each t in Ta,,,,, an anchor document. However, it is possible that some topics learned by LDA are only “background” topics which have significant non-trivial probabilities over many documents (Song et al., 2009). Since background topics are often uninteresting ones, we use a weighted anchor topic selection method to filter them. A weighted kth component of 9&apos;&apos; for document d is dk defined as follows: 9&apos;&apos;dk = 9dk/ EDi�0 9ik. Therefore, instead of keeping the topics with non-trivial values, we keep those whose weighted values are non-trivial. 3 Evaluation In this section, we evaluate our active learning framework. Topic models are often evaluated using perplexity on held-out test data. However, recent work (Boyd-Graber et al., 2009; Chuang et al., 2013) has shown that human judgment sometimes is contra</context>
</contexts>
<marker>Song, Pan, Liu, Zhou, Qian, 2009</marker>
<rawString>Yangqiu Song, Shimei Pan, Shixia Liu, Michelle X. Zhou, and Weihong Qian. 2009. Topic and keyword re-ranking for lda-based topic modeling. In CIKM, pages 1757–1760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
<author>David M Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Rethinking lda: Why priors matter.</title>
<date>2009</date>
<booktitle>In NIPS,</booktitle>
<pages>1973--1981</pages>
<contexts>
<context position="3627" citStr="Wallach et al., 2009" startWordPosition="570" endWordPosition="573">ndling pairwise document constraints. We present several design choices and the pros and cons of each choice. We also conduct simulated experiments to demonstrate the effectiveness of the approach. 2 Active Learning With Constrained Topic Modeling In this section, we first summarize our work on constrained topic modeling. Then, we introduce an active topic learning framework that employs constrained topic modeling. In LDA, a document’s topic distribution 9 is drawn from a Dirichlet distribution with prior a. A simple and commonly used Dirichlet distribution uses a symmetric a prior. However, (Wallach et al., 2009) has shown that an asymmetric Dirichlet prior over the document-topic distributions Wand a symmetric Dirichlet prior over the topic-word distributions � yield significant improvements in model performance. Our constrained topic model uses asymmetric priors to encode constraints. To incorporate user feedback, we focus on two 130 Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 30–33, Baltimore, Maryland, USA, June 27, 2014. @c 2014 Association for Computational Linguistics Figure 1: Diagram illustrating the topic model active learning framework.</context>
</contexts>
<marker>Wallach, Mimno, McCallum, 2009</marker>
<rawString>Hanna M. Wallach, David M. Mimno, and Andrew McCallum. 2009. Rethinking lda: Why priors matter. In NIPS, pages 1973–1981.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
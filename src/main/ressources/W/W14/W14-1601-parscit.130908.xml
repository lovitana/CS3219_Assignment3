<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000070">
<title confidence="0.994529">
What’s in a p-value in NLP?
</title>
<author confidence="0.999672">
Anders Søgaard, Anders Johannsen, Barbara Plank, Dirk Hovy and Hector Martinez
</author>
<affiliation confidence="0.998379">
Center for Language Technology
University of Copenhagen
</affiliation>
<email confidence="0.989487">
soegaard@hum.ku.dk
</email>
<sectionHeader confidence="0.993706" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945111111111">
In NLP, we need to document that our pro-
posed methods perform significantly bet-
ter with respect to standard metrics than
previous approaches, typically by re-
porting p-values obtained by rank- or
randomization-based tests. We show that
significance results following current re-
search standards are unreliable and, in ad-
dition, very sensitive to sample size, co-
variates such as sentence length, as well as
to the existence of multiple metrics. We
estimate that under the assumption of per-
fect metrics and unbiased data, we need a
significance cut-off at ⇠0.0025 to reduce
the risk of false positive results to &lt;5%.
Since in practice we often have consider-
able selection bias and poor metrics, this,
however, will not do alone.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994075">
In NLP, we try to improve upon state of the art
language technologies, guided by experience and
intuition, as well as error analysis from previous
experiments, and research findings often consist in
system comparisons showing that System A is bet-
ter than System B.
Effect size, i.e., one system’s improvements
over another, can be seen as a random variable.
If the random variable follows a known distribu-
tion, e.g., a normal distribution, we can use para-
metric tests to estimate whether System A is bet-
ter than System B. If it follows a normal dis-
tribution, we can use Student’s t-test, for exam-
ple. Effect sizes in NLP are generally not nor-
mally distributed or follow any of the other well-
studied distributions (Yeh, 2000; Søgaard, 2013).
The standard significance testing methods in NLP
are therefore rank- or randomization-based non-
parametric tests (Yeh, 2000; Riezler and Maxwell,
2005; Berg-Kirkpatrick et al., 2012). Specifi-
cally, most system comparisons across words, sen-
tences or documents use bootstrap tests (Efron and
Tibshirani, 1993) or approximate randomization
(Noreen, 1989), while studies that compare perfor-
mance across data sets use rank-based tests such as
Wilcoxon’s test.
The question we wish to address here is: how
likely is a research finding in NLP to be false?
Naively, we would expect all reported findings to
be true, but significance tests have their weak-
nesses, and sometimes researchers are forced
to violate test assumptions and basic statistical
methodology, e.g., when there is no one estab-
lished metric, when we can’t run our models on
full-length sentences, or when data is biased. For
example, one such well-known bias from the tag-
ging and parsing literature is what we may refer to
as the WSJ FALLACY. This is the false belief that
performance on the test section of the Wall Street
Journal (WSJ) part of the English Penn treebank
is representative for performance on other texts in
English. In other words, it is the belief that our
samples are always representative. However, (the
unawareness of) selection bias is not the only rea-
son research findings in NLP may be false.
In this paper, we critically examine significance
results in NLP by simulations, as well as running
a series of experiments comparing state-of-the-art
POS taggers, dependency parsers, and NER sys-
tems, focusing on the sensitivity of p-values to var-
ious factors.
Specifically, we address three important factors:
Sample size. When system A is reported to be
better than system B, this may not hold across do-
mains (cf. WSJ FALLACY). More importantly,
though, it may not even hold on a sub-sample of
the test data, or if we added more data points to
the test set. Below, we show that in 6/10 of our
POS tagger evaluations, significant effects become
insignificant by (randomly) adding more test data.
</bodyText>
<page confidence="0.829165">
1
</page>
<note confidence="0.693699">
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 1–10,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999583217391304">
Covariates. Sometimes we may bin our results
by variables that are actually predictive of the out-
come (covariates) (Simmons et al., 2011). In some
subfields of NLP, such as machine translation or
(unsupervised) syntactic parsing, for example, it
is common to report results that only hold for sen-
tences up to some length. If a system A is reported
to be better than a system B on sentences up to
some length, A need not be better than B, neither
for a different length nor in general, since sentence
length may actually be predictive of A being better
than B.
Multiple metrics. In several subfields of NLP,
we have various evaluation metrics. However, if
a system A is reported to be better than a system
B with respect to some metric M1, it need not be
better with respect to some other metric M2. We
show that even in POS tagging it is sometimes the
case that results are significant with respect to one
metric, but not with respect to others.
While these caveats should ideally be avoided
by reporting significance over varying sample
sizes and multiple metrics, some of these effects
also stem from the p-value cut-off chosen in the
NLP literature. In some fields, p-values are re-
quired to be much smaller, e.g., in physics, where
the 5 Q criterion is used, and maybe we should also
be more conservative in NLP?
We address this question by a simulation of the
interaction of type 1 and type 2 error in NLP and
arrive at an estimate that more than half of research
findings in NLP with p &lt; 0.05 are likely to be
false, even with a valid metric and in the absence
of selection bias. From the same simulations, we
propose a new cut-off level at 0.0025 or smaller
for cases where the metric can be assumed to be
valid, and where there is no selection bias.1 We
briefly discuss what to do in case of selection bias
or imperfect metrics.
Note that we do not discuss false discovery rate
control or family wise error rate procedures here.
While testing with different sample sizes could
be be considered multiple hypothesis testing, as
pointed out by one of our anonymous reviewers,
NLP results should be robust across sample sizes.
Note that the p &lt; 0.0025 cut-off level corresponds
</bodyText>
<footnote confidence="0.883125428571429">
1In many fields, including NLP, it has become good prac-
tice to report actual P-values, but we still need to understand
how significance levels relate to the probability that research
findings are false, to interpret such values. The fact that we
propose a new cut-off level for the ideal case with perfect
metrics and no bias does not mean that we do not recommend
reporting actual P-values.
</footnote>
<bodyText confidence="0.9518075">
to a Bonferroni correction for a family of m = 20
hypotheses.
</bodyText>
<subsectionHeader confidence="0.730663">
Our contributions
</subsectionHeader>
<bodyText confidence="0.9989395">
Several authors have discussed significance test-
ing in NLP before us (Yeh, 2000; Riezler and
Maxwell, 2005; Berg-Kirkpatrick et al., 2012), but
while our discussion touches on many of the same
topics, this paper is to the best of our knowledge
the first to:
</bodyText>
<listItem confidence="0.858728307692308">
a) show experimentally how sensitive p-values
are to sample size, i.e., that in standard NLP
experiments, significant effects may actually
disappear by adding more data.
b) show experimentally that multiple metrics
and the use of covariates in evaluation in-
crease the probability of positive test results.
c) show that even under the assumption of per-
fect metrics and unbiased data, as well as our
estimates of type 1 and 2 error in NLP, you
need at least p &lt; 0.0025 to reduce the prob-
ability of a research finding being false to be
&lt; 5%.
</listItem>
<sectionHeader confidence="0.393628" genericHeader="method">
2 Significance testing in NLP
</sectionHeader>
<bodyText confidence="0.999806730769231">
Most NLP metric for comparing system outputs
can be shown to be non-normally distributed
(Søgaard, 2013) and hence, we generally cannot
use statistical tests that rely on such an assump-
tion, e.g., Student’s t-test. One alternative to such
tests are non-parametric rank-based tests such as
Wilcoxon’s test. Rank-based tests are sometimes
used in NLP, and especially when the number of
observations is low, e.g., when evaluating perfor-
mance across data sets, such tests seem to be the
right choice (Demsar, 2006; Søgaard, 2013). The
draw-back of rank-based tests is their relatively
weak statistical power. When we reduce scores to
ranks, we throw away information, and rank-based
tests are therefore relatively conservative, poten-
tially leading to high type 2 error rate (Q, i.e., the
number of false negatives over trials). An alterna-
tive, however, are randomization-based tests such
as the bootstrap test (Efron and Tibshirani, 1993)
and approximate randomization (Noreen, 1989),
which are the de facto standards in NLP. In this
paper, we follow Berg-Kirkpatrick et al. (2012) in
focusing on the bootstrap test. The bootstrap test is
non-parametric and stronger than rank-based test-
ing, i.e., introduces fewer type 2 errors. For small
samples, however, it does so at the expense of a
</bodyText>
<page confidence="0.988288">
2
</page>
<bodyText confidence="0.999867615384615">
higher type 1 error (a, i.e., the number of false
positives). The reason for this is that for the boot-
strap test to work, the original sample has to cap-
ture most of the variation in the population. If the
sample is very small, though, this is likely not the
case. Consequently, with small sample sizes, there
is a risk that the calculated p-value will be arti-
ficially low—simply because the bootstrap sam-
ples are too similar. In our experiments below, we
make sure only to use bootstrap when sample size
is &gt; 200, unless otherwise stated. In our experi-
ments, we average across 3 runs for POS and NER
and 10 runs for dependency parsing.
</bodyText>
<table confidence="0.880785611111111">
DOMAIN #WORDS POS TASKS NER
Dep.
CONLL 2007
Bio 4k •
Chem 5k •
SWITCHBOARD 4
Spoken 162k •
ENGLISH WEB TREEBANK
Answers 29k • •
Emails 28k • •
Newsgrs 21k • •
Reviews 28k • •
Weblogs 20k • •
WSJ 40k • •
FOSTER
Twitter 3k •
CONLL 2003
News 50k •
</table>
<tableCaption confidence="0.999612">
Table 1: Evaluation data.
</tableCaption>
<sectionHeader confidence="0.992999" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999975454545455">
Throughout the rest of the paper, we use four run-
ning examples: a synthetic toy example and three
standard experimental NLP tasks, namely POS
tagging, dependency parsing and NER. The toy
example is supposed to illustrate the logic behind
our reasoning and is not specific to NLP. It shows
how likely we are to obtain a low p-value for the
difference in means when sampling from exactly
the same (Gaussian) distributions. For the NLP
setups (2-4), we use off-the-shelf models or avail-
able runs, as described next.
</bodyText>
<subsectionHeader confidence="0.999919">
3.1 Models and data
</subsectionHeader>
<bodyText confidence="0.999312">
We use pre-trained models for POS tagging and
dependency parsing. For NER, we use the output
of the best performing systems from the CoNLL
2003 shared task. In all three NLP setups, we
compare the outcome of pairs of systems. The
data sets we use for each of the NLP tasks are
listed in Table 1 (Nivre et al., 2007a; Foster et
</bodyText>
<figureCaption confidence="0.955734">
Figure 1: Accuracies of LAPOS VS. STANFORD
across 10 data sets.
al., 2011; Tjong Kim Sang and De Meulder, 2003,
LDC99T42; LDC2012T13).
</figureCaption>
<bodyText confidence="0.99890455">
POS tagging. We compare the performance
of two state-of-the-art newswire taggers across 10
evaluation data sets (see Table 1), namely the LA-
POS tagger (Tsuruoka et al., 2011) and the STAN-
FORD tagger (Toutanova et al., 2003), both trained
on WSJ00–18. We use the publicly available pre-
trained models from the associated websites.2
Dependency parsing. Here we compare the
pre-trained linear SVM MaltParser model for En-
glish (Nivre et al., 2007b) to the compositional
vector grammar model for the Stanford parser
(Socher et al., 2013). For this task, we use the sub-
set of the POS data sets that comes with Stanford-
style syntactic dependencies (cf. Table 1), exclud-
ing the Twitter data set which we found too small
to produce reliable results.
NER. We use the publicly available runs of
the two best systems from the CoNLL 2003
shared task, namely FLORIAN (Florian et al.,
2003) and CHIEU-NG (Chieu and Ng, 2003).3
</bodyText>
<subsectionHeader confidence="0.999915">
3.2 Standard comparisons
</subsectionHeader>
<bodyText confidence="0.999953375">
POS tagging. Figure 1 shows that the LAPOS
tagger is marginally better than STANFORD on
macro-average, but it is also significantly better? If
we use the bootstrap test over tagging accuracies,
the difference between the two taggers is only sig-
nificant (p &lt; 0.05) in 3/10 cases (see Table 2),
namely SPOKEN, ANSWERS and REVIEWS. In
two of these cases, LAPOS is significantly better
</bodyText>
<footnote confidence="0.999093">
2http://www.logos.ic.i.u-tokyo.ac.jp/
˜tsuruoka/lapos/ and http://nlp.stanford.
edu/software/tagger.shtml
3http://www.cnts.ua.ac.be/conll2003/
ner/
</footnote>
<page confidence="0.993557">
3
</page>
<table confidence="0.999562636363636">
TA (b) UA (b) SA (b) SA(w)
Bio 0.3445 0.0430 0.3788 0.9270
Chem 0.3569 0.2566 0.4515 0.9941
Spoken &lt;0.001 &lt;0.001 &lt;0.001 &lt;0.001
Answers &lt;0.001 0.0143 &lt;0.001 &lt;0.001
Emails 0.2020 &lt;0.001 0.1622 0.0324
Newsgrs 0.3965 0.0210 0.1238 0.6602
Reviews 0.0020 0.0543 0.0585 0.0562
Weblogs 0.2480 0.0024 0.2435 0.9390
WSJ 0.4497 0.0024 0.2435 0.9390
Twitter 0.4497 0.0924 0.1111 0.7853
</table>
<tableCaption confidence="0.763501166666667">
Table 2: POS tagging p-values across tagging ac-
curacy (TA), accuracy for unseen words (UA) and
sentence-level accuracy (SA) with bootstrap (b)
and Wilcoxon (w) (p &lt; 0.05 gray-shaded).
LAS UAS
Table 3: Parsing p-values (MALT-LIN
</tableCaption>
<bodyText confidence="0.96171656">
VS. STANFORD-RNN) across LAS and UAS
(p &lt; 0.05 gray-shaded).
than STANFORD, but in one case it is the other way
around. If we do a Wilcoxon test over the results
on the 10 data sets, following the methodology
in Demsar (2006) and Søgaard (2013), the differ-
ence, which is -0.12% on macro-average, is not
significant (p - 0.1394). LAPOS is thus not sig-
nificantly better than STANFORD across data sets,
but as we have already seen, it is significantly bet-
ter on some data sets. So if we allow ourselves
to cherry-pick our data sets and report significance
over word-level tagging accuracies, we can at least
report significant improvements across a few data
sets.
Dependency parsing. Using the bootstrap test
over sentences, we get the p-values in Table 3.
We see that differences are always significant
wrt. UAS, and in most cases wrt. LAS.
NER. Here we use the macro-fl as our stan-
dard metric. FLORIAN is not significantly bet-
ter than CHIEU-NG with p &lt; 0.05 as our cut-
off (p - 0.15). The two systems were also re-
ported to have overlapping confidence intervals in
the shared task.
</bodyText>
<subsectionHeader confidence="0.998367">
3.3 p-values across metrics
</subsectionHeader>
<bodyText confidence="0.999970555555556">
In several NLP subfields, multiple metrics are in
use. This happens in dependency parsing where
multiple metrics (Schwartz et al., 2011; Tsarfaty
et al., 2012) have been proposed in addition to un-
labeled and labeled attachment scores, as well as
exact matches. Perhaps more famously, in ma-
chine translation and summarization it is com-
mon practice to use multiple metrics, and there
exists a considerable literature on that topic (Pa-
pineni et al., 2002; Lin, 2004; Banerjee and Lavie,
2005; Clark et al., 2011; Rankel et al., 2011).
Even in POS tagging, some report tagging ac-
curacies, tagging accuracies over unseen words,
macro-averages over sentence-level accuracies, or
number of exact matches.
The existence of several metrics is not in it-
self a problem, but if researchers can cherry-pick
their favorite metric when reporting results, this
increases the a priori chance of establishing sig-
nificance. In POS tagging, most papers report sig-
nificant improvements over tagging accuracy, but
some report significant improvements over tag-
ging accuracy of unknown words, e.g., Denis and
Sagot (2009) and Umansky-Pesin et al. (2010).
This corresponds to the situation in psychology
where researchers cherry-pick between several de-
pendent variables (Simmons et al., 2011), which
also increases the chance of finding a significant
correlation.
Toy example. We draw two times 100 val-
ues from identical (0,1)-Gaussians 1000 times
and calculate a t-test for two independent sam-
ples. This corresponds to testing the effect size
between two systems on a 1000 randomly cho-
sen test sets with N = 100. Since we are sam-
pling from the same distribution, the chance of
p &lt; n should be smaller than t.. In our simula-
tion, the empirical chance of obtaining p &lt; 0.01
is .8%, and the chance of obtaining p &lt; 0.05 is
4.8%, as expected. If we simulate a free choice
between two metrics by introducing choice be-
tween a pair of samples and a distorted copy of
that pair (inducing random noise at 10%), simu-
lating the scenario where we have a perfect metric
and a suboptimal metric, the chance of obtaining
p &lt; 0.05 is 10.0%. We see a significant correla-
tion (p &lt; 0.0001) between Pearson’s p between
the two metrics, and the p-value. The less the two
metrics are correlated, the more likely we are to
obtain p &lt; 0.05. If we allow for a choice between
two metrics, the chance of finding a significant dif-
ference increases considerably. If the two metrics
are identical, but independent (introducing a free
choice between two pairs of samples), we have
</bodyText>
<figure confidence="0.997371769230769">
Answers
Emails
Newsgroups
Reviews
Weblogs
WSJ
0.020 &lt;0.001
0.083
0.049 &lt;0.001
&lt;0.001 &lt;0.001
&lt;0.001 &lt;0.001
&lt;0.001 &lt;0.001
&lt;0.001
</figure>
<page confidence="0.954595">
4
</page>
<bodyText confidence="0.999309767441861">
P(A V B) = P(A) + P(B) — P(A)P(B), hence
the chance of obtaining p &lt; 0.01 is 1.9%, and the
chance of obtaining p &lt; 0.05 is 9.75%.
POS tagging. In our POS-tagging experiments,
we saw a significant improvement in 3/10 cases
following the standard evaluation methodology
(see Table 2). If we allow for a choice between
tagging accuracy and sentence-level accuracy, we
see a significant improvement in 4/10 cases, i.e.,
for 4/10 data sets the effect is significance wrt. at
least one metric. If we allow for a free choice be-
tween all three metrics (TA, UA, and SA), we ob-
serve significance in 9/10 cases. This way the ex-
istence of multiple metrics almost guarantees sig-
nificant differences. Note that there are only two
data sets (Answers and Spoken), where all metric
differences appear significant.
Dependency parsing. While there are multi-
ple metrics in dependency parsing (Schwartz et
al., 2011; Tsarfaty et al., 2012), we focus on
the two standard metrics: labeled (LAS) and un-
labeled attachment score (UAS) (Buchholz and
Marsi, 2006). If we just consider the results in
Table 3, i.e., only the comparison of MALT-LIN
VS. STANFORD-RNN, we observe significant im-
provements in all cases, if we allow for a free
choice between metrics. Bod (2000) provides a
good example of a parsing paper evaluating mod-
els using different metrics on different test sets.
Chen et al. (2008), similarly, only report UAS.
NER. While macro-f1 is fairly standard in
NER, we do have several available multiple met-
rics, including the unlabeled f1 score (collapsing
all entity types), as well as the f1 scores for each
of the individual entity types (see Derczynski and
Bontcheva (2014) for an example of only report-
ing f1 for one entity type). With macro-f1 and
f1 for the individual entity types, we observe that,
while the average p-value for bootstrap tests over
five runs is around 0.15, the average p-value with a
free choice of metrics is 0.02. Hence, if we allow
for a free choice of metrics, FLORIAN comes out
significantly better than CHIEU-NG.
</bodyText>
<subsectionHeader confidence="0.963575">
3.4 p-values across sample size
</subsectionHeader>
<bodyText confidence="0.998605">
We now show that p-values are sensitive to sam-
ple size. While it is well-known that studies with
low statistical power have a reduced chance of
detecting true effects, studies with low statistical
power are also more likely to introduce false pos-
itives (Button et al., 2013). This, combined with
the fact that free choice between different sample
</bodyText>
<figureCaption confidence="0.997005">
Figure 2: The distribution of p-values with (above)
and without (below) multiple metrics.
Figure 3: POS tagging p-values varying sample
sizes (p &lt; 0.05 shaded).
</figureCaption>
<bodyText confidence="0.9805312">
sizes also increases the chance of false positives
(Simmons et al., 2011), is a potential source of er-
ror in NLP.
Toy example. The plot in Figure 2 shows the
distribution of p-values across 1000 bootstrap tests
(above), compared to the distribution of p-values
with a free choice of four sample sizes. It is clear
that the existence of multiple metrics makes the
probability of a positive result much higher.
POS tagging. The same holds for POS tag-
ging. We plot the p-values across various sample
sizes in Figure 3. Note that even when we ignore
the smallest sample size (500 words), where re-
sults may be rather unreliable, it still holds that for
Twitter, Answers, Newsgrs, Reviews, Weblogs and
WSJ, i.e., more than half of the data sets, a sig-
nificant result (p &lt; 0.05) becomes insignificant
by increasing the sample size. This shows how
unreliable significance results in NLP with cut-off
p &lt; 0.05 are.
</bodyText>
<page confidence="0.989513">
5
</page>
<figureCaption confidence="0.84279825">
Figure 4: Parsing p-values varying sample sizes
(p &lt; 0.05 shaded)
Figure 5: NER p-values varying sample sizes (p &lt;
0.05 shaded)
</figureCaption>
<bodyText confidence="0.944043764705882">
Dependency parsing. We performed simi-
lar experiments with dependency parsers, seeing
much the same picture. Our plots are presented in
Figure 4. We see that while effect sizes are al-
ways significant wrt. UAS, LAS differences be-
come significant when adding more data in 4/6
cases. An alternative experiment is to see how
often a bootstrap test at a particular sample size
comes out significant. The idea is to sample, say,
10% of the test data 100 times and report the ra-
tio of positive results. We only present the results
for MALT-LIN VS. STANFORD-RNN in Table 4,
but the full set of results (including comparisons of
more MaltParser and Stanford parser models) are
made available at http://lowlands.ku.dk.
For MALT-LIN VS. STANFORD-RNN differ-
ences on the full Emails data set are consistently
insignificant, but on small sample sizes we do get
significant test results in more than 1/10 cases. We
see the same picture with Newsgrs and Reviews.
On Weblogs and WSJ, the differences on the full
data sets are consistently significant, but here we
see that the test is underpowered at small sam-
ple sizes. Note that we use bootstrap tests over
sentences, so results with small samples may be
somewhat unreliable. In sum, these experiments
show how small sample sizes not only increase the
chance of false negatives, but also the chance of
false positives (Button et al., 2013).
NER. Our plots for NER are presented in Fig-
ure 5. Here, we see significance at small sam-
ple sizes, but the effect disappears with more data.
This is an example of how underpowered studies
may introduce false positives (Button et al., 2013).
</bodyText>
<subsectionHeader confidence="0.993321">
3.5 p-values across covariates
</subsectionHeader>
<bodyText confidence="0.998059214285714">
Toy example. If we allow for a choice between
two subsamples, using a covariate to single out a
subset of the data, the chance of finding a signifi-
cant difference increases. Even if we let the subset
be a random 50-50 split, the chance of obtaining
p &lt; 0.01 becomes 2.7%, and the chance of obtain-
ing p &lt; 0.05 is 9.5%. If we allow for both a choice
of dependent variables and a random covariate, the
chance of obtaining p &lt; 0.01 is 3.7%, and the
chance of obtaining p &lt; 0.05 is 16.2%. So iden-
tical Gaussian variables will appear significantly
different in 1/6 cases, if our sample size is 100,
and if we are allowed a choice between two iden-
tical, but independent dependent variables, and a
choice between two subsamples provided by a ran-
dom covariate.
POS We see from Figure 6 that p-values are
also very sensitive to sentence length cut-offs. For
instance, LAPOS is significantly (p &lt; 0.05) bet-
ter than STANFORD on sentences shorter than 16
words in EMAILS, but not on sentences shorter
than 14 words. On the other hand, when longer
sentences are included, e.g., up to 22 words, the
effect no longer appears significant. On full sen-
tence length, four differences seem significant, but
if we allow ourselves to cherry-pick a maximum
sentence length, we can observe significant differ-
ences in 8/10 cases.
</bodyText>
<figureCaption confidence="0.537527">
Figure 6: POS tagging p-values varying sentence
length (p &lt; 0.05 shaded)
</figureCaption>
<bodyText confidence="0.999745625">
We observe similar results in Dependency
parsing and NER when varying sentence length,
but do not include them here for space rea-
sons. The results are available at http://
lowlands.ku.dk. We also found that other
covariates are used in evaluations of dependency
parsers and NER systems. In dependency pars-
ing, for example, parsers can either be evaluated
</bodyText>
<page confidence="0.999039">
6
</page>
<table confidence="0.999533857142857">
N Emails Newsgrs Reviews Weblogs WSJ
LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS
10% 14 % 100 % 9 % 100 % 33% 100 % 42 % 99 % 28 % 75 %
25% 15 % 100 % 23 % 100 % 52% 100 % 68 % 100 % 27 % 98 %
50% 19 % 100 % 25 % 100 % 78% 100 % 100 % 100 % 60 % 100 %
75% 22 % 100 % 41 % 100 % 97% 100 % 100 % 100 % 80 % 100 %
100% 0 % 100 % 36 % 100 % 100% 100 % 100 % 100 % 100 % 100 %
</table>
<tableCaption confidence="0.998969">
Table 4: Ratio of positive results (p &lt; 0.05) for MALT-LIN VS. STANFORD-RNN at sample sizes (N)
</tableCaption>
<bodyText confidence="0.9733055">
.
on naturally occurring text such as in our experi-
ments or at tailored test suites, typically focusing
on hard phenomena (Rimell et al., 2009). While
such test suites are valuable resources, cf. Man-
ning (2011), they do introduce free choices for re-
searchers, increasing the a priori chance of posi-
tive results. In NER, it is not uncommon to leave
out sentences without any entity types from eval-
uation data. This biases evaluation toward high
recall systems, and the choice between including
them or not increases chances of positive results.
</bodyText>
<sectionHeader confidence="0.598684" genericHeader="method">
4 How likely are NLP findings to be
false?
</sectionHeader>
<bodyText confidence="0.999968857142857">
The previous sections have demonstrated how
many factors can contribute to reporting an erro-
neously significant result. Given those risks, it is
natural to wonder how likely we are as a field to
report false positives. This can be quantified by
the positive predictive value (PPV), or probability
that a research finding is true. PPV is defined as
</bodyText>
<equation confidence="0.993901">
(1-,3)R (1)
R—,C3R+↵
</equation>
<bodyText confidence="0.999784113636364">
The PPV depends on the type 1 and 2 error rates
(a and Q) and the ratio of true relations over null
relations in the field (R) (Ioannidis, 2005).
R. The likelihood that a research finding is true
depends on the ratio of true relations over null re-
lations in the field, usually denoted R (Ioannidis,
2005). Out of the systems that researchers in the
field would test out (not rejecting them a priori),
how many of them are better than the current state
of the art? The a priori likelihood of a relation be-
ing true, i.e., a new system being better than state
of the art, is R/(R + 1). Note that while the space
of reasonably motivated methods may seem big to
researchers in the field, there is often more than
one method that is better than the current state of
the art. Obviously, as the state of the art improves,
R drops. On the other hand, if R becomes very
low, researchers are likely to move on to new ap-
plications where R is higher.
The type 1 error rate (a) is also known as the
false positive rate, or the likelihood to accept a
non-significant result. Since our experiments are
fully automated and deterministic, and precision
usually high, the type 1 error rate is low in NLP.
What is not always appreciated in the field is that
this should lead us to expect true effects to be
highly significant with very low p-values, much
like in physics. The type 2 error rate (Q) is the
false negative rate, i.e., the likelihood that a true
relation is never found. This factors into the recall
of our experimental set-ups.
So what values should we use to estimate PPV?
Our estimate for R (how often reasonable hy-
potheses lead to improvements over state of the
art) is around 0.1. This is based on a sociolog-
ical rather than an ontological argument. With
a = 0.05 and R = 0.1, researchers get positive
results in R+ (1— R)a cases, i.e., ⇠ 1/7 cases. If
researchers needed to test more than 7 approaches
to ”hit the nail”, they would never get to write pa-
pers. With a = 0.05, and Q set to 0.5, we find that
the probability of a research finding being true –
given there is no selection bias and with perfectly
valid metrics – is just 50%:
</bodyText>
<equation confidence="0.982142">
PPV = (1-1)R
R- R+↵
5⇥0.1
0.1-0.05+0.05
</equation>
<bodyText confidence="0.999840428571429">
In other words, if researchers do a perfect experi-
ment and report p &lt; 0.05, the chance of that find-
ing being true is the chance of seeing tail when
flipping a coin. With p &lt; 0.01, the chance is 5/6,
i.e., the chance of not getting a 3 when rolling a
die. Of course these parameters are somewhat ar-
bitrary. Figure 7 shows PPV for various values of
a.
In the experiments in Section 3, we consistently
used the standard p-value cut-off of 0.05. How-
ever, our experiments have shown that significance
results at this threshold are unreliable and very
sensitive to the choice of sample size, covariates,
or metrics. Based on the curves in Figure 7, we
</bodyText>
<equation confidence="0.578444333333333">
=
0.05
0.1 = 0.5 (2)
</equation>
<page confidence="0.973042">
7
</page>
<figureCaption confidence="0.999503">
Figure 7: PPV for different a (horizontal line is PPV for p = 0.05, vertical line is a for PPV=0.95).
</figureCaption>
<bodyText confidence="0.999979333333333">
could propose a p-value cut-off at p &lt; 0.0025.
This is the cut-off that – in the absence of bias and
with perfect metrics – gives us the level of con-
fidence we expect as a research community, i.e.,
PPV = 0.95. Significance results would thus be
more reliable and reduce type 1 error.
</bodyText>
<sectionHeader confidence="0.99965" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.99998225">
Incidentally, the p &lt; 0.0025 cut-off also leads to
a 95% chance of seeing the same effect on held-
out test data in Berg-Kirkpatrick et al. (2012) (see
their Table 1, first row). The caveat is that this
holds only in the absence of bias and with perfect
metrics. In reality, though, our data sets are of-
ten severely biased (Berg-Kirkpatrick et al., 2012;
Søgaard, 2013), and our metrics are far from per-
fect (Papineni et al., 2002; Lin, 2004; Banerjee
and Lavie, 2005; Schwartz et al., 2011; Tsarfaty et
al., 2012). Here, we discuss how to address these
challenges.
Selection bias. The WSJ FALLACY (Section
1) has been widely discussed in the NLP litera-
ture (Blitzer et al., 2006; Daume III, 2007; Jiang
and Zhai, 2007; Plank and van Noord, 2011). But
if our test data is biased, how do we test whether
System A performs better than System B in gen-
eral? Søgaard (2013) suggests to predict signif-
icance across data sets. This only assumes that
data sets are randomly chosen, e.g., not all from
newswire corpora. This is also standard practice in
the machine learning community (Demsar, 2006).
Poor metrics. For tasks such as POS tagging
and dependency parsing, our metrics are subopti-
mal (Manning, 2011; Schwartz et al., 2011; Tsar-
faty et al., 2012). System A and System B may
perform equally well as measured by some met-
ric, but contribute very differently to downstream
tasks. Elming et al. (2013) show how parsers
trained on different annotation schemes lead to
very different downstream results. This suggests
that being wrong with respect to a gold standard,
e.g., choosing NP analysis over a “correct” DP
analysis, may in some cases lead to better down-
stream performance. See the discussion in Man-
ning (2011) for POS tagging. One simple ap-
proach to this problem is to report results across
available metrics. If System A improves over Sys-
tem B wrt. most metrics, we obtain significance
against the odds. POS taggers and dependency
parsers should also be evaluated by their impact
on downstream performance, but of course down-
stream tasks may also introduce multiple metrics.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99985725">
In sum, we have shown that significance results
with current research standards are unreliable, and
we have provided a more adequate p-value cut-off
under the assumption of perfect metrics and unbi-
</bodyText>
<page confidence="0.991759">
8
</page>
<bodyText confidence="0.983829">
ased data. In the cases where these assumptions
cannot be met, we suggest reporting significance
results across datasets wrt. all available metrics.
Leon Derczynski and Kalina Bontcheva. 2014.
Passive-aggressive sequence labeling with discrim-
inative post-editing for recognising person entities
in tweets. In EACL.
</bodyText>
<sectionHeader confidence="0.99028" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9979442">
We would like to thank the anonymous review-
ers, as well as Jakob Elming, Matthias Gondan,
and Natalie Schluter for invaluable comments and
feedback. This research is funded by the ERC
Starting Grant LOWLANDS No. 313695.
</bodyText>
<sectionHeader confidence="0.997942" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9984872">
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: an automatic metric for MT evaluation with
improved correlation with human judgments. In
ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for MT and/or Summarization.
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statis-
tical significance in nlp. In EMNLP.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Rens Bod. 2000. Parsing with the shortest derivation.
In COLING.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In CoNLL.
Katherine Button, John Ioannidis, Claire Mokrysz,
Brian Nosek, Jonathan Flint, Emma Robinson, and
Marcus Munafo. 2013. Power failure: why small
sample size undermines the reliability of neuro-
science. Nature Reviews Neuroscience, 14:365–376.
Wenliang Chen, Youzheng Wu, and Hitoshi Isahara.
2008. Learning Reliable Information for Depen-
dency Parsing Adaptation. In COLING.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach.
In CoNLL.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: controlling for opti-
mizer instability. In ACL.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Janez Demsar. 2006. Statistical comparisons of clas-
sifiers over multiple data sets. Journal of Machine
Learning Research, 7:1–30.
Pascal Denis and Benoit Sagot. 2009. Coupling an
annotated corpus and a morphosyntactic lexicon for
state-of-the-art pos tagging with less human effort.
In PACLIC.
Bradley Efron and Robert Tibshirani. 1993. An intro-
duction to the bootstrap. Chapman &amp; Hall, Boca
Raton, FL.
Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, Hector Martinez Alonso, and
Anders Søgaard. 2013. Down-stream effects of
tree-to-dependency conversions. In NAACL.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In CoNLL.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
John Ioannidis. 2005. Why most published research
findings are false. PLoS Medicine, 2(8):696–701.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In ACL.
Chin-Yew Lin. 2004. ROUGE: a package for auto-
matic evaluation of summaries. In WAS.
Chris Manning. 2011. Part-of-speech tagging from
97% to 100%: Is it time for some linguistics? In
CICLing.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 Shared Task on
Dependency Parsing. In EMNLP-CoNLL.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007b. MaltParser:
a language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.
Eric Noreen. 1989. Computer intensive methods for
testing hypotheses. Wiley.
Kishore Papineni, Salim Roukus, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL, pages
311–318, Philadelphia, Pennsylvania.
Barbara Plank and Gertjan van Noord. 2011. Effective
measures of domain similarity for parsing. In ACL.
Peter Rankel, John Conroy, Eric Slud, and Dianne
O’Leary. 2011. Ranking human and machine sum-
marization systems. In EMNLP.
</reference>
<page confidence="0.94457">
9
</page>
<reference confidence="0.999187380952381">
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance test-
ing for MT. In ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization.
Laura Rimell, Stephen Clark, and Mark Steedman.
2009. Unbounded dependency recovery for parser
evaluation. In EMNLP.
Roy Schwartz, and Omri Abend, Roi Reichart, and
Ari Rappoport. 2011. Neutralizing linguisti-
cally problematic annotations in unsupervised de-
pendency parsing evaluation. In ACL.
Joseph Simmons, Leif Nelson, and Uri Simonsohn.
2011. False-positive psychology: undisclosed flexi-
bility in data collection and analysis allows present-
ing anything as significant. Psychological Science,
22(11):1359–1366.
Richard Socher, John Bauer, Chris Manning, and An-
drew Ng. 2013. Parsing with compositional vector
grammars. In ACL.
Anders Søgaard. 2013. Estimating effect size across
datasets. In NAACL.
Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
In CoNLL.
Kristina Toutanova, Dan Klein, Chris Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In
NAACL.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical
parsing. In EACL.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi
Kazama. 2011. Learning with lookahead: can
history-based models rival globally optimized mod-
els? In CoNLL.
Shulamit Umansky-Pesin, Roi Reichart, and Ari Rap-
poport. 2010. A multi-domain web-based algorithm
for POS tagging of unknown words. In COLING.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In ACL.
</reference>
<page confidence="0.997782">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.870920">
<title confidence="0.999686">in a in NLP?</title>
<author confidence="0.988643">Anders Søgaard</author>
<author confidence="0.988643">Anders Johannsen</author>
<author confidence="0.988643">Barbara Plank</author>
<author confidence="0.988643">Dirk Hovy</author>
<author confidence="0.988643">Hector</author>
<affiliation confidence="0.998723">Center for Language University of</affiliation>
<email confidence="0.939954">soegaard@hum.ku.dk</email>
<abstract confidence="0.996720105263158">In NLP, we need to document that our promethods perform better with respect to standard metrics than previous approaches, typically by reobtained by rankor randomization-based tests. We show that significance results following current research standards are unreliable and, in addition, very sensitive to sample size, covariates such as sentence length, as well as to the existence of multiple metrics. We estimate that under the assumption of perfect metrics and unbiased data, we need a cut-off at to reduce risk of false positive results to Since in practice we often have considerable selection bias and poor metrics, this, however, will not do alone.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: an automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</booktitle>
<contexts>
<context position="14274" citStr="Banerjee and Lavie, 2005" startWordPosition="2398" endWordPosition="2401">(p - 0.15). The two systems were also reported to have overlapping confidence intervals in the shared task. 3.3 p-values across metrics In several NLP subfields, multiple metrics are in use. This happens in dependency parsing where multiple metrics (Schwartz et al., 2011; Tsarfaty et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric when reporting results, this increases the a priori chance of establishing significance. In POS tagging, most papers report significant improvements over tagging accuracy, but some report significant improvements over tagging accuracy of unknown words, e.g., Denis and Sag</context>
<context position="28557" citStr="Banerjee and Lavie, 2005" startWordPosition="4945" endWordPosition="4948">evel of confidence we expect as a research community, i.e., PPV = 0.95. Significance results would thus be more reliable and reduce type 1 error. 5 Discussion Incidentally, the p &lt; 0.0025 cut-off also leads to a 95% chance of seeing the same effect on heldout test data in Berg-Kirkpatrick et al. (2012) (see their Table 1, first row). The caveat is that this holds only in the absence of bias and with perfect metrics. In reality, though, our data sets are often severely biased (Berg-Kirkpatrick et al., 2012; Søgaard, 2013), and our metrics are far from perfect (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Schwartz et al., 2011; Tsarfaty et al., 2012). Here, we discuss how to address these challenges. Selection bias. The WSJ FALLACY (Section 1) has been widely discussed in the NLP literature (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007; Plank and van Noord, 2011). But if our test data is biased, how do we test whether System A performs better than System B in general? Søgaard (2013) suggests to predict significance across data sets. This only assumes that data sets are randomly chosen, e.g., not all from newswire corpora. This is also standard practice in the machine learning c</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>An empirical investigation of statistical significance in nlp.</title>
<date>2012</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1853" citStr="Berg-Kirkpatrick et al., 2012" startWordPosition="295" endWordPosition="298">em’s improvements over another, can be seen as a random variable. If the random variable follows a known distribution, e.g., a normal distribution, we can use parametric tests to estimate whether System A is better than System B. If it follows a normal distribution, we can use Student’s t-test, for example. Effect sizes in NLP are generally not normally distributed or follow any of the other wellstudied distributions (Yeh, 2000; Søgaard, 2013). The standard significance testing methods in NLP are therefore rank- or randomization-based nonparametric tests (Yeh, 2000; Riezler and Maxwell, 2005; Berg-Kirkpatrick et al., 2012). Specifically, most system comparisons across words, sentences or documents use bootstrap tests (Efron and Tibshirani, 1993) or approximate randomization (Noreen, 1989), while studies that compare performance across data sets use rank-based tests such as Wilcoxon’s test. The question we wish to address here is: how likely is a research finding in NLP to be false? Naively, we would expect all reported findings to be true, but significance tests have their weaknesses, and sometimes researchers are forced to violate test assumptions and basic statistical methodology, e.g., when there is no one e</context>
<context position="6717" citStr="Berg-Kirkpatrick et al., 2012" startWordPosition="1124" endWordPosition="1127">f level corresponds 1In many fields, including NLP, it has become good practice to report actual P-values, but we still need to understand how significance levels relate to the probability that research findings are false, to interpret such values. The fact that we propose a new cut-off level for the ideal case with perfect metrics and no bias does not mean that we do not recommend reporting actual P-values. to a Bonferroni correction for a family of m = 20 hypotheses. Our contributions Several authors have discussed significance testing in NLP before us (Yeh, 2000; Riezler and Maxwell, 2005; Berg-Kirkpatrick et al., 2012), but while our discussion touches on many of the same topics, this paper is to the best of our knowledge the first to: a) show experimentally how sensitive p-values are to sample size, i.e., that in standard NLP experiments, significant effects may actually disappear by adding more data. b) show experimentally that multiple metrics and the use of covariates in evaluation increase the probability of positive test results. c) show that even under the assumption of perfect metrics and unbiased data, as well as our estimates of type 1 and 2 error in NLP, you need at least p &lt; 0.0025 to reduce the</context>
<context position="8483" citStr="Berg-Kirkpatrick et al. (2012)" startWordPosition="1414" endWordPosition="1417">ata sets, such tests seem to be the right choice (Demsar, 2006; Søgaard, 2013). The draw-back of rank-based tests is their relatively weak statistical power. When we reduce scores to ranks, we throw away information, and rank-based tests are therefore relatively conservative, potentially leading to high type 2 error rate (Q, i.e., the number of false negatives over trials). An alternative, however, are randomization-based tests such as the bootstrap test (Efron and Tibshirani, 1993) and approximate randomization (Noreen, 1989), which are the de facto standards in NLP. In this paper, we follow Berg-Kirkpatrick et al. (2012) in focusing on the bootstrap test. The bootstrap test is non-parametric and stronger than rank-based testing, i.e., introduces fewer type 2 errors. For small samples, however, it does so at the expense of a 2 higher type 1 error (a, i.e., the number of false positives). The reason for this is that for the bootstrap test to work, the original sample has to capture most of the variation in the population. If the sample is very small, though, this is likely not the case. Consequently, with small sample sizes, there is a risk that the calculated p-value will be artificially low—simply because the</context>
<context position="28236" citStr="Berg-Kirkpatrick et al. (2012)" startWordPosition="4888" endWordPosition="4891">ze, covariates, or metrics. Based on the curves in Figure 7, we = 0.05 0.1 = 0.5 (2) 7 Figure 7: PPV for different a (horizontal line is PPV for p = 0.05, vertical line is a for PPV=0.95). could propose a p-value cut-off at p &lt; 0.0025. This is the cut-off that – in the absence of bias and with perfect metrics – gives us the level of confidence we expect as a research community, i.e., PPV = 0.95. Significance results would thus be more reliable and reduce type 1 error. 5 Discussion Incidentally, the p &lt; 0.0025 cut-off also leads to a 95% chance of seeing the same effect on heldout test data in Berg-Kirkpatrick et al. (2012) (see their Table 1, first row). The caveat is that this holds only in the absence of bias and with perfect metrics. In reality, though, our data sets are often severely biased (Berg-Kirkpatrick et al., 2012; Søgaard, 2013), and our metrics are far from perfect (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Schwartz et al., 2011; Tsarfaty et al., 2012). Here, we discuss how to address these challenges. Selection bias. The WSJ FALLACY (Section 1) has been widely discussed in the NLP literature (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007; Plank and van Noord, 2011)</context>
</contexts>
<marker>Berg-Kirkpatrick, Burkett, Klein, 2012</marker>
<rawString>Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An empirical investigation of statistical significance in nlp. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="28769" citStr="Blitzer et al., 2006" startWordPosition="4981" endWordPosition="4984">chance of seeing the same effect on heldout test data in Berg-Kirkpatrick et al. (2012) (see their Table 1, first row). The caveat is that this holds only in the absence of bias and with perfect metrics. In reality, though, our data sets are often severely biased (Berg-Kirkpatrick et al., 2012; Søgaard, 2013), and our metrics are far from perfect (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Schwartz et al., 2011; Tsarfaty et al., 2012). Here, we discuss how to address these challenges. Selection bias. The WSJ FALLACY (Section 1) has been widely discussed in the NLP literature (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007; Plank and van Noord, 2011). But if our test data is biased, how do we test whether System A performs better than System B in general? Søgaard (2013) suggests to predict significance across data sets. This only assumes that data sets are randomly chosen, e.g., not all from newswire corpora. This is also standard practice in the machine learning community (Demsar, 2006). Poor metrics. For tasks such as POS tagging and dependency parsing, our metrics are suboptimal (Manning, 2011; Schwartz et al., 2011; Tsarfaty et al., 2012). System A and System B may per</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Parsing with the shortest derivation.</title>
<date>2000</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="17685" citStr="Bod (2000)" startWordPosition="2976" endWordPosition="2977">e metrics almost guarantees significant differences. Note that there are only two data sets (Answers and Spoken), where all metric differences appear significant. Dependency parsing. While there are multiple metrics in dependency parsing (Schwartz et al., 2011; Tsarfaty et al., 2012), we focus on the two standard metrics: labeled (LAS) and unlabeled attachment score (UAS) (Buchholz and Marsi, 2006). If we just consider the results in Table 3, i.e., only the comparison of MALT-LIN VS. STANFORD-RNN, we observe significant improvements in all cases, if we allow for a free choice between metrics. Bod (2000) provides a good example of a parsing paper evaluating models using different metrics on different test sets. Chen et al. (2008), similarly, only report UAS. NER. While macro-f1 is fairly standard in NER, we do have several available multiple metrics, including the unlabeled f1 score (collapsing all entity types), as well as the f1 scores for each of the individual entity types (see Derczynski and Bontcheva (2014) for an example of only reporting f1 for one entity type). With macro-f1 and f1 for the individual entity types, we observe that, while the average p-value for bootstrap tests over fi</context>
</contexts>
<marker>Bod, 2000</marker>
<rawString>Rens Bod. 2000. Parsing with the shortest derivation. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X Shared Task on Multilingual Dependency Parsing. In CoNLL.</title>
<date>2006</date>
<contexts>
<context position="17476" citStr="Buchholz and Marsi, 2006" startWordPosition="2938" endWordPosition="2941">i.e., for 4/10 data sets the effect is significance wrt. at least one metric. If we allow for a free choice between all three metrics (TA, UA, and SA), we observe significance in 9/10 cases. This way the existence of multiple metrics almost guarantees significant differences. Note that there are only two data sets (Answers and Spoken), where all metric differences appear significant. Dependency parsing. While there are multiple metrics in dependency parsing (Schwartz et al., 2011; Tsarfaty et al., 2012), we focus on the two standard metrics: labeled (LAS) and unlabeled attachment score (UAS) (Buchholz and Marsi, 2006). If we just consider the results in Table 3, i.e., only the comparison of MALT-LIN VS. STANFORD-RNN, we observe significant improvements in all cases, if we allow for a free choice between metrics. Bod (2000) provides a good example of a parsing paper evaluating models using different metrics on different test sets. Chen et al. (2008), similarly, only report UAS. NER. While macro-f1 is fairly standard in NER, we do have several available multiple metrics, including the unlabeled f1 score (collapsing all entity types), as well as the f1 scores for each of the individual entity types (see Dercz</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X Shared Task on Multilingual Dependency Parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katherine Button</author>
<author>John Ioannidis</author>
<author>Claire Mokrysz</author>
<author>Brian Nosek</author>
<author>Jonathan Flint</author>
<author>Emma Robinson</author>
<author>Marcus Munafo</author>
</authors>
<title>Power failure: why small sample size undermines the reliability of neuroscience.</title>
<date>2013</date>
<journal>Nature Reviews Neuroscience,</journal>
<pages>14--365</pages>
<contexts>
<context position="18777" citStr="Button et al., 2013" startWordPosition="3160" endWordPosition="3163">e). With macro-f1 and f1 for the individual entity types, we observe that, while the average p-value for bootstrap tests over five runs is around 0.15, the average p-value with a free choice of metrics is 0.02. Hence, if we allow for a free choice of metrics, FLORIAN comes out significantly better than CHIEU-NG. 3.4 p-values across sample size We now show that p-values are sensitive to sample size. While it is well-known that studies with low statistical power have a reduced chance of detecting true effects, studies with low statistical power are also more likely to introduce false positives (Button et al., 2013). This, combined with the fact that free choice between different sample Figure 2: The distribution of p-values with (above) and without (below) multiple metrics. Figure 3: POS tagging p-values varying sample sizes (p &lt; 0.05 shaded). sizes also increases the chance of false positives (Simmons et al., 2011), is a potential source of error in NLP. Toy example. The plot in Figure 2 shows the distribution of p-values across 1000 bootstrap tests (above), compared to the distribution of p-values with a free choice of four sample sizes. It is clear that the existence of multiple metrics makes the pro</context>
<context position="21420" citStr="Button et al., 2013" startWordPosition="3607" endWordPosition="3610">the full Emails data set are consistently insignificant, but on small sample sizes we do get significant test results in more than 1/10 cases. We see the same picture with Newsgrs and Reviews. On Weblogs and WSJ, the differences on the full data sets are consistently significant, but here we see that the test is underpowered at small sample sizes. Note that we use bootstrap tests over sentences, so results with small samples may be somewhat unreliable. In sum, these experiments show how small sample sizes not only increase the chance of false negatives, but also the chance of false positives (Button et al., 2013). NER. Our plots for NER are presented in Figure 5. Here, we see significance at small sample sizes, but the effect disappears with more data. This is an example of how underpowered studies may introduce false positives (Button et al., 2013). 3.5 p-values across covariates Toy example. If we allow for a choice between two subsamples, using a covariate to single out a subset of the data, the chance of finding a significant difference increases. Even if we let the subset be a random 50-50 split, the chance of obtaining p &lt; 0.01 becomes 2.7%, and the chance of obtaining p &lt; 0.05 is 9.5%. If we al</context>
</contexts>
<marker>Button, Ioannidis, Mokrysz, Nosek, Flint, Robinson, Munafo, 2013</marker>
<rawString>Katherine Button, John Ioannidis, Claire Mokrysz, Brian Nosek, Jonathan Flint, Emma Robinson, and Marcus Munafo. 2013. Power failure: why small sample size undermines the reliability of neuroscience. Nature Reviews Neuroscience, 14:365–376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Youzheng Wu</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Learning Reliable Information for Dependency Parsing Adaptation.</title>
<date>2008</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="17813" citStr="Chen et al. (2008)" startWordPosition="2996" endWordPosition="2999">all metric differences appear significant. Dependency parsing. While there are multiple metrics in dependency parsing (Schwartz et al., 2011; Tsarfaty et al., 2012), we focus on the two standard metrics: labeled (LAS) and unlabeled attachment score (UAS) (Buchholz and Marsi, 2006). If we just consider the results in Table 3, i.e., only the comparison of MALT-LIN VS. STANFORD-RNN, we observe significant improvements in all cases, if we allow for a free choice between metrics. Bod (2000) provides a good example of a parsing paper evaluating models using different metrics on different test sets. Chen et al. (2008), similarly, only report UAS. NER. While macro-f1 is fairly standard in NER, we do have several available multiple metrics, including the unlabeled f1 score (collapsing all entity types), as well as the f1 scores for each of the individual entity types (see Derczynski and Bontcheva (2014) for an example of only reporting f1 for one entity type). With macro-f1 and f1 for the individual entity types, we observe that, while the average p-value for bootstrap tests over five runs is around 0.15, the average p-value with a free choice of metrics is 0.02. Hence, if we allow for a free choice of metri</context>
</contexts>
<marker>Chen, Wu, Isahara, 2008</marker>
<rawString>Wenliang Chen, Youzheng Wu, and Hitoshi Isahara. 2008. Learning Reliable Information for Dependency Parsing Adaptation. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Named entity recognition with a maximum entropy approach.</title>
<date>2003</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="11517" citStr="Chieu and Ng, 2003" startWordPosition="1955" endWordPosition="1958">ed models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available runs of the two best systems from the CoNLL 2003 shared task, namely FLORIAN (Florian et al., 2003) and CHIEU-NG (Chieu and Ng, 2003).3 3.2 Standard comparisons POS tagging. Figure 1 shows that the LAPOS tagger is marginally better than STANFORD on macro-average, but it is also significantly better? If we use the bootstrap test over tagging accuracies, the difference between the two taggers is only significant (p &lt; 0.05) in 3/10 cases (see Table 2), namely SPOKEN, ANSWERS and REVIEWS. In two of these cases, LAPOS is significantly better 2http://www.logos.ic.i.u-tokyo.ac.jp/ ˜tsuruoka/lapos/ and http://nlp.stanford. edu/software/tagger.shtml 3http://www.cnts.ua.ac.be/conll2003/ ner/ 3 TA (b) UA (b) SA (b) SA(w) Bio 0.3445 0.</context>
</contexts>
<marker>Chieu, Ng, 2003</marker>
<rawString>Hai Leong Chieu and Hwee Tou Ng. 2003. Named entity recognition with a maximum entropy approach. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="14294" citStr="Clark et al., 2011" startWordPosition="2402" endWordPosition="2405">s were also reported to have overlapping confidence intervals in the shared task. 3.3 p-values across metrics In several NLP subfields, multiple metrics are in use. This happens in dependency parsing where multiple metrics (Schwartz et al., 2011; Tsarfaty et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric when reporting results, this increases the a priori chance of establishing significance. In POS tagging, most papers report significant improvements over tagging accuracy, but some report significant improvements over tagging accuracy of unknown words, e.g., Denis and Sagot (2009) and Umansk</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: controlling for optimizer instability. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<marker>Daume, 2007</marker>
<rawString>Hal Daume III. 2007. Frustratingly easy domain adaptation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janez Demsar</author>
</authors>
<title>Statistical comparisons of classifiers over multiple data sets.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--1</pages>
<contexts>
<context position="7915" citStr="Demsar, 2006" startWordPosition="1330" endWordPosition="1331"> to reduce the probability of a research finding being false to be &lt; 5%. 2 Significance testing in NLP Most NLP metric for comparing system outputs can be shown to be non-normally distributed (Søgaard, 2013) and hence, we generally cannot use statistical tests that rely on such an assumption, e.g., Student’s t-test. One alternative to such tests are non-parametric rank-based tests such as Wilcoxon’s test. Rank-based tests are sometimes used in NLP, and especially when the number of observations is low, e.g., when evaluating performance across data sets, such tests seem to be the right choice (Demsar, 2006; Søgaard, 2013). The draw-back of rank-based tests is their relatively weak statistical power. When we reduce scores to ranks, we throw away information, and rank-based tests are therefore relatively conservative, potentially leading to high type 2 error rate (Q, i.e., the number of false negatives over trials). An alternative, however, are randomization-based tests such as the bootstrap test (Efron and Tibshirani, 1993) and approximate randomization (Noreen, 1989), which are the de facto standards in NLP. In this paper, we follow Berg-Kirkpatrick et al. (2012) in focusing on the bootstrap te</context>
<context position="12904" citStr="Demsar (2006)" startWordPosition="2167" endWordPosition="2168">.0210 0.1238 0.6602 Reviews 0.0020 0.0543 0.0585 0.0562 Weblogs 0.2480 0.0024 0.2435 0.9390 WSJ 0.4497 0.0024 0.2435 0.9390 Twitter 0.4497 0.0924 0.1111 0.7853 Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p &lt; 0.05 gray-shaded). LAS UAS Table 3: Parsing p-values (MALT-LIN VS. STANFORD-RNN) across LAS and UAS (p &lt; 0.05 gray-shaded). than STANFORD, but in one case it is the other way around. If we do a Wilcoxon test over the results on the 10 data sets, following the methodology in Demsar (2006) and Søgaard (2013), the difference, which is -0.12% on macro-average, is not significant (p - 0.1394). LAPOS is thus not significantly better than STANFORD across data sets, but as we have already seen, it is significantly better on some data sets. So if we allow ourselves to cherry-pick our data sets and report significance over word-level tagging accuracies, we can at least report significant improvements across a few data sets. Dependency parsing. Using the bootstrap test over sentences, we get the p-values in Table 3. We see that differences are always significant wrt. UAS, and in most ca</context>
<context position="29180" citStr="Demsar, 2006" startWordPosition="5054" endWordPosition="5055"> et al., 2011; Tsarfaty et al., 2012). Here, we discuss how to address these challenges. Selection bias. The WSJ FALLACY (Section 1) has been widely discussed in the NLP literature (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007; Plank and van Noord, 2011). But if our test data is biased, how do we test whether System A performs better than System B in general? Søgaard (2013) suggests to predict significance across data sets. This only assumes that data sets are randomly chosen, e.g., not all from newswire corpora. This is also standard practice in the machine learning community (Demsar, 2006). Poor metrics. For tasks such as POS tagging and dependency parsing, our metrics are suboptimal (Manning, 2011; Schwartz et al., 2011; Tsarfaty et al., 2012). System A and System B may perform equally well as measured by some metric, but contribute very differently to downstream tasks. Elming et al. (2013) show how parsers trained on different annotation schemes lead to very different downstream results. This suggests that being wrong with respect to a gold standard, e.g., choosing NP analysis over a “correct” DP analysis, may in some cases lead to better downstream performance. See the discu</context>
</contexts>
<marker>Demsar, 2006</marker>
<rawString>Janez Demsar. 2006. Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7:1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Benoit Sagot</author>
</authors>
<title>Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art pos tagging with less human effort.</title>
<date>2009</date>
<booktitle>In PACLIC.</booktitle>
<contexts>
<context position="14883" citStr="Denis and Sagot (2009)" startWordPosition="2491" endWordPosition="2494">d Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric when reporting results, this increases the a priori chance of establishing significance. In POS tagging, most papers report significant improvements over tagging accuracy, but some report significant improvements over tagging accuracy of unknown words, e.g., Denis and Sagot (2009) and Umansky-Pesin et al. (2010). This corresponds to the situation in psychology where researchers cherry-pick between several dependent variables (Simmons et al., 2011), which also increases the chance of finding a significant correlation. Toy example. We draw two times 100 values from identical (0,1)-Gaussians 1000 times and calculate a t-test for two independent samples. This corresponds to testing the effect size between two systems on a 1000 randomly chosen test sets with N = 100. Since we are sampling from the same distribution, the chance of p &lt; n should be smaller than t.. In our simu</context>
</contexts>
<marker>Denis, Sagot, 2009</marker>
<rawString>Pascal Denis and Benoit Sagot. 2009. Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art pos tagging with less human effort. In PACLIC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert Tibshirani</author>
</authors>
<title>An introduction to the bootstrap.</title>
<date>1993</date>
<publisher>Chapman &amp; Hall,</publisher>
<location>Boca Raton, FL.</location>
<contexts>
<context position="1978" citStr="Efron and Tibshirani, 1993" startWordPosition="313" endWordPosition="316">ormal distribution, we can use parametric tests to estimate whether System A is better than System B. If it follows a normal distribution, we can use Student’s t-test, for example. Effect sizes in NLP are generally not normally distributed or follow any of the other wellstudied distributions (Yeh, 2000; Søgaard, 2013). The standard significance testing methods in NLP are therefore rank- or randomization-based nonparametric tests (Yeh, 2000; Riezler and Maxwell, 2005; Berg-Kirkpatrick et al., 2012). Specifically, most system comparisons across words, sentences or documents use bootstrap tests (Efron and Tibshirani, 1993) or approximate randomization (Noreen, 1989), while studies that compare performance across data sets use rank-based tests such as Wilcoxon’s test. The question we wish to address here is: how likely is a research finding in NLP to be false? Naively, we would expect all reported findings to be true, but significance tests have their weaknesses, and sometimes researchers are forced to violate test assumptions and basic statistical methodology, e.g., when there is no one established metric, when we can’t run our models on full-length sentences, or when data is biased. For example, one such well-</context>
<context position="8340" citStr="Efron and Tibshirani, 1993" startWordPosition="1392" endWordPosition="1395">ank-based tests are sometimes used in NLP, and especially when the number of observations is low, e.g., when evaluating performance across data sets, such tests seem to be the right choice (Demsar, 2006; Søgaard, 2013). The draw-back of rank-based tests is their relatively weak statistical power. When we reduce scores to ranks, we throw away information, and rank-based tests are therefore relatively conservative, potentially leading to high type 2 error rate (Q, i.e., the number of false negatives over trials). An alternative, however, are randomization-based tests such as the bootstrap test (Efron and Tibshirani, 1993) and approximate randomization (Noreen, 1989), which are the de facto standards in NLP. In this paper, we follow Berg-Kirkpatrick et al. (2012) in focusing on the bootstrap test. The bootstrap test is non-parametric and stronger than rank-based testing, i.e., introduces fewer type 2 errors. For small samples, however, it does so at the expense of a 2 higher type 1 error (a, i.e., the number of false positives). The reason for this is that for the bootstrap test to work, the original sample has to capture most of the variation in the population. If the sample is very small, though, this is like</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>Bradley Efron and Robert Tibshirani. 1993. An introduction to the bootstrap. Chapman &amp; Hall, Boca Raton, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Elming</author>
<author>Anders Johannsen</author>
<author>Sigrid Klerke</author>
<author>Emanuele Lapponi</author>
<author>Hector Martinez Alonso</author>
<author>Anders Søgaard</author>
</authors>
<title>Down-stream effects of tree-to-dependency conversions.</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="29488" citStr="Elming et al. (2013)" startWordPosition="5105" endWordPosition="5108">, how do we test whether System A performs better than System B in general? Søgaard (2013) suggests to predict significance across data sets. This only assumes that data sets are randomly chosen, e.g., not all from newswire corpora. This is also standard practice in the machine learning community (Demsar, 2006). Poor metrics. For tasks such as POS tagging and dependency parsing, our metrics are suboptimal (Manning, 2011; Schwartz et al., 2011; Tsarfaty et al., 2012). System A and System B may perform equally well as measured by some metric, but contribute very differently to downstream tasks. Elming et al. (2013) show how parsers trained on different annotation schemes lead to very different downstream results. This suggests that being wrong with respect to a gold standard, e.g., choosing NP analysis over a “correct” DP analysis, may in some cases lead to better downstream performance. See the discussion in Manning (2011) for POS tagging. One simple approach to this problem is to report results across available metrics. If System A improves over System B wrt. most metrics, we obtain significance against the odds. POS taggers and dependency parsers should also be evaluated by their impact on downstream</context>
</contexts>
<marker>Elming, Johannsen, Klerke, Lapponi, Alonso, Søgaard, 2013</marker>
<rawString>Jakob Elming, Anders Johannsen, Sigrid Klerke, Emanuele Lapponi, Hector Martinez Alonso, and Anders Søgaard. 2013. Down-stream effects of tree-to-dependency conversions. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Tong Zhang</author>
</authors>
<title>Named entity recognition through classifier combination.</title>
<date>2003</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="11483" citStr="Florian et al., 2003" startWordPosition="1949" endWordPosition="1952"> use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available runs of the two best systems from the CoNLL 2003 shared task, namely FLORIAN (Florian et al., 2003) and CHIEU-NG (Chieu and Ng, 2003).3 3.2 Standard comparisons POS tagging. Figure 1 shows that the LAPOS tagger is marginally better than STANFORD on macro-average, but it is also significantly better? If we use the bootstrap test over tagging accuracies, the difference between the two taggers is only significant (p &lt; 0.05) in 3/10 cases (see Table 2), namely SPOKEN, ANSWERS and REVIEWS. In two of these cases, LAPOS is significantly better 2http://www.logos.ic.i.u-tokyo.ac.jp/ ˜tsuruoka/lapos/ and http://nlp.stanford. edu/software/tagger.shtml 3http://www.cnts.ua.ac.be/conll2003/ ner/ 3 TA (b)</context>
</contexts>
<marker>Florian, Ittycheriah, Jing, Zhang, 2003</marker>
<rawString>Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong Zhang. 2003. Named entity recognition through classifier combination. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Ozlem Cetinoglu</author>
<author>Joachim Wagner</author>
<author>Josef Le Roux</author>
<author>Joakim Nivre</author>
<author>Deirde Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>From news to comments: Resources and benchmarks for parsing the language of Web 2.0.</title>
<date>2011</date>
<booktitle>In IJCNLP.</booktitle>
<marker>Foster, Cetinoglu, Wagner, Le Roux, Nivre, Hogan, van Genabith, 2011</marker>
<rawString>Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Josef Le Roux, Joakim Nivre, Deirde Hogan, and Josef van Genabith. 2011. From news to comments: Resources and benchmarks for parsing the language of Web 2.0. In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Ioannidis</author>
</authors>
<title>Why most published research findings are false.</title>
<date>2005</date>
<journal>PLoS Medicine,</journal>
<volume>2</volume>
<issue>8</issue>
<contexts>
<context position="24992" citStr="Ioannidis, 2005" startWordPosition="4282" endWordPosition="4283">ncluding them or not increases chances of positive results. 4 How likely are NLP findings to be false? The previous sections have demonstrated how many factors can contribute to reporting an erroneously significant result. Given those risks, it is natural to wonder how likely we are as a field to report false positives. This can be quantified by the positive predictive value (PPV), or probability that a research finding is true. PPV is defined as (1-,3)R (1) R—,C3R+↵ The PPV depends on the type 1 and 2 error rates (a and Q) and the ratio of true relations over null relations in the field (R) (Ioannidis, 2005). R. The likelihood that a research finding is true depends on the ratio of true relations over null relations in the field, usually denoted R (Ioannidis, 2005). Out of the systems that researchers in the field would test out (not rejecting them a priori), how many of them are better than the current state of the art? The a priori likelihood of a relation being true, i.e., a new system being better than state of the art, is R/(R + 1). Note that while the space of reasonably motivated methods may seem big to researchers in the field, there is often more than one method that is better than the c</context>
</contexts>
<marker>Ioannidis, 2005</marker>
<rawString>John Ioannidis. 2005. Why most published research findings are false. PLoS Medicine, 2(8):696–701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in NLP.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28808" citStr="Jiang and Zhai, 2007" startWordPosition="4988" endWordPosition="4991">dout test data in Berg-Kirkpatrick et al. (2012) (see their Table 1, first row). The caveat is that this holds only in the absence of bias and with perfect metrics. In reality, though, our data sets are often severely biased (Berg-Kirkpatrick et al., 2012; Søgaard, 2013), and our metrics are far from perfect (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Schwartz et al., 2011; Tsarfaty et al., 2012). Here, we discuss how to address these challenges. Selection bias. The WSJ FALLACY (Section 1) has been widely discussed in the NLP literature (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007; Plank and van Noord, 2011). But if our test data is biased, how do we test whether System A performs better than System B in general? Søgaard (2013) suggests to predict significance across data sets. This only assumes that data sets are randomly chosen, e.g., not all from newswire corpora. This is also standard practice in the machine learning community (Demsar, 2006). Poor metrics. For tasks such as POS tagging and dependency parsing, our metrics are suboptimal (Manning, 2011; Schwartz et al., 2011; Tsarfaty et al., 2012). System A and System B may perform equally well as measured by some m</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: a package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In WAS.</booktitle>
<contexts>
<context position="14248" citStr="Lin, 2004" startWordPosition="2396" endWordPosition="2397">our cutoff (p - 0.15). The two systems were also reported to have overlapping confidence intervals in the shared task. 3.3 p-values across metrics In several NLP subfields, multiple metrics are in use. This happens in dependency parsing where multiple metrics (Schwartz et al., 2011; Tsarfaty et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric when reporting results, this increases the a priori chance of establishing significance. In POS tagging, most papers report significant improvements over tagging accuracy, but some report significant improvements over tagging accuracy of unknown </context>
<context position="28531" citStr="Lin, 2004" startWordPosition="4943" endWordPosition="4944">es us the level of confidence we expect as a research community, i.e., PPV = 0.95. Significance results would thus be more reliable and reduce type 1 error. 5 Discussion Incidentally, the p &lt; 0.0025 cut-off also leads to a 95% chance of seeing the same effect on heldout test data in Berg-Kirkpatrick et al. (2012) (see their Table 1, first row). The caveat is that this holds only in the absence of bias and with perfect metrics. In reality, though, our data sets are often severely biased (Berg-Kirkpatrick et al., 2012; Søgaard, 2013), and our metrics are far from perfect (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Schwartz et al., 2011; Tsarfaty et al., 2012). Here, we discuss how to address these challenges. Selection bias. The WSJ FALLACY (Section 1) has been widely discussed in the NLP literature (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007; Plank and van Noord, 2011). But if our test data is biased, how do we test whether System A performs better than System B in general? Søgaard (2013) suggests to predict significance across data sets. This only assumes that data sets are randomly chosen, e.g., not all from newswire corpora. This is also standard practice</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: a package for automatic evaluation of summaries. In WAS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Manning</author>
</authors>
<title>Part-of-speech tagging from 97% to 100%: Is it time for some linguistics? In CICLing.</title>
<date>2011</date>
<contexts>
<context position="24102" citStr="Manning (2011)" startWordPosition="4127" endWordPosition="4129"> LAS UAS LAS UAS 10% 14 % 100 % 9 % 100 % 33% 100 % 42 % 99 % 28 % 75 % 25% 15 % 100 % 23 % 100 % 52% 100 % 68 % 100 % 27 % 98 % 50% 19 % 100 % 25 % 100 % 78% 100 % 100 % 100 % 60 % 100 % 75% 22 % 100 % 41 % 100 % 97% 100 % 100 % 100 % 80 % 100 % 100% 0 % 100 % 36 % 100 % 100% 100 % 100 % 100 % 100 % 100 % Table 4: Ratio of positive results (p &lt; 0.05) for MALT-LIN VS. STANFORD-RNN at sample sizes (N) . on naturally occurring text such as in our experiments or at tailored test suites, typically focusing on hard phenomena (Rimell et al., 2009). While such test suites are valuable resources, cf. Manning (2011), they do introduce free choices for researchers, increasing the a priori chance of positive results. In NER, it is not uncommon to leave out sentences without any entity types from evaluation data. This biases evaluation toward high recall systems, and the choice between including them or not increases chances of positive results. 4 How likely are NLP findings to be false? The previous sections have demonstrated how many factors can contribute to reporting an erroneously significant result. Given those risks, it is natural to wonder how likely we are as a field to report false positives. This</context>
<context position="29291" citStr="Manning, 2011" startWordPosition="5072" endWordPosition="5073">J FALLACY (Section 1) has been widely discussed in the NLP literature (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007; Plank and van Noord, 2011). But if our test data is biased, how do we test whether System A performs better than System B in general? Søgaard (2013) suggests to predict significance across data sets. This only assumes that data sets are randomly chosen, e.g., not all from newswire corpora. This is also standard practice in the machine learning community (Demsar, 2006). Poor metrics. For tasks such as POS tagging and dependency parsing, our metrics are suboptimal (Manning, 2011; Schwartz et al., 2011; Tsarfaty et al., 2012). System A and System B may perform equally well as measured by some metric, but contribute very differently to downstream tasks. Elming et al. (2013) show how parsers trained on different annotation schemes lead to very different downstream results. This suggests that being wrong with respect to a gold standard, e.g., choosing NP analysis over a “correct” DP analysis, may in some cases lead to better downstream performance. See the discussion in Manning (2011) for POS tagging. One simple approach to this problem is to report results across availa</context>
</contexts>
<marker>Manning, 2011</marker>
<rawString>Chris Manning. 2011. Part-of-speech tagging from 97% to 100%: Is it time for some linguistics? In CICLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>Shared Task on Dependency Parsing.</title>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>In EMNLP-CoNLL.</booktitle>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007a. The CoNLL 2007 Shared Task on Dependency Parsing. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="10461" citStr="Nivre et al., 2007" startWordPosition="1778" endWordPosition="1781">c behind our reasoning and is not specific to NLP. It shows how likely we are to obtain a low p-value for the difference in means when sampling from exactly the same (Gaussian) distributions. For the NLP setups (2-4), we use off-the-shelf models or available runs, as described next. 3.1 Models and data We use pre-trained models for POS tagging and dependency parsing. For NER, we use the output of the best performing systems from the CoNLL 2003 shared task. In all three NLP setups, we compare the outcome of pairs of systems. The data sets we use for each of the NLP tasks are listed in Table 1 (Nivre et al., 2007a; Foster et Figure 1: Accuracies of LAPOS VS. STANFORD across 10 data sets. al., 2011; Tjong Kim Sang and De Meulder, 2003, LDC99T42; LDC2012T13). POS tagging. We compare the performance of two state-of-the-art newswire taggers across 10 evaluation data sets (see Table 1), namely the LAPOS tagger (Tsuruoka et al., 2011) and the STANFORD tagger (Toutanova et al., 2003), both trained on WSJ00–18. We use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the c</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007b. MaltParser: a language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Noreen</author>
</authors>
<title>Computer intensive methods for testing hypotheses.</title>
<date>1989</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="2022" citStr="Noreen, 1989" startWordPosition="320" endWordPosition="321">e whether System A is better than System B. If it follows a normal distribution, we can use Student’s t-test, for example. Effect sizes in NLP are generally not normally distributed or follow any of the other wellstudied distributions (Yeh, 2000; Søgaard, 2013). The standard significance testing methods in NLP are therefore rank- or randomization-based nonparametric tests (Yeh, 2000; Riezler and Maxwell, 2005; Berg-Kirkpatrick et al., 2012). Specifically, most system comparisons across words, sentences or documents use bootstrap tests (Efron and Tibshirani, 1993) or approximate randomization (Noreen, 1989), while studies that compare performance across data sets use rank-based tests such as Wilcoxon’s test. The question we wish to address here is: how likely is a research finding in NLP to be false? Naively, we would expect all reported findings to be true, but significance tests have their weaknesses, and sometimes researchers are forced to violate test assumptions and basic statistical methodology, e.g., when there is no one established metric, when we can’t run our models on full-length sentences, or when data is biased. For example, one such well-known bias from the tagging and parsing lite</context>
<context position="8385" citStr="Noreen, 1989" startWordPosition="1399" endWordPosition="1400">hen the number of observations is low, e.g., when evaluating performance across data sets, such tests seem to be the right choice (Demsar, 2006; Søgaard, 2013). The draw-back of rank-based tests is their relatively weak statistical power. When we reduce scores to ranks, we throw away information, and rank-based tests are therefore relatively conservative, potentially leading to high type 2 error rate (Q, i.e., the number of false negatives over trials). An alternative, however, are randomization-based tests such as the bootstrap test (Efron and Tibshirani, 1993) and approximate randomization (Noreen, 1989), which are the de facto standards in NLP. In this paper, we follow Berg-Kirkpatrick et al. (2012) in focusing on the bootstrap test. The bootstrap test is non-parametric and stronger than rank-based testing, i.e., introduces fewer type 2 errors. For small samples, however, it does so at the expense of a 2 higher type 1 error (a, i.e., the number of false positives). The reason for this is that for the bootstrap test to work, the original sample has to capture most of the variation in the population. If the sample is very small, though, this is likely not the case. Consequently, with small sam</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric Noreen. 1989. Computer intensive methods for testing hypotheses. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukus</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="14237" citStr="Papineni et al., 2002" startWordPosition="2391" endWordPosition="2395">EU-NG with p &lt; 0.05 as our cutoff (p - 0.15). The two systems were also reported to have overlapping confidence intervals in the shared task. 3.3 p-values across metrics In several NLP subfields, multiple metrics are in use. This happens in dependency parsing where multiple metrics (Schwartz et al., 2011; Tsarfaty et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric when reporting results, this increases the a priori chance of establishing significance. In POS tagging, most papers report significant improvements over tagging accuracy, but some report significant improvements over tagging accuracy </context>
<context position="28520" citStr="Papineni et al., 2002" startWordPosition="4939" endWordPosition="4942">h perfect metrics – gives us the level of confidence we expect as a research community, i.e., PPV = 0.95. Significance results would thus be more reliable and reduce type 1 error. 5 Discussion Incidentally, the p &lt; 0.0025 cut-off also leads to a 95% chance of seeing the same effect on heldout test data in Berg-Kirkpatrick et al. (2012) (see their Table 1, first row). The caveat is that this holds only in the absence of bias and with perfect metrics. In reality, though, our data sets are often severely biased (Berg-Kirkpatrick et al., 2012; Søgaard, 2013), and our metrics are far from perfect (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Schwartz et al., 2011; Tsarfaty et al., 2012). Here, we discuss how to address these challenges. Selection bias. The WSJ FALLACY (Section 1) has been widely discussed in the NLP literature (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007; Plank and van Noord, 2011). But if our test data is biased, how do we test whether System A performs better than System B in general? Søgaard (2013) suggests to predict significance across data sets. This only assumes that data sets are randomly chosen, e.g., not all from newswire corpora. This is also standa</context>
</contexts>
<marker>Papineni, Roukus, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukus, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL, pages 311–318, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Gertjan van Noord</author>
</authors>
<title>Effective measures of domain similarity for parsing.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<marker>Plank, van Noord, 2011</marker>
<rawString>Barbara Plank and Gertjan van Noord. 2011. Effective measures of domain similarity for parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Rankel</author>
<author>John Conroy</author>
<author>Eric Slud</author>
<author>Dianne O’Leary</author>
</authors>
<title>Ranking human and machine summarization systems.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<marker>Rankel, Conroy, Slud, O’Leary, 2011</marker>
<rawString>Peter Rankel, John Conroy, Eric Slud, and Dianne O’Leary. 2011. Ranking human and machine summarization systems. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="1821" citStr="Riezler and Maxwell, 2005" startWordPosition="291" endWordPosition="294">Effect size, i.e., one system’s improvements over another, can be seen as a random variable. If the random variable follows a known distribution, e.g., a normal distribution, we can use parametric tests to estimate whether System A is better than System B. If it follows a normal distribution, we can use Student’s t-test, for example. Effect sizes in NLP are generally not normally distributed or follow any of the other wellstudied distributions (Yeh, 2000; Søgaard, 2013). The standard significance testing methods in NLP are therefore rank- or randomization-based nonparametric tests (Yeh, 2000; Riezler and Maxwell, 2005; Berg-Kirkpatrick et al., 2012). Specifically, most system comparisons across words, sentences or documents use bootstrap tests (Efron and Tibshirani, 1993) or approximate randomization (Noreen, 1989), while studies that compare performance across data sets use rank-based tests such as Wilcoxon’s test. The question we wish to address here is: how likely is a research finding in NLP to be false? Naively, we would expect all reported findings to be true, but significance tests have their weaknesses, and sometimes researchers are forced to violate test assumptions and basic statistical methodolo</context>
<context position="6685" citStr="Riezler and Maxwell, 2005" startWordPosition="1120" endWordPosition="1123"> that the p &lt; 0.0025 cut-off level corresponds 1In many fields, including NLP, it has become good practice to report actual P-values, but we still need to understand how significance levels relate to the probability that research findings are false, to interpret such values. The fact that we propose a new cut-off level for the ideal case with perfect metrics and no bias does not mean that we do not recommend reporting actual P-values. to a Bonferroni correction for a family of m = 20 hypotheses. Our contributions Several authors have discussed significance testing in NLP before us (Yeh, 2000; Riezler and Maxwell, 2005; Berg-Kirkpatrick et al., 2012), but while our discussion touches on many of the same topics, this paper is to the best of our knowledge the first to: a) show experimentally how sensitive p-values are to sample size, i.e., that in standard NLP experiments, significant effects may actually disappear by adding more data. b) show experimentally that multiple metrics and the use of covariates in evaluation increase the probability of positive test results. c) show that even under the assumption of perfect metrics and unbiased data, as well as our estimates of type 1 and 2 error in NLP, you need a</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
</authors>
<title>Unbounded dependency recovery for parser evaluation.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="24035" citStr="Rimell et al., 2009" startWordPosition="4115" endWordPosition="4118"> evaluated 6 N Emails Newsgrs Reviews Weblogs WSJ LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS 10% 14 % 100 % 9 % 100 % 33% 100 % 42 % 99 % 28 % 75 % 25% 15 % 100 % 23 % 100 % 52% 100 % 68 % 100 % 27 % 98 % 50% 19 % 100 % 25 % 100 % 78% 100 % 100 % 100 % 60 % 100 % 75% 22 % 100 % 41 % 100 % 97% 100 % 100 % 100 % 80 % 100 % 100% 0 % 100 % 36 % 100 % 100% 100 % 100 % 100 % 100 % 100 % Table 4: Ratio of positive results (p &lt; 0.05) for MALT-LIN VS. STANFORD-RNN at sample sizes (N) . on naturally occurring text such as in our experiments or at tailored test suites, typically focusing on hard phenomena (Rimell et al., 2009). While such test suites are valuable resources, cf. Manning (2011), they do introduce free choices for researchers, increasing the a priori chance of positive results. In NER, it is not uncommon to leave out sentences without any entity types from evaluation data. This biases evaluation toward high recall systems, and the choice between including them or not increases chances of positive results. 4 How likely are NLP findings to be false? The previous sections have demonstrated how many factors can contribute to reporting an erroneously significant result. Given those risks, it is natural to </context>
</contexts>
<marker>Rimell, Clark, Steedman, 2009</marker>
<rawString>Laura Rimell, Stephen Clark, and Mark Steedman. 2009. Unbounded dependency recovery for parser evaluation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Schwartz</author>
<author>Omri Abend</author>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="13921" citStr="Schwartz et al., 2011" startWordPosition="2339" endWordPosition="2342">nt improvements across a few data sets. Dependency parsing. Using the bootstrap test over sentences, we get the p-values in Table 3. We see that differences are always significant wrt. UAS, and in most cases wrt. LAS. NER. Here we use the macro-fl as our standard metric. FLORIAN is not significantly better than CHIEU-NG with p &lt; 0.05 as our cutoff (p - 0.15). The two systems were also reported to have overlapping confidence intervals in the shared task. 3.3 p-values across metrics In several NLP subfields, multiple metrics are in use. This happens in dependency parsing where multiple metrics (Schwartz et al., 2011; Tsarfaty et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. The existence of several metrics is n</context>
<context position="17335" citStr="Schwartz et al., 2011" startWordPosition="2915" endWordPosition="2918">Table 2). If we allow for a choice between tagging accuracy and sentence-level accuracy, we see a significant improvement in 4/10 cases, i.e., for 4/10 data sets the effect is significance wrt. at least one metric. If we allow for a free choice between all three metrics (TA, UA, and SA), we observe significance in 9/10 cases. This way the existence of multiple metrics almost guarantees significant differences. Note that there are only two data sets (Answers and Spoken), where all metric differences appear significant. Dependency parsing. While there are multiple metrics in dependency parsing (Schwartz et al., 2011; Tsarfaty et al., 2012), we focus on the two standard metrics: labeled (LAS) and unlabeled attachment score (UAS) (Buchholz and Marsi, 2006). If we just consider the results in Table 3, i.e., only the comparison of MALT-LIN VS. STANFORD-RNN, we observe significant improvements in all cases, if we allow for a free choice between metrics. Bod (2000) provides a good example of a parsing paper evaluating models using different metrics on different test sets. Chen et al. (2008), similarly, only report UAS. NER. While macro-f1 is fairly standard in NER, we do have several available multiple metrics</context>
<context position="28580" citStr="Schwartz et al., 2011" startWordPosition="4949" endWordPosition="4952">ct as a research community, i.e., PPV = 0.95. Significance results would thus be more reliable and reduce type 1 error. 5 Discussion Incidentally, the p &lt; 0.0025 cut-off also leads to a 95% chance of seeing the same effect on heldout test data in Berg-Kirkpatrick et al. (2012) (see their Table 1, first row). The caveat is that this holds only in the absence of bias and with perfect metrics. In reality, though, our data sets are often severely biased (Berg-Kirkpatrick et al., 2012; Søgaard, 2013), and our metrics are far from perfect (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Schwartz et al., 2011; Tsarfaty et al., 2012). Here, we discuss how to address these challenges. Selection bias. The WSJ FALLACY (Section 1) has been widely discussed in the NLP literature (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007; Plank and van Noord, 2011). But if our test data is biased, how do we test whether System A performs better than System B in general? Søgaard (2013) suggests to predict significance across data sets. This only assumes that data sets are randomly chosen, e.g., not all from newswire corpora. This is also standard practice in the machine learning community (Demsar, 2006)</context>
</contexts>
<marker>Schwartz, Abend, Reichart, Rappoport, 2011</marker>
<rawString>Roy Schwartz, and Omri Abend, Roi Reichart, and Ari Rappoport. 2011. Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Simmons</author>
<author>Leif Nelson</author>
<author>Uri Simonsohn</author>
</authors>
<title>False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant.</title>
<date>2011</date>
<journal>Psychological Science,</journal>
<volume>22</volume>
<issue>11</issue>
<contexts>
<context position="4063" citStr="Simmons et al., 2011" startWordPosition="654" endWordPosition="657"> across domains (cf. WSJ FALLACY). More importantly, though, it may not even hold on a sub-sample of the test data, or if we added more data points to the test set. Below, we show that in 6/10 of our POS tagger evaluations, significant effects become insignificant by (randomly) adding more test data. 1 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 1–10, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics Covariates. Sometimes we may bin our results by variables that are actually predictive of the outcome (covariates) (Simmons et al., 2011). In some subfields of NLP, such as machine translation or (unsupervised) syntactic parsing, for example, it is common to report results that only hold for sentences up to some length. If a system A is reported to be better than a system B on sentences up to some length, A need not be better than B, neither for a different length nor in general, since sentence length may actually be predictive of A being better than B. Multiple metrics. In several subfields of NLP, we have various evaluation metrics. However, if a system A is reported to be better than a system B with respect to some metric M1</context>
<context position="15053" citStr="Simmons et al., 2011" startWordPosition="2515" endWordPosition="2518">tence-level accuracies, or number of exact matches. The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric when reporting results, this increases the a priori chance of establishing significance. In POS tagging, most papers report significant improvements over tagging accuracy, but some report significant improvements over tagging accuracy of unknown words, e.g., Denis and Sagot (2009) and Umansky-Pesin et al. (2010). This corresponds to the situation in psychology where researchers cherry-pick between several dependent variables (Simmons et al., 2011), which also increases the chance of finding a significant correlation. Toy example. We draw two times 100 values from identical (0,1)-Gaussians 1000 times and calculate a t-test for two independent samples. This corresponds to testing the effect size between two systems on a 1000 randomly chosen test sets with N = 100. Since we are sampling from the same distribution, the chance of p &lt; n should be smaller than t.. In our simulation, the empirical chance of obtaining p &lt; 0.01 is .8%, and the chance of obtaining p &lt; 0.05 is 4.8%, as expected. If we simulate a free choice between two metrics by </context>
<context position="19084" citStr="Simmons et al., 2011" startWordPosition="3208" endWordPosition="3211">IEU-NG. 3.4 p-values across sample size We now show that p-values are sensitive to sample size. While it is well-known that studies with low statistical power have a reduced chance of detecting true effects, studies with low statistical power are also more likely to introduce false positives (Button et al., 2013). This, combined with the fact that free choice between different sample Figure 2: The distribution of p-values with (above) and without (below) multiple metrics. Figure 3: POS tagging p-values varying sample sizes (p &lt; 0.05 shaded). sizes also increases the chance of false positives (Simmons et al., 2011), is a potential source of error in NLP. Toy example. The plot in Figure 2 shows the distribution of p-values across 1000 bootstrap tests (above), compared to the distribution of p-values with a free choice of four sample sizes. It is clear that the existence of multiple metrics makes the probability of a positive result much higher. POS tagging. The same holds for POS tagging. We plot the p-values across various sample sizes in Figure 3. Note that even when we ignore the smallest sample size (500 words), where results may be rather unreliable, it still holds that for Twitter, Answers, Newsgrs</context>
</contexts>
<marker>Simmons, Nelson, Simonsohn, 2011</marker>
<rawString>Joseph Simmons, Leif Nelson, and Uri Simonsohn. 2011. False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11):1359–1366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Chris Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="11140" citStr="Socher et al., 2013" startWordPosition="1887" endWordPosition="1890">oss 10 data sets. al., 2011; Tjong Kim Sang and De Meulder, 2003, LDC99T42; LDC2012T13). POS tagging. We compare the performance of two state-of-the-art newswire taggers across 10 evaluation data sets (see Table 1), namely the LAPOS tagger (Tsuruoka et al., 2011) and the STANFORD tagger (Toutanova et al., 2003), both trained on WSJ00–18. We use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available runs of the two best systems from the CoNLL 2003 shared task, namely FLORIAN (Florian et al., 2003) and CHIEU-NG (Chieu and Ng, 2003).3 3.2 Standard comparisons POS tagging. Figure 1 shows that the LAPOS tagger is marginally better than STANFORD on macro-average, but it is also significantly better? If we use the bootstrap test over tagging accuracies, t</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Chris Manning, and Andrew Ng. 2013. Parsing with compositional vector grammars. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Estimating effect size across datasets. In NAACL.</title>
<date>2013</date>
<contexts>
<context position="1670" citStr="Søgaard, 2013" startWordPosition="272" endWordPosition="273">nalysis from previous experiments, and research findings often consist in system comparisons showing that System A is better than System B. Effect size, i.e., one system’s improvements over another, can be seen as a random variable. If the random variable follows a known distribution, e.g., a normal distribution, we can use parametric tests to estimate whether System A is better than System B. If it follows a normal distribution, we can use Student’s t-test, for example. Effect sizes in NLP are generally not normally distributed or follow any of the other wellstudied distributions (Yeh, 2000; Søgaard, 2013). The standard significance testing methods in NLP are therefore rank- or randomization-based nonparametric tests (Yeh, 2000; Riezler and Maxwell, 2005; Berg-Kirkpatrick et al., 2012). Specifically, most system comparisons across words, sentences or documents use bootstrap tests (Efron and Tibshirani, 1993) or approximate randomization (Noreen, 1989), while studies that compare performance across data sets use rank-based tests such as Wilcoxon’s test. The question we wish to address here is: how likely is a research finding in NLP to be false? Naively, we would expect all reported findings to </context>
<context position="7510" citStr="Søgaard, 2013" startWordPosition="1265" endWordPosition="1266"> size, i.e., that in standard NLP experiments, significant effects may actually disappear by adding more data. b) show experimentally that multiple metrics and the use of covariates in evaluation increase the probability of positive test results. c) show that even under the assumption of perfect metrics and unbiased data, as well as our estimates of type 1 and 2 error in NLP, you need at least p &lt; 0.0025 to reduce the probability of a research finding being false to be &lt; 5%. 2 Significance testing in NLP Most NLP metric for comparing system outputs can be shown to be non-normally distributed (Søgaard, 2013) and hence, we generally cannot use statistical tests that rely on such an assumption, e.g., Student’s t-test. One alternative to such tests are non-parametric rank-based tests such as Wilcoxon’s test. Rank-based tests are sometimes used in NLP, and especially when the number of observations is low, e.g., when evaluating performance across data sets, such tests seem to be the right choice (Demsar, 2006; Søgaard, 2013). The draw-back of rank-based tests is their relatively weak statistical power. When we reduce scores to ranks, we throw away information, and rank-based tests are therefore relat</context>
<context position="12923" citStr="Søgaard (2013)" startWordPosition="2170" endWordPosition="2171">2 Reviews 0.0020 0.0543 0.0585 0.0562 Weblogs 0.2480 0.0024 0.2435 0.9390 WSJ 0.4497 0.0024 0.2435 0.9390 Twitter 0.4497 0.0924 0.1111 0.7853 Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p &lt; 0.05 gray-shaded). LAS UAS Table 3: Parsing p-values (MALT-LIN VS. STANFORD-RNN) across LAS and UAS (p &lt; 0.05 gray-shaded). than STANFORD, but in one case it is the other way around. If we do a Wilcoxon test over the results on the 10 data sets, following the methodology in Demsar (2006) and Søgaard (2013), the difference, which is -0.12% on macro-average, is not significant (p - 0.1394). LAPOS is thus not significantly better than STANFORD across data sets, but as we have already seen, it is significantly better on some data sets. So if we allow ourselves to cherry-pick our data sets and report significance over word-level tagging accuracies, we can at least report significant improvements across a few data sets. Dependency parsing. Using the bootstrap test over sentences, we get the p-values in Table 3. We see that differences are always significant wrt. UAS, and in most cases wrt. LAS. NER. </context>
<context position="28459" citStr="Søgaard, 2013" startWordPosition="4929" endWordPosition="4930">s is the cut-off that – in the absence of bias and with perfect metrics – gives us the level of confidence we expect as a research community, i.e., PPV = 0.95. Significance results would thus be more reliable and reduce type 1 error. 5 Discussion Incidentally, the p &lt; 0.0025 cut-off also leads to a 95% chance of seeing the same effect on heldout test data in Berg-Kirkpatrick et al. (2012) (see their Table 1, first row). The caveat is that this holds only in the absence of bias and with perfect metrics. In reality, though, our data sets are often severely biased (Berg-Kirkpatrick et al., 2012; Søgaard, 2013), and our metrics are far from perfect (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Schwartz et al., 2011; Tsarfaty et al., 2012). Here, we discuss how to address these challenges. Selection bias. The WSJ FALLACY (Section 1) has been widely discussed in the NLP literature (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007; Plank and van Noord, 2011). But if our test data is biased, how do we test whether System A performs better than System B in general? Søgaard (2013) suggests to predict significance across data sets. This only assumes that data sets are randomly cho</context>
</contexts>
<marker>Søgaard, 2013</marker>
<rawString>Anders Søgaard. 2013. Estimating effect size across datasets. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In CoNLL.</booktitle>
<marker>Erik, 2003</marker>
<rawString>Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Chris Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="10832" citStr="Toutanova et al., 2003" startWordPosition="1839" endWordPosition="1842">ng. For NER, we use the output of the best performing systems from the CoNLL 2003 shared task. In all three NLP setups, we compare the outcome of pairs of systems. The data sets we use for each of the NLP tasks are listed in Table 1 (Nivre et al., 2007a; Foster et Figure 1: Accuracies of LAPOS VS. STANFORD across 10 data sets. al., 2011; Tjong Kim Sang and De Meulder, 2003, LDC99T42; LDC2012T13). POS tagging. We compare the performance of two state-of-the-art newswire taggers across 10 evaluation data sets (see Table 1), namely the LAPOS tagger (Tsuruoka et al., 2011) and the STANFORD tagger (Toutanova et al., 2003), both trained on WSJ00–18. We use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available runs of the two best systems from the CoNLL 2003</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Chris Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Joakim Nivre</author>
<author>Evelina Andersson</author>
</authors>
<title>Cross-framework evaluation for statistical parsing.</title>
<date>2012</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="13945" citStr="Tsarfaty et al., 2012" startWordPosition="2343" endWordPosition="2346">a few data sets. Dependency parsing. Using the bootstrap test over sentences, we get the p-values in Table 3. We see that differences are always significant wrt. UAS, and in most cases wrt. LAS. NER. Here we use the macro-fl as our standard metric. FLORIAN is not significantly better than CHIEU-NG with p &lt; 0.05 as our cutoff (p - 0.15). The two systems were also reported to have overlapping confidence intervals in the shared task. 3.3 p-values across metrics In several NLP subfields, multiple metrics are in use. This happens in dependency parsing where multiple metrics (Schwartz et al., 2011; Tsarfaty et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. The existence of several metrics is not in itself a problem, </context>
<context position="17359" citStr="Tsarfaty et al., 2012" startWordPosition="2919" endWordPosition="2922">or a choice between tagging accuracy and sentence-level accuracy, we see a significant improvement in 4/10 cases, i.e., for 4/10 data sets the effect is significance wrt. at least one metric. If we allow for a free choice between all three metrics (TA, UA, and SA), we observe significance in 9/10 cases. This way the existence of multiple metrics almost guarantees significant differences. Note that there are only two data sets (Answers and Spoken), where all metric differences appear significant. Dependency parsing. While there are multiple metrics in dependency parsing (Schwartz et al., 2011; Tsarfaty et al., 2012), we focus on the two standard metrics: labeled (LAS) and unlabeled attachment score (UAS) (Buchholz and Marsi, 2006). If we just consider the results in Table 3, i.e., only the comparison of MALT-LIN VS. STANFORD-RNN, we observe significant improvements in all cases, if we allow for a free choice between metrics. Bod (2000) provides a good example of a parsing paper evaluating models using different metrics on different test sets. Chen et al. (2008), similarly, only report UAS. NER. While macro-f1 is fairly standard in NER, we do have several available multiple metrics, including the unlabele</context>
<context position="28604" citStr="Tsarfaty et al., 2012" startWordPosition="4953" endWordPosition="4956">ity, i.e., PPV = 0.95. Significance results would thus be more reliable and reduce type 1 error. 5 Discussion Incidentally, the p &lt; 0.0025 cut-off also leads to a 95% chance of seeing the same effect on heldout test data in Berg-Kirkpatrick et al. (2012) (see their Table 1, first row). The caveat is that this holds only in the absence of bias and with perfect metrics. In reality, though, our data sets are often severely biased (Berg-Kirkpatrick et al., 2012; Søgaard, 2013), and our metrics are far from perfect (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Schwartz et al., 2011; Tsarfaty et al., 2012). Here, we discuss how to address these challenges. Selection bias. The WSJ FALLACY (Section 1) has been widely discussed in the NLP literature (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007; Plank and van Noord, 2011). But if our test data is biased, how do we test whether System A performs better than System B in general? Søgaard (2013) suggests to predict significance across data sets. This only assumes that data sets are randomly chosen, e.g., not all from newswire corpora. This is also standard practice in the machine learning community (Demsar, 2006). Poor metrics. For task</context>
</contexts>
<marker>Tsarfaty, Nivre, Andersson, 2012</marker>
<rawString>Reut Tsarfaty, Joakim Nivre, and Evelina Andersson. 2012. Cross-framework evaluation for statistical parsing. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Kazama</author>
</authors>
<title>Learning with lookahead: can history-based models rival globally optimized models? In CoNLL.</title>
<date>2011</date>
<contexts>
<context position="10783" citStr="Tsuruoka et al., 2011" startWordPosition="1830" endWordPosition="1833">ined models for POS tagging and dependency parsing. For NER, we use the output of the best performing systems from the CoNLL 2003 shared task. In all three NLP setups, we compare the outcome of pairs of systems. The data sets we use for each of the NLP tasks are listed in Table 1 (Nivre et al., 2007a; Foster et Figure 1: Accuracies of LAPOS VS. STANFORD across 10 data sets. al., 2011; Tjong Kim Sang and De Meulder, 2003, LDC99T42; LDC2012T13). POS tagging. We compare the performance of two state-of-the-art newswire taggers across 10 evaluation data sets (see Table 1), namely the LAPOS tagger (Tsuruoka et al., 2011) and the STANFORD tagger (Toutanova et al., 2003), both trained on WSJ00–18. We use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available</context>
</contexts>
<marker>Tsuruoka, Miyao, Kazama, 2011</marker>
<rawString>Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Kazama. 2011. Learning with lookahead: can history-based models rival globally optimized models? In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shulamit Umansky-Pesin</author>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>A multi-domain web-based algorithm for POS tagging of unknown words.</title>
<date>2010</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="14915" citStr="Umansky-Pesin et al. (2010)" startWordPosition="2496" endWordPosition="2499">, 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric when reporting results, this increases the a priori chance of establishing significance. In POS tagging, most papers report significant improvements over tagging accuracy, but some report significant improvements over tagging accuracy of unknown words, e.g., Denis and Sagot (2009) and Umansky-Pesin et al. (2010). This corresponds to the situation in psychology where researchers cherry-pick between several dependent variables (Simmons et al., 2011), which also increases the chance of finding a significant correlation. Toy example. We draw two times 100 values from identical (0,1)-Gaussians 1000 times and calculate a t-test for two independent samples. This corresponds to testing the effect size between two systems on a 1000 randomly chosen test sets with N = 100. Since we are sampling from the same distribution, the chance of p &lt; n should be smaller than t.. In our simulation, the empirical chance of </context>
</contexts>
<marker>Umansky-Pesin, Reichart, Rappoport, 2010</marker>
<rawString>Shulamit Umansky-Pesin, Roi Reichart, and Ari Rappoport. 2010. A multi-domain web-based algorithm for POS tagging of unknown words. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1654" citStr="Yeh, 2000" startWordPosition="270" endWordPosition="271"> as error analysis from previous experiments, and research findings often consist in system comparisons showing that System A is better than System B. Effect size, i.e., one system’s improvements over another, can be seen as a random variable. If the random variable follows a known distribution, e.g., a normal distribution, we can use parametric tests to estimate whether System A is better than System B. If it follows a normal distribution, we can use Student’s t-test, for example. Effect sizes in NLP are generally not normally distributed or follow any of the other wellstudied distributions (Yeh, 2000; Søgaard, 2013). The standard significance testing methods in NLP are therefore rank- or randomization-based nonparametric tests (Yeh, 2000; Riezler and Maxwell, 2005; Berg-Kirkpatrick et al., 2012). Specifically, most system comparisons across words, sentences or documents use bootstrap tests (Efron and Tibshirani, 1993) or approximate randomization (Noreen, 1989), while studies that compare performance across data sets use rank-based tests such as Wilcoxon’s test. The question we wish to address here is: how likely is a research finding in NLP to be false? Naively, we would expect all repor</context>
<context position="6658" citStr="Yeh, 2000" startWordPosition="1118" endWordPosition="1119">sizes. Note that the p &lt; 0.0025 cut-off level corresponds 1In many fields, including NLP, it has become good practice to report actual P-values, but we still need to understand how significance levels relate to the probability that research findings are false, to interpret such values. The fact that we propose a new cut-off level for the ideal case with perfect metrics and no bias does not mean that we do not recommend reporting actual P-values. to a Bonferroni correction for a family of m = 20 hypotheses. Our contributions Several authors have discussed significance testing in NLP before us (Yeh, 2000; Riezler and Maxwell, 2005; Berg-Kirkpatrick et al., 2012), but while our discussion touches on many of the same topics, this paper is to the best of our knowledge the first to: a) show experimentally how sensitive p-values are to sample size, i.e., that in standard NLP experiments, significant effects may actually disappear by adding more data. b) show experimentally that multiple metrics and the use of covariates in evaluation increase the probability of positive test results. c) show that even under the assumption of perfect metrics and unbiased data, as well as our estimates of type 1 and</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000322">
<title confidence="0.993737">
Information Navigation System Based on POMDP that Tracks User Focus
</title>
<author confidence="0.996741">
Koichiro Yoshino Tatsuya Kawahara
</author>
<affiliation confidence="0.996143">
School of Informatics, Kyoto University
</affiliation>
<address confidence="0.513003">
Sakyo-ku, Kyoto, 606-8501, Japan
</address>
<email confidence="0.994159">
yoshino@ar.media.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.993786" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933769230769">
We present a spoken dialogue system for
navigating information (such as news ar-
ticles), and which can engage in small
talk. At the core is a partially observ-
able Markov decision process (POMDP),
which tracks user’s state and focus of at-
tention. The input to the POMDP is pro-
vided by a spoken language understanding
(SLU) component implemented with lo-
gistic regression (LR) and conditional ran-
dom fields (CRFs). The POMDP selects
one of six action classes; each action class
is implemented with its own module.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963142857143">
A large number of spoken dialogue systems have
been investigated and many systems are deployed
in the real world. Spoken dialogue applications
that interact with a diversity of users are avail-
able on smart-phones. However, current appli-
cations are based on simple question answering
and the system requires a clear query or a def-
inite task goal. Therefore, next-generation dia-
logue systems should engage in casual interactions
with users who do not have a clear intention or a
task goal. Such systems include a sightseeing nav-
igation system that uses tour guide books or doc-
uments in Wikipedia (Misu and Kawahara, 2010),
and a news navigation system that introduces news
articles updated day-by-day (Yoshino et al., 2011;
Pan et al., 2012). In this paper, we develop an in-
formation navigation system that provides infor-
mation even if the user request is not necessarily
clear and there is not a matching document in the
knowledge base. The user and the system converse
on the current topic and the system provides po-
tentially useful information for the user.
Dialogue management of this kind of systems
was usually made in a heuristic manner and based
on simple rules (Dahl et al., 1994; Bohus and Rud-
nicky, 2003). There is not a clear principle nor
established methodology to design and implement
casual conversation systems. In the past years, ma-
chine learning, particularly reinforcement learn-
ing, have been investigated for dialogue manage-
ment. MDPs and POMDPs are now widely used
to model and train dialogue managers (Levin et
al., 2000; Williams and Young, 2007; Young et
al., 2010; Yoshino et al., 2013b). However, the
conventional scheme assumes that the task and di-
alogue goal can be clearly stated and readily en-
coded in the RL reward function. This is not true
in casual conversation or when browsing informa-
tion.
Some previous work has tackled with this prob-
lem. In a conversational chatting system (Shibata
et al., 2014), users were asked to make evalua-
tion at the end of each dialogue session, to define
rewards for reinforcement learning. In a listen-
ing dialogue system (Meguro et al., 2010), levels
of satisfaction were annotated in logs of dialogue
sessions to train a discriminative model. These
approaches require costly input from users or de-
velopers, who provide labels and evaluative judg-
ments.
In this work, we present a framework in which
reward is defined for the quality of system actions
and also for encouraging long interactions, in con-
trast to the conventional framework. Moreover,
user focus is tracked to make appropriate actions,
which are more rewarded.
</bodyText>
<sectionHeader confidence="0.952966" genericHeader="introduction">
2 Conversational Information
Navigation System
</sectionHeader>
<bodyText confidence="0.999410333333333">
In natural human-human conversation, partici-
pants have topics they plan to talk about, and they
progress through the dialogue in accordance with
the topics (Schegloff and Sacks, 1973). We call
this dialogue style “information navigation.” An
example is shown in Figure 1. First, the speaker
</bodyText>
<page confidence="0.988289">
32
</page>
<note confidence="0.918322">
Proceedings of the SIGDIAL 2014 Conference, pages 32–40,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.99923">
Figure 1: An example of information navigation.
</figureCaption>
<figure confidence="0.850425">
Other modules
</figure>
<figureCaption confidence="0.9764705">
Figure 2: Overview of the information navigation
system.
</figureCaption>
<bodyText confidence="0.999454071428571">
offers a new topic and probes the interest of the
listener. If the listener shows interest, the speaker
describes details of the topic. If the listener asks
a specific question, the speaker answers the ques-
tion. On the other hand, if the listener is not inter-
ested in the topic, the speaker avoids the details of
that topic, and changes the topic. Topics are often
taken from current news.
In our past work, we have developed a news
navigation system (Yoshino et al., 2011) based on
this dialogue structure. The system provides top-
ics collected from Web news texts, and the user
gets information according to his interests and
queries.
</bodyText>
<subsectionHeader confidence="0.997452">
2.1 System overview
</subsectionHeader>
<bodyText confidence="0.999919113636364">
An overview of the proposed system is depicted
in Figure 2. The system has six modules, each of
which implements a class of actions. Each module
takes as input a recognized user utterance, an an-
alyzed predicate-argument (P-A) structure and the
detected user focus.
The system begins dialogues by selecting the
“topic presentation (TP)” module, which presents
a new topic selected from a news article. The sys-
tem chooses the next module based on the user’s
response. In our task, the system assumes that
each news article corresponds to a single topic,
and the system presents a headline of news in the
TP module. If the user shows interest (positive
response) in the topic without any specific ques-
tions, the system selects the “story telling (ST)”
module to give details of the news. In the ST mod-
ule, the system provides a summary of the news
article by using lead sentences. The system can
also provide related topics with the “proactive pre-
sentation (PP)” module. This module is invoked
by system initiative; this module is not invoked by
any user request. If the user makes a specific ques-
tion regarding the topic, the system switches to the
“question answering (QA)” module to answer the
question. This module answers questions on the
presented topic and related topics.
The modules of PP and QA are based on a di-
alogue framework which uses the similarity of P-
A structures (Yoshino et al., 2011). This frame-
work defines the similarity of P-A structures be-
tween user queries and news articles, and retrieves
or recommends the appropriate sentence from the
news articles. This method searches for appropri-
ate information from automatically parsed docu-
ments by referring to domain knowledge that is
automatically extracted from domain corpus.
Transitions between the modules are allowed as
shown in Figure 2. The modules “greeting (GR)”
and “keep silence (KS)” are also implemented.
GR module generates fixed greeting patterns by
using regular expression matching. In terms of
dialogue flow, these modules can be used at any
time.
</bodyText>
<subsectionHeader confidence="0.999286">
2.2 User focus in information navigation
</subsectionHeader>
<bodyText confidence="0.999898363636364">
“Focus” in discourse is “attentional state (that)
contains information about the objects, properties,
relations, and discourse intentions that are most
salient at any given point.” (Grosz and Sidner,
1986). The user has specific attention to an ob-
ject if the user utterance contains the focus. In
this work, we define the user focus as “the main
piece of information of interest to the user.” It
makes a central component when making a reply
or selecting relevant topics at the current dialogue
state. For example, given “Did Ichiro perform bril-
</bodyText>
<figure confidence="0.9997673">
Speaker (system) Listener (user)
Topic 1
Offer a topic
Be interested in the
topic
Present the detail
Make a question
Answer the
question
Be silent
Topic 2
Offer a new topic
(topic 2)
Not be interested in
Topic 3
Offer a new topic
(topic 3)
Make a question
・・・
Dialogue states
・・・
Modules of current topic
Modules of related topics
Topic
Presentation
(TP)
Related topics
Topic Topic
Topic
Topic 1
Topic
Topic
Topic 2
Selected
topic
Topic 3
Draw new topic
Topic
・・・
Topic
Topic N
・・・
・・・
・・・
Story Telling
(ST)
System-
initiative
Question Answering
(QA)
User-initiative
Proactive
Presentation
(PP)
System-
initiative
Greeting
(GR)
Keep silence
(KS)
</figure>
<page confidence="0.992902">
33
</page>
<bodyText confidence="0.9109102">
SYS, tp, “Matsui, a part-time coach for the Giants, demonstrated 5 homeruns. The Giants president
told him &amp;quot;Come back as a player!”
USR, st, 0, ” Give me the detail?”, ”Give:me/0 the:detail/0”
SYS, st, ”Matsui joined the extra batting training in the Giants&apos; camp, and demonstrated his batting
to young players.”
USR, qa, 1, “Will Matsui become the Giants&apos; manager in the future?”, ”Matsui/0 will:become/0
the Giants&apos; manager/1 in the future/0”
SYS, qa, ”The director of Giants told him &amp;quot;Come back as a player!””
USR, tp, 0, ”Next news, please.”, ”Next:news/0 please/0”
SYS, tp, ” To be a ace pitcher, has Fujinami improved from the rookie year?
Figure 3: An example of annotation for collected dialogue. System utterances have a tuple of three
elements separated by a comma: speaker, called module and utterance. User utterances have a tuple of
four elements: speaker, the module the user request falls in, binary information of user focus, utterance
and user focus annotation on each phrase or P-A element. (This example is translated from Japanese)
liantly?,” user focus is “Ichiro” because the sys-
tem reply should include information on Ichiro.
This information is annotated on content words or
named entities in a user utterance. In the POMDP,
decisions are made based on whether any user fo-
cus was detected in the user’s utterance.
</bodyText>
<sectionHeader confidence="0.986739" genericHeader="method">
3 Spoken Language Understanding
(SLU)
</sectionHeader>
<bodyText confidence="0.999969285714286">
In this section, we present the spoken language un-
derstanding components of our system. It detects
the user’s focus and intention and provides these
to the dialogue manager. These spoken language
understanding modules are formulated with a sta-
tistical model to give likelihoods which are used
in POMDP.
</bodyText>
<subsectionHeader confidence="0.99937">
3.1 Dialogue data
</subsectionHeader>
<bodyText confidence="0.999988736842105">
We collected 606 utterances (from 10 users) with a
rule-based dialogue system (Yoshino et al., 2011).
We annotated two kinds of tags: user intention (6
tags defined in Section 3.3), and focus information
defined in Section 2.2. An example of annotation
is shown in Figure 3. We highlighted annotation
points in the bold font.
To prepare the training data, each utterance was
labeled with one of the six modules, indicating the
best module to respond. In addition, each phrase
or P-A elements is labeled to indicated whether it
is the user’s focus or not. The user focus is deter-
mined by the attributes (=specifications of words
in the domain) and preference order of phrases to
identify the most appropriate information that the
user wants to know. For example, in the second
user utterance in Figure 3, the user’s focus is the
phrase “the Giants’ manager”. These tags are an-
notated by one person.
</bodyText>
<subsectionHeader confidence="0.998949">
3.2 User focus detection based on CRF
</subsectionHeader>
<bodyText confidence="0.999983896551724">
To detect the user focus, we use a conditional
random field (CRF) 1. The problem is defined as
a sequential labeling of the focus labels to a se-
quence of the phrases of the user utterance. Fea-
tures used are shown in the Table 1. ORDER fea-
tures are the order of the phrase in the sequence
and in the P-A structure. We incorporate these
features because the user focus often appears in
the first phrase of the user utterance. POS fea-
tures are part-of-speech (POS) tags and their pairs
in the phrase. P-A features are semantic role of the
P-A structure. We also incorporate the domain-
dependent predicate-argument (P-A) scores that
are defined with an unsupervised method (Yoshino
et al., 2011). The score is discretized to 0.01, 0.02,
0.05, 0.1, 0.2, 0.5.
Table 2 shows the accuracy of user focus de-
tection, which was conducted via five-fold cross-
validation. “Phrase” is phrase-base accuracy and
“sentence” indicates whether the presence of any
user focus phrase was correctly detected (or not),
regardless of whether the correct phrase was iden-
tified. This table indicates that WORD features
are effective for detecting the user focus, but they
are not essential for in the sentence-level accuracy.
In this paper, we aim for portability across do-
mains; therefore the dialogue manager only uses
the sentence-level feature, so in our system we do
not user the WORD features.
</bodyText>
<subsectionHeader confidence="0.996963">
3.3 User intention analysis based on LR
</subsectionHeader>
<bodyText confidence="0.993382">
The module classifies the user intention from the
user utterance. We define six intentions as below.
</bodyText>
<listItem confidence="0.7911895">
• TP: request to the TP module.
&apos;CRFsuite (Okazaki, 2007).
</listItem>
<page confidence="0.998711">
34
</page>
<tableCaption confidence="0.997671">
Table 1: Features of user focus detection.
</tableCaption>
<table confidence="0.9996223125">
feature type feature
ORDER Rank in a sequence of phrases
Rank in a sequence of elements of P-A
POS POS tags in the phrase
POS tag sequence
POSORDER Pair of POS tag and its order in the
phrase
P-A Which semantic role the phrase has
Which semantic roles exist on the
utterance
P-AORDER Pair of semantic role and its order in
the utterance
P-A score P-A templates score
WORD Words in the phrase
Pair of words in the phrase
Pair of word and its order in the phrase
</table>
<tableCaption confidence="0.980629">
Table 2: Accuracy of user focus detection.
</tableCaption>
<table confidence="0.8871868">
Accuracy
phrase 86.7%
phrase + (WORD) 90.3%
sentence (focus exist or not) 99.8%
sentence (focus exist or not) + (WORD) 99.8%
</table>
<listItem confidence="0.999894">
• ST: request to the ST module.
• QA: request to the QA module.
• GR: greeting to the GR module.
• NR: silence longer than a threshold.
• II: irrelevant input due to ASR errors or noise.
</listItem>
<bodyText confidence="0.998443">
We adopt logistic regression (LR)-based dia-
logue act tagging approach (Tur et al., 2006). The
probability of user intention o given an ASR result
of the user utterance h is defined as,
</bodyText>
<equation confidence="0.9979055">
exp(ω · ϕ(h, o))
P(o|h) = Σoexp(ω · ϕ(h, o)). (1)
</equation>
<bodyText confidence="0.998288181818182">
Here, ω is a vector of feature weights and ϕ(h, o)
is a feature vector. We use POS, P-A and P-A tem-
plates score as a feature set. In addition, we add a
typical expression feature (TYPICAL) to classify
TP, ST or GR tags. For example, typical expres-
sions in conversation are “Hello” or “Go on,” and
those in information navigation are “News of the
day” or “Tell me in detail.” Features for the clas-
sifier are shown in the Table 3.
The accuracy of the classification in five-fold
cross-validation is shown in Table 4. The TYP-
</bodyText>
<tableCaption confidence="0.974772">
Table 3: Features of user intention analysis.
</tableCaption>
<table confidence="0.999487875">
feature type feature
POS Bag of POS tags
Bag of POS bi-gram
P-A Bag of semantic role labels
Bag of semantic role labels bi-gram
Pair of semantic role label and its rank
P-A score P-A templates score
TYPICAL Occurrence of typical expressions
</table>
<tableCaption confidence="0.99455">
Table 4: Accuracy of user intention analysis.
</tableCaption>
<table confidence="0.999762857142857">
All features without TYPICAL
TP 100% 100%
ST 75.3% 64.2%
QA 94.1% 93.5%
GR 100% 100%
II 16.7% 16.7%
All 92.1% 90.2%
</table>
<bodyText confidence="0.9752125">
ICAL feature improves the classification accuracy
while keeping the domain portability.
</bodyText>
<sectionHeader confidence="0.587757" genericHeader="method">
3.4 SLU for ASR output
</sectionHeader>
<bodyText confidence="0.999381">
ASR and intention analysis involves errors. Here,
s is a true user intention and o is an observed in-
tention. The observation model P(o|s) is given
by the likelihood of ASR result P(h|u) (Komatani
and Kawahara, 2000) and the likelihood of the in-
tention analysis P(o|h),
</bodyText>
<equation confidence="0.993593">
P(o|s) = ∑ P(o, h|s) (2)
h
∑≈ P(o|h)P(h|u). (3)
h
</equation>
<bodyText confidence="0.996138">
Here, u is an utterance of the user. We combine
the N-best (N = 5) hypotheses of the ASR result
h.
</bodyText>
<sectionHeader confidence="0.9866585" genericHeader="method">
4 Dialogue Management for Information
Navigation
</sectionHeader>
<bodyText confidence="0.999863444444444">
The conventional dialogue management for task-
oriented dialogue systems is designed to reach a
task goal as soon as possible (Williams and Young,
2007). In contrast, information navigation does
not always have a clear goal, and the aim of infor-
mation navigation is to provide as much relevant
information as the user is interested in. Therefore,
our dialogue manager refers user involvement or
engagement (=level of interest) and the user focus
</bodyText>
<page confidence="0.996534">
35
</page>
<bodyText confidence="0.99800575">
(=object of interest). This section describes the
general dialogue management based on POMDP,
and then gives an explanation of the proposed dia-
logue management using the user focus.
</bodyText>
<subsectionHeader confidence="0.967752">
4.1 Dialogue management based on POMDP
</subsectionHeader>
<bodyText confidence="0.997978333333333">
The POMDP-based statistical dialogue manage-
ment is formulated as below. The random vari-
ables involved at a dialogue turn t are as follows:
</bodyText>
<listItem confidence="0.8937986">
• s E Is: user state
User intention.
• a E K: system action
Module that the system selects.
• o E Is: observation
Observed user state, including ASR and in-
tention analysis errors.
• bsi = P(si|o1:t): belief
Stochastic variable of the user state.
• π: policy function
</listItem>
<bodyText confidence="0.999666666666667">
This function determines a system action a
given a belief of user b. π∗ is the optimal pol-
icy function that is acquired by the training.
</bodyText>
<listItem confidence="0.999161">
• r: reward function
</listItem>
<bodyText confidence="0.999780444444445">
This function gives a reward to a pair of the
user state s and the system action a.
The aim of the statistical dialogue management is
to output an optimal system action ˆat given a se-
quence of observation o1:t from 1 to t time-steps.
Next, we give the belief update that includes the
observation and state transition function. The be-
lief update of user state si in time-step t is defined
as,
</bodyText>
<equation confidence="0.9697475">
bt+1 j)
s′ j a P(ot+1|s′
 |{z }
Obs.
</equation>
<bodyText confidence="0.9995988">
Obs. is an observation function which is defined
in Equation (3) and Trans. is a state transition
probability of the user state. Once the system es-
timates the belief btsi, the policy function outputs
the optimal action aˆ as follows:
</bodyText>
<equation confidence="0.95515">
aˆ = π∗(bt). (5)
</equation>
<subsectionHeader confidence="0.998091">
4.2 Training of POMDP
</subsectionHeader>
<bodyText confidence="0.99551625">
We applied Q-learning (Monahan, 1982; Watkins
and Dayan, 1992) to acquire the optimal policy
π∗. Q-learning relies on the estimation of a Q-
function, which maximizes the discounted sum of
future rewards of the system action at at a dialogue
turn t given the current belief bt. Q-learning is
performed by iterative updates on the training dia-
logue data:
</bodyText>
<equation confidence="0.997866333333333">
Q(bt, at) &lt;-- (1 − ε)Q(bt, at)
+ ε[R(st, at) + γ max
at�� Q(bt+1,at+1)], (6)
</equation>
<bodyText confidence="0.9947945">
where ε is a learning rate, γ is a discount factor of
a future reward. We experimentally decided ε =
0.01 and γ = 0.9. The optimal policy given by the
Q-function is determined as,
</bodyText>
<equation confidence="0.987979">
π∗(bt) = argmax Q(bt, at). (7)
at
</equation>
<bodyText confidence="0.9985184">
However, it is impossible to calculate the Q-
function for all possible real values of belief b.
Thus, we train a limited Q-function given by a
Grid-based Value Iteration (Bonet, 2002). The be-
lief is given by a function,
</bodyText>
<equation confidence="0.95564525">
{
ηif s = i
bsi = .1−η if s � i (8 )
|Is|
</equation>
<bodyText confidence="0.996720666666667">
Here, η is a likelihood of s = i that is output
of the intention analyzer, and we selected 11 dis-
crete points from 0.0 to 1.0 by 0.1. We also added
the case of uniform distribution. The observation
function of the belief update is also given in a sim-
ilar manner.
</bodyText>
<subsectionHeader confidence="0.987388">
4.3 Dialogue management using user focus
</subsectionHeader>
<bodyText confidence="0.9994275">
Our POMDP-based dialogue management
chooses actions based on its belief in: the user
intention s and the user focus f (0 or 1 E Jf).
The observation o is controlled by hidden states
f and s that are decided by the state transition
probabilities,
</bodyText>
<equation confidence="0.9987625">
P(ft+1|ft, st, at), (9)
P(st+1|ft+1, ft, st, at). (10)
</equation>
<bodyText confidence="0.999625428571428">
We constructed a user simulator by using the an-
notated data described in Section 3.1.
Equation (10) is also used for the state transition
probability of the belief update. The equation of
the belief update (4) is extended by introducing the
previous user focus fl and current user focus f′m
information,
</bodyText>
<equation confidence="0.9982265">
bt±1 = P(ot+1|s′j)
sj  |{z }
Obs.
P(s′j|f′m, fl, si, ˆak)
|
∑ P(s′j|si, ˆak) btsi. (4)
si  |{z }
Trans.
∑x
i
{z }
Trans.
t
bsi,fl. (11)
</equation>
<page confidence="0.993526">
36
</page>
<table confidence="0.542827">
Average of rewards
</table>
<tableCaption confidence="0.902961">
Table 5: Rewards in each turn.
</tableCaption>
<figure confidence="0.708038586206897">
state focus action a
s f
TP ST QA PP GR KS
TP 0 +10 -10 -10 -10 -10 -10
1
ST 0 -10 +10 -10 0 -10 -10
1
QA 0 -10 +10 +10 -10 -10 -10
1 -10 +30 +10
GR 0 -10 -10 -10 -10 +10 -10
1
NR 0 +10 -10 -10 -10 -10 0
1 -10 +10
II 0 -10 -10 -10 -10 -10 +10
1
350
300
250
200
150
100
50
Noise
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
-100
-150
-200
-250
w. focus w.o. focus
</figure>
<figureCaption confidence="0.9941395">
Figure 4: Effect of introduction of the user focus
in simulation.
</figureCaption>
<figure confidence="0.972199">
0
-50
The resultant optimal policy is,
a = 7r*(6&apos;, fl). (12)
</figure>
<subsectionHeader confidence="0.998511">
4.4 Definition of rewards
</subsectionHeader>
<bodyText confidence="0.999972692307692">
Table 5 defines a reward list at the end of a each
turn. The reward of +10 is given to appropriate
actions, 0 to acceptable actions, and -10 to inap-
propriate actions.
In Table 5, pairs of a state and its apparently
corresponding action, TP and TP, ST and ST, QA
and QA, GR and GR, and II and KS, have posi-
tive rewards. Rewards in bold fonts (+10) are de-
fined for the following reasons. If the user asks a
question (QA) without a focus (e.g. “What hap-
pened on the game?”), the system can continue by
story telling (ST). But when the question has a fo-
cus, the system should answer the question (QA),
which is highly rewarded (+30). If the system can-
not find an answer, it can present relevant informa-
tion (PP). When the user says nothing (NR), the
system action should be decided by considering
the user focus; present a new topic if the user is
not interested in the current topic (f=0) or present
an article related to the dialogue history (f=1).
Reward of +200 is given if 20 turns are passed,
to reward a long continued dialogue. The user sim-
ulator terminates the dialogue if the system selects
an inappropriate action (action of r = −10) five
times, and a large penalty -200 is given to the sys-
tem.
</bodyText>
<sectionHeader confidence="0.886075" genericHeader="method">
5 Evaluations of Dialogue
</sectionHeader>
<bodyText confidence="0.989300833333333">
We evaluated the proposed system with two exper-
iments; dialogue state tracking with real users and
average reward with a user simulator. For the eval-
uation, we collected an additional 312 utterances
(8 users, 24 dialogues) with the proposed dialogue
system.
</bodyText>
<subsectionHeader confidence="0.985636">
5.1 Evaluation of dialogue manager with
user simulator
</subsectionHeader>
<bodyText confidence="0.999989166666667">
First, we evaluated the dialogue manager with
user simulation that is constructed from the train-
ing corpus (Section 3.1). In this evaluation, the
system calculated average reward of 100,000 di-
alogue sessions between the system and the user
simulator given a fixed noise rate. Figure 4 shows
the effect of the user focus. By introducing the
user focus, the system receives higher rewards
than the model without the user focus. Especially,
the proposed model is more robust with a noise
level of 10–30% that spoken dialogue systems of-
ten encounter (Yoshino et al., 2013a).
</bodyText>
<subsectionHeader confidence="0.901399">
5.2 Evaluation of dialogue state tracking
with real users
</subsectionHeader>
<bodyText confidence="0.999971235294118">
Dialogue state tracking (DST) is a task of track-
ing the correct user state with a noisy input (e.g.
ASR and NLU errors) (Williams et al., 2013).
It tries to maximize the probability of the belief
of the correct states, but we evaluated the accu-
racy of the 1-best result of the belief update. We
also evaluated the average reward of each dia-
logue. The baseline system is a rule-based dia-
logue manager which we previously implemented.
The baseline system is operated by a score of
the question-answering module using predicate-
argument structures (Yoshino et al., 2011) and reg-
ular expressions for TP and GR modules.
The DST accuracy and average reward are
shown in Table 6. This result shows that the pro-
posed method tracks the dialogue state of the user
with a high accuracy. Our statistical framework
</bodyText>
<page confidence="0.996638">
37
</page>
<figure confidence="0.814482">
Dialogue 1
</figure>
<figureCaption confidence="0.992579">
Figure 5: A dialogue example. (This example is translated from Japanese)
</figureCaption>
<table confidence="0.976198610169491">
Sys: Hello.
Usr: Hello. Tell me today&apos;s news.
Sys: Tanaka showed a crazy split. His colleague
Sabathia was stunned.
Usr: What is the crazy split?
Sys: His split comes just like a four-seam, but it
drops quickly in front of the home plate.
Usr: Where was he training?
Sys: Tanaka practiced at the bullpen in the New
York Yankees&apos; camp on Feb. 20th.
Usr: (pause)
Sys: By the way, he also practiced a drop
two-seam on Feb. 18.
Usr: Does his two-seam drop?
Sys: The catcher said &amp;quot;Tanaka&apos;s two-seam not
only slides, but also drops.&amp;quot;
Usr: OK, any other news?
Sys: Yankees&apos; Ichiro is training strenuously!
Usr: Tell me more.
Sys: Ichiro came to the field an hour earlier than
usual and trained strenuously.
s=TP, P o s =0.8, b(s=TP)=0.800, f=0
a=TP, b(s=TP f=0)=0.8
s=QA, P o s =0.4, b(s=QA)=0.934, f=1
a=QA, b(s=QA, f=1)=0.9
s=QA, P o s =0.8, b(s=QA)=0.973, f=1
a=QA, b(s=QA, f=1)=1.0
s=NR, P o s =1.0, b(s=NR)=1.00, f=1
a=PP, b(s=NR, f=1)=1.0
s=QA, P o s =0.6, b(s=QA)=0.867, f=1
a=QA, b(s=QA, f=1)=0.9
s=TP, P o s =0.8, b(s=TP)=0.984, f=0
a=TP, b(s=TP, f=0)=1.0
s=ST, P o s =0.9, b(s=ST)=0.871, f=0
a=ST, b(s=ST, f=0)=0.9
Dialogue 2
...
Sys: Hello.
Usr: Hello. Tell me today&apos;s news?
Sys: Matsui, a part-time coach for the Giants,
demonstrated 5 homeruns. The Giants
president told him &amp;quot;Come back as a player!&amp;quot;
Usr: What happened?
Sys: The godzilla cannon came back to the Giants‘
spring camp in Miyazaki.
Usr: What is the Godzilla cannon?
Sys: The Godzilla coach demonstrated his
home-run batting to young players.
Usr: Next please.
Sys: To be a ace pitcher, has Fujinami improved
from the rookie year?
s=TP, P o s =0.8, b(s=TP)=0.800, f=0
a=TP, b(s=TP f=0)=0.8
s=QA, P o s =0.8, b(s=QA)=0.532, f=0
a=ST, b(s=QA, f=0)=0.5
s=QA, P o s =0.8, b(s=QA)=0.806, f=1
a=QA, b(s=QA, f=1)=0.8
s=TP, P o s =0.8, b(s=TP)=0.986, f=0
a=TP, b(s=TP, f=0)=1.0
</table>
<tableCaption confidence="0.985007">
Table 6: Accuracy of dialogue state tracking.
</tableCaption>
<table confidence="0.9799165">
rule focus POMDP
Accuracy of tracking 0.561 0.869
(1-best) (=175/312) (=271/312)
Average reward -22.9 188.6
</table>
<bodyText confidence="0.993634">
improved SLU accuracy and robustness against
ASR errors, especially reducing confusions be-
tween question answering (QA) and topic presen-
tation (TP). Moreover, belief update can detect the
TP state even if the SLU incorrectly predicts QA
or ST.
</bodyText>
<subsectionHeader confidence="0.988255">
5.3 Discussion of trained policy
</subsectionHeader>
<bodyText confidence="0.9999756">
An example dialogue is shown in Figure 5. In
the example, the system selects appropriate ac-
tions even if the observation likelihood is low. At
the 4th turn of Dialogue 1 in this example, the sys-
tem with the user focus responds with an action of
proactive presentation a=PP, but the system with-
out the user focus responds with an action of topic
presentation a=TP. At the 2nd turn of Dialogue 2,
the user asks a question without a focus. The con-
fidence of s=QA is lowered by the belief update,
and the system selects the story telling module
a=ST. These examples show that the training re-
sult (=learned policy) reflects our design described
in Section 4.4: It is better to make a proactive pre-
sentation when the user is interested in the topic.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999943">
We constructed a spoken dialogue system for in-
formation navigation of Web news articles updated
day-by-day. The system presents relevant infor-
</bodyText>
<page confidence="0.996992">
38
</page>
<bodyText confidence="0.999978">
mation according to the user’s interest, by track-
ing the user focus. We introduce the user focus
detection model, and developed a POMDP frame-
work which tracks user focus to select the appro-
priate action class (module) of the dialogue sys-
tem. In experimental evaluations, the proposed di-
alogue management approach determines the state
of the user more accurately than the existing sys-
tem based on rules. An evaluation with a user sim-
ulator shows that including user focus in the dia-
logue manager’s belief state improves robustness
to ASR/SLU errors.
In future work, we plan to evaluate the system
with a large number of real users on a variety of
domains, and optimize the reward function for the
information navigation task.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999125">
We thank Dr. Jason Williams for his valuable and
detailed advice to improve this paper on SIGDIAL
mentoring program. This work was supported by
Grant-in-Aid for JSPS Fellows 25-4537.
</bodyText>
<sectionHeader confidence="0.98688" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.972412024096385">
Dan Bohus and Alexander I. Rudnicky. 2003. Raven-
claw: Dialog management using hierarchical task
decomposition and an expectation agenda. In Pro-
ceedings of the 8th European Conference on Speech
Communication and Technology, pages 597–600.
Blai Bonet. 2002. An e-optimal grid-based algorithm
for partially observable Markov decision processes.
In Proceedings of International Conference on Ma-
chine Learning, pages 51–58.
Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
task: the ATIS-3 corpus. In Proceedings of the
workshop on Human Language Technology, pages
43–48.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175–204.
Ryuichiro Higashinaka, Katsuhito Sudoh, and Mikio
Nakano. 2006. Incorporating discourse features
into confidence scoring of intention recognition re-
sults in spoken dialogue systems. Speech Communi-
cation, 48(3):417–436.
Tatsuya Kawahara. 2009. New perspectives on spoken
language understanding: Does machine need to fully
understand speech? In Proceedings of IEEE work-
shop on Automatic Speech Recognition and Under-
standing, pages 46–50.
Kazunori Komatani and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management us-
ing concept-level confidence measures of speech
recognizer output. In Proceedings of the 18th con-
ference on Computational linguistics, pages 467–
473.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A stochastic model of human-machine inter-
action for learning dialog strategies. IEEE Transac-
tions on Speech and Audio Processing, 8(1):11–23.
Toyomi Meguro, Ryuichiro Higashinaka, Yasuhiro Mi-
nami, and Kohji Dohsaka. 2010. Controlling
listening-oriented dialogue using partially observ-
able markov decision processes. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 761–769.
Teruhisa Misu and Tatsuya Kawahara. 2010. Bayes
risk-based dialogue management for document re-
trieval system with speech interface. Speech Com-
munication, 52(1):61–71.
George E. Monahan. 1982. State of the art? a survey
of partially observable Markov decision processes:
Theory, models, and algorithms. Management Sci-
ence, 28(1):1–16.
Naoaki Okazaki. 2007. CRFsuite: a fast implementa-
tion of Conditional Random Fields (CRFs).
Yi-Cheng Pan, Hung yi Lee, and Lin shan Lee. 2012.
Interactive spoken document retrieval with sug-
gested key terms ranked by a markov decision pro-
cess. IEEE Transactions on Audio, Speech, and
Language Processing, 20(2):632–645.
Emanuel A. Schegloff and Harvey Sacks. 1973. Open-
ing up closings. Semiotica, 8(4):289–327.
Tomohide Shibata, Yusuke Egashira, and Sadao Kuro-
hashi. 2014. Chat-like conversational system based
on selection of reply generating module with rein-
forcement learning. In Proceedings of the 5th In-
ternational Workshop Series on Spoken Dialog Sys-
tems.
Gokhan Tur, Umit Guz, and Dilek Hakkani-Tur. 2006.
Model adaptation for dialog act tagging. In Pro-
ceedings of IEEE workshop on Spoken Language
Technology, pages 94–97. IEEE.
Christopher JCH Watkins and Peter Dayan. 1992. Q-
learning. Machine learning, 8(3):279–292.
Jason D. Williams and Steve Young. 2007. Par-
tially observable Markov decision processes for spo-
ken dialog systems. Computer Speech &amp; Language,
21(2):393–422.
Jason D. Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013. The dialog state track-
ing challenge. In Proceedings of the 14th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 404–413.
</reference>
<page confidence="0.988479">
39
</page>
<reference confidence="0.999325625">
Koichiro Yoshino, Shinsuke Mori, and Tatsuya Kawa-
hara. 2011. Spoken dialogue system based on infor-
mation extraction using similarity of predicate argu-
ment structures. In Proceedings of the 12th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 59–66.
Koichiro Yoshino, Shinsuke Mori, and Tatsuya Kawa-
hara. 2013a. Incorporating semantic information to
selection of web texts for language model of spoken
dialogue system. In Proceedings of IEEE Interna-
tional Conference on Acoustic, Speech and Signal
Processing, pages 8252–8256.
Koichiro Yoshino, Shinji Watanabe, Jonathan Le Roux,
and John R. Hershey. 2013b. Statistical dialogue
management using intention dependency graph. In
Proceedings of the 6th International Joint Confer-
ence on Natural Language Processing, pages 962–
966.
Steve Young, Milica Gaˇsi´c, Simon Keizer, Franc¸ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for POMDP-based spoken
dialogue management. Computer Speech &amp; Lan-
guage, 24(2):150–174.
</reference>
<page confidence="0.998629">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.974144">
<title confidence="0.999906">Information Navigation System Based on POMDP that Tracks User Focus</title>
<author confidence="0.998983">Koichiro Yoshino Tatsuya Kawahara</author>
<affiliation confidence="0.999998">School of Informatics, Kyoto University</affiliation>
<address confidence="0.995016">Sakyo-ku, Kyoto, 606-8501, Japan</address>
<email confidence="0.985986">yoshino@ar.media.kyoto-u.ac.jp</email>
<abstract confidence="0.999534142857143">We present a spoken dialogue system for navigating information (such as news articles), and which can engage in small talk. At the core is a partially observable Markov decision process (POMDP), which tracks user’s state and focus of attention. The input to the POMDP is provided by a spoken language understanding (SLU) component implemented with logistic regression (LR) and conditional random fields (CRFs). The POMDP selects one of six action classes; each action class is implemented with its own module.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dan Bohus</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Ravenclaw: Dialog management using hierarchical task decomposition and an expectation agenda.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th European Conference on Speech Communication and Technology,</booktitle>
<pages>597--600</pages>
<contexts>
<context position="1953" citStr="Bohus and Rudnicky, 2003" startWordPosition="312" endWordPosition="316">pedia (Misu and Kawahara, 2010), and a news navigation system that introduces news articles updated day-by-day (Yoshino et al., 2011; Pan et al., 2012). In this paper, we develop an information navigation system that provides information even if the user request is not necessarily clear and there is not a matching document in the knowledge base. The user and the system converse on the current topic and the system provides potentially useful information for the user. Dialogue management of this kind of systems was usually made in a heuristic manner and based on simple rules (Dahl et al., 1994; Bohus and Rudnicky, 2003). There is not a clear principle nor established methodology to design and implement casual conversation systems. In the past years, machine learning, particularly reinforcement learning, have been investigated for dialogue management. MDPs and POMDPs are now widely used to model and train dialogue managers (Levin et al., 2000; Williams and Young, 2007; Young et al., 2010; Yoshino et al., 2013b). However, the conventional scheme assumes that the task and dialogue goal can be clearly stated and readily encoded in the RL reward function. This is not true in casual conversation or when browsing i</context>
</contexts>
<marker>Bohus, Rudnicky, 2003</marker>
<rawString>Dan Bohus and Alexander I. Rudnicky. 2003. Ravenclaw: Dialog management using hierarchical task decomposition and an expectation agenda. In Proceedings of the 8th European Conference on Speech Communication and Technology, pages 597–600.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blai Bonet</author>
</authors>
<title>An e-optimal grid-based algorithm for partially observable Markov decision processes.</title>
<date>2002</date>
<booktitle>In Proceedings of International Conference on Machine Learning,</booktitle>
<pages>51--58</pages>
<contexts>
<context position="17463" citStr="Bonet, 2002" startWordPosition="2946" endWordPosition="2947">em action at at a dialogue turn t given the current belief bt. Q-learning is performed by iterative updates on the training dialogue data: Q(bt, at) &lt;-- (1 − ε)Q(bt, at) + ε[R(st, at) + γ max at�� Q(bt+1,at+1)], (6) where ε is a learning rate, γ is a discount factor of a future reward. We experimentally decided ε = 0.01 and γ = 0.9. The optimal policy given by the Q-function is determined as, π∗(bt) = argmax Q(bt, at). (7) at However, it is impossible to calculate the Qfunction for all possible real values of belief b. Thus, we train a limited Q-function given by a Grid-based Value Iteration (Bonet, 2002). The belief is given by a function, { ηif s = i bsi = .1−η if s � i (8 ) |Is| Here, η is a likelihood of s = i that is output of the intention analyzer, and we selected 11 discrete points from 0.0 to 1.0 by 0.1. We also added the case of uniform distribution. The observation function of the belief update is also given in a similar manner. 4.3 Dialogue management using user focus Our POMDP-based dialogue management chooses actions based on its belief in: the user intention s and the user focus f (0 or 1 E Jf). The observation o is controlled by hidden states f and s that are decided by the sta</context>
</contexts>
<marker>Bonet, 2002</marker>
<rawString>Blai Bonet. 2002. An e-optimal grid-based algorithm for partially observable Markov decision processes. In Proceedings of International Conference on Machine Learning, pages 51–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah A Dahl</author>
<author>Madeleine Bates</author>
<author>Michael Brown</author>
<author>William Fisher</author>
<author>Kate Hunicke-Smith</author>
<author>David Pallett</author>
<author>Christine Pao</author>
<author>Alexander Rudnicky</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Expanding the scope of the ATIS task: the ATIS-3 corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the workshop on Human Language Technology,</booktitle>
<pages>43--48</pages>
<contexts>
<context position="1926" citStr="Dahl et al., 1994" startWordPosition="308" endWordPosition="311">r documents in Wikipedia (Misu and Kawahara, 2010), and a news navigation system that introduces news articles updated day-by-day (Yoshino et al., 2011; Pan et al., 2012). In this paper, we develop an information navigation system that provides information even if the user request is not necessarily clear and there is not a matching document in the knowledge base. The user and the system converse on the current topic and the system provides potentially useful information for the user. Dialogue management of this kind of systems was usually made in a heuristic manner and based on simple rules (Dahl et al., 1994; Bohus and Rudnicky, 2003). There is not a clear principle nor established methodology to design and implement casual conversation systems. In the past years, machine learning, particularly reinforcement learning, have been investigated for dialogue management. MDPs and POMDPs are now widely used to model and train dialogue managers (Levin et al., 2000; Williams and Young, 2007; Young et al., 2010; Yoshino et al., 2013b). However, the conventional scheme assumes that the task and dialogue goal can be clearly stated and readily encoded in the RL reward function. This is not true in casual conv</context>
</contexts>
<marker>Dahl, Bates, Brown, Fisher, Hunicke-Smith, Pallett, Pao, Rudnicky, Shriberg, 1994</marker>
<rawString>Deborah A. Dahl, Madeleine Bates, Michael Brown, William Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao, Alexander Rudnicky, and Elizabeth Shriberg. 1994. Expanding the scope of the ATIS task: the ATIS-3 corpus. In Proceedings of the workshop on Human Language Technology, pages 43–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="6869" citStr="Grosz and Sidner, 1986" startWordPosition="1109" endWordPosition="1112">nts by referring to domain knowledge that is automatically extracted from domain corpus. Transitions between the modules are allowed as shown in Figure 2. The modules “greeting (GR)” and “keep silence (KS)” are also implemented. GR module generates fixed greeting patterns by using regular expression matching. In terms of dialogue flow, these modules can be used at any time. 2.2 User focus in information navigation “Focus” in discourse is “attentional state (that) contains information about the objects, properties, relations, and discourse intentions that are most salient at any given point.” (Grosz and Sidner, 1986). The user has specific attention to an object if the user utterance contains the focus. In this work, we define the user focus as “the main piece of information of interest to the user.” It makes a central component when making a reply or selecting relevant topics at the current dialogue state. For example, given “Did Ichiro perform brilSpeaker (system) Listener (user) Topic 1 Offer a topic Be interested in the topic Present the detail Make a question Answer the question Be silent Topic 2 Offer a new topic (topic 2) Not be interested in Topic 3 Offer a new topic (topic 3) Make a question ・・・ </context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryuichiro Higashinaka</author>
<author>Katsuhito Sudoh</author>
<author>Mikio Nakano</author>
</authors>
<title>Incorporating discourse features into confidence scoring of intention recognition results in spoken dialogue systems.</title>
<date>2006</date>
<journal>Speech Communication,</journal>
<volume>48</volume>
<issue>3</issue>
<marker>Higashinaka, Sudoh, Nakano, 2006</marker>
<rawString>Ryuichiro Higashinaka, Katsuhito Sudoh, and Mikio Nakano. 2006. Incorporating discourse features into confidence scoring of intention recognition results in spoken dialogue systems. Speech Communication, 48(3):417–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tatsuya Kawahara</author>
</authors>
<title>New perspectives on spoken language understanding: Does machine need to fully understand speech?</title>
<date>2009</date>
<booktitle>In Proceedings of IEEE workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>46--50</pages>
<marker>Kawahara, 2009</marker>
<rawString>Tatsuya Kawahara. 2009. New perspectives on spoken language understanding: Does machine need to fully understand speech? In Proceedings of IEEE workshop on Automatic Speech Recognition and Understanding, pages 46–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazunori Komatani</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>Flexible mixed-initiative dialogue management using concept-level confidence measures of speech recognizer output.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics,</booktitle>
<pages>467--473</pages>
<contexts>
<context position="14453" citStr="Komatani and Kawahara, 2000" startWordPosition="2411" endWordPosition="2414">mantic role labels bi-gram Pair of semantic role label and its rank P-A score P-A templates score TYPICAL Occurrence of typical expressions Table 4: Accuracy of user intention analysis. All features without TYPICAL TP 100% 100% ST 75.3% 64.2% QA 94.1% 93.5% GR 100% 100% II 16.7% 16.7% All 92.1% 90.2% ICAL feature improves the classification accuracy while keeping the domain portability. 3.4 SLU for ASR output ASR and intention analysis involves errors. Here, s is a true user intention and o is an observed intention. The observation model P(o|s) is given by the likelihood of ASR result P(h|u) (Komatani and Kawahara, 2000) and the likelihood of the intention analysis P(o|h), P(o|s) = ∑ P(o, h|s) (2) h ∑≈ P(o|h)P(h|u). (3) h Here, u is an utterance of the user. We combine the N-best (N = 5) hypotheses of the ASR result h. 4 Dialogue Management for Information Navigation The conventional dialogue management for taskoriented dialogue systems is designed to reach a task goal as soon as possible (Williams and Young, 2007). In contrast, information navigation does not always have a clear goal, and the aim of information navigation is to provide as much relevant information as the user is interested in. Therefore, our</context>
</contexts>
<marker>Komatani, Kawahara, 2000</marker>
<rawString>Kazunori Komatani and Tatsuya Kawahara. 2000. Flexible mixed-initiative dialogue management using concept-level confidence measures of speech recognizer output. In Proceedings of the 18th conference on Computational linguistics, pages 467– 473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esther Levin</author>
<author>Roberto Pieraccini</author>
<author>Wieland Eckert</author>
</authors>
<title>A stochastic model of human-machine interaction for learning dialog strategies.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="2281" citStr="Levin et al., 2000" startWordPosition="364" endWordPosition="367">wledge base. The user and the system converse on the current topic and the system provides potentially useful information for the user. Dialogue management of this kind of systems was usually made in a heuristic manner and based on simple rules (Dahl et al., 1994; Bohus and Rudnicky, 2003). There is not a clear principle nor established methodology to design and implement casual conversation systems. In the past years, machine learning, particularly reinforcement learning, have been investigated for dialogue management. MDPs and POMDPs are now widely used to model and train dialogue managers (Levin et al., 2000; Williams and Young, 2007; Young et al., 2010; Yoshino et al., 2013b). However, the conventional scheme assumes that the task and dialogue goal can be clearly stated and readily encoded in the RL reward function. This is not true in casual conversation or when browsing information. Some previous work has tackled with this problem. In a conversational chatting system (Shibata et al., 2014), users were asked to make evaluation at the end of each dialogue session, to define rewards for reinforcement learning. In a listening dialogue system (Meguro et al., 2010), levels of satisfaction were annot</context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 2000</marker>
<rawString>Esther Levin, Roberto Pieraccini, and Wieland Eckert. 2000. A stochastic model of human-machine interaction for learning dialog strategies. IEEE Transactions on Speech and Audio Processing, 8(1):11–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toyomi Meguro</author>
<author>Ryuichiro Higashinaka</author>
<author>Yasuhiro Minami</author>
<author>Kohji Dohsaka</author>
</authors>
<title>Controlling listening-oriented dialogue using partially observable markov decision processes.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>761--769</pages>
<contexts>
<context position="2846" citStr="Meguro et al., 2010" startWordPosition="461" endWordPosition="464">o model and train dialogue managers (Levin et al., 2000; Williams and Young, 2007; Young et al., 2010; Yoshino et al., 2013b). However, the conventional scheme assumes that the task and dialogue goal can be clearly stated and readily encoded in the RL reward function. This is not true in casual conversation or when browsing information. Some previous work has tackled with this problem. In a conversational chatting system (Shibata et al., 2014), users were asked to make evaluation at the end of each dialogue session, to define rewards for reinforcement learning. In a listening dialogue system (Meguro et al., 2010), levels of satisfaction were annotated in logs of dialogue sessions to train a discriminative model. These approaches require costly input from users or developers, who provide labels and evaluative judgments. In this work, we present a framework in which reward is defined for the quality of system actions and also for encouraging long interactions, in contrast to the conventional framework. Moreover, user focus is tracked to make appropriate actions, which are more rewarded. 2 Conversational Information Navigation System In natural human-human conversation, participants have topics they plan</context>
</contexts>
<marker>Meguro, Higashinaka, Minami, Dohsaka, 2010</marker>
<rawString>Toyomi Meguro, Ryuichiro Higashinaka, Yasuhiro Minami, and Kohji Dohsaka. 2010. Controlling listening-oriented dialogue using partially observable markov decision processes. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 761–769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teruhisa Misu</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>Bayes risk-based dialogue management for document retrieval system with speech interface.</title>
<date>2010</date>
<journal>Speech Communication,</journal>
<volume>52</volume>
<issue>1</issue>
<contexts>
<context position="1359" citStr="Misu and Kawahara, 2010" startWordPosition="211" endWordPosition="214">tion A large number of spoken dialogue systems have been investigated and many systems are deployed in the real world. Spoken dialogue applications that interact with a diversity of users are available on smart-phones. However, current applications are based on simple question answering and the system requires a clear query or a definite task goal. Therefore, next-generation dialogue systems should engage in casual interactions with users who do not have a clear intention or a task goal. Such systems include a sightseeing navigation system that uses tour guide books or documents in Wikipedia (Misu and Kawahara, 2010), and a news navigation system that introduces news articles updated day-by-day (Yoshino et al., 2011; Pan et al., 2012). In this paper, we develop an information navigation system that provides information even if the user request is not necessarily clear and there is not a matching document in the knowledge base. The user and the system converse on the current topic and the system provides potentially useful information for the user. Dialogue management of this kind of systems was usually made in a heuristic manner and based on simple rules (Dahl et al., 1994; Bohus and Rudnicky, 2003). Ther</context>
</contexts>
<marker>Misu, Kawahara, 2010</marker>
<rawString>Teruhisa Misu and Tatsuya Kawahara. 2010. Bayes risk-based dialogue management for document retrieval system with speech interface. Speech Communication, 52(1):61–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George E Monahan</author>
</authors>
<title>State of the art? a survey of partially observable Markov decision processes: Theory, models, and algorithms.</title>
<date>1982</date>
<journal>Management Science,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="16674" citStr="Monahan, 1982" startWordPosition="2803" endWordPosition="2804">gement is to output an optimal system action ˆat given a sequence of observation o1:t from 1 to t time-steps. Next, we give the belief update that includes the observation and state transition function. The belief update of user state si in time-step t is defined as, bt+1 j) s′ j a P(ot+1|s′ |{z } Obs. Obs. is an observation function which is defined in Equation (3) and Trans. is a state transition probability of the user state. Once the system estimates the belief btsi, the policy function outputs the optimal action aˆ as follows: aˆ = π∗(bt). (5) 4.2 Training of POMDP We applied Q-learning (Monahan, 1982; Watkins and Dayan, 1992) to acquire the optimal policy π∗. Q-learning relies on the estimation of a Qfunction, which maximizes the discounted sum of future rewards of the system action at at a dialogue turn t given the current belief bt. Q-learning is performed by iterative updates on the training dialogue data: Q(bt, at) &lt;-- (1 − ε)Q(bt, at) + ε[R(st, at) + γ max at�� Q(bt+1,at+1)], (6) where ε is a learning rate, γ is a discount factor of a future reward. We experimentally decided ε = 0.01 and γ = 0.9. The optimal policy given by the Q-function is determined as, π∗(bt) = argmax Q(bt, at). </context>
</contexts>
<marker>Monahan, 1982</marker>
<rawString>George E. Monahan. 1982. State of the art? a survey of partially observable Markov decision processes: Theory, models, and algorithms. Management Science, 28(1):1–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
</authors>
<title>CRFsuite: a fast implementation of Conditional Random Fields (CRFs).</title>
<date>2007</date>
<contexts>
<context position="12057" citStr="Okazaki, 2007" startWordPosition="1980" endWordPosition="1981">rase was correctly detected (or not), regardless of whether the correct phrase was identified. This table indicates that WORD features are effective for detecting the user focus, but they are not essential for in the sentence-level accuracy. In this paper, we aim for portability across domains; therefore the dialogue manager only uses the sentence-level feature, so in our system we do not user the WORD features. 3.3 User intention analysis based on LR The module classifies the user intention from the user utterance. We define six intentions as below. • TP: request to the TP module. &apos;CRFsuite (Okazaki, 2007). 34 Table 1: Features of user focus detection. feature type feature ORDER Rank in a sequence of phrases Rank in a sequence of elements of P-A POS POS tags in the phrase POS tag sequence POSORDER Pair of POS tag and its order in the phrase P-A Which semantic role the phrase has Which semantic roles exist on the utterance P-AORDER Pair of semantic role and its order in the utterance P-A score P-A templates score WORD Words in the phrase Pair of words in the phrase Pair of word and its order in the phrase Table 2: Accuracy of user focus detection. Accuracy phrase 86.7% phrase + (WORD) 90.3% sent</context>
</contexts>
<marker>Okazaki, 2007</marker>
<rawString>Naoaki Okazaki. 2007. CRFsuite: a fast implementation of Conditional Random Fields (CRFs).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi-Cheng Pan</author>
<author>Hung yi Lee</author>
<author>Lin shan Lee</author>
</authors>
<title>Interactive spoken document retrieval with suggested key terms ranked by a markov decision process.</title>
<date>2012</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="1479" citStr="Pan et al., 2012" startWordPosition="230" endWordPosition="233">ialogue applications that interact with a diversity of users are available on smart-phones. However, current applications are based on simple question answering and the system requires a clear query or a definite task goal. Therefore, next-generation dialogue systems should engage in casual interactions with users who do not have a clear intention or a task goal. Such systems include a sightseeing navigation system that uses tour guide books or documents in Wikipedia (Misu and Kawahara, 2010), and a news navigation system that introduces news articles updated day-by-day (Yoshino et al., 2011; Pan et al., 2012). In this paper, we develop an information navigation system that provides information even if the user request is not necessarily clear and there is not a matching document in the knowledge base. The user and the system converse on the current topic and the system provides potentially useful information for the user. Dialogue management of this kind of systems was usually made in a heuristic manner and based on simple rules (Dahl et al., 1994; Bohus and Rudnicky, 2003). There is not a clear principle nor established methodology to design and implement casual conversation systems. In the past </context>
</contexts>
<marker>Pan, Lee, Lee, 2012</marker>
<rawString>Yi-Cheng Pan, Hung yi Lee, and Lin shan Lee. 2012. Interactive spoken document retrieval with suggested key terms ranked by a markov decision process. IEEE Transactions on Audio, Speech, and Language Processing, 20(2):632–645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel A Schegloff</author>
<author>Harvey Sacks</author>
</authors>
<title>Opening up closings.</title>
<date>1973</date>
<journal>Semiotica,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="3558" citStr="Schegloff and Sacks, 1973" startWordPosition="570" endWordPosition="573">minative model. These approaches require costly input from users or developers, who provide labels and evaluative judgments. In this work, we present a framework in which reward is defined for the quality of system actions and also for encouraging long interactions, in contrast to the conventional framework. Moreover, user focus is tracked to make appropriate actions, which are more rewarded. 2 Conversational Information Navigation System In natural human-human conversation, participants have topics they plan to talk about, and they progress through the dialogue in accordance with the topics (Schegloff and Sacks, 1973). We call this dialogue style “information navigation.” An example is shown in Figure 1. First, the speaker 32 Proceedings of the SIGDIAL 2014 Conference, pages 32–40, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics Figure 1: An example of information navigation. Other modules Figure 2: Overview of the information navigation system. offers a new topic and probes the interest of the listener. If the listener shows interest, the speaker describes details of the topic. If the listener asks a specific question, the speaker answers the question. On the other </context>
</contexts>
<marker>Schegloff, Sacks, 1973</marker>
<rawString>Emanuel A. Schegloff and Harvey Sacks. 1973. Opening up closings. Semiotica, 8(4):289–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomohide Shibata</author>
<author>Yusuke Egashira</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Chat-like conversational system based on selection of reply generating module with reinforcement learning.</title>
<date>2014</date>
<booktitle>In Proceedings of the 5th International Workshop Series on Spoken Dialog Systems.</booktitle>
<contexts>
<context position="2673" citStr="Shibata et al., 2014" startWordPosition="431" endWordPosition="434">ation systems. In the past years, machine learning, particularly reinforcement learning, have been investigated for dialogue management. MDPs and POMDPs are now widely used to model and train dialogue managers (Levin et al., 2000; Williams and Young, 2007; Young et al., 2010; Yoshino et al., 2013b). However, the conventional scheme assumes that the task and dialogue goal can be clearly stated and readily encoded in the RL reward function. This is not true in casual conversation or when browsing information. Some previous work has tackled with this problem. In a conversational chatting system (Shibata et al., 2014), users were asked to make evaluation at the end of each dialogue session, to define rewards for reinforcement learning. In a listening dialogue system (Meguro et al., 2010), levels of satisfaction were annotated in logs of dialogue sessions to train a discriminative model. These approaches require costly input from users or developers, who provide labels and evaluative judgments. In this work, we present a framework in which reward is defined for the quality of system actions and also for encouraging long interactions, in contrast to the conventional framework. Moreover, user focus is tracked</context>
</contexts>
<marker>Shibata, Egashira, Kurohashi, 2014</marker>
<rawString>Tomohide Shibata, Yusuke Egashira, and Sadao Kurohashi. 2014. Chat-like conversational system based on selection of reply generating module with reinforcement learning. In Proceedings of the 5th International Workshop Series on Spoken Dialog Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gokhan Tur</author>
<author>Umit Guz</author>
<author>Dilek Hakkani-Tur</author>
</authors>
<title>Model adaptation for dialog act tagging.</title>
<date>2006</date>
<booktitle>In Proceedings of IEEE workshop on Spoken Language Technology,</booktitle>
<pages>94--97</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="13009" citStr="Tur et al., 2006" startWordPosition="2154" endWordPosition="2157">r of semantic role and its order in the utterance P-A score P-A templates score WORD Words in the phrase Pair of words in the phrase Pair of word and its order in the phrase Table 2: Accuracy of user focus detection. Accuracy phrase 86.7% phrase + (WORD) 90.3% sentence (focus exist or not) 99.8% sentence (focus exist or not) + (WORD) 99.8% • ST: request to the ST module. • QA: request to the QA module. • GR: greeting to the GR module. • NR: silence longer than a threshold. • II: irrelevant input due to ASR errors or noise. We adopt logistic regression (LR)-based dialogue act tagging approach (Tur et al., 2006). The probability of user intention o given an ASR result of the user utterance h is defined as, exp(ω · ϕ(h, o)) P(o|h) = Σoexp(ω · ϕ(h, o)). (1) Here, ω is a vector of feature weights and ϕ(h, o) is a feature vector. We use POS, P-A and P-A templates score as a feature set. In addition, we add a typical expression feature (TYPICAL) to classify TP, ST or GR tags. For example, typical expressions in conversation are “Hello” or “Go on,” and those in information navigation are “News of the day” or “Tell me in detail.” Features for the classifier are shown in the Table 3. The accuracy of the clas</context>
</contexts>
<marker>Tur, Guz, Hakkani-Tur, 2006</marker>
<rawString>Gokhan Tur, Umit Guz, and Dilek Hakkani-Tur. 2006. Model adaptation for dialog act tagging. In Proceedings of IEEE workshop on Spoken Language Technology, pages 94–97. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher JCH Watkins</author>
<author>Peter Dayan</author>
</authors>
<date>1992</date>
<booktitle>Qlearning. Machine learning,</booktitle>
<pages>8--3</pages>
<contexts>
<context position="16700" citStr="Watkins and Dayan, 1992" startWordPosition="2805" endWordPosition="2808">tput an optimal system action ˆat given a sequence of observation o1:t from 1 to t time-steps. Next, we give the belief update that includes the observation and state transition function. The belief update of user state si in time-step t is defined as, bt+1 j) s′ j a P(ot+1|s′ |{z } Obs. Obs. is an observation function which is defined in Equation (3) and Trans. is a state transition probability of the user state. Once the system estimates the belief btsi, the policy function outputs the optimal action aˆ as follows: aˆ = π∗(bt). (5) 4.2 Training of POMDP We applied Q-learning (Monahan, 1982; Watkins and Dayan, 1992) to acquire the optimal policy π∗. Q-learning relies on the estimation of a Qfunction, which maximizes the discounted sum of future rewards of the system action at at a dialogue turn t given the current belief bt. Q-learning is performed by iterative updates on the training dialogue data: Q(bt, at) &lt;-- (1 − ε)Q(bt, at) + ε[R(st, at) + γ max at�� Q(bt+1,at+1)], (6) where ε is a learning rate, γ is a discount factor of a future reward. We experimentally decided ε = 0.01 and γ = 0.9. The optimal policy given by the Q-function is determined as, π∗(bt) = argmax Q(bt, at). (7) at However, it is impo</context>
</contexts>
<marker>Watkins, Dayan, 1992</marker>
<rawString>Christopher JCH Watkins and Peter Dayan. 1992. Qlearning. Machine learning, 8(3):279–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Steve Young</author>
</authors>
<title>Partially observable Markov decision processes for spoken dialog systems.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="2307" citStr="Williams and Young, 2007" startWordPosition="368" endWordPosition="371">r and the system converse on the current topic and the system provides potentially useful information for the user. Dialogue management of this kind of systems was usually made in a heuristic manner and based on simple rules (Dahl et al., 1994; Bohus and Rudnicky, 2003). There is not a clear principle nor established methodology to design and implement casual conversation systems. In the past years, machine learning, particularly reinforcement learning, have been investigated for dialogue management. MDPs and POMDPs are now widely used to model and train dialogue managers (Levin et al., 2000; Williams and Young, 2007; Young et al., 2010; Yoshino et al., 2013b). However, the conventional scheme assumes that the task and dialogue goal can be clearly stated and readily encoded in the RL reward function. This is not true in casual conversation or when browsing information. Some previous work has tackled with this problem. In a conversational chatting system (Shibata et al., 2014), users were asked to make evaluation at the end of each dialogue session, to define rewards for reinforcement learning. In a listening dialogue system (Meguro et al., 2010), levels of satisfaction were annotated in logs of dialogue s</context>
<context position="14855" citStr="Williams and Young, 2007" startWordPosition="2482" endWordPosition="2485"> output ASR and intention analysis involves errors. Here, s is a true user intention and o is an observed intention. The observation model P(o|s) is given by the likelihood of ASR result P(h|u) (Komatani and Kawahara, 2000) and the likelihood of the intention analysis P(o|h), P(o|s) = ∑ P(o, h|s) (2) h ∑≈ P(o|h)P(h|u). (3) h Here, u is an utterance of the user. We combine the N-best (N = 5) hypotheses of the ASR result h. 4 Dialogue Management for Information Navigation The conventional dialogue management for taskoriented dialogue systems is designed to reach a task goal as soon as possible (Williams and Young, 2007). In contrast, information navigation does not always have a clear goal, and the aim of information navigation is to provide as much relevant information as the user is interested in. Therefore, our dialogue manager refers user involvement or engagement (=level of interest) and the user focus 35 (=object of interest). This section describes the general dialogue management based on POMDP, and then gives an explanation of the proposed dialogue management using the user focus. 4.1 Dialogue management based on POMDP The POMDP-based statistical dialogue management is formulated as below. The random</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>Jason D. Williams and Steve Young. 2007. Partially observable Markov decision processes for spoken dialog systems. Computer Speech &amp; Language, 21(2):393–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Antoine Raux</author>
<author>Deepak Ramachandran</author>
<author>Alan Black</author>
</authors>
<title>The dialog state tracking challenge.</title>
<date>2013</date>
<booktitle>In Proceedings of the 14th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>404--413</pages>
<contexts>
<context position="21462" citStr="Williams et al., 2013" startWordPosition="3701" endWordPosition="3704">em calculated average reward of 100,000 dialogue sessions between the system and the user simulator given a fixed noise rate. Figure 4 shows the effect of the user focus. By introducing the user focus, the system receives higher rewards than the model without the user focus. Especially, the proposed model is more robust with a noise level of 10–30% that spoken dialogue systems often encounter (Yoshino et al., 2013a). 5.2 Evaluation of dialogue state tracking with real users Dialogue state tracking (DST) is a task of tracking the correct user state with a noisy input (e.g. ASR and NLU errors) (Williams et al., 2013). It tries to maximize the probability of the belief of the correct states, but we evaluated the accuracy of the 1-best result of the belief update. We also evaluated the average reward of each dialogue. The baseline system is a rule-based dialogue manager which we previously implemented. The baseline system is operated by a score of the question-answering module using predicateargument structures (Yoshino et al., 2011) and regular expressions for TP and GR modules. The DST accuracy and average reward are shown in Table 6. This result shows that the proposed method tracks the dialogue state of</context>
</contexts>
<marker>Williams, Raux, Ramachandran, Black, 2013</marker>
<rawString>Jason D. Williams, Antoine Raux, Deepak Ramachandran, and Alan Black. 2013. The dialog state tracking challenge. In Proceedings of the 14th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 404–413.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koichiro Yoshino</author>
<author>Shinsuke Mori</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>Spoken dialogue system based on information extraction using similarity of predicate argument structures.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>59--66</pages>
<contexts>
<context position="1460" citStr="Yoshino et al., 2011" startWordPosition="226" endWordPosition="229">e real world. Spoken dialogue applications that interact with a diversity of users are available on smart-phones. However, current applications are based on simple question answering and the system requires a clear query or a definite task goal. Therefore, next-generation dialogue systems should engage in casual interactions with users who do not have a clear intention or a task goal. Such systems include a sightseeing navigation system that uses tour guide books or documents in Wikipedia (Misu and Kawahara, 2010), and a news navigation system that introduces news articles updated day-by-day (Yoshino et al., 2011; Pan et al., 2012). In this paper, we develop an information navigation system that provides information even if the user request is not necessarily clear and there is not a matching document in the knowledge base. The user and the system converse on the current topic and the system provides potentially useful information for the user. Dialogue management of this kind of systems was usually made in a heuristic manner and based on simple rules (Dahl et al., 1994; Bohus and Rudnicky, 2003). There is not a clear principle nor established methodology to design and implement casual conversation sy</context>
<context position="4406" citStr="Yoshino et al., 2011" startWordPosition="707" endWordPosition="710">or Computational Linguistics Figure 1: An example of information navigation. Other modules Figure 2: Overview of the information navigation system. offers a new topic and probes the interest of the listener. If the listener shows interest, the speaker describes details of the topic. If the listener asks a specific question, the speaker answers the question. On the other hand, if the listener is not interested in the topic, the speaker avoids the details of that topic, and changes the topic. Topics are often taken from current news. In our past work, we have developed a news navigation system (Yoshino et al., 2011) based on this dialogue structure. The system provides topics collected from Web news texts, and the user gets information according to his interests and queries. 2.1 System overview An overview of the proposed system is depicted in Figure 2. The system has six modules, each of which implements a class of actions. Each module takes as input a recognized user utterance, an analyzed predicate-argument (P-A) structure and the detected user focus. The system begins dialogues by selecting the “topic presentation (TP)” module, which presents a new topic selected from a news article. The system choos</context>
<context position="5990" citStr="Yoshino et al., 2011" startWordPosition="977" endWordPosition="980">ews. In the ST module, the system provides a summary of the news article by using lead sentences. The system can also provide related topics with the “proactive presentation (PP)” module. This module is invoked by system initiative; this module is not invoked by any user request. If the user makes a specific question regarding the topic, the system switches to the “question answering (QA)” module to answer the question. This module answers questions on the presented topic and related topics. The modules of PP and QA are based on a dialogue framework which uses the similarity of PA structures (Yoshino et al., 2011). This framework defines the similarity of P-A structures between user queries and news articles, and retrieves or recommends the appropriate sentence from the news articles. This method searches for appropriate information from automatically parsed documents by referring to domain knowledge that is automatically extracted from domain corpus. Transitions between the modules are allowed as shown in Figure 2. The modules “greeting (GR)” and “keep silence (KS)” are also implemented. GR module generates fixed greeting patterns by using regular expression matching. In terms of dialogue flow, these </context>
<context position="9652" citStr="Yoshino et al., 2011" startWordPosition="1569" endWordPosition="1572">ed on content words or named entities in a user utterance. In the POMDP, decisions are made based on whether any user focus was detected in the user’s utterance. 3 Spoken Language Understanding (SLU) In this section, we present the spoken language understanding components of our system. It detects the user’s focus and intention and provides these to the dialogue manager. These spoken language understanding modules are formulated with a statistical model to give likelihoods which are used in POMDP. 3.1 Dialogue data We collected 606 utterances (from 10 users) with a rule-based dialogue system (Yoshino et al., 2011). We annotated two kinds of tags: user intention (6 tags defined in Section 3.3), and focus information defined in Section 2.2. An example of annotation is shown in Figure 3. We highlighted annotation points in the bold font. To prepare the training data, each utterance was labeled with one of the six modules, indicating the best module to respond. In addition, each phrase or P-A elements is labeled to indicated whether it is the user’s focus or not. The user focus is determined by the attributes (=specifications of words in the domain) and preference order of phrases to identify the most appr</context>
<context position="11178" citStr="Yoshino et al., 2011" startWordPosition="1835" endWordPosition="1838">The problem is defined as a sequential labeling of the focus labels to a sequence of the phrases of the user utterance. Features used are shown in the Table 1. ORDER features are the order of the phrase in the sequence and in the P-A structure. We incorporate these features because the user focus often appears in the first phrase of the user utterance. POS features are part-of-speech (POS) tags and their pairs in the phrase. P-A features are semantic role of the P-A structure. We also incorporate the domaindependent predicate-argument (P-A) scores that are defined with an unsupervised method (Yoshino et al., 2011). The score is discretized to 0.01, 0.02, 0.05, 0.1, 0.2, 0.5. Table 2 shows the accuracy of user focus detection, which was conducted via five-fold crossvalidation. “Phrase” is phrase-base accuracy and “sentence” indicates whether the presence of any user focus phrase was correctly detected (or not), regardless of whether the correct phrase was identified. This table indicates that WORD features are effective for detecting the user focus, but they are not essential for in the sentence-level accuracy. In this paper, we aim for portability across domains; therefore the dialogue manager only use</context>
<context position="21885" citStr="Yoshino et al., 2011" startWordPosition="3771" endWordPosition="3774"> Evaluation of dialogue state tracking with real users Dialogue state tracking (DST) is a task of tracking the correct user state with a noisy input (e.g. ASR and NLU errors) (Williams et al., 2013). It tries to maximize the probability of the belief of the correct states, but we evaluated the accuracy of the 1-best result of the belief update. We also evaluated the average reward of each dialogue. The baseline system is a rule-based dialogue manager which we previously implemented. The baseline system is operated by a score of the question-answering module using predicateargument structures (Yoshino et al., 2011) and regular expressions for TP and GR modules. The DST accuracy and average reward are shown in Table 6. This result shows that the proposed method tracks the dialogue state of the user with a high accuracy. Our statistical framework 37 Dialogue 1 Figure 5: A dialogue example. (This example is translated from Japanese) Sys: Hello. Usr: Hello. Tell me today&apos;s news. Sys: Tanaka showed a crazy split. His colleague Sabathia was stunned. Usr: What is the crazy split? Sys: His split comes just like a four-seam, but it drops quickly in front of the home plate. Usr: Where was he training? Sys: Tanaka</context>
</contexts>
<marker>Yoshino, Mori, Kawahara, 2011</marker>
<rawString>Koichiro Yoshino, Shinsuke Mori, and Tatsuya Kawahara. 2011. Spoken dialogue system based on information extraction using similarity of predicate argument structures. In Proceedings of the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koichiro Yoshino</author>
<author>Shinsuke Mori</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>Incorporating semantic information to selection of web texts for language model of spoken dialogue system.</title>
<date>2013</date>
<booktitle>In Proceedings of IEEE International Conference on Acoustic, Speech and Signal Processing,</booktitle>
<pages>8252--8256</pages>
<contexts>
<context position="2349" citStr="Yoshino et al., 2013" startWordPosition="376" endWordPosition="379"> and the system provides potentially useful information for the user. Dialogue management of this kind of systems was usually made in a heuristic manner and based on simple rules (Dahl et al., 1994; Bohus and Rudnicky, 2003). There is not a clear principle nor established methodology to design and implement casual conversation systems. In the past years, machine learning, particularly reinforcement learning, have been investigated for dialogue management. MDPs and POMDPs are now widely used to model and train dialogue managers (Levin et al., 2000; Williams and Young, 2007; Young et al., 2010; Yoshino et al., 2013b). However, the conventional scheme assumes that the task and dialogue goal can be clearly stated and readily encoded in the RL reward function. This is not true in casual conversation or when browsing information. Some previous work has tackled with this problem. In a conversational chatting system (Shibata et al., 2014), users were asked to make evaluation at the end of each dialogue session, to define rewards for reinforcement learning. In a listening dialogue system (Meguro et al., 2010), levels of satisfaction were annotated in logs of dialogue sessions to train a discriminative model. T</context>
<context position="21257" citStr="Yoshino et al., 2013" startWordPosition="3665" endWordPosition="3668">1 Evaluation of dialogue manager with user simulator First, we evaluated the dialogue manager with user simulation that is constructed from the training corpus (Section 3.1). In this evaluation, the system calculated average reward of 100,000 dialogue sessions between the system and the user simulator given a fixed noise rate. Figure 4 shows the effect of the user focus. By introducing the user focus, the system receives higher rewards than the model without the user focus. Especially, the proposed model is more robust with a noise level of 10–30% that spoken dialogue systems often encounter (Yoshino et al., 2013a). 5.2 Evaluation of dialogue state tracking with real users Dialogue state tracking (DST) is a task of tracking the correct user state with a noisy input (e.g. ASR and NLU errors) (Williams et al., 2013). It tries to maximize the probability of the belief of the correct states, but we evaluated the accuracy of the 1-best result of the belief update. We also evaluated the average reward of each dialogue. The baseline system is a rule-based dialogue manager which we previously implemented. The baseline system is operated by a score of the question-answering module using predicateargument struc</context>
</contexts>
<marker>Yoshino, Mori, Kawahara, 2013</marker>
<rawString>Koichiro Yoshino, Shinsuke Mori, and Tatsuya Kawahara. 2013a. Incorporating semantic information to selection of web texts for language model of spoken dialogue system. In Proceedings of IEEE International Conference on Acoustic, Speech and Signal Processing, pages 8252–8256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koichiro Yoshino</author>
<author>Shinji Watanabe</author>
<author>Jonathan Le Roux</author>
<author>John R Hershey</author>
</authors>
<title>Statistical dialogue management using intention dependency graph.</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th International Joint Conference on Natural Language Processing,</booktitle>
<pages>962--966</pages>
<marker>Yoshino, Watanabe, Le Roux, Hershey, 2013</marker>
<rawString>Koichiro Yoshino, Shinji Watanabe, Jonathan Le Roux, and John R. Hershey. 2013b. Statistical dialogue management using intention dependency graph. In Proceedings of the 6th International Joint Conference on Natural Language Processing, pages 962– 966.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Milica Gaˇsi´c</author>
<author>Simon Keizer</author>
<author>Franc¸ois Mairesse</author>
<author>Jost Schatzmann</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
</authors>
<title>The hidden information state model: A practical framework for POMDP-based spoken dialogue management.</title>
<date>2010</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>24</volume>
<issue>2</issue>
<marker>Young, Gaˇsi´c, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2010</marker>
<rawString>Steve Young, Milica Gaˇsi´c, Simon Keizer, Franc¸ois Mairesse, Jost Schatzmann, Blaise Thomson, and Kai Yu. 2010. The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech &amp; Language, 24(2):150–174.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
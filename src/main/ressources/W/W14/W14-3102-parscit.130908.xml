<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000216">
<title confidence="0.9979455">
Interactive Learning of Spatial Knowledge
for Text to 3D Scene Generation
</title>
<author confidence="0.9979">
Angel X. Chang, Manolis Savva and Christopher D. Manning
</author>
<affiliation confidence="0.996016">
Computer Science Department, Stanford University
</affiliation>
<email confidence="0.995958">
{angelx,msavva,manning}@cs.stanford.edu
</email>
<sectionHeader confidence="0.993833" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998721875">
We present an interactive text to 3D scene
generation system that learns the expected
spatial layout of objects from data. A user
provides input natural language text from
which we extract explicit constraints on
the objects that should appear in the scene.
Given these explicit constraints, the sys-
tem then uses prior observations of spa-
tial arrangements in a database of scenes
to infer the most likely layout of the ob-
jects in the scene. Through further user
interaction, the system gradually adjusts
and improves its estimates of where ob-
jects should be placed. We present exam-
ple generated scenes and user interaction
scenarios.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972612903226">
People possess the power of visual imagination
that allows them to turn descriptions of scenes into
imagery. The conceptual simplicity of generating
pictures from descriptions has spurred the desire
to make systems capable of this task. However, re-
search into computational systems for creating im-
agery from textual descriptions has seen only lim-
ited success.
Most current 3D scene design systems require
the user to learn complex manipulation interfaces
through which objects are constructed and pre-
cisely positioned within scenes. However, arrang-
ing objects in scenes can much more easily be
achieved using natural language. For instance, it
is much easier to say “Put a cup on the table’,
rather than having to search for a 3D model of a
cup, insert it into the scene, scale it to the correct
size, orient it, and position it on a table ensuring
it maintains contact with the table. By making
3D scene design more accessible to novice users
we empower a broader demographic to create 3D
scenes for use cases such as interior design, virtual
storyboarding and personalized augmented reality.
Unfortunately, several key technical challenges
restrict our ability to create text to 3D scene sys-
tems. Natural language is difficult to map to for-
mal representations of spatial knowledge and con-
straints. Furthermore, language rarely mentions
common sense facts about the world, that contain
critically important spatial knowledge. For exam-
ple, people do not usually mention the presence of
the ground or that most objects are supported by it.
As a consequence, spatial knowledge is severely
lacking in current computational systems.
Pioneering work in mapping text to 3D scene
representations has taken two approaches to ad-
dress these challenges. First, by restricting the dis-
course domain to a micro-world with simple geo-
metric shapes, the SHRDLU system demonstrated
parsing of natural language input for manipulating
the scene, and learning of procedural knowledge
through interaction (Winograd, 1972). However,
generalization to scenes with more complex ob-
jects and spatial relations is very hard to attain.
More recently, the WordsEye system has fo-
cused on the general text to 3D scene generation
task (Coyne and Sproat, 2001), allowing a user
to generate a 3D scene directly from a textual de-
scription of the objects present, their properties and
their spatial arrangement. The authors of Words-
Eye demonstrated the promise of text to scene gen-
eration systems but also pointed out some funda-
mental issues which restrict the success of their
system: a lot of spatial knowledge is required
which is hard to obtain. As a result, the user has to
use unnatural language (e.g. “the stool is 1 feet to
the south of the table”) to express their intent.
For a text to scene system to understand more
natural text, it must be able to infer implicit in-
formation not explicitly stated in the text. For in-
stance, given the sentence “there is an office with
a red chair”, the system should be able to infer
</bodyText>
<page confidence="0.990452">
14
</page>
<note confidence="0.789528">
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 14–21,
Baltimore, Maryland, USA, June 27, 2014. @c 2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99990264">
that the office also has a desk in front of the chair.
This sort of inference requires a source of prior
spatial knowledge. We propose learning this spa-
tial knowledge from existing 3D scene data. How-
ever, since the number of available scenes is small,
it is difficult to have broad coverage. Therefore,
we also rely on user interaction to augment and
grow the spatial knowledge. Luckily, user inter-
action is also natural for scene design since it is an
inherently interactive process where user input is
needed for refinement.
Our contributions address the fundamental chal-
lenges of establishing and interactively expanding
a spatial knowledge base. We build on prior work
in data-driven scene synthesis (Fisher et al., 2012)
to automatically extract general spatial knowledge
from data: knowledge of what objects occur in
scenes, and their expected spatial relations. Our
system then uses this knowledge to generate scenes
from natural text inferring implicit constraints. It
then leverages user interaction to allow refinement
of the scene, and improve the spatial knowledge
base. We demonstrate that user interaction is criti-
cal in expanding and improving spatial knowledge
learned from data.
</bodyText>
<sectionHeader confidence="0.971235" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999946866666667">
A key insight for enabling text to scene generation
is that linguistic and non-linguistic spatial knowl-
edge is critical for this task and can be learned di-
rectly from data representing the physical world
and from interactions of people with such data.
User feedback allows us to interactively update
spatial knowledge, an idea that we illustrate here
in the domain of spatial relations. Early work on
the PUT system (Clay and Wilhelms,1996) and the
SHRDLU system (Winograd, 1972) gives a good
formalization of the interactive linguistic manipu-
lation of objects in 3D scenes. Recently, there has
been promising work on generating 2D clipart for
sentences using probabilistic models with place-
ment priors learned from data (Zitnick et al., 2013).
</bodyText>
<subsectionHeader confidence="0.997423">
2.1 Text to Scene Systems
</subsectionHeader>
<bodyText confidence="0.99997055">
Prior work on text to 3D scene generation has re-
sulted in systems such as WordsEye (Coyne and
Sproat, 2001) and other similar approaches (Sev-
ersky and Yin, 2006). These systems are typi-
cally not designed to be fully interactive and do not
leverage user interaction to improve their results.
Furthermore, they mostly rely on manual annota-
tion of 3D models and on hand crafted rules to map
text to object placement decisions, which makes
them hard to extend and generalize. More re-
cent work has used crowdsourcing platforms, such
as Amazon Mechanical Turk, to collect necessary
annotations (Coyne et al., 2012). However, this
data collection is treated as a separate pre-process
and the user still has no influence on the system’s
knowledge base. We address one part of this is-
sue: learning simple spatial knowledge from data
and interactively updating it through user feed-
back. We also infer unstated implicit constraints
thus allowing for more natural text input.
</bodyText>
<subsectionHeader confidence="0.999312">
2.2 Automatic Scene Layout
</subsectionHeader>
<bodyText confidence="0.99988356">
Prior work on scene layout has focused largely on
room interiors and determining good furniture lay-
outs by optimizing energy functions that capture
the quality of a proposed layout. These energy
functions are encoded from interior design guide-
lines (Merrell et al., 2011) or learned from input
scene data (Fisher et al., 2012). Knowledge of ob-
ject co-occurrences and spatial relations is repre-
sented by simple models such as mixtures of Gaus-
sians on pairwise object positions and orientations.
Methods to learn scene structure have been demon-
strated using various data sources including sim-
ulation of human agents in 3D scenes (Jiang et
al., 2012; Jiang and Saxena, 2013), and analysis
of supporting contact points in scanned environ-
ments (Rosman and Ramamoorthy, 2011).
However, prior work has not explored methods
for enabling users of scene generation algorithms
to interactively refine and improve an underlying
spatial knowledge model – a capability which is
critically important. Our work focuses on demon-
strating an interactive system which allows a user
to manipulate and refine such spatial knowledge.
Such a system is useful regardless of the algorithm
used to get the input spatial knowledge.
</bodyText>
<subsectionHeader confidence="0.995497">
2.3 Interactive Learning
</subsectionHeader>
<bodyText confidence="0.9999776">
In many tasks, user interaction can provide feed-
back to an automated system and guide it towards
a desired goal. There is much prior work in various
domains including interactive systems for refin-
ing image search algorithms (Fogarty et al., 2008)
and for manipulating social network group cre-
ation (Amershi et al., 2012). We focus on the do-
main of text to 3D scene generation where despite
the success of data-driven methods there has been
little work on interactive learning systems.
</bodyText>
<page confidence="0.995435">
15
</page>
<sectionHeader confidence="0.983176" genericHeader="method">
3 Approach Overview
</sectionHeader>
<bodyText confidence="0.999802510204082">
What should an interactive text to scene system
look like from the perspective of a user? The user
should be able to provide a brief scene description
in natural language as input. The system parses
this text to a set of explicitly provided constraints
on what objects should be present, and how they
are arranged. This set of constraints should be au-
tomatically expanded by using prior knowledge so
that “common sense” facts are reflected in the gen-
eral scene – an example is the static support hier-
archy for objects in the scene (i.e. plate goes on
table, table goes on ground). The system gener-
ates a candidate scene and then the user is free to
interact with it by direct control or through textual
commands. The system can then leverage user in-
teraction to update its spatial knowledge and inte-
grate newly learned constraints or relations. The
final output is a 3D scene that can be viewed from
any position and rendered by a graphics engine. In
this paper we select an initial viewpoint such that
objects are in the frame and view-based spatial re-
lations are satisfied.
How might we create such a system? Spatial
knowledge is critical for this task. We need it to
understand spatial language, to plausibly position
objects within scenes and to allow users to manip-
ulate them. We learn spatial knowledge from ex-
ample scene data to ensure that our approach can
be generalized to different scenarios. We also learn
from user interaction to refine and expand existing
spatial knowledge. In §5 we describe the spatial
knowledge used by our system.
We define our problem as the task of taking text
describing a scene as input, and generating a plau-
sible 3D scene described by that text as output.
More concretely, based on the input text, we se-
lect objects from a dataset of 3D models (§4) and
arrange them to generate output scenes. See Fig-
ure 1 for an illustration of the system architecture.
We break the system down into several subtasks:
Constraint Parsing (§6): Parse the input textual
description of a concrete scene into a set of con-
straints on the objects present and spatial relations
between them. Automatically expand this set of
constraints to account for implicit constraints not
specified in the text.
Scene Generation (§7): Using above constraints
and prior knowledge on the spatial arrangement of
objects, construct a scene template. Next, sample
</bodyText>
<figure confidence="0.305919">
Spatial KB 3D Models
</figure>
<figureCaption confidence="0.9904125">
Figure 1: Diagram illustrating the architecture of
our system.
</figureCaption>
<bodyText confidence="0.931818375">
the template and select a set of objects to be in-
stantiated. Finally, optimize the placement of the
objects to finalize the arrangement of the scene.
Interaction and Learning (§8): Provide means
for a user to interactively adjust the scene through
direct manipulation and textual commands. Use
any such interaction to update the system’s spatial
knowledge so it better captures the user’s intent.
</bodyText>
<sectionHeader confidence="0.967427" genericHeader="method">
4 Object Knowledge from 3D Models
</sectionHeader>
<bodyText confidence="0.999877368421053">
To generate scenes we need to have a collection
of 3D models for representing physical objects.
We use a 3D model dataset collected from Google
3D Warehouse by prior work in scene synthe-
sis and containing about 12490 mostly indoor ob-
jects (Fisher et al., 2012). These models have text
associated with them in the form of names and
tags. In addition, we semi-automatically annotated
models with object category labels (roughly 270
classes). We used model tags to set these labels,
and verified and augmented them manually.
In addition, we automatically rescale models so
that they have physically plausible sizes and orient
them so that they have a consistent up and front
direction (Savva et al., 2014). Due to the num-
ber of models in the database, not all models were
rescaled and re-oriented. We then indexed all mod-
els in a database that we query at run-time for re-
trieval based on category and tag labels.
</bodyText>
<sectionHeader confidence="0.996868" genericHeader="method">
5 Spatial Knowledge
</sectionHeader>
<bodyText confidence="0.999952333333333">
Here we describe how we learn spatial knowledge
from existing scene data. We base our approach
on that of (Fisher et al., 2012) and use their dataset
</bodyText>
<figure confidence="0.996748523809524">
Identify
objects and
relationships
Objects:
CAKE, TABLE
ON(CAKE, TABLE)
Objects:
PLATE, FORK
ON(FORK, TABLE)
ON(PLATE, TABLE)
ON(CAKE, PLATE)
INTERACTION
Create
Scene
Identify
missing
objects
CONSTRAINT
PARSING
“There is a piece of
cake on a table.”
</figure>
<page confidence="0.980128">
16
</page>
<bodyText confidence="0.9998246">
of 133 small indoor scenes created with 1723 3D
Warehouse models. Relative object-to-object po-
sition and orientation priors can also be learned
from the scene data but we have not yet incorpo-
rated them in the results for this paper.
</bodyText>
<subsectionHeader confidence="0.992992">
5.1 Support Hierarchy
</subsectionHeader>
<bodyText confidence="0.9999717">
We observe the static support relations of objects
in existing scenes to establish a prior over what ob-
jects go on top of what other objects. As an exam-
ple, by observing plates and forks on tables most
of the time, we establish that tables are more likely
to support plates and forks than chairs. We esti-
mate the probability of a parent category Cp sup-
porting a given child category Cc as a simple con-
ditional probability based on normalized observa-
tion counts.
</bodyText>
<subsectionHeader confidence="0.999142">
5.2 Supporting surfaces
</subsectionHeader>
<bodyText confidence="0.997727357142857">
To identify which surfaces on parent objects sup-
port child objects, we first segment parent models
into planar surfaces using a simple region-growing
algorithm based on (Kalvin and Taylor, 1996). We
characterize support surfaces by the direction of
their normal vector limited to the six canonical di-
rections: up, down, left, right, front, back. We then
learn a probability of supporting surface normal
direction Sn given child object category Cc. For
example, posters are typically found on walls so
their support normal vectors are in the horizontal
directions. Any unobserved child categories are
assumed to have Psurf(Sn = up|Cc) = 1 since
most things rest on a horizontal surface (e.g. floor).
</bodyText>
<equation confidence="0.96912">
Psurf(Sn|Cc) = count(Cc)
</equation>
<bodyText confidence="0.48717">
count(Cc on surface with Sn)
</bodyText>
<subsectionHeader confidence="0.98696">
5.3 Spatial Relations
</subsectionHeader>
<bodyText confidence="0.999870076923077">
For spatial relations we use a set of predefined re-
lations: left, right, above, below, front, back, on
top of, next to, near, inside, and outside. These
are measured using axis-aligned bounding boxes
from the viewer’s perspective. More concretely,
the bounding boxes of the two objects involved in
a spatial relation are compared to determine vol-
ume overlap or closest distance (for proximity re-
lations). Table 1 gives a few examples of the defi-
nitions of these spatial relations.
Since these spatial relations are resolved with re-
spect to the current view of the scene, they corre-
spond to view-centric definitions of these spatial
</bodyText>
<equation confidence="0.9966232">
Relation P(relation)
Vol(AnB)
V ol(A)
1 - Vol(AnB)
V ol(A)
V ol(An left (B))
Vol(A)
V ol(An right (B))
V ol(A)
1(dist(A, B) &lt; tnear)
</equation>
<tableCaption confidence="0.881908">
Table 1: Definitions of spatial relation using object
</tableCaption>
<bodyText confidence="0.922141571428572">
bounding box computations. Note that dist(A, B)
is normalized with respect to the maximum extent
of the bounding box of B.
concepts. An interesting line of future work would
be to explore when ego-centric and object-centric
spatial reference models are more likely in a given
utterance, and resolve the spatial term accordingly.
</bodyText>
<sectionHeader confidence="0.991383" genericHeader="method">
6 Constraint Parsing
</sectionHeader>
<bodyText confidence="0.999690470588235">
During constraint parsing we take the input text
and identify the objects and the relations between
them. For each object, we also identify proper-
ties associated with it such as category label, ba-
sic attributes such as color and material, and num-
ber of occurrences in the scene. Based on the ob-
ject category and attributes, and other words in
the noun phrase mentioning the object, we iden-
tify a set of associated keywords to be used later
for querying the 3D model database. Spatial re-
lations between objects are extracted as predicates
of the form on(A,B) or left(A,B) where A and B are
recognized objects.
As an example, given the input “There is a
room with a desk and a red chair. The chair is
to the left of the desk.” we extract the following
objects and spatial relations:
</bodyText>
<table confidence="0.9192915">
Objects:
index category attributes keywords
0 room room
1 desk desk
2 chair color:red chair, red
Relations: left(chair, desk)
</table>
<bodyText confidence="0.999752428571429">
The input text is processed using the Stanford
CoreNLP pipeline1. We use the Stanford corefer-
ence system to determine when the same object is
being referred to. To identify objects, we look for
noun phrases and use the head word as the cate-
gory, filtering with WordNet (Miller, 1995) to de-
termine which objects are visualizable (under the
</bodyText>
<footnote confidence="0.707824">
1http://nlp.stanford.edu/software/corenlp.shtml
</footnote>
<equation confidence="0.939731571428571">
Psupport(Cp�Cc) = count(Cc)
count(Cc on Cp)
inside(A,B)
outside(A,B)
left(A,B)
right(A,B)
near(A,B)
</equation>
<page confidence="0.926483">
17
</page>
<table confidence="0.442034666666667">
tag:VBN=verb &gt;nsubjpass =nsubj &gt;prep (=prep &gt;pobj =pobj)
tag:VB=verb &gt;dobj =dobj &gt;prep (=prep &gt;pobj =pobj)
The chair[nsubj] is made[verb] of[prep] wood[pobj]
Put[verb] the cup[dobj] on[prep] the table[pobj]
Example Text
Dependency Pattern
</table>
<tableCaption confidence="0.844338">
Table 2: Example dependency patterns for extracting spatial relations.
</tableCaption>
<figureCaption confidence="0.9990744">
Figure 2: Generated scene for “There is a room
with a desk and a lamp. There is a chair to the
right of the desk.” The inferred scene hierarchy is
overlayed in the center.
Figure 4: Generated scene for “There is a room
</figureCaption>
<bodyText confidence="0.943726866666667">
with a table and a sandwich.” Note that the plate is
not explicitly stated, but is inferred by the system.
physical object synset, excluding locations). To
identify properties of the objects, we extract other
adjectives and nouns in the noun phrase. We also
match dependency patterns such as “X is made of
Y” to extract more attributes and keywords. Fi-
nally, we use dependency patterns to extract spa-
tial relations between objects (see Table 2 for some
example patterns).
We used a fairly simple deterministic approach
to map text to the scene template and user actions
on the scene. An interesting avenue for future re-
search is to automatically learn how to map text
using more advanced semantic parsing methods.
</bodyText>
<sectionHeader confidence="0.98389" genericHeader="method">
7 Scene Generation
</sectionHeader>
<bodyText confidence="0.99546825">
During scene generation we aim to find the most
likely scene given the input utterance, and prior
knowledge. Once we have determined from the
input text what objects exist and their spatial re-
</bodyText>
<figureCaption confidence="0.751051">
Figure 3: Generated scene for “There is a room
with a poster bed and a poster.”
</figureCaption>
<bodyText confidence="0.99665403125">
lations in the scene, we select 3D models match-
ing the objects and their associated properties. We
sample the support hierarchy prior Psupport to ob-
tain the support hierarchy for the scene.
We then initialize the positions of objects within
the scene by traversing the support hierarchy in
depth-first order, positioning the largest available
child node and recursing. Child nodes are posi-
tioned by selecting a supporting surface on a can-
didate parent object through sampling of Psurf and
ensuring no collisions exist with other objects. If
there are any spatial constraints that are not satis-
fied, we remove and randomly reposition the ob-
jects violating the constraints, and iterate to im-
prove the layout. The resulting scene is rendered
and presented to the user.
Figure 2 shows a rendering of a generated scene
along with the support hierarchy and input text.
Even though the spatial relation between lamp and
desk was not mentioned explicitly, we infer that
the lamp is supported by the top surface of the
desk. In Figure 3 we show another example of
a generated scene for the input “There is a room
with a poster bed and a poster”. Note that the sys-
tem differentiates between a “poster” and a “poster
bed” – it correctly selects and places the bed on the
floor, while the poster is placed on the wall.
Figure 4 shows an example of inferring missing
objects. Even though the plate was not explicitly
mentioned in the input, we infer that the sandwich
is more likely to be supported by a plate rather than
directly placed on the table. Without this infer-
</bodyText>
<page confidence="0.998387">
18
</page>
<figureCaption confidence="0.559018666666667">
Figure 5: Left: chair is selected using “the chair to
the right of the table” or “the object to the right of
the table”. Chair is not selected for “the cup to the
</figureCaption>
<bodyText confidence="0.989334571428571">
right of the table”. Right: Different view results
in different chair being selected for the input “the
chair to the right of the table”.
ence, the user would need to be much more verbose
with text such as “There is a room with a table, a
plate and a sandwich. The sandwich is on the plate,
and the plate is on the table.”
</bodyText>
<sectionHeader confidence="0.992529" genericHeader="method">
8 Interactive System
</sectionHeader>
<bodyText confidence="0.999955764705882">
Once a scene is generated, the user can view the
scene and manipulate it using both simple action
phrases and mouse interaction. The system sup-
ports traditional 3D scene interaction mechanisms
such as navigating the viewpoint with mouse and
keyboard, selection and movement of object mod-
els by clicking. In addition, a user can give simple
textual commands to select and modify objects, or
to refine the scene. For example, a user can re-
quest to “remove the chair” or “put a pot on the
table” which requires the system to resolve refer-
ents to objects in the scene (see §8.1). The system
tracks user interactions throughout this process and
can adjust its spatial knowledge accordingly. In
the following sections, we give some examples of
how the user can interact with the system and how
the system learns from this interaction.
</bodyText>
<subsectionHeader confidence="0.998349">
8.1 View centric spatial relations
</subsectionHeader>
<bodyText confidence="0.956436764705883">
During interaction, the user can refer to objects
with their categories and with spatial relations be-
tween them. Objects are disambiguated by both
category and view-centric spatial relations. We use
the WordNet hierarchy to resolve hyponym or hy-
pernym referents to objects in the scene. In the left
screenshot in Figure 5, the user can select a chair
to the right of the table using the phrase “chair to
the right of the table” or “object to the right of the
table”. The user can then change their viewpoint
by rotating and moving around. Since spatial rela-
tions are resolved with respect to the current view-
point, we see that a different chair is selected for
Figure 6: Left: initial scene. Right: after input
“Put a lamp on the table”.
the same phrase from the different viewpoint in the
right screenshot.
</bodyText>
<subsectionHeader confidence="0.996283">
8.2 Scene Editing with Text
</subsectionHeader>
<bodyText confidence="0.999993">
By using simple textual commands the user can
edit the scene. For example, given the initial scene
on the left in Figure 6, the user can then issue the
command “put a lamp on the table” which results
in the scene on the right. The system currently al-
lows for adding objects to new positions and re-
moving existing objects. Currently, repositioning
of objects is performed only with direct control,
but in the future we also plan to support reposi-
tioning of objects by using textual commands.
</bodyText>
<subsectionHeader confidence="0.998964">
8.3 Learning Support Hierarchy
</subsectionHeader>
<bodyText confidence="0.9999594">
After a user requests that a lamp be placed on a ta-
ble, the system updates its prior on the likelihood
of a lamp being supported by a table. Based on
prior observations the likelihood of lamps being
placed on tables was very low (4%) since very few
lamps were observed on tables in the scene dataset.
However, after the user interaction, we recompute
the prior including the scene that the user has cre-
ated and the probability of lamp on table increases
to 12% (see Figure 7).
</bodyText>
<subsectionHeader confidence="0.994902">
8.4 Learning Object Names
</subsectionHeader>
<bodyText confidence="0.999987571428571">
Often, objects or parts may not have associated la-
bels that the user would use to refer to the objects.
In those cases, the system can inform the user that
it cannot resolve a given name, and the user can
then select the objector part of the object they were
referring to and annotate it with a label. For in-
stance, in Figure 8, the user annotated the differ-
ent parts of the room as “floor”, “wall”, “window”,
and “door”. Before annotation, the system did not
know any labels for these parts of the room. After
annotation, the user can select these parts using the
associated names. In addition, the system updates
its spatial knowledge base and can now predict that
the probability of a poster being placed on a wall
</bodyText>
<page confidence="0.998344">
19
</page>
<figureCaption confidence="0.9913108">
Figure 7: Probability of supporting parent categories for lamps before and after the user explicitly requests
a lamp on a table.
Figure 8: The user clicks and selects parts of the scene, annotating them as “floor”, “wall”, “window”,
“door”. After annotation, the user can also refer to these parts with the associated names. The system
spatial knowledge base is updated accordingly.
</figureCaption>
<figure confidence="0.992479818181818">
0% 25% 50% 75% 100%
Table
Desk
Room
Desk
Room
Nightstand
Table
Before
After
Nightstand
</figure>
<bodyText confidence="0.993590833333333">
is 40%, and that the probability of a table being
placed on the floor is 23%. Note that these prob-
abilities are based on multiple observations of the
annotated room. Accumulating annotations such
as these and propagating labels to new models is
an effective way to expand spatial knowledge.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="method">
9 Future Work
</sectionHeader>
<bodyText confidence="0.9999975">
We described a preliminary interactive text to 3D
scene generation system that can learn from prior
data and user interaction. We hope to improve
the system by incorporating more feedback mech-
anisms for the user, and the learning algorithm.
If the user requests a particular object be se-
lected but the system gets the referent wrong, the
user could then indicate the error and provide a cor-
rection. We can then use this feedback as a source
oftraining data to improve the interpretation oftext
to the desired user action. For example, if the user
asks to “select the red bowl” and the system could
not resolve “red bowl” to the correct object, the
user could intervene by clicking on the correct ref-
erent object. Simple interactions such as this are
incredibly powerful for providing additional data
for learning. Though we did not focus on this as-
pect, a dialogue-based interaction pattern is natural
for our system. The user can converse with the sys-
tem to iteratively refine the scene and the system
can ask for clarifications at any point – when and
how the system should inquire for more informa-
tion is interesting future research.
To evaluate whether the generated scenes are
satisfactory, we can ask people to rate them against
input text descriptions. We can also study usage
of the system in concrete tasks to see how often
users need to provide corrections and manually
manipulate the scene. A useful baseline to com-
pare against would be a traditional scene manipula-
tion system. By doing these studies at a large scale,
for instance by making the interface available on
</bodyText>
<page confidence="0.979911">
20
</page>
<bodyText confidence="0.999968">
the web, we can crowdsource the accumulation of
user interactions and gathering of spatial knowl-
edge. Simultaneously, running formal user stud-
ies to better understand preference for text-based
versus direct interactions during different actions
would be very beneficial for more informed design
of text-to-scene generation systems.
</bodyText>
<sectionHeader confidence="0.99274" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999993290322581">
We have demonstrated the usefulness of an inter-
active text to 3D scene generation system. Spatial
knowledge is essential for text to 3D scene gener-
ation. While it is possible to learn spatial knowl-
edge purely from data, it is hard to have complete
coverage of all possible scenarios. Interaction and
user feedback is a good way to improve coverage
and to refine spatial knowledge. In addition, in-
teraction is a natural mode of user involvement in
scene generation and creative tasks.
Little prior work has addressed the need for in-
teraction or the need for recovering implicit spatial
constraints. We propose that the resolution of un-
mentioned spatial constraints, and leveraging user
interaction to acquire spatial knowledge are criti-
cal for enabling natural text to scene generation.
User interaction is essential for text to scene
generation since the process is fundamentally
under-constrained. Most natural textual descrip-
tions of scenes will not mention many visual as-
pects of a physical scene. However, it is still pos-
sible to automatically generate a plausible starting
scene for refinement.
Our work focused on showing that user interac-
tion is both natural and useful for a text to scene
generation system. Furthermore, refining spatial
knowledge through interaction is a promising way
of acquiring more implicit knowledge. Finally,
any practically useful text to scene generation will
by necessity involve interaction with users who
have particular goals and tasks in mind.
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999840919354839">
Saleema Amershi, James Fogarty, and Daniel Weld.
2012. Regroup: interactive machine learning for on-
demand group creation in social networks. In Pro-
ceedings of the SIGCHI Conference on Human Fac-
tors in Computing Systems.
Sharon Rose Clay and Jane Wilhelms. 1996. Put:
Language-based interactive manipulation of objects.
Computer Graphics and Applications, IEEE.
Bob Coyne and Richard Sproat. 2001. WordsEye: an
automatic text-to-scene conversion system. In Pro-
ceedings of the 28th annual conference on Computer
graphics and interactive techniques.
Bob Coyne, Alexander Klapheke, Masoud Rouhizadeh,
Richard Sproat, and Daniel Bauer. 2012. Annota-
tion tools and knowledge representation for a text-to-
scene system. Proceedings of COLING 2012: Tech-
nical Papers.
Matthew Fisher, Daniel Ritchie, Manolis Savva,
Thomas Funkhouser, and Pat Hanrahan. 2012.
Example-based synthesis of 3D object arrangements.
ACM Transactions on Graphics (TOG).
James Fogarty, Desney Tan, Ashish Kapoor, and Simon
Winder. 2008. CueFlik: interactive concept learn-
ing in image search. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems.
Yun Jiang and Ashutosh Saxena. 2013. Infinite la-
tent conditional random fields for modeling environ-
ments through humans.
Yun Jiang, Marcus Lim, and Ashutosh Saxena. 2012.
Learning object arrangements in 3D scenes using hu-
man context. In Proceedings of the 29th Interna-
tional Conference on Machine Learning (ICML-12).
Alan D Kalvin and Russell H Taylor. 1996. Superfaces:
Polygonal mesh simplification with bounded error.
Computer Graphics and Applications, IEEE.
Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh
Agrawala, and Vladlen Koltun. 2011. Interactive
furniture layout using interior design guidelines. In
ACM Transactions on Graphics (TOG).
G.A. Miller. 1995. WordNet: a lexical database for
english. CACM.
Benjamin Rosman and Subramanian Ramamoorthy.
2011. Learning spatial relationships between ob-
jects. The International Journal of Robotics Re-
search.
Manolis Savva, Angel X. Chang, Gilbert Bernstein,
Christopher D. Manning, and Pat Hanrahan. 2014.
On being the right scale: Sizing large collections of
3D models. Stanford University Technical Report
CSTR 2014-03.
Lee M Seversky and Lijun Yin. 2006. Real-time au-
tomatic 3D scene generation from natural language
voice and text descriptions. In Proceedings of the
14th annual ACM international conference on Mul-
timedia.
Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive psychology.
C Lawrence Zitnick, Devi Parikh, and Lucy Vander-
wende. 2013. Learning the visual interpretation
of sentences. In IEEE Intenational Conference on
Computer Vision (ICCV).
</reference>
<page confidence="0.999438">
21
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875717">
<title confidence="0.998946">Interactive Learning of Spatial for Text to 3D Scene Generation</title>
<author confidence="0.953327">X Chang</author>
<author confidence="0.953327">Manolis Savva D</author>
<affiliation confidence="0.976598">Computer Science Department, Stanford</affiliation>
<email confidence="0.999585">angelx@cs.stanford.edu</email>
<email confidence="0.999585">msavva@cs.stanford.edu</email>
<email confidence="0.999585">manning@cs.stanford.edu</email>
<abstract confidence="0.995812294117647">We present an interactive text to 3D scene generation system that learns the expected spatial layout of objects from data. A user provides input natural language text from which we extract explicit constraints on the objects that should appear in the scene. Given these explicit constraints, the system then uses prior observations of spatial arrangements in a database of scenes to infer the most likely layout of the objects in the scene. Through further user interaction, the system gradually adjusts and improves its estimates of where objects should be placed. We present example generated scenes and user interaction scenarios.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Saleema Amershi</author>
<author>James Fogarty</author>
<author>Daniel Weld</author>
</authors>
<title>Regroup: interactive machine learning for ondemand group creation in social networks.</title>
<date>2012</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.</booktitle>
<contexts>
<context position="8591" citStr="Amershi et al., 2012" startWordPosition="1370" endWordPosition="1373">ledge model – a capability which is critically important. Our work focuses on demonstrating an interactive system which allows a user to manipulate and refine such spatial knowledge. Such a system is useful regardless of the algorithm used to get the input spatial knowledge. 2.3 Interactive Learning In many tasks, user interaction can provide feedback to an automated system and guide it towards a desired goal. There is much prior work in various domains including interactive systems for refining image search algorithms (Fogarty et al., 2008) and for manipulating social network group creation (Amershi et al., 2012). We focus on the domain of text to 3D scene generation where despite the success of data-driven methods there has been little work on interactive learning systems. 15 3 Approach Overview What should an interactive text to scene system look like from the perspective of a user? The user should be able to provide a brief scene description in natural language as input. The system parses this text to a set of explicitly provided constraints on what objects should be present, and how they are arranged. This set of constraints should be automatically expanded by using prior knowledge so that “common</context>
</contexts>
<marker>Amershi, Fogarty, Weld, 2012</marker>
<rawString>Saleema Amershi, James Fogarty, and Daniel Weld. 2012. Regroup: interactive machine learning for ondemand group creation in social networks. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Rose Clay</author>
<author>Jane Wilhelms</author>
</authors>
<title>Put: Language-based interactive manipulation of objects. Computer Graphics and Applications,</title>
<date>1996</date>
<publisher>IEEE.</publisher>
<marker>Clay, Wilhelms, 1996</marker>
<rawString>Sharon Rose Clay and Jane Wilhelms. 1996. Put: Language-based interactive manipulation of objects. Computer Graphics and Applications, IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coyne</author>
<author>Richard Sproat</author>
</authors>
<title>WordsEye: an automatic text-to-scene conversion system.</title>
<date>2001</date>
<booktitle>In Proceedings of the 28th annual conference on Computer graphics and interactive techniques.</booktitle>
<contexts>
<context position="3104" citStr="Coyne and Sproat, 2001" startWordPosition="482" endWordPosition="485">nt computational systems. Pioneering work in mapping text to 3D scene representations has taken two approaches to address these challenges. First, by restricting the discourse domain to a micro-world with simple geometric shapes, the SHRDLU system demonstrated parsing of natural language input for manipulating the scene, and learning of procedural knowledge through interaction (Winograd, 1972). However, generalization to scenes with more complex objects and spatial relations is very hard to attain. More recently, the WordsEye system has focused on the general text to 3D scene generation task (Coyne and Sproat, 2001), allowing a user to generate a 3D scene directly from a textual description of the objects present, their properties and their spatial arrangement. The authors of WordsEye demonstrated the promise of text to scene generation systems but also pointed out some fundamental issues which restrict the success of their system: a lot of spatial knowledge is required which is hard to obtain. As a result, the user has to use unnatural language (e.g. “the stool is 1 feet to the south of the table”) to express their intent. For a text to scene system to understand more natural text, it must be able to in</context>
<context position="6158" citStr="Coyne and Sproat, 2001" startWordPosition="981" endWordPosition="984">r feedback allows us to interactively update spatial knowledge, an idea that we illustrate here in the domain of spatial relations. Early work on the PUT system (Clay and Wilhelms,1996) and the SHRDLU system (Winograd, 1972) gives a good formalization of the interactive linguistic manipulation of objects in 3D scenes. Recently, there has been promising work on generating 2D clipart for sentences using probabilistic models with placement priors learned from data (Zitnick et al., 2013). 2.1 Text to Scene Systems Prior work on text to 3D scene generation has resulted in systems such as WordsEye (Coyne and Sproat, 2001) and other similar approaches (Seversky and Yin, 2006). These systems are typically not designed to be fully interactive and do not leverage user interaction to improve their results. Furthermore, they mostly rely on manual annotation of 3D models and on hand crafted rules to map text to object placement decisions, which makes them hard to extend and generalize. More recent work has used crowdsourcing platforms, such as Amazon Mechanical Turk, to collect necessary annotations (Coyne et al., 2012). However, this data collection is treated as a separate pre-process and the user still has no infl</context>
</contexts>
<marker>Coyne, Sproat, 2001</marker>
<rawString>Bob Coyne and Richard Sproat. 2001. WordsEye: an automatic text-to-scene conversion system. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coyne</author>
<author>Alexander Klapheke</author>
<author>Masoud Rouhizadeh</author>
<author>Richard Sproat</author>
<author>Daniel Bauer</author>
</authors>
<title>Annotation tools and knowledge representation for a text-toscene system.</title>
<date>2012</date>
<booktitle>Proceedings of COLING 2012: Technical Papers.</booktitle>
<contexts>
<context position="6659" citStr="Coyne et al., 2012" startWordPosition="1063" endWordPosition="1066">ne Systems Prior work on text to 3D scene generation has resulted in systems such as WordsEye (Coyne and Sproat, 2001) and other similar approaches (Seversky and Yin, 2006). These systems are typically not designed to be fully interactive and do not leverage user interaction to improve their results. Furthermore, they mostly rely on manual annotation of 3D models and on hand crafted rules to map text to object placement decisions, which makes them hard to extend and generalize. More recent work has used crowdsourcing platforms, such as Amazon Mechanical Turk, to collect necessary annotations (Coyne et al., 2012). However, this data collection is treated as a separate pre-process and the user still has no influence on the system’s knowledge base. We address one part of this issue: learning simple spatial knowledge from data and interactively updating it through user feedback. We also infer unstated implicit constraints thus allowing for more natural text input. 2.2 Automatic Scene Layout Prior work on scene layout has focused largely on room interiors and determining good furniture layouts by optimizing energy functions that capture the quality of a proposed layout. These energy functions are encoded </context>
</contexts>
<marker>Coyne, Klapheke, Rouhizadeh, Sproat, Bauer, 2012</marker>
<rawString>Bob Coyne, Alexander Klapheke, Masoud Rouhizadeh, Richard Sproat, and Daniel Bauer. 2012. Annotation tools and knowledge representation for a text-toscene system. Proceedings of COLING 2012: Technical Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Fisher</author>
<author>Daniel Ritchie</author>
<author>Manolis Savva</author>
<author>Thomas Funkhouser</author>
<author>Pat Hanrahan</author>
</authors>
<title>Example-based synthesis of 3D object arrangements.</title>
<date>2012</date>
<journal>ACM Transactions on Graphics (TOG).</journal>
<contexts>
<context position="4796" citStr="Fisher et al., 2012" startWordPosition="766" endWordPosition="769">spatial knowledge. We propose learning this spatial knowledge from existing 3D scene data. However, since the number of available scenes is small, it is difficult to have broad coverage. Therefore, we also rely on user interaction to augment and grow the spatial knowledge. Luckily, user interaction is also natural for scene design since it is an inherently interactive process where user input is needed for refinement. Our contributions address the fundamental challenges of establishing and interactively expanding a spatial knowledge base. We build on prior work in data-driven scene synthesis (Fisher et al., 2012) to automatically extract general spatial knowledge from data: knowledge of what objects occur in scenes, and their expected spatial relations. Our system then uses this knowledge to generate scenes from natural text inferring implicit constraints. It then leverages user interaction to allow refinement of the scene, and improve the spatial knowledge base. We demonstrate that user interaction is critical in expanding and improving spatial knowledge learned from data. 2 Background A key insight for enabling text to scene generation is that linguistic and non-linguistic spatial knowledge is criti</context>
<context position="7368" citStr="Fisher et al., 2012" startWordPosition="1177" endWordPosition="1180">as no influence on the system’s knowledge base. We address one part of this issue: learning simple spatial knowledge from data and interactively updating it through user feedback. We also infer unstated implicit constraints thus allowing for more natural text input. 2.2 Automatic Scene Layout Prior work on scene layout has focused largely on room interiors and determining good furniture layouts by optimizing energy functions that capture the quality of a proposed layout. These energy functions are encoded from interior design guidelines (Merrell et al., 2011) or learned from input scene data (Fisher et al., 2012). Knowledge of object co-occurrences and spatial relations is represented by simple models such as mixtures of Gaussians on pairwise object positions and orientations. Methods to learn scene structure have been demonstrated using various data sources including simulation of human agents in 3D scenes (Jiang et al., 2012; Jiang and Saxena, 2013), and analysis of supporting contact points in scanned environments (Rosman and Ramamoorthy, 2011). However, prior work has not explored methods for enabling users of scene generation algorithms to interactively refine and improve an underlying spatial kn</context>
<context position="11918" citStr="Fisher et al., 2012" startWordPosition="1935" endWordPosition="1938">ptimize the placement of the objects to finalize the arrangement of the scene. Interaction and Learning (§8): Provide means for a user to interactively adjust the scene through direct manipulation and textual commands. Use any such interaction to update the system’s spatial knowledge so it better captures the user’s intent. 4 Object Knowledge from 3D Models To generate scenes we need to have a collection of 3D models for representing physical objects. We use a 3D model dataset collected from Google 3D Warehouse by prior work in scene synthesis and containing about 12490 mostly indoor objects (Fisher et al., 2012). These models have text associated with them in the form of names and tags. In addition, we semi-automatically annotated models with object category labels (roughly 270 classes). We used model tags to set these labels, and verified and augmented them manually. In addition, we automatically rescale models so that they have physically plausible sizes and orient them so that they have a consistent up and front direction (Savva et al., 2014). Due to the number of models in the database, not all models were rescaled and re-oriented. We then indexed all models in a database that we query at run-tim</context>
</contexts>
<marker>Fisher, Ritchie, Savva, Funkhouser, Hanrahan, 2012</marker>
<rawString>Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas Funkhouser, and Pat Hanrahan. 2012. Example-based synthesis of 3D object arrangements. ACM Transactions on Graphics (TOG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Fogarty</author>
<author>Desney Tan</author>
<author>Ashish Kapoor</author>
<author>Simon Winder</author>
</authors>
<title>CueFlik: interactive concept learning in image search.</title>
<date>2008</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.</booktitle>
<contexts>
<context position="8517" citStr="Fogarty et al., 2008" startWordPosition="1358" endWordPosition="1361"> algorithms to interactively refine and improve an underlying spatial knowledge model – a capability which is critically important. Our work focuses on demonstrating an interactive system which allows a user to manipulate and refine such spatial knowledge. Such a system is useful regardless of the algorithm used to get the input spatial knowledge. 2.3 Interactive Learning In many tasks, user interaction can provide feedback to an automated system and guide it towards a desired goal. There is much prior work in various domains including interactive systems for refining image search algorithms (Fogarty et al., 2008) and for manipulating social network group creation (Amershi et al., 2012). We focus on the domain of text to 3D scene generation where despite the success of data-driven methods there has been little work on interactive learning systems. 15 3 Approach Overview What should an interactive text to scene system look like from the perspective of a user? The user should be able to provide a brief scene description in natural language as input. The system parses this text to a set of explicitly provided constraints on what objects should be present, and how they are arranged. This set of constraints</context>
</contexts>
<marker>Fogarty, Tan, Kapoor, Winder, 2008</marker>
<rawString>James Fogarty, Desney Tan, Ashish Kapoor, and Simon Winder. 2008. CueFlik: interactive concept learning in image search. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun Jiang</author>
<author>Ashutosh Saxena</author>
</authors>
<title>Infinite latent conditional random fields for modeling environments through humans.</title>
<date>2013</date>
<contexts>
<context position="7713" citStr="Jiang and Saxena, 2013" startWordPosition="1233" endWordPosition="1236">n room interiors and determining good furniture layouts by optimizing energy functions that capture the quality of a proposed layout. These energy functions are encoded from interior design guidelines (Merrell et al., 2011) or learned from input scene data (Fisher et al., 2012). Knowledge of object co-occurrences and spatial relations is represented by simple models such as mixtures of Gaussians on pairwise object positions and orientations. Methods to learn scene structure have been demonstrated using various data sources including simulation of human agents in 3D scenes (Jiang et al., 2012; Jiang and Saxena, 2013), and analysis of supporting contact points in scanned environments (Rosman and Ramamoorthy, 2011). However, prior work has not explored methods for enabling users of scene generation algorithms to interactively refine and improve an underlying spatial knowledge model – a capability which is critically important. Our work focuses on demonstrating an interactive system which allows a user to manipulate and refine such spatial knowledge. Such a system is useful regardless of the algorithm used to get the input spatial knowledge. 2.3 Interactive Learning In many tasks, user interaction can provid</context>
</contexts>
<marker>Jiang, Saxena, 2013</marker>
<rawString>Yun Jiang and Ashutosh Saxena. 2013. Infinite latent conditional random fields for modeling environments through humans.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun Jiang</author>
<author>Marcus Lim</author>
<author>Ashutosh Saxena</author>
</authors>
<title>Learning object arrangements in 3D scenes using human context.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine Learning (ICML-12).</booktitle>
<contexts>
<context position="7688" citStr="Jiang et al., 2012" startWordPosition="1229" endWordPosition="1232">as focused largely on room interiors and determining good furniture layouts by optimizing energy functions that capture the quality of a proposed layout. These energy functions are encoded from interior design guidelines (Merrell et al., 2011) or learned from input scene data (Fisher et al., 2012). Knowledge of object co-occurrences and spatial relations is represented by simple models such as mixtures of Gaussians on pairwise object positions and orientations. Methods to learn scene structure have been demonstrated using various data sources including simulation of human agents in 3D scenes (Jiang et al., 2012; Jiang and Saxena, 2013), and analysis of supporting contact points in scanned environments (Rosman and Ramamoorthy, 2011). However, prior work has not explored methods for enabling users of scene generation algorithms to interactively refine and improve an underlying spatial knowledge model – a capability which is critically important. Our work focuses on demonstrating an interactive system which allows a user to manipulate and refine such spatial knowledge. Such a system is useful regardless of the algorithm used to get the input spatial knowledge. 2.3 Interactive Learning In many tasks, us</context>
</contexts>
<marker>Jiang, Lim, Saxena, 2012</marker>
<rawString>Yun Jiang, Marcus Lim, and Ashutosh Saxena. 2012. Learning object arrangements in 3D scenes using human context. In Proceedings of the 29th International Conference on Machine Learning (ICML-12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan D Kalvin</author>
<author>Russell H Taylor</author>
</authors>
<title>Superfaces: Polygonal mesh simplification with bounded error. Computer Graphics and Applications,</title>
<date>1996</date>
<publisher>IEEE.</publisher>
<contexts>
<context position="13926" citStr="Kalvin and Taylor, 1996" startWordPosition="2272" endWordPosition="2275">isting scenes to establish a prior over what objects go on top of what other objects. As an example, by observing plates and forks on tables most of the time, we establish that tables are more likely to support plates and forks than chairs. We estimate the probability of a parent category Cp supporting a given child category Cc as a simple conditional probability based on normalized observation counts. 5.2 Supporting surfaces To identify which surfaces on parent objects support child objects, we first segment parent models into planar surfaces using a simple region-growing algorithm based on (Kalvin and Taylor, 1996). We characterize support surfaces by the direction of their normal vector limited to the six canonical directions: up, down, left, right, front, back. We then learn a probability of supporting surface normal direction Sn given child object category Cc. For example, posters are typically found on walls so their support normal vectors are in the horizontal directions. Any unobserved child categories are assumed to have Psurf(Sn = up|Cc) = 1 since most things rest on a horizontal surface (e.g. floor). Psurf(Sn|Cc) = count(Cc) count(Cc on surface with Sn) 5.3 Spatial Relations For spatial relatio</context>
</contexts>
<marker>Kalvin, Taylor, 1996</marker>
<rawString>Alan D Kalvin and Russell H Taylor. 1996. Superfaces: Polygonal mesh simplification with bounded error. Computer Graphics and Applications, IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Merrell</author>
</authors>
<title>Eric Schkufza, Zeyang Li, Maneesh Agrawala, and Vladlen Koltun.</title>
<date>2011</date>
<journal>ACM Transactions on Graphics (TOG).</journal>
<marker>Merrell, 2011</marker>
<rawString>Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh Agrawala, and Vladlen Koltun. 2011. Interactive furniture layout using interior design guidelines. In ACM Transactions on Graphics (TOG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: a lexical database for english.</title>
<date>1995</date>
<publisher>CACM.</publisher>
<contexts>
<context position="16866" citStr="Miller, 1995" startWordPosition="2766" endWordPosition="2767">A and B are recognized objects. As an example, given the input “There is a room with a desk and a red chair. The chair is to the left of the desk.” we extract the following objects and spatial relations: Objects: index category attributes keywords 0 room room 1 desk desk 2 chair color:red chair, red Relations: left(chair, desk) The input text is processed using the Stanford CoreNLP pipeline1. We use the Stanford coreference system to determine when the same object is being referred to. To identify objects, we look for noun phrases and use the head word as the category, filtering with WordNet (Miller, 1995) to determine which objects are visualizable (under the 1http://nlp.stanford.edu/software/corenlp.shtml Psupport(Cp�Cc) = count(Cc) count(Cc on Cp) inside(A,B) outside(A,B) left(A,B) right(A,B) near(A,B) 17 tag:VBN=verb &gt;nsubjpass =nsubj &gt;prep (=prep &gt;pobj =pobj) tag:VB=verb &gt;dobj =dobj &gt;prep (=prep &gt;pobj =pobj) The chair[nsubj] is made[verb] of[prep] wood[pobj] Put[verb] the cup[dobj] on[prep] the table[pobj] Example Text Dependency Pattern Table 2: Example dependency patterns for extracting spatial relations. Figure 2: Generated scene for “There is a room with a desk and a lamp. There is a c</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G.A. Miller. 1995. WordNet: a lexical database for english. CACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Rosman</author>
<author>Subramanian Ramamoorthy</author>
</authors>
<title>Learning spatial relationships between objects.</title>
<date>2011</date>
<journal>The International Journal of Robotics Research.</journal>
<contexts>
<context position="7811" citStr="Rosman and Ramamoorthy, 2011" startWordPosition="1247" endWordPosition="1250">capture the quality of a proposed layout. These energy functions are encoded from interior design guidelines (Merrell et al., 2011) or learned from input scene data (Fisher et al., 2012). Knowledge of object co-occurrences and spatial relations is represented by simple models such as mixtures of Gaussians on pairwise object positions and orientations. Methods to learn scene structure have been demonstrated using various data sources including simulation of human agents in 3D scenes (Jiang et al., 2012; Jiang and Saxena, 2013), and analysis of supporting contact points in scanned environments (Rosman and Ramamoorthy, 2011). However, prior work has not explored methods for enabling users of scene generation algorithms to interactively refine and improve an underlying spatial knowledge model – a capability which is critically important. Our work focuses on demonstrating an interactive system which allows a user to manipulate and refine such spatial knowledge. Such a system is useful regardless of the algorithm used to get the input spatial knowledge. 2.3 Interactive Learning In many tasks, user interaction can provide feedback to an automated system and guide it towards a desired goal. There is much prior work in</context>
</contexts>
<marker>Rosman, Ramamoorthy, 2011</marker>
<rawString>Benjamin Rosman and Subramanian Ramamoorthy. 2011. Learning spatial relationships between objects. The International Journal of Robotics Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manolis Savva</author>
<author>Angel X Chang</author>
<author>Gilbert Bernstein</author>
<author>Christopher D Manning</author>
<author>Pat Hanrahan</author>
</authors>
<title>On being the right scale: Sizing large collections of 3D models.</title>
<date>2014</date>
<tech>Technical Report CSTR 2014-03.</tech>
<institution>Stanford University</institution>
<contexts>
<context position="12360" citStr="Savva et al., 2014" startWordPosition="2006" endWordPosition="2009">cal objects. We use a 3D model dataset collected from Google 3D Warehouse by prior work in scene synthesis and containing about 12490 mostly indoor objects (Fisher et al., 2012). These models have text associated with them in the form of names and tags. In addition, we semi-automatically annotated models with object category labels (roughly 270 classes). We used model tags to set these labels, and verified and augmented them manually. In addition, we automatically rescale models so that they have physically plausible sizes and orient them so that they have a consistent up and front direction (Savva et al., 2014). Due to the number of models in the database, not all models were rescaled and re-oriented. We then indexed all models in a database that we query at run-time for retrieval based on category and tag labels. 5 Spatial Knowledge Here we describe how we learn spatial knowledge from existing scene data. We base our approach on that of (Fisher et al., 2012) and use their dataset Identify objects and relationships Objects: CAKE, TABLE ON(CAKE, TABLE) Objects: PLATE, FORK ON(FORK, TABLE) ON(PLATE, TABLE) ON(CAKE, PLATE) INTERACTION Create Scene Identify missing objects CONSTRAINT PARSING “There is a</context>
</contexts>
<marker>Savva, Chang, Bernstein, Manning, Hanrahan, 2014</marker>
<rawString>Manolis Savva, Angel X. Chang, Gilbert Bernstein, Christopher D. Manning, and Pat Hanrahan. 2014. On being the right scale: Sizing large collections of 3D models. Stanford University Technical Report CSTR 2014-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee M Seversky</author>
<author>Lijun Yin</author>
</authors>
<title>Real-time automatic 3D scene generation from natural language voice and text descriptions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 14th annual ACM international conference on Multimedia.</booktitle>
<contexts>
<context position="6212" citStr="Seversky and Yin, 2006" startWordPosition="989" endWordPosition="993">nowledge, an idea that we illustrate here in the domain of spatial relations. Early work on the PUT system (Clay and Wilhelms,1996) and the SHRDLU system (Winograd, 1972) gives a good formalization of the interactive linguistic manipulation of objects in 3D scenes. Recently, there has been promising work on generating 2D clipart for sentences using probabilistic models with placement priors learned from data (Zitnick et al., 2013). 2.1 Text to Scene Systems Prior work on text to 3D scene generation has resulted in systems such as WordsEye (Coyne and Sproat, 2001) and other similar approaches (Seversky and Yin, 2006). These systems are typically not designed to be fully interactive and do not leverage user interaction to improve their results. Furthermore, they mostly rely on manual annotation of 3D models and on hand crafted rules to map text to object placement decisions, which makes them hard to extend and generalize. More recent work has used crowdsourcing platforms, such as Amazon Mechanical Turk, to collect necessary annotations (Coyne et al., 2012). However, this data collection is treated as a separate pre-process and the user still has no influence on the system’s knowledge base. We address one p</context>
</contexts>
<marker>Seversky, Yin, 2006</marker>
<rawString>Lee M Seversky and Lijun Yin. 2006. Real-time automatic 3D scene generation from natural language voice and text descriptions. In Proceedings of the 14th annual ACM international conference on Multimedia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Winograd</author>
</authors>
<title>Understanding natural language. Cognitive psychology.</title>
<date>1972</date>
<contexts>
<context position="2877" citStr="Winograd, 1972" startWordPosition="446" endWordPosition="447">in critically important spatial knowledge. For example, people do not usually mention the presence of the ground or that most objects are supported by it. As a consequence, spatial knowledge is severely lacking in current computational systems. Pioneering work in mapping text to 3D scene representations has taken two approaches to address these challenges. First, by restricting the discourse domain to a micro-world with simple geometric shapes, the SHRDLU system demonstrated parsing of natural language input for manipulating the scene, and learning of procedural knowledge through interaction (Winograd, 1972). However, generalization to scenes with more complex objects and spatial relations is very hard to attain. More recently, the WordsEye system has focused on the general text to 3D scene generation task (Coyne and Sproat, 2001), allowing a user to generate a 3D scene directly from a textual description of the objects present, their properties and their spatial arrangement. The authors of WordsEye demonstrated the promise of text to scene generation systems but also pointed out some fundamental issues which restrict the success of their system: a lot of spatial knowledge is required which is ha</context>
<context position="5759" citStr="Winograd, 1972" startWordPosition="917" endWordPosition="918">dge base. We demonstrate that user interaction is critical in expanding and improving spatial knowledge learned from data. 2 Background A key insight for enabling text to scene generation is that linguistic and non-linguistic spatial knowledge is critical for this task and can be learned directly from data representing the physical world and from interactions of people with such data. User feedback allows us to interactively update spatial knowledge, an idea that we illustrate here in the domain of spatial relations. Early work on the PUT system (Clay and Wilhelms,1996) and the SHRDLU system (Winograd, 1972) gives a good formalization of the interactive linguistic manipulation of objects in 3D scenes. Recently, there has been promising work on generating 2D clipart for sentences using probabilistic models with placement priors learned from data (Zitnick et al., 2013). 2.1 Text to Scene Systems Prior work on text to 3D scene generation has resulted in systems such as WordsEye (Coyne and Sproat, 2001) and other similar approaches (Seversky and Yin, 2006). These systems are typically not designed to be fully interactive and do not leverage user interaction to improve their results. Furthermore, they</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Terry Winograd. 1972. Understanding natural language. Cognitive psychology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lawrence Zitnick</author>
<author>Devi Parikh</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Learning the visual interpretation of sentences.</title>
<date>2013</date>
<booktitle>In IEEE Intenational Conference on Computer Vision (ICCV).</booktitle>
<contexts>
<context position="6023" citStr="Zitnick et al., 2013" startWordPosition="956" endWordPosition="959">r this task and can be learned directly from data representing the physical world and from interactions of people with such data. User feedback allows us to interactively update spatial knowledge, an idea that we illustrate here in the domain of spatial relations. Early work on the PUT system (Clay and Wilhelms,1996) and the SHRDLU system (Winograd, 1972) gives a good formalization of the interactive linguistic manipulation of objects in 3D scenes. Recently, there has been promising work on generating 2D clipart for sentences using probabilistic models with placement priors learned from data (Zitnick et al., 2013). 2.1 Text to Scene Systems Prior work on text to 3D scene generation has resulted in systems such as WordsEye (Coyne and Sproat, 2001) and other similar approaches (Seversky and Yin, 2006). These systems are typically not designed to be fully interactive and do not leverage user interaction to improve their results. Furthermore, they mostly rely on manual annotation of 3D models and on hand crafted rules to map text to object placement decisions, which makes them hard to extend and generalize. More recent work has used crowdsourcing platforms, such as Amazon Mechanical Turk, to collect necess</context>
</contexts>
<marker>Zitnick, Parikh, Vanderwende, 2013</marker>
<rawString>C Lawrence Zitnick, Devi Parikh, and Lucy Vanderwende. 2013. Learning the visual interpretation of sentences. In IEEE Intenational Conference on Computer Vision (ICCV).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
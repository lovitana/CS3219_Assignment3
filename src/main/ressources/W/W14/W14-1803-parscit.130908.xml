<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007853">
<title confidence="0.954222">
Automatic detection of plagiarized spoken responses
</title>
<author confidence="0.862819">
Keelan Evanini and Xinhao Wang
</author>
<affiliation confidence="0.807078">
Educational Testing Service
</affiliation>
<address confidence="0.589229">
660 Rosedale Road, Princeton, NJ, USA
</address>
<email confidence="0.989805">
{kevanini,xwang002}@ets.org
</email>
<sectionHeader confidence="0.99728" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976272727273">
This paper addresses the task of auto-
matically detecting plagiarized responses
in the context of a test of spoken En-
glish proficiency for non-native speakers.
A corpus of spoken responses containing
plagiarized content was collected from a
high-stakes assessment of English profi-
ciency for non-native speakers, and sev-
eral text-to-text similarity metrics were
implemented to compare these responses
to a set of materials that were identified
as likely sources for the plagiarized con-
tent. Finally, a classifier was trained using
these similarity metrics to predict whether
a given spoken response is plagiarized or
not. The classifier was evaluated on a
data set containing the responses with pla-
giarized content and non-plagiarized con-
trol responses and achieved accuracies of
92.0% using transcriptions and 87.1% us-
ing ASR output (with a baseline accuracy
of 50.0%).
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999989636363636">
The automated detection of plagiarism has been
widely studied in the domain of written student
essays, and several online services exist for this
purpose.1 In addition, there has been a series of
shared tasks using common data sets of written
language to compare the performance of a vari-
ety of approaches to plagiarism detection (Potthast
et al., 2013). In contrast, the automated detection
of plagiarized spoken responses has received little
attention from both the NLP and assessment com-
munities, mostly due to the limited application of
</bodyText>
<footnote confidence="0.9931208">
1For example, http://turnitin.com/en_
us/features/originalitycheck, http:
//www.grammarly.com/plagiarism-checker/,
and http://www.paperrater.com/plagiarism_
checker.
</footnote>
<bodyText confidence="0.998232205128205">
automated speech scoring for the types of spo-
ken responses that could be affected by plagiarism.
Due to a variety of factors, though, this is likely to
change in the near future, and the automated detec-
tion of plagiarism in spoken language will become
an increasingly important application.
First of all, English continues its spread as the
global language of education and commerce, and
there is a need to assess the communicative com-
pentance of high volumes of highly proficient non-
native speakers. In order to provide a valid evalua-
tion of the complex linguistic skills that are nec-
essary for these speakers, the assessment must
contain test items that elicit spontaneous speech,
such as the Independent and Integrated Speaking
items in the TOEFL iBT test (ETS, 2012), the
Retell Lecture item in the Pearson Test of English
Academic (Longman, 2010), and the oral inter-
view in the IELTS Academic assessment (Cullen
et al., 2014). However, with the increased em-
phasis on complex linguistic skills in assessments
of non-native speech, there is an increased chance
that test takers will prepare canned answers using
test preparation materials prior to the examination.
Therefore, research should also be conducted on
detecting spoken plagiarized responses in order to
prevent this type of cheating strategy.
In addition, there will also likely be an increase
in spoken language assessments for native speak-
ers in the K-12 domain in the near future. Curricu-
lum developers and assessment designers are rec-
ognizing that the assessment of spoken commu-
nication skills is important for determining a stu-
dent’s college readiness. For example, the Com-
mon Core State Standards include Speaking &amp;
Listening English Language Arts standards for
each grade that pertain to a student’s ability to
communicate information and ideas using spoken
language.2 In order to assess these standards, it
</bodyText>
<footnote confidence="0.9939995">
2http://www.corestandards.org/
ELA-Literacy/SL/
</footnote>
<page confidence="0.989853">
22
</page>
<bodyText confidence="0.932232157894737">
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 22–27,
Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics
will be necessary to develop standardized assess-
ments for the K-12 domain that contain items elic-
iting spontaneous speech from the student, such as
presentations, group discussions, etc. Again, with
the introduction of these types of tasks, there is a
risk that a test taker’s spoken response will contain
prepared material drawn from an external source,
and there will be a need to automatically detect
this type of plagiarism on a large scale, in order to
provide fair and valid assessments.
In this paper, we present an initial study of au-
tomated plagiarism detection on spoken responses
containing spontaneous non-native speech. A data
set of actual plagiarized responses was collected,
and text-to-text similarity metrics were applied to
the task of classifying responses as plagiarized or
non-plagiarized.
</bodyText>
<sectionHeader confidence="0.997685" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.98144387804878">
A wide variety of techniques have been employed
in previous studies for the task of detecting plagia-
rized written documents, including n-gram over-
lap (Lyon et al., 2006), document fingerprinting
(Brin et al., 1995), word frequency statistics (Shiv-
akumar and Garcia-Molina, 1995), Information
Retrieval-based metrics (Hoad and Zobel, 2003),
text summarization evaluation metrics (Chen et
al., 2010), WordNet-based features (Nahnsen et
al., 2005), and features based on shared syntactic
patterns (Uzuner et al., 2005). This task is also
related to the widely studied task of paraphrase
recognition, which benefits from similar types of
features (Finch et al., 2005; Madnani et al., 2012).
The current study adopts several of these features
that are designed to be robust to the presence of
word-level modifications between the source and
the plagiarized text; since this study focuses on
spoken responses that are reproduced from mem-
ory and subsequently processed by a speech recog-
nizer, metrics that rely on exact matches are likely
to perform sub-optimally. To our knowledge, no
previous work has been reported on automatically
detecting similar spoken documents, although re-
search in the field of Spoken Document Retrieval
(Haputmann, 2006) is relevant.
Due to the difficulties involved in collecting cor-
pora of actual plagiarized material, nearly all pub-
lished results of approaches to the task of plagia-
rism detection have relied on either simulated pla-
giarism (i.e., plagiarized texts generated by experi-
mental human participants in a controlled environ-
ment) or artificial plagiarism (i.e., plagiarized texts
generated by algorithmically modifying a source
text) (Potthast et al., 2010). These results, how-
ever, may not reflect actual performance in a de-
ployed setting, since the characteristics of the pla-
giarized material may differ from actual plagia-
rized responses. To overcome this limitation, the
current study is based on a set of actual plagiarized
responses drawn from a large-scale assessment.
</bodyText>
<sectionHeader confidence="0.99621" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.99994615">
The data used in this study was drawn from the
TOEFL® Internet-based test (TOEFL® iBT), a
large-scale, high-stakes assessment of English for
non-native speakers, which assesses English com-
munication skills for academic purposes. The
Speaking section of TOEFL iBT contains six
tasks, each of which requires the test taker to pro-
vide an extended response containing spontaneous
speech. Two of the tasks are referred to as In-
dependent tasks; these tasks cover topics that are
familiar to test takers and ask test takers to draw
upon their own ideas, opinions, and experiences in
a 45-second spoken response (ETS, 2012). Since
these two Independent tasks ask questions that are
not based on any stimulus materials that were pro-
vided to the test taker (such as a reading passage,
figure, etc.), the test takers can provide responses
that contain a wide variety of specific examples.
In some cases, test takers may attempt to game
the assessment by memorizing canned material
from an external source and adapting it to a ques-
tion that is asked in one of the Independent tasks.
This type of plagiarism can affect the validity of
a test taker’s speaking score; however, it is often
difficult even for trained human raters to recog-
nize plagiarized spoken responses, due to the large
number and variety of external sources that are
available from online test preparation sites.
In order to better understand the strategies used
by test takers who incorporated material from ex-
ternal sources into their spoken responses and to
develop a capability for automated plagiarism de-
tection for speaking items, a data set of opera-
tional spoken responses containing potentially pla-
giarized material was collected. This data set con-
tains responses that were flagged by human raters
as potentially containing plagiarized material and
then subsequently reviewed by rater supervisors.
In the review process, the responses were tran-
scribed and compared to external source materi-
</bodyText>
<page confidence="0.988175">
23
</page>
<bodyText confidence="0.999964568181818">
als obtained through manual internet searches; if
it was determined that the presence of plagiarized
material made it impossible to provide a valid as-
sessment of the test taker’s performance on the
task, the response was assigned a score of 0. This
study investigates a set of 719 responses that were
flagged as potentially plagiarized between Octo-
ber 2010 and December 2011; in this set, 239 re-
sponses were assigned a score of 0 due to the pres-
ence of a significant amount of plagiarized con-
tent from an identified source. This set of 239 re-
sponses is used in the experiments described be-
low.
During the process of reviewing potentially pla-
giarized responses, the raters also collected a data
set of external sources that appeared to have been
used by test takers in their responses. In some
cases, the test taker’s spoken response was nearly
identical to an identified source; in other cases,
several sentences or phrases were clearly drawn
from a particular source, although some modifi-
cations were apparent. Table 1 presents a sample
source that was identified for several of the 239 re-
sponses in the data set.3 Many of the plagiarized
responses contained extended sequences of words
that directly match idiosyncratic features of this
source, such as the phrases “how romantic it can
ever be” and “just relax yourself on the beach.”
In total, 49 different source materials were iden-
tified for all of the potentially plagiarized re-
sponses in the corpus.4 In addition to the source
materials and the plagiarized responses, a set of
non-plagiarized control responses was also ob-
tained in order to conduct classification experi-
ments between plagiarized and non-plagiarized re-
sponses. Since the plagiarized responses were
collected over the course of more than one year,
they were drawn from many different TOEFL iBT
test forms; in total, the 239 plagiarized responses
comprise 103 distinct Independent test questions.
Therefore, it was not practical to obtain control
data from all of the test items that were represented
in the plagiarized set; rather, approximately 300
responses were extracted from each of the four test
</bodyText>
<footnote confidence="0.715310555555556">
3This source is available from several online test prepara-
tion websites, for example http://www.mhdenglish.
com/eoenglish_article_view_1195.html.
4A total of 39 sources were identified for the set of 239
responses in the Plagiarized set; however, all 49 identified
sources were used in the experiments in order to make the
experimental design more similar to an operational set-up in
which the exact set of source texts that will be represented in
a given set of plagiarized responses is not known.
</footnote>
<bodyText confidence="0.999094882352941">
Well, the place I enjoy the most is a small
town located in France. I like this small town
because it has very charming ocean view. I
mean the sky there is so blue and the beach
is always full of sunshine. You know how
romantic it can ever be, just relax yourself
on the beach, when the sun is setting down,
when the ocean breeze is blowing and the
seabirds are singing. Of course I like this
small French town also because there are
many great French restaurants. They offer
the best seafood in the world like lobsters and
tuna fishes. The most important, I have been
benefited a lot from this trip to France because
I made friends with some gorgeous French
girls. One of them even gave me a little watch
as a souvenir of our friendship.
</bodyText>
<tableCaption confidence="0.94221">
Table 1: Sample source passage used in plagia-
rized responses
</tableCaption>
<bodyText confidence="0.994944">
items that were most frequently represented in the
set of plagiarized responses. Table 2 provides a
summary of the three data sets used in the study,
along with summary statistics about the length of
the responses in each set.
</bodyText>
<table confidence="0.9998698">
Data Set N Number of Words
Mean Std. Dev.
Sources 49 122.5 36.5
Plagiarized 239 109.1 18.9
Control 1196 84.9 24.1
</table>
<tableCaption confidence="0.999478">
Table 2: Summary of the data sets
</tableCaption>
<bodyText confidence="0.999989125">
As Table 2 shows, the plagiarized responses
are on average a little longer than the control re-
sponses. This is likely due to the fact that the pla-
giarized responses contain a large percentage of
memorized material, which the test takers are able
to produce using a fast rate of speech, since they
had likely rehearsed the content several times be-
fore taking the assessment.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="method">
4 Methodology
</sectionHeader>
<bodyText confidence="0.999318666666667">
The general approach taken in this study for deter-
mining whether a spoken response is plagiarized
or not was to compare its content to the content of
each of the source materials that had been iden-
tified for the responses in this corpus. Given a
test response, a comparison was made with each
</bodyText>
<page confidence="0.994041">
24
</page>
<bodyText confidence="0.999994404255319">
of the 49 reference sources using the following 9
text-to-text similarity metrics: 1) Word Error Rate
(WER), or edit distance between the response and
the source; 2) TER, similar to WER, but allowing
shifts of words within the text at a low edit cost
(Snover et al., 2006); 3) TER-Plus, an extension of
TER that includes matching based on paraphrases,
stemming, and synonym substitution (Snover et
al., 2008); 4) a WordNet similarity metric based on
presence in the same synset;5 5) a WordNet sim-
ilarity metric based on the shortest path between
two words in the is-a taxonomy; 6) a WordNet
similarity metric similar to (5) that also takes into
account the maximum depth of the taxonomy in
which the words occur (Leacock and Chodorow,
1998); 7) a WordNet similarity metric based on the
depth of the Least Common Subsumer of the two
words (Wu and Palmer, 1994); 8) Latent Semantic
Analysis, using a model trained on the British Na-
tional Corpus (BNC, 2007); 9) BLEU (Papineni et
al., 2002). Most of these similarity metrics (with
the exception of WER and TER) are expected to
be robust to modifications between the source text
and the plagiarized response, since they do not rely
on exact string matches.
Each similarity metric was used to compute 4
different features comparing the test response to
each of the 49 source texts: 1) the document-level
similarity between the test response and the source
text; 2) the single maximum similarity value from
a sentence-by-sentence comparison between the
test response and the source text; 3) the average of
the similarity values for all sentence-by-sentence
comparisons between the test response and the
source text; 4) the average of the maximum simi-
larity values for each sentence in the test response,
where the maximum similarity of a sentence is ob-
tained by comparing it with each sentence in the
source text. The intuition behind using the fea-
tures that compare sentence-to-sentence similarity
as opposed to only the document-level similarity
feature is that test responses may contain a combi-
nation of both passages that were memorized from
a source text and novel content. Depending on the
amount of the response that was plagiarized, these
types of responses may also receive a score of 0;
so, in order to also detect these responses as pla-
</bodyText>
<footnote confidence="0.7474168">
5For the WordNet-based similarity metrics, the similarity
scores for pairs of words were combined to obtain document-
and sentence-level similarity scores by taking the average
maximum pairwise similarity values, similar to the sentence-
level similarity feature defined in (4) below.
</footnote>
<bodyText confidence="0.9995101">
giarized, a sentence-by-sentence comparison ap-
proach may be more effective.
The experiments described below were con-
ducted using both human transcriptions of the spo-
ken responses as well as the output from an au-
tomated speech recognition (ASR) system. The
ASR system was trained on approximately 800
hours of TOEFL iBT responses; the system’s
WER on the data used in this study was 0.411 for
the Plagiarized set and 0.362 for the Control set.
Since the ASR output does not contain sentence
boundaries, these were obtained using a Maxi-
mum Entropy sentence boundary detection system
based on lexical features (Chen and Yoon, 2011).
Before calculating the similarity features, all of the
texts were preprocessed to normalize case, seg-
ment the text into sentences, and remove disfluen-
cies, including filled pauses (such as uh and um)
and repeated words. No stemming was performed
on the words in the texts for this study.
</bodyText>
<sectionHeader confidence="0.999955" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999992952380952">
As described in Section 4, 36 similarity features
were calculated between each spoken response
and each of the 49 source texts. In order to exam-
ine the performance of these features in discrim-
inating between plagiarized and non-plagiarized
responses, classification experiments were con-
ducted on balanced sets of Plagiarized and Con-
trol responses, and the results were averaged using
1000 random subsets of 239 responses from the
Control set.6 In addition, the following different
feature sets were compared: All (all 36 features),
Doc (the 9 document-level features), and Sent (the
27 features based on sentence-level comparisons).
The J48 decision tree model from the Weka toolkit
(with the default parameter settings) was used
for classification, and 10-fold cross-validation was
performed using both transcriptions and ASR out-
put. Table 3 presents the results of these experi-
ments, including the means (and standard devia-
tions) of the accuracy and kappa (κ) values (for all
experiments, the baseline accuracy is 50%).
</bodyText>
<sectionHeader confidence="0.997295" genericHeader="conclusions">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999886">
As Table 3 shows, the classifier achieved a higher
accuracy when using the 9 document-level simi-
larity features compared to using the 27 sentence-
</bodyText>
<footnote confidence="0.966577333333333">
6Experiments were also conducted using the full Control
set, and the results showed a similar relative performance of
the feature sets.
</footnote>
<page confidence="0.994488">
25
</page>
<table confidence="0.999742857142857">
Text Features Accuracy κ
Trans. All 0.903 (0.01) 0.807 (0.02)
Doc 0.920 (0.01) 0.839 (0.02)
Sent 0.847 (0.01) 0.693 (0.03)
ASR All 0.852 (0.02) 0.703 (0.03)
Doc 0.871 (0.01) 0.742 (0.03)
Sent 0.735 (0.02) 0.470 (0.04)
</table>
<tableCaption confidence="0.998934">
Table 3: Mean Accuracy and κ values (and stan-
</tableCaption>
<bodyText confidence="0.983940532608696">
dard deviations) for classification results using the
239 responses in the Plagiarized set and 1000 ran-
dom subsets of 239 responses from the Control set
level similarity features. In addition, the combined
set of 36 features resulted in a slightly lower per-
formance than when only the 9 document-level
features were used. This suggests that the sentence
level features are not as robust as the document-
level features, probably due to the increased like-
lihood of chance similarities between sentences in
the response and a source text. Despite the fact
that the plagiarized spoken responses in this data
set may contain some original content (in particu-
lar, introductory material provided by the test taker
in an attempt to make the plagiarized content seem
more relevant to the specific test question), it ap-
pears that the document-level features are most ef-
fective. Table 3 also indicates that the performance
of the classifier decreases by approximately 5% -
10% when ASR output is used. This indicates that
the similarity metrics are reasonably robust to the
presence of speech recognition errors in the text,
and that the approach is viable in an operational
setting in which transcriptions of the spoken re-
sponses are not available.
A more detailed error analysis indicates that the
precision of the classifier, with respect to the Pla-
giarized class, is higher than the recall: on the
transcriptions, the average precision using the Doc
features was 0.948 (s.d.= 0.01), whereas the av-
erage recall was 0.888 (s.d.=0.01); for the ASR
set, the average precision was 0.904 (s.d.=0.02),
whereas the average recall was 0.831 (s.d.=0.02).
This means that the rate of false positives pro-
duced by this classifier is somewhat lower than the
rate of false negatives. In an operational scenario,
an automated plagiarized spoken response detec-
tion system such as this one would likely be de-
ployed in tandem with human raters to review the
results and provide a final decision about whether
a given spoken response was plagiarized or not. In
that case, it may be desirable to tune the classi-
fier parameters to increase the recall so that fewer
cases of plagiarism would go undetected, assum-
ing that there are suffient human reviewers avail-
able to process the increased number of false pos-
itives that would result from this approach. Im-
proving the classifier’s recall is also important for
practical applications of this approach, since the
distribution of actual responses is heavily imbal-
anced in favor of the non-plagiarized class. The
current set of experiments only used a relatively
small Control set of 1196 responses for which
transcriptions could be obtained in a cost effective
manner in order to be able to compare the system’s
performance using transcriptions and ASR output.
Since there was only a minor degradation in per-
formance when ASR output was used, future ex-
periments will be conducted using a much larger
Control set in order to approximate the distribution
of categories that would be observed in practice.
One drawback of the method described in this
study is that it requires matching source texts in
order to detect a plagiarized spoken response. This
means that plagiarized spoken responses based on
a given source text will not be detected by the
system until the appropriate source text has been
identified, thus limiting the system’s recall. Be-
sides attempting to obtain additional source texts
(either manually, as was done for this study, or by
automated means), this could also be addressed
by comparing a test response to all previously
collected spoken responses for a given popula-
tion of test takers in order to flag pairs of sim-
ilar responses. While this method would likely
produce a high number of false positives when
the ASR output was used, due to chance simi-
larities between two responses in a large pool of
test taker responses resulting from imperfect ASR,
performance could be improved by considering
additional information from the speech recognizer
when computing the similarity metrics, such as
the N-best list. Additional sources of informa-
tion that could be used for detecting plagiarized re-
sponses include stylistic patterns and prosodic fea-
tures; for example, spoken responses that are re-
produced from memory likely contain fewer filled
pauses and have a faster rate of speech than non-
plagiarized responses; these types of non-lexical
features should also be investigated in future re-
search into the detection of plagiarized spoken re-
sponses.
</bodyText>
<page confidence="0.994178">
26
</page>
<sectionHeader confidence="0.999207" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99991">
We would like to thank Beata Beigman Klebanov,
Dan Blanchard, Nitin Madnani, and three anony-
mous BEA-9 reviewers for their helpful com-
ments.
</bodyText>
<sectionHeader confidence="0.997793" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995867377551021">
BNC. 2007. The British National Corpus, version 3.
Distributed by Oxford University Computing Ser-
vices on behalf of the BNC Consortium, http:
//www.natcorp.ox.ac.uk/.
Sergey Brin, James Davis, and Hector Garcia-Molina.
1995. Copy detection mechanisms for digital docu-
ments. In Proceedings of the ACM SIGMOD Annual
Conference, pages 398–409.
Lei Chen and Su-Youn Yoon. 2011. Detecting
structural events for assessing non-native speech.
In Proceedings of the 6th Workshop on Innovative
Use of NLP for Building Educational Applications,
NAACL-HLT, pages 38–45, Portland, OR. Associa-
tion for Computational Linguistics.
Chien-Ying Chen, Jen-Yuan Yeh, and Hao-Ren Ke.
2010. Plagiarism detection using ROUGE and
WordNet. Journal of Computing, 2(3):34–44.
Pauline Cullen, Amanda French, and Vanessa Jakeman.
2014. The Official Cambridge Guide to IELTS.
Cambridge University Press.
ETS. 2012. The Official Guide to the TOEFL O Test,
Fourth Edition. McGraw-Hill.
Andrew Finch, Young-Sook Hwang, and Eiichiro
Sumita. 2005. Using machine translation evalua-
tion techniques to determine sentence-level seman-
tic equivalence. In Proceedings of the Third Inter-
national Workshop on Paraphrasing, pages 17–24.
Alexander Haputmann. 2006. Automatic spoken doc-
ument retrieval. In Ketih Brown, editor, Encylclope-
dia of Language and Linguistics (Second Edition),
pages 95–103. Elsevier Science.
Timothy C. Hoad and Justin Zobel. 2003. Methods
for identifying versioned and plagiarised documents.
Journal of the American Society for Information Sci-
ence and Technology, 54:203–215.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical Database,
pages 305–332. MIT Press.
Pearson Longman. 2010. The Official Guide to Pear-
son Test of English Academic. Pearson Education
ESL.
Caroline Lyon, Ruth Barrett, and James Malcolm.
2006. Plagiarism is easy, but also easy to detect.
Plagiary, 1:57–65.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 182–
190, Montr´eal, Canada, June. Association for Com-
putational Linguistics.
Thade Nahnsen, ¨Ozlem Uzuner, and Boris Katz. 2005.
Lexical chains and sliding locality windows in
content-based text similarity detection. CSAIL
Technical Report, MIT-CSAIL-TR-2005-034.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.
Martin Potthast, Benno Stein, Alberto Barr´on-Cede˜no,
and Paolo Rosso. 2010. An evaluation frame-
work for plagiarism detection. In Proceedings of the
23rd International Conference on Computational
Linguistics.
Martin Potthast, Matthias Hagen, Tim Gollub, Martin
Tippmann, Johannes Kiesel, Paolo Rosso, Efstathios
Stamatatos, and Benno Stein. 2013. Overview of
the 5th International Competition on Plagiarism De-
tection. In Pamela Forner, Roberto Navigli, and
Dan Tufis, editors, CLEF 2013 Evaluation Labs and
Workshop – Working Notes Papers.
Narayanan Shivakumar and Hector Garcia-Molina.
1995. SCAM: A copy detection mechanism for digi-
tal documents. In Proceedings of the Second Annual
Conference on the Theory and Practice of Digital
Libraries.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223–231.
Matt Snover, Nitin Madnani, Bonnie Dorr, and Richard
Schwartz. 2008. TERp: A system descrip-
tion. In Proceedings of the First NIST Metrics
for Machine Translation Challenge (MetricsMATR),
Waikiki, Hawaii, October.
¨Ozlem Uzuner, Boris Katz, and Thade Nahnsen. 2005.
Using syntactic information to identify plagiarism.
In Proceedings of the 2nd Workshop on Building Ed-
ucational Applications using NLP. Ann Arbor.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In Proceedings of the 32nd
Annual Mmeeting of the Association for Computa-
tional Linguistics (ACL).
</reference>
<page confidence="0.998797">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.695108">
<title confidence="0.999679">Automatic detection of plagiarized spoken responses</title>
<author confidence="0.883256">Keelan Evanini</author>
<author confidence="0.883256">Xinhao</author>
<affiliation confidence="0.815064">Educational Testing</affiliation>
<address confidence="0.935786">660 Rosedale Road, Princeton, NJ,</address>
<abstract confidence="0.99392947826087">This paper addresses the task of automatically detecting plagiarized responses in the context of a test of spoken English proficiency for non-native speakers. A corpus of spoken responses containing plagiarized content was collected from a high-stakes assessment of English proficiency for non-native speakers, and several text-to-text similarity metrics were implemented to compare these responses to a set of materials that were identified as likely sources for the plagiarized content. Finally, a classifier was trained using these similarity metrics to predict whether a given spoken response is plagiarized or not. The classifier was evaluated on a data set containing the responses with plagiarized content and non-plagiarized control responses and achieved accuracies of 92.0% using transcriptions and 87.1% using ASR output (with a baseline accuracy of 50.0%).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>BNC</author>
</authors>
<title>The British National Corpus, version 3. Distributed by</title>
<date>2007</date>
<booktitle>Computing Services on behalf of the BNC Consortium,</booktitle>
<pages>//www.natcorp.ox.ac.uk/.</pages>
<institution>Oxford University</institution>
<location>http:</location>
<contexts>
<context position="14143" citStr="BNC, 2007" startWordPosition="2262" endWordPosition="2263">hrases, stemming, and synonym substitution (Snover et al., 2008); 4) a WordNet similarity metric based on presence in the same synset;5 5) a WordNet similarity metric based on the shortest path between two words in the is-a taxonomy; 6) a WordNet similarity metric similar to (5) that also takes into account the maximum depth of the taxonomy in which the words occur (Leacock and Chodorow, 1998); 7) a WordNet similarity metric based on the depth of the Least Common Subsumer of the two words (Wu and Palmer, 1994); 8) Latent Semantic Analysis, using a model trained on the British National Corpus (BNC, 2007); 9) BLEU (Papineni et al., 2002). Most of these similarity metrics (with the exception of WER and TER) are expected to be robust to modifications between the source text and the plagiarized response, since they do not rely on exact string matches. Each similarity metric was used to compute 4 different features comparing the test response to each of the 49 source texts: 1) the document-level similarity between the test response and the source text; 2) the single maximum similarity value from a sentence-by-sentence comparison between the test response and the source text; 3) the average of the </context>
</contexts>
<marker>BNC, 2007</marker>
<rawString>BNC. 2007. The British National Corpus, version 3. Distributed by Oxford University Computing Services on behalf of the BNC Consortium, http: //www.natcorp.ox.ac.uk/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>James Davis</author>
<author>Hector Garcia-Molina</author>
</authors>
<title>Copy detection mechanisms for digital documents.</title>
<date>1995</date>
<booktitle>In Proceedings of the ACM SIGMOD Annual Conference,</booktitle>
<pages>398--409</pages>
<contexts>
<context position="4950" citStr="Brin et al., 1995" startWordPosition="743" endWordPosition="746">ism on a large scale, in order to provide fair and valid assessments. In this paper, we present an initial study of automated plagiarism detection on spoken responses containing spontaneous non-native speech. A data set of actual plagiarized responses was collected, and text-to-text similarity metrics were applied to the task of classifying responses as plagiarized or non-plagiarized. 2 Previous Work A wide variety of techniques have been employed in previous studies for the task of detecting plagiarized written documents, including n-gram overlap (Lyon et al., 2006), document fingerprinting (Brin et al., 1995), word frequency statistics (Shivakumar and Garcia-Molina, 1995), Information Retrieval-based metrics (Hoad and Zobel, 2003), text summarization evaluation metrics (Chen et al., 2010), WordNet-based features (Nahnsen et al., 2005), and features based on shared syntactic patterns (Uzuner et al., 2005). This task is also related to the widely studied task of paraphrase recognition, which benefits from similar types of features (Finch et al., 2005; Madnani et al., 2012). The current study adopts several of these features that are designed to be robust to the presence of word-level modifications b</context>
</contexts>
<marker>Brin, Davis, Garcia-Molina, 1995</marker>
<rawString>Sergey Brin, James Davis, and Hector Garcia-Molina. 1995. Copy detection mechanisms for digital documents. In Proceedings of the ACM SIGMOD Annual Conference, pages 398–409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Chen</author>
<author>Su-Youn Yoon</author>
</authors>
<title>Detecting structural events for assessing non-native speech.</title>
<date>2011</date>
<booktitle>In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications, NAACL-HLT,</booktitle>
<pages>38--45</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, OR.</location>
<contexts>
<context position="16390" citStr="Chen and Yoon, 2011" startWordPosition="2623" endWordPosition="2626">, a sentence-by-sentence comparison approach may be more effective. The experiments described below were conducted using both human transcriptions of the spoken responses as well as the output from an automated speech recognition (ASR) system. The ASR system was trained on approximately 800 hours of TOEFL iBT responses; the system’s WER on the data used in this study was 0.411 for the Plagiarized set and 0.362 for the Control set. Since the ASR output does not contain sentence boundaries, these were obtained using a Maximum Entropy sentence boundary detection system based on lexical features (Chen and Yoon, 2011). Before calculating the similarity features, all of the texts were preprocessed to normalize case, segment the text into sentences, and remove disfluencies, including filled pauses (such as uh and um) and repeated words. No stemming was performed on the words in the texts for this study. 5 Results As described in Section 4, 36 similarity features were calculated between each spoken response and each of the 49 source texts. In order to examine the performance of these features in discriminating between plagiarized and non-plagiarized responses, classification experiments were conducted on bala</context>
</contexts>
<marker>Chen, Yoon, 2011</marker>
<rawString>Lei Chen and Su-Youn Yoon. 2011. Detecting structural events for assessing non-native speech. In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications, NAACL-HLT, pages 38–45, Portland, OR. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chien-Ying Chen</author>
<author>Jen-Yuan Yeh</author>
<author>Hao-Ren Ke</author>
</authors>
<title>Plagiarism detection using ROUGE and WordNet.</title>
<date>2010</date>
<journal>Journal of Computing,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="5133" citStr="Chen et al., 2010" startWordPosition="766" endWordPosition="769">aneous non-native speech. A data set of actual plagiarized responses was collected, and text-to-text similarity metrics were applied to the task of classifying responses as plagiarized or non-plagiarized. 2 Previous Work A wide variety of techniques have been employed in previous studies for the task of detecting plagiarized written documents, including n-gram overlap (Lyon et al., 2006), document fingerprinting (Brin et al., 1995), word frequency statistics (Shivakumar and Garcia-Molina, 1995), Information Retrieval-based metrics (Hoad and Zobel, 2003), text summarization evaluation metrics (Chen et al., 2010), WordNet-based features (Nahnsen et al., 2005), and features based on shared syntactic patterns (Uzuner et al., 2005). This task is also related to the widely studied task of paraphrase recognition, which benefits from similar types of features (Finch et al., 2005; Madnani et al., 2012). The current study adopts several of these features that are designed to be robust to the presence of word-level modifications between the source and the plagiarized text; since this study focuses on spoken responses that are reproduced from memory and subsequently processed by a speech recognizer, metrics tha</context>
</contexts>
<marker>Chen, Yeh, Ke, 2010</marker>
<rawString>Chien-Ying Chen, Jen-Yuan Yeh, and Hao-Ren Ke. 2010. Plagiarism detection using ROUGE and WordNet. Journal of Computing, 2(3):34–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pauline Cullen</author>
<author>Amanda French</author>
<author>Vanessa Jakeman</author>
</authors>
<title>The Official Cambridge Guide to IELTS.</title>
<date>2014</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2707" citStr="Cullen et al., 2014" startWordPosition="403" endWordPosition="406">sh continues its spread as the global language of education and commerce, and there is a need to assess the communicative compentance of high volumes of highly proficient nonnative speakers. In order to provide a valid evaluation of the complex linguistic skills that are necessary for these speakers, the assessment must contain test items that elicit spontaneous speech, such as the Independent and Integrated Speaking items in the TOEFL iBT test (ETS, 2012), the Retell Lecture item in the Pearson Test of English Academic (Longman, 2010), and the oral interview in the IELTS Academic assessment (Cullen et al., 2014). However, with the increased emphasis on complex linguistic skills in assessments of non-native speech, there is an increased chance that test takers will prepare canned answers using test preparation materials prior to the examination. Therefore, research should also be conducted on detecting spoken plagiarized responses in order to prevent this type of cheating strategy. In addition, there will also likely be an increase in spoken language assessments for native speakers in the K-12 domain in the near future. Curriculum developers and assessment designers are recognizing that the assessment</context>
</contexts>
<marker>Cullen, French, Jakeman, 2014</marker>
<rawString>Pauline Cullen, Amanda French, and Vanessa Jakeman. 2014. The Official Cambridge Guide to IELTS. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ETS</author>
</authors>
<title>The Official Guide to the TOEFL O Test, Fourth Edition.</title>
<date>2012</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="2547" citStr="ETS, 2012" startWordPosition="378" endWordPosition="379">n the near future, and the automated detection of plagiarism in spoken language will become an increasingly important application. First of all, English continues its spread as the global language of education and commerce, and there is a need to assess the communicative compentance of high volumes of highly proficient nonnative speakers. In order to provide a valid evaluation of the complex linguistic skills that are necessary for these speakers, the assessment must contain test items that elicit spontaneous speech, such as the Independent and Integrated Speaking items in the TOEFL iBT test (ETS, 2012), the Retell Lecture item in the Pearson Test of English Academic (Longman, 2010), and the oral interview in the IELTS Academic assessment (Cullen et al., 2014). However, with the increased emphasis on complex linguistic skills in assessments of non-native speech, there is an increased chance that test takers will prepare canned answers using test preparation materials prior to the examination. Therefore, research should also be conducted on detecting spoken plagiarized responses in order to prevent this type of cheating strategy. In addition, there will also likely be an increase in spoken la</context>
<context position="7366" citStr="ETS, 2012" startWordPosition="1117" endWordPosition="1118">ata used in this study was drawn from the TOEFL® Internet-based test (TOEFL® iBT), a large-scale, high-stakes assessment of English for non-native speakers, which assesses English communication skills for academic purposes. The Speaking section of TOEFL iBT contains six tasks, each of which requires the test taker to provide an extended response containing spontaneous speech. Two of the tasks are referred to as Independent tasks; these tasks cover topics that are familiar to test takers and ask test takers to draw upon their own ideas, opinions, and experiences in a 45-second spoken response (ETS, 2012). Since these two Independent tasks ask questions that are not based on any stimulus materials that were provided to the test taker (such as a reading passage, figure, etc.), the test takers can provide responses that contain a wide variety of specific examples. In some cases, test takers may attempt to game the assessment by memorizing canned material from an external source and adapting it to a question that is asked in one of the Independent tasks. This type of plagiarism can affect the validity of a test taker’s speaking score; however, it is often difficult even for trained human raters t</context>
</contexts>
<marker>ETS, 2012</marker>
<rawString>ETS. 2012. The Official Guide to the TOEFL O Test, Fourth Edition. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Young-Sook Hwang</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Using machine translation evaluation techniques to determine sentence-level semantic equivalence.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Workshop on Paraphrasing,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="5398" citStr="Finch et al., 2005" startWordPosition="807" endWordPosition="810">yed in previous studies for the task of detecting plagiarized written documents, including n-gram overlap (Lyon et al., 2006), document fingerprinting (Brin et al., 1995), word frequency statistics (Shivakumar and Garcia-Molina, 1995), Information Retrieval-based metrics (Hoad and Zobel, 2003), text summarization evaluation metrics (Chen et al., 2010), WordNet-based features (Nahnsen et al., 2005), and features based on shared syntactic patterns (Uzuner et al., 2005). This task is also related to the widely studied task of paraphrase recognition, which benefits from similar types of features (Finch et al., 2005; Madnani et al., 2012). The current study adopts several of these features that are designed to be robust to the presence of word-level modifications between the source and the plagiarized text; since this study focuses on spoken responses that are reproduced from memory and subsequently processed by a speech recognizer, metrics that rely on exact matches are likely to perform sub-optimally. To our knowledge, no previous work has been reported on automatically detecting similar spoken documents, although research in the field of Spoken Document Retrieval (Haputmann, 2006) is relevant. Due to </context>
</contexts>
<marker>Finch, Hwang, Sumita, 2005</marker>
<rawString>Andrew Finch, Young-Sook Hwang, and Eiichiro Sumita. 2005. Using machine translation evaluation techniques to determine sentence-level semantic equivalence. In Proceedings of the Third International Workshop on Paraphrasing, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Haputmann</author>
</authors>
<title>Automatic spoken document retrieval.</title>
<date>2006</date>
<booktitle>Encylclopedia of Language and Linguistics (Second Edition),</booktitle>
<pages>95--103</pages>
<editor>In Ketih Brown, editor,</editor>
<publisher>Elsevier Science.</publisher>
<contexts>
<context position="5977" citStr="Haputmann, 2006" startWordPosition="899" endWordPosition="900">types of features (Finch et al., 2005; Madnani et al., 2012). The current study adopts several of these features that are designed to be robust to the presence of word-level modifications between the source and the plagiarized text; since this study focuses on spoken responses that are reproduced from memory and subsequently processed by a speech recognizer, metrics that rely on exact matches are likely to perform sub-optimally. To our knowledge, no previous work has been reported on automatically detecting similar spoken documents, although research in the field of Spoken Document Retrieval (Haputmann, 2006) is relevant. Due to the difficulties involved in collecting corpora of actual plagiarized material, nearly all published results of approaches to the task of plagiarism detection have relied on either simulated plagiarism (i.e., plagiarized texts generated by experimental human participants in a controlled environment) or artificial plagiarism (i.e., plagiarized texts generated by algorithmically modifying a source text) (Potthast et al., 2010). These results, however, may not reflect actual performance in a deployed setting, since the characteristics of the plagiarized material may differ fr</context>
</contexts>
<marker>Haputmann, 2006</marker>
<rawString>Alexander Haputmann. 2006. Automatic spoken document retrieval. In Ketih Brown, editor, Encylclopedia of Language and Linguistics (Second Edition), pages 95–103. Elsevier Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy C Hoad</author>
<author>Justin Zobel</author>
</authors>
<title>Methods for identifying versioned and plagiarised documents.</title>
<date>2003</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<pages>54--203</pages>
<contexts>
<context position="5074" citStr="Hoad and Zobel, 2003" startWordPosition="758" endWordPosition="761">ated plagiarism detection on spoken responses containing spontaneous non-native speech. A data set of actual plagiarized responses was collected, and text-to-text similarity metrics were applied to the task of classifying responses as plagiarized or non-plagiarized. 2 Previous Work A wide variety of techniques have been employed in previous studies for the task of detecting plagiarized written documents, including n-gram overlap (Lyon et al., 2006), document fingerprinting (Brin et al., 1995), word frequency statistics (Shivakumar and Garcia-Molina, 1995), Information Retrieval-based metrics (Hoad and Zobel, 2003), text summarization evaluation metrics (Chen et al., 2010), WordNet-based features (Nahnsen et al., 2005), and features based on shared syntactic patterns (Uzuner et al., 2005). This task is also related to the widely studied task of paraphrase recognition, which benefits from similar types of features (Finch et al., 2005; Madnani et al., 2012). The current study adopts several of these features that are designed to be robust to the presence of word-level modifications between the source and the plagiarized text; since this study focuses on spoken responses that are reproduced from memory and</context>
</contexts>
<marker>Hoad, Zobel, 2003</marker>
<rawString>Timothy C. Hoad and Justin Zobel. 2003. Methods for identifying versioned and plagiarised documents. Journal of the American Society for Information Science and Technology, 54:203–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification.</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database,</booktitle>
<pages>305--332</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="13929" citStr="Leacock and Chodorow, 1998" startWordPosition="2222" endWordPosition="2225">dit distance between the response and the source; 2) TER, similar to WER, but allowing shifts of words within the text at a low edit cost (Snover et al., 2006); 3) TER-Plus, an extension of TER that includes matching based on paraphrases, stemming, and synonym substitution (Snover et al., 2008); 4) a WordNet similarity metric based on presence in the same synset;5 5) a WordNet similarity metric based on the shortest path between two words in the is-a taxonomy; 6) a WordNet similarity metric similar to (5) that also takes into account the maximum depth of the taxonomy in which the words occur (Leacock and Chodorow, 1998); 7) a WordNet similarity metric based on the depth of the Least Common Subsumer of the two words (Wu and Palmer, 1994); 8) Latent Semantic Analysis, using a model trained on the British National Corpus (BNC, 2007); 9) BLEU (Papineni et al., 2002). Most of these similarity metrics (with the exception of WER and TER) are expected to be robust to modifications between the source text and the plagiarized response, since they do not rely on exact string matches. Each similarity metric was used to compute 4 different features comparing the test response to each of the 49 source texts: 1) the docume</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database, pages 305–332. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pearson Longman</author>
</authors>
<title>The Official Guide to Pearson Test of English Academic.</title>
<date>2010</date>
<publisher>Pearson Education ESL.</publisher>
<contexts>
<context position="2628" citStr="Longman, 2010" startWordPosition="391" endWordPosition="392">ge will become an increasingly important application. First of all, English continues its spread as the global language of education and commerce, and there is a need to assess the communicative compentance of high volumes of highly proficient nonnative speakers. In order to provide a valid evaluation of the complex linguistic skills that are necessary for these speakers, the assessment must contain test items that elicit spontaneous speech, such as the Independent and Integrated Speaking items in the TOEFL iBT test (ETS, 2012), the Retell Lecture item in the Pearson Test of English Academic (Longman, 2010), and the oral interview in the IELTS Academic assessment (Cullen et al., 2014). However, with the increased emphasis on complex linguistic skills in assessments of non-native speech, there is an increased chance that test takers will prepare canned answers using test preparation materials prior to the examination. Therefore, research should also be conducted on detecting spoken plagiarized responses in order to prevent this type of cheating strategy. In addition, there will also likely be an increase in spoken language assessments for native speakers in the K-12 domain in the near future. Cur</context>
</contexts>
<marker>Longman, 2010</marker>
<rawString>Pearson Longman. 2010. The Official Guide to Pearson Test of English Academic. Pearson Education ESL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Lyon</author>
<author>Ruth Barrett</author>
<author>James Malcolm</author>
</authors>
<title>Plagiarism is easy, but also easy to detect.</title>
<date>2006</date>
<tech>Plagiary,</tech>
<pages>1--57</pages>
<contexts>
<context position="4905" citStr="Lyon et al., 2006" startWordPosition="737" endWordPosition="740"> to automatically detect this type of plagiarism on a large scale, in order to provide fair and valid assessments. In this paper, we present an initial study of automated plagiarism detection on spoken responses containing spontaneous non-native speech. A data set of actual plagiarized responses was collected, and text-to-text similarity metrics were applied to the task of classifying responses as plagiarized or non-plagiarized. 2 Previous Work A wide variety of techniques have been employed in previous studies for the task of detecting plagiarized written documents, including n-gram overlap (Lyon et al., 2006), document fingerprinting (Brin et al., 1995), word frequency statistics (Shivakumar and Garcia-Molina, 1995), Information Retrieval-based metrics (Hoad and Zobel, 2003), text summarization evaluation metrics (Chen et al., 2010), WordNet-based features (Nahnsen et al., 2005), and features based on shared syntactic patterns (Uzuner et al., 2005). This task is also related to the widely studied task of paraphrase recognition, which benefits from similar types of features (Finch et al., 2005; Madnani et al., 2012). The current study adopts several of these features that are designed to be robust </context>
</contexts>
<marker>Lyon, Barrett, Malcolm, 2006</marker>
<rawString>Caroline Lyon, Ruth Barrett, and James Malcolm. 2006. Plagiarism is easy, but also easy to detect. Plagiary, 1:57–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining machine translation metrics for paraphrase identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>182--190</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="5421" citStr="Madnani et al., 2012" startWordPosition="811" endWordPosition="814">ies for the task of detecting plagiarized written documents, including n-gram overlap (Lyon et al., 2006), document fingerprinting (Brin et al., 1995), word frequency statistics (Shivakumar and Garcia-Molina, 1995), Information Retrieval-based metrics (Hoad and Zobel, 2003), text summarization evaluation metrics (Chen et al., 2010), WordNet-based features (Nahnsen et al., 2005), and features based on shared syntactic patterns (Uzuner et al., 2005). This task is also related to the widely studied task of paraphrase recognition, which benefits from similar types of features (Finch et al., 2005; Madnani et al., 2012). The current study adopts several of these features that are designed to be robust to the presence of word-level modifications between the source and the plagiarized text; since this study focuses on spoken responses that are reproduced from memory and subsequently processed by a speech recognizer, metrics that rely on exact matches are likely to perform sub-optimally. To our knowledge, no previous work has been reported on automatically detecting similar spoken documents, although research in the field of Spoken Document Retrieval (Haputmann, 2006) is relevant. Due to the difficulties involv</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining machine translation metrics for paraphrase identification. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182– 190, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thade Nahnsen</author>
<author>¨Ozlem Uzuner</author>
<author>Boris Katz</author>
</authors>
<title>Lexical chains and sliding locality windows in content-based text similarity detection.</title>
<date>2005</date>
<tech>CSAIL Technical Report,</tech>
<pages>2005--034</pages>
<contexts>
<context position="5180" citStr="Nahnsen et al., 2005" startWordPosition="772" endWordPosition="775">al plagiarized responses was collected, and text-to-text similarity metrics were applied to the task of classifying responses as plagiarized or non-plagiarized. 2 Previous Work A wide variety of techniques have been employed in previous studies for the task of detecting plagiarized written documents, including n-gram overlap (Lyon et al., 2006), document fingerprinting (Brin et al., 1995), word frequency statistics (Shivakumar and Garcia-Molina, 1995), Information Retrieval-based metrics (Hoad and Zobel, 2003), text summarization evaluation metrics (Chen et al., 2010), WordNet-based features (Nahnsen et al., 2005), and features based on shared syntactic patterns (Uzuner et al., 2005). This task is also related to the widely studied task of paraphrase recognition, which benefits from similar types of features (Finch et al., 2005; Madnani et al., 2012). The current study adopts several of these features that are designed to be robust to the presence of word-level modifications between the source and the plagiarized text; since this study focuses on spoken responses that are reproduced from memory and subsequently processed by a speech recognizer, metrics that rely on exact matches are likely to perform s</context>
</contexts>
<marker>Nahnsen, Uzuner, Katz, 2005</marker>
<rawString>Thade Nahnsen, ¨Ozlem Uzuner, and Boris Katz. 2005. Lexical chains and sliding locality windows in content-based text similarity detection. CSAIL Technical Report, MIT-CSAIL-TR-2005-034.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14176" citStr="Papineni et al., 2002" startWordPosition="2266" endWordPosition="2269"> synonym substitution (Snover et al., 2008); 4) a WordNet similarity metric based on presence in the same synset;5 5) a WordNet similarity metric based on the shortest path between two words in the is-a taxonomy; 6) a WordNet similarity metric similar to (5) that also takes into account the maximum depth of the taxonomy in which the words occur (Leacock and Chodorow, 1998); 7) a WordNet similarity metric based on the depth of the Least Common Subsumer of the two words (Wu and Palmer, 1994); 8) Latent Semantic Analysis, using a model trained on the British National Corpus (BNC, 2007); 9) BLEU (Papineni et al., 2002). Most of these similarity metrics (with the exception of WER and TER) are expected to be robust to modifications between the source text and the plagiarized response, since they do not rely on exact string matches. Each similarity metric was used to compute 4 different features comparing the test response to each of the 49 source texts: 1) the document-level similarity between the test response and the source text; 2) the single maximum similarity value from a sentence-by-sentence comparison between the test response and the source text; 3) the average of the similarity values for all sentenc</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Potthast</author>
<author>Benno Stein</author>
<author>Alberto Barr´on-Cede˜no</author>
<author>Paolo Rosso</author>
</authors>
<title>An evaluation framework for plagiarism detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics.</booktitle>
<marker>Potthast, Stein, Barr´on-Cede˜no, Rosso, 2010</marker>
<rawString>Martin Potthast, Benno Stein, Alberto Barr´on-Cede˜no, and Paolo Rosso. 2010. An evaluation framework for plagiarism detection. In Proceedings of the 23rd International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<booktitle>Efstathios Stamatatos, and Benno Stein. 2013. Overview of the 5th International Competition on Plagiarism Detection. In</booktitle>
<editor>Martin Potthast, Matthias Hagen, Tim Gollub, Martin Tippmann, Johannes Kiesel, Paolo Rosso,</editor>
<marker></marker>
<rawString>Martin Potthast, Matthias Hagen, Tim Gollub, Martin Tippmann, Johannes Kiesel, Paolo Rosso, Efstathios Stamatatos, and Benno Stein. 2013. Overview of the 5th International Competition on Plagiarism Detection. In Pamela Forner, Roberto Navigli, and Dan Tufis, editors, CLEF 2013 Evaluation Labs and Workshop – Working Notes Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Narayanan Shivakumar</author>
<author>Hector Garcia-Molina</author>
</authors>
<title>SCAM: A copy detection mechanism for digital documents.</title>
<date>1995</date>
<booktitle>In Proceedings of the Second Annual Conference on the Theory and Practice of Digital Libraries.</booktitle>
<contexts>
<context position="5014" citStr="Shivakumar and Garcia-Molina, 1995" startWordPosition="750" endWordPosition="754">and valid assessments. In this paper, we present an initial study of automated plagiarism detection on spoken responses containing spontaneous non-native speech. A data set of actual plagiarized responses was collected, and text-to-text similarity metrics were applied to the task of classifying responses as plagiarized or non-plagiarized. 2 Previous Work A wide variety of techniques have been employed in previous studies for the task of detecting plagiarized written documents, including n-gram overlap (Lyon et al., 2006), document fingerprinting (Brin et al., 1995), word frequency statistics (Shivakumar and Garcia-Molina, 1995), Information Retrieval-based metrics (Hoad and Zobel, 2003), text summarization evaluation metrics (Chen et al., 2010), WordNet-based features (Nahnsen et al., 2005), and features based on shared syntactic patterns (Uzuner et al., 2005). This task is also related to the widely studied task of paraphrase recognition, which benefits from similar types of features (Finch et al., 2005; Madnani et al., 2012). The current study adopts several of these features that are designed to be robust to the presence of word-level modifications between the source and the plagiarized text; since this study foc</context>
</contexts>
<marker>Shivakumar, Garcia-Molina, 1995</marker>
<rawString>Narayanan Shivakumar and Hector Garcia-Molina. 1995. SCAM: A copy detection mechanism for digital documents. In Proceedings of the Second Annual Conference on the Theory and Practice of Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="13461" citStr="Snover et al., 2006" startWordPosition="2144" endWordPosition="2147">l times before taking the assessment. 4 Methodology The general approach taken in this study for determining whether a spoken response is plagiarized or not was to compare its content to the content of each of the source materials that had been identified for the responses in this corpus. Given a test response, a comparison was made with each 24 of the 49 reference sources using the following 9 text-to-text similarity metrics: 1) Word Error Rate (WER), or edit distance between the response and the source; 2) TER, similar to WER, but allowing shifts of words within the text at a low edit cost (Snover et al., 2006); 3) TER-Plus, an extension of TER that includes matching based on paraphrases, stemming, and synonym substitution (Snover et al., 2008); 4) a WordNet similarity metric based on presence in the same synset;5 5) a WordNet similarity metric based on the shortest path between two words in the is-a taxonomy; 6) a WordNet similarity metric similar to (5) that also takes into account the maximum depth of the taxonomy in which the words occur (Leacock and Chodorow, 1998); 7) a WordNet similarity metric based on the depth of the Least Common Subsumer of the two words (Wu and Palmer, 1994); 8) Latent S</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>TERp: A system description.</title>
<date>2008</date>
<booktitle>In Proceedings of the First NIST Metrics for Machine Translation Challenge (MetricsMATR),</booktitle>
<location>Waikiki, Hawaii,</location>
<contexts>
<context position="13597" citStr="Snover et al., 2008" startWordPosition="2164" endWordPosition="2167">s plagiarized or not was to compare its content to the content of each of the source materials that had been identified for the responses in this corpus. Given a test response, a comparison was made with each 24 of the 49 reference sources using the following 9 text-to-text similarity metrics: 1) Word Error Rate (WER), or edit distance between the response and the source; 2) TER, similar to WER, but allowing shifts of words within the text at a low edit cost (Snover et al., 2006); 3) TER-Plus, an extension of TER that includes matching based on paraphrases, stemming, and synonym substitution (Snover et al., 2008); 4) a WordNet similarity metric based on presence in the same synset;5 5) a WordNet similarity metric based on the shortest path between two words in the is-a taxonomy; 6) a WordNet similarity metric similar to (5) that also takes into account the maximum depth of the taxonomy in which the words occur (Leacock and Chodorow, 1998); 7) a WordNet similarity metric based on the depth of the Least Common Subsumer of the two words (Wu and Palmer, 1994); 8) Latent Semantic Analysis, using a model trained on the British National Corpus (BNC, 2007); 9) BLEU (Papineni et al., 2002). Most of these simil</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2008</marker>
<rawString>Matt Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2008. TERp: A system description. In Proceedings of the First NIST Metrics for Machine Translation Challenge (MetricsMATR), Waikiki, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>¨Ozlem Uzuner</author>
<author>Boris Katz</author>
<author>Thade Nahnsen</author>
</authors>
<title>Using syntactic information to identify plagiarism.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd Workshop on Building Educational Applications using NLP. Ann Arbor.</booktitle>
<contexts>
<context position="5251" citStr="Uzuner et al., 2005" startWordPosition="783" endWordPosition="786">ics were applied to the task of classifying responses as plagiarized or non-plagiarized. 2 Previous Work A wide variety of techniques have been employed in previous studies for the task of detecting plagiarized written documents, including n-gram overlap (Lyon et al., 2006), document fingerprinting (Brin et al., 1995), word frequency statistics (Shivakumar and Garcia-Molina, 1995), Information Retrieval-based metrics (Hoad and Zobel, 2003), text summarization evaluation metrics (Chen et al., 2010), WordNet-based features (Nahnsen et al., 2005), and features based on shared syntactic patterns (Uzuner et al., 2005). This task is also related to the widely studied task of paraphrase recognition, which benefits from similar types of features (Finch et al., 2005; Madnani et al., 2012). The current study adopts several of these features that are designed to be robust to the presence of word-level modifications between the source and the plagiarized text; since this study focuses on spoken responses that are reproduced from memory and subsequently processed by a speech recognizer, metrics that rely on exact matches are likely to perform sub-optimally. To our knowledge, no previous work has been reported on a</context>
</contexts>
<marker>Uzuner, Katz, Nahnsen, 2005</marker>
<rawString>¨Ozlem Uzuner, Boris Katz, and Thade Nahnsen. 2005. Using syntactic information to identify plagiarism. In Proceedings of the 2nd Workshop on Building Educational Applications using NLP. Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verb semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Mmeeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="14048" citStr="Wu and Palmer, 1994" startWordPosition="2244" endWordPosition="2247">w edit cost (Snover et al., 2006); 3) TER-Plus, an extension of TER that includes matching based on paraphrases, stemming, and synonym substitution (Snover et al., 2008); 4) a WordNet similarity metric based on presence in the same synset;5 5) a WordNet similarity metric based on the shortest path between two words in the is-a taxonomy; 6) a WordNet similarity metric similar to (5) that also takes into account the maximum depth of the taxonomy in which the words occur (Leacock and Chodorow, 1998); 7) a WordNet similarity metric based on the depth of the Least Common Subsumer of the two words (Wu and Palmer, 1994); 8) Latent Semantic Analysis, using a model trained on the British National Corpus (BNC, 2007); 9) BLEU (Papineni et al., 2002). Most of these similarity metrics (with the exception of WER and TER) are expected to be robust to modifications between the source text and the plagiarized response, since they do not rely on exact string matches. Each similarity metric was used to compute 4 different features comparing the test response to each of the 49 source texts: 1) the document-level similarity between the test response and the source text; 2) the single maximum similarity value from a senten</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verb semantics and lexical selection. In Proceedings of the 32nd Annual Mmeeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
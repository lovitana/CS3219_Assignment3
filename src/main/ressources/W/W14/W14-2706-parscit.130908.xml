<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.982013">
Self-disclosure topic model for Twitter conversations
</title>
<author confidence="0.996701">
JinYeong Bak Chin-Yew Lin Alice Oh
</author>
<affiliation confidence="0.924492">
Department of Computer Science Microsoft Research Asia Department of Computer Science
KAIST Beijing 100080, P.R. China KAIST
</affiliation>
<address confidence="0.422697">
Daejeon, South Korea cyl@microsoft.com Daejeon, South Korea
</address>
<email confidence="0.997504">
jy.bak@kaist.ac.kr alice.oh@kaist.edu
</email>
<sectionHeader confidence="0.997358" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949066666667">
Self-disclosure, the act of revealing one-
self to others, is an important social be-
havior that contributes positively to inti-
macy and social support from others. It
is a natural behavior, and social scien-
tists have carried out numerous quantita-
tive analyses of it through manual tagging
and survey questionnaires. Recently, the
flood of data from online social networks
(OSN) offers a practical way to observe
and analyze self-disclosure behavior at an
unprecedented scale. The challenge with
such analysis is that OSN data come with
no annotations, and it would be impos-
sible to manually annotate the data for a
quantitative analysis of self-disclosure. As
a solution, we propose a semi-supervised
machine learning approach, using a vari-
ant of latent Dirichlet allocation for au-
tomatically classifying self-disclosure in a
massive dataset of Twitter conversations.
For measuring the accuracy of our model,
we manually annotate a small subset of
our dataset, and we show that our model
shows significantly higher accuracy and
F-measure than various other methods.
With the results our model, we uncover
a positive and significant relationship be-
tween self-disclosure and online conversa-
tion frequency over time.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9985451875">
Self-disclosure is an important and pervasive so-
cial behavior. People disclose personal informa-
tion about themselves to improve and maintain
relationships (Jourard, 1971; Joinson and Paine,
2007). For example, when two people meet for
the first time, they disclose their names and in-
terests. One positive outcome of self-disclosure
is social support from others (Wills, 1985; Der-
lega et al., 1993), shown also in online social net-
works (OSN) such as Twitter (Kim et al., 2012).
Receiving social support would then lead the user
to be more active on OSN (Steinfield et al., 2008;
Trepte and Reinecke, 2013). In this paper, we seek
to understand this important social behavior using
a large-scale Twitter conversation data, automati-
cally classifying the level of self-disclosure using
machine learning and correlating the patterns with
subsequent OSN usage.
Twitter conversation data, explained in more de-
tail in section 4.1, enable a significantly larger
scale study of naturally-occurring self-disclosure
behavior, compared to traditional social science
studies. One challenge of such large scale study,
though, remains in the lack of labeled ground-
truth data of self-disclosure level. That is,
naturally-occurring Twitter conversations do not
come tagged with the level of self-disclosure in
each conversation. To overcome that challenge,
we propose a semi-supervised machine learning
approach using probabilistic topic modeling. Our
self-disclosure topic model (SDTM) assumes that
self-disclosure behavior can be modeled using a
combination of simple linguistic features (e.g.,
pronouns) with automatically discovered seman-
tic themes (i.e., topics). For instance, an utterance
“I am finally through with this disastrous relation-
ship” uses a first-person pronoun and contains a
topic about personal relationships.
In comparison with various other models,
SDTM shows the highest accuracy, and the result-
ing self-disclosure patterns of the users are cor-
related significantly with their future OSN usage.
Our contributions to the research community in-
clude the following:
• We present a topic model that explicitly in-
cludes the level of self-disclosure in a conver-
sation using linguistic features and the latent
semantic topics (Sec. 3).
</bodyText>
<page confidence="0.990185">
42
</page>
<note confidence="0.5382355">
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 42–49,
Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics
</note>
<listItem confidence="0.8948822">
• We collect a large dataset of Twitter conver-
sations over three years and annotate a small
subset with self-disclosure level (Sec. 4).
• We compare the classification accuracy of
SDTM with other models and show that it
performs the best (Sec. 5).
• We correlate the self-disclosure patterns of
users and their subsequent OSN usage to
show that there is a positive and significant
relationship (Sec. 6).
</listItem>
<sectionHeader confidence="0.981178" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999811868421053">
In this section, we review literature on the relevant
aspects of self-disclosure.
Self-disclosure (SD) level: To quantitatively
analyze self-disclosure, researchers categorize
self-disclosure language into three levels: G (gen-
eral) for no disclosure, M for medium disclosure,
and H for high disclosure (Vondracek and Von-
dracek, 1971; Barak and Gluck-Ofri, 2007). Ut-
terances that contain general (non-sensitive) infor-
mation about the self or someone close (e.g., a
family member) are categorized as M. Examples
are personal events, past history, or future plans.
Utterances about age, occupation and hobbies are
also included. Utterances that contain sensitive in-
formation about the self or someone close are cat-
egorized as H. Sensitive information includes per-
sonal characteristics, problematic behaviors, phys-
ical appearance and wishful ideas. Generally,
these are thoughts and information that one would
generally keep as secrets to himself. All other
utterances, those that do not contain information
about the self or someone close are categorized
as G. Examples include gossip about celebrities or
factual discourse about current events.
Classifying self-disclosure level: Prior work
on quantitatively analyzing self-disclosure has re-
lied on user surveys (Trepte and Reinecke, 2013;
Ledbetter et al., 2011) or human annotation (Barak
and Gluck-Ofri, 2007). These methods consume
much time and effort, so they are not suitable for
large-scale studies. In prior work closest to ours,
Bak et al. (2012) showed that a topic model can
be used to identify self-disclosure, but that work
applies a two-step process in which a basic topic
model is first applied to find the topics, and then
the topics are post-processed for binary classifica-
tion of self-disclosure. We improve upon this work
by applying a single unified model of topics and
</bodyText>
<figureCaption confidence="0.974987">
Figure 1: Graphical model of SDTM
</figureCaption>
<bodyText confidence="0.986972055555556">
self-disclosure for high accuracy in classifying the
three levels of self-disclosure.
Self-disclosure and online social network:
According to social psychology, when someone
discloses about himself, he will receive social sup-
port from those around him (Wills, 1985; Derlega
et al., 1993), and this pattern of self-disclosure
and social support was verified for Twitter con-
versation data (Kim et al., 2012). Social support
is a major motivation for active usage of social
networks services (SNS), and there are findings
that show self-disclosure on SNS has a positive
longitudinal effect on future SNS use (Trepte and
Reinecke, 2013; Ledbetter et al., 2011). While
these previous studies focused on small, qualita-
tive studies, we conduct a large-scale, machine
learning driven study to approach the question of
self-disclosure behavior and SNS use.
</bodyText>
<sectionHeader confidence="0.979477" genericHeader="method">
3 Self-Disclosure Topic Model
</sectionHeader>
<bodyText confidence="0.99988275">
This section describes our model, the self-
disclosure topic model (SDTM), for classifying
self-disclosure level and discovering topics for
each self-disclosure level.
</bodyText>
<subsectionHeader confidence="0.998734">
3.1 Model
</subsectionHeader>
<bodyText confidence="0.994939461538461">
We make two important assumptions based on our
observations of the data. First, first-person pro-
nouns (I, my, me) are good indicators for medium
level of self-disclosure. For example, phrases such
as ‘I live’ or ‘My age is’ occur in utterances that re-
veal personal information. Second, there are top-
ics that occur much more frequently at a particular
SD level. For instance, topics such as physical
appearance and mental health occur frequently at
level H, whereas topics such as birthday and hob-
bies occur frequently at level M.
Figure 1 illustrates the graphical model of
SDTM and how these assumptions are embodied
</bodyText>
<figure confidence="0.972349263157895">
Rl
�l
Kl
3
Ir
Y
r
al
3
W
a
Z
N
W
Y
A
X
T
C
</figure>
<page confidence="0.999653">
43
</page>
<table confidence="0.989174">
Notation Description
G; M; H {general; medium; high} SD level
C; T; N Number of conversations; tweets;
KG; KM; KH words
c; ct Number of topics for {G; M; H}
Yct Conversation; tweet in conversation c
rct SD level of tweet ct, G or M/H
Zct SD level of tweet ct, M or H
wctn Topic of tweet ct
nth word in tweet ct
A Learned Maximum entropy parame-
xct ters
Wct First-person pronouns features
ac Distribution over SD level of tweet ct
6Gc ; 6Mc ; 6H SD level proportion of conversation c
OG; OM; OH Topic proportion of {G; M; H} in con-
a; ^y versation c
OG, OM; pH Word distribution of {G; M; H}
Dirichlet prior for B; π
Dirichlet prior for OG; OM; OH
ncl Number of tweets assigned SD level l
nl in conversation c
ck Number of tweets assigned SD level l
nl and topic k in conversation c
kv Number of instances of word v as-
Mctkv signed SD level l and topic k
Number of instances of word v as-
signed topic k in tweet ct
</table>
<tableCaption confidence="0.999667">
Table 1: Summary of notations used in SDTM.
</tableCaption>
<bodyText confidence="0.999954181818182">
in it. The first assumption about the first-person
pronouns is implemented by the observed variable
xct and the parameters A from a maximum en-
tropy classifier for G vs. M/H level. The second
assumption is implemented by the three separate
word-topic probability vectors for the three lev-
els of SD: 0l which has a Bayesian informative
prior )3l where l ∈ {G, M, H}, the three levels
of self-disclosure. Table 1 lists the notations used
in the model and the generative process, Figure 2
describes the generative process.
</bodyText>
<subsectionHeader confidence="0.999308">
3.2 Classifying G vs M/H levels
</subsectionHeader>
<bodyText confidence="0.999839285714286">
Classifying the SD level for each tweet is done in
two parts, and the first part classifies G vs. M/H
levels with first-person pronouns (I, my, me). In
the graphical model, y is the latent variable that
represents this classification, and w is the distri-
bution over y. x is the observation of the first-
person pronoun in the tweets, and A are the param-
eters learned from the maximum entropy classifier.
With the annotated Twitter conversation dataset
(described in Section 4.2), we experimented with
several classifiers (Decision tree, Naive Bayes)
and chose the maximum entropy classifier because
it performed the best, similar to other joint topic
models (Zhao et al., 2010; Mukherjee et al., 2013).
</bodyText>
<figure confidence="0.882085304347826">
1. For each level l ∈ {G, M, H}:
For each topic k ∈ {1,... , Kl}:
Draw olk ∼ Dir(3l)
2. For each conversation c ∈ {1,..., C}:
(a) Draw BG∼ Dir(ct)
(b) Draw B%&amp;quot;&apos;∼ Dir(ct)
(c) Draw B� ∼ Dir(ct)
(d) Draw ac ∼ Dir(-y)
(e) For each message t ∈ {1, ... , T}:
i. Observe first-person pronouns features xct
ii. Draw wct∼ MaxEnt(xct,X)
iii. Draw gct ∼ Bernoulli(wct)
iv. If gct = 0 which is G level:
A. Draw zct ∼ Mult(BGc )
B. For each word n ∈ {1, ... , N}:
Draw word wctn ∼ Mult(oG zct)
Else which can be M or H level:
A. Draw rct ∼ Mult(wc)
B. Draw zct ∼ Mult(Brct
c )
C. For each word n ∈ {1, ... , N}:
Draw word wctn ∼ Mult(o za
`�)
</figure>
<figureCaption confidence="0.999358">
Figure 2: Generative process of SDTM.
</figureCaption>
<subsectionHeader confidence="0.971694">
3.3 Classifying M vs H levels
</subsectionHeader>
<bodyText confidence="0.999801052631579">
The second part of the classification, the M and the
H level, is driven by informative priors with seed
words and seed trigrams.
Utterances with M level include two types:
1) information related with past events and fu-
ture plans, and 2) general information about self
(Barak and Gluck-Ofri, 2007). For the former, we
add as seed trigrams ‘I have been’ and ‘I will’.
For the latter, we use seven types of information
generally accepted to be personally identifiable in-
formation (McCallister, 2010), as listed in the left
column of Table 2. To find the appropriate tri-
grams for those, we take Twitter conversation data
(described in Section 4.1) and look for trigrams
that begin with ‘I’ and ‘my’ and occur more than
200 times. We then check each one to see whether
it is related with any of the seven types listed in
the table. As a result, we find 57 seed trigrams for
M level. Table 2 shows several examples.
</bodyText>
<table confidence="0.983102125">
Type Trigram
My name is, My last name
My birthday is, My birthday party
I live in, I lived in, I live on
My email address, My phone number
My job is, My new job
My high school, My college is
My dad is, My mom is, My family is
</table>
<tableCaption confidence="0.997903">
Table 2: Example seed trigrams for identifying M
</tableCaption>
<bodyText confidence="0.89009675">
level of SD. There are 51 of these used in SDTM.
Utterances with H level express secretive wishes
or sensitive information that exposes self or some-
one close (Barak and Gluck-Ofri, 2007). These are
</bodyText>
<figure confidence="0.894910714285714">
Name
Birthday
Location
Contact
Occupation
Education
Family
</figure>
<page confidence="0.933871">
44
</page>
<table confidence="0.9665146">
Category Keywords 4 Data Collection and Annotation
physical acne, hair, overweight, stomach, chest,
appearance hand, scar, thighs, chubby, head, skinny
mental/physical addicted, bulimia, doctor, illness, alco-
condition holic, disease, drugs, pills, anorexic
</table>
<tableCaption confidence="0.989838">
Table 3: Example words for identifying H level of
SD. Categories are hand-labeled.
</tableCaption>
<bodyText confidence="0.9008356">
generally keep as secrests. With this intuition, we
crawled 26,523 secret posts from Six Billion Se-
crets 1 site where users post secrets anonymously.
To extract seed words that might express secre-
tive personal information, we compute mutual in-
formation (Manning et al., 2008) with the secret
posts and 24,610 randomly selected tweets. We
select 1,000 words with high mutual information
and filter out stop words. Table 3 shows some of
these words. To extract seed trigrams of secretive
wishes, we again look for trigrams that start with
‘I’ or ‘my’, occur more than 200 times, and select
trigrams of wishful thinking, such as ‘I want to’,
and ‘I wish I’. In total, there are 88 seed words
and 8 seed trigrams for H.
</bodyText>
<sectionHeader confidence="0.702903" genericHeader="method">
3.4 Inference
</sectionHeader>
<bodyText confidence="0.962716571428571">
For posterior inference of SDTM, we use col-
lapsed Gibbs sampling which integrates out la-
tent random variables ω, π, θ, and φ. Then we
only need to compute y, r and z for each tweet.
We compute full conditional distribution p(yct =
j0,rct = l0,zct =k0|y−ct, r− ct, z−ct, w, x) for
tweet ct as follows:
</bodyText>
<equation confidence="0.984646166666666">
p(yct = 0, zct = k0|y−ct, r−ct, z−ct, w, x)
exp(λ0 &apos; xct) � �
a P1j=0 exp(λj &apos; xct) g(c, t, l )
p(yct = 1, rct = l0, zct = k0|y−ct, r−ct, z−ct, w, x)
a i xp(λ1 &apos; xct) (γl0 + n�0ct)) g(c, t, l0, k0)
Pj=0 exp(λj &apos; xct)
</equation>
<bodyText confidence="0.990999666666667">
where z−ct, r−ct, y−ct are z, r, y without tweet
ct, mctk0(·) is the marginalized sum over word v of
mctk0v and the function g(c, t, l0, k0) as follows:
</bodyText>
<equation confidence="0.9977766">
g(c, t, l0, k0) = Γ(PV v=1 βl0 v + nl0−(ct)
Γ(PVv=1 βl0v + nl0−(ct)
k0v + mctk0(·))
k0v )
!YV
αk0 + nl0(−ct) Γ(βl0 v + nl0−(ct)
k0v + mctk0v)
ck0
PK k=1 αk + nl0 Γ(βl0 v + nl0−(ct)
ck v=1 k0v )
</equation>
<footnote confidence="0.814491">
1http://www.sixbillionsecrets.com
</footnote>
<bodyText confidence="0.999979166666667">
To answer our research questions, we need a
large longitudinal dataset of conversations such
that we can analyze the relationship between self-
disclosure behavior and conversation frequency
over time. We chose to crawl Twitter because it
offers a practical and large source of conversations
(Ritter et al., 2010). Others have also analyzed
Twitter conversations for natural language and so-
cial media research (Boyd et al., 2010; Danescu-
Niculescu-Mizil et al., 2011), but we collect con-
versations from the same set of dyads over several
months for a unique longitudinal dataset.
</bodyText>
<subsectionHeader confidence="0.999359">
4.1 Collecting Twitter conversations
</subsectionHeader>
<bodyText confidence="0.999989083333333">
We define a Twitter conversation as a chain of
tweets where two users are consecutively replying
to each other’s tweets using the Twitter reply but-
ton. We identify dyads of English-tweeting users
with at least twenty conversations and collect their
tweets. We use an open source tool for detect-
ing English tweets 2, and to protect users’ privacy,
we replace Twitter userid, usernames and url in
tweets with random strings. This dataset consists
of 101,686 users, 61,451 dyads, 1,956,993 conver-
sations and 17,178,638 tweets which were posted
between August 2007 to July 2013.
</bodyText>
<subsectionHeader confidence="0.999207">
4.2 Annotating self-disclosure level
</subsectionHeader>
<bodyText confidence="0.9999426">
To measure the accuracy of our model, we ran-
domly sample 101 conversations, each with ten
or fewer tweets, and ask three judges, fluent in
English, to annotate each tweet with the level of
self-disclosure. Judges first read and discussed
the definitions and examples of self-disclosure
level shown in (Barak and Gluck-Ofri, 2007), then
they worked separately on a Web-based platform.
Inter-rater agreement using Fleiss kappa (Fleiss,
1971) is 0.67.
</bodyText>
<sectionHeader confidence="0.919843" genericHeader="method">
5 Classification of Self-Disclosure Level
</sectionHeader>
<bodyText confidence="0.9999705">
This section describes experiments and results of
SDTM as well as several other methods for classi-
fication of self-disclosure level.
We first start with the annotated dataset in sec-
tion 4.2 in which each tweet is annotated with SD
level. We then aggregate all of the tweets of a
conversation, and we compute the proportions of
tweets in each SD level. When the proportion of
</bodyText>
<footnote confidence="0.965024">
2https://github.com/shuyo/ldig
</footnote>
<page confidence="0.999482">
45
</page>
<bodyText confidence="0.999776">
tweets at M or H level is equal to or greater than 0.2,
we take the level of the larger proportion and as-
sign that level to the conversation. When the pro-
portions of tweets at M or H level are both less than
0.2, we assign G to the 5D level.
We compare SDTM with the following methods
for classifying tweets for 5D level:
</bodyText>
<listItem confidence="0.988347272727273">
• LDA (Blei et al., 2003): A Bayesian topic
model. Each conversation is treated as a doc-
ument. Used in previous work (Bak et al.,
2012).
• MedLDA (Zhu et al., 2012): A super-
vised topic model for document classifica-
tion. Each conversation is treated as a doc-
ument and response variable can be mapped
to a 5D level.
• LIWC (Tausczik and Pennebaker, 2010):
Word counts of particular categories. Used
in previous work (Houghton and Joinson,
2012).
• Seed words and trigrams (SEED): Occur-
rence of seed words and trigrams which are
described in section 3.3.
• ASUM (Jo and Oh, 2011): A joint model of
sentiment and topic using seed words. Each
sentiment can be mapped to a 5D level. Used
in previous work (Bak et al., 2012).
• First-person pronouns (FirstP): Occurrence
of first-person pronouns which are described
</listItem>
<bodyText confidence="0.992457944444445">
in section 3.2. To identify first-person pro-
nouns, we tagged parts of speech in each
tweet with the Twitter POS tagger (Owoputi
et al., 2013).
SEED, LIWC, LDA and FirstP cannot be used
directly for classification, so we use Maximum en-
tropy model with outputs of each of those models
as features. We run MedLDA, ASUM and SDTM
20 times each and compute the average accuracies
and F-measure for each level. We set 40 topics
for LDA, MedLDA and ASUM, 60; 40; 40 top-
ics for SDTM KG, KM and KH respectively, and
set α = γ = 0.1. To incorporate the seed words
and trigrams into ASUM and SDTM, we initial-
ize βG, βM and βH differently. We assign a high
value of 2.0 for each seed word and trigram for
that level, and a low value of 10−6 for each word
that is a seed word for another level, and a default
</bodyText>
<table confidence="0.999496875">
Method Acc G F1 M F1 H F1 Avg F1
LDA 49.2 0.000 0.650 0.050 0.233
MedLDA 43.3 0.406 0.516 0.093 0.338
LIWC 49.2 0.341 0.607 0.180 0.376
SEED 52.0 0.412 0.600 0.178 0.397
ASUM 56.6 0.320 0.704 0.375 0.466
FirstP 63.2 0.630 0.689 0.095 0.472
SDTM 64.5 0.611 0.706 0.431 0.583
</table>
<tableCaption confidence="0.810545">
Table 4: 5D level classification accuracies and F-
measures using annotated data. Acc is accuracy,
</tableCaption>
<bodyText confidence="0.99973875862069">
and G F1 is F-measure for classifying the G level.
Avg F1 is the average value of G F1, M F1 and H
F1. SDTM outperforms all other methods com-
pared. The difference between SDTM and FirstP
is statistically significant (p-value &lt; 0.05 for ac-
curacy, &lt; 0.0001 for Avg F1).
value of 0.01 for all other words. This approach
is same as other topic model works (Jo and Oh,
2011; Kim et al., 2013).
As Table 4 shows, SDTM performs better than
other methods by accuracy and F-measure. LDA
and MedLDA generally show the lowest perfor-
mance, which is not surprising given these mod-
els are quite general and not tuned specifically
for this type of semi-supervised classification task.
LIWC and SEED perform better than LDA, but
these have quite low F-measure for G and H lev-
els. ASUM shows better performance for classi-
fying H level than others, but not for classifying
the G level. FirstP shows good F-measure for the
G level, but the H level F-measure is quite low,
even lower than SEED. Finally, SDTM has sim-
ilar performance in G and M level with FirstP, but
it performs better in H level than others. Classi-
fying the H level well is important because as we
will discuss later, the H level has the strongest rela-
tionship with longitudinal OSN usage (see Section
6.2), so SDTM is overall the best model for clas-
sifying self-disclosure levels.
</bodyText>
<sectionHeader confidence="0.943359" genericHeader="method">
6 Self-Disclosure and Conversation
Frequency
</sectionHeader>
<bodyText confidence="0.999606285714286">
In this section, we investigate whether there is a
relationship between self-disclosure and conversa-
tion frequency over time. (Trepte and Reinecke,
2013) showed that frequent or high-level of self-
disclosure in online social networks (OSN) con-
tributes positively to OSN usage, and vice versa.
They showed this through an online survey with
</bodyText>
<page confidence="0.998455">
46
</page>
<bodyText confidence="0.999710125">
Facebook and StudiVZ users. With SDTM, we
can automatically classify self-disclosure level of
a large number of conversations, so we investi-
gate whether there is a similar relationship be-
tween self-disclosure in conversations and subse-
quent frequency of conversations with the same
partner on Twitter. More specifically, we ask the
following two questions:
</bodyText>
<listItem confidence="0.99813775">
1. If a dyad displays high 5D level in their con-
versations at a particular time period, would
they have more frequent conversations subse-
quently?
2. If a dyad shows high conversation frequency
at a particular time period, would they dis-
play higher 5D in their subsequent conver-
sations?
</listItem>
<subsectionHeader confidence="0.998701">
6.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.9998335">
We first run SDTM with all of our Twitter con-
versation data with 150; 120; 120 topics for
SDTM KG, KM and KH respectively. The
hyper-parameters are the same as in section 5. To
handle a large dataset, we employ a distributed al-
gorithm (Newman et al., 2009).
Table 5 shows some of the topics that were
prominent in each 5D level by KL-divergence. As
expected, G level includes general topics such as
food, celebrity, soccer and IT devices, M level in-
cludes personal communication and birthday, and
finally, H level includes sickness and profanity.
For comparing conversation frequencies over
time, we divided the conversations into two sets
for each dyad. For the initial period, we include
conversations from the dyad’s first conversation to
60 days later. And for the subsequent period,
we include conversations during the subsequent 30
days.
We compute proportions of conversation for
each 5D level for each dyad in the initial and
subsequent periods. Also, we define a new mea-
surement, 5D level score for a dyad in the period,
which is a weighted sum of each conversation with
5D levels mapped to 1, 2, and 3, for the levels G,
M, and H, respectively.
</bodyText>
<subsectionHeader confidence="0.976399">
6.2 Does self-disclosure lead to more frequent
conversations?
</subsectionHeader>
<bodyText confidence="0.91302575">
We investigate the effect of the level self-
disclosure on long-term use of OSN. We run lin-
ear regression with the intial 5D level score as
Figure 3: Relationship between initial 5D level
and conversation frequency changes over time.
The solid line is the linear regression line, and the
coefficient is 0.118 with p &lt; 0.001, which shows
a significant positive relationship.
</bodyText>
<table confidence="0.390288666666667">
G level M level H level
0.094 0.419 0.464
0.1042 &lt; 0.0001 &lt; 0.0001
</table>
<tableCaption confidence="0.632868">
Table 6: Relationship between initial 5D level
</tableCaption>
<bodyText confidence="0.993788346153846">
proportions and changes in conversation fre-
quency. For M and H levels, there is significant
positive relationship (p &lt; 0.0001), but for the G
level, there is not (p &gt; 0.1).
the independent variable, and the rate of change
in conversation frequency between initial period
and subsequent period as the dependent variable.
The result of regression is that the independent
variable’s coefficient is 0.118 with a low p-value
(p &lt; 0.001). Figure 3 shows the scatter plot with
the regression line, and we can see that the slope
of regression line is positive.
We also investigate the importance of each 5D
level for changes in conversation frequency. We
run linear regression with initial proportions of
each 5D level as the independent variable, and
the same dependent variable as above. As ta-
ble 6 shows, there is no significant relationship
between the initial proportion of the G level and
the changes in conversation frequency (p &gt; 0.1).
But for the M and H levels, the initial proportions
show positive and significant relationships with
the subsequent changes to the conversation fre-
quency (p &lt; 0.0001). These results show that M
and H levels are correlated with changes to the fre-
quency of conversation.
</bodyText>
<figure confidence="0.99527825">
1.5
# Conversaction changes proportion over time
0.0
0.5
.0 1.5 2.0 2.5 3.0
Initial SD level
1.0
1
1.0
0.5
Coeff (Q)
p-value
</figure>
<page confidence="0.982671">
47
</page>
<table confidence="0.9723311">
G level M level H level
101 184 176 36 104 82 113 33 19
chocolate obama league send twitter going ass better lips
butter he’s win email follow party bitch sick kisses
good romney game i’ll tumblr weekend fuck feel love
cake vote season sent tweet day yo throat smiles
peanut right team dm following night shit cold softly
milk president cup address account dinner fucking hope hand
sugar people city know fb birthday lmao pain eyes
cream good arsenal check followers tomorrow shut good neck
</table>
<tableCaption confidence="0.999184">
Table 5: High ranked topics in each level by comparing KL-divergence with other level’s topics
</tableCaption>
<figure confidence="0.595292">
2.05
</figure>
<figureCaption confidence="0.957507">
Figure 4: Relationship between initial conversa-
</figureCaption>
<bodyText confidence="0.877672684210527">
tion frequency and subsequent 5D level. The
solid line is the linear regression line, and the co-
efficient is 0.0016 with p &lt; 0.0001, which shows
a significant positive relationship.
6.3 Does high frequency of conversation lead
to more self-disclosure?
Now we investigate whether the initial conversa-
tion frequency is correlated with the 5D level in
the subsequent period. We run linear regression
with the initial conversation frequency as the inde-
pendent variable, and 5D level in the subsequent
period as the dependent variable.
The regression coefficient is 0.0016 with low p-
value (p &lt; 0.0001). Figure 4 shows the scatter
plot. We can see that the slope of the regression
line is positive. This result supports previous re-
sults in social psychology (Leung, 2002) that fre-
quency of instant chat program ICQ and session
time were correlated to depth of SD in message.
</bodyText>
<sectionHeader confidence="0.997074" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99992028125">
In this paper, we have presented the self-disclosure
topic model (SDTM) for discovering topics and
classifying SD levels from Twitter conversation
data. We devised a set of effective seed words and
trigrams, mined from a dataset of secrets. We also
annotated Twitter conversations to make a ground-
truth dataset for SD level. With annotated data, we
showed that SDTM outperforms previous methods
in classification accuracy and F-measure.
We also analyzed the relationship between SD
level and conversation frequency over time. We
found that there is a positive correlation between
initial SD level and subsequent conversation fre-
quency. Also, dyads show higher level of SD if
they initially display high conversation frequency.
These results support previous results in social
psychology research with more robust results from
a large-scale dataset, and show importance of
looking at SD behavior in OSN.
There are several future directions for this re-
search. First, we can improve our modeling for
higher accuracy and better interpretability. For
instance, SDTM only considers first-person pro-
nouns and topics. Naturally, there are patterns
that can be identified by humans but not captured
by pronouns and topics. Second, the number of
topics for each level is varied, and so we can
explore nonparametric topic models (Teh et al.,
2006) which infer the number of topics from the
data. Third, we can look at the relationship be-
tween self-disclosure behavior and general online
social network usage beyond conversations.
</bodyText>
<sectionHeader confidence="0.998816" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.97930975">
We thank the anonymous reviewers for helpful
comments. Alice Oh was supported by the IT
R&amp;D Program of MSIP/KEIT. [10041313, UX-
oriented Mobile SW Platform]
</bodyText>
<figure confidence="0.995512125">
2.00
1.95
1.90
1.85
1.80
0 20 40 60 80 100
Initial conversation frequency
Subsequent SD level
</figure>
<page confidence="0.997185">
48
</page>
<sectionHeader confidence="0.996019" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999759883495146">
JinYeong Bak, Suin Kim, and Alice Oh. 2012. Self-
disclosure and relationship strength in twitter con-
versations. In Proceedings of ACL.
Azy Barak and Orit Gluck-Ofri. 2007. Degree and
reciprocity of self-disclosure in online forums. Cy-
berPsychology &amp; Behavior, 10(3):407–417.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Danah Boyd, Scott Golder, and Gilad Lotan. 2010.
Tweet, tweet, retweet: Conversational aspects of
retweeting on twitter. In Proceedings of HICSS.
Cristian Danescu-Niculescu-Mizil, Michael Gamon,
and Susan Dumais. 2011. Mark my words!: Lin-
guistic style accommodation in social media. In
Proceedings of WWW.
Valerian J. Derlega, Sandra Metts, Sandra Petronio,
and Stephen T. Margulis. 1993. Self-Disclosure,
volume 5 of SAGE Series on Close Relationships.
SAGE Publications, Inc.
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378.
David J Houghton and Adam N Joinson. 2012.
Linguistic markers of secrets and sensitive self-
disclosure in twitter. In Proceedings of HICSS.
Yohan Jo and Alice H Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of WSDM.
Adam N Joinson and Carina B Paine. 2007. Self-
disclosure, privacy and the internet. The Oxford
handbook of Internet psychology, pages 237–252.
Sidney M Jourard. 1971. Self-disclosure: An experi-
mental analysis of the transparent self.
Suin Kim, JinYeong Bak, and Alice Haeyun Oh. 2012.
Do you feel what i feel? social aspects of emotions
in twitter conversations. In Proceedings of ICWSM.
Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and
Shixia Liu. 2013. A hierarchical aspect-sentiment
model for online reviews. In Proceedings of AAAI.
Andrew M Ledbetter, Joseph P Mazer, Jocelyn M DeG-
root, Kevin R Meyer, Yuping Mao, and Brian Swaf-
ford. 2011. Attitudes toward online social con-
nection and self-disclosure as predictors of facebook
communication and relational closeness. Communi-
cation Research, 38(1):27–53.
Louis Leung. 2002. Loneliness, self-disclosure, and
icq (” i seek you”) use. CyberPsychology &amp; Behav-
ior, 5(3):241–251.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Erika McCallister. 2010. Guide to protecting the confi-
dentiality ofpersonally identifiable information. DI-
ANE Publishing.
Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and
Sharon Meraz. 2013. Public dialogue: Analysis of
tolerance in online discussions. In Proceedings of
ACL.
David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms
for topic models. Journal of Machine Learning Re-
search, 10:1801–1828.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of HLT-NAACL.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Pro-
ceedings of HLT-NAACL.
Charles Steinfield, Nicole B Ellison, and Cliff Lampe.
2008. Social capital, self-esteem, and use of on-
line social network sites: A longitudinal analy-
sis. Journal of Applied Developmental Psychology,
29(6):434–445.
Yla R Tausczik and James W Pennebaker. 2010. The
psychological meaning of words: Liwc and comput-
erized text analysis methods. Journal of Language
and Social Psychology.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the american statistical associ-
ation, 101(476).
Sabine Trepte and Leonard Reinecke. 2013. The re-
ciprocal effects of social network site use and the
disposition for self-disclosure: A longitudinal study.
Computers in Human Behavior, 29(3):1102 – 1112.
Sarah I Vondracek and Fred W Vondracek. 1971. The
manipulation and measurement of self-disclosure in
preadolescents. Merrill-Palmer Quarterly of Behav-
ior and Development, 17(1):51–58.
Thomas Ashby Wills. 1985. Supportive functions
of interpersonal relationships. Social support and
health, xvii:61–82.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a maxent-lda hybrid. In Proceedings of
EMNLP.
Jun Zhu, Amr Ahmed, and Eric P Xing. 2012. Medlda:
maximum margin supervised topic models. Journal
of Machine Learning Research, 13:2237–2278.
</reference>
<page confidence="0.999544">
49
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.354681">
<title confidence="0.995264">Self-disclosure topic model for Twitter conversations</title>
<author confidence="0.999721">JinYeong Bak Chin-Yew Lin Alice Oh</author>
<affiliation confidence="0.99962">Department of Computer Science Microsoft Research Asia Department of Computer Science</affiliation>
<address confidence="0.415284">KAIST Beijing 100080, P.R. China KAIST</address>
<author confidence="0.77892">South Korea South Korea</author>
<email confidence="0.996707">jy.bak@kaist.ac.kralice.oh@kaist.edu</email>
<abstract confidence="0.999070516129032">Self-disclosure, the act of revealing oneself to others, is an important social behavior that contributes positively to intimacy and social support from others. It is a natural behavior, and social scientists have carried out numerous quantitative analyses of it through manual tagging and survey questionnaires. Recently, the flood of data from online social networks (OSN) offers a practical way to observe and analyze self-disclosure behavior at an unprecedented scale. The challenge with such analysis is that OSN data come with no annotations, and it would be impossible to manually annotate the data for a quantitative analysis of self-disclosure. As a solution, we propose a semi-supervised machine learning approach, using a variant of latent Dirichlet allocation for automatically classifying self-disclosure in a massive dataset of Twitter conversations. For measuring the accuracy of our model, we manually annotate a small subset of our dataset, and we show that our model shows significantly higher accuracy and F-measure than various other methods. With the results our model, we uncover a positive and significant relationship between self-disclosure and online conversation frequency over time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>JinYeong Bak</author>
<author>Suin Kim</author>
<author>Alice Oh</author>
</authors>
<title>Selfdisclosure and relationship strength in twitter conversations.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5902" citStr="Bak et al. (2012)" startWordPosition="876" endWordPosition="879">ion that one would generally keep as secrets to himself. All other utterances, those that do not contain information about the self or someone close are categorized as G. Examples include gossip about celebrities or factual discourse about current events. Classifying self-disclosure level: Prior work on quantitatively analyzing self-disclosure has relied on user surveys (Trepte and Reinecke, 2013; Ledbetter et al., 2011) or human annotation (Barak and Gluck-Ofri, 2007). These methods consume much time and effort, so they are not suitable for large-scale studies. In prior work closest to ours, Bak et al. (2012) showed that a topic model can be used to identify self-disclosure, but that work applies a two-step process in which a basic topic model is first applied to find the topics, and then the topics are post-processed for binary classification of self-disclosure. We improve upon this work by applying a single unified model of topics and Figure 1: Graphical model of SDTM self-disclosure for high accuracy in classifying the three levels of self-disclosure. Self-disclosure and online social network: According to social psychology, when someone discloses about himself, he will receive social support f</context>
<context position="16890" citStr="Bak et al., 2012" startWordPosition="2816" endWordPosition="2819">regate all of the tweets of a conversation, and we compute the proportions of tweets in each SD level. When the proportion of 2https://github.com/shuyo/ldig 45 tweets at M or H level is equal to or greater than 0.2, we take the level of the larger proportion and assign that level to the conversation. When the proportions of tweets at M or H level are both less than 0.2, we assign G to the 5D level. We compare SDTM with the following methods for classifying tweets for 5D level: • LDA (Blei et al., 2003): A Bayesian topic model. Each conversation is treated as a document. Used in previous work (Bak et al., 2012). • MedLDA (Zhu et al., 2012): A supervised topic model for document classification. Each conversation is treated as a document and response variable can be mapped to a 5D level. • LIWC (Tausczik and Pennebaker, 2010): Word counts of particular categories. Used in previous work (Houghton and Joinson, 2012). • Seed words and trigrams (SEED): Occurrence of seed words and trigrams which are described in section 3.3. • ASUM (Jo and Oh, 2011): A joint model of sentiment and topic using seed words. Each sentiment can be mapped to a 5D level. Used in previous work (Bak et al., 2012). • First-person p</context>
</contexts>
<marker>Bak, Kim, Oh, 2012</marker>
<rawString>JinYeong Bak, Suin Kim, and Alice Oh. 2012. Selfdisclosure and relationship strength in twitter conversations. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Azy Barak</author>
<author>Orit Gluck-Ofri</author>
</authors>
<title>Degree and reciprocity of self-disclosure in online forums.</title>
<date>2007</date>
<journal>CyberPsychology &amp; Behavior,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="4759" citStr="Barak and Gluck-Ofri, 2007" startWordPosition="705" endWordPosition="708">e classification accuracy of SDTM with other models and show that it performs the best (Sec. 5). • We correlate the self-disclosure patterns of users and their subsequent OSN usage to show that there is a positive and significant relationship (Sec. 6). 2 Background In this section, we review literature on the relevant aspects of self-disclosure. Self-disclosure (SD) level: To quantitatively analyze self-disclosure, researchers categorize self-disclosure language into three levels: G (general) for no disclosure, M for medium disclosure, and H for high disclosure (Vondracek and Vondracek, 1971; Barak and Gluck-Ofri, 2007). Utterances that contain general (non-sensitive) information about the self or someone close (e.g., a family member) are categorized as M. Examples are personal events, past history, or future plans. Utterances about age, occupation and hobbies are also included. Utterances that contain sensitive information about the self or someone close are categorized as H. Sensitive information includes personal characteristics, problematic behaviors, physical appearance and wishful ideas. Generally, these are thoughts and information that one would generally keep as secrets to himself. All other utteran</context>
<context position="11189" citStr="Barak and Gluck-Ofri, 2007" startWordPosition="1823" endWordPosition="1826">t = 0 which is G level: A. Draw zct ∼ Mult(BGc ) B. For each word n ∈ {1, ... , N}: Draw word wctn ∼ Mult(oG zct) Else which can be M or H level: A. Draw rct ∼ Mult(wc) B. Draw zct ∼ Mult(Brct c ) C. For each word n ∈ {1, ... , N}: Draw word wctn ∼ Mult(o za `�) Figure 2: Generative process of SDTM. 3.3 Classifying M vs H levels The second part of the classification, the M and the H level, is driven by informative priors with seed words and seed trigrams. Utterances with M level include two types: 1) information related with past events and future plans, and 2) general information about self (Barak and Gluck-Ofri, 2007). For the former, we add as seed trigrams ‘I have been’ and ‘I will’. For the latter, we use seven types of information generally accepted to be personally identifiable information (McCallister, 2010), as listed in the left column of Table 2. To find the appropriate trigrams for those, we take Twitter conversation data (described in Section 4.1) and look for trigrams that begin with ‘I’ and ‘my’ and occur more than 200 times. We then check each one to see whether it is related with any of the seven types listed in the table. As a result, we find 57 seed trigrams for M level. Table 2 shows seve</context>
<context position="15862" citStr="Barak and Gluck-Ofri, 2007" startWordPosition="2636" endWordPosition="2639">, and to protect users’ privacy, we replace Twitter userid, usernames and url in tweets with random strings. This dataset consists of 101,686 users, 61,451 dyads, 1,956,993 conversations and 17,178,638 tweets which were posted between August 2007 to July 2013. 4.2 Annotating self-disclosure level To measure the accuracy of our model, we randomly sample 101 conversations, each with ten or fewer tweets, and ask three judges, fluent in English, to annotate each tweet with the level of self-disclosure. Judges first read and discussed the definitions and examples of self-disclosure level shown in (Barak and Gluck-Ofri, 2007), then they worked separately on a Web-based platform. Inter-rater agreement using Fleiss kappa (Fleiss, 1971) is 0.67. 5 Classification of Self-Disclosure Level This section describes experiments and results of SDTM as well as several other methods for classification of self-disclosure level. We first start with the annotated dataset in section 4.2 in which each tweet is annotated with SD level. We then aggregate all of the tweets of a conversation, and we compute the proportions of tweets in each SD level. When the proportion of 2https://github.com/shuyo/ldig 45 tweets at M or H level is equ</context>
</contexts>
<marker>Barak, Gluck-Ofri, 2007</marker>
<rawString>Azy Barak and Orit Gluck-Ofri. 2007. Degree and reciprocity of self-disclosure in online forums. CyberPsychology &amp; Behavior, 10(3):407–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="16780" citStr="Blei et al., 2003" startWordPosition="2796" endWordPosition="2799">rst start with the annotated dataset in section 4.2 in which each tweet is annotated with SD level. We then aggregate all of the tweets of a conversation, and we compute the proportions of tweets in each SD level. When the proportion of 2https://github.com/shuyo/ldig 45 tweets at M or H level is equal to or greater than 0.2, we take the level of the larger proportion and assign that level to the conversation. When the proportions of tweets at M or H level are both less than 0.2, we assign G to the 5D level. We compare SDTM with the following methods for classifying tweets for 5D level: • LDA (Blei et al., 2003): A Bayesian topic model. Each conversation is treated as a document. Used in previous work (Bak et al., 2012). • MedLDA (Zhu et al., 2012): A supervised topic model for document classification. Each conversation is treated as a document and response variable can be mapped to a 5D level. • LIWC (Tausczik and Pennebaker, 2010): Word counts of particular categories. Used in previous work (Houghton and Joinson, 2012). • Seed words and trigrams (SEED): Occurrence of seed words and trigrams which are described in section 3.3. • ASUM (Jo and Oh, 2011): A joint model of sentiment and topic using seed</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danah Boyd</author>
<author>Scott Golder</author>
<author>Gilad Lotan</author>
</authors>
<title>Tweet, tweet, retweet: Conversational aspects of retweeting on twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of HICSS.</booktitle>
<contexts>
<context position="14733" citStr="Boyd et al., 2010" startWordPosition="2458" endWordPosition="2461">0−(ct) Γ(PVv=1 βl0v + nl0−(ct) k0v + mctk0(·)) k0v ) !YV αk0 + nl0(−ct) Γ(βl0 v + nl0−(ct) k0v + mctk0v) ck0 PK k=1 αk + nl0 Γ(βl0 v + nl0−(ct) ck v=1 k0v ) 1http://www.sixbillionsecrets.com To answer our research questions, we need a large longitudinal dataset of conversations such that we can analyze the relationship between selfdisclosure behavior and conversation frequency over time. We chose to crawl Twitter because it offers a practical and large source of conversations (Ritter et al., 2010). Others have also analyzed Twitter conversations for natural language and social media research (Boyd et al., 2010; DanescuNiculescu-Mizil et al., 2011), but we collect conversations from the same set of dyads over several months for a unique longitudinal dataset. 4.1 Collecting Twitter conversations We define a Twitter conversation as a chain of tweets where two users are consecutively replying to each other’s tweets using the Twitter reply button. We identify dyads of English-tweeting users with at least twenty conversations and collect their tweets. We use an open source tool for detecting English tweets 2, and to protect users’ privacy, we replace Twitter userid, usernames and url in tweets with rando</context>
</contexts>
<marker>Boyd, Golder, Lotan, 2010</marker>
<rawString>Danah Boyd, Scott Golder, and Gilad Lotan. 2010. Tweet, tweet, retweet: Conversational aspects of retweeting on twitter. In Proceedings of HICSS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Michael Gamon</author>
<author>Susan Dumais</author>
</authors>
<title>Mark my words!: Linguistic style accommodation in social media.</title>
<date>2011</date>
<booktitle>In Proceedings of WWW.</booktitle>
<marker>Danescu-Niculescu-Mizil, Gamon, Dumais, 2011</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Michael Gamon, and Susan Dumais. 2011. Mark my words!: Linguistic style accommodation in social media. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valerian J Derlega</author>
<author>Sandra Metts</author>
<author>Sandra Petronio</author>
<author>Stephen T Margulis</author>
</authors>
<date>1993</date>
<journal>Self-Disclosure,</journal>
<booktitle>of SAGE Series on Close Relationships.</booktitle>
<volume>5</volume>
<publisher>SAGE Publications, Inc.</publisher>
<contexts>
<context position="1944" citStr="Derlega et al., 1993" startWordPosition="285" endWordPosition="289">ly higher accuracy and F-measure than various other methods. With the results our model, we uncover a positive and significant relationship between self-disclosure and online conversation frequency over time. 1 Introduction Self-disclosure is an important and pervasive social behavior. People disclose personal information about themselves to improve and maintain relationships (Jourard, 1971; Joinson and Paine, 2007). For example, when two people meet for the first time, they disclose their names and interests. One positive outcome of self-disclosure is social support from others (Wills, 1985; Derlega et al., 1993), shown also in online social networks (OSN) such as Twitter (Kim et al., 2012). Receiving social support would then lead the user to be more active on OSN (Steinfield et al., 2008; Trepte and Reinecke, 2013). In this paper, we seek to understand this important social behavior using a large-scale Twitter conversation data, automatically classifying the level of self-disclosure using machine learning and correlating the patterns with subsequent OSN usage. Twitter conversation data, explained in more detail in section 4.1, enable a significantly larger scale study of naturally-occurring self-dis</context>
<context position="6558" citStr="Derlega et al., 1993" startWordPosition="980" endWordPosition="983">sed to identify self-disclosure, but that work applies a two-step process in which a basic topic model is first applied to find the topics, and then the topics are post-processed for binary classification of self-disclosure. We improve upon this work by applying a single unified model of topics and Figure 1: Graphical model of SDTM self-disclosure for high accuracy in classifying the three levels of self-disclosure. Self-disclosure and online social network: According to social psychology, when someone discloses about himself, he will receive social support from those around him (Wills, 1985; Derlega et al., 1993), and this pattern of self-disclosure and social support was verified for Twitter conversation data (Kim et al., 2012). Social support is a major motivation for active usage of social networks services (SNS), and there are findings that show self-disclosure on SNS has a positive longitudinal effect on future SNS use (Trepte and Reinecke, 2013; Ledbetter et al., 2011). While these previous studies focused on small, qualitative studies, we conduct a large-scale, machine learning driven study to approach the question of self-disclosure behavior and SNS use. 3 Self-Disclosure Topic Model This sect</context>
</contexts>
<marker>Derlega, Metts, Petronio, Margulis, 1993</marker>
<rawString>Valerian J. Derlega, Sandra Metts, Sandra Petronio, and Stephen T. Margulis. 1993. Self-Disclosure, volume 5 of SAGE Series on Close Relationships. SAGE Publications, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="15972" citStr="Fleiss, 1971" startWordPosition="2653" endWordPosition="2654">nsists of 101,686 users, 61,451 dyads, 1,956,993 conversations and 17,178,638 tweets which were posted between August 2007 to July 2013. 4.2 Annotating self-disclosure level To measure the accuracy of our model, we randomly sample 101 conversations, each with ten or fewer tweets, and ask three judges, fluent in English, to annotate each tweet with the level of self-disclosure. Judges first read and discussed the definitions and examples of self-disclosure level shown in (Barak and Gluck-Ofri, 2007), then they worked separately on a Web-based platform. Inter-rater agreement using Fleiss kappa (Fleiss, 1971) is 0.67. 5 Classification of Self-Disclosure Level This section describes experiments and results of SDTM as well as several other methods for classification of self-disclosure level. We first start with the annotated dataset in section 4.2 in which each tweet is annotated with SD level. We then aggregate all of the tweets of a conversation, and we compute the proportions of tweets in each SD level. When the proportion of 2https://github.com/shuyo/ldig 45 tweets at M or H level is equal to or greater than 0.2, we take the level of the larger proportion and assign that level to the conversatio</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Houghton</author>
<author>Adam N Joinson</author>
</authors>
<title>Linguistic markers of secrets and sensitive selfdisclosure in twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of HICSS.</booktitle>
<contexts>
<context position="17197" citStr="Houghton and Joinson, 2012" startWordPosition="2868" endWordPosition="2871">ion. When the proportions of tweets at M or H level are both less than 0.2, we assign G to the 5D level. We compare SDTM with the following methods for classifying tweets for 5D level: • LDA (Blei et al., 2003): A Bayesian topic model. Each conversation is treated as a document. Used in previous work (Bak et al., 2012). • MedLDA (Zhu et al., 2012): A supervised topic model for document classification. Each conversation is treated as a document and response variable can be mapped to a 5D level. • LIWC (Tausczik and Pennebaker, 2010): Word counts of particular categories. Used in previous work (Houghton and Joinson, 2012). • Seed words and trigrams (SEED): Occurrence of seed words and trigrams which are described in section 3.3. • ASUM (Jo and Oh, 2011): A joint model of sentiment and topic using seed words. Each sentiment can be mapped to a 5D level. Used in previous work (Bak et al., 2012). • First-person pronouns (FirstP): Occurrence of first-person pronouns which are described in section 3.2. To identify first-person pronouns, we tagged parts of speech in each tweet with the Twitter POS tagger (Owoputi et al., 2013). SEED, LIWC, LDA and FirstP cannot be used directly for classification, so we use Maximum e</context>
</contexts>
<marker>Houghton, Joinson, 2012</marker>
<rawString>David J Houghton and Adam N Joinson. 2012. Linguistic markers of secrets and sensitive selfdisclosure in twitter. In Proceedings of HICSS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohan Jo</author>
<author>Alice H Oh</author>
</authors>
<title>Aspect and sentiment unification model for online review analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of WSDM.</booktitle>
<contexts>
<context position="17331" citStr="Jo and Oh, 2011" startWordPosition="2893" endWordPosition="2896">ds for classifying tweets for 5D level: • LDA (Blei et al., 2003): A Bayesian topic model. Each conversation is treated as a document. Used in previous work (Bak et al., 2012). • MedLDA (Zhu et al., 2012): A supervised topic model for document classification. Each conversation is treated as a document and response variable can be mapped to a 5D level. • LIWC (Tausczik and Pennebaker, 2010): Word counts of particular categories. Used in previous work (Houghton and Joinson, 2012). • Seed words and trigrams (SEED): Occurrence of seed words and trigrams which are described in section 3.3. • ASUM (Jo and Oh, 2011): A joint model of sentiment and topic using seed words. Each sentiment can be mapped to a 5D level. Used in previous work (Bak et al., 2012). • First-person pronouns (FirstP): Occurrence of first-person pronouns which are described in section 3.2. To identify first-person pronouns, we tagged parts of speech in each tweet with the Twitter POS tagger (Owoputi et al., 2013). SEED, LIWC, LDA and FirstP cannot be used directly for classification, so we use Maximum entropy model with outputs of each of those models as features. We run MedLDA, ASUM and SDTM 20 times each and compute the average accu</context>
<context position="19098" citStr="Jo and Oh, 2011" startWordPosition="3223" endWordPosition="3226">0.180 0.376 SEED 52.0 0.412 0.600 0.178 0.397 ASUM 56.6 0.320 0.704 0.375 0.466 FirstP 63.2 0.630 0.689 0.095 0.472 SDTM 64.5 0.611 0.706 0.431 0.583 Table 4: 5D level classification accuracies and Fmeasures using annotated data. Acc is accuracy, and G F1 is F-measure for classifying the G level. Avg F1 is the average value of G F1, M F1 and H F1. SDTM outperforms all other methods compared. The difference between SDTM and FirstP is statistically significant (p-value &lt; 0.05 for accuracy, &lt; 0.0001 for Avg F1). value of 0.01 for all other words. This approach is same as other topic model works (Jo and Oh, 2011; Kim et al., 2013). As Table 4 shows, SDTM performs better than other methods by accuracy and F-measure. LDA and MedLDA generally show the lowest performance, which is not surprising given these models are quite general and not tuned specifically for this type of semi-supervised classification task. LIWC and SEED perform better than LDA, but these have quite low F-measure for G and H levels. ASUM shows better performance for classifying H level than others, but not for classifying the G level. FirstP shows good F-measure for the G level, but the H level F-measure is quite low, even lower than</context>
</contexts>
<marker>Jo, Oh, 2011</marker>
<rawString>Yohan Jo and Alice H Oh. 2011. Aspect and sentiment unification model for online review analysis. In Proceedings of WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam N Joinson</author>
<author>Carina B Paine</author>
</authors>
<title>Selfdisclosure, privacy and the internet. The Oxford handbook of Internet psychology,</title>
<date>2007</date>
<pages>237--252</pages>
<contexts>
<context position="1742" citStr="Joinson and Paine, 2007" startWordPosition="252" endWordPosition="255">ying self-disclosure in a massive dataset of Twitter conversations. For measuring the accuracy of our model, we manually annotate a small subset of our dataset, and we show that our model shows significantly higher accuracy and F-measure than various other methods. With the results our model, we uncover a positive and significant relationship between self-disclosure and online conversation frequency over time. 1 Introduction Self-disclosure is an important and pervasive social behavior. People disclose personal information about themselves to improve and maintain relationships (Jourard, 1971; Joinson and Paine, 2007). For example, when two people meet for the first time, they disclose their names and interests. One positive outcome of self-disclosure is social support from others (Wills, 1985; Derlega et al., 1993), shown also in online social networks (OSN) such as Twitter (Kim et al., 2012). Receiving social support would then lead the user to be more active on OSN (Steinfield et al., 2008; Trepte and Reinecke, 2013). In this paper, we seek to understand this important social behavior using a large-scale Twitter conversation data, automatically classifying the level of self-disclosure using machine lear</context>
</contexts>
<marker>Joinson, Paine, 2007</marker>
<rawString>Adam N Joinson and Carina B Paine. 2007. Selfdisclosure, privacy and the internet. The Oxford handbook of Internet psychology, pages 237–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney M Jourard</author>
</authors>
<title>Self-disclosure: An experimental analysis of the transparent self.</title>
<date>1971</date>
<contexts>
<context position="1716" citStr="Jourard, 1971" startWordPosition="250" endWordPosition="251">tically classifying self-disclosure in a massive dataset of Twitter conversations. For measuring the accuracy of our model, we manually annotate a small subset of our dataset, and we show that our model shows significantly higher accuracy and F-measure than various other methods. With the results our model, we uncover a positive and significant relationship between self-disclosure and online conversation frequency over time. 1 Introduction Self-disclosure is an important and pervasive social behavior. People disclose personal information about themselves to improve and maintain relationships (Jourard, 1971; Joinson and Paine, 2007). For example, when two people meet for the first time, they disclose their names and interests. One positive outcome of self-disclosure is social support from others (Wills, 1985; Derlega et al., 1993), shown also in online social networks (OSN) such as Twitter (Kim et al., 2012). Receiving social support would then lead the user to be more active on OSN (Steinfield et al., 2008; Trepte and Reinecke, 2013). In this paper, we seek to understand this important social behavior using a large-scale Twitter conversation data, automatically classifying the level of self-dis</context>
</contexts>
<marker>Jourard, 1971</marker>
<rawString>Sidney M Jourard. 1971. Self-disclosure: An experimental analysis of the transparent self.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suin Kim</author>
<author>JinYeong Bak</author>
<author>Alice Haeyun Oh</author>
</authors>
<title>Do you feel what i feel? social aspects of emotions in twitter conversations.</title>
<date>2012</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<contexts>
<context position="2023" citStr="Kim et al., 2012" startWordPosition="301" endWordPosition="304">odel, we uncover a positive and significant relationship between self-disclosure and online conversation frequency over time. 1 Introduction Self-disclosure is an important and pervasive social behavior. People disclose personal information about themselves to improve and maintain relationships (Jourard, 1971; Joinson and Paine, 2007). For example, when two people meet for the first time, they disclose their names and interests. One positive outcome of self-disclosure is social support from others (Wills, 1985; Derlega et al., 1993), shown also in online social networks (OSN) such as Twitter (Kim et al., 2012). Receiving social support would then lead the user to be more active on OSN (Steinfield et al., 2008; Trepte and Reinecke, 2013). In this paper, we seek to understand this important social behavior using a large-scale Twitter conversation data, automatically classifying the level of self-disclosure using machine learning and correlating the patterns with subsequent OSN usage. Twitter conversation data, explained in more detail in section 4.1, enable a significantly larger scale study of naturally-occurring self-disclosure behavior, compared to traditional social science studies. One challenge</context>
<context position="6676" citStr="Kim et al., 2012" startWordPosition="999" endWordPosition="1002"> find the topics, and then the topics are post-processed for binary classification of self-disclosure. We improve upon this work by applying a single unified model of topics and Figure 1: Graphical model of SDTM self-disclosure for high accuracy in classifying the three levels of self-disclosure. Self-disclosure and online social network: According to social psychology, when someone discloses about himself, he will receive social support from those around him (Wills, 1985; Derlega et al., 1993), and this pattern of self-disclosure and social support was verified for Twitter conversation data (Kim et al., 2012). Social support is a major motivation for active usage of social networks services (SNS), and there are findings that show self-disclosure on SNS has a positive longitudinal effect on future SNS use (Trepte and Reinecke, 2013; Ledbetter et al., 2011). While these previous studies focused on small, qualitative studies, we conduct a large-scale, machine learning driven study to approach the question of self-disclosure behavior and SNS use. 3 Self-Disclosure Topic Model This section describes our model, the selfdisclosure topic model (SDTM), for classifying self-disclosure level and discovering </context>
</contexts>
<marker>Kim, Bak, Oh, 2012</marker>
<rawString>Suin Kim, JinYeong Bak, and Alice Haeyun Oh. 2012. Do you feel what i feel? social aspects of emotions in twitter conversations. In Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suin Kim</author>
<author>Jianwen Zhang</author>
<author>Zheng Chen</author>
<author>Alice Oh</author>
<author>Shixia Liu</author>
</authors>
<title>A hierarchical aspect-sentiment model for online reviews.</title>
<date>2013</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="19117" citStr="Kim et al., 2013" startWordPosition="3227" endWordPosition="3230">52.0 0.412 0.600 0.178 0.397 ASUM 56.6 0.320 0.704 0.375 0.466 FirstP 63.2 0.630 0.689 0.095 0.472 SDTM 64.5 0.611 0.706 0.431 0.583 Table 4: 5D level classification accuracies and Fmeasures using annotated data. Acc is accuracy, and G F1 is F-measure for classifying the G level. Avg F1 is the average value of G F1, M F1 and H F1. SDTM outperforms all other methods compared. The difference between SDTM and FirstP is statistically significant (p-value &lt; 0.05 for accuracy, &lt; 0.0001 for Avg F1). value of 0.01 for all other words. This approach is same as other topic model works (Jo and Oh, 2011; Kim et al., 2013). As Table 4 shows, SDTM performs better than other methods by accuracy and F-measure. LDA and MedLDA generally show the lowest performance, which is not surprising given these models are quite general and not tuned specifically for this type of semi-supervised classification task. LIWC and SEED perform better than LDA, but these have quite low F-measure for G and H levels. ASUM shows better performance for classifying H level than others, but not for classifying the G level. FirstP shows good F-measure for the G level, but the H level F-measure is quite low, even lower than SEED. Finally, SDT</context>
</contexts>
<marker>Kim, Zhang, Chen, Oh, Liu, 2013</marker>
<rawString>Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and Shixia Liu. 2013. A hierarchical aspect-sentiment model for online reviews. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew M Ledbetter</author>
<author>Joseph P Mazer</author>
<author>Jocelyn M DeGroot</author>
<author>Kevin R Meyer</author>
<author>Yuping Mao</author>
<author>Brian Swafford</author>
</authors>
<title>Attitudes toward online social connection and self-disclosure as predictors of facebook communication and relational closeness.</title>
<date>2011</date>
<journal>Communication Research,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="5709" citStr="Ledbetter et al., 2011" startWordPosition="844" endWordPosition="847">r someone close are categorized as H. Sensitive information includes personal characteristics, problematic behaviors, physical appearance and wishful ideas. Generally, these are thoughts and information that one would generally keep as secrets to himself. All other utterances, those that do not contain information about the self or someone close are categorized as G. Examples include gossip about celebrities or factual discourse about current events. Classifying self-disclosure level: Prior work on quantitatively analyzing self-disclosure has relied on user surveys (Trepte and Reinecke, 2013; Ledbetter et al., 2011) or human annotation (Barak and Gluck-Ofri, 2007). These methods consume much time and effort, so they are not suitable for large-scale studies. In prior work closest to ours, Bak et al. (2012) showed that a topic model can be used to identify self-disclosure, but that work applies a two-step process in which a basic topic model is first applied to find the topics, and then the topics are post-processed for binary classification of self-disclosure. We improve upon this work by applying a single unified model of topics and Figure 1: Graphical model of SDTM self-disclosure for high accuracy in c</context>
</contexts>
<marker>Ledbetter, Mazer, DeGroot, Meyer, Mao, Swafford, 2011</marker>
<rawString>Andrew M Ledbetter, Joseph P Mazer, Jocelyn M DeGroot, Kevin R Meyer, Yuping Mao, and Brian Swafford. 2011. Attitudes toward online social connection and self-disclosure as predictors of facebook communication and relational closeness. Communication Research, 38(1):27–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louis Leung</author>
</authors>
<title>Loneliness, self-disclosure, and icq (” i seek you”) use.</title>
<date>2002</date>
<journal>CyberPsychology &amp; Behavior,</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="25550" citStr="Leung, 2002" startWordPosition="4307" endWordPosition="4308">icant positive relationship. 6.3 Does high frequency of conversation lead to more self-disclosure? Now we investigate whether the initial conversation frequency is correlated with the 5D level in the subsequent period. We run linear regression with the initial conversation frequency as the independent variable, and 5D level in the subsequent period as the dependent variable. The regression coefficient is 0.0016 with low pvalue (p &lt; 0.0001). Figure 4 shows the scatter plot. We can see that the slope of the regression line is positive. This result supports previous results in social psychology (Leung, 2002) that frequency of instant chat program ICQ and session time were correlated to depth of SD in message. 7 Conclusion and Future Work In this paper, we have presented the self-disclosure topic model (SDTM) for discovering topics and classifying SD levels from Twitter conversation data. We devised a set of effective seed words and trigrams, mined from a dataset of secrets. We also annotated Twitter conversations to make a groundtruth dataset for SD level. With annotated data, we showed that SDTM outperforms previous methods in classification accuracy and F-measure. We also analyzed the relations</context>
</contexts>
<marker>Leung, 2002</marker>
<rawString>Louis Leung. 2002. Loneliness, self-disclosure, and icq (” i seek you”) use. CyberPsychology &amp; Behavior, 5(3):241–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to information retrieval, volume 1.</title>
<date>2008</date>
<publisher>Cambridge University Press Cambridge.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to information retrieval, volume 1. Cambridge University Press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erika McCallister</author>
</authors>
<title>Guide to protecting the confidentiality ofpersonally identifiable information.</title>
<date>2010</date>
<publisher>DIANE Publishing.</publisher>
<contexts>
<context position="11389" citStr="McCallister, 2010" startWordPosition="1858" endWordPosition="1859">word n ∈ {1, ... , N}: Draw word wctn ∼ Mult(o za `�) Figure 2: Generative process of SDTM. 3.3 Classifying M vs H levels The second part of the classification, the M and the H level, is driven by informative priors with seed words and seed trigrams. Utterances with M level include two types: 1) information related with past events and future plans, and 2) general information about self (Barak and Gluck-Ofri, 2007). For the former, we add as seed trigrams ‘I have been’ and ‘I will’. For the latter, we use seven types of information generally accepted to be personally identifiable information (McCallister, 2010), as listed in the left column of Table 2. To find the appropriate trigrams for those, we take Twitter conversation data (described in Section 4.1) and look for trigrams that begin with ‘I’ and ‘my’ and occur more than 200 times. We then check each one to see whether it is related with any of the seven types listed in the table. As a result, we find 57 seed trigrams for M level. Table 2 shows several examples. Type Trigram My name is, My last name My birthday is, My birthday party I live in, I lived in, I live on My email address, My phone number My job is, My new job My high school, My colleg</context>
</contexts>
<marker>McCallister, 2010</marker>
<rawString>Erika McCallister. 2010. Guide to protecting the confidentiality ofpersonally identifiable information. DIANE Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjun Mukherjee</author>
<author>Vivek Venkataraman</author>
<author>Bing Liu</author>
<author>Sharon Meraz</author>
</authors>
<title>Public dialogue: Analysis of tolerance in online discussions.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="10193" citStr="Mukherjee et al., 2013" startWordPosition="1621" endWordPosition="1624">part classifies G vs. M/H levels with first-person pronouns (I, my, me). In the graphical model, y is the latent variable that represents this classification, and w is the distribution over y. x is the observation of the firstperson pronoun in the tweets, and A are the parameters learned from the maximum entropy classifier. With the annotated Twitter conversation dataset (described in Section 4.2), we experimented with several classifiers (Decision tree, Naive Bayes) and chose the maximum entropy classifier because it performed the best, similar to other joint topic models (Zhao et al., 2010; Mukherjee et al., 2013). 1. For each level l ∈ {G, M, H}: For each topic k ∈ {1,... , Kl}: Draw olk ∼ Dir(3l) 2. For each conversation c ∈ {1,..., C}: (a) Draw BG∼ Dir(ct) (b) Draw B%&amp;quot;&apos;∼ Dir(ct) (c) Draw B� ∼ Dir(ct) (d) Draw ac ∼ Dir(-y) (e) For each message t ∈ {1, ... , T}: i. Observe first-person pronouns features xct ii. Draw wct∼ MaxEnt(xct,X) iii. Draw gct ∼ Bernoulli(wct) iv. If gct = 0 which is G level: A. Draw zct ∼ Mult(BGc ) B. For each word n ∈ {1, ... , N}: Draw word wctn ∼ Mult(oG zct) Else which can be M or H level: A. Draw rct ∼ Mult(wc) B. Draw zct ∼ Mult(Brct c ) C. For each word n ∈ {1, ... , N}:</context>
</contexts>
<marker>Mukherjee, Venkataraman, Liu, Meraz, 2013</marker>
<rawString>Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and Sharon Meraz. 2013. Public dialogue: Analysis of tolerance in online discussions. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Arthur Asuncion</author>
<author>Padhraic Smyth</author>
<author>Max Welling</author>
</authors>
<title>Distributed algorithms for topic models.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>10--1801</pages>
<contexts>
<context position="21367" citStr="Newman et al., 2009" startWordPosition="3603" endWordPosition="3606">. More specifically, we ask the following two questions: 1. If a dyad displays high 5D level in their conversations at a particular time period, would they have more frequent conversations subsequently? 2. If a dyad shows high conversation frequency at a particular time period, would they display higher 5D in their subsequent conversations? 6.1 Experiment Setup We first run SDTM with all of our Twitter conversation data with 150; 120; 120 topics for SDTM KG, KM and KH respectively. The hyper-parameters are the same as in section 5. To handle a large dataset, we employ a distributed algorithm (Newman et al., 2009). Table 5 shows some of the topics that were prominent in each 5D level by KL-divergence. As expected, G level includes general topics such as food, celebrity, soccer and IT devices, M level includes personal communication and birthday, and finally, H level includes sickness and profanity. For comparing conversation frequencies over time, we divided the conversations into two sets for each dyad. For the initial period, we include conversations from the dyad’s first conversation to 60 days later. And for the subsequent period, we include conversations during the subsequent 30 days. We compute p</context>
</contexts>
<marker>Newman, Asuncion, Smyth, Welling, 2009</marker>
<rawString>David Newman, Arthur Asuncion, Padhraic Smyth, and Max Welling. 2009. Distributed algorithms for topic models. Journal of Machine Learning Research, 10:1801–1828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan OConnor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="17705" citStr="Owoputi et al., 2013" startWordPosition="2956" endWordPosition="2959">k and Pennebaker, 2010): Word counts of particular categories. Used in previous work (Houghton and Joinson, 2012). • Seed words and trigrams (SEED): Occurrence of seed words and trigrams which are described in section 3.3. • ASUM (Jo and Oh, 2011): A joint model of sentiment and topic using seed words. Each sentiment can be mapped to a 5D level. Used in previous work (Bak et al., 2012). • First-person pronouns (FirstP): Occurrence of first-person pronouns which are described in section 3.2. To identify first-person pronouns, we tagged parts of speech in each tweet with the Twitter POS tagger (Owoputi et al., 2013). SEED, LIWC, LDA and FirstP cannot be used directly for classification, so we use Maximum entropy model with outputs of each of those models as features. We run MedLDA, ASUM and SDTM 20 times each and compute the average accuracies and F-measure for each level. We set 40 topics for LDA, MedLDA and ASUM, 60; 40; 40 topics for SDTM KG, KM and KH respectively, and set α = γ = 0.1. To incorporate the seed words and trigrams into ASUM and SDTM, we initialize βG, βM and βH differently. We assign a high value of 2.0 for each seed word and trigram for that level, and a low value of 10−6 for each word</context>
</contexts>
<marker>Owoputi, OConnor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan OConnor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of twitter conversations.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="14618" citStr="Ritter et al., 2010" startWordPosition="2440" endWordPosition="2443">ginalized sum over word v of mctk0v and the function g(c, t, l0, k0) as follows: g(c, t, l0, k0) = Γ(PV v=1 βl0 v + nl0−(ct) Γ(PVv=1 βl0v + nl0−(ct) k0v + mctk0(·)) k0v ) !YV αk0 + nl0(−ct) Γ(βl0 v + nl0−(ct) k0v + mctk0v) ck0 PK k=1 αk + nl0 Γ(βl0 v + nl0−(ct) ck v=1 k0v ) 1http://www.sixbillionsecrets.com To answer our research questions, we need a large longitudinal dataset of conversations such that we can analyze the relationship between selfdisclosure behavior and conversation frequency over time. We chose to crawl Twitter because it offers a practical and large source of conversations (Ritter et al., 2010). Others have also analyzed Twitter conversations for natural language and social media research (Boyd et al., 2010; DanescuNiculescu-Mizil et al., 2011), but we collect conversations from the same set of dyads over several months for a unique longitudinal dataset. 4.1 Collecting Twitter conversations We define a Twitter conversation as a chain of tweets where two users are consecutively replying to each other’s tweets using the Twitter reply button. We identify dyads of English-tweeting users with at least twenty conversations and collect their tweets. We use an open source tool for detecting</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Steinfield</author>
<author>Nicole B Ellison</author>
<author>Cliff Lampe</author>
</authors>
<title>Social capital, self-esteem, and use of online social network sites: A longitudinal analysis.</title>
<date>2008</date>
<journal>Journal of Applied Developmental Psychology,</journal>
<volume>29</volume>
<issue>6</issue>
<contexts>
<context position="2124" citStr="Steinfield et al., 2008" startWordPosition="319" endWordPosition="322">versation frequency over time. 1 Introduction Self-disclosure is an important and pervasive social behavior. People disclose personal information about themselves to improve and maintain relationships (Jourard, 1971; Joinson and Paine, 2007). For example, when two people meet for the first time, they disclose their names and interests. One positive outcome of self-disclosure is social support from others (Wills, 1985; Derlega et al., 1993), shown also in online social networks (OSN) such as Twitter (Kim et al., 2012). Receiving social support would then lead the user to be more active on OSN (Steinfield et al., 2008; Trepte and Reinecke, 2013). In this paper, we seek to understand this important social behavior using a large-scale Twitter conversation data, automatically classifying the level of self-disclosure using machine learning and correlating the patterns with subsequent OSN usage. Twitter conversation data, explained in more detail in section 4.1, enable a significantly larger scale study of naturally-occurring self-disclosure behavior, compared to traditional social science studies. One challenge of such large scale study, though, remains in the lack of labeled groundtruth data of self-disclosur</context>
</contexts>
<marker>Steinfield, Ellison, Lampe, 2008</marker>
<rawString>Charles Steinfield, Nicole B Ellison, and Cliff Lampe. 2008. Social capital, self-esteem, and use of online social network sites: A longitudinal analysis. Journal of Applied Developmental Psychology, 29(6):434–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yla R Tausczik</author>
<author>James W Pennebaker</author>
</authors>
<title>The psychological meaning of words: Liwc and computerized text analysis methods.</title>
<date>2010</date>
<journal>Journal of Language and Social Psychology.</journal>
<contexts>
<context position="17107" citStr="Tausczik and Pennebaker, 2010" startWordPosition="2855" endWordPosition="2858">r than 0.2, we take the level of the larger proportion and assign that level to the conversation. When the proportions of tweets at M or H level are both less than 0.2, we assign G to the 5D level. We compare SDTM with the following methods for classifying tweets for 5D level: • LDA (Blei et al., 2003): A Bayesian topic model. Each conversation is treated as a document. Used in previous work (Bak et al., 2012). • MedLDA (Zhu et al., 2012): A supervised topic model for document classification. Each conversation is treated as a document and response variable can be mapped to a 5D level. • LIWC (Tausczik and Pennebaker, 2010): Word counts of particular categories. Used in previous work (Houghton and Joinson, 2012). • Seed words and trigrams (SEED): Occurrence of seed words and trigrams which are described in section 3.3. • ASUM (Jo and Oh, 2011): A joint model of sentiment and topic using seed words. Each sentiment can be mapped to a 5D level. Used in previous work (Bak et al., 2012). • First-person pronouns (FirstP): Occurrence of first-person pronouns which are described in section 3.2. To identify first-person pronouns, we tagged parts of speech in each tweet with the Twitter POS tagger (Owoputi et al., 2013). </context>
</contexts>
<marker>Tausczik, Pennebaker, 2010</marker>
<rawString>Yla R Tausczik and James W Pennebaker. 2010. The psychological meaning of words: Liwc and computerized text analysis methods. Journal of Language and Social Psychology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the american statistical association,</journal>
<volume>101</volume>
<issue>476</issue>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. 2006. Hierarchical dirichlet processes. Journal of the american statistical association, 101(476).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Trepte</author>
<author>Leonard Reinecke</author>
</authors>
<title>The reciprocal effects of social network site use and the disposition for self-disclosure: A longitudinal study.</title>
<date>2013</date>
<journal>Computers in Human Behavior,</journal>
<volume>29</volume>
<issue>3</issue>
<pages>1112</pages>
<contexts>
<context position="2152" citStr="Trepte and Reinecke, 2013" startWordPosition="323" endWordPosition="326">time. 1 Introduction Self-disclosure is an important and pervasive social behavior. People disclose personal information about themselves to improve and maintain relationships (Jourard, 1971; Joinson and Paine, 2007). For example, when two people meet for the first time, they disclose their names and interests. One positive outcome of self-disclosure is social support from others (Wills, 1985; Derlega et al., 1993), shown also in online social networks (OSN) such as Twitter (Kim et al., 2012). Receiving social support would then lead the user to be more active on OSN (Steinfield et al., 2008; Trepte and Reinecke, 2013). In this paper, we seek to understand this important social behavior using a large-scale Twitter conversation data, automatically classifying the level of self-disclosure using machine learning and correlating the patterns with subsequent OSN usage. Twitter conversation data, explained in more detail in section 4.1, enable a significantly larger scale study of naturally-occurring self-disclosure behavior, compared to traditional social science studies. One challenge of such large scale study, though, remains in the lack of labeled groundtruth data of self-disclosure level. That is, naturally-</context>
<context position="5684" citStr="Trepte and Reinecke, 2013" startWordPosition="840" endWordPosition="843">nformation about the self or someone close are categorized as H. Sensitive information includes personal characteristics, problematic behaviors, physical appearance and wishful ideas. Generally, these are thoughts and information that one would generally keep as secrets to himself. All other utterances, those that do not contain information about the self or someone close are categorized as G. Examples include gossip about celebrities or factual discourse about current events. Classifying self-disclosure level: Prior work on quantitatively analyzing self-disclosure has relied on user surveys (Trepte and Reinecke, 2013; Ledbetter et al., 2011) or human annotation (Barak and Gluck-Ofri, 2007). These methods consume much time and effort, so they are not suitable for large-scale studies. In prior work closest to ours, Bak et al. (2012) showed that a topic model can be used to identify self-disclosure, but that work applies a two-step process in which a basic topic model is first applied to find the topics, and then the topics are post-processed for binary classification of self-disclosure. We improve upon this work by applying a single unified model of topics and Figure 1: Graphical model of SDTM self-disclosu</context>
<context position="20258" citStr="Trepte and Reinecke, 2013" startWordPosition="3419" endWordPosition="3422">e G level, but the H level F-measure is quite low, even lower than SEED. Finally, SDTM has similar performance in G and M level with FirstP, but it performs better in H level than others. Classifying the H level well is important because as we will discuss later, the H level has the strongest relationship with longitudinal OSN usage (see Section 6.2), so SDTM is overall the best model for classifying self-disclosure levels. 6 Self-Disclosure and Conversation Frequency In this section, we investigate whether there is a relationship between self-disclosure and conversation frequency over time. (Trepte and Reinecke, 2013) showed that frequent or high-level of selfdisclosure in online social networks (OSN) contributes positively to OSN usage, and vice versa. They showed this through an online survey with 46 Facebook and StudiVZ users. With SDTM, we can automatically classify self-disclosure level of a large number of conversations, so we investigate whether there is a similar relationship between self-disclosure in conversations and subsequent frequency of conversations with the same partner on Twitter. More specifically, we ask the following two questions: 1. If a dyad displays high 5D level in their conversat</context>
</contexts>
<marker>Trepte, Reinecke, 2013</marker>
<rawString>Sabine Trepte and Leonard Reinecke. 2013. The reciprocal effects of social network site use and the disposition for self-disclosure: A longitudinal study. Computers in Human Behavior, 29(3):1102 – 1112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah I Vondracek</author>
<author>Fred W Vondracek</author>
</authors>
<title>The manipulation and measurement of self-disclosure in preadolescents.</title>
<date>1971</date>
<booktitle>Merrill-Palmer Quarterly of Behavior and Development,</booktitle>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="4730" citStr="Vondracek and Vondracek, 1971" startWordPosition="700" endWordPosition="704">level (Sec. 4). • We compare the classification accuracy of SDTM with other models and show that it performs the best (Sec. 5). • We correlate the self-disclosure patterns of users and their subsequent OSN usage to show that there is a positive and significant relationship (Sec. 6). 2 Background In this section, we review literature on the relevant aspects of self-disclosure. Self-disclosure (SD) level: To quantitatively analyze self-disclosure, researchers categorize self-disclosure language into three levels: G (general) for no disclosure, M for medium disclosure, and H for high disclosure (Vondracek and Vondracek, 1971; Barak and Gluck-Ofri, 2007). Utterances that contain general (non-sensitive) information about the self or someone close (e.g., a family member) are categorized as M. Examples are personal events, past history, or future plans. Utterances about age, occupation and hobbies are also included. Utterances that contain sensitive information about the self or someone close are categorized as H. Sensitive information includes personal characteristics, problematic behaviors, physical appearance and wishful ideas. Generally, these are thoughts and information that one would generally keep as secrets </context>
</contexts>
<marker>Vondracek, Vondracek, 1971</marker>
<rawString>Sarah I Vondracek and Fred W Vondracek. 1971. The manipulation and measurement of self-disclosure in preadolescents. Merrill-Palmer Quarterly of Behavior and Development, 17(1):51–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Ashby Wills</author>
</authors>
<title>Supportive functions of interpersonal relationships. Social support and health,</title>
<date>1985</date>
<pages>61--82</pages>
<contexts>
<context position="1921" citStr="Wills, 1985" startWordPosition="283" endWordPosition="284">s significantly higher accuracy and F-measure than various other methods. With the results our model, we uncover a positive and significant relationship between self-disclosure and online conversation frequency over time. 1 Introduction Self-disclosure is an important and pervasive social behavior. People disclose personal information about themselves to improve and maintain relationships (Jourard, 1971; Joinson and Paine, 2007). For example, when two people meet for the first time, they disclose their names and interests. One positive outcome of self-disclosure is social support from others (Wills, 1985; Derlega et al., 1993), shown also in online social networks (OSN) such as Twitter (Kim et al., 2012). Receiving social support would then lead the user to be more active on OSN (Steinfield et al., 2008; Trepte and Reinecke, 2013). In this paper, we seek to understand this important social behavior using a large-scale Twitter conversation data, automatically classifying the level of self-disclosure using machine learning and correlating the patterns with subsequent OSN usage. Twitter conversation data, explained in more detail in section 4.1, enable a significantly larger scale study of natur</context>
<context position="6535" citStr="Wills, 1985" startWordPosition="978" endWordPosition="979">odel can be used to identify self-disclosure, but that work applies a two-step process in which a basic topic model is first applied to find the topics, and then the topics are post-processed for binary classification of self-disclosure. We improve upon this work by applying a single unified model of topics and Figure 1: Graphical model of SDTM self-disclosure for high accuracy in classifying the three levels of self-disclosure. Self-disclosure and online social network: According to social psychology, when someone discloses about himself, he will receive social support from those around him (Wills, 1985; Derlega et al., 1993), and this pattern of self-disclosure and social support was verified for Twitter conversation data (Kim et al., 2012). Social support is a major motivation for active usage of social networks services (SNS), and there are findings that show self-disclosure on SNS has a positive longitudinal effect on future SNS use (Trepte and Reinecke, 2013; Ledbetter et al., 2011). While these previous studies focused on small, qualitative studies, we conduct a large-scale, machine learning driven study to approach the question of self-disclosure behavior and SNS use. 3 Self-Disclosur</context>
</contexts>
<marker>Wills, 1985</marker>
<rawString>Thomas Ashby Wills. 1985. Supportive functions of interpersonal relationships. Social support and health, xvii:61–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Xin Zhao</author>
<author>Jing Jiang</author>
<author>Hongfei Yan</author>
<author>Xiaoming Li</author>
</authors>
<title>Jointly modeling aspects and opinions with a maxent-lda hybrid.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="10168" citStr="Zhao et al., 2010" startWordPosition="1617" endWordPosition="1620">rts, and the first part classifies G vs. M/H levels with first-person pronouns (I, my, me). In the graphical model, y is the latent variable that represents this classification, and w is the distribution over y. x is the observation of the firstperson pronoun in the tweets, and A are the parameters learned from the maximum entropy classifier. With the annotated Twitter conversation dataset (described in Section 4.2), we experimented with several classifiers (Decision tree, Naive Bayes) and chose the maximum entropy classifier because it performed the best, similar to other joint topic models (Zhao et al., 2010; Mukherjee et al., 2013). 1. For each level l ∈ {G, M, H}: For each topic k ∈ {1,... , Kl}: Draw olk ∼ Dir(3l) 2. For each conversation c ∈ {1,..., C}: (a) Draw BG∼ Dir(ct) (b) Draw B%&amp;quot;&apos;∼ Dir(ct) (c) Draw B� ∼ Dir(ct) (d) Draw ac ∼ Dir(-y) (e) For each message t ∈ {1, ... , T}: i. Observe first-person pronouns features xct ii. Draw wct∼ MaxEnt(xct,X) iii. Draw gct ∼ Bernoulli(wct) iv. If gct = 0 which is G level: A. Draw zct ∼ Mult(BGc ) B. For each word n ∈ {1, ... , N}: Draw word wctn ∼ Mult(oG zct) Else which can be M or H level: A. Draw rct ∼ Mult(wc) B. Draw zct ∼ Mult(Brct c ) C. For ea</context>
</contexts>
<marker>Zhao, Jiang, Yan, Li, 2010</marker>
<rawString>Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming Li. 2010. Jointly modeling aspects and opinions with a maxent-lda hybrid. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Medlda: maximum margin supervised topic models.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>13--2237</pages>
<contexts>
<context position="16919" citStr="Zhu et al., 2012" startWordPosition="2822" endWordPosition="2825"> conversation, and we compute the proportions of tweets in each SD level. When the proportion of 2https://github.com/shuyo/ldig 45 tweets at M or H level is equal to or greater than 0.2, we take the level of the larger proportion and assign that level to the conversation. When the proportions of tweets at M or H level are both less than 0.2, we assign G to the 5D level. We compare SDTM with the following methods for classifying tweets for 5D level: • LDA (Blei et al., 2003): A Bayesian topic model. Each conversation is treated as a document. Used in previous work (Bak et al., 2012). • MedLDA (Zhu et al., 2012): A supervised topic model for document classification. Each conversation is treated as a document and response variable can be mapped to a 5D level. • LIWC (Tausczik and Pennebaker, 2010): Word counts of particular categories. Used in previous work (Houghton and Joinson, 2012). • Seed words and trigrams (SEED): Occurrence of seed words and trigrams which are described in section 3.3. • ASUM (Jo and Oh, 2011): A joint model of sentiment and topic using seed words. Each sentiment can be mapped to a 5D level. Used in previous work (Bak et al., 2012). • First-person pronouns (FirstP): Occurrence </context>
</contexts>
<marker>Zhu, Ahmed, Xing, 2012</marker>
<rawString>Jun Zhu, Amr Ahmed, and Eric P Xing. 2012. Medlda: maximum margin supervised topic models. Journal of Machine Learning Research, 13:2237–2278.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
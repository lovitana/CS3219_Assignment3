<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.050649">
<title confidence="0.999115">
Automatic Generation of Challenging Distractors
Using Context-Sensitive Inference Rules
</title>
<author confidence="0.975206">
Torsten Zesch
</author>
<affiliation confidence="0.979937">
Language Technology Lab
University of Duisburg-Essen
</affiliation>
<email confidence="0.671402">
torsten.zesch@uni-due.de
</email>
<sectionHeader confidence="0.988092" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999427583333333">
Automatically generating challenging dis-
tractors for multiple-choice gap-fill items
is still an unsolved problem. We propose
to employ context-sensitive lexical infer-
ence rules in order to generate distractors
that are semantically similar to the gap tar-
get word in some sense, but not in the par-
ticular sense induced by the gap-fill con-
text. We hypothesize that such distrac-
tors should be particularly hard to distin-
guish from the correct answer. We focus
on verbs as they are especially difficult to
master for language learners and find that
our approach is quite effective. In our test
set of 20 items, our proposed method de-
creases the number of invalid distractors in
90% of the cases, and fully eliminates all
of them in 65%. Further analysis on that
dataset does not support our hypothesis re-
garding item difficulty as measured by av-
erage error rate of language learners. We
conjecture that this may be due to limita-
tions in our evaluation setting, which we
plan to address in future work.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999834">
Multiple-choice gap-fill items as illustrated in Fig-
ure 1 are frequently used for both testing lan-
guage proficiency and as a learning device. Each
item consists of a carrier sentence that provides
the context to a target word. The target word is
blanked and presented as one possible gap-fill an-
swer together with a certain number (usually 3)
of distractors. Given a desired target word, car-
rier sentences containing it can be automatically
selected from a corpus. Some methods even select
only sentences where the target word is used in a
certain sense (Liu et al., 2005). Then, the main
problem is to pick challenging distractors that are
</bodyText>
<author confidence="0.859086">
Oren Melamud
</author>
<affiliation confidence="0.9241805">
Computer Science Department
Bar-Ilan University
</affiliation>
<email confidence="0.956449">
melamuo@cs.biu.ac.il
</email>
<figureCaption confidence="0.999592">
Figure 1: Multiple-choice gap-fill item.
</figureCaption>
<bodyText confidence="0.999445590909091">
reasonably hard to distinguish from the correct an-
swer (i.e. the target word) on one hand, yet cannot
be considered as correct answers on the other.
In this paper we propose to generate distrac-
tors that are semantically similar to the gap tar-
get word in some sense, but not in the particu-
lar sense induced by the gap-fill context, thereby
making them difficult to distinguish from the tar-
get word. For example, the distractor gain in Fig-
ure 1 is semantically similar to acquire, but is not
appropriate in the particular context of purchasing
companies, and therefore has high distractive po-
tential. On the other hand, the distractor purchase
is a correct answer in this context and is therefore
an invalid distractor. To generate challenging dis-
tractors, we utilize context-sensitive lexical infer-
ence rules that can discriminate between appropri-
ate substitutes of a target word given its context
and other inappropriate substitutes.
In the next section, we give an overview of pre-
vious work in order to place our contribution into
context.
</bodyText>
<sectionHeader confidence="0.995624" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.99994625">
The process of finding good distractors involves
two steps: Candidate Selection controls the diffi-
culty of the items, while Reliability Checking en-
sures that the items remain solvable, i.e. it ensures
</bodyText>
<page confidence="0.987274">
143
</page>
<bodyText confidence="0.814900142857143">
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications, pages 143–148,
Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics
that there is only one correct answer. We note
that this work is focused on single-word distrac-
tors rather than phrases (Gates et al., 2011), and
only on target isolated carrier sentences rather than
longer texts as in (Mostow and Jang, 2012).
</bodyText>
<subsectionHeader confidence="0.994522">
2.1 Candidates Selection
</subsectionHeader>
<bodyText confidence="0.999983906976744">
In some settings the set of possible distractors is
known in advance, e.g. the set of English prepo-
sitions in preposition exercises (Lee and Seneff,
2007) or a confusion set with previously known
errors like {two, too, to}. Sakaguchi et al. (2013)
use data from the Lang-8 platform (a corpus of
manually annotated errors1) in order to determine
typical learner errors and use them as distractors.
However, in the common setting only the target
word is known and the set of distractors needs to
be automatically generated.
Randomly selecting distractors is a valid strat-
egy (Mostow and Jang, 2012), but it is only suit-
able for the most beginner learners. More ad-
vanced learners can easily rule out distractors that
do not fit grammatically or are too unrelated se-
mantically. Thus, more advanced approaches usu-
ally employ basic strategies, such as choosing dis-
tractors with the same part-of-speech tag as the
target word, or distractors with a corpus frequency
comparable to the target word (Hoshino and Naka-
gawa, 2007) (based on the assumption that corpus
frequency roughly correlates with word difficulty).
Pino and Eskenazi (2009) use distractors that are
morphologically, orthographically, or phonetically
similar (e.g. bread – beard).
Another approach used in previous works to
make distractors more challenging is utilizing the-
sauri (Sumita et al., 2005; Smith and Avinesh,
2010) or taxonomies (Hoshino and Nakagawa,
2007; Mitkov et al., 2009) to select words that are
semantically similar to the target word. In addi-
tion to the target word, some approaches also con-
sider the semantic relatedness of distractors with
the whole carrier sentence or paragraph (Pino et
al., 2008; Agarwal and Mannem, 2011; Mostow
and Jang, 2012), i.e. they pick distractors that are
from the same domain as the target word.
Generally, selecting more challenging distrac-
tors usually means making them more similar to
the target word. As this increases the probability
that a distractor might actually be another correct
answer, we need a more sophisticated approach for
</bodyText>
<footnote confidence="0.951337">
1http://cl.naist.jp/nldata/lang-8/
</footnote>
<bodyText confidence="0.796093">
checking the reliability of the distractor set.
</bodyText>
<subsectionHeader confidence="0.997789">
2.2 Reliability Checking
</subsectionHeader>
<bodyText confidence="0.999994972972973">
In order to make sure that there is only one correct
answer to a gap-fill item, there needs to be a way
to decide for each distractor whether it fits into the
context of the carrier sentence or not. In those
cases, where we have a limited list of potential tar-
get words and distractors, e.g. in preposition exer-
cises (Lee and Seneff, 2007), a supervised classi-
fier can be trained to do this job. Given enough
training data, this approach yields very high preci-
sion, but it cannot be easily applied to open word
classes like nouns or verbs, which are much larger
and dynamic in nature.
When we do not have a closed list of potential
distractors at hand, one way to perform reliabil-
ity checking is by considering collocations involv-
ing the target word (Pino et al., 2008; Smith and
Avinesh, 2010). For example, if the target word
is strong, we can find the collocation strong tea.
Then we can use powerful as a distractor because
it is semantically similar to strong, yet *powerful
tea is not a valid collocation. This approach is ef-
fective, but requires strong collocations to discrim-
inate between valid and invalid distractors. There-
fore it cannot be used with carrier sentences that
do not contain strong collocations, such as the sen-
tence in Figure 1.
Sumita et al. (2005) apply a simple web search
approach to judge the reliability of an item. They
check whether the carrier sentence with the target
word replaced by the distractor can be found on
the web. If such a sentence is found, the distrac-
tor is discarded. We note that the applicability of
this approach is limited, as finding exact matches
for such artificial sentences can be unlikely due
to sparseness of natural languages. Therefore not
finding an exact match does not necessarily rule
out the possibility of an invalid distractor.
</bodyText>
<sectionHeader confidence="0.993457" genericHeader="method">
3 Automatic Generation of Challenging
Distractors
</sectionHeader>
<bodyText confidence="0.99995525">
Our goal is to automatically generate distractors
that are as ‘close’ to the target word as possible,
yet do not fit the carrier sentence context. To ac-
complish this, our strategy is to first generate a set
of distractor candidates, which are semantically
similar to the target word. Then we use context-
sensitive lexical inference rules to filter candidates
that fit the context, and thus cannot be used as dis-
</bodyText>
<page confidence="0.993345">
144
</page>
<bodyText confidence="0.9988645">
tractors. In the remainder of this section we de-
scribe this procedure in more detail.
</bodyText>
<subsectionHeader confidence="0.999459">
3.1 Context-Sensitive Inference Rules
</subsectionHeader>
<bodyText confidence="0.9999948">
A lexical inference rule ‘LHS → RHS’, such as
‘acquire → purchase’, specifies a directional in-
ference relation between two words (or terms). A
rule can be applied when its LHS matches a word
in a text T, and then that word is substituted for
RHS, yielding the modified text H. For example,
applying the rule above to “Microsoft acquired
Skype”, yields “Microsoft purchased Skype”. If the
rule is true then the meaning of H is inferred from
the meaning of T. A popular way to learn lex-
ical inference rules in an unsupervised setting is
by using distributional similarity models (Lin and
Pantel, 2001; Kotlerman et al., 2010). Under this
approach, target words are represented as vectors
of context features, and the score of a rule between
two target words is based on vector arithmetics.
One of the main shortcomings of such rules is
that they are context-insensitive, i.e. they have a
single score, which is not assessed with respect to
the concrete context T under which they are ap-
plied. However, the appropriateness of an infer-
ence rule may in fact depend on this context. For
example, ‘Microsoft acquire Skype → Microsoft
purchase Skype’, is an appropriate application of
the rule ‘acquire → purchase’, while ‘Children
acquire skills → Children purchase skills’ is not.
To address this issue, additional models were in-
troduced that compute a different context-sensitive
score per each context T, under which it is applied
(Dinu and Lapata, 2010; Melamud et al., 2013).
In this work, we use the resource provided
by Melamud et al. (2013), which includes both
context-sensitive and context-insensitive rules for
over 2,000 frequent verbs.2 We use these rules to
generate challenging distractors as we show next.
</bodyText>
<subsectionHeader confidence="0.9903">
3.2 Distractor Selection &amp; Reliability
</subsectionHeader>
<bodyText confidence="0.999218111111111">
We start with the following illustrative example to
motivate our approach. While the words purchase
and acquire are considered to be almost perfect
synonyms in sentences like Microsoft acquires
Skype and Microsoft purchases Skype, this is not
true for all contexts. For example, in Children
acquire skills vs. Children purchase skills, the
meaning is clearly not equivalent. These context-
dependent senses, which are particularly typical to
</bodyText>
<footnote confidence="0.940607">
2http://www.cs.biu.ac.il/nlp/downloads/wt-rules.html
</footnote>
<figureCaption confidence="0.990981">
Figure 2: Filtering context-insensitive substitu-
tions with context-sensitive ones in order to get
challenging distractors.
</figureCaption>
<bodyText confidence="0.998567297297297">
verbs, make it difficult for learners to understand
how to properly use these words.
Acquiring such fine-grained sense distinction
skills is a prerequisite for really competent lan-
guage usage. These skills can be trained and tested
with distractors, such as purchase in the exam-
ple above. Therefore, such items are good indi-
cators in language proficiency testing, and should
be specifically trained when learning a language.
To generate such challenging distractors, we
first use the context-insensitive rules, whose LHS
matches the carrier sentence target word, to create
a distractor candidate set as illustrated on the left-
hand side of Figure 2. We include in this set the
top-n inferred words that correspond to the high-
est rule scores. These candidate words are inferred
by the target word, but not necessarily in the par-
ticular context of the carrier sentence. Therefore,
we expect this set to include both correct answers,
which would render the item unreliable, as well
as good distractors that are semantically similar to
the target word in some sense, but not in the par-
ticular sense induced by the carrier sentence.
Next, we use context-sensitive rules to generate
a distractor black-list including the top-m words
that are inferred by the target word, but this time
taking the context of the carrier sentence into con-
sideration. In this case, we expect the words in
the list to comprise only the gap-fillers that fit the
given context as illustrated on the right-hand side
of Figure 2. Such gap-fillers are correct answers
and therefore cannot be used as distractors. Fi-
nally, we subtract the black-list distractors from
the initial distractor candidate set and expect the
remaining candidates to comprise only good dis-
tractors. We consider the candidates in this final
set as our generated distractors.
</bodyText>
<page confidence="0.994361">
145
</page>
<subsectionHeader confidence="0.984161">
3.3 Distractor Ranking
</subsectionHeader>
<bodyText confidence="0.999996785714286">
In case our approach returns a large number of
good distractors, we should use ranking to select
the most challenging ones. A simple strategy is
to rely on the corpus frequency of the distractor,
where less frequent means more challenging as it
will not be known to the learner. However, this
tends to put a focus on the more obscure words
of the vocabulary while actually the more frequent
words should be trained more often. Therefore, in
this work we use the scores that were assigned to
the distractors by the context-insensitive inference
rules. Accordingly, the more similar a distractor is
to the target word, the higher rank it will get (pro-
vided that it was not in the distractor black-list).
</bodyText>
<sectionHeader confidence="0.998246" genericHeader="evaluation">
4 Experiments &amp; Results
</sectionHeader>
<bodyText confidence="0.9999024">
In our experiments we wanted to test two hy-
potheses: (i) whether context-sensitive inference
rules are able to reliably distinguish between valid
and invalid distractors, and (ii) whether the gener-
ated distractors are more challenging for language
learners than randomly chosen ones.
We used the Brown corpus (Nelson Francis and
Kuc¸era, 1964) as a source for carrier sentences and
selected medium-sized (5-12 tokens long) sen-
tences that contain a main verb. We then manu-
ally inspected this set, keeping only well-formed
sentences that are understandable by a general au-
dience without requiring too much context knowl-
edge. In a production system, this manual pro-
cess would be replaced by a sophisticated method
for obtaining good carrier sentences, but this is be-
yond the scope of this paper. Finally, for this ex-
ploratory study, we only used the first 20 selected
sentences from a much larger set of possible car-
rier sentences.
</bodyText>
<subsectionHeader confidence="0.959764">
4.1 Reliability
</subsectionHeader>
<bodyText confidence="0.999673833333333">
Our first goal was to study the effectiveness of our
approach in generating reliable items, i.e. items
where the target word is the only correct answer.
In order to minimize impact of pre-processing and
lemmatization, we provided the context-sensitive
inference rules with correctly lemmatized carrier
sentences and marked the target verbs. We found
that we get better results when using a distractor
black-list that is larger than the distractor candi-
date set, as this more aggressively filters invalid
distractors. We used the top-20 distractor black-
list and top-10 distractor candidate set, which lead
</bodyText>
<table confidence="0.816381333333333">
Only valid distractors 13/20 (65%)
Mix of valid and invalid 5/20 (25%)
Only invalid distractors 2/20 (10%)
</table>
<tableCaption confidence="0.999028">
Table 1: Reliability of items after filtering
</tableCaption>
<bodyText confidence="0.999911434782609">
to generating on average 3.3 distractors per item.
All our generated distractors were checked by
two native English speakers. We count a distrac-
tor as “invalid” if it was ruled out by at least one
annotator. Table 1 summarizes the results. We
found that in 13 of the 20 items (65%) all distrac-
tors generated by our approach were valid, while
only for 2 items all generated distractors were in-
valid. For the remaining 5 items, our approach re-
turned a mix of valid and invalid distractors. We
note that the unfiltered distractor candidate set al-
ways contained invalid distractors and in 90% of
the items it contained a higher proportion of in-
valid distractors than the filtered one. This sug-
gests that the context-sensitive inference rules are
quite effective in differentiating between the dif-
ferent senses of the verbs.
A main source of error are sentences that do not
provide enough context, e.g. because the subject
is a pronoun. In She [served] one four-year term
on the national committee, it would be acceptable
to insert sold in the context of a report on po-
litical corruption, but a more precise subject like
Barack Obama would render that reading much
more unlikely. Therefore, more emphasis should
be put on selecting better carrier sentences. Se-
lecting longer sentences that provide a richer con-
text would help to rule out more distractor candi-
dates and may also lead to better results when us-
ing the context-sensitive inference rules. However,
long sentences are also more difficult for language
learners, so there will probably be some trade-off.
A qualitative analysis of the results shows that
especially for verbs with clearly distinct senses,
our approach yields good results. For example
in He [played] basketball there while working to-
ward a law degree, our method generates the dis-
tractors compose and tune which are both related
to the “play a musical instrument” sense. An-
other example is His petition [charged] mental
cruelty, where our method generates among oth-
ers the distractors pay and collect that are both re-
lated to the “charge taxes” reading of the verb. The
ball [floated] downstream is an example where our
method did not work well. It generated the distrac-
tors glide and travel which also fit the context and
</bodyText>
<page confidence="0.997089">
146
</page>
<table confidence="0.928293666666667">
Group 1 Group 2
Control Items 0.24 f 0.12 0.20 f 0.12
Test Items 0.18 f 0.17 0.18 f 0.15
</table>
<tableCaption confidence="0.998926">
Table 2: Average error rates on our dataset
</tableCaption>
<bodyText confidence="0.9996616">
should thus not be used as distractors. The verb
float is different from the previous examples, as
all its dominant senses involve some kind of “float-
ing” even if only metaphorically used. This results
in similar senses that are harder to differentiate.
</bodyText>
<subsectionHeader confidence="0.984323">
4.2 Difficulty
</subsectionHeader>
<bodyText confidence="0.999986905660377">
Next, we wanted to examine whether our approach
leads to more challenging distractors. For that
purpose we removed the distractors that our an-
notators identified as invalid in the previous step.
We then ranked the remaining distractors accord-
ing to the scores assigned to them by the context-
sensitive inference rules and selected the top-3 dis-
tractors. If our method generated less than 3 dis-
tractors, we randomly generated additional dis-
tractors from the same frequency range as the tar-
get word.
We compared our approach with randomly se-
lected distractors that are in the same order of
magnitude with respect to corpus frequency as the
distractors generated by our method. This way we
ensure that a possible change in distractor diffi-
culty cannot simply be attributed to differences in
the learners’ familiarity with the distractor verbs
due to their corpus frequency. We note that ran-
dom selection repeatedly created invalid distrac-
tors that we needed to manually filter out. This
shows that better methods for checking the relia-
bility of items like in our approach are definitely
required.
We randomly split 52 participants (all non-
natives) into two groups, each assigned with a dif-
ferent test version. Table 2 summarizes the results.
For both groups, the first 7 test items were identi-
cal and contained only randomly selected distrac-
tors. Average error rate for these items was 0.24
(5D 0.12) for the first group, and 0.20 (5D 0.12)
for the second group, suggesting that the results of
the two groups on the remaining items can be com-
pared meaningfully. The first group was tested
on the remaining 13 items with randomly selected
distractors, while the second group got the same
items but with distractors created by our method.
Contrary to our hypothesis, the average error
rate for both groups was equal (0.18, 5D1=0.17,
5D2=0.15). One reason might be that the English
language skills of the participants (mostly com-
puter science students or faculty) were rather high,
close to the native level, as shown by the low error
rates. Furthermore, even if the participants were
more challenged by our distractors, they might
have been able to finally select the right answer
with no measurable effect on error rate. Thus, in
future work we want measure answer time instead
of average error rate, in order to counter this effect.
We also want to re-run the experiment with lower
grade students, who might not have mastered the
kind of sense distinctions that our approach is fo-
cused on.
</bodyText>
<sectionHeader confidence="0.999719" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999711727272727">
In this paper we have tackled the task of generating
challenging distractors for multiple-choice gap-fill
items. We propose to employ context-sensitive
lexical inference rules in order to generate distrac-
tors that are semantically similar to the gap target
word in some sense, but not in the particular sense
induced by the gap-fill context.
Our results suggest that our approach is quite ef-
fective, reducing the number of invalid distractors
in 90% of the cases, and fully eliminating all of
them in 65%. We did not find a difference in aver-
age error rate between distractors generated with
our method and randomly chosen distractors from
the same corpus frequency range. We conjecture
that this may be due to limitations in the setup of
our experiment.
Thus, in future work we want to re-run the ex-
periment with less experienced participants. We
also wish to measure answer time in addition to
error rate, as the distractive powers of a gap-filler
might be reflected in longer answer times more
than in higher error rates.
</bodyText>
<sectionHeader confidence="0.996514" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999504857142857">
We thank all participants of the gap-fill survey,
and Emily Jamison and Tristan Miller for their
help with the annotation study. This work was
partially supported by the European Community’s
Seventh Framework Programme (FP7/2007-2013)
under grant agreement no. 287923 (EXCITE-
MENT).
</bodyText>
<page confidence="0.997376">
147
</page>
<sectionHeader confidence="0.989921" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999515811764706">
Manish Agarwal and Prashanth Mannem. 2011. Au-
tomatic Gap-fill Question Generation from Text
Books. In Proceedings of the Sixth Workshop on In-
novative Use of NLP for Building Educational Ap-
plications, pages 56–64.
Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing Distributional Similarity in Context. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1162–1172.
Donna Gates, Margaret Mckeown, Juliet Bey, Forbes
Ave, and Ross Hall. 2011. How to Generate
Cloze Questions from Definitions: A Syntactic Ap-
proach. In Proceedings of the AAAI Fall Symposium
on Question Generation, pages 19–22.
Ayako Hoshino and Hiroshi Nakagawa. 2007. As-
sisting Cloze Test Making with a Web Application.
In Proceedings of the Society for Information Tech-
nology and Teacher Education International Confer-
ence, pages 2807– 2814.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional Distribu-
tional Similarity for Lexical Inference. Natural Lan-
guage Engineering, 16(4):359–389.
John Lee and Stephanie Seneff. 2007. Automatic
Generation of Cloze Items for Prepositions. In
Proceedings of INTERSPEECH, pages 2173–2176,
Antwerp, Belgium.
Dekang Lin and Patrick Pantel. 2001. DIRT – Discov-
ery of Inference Rules from Text. In Proceedings of
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining 2001.
Chao-lin Liu, Chun-hung Wang, and Zhao-ming Gao.
2005. Using Lexical Constraints to Enhance the
Quality of Computer-Generated Multiple-Choice
Cloze Items. Computational Linguistics and Chi-
nese Language Processing, 10(3):303–328.
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013. A Two Level
Model for Context Sensitive Inference Rules. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1331–1340, Sofia, Bulgaria.
Ruslan Mitkov, Le An Ha, Andrea Varga, and Luz
Rello. 2009. Semantic Similarity of Distractors in
Multiple-choice Tests: Extrinsic Evaluation. In Pro-
ceedings of the Workshop on Geometrical Models of
Natural Language Semantics, pages 49–56.
Jack Mostow and Hyeju Jang. 2012. Generating
Diagnostic Multiple Choice Comprehension Cloze
Questions. In Proceedings of the Seventh Workshop
on Building Educational Applications Using NLP,
pages 136–146, Stroudsburg, PA, USA.
W. Nelson Francis and Henry Kuc¸era. 1964. Manual
of Information to Accompany a Standard Corpus of
Present-day Edited American English, for use with
Digital Computers.
Juan Pino and Maxine Eskenazi. 2009. Semi-
Automatic Generation of Cloze Question Distrac-
tors Effect of Students L1. In SLaTE Workshop on
Speech and Language Technology in Education.
Juan Pino, Michael Heilman, and Maxine Eskenazi.
2008. A Selection Strategy to Improve Cloze Ques-
tion Quality. In Proceedings of the Workshop on In-
telligent Tutoring Systems for Ill-Defined Domains
at the 9th Internationnal Conference on Intelligent
Tutoring Systems.
Keisuke Sakaguchi, Yuki Arase, and Mamoru Ko-
machi. 2013. Discriminative Approach to Fill-in-
the-Blank Quiz Generation for Language Learners.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 238–242, Sofia, Bulgaria.
Simon Smith and P V S Avinesh. 2010. Gap-fill Tests
for Language Learners: Corpus-Driven Item Gener-
ation. In Proceedings of ICON-2010: 8th Interna-
tional Conference on Natural Language Processing.
Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-
mamoto. 2005. Measuring Non-native Speakers’
Proficiency of English by Using a Test with
Automatically-generated Fill-in-the-blank Ques-
tions. In Proceedings of the second workshop on
Building Educational Applications Using NLP,
EdAppsNLP 05, pages 61–68, Stroudsburg, PA,
USA.
</reference>
<page confidence="0.996817">
148
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.879167">
<title confidence="0.999617">Automatic Generation of Challenging Using Context-Sensitive Inference Rules</title>
<author confidence="0.942532">Torsten</author>
<affiliation confidence="0.988846">Language Technology University of</affiliation>
<email confidence="0.960971">torsten.zesch@uni-due.de</email>
<abstract confidence="0.99933728">Automatically generating challenging distractors for multiple-choice gap-fill items is still an unsolved problem. We propose to employ context-sensitive lexical inference rules in order to generate distractors that are semantically similar to the gap target word in some sense, but not in the particular sense induced by the gap-fill context. We hypothesize that such distractors should be particularly hard to distinguish from the correct answer. We focus on verbs as they are especially difficult to master for language learners and find that our approach is quite effective. In our test set of 20 items, our proposed method decreases the number of invalid distractors in 90% of the cases, and fully eliminates all of them in 65%. Further analysis on that dataset does not support our hypothesis regarding item difficulty as measured by average error rate of language learners. We conjecture that this may be due to limitations in our evaluation setting, which we plan to address in future work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Manish Agarwal</author>
<author>Prashanth Mannem</author>
</authors>
<title>Automatic Gap-fill Question Generation from Text Books.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>56--64</pages>
<contexts>
<context position="5404" citStr="Agarwal and Mannem, 2011" startWordPosition="861" endWordPosition="864">es with word difficulty). Pino and Eskenazi (2009) use distractors that are morphologically, orthographically, or phonetically similar (e.g. bread – beard). Another approach used in previous works to make distractors more challenging is utilizing thesauri (Sumita et al., 2005; Smith and Avinesh, 2010) or taxonomies (Hoshino and Nakagawa, 2007; Mitkov et al., 2009) to select words that are semantically similar to the target word. In addition to the target word, some approaches also consider the semantic relatedness of distractors with the whole carrier sentence or paragraph (Pino et al., 2008; Agarwal and Mannem, 2011; Mostow and Jang, 2012), i.e. they pick distractors that are from the same domain as the target word. Generally, selecting more challenging distractors usually means making them more similar to the target word. As this increases the probability that a distractor might actually be another correct answer, we need a more sophisticated approach for 1http://cl.naist.jp/nldata/lang-8/ checking the reliability of the distractor set. 2.2 Reliability Checking In order to make sure that there is only one correct answer to a gap-fill item, there needs to be a way to decide for each distractor whether it</context>
</contexts>
<marker>Agarwal, Mannem, 2011</marker>
<rawString>Manish Agarwal and Prashanth Mannem. 2011. Automatic Gap-fill Question Generation from Text Books. In Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 56–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Measuring Distributional Similarity in Context.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1162--1172</pages>
<contexts>
<context position="9695" citStr="Dinu and Lapata, 2010" startWordPosition="1582" endWordPosition="1585">es is that they are context-insensitive, i.e. they have a single score, which is not assessed with respect to the concrete context T under which they are applied. However, the appropriateness of an inference rule may in fact depend on this context. For example, ‘Microsoft acquire Skype → Microsoft purchase Skype’, is an appropriate application of the rule ‘acquire → purchase’, while ‘Children acquire skills → Children purchase skills’ is not. To address this issue, additional models were introduced that compute a different context-sensitive score per each context T, under which it is applied (Dinu and Lapata, 2010; Melamud et al., 2013). In this work, we use the resource provided by Melamud et al. (2013), which includes both context-sensitive and context-insensitive rules for over 2,000 frequent verbs.2 We use these rules to generate challenging distractors as we show next. 3.2 Distractor Selection &amp; Reliability We start with the following illustrative example to motivate our approach. While the words purchase and acquire are considered to be almost perfect synonyms in sentences like Microsoft acquires Skype and Microsoft purchases Skype, this is not true for all contexts. For example, in Children acqu</context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Measuring Distributional Similarity in Context. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1162–1172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Gates</author>
<author>Margaret Mckeown</author>
<author>Juliet Bey</author>
<author>Forbes Ave</author>
<author>Ross Hall</author>
</authors>
<title>How to Generate Cloze Questions from Definitions: A Syntactic Approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the AAAI Fall Symposium on Question Generation,</booktitle>
<pages>pages</pages>
<contexts>
<context position="3570" citStr="Gates et al., 2011" startWordPosition="567" endWordPosition="570">s work in order to place our contribution into context. 2 Previous Work The process of finding good distractors involves two steps: Candidate Selection controls the difficulty of the items, while Reliability Checking ensures that the items remain solvable, i.e. it ensures 143 Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications, pages 143–148, Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics that there is only one correct answer. We note that this work is focused on single-word distractors rather than phrases (Gates et al., 2011), and only on target isolated carrier sentences rather than longer texts as in (Mostow and Jang, 2012). 2.1 Candidates Selection In some settings the set of possible distractors is known in advance, e.g. the set of English prepositions in preposition exercises (Lee and Seneff, 2007) or a confusion set with previously known errors like {two, too, to}. Sakaguchi et al. (2013) use data from the Lang-8 platform (a corpus of manually annotated errors1) in order to determine typical learner errors and use them as distractors. However, in the common setting only the target word is known and the set o</context>
</contexts>
<marker>Gates, Mckeown, Bey, Ave, Hall, 2011</marker>
<rawString>Donna Gates, Margaret Mckeown, Juliet Bey, Forbes Ave, and Ross Hall. 2011. How to Generate Cloze Questions from Definitions: A Syntactic Approach. In Proceedings of the AAAI Fall Symposium on Question Generation, pages 19–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ayako Hoshino</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Assisting Cloze Test Making with a Web Application.</title>
<date>2007</date>
<booktitle>In Proceedings of the Society for Information Technology and Teacher Education International Conference,</booktitle>
<pages>2807--2814</pages>
<contexts>
<context position="4716" citStr="Hoshino and Nakagawa, 2007" startWordPosition="755" endWordPosition="759">ctors. However, in the common setting only the target word is known and the set of distractors needs to be automatically generated. Randomly selecting distractors is a valid strategy (Mostow and Jang, 2012), but it is only suitable for the most beginner learners. More advanced learners can easily rule out distractors that do not fit grammatically or are too unrelated semantically. Thus, more advanced approaches usually employ basic strategies, such as choosing distractors with the same part-of-speech tag as the target word, or distractors with a corpus frequency comparable to the target word (Hoshino and Nakagawa, 2007) (based on the assumption that corpus frequency roughly correlates with word difficulty). Pino and Eskenazi (2009) use distractors that are morphologically, orthographically, or phonetically similar (e.g. bread – beard). Another approach used in previous works to make distractors more challenging is utilizing thesauri (Sumita et al., 2005; Smith and Avinesh, 2010) or taxonomies (Hoshino and Nakagawa, 2007; Mitkov et al., 2009) to select words that are semantically similar to the target word. In addition to the target word, some approaches also consider the semantic relatedness of distractors w</context>
</contexts>
<marker>Hoshino, Nakagawa, 2007</marker>
<rawString>Ayako Hoshino and Hiroshi Nakagawa. 2007. Assisting Cloze Test Making with a Web Application. In Proceedings of the Society for Information Technology and Teacher Education International Conference, pages 2807– 2814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional Distributional Similarity for Lexical Inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="8869" citStr="Kotlerman et al., 2010" startWordPosition="1447" endWordPosition="1450">A lexical inference rule ‘LHS → RHS’, such as ‘acquire → purchase’, specifies a directional inference relation between two words (or terms). A rule can be applied when its LHS matches a word in a text T, and then that word is substituted for RHS, yielding the modified text H. For example, applying the rule above to “Microsoft acquired Skype”, yields “Microsoft purchased Skype”. If the rule is true then the meaning of H is inferred from the meaning of T. A popular way to learn lexical inference rules in an unsupervised setting is by using distributional similarity models (Lin and Pantel, 2001; Kotlerman et al., 2010). Under this approach, target words are represented as vectors of context features, and the score of a rule between two target words is based on vector arithmetics. One of the main shortcomings of such rules is that they are context-insensitive, i.e. they have a single score, which is not assessed with respect to the concrete context T under which they are applied. However, the appropriateness of an inference rule may in fact depend on this context. For example, ‘Microsoft acquire Skype → Microsoft purchase Skype’, is an appropriate application of the rule ‘acquire → purchase’, while ‘Children</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional Distributional Similarity for Lexical Inference. Natural Language Engineering, 16(4):359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Stephanie Seneff</author>
</authors>
<title>Automatic Generation of Cloze Items for Prepositions.</title>
<date>2007</date>
<booktitle>In Proceedings of INTERSPEECH,</booktitle>
<pages>2173--2176</pages>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="3853" citStr="Lee and Seneff, 2007" startWordPosition="613" endWordPosition="616">eedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications, pages 143–148, Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics that there is only one correct answer. We note that this work is focused on single-word distractors rather than phrases (Gates et al., 2011), and only on target isolated carrier sentences rather than longer texts as in (Mostow and Jang, 2012). 2.1 Candidates Selection In some settings the set of possible distractors is known in advance, e.g. the set of English prepositions in preposition exercises (Lee and Seneff, 2007) or a confusion set with previously known errors like {two, too, to}. Sakaguchi et al. (2013) use data from the Lang-8 platform (a corpus of manually annotated errors1) in order to determine typical learner errors and use them as distractors. However, in the common setting only the target word is known and the set of distractors needs to be automatically generated. Randomly selecting distractors is a valid strategy (Mostow and Jang, 2012), but it is only suitable for the most beginner learners. More advanced learners can easily rule out distractors that do not fit grammatically or are too unre</context>
<context position="6199" citStr="Lee and Seneff, 2007" startWordPosition="991" endWordPosition="994">them more similar to the target word. As this increases the probability that a distractor might actually be another correct answer, we need a more sophisticated approach for 1http://cl.naist.jp/nldata/lang-8/ checking the reliability of the distractor set. 2.2 Reliability Checking In order to make sure that there is only one correct answer to a gap-fill item, there needs to be a way to decide for each distractor whether it fits into the context of the carrier sentence or not. In those cases, where we have a limited list of potential target words and distractors, e.g. in preposition exercises (Lee and Seneff, 2007), a supervised classifier can be trained to do this job. Given enough training data, this approach yields very high precision, but it cannot be easily applied to open word classes like nouns or verbs, which are much larger and dynamic in nature. When we do not have a closed list of potential distractors at hand, one way to perform reliability checking is by considering collocations involving the target word (Pino et al., 2008; Smith and Avinesh, 2010). For example, if the target word is strong, we can find the collocation strong tea. Then we can use powerful as a distractor because it is seman</context>
</contexts>
<marker>Lee, Seneff, 2007</marker>
<rawString>John Lee and Stephanie Seneff. 2007. Automatic Generation of Cloze Items for Prepositions. In Proceedings of INTERSPEECH, pages 2173–2176, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT – Discovery of Inference Rules from Text.</title>
<date>2001</date>
<booktitle>In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining</booktitle>
<contexts>
<context position="8844" citStr="Lin and Pantel, 2001" startWordPosition="1443" endWordPosition="1446">itive Inference Rules A lexical inference rule ‘LHS → RHS’, such as ‘acquire → purchase’, specifies a directional inference relation between two words (or terms). A rule can be applied when its LHS matches a word in a text T, and then that word is substituted for RHS, yielding the modified text H. For example, applying the rule above to “Microsoft acquired Skype”, yields “Microsoft purchased Skype”. If the rule is true then the meaning of H is inferred from the meaning of T. A popular way to learn lexical inference rules in an unsupervised setting is by using distributional similarity models (Lin and Pantel, 2001; Kotlerman et al., 2010). Under this approach, target words are represented as vectors of context features, and the score of a rule between two target words is based on vector arithmetics. One of the main shortcomings of such rules is that they are context-insensitive, i.e. they have a single score, which is not assessed with respect to the concrete context T under which they are applied. However, the appropriateness of an inference rule may in fact depend on this context. For example, ‘Microsoft acquire Skype → Microsoft purchase Skype’, is an appropriate application of the rule ‘acquire → p</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT – Discovery of Inference Rules from Text. In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao-lin Liu</author>
<author>Chun-hung Wang</author>
<author>Zhao-ming Gao</author>
</authors>
<date>2005</date>
<booktitle>Using Lexical Constraints to Enhance the Quality of Computer-Generated Multiple-Choice Cloze Items. Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="1774" citStr="Liu et al., 2005" startWordPosition="283" endWordPosition="286">an to address in future work. 1 Introduction Multiple-choice gap-fill items as illustrated in Figure 1 are frequently used for both testing language proficiency and as a learning device. Each item consists of a carrier sentence that provides the context to a target word. The target word is blanked and presented as one possible gap-fill answer together with a certain number (usually 3) of distractors. Given a desired target word, carrier sentences containing it can be automatically selected from a corpus. Some methods even select only sentences where the target word is used in a certain sense (Liu et al., 2005). Then, the main problem is to pick challenging distractors that are Oren Melamud Computer Science Department Bar-Ilan University melamuo@cs.biu.ac.il Figure 1: Multiple-choice gap-fill item. reasonably hard to distinguish from the correct answer (i.e. the target word) on one hand, yet cannot be considered as correct answers on the other. In this paper we propose to generate distractors that are semantically similar to the gap target word in some sense, but not in the particular sense induced by the gap-fill context, thereby making them difficult to distinguish from the target word. For exampl</context>
</contexts>
<marker>Liu, Wang, Gao, 2005</marker>
<rawString>Chao-lin Liu, Chun-hung Wang, and Zhao-ming Gao. 2005. Using Lexical Constraints to Enhance the Quality of Computer-Generated Multiple-Choice Cloze Items. Computational Linguistics and Chinese Language Processing, 10(3):303–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Melamud</author>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
<author>Idan Szpektor</author>
</authors>
<title>A Two Level Model for Context Sensitive Inference Rules.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1331--1340</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="9718" citStr="Melamud et al., 2013" startWordPosition="1586" endWordPosition="1589">text-insensitive, i.e. they have a single score, which is not assessed with respect to the concrete context T under which they are applied. However, the appropriateness of an inference rule may in fact depend on this context. For example, ‘Microsoft acquire Skype → Microsoft purchase Skype’, is an appropriate application of the rule ‘acquire → purchase’, while ‘Children acquire skills → Children purchase skills’ is not. To address this issue, additional models were introduced that compute a different context-sensitive score per each context T, under which it is applied (Dinu and Lapata, 2010; Melamud et al., 2013). In this work, we use the resource provided by Melamud et al. (2013), which includes both context-sensitive and context-insensitive rules for over 2,000 frequent verbs.2 We use these rules to generate challenging distractors as we show next. 3.2 Distractor Selection &amp; Reliability We start with the following illustrative example to motivate our approach. While the words purchase and acquire are considered to be almost perfect synonyms in sentences like Microsoft acquires Skype and Microsoft purchases Skype, this is not true for all contexts. For example, in Children acquire skills vs. Children</context>
</contexts>
<marker>Melamud, Berant, Dagan, Goldberger, Szpektor, 2013</marker>
<rawString>Oren Melamud, Jonathan Berant, Ido Dagan, Jacob Goldberger, and Idan Szpektor. 2013. A Two Level Model for Context Sensitive Inference Rules. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1331–1340, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
<author>Le An Ha</author>
<author>Andrea Varga</author>
<author>Luz Rello</author>
</authors>
<title>Semantic Similarity of Distractors in Multiple-choice Tests: Extrinsic Evaluation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="5146" citStr="Mitkov et al., 2009" startWordPosition="818" endWordPosition="821">trategies, such as choosing distractors with the same part-of-speech tag as the target word, or distractors with a corpus frequency comparable to the target word (Hoshino and Nakagawa, 2007) (based on the assumption that corpus frequency roughly correlates with word difficulty). Pino and Eskenazi (2009) use distractors that are morphologically, orthographically, or phonetically similar (e.g. bread – beard). Another approach used in previous works to make distractors more challenging is utilizing thesauri (Sumita et al., 2005; Smith and Avinesh, 2010) or taxonomies (Hoshino and Nakagawa, 2007; Mitkov et al., 2009) to select words that are semantically similar to the target word. In addition to the target word, some approaches also consider the semantic relatedness of distractors with the whole carrier sentence or paragraph (Pino et al., 2008; Agarwal and Mannem, 2011; Mostow and Jang, 2012), i.e. they pick distractors that are from the same domain as the target word. Generally, selecting more challenging distractors usually means making them more similar to the target word. As this increases the probability that a distractor might actually be another correct answer, we need a more sophisticated approac</context>
</contexts>
<marker>Mitkov, Ha, Varga, Rello, 2009</marker>
<rawString>Ruslan Mitkov, Le An Ha, Andrea Varga, and Luz Rello. 2009. Semantic Similarity of Distractors in Multiple-choice Tests: Extrinsic Evaluation. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack Mostow</author>
<author>Hyeju Jang</author>
</authors>
<title>Generating Diagnostic Multiple Choice Comprehension Cloze Questions.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>136--146</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3672" citStr="Mostow and Jang, 2012" startWordPosition="584" endWordPosition="587"> distractors involves two steps: Candidate Selection controls the difficulty of the items, while Reliability Checking ensures that the items remain solvable, i.e. it ensures 143 Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications, pages 143–148, Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics that there is only one correct answer. We note that this work is focused on single-word distractors rather than phrases (Gates et al., 2011), and only on target isolated carrier sentences rather than longer texts as in (Mostow and Jang, 2012). 2.1 Candidates Selection In some settings the set of possible distractors is known in advance, e.g. the set of English prepositions in preposition exercises (Lee and Seneff, 2007) or a confusion set with previously known errors like {two, too, to}. Sakaguchi et al. (2013) use data from the Lang-8 platform (a corpus of manually annotated errors1) in order to determine typical learner errors and use them as distractors. However, in the common setting only the target word is known and the set of distractors needs to be automatically generated. Randomly selecting distractors is a valid strategy </context>
<context position="5428" citStr="Mostow and Jang, 2012" startWordPosition="865" endWordPosition="868">Pino and Eskenazi (2009) use distractors that are morphologically, orthographically, or phonetically similar (e.g. bread – beard). Another approach used in previous works to make distractors more challenging is utilizing thesauri (Sumita et al., 2005; Smith and Avinesh, 2010) or taxonomies (Hoshino and Nakagawa, 2007; Mitkov et al., 2009) to select words that are semantically similar to the target word. In addition to the target word, some approaches also consider the semantic relatedness of distractors with the whole carrier sentence or paragraph (Pino et al., 2008; Agarwal and Mannem, 2011; Mostow and Jang, 2012), i.e. they pick distractors that are from the same domain as the target word. Generally, selecting more challenging distractors usually means making them more similar to the target word. As this increases the probability that a distractor might actually be another correct answer, we need a more sophisticated approach for 1http://cl.naist.jp/nldata/lang-8/ checking the reliability of the distractor set. 2.2 Reliability Checking In order to make sure that there is only one correct answer to a gap-fill item, there needs to be a way to decide for each distractor whether it fits into the context o</context>
</contexts>
<marker>Mostow, Jang, 2012</marker>
<rawString>Jack Mostow and Hyeju Jang. 2012. Generating Diagnostic Multiple Choice Comprehension Cloze Questions. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 136–146, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Nelson Francis</author>
<author>Henry Kuc¸era</author>
</authors>
<title>Manual of Information to Accompany a Standard Corpus of Present-day Edited American English, for use with Digital Computers.</title>
<date>1964</date>
<marker>Francis, Kuc¸era, 1964</marker>
<rawString>W. Nelson Francis and Henry Kuc¸era. 1964. Manual of Information to Accompany a Standard Corpus of Present-day Edited American English, for use with Digital Computers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan Pino</author>
<author>Maxine Eskenazi</author>
</authors>
<title>SemiAutomatic Generation of Cloze Question Distractors Effect of Students L1.</title>
<date>2009</date>
<booktitle>In SLaTE Workshop on Speech and Language Technology in Education.</booktitle>
<contexts>
<context position="4830" citStr="Pino and Eskenazi (2009)" startWordPosition="772" endWordPosition="775">lly generated. Randomly selecting distractors is a valid strategy (Mostow and Jang, 2012), but it is only suitable for the most beginner learners. More advanced learners can easily rule out distractors that do not fit grammatically or are too unrelated semantically. Thus, more advanced approaches usually employ basic strategies, such as choosing distractors with the same part-of-speech tag as the target word, or distractors with a corpus frequency comparable to the target word (Hoshino and Nakagawa, 2007) (based on the assumption that corpus frequency roughly correlates with word difficulty). Pino and Eskenazi (2009) use distractors that are morphologically, orthographically, or phonetically similar (e.g. bread – beard). Another approach used in previous works to make distractors more challenging is utilizing thesauri (Sumita et al., 2005; Smith and Avinesh, 2010) or taxonomies (Hoshino and Nakagawa, 2007; Mitkov et al., 2009) to select words that are semantically similar to the target word. In addition to the target word, some approaches also consider the semantic relatedness of distractors with the whole carrier sentence or paragraph (Pino et al., 2008; Agarwal and Mannem, 2011; Mostow and Jang, 2012), </context>
</contexts>
<marker>Pino, Eskenazi, 2009</marker>
<rawString>Juan Pino and Maxine Eskenazi. 2009. SemiAutomatic Generation of Cloze Question Distractors Effect of Students L1. In SLaTE Workshop on Speech and Language Technology in Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan Pino</author>
<author>Michael Heilman</author>
<author>Maxine Eskenazi</author>
</authors>
<title>A Selection Strategy to Improve Cloze Question Quality.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Intelligent Tutoring Systems for Ill-Defined Domains at the 9th Internationnal Conference on Intelligent Tutoring Systems.</booktitle>
<contexts>
<context position="5378" citStr="Pino et al., 2008" startWordPosition="857" endWordPosition="860">cy roughly correlates with word difficulty). Pino and Eskenazi (2009) use distractors that are morphologically, orthographically, or phonetically similar (e.g. bread – beard). Another approach used in previous works to make distractors more challenging is utilizing thesauri (Sumita et al., 2005; Smith and Avinesh, 2010) or taxonomies (Hoshino and Nakagawa, 2007; Mitkov et al., 2009) to select words that are semantically similar to the target word. In addition to the target word, some approaches also consider the semantic relatedness of distractors with the whole carrier sentence or paragraph (Pino et al., 2008; Agarwal and Mannem, 2011; Mostow and Jang, 2012), i.e. they pick distractors that are from the same domain as the target word. Generally, selecting more challenging distractors usually means making them more similar to the target word. As this increases the probability that a distractor might actually be another correct answer, we need a more sophisticated approach for 1http://cl.naist.jp/nldata/lang-8/ checking the reliability of the distractor set. 2.2 Reliability Checking In order to make sure that there is only one correct answer to a gap-fill item, there needs to be a way to decide for </context>
<context position="6628" citStr="Pino et al., 2008" startWordPosition="1068" endWordPosition="1071">nto the context of the carrier sentence or not. In those cases, where we have a limited list of potential target words and distractors, e.g. in preposition exercises (Lee and Seneff, 2007), a supervised classifier can be trained to do this job. Given enough training data, this approach yields very high precision, but it cannot be easily applied to open word classes like nouns or verbs, which are much larger and dynamic in nature. When we do not have a closed list of potential distractors at hand, one way to perform reliability checking is by considering collocations involving the target word (Pino et al., 2008; Smith and Avinesh, 2010). For example, if the target word is strong, we can find the collocation strong tea. Then we can use powerful as a distractor because it is semantically similar to strong, yet *powerful tea is not a valid collocation. This approach is effective, but requires strong collocations to discriminate between valid and invalid distractors. Therefore it cannot be used with carrier sentences that do not contain strong collocations, such as the sentence in Figure 1. Sumita et al. (2005) apply a simple web search approach to judge the reliability of an item. They check whether th</context>
</contexts>
<marker>Pino, Heilman, Eskenazi, 2008</marker>
<rawString>Juan Pino, Michael Heilman, and Maxine Eskenazi. 2008. A Selection Strategy to Improve Cloze Question Quality. In Proceedings of the Workshop on Intelligent Tutoring Systems for Ill-Defined Domains at the 9th Internationnal Conference on Intelligent Tutoring Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keisuke Sakaguchi</author>
<author>Yuki Arase</author>
<author>Mamoru Komachi</author>
</authors>
<title>Discriminative Approach to Fill-inthe-Blank Quiz Generation for Language Learners.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>238--242</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="3946" citStr="Sakaguchi et al. (2013)" startWordPosition="629" endWordPosition="632">s, pages 143–148, Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics that there is only one correct answer. We note that this work is focused on single-word distractors rather than phrases (Gates et al., 2011), and only on target isolated carrier sentences rather than longer texts as in (Mostow and Jang, 2012). 2.1 Candidates Selection In some settings the set of possible distractors is known in advance, e.g. the set of English prepositions in preposition exercises (Lee and Seneff, 2007) or a confusion set with previously known errors like {two, too, to}. Sakaguchi et al. (2013) use data from the Lang-8 platform (a corpus of manually annotated errors1) in order to determine typical learner errors and use them as distractors. However, in the common setting only the target word is known and the set of distractors needs to be automatically generated. Randomly selecting distractors is a valid strategy (Mostow and Jang, 2012), but it is only suitable for the most beginner learners. More advanced learners can easily rule out distractors that do not fit grammatically or are too unrelated semantically. Thus, more advanced approaches usually employ basic strategies, such as c</context>
</contexts>
<marker>Sakaguchi, Arase, Komachi, 2013</marker>
<rawString>Keisuke Sakaguchi, Yuki Arase, and Mamoru Komachi. 2013. Discriminative Approach to Fill-inthe-Blank Quiz Generation for Language Learners. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 238–242, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Smith</author>
<author>P V S Avinesh</author>
</authors>
<title>Gap-fill Tests for Language Learners: Corpus-Driven Item Generation.</title>
<date>2010</date>
<booktitle>In Proceedings of ICON-2010: 8th International Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="5082" citStr="Smith and Avinesh, 2010" startWordPosition="808" endWordPosition="811"> semantically. Thus, more advanced approaches usually employ basic strategies, such as choosing distractors with the same part-of-speech tag as the target word, or distractors with a corpus frequency comparable to the target word (Hoshino and Nakagawa, 2007) (based on the assumption that corpus frequency roughly correlates with word difficulty). Pino and Eskenazi (2009) use distractors that are morphologically, orthographically, or phonetically similar (e.g. bread – beard). Another approach used in previous works to make distractors more challenging is utilizing thesauri (Sumita et al., 2005; Smith and Avinesh, 2010) or taxonomies (Hoshino and Nakagawa, 2007; Mitkov et al., 2009) to select words that are semantically similar to the target word. In addition to the target word, some approaches also consider the semantic relatedness of distractors with the whole carrier sentence or paragraph (Pino et al., 2008; Agarwal and Mannem, 2011; Mostow and Jang, 2012), i.e. they pick distractors that are from the same domain as the target word. Generally, selecting more challenging distractors usually means making them more similar to the target word. As this increases the probability that a distractor might actually</context>
<context position="6654" citStr="Smith and Avinesh, 2010" startWordPosition="1072" endWordPosition="1075">the carrier sentence or not. In those cases, where we have a limited list of potential target words and distractors, e.g. in preposition exercises (Lee and Seneff, 2007), a supervised classifier can be trained to do this job. Given enough training data, this approach yields very high precision, but it cannot be easily applied to open word classes like nouns or verbs, which are much larger and dynamic in nature. When we do not have a closed list of potential distractors at hand, one way to perform reliability checking is by considering collocations involving the target word (Pino et al., 2008; Smith and Avinesh, 2010). For example, if the target word is strong, we can find the collocation strong tea. Then we can use powerful as a distractor because it is semantically similar to strong, yet *powerful tea is not a valid collocation. This approach is effective, but requires strong collocations to discriminate between valid and invalid distractors. Therefore it cannot be used with carrier sentences that do not contain strong collocations, such as the sentence in Figure 1. Sumita et al. (2005) apply a simple web search approach to judge the reliability of an item. They check whether the carrier sentence with th</context>
</contexts>
<marker>Smith, Avinesh, 2010</marker>
<rawString>Simon Smith and P V S Avinesh. 2010. Gap-fill Tests for Language Learners: Corpus-Driven Item Generation. In Proceedings of ICON-2010: 8th International Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eiichiro Sumita</author>
<author>Fumiaki Sugaya</author>
<author>Seiichi Yamamoto</author>
</authors>
<title>Measuring Non-native Speakers’ Proficiency of English by Using a Test with Automatically-generated Fill-in-the-blank Questions.</title>
<date>2005</date>
<booktitle>In Proceedings of the second workshop on Building Educational Applications Using NLP, EdAppsNLP 05,</booktitle>
<pages>61--68</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5056" citStr="Sumita et al., 2005" startWordPosition="804" endWordPosition="807"> or are too unrelated semantically. Thus, more advanced approaches usually employ basic strategies, such as choosing distractors with the same part-of-speech tag as the target word, or distractors with a corpus frequency comparable to the target word (Hoshino and Nakagawa, 2007) (based on the assumption that corpus frequency roughly correlates with word difficulty). Pino and Eskenazi (2009) use distractors that are morphologically, orthographically, or phonetically similar (e.g. bread – beard). Another approach used in previous works to make distractors more challenging is utilizing thesauri (Sumita et al., 2005; Smith and Avinesh, 2010) or taxonomies (Hoshino and Nakagawa, 2007; Mitkov et al., 2009) to select words that are semantically similar to the target word. In addition to the target word, some approaches also consider the semantic relatedness of distractors with the whole carrier sentence or paragraph (Pino et al., 2008; Agarwal and Mannem, 2011; Mostow and Jang, 2012), i.e. they pick distractors that are from the same domain as the target word. Generally, selecting more challenging distractors usually means making them more similar to the target word. As this increases the probability that a</context>
<context position="7134" citStr="Sumita et al. (2005)" startWordPosition="1154" endWordPosition="1157">ne way to perform reliability checking is by considering collocations involving the target word (Pino et al., 2008; Smith and Avinesh, 2010). For example, if the target word is strong, we can find the collocation strong tea. Then we can use powerful as a distractor because it is semantically similar to strong, yet *powerful tea is not a valid collocation. This approach is effective, but requires strong collocations to discriminate between valid and invalid distractors. Therefore it cannot be used with carrier sentences that do not contain strong collocations, such as the sentence in Figure 1. Sumita et al. (2005) apply a simple web search approach to judge the reliability of an item. They check whether the carrier sentence with the target word replaced by the distractor can be found on the web. If such a sentence is found, the distractor is discarded. We note that the applicability of this approach is limited, as finding exact matches for such artificial sentences can be unlikely due to sparseness of natural languages. Therefore not finding an exact match does not necessarily rule out the possibility of an invalid distractor. 3 Automatic Generation of Challenging Distractors Our goal is to automatical</context>
</contexts>
<marker>Sumita, Sugaya, Yamamoto, 2005</marker>
<rawString>Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Yamamoto. 2005. Measuring Non-native Speakers’ Proficiency of English by Using a Test with Automatically-generated Fill-in-the-blank Questions. In Proceedings of the second workshop on Building Educational Applications Using NLP, EdAppsNLP 05, pages 61–68, Stroudsburg, PA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
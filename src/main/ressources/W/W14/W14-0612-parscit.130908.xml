<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.235664">
<title confidence="0.996191">
CorA: A web-based annotation tool for historical
and other non-standard language data
</title>
<author confidence="0.998734">
Marcel Bollmann, Florian Petran, Stefanie Dipper, Julia Krasselt
</author>
<affiliation confidence="0.858705">
Department of Linguistics
Ruhr-University Bochum, 44780 Bochum, Germany
</affiliation>
<email confidence="0.996209">
{bollmann|petran|dipper|krasselt}@linguistics.rub.de
</email>
<sectionHeader confidence="0.993849" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999937714285714">
We present CorA, a web-based annotation
tool for manual annotation of historical and
other non-standard language data. It allows
for editing the primary data and modify-
ing token boundaries during the annotation
process. Further, it supports immediate re-
training of taggers on newly annotated data.
</bodyText>
<sectionHeader confidence="0.806055" genericHeader="keywords">
1 Introduction1
</sectionHeader>
<bodyText confidence="0.999959071428571">
In recent years, the focus of research in natural
language processing has shifted from highly stan-
dardized text types, such as newspaper texts, to text
types that often infringe orthographic, grammatical
and stylistic norms normally associated with writ-
ten language. Prime examples are language data
produced in the context of computer-mediated com-
munication (CMC), such as Twitter or SMS data,
or contributions in chat rooms. Further examples
are data produced by learners or historical texts.
Tools trained on standardized data perform con-
siderably worse on “non-standard varieties” such
as internet data (cf. Giesbrecht and Evert (2009)’s
work on tagging the web or Foster et al. (2011)’s
results for parsing Twitter data) or historical lan-
guage data (Rayson et al., 2007; Scheible et al.,
2011). This can mainly be attributed to the facts
that tools are applied out of domain, or only small
amounts of manually-annotated training data are
available.
A more fundamental problem is that common
and established methods and categories for lan-
guage analysis often do not fit the phenomena oc-
curring in non-standard data. For instance, gram-
maticalization is a process of language evolution
where new parts of speech are created or words
switch from one class to another. It is difficult to
draw strict categorial boundaries between words
</bodyText>
<footnote confidence="0.996533">
1The research reported here was financed by Deutsche
Forschungsgemeinschaft (DFG), Grant DI 1558/5-1.
</footnote>
<bodyText confidence="0.999328434782609">
that take part in a continuous smooth transition of
categories. Factors like these can also affect the
way the data should be tokenized, along with other
problems such as the lack of a fixed orthography.
In the light of the above, we developed a web-
based tool for manual annotation of non-standard
data. It allows for editing the primary data, e.g.
for correcting OCR errors of historical texts, or
for modifying token boundaries during the annota-
tion process. Furthermore, it supports immediate
retraining of taggers on newly annotated data, to
attenuate the problem of sparse training data.
CorA is currently used in several projects that
annotate historical data, and one project that ana-
lyzes chat data. So far, about 200,000 tokens in
84 texts have been annotated in CorA. Once the
annotation process is completed, the transcriptions
and their annotations are imported into the ANNIS
corpus tool (Zeldes et al., 2009) where they can be
searched and visualized.
The paper focuses on the annotation of historical
data. Sec. 2 presents the tool, and Sec. 3 describes
the data model. Sec. 4 concludes.
</bodyText>
<sectionHeader confidence="0.983752" genericHeader="introduction">
2 Tool Description
</sectionHeader>
<bodyText confidence="0.9998221">
CorA uses a web-based architecture:2 All data
is stored on a server, while users can access and
edit annotations from anywhere using their web
browser. This approach greatly simplifies collabo-
rative work within a project, as it ensures that all
users are working on the same version of the data
at all times, and requires no software installation
on the user’s side. Users can be assigned to indi-
vidual project groups and are only able to access
documents within their group(s).
</bodyText>
<subsectionHeader confidence="0.995065">
2.1 The annotation editor
</subsectionHeader>
<bodyText confidence="0.943067">
All annotation in CorA is done on a token level;
the currently supported annotation types are part-
</bodyText>
<footnote confidence="0.976814">
2It implements a standard AJAX architecture using PHP 5,
MySQL, and JavaScript.
</footnote>
<page confidence="0.969652">
86
</page>
<note confidence="0.9624025">
Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 86–90,
Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999817">
Figure 1: Web interface of CorA showing the annotation editor
</figureCaption>
<bodyText confidence="0.986467363636364">
of-speech tags, morphology tags, lemmatization,
and (spelling) normalization. The tool is designed
to increase productivity for these particular an-
notation tasks, while sacrificing some amount of
flexibility (e.g., using different annotation layers,
or annotating spans of tokens). Note that this is
mainly a restriction of the web interface; the under-
lying database structure is much more flexible (cf.
Sec. 3), facilitating the later addition of other types
of annotation, if desired.
Tokens are displayed vertically, i.e., one token
per line. This way, the annotations also line up
vertically and are always within view. Addition-
ally, a horizontal text preview can be displayed at
the bottom of the screen, which makes it easier
to read a continuous text passage. Fig. 1 shows a
sample screenshot of the editor window.3 Users
can customize the editor, e.g. by hiding selected
columns.
Parts-of-speech and morphology Within the
editor, both POS and morphology tags can be se-
lected from a dropdown box, which has the ad-
vantage of allowing both mouse-based and faster
keyboard-based input. Tagsets can be defined in-
dividually for each text. If morphology tags are
used, the selection of tags in the dropdown box is
restricted by the chosen POS tag.
3The user interface is only available in German at the time
of writing, but an English version is planned.
Lemmatization Lemma forms are entered into
a text field, which can optionally be linked to a
pre-defined lexicon from which it retrieves auto-
completion suggestions. Furthermore, if an identi-
cal token has already been annotated with a lemma
form elsewhere within the same project, that lemma
is always displayed as a highlighted suggestion.
Normalization For corpora of non-standard lan-
guage varieties, spelling normalization is often
found as an annotation layer, see, e.g., Scheible
et al. (2011) for historical data and Reznicek et al.
(2013) for learner data.
In addition to normalization, an optional mod-
ernization layer can be used that defaults to the
content of the normalization field. The normaliza-
tion layer can be used for standardizing spelling,
and the modernization layer for standardizing in-
flection and semantics (Bollmann et al., 2012).
Meta information CorA features a progress indi-
cator which can be used to mark annotations as ver-
ified (see the green bar in Fig. 1). Besides serving
as a visual aid for the annotator, it is also used for
the automatic annotation component (cf. Sec. 2.2).
Additionally, tokens can be marked as needing fur-
ther review (indicated with a red checkbox), and
comments can be added.
</bodyText>
<subsectionHeader confidence="0.992421">
2.2 Automatic annotation
</subsectionHeader>
<bodyText confidence="0.9206225">
CorA supports (semi-)automatic annotation by in-
tegrating external annotation software on the server
</bodyText>
<page confidence="0.99529">
87
</page>
<bodyText confidence="0.999931210526316">
side. Currently, RFTagger (Schmid and Laws,
2008) and the Norma tool for automatic normaliza-
tion (Bollmann, 2012) are supported, but in princi-
ple any other annotation tool can be integrated as
well. The “retraining” feature collects all verified
annotations from a project and feeds them to the
tools’ training functions. The user is then able to
invoke the automatic annotation process using the
newly trained parametrizations, which causes all
tokens not yet marked as verified to be overwritten
with the new annotations.
The retraining module is particularly relevant for
non-standard language varieties where appropriate
language models may not be available. The idea
is that as more data is manually annotated within
a corpus, the performance of automatic annotation
tools increases when retrained on that data. This
in turn makes it desirable to re-apply the automatic
tools during the annotation process.
</bodyText>
<subsectionHeader confidence="0.999329">
2.3 Editing primary data
</subsectionHeader>
<bodyText confidence="0.999987916666667">
In diplomatic transcriptions of historical
manuscripts, the transcripts reproduce the
manuscripts in the most accurate way, by encoding
all relevant details of special graphemes and
diacritics, and also preserving layout information.
Transcribers often use ASCII-based encodings for
special characters, e.g., the dollar sign $ in place
of a long s (‘f’).
The data model of CorA (cf. Sec. 3) distin-
guishes between different types of token representa-
tions. In the annotation editor, the user can choose
to display either the original transcription layer or
the UTF-8 representation.
If an error in the primary data—e.g., a transcrip-
tion error or wrong tokenization—is noticed during
the annotation, it can be corrected directly within
the editor. CorA provides functionality to edit, add,
or delete existing tokens. Furthermore, external
scripts can be embedded to process any changes,
by checking an edited token for validity (e.g., if
tokens need to conform to a certain transcription
format), or generating the UTF-8 representation
by interpreting special characters (e.g., mapping $
to 0.
</bodyText>
<subsectionHeader confidence="0.995611">
2.4 Comparison to related tools
</subsectionHeader>
<bodyText confidence="0.9999784">
There is a range of annotation tools that can be
used for enriching data with different kinds of an-
notations. Prominent examples are GATE, EX-
MARaLDA, MMAX2, brat, and WebAnno.4 Many
annotation projects nowadays require distributed
collaborative working of multiple parties. The cur-
rently preferred solution is to use a tool with an
underlying database which is operated through a
standard web-browser. Among the tools above,
only brat and WebAnno are web-based tools. Com-
pared to CorA, these tools are more flexible in
that they support more annotation layers and more
complex (e.g., multi-word) annotations. WebAnno,
in addition, offers facilities for measuring inter-
annotator agreement and data curation. However,
brat and WebAnno do not allow edits to the source
document from within the tool, which is particu-
larly relevant for non-standard language varieties.
Similarly, they do not support retraining on newly
annotated data.
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="method">
3 Data Model
</sectionHeader>
<bodyText confidence="0.999991333333333">
The requirements described in Sec. 2 present vari-
ous challenges to the data storage, which necessi-
tated the development of our own data model. A
data model in this context is a conceptual model
of the data structure that allows serialization into
various representations such as XML or databases.
Such a model also allows for easy conversion be-
tween serializations and hence facilitates interop-
erability with existing formats and tools. The
complex, multi-layered layout, the differences in
tokenization, and the fine-grained description of
graphematic pecularities in the primary data cannot
be captured well using existing formats. For exam-
ple, tokenization differences as they are handled
by formats such as &lt;tiger2/&gt; (Bosch et al., 2012)
pertain only to the contraction of underlying units
to original forms, and not the other way around.
This means that while a conversion in such formats
is easily possible, some of the data structure that
is captured by our model is necessarily lost in the
process. To come up with a data model that min-
imizes redundancy and allows for flexibility and
extensibility, and accomodates the work flow of
our transcriptors and annotators, we employed nor-
malization techniques from database development.
A slightly simplified version of the data model is
shown in Fig. 2.
</bodyText>
<footnote confidence="0.9985478">
4GATE:http://gate.ac.uk/
EXMARaLDA: http://www.exmaralda.org/
MMAX2:http://mmax2.sourceforge.net/
brat: http://brat.nlplab.org/
WebAnno:https://code.google.com/p/webanno/
</footnote>
<page confidence="0.998103">
88
</page>
<figureCaption confidence="0.997464">
Figure 2: Data model used for CorA
</figureCaption>
<bodyText confidence="0.999939787878788">
Token and Text The model is centered around
two units, a text and a token. A token is a virtual
unit that can manifest in two ways, the diplomatic
token and the modern token, each of which has
a one-to-many relation with a token (cf. Fig. 3).
Diplomatic tokens are tokens as they appear in
the original, historical text, while modern tokens
mirror modern conventions for token boundaries,
representing suitable units for further annotations,
e.g. with POS tags. All physical layout information
on the other hand relates to the diplomatic token.
The text is the entirety of a transcribed document
that can be partitioned in various ways. The layout
is captured by its relation to the page, column, and
line, which in turn relate to the diplomatic tokens.
Furthermore, a text can be assigned one or more
tagsets. The tagsets in turn can be open, such as
lemmatization tags, or closed, such as POS tags.
Each text can be assigned different tagsets.
Extensions In addition, the data model also al-
lows for the import of markup annotations with the
texts, which may denote layout-related or linguistic
peculiarities encoded by the transcriptors, as well
as information about its annotation status such as
progress, or dubious annotations. The model is
easily extendable for user management that can tie
in to the text table, e.g., a user can be set as owner
or creator of a text.
As XML serialization is not optimized for data
which is not strictly hierarchically structured, stor-
age and retrieval is rather inefficient, and extensions
are not easily possible. For this reason, we chose
to implement the application with an SQL database
</bodyText>
<figure confidence="0.998104">
&lt;token&gt;
&lt;!-- diplomatic tokenization --&gt;
&lt;dipl trans=&amp;quot;ober&amp;quot;/&gt;
&lt;dipl trans=&amp;quot;czugemich&amp;quot;/&gt;
&lt;!-- modern tokenization --&gt;
&lt;mod trans=&amp;quot;oberczuge&amp;quot;&gt;
&lt;norm tag=&amp;quot;überzeuge&amp;quot;/&gt;
&lt;pos tag=&amp;quot;VVIMP.Sg&amp;quot;/&gt;
&lt;/mod&gt;
&lt;mod trans=&amp;quot;mich&amp;quot;&gt;
&lt;norm tag=&amp;quot;mich&amp;quot;/&gt;
&lt;pos tag=&amp;quot;PPER.1.Sg.*.Acc&amp;quot;/&gt;
&lt;/mod&gt;
&lt;/token&gt;
</figure>
<figureCaption confidence="0.9977075">
Figure 3: Example serialization of ober czugemich
(modern überzeuge mich ‘convince me’) in XML
</figureCaption>
<bodyText confidence="0.981394">
serialization of the data model.
</bodyText>
<sectionHeader confidence="0.998667" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999741533333333">
We described CorA, a web-based annotation tool.
Its main features are the integration of automatic
annotation software, the possibility of making edits
to the source document, and the conceptual dis-
tinction between diplomatic and modern tokens in
the data model. We believe that these features are
particularly useful for annotators of non-standard
language data such as historical texts, and set CorA
apart from other existing annotation tools.
We plan to make the tool available under an
open source license eventually. However, we are
currently still working on implementing additional
functionality. In future work, we plan to integrate
features to evaluate annotation quality, such as au-
tomatically calculating inter-annotator agreement.
</bodyText>
<page confidence="0.99968">
89
</page>
<sectionHeader confidence="0.99021" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999786672413793">
Marcel Bollmann, Stefanie Dipper, Julia Krasselt, and
Florian Petran. 2012. Manual and semi-automatic
normalization of historical spelling – case studies
from Early New High German. In Proceedings of
the First International Workshop on Language Tech-
nology for Historical Text(s) (LThist2012), Vienna,
Austria.
Marcel Bollmann. 2012. (Semi-)automatic normaliza-
tion of historical texts using distance measures and
the Norma tool. In Proceedings of the Second Work-
shop on Annotation of Corpora for Research in the
Humanities (ACRH-2), Lisbon, Portugal.
Sonja Bosch, Key-Sun Choi, Éric de la Clergerie, Alex
Chengyu Fang, Gertrud Faaß, Kiyong Lee, Antonio
Pareja-Lora, Laurent Romary, Andreas Witt, Amir
Zeldes, and Florian Zipser. 2012. &lt;tiger2/&gt; as a
standardised serialisation for ISO 24615. In Pro-
ceedings of the 11th Workshop on Treebanks and
Linguistic Theory (TLT), Lisbon, Portugal.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011.
#hardtoparse: POS tagging and parsing the twitter-
verse. In Proceedings of AAAI-11 Workshop on
Analysing Microtext, San Francisco, CA.
Eugenie Giesbrecht and Stefan Evert. 2009. Part-of-
speech tagging — a solved task? An evaluation of
POS taggers for the German Web as Corpus. In
Proceedings of the 5th Web as Corpus Workshop
(WAC5), pages 27–35, San Sebastian, Spain.
Paul Rayson, Dawn Archer, Alistair Baron, Jonathan
Culpeper, and Nicholas Smith. 2007. Tagging the
Bard: Evaluating the accuracy of a modern POS tag-
ger on Early Modern English corpora. In Proceed-
ings of Corpus Linguistics 2007, University of Birm-
ingham, UK.
Marc Reznicek, Anke Lüdeling, and Hagen
Hirschmann. 2013. Competing target hypotheses
in the Falko Corpus: A flexible multi-layer corpus
architecture. In Ana Díaz-Negrillo, Nicolas Ballier,
and Paul Thompson, editors, Automatic Treatment
and Analysis of Learner Corpus Data, pages
101–123. Amsterdam: Benjamins.
Silke Scheible, Richard J. Whitt, Martin Durrell, and
Paul Bennett. 2011. Evaluating an ‘off-the-shelf’
POS-tagger on Early Modern German text. In Pro-
ceedings of the ACL-HLT 2011 Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities (LaTeCH 2011), pages 19–
23, Portland, Oregon, USA.
Helmut Schmid and Florian Laws. 2008. Estimation of
conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of COLING ’08, Manchester, Great Britain.
Amir Zeldes, Julia Ritz, Anke Lüdeling, and Christian
Chiarcos. 2009. ANNIS: a search tool for multi-
layer annotated corpora. In Proceedings of Corpus
Linguistics, Liverpool, UK.
</reference>
<page confidence="0.998641">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.98919">CorA: A web-based annotation tool for and other non-standard language data</title>
<author confidence="0.999607">Marcel Bollmann</author>
<author confidence="0.999607">Florian Petran</author>
<author confidence="0.999607">Stefanie Dipper</author>
<author confidence="0.999607">Julia</author>
<affiliation confidence="0.995704">Department of</affiliation>
<address confidence="0.797071">Ruhr-University Bochum, 44780 Bochum,</address>
<email confidence="0.998027">{bollmann|petran|dipper|krasselt}@linguistics.rub.de</email>
<abstract confidence="0.981732315960913">We present CorA, a web-based annotation tool for manual annotation of historical and other non-standard language data. It allows for editing the primary data and modifying token boundaries during the annotation process. Further, it supports immediate retraining of taggers on newly annotated data. In recent years, the focus of research in natural language processing has shifted from highly standardized text types, such as newspaper texts, to text types that often infringe orthographic, grammatical and stylistic norms normally associated with written language. Prime examples are language data in the context of comsuch as Twitter or SMS data, or contributions in chat rooms. Further examples are data produced by learners or historical texts. Tools trained on standardized data perform considerably worse on “non-standard varieties” such as internet data (cf. Giesbrecht and Evert (2009)’s work on tagging the web or Foster et al. (2011)’s results for parsing Twitter data) or historical language data (Rayson et al., 2007; Scheible et al., 2011). This can mainly be attributed to the facts that tools are applied out of domain, or only small amounts of manually-annotated training data are available. A more fundamental problem is that common and established methods and categories for language analysis often do not fit the phenomena occurring in non-standard data. For instance, grammaticalization is a process of language evolution where new parts of speech are created or words switch from one class to another. It is difficult to draw strict categorial boundaries between words research reported here was financed by Deutsche Forschungsgemeinschaft (DFG), Grant DI 1558/5-1. that take part in a continuous smooth transition of categories. Factors like these can also affect the way the data should be tokenized, along with other problems such as the lack of a fixed orthography. In the light of the above, we developed a webbased tool for manual annotation of non-standard data. It allows for editing the primary data, e.g. for correcting OCR errors of historical texts, or for modifying token boundaries during the annotation process. Furthermore, it supports immediate retraining of taggers on newly annotated data, to attenuate the problem of sparse training data. CorA is currently used in several projects that annotate historical data, and one project that analyzes chat data. So far, about 200,000 tokens in 84 texts have been annotated in CorA. Once the annotation process is completed, the transcriptions and their annotations are imported into the ANNIS corpus tool (Zeldes et al., 2009) where they can be searched and visualized. The paper focuses on the annotation of historical data. Sec. 2 presents the tool, and Sec. 3 describes the data model. Sec. 4 concludes. 2 Tool Description uses a web-based All data is stored on a server, while users can access and edit annotations from anywhere using their web browser. This approach greatly simplifies collaborative work within a project, as it ensures that all users are working on the same version of the data at all times, and requires no software installation on the user’s side. Users can be assigned to individual project groups and are only able to access documents within their group(s). 2.1 The annotation editor All annotation in CorA is done on a token level; currently supported annotation types are partimplements a standard AJAX architecture using PHP 5, MySQL, and JavaScript. 86 of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL pages 86–90, Sweden, April 26 2014. Association for Computational Linguistics Figure 1: Web interface of CorA showing the annotation editor of-speech tags, morphology tags, lemmatization, and (spelling) normalization. The tool is designed to increase productivity for these particular annotation tasks, while sacrificing some amount of flexibility (e.g., using different annotation layers, or annotating spans of tokens). Note that this is mainly a restriction of the web interface; the underlying database structure is much more flexible (cf. Sec. 3), facilitating the later addition of other types of annotation, if desired. Tokens are displayed vertically, i.e., one token per line. This way, the annotations also line up vertically and are always within view. Additionally, a horizontal text preview can be displayed at the bottom of the screen, which makes it easier to read a continuous text passage. Fig. 1 shows a screenshot of the editor Users can customize the editor, e.g. by hiding selected columns. and morphology the editor, both POS and morphology tags can be selected from a dropdown box, which has the advantage of allowing both mouse-based and faster keyboard-based input. Tagsets can be defined individually for each text. If morphology tags are used, the selection of tags in the dropdown box is restricted by the chosen POS tag. user interface is only available in German at the time of writing, but an English version is planned. forms are entered into a text field, which can optionally be linked to a pre-defined lexicon from which it retrieves autocompletion suggestions. Furthermore, if an identical token has already been annotated with a lemma form elsewhere within the same project, that lemma is always displayed as a highlighted suggestion. corpora of non-standard language varieties, spelling normalization is often found as an annotation layer, see, e.g., Scheible et al. (2011) for historical data and Reznicek et al. (2013) for learner data. In addition to normalization, an optional modernization layer can be used that defaults to the content of the normalization field. The normalization layer can be used for standardizing spelling, and the modernization layer for standardizing inflection and semantics (Bollmann et al., 2012). information features a progress indicator which can be used to mark annotations as verified (see the green bar in Fig. 1). Besides serving as a visual aid for the annotator, it is also used for the automatic annotation component (cf. Sec. 2.2). Additionally, tokens can be marked as needing further review (indicated with a red checkbox), and comments can be added. 2.2 Automatic annotation CorA supports (semi-)automatic annotation by integrating external annotation software on the server 87 side. Currently, RFTagger (Schmid and Laws, 2008) and the Norma tool for automatic normalization (Bollmann, 2012) are supported, but in principle any other annotation tool can be integrated as well. The “retraining” feature collects all verified annotations from a project and feeds them to the tools’ training functions. The user is then able to invoke the automatic annotation process using the newly trained parametrizations, which causes all tokens not yet marked as verified to be overwritten with the new annotations. The retraining module is particularly relevant for non-standard language varieties where appropriate language models may not be available. The idea is that as more data is manually annotated within a corpus, the performance of automatic annotation tools increases when retrained on that data. This in turn makes it desirable to re-apply the automatic tools during the annotation process. 2.3 Editing primary data In diplomatic transcriptions of historical manuscripts, the transcripts reproduce the manuscripts in the most accurate way, by encoding all relevant details of special graphemes and diacritics, and also preserving layout information. Transcribers often use ASCII-based encodings for special characters, e.g., the dollar sign $ in place a long s The data model of CorA (cf. Sec. 3) distinguishes between different types of token representations. In the annotation editor, the user can choose to display either the original transcription layer or the UTF-8 representation. If an error in the primary data—e.g., a transcription error or wrong tokenization—is noticed during the annotation, it can be corrected directly within the editor. CorA provides functionality to edit, add, or delete existing tokens. Furthermore, external scripts can be embedded to process any changes, by checking an edited token for validity (e.g., if tokens need to conform to a certain transcription format), or generating the UTF-8 representation by interpreting special characters (e.g., mapping $ 2.4 Comparison to related tools There is a range of annotation tools that can be used for enriching data with different kinds of an- Prominent examples are GATE, EX- MMAX2, brat, and Many annotation projects nowadays require distributed collaborative working of multiple parties. The currently preferred solution is to use a tool with an underlying database which is operated through a standard web-browser. Among the tools above, only brat and WebAnno are web-based tools. Compared to CorA, these tools are more flexible in that they support more annotation layers and more complex (e.g., multi-word) annotations. WebAnno, in addition, offers facilities for measuring interannotator agreement and data curation. However, brat and WebAnno do not allow edits to the source document from within the tool, which is particularly relevant for non-standard language varieties. Similarly, they do not support retraining on newly annotated data. 3 Data Model The requirements described in Sec. 2 present various challenges to the data storage, which necessitated the development of our own data model. A data model in this context is a conceptual model of the data structure that allows serialization into various representations such as XML or databases. Such a model also allows for easy conversion between serializations and hence facilitates interoperability with existing formats and tools. The complex, multi-layered layout, the differences in tokenization, and the fine-grained description of graphematic pecularities in the primary data cannot be captured well using existing formats. For example, tokenization differences as they are handled by formats such as &lt;tiger2/&gt; (Bosch et al., 2012) pertain only to the contraction of underlying units to original forms, and not the other way around. This means that while a conversion in such formats is easily possible, some of the data structure that is captured by our model is necessarily lost in the process. To come up with a data model that minimizes redundancy and allows for flexibility and extensibility, and accomodates the work flow of our transcriptors and annotators, we employed normalization techniques from database development. A slightly simplified version of the data model is shown in Fig. 2. 88 Figure 2: Data model used for CorA and Text model is centered around two units, a text and a token. A token is a virtual unit that can manifest in two ways, the diplomatic token and the modern token, each of which has a one-to-many relation with a token (cf. Fig. 3). Diplomatic tokens are tokens as they appear in the original, historical text, while modern tokens mirror modern conventions for token boundaries, representing suitable units for further annotations, e.g. with POS tags. All physical layout information on the other hand relates to the diplomatic token. The text is the entirety of a transcribed document that can be partitioned in various ways. The layout is captured by its relation to the page, column, and line, which in turn relate to the diplomatic tokens. Furthermore, a text can be assigned one or more tagsets. The tagsets in turn can be open, such as lemmatization tags, or closed, such as POS tags. Each text can be assigned different tagsets. addition, the data model also allows for the import of markup annotations with the texts, which may denote layout-related or linguistic peculiarities encoded by the transcriptors, as well as information about its annotation status such as progress, or dubious annotations. The model is easily extendable for user management that can tie in to the text table, e.g., a user can be set as owner or creator of a text. As XML serialization is not optimized for data which is not strictly hierarchically structured, storage and retrieval is rather inefficient, and extensions are not easily possible. For this reason, we chose to implement the application with an SQL database &lt;token&gt; &lt;!-diplomatic tokenization --&gt; &lt;dipl trans=&amp;quot;ober&amp;quot;/&gt; &lt;dipl trans=&amp;quot;czugemich&amp;quot;/&gt; &lt;!-modern tokenization --&gt; &lt;mod trans=&amp;quot;oberczuge&amp;quot;&gt; &lt;norm tag=&amp;quot;überzeuge&amp;quot;/&gt; &lt;pos tag=&amp;quot;VVIMP.Sg&amp;quot;/&gt; &lt;/mod&gt; &lt;mod trans=&amp;quot;mich&amp;quot;&gt; &lt;norm tag=&amp;quot;mich&amp;quot;/&gt; &lt;pos tag=&amp;quot;PPER.1.Sg.*.Acc&amp;quot;/&gt; &lt;/mod&gt; &lt;/token&gt; 3: Example serialization of czugemich mich me’) in XML serialization of the data model. 4 Conclusion We described CorA, a web-based annotation tool. Its main features are the integration of automatic annotation software, the possibility of making edits to the source document, and the conceptual distinction between diplomatic and modern tokens in the data model. We believe that these features are particularly useful for annotators of non-standard language data such as historical texts, and set CorA apart from other existing annotation tools. We plan to make the tool available under an open source license eventually. However, we are currently still working on implementing additional functionality. In future work, we plan to integrate features to evaluate annotation quality, such as automatically calculating inter-annotator agreement. 89 References Marcel Bollmann, Stefanie Dipper, Julia Krasselt, and Florian Petran. 2012. Manual and semi-automatic normalization of historical spelling – case studies Early New High German. In of the First International Workshop on Language Tech-</abstract>
<affiliation confidence="0.464102">for Historical Text(s) Vienna,</affiliation>
<address confidence="0.638558">Austria.</address>
<author confidence="0.588237">automatic normaliza-</author>
<affiliation confidence="0.409506">tion of historical texts using distance measures and</affiliation>
<note confidence="0.5268606">Norma tool. In of the Second Workshop on Annotation of Corpora for Research in the Lisbon, Portugal. Sonja Bosch, Key-Sun Choi, Éric de la Clergerie, Alex Chengyu Fang, Gertrud Faaß, Kiyong Lee, Antonio Pareja-Lora, Laurent Romary, Andreas Witt, Amir Zeldes, and Florian Zipser. 2012. &lt;tiger2/&gt; as a serialisation for ISO 24615. In Proceedings of the 11th Workshop on Treebanks and Theory Lisbon, Portugal.</note>
<author confidence="0.646268">Jennifer Foster</author>
<author confidence="0.646268">Ozlem Cetinoglu</author>
<author confidence="0.646268">Joachim Wagner</author>
<author confidence="0.646268">Joseph Le_Roux</author>
<author confidence="0.646268">Stephen Hogan</author>
<author confidence="0.646268">Joakim Nivre</author>
<note confidence="0.550060409090909">Deirdre Hogan, and Josef van Genabith. 2011. In of AAAI-11 Workshop on San Francisco, CA. Eugenie Giesbrecht and Stefan Evert. 2009. Part-ofspeech tagging — a solved task? An evaluation of POS taggers for the German Web as Corpus. In Proceedings of the 5th Web as Corpus Workshop pages 27–35, San Sebastian, Spain. Paul Rayson, Dawn Archer, Alistair Baron, Jonathan Culpeper, and Nicholas Smith. 2007. Tagging the Bard: Evaluating the accuracy of a modern POS tagon Early Modern English corpora. In Proceedof Corpus Linguistics University of Birmingham, UK. Marc Reznicek, Anke Lüdeling, and Hagen Hirschmann. 2013. Competing target hypotheses in the Falko Corpus: A flexible multi-layer corpus architecture. In Ana Díaz-Negrillo, Nicolas Ballier, Paul Thompson, editors, Treatment Analysis of Learner Corpus pages 101–123. Amsterdam: Benjamins. Silke Scheible, Richard J. Whitt, Martin Durrell, and</note>
<author confidence="0.7431705">Evaluating an ‘off-the-shelf’ on Early Modern German text In Pro-</author>
<affiliation confidence="0.751015">ceedings of the ACL-HLT 2011 Workshop on Language Technology for Cultural Heritage, Social Sciand Humanities (LaTeCH pages 19–</affiliation>
<address confidence="0.847653">23, Portland, Oregon, USA.</address>
<note confidence="0.846065555555556">Helmut Schmid and Florian Laws. 2008. Estimation of conditional probabilities with decision trees and an to fine-grained POS tagging. In Proceedof COLING Manchester, Great Britain. Amir Zeldes, Julia Ritz, Anke Lüdeling, and Christian Chiarcos. 2009. ANNIS: a search tool for multiannotated corpora. In of Corpus Liverpool, UK. 90</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marcel Bollmann</author>
<author>Stefanie Dipper</author>
<author>Julia Krasselt</author>
<author>Florian Petran</author>
</authors>
<title>Manual and semi-automatic normalization of historical spelling – case studies from Early New High German.</title>
<date>2012</date>
<booktitle>In Proceedings of the First International Workshop on Language Technology for Historical Text(s) (LThist2012),</booktitle>
<location>Vienna, Austria.</location>
<contexts>
<context position="6344" citStr="Bollmann et al., 2012" startWordPosition="988" endWordPosition="991">h a lemma form elsewhere within the same project, that lemma is always displayed as a highlighted suggestion. Normalization For corpora of non-standard language varieties, spelling normalization is often found as an annotation layer, see, e.g., Scheible et al. (2011) for historical data and Reznicek et al. (2013) for learner data. In addition to normalization, an optional modernization layer can be used that defaults to the content of the normalization field. The normalization layer can be used for standardizing spelling, and the modernization layer for standardizing inflection and semantics (Bollmann et al., 2012). Meta information CorA features a progress indicator which can be used to mark annotations as verified (see the green bar in Fig. 1). Besides serving as a visual aid for the annotator, it is also used for the automatic annotation component (cf. Sec. 2.2). Additionally, tokens can be marked as needing further review (indicated with a red checkbox), and comments can be added. 2.2 Automatic annotation CorA supports (semi-)automatic annotation by integrating external annotation software on the server 87 side. Currently, RFTagger (Schmid and Laws, 2008) and the Norma tool for automatic normalizati</context>
</contexts>
<marker>Bollmann, Dipper, Krasselt, Petran, 2012</marker>
<rawString>Marcel Bollmann, Stefanie Dipper, Julia Krasselt, and Florian Petran. 2012. Manual and semi-automatic normalization of historical spelling – case studies from Early New High German. In Proceedings of the First International Workshop on Language Technology for Historical Text(s) (LThist2012), Vienna, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcel Bollmann</author>
</authors>
<title>(Semi-)automatic normalization of historical texts using distance measures and the Norma tool.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Annotation of Corpora for Research in the Humanities (ACRH-2),</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="6963" citStr="Bollmann, 2012" startWordPosition="1090" endWordPosition="1091">ta information CorA features a progress indicator which can be used to mark annotations as verified (see the green bar in Fig. 1). Besides serving as a visual aid for the annotator, it is also used for the automatic annotation component (cf. Sec. 2.2). Additionally, tokens can be marked as needing further review (indicated with a red checkbox), and comments can be added. 2.2 Automatic annotation CorA supports (semi-)automatic annotation by integrating external annotation software on the server 87 side. Currently, RFTagger (Schmid and Laws, 2008) and the Norma tool for automatic normalization (Bollmann, 2012) are supported, but in principle any other annotation tool can be integrated as well. The “retraining” feature collects all verified annotations from a project and feeds them to the tools’ training functions. The user is then able to invoke the automatic annotation process using the newly trained parametrizations, which causes all tokens not yet marked as verified to be overwritten with the new annotations. The retraining module is particularly relevant for non-standard language varieties where appropriate language models may not be available. The idea is that as more data is manually annotate</context>
</contexts>
<marker>Bollmann, 2012</marker>
<rawString>Marcel Bollmann. 2012. (Semi-)automatic normalization of historical texts using distance measures and the Norma tool. In Proceedings of the Second Workshop on Annotation of Corpora for Research in the Humanities (ACRH-2), Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Bosch</author>
<author>Key-Sun Choi</author>
</authors>
<title>Éric de la Clergerie, Alex Chengyu Fang, Gertrud Faaß, Kiyong Lee, Antonio Pareja-Lora, Laurent Romary, Andreas Witt, Amir Zeldes, and Florian Zipser.</title>
<date>2012</date>
<booktitle>In Proceedings of the 11th Workshop on Treebanks and Linguistic Theory (TLT),</booktitle>
<location>Lisbon, Portugal.</location>
<marker>Bosch, Choi, 2012</marker>
<rawString>Sonja Bosch, Key-Sun Choi, Éric de la Clergerie, Alex Chengyu Fang, Gertrud Faaß, Kiyong Lee, Antonio Pareja-Lora, Laurent Romary, Andreas Witt, Amir Zeldes, and Florian Zipser. 2012. &lt;tiger2/&gt; as a standardised serialisation for ISO 24615. In Proceedings of the 11th Workshop on Treebanks and Linguistic Theory (TLT), Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Ozlem Cetinoglu</author>
<author>Joachim Wagner</author>
<author>Joseph Le Roux</author>
<author>Stephen Hogan</author>
<author>Joakim Nivre</author>
<author>Deirdre Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>hardtoparse: POS tagging and parsing the twitterverse.</title>
<date>2011</date>
<booktitle>In Proceedings of AAAI-11 Workshop on Analysing Microtext,</booktitle>
<location>San Francisco, CA.</location>
<marker>Foster, Cetinoglu, Wagner, Le Roux, Hogan, Nivre, Hogan, van Genabith, 2011</marker>
<rawString>Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Joseph Le Roux, Stephen Hogan, Joakim Nivre, Deirdre Hogan, and Josef van Genabith. 2011. #hardtoparse: POS tagging and parsing the twitterverse. In Proceedings of AAAI-11 Workshop on Analysing Microtext, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugenie Giesbrecht</author>
<author>Stefan Evert</author>
</authors>
<title>Part-ofspeech tagging — a solved task? An evaluation of POS taggers for the German Web as Corpus.</title>
<date>2009</date>
<booktitle>In Proceedings of the 5th Web as Corpus Workshop (WAC5),</booktitle>
<pages>27--35</pages>
<location>San Sebastian,</location>
<contexts>
<context position="1238" citStr="Giesbrecht and Evert (2009)" startWordPosition="169" endWordPosition="172">ars, the focus of research in natural language processing has shifted from highly standardized text types, such as newspaper texts, to text types that often infringe orthographic, grammatical and stylistic norms normally associated with written language. Prime examples are language data produced in the context of computer-mediated communication (CMC), such as Twitter or SMS data, or contributions in chat rooms. Further examples are data produced by learners or historical texts. Tools trained on standardized data perform considerably worse on “non-standard varieties” such as internet data (cf. Giesbrecht and Evert (2009)’s work on tagging the web or Foster et al. (2011)’s results for parsing Twitter data) or historical language data (Rayson et al., 2007; Scheible et al., 2011). This can mainly be attributed to the facts that tools are applied out of domain, or only small amounts of manually-annotated training data are available. A more fundamental problem is that common and established methods and categories for language analysis often do not fit the phenomena occurring in non-standard data. For instance, grammaticalization is a process of language evolution where new parts of speech are created or words swit</context>
</contexts>
<marker>Giesbrecht, Evert, 2009</marker>
<rawString>Eugenie Giesbrecht and Stefan Evert. 2009. Part-ofspeech tagging — a solved task? An evaluation of POS taggers for the German Web as Corpus. In Proceedings of the 5th Web as Corpus Workshop (WAC5), pages 27–35, San Sebastian, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Rayson</author>
<author>Dawn Archer</author>
<author>Alistair Baron</author>
<author>Jonathan Culpeper</author>
<author>Nicholas Smith</author>
</authors>
<title>Tagging the Bard: Evaluating the accuracy of a modern POS tagger on Early Modern English corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of Corpus Linguistics</booktitle>
<institution>University of Birmingham, UK.</institution>
<contexts>
<context position="1373" citStr="Rayson et al., 2007" startWordPosition="193" endWordPosition="196">es that often infringe orthographic, grammatical and stylistic norms normally associated with written language. Prime examples are language data produced in the context of computer-mediated communication (CMC), such as Twitter or SMS data, or contributions in chat rooms. Further examples are data produced by learners or historical texts. Tools trained on standardized data perform considerably worse on “non-standard varieties” such as internet data (cf. Giesbrecht and Evert (2009)’s work on tagging the web or Foster et al. (2011)’s results for parsing Twitter data) or historical language data (Rayson et al., 2007; Scheible et al., 2011). This can mainly be attributed to the facts that tools are applied out of domain, or only small amounts of manually-annotated training data are available. A more fundamental problem is that common and established methods and categories for language analysis often do not fit the phenomena occurring in non-standard data. For instance, grammaticalization is a process of language evolution where new parts of speech are created or words switch from one class to another. It is difficult to draw strict categorial boundaries between words 1The research reported here was financ</context>
</contexts>
<marker>Rayson, Archer, Baron, Culpeper, Smith, 2007</marker>
<rawString>Paul Rayson, Dawn Archer, Alistair Baron, Jonathan Culpeper, and Nicholas Smith. 2007. Tagging the Bard: Evaluating the accuracy of a modern POS tagger on Early Modern English corpora. In Proceedings of Corpus Linguistics 2007, University of Birmingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Reznicek</author>
<author>Anke Lüdeling</author>
<author>Hagen Hirschmann</author>
</authors>
<title>Competing target hypotheses in the Falko Corpus: A flexible multi-layer corpus architecture.</title>
<date>2013</date>
<booktitle>Automatic Treatment and Analysis of Learner Corpus Data,</booktitle>
<pages>101--123</pages>
<editor>In Ana Díaz-Negrillo, Nicolas Ballier, and Paul Thompson, editors,</editor>
<location>Amsterdam: Benjamins.</location>
<contexts>
<context position="6036" citStr="Reznicek et al. (2013)" startWordPosition="940" endWordPosition="943">vailable in German at the time of writing, but an English version is planned. Lemmatization Lemma forms are entered into a text field, which can optionally be linked to a pre-defined lexicon from which it retrieves autocompletion suggestions. Furthermore, if an identical token has already been annotated with a lemma form elsewhere within the same project, that lemma is always displayed as a highlighted suggestion. Normalization For corpora of non-standard language varieties, spelling normalization is often found as an annotation layer, see, e.g., Scheible et al. (2011) for historical data and Reznicek et al. (2013) for learner data. In addition to normalization, an optional modernization layer can be used that defaults to the content of the normalization field. The normalization layer can be used for standardizing spelling, and the modernization layer for standardizing inflection and semantics (Bollmann et al., 2012). Meta information CorA features a progress indicator which can be used to mark annotations as verified (see the green bar in Fig. 1). Besides serving as a visual aid for the annotator, it is also used for the automatic annotation component (cf. Sec. 2.2). Additionally, tokens can be marked </context>
</contexts>
<marker>Reznicek, Lüdeling, Hirschmann, 2013</marker>
<rawString>Marc Reznicek, Anke Lüdeling, and Hagen Hirschmann. 2013. Competing target hypotheses in the Falko Corpus: A flexible multi-layer corpus architecture. In Ana Díaz-Negrillo, Nicolas Ballier, and Paul Thompson, editors, Automatic Treatment and Analysis of Learner Corpus Data, pages 101–123. Amsterdam: Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silke Scheible</author>
<author>Richard J Whitt</author>
<author>Martin Durrell</author>
<author>Paul Bennett</author>
</authors>
<title>Evaluating an ‘off-the-shelf’ POS-tagger on Early Modern German text.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL-HLT 2011 Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH</booktitle>
<pages>19--23</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="1397" citStr="Scheible et al., 2011" startWordPosition="197" endWordPosition="200">e orthographic, grammatical and stylistic norms normally associated with written language. Prime examples are language data produced in the context of computer-mediated communication (CMC), such as Twitter or SMS data, or contributions in chat rooms. Further examples are data produced by learners or historical texts. Tools trained on standardized data perform considerably worse on “non-standard varieties” such as internet data (cf. Giesbrecht and Evert (2009)’s work on tagging the web or Foster et al. (2011)’s results for parsing Twitter data) or historical language data (Rayson et al., 2007; Scheible et al., 2011). This can mainly be attributed to the facts that tools are applied out of domain, or only small amounts of manually-annotated training data are available. A more fundamental problem is that common and established methods and categories for language analysis often do not fit the phenomena occurring in non-standard data. For instance, grammaticalization is a process of language evolution where new parts of speech are created or words switch from one class to another. It is difficult to draw strict categorial boundaries between words 1The research reported here was financed by Deutsche Forschung</context>
<context position="5989" citStr="Scheible et al. (2011)" startWordPosition="932" endWordPosition="935">e chosen POS tag. 3The user interface is only available in German at the time of writing, but an English version is planned. Lemmatization Lemma forms are entered into a text field, which can optionally be linked to a pre-defined lexicon from which it retrieves autocompletion suggestions. Furthermore, if an identical token has already been annotated with a lemma form elsewhere within the same project, that lemma is always displayed as a highlighted suggestion. Normalization For corpora of non-standard language varieties, spelling normalization is often found as an annotation layer, see, e.g., Scheible et al. (2011) for historical data and Reznicek et al. (2013) for learner data. In addition to normalization, an optional modernization layer can be used that defaults to the content of the normalization field. The normalization layer can be used for standardizing spelling, and the modernization layer for standardizing inflection and semantics (Bollmann et al., 2012). Meta information CorA features a progress indicator which can be used to mark annotations as verified (see the green bar in Fig. 1). Besides serving as a visual aid for the annotator, it is also used for the automatic annotation component (cf.</context>
</contexts>
<marker>Scheible, Whitt, Durrell, Bennett, 2011</marker>
<rawString>Silke Scheible, Richard J. Whitt, Martin Durrell, and Paul Bennett. 2011. Evaluating an ‘off-the-shelf’ POS-tagger on Early Modern German text. In Proceedings of the ACL-HLT 2011 Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH 2011), pages 19– 23, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
<author>Florian Laws</author>
</authors>
<title>Estimation of conditional probabilities with decision trees and an application to fine-grained POS tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING ’08,</booktitle>
<location>Manchester, Great Britain.</location>
<contexts>
<context position="6899" citStr="Schmid and Laws, 2008" startWordPosition="1078" endWordPosition="1081"> for standardizing inflection and semantics (Bollmann et al., 2012). Meta information CorA features a progress indicator which can be used to mark annotations as verified (see the green bar in Fig. 1). Besides serving as a visual aid for the annotator, it is also used for the automatic annotation component (cf. Sec. 2.2). Additionally, tokens can be marked as needing further review (indicated with a red checkbox), and comments can be added. 2.2 Automatic annotation CorA supports (semi-)automatic annotation by integrating external annotation software on the server 87 side. Currently, RFTagger (Schmid and Laws, 2008) and the Norma tool for automatic normalization (Bollmann, 2012) are supported, but in principle any other annotation tool can be integrated as well. The “retraining” feature collects all verified annotations from a project and feeds them to the tools’ training functions. The user is then able to invoke the automatic annotation process using the newly trained parametrizations, which causes all tokens not yet marked as verified to be overwritten with the new annotations. The retraining module is particularly relevant for non-standard language varieties where appropriate language models may not </context>
</contexts>
<marker>Schmid, Laws, 2008</marker>
<rawString>Helmut Schmid and Florian Laws. 2008. Estimation of conditional probabilities with decision trees and an application to fine-grained POS tagging. In Proceedings of COLING ’08, Manchester, Great Britain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Zeldes</author>
<author>Julia Ritz</author>
<author>Anke Lüdeling</author>
<author>Christian Chiarcos</author>
</authors>
<title>ANNIS: a search tool for multilayer annotated corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of Corpus Linguistics,</booktitle>
<location>Liverpool, UK.</location>
<contexts>
<context position="2959" citStr="Zeldes et al., 2009" startWordPosition="448" endWordPosition="451">. It allows for editing the primary data, e.g. for correcting OCR errors of historical texts, or for modifying token boundaries during the annotation process. Furthermore, it supports immediate retraining of taggers on newly annotated data, to attenuate the problem of sparse training data. CorA is currently used in several projects that annotate historical data, and one project that analyzes chat data. So far, about 200,000 tokens in 84 texts have been annotated in CorA. Once the annotation process is completed, the transcriptions and their annotations are imported into the ANNIS corpus tool (Zeldes et al., 2009) where they can be searched and visualized. The paper focuses on the annotation of historical data. Sec. 2 presents the tool, and Sec. 3 describes the data model. Sec. 4 concludes. 2 Tool Description CorA uses a web-based architecture:2 All data is stored on a server, while users can access and edit annotations from anywhere using their web browser. This approach greatly simplifies collaborative work within a project, as it ensures that all users are working on the same version of the data at all times, and requires no software installation on the user’s side. Users can be assigned to individu</context>
</contexts>
<marker>Zeldes, Ritz, Lüdeling, Chiarcos, 2009</marker>
<rawString>Amir Zeldes, Julia Ritz, Anke Lüdeling, and Christian Chiarcos. 2009. ANNIS: a search tool for multilayer annotated corpora. In Proceedings of Corpus Linguistics, Liverpool, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
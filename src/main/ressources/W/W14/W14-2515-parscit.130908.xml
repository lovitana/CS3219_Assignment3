<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000827">
<title confidence="0.980934">
Predicting Fine-grained Social Roles with Selectional Preferences
</title>
<author confidence="0.94702">
Charley Beller Craig Harman Benjamin Van Durme
</author>
<email confidence="0.71174">
charleybeller@jhu.edu craig@craigharman.net vandurme@cs.jhu.edu
</email>
<affiliation confidence="0.857614">
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD USA
</affiliation>
<sectionHeader confidence="0.977094" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999708466666667">
Selectional preferences, the tendencies of
predicates to select for certain semantic
classes of arguments, have been success-
fully applied to a number of tasks in
computational linguistics including word
sense disambiguation, semantic role label-
ing, relation extraction, and textual infer-
ence. Here we leverage the information
encoded in selectional preferences to the
task of predicting fine-grained categories
of authors on the social media platform
Twitter. First person uses of verbs that se-
lect for a given social role as subject (e.g.
I teach ... for teacher) are used to quickly
build up binary classifiers for that role.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964433962264">
It has long been recognized that linguistic pred-
icates preferentially select arguments that meet
certain semantic criteria (Katz and Fodor, 1963;
Chomsky, 1965). The verb eat for example se-
lects for an animate subject and a comestible ob-
ject. While the information encoded by selectional
preferences can and has been used to support nat-
ural language processing tasks such as word sense
disambiguation (Resnik, 1997), syntactic disam-
biguation (Li and Abe, 1998) and semantic role
labeling (Gildea and Jurafsky, 2002), much of
the work on the topic revolves around developing
methods to induce selectional preferences from
data. In this setting, end-tasks can be used for
evaluation of the resulting collection. Ritter et al.
(2010) gave a recent overview of this work, break-
ing it down into class-based approaches (Resnik,
1996; Li and Abe, 1998; Clark and Weir, 2002;
Pantel et al., 2007), similarity-based approaches
(Dagan et al., 1999; Erk, 2007), and approaches
using discriminative (Bergsma et al., 2008) or gen-
erative probabilistic models (Rooth et al., 1999)
like their own.
One of our contributions here is to show that
the literature on selectional preferences relates to
the analysis of the first person content transmitted
through social media. We make use of a “quick
and dirty” method for inducing selectional pref-
erences and apply the resulting collections to the
task of predicting fine-grained latent author at-
tributes on Twitter. Our method for inducing se-
lectional preferences is most similar to class-based
approaches, though unlike approaches such as by
Resnik (1996) we do not require a WordNet-like
ontology.
The vast quantity of informal, first-person text
data made available by the rise of social me-
dia platforms has encouraged researchers to de-
velop models that predict broad user categories
like age, gender, and political preference (Garera
and Yarowsky, 2009; Rao et al., 2010; Burger et
al., 2011; Van Durme, 2012b; Zamal et al., 2012).
Such information is useful for large scale demo-
graphic research that can fuel computational social
science advertising.
Similarly to Beller et al. (2014), we are inter-
ested in classification that is finer-grained than
gender or political affiliation, seeking instead to
predict social roles like smoker, student, and
artist. We make use of a light-weight, unsuper-
vised method to identify selectional preferences
and use the resulting information to rapidly boot-
strap classification models.
</bodyText>
<sectionHeader confidence="0.833638" genericHeader="method">
2 Inducing Selectional Preferences
</sectionHeader>
<bodyText confidence="0.996663333333333">
Consider the task of predicting social roles in more
detail: For a given role, e.g. artist, we want a way
to distinguish role-bearing from non-role-bearing
users. We can view each social role as being a
fine-grained version of a semantic class of the sort
required by class-based approaches to selectional
preferences (e.g. the work by Resnik (1996) and
those reviewed by Light and Greiff (2002)). The
goal then is to identify a set of verbs that preferen-
</bodyText>
<page confidence="0.957298">
50
</page>
<note confidence="0.502279">
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 50–55,
</note>
<page confidence="0.343252">
Baltimore, Maryland, USA, June 26, 2014. c�2014 Association for Computational Linguistics
</page>
<bodyText confidence="0.998365454545455">
tially select that particular class as argument. Once
we have a set of verbs for a given role, simple pat-
tern matches against first person subject templates
like I can be used to identify authors that bear
that social role.
In order to identify verbs that select for a given
role r as subject we use an unsupervised method
inspired by Bergsma and Van Durme (2013) that
extracts features from third-person content (i.e.
newswire) to build classifiers on first-person con-
tent (i.e. tweets). For example, if we read in a
news article that an artist drew ..., we can take a
tweet saying I drew ... as potential evidence that
the author bears the artist social role.
We first count all verbs v that appear with role r
as subject in the web-scale, part-of-speech tagged
n-gram corpus, Google V2 (Lin et al., 2010).
The resulting collection of verbs is then ranked
by computing their pointwise mutual information
(Church and Hanks, 1990) with the subject role r.
The PMI of a given role r and a verb v that takes
r as subject is given as:
</bodyText>
<equation confidence="0.999433">
P (r, v)
PMI(r, v) = log P(r)P(v)
</equation>
<bodyText confidence="0.999672960784314">
Probabilities are estimated from counts of the
role-verb pairs along with counts matching the
generic subject patterns he and she which
serve as general background cases. This gives us a
set of verbs that preferentially select for the subset
of persons filling the given role.
The output of the PMI ranking is a high-recall
list of verbs that preferentially select the given so-
cial role as subject over a background population.
Each such list then underwent a manual filtering
step to rapidly remove non-discriminative verbs
and corpus artifacts. One such artifact from our
corpus was the term wannabe which was spuri-
ously elevated in the PMI ranking based on the
relative frequency of the bigram artist wannabe as
compared to she wannabe. Note that in the first
case wannabe is best analyzed as a noun, while in
the second case a verbal analysis is more plausi-
ble. The filtering was performed by one of the au-
thors and generally took less than two minutes per
list. The rapidity of the filtering step is in line with
findings such as by Jacoby et al. (1979) that rele-
vance based filtering involves less cognitive effort
than generation. After filtering the lists contained
fewer than 40 verbs selecting each social role.
In part because of the pivot from third- to first-
person text we performed a precision test on the
remaining verbs to identify which of them are
likely to be useful in classifying twitter users. For
each remaining verb we extracted all tweets that
contained the first person subject pattern I from
a small corpus of tweets drawn from the free pub-
lic 1% sample of the Twitter Firehose over a single
month in 2013. Verbs that had no matches which
appeared to be composed by a member of the as-
sociated social role were discarded. Using this
smaller high-precision set of verbs, we collected
tweets from a much larger corpus drawn from 1%
sample over the period 2011-2013.
One notable feature of the written English in
social media is that sentence subjects can be op-
tionally omitted. Subject-drop is a recognized fea-
ture of other informal spoken and written registers
of English, particularly ‘diary dialects’ (Thrasher,
1977; Napoli, 1982; Haegeman and Ihsane, 2001;
Weir, 2012; Haegeman, 2013; Scott, 2013). Be-
cause of the prevalence of subjectless cases we
collected two sets of tweets: those matching the
first person subject pattern I and those where
the verb was tweet initial. Example tweets for each
of our social roles can be seen in Table 2.
</bodyText>
<sectionHeader confidence="0.981614" genericHeader="method">
3 Classification via selectional
preferences
</sectionHeader>
<bodyText confidence="0.999405565217391">
We conducted a set of experiments to gauge the
strength of the selectional preference indicators
for each social role. For each experiment we used
balanced datasets for training and testing with half
of the users taken from a random background sam-
ple and half from a collection of users identified
as belonging to the social role. Base accuracy was
thus 50%.
To curate the collections of positively identified
users we crowdsourced a manual verification pro-
cedure. We use the popular crowdsourcing plat-
form Mechanical Turk1 to judge whether, for a
tweet containing a given verb, the author held the
role that verb prefers as subject. Each tweet was
judged using 5-way redundancy.
Mechanical Turk judges (“Turkers”) were pre-
sented with a tweet and the prompt: Based on this
tweet, would you think this person is a ARTIST?
along with four response options: Yes, Maybe,
Hard to tell, and No. An example is shown in Fig-
ure 1.
We piloted this labeling task with a goal of
20 tweets per verb over a variety of social roles.
</bodyText>
<footnote confidence="0.987446">
1https://www.mturk.com/mturk/
</footnote>
<page confidence="0.995627">
51
</page>
<figure confidence="0.704394545454545">
Artist
draw Yeaa this a be the first time I draw my
shit onn
Athlete
play @[user] @[user] i have got the night off
tonight because I played last night and I
am going out for dinner so won’t be able
to come”
Blogger
blogged @[user] I decided not to renew. I
blogged about it on the fan club. a bit
shocked no neg comments back to me
Cheerleader
cheer I really dont wanna cheer for this game
I have soo much to do
Christian
thank Had my bday yesterday 3011 nd had a
good night with my friends. I thank God
4 His blessings in my life nd praise Him
4 adding another year.
DJ
spin Quick cut session before I spin tonight
</figure>
<subsectionHeader confidence="0.687477">
Filmmaker
</subsectionHeader>
<bodyText confidence="0.9951265">
film @[user] apparently there was no au-
dio on the volleyball game I filmed
so...there will be no “NAT sound” cause
I have no audio at all
</bodyText>
<subsectionHeader confidence="0.88781">
Media Host
</subsectionHeader>
<bodyText confidence="0.811768333333333">
interview Oh. I interviewed her on the @[user] .
You should listen to the interview. Its
awesome! @[user] @[user] @[user]
</bodyText>
<figure confidence="0.8605486">
Performer
perform I perform the flute... kareem shocked...
Producer
produce RT @[user]: Wow 2 films in Urban-
world this year-1 I produced ... [URL]
Smoker
smoke I smoke , i drank .. was my shit bra !
Stoner
puff I’m a cigarello fiend smokin weed like
its oxygen Puff pass, nigga I puff grass
till I pass out
Student
finish I finish school in March and my friend
birthday in March ...
Teacher
</figure>
<tableCaption confidence="0.785187166666667">
teach @[user] home schooled I really wanna
find out wat it’s like n making new
friends but home schooling is cool I
teach myself mums ill
Table 1: Example verbs and sample tweets collected using
them in the first person subject pattern (I ).
</tableCaption>
<bodyText confidence="0.999761125">
Each answer was associated with a score (Yes = 1,
Maybe = .5, Hard to tell = No = 0) and aggregated
across the five judges, leading to a range of pos-
sible scores from 0.0 to 5.0 per tweet. We found
in development that an aggregate score of 4.0 led
to an acceptable agreement rate between the Turk-
ers and the experimenters, when the tweets were
randomly sampled and judged internally.
Verbs were discarded for being either insuffi-
ciently accurate or insufficiently prevalent in the
corpus. From the remaining verbs, we identified
users with tweets scoring 4.0 or better as the posi-
tive examples of the associated social roles. These
positively identified user’s tweets were scraped us-
ing the Twitter API in order to construct user-
specific corpora of positive examples for each role.
</bodyText>
<figureCaption confidence="0.9489885">
Figure 1: Mechanical Turk presentation
Figure 2: Accuracy of classifier trained and tested on bal-
anced set contrasting agreed upon Twitter users of a given
role, against users pulled at random from the 1% stream.
</figureCaption>
<subsectionHeader confidence="0.99151">
3.1 General Classification
</subsectionHeader>
<bodyText confidence="0.9999705">
The positively annotated examples were balanced
with data from a background set of Twitter users
to produce training and test sets. These test sets
were usually of size 40 (20 positive, 20 back-
ground), with a few classes being sparser (the
smallest test set had only 28 instances). We used
the Jerboa (Van Durme, 2012a) platform to con-
vert our data to binary feature vectors over a uni-
gram vocabulary filtered such that the minimum
frequency was 5 (across unique users). Training
and testing was done with a log-linear model via
LibLinear (Fan et al., 2008). Results are shown
in Figure 2. As can be seen, a variety of classes in
this balanced setup can be predicted with accura-
cies in the range of 80%. This shows that the in-
formation encoded in selectional preferences con-
tains discriminating signal for a variety of these
social roles.
</bodyText>
<subsectionHeader confidence="0.997721">
3.2 Conditional Classification
</subsectionHeader>
<bodyText confidence="0.9988866">
How accurately can we predict membership in a
given class when a Twitter user sends a tweet
matching one of the collected verbs? For exam-
ple, if one sends a tweet saying I race ..., then how
likely is it that the author is an athlete?
</bodyText>
<figure confidence="0.9961884">
Accuracy
0.8
0.7
0.6
0.5
</figure>
<page confidence="0.814837">
52
</page>
<figureCaption confidence="0.99672475">
Figure 3: Results of positive vs negative by verb. Given
that a user writes a tweet containing I interview ... or Inter-
viewing ... we are about 75% accurate in identifying whether
or not the user is a Radio/Podcast Host.
</figureCaption>
<table confidence="0.999955">
# Users # labeled # Pos # Neg Attribute
199022 516 63 238 Artist-draw
45162 566 40 284 Athlete-race
1074289 1000 54 731 Athlete-run
9960 15 14 0 Blogger-blog
2204 140 57 18 College Student-enroll
247231 1000 85 564 College Student-finish
60486 845 61 524 Cheerleader-cheer
448738 561 133 95 Christian-pray
92223 286 59 180 Christian-serve
428337 307 78 135 Christian-thank
17408 246 17 151 DJ-spin
153793 621 53 332 Filmmaker-film
36991 554 42 223 Radio Host-interview
43997 297 81 97 Performer-perform
69463 315 71 100 Producer-produce
513096 144 74 8 Smoker-smoke
5542 124 49 15 Stoner-puff
5526 229 59 51 Stoner-spark
149244 495 133 208 Teacher-teach
</table>
<tableCaption confidence="0.9973345">
Table 2: Numbers of positively and negatively identified
users by indicative verb.
</tableCaption>
<bodyText confidence="0.999798588235294">
Using the same collection as the previous ex-
periment, we trained classifiers conditioned on a
given verb term. Positive instances were taken to
be those with a score of 4.0 or higher, with nega-
tive instances taken to be those with scores of 1.0
or lower (strong agreement by judges that the orig-
inal tweet did not provide evidence of the given
role). Classification results are shown in figure 3.
Note that for a number of verb terms these thresh-
olds left very sparse collections of users. There
were only 8 users, for example, that tweeted the
phrase I smoke ... but were labeled as negative in-
stances of Smokers. Counts are given in Table 2.
Despite the sparsity of some of these classes,
many of the features learned by our classifiers
make intuitive sense. Highlights of the most
highly weighted unigrams from the classification
</bodyText>
<table confidence="0.95098145">
Verb Feature( Rank)
draw drawing, art, book4, sketch14, paper19
race race, hard, winter, won11, training16, run17
run awesome, nike6, fast9, marathon20
blog notes, boom, hacked4, perspective9
cheer cheer, pictures, omg, text, literally
pray through, jesus3, prayers7, lord14, thank17
serve lord, jesus, church, blessed, pray, grace
thank [ ], blessed, lord, trust11, pray12
enroll fall, fat, carry, job, spend, fail15
finish hey, wrong, may8, move9, officially14
spin show, dj, music, dude, ladies, posted, listen
film please, wow, youtube, send, music8
perform [ ], stuck, act, song, tickets7, support16
produce follow, video8, listen10, single11, studio13,
interview fan, latest, awesome, seems
smoke weakness, runs, ti, simply
puff bout, $7, smh9, weed10
spark dont, fat5, blunt6, smoke11
teach forward, amazing, students, great, teacher7
</table>
<tableCaption confidence="0.794667">
Table 3: Most-highly indicative features that a user holds
the associated role given that they used the phrase I VERB
along with select features within the top 20.
</tableCaption>
<bodyText confidence="0.999948">
experiments are shown in Table 3. Taken together
these features suggest that several of our roles can
be distinguished from the background population
by focussing on typical language use. The use of
terms like, e.g., sketch by artists, training by ath-
letes, jesus by Chrisitians, and students by teach-
ers conforms to expected pattern of language use.
</bodyText>
<sectionHeader confidence="0.995944" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999791315789474">
We have shown that verb-argument selectional
preferences relates to the content-based classifica-
tion strategy for latent author attributes. In particu-
lar, we have presented initial studies showing that
mining selectional preferences from third-person
content, such as newswire, can be used to inform
latent author attribute prediction based on first-
person content, such as that appearing in social
media services like Twitter.
Future work should consider the question of
priors. Our study here relied on balanced class
experiments, but the more fine-grained the social
role, the smaller the subset of the population we
might expect will possess that role. Estimating
these priors is thus an important point for future
work, especially if we wish to couple such demo-
graphic predictions within a larger automatic sys-
tem, such as the aggregate prediction of targeted
sentiment (Jiang et al., 2011).
</bodyText>
<footnote confidence="0.62760725">
Acknowledgements This material is partially based
on research sponsored by the NSF under grant IIS-1249516
and by DARPA under agreement number FA8750-13-2-0017
(DEFT).
</footnote>
<figure confidence="0.9867708">
0.8
Accuracy
0.7
0.6
0.5
</figure>
<page confidence="0.995591">
53
</page>
<sectionHeader confidence="0.9898" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999676196078431">
Charley Beller, Rebecca Knowles, Craig Harman,
Shane Bergsma, Margaret Mitchell, and Benjamin
Van Durme. 2014. I’m a belieber: Social roles via
self-identification and conceptual attributes. In Pro-
ceedings of the 52nd Annual Meeting of the Associ-
ation for Computational Linguistics.
Shane Bergsma and Benjamin Van Durme. 2013. Us-
ing Conceptual Class Attributes to Characterize So-
cial Media Users. In Proceedings of ACL.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Discriminative learning of selectional pref-
erence from unlabeled text. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 59–68. Association for
Computational Linguistics.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
Twitter. In Proceedings of EMNLP.
Noam Chomsky. 1965. Aspects of the Theory of Syn-
tax. Number 11. MIT press.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2).
Ido Dagan, Lillian Lee, and Fernando CN Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. Machine Learning, 34(1-3):43–
69.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceeding of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, volume 45, page 216.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, (9).
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of ACL.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245–288.
Liliane Haegeman and Tabea Ihsane. 2001. Adult null
subjects in the non-pro-drop languages: Two diary
dialects. Language acquisition, 9(4):329–346.
Liliane Haegeman. 2013. The syntax of registers: Di-
ary subject omission and the privilege of the root.
Lingua, 130:88–110.
Larry L Jacoby, Fergus IM Craik, and Ian Begg. 1979.
Effects of decision difficulty on recognition and re-
call. Journal of Verbal Learning and Verbal Behav-
ior, 18(5):585–600.
Long Jiang, Mo Yu, Xiaohua Liu, and Tiejun Zhao.
2011. Target-dependent twitter sentiment classifi-
cation. In Proceedings of ACL.
Jerrold J Katz and Jerry A Fodor. 1963. The structure
of a semantic theory. language, pages 170–210.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the MDL principle.
Computational linguistics, 24(2):217–244.
Marc Light and Warren Greiff. 2002. Statistical mod-
els for the induction and use of selectional prefer-
ences. Cognitive Science, 26(3):269–281.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale n-grams. In Proc. LREC, pages 2221–
2227.
Donna Jo Napoli. 1982. Initial material deletion in
English. Glossa, 16(1):5–111.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard H Hovy. 2007.
ISP: Learning inferential selectional preferences. In
HLT-NAACL, pages 564–571.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proceedings of the Work-
shop on Search and Mining User-generated Con-
tents (SMUC).
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61(1):127–159.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How, pages 52–57. Washington,
DC.
Alan Ritter, Masaum, and Oren Etzioni. 2010. A la-
tent dirichlet allocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 424–434. Association for Computational
Linguistics.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via EM-based clustering. In
Proceedings of the 37th annual meeting of the Asso-
ciation for Computational Linguistics, pages 104–
111. Association for Computational Linguistics.
</reference>
<page confidence="0.978368">
54
</page>
<reference confidence="0.9997801">
Kate Scott. 2013. Pragmatically motivated null sub-
jects in English: A relevance theory perspective.
Journal of Pragmatics, 53:68–83.
Randolph Thrasher. 1977. One way to say more by
saying less: A study of so-called subjectless sen-
tences. Kwansei Gakuin University Monograph Se-
ries, 11.
Benjamin Van Durme. 2012a. Jerboa: A toolkit for
randomized and streaming algorithms. Technical
Report 7, Human Language Technology Center of
Excellence, Johns Hopkins University.
Benjamin Van Durme. 2012b. Streaming analysis of
discourse participants. In Proceedings of EMNLP.
Andrew Weir. 2012. Left-edge deletion in English and
subject omission in diaries. English Language and
Linguistics, 16(01):105–129.
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of ICWSM.
</reference>
<page confidence="0.999062">
55
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.782340">
<title confidence="0.999938">Predicting Fine-grained Social Roles with Selectional Preferences</title>
<author confidence="0.99992">Charley Beller Craig Harman Benjamin Van_Durme</author>
<email confidence="0.916189">charleybeller@jhu.educraig@craigharman.netvandurme@cs.jhu.edu</email>
<affiliation confidence="0.925332">Human Language Technology Center of Excellence</affiliation>
<address confidence="0.929105">Johns Hopkins University, Baltimore, MD USA</address>
<abstract confidence="0.9979119375">Selectional preferences, the tendencies of predicates to select for certain semantic classes of arguments, have been successfully applied to a number of tasks in computational linguistics including word sense disambiguation, semantic role labeling, relation extraction, and textual inference. Here we leverage the information encoded in selectional preferences to the task of predicting fine-grained categories of authors on the social media platform Twitter. First person uses of verbs that select for a given social role as subject (e.g. teach ... are used to quickly build up binary classifiers for that role.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Charley Beller</author>
<author>Rebecca Knowles</author>
<author>Craig Harman</author>
<author>Shane Bergsma</author>
<author>Margaret Mitchell</author>
<author>Benjamin Van Durme</author>
</authors>
<title>I’m a belieber: Social roles via self-identification and conceptual attributes.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Beller, Knowles, Harman, Bergsma, Mitchell, Van Durme, 2014</marker>
<rawString>Charley Beller, Rebecca Knowles, Craig Harman, Shane Bergsma, Margaret Mitchell, and Benjamin Van Durme. 2014. I’m a belieber: Social roles via self-identification and conceptual attributes. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Using Conceptual Class Attributes to Characterize Social Media Users.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Bergsma, Van Durme, 2013</marker>
<rawString>Shane Bergsma and Benjamin Van Durme. 2013. Using Conceptual Class Attributes to Characterize Social Media Users. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Randy Goebel</author>
</authors>
<title>Discriminative learning of selectional preference from unlabeled text.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>59--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1929" citStr="Bergsma et al., 2008" startWordPosition="283" endWordPosition="286">sambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., 1999) like their own. One of our contributions here is to show that the literature on selectional preferences relates to the analysis of the first person content transmitted through social media. We make use of a “quick and dirty” method for inducing selectional preferences and apply the resulting collections to the task of predicting fine-grained latent author attributes on Twitter. Our method for inducing selectional preferences is most similar to class-based approaches, though unlike approaches such as by Resnik (1996) we do not require a W</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2008</marker>
<rawString>Shane Bergsma, Dekang Lin, and Randy Goebel. 2008. Discriminative learning of selectional preference from unlabeled text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 59–68. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Burger</author>
<author>John Henderson</author>
<author>George Kim</author>
<author>Guido Zarrella</author>
</authors>
<title>Discriminating gender on Twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2846" citStr="Burger et al., 2011" startWordPosition="429" endWordPosition="432">ing selectional preferences and apply the resulting collections to the task of predicting fine-grained latent author attributes on Twitter. Our method for inducing selectional preferences is most similar to class-based approaches, though unlike approaches such as by Resnik (1996) we do not require a WordNet-like ontology. The vast quantity of informal, first-person text data made available by the rise of social media platforms has encouraged researchers to develop models that predict broad user categories like age, gender, and political preference (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b; Zamal et al., 2012). Such information is useful for large scale demographic research that can fuel computational social science advertising. Similarly to Beller et al. (2014), we are interested in classification that is finer-grained than gender or political affiliation, seeking instead to predict social roles like smoker, student, and artist. We make use of a light-weight, unsupervised method to identify selectional preferences and use the resulting information to rapidly bootstrap classification models. 2 Inducing Selectional Preferences Consider the task of predicting so</context>
</contexts>
<marker>Burger, Henderson, Kim, Zarrella, 2011</marker>
<rawString>John D. Burger, John Henderson, George Kim, and Guido Zarrella. 2011. Discriminating gender on Twitter. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax.</title>
<date>1965</date>
<journal>Number</journal>
<volume>11</volume>
<publisher>MIT press.</publisher>
<contexts>
<context position="1080" citStr="Chomsky, 1965" startWordPosition="150" endWordPosition="151">ncluding word sense disambiguation, semantic role labeling, relation extraction, and textual inference. Here we leverage the information encoded in selectional preferences to the task of predicting fine-grained categories of authors on the social media platform Twitter. First person uses of verbs that select for a given social role as subject (e.g. I teach ... for teacher) are used to quickly build up binary classifiers for that role. 1 Introduction It has long been recognized that linguistic predicates preferentially select arguments that meet certain semantic criteria (Katz and Fodor, 1963; Chomsky, 1965). The verb eat for example selects for an animate subject and a comestible object. While the information encoded by selectional preferences can and has been used to support natural language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of thi</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Noam Chomsky. 1965. Aspects of the Theory of Syntax. Number 11. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<booktitle>Computational linguistics,</booktitle>
<pages>16--1</pages>
<contexts>
<context position="4999" citStr="Church and Hanks, 1990" startWordPosition="777" endWordPosition="780">pervised method inspired by Bergsma and Van Durme (2013) that extracts features from third-person content (i.e. newswire) to build classifiers on first-person content (i.e. tweets). For example, if we read in a news article that an artist drew ..., we can take a tweet saying I drew ... as potential evidence that the author bears the artist social role. We first count all verbs v that appear with role r as subject in the web-scale, part-of-speech tagged n-gram corpus, Google V2 (Lin et al., 2010). The resulting collection of verbs is then ranked by computing their pointwise mutual information (Church and Hanks, 1990) with the subject role r. The PMI of a given role r and a verb v that takes r as subject is given as: P (r, v) PMI(r, v) = log P(r)P(v) Probabilities are estimated from counts of the role-verb pairs along with counts matching the generic subject patterns he and she which serve as general background cases. This gives us a set of verbs that preferentially select for the subset of persons filling the given role. The output of the PMI ranking is a high-recall list of verbs that preferentially select the given social role as subject over a background population. Each such list then underwent a manu</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="1786" citStr="Clark and Weir, 2002" startWordPosition="263" endWordPosition="266">le the information encoded by selectional preferences can and has been used to support natural language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., 1999) like their own. One of our contributions here is to show that the literature on selectional preferences relates to the analysis of the first person content transmitted through social media. We make use of a “quick and dirty” method for inducing selectional preferences and apply the resulting collections to the task of predicting fine-grained latent author attributes on Twitter. Our method for indu</context>
</contexts>
<marker>Clark, Weir, 2002</marker>
<rawString>Stephen Clark and David Weir. 2002. Class-based probability estimation using a semantic hierarchy. Computational Linguistics, 28(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Similarity-based models of word cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1857" citStr="Dagan et al., 1999" startWordPosition="273" endWordPosition="276">ed to support natural language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., 1999) like their own. One of our contributions here is to show that the literature on selectional preferences relates to the analysis of the first person content transmitted through social media. We make use of a “quick and dirty” method for inducing selectional preferences and apply the resulting collections to the task of predicting fine-grained latent author attributes on Twitter. Our method for inducing selectional preferences is most similar to class-based approaches,</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>Ido Dagan, Lillian Lee, and Fernando CN Pereira. 1999. Similarity-based models of word cooccurrence probabilities. Machine Learning, 34(1-3):43– 69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceeding of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>45</volume>
<pages>216</pages>
<contexts>
<context position="1869" citStr="Erk, 2007" startWordPosition="277" endWordPosition="278">l language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., 1999) like their own. One of our contributions here is to show that the literature on selectional preferences relates to the analysis of the first person content transmitted through social media. We make use of a “quick and dirty” method for inducing selectional preferences and apply the resulting collections to the task of predicting fine-grained latent author attributes on Twitter. Our method for inducing selectional preferences is most similar to class-based approaches, though unli</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Katrin Erk. 2007. A simple, similarity-based model for selectional preferences. In Proceeding of the 45th Annual Meeting of the Association for Computational Linguistics, volume 45, page 216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsief</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>9</volume>
<contexts>
<context position="11810" citStr="Fan et al., 2008" startWordPosition="1974" endWordPosition="1977">ed at random from the 1% stream. 3.1 General Classification The positively annotated examples were balanced with data from a background set of Twitter users to produce training and test sets. These test sets were usually of size 40 (20 positive, 20 background), with a few classes being sparser (the smallest test set had only 28 instances). We used the Jerboa (Van Durme, 2012a) platform to convert our data to binary feature vectors over a unigram vocabulary filtered such that the minimum frequency was 5 (across unique users). Training and testing was done with a log-linear model via LibLinear (Fan et al., 2008). Results are shown in Figure 2. As can be seen, a variety of classes in this balanced setup can be predicted with accuracies in the range of 80%. This shows that the information encoded in selectional preferences contains discriminating signal for a variety of these social roles. 3.2 Conditional Classification How accurately can we predict membership in a given class when a Twitter user sends a tweet matching one of the collected verbs? For example, if one sends a tweet saying I race ..., then how likely is it that the author is an athlete? Accuracy 0.8 0.7 0.6 0.5 52 Figure 3: Results of pos</context>
</contexts>
<marker>Fan, Chang, Hsief, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. Journal of Machine Learning Research, (9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikesh Garera</author>
<author>David Yarowsky</author>
</authors>
<title>Modeling latent biographic attributes in conversational genres.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2807" citStr="Garera and Yarowsky, 2009" startWordPosition="421" endWordPosition="424">e use of a “quick and dirty” method for inducing selectional preferences and apply the resulting collections to the task of predicting fine-grained latent author attributes on Twitter. Our method for inducing selectional preferences is most similar to class-based approaches, though unlike approaches such as by Resnik (1996) we do not require a WordNet-like ontology. The vast quantity of informal, first-person text data made available by the rise of social media platforms has encouraged researchers to develop models that predict broad user categories like age, gender, and political preference (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b; Zamal et al., 2012). Such information is useful for large scale demographic research that can fuel computational social science advertising. Similarly to Beller et al. (2014), we are interested in classification that is finer-grained than gender or political affiliation, seeking instead to predict social roles like smoker, student, and artist. We make use of a light-weight, unsupervised method to identify selectional preferences and use the resulting information to rapidly bootstrap classification models. 2 Inducing Selectional Prefere</context>
</contexts>
<marker>Garera, Yarowsky, 2009</marker>
<rawString>Nikesh Garera and David Yarowsky. 2009. Modeling latent biographic attributes in conversational genres. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational linguistics,</journal>
<pages>28--3</pages>
<contexts>
<context position="1435" citStr="Gildea and Jurafsky, 2002" startWordPosition="205" endWordPosition="208">t (e.g. I teach ... for teacher) are used to quickly build up binary classifiers for that role. 1 Introduction It has long been recognized that linguistic predicates preferentially select arguments that meet certain semantic criteria (Katz and Fodor, 1963; Chomsky, 1965). The verb eat for example selects for an animate subject and a comestible object. While the information encoded by selectional preferences can and has been used to support natural language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., 1999) like their own. One of our contributions here is </context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liliane Haegeman</author>
<author>Tabea Ihsane</author>
</authors>
<title>Adult null subjects in the non-pro-drop languages: Two diary dialects. Language acquisition,</title>
<date>2001</date>
<pages>9--4</pages>
<contexts>
<context position="7319" citStr="Haegeman and Ihsane, 2001" startWordPosition="1181" endWordPosition="1184">free public 1% sample of the Twitter Firehose over a single month in 2013. Verbs that had no matches which appeared to be composed by a member of the associated social role were discarded. Using this smaller high-precision set of verbs, we collected tweets from a much larger corpus drawn from 1% sample over the period 2011-2013. One notable feature of the written English in social media is that sentence subjects can be optionally omitted. Subject-drop is a recognized feature of other informal spoken and written registers of English, particularly ‘diary dialects’ (Thrasher, 1977; Napoli, 1982; Haegeman and Ihsane, 2001; Weir, 2012; Haegeman, 2013; Scott, 2013). Because of the prevalence of subjectless cases we collected two sets of tweets: those matching the first person subject pattern I and those where the verb was tweet initial. Example tweets for each of our social roles can be seen in Table 2. 3 Classification via selectional preferences We conducted a set of experiments to gauge the strength of the selectional preference indicators for each social role. For each experiment we used balanced datasets for training and testing with half of the users taken from a random background sample and half from a co</context>
</contexts>
<marker>Haegeman, Ihsane, 2001</marker>
<rawString>Liliane Haegeman and Tabea Ihsane. 2001. Adult null subjects in the non-pro-drop languages: Two diary dialects. Language acquisition, 9(4):329–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liliane Haegeman</author>
</authors>
<title>The syntax of registers: Diary subject omission and the privilege of the root.</title>
<date>2013</date>
<journal>Lingua,</journal>
<pages>130--88</pages>
<contexts>
<context position="7347" citStr="Haegeman, 2013" startWordPosition="1187" endWordPosition="1188">rehose over a single month in 2013. Verbs that had no matches which appeared to be composed by a member of the associated social role were discarded. Using this smaller high-precision set of verbs, we collected tweets from a much larger corpus drawn from 1% sample over the period 2011-2013. One notable feature of the written English in social media is that sentence subjects can be optionally omitted. Subject-drop is a recognized feature of other informal spoken and written registers of English, particularly ‘diary dialects’ (Thrasher, 1977; Napoli, 1982; Haegeman and Ihsane, 2001; Weir, 2012; Haegeman, 2013; Scott, 2013). Because of the prevalence of subjectless cases we collected two sets of tweets: those matching the first person subject pattern I and those where the verb was tweet initial. Example tweets for each of our social roles can be seen in Table 2. 3 Classification via selectional preferences We conducted a set of experiments to gauge the strength of the selectional preference indicators for each social role. For each experiment we used balanced datasets for training and testing with half of the users taken from a random background sample and half from a collection of users identified</context>
</contexts>
<marker>Haegeman, 2013</marker>
<rawString>Liliane Haegeman. 2013. The syntax of registers: Diary subject omission and the privilege of the root. Lingua, 130:88–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larry L Jacoby</author>
<author>Fergus IM Craik</author>
<author>Ian Begg</author>
</authors>
<title>Effects of decision difficulty on recognition and recall.</title>
<date>1979</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<volume>18</volume>
<issue>5</issue>
<contexts>
<context position="6193" citStr="Jacoby et al. (1979)" startWordPosition="993" endWordPosition="996">list then underwent a manual filtering step to rapidly remove non-discriminative verbs and corpus artifacts. One such artifact from our corpus was the term wannabe which was spuriously elevated in the PMI ranking based on the relative frequency of the bigram artist wannabe as compared to she wannabe. Note that in the first case wannabe is best analyzed as a noun, while in the second case a verbal analysis is more plausible. The filtering was performed by one of the authors and generally took less than two minutes per list. The rapidity of the filtering step is in line with findings such as by Jacoby et al. (1979) that relevance based filtering involves less cognitive effort than generation. After filtering the lists contained fewer than 40 verbs selecting each social role. In part because of the pivot from third- to firstperson text we performed a precision test on the remaining verbs to identify which of them are likely to be useful in classifying twitter users. For each remaining verb we extracted all tweets that contained the first person subject pattern I from a small corpus of tweets drawn from the free public 1% sample of the Twitter Firehose over a single month in 2013. Verbs that had no matche</context>
</contexts>
<marker>Jacoby, Craik, Begg, 1979</marker>
<rawString>Larry L Jacoby, Fergus IM Craik, and Ian Begg. 1979. Effects of decision difficulty on recognition and recall. Journal of Verbal Learning and Verbal Behavior, 18(5):585–600.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Mo Yu</author>
<author>Xiaohua Liu</author>
<author>Tiejun Zhao</author>
</authors>
<title>Target-dependent twitter sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Jiang, Yu, Liu, Zhao, 2011</marker>
<rawString>Long Jiang, Mo Yu, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter sentiment classification. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerrold J Katz</author>
<author>Jerry A Fodor</author>
</authors>
<title>The structure of a semantic theory. language,</title>
<date>1963</date>
<pages>170--210</pages>
<contexts>
<context position="1064" citStr="Katz and Fodor, 1963" startWordPosition="146" endWordPosition="149">tational linguistics including word sense disambiguation, semantic role labeling, relation extraction, and textual inference. Here we leverage the information encoded in selectional preferences to the task of predicting fine-grained categories of authors on the social media platform Twitter. First person uses of verbs that select for a given social role as subject (e.g. I teach ... for teacher) are used to quickly build up binary classifiers for that role. 1 Introduction It has long been recognized that linguistic predicates preferentially select arguments that meet certain semantic criteria (Katz and Fodor, 1963; Chomsky, 1965). The verb eat for example selects for an animate subject and a comestible object. While the information encoded by selectional preferences can and has been used to support natural language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent</context>
</contexts>
<marker>Katz, Fodor, 1963</marker>
<rawString>Jerrold J Katz and Jerry A Fodor. 1963. The structure of a semantic theory. language, pages 170–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1998</date>
<journal>Computational linguistics,</journal>
<pages>24--2</pages>
<contexts>
<context position="1380" citStr="Li and Abe, 1998" startWordPosition="197" endWordPosition="200"> that select for a given social role as subject (e.g. I teach ... for teacher) are used to quickly build up binary classifiers for that role. 1 Introduction It has long been recognized that linguistic predicates preferentially select arguments that meet certain semantic criteria (Katz and Fodor, 1963; Chomsky, 1965). The verb eat for example selects for an animate subject and a comestible object. While the information encoded by selectional preferences can and has been used to support natural language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., </context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Hang Li and Naoki Abe. 1998. Generalizing case frames using a thesaurus and the MDL principle. Computational linguistics, 24(2):217–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Light</author>
<author>Warren Greiff</author>
</authors>
<title>Statistical models for the induction and use of selectional preferences.</title>
<date>2002</date>
<journal>Cognitive Science,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="3807" citStr="Light and Greiff (2002)" startWordPosition="577" endWordPosition="580">moker, student, and artist. We make use of a light-weight, unsupervised method to identify selectional preferences and use the resulting information to rapidly bootstrap classification models. 2 Inducing Selectional Preferences Consider the task of predicting social roles in more detail: For a given role, e.g. artist, we want a way to distinguish role-bearing from non-role-bearing users. We can view each social role as being a fine-grained version of a semantic class of the sort required by class-based approaches to selectional preferences (e.g. the work by Resnik (1996) and those reviewed by Light and Greiff (2002)). The goal then is to identify a set of verbs that preferen50 Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 50–55, Baltimore, Maryland, USA, June 26, 2014. c�2014 Association for Computational Linguistics tially select that particular class as argument. Once we have a set of verbs for a given role, simple pattern matches against first person subject templates like I can be used to identify authors that bear that social role. In order to identify verbs that select for a given role r as subject we use an unsupervised method inspired by Ber</context>
</contexts>
<marker>Light, Greiff, 2002</marker>
<rawString>Marc Light and Warren Greiff. 2002. Statistical models for the induction and use of selectional preferences. Cognitive Science, 26(3):269–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Kenneth Church</author>
<author>Heng Ji</author>
<author>Satoshi Sekine</author>
<author>David Yarowsky</author>
<author>Shane Bergsma</author>
</authors>
<title>New tools for web-scale n-grams.</title>
<date>2010</date>
<booktitle>In Proc. LREC,</booktitle>
<pages>2221--2227</pages>
<institution>Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant Narsale.</institution>
<contexts>
<context position="4876" citStr="Lin et al., 2010" startWordPosition="759" endWordPosition="762">thors that bear that social role. In order to identify verbs that select for a given role r as subject we use an unsupervised method inspired by Bergsma and Van Durme (2013) that extracts features from third-person content (i.e. newswire) to build classifiers on first-person content (i.e. tweets). For example, if we read in a news article that an artist drew ..., we can take a tweet saying I drew ... as potential evidence that the author bears the artist social role. We first count all verbs v that appear with role r as subject in the web-scale, part-of-speech tagged n-gram corpus, Google V2 (Lin et al., 2010). The resulting collection of verbs is then ranked by computing their pointwise mutual information (Church and Hanks, 1990) with the subject role r. The PMI of a given role r and a verb v that takes r as subject is given as: P (r, v) PMI(r, v) = log P(r)P(v) Probabilities are estimated from counts of the role-verb pairs along with counts matching the generic subject patterns he and she which serve as general background cases. This gives us a set of verbs that preferentially select for the subset of persons filling the given role. The output of the PMI ranking is a high-recall list of verbs tha</context>
</contexts>
<marker>Lin, Church, Ji, Sekine, Yarowsky, Bergsma, 2010</marker>
<rawString>Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine, David Yarowsky, Shane Bergsma, Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant Narsale. 2010. New tools for web-scale n-grams. In Proc. LREC, pages 2221– 2227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Jo Napoli</author>
</authors>
<title>Initial material deletion in English.</title>
<date>1982</date>
<journal>Glossa,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="7292" citStr="Napoli, 1982" startWordPosition="1179" endWordPosition="1180">rawn from the free public 1% sample of the Twitter Firehose over a single month in 2013. Verbs that had no matches which appeared to be composed by a member of the associated social role were discarded. Using this smaller high-precision set of verbs, we collected tweets from a much larger corpus drawn from 1% sample over the period 2011-2013. One notable feature of the written English in social media is that sentence subjects can be optionally omitted. Subject-drop is a recognized feature of other informal spoken and written registers of English, particularly ‘diary dialects’ (Thrasher, 1977; Napoli, 1982; Haegeman and Ihsane, 2001; Weir, 2012; Haegeman, 2013; Scott, 2013). Because of the prevalence of subjectless cases we collected two sets of tweets: those matching the first person subject pattern I and those where the verb was tweet initial. Example tweets for each of our social roles can be seen in Table 2. 3 Classification via selectional preferences We conducted a set of experiments to gauge the strength of the selectional preference indicators for each social role. For each experiment we used balanced datasets for training and testing with half of the users taken from a random backgroun</context>
</contexts>
<marker>Napoli, 1982</marker>
<rawString>Donna Jo Napoli. 1982. Initial material deletion in English. Glossa, 16(1):5–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Rahul Bhagat</author>
<author>Bonaventura Coppola</author>
<author>Timothy Chklovski</author>
<author>Eduard H Hovy</author>
</authors>
<title>ISP: Learning inferential selectional preferences.</title>
<date>2007</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>564--571</pages>
<contexts>
<context position="1808" citStr="Pantel et al., 2007" startWordPosition="267" endWordPosition="270">oded by selectional preferences can and has been used to support natural language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., 1999) like their own. One of our contributions here is to show that the literature on selectional preferences relates to the analysis of the first person content transmitted through social media. We make use of a “quick and dirty” method for inducing selectional preferences and apply the resulting collections to the task of predicting fine-grained latent author attributes on Twitter. Our method for inducing selectional prefe</context>
</contexts>
<marker>Pantel, Bhagat, Coppola, Chklovski, Hovy, 2007</marker>
<rawString>Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy Chklovski, and Eduard H Hovy. 2007. ISP: Learning inferential selectional preferences. In HLT-NAACL, pages 564–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>David Yarowsky</author>
<author>Abhishek Shreevats</author>
<author>Manaswi Gupta</author>
</authors>
<title>Classifying latent user attributes in Twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Search and Mining User-generated Contents (SMUC).</booktitle>
<contexts>
<context position="2825" citStr="Rao et al., 2010" startWordPosition="425" endWordPosition="428">” method for inducing selectional preferences and apply the resulting collections to the task of predicting fine-grained latent author attributes on Twitter. Our method for inducing selectional preferences is most similar to class-based approaches, though unlike approaches such as by Resnik (1996) we do not require a WordNet-like ontology. The vast quantity of informal, first-person text data made available by the rise of social media platforms has encouraged researchers to develop models that predict broad user categories like age, gender, and political preference (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b; Zamal et al., 2012). Such information is useful for large scale demographic research that can fuel computational social science advertising. Similarly to Beller et al. (2014), we are interested in classification that is finer-grained than gender or political affiliation, seeking instead to predict social roles like smoker, student, and artist. We make use of a light-weight, unsupervised method to identify selectional preferences and use the resulting information to rapidly bootstrap classification models. 2 Inducing Selectional Preferences Consider the </context>
</contexts>
<marker>Rao, Yarowsky, Shreevats, Gupta, 2010</marker>
<rawString>Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta. 2010. Classifying latent user attributes in Twitter. In Proceedings of the Workshop on Search and Mining User-generated Contents (SMUC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional constraints: An information-theoretic model and its computational realization.</title>
<date>1996</date>
<journal>Cognition,</journal>
<volume>61</volume>
<issue>1</issue>
<contexts>
<context position="1746" citStr="Resnik, 1996" startWordPosition="257" endWordPosition="258">ect and a comestible object. While the information encoded by selectional preferences can and has been used to support natural language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., 1999) like their own. One of our contributions here is to show that the literature on selectional preferences relates to the analysis of the first person content transmitted through social media. We make use of a “quick and dirty” method for inducing selectional preferences and apply the resulting collections to the task of predicting fine-grained latent author at</context>
<context position="3761" citStr="Resnik (1996)" startWordPosition="571" endWordPosition="572">stead to predict social roles like smoker, student, and artist. We make use of a light-weight, unsupervised method to identify selectional preferences and use the resulting information to rapidly bootstrap classification models. 2 Inducing Selectional Preferences Consider the task of predicting social roles in more detail: For a given role, e.g. artist, we want a way to distinguish role-bearing from non-role-bearing users. We can view each social role as being a fine-grained version of a semantic class of the sort required by class-based approaches to selectional preferences (e.g. the work by Resnik (1996) and those reviewed by Light and Greiff (2002)). The goal then is to identify a set of verbs that preferen50 Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 50–55, Baltimore, Maryland, USA, June 26, 2014. c�2014 Association for Computational Linguistics tially select that particular class as argument. Once we have a set of verbs for a given role, simple pattern matches against first person subject templates like I can be used to identify authors that bear that social role. In order to identify verbs that select for a given role r as subject</context>
</contexts>
<marker>Resnik, 1996</marker>
<rawString>Philip Resnik. 1996. Selectional constraints: An information-theoretic model and its computational realization. Cognition, 61(1):127–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How,</booktitle>
<pages>52--57</pages>
<location>Washington, DC.</location>
<contexts>
<context position="1335" citStr="Resnik, 1997" startWordPosition="192" endWordPosition="193">tform Twitter. First person uses of verbs that select for a given social role as subject (e.g. I teach ... for teacher) are used to quickly build up binary classifiers for that role. 1 Introduction It has long been recognized that linguistic predicates preferentially select arguments that meet certain semantic criteria (Katz and Fodor, 1963; Chomsky, 1965). The verb eat for example selects for an animate subject and a comestible object. While the information encoded by selectional preferences can and has been used to support natural language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or ge</context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Philip Resnik. 1997. Selectional preference and sense disambiguation. In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How, pages 52–57. Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Masaum</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>424--434</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1650" citStr="Ritter et al. (2010)" startWordPosition="239" endWordPosition="242">tic criteria (Katz and Fodor, 1963; Chomsky, 1965). The verb eat for example selects for an animate subject and a comestible object. While the information encoded by selectional preferences can and has been used to support natural language processing tasks such as word sense disambiguation (Resnik, 1997), syntactic disambiguation (Li and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., 1999) like their own. One of our contributions here is to show that the literature on selectional preferences relates to the analysis of the first person content transmitted through social media. We make use of a “quick and dirty” method for inducing selectional prefere</context>
</contexts>
<marker>Ritter, Masaum, Etzioni, 2010</marker>
<rawString>Alan Ritter, Masaum, and Oren Etzioni. 2010. A latent dirichlet allocation method for selectional preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424–434. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mats Rooth</author>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Glenn Carroll</author>
<author>Franz Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via EM-based clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1985" citStr="Rooth et al., 1999" startWordPosition="292" endWordPosition="295">and Abe, 1998) and semantic role labeling (Gildea and Jurafsky, 2002), much of the work on the topic revolves around developing methods to induce selectional preferences from data. In this setting, end-tasks can be used for evaluation of the resulting collection. Ritter et al. (2010) gave a recent overview of this work, breaking it down into class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity-based approaches (Dagan et al., 1999; Erk, 2007), and approaches using discriminative (Bergsma et al., 2008) or generative probabilistic models (Rooth et al., 1999) like their own. One of our contributions here is to show that the literature on selectional preferences relates to the analysis of the first person content transmitted through social media. We make use of a “quick and dirty” method for inducing selectional preferences and apply the resulting collections to the task of predicting fine-grained latent author attributes on Twitter. Our method for inducing selectional preferences is most similar to class-based approaches, though unlike approaches such as by Resnik (1996) we do not require a WordNet-like ontology. The vast quantity of informal, fir</context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1999</marker>
<rawString>Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil. 1999. Inducing a semantically annotated lexicon via EM-based clustering. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics, pages 104– 111. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Scott</author>
</authors>
<title>Pragmatically motivated null subjects in English: A relevance theory perspective.</title>
<date>2013</date>
<journal>Journal of Pragmatics,</journal>
<pages>53--68</pages>
<contexts>
<context position="7361" citStr="Scott, 2013" startWordPosition="1189" endWordPosition="1190">ngle month in 2013. Verbs that had no matches which appeared to be composed by a member of the associated social role were discarded. Using this smaller high-precision set of verbs, we collected tweets from a much larger corpus drawn from 1% sample over the period 2011-2013. One notable feature of the written English in social media is that sentence subjects can be optionally omitted. Subject-drop is a recognized feature of other informal spoken and written registers of English, particularly ‘diary dialects’ (Thrasher, 1977; Napoli, 1982; Haegeman and Ihsane, 2001; Weir, 2012; Haegeman, 2013; Scott, 2013). Because of the prevalence of subjectless cases we collected two sets of tweets: those matching the first person subject pattern I and those where the verb was tweet initial. Example tweets for each of our social roles can be seen in Table 2. 3 Classification via selectional preferences We conducted a set of experiments to gauge the strength of the selectional preference indicators for each social role. For each experiment we used balanced datasets for training and testing with half of the users taken from a random background sample and half from a collection of users identified as belonging </context>
</contexts>
<marker>Scott, 2013</marker>
<rawString>Kate Scott. 2013. Pragmatically motivated null subjects in English: A relevance theory perspective. Journal of Pragmatics, 53:68–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Thrasher</author>
</authors>
<title>One way to say more by saying less: A study of so-called subjectless sentences.</title>
<date>1977</date>
<journal>Kwansei Gakuin University Monograph Series,</journal>
<volume>11</volume>
<contexts>
<context position="7278" citStr="Thrasher, 1977" startWordPosition="1177" endWordPosition="1178">rpus of tweets drawn from the free public 1% sample of the Twitter Firehose over a single month in 2013. Verbs that had no matches which appeared to be composed by a member of the associated social role were discarded. Using this smaller high-precision set of verbs, we collected tweets from a much larger corpus drawn from 1% sample over the period 2011-2013. One notable feature of the written English in social media is that sentence subjects can be optionally omitted. Subject-drop is a recognized feature of other informal spoken and written registers of English, particularly ‘diary dialects’ (Thrasher, 1977; Napoli, 1982; Haegeman and Ihsane, 2001; Weir, 2012; Haegeman, 2013; Scott, 2013). Because of the prevalence of subjectless cases we collected two sets of tweets: those matching the first person subject pattern I and those where the verb was tweet initial. Example tweets for each of our social roles can be seen in Table 2. 3 Classification via selectional preferences We conducted a set of experiments to gauge the strength of the selectional preference indicators for each social role. For each experiment we used balanced datasets for training and testing with half of the users taken from a ra</context>
</contexts>
<marker>Thrasher, 1977</marker>
<rawString>Randolph Thrasher. 1977. One way to say more by saying less: A study of so-called subjectless sentences. Kwansei Gakuin University Monograph Series, 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
</authors>
<title>Jerboa: A toolkit for randomized and streaming algorithms.</title>
<date>2012</date>
<tech>Technical Report 7,</tech>
<institution>Human Language Technology Center of Excellence, Johns Hopkins University.</institution>
<marker>Van Durme, 2012</marker>
<rawString>Benjamin Van Durme. 2012a. Jerboa: A toolkit for randomized and streaming algorithms. Technical Report 7, Human Language Technology Center of Excellence, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
</authors>
<title>Streaming analysis of discourse participants.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Van Durme, 2012</marker>
<rawString>Benjamin Van Durme. 2012b. Streaming analysis of discourse participants. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Weir</author>
</authors>
<title>Left-edge deletion in English and subject omission in diaries.</title>
<date>2012</date>
<journal>English Language and Linguistics,</journal>
<volume>16</volume>
<issue>01</issue>
<contexts>
<context position="7331" citStr="Weir, 2012" startWordPosition="1185" endWordPosition="1186">e Twitter Firehose over a single month in 2013. Verbs that had no matches which appeared to be composed by a member of the associated social role were discarded. Using this smaller high-precision set of verbs, we collected tweets from a much larger corpus drawn from 1% sample over the period 2011-2013. One notable feature of the written English in social media is that sentence subjects can be optionally omitted. Subject-drop is a recognized feature of other informal spoken and written registers of English, particularly ‘diary dialects’ (Thrasher, 1977; Napoli, 1982; Haegeman and Ihsane, 2001; Weir, 2012; Haegeman, 2013; Scott, 2013). Because of the prevalence of subjectless cases we collected two sets of tweets: those matching the first person subject pattern I and those where the verb was tweet initial. Example tweets for each of our social roles can be seen in Table 2. 3 Classification via selectional preferences We conducted a set of experiments to gauge the strength of the selectional preference indicators for each social role. For each experiment we used balanced datasets for training and testing with half of the users taken from a random background sample and half from a collection of </context>
</contexts>
<marker>Weir, 2012</marker>
<rawString>Andrew Weir. 2012. Left-edge deletion in English and subject omission in diaries. English Language and Linguistics, 16(01):105–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Faiyaz Al Zamal</author>
<author>Wendy Liu</author>
<author>Derek Ruths</author>
</authors>
<title>Homophily and latent attribute inference: Inferring latent attributes of Twitter users from neighbors.</title>
<date>2012</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<contexts>
<context position="2885" citStr="Zamal et al., 2012" startWordPosition="436" endWordPosition="439">he resulting collections to the task of predicting fine-grained latent author attributes on Twitter. Our method for inducing selectional preferences is most similar to class-based approaches, though unlike approaches such as by Resnik (1996) we do not require a WordNet-like ontology. The vast quantity of informal, first-person text data made available by the rise of social media platforms has encouraged researchers to develop models that predict broad user categories like age, gender, and political preference (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b; Zamal et al., 2012). Such information is useful for large scale demographic research that can fuel computational social science advertising. Similarly to Beller et al. (2014), we are interested in classification that is finer-grained than gender or political affiliation, seeking instead to predict social roles like smoker, student, and artist. We make use of a light-weight, unsupervised method to identify selectional preferences and use the resulting information to rapidly bootstrap classification models. 2 Inducing Selectional Preferences Consider the task of predicting social roles in more detail: For a given </context>
</contexts>
<marker>Zamal, Liu, Ruths, 2012</marker>
<rawString>Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012. Homophily and latent attribute inference: Inferring latent attributes of Twitter users from neighbors. In Proceedings of ICWSM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
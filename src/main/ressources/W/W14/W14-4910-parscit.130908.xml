<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008981">
<title confidence="0.998713">
A Corpus Study for Identifying Evidence on Microblogs
</title>
<author confidence="0.987124">
Paul Reiserts Junta Mizuno2 Miwa Kannos Naoaki Okazakis,3 Kentaro Inuis
</author>
<affiliation confidence="0.964515">
1 Gradute School of Information Sciences, Tohoku University / Miyagi, Japan
</affiliation>
<address confidence="0.870339">
2 Resilient ICT Research Center, NICT / Miyagi, Japan 3 Japan Science and Technology Agency (JST) / Tokyo, Japan
</address>
<email confidence="0.966738">
preisert@ecei.tohoku.ac.jp junta-m@nict.go.jp {meihe, okazaki, inui}@ecei.tohoku.ac.jp
</email>
<sectionHeader confidence="0.997006" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999803222222222">
Microblogs are a popular way for users to communicate and have recently caught the attention
of researchers in the natural language processing (NLP) field. However, regardless of their rising
popularity, little attention has been given towards determining the properties of discourse rela-
tions for the rapid, large-scale microblog data. Therefore, given their importance for various NLP
tasks, we begin a study of discourse relations on microblogs by focusing on evidence relations.
As no annotated corpora for evidence relations on microblogs exist, we conduct a corpus study
to identify such relations on Twitter, a popular microblogging service. We create annotation
guidelines, conduct a large-scale annotation phase, and develop a corpus of annotated evidence
relations. Finally, we report our observations, annotation difficulties, and data statistics.
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999918785714286">
Microblogs have become a popular method for users to express their ideas and communicate with other
users. Twitter1, a popular microblogging service, has recently been the attraction of many natural lan-
guage processing (NLP) tasks ranging from flu epidemic detection (Aramaki et al., 2011) to gender
inference for its users (Ciot et al., 2013). While various tasks are available, despite its daily, rapid large-
scale data, evidence relation studies have yet to be explored using Twitter data. Previous research exists
for determining the credibility of information on Twitter (Castillo et al., 2011); however, the focus of this
work is to determine and annotate evidence relations on microblogs.
Our primary motivation behind focusing on evidence relations includes the possibility of discovering
support for a claim which can support the debunking of false information. During the March 2011
Great East Japan Earthquake and Tsunami disaster, victims turned to the Internet in order to obtain
information on current conditions, such as family member whereabouts, refuge center information, and
general information (Sakaki et al., 2011). However, false information, such as the popular Cosmo Oil
explosion causing toxic rain, interfered with those looking to find correct information on the status of
the disaster areas (Okazaki et al., 2013). This is a scenario in which identification of potentially false
information is necessary in order to provide accurate information to victims and others relying on and
trusting in the Internet. Therefore, as a start to find support for counterclaims for false information such
as the Cosmo Oil explosion, we focus on dialogue between two individuals: a topic starter, or a post
with no parent; and a respondent who provides either an agreeing or disagreeing claim and support for
their claim. An example is provided in Figure 1.
We note that our task can appear similar to the field of Why-QA (Verberne, 2006; Oh et al., 2013;
Mrozinski et al., 2008), which attempts to discover the answer for Why questions. Given our task of
discovering agreeing or conflicting claims, and finding specific reasoning to support the claim, we end
up with a Why question similar to Why is it true/not true that X, where X is the contents of the claim
found in the parent post. However, we consider source mentions or hyperlinks, which can either stand
alone or be contained in a statement, question, or request, as a way to answer the above question.
To the best of our knowledge, no corpora for evidence relations on microblogs currently exists. In
terms of argumentation corpora, the Araucaria Argumentation Corpus 2 exists which utilizes various
argumentation schemes (Walton, 1996; Katzav and Reed, 2004; Pollock, 1995). In this work, we
</bodyText>
<footnote confidence="0.9956925">
1https://twitter.com
2http://araucaria.computing.dundee.ac.uk/doku.php
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
</footnote>
<page confidence="0.952475">
70
</page>
<note confidence="0.277627">
LAW VIII - The 8th Linguistic Annotation Workshop, pages 70–74,
Dublin, Ireland, August 23-24 2014.
</note>
<figure confidence="0.976417166666667">
�$����&amp;quot;�+����!�/.�-#$3297�� :58; ����-*�
A cat won’t catch a cold that way. A cat cold is caused by a virus preventable by vaccine.
�#���� ���,��&apos;�� (�416;���% ��!�!���&amp;��)�0�+&amp;quot;
My cat is limping due to the heat. But, if I keep the air conditioner on, he’ll catch a cold.
Topic Starter Tweet
Respondent Tweet
</figure>
<figureCaption confidence="0.999962">
Figure 1: topic starter post and respondent post on the microbloging service Twitter.
</figureCaption>
<bodyText confidence="0.999519166666667">
manually annotate evidence relation claim and support. We conduct a corpus study that uses both current
data and March 2011 data from Twitter, manually observing its structure and evidence, and devising
guidelines based on our findings. We utilize these guidelines for conducting a large-scale annotation
stage and develop a corpus with our results. We present our findings, challenges in annotation, and also
the result statistics in the later sections. The corpora and annotation guidelines are currently available at:
http://www.cl.ecei.tohoku.ac.jp/index.php?Open%20Resources%2FEvidence%20Relation%20Corpusw
</bodyText>
<sectionHeader confidence="0.974362" genericHeader="method">
2 Annotation Method
</sectionHeader>
<bodyText confidence="0.998694666666667">
In this section, we describe evidence relation structure, target data, and our annotation method outline.
Evidence relations, defined by Mann and Thompson (1988) consist of a claim, or something that an
author wishes for a reader to believe, and support, or something that increases the believability of the
claim, and it can be understood by the following: The program as published for calendar year 1980
really works. In only a few minutes, I entered all the figures from my 1980 tax return and got a result
which agreed with my hand calculations to the penny., where the latter is support to the former claim.
With this in mind, we aim to explore what type of claims and support units exist on microblogs. Our
microblog choice is Twitter, where users post tweets containing up to 140 characters. Tweets may then
be replied to by other users. Each pair in our corpus consists of, what we refer to as, a topic starter’s
tweet and all of its direct reply tweets, or respondent’s tweets. The topic starter’s tweet is a top-level
tweet not in response to another tweet, and the respondent tweet consists of a tweet directly in reply to
the topic starter’s tweet. We then discover respondent claims that agree or disagree with the topic starter.
In addition, we target only pairs which contain an evidence relation.
The outline for annotation is as follows: 1) Given two tweets (topic starter and respondent), detect
relation at agreeing or disagreeing level 2) Mark the claim and support in the respondent tweet
</bodyText>
<figureCaption confidence="0.98894">
Figure 2: Evidence relation within a response.
</figureCaption>
<bodyText confidence="0.999989666666667">
We target this scenario because: 1) we assume that a topic will be presented in a topic starter’s tweet
and a respondent’s direct reply will be responding to the topic content, and 2) it is possible to lose
important information, such as topic keywords, for a reply tweet not in reply to a topic starter tweet.
Using this outline, we utilized Japanese Twitter data from the 2011 Great Eastern Tohoku Earthquake,
specifically hottolink3 data, and created a list of guidelines to use for our large-scale annotation in the
next section by manually observing roughly 6,000 tweet pairs.
</bodyText>
<sectionHeader confidence="0.997538" genericHeader="method">
3 Large-scale Annotation
</sectionHeader>
<bodyText confidence="0.999701">
In order to obtain and observe more evidence relations, we conducted a large-scale annotation stage. We
discuss the data and our statistics and observations.
</bodyText>
<subsectionHeader confidence="0.995055">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.995391">
Using the guidelines in the previous section, we composed a list of 56,033 filtered tweets from various
time periods, shown in the table below.
</bodyText>
<page confidence="0.400615">
3http://www.hottolink.co.jp/press/936
</page>
<figure confidence="0.9962595">
Topic Starter
Respondent
千葉の火災の影響で雨降ったら危ないって言われたけど、本当なのだろうか?
それは数日前から広まってるデマっぽいです。 ご注意下さい。 http://...
I’ve heard that rain will be dangerous due to the fire in Chiba, but is that true?
Claim
That’s false information spread from a few days ago. Be cautious. http://...
Counter Claim
Evidence Relation
Support
</figure>
<page confidence="0.987233">
71
</page>
<tableCaption confidence="0.985506">
Table 1: Data for large-scale annotation phase (A = Agree, D = Disagree, P = Partly A/D, O = Other)
</tableCaption>
<table confidence="0.999076125">
# Set Pairs Evidence A D P O
1 3-11 False Rumor topic starter Data 5753 1029 177 637 74 141
2 Togetter Controversial Category Data 2410 283 164 105 12 2
3 Togetter Negative Tag Data 1233 129 51 71 7 0
4 Twitter Random Controversial Topic Filtered Data 6918 277 168 94 14 1
5 3-11 Random Data 13064 381 241 115 21 4
6 3-11 Negative respondent Keyword Data 26655 1543 836 521 126 60
Total 56033 3642 1637 1543 254 208
</table>
<bodyText confidence="0.993371058823529">
For Sets 1, 5, 6, we utilize the hottolink corpus mentioned briefly in the previous section. Set 1
consists of filtered pairs containing a well-known rumored topic from a list of 10 topics, such as Cosmo
Oil Toxic Rain and Drinking Isodine for Radiation Prevention, and also contained a negative keyword in
the respondent’s tweet. We also included all other direct replies for the topic starter’s tweet. Similarly,
Set 5 contains random data from the hottolink corpus, unfiltered, and Set 6 contains pairs filtered via a
negative keyword in the reply only.
Set 2 consists of crawled data from Togetter4. Togetter offers a summarization of popular, and poten-
tially controversial, tweets for various categories, such as news, society, and sports. We first crawled all
popular categories around January 2014 and obtained unique tweet IDs. We then used the Twitter API5
to extract the tweet information from its ID in order to determine if it was a direct respondent tweet. If
so, we obtained its topic starter tweet and thus created our pairs.
For Set 3, we appended negative keywords to the Togetter hyperlink (e.g. http://togetter.com/t/デマ)
in order to obtain tweets that had been tagged with a negative keyword. We then used the same procedure
as Set 2 in order to obtain topic starter and respondent tweet pairs.
Finally, Set 4 consists of 6,918 tweet pairs randomly selected from a collection of tweet pairs from
Togetter, where each topic starter tweet is filtered by a topic from a list of around 300 controversial
topics.
</bodyText>
<subsectionHeader confidence="0.999355">
3.2 Statistics and Observations
</subsectionHeader>
<bodyText confidence="0.9999775">
In this section, we summarize the results of the annotated large-scale corpus by first providing infor-
mation on the discovered evidence relations. Of 56,033 pairs, 3,642, or roughly 6.5%, were labeled as
containing an evidence relation. Shown in Table 1 are the specific amount of evidence relations found
in each set, along with the exact amount of claims that either agree, disagree, partly agree and disagree,
and other. Also shown in Table 1 is the number of agreeing, disagreeing, partly agreeing/disagreeing,
and other statistics for pairs labeled as evidence for each set.
</bodyText>
<subsectionHeader confidence="0.717583">
3.2.1 Type Distribution
</subsectionHeader>
<bodyText confidence="0.918317833333333">
Since an important goal of this paper was to deter-
mine what types of claims and support we would dis-
cover, we classified random annotated tweet pairs by
claim type and support type.
attitude Claim contains only reply user attitude
(e.g., “I agree with you” or “It’s false information”)
request Claim requests some action (e.g., “Please
delete and correct your tweet immediately”)
question Claim is a question regarding the original
tweet (e.g., “Why do you think so?” or only “?”)
statement Claim is an opinion of a reply user (e.g.,
“Radiation cannot be reduced by a normal filter.”)
</bodyText>
<footnote confidence="0.998275">
4http://togetter.com
5https://dev.twitter.com/docs/api
</footnote>
<tableCaption confidence="0.718982">
Table 2: Type distribution results
</tableCaption>
<table confidence="0.987029928571428">
Claim Type Support Type Disaster General
causality 36 27
attitude elaboration 45 40
source 35 7
causality 13 9
request elaboration 4 8
source 15 0
causality 21 22
statement elaboration 49 31
source 12 7
causality 1 2
question elaboration 3 9
source 2 0
summary 236 162
</table>
<page confidence="0.997377">
72
</page>
<bodyText confidence="0.999616416666667">
Each of the three types of support (below) are in square brackets.
causality Support is a reason of a claim (e.g., “Isodine is no good because [it will ruin your health]”)
elaboration Support is not a reason of a claim, but an elaboration (e.g., “topic starter: I definitely do
not ride side by side with a car when I’m on my bicycle. respondent: Me too. [I do not ride side by side
even when I ride a motorbike]”)
source Support contains source information of the claim, such as hyperlink and name of the media
(e.g., “Please read this web site [URL]” or “I saw it on the TV”)
From Table 2, we found many source samples during disaster times but not for non-disaster periods.
For our second finding, we discovered that attitude and statement were tweeted with support, while
request and question were not. This indicates that people require some action without any support. For
our third finding, we found that there were many replies which contain a statement and its support, while
Twitter allows only 140 characters. This indicates many informative support segments on Twitter.
</bodyText>
<subsectionHeader confidence="0.893767">
3.2.2 Annotation Issues
</subsectionHeader>
<bodyText confidence="0.945811210526316">
Below we enumerate issues that were encountered during our annotation process.
Reliability For determining annotation reliability, we had 10% of random samples from Set 1 anno-
tated by another annotator and found that the inter-annotator agreement Cohen’s kappa value was only
.476. Both annotators marked 45 of the same pairs as evidence. Annotator A marked 60 other pairs
as evidence, while Annotator B marked 15 other pairs as evidence. We believe this statistic is because
tweets with evidence were infrequent and that many examples contained implicit relations, opposed to
containing a discourse marker. From Annotator A’s results, we found that only 9 examples contained an
explicit discourse marker and 96 did not. Prasad et al. (2008) has already recognized that it is difficult
to annotate relations when no discourse marker is present. We plan to automatically annotate evidence
relations via machine learning and provide a probability that a pair is evidence to help manual annotation.
Multiple Claims With Twitter’s character constraints, we expected to discover only one claim per
reply with multiple support segments. However, we found that a few of our annotated segments contained
multiple claim and multiple support segments.
Range Annotation range was a problem we discovered after observing our annotated data. Although
such annotated cases were small (only 2 respondent tweets), most likely due to annotators avoiding such
annotations, we still believe this type of annotation is important for future work. The example below was
labeled as unsure:
{n-7,_T__石油o件lJ:}CLAIM{本社HP &amp;quot;}SUPPORT{T �: tlZL�JZto }CLAIM ({The
Cosmo Oil case,}CLAIM {on the official HP,}SUPPORT {is publicly announced false.}CLAIM)
</bodyText>
<sectionHeader confidence="0.998445" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999939538461538">
As no corpora exists for evidence relations on microblogs, we conducted a corpus study using
the popular microblogging service, Twitter. We created a list of guidelines for evidence rela-
tion annotation by observing roughly 6,000 tweet pairs from March 2011 Twitter data, or disaster-
specific data. Next, we conducted a large-scale annotation stage, consisting of 56,033 tweets, and
discovered 3,642 contained a type of evidence relation. Our annotated data set is available at:
http://www.cl.ecei.tohoku.ac.jp/index.php?Open%20Resources%2FEvidence%20Relation%20Corpus
We manually observed that the presence of evidence relations do indeed exist on microblogs; however,
their existence is rather infrequent. To address this sparsity issue for future annotation, we plan to
increase the number of pairs containing an evidence relation per data set by constructing a model that
can automatically annotate evidence relations and provide a probability that a pair contains an evidence
relation. In this work, we did not analyze the quality of evidence we discovered. Therefore, we aim
towards determining the factuality, or degree of certainty, for a given claim and support in order to
determine the evidence relation’s overall quality.
</bodyText>
<page confidence="0.998453">
73
</page>
<sectionHeader confidence="0.999214" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9924894">
We would like to acknowledge MEXT (Ministry of Education, Culture, Sports, Science and Technol-
ogy) for their generous financial support via the Research Student Scholarship. This study was partly
supported by Japan Society for the Promotion of Science (JSPS) KAKENHI Grant No. 23240018 and
Japan Science and Technology Agency (JST). Furthermore, we would like to also thank Eric Nichols
(Honda Research Institute Japan Co., Ltd.) for his discussions on the topic of evidence relations.
</bodyText>
<sectionHeader confidence="0.999415" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999446517241379">
Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita. 2011. Twitter catches the flu: Detecting influenza epi-
demics using Twitter. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
EMNLP ’11, pages 1568–1576.
Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. 2011. Information credibility on Twitter. In Proceedings
of the 20th International Conference on World Wide Web, WWW ’11, pages 675–684.
Morgane Ciot, Morgan Sonderegger, and Derek Ruths. 2013. Gender inference of Twitter users in non-English
contexts. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages
1136–1145.
Joel Katzav and Chris Reed. 2004. A classification system for arguments. Technical Report.
William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243–281.
Joanna Mrozinski, Edward Whittaker, and Sadaoki Furui. 2008. Collecting a why-question corpus for develop-
ment and evaluation of an automatic QA-system. In Proceedings of ACL-08: HLT, pages 443–451.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto, Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake. 2013.
Why-question answering using intra- and inter-sentential causal relations. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1733–1743.
Naoaki Okazaki, Keita Nabeshima, Kento Watanabe, Junta Mizuno, and Kentaro Inui. 2013. Extracting and
aggregating false information from microblogs. In Proceedings of the Workshop on Language Processing and
Crisis Information, pages 36–43.
John L. Pollock. 1995. Cognitive Carpentry: A Blueprint for How to Build a Person. MIT Press.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.
2008. The penn discourse treebank 2.0. In Proceedings of the Sixth International Conference on Language
Resources and Evaluation (LREC’08).
Takeshi Sakaki, Fujio Toriumi, and Yutaka Matsuo. 2011. Tweet trend analysis in an emergency situation. In
Proceedings of the Special Workshop on Internet and Disasters, SWID ’11, pages 3:1–3:8.
Suzan Verberne. 2006. Developing an approach for why-question answering. In Proceedings of the Eleventh Con-
ference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop,
EACL ’06, pages 39–46.
Douglas N. Walton. 1996. Argumentation Schemes for Presumptive Reasoning. Psychology Press.
</reference>
<page confidence="0.999131">
74
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.262761">
<title confidence="0.997188">A Corpus Study for Identifying Evidence on Microblogs</title>
<author confidence="0.982635">Junta Miwa Naoaki Kentaro</author>
<affiliation confidence="0.901591">School of Information Sciences, Tohoku University / Miyagi, Japan</affiliation>
<note confidence="0.4271395">ICT Research Center, NICT / Miyagi, Japan 3Japan Science and Technology Agency (JST) / Tokyo, Japan junta-m@nict.go.jp okazaki,</note>
<abstract confidence="0.9968474">Microblogs are a popular way for users to communicate and have recently caught the attention of researchers in the natural language processing (NLP) field. However, regardless of their rising popularity, little attention has been given towards determining the properties of discourse relations for the rapid, large-scale microblog data. Therefore, given their importance for various NLP tasks, we begin a study of discourse relations on microblogs by focusing on evidence relations. As no annotated corpora for evidence relations on microblogs exist, we conduct a corpus study to identify such relations on Twitter, a popular microblogging service. We create annotation guidelines, conduct a large-scale annotation phase, and develop a corpus of annotated evidence relations. Finally, we report our observations, annotation difficulties, and data statistics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eiji Aramaki</author>
<author>Sachiko Maskawa</author>
<author>Mizuki Morita</author>
</authors>
<title>Twitter catches the flu: Detecting influenza epidemics using Twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1568--1576</pages>
<contexts>
<context position="1574" citStr="Aramaki et al., 2011" startWordPosition="219" endWordPosition="222">s exist, we conduct a corpus study to identify such relations on Twitter, a popular microblogging service. We create annotation guidelines, conduct a large-scale annotation phase, and develop a corpus of annotated evidence relations. Finally, we report our observations, annotation difficulties, and data statistics. 1 Introduction Microblogs have become a popular method for users to express their ideas and communicate with other users. Twitter1, a popular microblogging service, has recently been the attraction of many natural language processing (NLP) tasks ranging from flu epidemic detection (Aramaki et al., 2011) to gender inference for its users (Ciot et al., 2013). While various tasks are available, despite its daily, rapid largescale data, evidence relation studies have yet to be explored using Twitter data. Previous research exists for determining the credibility of information on Twitter (Castillo et al., 2011); however, the focus of this work is to determine and annotate evidence relations on microblogs. Our primary motivation behind focusing on evidence relations includes the possibility of discovering support for a claim which can support the debunking of false information. During the March 20</context>
</contexts>
<marker>Aramaki, Maskawa, Morita, 2011</marker>
<rawString>Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita. 2011. Twitter catches the flu: Detecting influenza epidemics using Twitter. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1568–1576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Castillo</author>
<author>Marcelo Mendoza</author>
<author>Barbara Poblete</author>
</authors>
<title>Information credibility on Twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th International Conference on World Wide Web, WWW ’11,</booktitle>
<pages>675--684</pages>
<contexts>
<context position="1883" citStr="Castillo et al., 2011" startWordPosition="267" endWordPosition="270">tistics. 1 Introduction Microblogs have become a popular method for users to express their ideas and communicate with other users. Twitter1, a popular microblogging service, has recently been the attraction of many natural language processing (NLP) tasks ranging from flu epidemic detection (Aramaki et al., 2011) to gender inference for its users (Ciot et al., 2013). While various tasks are available, despite its daily, rapid largescale data, evidence relation studies have yet to be explored using Twitter data. Previous research exists for determining the credibility of information on Twitter (Castillo et al., 2011); however, the focus of this work is to determine and annotate evidence relations on microblogs. Our primary motivation behind focusing on evidence relations includes the possibility of discovering support for a claim which can support the debunking of false information. During the March 2011 Great East Japan Earthquake and Tsunami disaster, victims turned to the Internet in order to obtain information on current conditions, such as family member whereabouts, refuge center information, and general information (Sakaki et al., 2011). However, false information, such as the popular Cosmo Oil expl</context>
</contexts>
<marker>Castillo, Mendoza, Poblete, 2011</marker>
<rawString>Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. 2011. Information credibility on Twitter. In Proceedings of the 20th International Conference on World Wide Web, WWW ’11, pages 675–684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morgane Ciot</author>
<author>Morgan Sonderegger</author>
<author>Derek Ruths</author>
</authors>
<title>Gender inference of Twitter users in non-English contexts.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1136--1145</pages>
<contexts>
<context position="1628" citStr="Ciot et al., 2013" startWordPosition="229" endWordPosition="232">ions on Twitter, a popular microblogging service. We create annotation guidelines, conduct a large-scale annotation phase, and develop a corpus of annotated evidence relations. Finally, we report our observations, annotation difficulties, and data statistics. 1 Introduction Microblogs have become a popular method for users to express their ideas and communicate with other users. Twitter1, a popular microblogging service, has recently been the attraction of many natural language processing (NLP) tasks ranging from flu epidemic detection (Aramaki et al., 2011) to gender inference for its users (Ciot et al., 2013). While various tasks are available, despite its daily, rapid largescale data, evidence relation studies have yet to be explored using Twitter data. Previous research exists for determining the credibility of information on Twitter (Castillo et al., 2011); however, the focus of this work is to determine and annotate evidence relations on microblogs. Our primary motivation behind focusing on evidence relations includes the possibility of discovering support for a claim which can support the debunking of false information. During the March 2011 Great East Japan Earthquake and Tsunami disaster, v</context>
</contexts>
<marker>Ciot, Sonderegger, Ruths, 2013</marker>
<rawString>Morgane Ciot, Morgan Sonderegger, and Derek Ruths. 2013. Gender inference of Twitter users in non-English contexts. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1136–1145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Katzav</author>
<author>Chris Reed</author>
</authors>
<title>A classification system for arguments.</title>
<date>2004</date>
<tech>Technical Report.</tech>
<contexts>
<context position="4016" citStr="Katzav and Reed, 2004" startWordPosition="609" endWordPosition="612">, and finding specific reasoning to support the claim, we end up with a Why question similar to Why is it true/not true that X, where X is the contents of the claim found in the parent post. However, we consider source mentions or hyperlinks, which can either stand alone or be contained in a statement, question, or request, as a way to answer the above question. To the best of our knowledge, no corpora for evidence relations on microblogs currently exists. In terms of argumentation corpora, the Araucaria Argumentation Corpus 2 exists which utilizes various argumentation schemes (Walton, 1996; Katzav and Reed, 2004; Pollock, 1995). In this work, we 1https://twitter.com 2http://araucaria.computing.dundee.ac.uk/doku.php This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 70 LAW VIII - The 8th Linguistic Annotation Workshop, pages 70–74, Dublin, Ireland, August 23-24 2014. �$����&amp;quot;�+����!�/.�-#$3297�� :58; ����-*� A cat won’t catch a cold that way. A cat cold is caused by a virus preventable by vaccine. �#���� ���,��&apos;�� (�416;���% ��!�!���&amp;��)�0�+&amp;quot; M</context>
</contexts>
<marker>Katzav, Reed, 2004</marker>
<rawString>Joel Katzav and Chris Reed. 2004. A classification system for arguments. Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="5625" citStr="Mann and Thompson (1988)" startWordPosition="827" endWordPosition="830">ucture and evidence, and devising guidelines based on our findings. We utilize these guidelines for conducting a large-scale annotation stage and develop a corpus with our results. We present our findings, challenges in annotation, and also the result statistics in the later sections. The corpora and annotation guidelines are currently available at: http://www.cl.ecei.tohoku.ac.jp/index.php?Open%20Resources%2FEvidence%20Relation%20Corpusw 2 Annotation Method In this section, we describe evidence relation structure, target data, and our annotation method outline. Evidence relations, defined by Mann and Thompson (1988) consist of a claim, or something that an author wishes for a reader to believe, and support, or something that increases the believability of the claim, and it can be understood by the following: The program as published for calendar year 1980 really works. In only a few minutes, I entered all the figures from my 1980 tax return and got a result which agreed with my hand calculations to the penny., where the latter is support to the former claim. With this in mind, we aim to explore what type of claims and support units exist on microblogs. Our microblog choice is Twitter, where users post tw</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joanna Mrozinski</author>
<author>Edward Whittaker</author>
<author>Sadaoki Furui</author>
</authors>
<title>Collecting a why-question corpus for development and evaluation of an automatic QA-system.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>443--451</pages>
<contexts>
<context position="3276" citStr="Mrozinski et al., 2008" startWordPosition="488" endWordPosition="491">h identification of potentially false information is necessary in order to provide accurate information to victims and others relying on and trusting in the Internet. Therefore, as a start to find support for counterclaims for false information such as the Cosmo Oil explosion, we focus on dialogue between two individuals: a topic starter, or a post with no parent; and a respondent who provides either an agreeing or disagreeing claim and support for their claim. An example is provided in Figure 1. We note that our task can appear similar to the field of Why-QA (Verberne, 2006; Oh et al., 2013; Mrozinski et al., 2008), which attempts to discover the answer for Why questions. Given our task of discovering agreeing or conflicting claims, and finding specific reasoning to support the claim, we end up with a Why question similar to Why is it true/not true that X, where X is the contents of the claim found in the parent post. However, we consider source mentions or hyperlinks, which can either stand alone or be contained in a statement, question, or request, as a way to answer the above question. To the best of our knowledge, no corpora for evidence relations on microblogs currently exists. In terms of argument</context>
</contexts>
<marker>Mrozinski, Whittaker, Furui, 2008</marker>
<rawString>Joanna Mrozinski, Edward Whittaker, and Sadaoki Furui. 2008. Collecting a why-question corpus for development and evaluation of an automatic QA-system. In Proceedings of ACL-08: HLT, pages 443–451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Kentaro Torisawa</author>
<author>Chikara Hashimoto</author>
<author>Motoki Sano</author>
<author>Stijn De Saeger</author>
<author>Kiyonori Ohtake</author>
</authors>
<title>Why-question answering using intra- and inter-sentential causal relations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1733--1743</pages>
<marker>Oh, Torisawa, Hashimoto, Sano, De Saeger, Ohtake, 2013</marker>
<rawString>Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto, Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake. 2013. Why-question answering using intra- and inter-sentential causal relations. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1733–1743.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
<author>Keita Nabeshima</author>
<author>Kento Watanabe</author>
<author>Junta Mizuno</author>
<author>Kentaro Inui</author>
</authors>
<title>Extracting and aggregating false information from microblogs.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Language Processing and Crisis Information,</booktitle>
<pages>36--43</pages>
<contexts>
<context position="2625" citStr="Okazaki et al., 2013" startWordPosition="378" endWordPosition="381">ehind focusing on evidence relations includes the possibility of discovering support for a claim which can support the debunking of false information. During the March 2011 Great East Japan Earthquake and Tsunami disaster, victims turned to the Internet in order to obtain information on current conditions, such as family member whereabouts, refuge center information, and general information (Sakaki et al., 2011). However, false information, such as the popular Cosmo Oil explosion causing toxic rain, interfered with those looking to find correct information on the status of the disaster areas (Okazaki et al., 2013). This is a scenario in which identification of potentially false information is necessary in order to provide accurate information to victims and others relying on and trusting in the Internet. Therefore, as a start to find support for counterclaims for false information such as the Cosmo Oil explosion, we focus on dialogue between two individuals: a topic starter, or a post with no parent; and a respondent who provides either an agreeing or disagreeing claim and support for their claim. An example is provided in Figure 1. We note that our task can appear similar to the field of Why-QA (Verbe</context>
</contexts>
<marker>Okazaki, Nabeshima, Watanabe, Mizuno, Inui, 2013</marker>
<rawString>Naoaki Okazaki, Keita Nabeshima, Kento Watanabe, Junta Mizuno, and Kentaro Inui. 2013. Extracting and aggregating false information from microblogs. In Proceedings of the Workshop on Language Processing and Crisis Information, pages 36–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John L Pollock</author>
</authors>
<title>Cognitive Carpentry: A Blueprint for How to Build a Person.</title>
<date>1995</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4032" citStr="Pollock, 1995" startWordPosition="613" endWordPosition="614">reasoning to support the claim, we end up with a Why question similar to Why is it true/not true that X, where X is the contents of the claim found in the parent post. However, we consider source mentions or hyperlinks, which can either stand alone or be contained in a statement, question, or request, as a way to answer the above question. To the best of our knowledge, no corpora for evidence relations on microblogs currently exists. In terms of argumentation corpora, the Araucaria Argumentation Corpus 2 exists which utilizes various argumentation schemes (Walton, 1996; Katzav and Reed, 2004; Pollock, 1995). In this work, we 1https://twitter.com 2http://araucaria.computing.dundee.ac.uk/doku.php This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 70 LAW VIII - The 8th Linguistic Annotation Workshop, pages 70–74, Dublin, Ireland, August 23-24 2014. �$����&amp;quot;�+����!�/.�-#$3297�� :58; ����-*� A cat won’t catch a cold that way. A cat cold is caused by a virus preventable by vaccine. �#���� ���,��&apos;�� (�416;���% ��!�!���&amp;��)�0�+&amp;quot; My cat is limping</context>
</contexts>
<marker>Pollock, 1995</marker>
<rawString>John L. Pollock. 1995. Cognitive Carpentry: A Blueprint for How to Build a Person. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The penn discourse treebank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08).</booktitle>
<contexts>
<context position="13761" citStr="Prasad et al. (2008)" startWordPosition="2179" endWordPosition="2182">iability, we had 10% of random samples from Set 1 annotated by another annotator and found that the inter-annotator agreement Cohen’s kappa value was only .476. Both annotators marked 45 of the same pairs as evidence. Annotator A marked 60 other pairs as evidence, while Annotator B marked 15 other pairs as evidence. We believe this statistic is because tweets with evidence were infrequent and that many examples contained implicit relations, opposed to containing a discourse marker. From Annotator A’s results, we found that only 9 examples contained an explicit discourse marker and 96 did not. Prasad et al. (2008) has already recognized that it is difficult to annotate relations when no discourse marker is present. We plan to automatically annotate evidence relations via machine learning and provide a probability that a pair is evidence to help manual annotation. Multiple Claims With Twitter’s character constraints, we expected to discover only one claim per reply with multiple support segments. However, we found that a few of our annotated segments contained multiple claim and multiple support segments. Range Annotation range was a problem we discovered after observing our annotated data. Although suc</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The penn discourse treebank 2.0. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeshi Sakaki</author>
<author>Fujio Toriumi</author>
<author>Yutaka Matsuo</author>
</authors>
<title>Tweet trend analysis in an emergency situation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Special Workshop on Internet and Disasters, SWID ’11,</booktitle>
<pages>3--1</pages>
<contexts>
<context position="2419" citStr="Sakaki et al., 2011" startWordPosition="346" endWordPosition="349">ts for determining the credibility of information on Twitter (Castillo et al., 2011); however, the focus of this work is to determine and annotate evidence relations on microblogs. Our primary motivation behind focusing on evidence relations includes the possibility of discovering support for a claim which can support the debunking of false information. During the March 2011 Great East Japan Earthquake and Tsunami disaster, victims turned to the Internet in order to obtain information on current conditions, such as family member whereabouts, refuge center information, and general information (Sakaki et al., 2011). However, false information, such as the popular Cosmo Oil explosion causing toxic rain, interfered with those looking to find correct information on the status of the disaster areas (Okazaki et al., 2013). This is a scenario in which identification of potentially false information is necessary in order to provide accurate information to victims and others relying on and trusting in the Internet. Therefore, as a start to find support for counterclaims for false information such as the Cosmo Oil explosion, we focus on dialogue between two individuals: a topic starter, or a post with no parent;</context>
</contexts>
<marker>Sakaki, Toriumi, Matsuo, 2011</marker>
<rawString>Takeshi Sakaki, Fujio Toriumi, and Yutaka Matsuo. 2011. Tweet trend analysis in an emergency situation. In Proceedings of the Special Workshop on Internet and Disasters, SWID ’11, pages 3:1–3:8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzan Verberne</author>
</authors>
<title>Developing an approach for why-question answering.</title>
<date>2006</date>
<booktitle>In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, EACL ’06,</booktitle>
<pages>39--46</pages>
<contexts>
<context position="3234" citStr="Verberne, 2006" startWordPosition="482" endWordPosition="483">2013). This is a scenario in which identification of potentially false information is necessary in order to provide accurate information to victims and others relying on and trusting in the Internet. Therefore, as a start to find support for counterclaims for false information such as the Cosmo Oil explosion, we focus on dialogue between two individuals: a topic starter, or a post with no parent; and a respondent who provides either an agreeing or disagreeing claim and support for their claim. An example is provided in Figure 1. We note that our task can appear similar to the field of Why-QA (Verberne, 2006; Oh et al., 2013; Mrozinski et al., 2008), which attempts to discover the answer for Why questions. Given our task of discovering agreeing or conflicting claims, and finding specific reasoning to support the claim, we end up with a Why question similar to Why is it true/not true that X, where X is the contents of the claim found in the parent post. However, we consider source mentions or hyperlinks, which can either stand alone or be contained in a statement, question, or request, as a way to answer the above question. To the best of our knowledge, no corpora for evidence relations on microbl</context>
</contexts>
<marker>Verberne, 2006</marker>
<rawString>Suzan Verberne. 2006. Developing an approach for why-question answering. In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, EACL ’06, pages 39–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas N Walton</author>
</authors>
<title>Argumentation Schemes for Presumptive Reasoning.</title>
<date>1996</date>
<publisher>Psychology Press.</publisher>
<contexts>
<context position="3993" citStr="Walton, 1996" startWordPosition="607" endWordPosition="608">licting claims, and finding specific reasoning to support the claim, we end up with a Why question similar to Why is it true/not true that X, where X is the contents of the claim found in the parent post. However, we consider source mentions or hyperlinks, which can either stand alone or be contained in a statement, question, or request, as a way to answer the above question. To the best of our knowledge, no corpora for evidence relations on microblogs currently exists. In terms of argumentation corpora, the Araucaria Argumentation Corpus 2 exists which utilizes various argumentation schemes (Walton, 1996; Katzav and Reed, 2004; Pollock, 1995). In this work, we 1https://twitter.com 2http://araucaria.computing.dundee.ac.uk/doku.php This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 70 LAW VIII - The 8th Linguistic Annotation Workshop, pages 70–74, Dublin, Ireland, August 23-24 2014. �$����&amp;quot;�+����!�/.�-#$3297�� :58; ����-*� A cat won’t catch a cold that way. A cat cold is caused by a virus preventable by vaccine. �#���� ���,��&apos;�� (�416;�</context>
</contexts>
<marker>Walton, 1996</marker>
<rawString>Douglas N. Walton. 1996. Argumentation Schemes for Presumptive Reasoning. Psychology Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
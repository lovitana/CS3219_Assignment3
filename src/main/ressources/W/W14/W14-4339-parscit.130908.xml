<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000282">
<title confidence="0.957745">
Web-style ranking and SLU combination for dialog state tracking
</title>
<author confidence="0.978841">
Jason D. Williams
</author>
<affiliation confidence="0.953651">
Microsoft Research, Redmond, WA, USA
</affiliation>
<email confidence="0.982449">
jason.williams@microsoft.com
</email>
<sectionHeader confidence="0.993584" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999823">
In spoken dialog systems, statistical state
tracking aims to improve robustness to
speech recognition errors by tracking a
posterior distribution over hidden dialog
states. This paper introduces two novel
methods for this task. First, we explain
how state tracking is structurally simi-
lar to web-style ranking, enabling ma-
ture, powerful ranking algorithms to be ap-
plied. Second, we show how to use mul-
tiple spoken language understanding en-
gines (SLUs) in state tracking — multiple
SLUs can expand the set of dialog states
being tracked, and give more information
about each, thereby increasing both recall
and precision of state tracking. We eval-
uate on the second Dialog State Tracking
Challenge; together these two techniques
yield highest accuracy in 2 of 3 tasks, in-
cluding the most difficult and general task.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999818965517242">
Spoken dialog systems interact with users via nat-
ural language to help them achieve a goal. As the
interaction progresses, the dialog manager main-
tains a representation of the state of the dialog in
a process called dialog state tracking (Williams
et al., 2013; Henderson et al., 2014). For exam-
ple, in a restaurant search application, the dialog
state might indicate that the user is looking for an
inexpensive restaurant in the center of town. Di-
alog state tracking is difficult because errors in
automatic speech recognition (ASR) and spoken
language understanding (SLU) are common, and
can cause the system to misunderstand the user’s
needs. At the same time, state tracking is crucial
because the system relies on the estimated dia-
log state to choose actions – for example, which
restaurants to present to the user.
Historically, commercial systems have used
hand-crafted rules for state tracking, selecting the
SLU result with the highest confidence score ob-
served so far, and discarding alternatives. In con-
trast, statistical approaches compute a posterior
distribution over many hypotheses for the dialog
state, and in general these have been shown to be
superior (Horvitz and Paek, 1999; Williams and
Young, 2007; Young et al., 2009; Thomson and
Young, 2010; Bohus and Rudnicky, 2006; Met-
allinou et al., 2013; Williams et al., 2013).
This paper makes two contributions to the task
of statistical dialog state tracking. First, we show
how to cast dialog state tracking as web-style rank-
ing. Each dialog state can be viewed as a doc-
ument, and each dialog turn can be viewed as a
search instance. The benefit of this construction is
that it enables a rich literature of powerful rank-
ing algorithms to be applied. For example, the
ranker we apply constructs a forest of decision
trees, which — unlike existing work — automat-
ically encodes conjunctions of low-level features.
Conjunctions are attractive in dialog state tracking
where relationships exist between low-level con-
cepts like grounding and confidence score.
The second contribution is to incorporate the
output of multiple spoken language understanding
engines (SLUs) into dialog state tracking. Using
more than one SLU can increase the number of di-
alog states being tracked, improving the chances
of discovering the correct one. Moreover, addi-
tional SLUs supply more features, such as seman-
tic confidence scores, improving accuracy.
This paper is organized as follows. First, sec-
tion 2 states the problem formally and covers re-
lated work. Section 3 then lays out the data, fea-
tures, and experimental design. Section 4 applies
web-style ranking, and section 5 covers the usage
of multiple SLUs. Section 6 extends the types of
tracking tasks, section 7 compares performance to
other entries in DSTC2, and section 8 briefly con-
</bodyText>
<page confidence="0.951369">
282
</page>
<note confidence="0.807011">
Proceedings of the SIGDIAL 2014 Conference, pages 282–291,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.440069">
cludes.
</bodyText>
<sectionHeader confidence="0.968424" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99273592">
Statistical dialog state tracking can be formalized
as follows. At each turn in the dialog, the state
tracker maintains a set X of dialog state hypothe-
ses X = {x1, x2, ... , xN}. Each state hypothesis
corresponds to a possible true state of the dialog.
The posterior of a state xi at a certain turn in the
dialog is denoted P(xi).
Based on this posterior, the system takes an ac-
tion a, the user provides an utterance in reply,
and an automatic speech recognizer (ASR) con-
verts the user’s utterance into words. Since speech
recognition is an error prone process, the speech
recognizer outputs weighted alternatives, for ex-
ample an N-best list or a word-confusion network.
A spoken language understanding engine (SLU)
then converts the ASR output into a meaning rep-
resentation U for the user’s utterance, where U
can contain alternatives for the user’s meaning,
U = {u1, ... , uL}.
The state tracker then updates its internal state.
This is done in three stages. First, a hand-written
function G ingests the system’s last action s, the
meaning representation U, and the current set of
states X, and yields a new set of possible states,
X0 = G(s, U, X), where we denote the members
of X0 as {x01, x02, ... , x0 NJ. The number of ele-
ments in X0 may be different than X, and typi-
cally the number of states increases as the dialog
progresses, i.e. N0 &gt; N. In this work, G simply
takes the Cartesian product of X and U. Second,
for each new state hypothesis x0i, a vector of J fea-
tures is extracted, φ(x0i) = [φ1(x0i), ... , φJ(x0 i)].
In the third stage, a scoring process takes all of the
features for all of the new dialog states and scores
them to produce the new distribution over dialog
states, P0(x0i). This new distribution is used to
choose another system action, and the whole pro-
cess repeats.
Most early work cast dialog state tracking as a
generative model in which hidden user goals gen-
erate observations in the form of SLU hypothe-
ses (Horvitz and Paek, 1999; Williams and Young,
2007; Young et al., 2009; Thomson and Young,
2010). More recently, discriminatively trained di-
rect models have been applied, and two studies
on dialog data from two publicly deployed dialog
systems suggest direct models yield better perfor-
mance (Williams, 2012; Zilka et al., 2013). The
methods introduced in this paper also use discrim-
inative techniques.
One of the first approaches to direct models for
dialog state tracking was to consider a small, fixed
number of states and then apply a multinomial
classifier (Bohus and Rudnicky, 2006). Since a
multinomial classifier can make effective use of
more features than a generative model, this ap-
proach improves precision, but can decrease recall
by only considering a small number of states (e.g.
5 states). Another discriminative approach is to
score each state using a binary model, then some-
how combine the binary scores to form a distribu-
tion – see, for example (Henderson et al., 2013b)
which used a binary neural network. This ap-
proach scales to many states, but unlike a multi-
nomial classifier, each binary classifier isn’t aware
of its competitors, reducing accuracy. Also, when
training a binary model in the conventional way,
the training criteria is mis-matched, since the clas-
sifier is trained per hypothesis per timestep, but is
evaluated only once per timestep.
Maximum entropy (maxent) models have been
proposed which provide the strengths of both of
these approaches (Metallinou et al., 2013). The
probability of a dialog hypothesis xi being correct
(y = i) is computed as:
</bodyText>
<equation confidence="0.987937666666667">
P(y = i|X, λ) = Ex∈X exp(Ej∈J λjφj(x)) .
exp(Ej∈J λjφj(xi))
(1)
</equation>
<bodyText confidence="0.998873666666667">
Maximum entropy models yielded top perfor-
mance in the first dialog state tracking challenge
(Lee and Eskenazi, 2013). In this paper, we use
maxent models as a baseline.
A key limitation with linear (and log-linear)
models such as maximum entropy models is that
they do not automatically build conjunctions of
features. Conjunctions express conditional combi-
nations of features such as whether the system at-
tempted to confirm x and if “yes” was recognized
and if the confidence score of “yes” is high. Con-
junctions are important in dialog state tracking be-
cause they are often more discriminative than indi-
vidual features. Moreover, in linear models for di-
alog state tracking, one weight is learned per fea-
ture (equation 1) (Metallinou et al., 2013). As a re-
sult, if a feature takes the same value for every dia-
log hypothesis at a given timestep, its contribution
to every hypothesis will be the same, and it will
therefore have no effect on the ranking. For exam-
ple, features describing the current system action
</bodyText>
<page confidence="0.997071">
283
</page>
<bodyText confidence="0.999821923076923">
are identical for all state hypotheses. Concretely,
if φj(xi) = c for all i, then changing c causes no
change in P(y = i|X, A) for all i.
Past work has shown that conjunctions improve
dialog state tracking (Metallinou et al., 2013; Lee,
2013). However, past work has added conjunction
by hand, and this doesn’t scale: the number of pos-
sible conjunctions increases exponentially in the
number of terms in the conjunction, and it’s diffi-
cult to predict in advance which conjunctions will
be useful. This paper introduces algorithms from
web-style ranking as a mechanism for automati-
cally building feature conjunctions.
In this paper we also use score averaging, a
well-known machine learning technique for com-
bining the output of several models, where each
output class takes the average score assigned by
all the models. Under certain assumptions — most
importantly that errors are made independently —
score averaging is guaranteed to exceed the perfor-
mance of the best single model. Score averaging
has been applied to dialog state tracking in previ-
ous work (Lee and Eskenazi, 2013). Here we use
score averaging to maximize data use in cascaded
models, and as a hedge against unlucky parameter
settings.
</bodyText>
<sectionHeader confidence="0.997023" genericHeader="method">
3 Preliminaries
</sectionHeader>
<bodyText confidence="0.999052727272727">
In this paper we use data and evaluation metrics
from the second dialog state tracking challenge
(DSTC2) (Henderson et al., 2014; Henderson et
al., 2013a). Dialogs in DSTC2 are in the restau-
rant search domain. Users can search for restau-
rants in multiple ways, including via constraints,
or by name. The system can offer restaurants that
match, confirm user input, ask for additional con-
straints, etc.
There are three components to the hidden di-
alog state: user’s goal, search method, and re-
quested slots. The user’s goal specifies the
user’s search constraints, and consists of 4 slots:
area, pricerange, foodtype, and name. The num-
ber of values for the slots ranges from 4 to
113.1 In DSTC2, trackers output scored lists
for each slot, and also a scored list of joint hy-
potheses. For example, at a given timestep in
a given dialog, three joint goal hypothesis might
be (area=west,food=italian), (area=west), and (),
where () means the user hasn’t specified any con-
straints yet. Since tracking the joint user goal is
</bodyText>
<subsectionHeader confidence="0.8972">
1Including a special “don’t care” value.
</subsectionHeader>
<bodyText confidence="0.999882666666667">
the most general and most difficult task, we’ll fo-
cus on this first, and return to the other tasks in
section 6.
</bodyText>
<subsectionHeader confidence="0.999547">
3.1 User goal features
</subsectionHeader>
<bodyText confidence="0.999970038461538">
For features, we broadly follow past work (Lee
and Eskenazi, 2013; Lee, 2013; Metallinou et al.,
2013). For a hypothesis xi, for each slot the fea-
tures encode 253 low-level quantities, such as:
whether the slot value appears in this hypothesis;
how many times the slot value has been observed;
whether the slot value has been observed in this
turn; functions of recognition metrics such as con-
fidence score and position on N-best list; goal pri-
ors and confusion probabilities estimated on train-
ing data (Williams, 2012; Metallinou et al., 2013);
results of confirmation attempts (“Italian food, is
that right?”); output of the four rule-based base-
line trackers; and the system act and its relation to
the goal’s slot value (e.g., whether the system act
mentions this slot value).
Of these 253 features for each slot, 119 are the
same for all values of that slot in a given turn,
such as which system acts were observed in this
turn. For these, we add 238 conjunctions with
slot-specific features like confidence score, which
makes these features useful to our maxent base-
line. This results in a total of 253+238 = 491 fea-
tures per slot. The features for each of the 4 slots
are concatenated together to yield 491 ∗ 4 = 1964
features per joint hypothesis.
</bodyText>
<subsectionHeader confidence="0.998682">
3.2 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.999995058823529">
In DSTC2, there are 3 primary metrics for eval-
uation — accuracy of the top-scored hypothesis,
the L2 probability quality, and an ROC measure-
ment. The ROC measurement is only meaningful
when compared across systems with similar accu-
racy; since our variants differ in accuracy, we omit
ROC. However, note that all of the metrics, includ-
ing ROC, for our final entries on the development
set and test set are available for public download
from the DSTC2 website.2
The DSTC2 corpus consists of three partitions:
train, development, and test. Throughout sections
4-6, we report accuracy by training on the train-
ing set, and report accuracy on the development
set and test set. The development set was avail-
able during development of the models, whereas
the test set was not.
</bodyText>
<footnote confidence="0.353528">
2camdial.org/˜mh521/dstc/
</footnote>
<page confidence="0.992265">
284
</page>
<subsectionHeader confidence="0.772561">
3.3 Baselines forest of M trees, the score of a dialog state x is
</subsectionHeader>
<bodyText confidence="0.9997802">
We first compare to the four rule-based trackers
provided by DSTC2. These were carefully de-
signed by other research groups, and earlier ver-
sions of them scored very well in the first DSTC
(Wang and Lemon, 2013). In each column in Ta-
bles 2 and 3, we report the best result from any
rule-based tracker. We also compare to a maxent
model as in Eq 1. Our implementation includes
L1 and L2 regularization which was automatically
tuned via cross-validation.
</bodyText>
<sectionHeader confidence="0.990903" genericHeader="method">
4 Web-style ranking
</sectionHeader>
<bodyText confidence="0.993997833333333">
The ranking task is to order a set of N docu-
ments by relevance given a query. The input
to a ranker is a query Q and set of documents
X = {D1, ... , DN}, where each document is de-
scribed in terms of features of that document and
the query φ(DZ, Q). The output is a score for
each document, where the highest score indicates
the most relevant document. The overall objective
is to order the documents by relevance, given the
query. Training data indicates the relevance of ex-
ample query/document pairs. Training labels are
provided by judges, and relevance is typically de-
scribed in terms of several levels, such as “excel-
lent”, “good”, “fair”, and “not relevant”.
The application of ranking to dialog state track-
ing is straightforward: instead of ranking features
of documents and queries φ(DZ, Q), we rank fea-
tures of dialog states φ(XZ). For labeling, the cor-
rect dialog state is “relevant” and all other states
are “not relevant”.
Like dialog state tracking, ranking tasks often
have features which are constant over all docu-
ments – particularly features of the query. This is
one reason why ranking algorithms have incorpo-
rated methods for automatically building conjunc-
tions. The specific algorithm we use here is lamb-
daMART (Wu et al., 2010; Burges, 2010). Lamb-
daMART is a mature, scalable ranking algorithm:
it has underpinned the winning entry in a commu-
nity ranking challenge task (Chapelle and Chang,
2011), and is the foundation of the ranker in the
Bing search engine. LambdaMART constructs a
forest of M decision trees, where each tree con-
sists of binary branches on features, and the leaf
nodes are real values. Each binary branch speci-
fies a threshold to apply to a single feature. For a
</bodyText>
<equation confidence="0.997624">
M
F(x) = E αmfm(x) (2)
m=1
</equation>
<bodyText confidence="0.99994224">
where αm is the weight of tree m and fm(x) is the
value of the leaf node obtained by evaluating de-
cision tree m by features [O1(x), ... , OJ(x)]. The
training objective is to maximize ranking quality,
which here means one-best accuracy. The deci-
sion trees are learned by regularized gradient de-
scent, where trees are added successively to im-
prove ranking quality – in our case, to maximize
how often the correct dialog state is ranked first.
The number of trees to create and the number of
leaves per tree are tuning parameters. Through
cross-validation, we found that 500 decision trees
each with 32 leaves were the best settings. We use
the same set of 1964 features for lambdaMART as
was used for the maxent baseline.
Results are shown in row 3 of table 2 under
“Joint goal”. Ranking outperforms both baselines
on both the development and training set. This re-
sult illustrates that automatically-constructed con-
junctions do indeed improve accuracy in dialog
state tracking. An example of a single tree com-
puted by lambdaMART is shown in Appendix
A. The complexity of this tree suggests that hu-
man designers would find it difficult to specify a
tractable set of good conjunction features.
</bodyText>
<sectionHeader confidence="0.9917" genericHeader="method">
5 Multiple SLU engines
</sectionHeader>
<bodyText confidence="0.9999227">
As described in the introduction, dialog state
tracking typically proceeds in three stages: enu-
meration of the set of dialog states to score, fea-
ture extraction, and scoring. Incorporating the out-
put of multiple SLUs requires changing the first
two steps. Continuing with notation from sec-
tion 2, with a single SLU output U, the enumer-
ation step is X&apos; = G(s, U, X) — recall that U
is a set of SLU hypotheses from an SLU engine.
With multiple SLU engines we have K SLU out-
puts U1, ... , UK, and the enumeration step is thus
X&apos; = G(s, U1, ... , UK, X). In our implementa-
tion, we simply take the union of all concepts on
all SLU N-best lists and enumerate states as in the
single SLU case – i.e., the Cartesian product of di-
alog states X with concepts on the SLU output.
The feature extraction step is modified to out-
put features derived from all of the SLU engines.
Concretely, if a feature Oj(x) includes informa-
tion from an SLU engine (such as confidence score
</bodyText>
<page confidence="0.990431">
285
</page>
<bodyText confidence="0.9999171">
or position on the N-best list), it is duplicated K
times – i.e., once for each SLU engine. Additional
binary features are added to encode whether each
SLU engine has output the slot value of this dialog
state. This allows for the situation that a slot value
is not output by all SLU engines, in which case
its confidence score, N-best list position, etc. will
not be present from some SLU engines. Using two
SLU engines on our data increases the number of
features per joint goal from 1964 to 3140.
</bodyText>
<subsectionHeader confidence="0.97251">
5.1 SLU Engines
</subsectionHeader>
<bodyText confidence="0.99995979245283">
We built two new SLU engines, broadly following
(Henderson et al., 2012). Both consist of many
binary classifiers. In the first engine SLU1, a
binary classifier is estimated for each slot/value
pair, and predicts the presence of that slot/value
pair in the utterance. Similarly, a binary classi-
fier is estimated for each user dialog act. Input
features are word n-grams from the ASR N-best
list. We only considered n-grams which were ob-
served at least c times in the training data; infre-
quent n-grams were mapped to a special UNK
feature. For binary classification we used deci-
sion trees, which marginally outperformed logis-
tic regression, SVMs, and deep neural networks.
Through cross-validation we set n = 2 and c = 2
– i.e., uni-grams and bi-grams which appear at
least twice in the training data.
At runtime, the top SLU output on the N-best
list is formed by taking the most likely combina-
tion of all the binary classifiers; the second SLU
output is formed by taking the second most likely
combination of all the binary classifiers; and so on,
where only valid SLU combinations are consid-
ered. For example, the “bye” dialog act takes no
arguments, so if “bye” and “food=italian” were the
most likely combination, this combination would
be skipped. Scores are formed by taking the prod-
uct of all the binary classifiers, with some smooth-
ing.
The second SLU engine SLU2 is identical ex-
cept that it also includes features from the word
confusion network. Specifically, each word (uni-
gram) appearing in the word confusion network is
a feature. Bi-gram confusion network features did
not improve performance.
If we train a new SLU engine and a ranker on
the same data, this will introduce unwanted bias.
Therefore, we divided the training data in half, and
use the first half for training the SLU, and the sec-
ond for training the ranker. Table 1 shows several
evaluation metrics for each SLU engine, includ-
ing the SLU included in the corpus, which we de-
note SLU0. SLU precision, recall, and F-measure
are computed on the top hypotheses. Item cross-
entropy (ICE) (Thomson et al., 2008) measures the
quality of the scores for all the items on the SLU
N-best list. Table 1 also shows joint goal accu-
racy by using SLU0, SLU1, or SLU2, for either a
rule-based baseline or the ranking model. Over-
all, our SLU engines performed better on isolated
SLU metrics, but did not yield better state tracking
performance when used instead of the SLU results
in the corpus.
</bodyText>
<subsectionHeader confidence="0.964667">
5.2 Results with multiple SLU engines
</subsectionHeader>
<bodyText confidence="0.996286833333333">
Table 2, rows 4 and 7 show that an improvement
in performance does results from using 2 SLU en-
gines. In rows 4 and 7, the additional SLU en-
gine is trained on the first half of the data, and the
ranker is trained on the second half – we call this
arrangement Fold A. To maximize use of the data,
it’s possible to train a second SLU/ranker pair by
inverting the training data – i.e., train a second
SLU on the second half, and a second ranker (us-
ing the second SLU) on the first half. We call this
arrangement Fold B. These two configurations can
be combined by running both trackers on test data,
then averaging their scores. We call this arrange-
ment Fold AB. If a hypothesis is output by only
one configuration, it is assumed the other configu-
ration output a zero score.
Table 2, rows 5 and 8 show that the fold AB con-
figuration yields an additional performance gain.
</bodyText>
<subsectionHeader confidence="0.99949">
5.3 Model averaging
</subsectionHeader>
<bodyText confidence="0.926158375">
A small further improvement is possible by aver-
aging across multiple models (rankers) with dif-
ferent parameter settings. Since all of the mod-
els will be estimated on the same data, this is un-
likely to make a large improvement, but it can
hedge against an unlucky parameter setting, since
the performance after averaging is usually close to
the maximum.
To test this, we trained a second pair of rank-
ing models, with a different number of leaves per
tree (8 instead of 32). We then applied this sec-
ond model, and averaged the scores between the
two variants. Results are in Table 2, rows 6 and
9. Averaging scores across two parameter settings
generally results in performance equal to or better
than the maximum of the two models.
</bodyText>
<page confidence="0.995269">
286
</page>
<table confidence="0.997939">
SLU Dev set Goal track. acc. Test set Goal track. acc.
SLU Metrics SLU Metrics
source Prec. Recall F-meas. ICE Rules Ranking Prec. Recall F-meas. ICE Rules Ranking
SLU0 0.883 0.666 0.759 2.185 0.623 0.666 0.900 0.691 0.782 1.955 0.719 0.739
SLU1 0.818 0.729 0.771 2.189 0.598 0.637 0.846 0.762 0.802 1.943 0.667 0.709
SLU2 0.844 0.742 0.789 2.098 0.605 0.658 0.870 0.777 0.821 1.845 0.685 0.734
</table>
<tableCaption confidence="0.998616">
Table 1: Performance of three SLU engines. SLU0 is the DSTC2 corpus; SLU1 is our engine with
</tableCaption>
<bodyText confidence="0.941040714285714">
uni-grams and bi-grams of ASR results in the corpus; and SLU2 is SLU1 with the addition of unigram
features from the word confusion network. Precision, Recall, F-measure, and ICE evaluate the quality
of the SLU output, not state tracking. “ICE” is item-wise cross entropy — smaller numbers are better
(Thomson et al., 2008). “Rules” indicates dialog state tracking accuracy for user joint goals by running
the rule-based baseline tracker on the indicated SLU (alone); “Ranking” indicates joint goal accuracy of
running a ranker trained on the indicated SLU (alone). For training, goal tracking results use the “Fold
A” configuration (c.f. Section 5.2).
</bodyText>
<subsectionHeader confidence="0.97753">
5.4 Joint goal tracking summary
</subsectionHeader>
<bodyText confidence="0.999972470588235">
The overall process used to train the joint goal
tracker is summarized in Appendix B. For joint
goal tracking, web-style ranking and multiple
SLUs both yield improvements in accuracy on the
development and test sets, with the improvement
associated with multiple SLUs being larger. We
also observe that ranking produces relatively poor
L2 results. This can be attributed to its training
objective, which explicitly maximizes 1-best ac-
curacy without regard to the distribution of the
scores. This is in contrast to maxent models which
explicitly minimize the L2 loss. We examined the
distribution of scores, and qualitatively the ranker
is usually placing less mass on its top guess than
maxent, and spreading more mass out among other
(usually wrong) entries. We return to this in the
future work section.
</bodyText>
<sectionHeader confidence="0.967365" genericHeader="method">
6 Fixed-size state components
</sectionHeader>
<bodyText confidence="0.999892865384616">
DSTC2 consists of three tracking tasks: in addi-
tion to the user’s goal, the user’s search method
and which slots they requested to hear were also
tracked. These other two tasks were comparatively
simpler because their domains are of a small, fixed
size. Thus classical machine learning methods can
be applied – i.e., ranking is not directly applicable
to tracking the method and required slots. How-
ever, applying multiple SLU engines is still appli-
cable.
The search method specifies how the user
wants to search. There are 5 values: by-constraints
such as area=west,food=italian, by-name such as
“royal spice”, by-alternatives as in “do you have
any others like that?”, finished when the user is
done as in “thanks goodbye”, and none when the
method can’t be determined. At each turn, ex-
actly one of the 5 methods is active, so we view
the method component as a standard multinomial
classification task. For features, we use the score
for each method output by each of the 4 rule-
based baselines, and whether each of the methods
is available according to the SLU results observed
so far. We also take conjunctions for each method
with: whether each system dialog act is present in
the current turn, or has ever been used, and what
slots they mentioned; and whether each slot has
appeared in the SLU results from this turn, or any
turn. In total there are 640 features for the method
classifier (when using one SLU engine).
The requested slots are the pieces of informa-
tion the user wants to hear in that turn. The user
can request to hear a restaurant’s area, food-type,
name, price-range, address, phone number, post-
code, and/or signature dish. The user can ask to
hear any combination of slots in a turn – e.g., “tell
me their address and phone number”. Therefore
we view each requested slot as a binary classifica-
tion task, and estimate 8 binary classifiers, one for
each requestable slot. Each requested slot takes
as features: whether the slot could logically be re-
quested at this turn in the dialog; whether the SLU
output contained a “request” act and which slot
was requested; the score output by each of the 4
rule-based baselines; whether each system dialog
act is present in the current turn, or has ever been
used, and what slots they mentioned; and whether
each slot has appeared in the SLU results from this
turn, or any turn. For each requestable slot’s bi-
nary classifier, this results in 187 features (with
one SLU engine).
For each of these tasks, we applied a maxent
</bodyText>
<page confidence="0.993647">
287
</page>
<table confidence="0.828196916666667">
Row SLU Fold Model Model Joint goal Search method Requested slot
comb. Dev. set Test set Dev. set Test set Dev. set Test set
Acc. L2 Acc. L2 Acc. L2 Acc. L2 Acc. L2 Acc. L2
1 0 — rules N 0.623 0.601 0.719 0.464 0.860 0.217 0.897 0.158 0.903 0.155 0.884 0.196
2 0 all maxent N 0.649 0.532 0.692 0.480 0.890 0.177 0.909 0.143 0.952 0.078 0.967 0.054
3 0 all * N 0.666 0.739 0.739 0.721 — — — — — — — —
4 0+1 A * N 0.686 0.770 0.757 0.766 0.912 0.144 0.936 0.104 0.960 0.062 0.976 0.039
5 0+1 AB * N 0.697 0.749 0.769 0.748 0.913 0.135 0.938 0.097 0.962 0.060 0.978 0.037
6 0+1 AB ** Y 0.699 0.766 0.770 0.766 0.916 0.135 0.943 0.091 0.964 0.059 0.978 0.036
7 0+2 A * N 0.697 0.731 0.765 0.727 0.910 0.146 0.939 0.099 0.966 0.058 0.979 0.037
8 0+2 AB * N 0.711 0.725 0.778 0.721 0.913 0.133 0.943 0.092 0.967 0.058 0.980 0.033
9 0+2 AB ** Y 0.710 0.742 0.781 0.739 0.915 0.132 0.948 0.085 0.967 0.057 0.980 0.033
</table>
<tableCaption confidence="0.751560333333333">
Table 2: Summary of accuracy and L2 for the three tracking tasks, trained on the “train” set. In rows
marked (*), joint goal accuracy used ranking, and the other two tasks used maxent. In rows marked (**),
several model classes/parameter settings were used and combined with score averaging.
</tableCaption>
<bodyText confidence="0.9996565">
model; results for this and the best rule-based
baseline are in the rows 1 and 2 of Table 2. We
tried applying decision trees, but this did not im-
prove performance (not shown) as it did for goal
tracking. Note that in the goal tracking task,
one weight is learned for each feature for any
class (goal), whereas in standard multiclass and bi-
nary classification, one weight is learned for each
feature,class pair.3 Perhaps decision trees were
not effective in increasing accuracy for method
and requested slots because, compared to joint
goal tracking, some conjunctions are implicitly in-
cluded in linear models.
We then added a second SLU engine in the same
manner as for goal tracking. This increased the
number of features for the method task from 640
to 840, and from 187 to 217 for each binary re-
quested slot classifier. Results are shown in Ta-
ble 2; rows 4 and 7 show results with one fold,
and rows 5 and 8 show results with both folds.
Finally, we considered alternate model forms for
each classifier, and then combined them with score
averaging. For the method task, we used a sec-
ond maximum entropy model with different regu-
larization weights, and a multi-class decision tree.
For the requested slot binary classifiers, we added
a neural network classifier. As above, score av-
eraging across different model classes can yield
small gains (rows 6 and 9).
Overall, as with goal tracking, adding a sec-
ond SLU engine resulted in a substantial increase
in accuracy. Unlike goal tracking which used
a ranker, the standard classification models used
here are explicitly optimized for L2 performance
and as a result achieved very good L2 perfor-
mance.
</bodyText>
<footnote confidence="0.635598">
3Plus a constant term per class.
</footnote>
<sectionHeader confidence="0.640562" genericHeader="method">
7 Blind evaluation results
</sectionHeader>
<bodyText confidence="0.99996921875">
When preparing final entries for the DSTC2 blind
evaluation, we not longer needed a separate devel-
opment set, so our final models are trained on the
combined training and development sets. In the
DSTC2 results, we are team2. Our entry 0 and 1
use the process described above, including score
averaging across multiple models. Entry0 used
SLU0+1, and entry1 used SLU0+2. Entry3 used
a maxent model on SLU0+2, but without model
averaging since its parameters are set with cross-
validation.4
Results are summarized in Table 3. For accu-
racy for the joint goal and method tasks, our en-
tries had highest accuracy. After the evaluation,
we learned that we were the only team to use fea-
tures from the word confusion network (WCN).
Comparing our entry0, which does not use WCN
features, to the other teams shows that, given the
same input data, our entries were still best for the
joint goal and method tasks.
The blind evaluation results give a final oppor-
tunity to compare the maxent model with the rank-
ing model: entry1 and entry3 both use SLU0+2,
and score an identical set of dialog states using
identical features. Joint goal accuracy is better for
the ranking model. However, as noted above, L2
performance for the ranking model was substan-
tially worse than for the maxent model.
After the blind evaluation, we realized that we
had inadvertently omitted a key feature from the
“requested” binary classifiers — whether the “re-
quest” dialog act appeared in the SLU results.
</bodyText>
<footnote confidence="0.9996095">
4The other entries team2.entry2 and team2.entry4 are not
described in this paper. In brief, entry2 was based on a recur-
rent neural network, and entry4 was a combination of entries
1, 2, and 3.
</footnote>
<page confidence="0.983271">
288
</page>
<table confidence="0.997655142857143">
model Goal Method Requested Requested*
Acc. L2 Acc. L2 Acc. L2 Acc. L2
Best baseline 0.719 0.464 0.897 0.158 0.884 0.196 0.884 0.196
Best DSTC2 result from another team 0.768 0.346 0.940 0.095 0.978 0.035 0.978 0.035
SLU0+1, AB, model comb. (entry0) 0.775 0.758 0.944 0.092 0.954 0.073 0.977 0.037
SLU0+2, AB, model comb. (entry1) 0.784 0.735 0.947 0.087 0.957 0.068 0.980 0.034
SLU0+2, AB, maxent (entry3) 0.771 0.354 0.947 0.093 0.941 0.090 0.979 0.040
</table>
<tableCaption confidence="0.98665">
Table 3: Final DSTC2 evaluation results, training on the combined “train” and “development” sets. In
</tableCaption>
<bodyText confidence="0.991546">
the results, we are team2. “Model comb.” indicates score averaging over several model instances. For the
“requested” task, our entry in DSTC2 inadvertently omitted a key feature, which decreased performance
significantly. “Requested*” columns indicate results with this feature included. They were computed
after the blind evaluation and are not part of the official DSTC2 results.
Therefore table 3 shows results with and without
this feature. With the inclusion of this feature, the
requested classifiers also achieved best accuracy
and L2 scores, although we note that this is not
part of the official DSTC2 results. (The results in
the preceding sections of this paper included this
feature.)
</bodyText>
<sectionHeader confidence="0.98666" genericHeader="method">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999984685185185">
This paper has introduced two new methods for
dialog state tracking. First, we have shown how
to apply web-style ranking for scoring dialog state
hypotheses. Ranking is attractive because it can
construct a forest of decision trees which compute
feature conjunctions, and because it optimizes di-
rectly for 1-best accuracy. Second, we have in-
troduced the usage of multiple SLU engines. Us-
ing additional SLU engines is attractive because it
both adds more possible dialog states to score (in-
creasing recall), and adds features which help to
discriminate the best states (increasing precision).
In experiments, using multiple SLU engines im-
proved performance on all three of the tasks in the
second dialog state tracking challenge. Maximum
entropy models scored best in the previous dia-
log state tracking challenge; here we showed that
web-style ranking improved accuracy over max-
ent when using either a single or multiple SLU
engines. Thus, the two methods introduced here
are additive: they each yield gains separately, and
further gains in combination.
Comparing to other systems in the DSTC2 eval-
uation, these two techniques yielded highest accu-
racy in DSTC2 for 2 of 3 tasks. If we include a
feature accidentally omitted from the third task,
our methods yield highest accuracy for all three
tasks. This experience highlights the importance
of the manual task of extracting a set of informa-
tive features. Also, ranking improved accuracy,
but yielded poor probability quality. For rank-
ing, the L2 performance of ranking was among
the worst in DSTC2. By contrast, for the method
task, where standard classification could be ap-
plied, our entry yielded best L2 performance. The
relative importance of L2 vs. accuracy in dialog
state tracking is an open question.
In future work, we plan to investigate how to
improve the L2 performance of ranking. One ap-
proach is to train a maxent model on the output of
the ranker. On the test set, this yields an improve-
ment in L2 score from 0.735 to 0.587, and simply
clamping ranker’s best guess to 1.0 and all others
to 0.0 improves L2 to 0.431. This is a start, but
not competitive with the best result in DSTC2 of
0.346. Also, techniques which avoid the extrac-
tion of manual features altogether would be ideal,
particularly in light of experiences here.
Even so, for the difficult and general task of
user goal tracking, the techniques here yielded a
relative error rate reduction of 23% over the best
baseline, and exceeded the accuracy of any other
tracker in the second dialog state tracking chal-
lenge.
</bodyText>
<sectionHeader confidence="0.994956" genericHeader="method">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99869225">
Thanks to Dan Bohus for making his maxent soft-
ware available, to Andrzej Pastusiak for helpful
guidance with lambdaMART, and to Geoff Zweig
for several helpful conversations.
</bodyText>
<sectionHeader confidence="0.995115" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.7052162">
Dan Bohus and Alex Rudnicky. 2006. A ‘K hypothe-
ses + other’ belief updating model. In Proc Amer-
ican Association for Artificial Intelligence (AAAI)
Workshop on Statistical and Empirical Approaches
for Spoken Dialogue Systems, Boston.
</reference>
<page confidence="0.992769">
289
</page>
<reference confidence="0.998692087912088">
Christopher J.C. Burges. 2010. From ranknet to lamb-
darank to lambdamart: An overview. Technical Re-
port MSR-TR-2010-82, Microsoft Research.
Olivier Chapelle and Yi Chang. 2011. Yahoo! learning
to rank challenge. JMLR Workshop and Conference
Proceedings, 14:1–24.
Matthew Henderson, Milica Gasic, Blaise Thomson,
Pirros Tsiakoulis, Kai Yu, and Steve Young. 2012.
Discriminative spoken language understanding us-
ing word confusion networks. In Proc IEEE Work-
shop on Spoken Language Technologies (SLT), Mi-
ami, Florida, USA.
Matthew Henderson, Blaise Thomson, and Jason D.
Williams. 2013a. Handbook for the dialog state
tracking challenge 2 &amp; 3. Technical report, Cam-
bridge University.
Matthew Henderson, Blaise Thomson, and Steve
Young. 2013b. Deep neural network approach for
the dialog state tracking challenge. In Proceedings
of the SIGDIAL 2013 Conference, pages 467–471,
Metz, France, August. Association for Computa-
tional Linguistics.
Matthew Henderson, Blaise Thomson, and Jason
Williams. 2014. The second dialog state tracking
challenge. In Proceedings of the SIGdial 2014 Con-
ference, Baltimore, U.S.A., June.
Eric Horvitz and Tim Paek. 1999. A computational
architecture for conversation. In Proc 7th Interna-
tional Conference on User Modeling (UM), Banff,
Canada, pages 201–210.
Sungjin Lee and Maxine Eskenazi. 2013. Recipe for
building robust spoken dialog state trackers: Dialog
state tracking challenge system description. In Pro-
ceedings of the SIGDIAL 2013 Conference, pages
414–422, Metz, France, August. Association for
Computational Linguistics.
Sungjin Lee. 2013. Structured discriminative model
for dialog state tracking. In Proceedings of the
SIGDIAL 2013 Conference, pages 442–451, Metz,
France, August. Association for Computational Lin-
guistics.
Angeliki Metallinou, Dan Bohus, and Jason D.
Williams. 2013. Discriminative state tracking for
spoken dialog systems. In Proc Association for
Computational Linguistics, Sofia.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562–588.
B Thomson, K Yu, M Gasic, S Keizer, F Mairesse,
J Schatzmann, and S Young. 2008. Evaluating
semantic-level confidence scores with multiple hy-
potheses. In Proc Intl Conf on Spoken Language
Processing (ICSLP), Brisbane, Australia.
Zhuoran Wang and Oliver Lemon. 2013. A sim-
ple and generic belief tracking mechanism for the
dialog state tracking challenge: On the believabil-
ity of observed information. In Proceedings of the
SIGDIAL 2013 Conference, pages 423–432, Metz,
France, August. Association for Computational Lin-
guistics.
Jason D Williams and Steve Young. 2007. Partially
observable Markov decision processes for spoken
dialog systems. Computer Speech and Language,
21(2):393–422.
Jason D. Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013. The dialog state track-
ing challenge. In Proc SIGdial Workshop on Dis-
course and Dialogue, Metz, France.
Jason D. Williams. 2012. Challenges and oppor-
tunities for state tracking in statistical spoken dia-
log systems: Results from two public deployments.
IEEE Journal of Selected Topics in Signal Process-
ing, Special Issue on Advances in Spoken Dialogue
Systems and Mobile Interface, 6(8):959–970.
Qiang Wu, Christopher J. C. Burges, Krysta M. Svore,
and Jianfeng Gao. 2010. Adapting boosting for in-
formation retrieval measures. Journal of Informa-
tion Retrieval, 13(3):254–270.
Steve Young, Milica Gaˇsi´c, Simon Keizer, Franc¸ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2009. The hidden information state model:
a practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150–174.
Lukas Zilka, David Marek, Matej Korvas, and Filip Ju-
rcicek. 2013. Comparison of bayesian discrimina-
tive and generative models for dialogue state track-
ing. In Proceedings of the SIGDIAL 2013 Confer-
ence, pages 452–456, Metz, France, August. Asso-
ciation for Computational Linguistics.
</reference>
<sectionHeader confidence="0.939756" genericHeader="method">
Appendix A: Example decision tree
</sectionHeader>
<bodyText confidence="0.998682833333333">
Figure 1 shows an example decision tree gener-
ated by lambdaMART. Note how the tree is able to
combine features across different slots – for exam-
ple, following the right-most path tests the scores
of 3 different slots. Also, note how generally more
positive evidence leads to higher scores.
</bodyText>
<sectionHeader confidence="0.96652" genericHeader="method">
Appendix B: Schematic of approach
</sectionHeader>
<bodyText confidence="0.999289333333333">
Figure 2 shows a schematic diagram of our
overall approach for training the state tracker
(team2.entry0).
</bodyText>
<page confidence="0.979539">
290
</page>
<figure confidence="0.998950484848485">
area.baseline1.score
&gt; 0.45 ?
-0.32
area.best_conf_score &gt;
0.01
N Y
Y
-0.36 -0.47
# values recognized for
area &gt; 1 and food value
empty?
N Y
Is price value empty?
N
-0.08
N
+0.31
+0.33
Has system grounded
price?
food.baseline1.score
&gt; 0.52 ?
N
N
Y
N
price.baseline1.score
&gt; 0.61 ?
+0.48
Y
Y
+0.50
Y
</figure>
<figureCaption confidence="0.9991146">
Figure 1: Appendix A: Example decision tree with 8 leaves generated by lambdaMART. Each non-
terminal node contains a binary test; each terminal node contains a real value that linearly contributes to
the score of the dialog state being evaluated. “baseline1” refers to the output of one of the rule-based
baseline trackers, used in this classifier as an input feature.
Figure 2: Appendix B: Schematic diagram of our overall approach for training the state tracker, using
</figureCaption>
<bodyText confidence="0.955745">
SLU1 (team2.entry0). Cylinders represent data, rectangles are models, and scripts are tracker output.
Solid arrows are steps done at training time, and dotted arrows are steps done at test time. Approach for
</bodyText>
<equation confidence="0.645721">
SLU2 (team2.entry1) is identical except that additional features are used in training the SLU models.
</equation>
<bodyText confidence="0.891165">
All test data,
SLU 0
</bodyText>
<figure confidence="0.989333131578947">
All test data,
SLU0+1 / A
SLU 1
Fold A
2nd half train,
SLU0+1 / A
Ranker,
params I
Tracker ouput
SLU0+1 / A / I
Train, Fold A,
SLU0
Ranker,
params II
Tracker ouput
SLU0+1 / A / II
All train
Data, SLU0
Train, Fold B,
SLU0
Ranker,
params I
Tracker ouput
SLU0+1 / B / I
SLU 1
Fold B
1st half train,
SLU0+1 / B
Ranker,
params II
Tracker ouput
SLU0+1 / B / II
All test data,
SLU0+1 / B
Score
averaging
Final tracker
output
</figure>
<page confidence="0.963295">
291
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.775129">
<title confidence="0.99977">Web-style ranking and SLU combination for dialog state tracking</title>
<author confidence="0.999906">Jason D Williams</author>
<affiliation confidence="0.802499">Microsoft Research, Redmond, WA, USA</affiliation>
<email confidence="0.999786">jason.williams@microsoft.com</email>
<abstract confidence="0.998318476190476">spoken dialog systems, statistical to improve robustness to speech recognition errors by tracking a posterior distribution over hidden dialog states. This paper introduces two novel methods for this task. First, we explain how state tracking is structurally simito enabling mature, powerful ranking algorithms to be applied. Second, we show how to use multiple spoken language understanding engines (SLUs) in state tracking — multiple SLUs can expand the set of dialog states being tracked, and give more information about each, thereby increasing both recall and precision of state tracking. We evaluate on the second Dialog State Tracking Challenge; together these two techniques yield highest accuracy in 2 of 3 tasks, including the most difficult and general task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dan Bohus</author>
<author>Alex Rudnicky</author>
</authors>
<title>A ‘K hypotheses + other’ belief updating model.</title>
<date>2006</date>
<booktitle>In Proc American Association for Artificial Intelligence (AAAI) Workshop on Statistical and Empirical Approaches for Spoken Dialogue Systems,</booktitle>
<location>Boston.</location>
<contexts>
<context position="2279" citStr="Bohus and Rudnicky, 2006" startWordPosition="355" endWordPosition="358">e tracking is crucial because the system relies on the estimated dialog state to choose actions – for example, which restaurants to present to the user. Historically, commercial systems have used hand-crafted rules for state tracking, selecting the SLU result with the highest confidence score observed so far, and discarding alternatives. In contrast, statistical approaches compute a posterior distribution over many hypotheses for the dialog state, and in general these have been shown to be superior (Horvitz and Paek, 1999; Williams and Young, 2007; Young et al., 2009; Thomson and Young, 2010; Bohus and Rudnicky, 2006; Metallinou et al., 2013; Williams et al., 2013). This paper makes two contributions to the task of statistical dialog state tracking. First, we show how to cast dialog state tracking as web-style ranking. Each dialog state can be viewed as a document, and each dialog turn can be viewed as a search instance. The benefit of this construction is that it enables a rich literature of powerful ranking algorithms to be applied. For example, the ranker we apply constructs a forest of decision trees, which — unlike existing work — automatically encodes conjunctions of low-level features. Conjunctions</context>
<context position="6476" citStr="Bohus and Rudnicky, 2006" startWordPosition="1066" endWordPosition="1069">rvations in the form of SLU hypotheses (Horvitz and Paek, 1999; Williams and Young, 2007; Young et al., 2009; Thomson and Young, 2010). More recently, discriminatively trained direct models have been applied, and two studies on dialog data from two publicly deployed dialog systems suggest direct models yield better performance (Williams, 2012; Zilka et al., 2013). The methods introduced in this paper also use discriminative techniques. One of the first approaches to direct models for dialog state tracking was to consider a small, fixed number of states and then apply a multinomial classifier (Bohus and Rudnicky, 2006). Since a multinomial classifier can make effective use of more features than a generative model, this approach improves precision, but can decrease recall by only considering a small number of states (e.g. 5 states). Another discriminative approach is to score each state using a binary model, then somehow combine the binary scores to form a distribution – see, for example (Henderson et al., 2013b) which used a binary neural network. This approach scales to many states, but unlike a multinomial classifier, each binary classifier isn’t aware of its competitors, reducing accuracy. Also, when tra</context>
</contexts>
<marker>Bohus, Rudnicky, 2006</marker>
<rawString>Dan Bohus and Alex Rudnicky. 2006. A ‘K hypotheses + other’ belief updating model. In Proc American Association for Artificial Intelligence (AAAI) Workshop on Statistical and Empirical Approaches for Spoken Dialogue Systems, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher J C Burges</author>
</authors>
<title>From ranknet to lambdarank to lambdamart: An overview.</title>
<date>2010</date>
<tech>Technical Report MSR-TR-2010-82, Microsoft Research.</tech>
<contexts>
<context position="14849" citStr="Burges, 2010" startWordPosition="2483" endWordPosition="2484">“not relevant”. The application of ranking to dialog state tracking is straightforward: instead of ranking features of documents and queries φ(DZ, Q), we rank features of dialog states φ(XZ). For labeling, the correct dialog state is “relevant” and all other states are “not relevant”. Like dialog state tracking, ranking tasks often have features which are constant over all documents – particularly features of the query. This is one reason why ranking algorithms have incorporated methods for automatically building conjunctions. The specific algorithm we use here is lambdaMART (Wu et al., 2010; Burges, 2010). LambdaMART is a mature, scalable ranking algorithm: it has underpinned the winning entry in a community ranking challenge task (Chapelle and Chang, 2011), and is the foundation of the ranker in the Bing search engine. LambdaMART constructs a forest of M decision trees, where each tree consists of binary branches on features, and the leaf nodes are real values. Each binary branch specifies a threshold to apply to a single feature. For a M F(x) = E αmfm(x) (2) m=1 where αm is the weight of tree m and fm(x) is the value of the leaf node obtained by evaluating decision tree m by features [O1(x),</context>
</contexts>
<marker>Burges, 2010</marker>
<rawString>Christopher J.C. Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Technical Report MSR-TR-2010-82, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Chapelle</author>
<author>Yi Chang</author>
</authors>
<title>Yahoo! learning to rank challenge.</title>
<date>2011</date>
<booktitle>JMLR Workshop and Conference Proceedings,</booktitle>
<pages>14--1</pages>
<contexts>
<context position="15004" citStr="Chapelle and Chang, 2011" startWordPosition="2506" endWordPosition="2509">DZ, Q), we rank features of dialog states φ(XZ). For labeling, the correct dialog state is “relevant” and all other states are “not relevant”. Like dialog state tracking, ranking tasks often have features which are constant over all documents – particularly features of the query. This is one reason why ranking algorithms have incorporated methods for automatically building conjunctions. The specific algorithm we use here is lambdaMART (Wu et al., 2010; Burges, 2010). LambdaMART is a mature, scalable ranking algorithm: it has underpinned the winning entry in a community ranking challenge task (Chapelle and Chang, 2011), and is the foundation of the ranker in the Bing search engine. LambdaMART constructs a forest of M decision trees, where each tree consists of binary branches on features, and the leaf nodes are real values. Each binary branch specifies a threshold to apply to a single feature. For a M F(x) = E αmfm(x) (2) m=1 where αm is the weight of tree m and fm(x) is the value of the leaf node obtained by evaluating decision tree m by features [O1(x), ... , OJ(x)]. The training objective is to maximize ranking quality, which here means one-best accuracy. The decision trees are learned by regularized gra</context>
</contexts>
<marker>Chapelle, Chang, 2011</marker>
<rawString>Olivier Chapelle and Yi Chang. 2011. Yahoo! learning to rank challenge. JMLR Workshop and Conference Proceedings, 14:1–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Henderson</author>
<author>Milica Gasic</author>
<author>Blaise Thomson</author>
<author>Pirros Tsiakoulis</author>
<author>Kai Yu</author>
<author>Steve Young</author>
</authors>
<title>Discriminative spoken language understanding using word confusion networks.</title>
<date>2012</date>
<booktitle>In Proc IEEE Workshop on Spoken Language Technologies (SLT),</booktitle>
<location>Miami, Florida, USA.</location>
<contexts>
<context position="18085" citStr="Henderson et al., 2012" startWordPosition="3062" endWordPosition="3065">gine (such as confidence score 285 or position on the N-best list), it is duplicated K times – i.e., once for each SLU engine. Additional binary features are added to encode whether each SLU engine has output the slot value of this dialog state. This allows for the situation that a slot value is not output by all SLU engines, in which case its confidence score, N-best list position, etc. will not be present from some SLU engines. Using two SLU engines on our data increases the number of features per joint goal from 1964 to 3140. 5.1 SLU Engines We built two new SLU engines, broadly following (Henderson et al., 2012). Both consist of many binary classifiers. In the first engine SLU1, a binary classifier is estimated for each slot/value pair, and predicts the presence of that slot/value pair in the utterance. Similarly, a binary classifier is estimated for each user dialog act. Input features are word n-grams from the ASR N-best list. We only considered n-grams which were observed at least c times in the training data; infrequent n-grams were mapped to a special UNK feature. For binary classification we used decision trees, which marginally outperformed logistic regression, SVMs, and deep neural networks. </context>
</contexts>
<marker>Henderson, Gasic, Thomson, Tsiakoulis, Yu, Young, 2012</marker>
<rawString>Matthew Henderson, Milica Gasic, Blaise Thomson, Pirros Tsiakoulis, Kai Yu, and Steve Young. 2012. Discriminative spoken language understanding using word confusion networks. In Proc IEEE Workshop on Spoken Language Technologies (SLT), Miami, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Henderson</author>
<author>Blaise Thomson</author>
<author>Jason D Williams</author>
</authors>
<title>Handbook for the dialog state tracking challenge 2 &amp; 3. Technical report,</title>
<date>2013</date>
<institution>Cambridge University.</institution>
<contexts>
<context position="6875" citStr="Henderson et al., 2013" startWordPosition="1133" endWordPosition="1136">r also use discriminative techniques. One of the first approaches to direct models for dialog state tracking was to consider a small, fixed number of states and then apply a multinomial classifier (Bohus and Rudnicky, 2006). Since a multinomial classifier can make effective use of more features than a generative model, this approach improves precision, but can decrease recall by only considering a small number of states (e.g. 5 states). Another discriminative approach is to score each state using a binary model, then somehow combine the binary scores to form a distribution – see, for example (Henderson et al., 2013b) which used a binary neural network. This approach scales to many states, but unlike a multinomial classifier, each binary classifier isn’t aware of its competitors, reducing accuracy. Also, when training a binary model in the conventional way, the training criteria is mis-matched, since the classifier is trained per hypothesis per timestep, but is evaluated only once per timestep. Maximum entropy (maxent) models have been proposed which provide the strengths of both of these approaches (Metallinou et al., 2013). The probability of a dialog hypothesis xi being correct (y = i) is computed as:</context>
<context position="9930" citStr="Henderson et al., 2013" startWordPosition="1638" endWordPosition="1641">ach output class takes the average score assigned by all the models. Under certain assumptions — most importantly that errors are made independently — score averaging is guaranteed to exceed the performance of the best single model. Score averaging has been applied to dialog state tracking in previous work (Lee and Eskenazi, 2013). Here we use score averaging to maximize data use in cascaded models, and as a hedge against unlucky parameter settings. 3 Preliminaries In this paper we use data and evaluation metrics from the second dialog state tracking challenge (DSTC2) (Henderson et al., 2014; Henderson et al., 2013a). Dialogs in DSTC2 are in the restaurant search domain. Users can search for restaurants in multiple ways, including via constraints, or by name. The system can offer restaurants that match, confirm user input, ask for additional constraints, etc. There are three components to the hidden dialog state: user’s goal, search method, and requested slots. The user’s goal specifies the user’s search constraints, and consists of 4 slots: area, pricerange, foodtype, and name. The number of values for the slots ranges from 4 to 113.1 In DSTC2, trackers output scored lists for each slot, and also a sco</context>
</contexts>
<marker>Henderson, Thomson, Williams, 2013</marker>
<rawString>Matthew Henderson, Blaise Thomson, and Jason D. Williams. 2013a. Handbook for the dialog state tracking challenge 2 &amp; 3. Technical report, Cambridge University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Henderson</author>
<author>Blaise Thomson</author>
<author>Steve Young</author>
</authors>
<title>Deep neural network approach for the dialog state tracking challenge.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>467--471</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Metz, France,</location>
<contexts>
<context position="6875" citStr="Henderson et al., 2013" startWordPosition="1133" endWordPosition="1136">r also use discriminative techniques. One of the first approaches to direct models for dialog state tracking was to consider a small, fixed number of states and then apply a multinomial classifier (Bohus and Rudnicky, 2006). Since a multinomial classifier can make effective use of more features than a generative model, this approach improves precision, but can decrease recall by only considering a small number of states (e.g. 5 states). Another discriminative approach is to score each state using a binary model, then somehow combine the binary scores to form a distribution – see, for example (Henderson et al., 2013b) which used a binary neural network. This approach scales to many states, but unlike a multinomial classifier, each binary classifier isn’t aware of its competitors, reducing accuracy. Also, when training a binary model in the conventional way, the training criteria is mis-matched, since the classifier is trained per hypothesis per timestep, but is evaluated only once per timestep. Maximum entropy (maxent) models have been proposed which provide the strengths of both of these approaches (Metallinou et al., 2013). The probability of a dialog hypothesis xi being correct (y = i) is computed as:</context>
<context position="9930" citStr="Henderson et al., 2013" startWordPosition="1638" endWordPosition="1641">ach output class takes the average score assigned by all the models. Under certain assumptions — most importantly that errors are made independently — score averaging is guaranteed to exceed the performance of the best single model. Score averaging has been applied to dialog state tracking in previous work (Lee and Eskenazi, 2013). Here we use score averaging to maximize data use in cascaded models, and as a hedge against unlucky parameter settings. 3 Preliminaries In this paper we use data and evaluation metrics from the second dialog state tracking challenge (DSTC2) (Henderson et al., 2014; Henderson et al., 2013a). Dialogs in DSTC2 are in the restaurant search domain. Users can search for restaurants in multiple ways, including via constraints, or by name. The system can offer restaurants that match, confirm user input, ask for additional constraints, etc. There are three components to the hidden dialog state: user’s goal, search method, and requested slots. The user’s goal specifies the user’s search constraints, and consists of 4 slots: area, pricerange, foodtype, and name. The number of values for the slots ranges from 4 to 113.1 In DSTC2, trackers output scored lists for each slot, and also a sco</context>
</contexts>
<marker>Henderson, Thomson, Young, 2013</marker>
<rawString>Matthew Henderson, Blaise Thomson, and Steve Young. 2013b. Deep neural network approach for the dialog state tracking challenge. In Proceedings of the SIGDIAL 2013 Conference, pages 467–471, Metz, France, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Henderson</author>
<author>Blaise Thomson</author>
<author>Jason Williams</author>
</authors>
<title>The second dialog state tracking challenge.</title>
<date>2014</date>
<booktitle>In Proceedings of the SIGdial 2014 Conference,</booktitle>
<location>Baltimore, U.S.A.,</location>
<contexts>
<context position="1272" citStr="Henderson et al., 2014" startWordPosition="195" endWordPosition="198">pand the set of dialog states being tracked, and give more information about each, thereby increasing both recall and precision of state tracking. We evaluate on the second Dialog State Tracking Challenge; together these two techniques yield highest accuracy in 2 of 3 tasks, including the most difficult and general task. 1 Introduction Spoken dialog systems interact with users via natural language to help them achieve a goal. As the interaction progresses, the dialog manager maintains a representation of the state of the dialog in a process called dialog state tracking (Williams et al., 2013; Henderson et al., 2014). For example, in a restaurant search application, the dialog state might indicate that the user is looking for an inexpensive restaurant in the center of town. Dialog state tracking is difficult because errors in automatic speech recognition (ASR) and spoken language understanding (SLU) are common, and can cause the system to misunderstand the user’s needs. At the same time, state tracking is crucial because the system relies on the estimated dialog state to choose actions – for example, which restaurants to present to the user. Historically, commercial systems have used hand-crafted rules fo</context>
<context position="9906" citStr="Henderson et al., 2014" startWordPosition="1634" endWordPosition="1637"> several models, where each output class takes the average score assigned by all the models. Under certain assumptions — most importantly that errors are made independently — score averaging is guaranteed to exceed the performance of the best single model. Score averaging has been applied to dialog state tracking in previous work (Lee and Eskenazi, 2013). Here we use score averaging to maximize data use in cascaded models, and as a hedge against unlucky parameter settings. 3 Preliminaries In this paper we use data and evaluation metrics from the second dialog state tracking challenge (DSTC2) (Henderson et al., 2014; Henderson et al., 2013a). Dialogs in DSTC2 are in the restaurant search domain. Users can search for restaurants in multiple ways, including via constraints, or by name. The system can offer restaurants that match, confirm user input, ask for additional constraints, etc. There are three components to the hidden dialog state: user’s goal, search method, and requested slots. The user’s goal specifies the user’s search constraints, and consists of 4 slots: area, pricerange, foodtype, and name. The number of values for the slots ranges from 4 to 113.1 In DSTC2, trackers output scored lists for e</context>
</contexts>
<marker>Henderson, Thomson, Williams, 2014</marker>
<rawString>Matthew Henderson, Blaise Thomson, and Jason Williams. 2014. The second dialog state tracking challenge. In Proceedings of the SIGdial 2014 Conference, Baltimore, U.S.A., June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Horvitz</author>
<author>Tim Paek</author>
</authors>
<title>A computational architecture for conversation.</title>
<date>1999</date>
<booktitle>In Proc 7th International Conference on User Modeling (UM),</booktitle>
<pages>201--210</pages>
<location>Banff, Canada,</location>
<contexts>
<context position="2182" citStr="Horvitz and Paek, 1999" startWordPosition="339" endWordPosition="342"> are common, and can cause the system to misunderstand the user’s needs. At the same time, state tracking is crucial because the system relies on the estimated dialog state to choose actions – for example, which restaurants to present to the user. Historically, commercial systems have used hand-crafted rules for state tracking, selecting the SLU result with the highest confidence score observed so far, and discarding alternatives. In contrast, statistical approaches compute a posterior distribution over many hypotheses for the dialog state, and in general these have been shown to be superior (Horvitz and Paek, 1999; Williams and Young, 2007; Young et al., 2009; Thomson and Young, 2010; Bohus and Rudnicky, 2006; Metallinou et al., 2013; Williams et al., 2013). This paper makes two contributions to the task of statistical dialog state tracking. First, we show how to cast dialog state tracking as web-style ranking. Each dialog state can be viewed as a document, and each dialog turn can be viewed as a search instance. The benefit of this construction is that it enables a rich literature of powerful ranking algorithms to be applied. For example, the ranker we apply constructs a forest of decision trees, whic</context>
<context position="5913" citStr="Horvitz and Paek, 1999" startWordPosition="977" endWordPosition="980">N0 &gt; N. In this work, G simply takes the Cartesian product of X and U. Second, for each new state hypothesis x0i, a vector of J features is extracted, φ(x0i) = [φ1(x0i), ... , φJ(x0 i)]. In the third stage, a scoring process takes all of the features for all of the new dialog states and scores them to produce the new distribution over dialog states, P0(x0i). This new distribution is used to choose another system action, and the whole process repeats. Most early work cast dialog state tracking as a generative model in which hidden user goals generate observations in the form of SLU hypotheses (Horvitz and Paek, 1999; Williams and Young, 2007; Young et al., 2009; Thomson and Young, 2010). More recently, discriminatively trained direct models have been applied, and two studies on dialog data from two publicly deployed dialog systems suggest direct models yield better performance (Williams, 2012; Zilka et al., 2013). The methods introduced in this paper also use discriminative techniques. One of the first approaches to direct models for dialog state tracking was to consider a small, fixed number of states and then apply a multinomial classifier (Bohus and Rudnicky, 2006). Since a multinomial classifier can </context>
</contexts>
<marker>Horvitz, Paek, 1999</marker>
<rawString>Eric Horvitz and Tim Paek. 1999. A computational architecture for conversation. In Proc 7th International Conference on User Modeling (UM), Banff, Canada, pages 201–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sungjin Lee</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Recipe for building robust spoken dialog state trackers: Dialog state tracking challenge system description.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>414--422</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Metz, France,</location>
<contexts>
<context position="7656" citStr="Lee and Eskenazi, 2013" startWordPosition="1259" endWordPosition="1262">petitors, reducing accuracy. Also, when training a binary model in the conventional way, the training criteria is mis-matched, since the classifier is trained per hypothesis per timestep, but is evaluated only once per timestep. Maximum entropy (maxent) models have been proposed which provide the strengths of both of these approaches (Metallinou et al., 2013). The probability of a dialog hypothesis xi being correct (y = i) is computed as: P(y = i|X, λ) = Ex∈X exp(Ej∈J λjφj(x)) . exp(Ej∈J λjφj(xi)) (1) Maximum entropy models yielded top performance in the first dialog state tracking challenge (Lee and Eskenazi, 2013). In this paper, we use maxent models as a baseline. A key limitation with linear (and log-linear) models such as maximum entropy models is that they do not automatically build conjunctions of features. Conjunctions express conditional combinations of features such as whether the system attempted to confirm x and if “yes” was recognized and if the confidence score of “yes” is high. Conjunctions are important in dialog state tracking because they are often more discriminative than individual features. Moreover, in linear models for dialog state tracking, one weight is learned per feature (equat</context>
<context position="9640" citStr="Lee and Eskenazi, 2013" startWordPosition="1591" endWordPosition="1594">e which conjunctions will be useful. This paper introduces algorithms from web-style ranking as a mechanism for automatically building feature conjunctions. In this paper we also use score averaging, a well-known machine learning technique for combining the output of several models, where each output class takes the average score assigned by all the models. Under certain assumptions — most importantly that errors are made independently — score averaging is guaranteed to exceed the performance of the best single model. Score averaging has been applied to dialog state tracking in previous work (Lee and Eskenazi, 2013). Here we use score averaging to maximize data use in cascaded models, and as a hedge against unlucky parameter settings. 3 Preliminaries In this paper we use data and evaluation metrics from the second dialog state tracking challenge (DSTC2) (Henderson et al., 2014; Henderson et al., 2013a). Dialogs in DSTC2 are in the restaurant search domain. Users can search for restaurants in multiple ways, including via constraints, or by name. The system can offer restaurants that match, confirm user input, ask for additional constraints, etc. There are three components to the hidden dialog state: user’</context>
<context position="11038" citStr="Lee and Eskenazi, 2013" startWordPosition="1827" endWordPosition="1830">f values for the slots ranges from 4 to 113.1 In DSTC2, trackers output scored lists for each slot, and also a scored list of joint hypotheses. For example, at a given timestep in a given dialog, three joint goal hypothesis might be (area=west,food=italian), (area=west), and (), where () means the user hasn’t specified any constraints yet. Since tracking the joint user goal is 1Including a special “don’t care” value. the most general and most difficult task, we’ll focus on this first, and return to the other tasks in section 6. 3.1 User goal features For features, we broadly follow past work (Lee and Eskenazi, 2013; Lee, 2013; Metallinou et al., 2013). For a hypothesis xi, for each slot the features encode 253 low-level quantities, such as: whether the slot value appears in this hypothesis; how many times the slot value has been observed; whether the slot value has been observed in this turn; functions of recognition metrics such as confidence score and position on N-best list; goal priors and confusion probabilities estimated on training data (Williams, 2012; Metallinou et al., 2013); results of confirmation attempts (“Italian food, is that right?”); output of the four rule-based baseline trackers; and</context>
</contexts>
<marker>Lee, Eskenazi, 2013</marker>
<rawString>Sungjin Lee and Maxine Eskenazi. 2013. Recipe for building robust spoken dialog state trackers: Dialog state tracking challenge system description. In Proceedings of the SIGDIAL 2013 Conference, pages 414–422, Metz, France, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sungjin Lee</author>
</authors>
<title>Structured discriminative model for dialog state tracking.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>442--451</pages>
<location>Metz, France,</location>
<contexts>
<context position="8799" citStr="Lee, 2013" startWordPosition="1459" endWordPosition="1460">for dialog state tracking, one weight is learned per feature (equation 1) (Metallinou et al., 2013). As a result, if a feature takes the same value for every dialog hypothesis at a given timestep, its contribution to every hypothesis will be the same, and it will therefore have no effect on the ranking. For example, features describing the current system action 283 are identical for all state hypotheses. Concretely, if φj(xi) = c for all i, then changing c causes no change in P(y = i|X, A) for all i. Past work has shown that conjunctions improve dialog state tracking (Metallinou et al., 2013; Lee, 2013). However, past work has added conjunction by hand, and this doesn’t scale: the number of possible conjunctions increases exponentially in the number of terms in the conjunction, and it’s difficult to predict in advance which conjunctions will be useful. This paper introduces algorithms from web-style ranking as a mechanism for automatically building feature conjunctions. In this paper we also use score averaging, a well-known machine learning technique for combining the output of several models, where each output class takes the average score assigned by all the models. Under certain assumpti</context>
<context position="11049" citStr="Lee, 2013" startWordPosition="1831" endWordPosition="1832">anges from 4 to 113.1 In DSTC2, trackers output scored lists for each slot, and also a scored list of joint hypotheses. For example, at a given timestep in a given dialog, three joint goal hypothesis might be (area=west,food=italian), (area=west), and (), where () means the user hasn’t specified any constraints yet. Since tracking the joint user goal is 1Including a special “don’t care” value. the most general and most difficult task, we’ll focus on this first, and return to the other tasks in section 6. 3.1 User goal features For features, we broadly follow past work (Lee and Eskenazi, 2013; Lee, 2013; Metallinou et al., 2013). For a hypothesis xi, for each slot the features encode 253 low-level quantities, such as: whether the slot value appears in this hypothesis; how many times the slot value has been observed; whether the slot value has been observed in this turn; functions of recognition metrics such as confidence score and position on N-best list; goal priors and confusion probabilities estimated on training data (Williams, 2012; Metallinou et al., 2013); results of confirmation attempts (“Italian food, is that right?”); output of the four rule-based baseline trackers; and the system</context>
</contexts>
<marker>Lee, 2013</marker>
<rawString>Sungjin Lee. 2013. Structured discriminative model for dialog state tracking. In Proceedings of the SIGDIAL 2013 Conference, pages 442–451, Metz, France, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Metallinou</author>
<author>Dan Bohus</author>
<author>Jason D Williams</author>
</authors>
<title>Discriminative state tracking for spoken dialog systems.</title>
<date>2013</date>
<booktitle>In Proc Association for Computational Linguistics,</booktitle>
<location>Sofia.</location>
<contexts>
<context position="2304" citStr="Metallinou et al., 2013" startWordPosition="359" endWordPosition="363">use the system relies on the estimated dialog state to choose actions – for example, which restaurants to present to the user. Historically, commercial systems have used hand-crafted rules for state tracking, selecting the SLU result with the highest confidence score observed so far, and discarding alternatives. In contrast, statistical approaches compute a posterior distribution over many hypotheses for the dialog state, and in general these have been shown to be superior (Horvitz and Paek, 1999; Williams and Young, 2007; Young et al., 2009; Thomson and Young, 2010; Bohus and Rudnicky, 2006; Metallinou et al., 2013; Williams et al., 2013). This paper makes two contributions to the task of statistical dialog state tracking. First, we show how to cast dialog state tracking as web-style ranking. Each dialog state can be viewed as a document, and each dialog turn can be viewed as a search instance. The benefit of this construction is that it enables a rich literature of powerful ranking algorithms to be applied. For example, the ranker we apply constructs a forest of decision trees, which — unlike existing work — automatically encodes conjunctions of low-level features. Conjunctions are attractive in dialog</context>
<context position="7394" citStr="Metallinou et al., 2013" startWordPosition="1214" endWordPosition="1217"> then somehow combine the binary scores to form a distribution – see, for example (Henderson et al., 2013b) which used a binary neural network. This approach scales to many states, but unlike a multinomial classifier, each binary classifier isn’t aware of its competitors, reducing accuracy. Also, when training a binary model in the conventional way, the training criteria is mis-matched, since the classifier is trained per hypothesis per timestep, but is evaluated only once per timestep. Maximum entropy (maxent) models have been proposed which provide the strengths of both of these approaches (Metallinou et al., 2013). The probability of a dialog hypothesis xi being correct (y = i) is computed as: P(y = i|X, λ) = Ex∈X exp(Ej∈J λjφj(x)) . exp(Ej∈J λjφj(xi)) (1) Maximum entropy models yielded top performance in the first dialog state tracking challenge (Lee and Eskenazi, 2013). In this paper, we use maxent models as a baseline. A key limitation with linear (and log-linear) models such as maximum entropy models is that they do not automatically build conjunctions of features. Conjunctions express conditional combinations of features such as whether the system attempted to confirm x and if “yes” was recognized</context>
<context position="8787" citStr="Metallinou et al., 2013" startWordPosition="1455" endWordPosition="1458">reover, in linear models for dialog state tracking, one weight is learned per feature (equation 1) (Metallinou et al., 2013). As a result, if a feature takes the same value for every dialog hypothesis at a given timestep, its contribution to every hypothesis will be the same, and it will therefore have no effect on the ranking. For example, features describing the current system action 283 are identical for all state hypotheses. Concretely, if φj(xi) = c for all i, then changing c causes no change in P(y = i|X, A) for all i. Past work has shown that conjunctions improve dialog state tracking (Metallinou et al., 2013; Lee, 2013). However, past work has added conjunction by hand, and this doesn’t scale: the number of possible conjunctions increases exponentially in the number of terms in the conjunction, and it’s difficult to predict in advance which conjunctions will be useful. This paper introduces algorithms from web-style ranking as a mechanism for automatically building feature conjunctions. In this paper we also use score averaging, a well-known machine learning technique for combining the output of several models, where each output class takes the average score assigned by all the models. Under cert</context>
<context position="11075" citStr="Metallinou et al., 2013" startWordPosition="1833" endWordPosition="1836">4 to 113.1 In DSTC2, trackers output scored lists for each slot, and also a scored list of joint hypotheses. For example, at a given timestep in a given dialog, three joint goal hypothesis might be (area=west,food=italian), (area=west), and (), where () means the user hasn’t specified any constraints yet. Since tracking the joint user goal is 1Including a special “don’t care” value. the most general and most difficult task, we’ll focus on this first, and return to the other tasks in section 6. 3.1 User goal features For features, we broadly follow past work (Lee and Eskenazi, 2013; Lee, 2013; Metallinou et al., 2013). For a hypothesis xi, for each slot the features encode 253 low-level quantities, such as: whether the slot value appears in this hypothesis; how many times the slot value has been observed; whether the slot value has been observed in this turn; functions of recognition metrics such as confidence score and position on N-best list; goal priors and confusion probabilities estimated on training data (Williams, 2012; Metallinou et al., 2013); results of confirmation attempts (“Italian food, is that right?”); output of the four rule-based baseline trackers; and the system act and its relation to t</context>
</contexts>
<marker>Metallinou, Bohus, Williams, 2013</marker>
<rawString>Angeliki Metallinou, Dan Bohus, and Jason D. Williams. 2013. Discriminative state tracking for spoken dialog systems. In Proc Association for Computational Linguistics, Sofia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blaise Thomson</author>
<author>Steve Young</author>
</authors>
<title>Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems.</title>
<date>2010</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="2253" citStr="Thomson and Young, 2010" startWordPosition="351" endWordPosition="354">s. At the same time, state tracking is crucial because the system relies on the estimated dialog state to choose actions – for example, which restaurants to present to the user. Historically, commercial systems have used hand-crafted rules for state tracking, selecting the SLU result with the highest confidence score observed so far, and discarding alternatives. In contrast, statistical approaches compute a posterior distribution over many hypotheses for the dialog state, and in general these have been shown to be superior (Horvitz and Paek, 1999; Williams and Young, 2007; Young et al., 2009; Thomson and Young, 2010; Bohus and Rudnicky, 2006; Metallinou et al., 2013; Williams et al., 2013). This paper makes two contributions to the task of statistical dialog state tracking. First, we show how to cast dialog state tracking as web-style ranking. Each dialog state can be viewed as a document, and each dialog turn can be viewed as a search instance. The benefit of this construction is that it enables a rich literature of powerful ranking algorithms to be applied. For example, the ranker we apply constructs a forest of decision trees, which — unlike existing work — automatically encodes conjunctions of low-le</context>
<context position="5985" citStr="Thomson and Young, 2010" startWordPosition="989" endWordPosition="992"> Second, for each new state hypothesis x0i, a vector of J features is extracted, φ(x0i) = [φ1(x0i), ... , φJ(x0 i)]. In the third stage, a scoring process takes all of the features for all of the new dialog states and scores them to produce the new distribution over dialog states, P0(x0i). This new distribution is used to choose another system action, and the whole process repeats. Most early work cast dialog state tracking as a generative model in which hidden user goals generate observations in the form of SLU hypotheses (Horvitz and Paek, 1999; Williams and Young, 2007; Young et al., 2009; Thomson and Young, 2010). More recently, discriminatively trained direct models have been applied, and two studies on dialog data from two publicly deployed dialog systems suggest direct models yield better performance (Williams, 2012; Zilka et al., 2013). The methods introduced in this paper also use discriminative techniques. One of the first approaches to direct models for dialog state tracking was to consider a small, fixed number of states and then apply a multinomial classifier (Bohus and Rudnicky, 2006). Since a multinomial classifier can make effective use of more features than a generative model, this approa</context>
</contexts>
<marker>Thomson, Young, 2010</marker>
<rawString>Blaise Thomson and Steve Young. 2010. Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems. Computer Speech and Language, 24(4):562–588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Thomson</author>
<author>K Yu</author>
<author>M Gasic</author>
<author>S Keizer</author>
<author>F Mairesse</author>
<author>J Schatzmann</author>
<author>S Young</author>
</authors>
<title>Evaluating semantic-level confidence scores with multiple hypotheses.</title>
<date>2008</date>
<booktitle>In Proc Intl Conf on Spoken Language Processing (ICSLP),</booktitle>
<location>Brisbane, Australia.</location>
<contexts>
<context position="20095" citStr="Thomson et al., 2008" startWordPosition="3406" endWordPosition="3409">fically, each word (unigram) appearing in the word confusion network is a feature. Bi-gram confusion network features did not improve performance. If we train a new SLU engine and a ranker on the same data, this will introduce unwanted bias. Therefore, we divided the training data in half, and use the first half for training the SLU, and the second for training the ranker. Table 1 shows several evaluation metrics for each SLU engine, including the SLU included in the corpus, which we denote SLU0. SLU precision, recall, and F-measure are computed on the top hypotheses. Item crossentropy (ICE) (Thomson et al., 2008) measures the quality of the scores for all the items on the SLU N-best list. Table 1 also shows joint goal accuracy by using SLU0, SLU1, or SLU2, for either a rule-based baseline or the ranking model. Overall, our SLU engines performed better on isolated SLU metrics, but did not yield better state tracking performance when used instead of the SLU results in the corpus. 5.2 Results with multiple SLU engines Table 2, rows 4 and 7 show that an improvement in performance does results from using 2 SLU engines. In rows 4 and 7, the additional SLU engine is trained on the first half of the data, and</context>
<context position="22940" citStr="Thomson et al., 2008" startWordPosition="3914" endWordPosition="3917">85 0.623 0.666 0.900 0.691 0.782 1.955 0.719 0.739 SLU1 0.818 0.729 0.771 2.189 0.598 0.637 0.846 0.762 0.802 1.943 0.667 0.709 SLU2 0.844 0.742 0.789 2.098 0.605 0.658 0.870 0.777 0.821 1.845 0.685 0.734 Table 1: Performance of three SLU engines. SLU0 is the DSTC2 corpus; SLU1 is our engine with uni-grams and bi-grams of ASR results in the corpus; and SLU2 is SLU1 with the addition of unigram features from the word confusion network. Precision, Recall, F-measure, and ICE evaluate the quality of the SLU output, not state tracking. “ICE” is item-wise cross entropy — smaller numbers are better (Thomson et al., 2008). “Rules” indicates dialog state tracking accuracy for user joint goals by running the rule-based baseline tracker on the indicated SLU (alone); “Ranking” indicates joint goal accuracy of running a ranker trained on the indicated SLU (alone). For training, goal tracking results use the “Fold A” configuration (c.f. Section 5.2). 5.4 Joint goal tracking summary The overall process used to train the joint goal tracker is summarized in Appendix B. For joint goal tracking, web-style ranking and multiple SLUs both yield improvements in accuracy on the development and test sets, with the improvement </context>
</contexts>
<marker>Thomson, Yu, Gasic, Keizer, Mairesse, Schatzmann, Young, 2008</marker>
<rawString>B Thomson, K Yu, M Gasic, S Keizer, F Mairesse, J Schatzmann, and S Young. 2008. Evaluating semantic-level confidence scores with multiple hypotheses. In Proc Intl Conf on Spoken Language Processing (ICSLP), Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuoran Wang</author>
<author>Oliver Lemon</author>
</authors>
<title>A simple and generic belief tracking mechanism for the dialog state tracking challenge: On the believability of observed information.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>423--432</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Metz, France,</location>
<contexts>
<context position="13326" citStr="Wang and Lemon, 2013" startWordPosition="2218" endWordPosition="2221">ite.2 The DSTC2 corpus consists of three partitions: train, development, and test. Throughout sections 4-6, we report accuracy by training on the training set, and report accuracy on the development set and test set. The development set was available during development of the models, whereas the test set was not. 2camdial.org/˜mh521/dstc/ 284 3.3 Baselines forest of M trees, the score of a dialog state x is We first compare to the four rule-based trackers provided by DSTC2. These were carefully designed by other research groups, and earlier versions of them scored very well in the first DSTC (Wang and Lemon, 2013). In each column in Tables 2 and 3, we report the best result from any rule-based tracker. We also compare to a maxent model as in Eq 1. Our implementation includes L1 and L2 regularization which was automatically tuned via cross-validation. 4 Web-style ranking The ranking task is to order a set of N documents by relevance given a query. The input to a ranker is a query Q and set of documents X = {D1, ... , DN}, where each document is described in terms of features of that document and the query φ(DZ, Q). The output is a score for each document, where the highest score indicates the most relev</context>
</contexts>
<marker>Wang, Lemon, 2013</marker>
<rawString>Zhuoran Wang and Oliver Lemon. 2013. A simple and generic belief tracking mechanism for the dialog state tracking challenge: On the believability of observed information. In Proceedings of the SIGDIAL 2013 Conference, pages 423–432, Metz, France, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Steve Young</author>
</authors>
<title>Partially observable Markov decision processes for spoken dialog systems.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="2208" citStr="Williams and Young, 2007" startWordPosition="343" endWordPosition="346">se the system to misunderstand the user’s needs. At the same time, state tracking is crucial because the system relies on the estimated dialog state to choose actions – for example, which restaurants to present to the user. Historically, commercial systems have used hand-crafted rules for state tracking, selecting the SLU result with the highest confidence score observed so far, and discarding alternatives. In contrast, statistical approaches compute a posterior distribution over many hypotheses for the dialog state, and in general these have been shown to be superior (Horvitz and Paek, 1999; Williams and Young, 2007; Young et al., 2009; Thomson and Young, 2010; Bohus and Rudnicky, 2006; Metallinou et al., 2013; Williams et al., 2013). This paper makes two contributions to the task of statistical dialog state tracking. First, we show how to cast dialog state tracking as web-style ranking. Each dialog state can be viewed as a document, and each dialog turn can be viewed as a search instance. The benefit of this construction is that it enables a rich literature of powerful ranking algorithms to be applied. For example, the ranker we apply constructs a forest of decision trees, which — unlike existing work —</context>
<context position="5939" citStr="Williams and Young, 2007" startWordPosition="981" endWordPosition="984">simply takes the Cartesian product of X and U. Second, for each new state hypothesis x0i, a vector of J features is extracted, φ(x0i) = [φ1(x0i), ... , φJ(x0 i)]. In the third stage, a scoring process takes all of the features for all of the new dialog states and scores them to produce the new distribution over dialog states, P0(x0i). This new distribution is used to choose another system action, and the whole process repeats. Most early work cast dialog state tracking as a generative model in which hidden user goals generate observations in the form of SLU hypotheses (Horvitz and Paek, 1999; Williams and Young, 2007; Young et al., 2009; Thomson and Young, 2010). More recently, discriminatively trained direct models have been applied, and two studies on dialog data from two publicly deployed dialog systems suggest direct models yield better performance (Williams, 2012; Zilka et al., 2013). The methods introduced in this paper also use discriminative techniques. One of the first approaches to direct models for dialog state tracking was to consider a small, fixed number of states and then apply a multinomial classifier (Bohus and Rudnicky, 2006). Since a multinomial classifier can make effective use of more</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>Jason D Williams and Steve Young. 2007. Partially observable Markov decision processes for spoken dialog systems. Computer Speech and Language, 21(2):393–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Antoine Raux</author>
<author>Deepak Ramachandran</author>
<author>Alan Black</author>
</authors>
<title>The dialog state tracking challenge.</title>
<date>2013</date>
<booktitle>In Proc SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Metz, France.</location>
<contexts>
<context position="1247" citStr="Williams et al., 2013" startWordPosition="191" endWordPosition="194"> — multiple SLUs can expand the set of dialog states being tracked, and give more information about each, thereby increasing both recall and precision of state tracking. We evaluate on the second Dialog State Tracking Challenge; together these two techniques yield highest accuracy in 2 of 3 tasks, including the most difficult and general task. 1 Introduction Spoken dialog systems interact with users via natural language to help them achieve a goal. As the interaction progresses, the dialog manager maintains a representation of the state of the dialog in a process called dialog state tracking (Williams et al., 2013; Henderson et al., 2014). For example, in a restaurant search application, the dialog state might indicate that the user is looking for an inexpensive restaurant in the center of town. Dialog state tracking is difficult because errors in automatic speech recognition (ASR) and spoken language understanding (SLU) are common, and can cause the system to misunderstand the user’s needs. At the same time, state tracking is crucial because the system relies on the estimated dialog state to choose actions – for example, which restaurants to present to the user. Historically, commercial systems have u</context>
</contexts>
<marker>Williams, Raux, Ramachandran, Black, 2013</marker>
<rawString>Jason D. Williams, Antoine Raux, Deepak Ramachandran, and Alan Black. 2013. The dialog state tracking challenge. In Proc SIGdial Workshop on Discourse and Dialogue, Metz, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
</authors>
<title>Challenges and opportunities for state tracking in statistical spoken dialog systems: Results from two public deployments.</title>
<date>2012</date>
<journal>IEEE Journal of Selected Topics</journal>
<booktitle>in Signal Processing, Special Issue on Advances in Spoken Dialogue Systems and Mobile Interface,</booktitle>
<pages>6--8</pages>
<contexts>
<context position="6195" citStr="Williams, 2012" startWordPosition="1022" endWordPosition="1023">ores them to produce the new distribution over dialog states, P0(x0i). This new distribution is used to choose another system action, and the whole process repeats. Most early work cast dialog state tracking as a generative model in which hidden user goals generate observations in the form of SLU hypotheses (Horvitz and Paek, 1999; Williams and Young, 2007; Young et al., 2009; Thomson and Young, 2010). More recently, discriminatively trained direct models have been applied, and two studies on dialog data from two publicly deployed dialog systems suggest direct models yield better performance (Williams, 2012; Zilka et al., 2013). The methods introduced in this paper also use discriminative techniques. One of the first approaches to direct models for dialog state tracking was to consider a small, fixed number of states and then apply a multinomial classifier (Bohus and Rudnicky, 2006). Since a multinomial classifier can make effective use of more features than a generative model, this approach improves precision, but can decrease recall by only considering a small number of states (e.g. 5 states). Another discriminative approach is to score each state using a binary model, then somehow combine the</context>
<context position="11491" citStr="Williams, 2012" startWordPosition="1905" endWordPosition="1906">, we’ll focus on this first, and return to the other tasks in section 6. 3.1 User goal features For features, we broadly follow past work (Lee and Eskenazi, 2013; Lee, 2013; Metallinou et al., 2013). For a hypothesis xi, for each slot the features encode 253 low-level quantities, such as: whether the slot value appears in this hypothesis; how many times the slot value has been observed; whether the slot value has been observed in this turn; functions of recognition metrics such as confidence score and position on N-best list; goal priors and confusion probabilities estimated on training data (Williams, 2012; Metallinou et al., 2013); results of confirmation attempts (“Italian food, is that right?”); output of the four rule-based baseline trackers; and the system act and its relation to the goal’s slot value (e.g., whether the system act mentions this slot value). Of these 253 features for each slot, 119 are the same for all values of that slot in a given turn, such as which system acts were observed in this turn. For these, we add 238 conjunctions with slot-specific features like confidence score, which makes these features useful to our maxent baseline. This results in a total of 253+238 = 491 </context>
</contexts>
<marker>Williams, 2012</marker>
<rawString>Jason D. Williams. 2012. Challenges and opportunities for state tracking in statistical spoken dialog systems: Results from two public deployments. IEEE Journal of Selected Topics in Signal Processing, Special Issue on Advances in Spoken Dialogue Systems and Mobile Interface, 6(8):959–970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiang Wu</author>
<author>Christopher J C Burges</author>
<author>Krysta M Svore</author>
<author>Jianfeng Gao</author>
</authors>
<title>Adapting boosting for information retrieval measures.</title>
<date>2010</date>
<journal>Journal of Information Retrieval,</journal>
<volume>13</volume>
<issue>3</issue>
<contexts>
<context position="14834" citStr="Wu et al., 2010" startWordPosition="2479" endWordPosition="2482">od”, “fair”, and “not relevant”. The application of ranking to dialog state tracking is straightforward: instead of ranking features of documents and queries φ(DZ, Q), we rank features of dialog states φ(XZ). For labeling, the correct dialog state is “relevant” and all other states are “not relevant”. Like dialog state tracking, ranking tasks often have features which are constant over all documents – particularly features of the query. This is one reason why ranking algorithms have incorporated methods for automatically building conjunctions. The specific algorithm we use here is lambdaMART (Wu et al., 2010; Burges, 2010). LambdaMART is a mature, scalable ranking algorithm: it has underpinned the winning entry in a community ranking challenge task (Chapelle and Chang, 2011), and is the foundation of the ranker in the Bing search engine. LambdaMART constructs a forest of M decision trees, where each tree consists of binary branches on features, and the leaf nodes are real values. Each binary branch specifies a threshold to apply to a single feature. For a M F(x) = E αmfm(x) (2) m=1 where αm is the weight of tree m and fm(x) is the value of the leaf node obtained by evaluating decision tree m by f</context>
</contexts>
<marker>Wu, Burges, Svore, Gao, 2010</marker>
<rawString>Qiang Wu, Christopher J. C. Burges, Krysta M. Svore, and Jianfeng Gao. 2010. Adapting boosting for information retrieval measures. Journal of Information Retrieval, 13(3):254–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Milica Gaˇsi´c</author>
<author>Simon Keizer</author>
<author>Franc¸ois Mairesse</author>
<author>Jost Schatzmann</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
</authors>
<title>The hidden information state model: a practical framework for POMDP-based spoken dialogue management.</title>
<date>2009</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>2</issue>
<marker>Young, Gaˇsi´c, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2009</marker>
<rawString>Steve Young, Milica Gaˇsi´c, Simon Keizer, Franc¸ois Mairesse, Jost Schatzmann, Blaise Thomson, and Kai Yu. 2009. The hidden information state model: a practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2):150–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lukas Zilka</author>
<author>David Marek</author>
<author>Matej Korvas</author>
<author>Filip Jurcicek</author>
</authors>
<title>Comparison of bayesian discriminative and generative models for dialogue state tracking.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>452--456</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Metz, France,</location>
<contexts>
<context position="6216" citStr="Zilka et al., 2013" startWordPosition="1024" endWordPosition="1027">duce the new distribution over dialog states, P0(x0i). This new distribution is used to choose another system action, and the whole process repeats. Most early work cast dialog state tracking as a generative model in which hidden user goals generate observations in the form of SLU hypotheses (Horvitz and Paek, 1999; Williams and Young, 2007; Young et al., 2009; Thomson and Young, 2010). More recently, discriminatively trained direct models have been applied, and two studies on dialog data from two publicly deployed dialog systems suggest direct models yield better performance (Williams, 2012; Zilka et al., 2013). The methods introduced in this paper also use discriminative techniques. One of the first approaches to direct models for dialog state tracking was to consider a small, fixed number of states and then apply a multinomial classifier (Bohus and Rudnicky, 2006). Since a multinomial classifier can make effective use of more features than a generative model, this approach improves precision, but can decrease recall by only considering a small number of states (e.g. 5 states). Another discriminative approach is to score each state using a binary model, then somehow combine the binary scores to for</context>
</contexts>
<marker>Zilka, Marek, Korvas, Jurcicek, 2013</marker>
<rawString>Lukas Zilka, David Marek, Matej Korvas, and Filip Jurcicek. 2013. Comparison of bayesian discriminative and generative models for dialogue state tracking. In Proceedings of the SIGDIAL 2013 Conference, pages 452–456, Metz, France, August. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
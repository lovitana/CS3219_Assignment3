<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000110">
<title confidence="0.5596165">
Better Semantic Frame Based MT Evaluation
via Inversion Transduction Grammars
</title>
<author confidence="0.762198">
Dekai Wu Lo Chi-kiu Meriem BELouCIF Markus SAERs
</author>
<affiliation confidence="0.8929215">
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
</affiliation>
<email confidence="0.996287">
{jackielo|mbeloucif|masaers|dekai}@cs.ust.hk
</email>
<sectionHeader confidence="0.997368" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999927428571429">
We introduce an inversion transduc-
tion grammar based restructuring of
the MEANT automatic semantic frame
based MT evaluation metric, which,
by leveraging ITG language biases, is
able to further improve upon MEANT’s
already-high correlation with human
adequacy judgments. The new metric,
called IMEANT, uses bracketing ITGs to
biparse the reference and machine transla-
tions, but subject to obeying the semantic
frames in both. Resulting improvements
support the presumption that ITGs, which
constrain the allowable permutations
between compositional segments across
the reference and MT output, score the
phrasal similarity of the semantic role
fillers more accurately than the simple
word alignment heuristics (bag-of-word
alignment or maximum alignment) used
in previous version of MEANT. The
approach successfully integrates (1) the
previously demonstrated extremely high
coverage of cross-lingual semantic frame
alternations by ITGs, with (2) the high
accuracy of evaluating MT via weighted
f-scores on the degree of semantic frame
preservation.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985019607843">
There has been to date relatively little use of in-
version transduction grammars (Wu, 1997) to im-
prove the accuracy of MT evaluation metrics, de-
spite long empirical evidence the vast majority of
translation patterns between human languages can
be accommodated within ITG constraints (and the
observation that most current state-of-the-art SMT
systems employ ITG decoders). We show that
ITGs can be used to redesign the MEANT seman-
tic frame based MT evaluation metric (Lo et al.,
2012) to produce improvements in accuracy and
reliability. This work is driven by the motiva-
tion that especially when considering semantic MT
metrics, ITGs would be seem to be a natural basis
for several reasons.
To begin with, it is quite natural to think of
sentences as having been generated from an ab-
stract concept using a rewriting system: a stochas-
tic grammar predicts how frequently any particu-
lar realization of the abstract concept will be gen-
erated. The bilingual analogy is a transduction
grammar generating a pair of possible realizations
of the same underlying concept. Stochastic trans-
duction grammars predict how frequently a partic-
ular pair of realizations will be generated, and thus
represent a good way to evaluate how well a pair
of sentences correspond to each other.
The particular class of transduction gram-
mars known as ITGs tackle the problem that
the (bi)parsing complexity for general syntax-
directed transductions (Aho and Ullman, 1972)
is exponential. By constraining a syntax-directed
transduction grammar to allow only monotonic
straight and inverted reorderings, or equivalently
permitting only binary or ternary rank rules, it is
possible to isolate the low end of that hierarchy into
a single equivalence class of inversion transduc-
tions. ITGs are guaranteed to have a two-normal
form similar to context-free grammars, and can
be biparsed in polynomial time and space (O (n�)
time and O (n4) space). It is also possible to do ap-
proximate biparsing in O (n�) time (Saers et al.,
2009). These polynomial complexities makes it
feasible to estimate the parameters of an ITG us-
ing standard machine learning techniques such as
expectation maximization (Wu, 1995b) .
At the same time, inversion transductions have
also been directly shown to be more than sufficient
to account for the reordering that occur within se-
mantic frame alternations (Addanki et al., 2012).
This makes ITGs an appealing alternative for eval-
</bodyText>
<page confidence="0.976391">
22
</page>
<note confidence="0.7796595">
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999931545454545">
uating the possible links between both semantic
role fillers in different languages as well as the
predicates, and how these parts fit together to form
entire semantic frames. We believe that ITGs are
not only capable of generating the desired struc-
tural correspondences between the semantic struc-
tures of two languages, but also provide meaning-
ful constraints to prevent alignments from wander-
ing off in the wrong direction.
In this paper we show that IMEANT, a new met-
ric drawing from the strengths of both MEANT
and inversion transduction grammars, is able to
exploit bracketing ITGs (also known as BITGs
or BTGs) which are ITGs containing only a sin-
gle non-differentiated non terminal category (Wu,
1995a), so as to produce even higher correlation
with human adequacy judgments than any auto-
matic MEANT variants, or other common auto-
matic metrics. We argue that the constraints pro-
vided by BITGs over the semantic frames and ar-
guments of the reference and MT output sentences
are essential for accurate evaluation of the phrasal
similarity of the semantic role fillers.
In common with the various MEANT semantic
MT evaluation metrics (Lo and Wu, 2011a, 2012;
Lo et al., 2012; Lo and Wu, 2013b), our proposed
IMEANT metric measures the degree to which
the basic semantic event structure is preserved
by translation—the “who did what to whom, for
whom, when, where, how and why” (Pradhan et
al., 2004)—emphasizing that a good translation
is one that can successfully be understood by a
human. In the other versions of MEANT, sim-
ilarity between the MT output and the reference
translations is computed as a modified weighted f-
score over the semantic predicates and role fillers.
Across a variety of language pairs and genres, it
has been shown that MEANT correlates better with
human adequacy judgment than both n-gram based
MT evaluation metrics such as BLEU (Papineni
et al., 2002), NIST (Doddington, 2002), and ME-
TEOR (Banerjee and Lavie, 2005), as well as edit-
distance based metrics such as CDER (Leusch et
al., 2006), WER (Nießen et al., 2000), and TER
(Snover et al., 2006) when evaluating MT output
(Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and
Wu, 2013b; Macháček and Bojar, 2013). Further-
more, tuning the parameters of MT systems with
MEANT instead of BLEU or TER robustly im-
proves translation adequacy across different gen-
res and different languages (English and Chinese)
(Lo et al., 2013a; Lo and Wu, 2013a; Lo et al.,
2013b). This has motivated our choice of MEANT
as the basis on which to experiment with deploying
ITGs into semantic MT evaluation.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.994654">
2.1 ITGs and MT evaluation
</subsectionHeader>
<bodyText confidence="0.9999539375">
Relatively little investigation into the potential
benefits of ITGs is found in previous MT eval-
uation work. One exception is invWER, pro-
posed by Leusch et al. (2003) and Leusch and Ney
(2008). The invWER metric interprets weighted
BITGs as a generalization of the Levenshtein edit
distance, in which entire segments (blocks) can be
inverted, as long as this is done strictly compo-
sitionally so as not to violate legal ITG biparse
tree structures. The input and output languages
are considered to be those of the reference and ma-
chine translations, and thus are over the same vo-
cabulary (say,English). At the sentence level, cor-
relation of invWER with human adequacy judg-
ments was found to be among the best.
Our current approach differs in several key
respects from invWER. First,invWER operates
purely at the surface level of exact token match,
IMEANT mediates between segments of refer-
ence translation and MT output using lexical BITG
probabilities.
Secondly, there is no explicit semantic model-
ing in invWER. Providing they meet the BITG
constraints, the biparse trees in invWER are com-
pletely unconstrained. In contrast, IMEANT em-
ploys the same explicit, strong semantic frame
modeling as MEANT, on both the reference and
machine translations. In IMEANT, the semantic
frames always take precedence over pure BITG
biases. Compared to invWER, this strongly con-
strains the space of biparses that IMEANT permits
to be considered.
</bodyText>
<subsectionHeader confidence="0.998167">
2.2 MT evaluation metrics
</subsectionHeader>
<bodyText confidence="0.9897588">
Like invWER, other common surface-form ori-
ented metrics like BLEU (Papineni et al., 2002),
NIST (Doddington, 2002), METEOR (Banerjee
and Lavie, 2005; Denkowski and Lavie, 2014),
CDER (Leusch et al., 2006), WER (Nießen et
al., 2000), and TER (Snover et al., 2006) do
not correctly reflect the meaning similarities of
the input sentence. There are in fact several
large scale meta-evaluations (Callison-Burch et
al., 2006; Koehn and Monz, 2006) reporting cases
</bodyText>
<page confidence="0.996665">
23
</page>
<bodyText confidence="0.999969075">
where BLEU strongly disagrees with human judg-
ments of translation adequacy.
Such observations have generated a recent surge
of work on developing MT evaluation metrics that
would outperform BLEU in correlation with hu-
man adequacy judgment (HAJ). Like MEANT, the
TINE automatic recall-oriented evaluation metric
(Rios et al., 2011) aims to preserve basic event
structure. However, its correlation with human ad-
equacy judgment is comparable to that of BLEU
and not as high as that of METEOR. Owczarzak
et al. (2007a,b) improved correlation with human
fluency judgments by using LFG to extend the ap-
proach of evaluating syntactic dependency struc-
ture similarity proposed by Liu and Gildea (2005),
but did not achieve higher correlation with hu-
man adequacy judgments than metrics like ME-
TEOR. Another automatic metric, ULC (Giménez
and Màrquez, 2007, 2008), incorporates several
semantic similarity features and shows improved
correlation with human judgement of translation
quality (Callison-Burch et al., 2007; Giménez
and Màrquez, 2007; Callison-Burch et al., 2008;
Giménez and Màrquez, 2008) but no work has
been done towards tuning an SMT system using
a pure form of ULC perhaps due to its expensive
run time. Likewise, SPEDE (Wang and Manning,
2012) predicts the edit sequence needed to match
the machine translation to the reference translation
via an integrated probabilistic FSM and probabilis-
tic PDA model. The semantic textual similarity
metric Sagan (Castillo and Estrella, 2012) is based
on a complex textual entailment pipeline. These
aggregated metrics require sophisticated feature
extraction steps, contain many parameters that
need to be tuned, and employ expensive linguis-
tic resources such as WordNet or paraphrase tables.
The expensive training, tuning and/or running time
renders these metrics difficult to use in the SMT
training cycle.
</bodyText>
<sectionHeader confidence="0.999689" genericHeader="method">
3 IMEANT
</sectionHeader>
<bodyText confidence="0.999851666666667">
In this section we give a contrastive description
of IMEANT: we first summarize the MEANT ap-
proach, and then explain how IMEANT differs.
</bodyText>
<subsectionHeader confidence="0.999823">
3.1 Variants of MEANT
</subsectionHeader>
<bodyText confidence="0.998508857142857">
MEANT and its variants (Lo et al., 2012) measure
weighted f-scores over corresponding semantic
frames and role fillers in the reference and machine
translations. The automatic versions of MEANT
replace humans with automatic SRL and align-
ment algorithms. MEANT typically outperforms
BLEU, NIST, METEOR, WER, CDER and TER
in correlation with human adequacy judgment, and
is relatively easy to port to other languages, re-
quiring only an automatic semantic parser and a
monolingual corpus of the output language, which
is used to gauge lexical similarity between the se-
mantic role fillers of the reference and translation.
MEANT is computed as follows:
</bodyText>
<listItem confidence="0.995017">
1. Apply an automatic shallow semantic parser
to both the reference and machine transla-
tions. (Figure 1 shows examples of auto-
matic shallow semantic parses on both refer-
ence and MT.)
2. Apply the maximum weighted bipartite
matching algorithm to align the semantic
frames between the reference and machine
translations according to the lexical similari-
ties of the predicates. (Lo and Wu (2013a)
proposed a backoff algorithm that evaluates
the entire sentence of the MT output using the
lexical similarity based on the context vector
model, if the automatic shallow semantic
parser fails to parse the reference or machine
translations.)
3. For each pair of the aligned frames, apply the
maximum weighted bipartite matching algo-
rithm to align the arguments between the ref-
erence and MT output according to the lexical
similarity of role fillers.
4. Compute the weighted f-score over the
matching role labels of these aligned predi-
cates and role fillers according to the follow-
ing definitions:
</listItem>
<construct confidence="0.876849833333333">
q�i,j ≡ ARG j of aligned frame i in MT
qi,j ≡ ARG j of aligned frame i in REF
#tokens filled in aligned frame i of MT
total #tokens in MT
#tokens filled in aligned frame i of REF
total #tokens in REF
</construct>
<bodyText confidence="0.971253333333333">
wpred ≡ weight of similarity of predicates
wj ≡ weight of similarity of ARG j
ei,pred ≡ the pred string of the aligned frame i of MT
fi,pred ≡ the pred string of the aligned frame i of REF
ei,j ≡ the role fillers of ARG j of the aligned frame i of MT
fi,j ≡ the role fillers of ARG j of the aligned frame i of REF
</bodyText>
<equation confidence="0.85900925">
s(e, f) = lexical similarity of token e and f
w� i ≡
wi≡
1
</equation>
<page confidence="0.937163">
24
</page>
<figure confidence="0.980709666666667">
[IN] 4i k(; , it r&apos;W l)JA fkt T IFC Pq T )q M SK-II �� !&amp;quot;Ph �k fflt .
ARG0 PRED ARGM-LOC ARGM-TMP ARG1 ARGM-TMP PRED
[REF] Until after their sales had ceased in mainland China for almost two months , sales of the complete range of SK – II products have now been resumed .
ARG1
ARGM-TMP ARG0 PRED ARG0 PRED ARG1
[MT1] So far , nearly two months sk - ii the sale of products in the mainland of China to resume sales .
ARGM-TMP PRED PRED ARG1 ARG1 PRED
[MT2] So far , in the mainland of China to stop selling nearly two months of SK - 2 products sales resumed .
[MT3] So far , the sale in the mainland of China for nearly two months of SK - II line of products .
</figure>
<figureCaption confidence="0.836415">
Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations
are parsed using automatic English SRL. There are no semantic frames for MT3 since there is no predicate
in the MT output.
</figureCaption>
<equation confidence="0.9825154">
∑
eEe max s(e, f )
f Ef
 |e |
∑
fEf max s(e, f)
eEe
rece,f =  |f |
2 · precei,pred,fi,pred · recei,pred,fi,pred
precei,pred,fi,pred + recei,pred,fi,pred
2 · precei,j,fi,j ·recei,j ,fi,j
precei,j ,fi,j + recei,j,fi,j
precision = ∑0 wpredsi,pred+∑ j w j si,j
i wi wpred+∑ j wj |q?,j |
∑i w0 i
</equation>
<bodyText confidence="0.999984130434783">
where q9i,j and q�i,jare the argument of type j in
frame i in MT and REF respectively. w9 i and wz are
the weights for frame i in MT/REF respectively.
These weights estimate the degree of contribution
of each frame to the overall meaning of the sen-
tence. wpred and wj are the weights of the lexical
similarities of the predicates and role fillers of the
arguments of type j of all frame between the ref-
erence translations and the MT output.There is a
total of 12 weights for the set of semantic role la-
bels in MEANT as defined in Lo and Wu (2011b).
For MEANT, they are determined using super-
vised estimation via a simple grid search to opti-
mize the correlation with human adequacy judg-
ments (Lo and Wu, 2011a). For UMEANT (Lo and
Wu, 2012), they are estimated in an unsupervised
manner using relative frequency of each semantic
role label in the references and thus UMEANT is
useful when human judgments on adequacy of the
development set are unavailable.
si,pred and si,j are the lexical similarities based
on a context vector model of the predicates and role
fillers of the arguments of type j between the ref-
erence translations and the MT output. Lo et al.
(2012) and Tumuluru et al. (2012) described how
the lexical and phrasal similarities of the semantic
role fillers are computed. A subsequent variant of
the aggregation function inspired by Mihalcea et
al. (2006) that normalizes phrasal similarities ac-
cording to the phrase length more accurately was
used in more recent work (Lo et al., 2013a; Lo and
Wu, 2013a; Lo et al., 2013b). In this paper, we
will assess IMEANT against the latest version of
MEANT (Lo et al., 2014) which, as shown, uses
f-score to aggregate individual token similarities
into the composite phrasal similarities of semantic
role fillers,since this has been shown to be more ac-
curate than the previously used aggregation func-
tions.
Recent studies (Lo et al., 2013a; Lo and Wu,
2013a; Lo et al., 2013b) show that tuning MT sys-
tems against MEANT produces more robustly ad-
equate translations than the common practice of
tuning against BLEU or TER across different data
genres, such as formal newswire text, informal
web forum text and informal public speech.
</bodyText>
<equation confidence="0.9968853">
∑1 wpredsi,pred+∑ j w j si,j
i wi wpree�d+∑ j wj  |qs,j |
Lei w1i
2 · precision · recall
MEANT =
precision + recall
recall =
prece,f =
si,pred =
si,j =
</equation>
<page confidence="0.966203">
25
</page>
<bodyText confidence="0.999669444444444">
In an alternative quality-estimation oriented line
of research, Lo et al. (2014) describe a cross-
lingual variant called XMEANT capable of eval-
uating translation quality without the need for ex-
pensive human reference translations, by utiliz-
ing semantic parses of the original foreign in-
put sentence instead of a reference translation.
Since XMEANT’s results could have been due
to either (1) more accurate evaluation of phrasal
similarity via cross-lingual translation probabili-
ties, or (2) better match of semantic frames with-
out reference translations, there is no direct evi-
dence whether ITGs contribute to the improvement
in MEANT’s correlation with human adequacy
judgment. For the sake of better understanding
whether ITGs improve semantic MT evaluation,
we will also assess IMEANT against cross-lingual
XMEANT.
</bodyText>
<subsectionHeader confidence="0.977279">
3.2 The IMEANT metric
</subsectionHeader>
<bodyText confidence="0.999978156862745">
Although MEANT was previously shown to pro-
duce higher correlation with human adequacy
judgments compared to other automatic metrics,
our error analyses suggest that it still suffers from a
common weakness among metrics employing lex-
ical similarity, namely that word/token alignments
between the reference and machine translations
are severely under constrained. No bijectivity or
permutation restrictions are applied, even between
compositional segments where this should be nat-
ural. This can cause role fillers to be aligned even
when they should not be. IMEANT, in contrast,
uses a bracketing inversion transduction grammar
to constrain permissible token alignment patterns
between aligned role filler phrases. The semantic
frames above the token level also fits ITG com-
positional structure, consistent with the aforemen-
tioned semantic frame alternation coverage study
of Addanki et al. (2012). Figure 2 illustrates how
the ITG constraints are consistent with the needed
permutations between semantic role fillers across
the reference and machine translations for a sam-
ple sentence from our evaluation data, which as
we will see leads to higher HAJ correlations than
MEANT.
Subject to the structural ITG constraints,
IMEANT scores sentence translations in a spirit
similar to the way MEANT scores them: it utilizes
an aggregated score over the matched semantic
role labels of the automatically aligned semantic
frames and their role fillers between the reference
and machine translations. Despite the structural
differences, like MEANT, at the conceptual level
IMEANT still aims to evaluate MT output in
terms of the degree to which the translation has
preserved the essential “who did what to whom,for
whom, when, where, how and why” of the foreign
input sentence.
Unlike MEANT, however, IMEANT aligns and
scores under ITG assumptions. MEANT uses a
maximum alignment algorithm to align the tokens
in the role fillers between the reference and ma-
chine translations, and then scores by aggregating
the lexical similarities into a phrasal similarity us-
ing an f-measure. In contrast, IMEANT aligns and
scores by utilizing a length-normalized weighted
BITG (Wu, 1997; Zens and Ney, 2003; Saers and
Wu, 2009; Addanki et al., 2012). To be precise in
this regard, we can see IMEANT as differing from
the foregoing description of MEANT in the defi-
nition of si,pred and si,� , as follows.
</bodyText>
<equation confidence="0.9857287">
G ({A} , W°, W1, R, A)
R {A --+ [AA] , A --+ (AA), A --+ e/f}
p ([AA] |A) = p ((AA)|A) = 1
p (e/f|A) = s(e, f) l l
lg (P (A � ei,pred/fi,pred|G//
si,pred = lg−1  
max( |ei,pred|,|
red|
)
fi,
</equation>
<bodyText confidence="0.979700411764706">
the null token) and f E Wi
denoting a token
in the reference translation (or the null token). The
rule probability (or more accurately, rule weight)
function p is set to be 1 for structural transduction
rules, and for lexical transduction rules it is de-
fined using
context vector model based
lexical similarity measure. To calculate the inside
probability (or more accurately, inside score) of a
, we use the al-
gorithm described in Saers et al. (2009). Given
this, si,pred and
now represent the length nor-
malized BITG parse scores of the predicates and
role fillers of the arguments of type j between the
reference and machine tran
</bodyText>
<equation confidence="0.9322140625">
U{E}
MEANT’s
( )
pair of segments, P A � � e/f|G
si,�
slations.
1 (lg (P (A � ei,7 /fi,7 |G))
s
i,7
=
lg
max(|
ei,7
|,|
fi,7
|)
</equation>
<bodyText confidence="0.999364333333333">
where G is a bracketing ITG whose only non ter-
minal is A, and R is a set of transduction rules with
e E WEU{E} denoting a token in the MT output (or
</bodyText>
<sectionHeader confidence="0.999857" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.979970333333333">
In this section we discuss experiments indicating
that IMEANT further improves upon
MEANT’s
</bodyText>
<page confidence="0.982669">
26
</page>
<table confidence="0.947491588235294">
[MT2] The level of reduction is conducive to raising the inspection and supervision work efficiency .
ARG0 PRED ARG1
[REF] The reduction in hierarchy helps raise the efficiency of inspection and supervisory work .
ARG0 PRED ARG1
The work
reduction .
in
hierarchy
helps
raise
the
efficiency
of
inspection
and
supervisory
ARG0 pred ARG1
</table>
<figureCaption confidence="0.9544546">
Figure 2: An example of aligning automatic shallow semantic parses under ITGs, visualized using both
biparse tree and alignment matrix depictions, for the Chinese input sentence M级的减少有利于提⾼检查
监督⼯作的效率。. Both the reference and machine translations are parsed using automatic English SRL.
Compositional alignments between the semantic frames and the tokens within role filler phrases obey
inversion transduction grammars.
</figureCaption>
<figure confidence="0.907500333333333">
ARG0
pred
ARG1
</figure>
<bodyText confidence="0.907885588235295">
The
level
of
reduction
is
conducive
to
raising
the
inspection
and
supervision
work
efficiency
.
already-high correlation with human adequacy
judgments.
</bodyText>
<subsectionHeader confidence="0.975035">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999905133333333">
We perform the meta-evaluation upon two differ-
ent partitions of the DARPA GALE P2.5 Chinese-
English translation test set. The corpus includes
the Chinese input sentences, each accompanied by
one English reference translation and three partic-
ipating state-of-the-art MT systems’ output.
For the sake of consistent comparison, the first
evaluation partition, GALE-A, is the same as the
one used in Lo and Wu (2011a), and the second
evaluation partition, GALE-B, is the same as the
one used in Lo and Wu (2011b).
For both reference and machine translations, the
ASSERT (Pradhan et al., 2004) semantic role la-
beler was used to automatically predict semantic
parses.
</bodyText>
<page confidence="0.999471">
27
</page>
<tableCaption confidence="0.986805">
Table 1: Sentence-level correlation with human
</tableCaption>
<bodyText confidence="0.954408888888889">
adequacy judgements on different partitions of
GALE P2.5 data. IMEANT always yields top
correlations, and is more consistent than either
MEANT or its recent cross-lingual XMEANT
quality estimation variant. For reference, the hu-
man HMEANT upper bound is 0.53 for GALE-A
and 0.37 for GALE-B—thus, the fully automated
IMEANT approximation is not far from closing the
gap.
</bodyText>
<table confidence="0.998332083333333">
metric GALE-A GALE-B
IMEANT 0.51 0.33
XMEANT 0.51 0.20
MEANT 0.48 0.33
METEOR 1.5 (2014) 0.43 0.10
NIST 0.29 0.16
METEOR 0.4.3 (2005) 0.20 0.29
BLEU 0.20 0.27
TER 0.20 0.19
PER 0.20 0.18
CDER 0.12 0.16
WER 0.10 0.26
</table>
<sectionHeader confidence="0.727713" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999983825396826">
The sentence-level correlations in Table 1 show
that IMEANT outperforms other automatic met-
rics in correlation with human adequacy judgment.
Note that this was achieved with no tuning what-
soever of the default rule weights (suggesting that
the performance of IMEANT could be further im-
proved in the future by slightly optimizing the ITG
weights).
On the GALE-A partition, IMEANT shows 3
points improvement over MEANT, and is tied
with the cross-lingual XMEANT quality estimator
discussed earlier.IMEANT produces much higher
HAJ correlations than any of the other metrics.
On the GALE-B partition, IMEANT is tied with
MEANT, and is significantly better correlated with
HAJ than the XMEANT quality estimator. Again,
IMEANT produces much higher HAJ correlations
than any of the other metrics.
We note that we have also observed this pattern
consistently in smaller-scale experiments—while
the monolingual MEANT metric and its cross-
lingual XMEANT cousin vie with each other on
different data sets, IMEANT robustly and consis-
tently produces top HAJ correlations.
In both the GALE-A and GALE-B partitions,
IMEANT comes within a few points of the human
upper bound benchmark HAJ correlations com-
puted using the human labeled semantic frames
and alignments used in the HMEANT.
Data analysis reveals two reasons that IMEANT
correlates with human adequacy judgement more
closely than MEANT. First, BITG constraints in-
deed provide more accurate phrasal similarity ag-
gregation, compared to the naive bag-of-words
based heuristics employed in MEANT. Similar re-
sults have been observed while trying to estimate
word alignment probabilities where BITG con-
straints outperformed alignments from GIZA++
(Saers and Wu, 2009).
Secondly, the permutation and bijectivity con-
straints enforced by the ITG provide better lever-
age to reject token alignments when they are not
appropriate, compared with the maximal align-
ment approach which tends to be rather promiscu-
ous. A case of this can be seen in Figure 3, which
shows the result on the same example sentence as
in Figure 1. Disregarding the semantic parsing er-
rors arising from the current limitations of auto-
matic SRL tools, the ITG tends to provide clean,
sparse alignments for role fillers like the ARG1
of the resumed PRED, preferring to leave tokens
like complete and range unaligned instead of aligning
them anyway as MEANT’s maximal alignment al-
gorithm tends to do. Note that it is not simply a
matter of lowering thresholds for accepting token
alignments: Tumuluru et al. (2012) showed that
the competitive linking approach (Melamed, 1996)
which also generally produces sparser alignments
does not work as well in MEANT, whereas the ITG
appears to be selective about the token alignments
in a manner that better fits the semantic structure.
For contrast, Figure 4 shows a case where
IMEANT appropriately accepts dense alignments.
</bodyText>
<sectionHeader confidence="0.995183" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999962916666667">
We have presented IMEANT, an inversion trans-
duction grammar based rethinking of the MEANT
semantic frame based MT evaluation approach,
that achieves higher correlation with human ad-
equacy judgments of MT output quality than
MEANT and its variants, as well as other com-
mon evaluation metrics. Our results improve upon
previous research showing that MEANT’s explicit
use of semantic frames leads to state-of-the-art au-
tomatic MT evaluation. IMEANT achieves this
by aligning and scoring semantic frames under a
simple, consistent ITG that provides empirically
</bodyText>
<page confidence="0.996914">
28
</page>
<figure confidence="0.967691254901961">
ARGM-TMP PRED PRED ARG1 ARG1 PRED
[MT2] So far , in the mainland of China to stop selling nearly two months of SK - 2 products sales resumed .
[REF] Until after their sales had ceased in mainland China for almost two months , sales of the complete range of SK – II products have now been resumed .
ARG0 PRED ARGM-LOC ARGM-TMP ARG1 ARGM-TMP PRED
PRED
PRED
ARG1
ARG1
PRED
So
far
,
in
the
mainland
of
China
to
stop
selling
nearly
two
months
of
SK
-
2
products
sales
resumed
.
ARGM-TMP
Until months sales range
after , of of
their the SK
sale complete -
had II
ceased products
in have
mainland now
China been
for resumed
almost .
two
ARG0
PRED
ARG1
ARGM-TMP
ARGM-TMP
ARGM-LOC
PRED
</figure>
<figureCaption confidence="0.69347625">
Figure 3: An example where the ITG helps produce correctly sparse alignments by rejecting inappro-
priate token alignments in the ARG1 of the resumed PRED, instead of wrongly aligning tokens like the,
complete, and range as MEANT tends to do. (The semantic parse errors are due to limitations of automatic
SRL.)
</figureCaption>
<bodyText confidence="0.998282125">
informative permutation and bijectivity biases, in-
stead of the maximal alignment and bag-of-words
assumptions used by MEANT. At the same time,
IMEANT retains the Occam’s Razor style simplic-
ity and representational transparency characteris-
tics of MEANT.
Given the absence of any tuning of ITG weights
in this first version of IMEANT, we speculate that
</bodyText>
<page confidence="0.995843">
29
</page>
<figure confidence="0.998674102040816">
[MT2] Australian Prime Minister John Howard said that the Government might cancel the AWB company wheat monopoly next week .
[REF] Australian Prime Minister Howard said the government could cancel AWB &apos;s monopoly in the wheat business next week .
ARG0 PRED ARGM-MOD PRED ARG1 ARGM-TMP
ARG1
ARG1
ARG0
ARGM-MOD PRED ARG1 ARGM-TMP
ARG0
PRED
ARG0
Australian
Prime
Minister
John
Howard
said
that
the
Government
might
cancel
the
AWB
company
wheat
monopoly
next
week
.
Australian in the wheat
Prime business
Minister next
Howard week
said .
the
government
could
cancel
AWB
&apos;s
monopoly
ARG0
pred
ARG0
ARGM-MOD
pred
ARG1
ARG1
ARGM-TMP
</figure>
<figureCaption confidence="0.99989">
Figure 4: An example of dense alignments in IMEANT, for the Chinese input sentence��11L���
</figureCaption>
<bodyText confidence="0.980598">
���Y,C�7��� F1�Za AWB (The semantic parse errors are due to limitations
of automatic SRL.)
IMEANT could perform even better than it already rameter optimizations in the near future.
does here.We plan to investigate simple hyperpa-
</bodyText>
<figure confidence="0.995994">
ARG0
pred
ARG1
pred
ARG0
ARGM-TMP
ARGM-MOD
ARG1
</figure>
<page confidence="0.983828">
30
</page>
<sectionHeader confidence="0.997953" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999934222222222">
This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT contract
nos. HR0011-12-C-0014 and HR0011-12-C-0016,
and GALE contract nos. HR0011-06-C-0022 and
HR0011-06-C-0023; by the European Union un-
der the FP7 grant agreement no. 287658; and by
the Hong Kong Research Grants Council (RGC)
research grants GRF620811, GRF621008, and
GRF612806. Any opinions, findings and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessar-
ily reflect the views of DARPA, the EU, or RGC.
Thanks to Karteek Addanki for supporting work,
and to Pascale Fung, Yongsheng Yang and Zhao-
jun Wu for sharing the maximum entropy Chinese
segmenter and C-ASSERT, the Chinese semantic
parser.
</bodyText>
<sectionHeader confidence="0.998566" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.958642875">
Karteek Addanki, Chi-kiu Lo, Markus Saers, and
Dekai Wu. LTG vs. ITG coverage of cross-
lingual verb frame alternations. In 16th An-
nual Conference of the European Association
for Machine Translation (EAMT-2012), Trento,
Italy, May 2012.
Alfred V. Aho and Jeffrey D. Ullman. The The-
ory of Parsing, Translation, and Compiling.
Prentice-Halll, Englewood Cliffs, New Jersey,
1972.
Satanjeev Banerjee and Alon Lavie. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In
Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Sum-
marization, Ann Arbor, Michigan, June 2005.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in ma-
chine translation research. In 11th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL-2006), 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
(meta-) evaluation of machine translation. In
Second Workshop on Statistical Machine Trans-
lation (WMT-07), 2007.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
Further meta-evaluation of machine transla-
tion. In Third Workshop on Statistical Machine
Translation (WMT-08), 2008.
Julio Castillo and Paula Estrella. Semantic tex-
tual similarity for MT evaluation. In 7th Work-
shop on Statistical Machine Translation (WMT
2012), 2012.
Michael Denkowski and Alon Lavie. METEOR
universal: Language specific translation eval-
uation for any target language. In 9th Work-
shop on Statistical Machine Translation (WMT
2014), 2014.
</reference>
<bodyText confidence="0.792479972222222">
George Doddington. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In The second interna-
tional conference on Human Language Technol-
ogy Research (HLT ’02), San Diego, California,
2002.
Jesús Giménez and Lluís Màrquez. Linguistic fea-
tures for automatic evaluation of heterogenous
MT systems. In Second Workshop on Statisti-
cal Machine Translation (WMT-07), pages 256–
264, Prague, Czech Republic, June 2007.
Jesús Giménez and Lluís Màrquez. A smorgas-
bord of features for automatic MT evaluation. In
Third Workshop on Statistical Machine Transla-
tion (WMT-08), Columbus, Ohio, June 2008.
Philipp Koehn and Christof Monz. Manual and
automatic evaluation of machine translation be-
tween european languages. In Workshop on Sta-
tistical Machine Translation (WMT-06), 2006.
Gregor Leusch and Hermann Ney. Bleusp, invwer,
cder: Three improved mt evaluation measures.
In NIST Metrics for Machine Translation Chal-
lenge (MetricsMATR), at Eighth Conference of
the Association for Machine Translation in the
Americas (AMTA 2008), Waikiki, Hawaii, Oct
2008.
Gregor Leusch, Nicola Ueffing, and Hermann
Ney. A novel string-to-string distance measure
with applications to machine translation evalu-
ation. In Machine Translation Summit IY (MT
Summit IY), New Orleans, Sep 2003.
Gregor Leusch, Nicola Ueffing, and Hermann
Ney. CDer: Efficient MT evaluation using
block movements. In 11th Conference of the
European Chapter of the Association for Com-
putational Linguistics (EACL-2006), 2006.
</bodyText>
<page confidence="0.999658">
31
</page>
<reference confidence="0.98301234375">
Ding Liu and Daniel Gildea. Syntactic features for
evaluation of machine translation. In Workshop
on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization,
Ann Arbor, Michigan, June 2005.
Chi-kiu Lo and Dekai Wu. MEANT: An inexpen-
sive, high-accuracy, semi-automatic metric for
evaluating translation utility based on seman-
tic roles. In 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human
Language Technologies (ACL HLT 2011), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux:
How semantic frames evaluate MT more ac-
curately. In Twenty-second International Joint
Conference on Artificial Intelligence (IJCAI-
11), 2011.
Chi-kiu Lo and Dekai Wu. Unsupervised vs. super-
vised weight estimation for semantic MT evalu-
ation metrics. In Sixth Workshop on Syntax, Se-
mantics and Structure in Statistical Translation
(SSST-6), 2012.
Chi-kiu Lo and Dekai Wu. Can informal genres
be better translated by tuning on automatic se-
mantic metrics? In 14th Machine Translation
Summit (MT Summit XIV), 2013.
Chi-kiu Lo and Dekai Wu. MEANT at WMT
2013: A tunable, accurate yet inexpensive se-
mantic frame based mt evaluation metric. In
8th Workshop on Statistical Machine Transla-
tion (WMT 2013), 2013.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai
Wu. Fully automatic semantic MT evaluation.
In 7th Workshop on Statistical Machine Trans-
lation (WMT 2012), 2012.
Chi-kiu Lo, Karteek Addanki, Markus Saers, and
Dekai Wu. Improving machine translation by
training against an automatic semantic frame
based evaluation metric. In 51st Annual Meet-
ing of the Association for Computational Lin-
guistics (ACL 2013), 2013.
Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Im-
proving machine translation into Chinese by
tuning against Chinese MEANT. In Interna-
tional Workshop on Spoken Language Transla-
tion (IWSLT 2013), 2013.
Chi-kiu Lo, Meriem Beloucif, Markus Saers, and
Dekai Wu. XMEANT: Better semantic MT
evaluation without reference translations. In
52nd Annual Meeting of the Association for
Computational Linguistics (ACL 2014), 2014.
Matouš Macháček and Ondřej Bojar. Results of the
WMT13 metrics shared task. In Eighth Work-
shop on Statistical Machine Translation (WMT
2013), Sofia, Bulgaria, August 2013.
I. Dan Melamed. Automatic construction of
clean broad-coverage translation lexicons. In
2nd Conference of the Association for Ma-
chine Translation in the Americas (AMTA-
1996), 1996.
Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. Corpus-based and knowledge-based
measures of text semantic similarity. In The
Twenty-first National Conference on Artificial
Intelligence (AAAI-06), volume 21, 2006.
Sonja Nießen, Franz Josef Och, Gregor Leusch,
and Hermann Ney. A evaluation tool for ma-
chine translation: Fast evaluation for MT re-
search. In The Second International Conference
on Language Resources and Evaluation (LREC
2000), 2000.
Karolina Owczarzak, Josef van Genabith, and
Andy Way. Dependency-based automatic eval-
uation for machine translation. In Syntax
and Structure in Statistical Translation (SSST),
2007.
Karolina Owczarzak, Josef van Genabith, and
Andy Way. Evaluating machine translation
with LFG dependencies. Machine Translation,
21:95–119, 2007.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In 40th An-
nual Meeting of the Association for Compu-
tational Linguistics (ACL-02), pages 311–318,
Philadelphia, Pennsylvania, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. Shallow se-
mantic parsing using support vector machines.
In Human Language Technology Conference
of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-
NAACL 2004), 2004.
Miguel Rios, Wilker Aziz, and Lucia Specia.
TINE: A metric to assess MT adequacy. In
Sixth Workshop on Statistical Machine Transla-
tion (WMT 2011), 2011.
</reference>
<page confidence="0.984384">
32
</page>
<reference confidence="0.997235510204081">
Markus Saers and Dekai Wu. Improving phrase-
based translation via word alignments from
stochastic inversion transduction grammars. In
Third Workshop on Syntax and Structure in
Statistical Translation (SSST-3), pages 28–36,
Boulder, Colorado, June 2009.
Markus Saers, Joakim Nivre, and Dekai Wu.
Learning stochastic bracketing inversion trans-
duction grammars with a cubic time biparsing
algorithm. In 11th International Conference on
Parsing Technologies (IWPT’09), pages 29–32,
Paris, France, October 2009.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. A study
of translation edit rate with targeted human an-
notation. In 7th Biennial Conference Asso-
ciation for Machine Translation in the Ameri-
cas (AMTA 2006), pages 223–231, Cambridge,
Massachusetts, August 2006.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai
Wu. Accuracy and robustness in measuring the
lexical similarity of semantic role fillers for au-
tomatic semantic MT evaluation. In 26th Pa-
cific Asia Conference on Language, Informa-
tion, and Computation (PACLIC 26), 2012.
Mengqiu Wang and Christopher D. Manning.
SPEDE: Probabilistic edit distance metrics for
MT evaluation. In 7th Workshop on Statistical
Machine Translation (WMT 2012), 2012.
Dekai Wu. An algorithm for simultaneously brack-
eting parallel texts by aligning words. In 33rd
Annual Meeting of the Association for Compu-
tational Linguistics (ACL 95), pages 244–251,
Cambridge, Massachusetts, June 1995.
Dekai Wu. Trainable coarse bilingual grammars
for parallel text bracketing. In Third Annual
Workshop on Very Large Corpora (WVLC-3),
pages 69–81, Cambridge, Massachusetts, June
1995.
Dekai Wu. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377–
403, 1997.
Richard Zens and Hermann Ney. A compara-
tive study on reordering constraints in statisti-
cal machine translation. In 41st Annual Meeting
of the Association for Computational Linguis-
tics (ACL-2003), pages 144–151, Stroudsburg,
Pennsylvania, 2003.
</reference>
<page confidence="0.999367">
33
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.595502">
<title confidence="0.953574333333333">Better Semantic Frame Based MT via Inversion Transduction Grammars Dekai Wu Lo Chi-kiu Meriem BELouCIF Markus</title>
<author confidence="0.892067">Human Language Technology</author>
<affiliation confidence="0.997038">Department of Computer Science and</affiliation>
<author confidence="0.77618">Hong Kong University of Science</author>
<author confidence="0.77618">Technology</author>
<email confidence="0.984812">{jackielo|mbeloucif|masaers|dekai}@cs.ust.hk</email>
<abstract confidence="0.997921931034483">We introduce an inversion transduction grammar based restructuring of the MEANT automatic semantic frame based MT evaluation metric, which, by leveraging ITG language biases, is able to further improve upon MEANT’s already-high correlation with human adequacy judgments. The new metric, called IMEANT, uses bracketing ITGs to biparse the reference and machine translations, but subject to obeying the semantic frames in both. Resulting improvements support the presumption that ITGs, which constrain the allowable permutations between compositional segments across the reference and MT output, score the phrasal similarity of the semantic role fillers more accurately than the simple word alignment heuristics (bag-of-word alignment or maximum alignment) used in previous version of MEANT. The approach successfully integrates (1) the previously demonstrated extremely high coverage of cross-lingual semantic frame alternations by ITGs, with (2) the high accuracy of evaluating MT via weighted f-scores on the degree of semantic frame preservation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Karteek Addanki</author>
<author>Chi-kiu Lo</author>
<author>Markus Saers</author>
<author>Dekai Wu</author>
</authors>
<title>LTG vs. ITG coverage of crosslingual verb frame alternations.</title>
<date>2012</date>
<booktitle>In 16th Annual Conference of the European Association for Machine Translation (EAMT-2012),</booktitle>
<location>Trento, Italy,</location>
<contexts>
<context position="3747" citStr="Addanki et al., 2012" startWordPosition="565" endWordPosition="568">s. ITGs are guaranteed to have a two-normal form similar to context-free grammars, and can be biparsed in polynomial time and space (O (n�) time and O (n4) space). It is also possible to do approximate biparsing in O (n�) time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This makes ITGs an appealing alternative for eval22 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics uating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful co</context>
<context position="18011" citStr="Addanki et al. (2012)" startWordPosition="2945" endWordPosition="2948">ignments between the reference and machine translations are severely under constrained. No bijectivity or permutation restrictions are applied, even between compositional segments where this should be natural. This can cause role fillers to be aligned even when they should not be. IMEANT, in contrast, uses a bracketing inversion transduction grammar to constrain permissible token alignment patterns between aligned role filler phrases. The semantic frames above the token level also fits ITG compositional structure, consistent with the aforementioned semantic frame alternation coverage study of Addanki et al. (2012). Figure 2 illustrates how the ITG constraints are consistent with the needed permutations between semantic role fillers across the reference and machine translations for a sample sentence from our evaluation data, which as we will see leads to higher HAJ correlations than MEANT. Subject to the structural ITG constraints, IMEANT scores sentence translations in a spirit similar to the way MEANT scores them: it utilizes an aggregated score over the matched semantic role labels of the automatically aligned semantic frames and their role fillers between the reference and machine translations. Desp</context>
<context position="19343" citStr="Addanki et al., 2012" startWordPosition="3155" endWordPosition="3158">terms of the degree to which the translation has preserved the essential “who did what to whom,for whom, when, where, how and why” of the foreign input sentence. Unlike MEANT, however, IMEANT aligns and scores under ITG assumptions. MEANT uses a maximum alignment algorithm to align the tokens in the role fillers between the reference and machine translations, and then scores by aggregating the lexical similarities into a phrasal similarity using an f-measure. In contrast, IMEANT aligns and scores by utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). To be precise in this regard, we can see IMEANT as differing from the foregoing description of MEANT in the definition of si,pred and si,� , as follows. G ({A} , W°, W1, R, A) R {A --+ [AA] , A --+ (AA), A --+ e/f} p ([AA] |A) = p ((AA)|A) = 1 p (e/f|A) = s(e, f) l l lg (P (A � ei,pred/fi,pred|G// si,pred = lg−1   max( |ei,pred|,| red| ) fi, the null token) and f E Wi denoting a token in the reference translation (or the null token). The rule probability (or more accurately, rule weight) function p is set to be 1 for structural transduction rules, and for lexical transduction rules it is </context>
</contexts>
<marker>Addanki, Lo, Saers, Wu, 2012</marker>
<rawString>Karteek Addanki, Chi-kiu Lo, Markus Saers, and Dekai Wu. LTG vs. ITG coverage of crosslingual verb frame alternations. In 16th Annual Conference of the European Association for Machine Translation (EAMT-2012), Trento, Italy, May 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling. Prentice-Halll,</booktitle>
<location>Englewood Cliffs, New Jersey,</location>
<contexts>
<context position="2820" citStr="Aho and Ullman, 1972" startWordPosition="417" endWordPosition="420">m: a stochastic grammar predicts how frequently any particular realization of the abstract concept will be generated. The bilingual analogy is a transduction grammar generating a pair of possible realizations of the same underlying concept. Stochastic transduction grammars predict how frequently a particular pair of realizations will be generated, and thus represent a good way to evaluate how well a pair of sentences correspond to each other. The particular class of transduction grammars known as ITGs tackle the problem that the (bi)parsing complexity for general syntaxdirected transductions (Aho and Ullman, 1972) is exponential. By constraining a syntax-directed transduction grammar to allow only monotonic straight and inverted reorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar to context-free grammars, and can be biparsed in polynomial time and space (O (n�) time and O (n4) space). It is also possible to do approximate biparsing in O (n�) time (Saers et al., 2009). These polynomial complexities makes it feasib</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. The Theory of Parsing, Translation, and Compiling. Prentice-Halll, Englewood Cliffs, New Jersey, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="5944" citStr="Banerjee and Lavie, 2005" startWordPosition="921" endWordPosition="924">who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semant</context>
<context position="8209" citStr="Banerjee and Lavie, 2005" startWordPosition="1295" endWordPosition="1298">it semantic modeling in invWER. Providing they meet the BITG constraints, the biparse trees in invWER are completely unconstrained. In contrast, IMEANT employs the same explicit, strong semantic frame modeling as MEANT, on both the reference and machine translations. In IMEANT, the semantic frames always take precedence over pure BITG biases. Compared to invWER, this strongly constrains the space of biparses that IMEANT permits to be considered. 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) reporting cases 23 where BLEU strongly disagrees with human judgments of translation adequacy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-or</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Ann Arbor, Michigan, June 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>In 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006),</booktitle>
<contexts>
<context position="8480" citStr="Callison-Burch et al., 2006" startWordPosition="1338" endWordPosition="1341">In IMEANT, the semantic frames always take precedence over pure BITG biases. Compared to invWER, this strongly constrains the space of biparses that IMEANT permits to be considered. 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) reporting cases 23 where BLEU strongly disagrees with human judgments of translation adequacy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure. However, its correlation with human adequacy judgment is comparable to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency </context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of BLEU in machine translation research. In 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Second Workshop on Statistical Machine Translation (WMT-07),</booktitle>
<contexts>
<context position="9530" citStr="Callison-Burch et al., 2007" startWordPosition="1497" endWordPosition="1500">its correlation with human adequacy judgment is comparable to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. Another automatic metric, ULC (Giménez and Màrquez, 2007, 2008), incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to match the machine translation to the reference translation via an integrated probabilistic FSM and probabilistic PDA model. The semantic textual similarity metric Sagan (Castillo and Estrella, 2012) is based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction st</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. (meta-) evaluation of machine translation. In Second Workshop on Statistical Machine Translation (WMT-07), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Third Workshop on Statistical Machine Translation (WMT-08),</booktitle>
<contexts>
<context position="9586" citStr="Callison-Burch et al., 2008" startWordPosition="1505" endWordPosition="1508">le to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. Another automatic metric, ULC (Giménez and Màrquez, 2007, 2008), incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to match the machine translation to the reference translation via an integrated probabilistic FSM and probabilistic PDA model. The semantic textual similarity metric Sagan (Castillo and Estrella, 2012) is based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain many parameters that need to be tuned, and </context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. Further meta-evaluation of machine translation. In Third Workshop on Statistical Machine Translation (WMT-08), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio Castillo</author>
<author>Paula Estrella</author>
</authors>
<title>Semantic textual similarity for MT evaluation.</title>
<date>2012</date>
<booktitle>In 7th Workshop on Statistical Machine Translation (WMT</booktitle>
<contexts>
<context position="10010" citStr="Castillo and Estrella, 2012" startWordPosition="1573" endWordPosition="1576">porates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to match the machine translation to the reference translation via an integrated probabilistic FSM and probabilistic PDA model. The semantic textual similarity metric Sagan (Castillo and Estrella, 2012) is based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain many parameters that need to be tuned, and employ expensive linguistic resources such as WordNet or paraphrase tables. The expensive training, tuning and/or running time renders these metrics difficult to use in the SMT training cycle. 3 IMEANT In this section we give a contrastive description of IMEANT: we first summarize the MEANT approach, and then explain how IMEANT differs. 3.1 Variants of MEANT MEANT and its variants (Lo et al., 2012) measure weighted f-sco</context>
</contexts>
<marker>Castillo, Estrella, 2012</marker>
<rawString>Julio Castillo and Paula Estrella. Semantic textual similarity for MT evaluation. In 7th Workshop on Statistical Machine Translation (WMT 2012), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR universal: Language specific translation evaluation for any target language.</title>
<date>2014</date>
<booktitle>In 9th Workshop on Statistical Machine Translation (WMT</booktitle>
<contexts>
<context position="8237" citStr="Denkowski and Lavie, 2014" startWordPosition="1299" endWordPosition="1302">vWER. Providing they meet the BITG constraints, the biparse trees in invWER are completely unconstrained. In contrast, IMEANT employs the same explicit, strong semantic frame modeling as MEANT, on both the reference and machine translations. In IMEANT, the semantic frames always take precedence over pure BITG biases. Compared to invWER, this strongly constrains the space of biparses that IMEANT permits to be considered. 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) reporting cases 23 where BLEU strongly disagrees with human judgments of translation adequacy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Ri</context>
</contexts>
<marker>Denkowski, Lavie, 2014</marker>
<rawString>Michael Denkowski and Alon Lavie. METEOR universal: Language specific translation evaluation for any target language. In 9th Workshop on Statistical Machine Translation (WMT 2014), 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic features for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="9215" citStr="Liu and Gildea (2005)" startWordPosition="1453" endWordPosition="1456">acy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure. However, its correlation with human adequacy judgment is comparable to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. Another automatic metric, ULC (Giménez and Màrquez, 2007, 2008), incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to mat</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. Syntactic features for evaluation of machine translation. In Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Ann Arbor, Michigan, June 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles.</title>
<date>2011</date>
<booktitle>In 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT</booktitle>
<contexts>
<context position="5150" citStr="Lo and Wu, 2011" startWordPosition="789" endWordPosition="792">on grammars, is able to exploit bracketing ITGs (also known as BITGs or BTGs) which are ITGs containing only a single non-differentiated non terminal category (Wu, 1995a), so as to produce even higher correlation with human adequacy judgments than any automatic MEANT variants, or other common automatic metrics. We argue that the constraints provided by BITGs over the semantic frames and arguments of the reference and MT output sentences are essential for accurate evaluation of the phrasal similarity of the semantic role fillers. In common with the various MEANT semantic MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), our proposed IMEANT metric measures the degree to which the basic semantic event structure is preserved by translation—the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEAN</context>
<context position="14480" citStr="Lo and Wu (2011" startWordPosition="2380" endWordPosition="2383">sion = ∑0 wpredsi,pred+∑ j w j si,j i wi wpred+∑ j wj |q?,j | ∑i w0 i where q9i,j and q�i,jare the argument of type j in frame i in MT and REF respectively. w9 i and wz are the weights for frame i in MT/REF respectively. These weights estimate the degree of contribution of each frame to the overall meaning of the sentence. wpred and wj are the weights of the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between the reference translations and the MT output.There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. si,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT out</context>
<context position="22064" citStr="Lo and Wu (2011" startWordPosition="3623" endWordPosition="3626">tion grammars. ARG0 pred ARG1 The level of reduction is conducive to raising the inspection and supervision work efficiency . already-high correlation with human adequacy judgments. 4.1 Experimental setup We perform the meta-evaluation upon two different partitions of the DARPA GALE P2.5 ChineseEnglish translation test set. The corpus includes the Chinese input sentences, each accompanied by one English reference translation and three participating state-of-the-art MT systems’ output. For the sake of consistent comparison, the first evaluation partition, GALE-A, is the same as the one used in Lo and Wu (2011a), and the second evaluation partition, GALE-B, is the same as the one used in Lo and Wu (2011b). For both reference and machine translations, the ASSERT (Pradhan et al., 2004) semantic role labeler was used to automatically predict semantic parses. 27 Table 1: Sentence-level correlation with human adequacy judgements on different partitions of GALE P2.5 data. IMEANT always yields top correlations, and is more consistent than either MEANT or its recent cross-lingual XMEANT quality estimation variant. For reference, the human HMEANT upper bound is 0.53 for GALE-A and 0.37 for GALE-B—thus, the </context>
</contexts>
<marker>Lo, Wu, 2011</marker>
<rawString>Chi-kiu Lo and Dekai Wu. MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles. In 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT 2011), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>SMT vs. AI redux: How semantic frames evaluate MT more accurately.</title>
<date>2011</date>
<booktitle>In Twenty-second International Joint Conference on Artificial Intelligence (IJCAI11),</booktitle>
<contexts>
<context position="5150" citStr="Lo and Wu, 2011" startWordPosition="789" endWordPosition="792">on grammars, is able to exploit bracketing ITGs (also known as BITGs or BTGs) which are ITGs containing only a single non-differentiated non terminal category (Wu, 1995a), so as to produce even higher correlation with human adequacy judgments than any automatic MEANT variants, or other common automatic metrics. We argue that the constraints provided by BITGs over the semantic frames and arguments of the reference and MT output sentences are essential for accurate evaluation of the phrasal similarity of the semantic role fillers. In common with the various MEANT semantic MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), our proposed IMEANT metric measures the degree to which the basic semantic event structure is preserved by translation—the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEAN</context>
<context position="14480" citStr="Lo and Wu (2011" startWordPosition="2380" endWordPosition="2383">sion = ∑0 wpredsi,pred+∑ j w j si,j i wi wpred+∑ j wj |q?,j | ∑i w0 i where q9i,j and q�i,jare the argument of type j in frame i in MT and REF respectively. w9 i and wz are the weights for frame i in MT/REF respectively. These weights estimate the degree of contribution of each frame to the overall meaning of the sentence. wpred and wj are the weights of the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between the reference translations and the MT output.There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. si,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT out</context>
<context position="22064" citStr="Lo and Wu (2011" startWordPosition="3623" endWordPosition="3626">tion grammars. ARG0 pred ARG1 The level of reduction is conducive to raising the inspection and supervision work efficiency . already-high correlation with human adequacy judgments. 4.1 Experimental setup We perform the meta-evaluation upon two different partitions of the DARPA GALE P2.5 ChineseEnglish translation test set. The corpus includes the Chinese input sentences, each accompanied by one English reference translation and three participating state-of-the-art MT systems’ output. For the sake of consistent comparison, the first evaluation partition, GALE-A, is the same as the one used in Lo and Wu (2011a), and the second evaluation partition, GALE-B, is the same as the one used in Lo and Wu (2011b). For both reference and machine translations, the ASSERT (Pradhan et al., 2004) semantic role labeler was used to automatically predict semantic parses. 27 Table 1: Sentence-level correlation with human adequacy judgements on different partitions of GALE P2.5 data. IMEANT always yields top correlations, and is more consistent than either MEANT or its recent cross-lingual XMEANT quality estimation variant. For reference, the human HMEANT upper bound is 0.53 for GALE-A and 0.37 for GALE-B—thus, the </context>
</contexts>
<marker>Lo, Wu, 2011</marker>
<rawString>Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How semantic frames evaluate MT more accurately. In Twenty-second International Joint Conference on Artificial Intelligence (IJCAI11), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>Unsupervised vs. supervised weight estimation for semantic MT evaluation metrics.</title>
<date>2012</date>
<booktitle>In Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-6),</booktitle>
<contexts>
<context position="14674" citStr="Lo and Wu, 2012" startWordPosition="2414" endWordPosition="2417">e i in MT/REF respectively. These weights estimate the degree of contribution of each frame to the overall meaning of the sentence. wpred and wj are the weights of the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between the reference translations and the MT output.There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. si,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function in</context>
</contexts>
<marker>Lo, Wu, 2012</marker>
<rawString>Chi-kiu Lo and Dekai Wu. Unsupervised vs. supervised weight estimation for semantic MT evaluation metrics. In Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-6), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>Can informal genres be better translated by tuning on automatic semantic metrics?</title>
<date>2013</date>
<booktitle>In 14th Machine Translation Summit (MT Summit XIV),</booktitle>
<contexts>
<context position="5191" citStr="Lo and Wu, 2013" startWordPosition="798" endWordPosition="801">g ITGs (also known as BITGs or BTGs) which are ITGs containing only a single non-differentiated non terminal category (Wu, 1995a), so as to produce even higher correlation with human adequacy judgments than any automatic MEANT variants, or other common automatic metrics. We argue that the constraints provided by BITGs over the semantic frames and arguments of the reference and MT output sentences are essential for accurate evaluation of the phrasal similarity of the semantic role fillers. In common with the various MEANT semantic MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), our proposed IMEANT metric measures the degree to which the basic semantic event structure is preserved by translation—the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy j</context>
<context position="6416" citStr="Lo and Wu, 2013" startWordPosition="1005" endWordPosition="1008">an both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. 2 Related Work 2.1 ITGs and MT evaluation Relatively little investigation into the potential benefits of ITGs is found in previous MT evaluation work. One exception is invWER, proposed by Leusch et al. (2003) and Leusch and Ney (2008). The invWER metric interprets weighted BITGs as a generalization of the Levenshtein edit distance, in which entire segments (blocks) can be inverted, as long as this is done strictly compositionally so as not to violate</context>
<context position="11587" citStr="Lo and Wu (2013" startWordPosition="1820" endWordPosition="1823"> only an automatic semantic parser and a monolingual corpus of the output language, which is used to gauge lexical similarity between the semantic role fillers of the reference and translation. MEANT is computed as follows: 1. Apply an automatic shallow semantic parser to both the reference and machine translations. (Figure 1 shows examples of automatic shallow semantic parses on both reference and MT.) 2. Apply the maximum weighted bipartite matching algorithm to align the semantic frames between the reference and machine translations according to the lexical similarities of the predicates. (Lo and Wu (2013a) proposed a backoff algorithm that evaluates the entire sentence of the MT output using the lexical similarity based on the context vector model, if the automatic shallow semantic parser fails to parse the reference or machine translations.) 3. For each pair of the aligned frames, apply the maximum weighted bipartite matching algorithm to align the arguments between the reference and MT output according to the lexical similarity of role fillers. 4. Compute the weighted f-score over the matching role labels of these aligned predicates and role fillers according to the following definitions: q</context>
<context position="15454" citStr="Lo and Wu, 2013" startWordPosition="2544" endWordPosition="2547">on adequacy of the development set are unavailable. si,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we will assess IMEANT against the latest version of MEANT (Lo et al., 2014) which, as shown, uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers,since this has been shown to be more accurate than the previously used aggregation functions. Recent studies (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) show that tuning MT systems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as forma</context>
</contexts>
<marker>Lo, Wu, 2013</marker>
<rawString>Chi-kiu Lo and Dekai Wu. Can informal genres be better translated by tuning on automatic semantic metrics? In 14th Machine Translation Summit (MT Summit XIV), 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame based mt evaluation metric.</title>
<date>2013</date>
<booktitle>In 8th Workshop on Statistical Machine Translation (WMT</booktitle>
<contexts>
<context position="5191" citStr="Lo and Wu, 2013" startWordPosition="798" endWordPosition="801">g ITGs (also known as BITGs or BTGs) which are ITGs containing only a single non-differentiated non terminal category (Wu, 1995a), so as to produce even higher correlation with human adequacy judgments than any automatic MEANT variants, or other common automatic metrics. We argue that the constraints provided by BITGs over the semantic frames and arguments of the reference and MT output sentences are essential for accurate evaluation of the phrasal similarity of the semantic role fillers. In common with the various MEANT semantic MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), our proposed IMEANT metric measures the degree to which the basic semantic event structure is preserved by translation—the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy j</context>
<context position="6416" citStr="Lo and Wu, 2013" startWordPosition="1005" endWordPosition="1008">an both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. 2 Related Work 2.1 ITGs and MT evaluation Relatively little investigation into the potential benefits of ITGs is found in previous MT evaluation work. One exception is invWER, proposed by Leusch et al. (2003) and Leusch and Ney (2008). The invWER metric interprets weighted BITGs as a generalization of the Levenshtein edit distance, in which entire segments (blocks) can be inverted, as long as this is done strictly compositionally so as not to violate</context>
<context position="11587" citStr="Lo and Wu (2013" startWordPosition="1820" endWordPosition="1823"> only an automatic semantic parser and a monolingual corpus of the output language, which is used to gauge lexical similarity between the semantic role fillers of the reference and translation. MEANT is computed as follows: 1. Apply an automatic shallow semantic parser to both the reference and machine translations. (Figure 1 shows examples of automatic shallow semantic parses on both reference and MT.) 2. Apply the maximum weighted bipartite matching algorithm to align the semantic frames between the reference and machine translations according to the lexical similarities of the predicates. (Lo and Wu (2013a) proposed a backoff algorithm that evaluates the entire sentence of the MT output using the lexical similarity based on the context vector model, if the automatic shallow semantic parser fails to parse the reference or machine translations.) 3. For each pair of the aligned frames, apply the maximum weighted bipartite matching algorithm to align the arguments between the reference and MT output according to the lexical similarity of role fillers. 4. Compute the weighted f-score over the matching role labels of these aligned predicates and role fillers according to the following definitions: q</context>
<context position="15454" citStr="Lo and Wu, 2013" startWordPosition="2544" endWordPosition="2547">on adequacy of the development set are unavailable. si,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we will assess IMEANT against the latest version of MEANT (Lo et al., 2014) which, as shown, uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers,since this has been shown to be more accurate than the previously used aggregation functions. Recent studies (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) show that tuning MT systems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as forma</context>
</contexts>
<marker>Lo, Wu, 2013</marker>
<rawString>Chi-kiu Lo and Dekai Wu. MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame based mt evaluation metric. In 8th Workshop on Statistical Machine Translation (WMT 2013), 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
</authors>
<title>Anand Karthik Tumuluru, and Dekai Wu. Fully automatic semantic MT evaluation.</title>
<date>2012</date>
<booktitle>In 7th Workshop on Statistical Machine Translation (WMT</booktitle>
<marker>Lo, 2012</marker>
<rawString>Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. Fully automatic semantic MT evaluation. In 7th Workshop on Statistical Machine Translation (WMT 2012), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Karteek Addanki</author>
<author>Markus Saers</author>
<author>Dekai Wu</author>
</authors>
<title>Improving machine translation by training against an automatic semantic frame based evaluation metric.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="6398" citStr="Lo et al., 2013" startWordPosition="1001" endWordPosition="1004">equacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. 2 Related Work 2.1 ITGs and MT evaluation Relatively little investigation into the potential benefits of ITGs is found in previous MT evaluation work. One exception is invWER, proposed by Leusch et al. (2003) and Leusch and Ney (2008). The invWER metric interprets weighted BITGs as a generalization of the Levenshtein edit distance, in which entire segments (blocks) can be inverted, as long as this is done strictly compositionally so</context>
<context position="15436" citStr="Lo et al., 2013" startWordPosition="2540" endWordPosition="2543">n human judgments on adequacy of the development set are unavailable. si,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we will assess IMEANT against the latest version of MEANT (Lo et al., 2014) which, as shown, uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers,since this has been shown to be more accurate than the previously used aggregation functions. Recent studies (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) show that tuning MT systems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data gen</context>
</contexts>
<marker>Lo, Addanki, Saers, Wu, 2013</marker>
<rawString>Chi-kiu Lo, Karteek Addanki, Markus Saers, and Dekai Wu. Improving machine translation by training against an automatic semantic frame based evaluation metric. In 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Meriem Beloucif</author>
<author>Dekai Wu</author>
</authors>
<title>Improving machine translation into Chinese by tuning against Chinese MEANT.</title>
<date>2013</date>
<booktitle>In International Workshop on Spoken Language Translation (IWSLT</booktitle>
<contexts>
<context position="6398" citStr="Lo et al., 2013" startWordPosition="1001" endWordPosition="1004">equacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. 2 Related Work 2.1 ITGs and MT evaluation Relatively little investigation into the potential benefits of ITGs is found in previous MT evaluation work. One exception is invWER, proposed by Leusch et al. (2003) and Leusch and Ney (2008). The invWER metric interprets weighted BITGs as a generalization of the Levenshtein edit distance, in which entire segments (blocks) can be inverted, as long as this is done strictly compositionally so</context>
<context position="15436" citStr="Lo et al., 2013" startWordPosition="2540" endWordPosition="2543">n human judgments on adequacy of the development set are unavailable. si,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we will assess IMEANT against the latest version of MEANT (Lo et al., 2014) which, as shown, uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers,since this has been shown to be more accurate than the previously used aggregation functions. Recent studies (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) show that tuning MT systems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data gen</context>
</contexts>
<marker>Lo, Beloucif, Wu, 2013</marker>
<rawString>Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Improving machine translation into Chinese by tuning against Chinese MEANT. In International Workshop on Spoken Language Translation (IWSLT 2013), 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Meriem Beloucif</author>
<author>Markus Saers</author>
<author>Dekai Wu</author>
</authors>
<title>XMEANT: Better semantic MT evaluation without reference translations.</title>
<date>2014</date>
<booktitle>In 52nd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="15566" citStr="Lo et al., 2014" startWordPosition="2565" endWordPosition="2568">ntext vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we will assess IMEANT against the latest version of MEANT (Lo et al., 2014) which, as shown, uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers,since this has been shown to be more accurate than the previously used aggregation functions. Recent studies (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) show that tuning MT systems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. ∑1 wpredsi,pred+∑ j w j si,j i wi wpree�d+∑</context>
</contexts>
<marker>Lo, Beloucif, Saers, Wu, 2014</marker>
<rawString>Chi-kiu Lo, Meriem Beloucif, Markus Saers, and Dekai Wu. XMEANT: Better semantic MT evaluation without reference translations. In 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matouš Macháček</author>
<author>Ondřej Bojar</author>
</authors>
<title>Results of the WMT13 metrics shared task.</title>
<date>2013</date>
<booktitle>In Eighth Workshop on Statistical Machine Translation (WMT 2013),</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="6188" citStr="Macháček and Bojar, 2013" startWordPosition="967" endWordPosition="970"> reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. 2 Related Work 2.1 ITGs and MT evaluation Relatively little investigation into the potential benefits of ITGs is found in previous MT evaluation work. One exception is invWER, proposed by Leusch et al. (2003) and Leusch and Ne</context>
</contexts>
<marker>Macháček, Bojar, 2013</marker>
<rawString>Matouš Macháček and Ondřej Bojar. Results of the WMT13 metrics shared task. In Eighth Workshop on Statistical Machine Translation (WMT 2013), Sofia, Bulgaria, August 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Automatic construction of clean broad-coverage translation lexicons.</title>
<date>1996</date>
<booktitle>In 2nd Conference of the Association for Machine Translation in the Americas (AMTA1996),</booktitle>
<contexts>
<context position="25539" citStr="Melamed, 1996" startWordPosition="4177" endWordPosition="4178">n be seen in Figure 3, which shows the result on the same example sentence as in Figure 1. Disregarding the semantic parsing errors arising from the current limitations of automatic SRL tools, the ITG tends to provide clean, sparse alignments for role fillers like the ARG1 of the resumed PRED, preferring to leave tokens like complete and range unaligned instead of aligning them anyway as MEANT’s maximal alignment algorithm tends to do. Note that it is not simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al. (2012) showed that the competitive linking approach (Melamed, 1996) which also generally produces sparser alignments does not work as well in MEANT, whereas the ITG appears to be selective about the token alignments in a manner that better fits the semantic structure. For contrast, Figure 4 shows a case where IMEANT appropriately accepts dense alignments. 5 Conclusion We have presented IMEANT, an inversion transduction grammar based rethinking of the MEANT semantic frame based MT evaluation approach, that achieves higher correlation with human adequacy judgments of MT output quality than MEANT and its variants, as well as other common evaluation metrics. Our </context>
</contexts>
<marker>Melamed, 1996</marker>
<rawString>I. Dan Melamed. Automatic construction of clean broad-coverage translation lexicons. In 2nd Conference of the Association for Machine Translation in the Americas (AMTA1996), 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In The Twenty-first National Conference on Artificial Intelligence (AAAI-06),</booktitle>
<volume>21</volume>
<contexts>
<context position="15306" citStr="Mihalcea et al. (2006)" startWordPosition="2518" endWordPosition="2521"> estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. si,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we will assess IMEANT against the latest version of MEANT (Lo et al., 2014) which, as shown, uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers,since this has been shown to be more accurate than the previously used aggregation functions. Recent studies (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) show that tuning MT systems against M</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. Corpus-based and knowledge-based measures of text semantic similarity. In The Twenty-first National Conference on Artificial Intelligence (AAAI-06), volume 21, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Franz Josef Och</author>
<author>Gregor Leusch</author>
<author>Hermann Ney</author>
</authors>
<title>A evaluation tool for machine translation: Fast evaluation for MT research.</title>
<date>2000</date>
<booktitle>In The Second International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="6045" citStr="Nießen et al., 2000" startWordPosition="940" endWordPosition="943">ranslation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. 2 Related Work 2.1 ITGs and MT evaluation Relatively little investigation into the </context>
<context position="8292" citStr="Nießen et al., 2000" startWordPosition="1309" endWordPosition="1312">rees in invWER are completely unconstrained. In contrast, IMEANT employs the same explicit, strong semantic frame modeling as MEANT, on both the reference and machine translations. In IMEANT, the semantic frames always take precedence over pure BITG biases. Compared to invWER, this strongly constrains the space of biparses that IMEANT permits to be considered. 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) reporting cases 23 where BLEU strongly disagrees with human judgments of translation adequacy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure</context>
</contexts>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>Sonja Nießen, Franz Josef Och, Gregor Leusch, and Hermann Ney. A evaluation tool for machine translation: Fast evaluation for MT research. In The Second International Conference on Language Resources and Evaluation (LREC 2000), 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Dependency-based automatic evaluation for machine translation.</title>
<date>2007</date>
<booktitle>In Syntax and Structure in Statistical Translation (SSST),</booktitle>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>Karolina Owczarzak, Josef van Genabith, and Andy Way. Dependency-based automatic evaluation for machine translation. In Syntax and Structure in Statistical Translation (SSST), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Evaluating machine translation with LFG dependencies.</title>
<date>2007</date>
<journal>Machine Translation,</journal>
<volume>21</volume>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>Karolina Owczarzak, Josef van Genabith, and Andy Way. Evaluating machine translation with LFG dependencies. Machine Translation, 21:95–119, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In 40th Annual Meeting of the Association for Computational Linguistics (ACL-02),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania,</location>
<contexts>
<context position="5880" citStr="Papineni et al., 2002" startWordPosition="911" endWordPosition="914">ic semantic event structure is preserved by translation—the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as </context>
<context position="8150" citStr="Papineni et al., 2002" startWordPosition="1287" endWordPosition="1290"> lexical BITG probabilities. Secondly, there is no explicit semantic modeling in invWER. Providing they meet the BITG constraints, the biparse trees in invWER are completely unconstrained. In contrast, IMEANT employs the same explicit, strong semantic frame modeling as MEANT, on both the reference and machine translations. In IMEANT, the semantic frames always take precedence over pure BITG biases. Compared to invWER, this strongly constrains the space of biparses that IMEANT permits to be considered. 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) reporting cases 23 where BLEU strongly disagrees with human judgments of translation adequacy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequa</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In 40th Annual Meeting of the Association for Computational Linguistics (ACL-02), pages 311–318, Philadelphia, Pennsylvania, July 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLTNAACL</booktitle>
<contexts>
<context position="5399" citStr="Pradhan et al., 2004" startWordPosition="832" endWordPosition="835">than any automatic MEANT variants, or other common automatic metrics. We argue that the constraints provided by BITGs over the semantic frames and arguments of the reference and MT output sentences are essential for accurate evaluation of the phrasal similarity of the semantic role fillers. In common with the various MEANT semantic MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), our proposed IMEANT metric measures the degree to which the basic semantic event structure is preserved by translation—the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (L</context>
<context position="22241" citStr="Pradhan et al., 2004" startWordPosition="3653" endWordPosition="3656">judgments. 4.1 Experimental setup We perform the meta-evaluation upon two different partitions of the DARPA GALE P2.5 ChineseEnglish translation test set. The corpus includes the Chinese input sentences, each accompanied by one English reference translation and three participating state-of-the-art MT systems’ output. For the sake of consistent comparison, the first evaluation partition, GALE-A, is the same as the one used in Lo and Wu (2011a), and the second evaluation partition, GALE-B, is the same as the one used in Lo and Wu (2011b). For both reference and machine translations, the ASSERT (Pradhan et al., 2004) semantic role labeler was used to automatically predict semantic parses. 27 Table 1: Sentence-level correlation with human adequacy judgements on different partitions of GALE P2.5 data. IMEANT always yields top correlations, and is more consistent than either MEANT or its recent cross-lingual XMEANT quality estimation variant. For reference, the human HMEANT upper bound is 0.53 for GALE-A and 0.37 for GALE-B—thus, the fully automated IMEANT approximation is not far from closing the gap. metric GALE-A GALE-B IMEANT 0.51 0.33 XMEANT 0.51 0.20 MEANT 0.48 0.33 METEOR 1.5 (2014) 0.43 0.10 NIST 0.2</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Dan Jurafsky. Shallow semantic parsing using support vector machines. In Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLTNAACL 2004), 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel Rios</author>
<author>Wilker Aziz</author>
<author>Lucia Specia</author>
</authors>
<title>TINE: A metric to assess MT adequacy.</title>
<date>2011</date>
<booktitle>In Sixth Workshop on Statistical Machine Translation (WMT</booktitle>
<contexts>
<context position="8853" citStr="Rios et al., 2011" startWordPosition="1394" endWordPosition="1397">4), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) reporting cases 23 where BLEU strongly disagrees with human judgments of translation adequacy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure. However, its correlation with human adequacy judgment is comparable to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. Another automatic metric, ULC (Giménez and Màrquez, 2007, 2008), incorporates several semantic similarity features and shows improved correla</context>
</contexts>
<marker>Rios, Aziz, Specia, 2011</marker>
<rawString>Miguel Rios, Wilker Aziz, and Lucia Specia. TINE: A metric to assess MT adequacy. In Sixth Workshop on Statistical Machine Translation (WMT 2011), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Dekai Wu</author>
</authors>
<title>Improving phrasebased translation via word alignments from stochastic inversion transduction grammars.</title>
<date>2009</date>
<booktitle>In Third Workshop on Syntax and Structure in Statistical Translation (SSST-3),</booktitle>
<pages>28--36</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="19320" citStr="Saers and Wu, 2009" startWordPosition="3151" endWordPosition="3154">aluate MT output in terms of the degree to which the translation has preserved the essential “who did what to whom,for whom, when, where, how and why” of the foreign input sentence. Unlike MEANT, however, IMEANT aligns and scores under ITG assumptions. MEANT uses a maximum alignment algorithm to align the tokens in the role fillers between the reference and machine translations, and then scores by aggregating the lexical similarities into a phrasal similarity using an f-measure. In contrast, IMEANT aligns and scores by utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). To be precise in this regard, we can see IMEANT as differing from the foregoing description of MEANT in the definition of si,pred and si,� , as follows. G ({A} , W°, W1, R, A) R {A --+ [AA] , A --+ (AA), A --+ e/f} p ([AA] |A) = p ((AA)|A) = 1 p (e/f|A) = s(e, f) l l lg (P (A � ei,pred/fi,pred|G// si,pred = lg−1   max( |ei,pred|,| red| ) fi, the null token) and f E Wi denoting a token in the reference translation (or the null token). The rule probability (or more accurately, rule weight) function p is set to be 1 for structural transduction rules, and for lexical tr</context>
<context position="24667" citStr="Saers and Wu, 2009" startWordPosition="4030" endWordPosition="4033">B partitions, IMEANT comes within a few points of the human upper bound benchmark HAJ correlations computed using the human labeled semantic frames and alignments used in the HMEANT. Data analysis reveals two reasons that IMEANT correlates with human adequacy judgement more closely than MEANT. First, BITG constraints indeed provide more accurate phrasal similarity aggregation, compared to the naive bag-of-words based heuristics employed in MEANT. Similar results have been observed while trying to estimate word alignment probabilities where BITG constraints outperformed alignments from GIZA++ (Saers and Wu, 2009). Secondly, the permutation and bijectivity constraints enforced by the ITG provide better leverage to reject token alignments when they are not appropriate, compared with the maximal alignment approach which tends to be rather promiscuous. A case of this can be seen in Figure 3, which shows the result on the same example sentence as in Figure 1. Disregarding the semantic parsing errors arising from the current limitations of automatic SRL tools, the ITG tends to provide clean, sparse alignments for role fillers like the ARG1 of the resumed PRED, preferring to leave tokens like complete and ra</context>
</contexts>
<marker>Saers, Wu, 2009</marker>
<rawString>Markus Saers and Dekai Wu. Improving phrasebased translation via word alignments from stochastic inversion transduction grammars. In Third Workshop on Syntax and Structure in Statistical Translation (SSST-3), pages 28–36, Boulder, Colorado, June 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Joakim Nivre</author>
<author>Dekai Wu</author>
</authors>
<title>Learning stochastic bracketing inversion transduction grammars with a cubic time biparsing algorithm.</title>
<date>2009</date>
<booktitle>In 11th International Conference on Parsing Technologies (IWPT’09),</booktitle>
<pages>29--32</pages>
<location>Paris, France,</location>
<contexts>
<context position="3373" citStr="Saers et al., 2009" startWordPosition="507" endWordPosition="510">ty for general syntaxdirected transductions (Aho and Ullman, 1972) is exponential. By constraining a syntax-directed transduction grammar to allow only monotonic straight and inverted reorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar to context-free grammars, and can be biparsed in polynomial time and space (O (n�) time and O (n4) space). It is also possible to do approximate biparsing in O (n�) time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This makes ITGs an appealing alternative for eval22 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33, October 25, 2014, Doha, Qatar. c�2014 Association for Com</context>
<context position="20143" citStr="Saers et al. (2009)" startWordPosition="3310" endWordPosition="3313">A --+ [AA] , A --+ (AA), A --+ e/f} p ([AA] |A) = p ((AA)|A) = 1 p (e/f|A) = s(e, f) l l lg (P (A � ei,pred/fi,pred|G// si,pred = lg−1   max( |ei,pred|,| red| ) fi, the null token) and f E Wi denoting a token in the reference translation (or the null token). The rule probability (or more accurately, rule weight) function p is set to be 1 for structural transduction rules, and for lexical transduction rules it is defined using context vector model based lexical similarity measure. To calculate the inside probability (or more accurately, inside score) of a , we use the algorithm described in Saers et al. (2009). Given this, si,pred and now represent the length normalized BITG parse scores of the predicates and role fillers of the arguments of type j between the reference and machine tran U{E} MEANT’s ( ) pair of segments, P A � � e/f|G si,� slations. 1 (lg (P (A � ei,7 /fi,7 |G)) s i,7 = lg max(| ei,7 |,| fi,7 |) where G is a bracketing ITG whose only non terminal is A, and R is a set of transduction rules with e E WEU{E} denoting a token in the MT output (or 4 Experiments In this section we discuss experiments indicating that IMEANT further improves upon MEANT’s 26 [MT2] The level of reduction is </context>
</contexts>
<marker>Saers, Nivre, Wu, 2009</marker>
<rawString>Markus Saers, Joakim Nivre, and Dekai Wu. Learning stochastic bracketing inversion transduction grammars with a cubic time biparsing algorithm. In 11th International Conference on Parsing Technologies (IWPT’09), pages 29–32, Paris, France, October 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In 7th Biennial Conference Association for Machine Translation in the Americas (AMTA</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="6076" citStr="Snover et al., 2006" startWordPosition="946" endWordPosition="949">essfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. 2 Related Work 2.1 ITGs and MT evaluation Relatively little investigation into the potential benefits of ITGs is f</context>
<context position="8323" citStr="Snover et al., 2006" startWordPosition="1315" endWordPosition="1318">nconstrained. In contrast, IMEANT employs the same explicit, strong semantic frame modeling as MEANT, on both the reference and machine translations. In IMEANT, the semantic frames always take precedence over pure BITG biases. Compared to invWER, this strongly constrains the space of biparses that IMEANT permits to be considered. 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) reporting cases 23 where BLEU strongly disagrees with human judgments of translation adequacy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure. However, its correlation with</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. A study of translation edit rate with targeted human annotation. In 7th Biennial Conference Association for Machine Translation in the Americas (AMTA 2006), pages 223–231, Cambridge, Massachusetts, August 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anand Karthik Tumuluru</author>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>Accuracy and robustness in measuring the lexical similarity of semantic role fillers for automatic semantic MT evaluation.</title>
<date>2012</date>
<booktitle>In 26th Pacific Asia Conference on Language, Information, and Computation (PACLIC 26),</booktitle>
<contexts>
<context position="15128" citStr="Tumuluru et al. (2012)" startWordPosition="2491" endWordPosition="2494">termined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. si,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we will assess IMEANT against the latest version of MEANT (Lo et al., 2014) which, as shown, uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers,since this has been sho</context>
<context position="25478" citStr="Tumuluru et al. (2012)" startWordPosition="4167" endWordPosition="4170">ment approach which tends to be rather promiscuous. A case of this can be seen in Figure 3, which shows the result on the same example sentence as in Figure 1. Disregarding the semantic parsing errors arising from the current limitations of automatic SRL tools, the ITG tends to provide clean, sparse alignments for role fillers like the ARG1 of the resumed PRED, preferring to leave tokens like complete and range unaligned instead of aligning them anyway as MEANT’s maximal alignment algorithm tends to do. Note that it is not simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al. (2012) showed that the competitive linking approach (Melamed, 1996) which also generally produces sparser alignments does not work as well in MEANT, whereas the ITG appears to be selective about the token alignments in a manner that better fits the semantic structure. For contrast, Figure 4 shows a case where IMEANT appropriately accepts dense alignments. 5 Conclusion We have presented IMEANT, an inversion transduction grammar based rethinking of the MEANT semantic frame based MT evaluation approach, that achieves higher correlation with human adequacy judgments of MT output quality than MEANT and i</context>
</contexts>
<marker>Tumuluru, Lo, Wu, 2012</marker>
<rawString>Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai Wu. Accuracy and robustness in measuring the lexical similarity of semantic role fillers for automatic semantic MT evaluation. In 26th Pacific Asia Conference on Language, Information, and Computation (PACLIC 26), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>SPEDE: Probabilistic edit distance metrics for MT evaluation.</title>
<date>2012</date>
<booktitle>In 7th Workshop on Statistical Machine Translation (WMT</booktitle>
<contexts>
<context position="9774" citStr="Wang and Manning, 2012" startWordPosition="1539" endWordPosition="1542">ependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. Another automatic metric, ULC (Giménez and Màrquez, 2007, 2008), incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to match the machine translation to the reference translation via an integrated probabilistic FSM and probabilistic PDA model. The semantic textual similarity metric Sagan (Castillo and Estrella, 2012) is based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain many parameters that need to be tuned, and employ expensive linguistic resources such as WordNet or paraphrase tables. The expensive training, tuning and/or running time renders these metrics difficult to use in the SMT training cy</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Mengqiu Wang and Christopher D. Manning. SPEDE: Probabilistic edit distance metrics for MT evaluation. In 7th Workshop on Statistical Machine Translation (WMT 2012), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>An algorithm for simultaneously bracketing parallel texts by aligning words.</title>
<date>1995</date>
<booktitle>In 33rd Annual Meeting of the Association for Computational Linguistics (ACL 95),</booktitle>
<pages>244--251</pages>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="3545" citStr="Wu, 1995" startWordPosition="534" endWordPosition="535"> reorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar to context-free grammars, and can be biparsed in polynomial time and space (O (n�) time and O (n4) space). It is also possible to do approximate biparsing in O (n�) time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This makes ITGs an appealing alternative for eval22 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics uating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to fo</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu. An algorithm for simultaneously bracketing parallel texts by aligning words. In 33rd Annual Meeting of the Association for Computational Linguistics (ACL 95), pages 244–251, Cambridge, Massachusetts, June 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Trainable coarse bilingual grammars for parallel text bracketing.</title>
<date>1995</date>
<booktitle>In Third Annual Workshop on Very Large Corpora (WVLC-3),</booktitle>
<pages>69--81</pages>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="3545" citStr="Wu, 1995" startWordPosition="534" endWordPosition="535"> reorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar to context-free grammars, and can be biparsed in polynomial time and space (O (n�) time and O (n4) space). It is also possible to do approximate biparsing in O (n�) time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This makes ITGs an appealing alternative for eval22 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics uating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to fo</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu. Trainable coarse bilingual grammars for parallel text bracketing. In Third Annual Workshop on Very Large Corpora (WVLC-3), pages 69–81, Cambridge, Massachusetts, June 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<pages>403</pages>
<contexts>
<context position="1468" citStr="Wu, 1997" startWordPosition="201" endWordPosition="202">nal segments across the reference and MT output, score the phrasal similarity of the semantic role fillers more accurately than the simple word alignment heuristics (bag-of-word alignment or maximum alignment) used in previous version of MEANT. The approach successfully integrates (1) the previously demonstrated extremely high coverage of cross-lingual semantic frame alternations by ITGs, with (2) the high accuracy of evaluating MT via weighted f-scores on the degree of semantic frame preservation. 1 Introduction There has been to date relatively little use of inversion transduction grammars (Wu, 1997) to improve the accuracy of MT evaluation metrics, despite long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints (and the observation that most current state-of-the-art SMT systems employ ITG decoders). We show that ITGs can be used to redesign the MEANT semantic frame based MT evaluation metric (Lo et al., 2012) to produce improvements in accuracy and reliability. This work is driven by the motivation that especially when considering semantic MT metrics, ITGs would be seem to be a natural basis for several reasons.</context>
<context position="19280" citStr="Wu, 1997" startWordPosition="3145" endWordPosition="3146"> level IMEANT still aims to evaluate MT output in terms of the degree to which the translation has preserved the essential “who did what to whom,for whom, when, where, how and why” of the foreign input sentence. Unlike MEANT, however, IMEANT aligns and scores under ITG assumptions. MEANT uses a maximum alignment algorithm to align the tokens in the role fillers between the reference and machine translations, and then scores by aggregating the lexical similarities into a phrasal similarity using an f-measure. In contrast, IMEANT aligns and scores by utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). To be precise in this regard, we can see IMEANT as differing from the foregoing description of MEANT in the definition of si,pred and si,� , as follows. G ({A} , W°, W1, R, A) R {A --+ [AA] , A --+ (AA), A --+ e/f} p ([AA] |A) = p ((AA)|A) = 1 p (e/f|A) = s(e, f) l l lg (P (A � ei,pred/fi,pred|G// si,pred = lg−1   max( |ei,pred|,| red| ) fi, the null token) and f E Wi denoting a token in the reference translation (or the null token). The rule probability (or more accurately, rule weight) function p is set to be 1 for structura</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377– 403, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>A comparative study on reordering constraints in statistical machine translation.</title>
<date>2003</date>
<booktitle>In 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003),</booktitle>
<pages>144--151</pages>
<location>Stroudsburg, Pennsylvania,</location>
<contexts>
<context position="19300" citStr="Zens and Ney, 2003" startWordPosition="3147" endWordPosition="3150">ANT still aims to evaluate MT output in terms of the degree to which the translation has preserved the essential “who did what to whom,for whom, when, where, how and why” of the foreign input sentence. Unlike MEANT, however, IMEANT aligns and scores under ITG assumptions. MEANT uses a maximum alignment algorithm to align the tokens in the role fillers between the reference and machine translations, and then scores by aggregating the lexical similarities into a phrasal similarity using an f-measure. In contrast, IMEANT aligns and scores by utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). To be precise in this regard, we can see IMEANT as differing from the foregoing description of MEANT in the definition of si,pred and si,� , as follows. G ({A} , W°, W1, R, A) R {A --+ [AA] , A --+ (AA), A --+ e/f} p ([AA] |A) = p ((AA)|A) = 1 p (e/f|A) = s(e, f) l l lg (P (A � ei,pred/fi,pred|G// si,pred = lg−1   max( |ei,pred|,| red| ) fi, the null token) and f E Wi denoting a token in the reference translation (or the null token). The rule probability (or more accurately, rule weight) function p is set to be 1 for structural transduction rules</context>
</contexts>
<marker>Zens, Ney, 2003</marker>
<rawString>Richard Zens and Hermann Ney. A comparative study on reordering constraints in statistical machine translation. In 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003), pages 144–151, Stroudsburg, Pennsylvania, 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
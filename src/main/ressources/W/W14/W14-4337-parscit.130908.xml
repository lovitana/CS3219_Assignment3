<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000057">
<title confidence="0.57298">
The Second Dialog State Tracking Challenge
</title>
<author confidence="0.991195">
Matthew Henderson&apos;, Blaise Thomson&apos; and Jason Williams2
</author>
<affiliation confidence="0.811076">
&apos;Department of Engineering, University of Cambridge, U.K.
2Microsoft Research, Redmond, WA, USA
</affiliation>
<email confidence="0.980442">
mh521@eng.cam.ac.uk brmt2@eng.cam.ac.uk jason.williams@microsoft.com
</email>
<sectionHeader confidence="0.997174" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999903666666667">
A spoken dialog system, while commu-
nicating with a user, must keep track of
what the user wants from the system at
each step. This process, termed dialog
state tracking, is essential for a success-
ful dialog system as it directly informs the
system’s actions. The first Dialog State
Tracking Challenge allowed for evalua-
tion of different dialog state tracking tech-
niques, providing common testbeds and
evaluation suites. This paper presents a
second challenge, which continues this
tradition and introduces some additional
features – a new domain, changing user
goals and a richer dialog state. The chal-
lenge received 31 entries from 9 research
groups. The results suggest that while
large improvements on a competitive base-
line are possible, trackers are still prone
to degradation in mismatched conditions.
An investigation into ensemble learning
demonstrates the most accurate tracking
can be achieved by combining multiple
trackers.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986795407407407">
Spoken language provides a medium of communi-
cation that is natural to users as well as hands- and
eyes-free. Voice-based computer systems, called
spoken dialog systems, allow users to interact us-
ing speech to achieve a goal. Efficient operation of
a spoken dialog system requires a component that
can track what has happened in a dialog, incor-
porating system outputs, user speech and context
from previous turns. The building and evaluation
of these trackers is an important field of research
since the performance of dialog state tracking is
important for the final performance of a complete
system.
Until recently, it was difficult to compare ap-
proaches to state tracking because of the wide va-
riety of metrics and corpora used for evaluation.
The first dialog state tracking challenge (DSTC1)
attempted to overcome this by defining a challenge
task with standard test conditions, freely available
corpora and open access (Williams et al., 2013).
This paper presents the results of a second chal-
lenge, which continues in this tradition with the
inclusion of additional features relevant to the re-
search community.
Some key differences to the first challenge in-
clude:
• The domain is restaurant search instead of
bus timetable information. This provides par-
ticipants with a different category of interac-
tion where there is a database of matching en-
tities.
• Users’ goals are permitted to change. In the
first challenge, the user was assumed to al-
ways want a specific bus journey. In this chal-
lenge the user’s goal can change. For exam-
ple, they may want a ‘Chinese’ restaurant at
the start of the dialog but change to wanting
‘Italian’ food by the end.
• The dialog state uses a richer representa-
tion than in DSTC1, including not only the
slot/value attributes of the user goal, but also
their search method, and what information
they wanted the system to read out.
As well as presenting the results of the different
state trackers, this paper attempts to obtain some
insights into research progress by analysing their
performance. This includes analyses of the predic-
tive power of performance on the development set,
the effects of tracking the dialog state using joint
distributions, and the correlation between 1-best
accuracy and overall quality of probability distri-
butions output by trackers. An evaluation of the
effects of ensemble learning is also performed.
The paper begins with an overview of the chal-
</bodyText>
<page confidence="0.981456">
263
</page>
<note confidence="0.730458">
Proceedings of the SIGDIAL 2014 Conference, pages 263–272,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999729166666667">
lenge in section 2. The labelling scheme and met-
rics used for evaluation are discussed in section 3
followed by a summary of the results of the chal-
lenge in section 4. An analysis of ensemble learn-
ing is presented in section 5. Section 6 concludes
the paper.
</bodyText>
<sectionHeader confidence="0.935642" genericHeader="method">
2 Challenge overview
</sectionHeader>
<subsectionHeader confidence="0.983496">
2.1 Problem statement
</subsectionHeader>
<bodyText confidence="0.999101538461538">
This section defines the problem of dialog state
tracking as it is presented in the challenge. The
challenge evaluates state tracking for dialogs
where users search for restaurants by specifying
constraints, and may ask for information such as
the phone number. The dialog state is formu-
lated in a manner which is general to information
browsing tasks such as this.
Included with the data is an ontology1, which
gives details of all possible dialog states. The
ontology includes a list of attributes termed re-
questable slots which the user may request, such
as the food type or phone number. It also provides
a list of informable slots which are attributes that
may be provided as constraints. Each informable
slot has a set of possible values. Table 1 gives de-
tails on the ontology used in DSTC2.
The dialog state at each turn consists of three
components:
• The goal constraint for each informable slot.
This is either an assignment of a value from
the ontology which the user has specified as
a constraint, or is a special value — either
Dontcare which means the user has no pref-
erence, or None which means the user is yet
to specify a valid goal for this slot.
</bodyText>
<listItem confidence="0.934003461538462">
• A set of requested slots, i.e. those slots
whose values have been requested by the
user, and should be informed by the system.
• An assignment of the current dialog search
method. This is one of
– by constraints, if the user is attempting
to issue a constraint,
– by alternatives, if the user is requesting
alternative suitable venues,
– by name, if the user is attempting to ask
about a specific venue by its name,
– finished, if the user wants to end the call
– or none otherwise.
</listItem>
<bodyText confidence="0.81044">
Note that in DSTC1, the set of dialog states
</bodyText>
<footnote confidence="0.6037955">
1Note that this ontology includes only the schema for di-
alog states and not the database entries
</footnote>
<bodyText confidence="0.9999184">
was dependent on the hypotheses given by a Spo-
ken Language Understanding component (SLU)
(Williams et al., 2013), whereas here the state is
labelled independently of any SLU (see section 3).
Appendix B gives an example dialog with the state
labelled at each turn.
A tracker must use information up to a given
turn in the dialog, and output a probability distri-
bution over dialog states for the turn. Trackers
output separately the distributions for goal con-
straints, requested slots and the method. They may
either report a joint distribution over the goal con-
straints, or supply marginal distributions and let
the joint goal constraint distribution be calculated
as a product of the marginals.
</bodyText>
<subsectionHeader confidence="0.998879">
2.2 Challenge design
</subsectionHeader>
<bodyText confidence="0.999969529411765">
DSTC2 studies the problem of dialog state track-
ing as a corpus-based task, similar to DSTC1. The
challenge task is to re-run dialog state tracking
over a test corpus of dialogs.
A corpus-based challenge means all trackers
are evaluated on the same dialogs, allowing di-
rect comparison between trackers. There is also
no need for teams to expend time and money in
building an end-to-end system and getting users,
meaning a low barrier to entry.
When a tracker is deployed, it will inevitably al-
ter the performance of the dialog system it is part
of, relative to any previously collected dialogs. In
order to simulate this, and to penalise overfitting to
known conditions, evaluation dialogs in the chal-
lenge are drawn from dialogs with a dialog man-
ager which is not found in the training data.
</bodyText>
<subsectionHeader confidence="0.998468">
2.3 Data
</subsectionHeader>
<bodyText confidence="0.999951666666667">
A large corpus of dialogs with various telephone-
based dialog systems was collected using Ama-
zon Mechanical Turk. The dialogs used in the
challenge come from 6 conditions; all combina-
tions of 3 dialog managers and 2 speech recognis-
ers. There are roughly 500 dialogs in each condi-
tion, of average length 7.88 turns from 184 unique
callers.
The 3 dialog managers are:
</bodyText>
<listItem confidence="0.997941857142857">
• DM-HC, a simple tracker maintaining a sin-
gle top dialog state, and a hand-crafted policy
• DM-POMDPHC, a dynamic Bayesian net-
work for tracking a distribution of dialog
states, and a hand-crafted policy
• DM-POMDP, the same tracking method as
DM-POMDPHC, with a policy learnt using
</listItem>
<page confidence="0.99291">
264
</page>
<table confidence="0.7989926">
Slot Requestable Informable
area yes yes. 5 values; north,
south, east, west, centre
food yes yes, 91 possible values
name yes yes, 113 possible values
pricerange yes yes, 3 possible values
addr yes no
phone yes no
postcode yes no
signature yes no
</table>
<tableCaption confidence="0.9985765">
Table 1: Ontology used in DSTC2 for restaurant informa-
tion. Counts do not include the special Dontcare value.
</tableCaption>
<bodyText confidence="0.8909785">
POMDP reinforcement learning
The 2 speech recognisers are:
</bodyText>
<listItem confidence="0.90282925">
• ASR-degraded, speech recogniser with arti-
ficially degraded statistical acoustic models
• ASR-good, full speech recogniser optimised
for the domain
</listItem>
<bodyText confidence="0.999314111111111">
These give two acoustic conditions, the de-
graded model producing dialogs at higher error
rates. The degraded models simulate in-car con-
ditions and are described in Young et al. (2013).
The set of all calls with DM-POMDP, with both
speech recognition configurations, constitutes the
test set. All calls with the other two dialog man-
agers are used for the training and development
set. Specifically, the datasets are arranged as so:
</bodyText>
<listItem confidence="0.971701909090909">
• dstc2 train. Labelled dataset released in Oc-
tober 2013, with 1612 calls from DM-HC and
DM-POMDPHC, and both ASR conditions.
• dstc2 dev. Labelled dataset released at the
same time as dstc2 train, with 506 calls under
the same conditions as dstc2 train. No caller
in this set appears in dstc2 train.
• dstc2 test. Set used for evaluation. Released
unlabelled at the beginning of the evaluation
week. This consists of all 1117 dialogs with
DM-POMDP.
</listItem>
<bodyText confidence="0.996671333333333">
Paid Amazon Mechanical Turkers were as-
signed tasks and asked to call the dialog systems.
Callers were asked to find restaurants that matched
particular constraints on the slots area, pricerange
and food. To elicit more complex dialogs, includ-
ing changing goals (goals in DSTC1 were always
constant), the users were sometimes asked to find
more than one restaurant. In cases where a match-
ing restaurant did not exist they were required to
seek an alternative, for example finding an Indian
instead of an Italian restaurant.
A breakdown of the frequency of goal con-
straint changes is given in table 2, showing around
40% of all dialogs involved a change in goal con-
straint. The distribution of the goal constraints in
</bodyText>
<figureCaption confidence="0.964658666666667">
Figure 1: Histogram of values for the food constraint (ex-
cluding dontcare) in all data. The most frequent values are
Indian, Chinese, Italian and European.
</figureCaption>
<table confidence="0.980788428571429">
Dataset
train dev test
area 2.9% 1.4% 3.8%
food 37.3% 34.0% 40.9%
name 0.0% 0.0% 0.0%
pricerange 1.7% 1.6% 3.1%
any 40.1% 37.0% 44.5%
</table>
<tableCaption confidence="0.986049">
Table 2: Percentage of dialogs which included a change in
</tableCaption>
<bodyText confidence="0.971601173913044">
the goal constraint for each informable (and any slot). Barely
any users asked for restaurants by name.
the data was reasonably uniform across the area
and pricerange slots, but was skewed for food as
shown in figure 1. The skew arises from the distri-
bution of the restaurants in the system’s database;
many food types have very few matching venues.
Recently, researchers have started using word
confusion networks for spoken language under-
standing (Henderson et al., 2012; T¨ur et al., 2013).
Unfortunately, word confusion networks were not
logged at the time of collecting the dialog data. In
order to provide word confusion networks, ASR
was run offline in batch mode on each dialog us-
ing similar models as the live system. This gives
a second set of ASR results, labelled batch, which
not only includes ASR N-best lists (as in live re-
sults), but also word confusion networks.
For each dataset and speech recogniser, table 3
gives the Word Error Rate on the top ASR hypoth-
esis, and F-score for the top SLU hypothesis (cal-
culated as in Henderson et al. (2012)). Note the
batch ASR was always less accurate than the live.
</bodyText>
<table confidence="0.996989727272727">
Live Batch
Dataset ASR WER F-score WER
degraded 30.7% 72.4% 37.7%
train good 22.4% 78.7% 25.5%
all 26.4% 75.7% 31.3%
degraded 40.4% 67.3% 47.3%
dev good 25.2% 75.2% 30.0%
all 31.9% 71.6% 37.6%
degraded 33.6% 70.0% 41.1%
test good 23.5% 77.8% 27.1%
all 28.7% 73.8% 34.3%
</table>
<tableCaption confidence="0.99548">
Table 3: Word Error Rate on the top hypothesis, and F-score
on top SLU hypothesis.
</tableCaption>
<figure confidence="0.9563555">
200
150
100
50
</figure>
<page confidence="0.990822">
265
</page>
<sectionHeader confidence="0.953317" genericHeader="method">
3 Labelling and evaluation
</sectionHeader>
<bodyText confidence="0.999844274509804">
The output of each tracker is a distribution over
dialog states for each turn, as explained in section
2.1. To allow evaluation of the tracker output, the
single correct dialog state at each turn is labelled.
Labelling of the dialog state is facilitated by first
labelling each user utterance with its semantic rep-
resentation, in the dialog act format described in
Henderson et al. (2013) (some example seman-
tic representations are given in appendix B). The
semantic labelling was achieved by first crowd-
sourcing the transcription of the audio to text.
Next a semantic decoder was run over the tran-
scriptions, and the authors corrected the decoder’s
results by hand. Given the sequence of machine
actions and user actions, both represented seman-
tically, the true dialog state is computed determin-
istically using a simple set of rules.
Recall the dialog state is composed of multiple
components; the goal constraint for each slot, the
requested slots, and the method. Each of these
is evaluated separately, by comparing the tracker
output to the correct label. The joint over the goal
constraints is evaluated in the same way, where the
tracker may either explicitly enumerate and score
its joint hypotheses, or let the joint be computed as
the product of the distributions over the slots.
A bank of metrics which look at the tracker out-
put and the correct labels are calculated in the eval-
uation. These metrics are a slightly expanded set
of those calculated in DSTC1.
Denote an example probability distribution
given by a tracker as p and the correct label to be
i, so we have that the probability reported to the
correct hypothesis is pi, and Ej pj = 1.
Accuracy measures the fraction of turns where
the top hypothesis is correct, i.e. where i =
arg maxj pj. AvgP, average probability, mea-
sures the mean score of the correct hypothesis, pi.
This gives some idea of the quality of the score
given to the correct hypothesis, ignoring the rest
of the distribution. Neglogp is the mean nega-
tive logarithm of the score given to the correct hy-
pothesis, − log pi. Sometimes called the negative
log likelihood, this is a standard score in machine
learning tasks. MRR is the mean reciprocal rank
of the top hypothesis, i.e. 1
1+k where A = i and
pj0 ? pj, ? . . .. This metric measures the qual-
ity of the ranking, without necessarily treating the
scores as probabilities. L2 measures the square
of the l2 norm between the distribution and the
correct label, indicating quality of the whole re-
ported distribution. It is calculated for one turn
as (1 − pi)2 + Ej=,4ij. Two metrics, Update
precision and Update accuracy measure the ac-
curacy and precision of updates to the top scoring
hypothesis from one turn to the next. For more
details, see Higashinaka et al. (2004), which finds
these metrics to be highly correlated with dialog
success in their data.
Finally there is a set of measures relating to
the receiver operating characteristic (ROC) curves,
which measure the discrimination of the scores for
the highest-ranked hypotheses. Two versions of
ROC are computed, V1 and V2. V1 computes
correct-accepts (CA), false accepts (FA) and false-
rejects (FR) as fractions of all utterances. The
V2 metrics consider fractions of correctly classi-
fied utterances, meaning the values always reach
100% regardless of the accuracy. V2 metrics mea-
sure discrimination independently of the accuracy,
and are therefore only comparable between track-
ers with similar accuracies.
Several metrics are computed from the ROC
statistics. ROC V1 EER computes the false ac-
ceptance rate at the point where false-accepts are
equal to false-rejects. ROC V1 CA05, ROC V1
CA10, ROC V1 CA20 and ROC V2 CA05, ROC
V2 CA10, ROC V2 CA20, compute the correct
acceptance rates for both versions of ROC at false-
acceptance rates 0.05, 0.10, and 0.20.
Two schedules are used to decide which turns to
include when computing each metric. Schedule 1
includes every turn. Schedule 2 only includes a
turn if any SLU hypothesis up to and including the
turn contains some information about the compo-
nent of the dialog state in question, or if the correct
label is not None. E.g. for a goal constraint, this is
whether the slot has appeared with a value in any
SLU hypothesis, an affirm/negate act has appeared
after a system confirmation of the slot, or the user
has in fact informed the slot regardless of the SLU.
The data is labelled using two schemes. The
first, scheme A, is considered the standard la-
belling of the dialog state. Under this scheme,
each component of the state is defined as the most
recently asserted value given by the user. The
None value is used to indicate that a value is yet
to be given. Appendix B demonstrates labelling
under scheme A.
A second labelling scheme, scheme B, is in-
cluded in the evaluation, where labels are prop-
</bodyText>
<page confidence="0.99033">
266
</page>
<bodyText confidence="0.9999285">
agated backwards through the dialog. This la-
belling scheme is designed to assess whether a
tracker is able to predict a user’s intention be-
fore it has been stated. Under scheme B, the la-
bel at a current turn for a particular component of
the dialog state is considered to be the next value
which the user settles on, and is reset in the case
of goal constraints if the slot value pair is given in
a canthelp act by the system (i.e. the system has
informed that this constraint is not satisfiable).
</bodyText>
<subsectionHeader confidence="0.980302">
3.1 Featured metrics
</subsectionHeader>
<bodyText confidence="0.998965285714286">
All combinations of metrics, state components,
schedules and labelling schemes give rise to 815
total metrics calculated per tracker in evaluation.
Although each may have its particular motiva-
tion, many of the metrics will be highly corre-
lated. From the results of DSTC1 it was found
the metrics could be roughly split into 3 indepen-
dent groups; one measuring 1-best quality (e.g.
Acc), another measuring probability calibration
(e.g. L2), and the last measuring discrimination
(e.g. ROC metrics) (Williams et al., 2013).
By selecting a representative from each of these
groups, the following were chosen as featured
metrics:
</bodyText>
<listItem confidence="0.999883333333333">
• Accuracy, schedule 2, scheme A
• L2 norm, schedule 2, scheme A
• ROC V2 CA 5, schedule 2, scheme A
</listItem>
<bodyText confidence="0.999875923076923">
Accuracy is a particularly important measure
for dialog management techniques which only
consider the top dialog state hypothesis at each
turn, while L2 is of more importance when mul-
tiple dialog states are considered in action selec-
tion. Note that the ROC metric is only compara-
ble among systems operating at similar accuracies,
and while L2 should be minimised, Accuracy and
the ROC metric should be maximised.
Each of these, calculated for joint goal con-
straints, search method and combined re-
quested slots, gives 9 metrics altogether which
participants were advised to focus on optimizing.
</bodyText>
<subsectionHeader confidence="0.99765">
3.2 Baseline trackers
</subsectionHeader>
<bodyText confidence="0.999883">
Three baseline trackers were entered in the chal-
lenge, under the ID ‘team0’. Source code for
all the baseline systems is available on the DSTC
website2. The first, ‘team0.entry0’, follows sim-
ple rules commonly used in spoken dialog sys-
tems. It gives a single hypothesis for each slot,
</bodyText>
<footnote confidence="0.843806">
2http://camdial.org/˜mh521/dstc/
</footnote>
<bodyText confidence="0.920515454545454">
whose value is the top scoring suggestion so far in
the dialog. Note that this tracker does not account
well for goal constraint changes; the hypothesised
value for a slot will only change if a new value
occurs with a higher confidence.
The focus baseline, ‘team0.entry1’, includes a
simple model of changing goal constraints. Be-
liefs are updated for the goal constraint s = v, at
turn t, P (s = v), using the rule:
P (s = v)t = qtP (s = v)t−1 + SLU (s = v)t
where 0 &lt; SLU(s = v)t &lt; 1 is the evidence
</bodyText>
<equation confidence="0.829695666666667">
for s = v given by the SLU in turn t, and qt =
E
v, SLU(s = v% &lt; 1.
</equation>
<bodyText confidence="0.9998129375">
Another baseline tracker, based on the tracker
presented in Wang and Lemon (2013) is included
in the evaluation, labelled ‘team0.entry2’. This
tracker uses a selection of domain independent
rules to update the beliefs, similar to the focus
baseline. One rule uses a learnt parameter called
the noise adjustment, to adjust the SLU scores.
Full details of this and all baseline trackers are pro-
vided on the DSTC website.
Finally, an oracle tracker is included under the
label ‘team0.entry3’. This reports the correct la-
bel with score 1 for each component of the dialog
state, but only if it has been suggested in the dialog
so far by the SLU. This gives an upper-bound for
the performance of a tracker which uses only the
SLU and its suggested hypotheses.
</bodyText>
<sectionHeader confidence="0.999979" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.99916245">
Altogether 9 research teams participated in the
challenge. Each team could submit a maximum of
5 trackers, and 31 trackers were submitted in total.
Teams are identified by anonymous team numbers
team1-9, and baseline systems are grouped under
team0. Appendix A gives the results on the fea-
tured metrics for each entry submitted to the chal-
lenge. The full results, including tracker output,
details of each tracker and scripts to run the evalu-
ation are available on the DSTC2 website.
The table in appendix A specifies which of the
inputs available were used for each tracker- from
live ASR, live SLU and batch ASR. This facil-
itates comparisons between systems which used
the same information.
A variety of techniques were used in the sub-
mitted trackers. Some participants provided short
synopses, which are available in the download
from the DSTC2 website. Full details on the track-
ers themselves are published at SIGdial 2014.
</bodyText>
<page confidence="0.981947">
267
</page>
<bodyText confidence="0.999824238095238">
For the “requested slot” task, some trackers out-
performed the oracle tracker. This was possible
because trackers could guess a slot was requested
using dialog context, even if there was no mention
of it in the SLU output.
Participants were asked to report the results of
their trackers on the dstcs2 dev development set.
Figure 2 gives some insight into how well perfor-
mance on the development set predicted perfor-
mance on the test set. Metrics are reported as per-
centage improvement relative to the focus base-
line to normalise for the difficulty of the datasets;
in general trackers achieved higher accuracies on
the test set than on development. Figure 2 shows
that the development set provided reasonable pre-
dictions, though in all cases improvement rel-
ative to the baseline was overestimated, some-
times drastically. This suggests that approaches to
tracking have trouble with generalisation, under-
performing in the mismatched conditions of the
test set which used an unseen dialog manager.
</bodyText>
<figure confidence="0.718629">
Joint Goal Constraint Accuracy
</figure>
<figureCaption confidence="0.84413925">
Figure 2: Performance relative to the focus baseline (per-
centage increase) for dev set (white) and test set (grey). Top
entry for each team chosen based on joint goal constraint ac-
curacy. A lower L2 score is better.
</figureCaption>
<bodyText confidence="0.9996139">
Recall from section 2, trackers could output
joint distributions for goal constraints, or simply
output one distribution for each slot and allow the
joint to be calculated as the product. Two teams,
team2 and team8, opted to output a joint distribu-
tion for some of their entries. Figure 3 compares
performance on the test set for these trackers be-
tween the joint distributions they reported, and the
joint calculated as the product. The entries from
team2 were able to show an increase in the accu-
racy of the top joint goal constraint hypotheses,
but seemingly at a cost in terms of the L2 score.
Conversely the entries from team8, though oper-
ating at lower performance than the focus base-
line, were able to show an improvement in L2 at a
slight loss in accuracy. These results suggest that a
tracking method is yet to be proposed which can,
at least on this data, improve both accuracy and
the L2 score of tracker output by reporting joint
predictions of goal constraints.
</bodyText>
<figure confidence="0.924409666666667">
Accuracy
0.70 0.72 0.74 0.76 0.78
0.4 0.5 0.6 0.7
</figure>
<figureCaption confidence="0.9491666">
Figure 3: Influence of reporting a full joint distribution.
White bar shows test set performance computing the goal
constraints as a product of independent marginals; dark bar is
performance with a full joint distribution. All entries which
reported a full joint are shown. A lower L2 score is better.
</figureCaption>
<bodyText confidence="0.99895975">
It is of interest to investigate the correlation be-
tween accuracy and L2. Figure 4 plots these met-
rics for each tracker on joint goal constraints. We
see that in general a lower L2 score correlates with
a higher accuracy, but there are examples of high
accuracy trackers which do poorly in terms of L2.
This further justifies the reporting of these as two
separate featured metrics.
</bodyText>
<figure confidence="0.58848">
0.50 0.55 0.60 0.65 0.70 0.75 0.80
Accuracy
</figure>
<figureCaption confidence="0.981623">
Figure 4: Scatterplot of joint goal constraint accuracy and
joint goal constraint L2 for each entry. Plotted line is least-
squares linear regression, L2 = 1.53 − 1.43Accuracy
</figureCaption>
<figure confidence="0.999217759036145">
-0.2 0.2 0.4 0.6
team1entry0
team2entry1
team3entry0
team4entry0
team5entry4
team6entry2
team7entry0
team8entry1
team9entry0
-0.3 -0.2 -0.1 0.1
Joint Goal Constraint L2
team1entry0
team2entry1
team3entry0
team4entry0
team5entry4
team6entry2
team7entry0
team8entry1
team9entry0
1.34%
0.44%
-0.11%
0.20%
team0entry2
team2entry0
team2entry1
team2entry2
team2entry3
team2entry4
team8entry0
team8entry1
team8entry2
team8entry3
0.03%
-0.30%
-0.04%
-0.04%
-0.09%
-0.09%
L2
team0entry2
team0entr
team2entry0
team2entr
team2entry1
team2ent
team2entry2
team2entr
team2entry3
team2entr
team2entry4
team8entr
team8entry0
team8entr
team8entry1
team8ent
team8entry2
team8entr
team8entry3
team8entr
0.03%
38.21%
52.17%
0.22%
23.05%
-2.00%
-1.73%
-1.69%
-1.79%
-1.75%
focus baseline, team0entry2
team2entry0
team4entry0
team2entry3
team2entry1
L2 0.8
0.7
0.6
0.5
0.4
0.3
</figure>
<page confidence="0.989251">
268
</page>
<table confidence="0.999527727272727">
Tracker Joint goal Method Requested
Acc. L2 Acc. L2 Acc. L2
Single best entry 0.784 0.346 0.950 0.082 0.978 0.035
Score averaging: top 2 entries 0.787 0.364- 0.945- 0.083 0.976 0.039-
Score averaging: top 5 entries 0.777 0.347 0.945 0.089- 0.976 0.038
Score averaging: top 10 entries 0.760- 0.364- 0.934- 0.108- 0.967- 0.056-
Score averaging: all entries 0.765- 0.362- 0.934- 0.103- 0.971- 0.052-
Stacking: top 2 entries 0.789 0.322+ 0.949 0.085- 0.977 0.040-
Stacking: top 5 entries 0.795+ 0.315+ 0.949 0.084 0.978 0.037
Stacking: top 10 entries 0.796+ 0.312+ 0.949 0.083 0.979 0.035
Stacking: all entries 0.798+ 0.308+ 0.950 0.083 0.980 0.034
</table>
<tableCaption confidence="0.9993925">
Table 4: Accuracy and L2 for Joint goal constraint, Method, and Requested slots for the single best tracker (by accuracy) in
DSTC2, and various ensemble methods. “Top N entries” means the N entries with highest accuracies from distinct teams, where
the baselines are included as a team. +/- indicates statistically significantly better/worse than the single best entry (p &lt; 0.01),
computed with McNemar’s test for accuracy and the paired t-test for L2, both with Bonferroni correction for repeated tests.
</tableCaption>
<sectionHeader confidence="0.97871" genericHeader="method">
5 Ensemble learning
</sectionHeader>
<bodyText confidence="0.999961116883117">
The dialog state tracking challenge provides an
opportunity to study ensemble learning – i.e. syn-
thesizing the output of many trackers to improve
performance beyond any single tracker. Here we
consider two forms of ensemble learning: score
averaging and stacking.
In score averaging, the final score of a class is
computed as the mean of the scores output by all
trackers for that class. One of score averaging’s
strengths is that it requires no additional training
data beyond that used to train the constituent track-
ers. If each tracker’s output is correct more than
half the time, and if the errors made by trackers are
not correlated, then score averaging is guaranteed
to improve performance (since the majority vote
will be correct in the limit). In (Lee and Eskenazi,
2013), score averaging (there called “system com-
bination”) has been applied to combine the output
of four dialog state trackers. To help decorrelate
errors, constituent trackers were trained on differ-
ent subsets of data, and used different machine
learning methods. The relative error rate reduction
was 5.1% on the test set.
The second approach to ensemble learning is
stacking (Wolpert, 1992). In stacking, the scores
output by the constituent classifiers are fed to a
new classifier that makes a final prediction. In
other words, the output of each constituent classi-
fier is viewed as a feature, and the new final classi-
fier can learn the correlations and error patterns of
each. For this reason, stacking often outperforms
score averaging, particularly when errors are cor-
related. However, stacking requires a validation
set for training the final classifier. In DSTC2, we
only have access to trackers’ output on the test set.
Therefore, to estimate the performance of stack-
ing, we perform cross-validation on the test set:
the test set is divided into two folds. First, fold 1
is used for training the final classifier, and fold 2
is used for testing. Then the process is reversed.
The two test outputs are then concatenated. Note
that models are never trained and tested on the
same data. A maximum entropy model (maxent) is
used (details in (Metallinou et al., 2013)), which is
common practice for stacking classifiers. In addi-
tion, maxent was found to yield best performance
in DSTC1 (Lee and Eskenazi, 2013).
Table 4 reports accuracy and L2 for goal con-
straints, search method, and requested slots. For
each ensemble method and each quantity (column)
the table gives results for combining the top track-
ers from 2 or 5 distinct teams, for combining the
top tracker from each team, and combining all
trackers (including the baselines as a team). For
example, the joint goal constraint ensemble with
the top 2 entries was built from team2.entry1 &amp;
team4.entry0, and the method ensemble with the
top 2 entries from team2.entry4 &amp; team4.entry0.
Table 4 shows two interesting trends. The first
is that score averaging does not improve perfor-
mance, and performance declines as more track-
ers are combined, yielding a statistically signifi-
cant decrease across all metrics. This suggests that
the errors of the different trackers are correlated,
which is unsurprising since they were trained on
the same data. On the other hand, stacking yields
a statistically significant improvement in accuracy
for goal constraints, and doesn’t degrade accuracy
for the search method and requested slots. For
stacking, the trend is that adding more trackers in-
creases performance – for example, combining the
best tracker from every team improves goal con-
straint accuracy from 78.4% to 79.8%.
For completeness, we note that the additional
data could alternatively be used to improve the ac-
curacy of a constituent classifier; given the con-
straints of the challenge, we can’t assess the mag-
</bodyText>
<page confidence="0.995368">
269
</page>
<bodyText confidence="0.999966222222222">
nitude of that improvement, so it is an open ques-
tion whether stacking is the best use of additional
data. Also, the training and test conditions of
the final stacking classifier are not mis-matched,
whereas in practice they would be. Nonethe-
less, this result does suggest that, if additional
data is available, stacking can be used to success-
fully combine multiple trackers and achieve per-
formance better than the single best tracker.
</bodyText>
<sectionHeader confidence="0.999528" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999960117647059">
DSTC2 continues the tradition of DSTC1 by pro-
viding a common testbed for dialog state track-
ing, introducing some additional features relevant
to the research community– specifically a new
domain, changing user goals and a richer dialog
state. The data, evaluation scripts, and baseline
trackers will remain available and open to the re-
search community online.
Results from the previous challenge motivated
the selection of a few metrics as featured met-
rics, which facilitate comparisons between track-
ers. Analysis of the performance on the matched
development set and the mismatched test set sug-
gests that there still appears to be limitations on
generalisation, as found in DSTC1. The results
also suggest there are limitations in exploiting cor-
relations between slots, with few teams exploiting
joint distributions and the effects of doing so being
mixed. Investigating ensemble learning demon-
strates the effectiveness of combining tracker out-
puts. Ensemble learning exploits the strengths of
individual trackers to provide better quality output
than any constituent tracker in the group.
A follow up challenge, DSTC3, will present
the problem of adapting to a new domain with
very few example dialogs. Future work should
also verify that improvements in dialog state track-
ing translate to improvements in end-to-end dia-
log system performance. In this challenge, paid
subjects were used as users with real information
needs were not available. However, differences
between these two user groups have been shown
(Raux et al., 2005), so future studies should also
test on real users.
</bodyText>
<sectionHeader confidence="0.99776" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996010090909091">
The authors thank the advisory committee for
their valuable input: Paul Crook, Maxine Eske-
nazi, Milica Ga&amp;quot;si´c, Helen Hastie, Kee-Eung Kim,
Sungjin Lee, Oliver Lemon, Olivier Pietquin,
Joelle Pineau, Deepak Ramachandran, Brian
Strope and Steve Young. The authors also thank
Zhuoran Wang for providing a baseline tracker,
and DJ Kim, Sungjin Lee &amp; David Traum for com-
ments on evaluation metrics. Finally, thanks to
SIGdial for their endorsement, and to the partic-
ipants for making the challenge a success.
</bodyText>
<sectionHeader confidence="0.998957" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998416847826087">
Matthew Henderson, Milica Ga&amp;quot;si´c, Blaise Thom-
son, Pirros Tsiakoulis, Kai Yu, and Steve Young.
2012. Discriminative Spoken Language Under-
standing Using Word Confusion Networks. In Spo-
ken Language Technology Workshop, 2012. IEEE.
Matthew Henderson, Blaise Thomson, and Jason
Williams. 2013. Dialog State Tracking Challenge
2 &amp; 3 Handbook. camdial.org/˜mh521/dstc/.
Ryuichiro Higashinaka, Noboru Miyazaki, Mikio
Nakano, and Kiyoaki Aikawa. 2004. Evaluat-
ing discourse understanding in spoken dialogue sys-
tems. ACM Trans. Speech Lang. Process., Novem-
ber.
Sungjin Lee and Maxine Eskenazi. 2013. Recipe for
building robust spoken dialog state trackers: Dialog
state tracking challenge system description. In Pro-
ceedings of the SIGDIAL 2013 Conference.
Angeliki Metallinou, Dan Bohus, and Jason D.
Williams. 2013. Discriminative state tracking for
spoken dialog systems. In Proc Association for
Computational Linguistics, Sofia.
Antoine Raux, Brian Langner, Dan Bohus, Alan W
Black, and Maxine Eskenazi. 2005. Let’s go public!
Taking a spoken dialog system to the real world.
G¨okhan T¨ur, Anoop Deoras, and Dilek Hakkani-T¨ur.
2013. Semantic parsing using word confusion net-
works with conditional random fields. In INTER-
SPEECH.
Zhuoran Wang and Oliver Lemon. 2013. A simple
and generic belief tracking mechanism for the dia-
log state tracking challenge: On the believability of
observed information. In Proceedings of the SIG-
DIAL 2013 Conference.
Jason Williams, Antoine Raux, Deepak Ramachadran,
and Alan Black. 2013. The Dialog State Track-
ing Challenge. In Proceedings of the SIGDIAL 2013
Conference, Metz, France, August.
David H. Wolpert. 1992. Stacked generalization. Neu-
ral Networks, 5:241–259.
Steve Young, Catherine Breslin, Milica Ga&amp;quot;si´c,
Matthew Henderson, Dongho Kim, Martin Szum-
mer, Blaise Thomson, Pirros Tsiakoulis, and Eli
Tzirkel Hancock. 2013. Evaluation of Statistical
POMDP-based Dialogue Systems in Noisy Environ-
ment. In Proceedings of IWSDS, Napa, USA, Jan-
uary.
</reference>
<page confidence="0.994232">
270
</page>
<table confidence="0.974437714285714">
Appendix A: Featured results of evaluation
Tracker Inputs Joint Goal Constraints Search Method Requested Slots
team entry Live Batch Acc L2 ROC Acc L2 ROC Acc L2 ROC
Live SLU ASR
ASR
0* 0 ✓ 0.619 0.738 0.000 0.879 0.209 0.000 0.884 0.196 0.000
1 ✓ 0.719 0.464 0.000 0.867 0.210 0.349 0.879 0.206 0.000
2 ✓ 0.711 0.466 0.000 0.897 0.158 0.000 0.884 0.201 0.000
3 ✓† 0.850 0.300 0.000 0.986 0.028 0.000 0.957 0.086 0.000
1 0 ✓ 0.601 0.649 0.064 0.904 0.155 0.187 0.960 0.073 0.000
1 ✓ 0.596 0.671 0.036 0.877 0.204 0.397 0.957 0.081 0.000
2 0 ✓ ✓ 0.775 0.758 0.063 0.944 0.092 0.306 0.954 0.073 0.383
1 ✓ ✓ ✓ 0.784 0.735 0.065 0.947 0.087 0.355 0.957 0.068 0.446
2 ✓ 0.668 0.505 0.249 0.944 0.095 0.499 0.972 0.043 0.300
</table>
<figure confidence="0.840990038461539">
3 ✓ ✓ ✓ 0.771 0.354 0.313 0.947 0.093 0.294 0.941 0.090 0.262
4 ✓ ✓ ✓ 0.773 0.467 0.140 0.950 0.082 0.351 0.968 0.050 0.497
3 0 ✓ 0.729 0.452 0.000 0.878 0.210 0.000 0.889 0.188 0.000
4 0 ✓ 0.768 0.346 0.365 0.940 0.095 0.452 0.978 0.035 0.525
1 ✓ 0.746 0.381 0.383 0.939 0.097 0.423 0.977 0.038 0.490
2 ✓ 0.742 0.387 0.345 0.922 0.124 0.447 0.957 0.069 0.340
3 ✓ 0.737 0.406 0.321 0.922 0.125 0.406 0.957 0.073 0.385
5 0 ✓ ✓ 0.686 0.628 0.000 0.889 0.221 0.000 0.868 0.264 0.000
1 ✓ ✓ 0.609 0.782 0.000 0.927 0.147 0.000 0.974 0.053 0.000
2 ✓ ✓ 0.637 0.726 0.000 0.927 0.147 0.000 0.974 0.053 0.000
3 ✓ ✓ 0.609 0.782 0.000 0.927 0.147 0.000 0.974 0.053 0.000
4 ✓ ✓ 0.695 0.610 0.000 0.927 0.147 0.000 0.974 0.053 0.000
6 0 ✓ 0.713 0.461 0.100 0.865 0.228 0.199 0.932 0.118 0.057
1 ✓ 0.707 0.447 0.223 0.871 0.211 0.290 0.947 0.093 0.218
2 ✓ 0.718 0.437 0.207 0.871 0.210 0.287 0.951 0.085 0.225
7 0 ✓ 0.750 0.416 0.081 0.936 0.105 0.237 0.970 0.056 0.000
1 ✓ 0.739 0.428 0.159 0.921 0.161 0.554 0.970 0.056 0.000
2 ✓ 0.750 0.416 0.081 0.929 0.117 0.379 0.971 0.054 0.000
3 ✓ 0.725 0.432 0.105 0.936 0.105 0.237 0.972 0.047 0.000
4 ✓ 0.735 0.433 0.086 0.910 0.140 0.280 0.946 0.089 0.190
8 0 ✓ 0.692 0.505 0.071 0.899 0.153 0.000 0.935 0.106 0.000
1 ✓ 0.699 0.498 0.067 0.899 0.153 0.000 0.939 0.101 0.000
2 ✓ 0.698 0.504 0.067 0.899 0.153 0.000 0.939 0.101 0.000
3 ✓ 0.697 0.501 0.068 0.899 0.153 0.000 0.939 0.101 0.000
4 ✓ 0.697 0.508 0.068 0.899 0.153 0.000 0.939 0.101 0.000
9 0 ✓ 0.499 0.760 0.000 0.857 0.229 0.000 0.905 0.149 0.000
</figure>
<footnote confidence="0.8922802">
* The entries under team0 are the baseline systems mentioned in section 3.2. † team0.entry3 is the
oracle tracker, which uses the labels on the test set and limits itself to hypotheses suggested by the live
SLU.
The top score in each column is indicated by bold-type. The ROC metric is only comparable for trackers
operating at a similar accuracy, and so the highest values are not indicated.
</footnote>
<page confidence="0.990418">
271
</page>
<figure confidence="0.991968068965517">
S:
U:
Clown café is a cheap
restaurant in the
north part of town.
Do you have any
others like that,
maybe in the south
part of town?
reqalts(area=south)
requested=()
0.0 phone
0.0 address
requested=()
0.0 phone
0.0 address
0.7 reqalts(area=south)
0.2 reqmore()
area=south
pricerange=cheap
0.8 area=south
pricerange=cheap
0.1 area=north
pricerange=cheap
0.1 ()
0.6 byalternatives
0.2 byconstraints
method=byalternatives
requested=()
0.0 phone
0.0 address
S:
U:
A cheap place in
the north
inform(area=north,
pricerange=cheap)
Which part of town?
request(area)
0.8 inform(area=north),
inform(pricerange=cheap)
0.1 inform(area=north)
area=north
pricerange=cheap
0.7 area=north
pricerange=cheap
0.1 area=north
food=north_african
0.2 ()
0.9 byconstraints
0.1 none
method=byconstraints
Appendix B: Sample dialog, labels, and tracker output Example tracker output
Actual input and output SLU hypotheses and scores Labels
S: Which part of town? 0.2 inform(food=north_african) area=north 0.2 food=north_african
request(area) 0.1 inform(area=north) 0.1 area=north
U: The north uh area 0.7 ()
inform(area=north)
method=byconstraints 0.9 byconstraints
0.1 none
Correct?
requested= (phone,
address)
0.8 phone
0.3 address
area=south
pricerange=cheap
0.9 area=south
pricerange=cheap
0.1 area=north
pricerange=cheap
0.0 ()
0.5 byconstraints
0.4 byalternatives
method=byalternatives
S: Galleria is a cheap
restaurant in the
south.
U: What is their phone
number and
address?
request(phone),
request(address)
0.6 request(phone)
0.2 request(phone),
request(address)
0.1 request(address)
</figure>
<bodyText confidence="0.999337666666667">
Example dialog illustrating DSTC2 data, labels, and evaluation procedure. The left column shows the
actual system output and user input. The second column shows two SLU N-Best hypothesis and their
scores. In practice, up to 10 SLU N-Best hypotheses are output. In the right 3 columns, the three shaded
regions correspond to the three components of the dialog state output by a tracker at each turn. The blue
region corresponds to the user’s joint goal constraint; the red region to the user’s search method; and
the yellow region to the slots requested by the user. For space, only 2 of the 5 methods and 2 of the
8 requestable slots are shown. The third column shows the label (correct output) for each component.
The fourth column shows example tracker output for each of these three quantities, and the fifth column
indicates correctness. A goal constraint is correct if it exactly matches the label. Therefore, 0 or 1 of
the output goal constraints is correct, and all the others are incorrect. Accuracy is determined by the
correctness of the goal constraint with the highest tracker score. For search method, exactly one method
is correct at each turn, so correctness is determined by comparing the maximum scoring method to the
label. For requested slots, each slot can be requested (or not) in the same turn, so each requestable slot
is separately marked as correct or incorrect. The quantity requested.all averages the correctness of all
requested slots.
</bodyText>
<page confidence="0.995124">
272
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.939037">
<title confidence="0.999627">The Second Dialog State Tracking Challenge</title>
<author confidence="0.999422">Blaise Jason</author>
<affiliation confidence="0.999342">of Engineering, University of Cambridge,</affiliation>
<address confidence="0.993478">Research, Redmond, WA, USA</address>
<email confidence="0.990835">mh521@eng.cam.ac.ukbrmt2@eng.cam.ac.ukjason.williams@microsoft.com</email>
<abstract confidence="0.99820828">A spoken dialog system, while communicating with a user, must keep track of what the user wants from the system at step. This process, termed is essential for a successful dialog system as it directly informs the system’s actions. The first Dialog State Tracking Challenge allowed for evaluation of different dialog state tracking techniques, providing common testbeds and evaluation suites. This paper presents a second challenge, which continues this tradition and introduces some additional features – a new domain, changing user goals and a richer dialog state. The challenge received 31 entries from 9 research groups. The results suggest that while large improvements on a competitive baseline are possible, trackers are still prone to degradation in mismatched conditions. An investigation into ensemble learning demonstrates the most accurate tracking can be achieved by combining multiple trackers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew Henderson</author>
<author>Milica Gasi´c</author>
<author>Blaise Thomson</author>
<author>Pirros Tsiakoulis</author>
<author>Kai Yu</author>
<author>Steve Young</author>
</authors>
<title>Discriminative Spoken Language Understanding Using Word Confusion Networks.</title>
<date>2012</date>
<booktitle>In Spoken Language Technology Workshop,</booktitle>
<publisher>IEEE.</publisher>
<marker>Henderson, Gasi´c, Thomson, Tsiakoulis, Yu, Young, 2012</marker>
<rawString>Matthew Henderson, Milica Ga&amp;quot;si´c, Blaise Thomson, Pirros Tsiakoulis, Kai Yu, and Steve Young. 2012. Discriminative Spoken Language Understanding Using Word Confusion Networks. In Spoken Language Technology Workshop, 2012. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Henderson</author>
<author>Blaise Thomson</author>
<author>Jason Williams</author>
</authors>
<date>2013</date>
<booktitle>Dialog State Tracking Challenge 2 &amp; 3 Handbook. camdial.org/˜mh521/dstc/.</booktitle>
<contexts>
<context position="12472" citStr="Henderson et al. (2013)" startWordPosition="2073" endWordPosition="2076"> 30.0% all 31.9% 71.6% 37.6% degraded 33.6% 70.0% 41.1% test good 23.5% 77.8% 27.1% all 28.7% 73.8% 34.3% Table 3: Word Error Rate on the top hypothesis, and F-score on top SLU hypothesis. 200 150 100 50 265 3 Labelling and evaluation The output of each tracker is a distribution over dialog states for each turn, as explained in section 2.1. To allow evaluation of the tracker output, the single correct dialog state at each turn is labelled. Labelling of the dialog state is facilitated by first labelling each user utterance with its semantic representation, in the dialog act format described in Henderson et al. (2013) (some example semantic representations are given in appendix B). The semantic labelling was achieved by first crowdsourcing the transcription of the audio to text. Next a semantic decoder was run over the transcriptions, and the authors corrected the decoder’s results by hand. Given the sequence of machine actions and user actions, both represented semantically, the true dialog state is computed deterministically using a simple set of rules. Recall the dialog state is composed of multiple components; the goal constraint for each slot, the requested slots, and the method. Each of these is eval</context>
</contexts>
<marker>Henderson, Thomson, Williams, 2013</marker>
<rawString>Matthew Henderson, Blaise Thomson, and Jason Williams. 2013. Dialog State Tracking Challenge 2 &amp; 3 Handbook. camdial.org/˜mh521/dstc/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryuichiro Higashinaka</author>
<author>Noboru Miyazaki</author>
<author>Mikio Nakano</author>
<author>Kiyoaki Aikawa</author>
</authors>
<title>Evaluating discourse understanding in spoken dialogue systems.</title>
<date>2004</date>
<journal>ACM Trans. Speech Lang. Process.,</journal>
<contexts>
<context position="14844" citStr="Higashinaka et al. (2004)" startWordPosition="2486" endWordPosition="2489">chine learning tasks. MRR is the mean reciprocal rank of the top hypothesis, i.e. 1 1+k where A = i and pj0 ? pj, ? . . .. This metric measures the quality of the ranking, without necessarily treating the scores as probabilities. L2 measures the square of the l2 norm between the distribution and the correct label, indicating quality of the whole reported distribution. It is calculated for one turn as (1 − pi)2 + Ej=,4ij. Two metrics, Update precision and Update accuracy measure the accuracy and precision of updates to the top scoring hypothesis from one turn to the next. For more details, see Higashinaka et al. (2004), which finds these metrics to be highly correlated with dialog success in their data. Finally there is a set of measures relating to the receiver operating characteristic (ROC) curves, which measure the discrimination of the scores for the highest-ranked hypotheses. Two versions of ROC are computed, V1 and V2. V1 computes correct-accepts (CA), false accepts (FA) and falserejects (FR) as fractions of all utterances. The V2 metrics consider fractions of correctly classified utterances, meaning the values always reach 100% regardless of the accuracy. V2 metrics measure discrimination independent</context>
</contexts>
<marker>Higashinaka, Miyazaki, Nakano, Aikawa, 2004</marker>
<rawString>Ryuichiro Higashinaka, Noboru Miyazaki, Mikio Nakano, and Kiyoaki Aikawa. 2004. Evaluating discourse understanding in spoken dialogue systems. ACM Trans. Speech Lang. Process., November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sungjin Lee</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Recipe for building robust spoken dialog state trackers: Dialog state tracking challenge system description.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference.</booktitle>
<contexts>
<context position="27313" citStr="Lee and Eskenazi, 2013" startWordPosition="4546" endWordPosition="4549"> beyond any single tracker. Here we consider two forms of ensemble learning: score averaging and stacking. In score averaging, the final score of a class is computed as the mean of the scores output by all trackers for that class. One of score averaging’s strengths is that it requires no additional training data beyond that used to train the constituent trackers. If each tracker’s output is correct more than half the time, and if the errors made by trackers are not correlated, then score averaging is guaranteed to improve performance (since the majority vote will be correct in the limit). In (Lee and Eskenazi, 2013), score averaging (there called “system combination”) has been applied to combine the output of four dialog state trackers. To help decorrelate errors, constituent trackers were trained on different subsets of data, and used different machine learning methods. The relative error rate reduction was 5.1% on the test set. The second approach to ensemble learning is stacking (Wolpert, 1992). In stacking, the scores output by the constituent classifiers are fed to a new classifier that makes a final prediction. In other words, the output of each constituent classifier is viewed as a feature, and th</context>
<context position="28824" citStr="Lee and Eskenazi, 2013" startWordPosition="4794" endWordPosition="4797"> to trackers’ output on the test set. Therefore, to estimate the performance of stacking, we perform cross-validation on the test set: the test set is divided into two folds. First, fold 1 is used for training the final classifier, and fold 2 is used for testing. Then the process is reversed. The two test outputs are then concatenated. Note that models are never trained and tested on the same data. A maximum entropy model (maxent) is used (details in (Metallinou et al., 2013)), which is common practice for stacking classifiers. In addition, maxent was found to yield best performance in DSTC1 (Lee and Eskenazi, 2013). Table 4 reports accuracy and L2 for goal constraints, search method, and requested slots. For each ensemble method and each quantity (column) the table gives results for combining the top trackers from 2 or 5 distinct teams, for combining the top tracker from each team, and combining all trackers (including the baselines as a team). For example, the joint goal constraint ensemble with the top 2 entries was built from team2.entry1 &amp; team4.entry0, and the method ensemble with the top 2 entries from team2.entry4 &amp; team4.entry0. Table 4 shows two interesting trends. The first is that score avera</context>
</contexts>
<marker>Lee, Eskenazi, 2013</marker>
<rawString>Sungjin Lee and Maxine Eskenazi. 2013. Recipe for building robust spoken dialog state trackers: Dialog state tracking challenge system description. In Proceedings of the SIGDIAL 2013 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Metallinou</author>
<author>Dan Bohus</author>
<author>Jason D Williams</author>
</authors>
<title>Discriminative state tracking for spoken dialog systems.</title>
<date>2013</date>
<booktitle>In Proc Association for Computational Linguistics,</booktitle>
<location>Sofia.</location>
<contexts>
<context position="28681" citStr="Metallinou et al., 2013" startWordPosition="4771" endWordPosition="4774">cularly when errors are correlated. However, stacking requires a validation set for training the final classifier. In DSTC2, we only have access to trackers’ output on the test set. Therefore, to estimate the performance of stacking, we perform cross-validation on the test set: the test set is divided into two folds. First, fold 1 is used for training the final classifier, and fold 2 is used for testing. Then the process is reversed. The two test outputs are then concatenated. Note that models are never trained and tested on the same data. A maximum entropy model (maxent) is used (details in (Metallinou et al., 2013)), which is common practice for stacking classifiers. In addition, maxent was found to yield best performance in DSTC1 (Lee and Eskenazi, 2013). Table 4 reports accuracy and L2 for goal constraints, search method, and requested slots. For each ensemble method and each quantity (column) the table gives results for combining the top trackers from 2 or 5 distinct teams, for combining the top tracker from each team, and combining all trackers (including the baselines as a team). For example, the joint goal constraint ensemble with the top 2 entries was built from team2.entry1 &amp; team4.entry0, and t</context>
</contexts>
<marker>Metallinou, Bohus, Williams, 2013</marker>
<rawString>Angeliki Metallinou, Dan Bohus, and Jason D. Williams. 2013. Discriminative state tracking for spoken dialog systems. In Proc Association for Computational Linguistics, Sofia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Raux</author>
<author>Brian Langner</author>
<author>Dan Bohus</author>
<author>Alan W Black</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Let’s go public! Taking a spoken dialog system to the real world.</title>
<date>2005</date>
<marker>Raux, Langner, Bohus, Black, Eskenazi, 2005</marker>
<rawString>Antoine Raux, Brian Langner, Dan Bohus, Alan W Black, and Maxine Eskenazi. 2005. Let’s go public! Taking a spoken dialog system to the real world.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨okhan T¨ur</author>
<author>Anoop Deoras</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Semantic parsing using word confusion networks with conditional random fields.</title>
<date>2013</date>
<booktitle>In INTERSPEECH.</booktitle>
<marker>T¨ur, Deoras, Hakkani-T¨ur, 2013</marker>
<rawString>G¨okhan T¨ur, Anoop Deoras, and Dilek Hakkani-T¨ur. 2013. Semantic parsing using word confusion networks with conditional random fields. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuoran Wang</author>
<author>Oliver Lemon</author>
</authors>
<title>A simple and generic belief tracking mechanism for the dialog state tracking challenge: On the believability of observed information.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference.</booktitle>
<contexts>
<context position="19721" citStr="Wang and Lemon (2013)" startWordPosition="3330" endWordPosition="3333">g suggestion so far in the dialog. Note that this tracker does not account well for goal constraint changes; the hypothesised value for a slot will only change if a new value occurs with a higher confidence. The focus baseline, ‘team0.entry1’, includes a simple model of changing goal constraints. Beliefs are updated for the goal constraint s = v, at turn t, P (s = v), using the rule: P (s = v)t = qtP (s = v)t−1 + SLU (s = v)t where 0 &lt; SLU(s = v)t &lt; 1 is the evidence for s = v given by the SLU in turn t, and qt = E v, SLU(s = v% &lt; 1. Another baseline tracker, based on the tracker presented in Wang and Lemon (2013) is included in the evaluation, labelled ‘team0.entry2’. This tracker uses a selection of domain independent rules to update the beliefs, similar to the focus baseline. One rule uses a learnt parameter called the noise adjustment, to adjust the SLU scores. Full details of this and all baseline trackers are provided on the DSTC website. Finally, an oracle tracker is included under the label ‘team0.entry3’. This reports the correct label with score 1 for each component of the dialog state, but only if it has been suggested in the dialog so far by the SLU. This gives an upper-bound for the perfor</context>
</contexts>
<marker>Wang, Lemon, 2013</marker>
<rawString>Zhuoran Wang and Oliver Lemon. 2013. A simple and generic belief tracking mechanism for the dialog state tracking challenge: On the believability of observed information. In Proceedings of the SIGDIAL 2013 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Williams</author>
<author>Antoine Raux</author>
<author>Deepak Ramachadran</author>
<author>Alan Black</author>
</authors>
<title>The Dialog State Tracking Challenge.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<location>Metz, France,</location>
<contexts>
<context position="2171" citStr="Williams et al., 2013" startWordPosition="324" endWordPosition="327">in a dialog, incorporating system outputs, user speech and context from previous turns. The building and evaluation of these trackers is an important field of research since the performance of dialog state tracking is important for the final performance of a complete system. Until recently, it was difficult to compare approaches to state tracking because of the wide variety of metrics and corpora used for evaluation. The first dialog state tracking challenge (DSTC1) attempted to overcome this by defining a challenge task with standard test conditions, freely available corpora and open access (Williams et al., 2013). This paper presents the results of a second challenge, which continues in this tradition with the inclusion of additional features relevant to the research community. Some key differences to the first challenge include: • The domain is restaurant search instead of bus timetable information. This provides participants with a different category of interaction where there is a database of matching entities. • Users’ goals are permitted to change. In the first challenge, the user was assumed to always want a specific bus journey. In this challenge the user’s goal can change. For example, they ma</context>
<context position="5994" citStr="Williams et al., 2013" startWordPosition="982" endWordPosition="985">the system. • An assignment of the current dialog search method. This is one of – by constraints, if the user is attempting to issue a constraint, – by alternatives, if the user is requesting alternative suitable venues, – by name, if the user is attempting to ask about a specific venue by its name, – finished, if the user wants to end the call – or none otherwise. Note that in DSTC1, the set of dialog states 1Note that this ontology includes only the schema for dialog states and not the database entries was dependent on the hypotheses given by a Spoken Language Understanding component (SLU) (Williams et al., 2013), whereas here the state is labelled independently of any SLU (see section 3). Appendix B gives an example dialog with the state labelled at each turn. A tracker must use information up to a given turn in the dialog, and output a probability distribution over dialog states for the turn. Trackers output separately the distributions for goal constraints, requested slots and the method. They may either report a joint distribution over the goal constraints, or supply marginal distributions and let the joint goal constraint distribution be calculated as a product of the marginals. 2.2 Challenge des</context>
<context position="17930" citStr="Williams et al., 2013" startWordPosition="3011" endWordPosition="3014">e system (i.e. the system has informed that this constraint is not satisfiable). 3.1 Featured metrics All combinations of metrics, state components, schedules and labelling schemes give rise to 815 total metrics calculated per tracker in evaluation. Although each may have its particular motivation, many of the metrics will be highly correlated. From the results of DSTC1 it was found the metrics could be roughly split into 3 independent groups; one measuring 1-best quality (e.g. Acc), another measuring probability calibration (e.g. L2), and the last measuring discrimination (e.g. ROC metrics) (Williams et al., 2013). By selecting a representative from each of these groups, the following were chosen as featured metrics: • Accuracy, schedule 2, scheme A • L2 norm, schedule 2, scheme A • ROC V2 CA 5, schedule 2, scheme A Accuracy is a particularly important measure for dialog management techniques which only consider the top dialog state hypothesis at each turn, while L2 is of more importance when multiple dialog states are considered in action selection. Note that the ROC metric is only comparable among systems operating at similar accuracies, and while L2 should be minimised, Accuracy and the ROC metric s</context>
</contexts>
<marker>Williams, Raux, Ramachadran, Black, 2013</marker>
<rawString>Jason Williams, Antoine Raux, Deepak Ramachadran, and Alan Black. 2013. The Dialog State Tracking Challenge. In Proceedings of the SIGDIAL 2013 Conference, Metz, France, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H Wolpert</author>
</authors>
<title>Stacked generalization.</title>
<date>1992</date>
<journal>Neural Networks,</journal>
<pages>5--241</pages>
<contexts>
<context position="27702" citStr="Wolpert, 1992" startWordPosition="4608" endWordPosition="4609">ect more than half the time, and if the errors made by trackers are not correlated, then score averaging is guaranteed to improve performance (since the majority vote will be correct in the limit). In (Lee and Eskenazi, 2013), score averaging (there called “system combination”) has been applied to combine the output of four dialog state trackers. To help decorrelate errors, constituent trackers were trained on different subsets of data, and used different machine learning methods. The relative error rate reduction was 5.1% on the test set. The second approach to ensemble learning is stacking (Wolpert, 1992). In stacking, the scores output by the constituent classifiers are fed to a new classifier that makes a final prediction. In other words, the output of each constituent classifier is viewed as a feature, and the new final classifier can learn the correlations and error patterns of each. For this reason, stacking often outperforms score averaging, particularly when errors are correlated. However, stacking requires a validation set for training the final classifier. In DSTC2, we only have access to trackers’ output on the test set. Therefore, to estimate the performance of stacking, we perform </context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>David H. Wolpert. 1992. Stacked generalization. Neural Networks, 5:241–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Catherine Breslin</author>
<author>Milica Gasi´c</author>
<author>Matthew Henderson</author>
<author>Dongho Kim</author>
<author>Martin Szummer</author>
</authors>
<title>Blaise Thomson, Pirros Tsiakoulis, and Eli Tzirkel Hancock.</title>
<date>2013</date>
<booktitle>In Proceedings of IWSDS,</booktitle>
<location>Napa, USA,</location>
<marker>Young, Breslin, Gasi´c, Henderson, Kim, Szummer, 2013</marker>
<rawString>Steve Young, Catherine Breslin, Milica Ga&amp;quot;si´c, Matthew Henderson, Dongho Kim, Martin Szummer, Blaise Thomson, Pirros Tsiakoulis, and Eli Tzirkel Hancock. 2013. Evaluation of Statistical POMDP-based Dialogue Systems in Noisy Environment. In Proceedings of IWSDS, Napa, USA, January.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990627">
The Phenogrammar of Coordination
</title>
<author confidence="0.994323">
Chris Worth
</author>
<affiliation confidence="0.994207">
Department of Linguistics
The Ohio State University
</affiliation>
<email confidence="0.992134">
worth@ling.osu.edu
</email>
<sectionHeader confidence="0.997305" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999168130434782">
Linear Categorial Grammar (LinCG) is a
sign-based, Curryesque, relational, logi-
cal categorial grammar (CG) whose cen-
tral architecture is based on linear logic.
Curryesque grammars separate the ab-
stract combinatorics (tectogrammar) of
linguistic expressions from their concrete,
audible representations (phenogrammar).
Most of these grammars encode linear or-
der in string-based lambda terms, in which
there is no obvious way to distinguish right
from left. Without some notion of direc-
tionality, grammars are unable to differen-
tiate, say, subject and object for purposes
of building functorial coordinate struc-
tures. We introduce the notion of a phe-
nominator as a way to encode the term
structure of a functor separately from its
“string support”. This technology is then
employed to analyze a range of coordina-
tion phenomena typically left unaddressed
by Linear Logic-based Curryesque frame-
works.
</bodyText>
<sectionHeader confidence="0.997209" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.999949538461539">
Flexibility to the notion of constituency in con-
junction with introduction (and composition) rules
has allowed categorial grammars to successfully
address an entire host of coordination phenomena
in a transparent and compositional manner. While
“Curryesque” CGs as a rule do not suffer from
some of the other difficulties that plague Lambek
CGs, many are notably deficient in one area: co-
ordination. Lest we throw the baby out with the
bathwater, this is an issue that needs to be ad-
dressed. We take the following to be an exemplary
subset of the relevant data, and adopt a fragment
methodology to show how it may be analyzed.
</bodyText>
<listItem confidence="0.98841125">
(1) Tyrion and Joffrey drank.
(2) Joffrey whined and sniveled.
(3) Tyrion slapped and Tywin chastised Jof-
frey.
</listItem>
<bodyText confidence="0.999373791666667">
The first example is a straightforward instance
of noun phrase coordination. The second and third
are both instances of what has become known in
the categorial grammar literature as “functor co-
ordination”, that is, the coordination of linguistic
material that is in some way incomplete. The third
is particularly noteworthy as being an example of
a “right node raising” construction, whereby the
argument Joffrey serves as the object to both of
the higher NP-Verb complexes. We will show that
all three examples can be given an uncomplicated
account in the Curryesque framework of Linear
Categorial Grammar (LinCG), and that (2) and
(3) have more in common than not.
Section 1 provides an overview of the data and
and central issues surrounding an analysis of co-
ordination in Curryesque grammars. Section 2 in-
troduces the reader to the framework of LinCG,
and presents the technical innovations at the heart
of this paper. Section 3 gives lexical entries and
derivations for the examples in section 1, and sec-
tion 4 discusses our results and suggests some di-
rections for research in the near future, with refer-
ences following.
</bodyText>
<subsectionHeader confidence="0.9969625">
1.1 Curryesque grammars and Linear
Categorial Grammar
</subsectionHeader>
<bodyText confidence="0.9991212">
We take as our starting point the Curryesque (af-
ter Curry (1961)) tradition of categorial grammars,
making particular reference to those originating
with Oehrle (1994) and continuing with Abstract
Categorial Grammar (ACG) of de Groote (2001),
Muskens (2010)’s Lambda Grammar (λG), Kub-
ota and Levine’s Hybrid Type-Logical Catego-
rial Grammar (Kubota and Levine, 2012) and
to a lesser extent the Grammatical Framework
of Ranta (2004), and others. These dialects
</bodyText>
<page confidence="0.983127">
28
</page>
<note confidence="0.9917545">
Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 28–36,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999974753846154">
of categorial grammar make a distinction be-
tween Tectogrammar, or “abstract syntax”, and
Phenogrammar, or “concrete syntax”. Tec-
togrammar is primarily concerned with the struc-
tural properties of grammar, among them co-
occurrence, case, agreement, tense, and so forth.
Phenogrammar is concerned with computing a
pre-phonological representation of what will even-
tually be produced by the speaker, and encom-
passes word order, morphology, prosody, and the
like.
Linear Categorial Grammar (LinCG) is a sign-
based, Curryesque, relational, logical categorial
grammar whose central architecture is based on
linear logic. Abbreviatory overlap has been
a regrettably persistent problem, and LinCG is
the same in essence as the framework varyingly
called Linear Grammar (LG) and Pheno-Tecto-
Differentiated Categorial Grammar (PTDCG), and
developed in Smith (2010), Mihalicek (2012),
Martin (2013), Pollard and Smith (2012), and Pol-
lard (2013). In LinCG, the syntax-phonology and
syntax-semantics interfaces amount to noting that
the logics for the phenogrammar, the tectogram-
mar, and the semantics operate in parallel. This
stands in contrast to ‘syntactocentric’ theories of
grammar, where syntax is taken to be the fun-
damental domain within which expressions com-
bine, and then phonology and semantics are ‘read
off’ of the syntactic representation. LinCG is con-
ceptually different in that it has relational, rather
than functional, interfaces between the three com-
ponents of the grammar. Since we do not interpret
syntactic types into phenogrammatical or semantic
types, this allows us a great deal of freedom within
each logic, although in practice we maintain a
fairly tight connection between all three compo-
nents. Grammar rules take the form of derivational
rules which generate triples called signs, and they
bind together the three logics so that they operate
concurrently. While the invocation of a grammar
rule might simply be, say, point-wise application,
the ramifications for the three systems can in prin-
ciple be different; one can imagine expressions
which exhibit type asymmetry in various ways.
By way of example, one might think of ‘focus’
as an operation which has reflexes in all three as-
pects of the grammar: it applies pitch accents to
the target string(s) in the phenogrammar (the dif-
ference between accented and unaccented words
being reflected in the phenotype), it creates ‘low-
ering’ operators in the tectogrammar (that is, ex-
pressions which scope within a continuation), and
it ‘focuses’ a particular meaningful unit in the se-
mantics. A focused expression might share its tec-
totype ((NP —o S) —o S) with, say, a quantified
noun phrase, but the two could have different phe-
notypes, reflecting the accentuation or lack thereof
by placing the resulting expression in the domain
of prosodic boundary phenomena or not. Never-
theless, the system is constrained by the fact that
the tectogrammar is based on linear logic, so if we
take some care when writing grammar rules, we
should still find resource sensitivity to be at the
heart of the framework.
</bodyText>
<subsectionHeader confidence="0.9884035">
1.2 Why coordination is difficult for
Curryesque grammars
</subsectionHeader>
<bodyText confidence="0.99973696">
Most Curryesque CGs encode linear order in
lambda terms, and there is no obvious way to dis-
tinguish ‘right’ from ‘left’ by examining the types
(be they linear or intuitionistic).1 This is not a
problem when we are coordinating strings directly,
as de Groote and Maarek (2007) show, but an anal-
ysis of the more difficult case of functor coordi-
nation remains elusive.2 Without some notion of
directionality, grammars are unable to distinguish
between, say, subject and object. This would seem
to predict, for example, that As. s · SLAPPED ·
JOFFREY and As. TYRION · SLAPPED · s would
have the same syntactic category (NP —o S in the
tectogrammar, and St → St in the phenogram-
mar), and would thus be compatible under coor-
dination, but this is generally not the case. What
we need is a way to examine the structure of a
lambda term independently of the specific string
constants that comprise it. To put it another way,
in order to coordinate functors, we need to be able
to distinguish between what Oehrle (1995) calls
their string support, that is, the string constants
which make up the body of a particular functional
term, and the linearization structure such functors
impose on their arguments.
</bodyText>
<sectionHeader confidence="0.988993" genericHeader="introduction">
2 Linear Categorial Grammar (LinCG)
</sectionHeader>
<bodyText confidence="0.992296">
Curryesque grammars separate the notion of linear
order from the abstract combinatorics of linguis-
</bodyText>
<footnote confidence="0.999593142857143">
1A noteworthy exception is Ranta’s Grammatical Frame-
work (GF), explored in, e.g. Ranta (2004) and Ranta
(2009). GF also makes distinctions between tectogrammar
and phenogrammar, though it has a somewhat different con-
ception of each.
2A problem explicitly recognized by Kubota (2010) in
section 3.2.1.
</footnote>
<page confidence="0.993806">
29
</page>
<equation confidence="0.456291">
Γ,x : A ` b : B
</equation>
<bodyText confidence="0.999886814814815">
tic expressions, and as such base their tectogram-
mars around logics other than bilinear logic; the
Grammatical Framework is based on Martin-L¨of
type theory, and LinCG and its cousins ACG and
λG use linear logic. Linear logic is generally de-
scribed as being “resource-sensitive”, owing to the
lack of the structural rules of weakening and con-
traction. Resource sensitivity is an attractive no-
tion, theoretically, since it allows us to describe
processes of resource production, consumption,
and combination in a manner which is agnostic
about precisely how resources are combined. Cer-
tain problems which have been historically tricky
for Lambek categorial grammars (medial extrac-
tion, quantifier scope, etc.) are easily handled by
LinCG.
Since a full introduction to the framework is re-
grettably impossible given current constraints, we
refer the interested reader to the references in sec-
tion 1.1, which contain a more in-depth discus-
sion of the potential richness of the architecture
of LinCG. We do not wish to say anything new
about the semantics or the tectogrammar of coor-
dination in the current discussion, so we will ex-
pend our time fleshing out the phenogrammatical
component of the framework, and it is to this topic
that we now turn.
</bodyText>
<subsectionHeader confidence="0.989089">
2.1 LinCG Phenogrammar
</subsectionHeader>
<bodyText confidence="0.999994210526316">
LinCG grammar rules take the form of tripartite
inference rules, indicating what operations take
place pointwise within each component of the
signs in question. There are two main gram-
mar rules, called application (App) for combining
signs, and abstraction (Abs) for creating the po-
tential for combination through hypothetical rea-
soning. Aside from the lexical entries given as
axioms of the theory, it is also possible to obtain
typed variables using the rule of axiom (Ax), and
we make use of this rule in the analysis of right
node raising found in section 3.4. While the tec-
togrammar of LinCG is based on a fragment of
linear logic, the phenogrammatical and semantic
components are based on higher order logic. Since
we are concerned only with the phenogrammatical
component here, we have chosen to simplify the
exposition by presenting only the phenogrammat-
ical part of the rules of application and abstraction:
</bodyText>
<equation confidence="0.986682833333333">
f : A ` f : A
Γ ` f : A → B Δ ` a : A
App
Γ, Δ ` (f a) : B
Abs
Γ ` λx : A. b : A → B
</equation>
<bodyText confidence="0.875587909090909">
We additionally stipulate the following familiar
axioms governing the conversion and reduction of
lambda terms:3
` λx : A. b = λy : A. [y/x]b (α-conversion)
` (λx. b a) = [a/x]b (β-reduction)
As is common to any number of Curryesque
frameworks, we encode the phenogrammatical
parts of LinCG signs with typed lambda terms
consisting of strings, and functions over strings.4
We axiomatize our theory of strings in the familiar
way:
</bodyText>
<listItem confidence="0.85484625">
` E : St
` ·: St → St → St
` ∀stu : St. s · (t · u) = (s · t) · u
` ∀s : St. E · s = s = s · E
</listItem>
<bodyText confidence="0.988053">
The first axiom asserts that the empty string E is
a string. The second axiom asserts that concate-
nation, written ·, is a (curried) binary function on
strings. The third axiom represents the fact that
concatenation is associative, and the fourth, that
the empty string is a two-sided identity for con-
catenation. Because of the associativity of con-
catenation, we will drop parentheses as a matter
of convention.
The phenogrammar of a typical LinCG sign will
resemble the following (with one complication to
be added shortly):
` λs. s · SNIVELED : St → St
Since we treat St as the only base type, we will
generally omit typing judgments in lambda terms
when no confusion will result. Furthermore, we
use SMALL CAPS to indicate that a particular con-
stant is a string. So, the preceding lexical entry
provides us with a function from some string s, to
strings, which concatenates the string SNIVELED
to the right of s.
</bodyText>
<subsectionHeader confidence="0.752076">
2.1.1 Phenominators
</subsectionHeader>
<bodyText confidence="0.999954">
The center of our analysis of coordination is
the notion of a phenominator (short for pheno-
combinator), a particular variety of typed lambda
term. Intuitively, phenominators serve the same
purpose for LinCG that bilinear (slash) types do
for Lambek categorial grammars. Specifically,
</bodyText>
<footnote confidence="0.873194666666667">
3The exact status of the rule of η-conversion with respect
to this framework is currently unclear, and since we do not
make use of it, we omit its discussion here.
4although other structures have been proposed, e.g. the
node sets found in Muskens (2001).
Ax
</footnote>
<page confidence="0.992716">
30
</page>
<bodyText confidence="0.980683727272727">
they encode the linearization structure of a func-
tor, that is, where arguments may eventually oc-
cur with respect to its string support. To put it an-
other way, a phenominator describes the structure
a functor “projects”, in terms of linear order.
From a technical standpoint, we would like to
define a phenominator as a closed monoidal linear
lambda term, i.e. a term containing no constants
other than concatenation and the empty string.
The idea is that phenominators are the terms of
the higher order theory of monoids, and they in
some ways describe the abstract “shape” of pos-
sible string functions. For those accustomed to
thinking of “syntax” as being word order, then
phenominators can be thought of as a kind of syn-
tactic combinator. In practice, we will make use
only of what we call the unary phenominators, the
types of which we will refer to using the sort Φ
(with co used by custom as a metavariable over
unary phenominators, i.e. terms whose type is in
Φ). These are not unary in the strict sense, but they
will have as their centerpiece one particular string
variable, which will be bound with the highest
scope. We will generally abbreviate phenomina-
tors by the construction with which they are most
commonly associated: VP for verb phrases and
intransitive verbs, TV for transitive verbs, DTV
for ditransitive verbs, QNP for quantified noun
phrases, and RNR for right node raising construc-
tions. Here are examples of some of the most com-
mon phenominators we will make use of and the
abbreviations we customarily use for them:
Phenominator Abbreviation
</bodyText>
<equation confidence="0.898202166666667">
As.s (omitted)
Avs.s · v VP
Avst.t · v · s TV
Avstu.u · v · s · t DTV
AvP.(P v) QNP
Avs.v · s RNR
</equation>
<bodyText confidence="0.999855304347826">
As indicated previously, the first argument of a
phenominator always corresponds to what we re-
fer to (after Oehrle (1995)) as the string support
of a particular term. With the first argument dis-
pensed with, we have chosen the argument order
of the phenominators out of general concern for
what we perceive to be fairly uncontroversial cat-
egorial analyses of English grammatical phenom-
ena. That is, transitive verbs take their object ar-
guments first, and then their subject arguments, di-
transitives take their first and second object argu-
ments, followed by their subject argument, etc. As
long as the arguments in question are immediately
adjacent to the string support at each successive
application, it is possible to permute them to some
extent without losing the general thrust of the anal-
ysis. For example, the choice to have transitive
verbs take their object arguments first is insignifi-
cant.5 Since strings are implicitly under the image
of the identity phenominator As.s, we will consis-
tently omit this subscript.
We will be able to define a function we call
say, so that it will have the following property:
</bodyText>
<listItem confidence="0.334906">
�- Vs : St.Vco : Φ.say (co s) = s
</listItem>
<bodyText confidence="0.984071707317073">
That is, say is a left inverse for unary phenomi-
nators.
The function say is defined recursively via cer-
tain objects we call vacuities. The idea of a vacu-
ity is that it be in some way an “empty argument”
to which a functional term may apply. If we are
dealing with functions taking string arguments, it
seems obvious that the vacuity on strings should
be the empty string c. If we are dealing with
second-order functions taking St —* St arguments,
for example, quantified noun phrases like every-
one, then the vacuity on St —* St should be the
identity function on strings, As.s. Higher vacuities
than these become more complicated, and defin-
ing all of the higher-order vacuities is not entirely
straightforward, as certain types are not guaran-
teed to have a unique vacuity. Fortunately, we can
do it for any higher-function taking as an argument
another function under the image of a phenomina-
tor – then the vacuity on such a function is just the
phenominator applied to the empty string.6 The
central idea is easily understood when one asks
what, say, a vacuous transitive verb sounds like.
The answer seems to be: by itself, nothing, but
it imposes a certain order on its arguments. One
practical application of this clause is in analyzing
so-called “argument cluster coordination”, where
this definition will ensure that the argument cluster
gets linearized in the correct manner. This analy-
sis is regrettably just outside the scope of the cur-
rent inquiry, though the notion of the phenomina-
5Since we believe it is possible to embed Lambek catego-
rial grammars in LinCG, this fact reflects that the calculus we
are dealing with is similar to the associative Lambek Calcu-
lus.
6A reviewer suggests that this concept may be related to
the “context passing representation” of Hughes (1995), and
the association of a nil term with its continuation with re-
spect to contexts is assuredly evocative of the association of
the vacuity on a phenominator-indexed type with the contin-
uation of a with respect to a phenominator.
</bodyText>
<page confidence="0.999458">
31
</page>
<bodyText confidence="0.9982636">
tor can be profitably employed to provide exactly
such an analysis by adopting and reinterpreting a
categorial account along the lines of the one given
in Dowty (1988).
We formally define vacuities as follows:
</bodyText>
<equation confidence="0.3074495">
vacSt→St =def As.s
vacτϕ =def (cp E)
</equation>
<bodyText confidence="0.598384">
The reader should note that as a special case of the
second clause, we have
</bodyText>
<equation confidence="0.6363764">
vacSt = vacStas.s = (As.s E) = E
This in turn enables us to define say:
saySt =def As.s`
sayτ1→τ2 =def Ak : T1 → T2. sayτ2(k vacτ1)
say(τ1→τ2)ϕ =def sayτ1→τ2
</equation>
<bodyText confidence="0.99752175">
For an expedient example, we can apply say to
our putative lexical entry from earlier, and verify
that it will reduce to the string SNIVELED as de-
sired:
</bodyText>
<equation confidence="0.875699142857143">
saySt→St As. s · SNIVELED
= Ak : St → St.
(saySt(k vacSt)) As. s · SNIVELED
= saySt (As. s · SNIVELED vacSt)
= saySt (As. s · SNIVELED E)
= saySt E · SNIVELED
= saySt SNIVELED
</equation>
<sectionHeader confidence="0.7248905" genericHeader="method">
= As.s SNIVELED
= SNIVELED
</sectionHeader>
<subsectionHeader confidence="0.853588">
2.1.2 Subtyping by unary phenominators
</subsectionHeader>
<bodyText confidence="0.999966866666667">
In order to augment our type theory with the
relevant subtypes, we turn to Lambek and Scott
(1986), who hold that one way to do subtyping is
by defining predicates that amount to the charac-
teristic function of the particular subtype in ques-
tion, and then ensuring that these predicates meet
certain axioms embedding the subtype into the su-
pertype. We will be able to write such predicates
using phenominators. A unary phenominator is
one which has under its image a function whose
string support is a single contiguous string. With
this idea in place, we are able to assign subtypes
to functional types in the following way.
For T a (functional) type, we write Tϕ (with cp a
phenominator) as shorthand for Tϕ,, where:
</bodyText>
<equation confidence="0.814825">
cp0 = Af : T. ∃s : St.f = (cp s)
</equation>
<bodyText confidence="0.9987535">
Then cp0 constitutes a subtyping predicate in the
manner of Lambek and Scott (1986). For example,
let T = St → St and cp = Avs.s·v. Let us consider
the following (putative) lexical entry (pheno only):
</bodyText>
<equation confidence="0.8347442">
` As0. s0 · SNIVELED : (St → St)VP
Then our typing is justified along the following
lines:
Tϕ ::= (St → St)VP
::= (St → St)λvs.s·v
</equation>
<bodyText confidence="0.899854333333333">
::= (St → St)λf:St→St. ∃t:St.f=(λvs.s·v t)
So applying the subtyping predicate to the term in
question, we have
</bodyText>
<equation confidence="0.9008458">
(Af : St → St. ∃t : St.
f = (Avs.s · v t) As0. s0 · SNIVELED)
= ∃t : St.As0. s0 · SNIVELED = (Avs.s · v t)
= ∃t : St.As0. s0 · SNIVELED = As. s · t
= ∃t : St.As. s · SNIVELED = As. s · t
</equation>
<bodyText confidence="0.9992725">
which is true with t = SNIVELED, and the term is
shown to be well-typed.
</bodyText>
<sectionHeader confidence="0.99697" genericHeader="evaluation">
3 Analysis
</sectionHeader>
<bodyText confidence="0.999987913043478">
The basic strategy underlying our analysis of coor-
dination is that in order to coordinate two linguis-
tic signs, we need to track two things: their lin-
earization structure, and their string support. If we
have access to the linearization structure of each
conjunct, then we can check to see that it is the
same, and the signs are compatible for coordina-
tion. Furthermore, we will be able to maintain this
structure independent of the actual string support
of the individual signs.
Phenominators simultaneously allow us to
check the linearization structure of coordination
candidates and to reconstruct the relevant lin-
earization functions after coordination has taken
place. The function say addresses the second
point. For a given sign, we can apply say to it
in order to retrieve its string support. Then, we
will be able to directly coordinate the resulting
strings by concatenating them with a conjunction
in between. Finally, we can apply the phenomi-
nator to the resulting string and retrieve the new
linearization function, containing the entire coor-
dinate structure as its string support.
</bodyText>
<subsectionHeader confidence="0.999232">
3.1 Lexical entries
</subsectionHeader>
<bodyText confidence="0.9999385">
In LinCG, lexical entries constitute the (nonlogi-
cal) axioms of the proof theory. First we consider
the simplest elements of our fragment, the phenos
for the proper names Joffrey, Tyrion, and Tywin:
</bodyText>
<listItem confidence="0.955523666666667">
(4) a. ` JOFFREY : St
b. ` TYRION : St
c. ` TYWIN : St
</listItem>
<bodyText confidence="0.992076">
Next, we consider the intransitive verbs drank,
sniveled and whined. :
</bodyText>
<page confidence="0.99482">
32
</page>
<listItem confidence="0.743068">
(5) a. ` λs. s · DRANK: (St → St)VP
b. ` λs. s · SNIVELED : (St → St)VP
c. ` λs. s · WHINED : (St → St)VP
</listItem>
<bodyText confidence="0.999545333333333">
Each of these is a function from strings to strings,
seeking to linearize its ‘subject’ string argument to
the left of the verb. They are under the image of
the “verb phrase” phenominator λvs.s · v.
The transitive verbs chastised and slapped seek
to linearize their first string argument to the right,
resulting in a function under the image of the VP
phenominator, and their second argument to the
left, resulting in a string.
</bodyText>
<listItem confidence="0.9985705">
(6) a. ` λst. t · CHASTISED · s
: (St → St → St)TV
b. ` λst. t · SLAPPED · s
: (St → St → St)TV
</listItem>
<bodyText confidence="0.999397714285714">
Technically, this type could be written (St →
(St → St)VP)TV, but for the purposes of coordina-
tion, the present is sufficient. Each of these entries
is under the image of the “transitive verb” phe-
nominator λvst.t · v · s.
Finally, we come to the lexical entry schema for
and:
</bodyText>
<equation confidence="0.997888333333333">
(7) ` λc1 : τϕ. λc2 : τϕ.
ϕ ((sayτ, c2) · AND · (sayτ, c1))
: τϕ → τϕ → τϕ
</equation>
<bodyText confidence="0.999986428571429">
We note first that it takes two arguments of iden-
tical types τ, and furthermore that these must be
under the image of the same phenominator ϕ. It
then returns an expression of the same subtype.7
This mechanism bears more detailed examination.
First, each conjunct is subjected to the function
say, which, given its type, will return the string
support of the conjunct. Then, the resulting strings
are concatenated to either side of the string AND.
Finally, the phenominator of each argument is ap-
plied to the resulting string, creating a function
identical to the linearization functions of each of
the conjuncts, except with the coordinated string
in the relevant position.
</bodyText>
<subsectionHeader confidence="0.99981">
3.2 String coordination
</subsectionHeader>
<bodyText confidence="0.998616">
String coordination is direct and straightforward.
Since string-typed terms are under the image of
</bodyText>
<footnote confidence="0.847679666666667">
7Since ϕ occurs within both the body of the term and the
subtyping predicate, we note that this effectively takes us into
the realm of dependent types. Making the type theory of the
phenogrammar precise is an ongoing area of research, and we
are aware that constraining the type system is of paramount
importance for computational tractability.
</footnote>
<bodyText confidence="0.9993481875">
the identity phenominator, and since saySt is also
defined to be the identity on strings, the lexical en-
try we obtain for and simply concatenates each
argument string to either side of the string AND.
We give the full term reduction here, although this
version of and can be shown to be equal to the fol-
lowing:
` λc1c2 : St. c2 · AND · c1 : St → St → St
Since our terms at times become rather large,
we will adopt a convention where proof trees are
given with numerical indexes instead of sequents,
with the corresponding sequents following below
(at times on multiple lines). We will from time
to time elide multiple steps of reduction, noting in
passing the relevant definitions to consider when
reconstructing the proof.
</bodyText>
<equation confidence="0.688774846153846">
1 2
3 4
6 5
7
1. ` λc1 : St. λc2 : St.
λs.s ((saySt c2) · AND · (saySt c1))
: St → St → St
2. ` JOFFREY : St
3. ` λc2 : St.
λs.s ((saySt c2) · AND · (saySt JOFFREY))
= λc2 : St.
λs.s ((saySt c2) · AND · (λs.s JOFFREY))
= λc2 : St. λs.s ((saySt c2)·AND·JOFFREY)
</equation>
<listItem confidence="0.957091222222222">
: St → St
4. ` TYRION : St
5. ` λs.s ((saySt TYRION) · AND · JOFFREY) :
St
= λs.s ((λs.s TYRION)·AND·JOFFREY) : St
= λs.s (TYRION · AND · JOFFREY) : St
= TYRION · AND · JOFFREY : St
6. ` λs. s · DRANK: (St → St)VP
7. ` TYRION · AND · JOFFREY · DRANK: St
</listItem>
<subsectionHeader confidence="0.872353">
3.3 Functor coordination
</subsectionHeader>
<bodyText confidence="0.832046727272727">
Here, in order to understand the term appearing
in each conjunct, it is helpful to notice that the
following equality holds (with f a function from
strings to strings, under the image of the VP phe-
nominator):
f : (St → St)VP ` say(St→St)VP f
= saySt→St f
= saySt (f vacSt)
= saySt (f 0
= λs.s (f c)
=(f6):St
</bodyText>
<page confidence="0.997573">
33
</page>
<bodyText confidence="0.999570583333333">
This says that to coordinate VPs, we will first need
to reduce them to their string support by feeding
their linearization functions the empty string. For
the sake of brevity, this term reduction will be
elided from steps 5 and 8 in the derivations be-
low. Steps 2 and 6 constitute the hypothesizing
and subsequent withdrawal of an ‘object’ string
argument t0, as do steps 10 and 14 (s0). Format-
ting restrictions prohibit rule-labeling on the proof
trees, so we note that these are each instances of
the rules of axiom (Ax) and abstraction (Abs), re-
spectively.
</bodyText>
<figure confidence="0.793511833333333">
1 2
3 4
5 6
7
1. ` λc1 : (St → St)VP. λc2 : (St → St)VP.
λvs.s · v
</figure>
<listItem confidence="0.980008894736842">
((say(St→St)VP c2) · AND · (say(St→St)VP c1))
: (St → St)VP → (St → St)VP → (St → St)VP
2. ` λs. s · SNIVELED : (St → St)VP
3. ` λc2 : (St → St)VP. λvs.s · v
((say(St→St)VP c2) · AND
· (say(St→St)VP λs. s · SNIVELED))
...
= λc2 : (St → St)VP. λvs.s · v
((say(St→St)VP c2) · AND · SNIVELED)
: (St → St)VP → (St → St)VP
4. ` λs. s · WHINED : (St → St)VP
5. ` λvs.s · v ((say(St→St)VP λs. s · WHINED)
· AND · SNIVELED)
...
= (λvs.s · v WHINED · AND · SNIVELED)
= λs. s · WHINED · AND · SNIVELED
: (St → St)VP
6. ` JOFFREY : St
7. ` JOFFREY · WHINED · AND · SNIVELED: St
</listItem>
<subsectionHeader confidence="0.993244">
3.4 Right node raising
</subsectionHeader>
<bodyText confidence="0.963920413793104">
In the end, ‘right node raising’ constructions prove
only to be a special case of functor coordination.
The key here is the licensing of the ‘rightward-
looking’ functors, which are under the image of
the phenominator λvs.v · s. As was the case with
the ‘leftward-looking’ functor coordination exam-
ple in section 3.3, this analysis is essentially the
same as the well-known Lambek categorial gram-
mar analysis originating in Steedman (1985) and
continuing in Dowty (1988) and Morrill (1994).
The difference is that we encode directionality in
the phenominator, rather than in the type. Since
our system does not include function composition
as a rule, but as a theorem, we will need to make
use of hypothetical reasoning in order to permute
the order of the string arguments in order to con-
struct expressions with the correct structure.8
As was the case with the functor coordina-
tion example in section 3.3, applying say to the
conjuncts passes them the empty string, reducing
them to their string support, as shown here:
f : (St → St)RNR ` sa y(St→St)RNR f
= saySt→St f
= saySt (f vacSt)
= saySt (f 0
= λs.s (f c)
= (f 6):St
As before, this reduction is elided in the proof
given below, occurring in steps 8 and 15.
</bodyText>
<figure confidence="0.982648666666667">
1 2
3 4 9 10
11 12
8
15 16
17
</figure>
<listItem confidence="0.965927090909091">
1. ` λst. t · CHASTISED · s : (St → St → St)TV
2. t0 : St ` t0 : St
3. t0 : St ` λt. t · CHASTISED · t0 : (St → St)VP
4. ` TYWIN : St
5. t0 : St ` TYWIN · CHASTISED · t0 : St
6. ` λt0. TYWIN · CHASTISED ·t0 : (St → St)RNR
7. ` λc1 : (St → St)RNR. λc2 : (St → St)RNR.
λvs.v · s ((say(St→St)RNR c2)
· AND · (say(St→St)RNR c1))
: (St → St)RNR → (St → St)RNR
→ (St → St)RNR
</listItem>
<equation confidence="0.491797666666667">
8. ` λc2 : (St → St)RNR. λvs.v · s
((say(St→St)RNR c2) · AND
·(say(St→St)RNR λt0. TYWIN·CHASTISED·t0))
</equation>
<bodyText confidence="0.932050555555556">
...
= λc2 : (St → St)RNR. λvs.v · s
8Regrettably, space constraints prohibit a discussion veri-
fying the typing for the ‘right node raised’ terms. The reader
can verify that the terms are in fact well-typed given the sub-
typing schema in section 2.1.2. It is possible to write infer-
ence rules that speak directly to the introduction and elimina-
tion of the relevant functional subtypes, but these are omitted
here for the sake of brevity.
</bodyText>
<figure confidence="0.90982388">
5
7 6
13
14
34
((say(St→St)RNR c2)
· AND · TYWIN · CHASTISED)
: (St → St)RNR → (St → St)RNR
9. ` Ast. t · SLAPPED · s : (St → St → St)TV
10. s0 : St ` s0 : St
11. s0 : St ` At. t · SLAPPED · s0 : (St → St)VP
12. ` TYRION : St
13. s0 : St ` TYRION · SLAPPED · s0 : St
14. ` As0. TYRION · SLAPPED · s0 : (St → St)RNR
15. ` Avs.v · s ((say(St→St)RNR
As0. TYRION · SLAPPED · s0)
· AND · TYWIN · CHASTISED)
...
= (Avs.v · s TYRION · SLAPPED
· AND · TYWIN · CHASTISED)
= As. TYRION · SLAPPED · AND
· TYWIN · CHASTISED · s : (St → St)RNR
16. ` JOFFREY : St
17. ` TYRION · SLAPPED · AND
· TYWIN · CHASTISED · JOFFREY : St
</figure>
<sectionHeader confidence="0.994939" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999924615384616">
We provide a brief introduction to the framework
of Linear Categorial Grammar (LinCG). One of
the primary strengths of categorial grammar in
general has been its ability to address coordina-
tion phenomena. Coordination presents a uniquely
particular problem for grammars which distin-
guish between structural combination (tectogram-
mar) and the actual linear order of the strings gen-
erated by such grammars (part of phenogrammar).
Due to the inability to distinguish ‘directionality’
in string functors within a standard typed lambda
calculus, a general analysis of coordination seems
difficult.
We have elaborated LinCG’s concept of
phenogrammar by introducing phenominators,
closed monoidal linear lambda terms. We have
shown how the recursive function say provides a
left inverse for unaryphenominators, and we have
defined a more general notion of an ‘empty cate-
gory’ known as a vacuity, which say is defined
in terms of. It is then possible to describe sub-
types of functional types suitable to make the rele-
vant distinctions. These technologies enable us to
give analyses of various coordination phenomena
in LinCG, extending the empirical coverage of the
framework.
</bodyText>
<subsectionHeader confidence="0.611779">
4.1 Future work
</subsectionHeader>
<bodyText confidence="0.999975857142857">
It is possible to give an analysis of argument clus-
ter coordination using phenominators, instantiat-
ing the lexical entry for and with T as the type
(St → St → St → St)DTV → (St → St)VP and cp as
AvPs. s·(PEEE)·v, and using hypothetical reason-
ing. Regrettably, the necessity of brevity prohibits
a detailed account here.
Given that phenominators provide access to the
structure of functional terms which concatenate
strings to the right and left of their string support,
it is our belief that any Lambek categorial gram-
mar analysis can be recast in LinCG by an algo-
rithmic translation of directional slash types into
phenominator-indexed functional phenotypes, and
we are currently in the process of evaluating a po-
tential translation algorithm from directional slash
types to phenominators. This should in turn pro-
vide us with most of the details necessary to de-
scribe a system which emulates the HTLCG of
Kubota and Levine (2012), which provides anal-
yses of various gapping phenomena, greatly in-
creasing the overall empirical coverage.
There are a number of coordination phenomena
that require modifications to the tectogrammatical
component. We would like to be able to analyze
unlike category coordinations like rich and an ex-
cellent cook in the manner of Bayer (1996), as well
as Morrill (1996), which would require the addi-
tion of some variety of sum types in the tectogram-
mar. Further muddying the waters is so-called “it-
erated” or “list” coordination, which requires the
ability to generate coordinate structures contain-
ing a number of conjuncts with no coordinating
conjunction, as in Thurston, Kim, and Steve.
It is our intent to extend the use of phenom-
inators to analyze intonation as well, and we
expect that they can be fruitfully employed to
give accounts of focus, association with focus,
contrastive topicalization, “in-situ” topicalization,
alternative questions, and any number of other
phenomena which are at least partially realized
prosodically.
</bodyText>
<sectionHeader confidence="0.998718" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.993879">
I am grateful to Carl Pollard, Bob Levine, Yusuke
Kubota, Manjuan Duan, Gerald Penn, the TTNLS
2014 committee, and two anonymous reviewers
for their comments. Any errors or misunderstand-
ings rest solely on the shoulders of the author.
</bodyText>
<page confidence="0.996932">
35
</page>
<bodyText confidence="0.462557571428571">
Reinhard Muskens. 2010. New Directions in Type-
Theoretic Grammars. Journal of Logic, Lan-
guage and Information, 19(2):129–136. DOI
10.1007/s10849-009-9114-9.
References
Samuel Bayer. 1996. The coordination of unlike cate-
gories. Language, pages 579–616.
</bodyText>
<reference confidence="0.999232362318841">
Haskell B. Curry. 1961. Some Logical Aspects of
Grammatical Structure. In Roman. O. Jakobson, ed-
itor, Structure of Language and its Mathematical As-
pects, pages 56–68. American Mathematical Soci-
ety.
Philippe de Groote and Sarah Maarek. 2007. Type-
theoretic Extensions of Abstract Categorial Gram-
mars. In Reinhard Muskens, editor, Proceedings
of Workshop on New Directions in Type-Theoretic
Grammars.
Philippe de Groote. 2001. Towards Abstract Catego-
rial Grammars. In Association for Computational
Linguistics, 39th Annual Meeting and 10th Confer-
ence of the European Chapter, Proceedings of the
Conference, pages 148–155.
David Dowty. 1988. Type raising, functional composi-
tion, and non-constituent conjunction. In Richard T.
Oehrle, Emmon Bach, and Deirdre Wheeler, editors,
Categorial Grammars and Natural Language Struc-
tures, volume 32 of Studies in Linguistics and Phi-
losophy, pages 153–197. Springer Netherlands.
John Hughes. 1995. The design of a pretty-printing li-
brary. In Advanced Functional Programming, pages
53–96. Springer.
Yusuke Kubota and Robert Levine. 2012. Gapping
as like-category coordination. In D. B´echet and
A. Dikovsky, editors, Logical Aspects of Computa-
tional Linguistics (LACL) 2012.
Yusuke Kubota. 2010. (In)flexibility of Constituency in
Japanese in Multi-Modal Categorial Grammar with
Structured Phonology. Ph.D. thesis, The Ohio State
University.
J. Lambek and P.J. Scott. 1986. Introduction to
higher order categorical logic. Cambridge Univer-
sity Press.
Scott Martin. 2013. The Dynamics of Sense and Impli-
cature. Ph.D. thesis, The Ohio State University.
Vedrana Mihalicek. 2012. Serbo-Croatian Word Or-
der: A Logical Approach. Ph.D. thesis, The Ohio
State University.
Glyn V. Morrill. 1994. Type Logical Grammar.
Kluwer Academic Publishers.
Glyn Morrill. 1996. Grammar and logic*. Theoria,
62(3):260–293.
Reinhard Muskens. 2001. Lambda grammars and the
syntax-semantics interface. In Proceedings of the
Thirteenth Amsterdam Colloquium, pages 150–155.
Universiteit van Amsterdam.
Richard Oehrle. 1994. Term-Labeled Categorial Type
Systems. Linguistics and Philosophy, 17:633–678.
Dick Oehrle. 1995. Some 3-dimensional systems of
labelled deduction. Logic Journal of the IGPL, 3(2-
3):429–448.
Carl Pollard and E. Allyn Smith. 2012. A unified
analysis of the same, phrasal comparatives, and su-
perlatives. In Anca Chereches, editor, Proceedings
of SALT 22, pages 307–325. eLanguage.
Carl Pollard. 2013. Agnostic Hyperintensional Se-
mantics. Synthese. to appear.
Aarne Ranta. 2004. Grammatical Framework: A
Type-Theoretical Grammar Formalism. Journal of
Functional Programming, 14:145–189.
Aarne Ranta. 2009. The GF resource grammar library.
Linguistic Issues in Language Technology, 2(1).
E. Allyn Smith. 2010. Correlational Comparison in
English. Ph.D. thesis, The Ohio State University.
Mark Steedman. 1985. Dependency and co¨ordination
in the grammar of dutch and english. Language,
pages 523–568.
</reference>
<page confidence="0.998905">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.593594">
<title confidence="0.999727">The Phenogrammar of Coordination</title>
<author confidence="0.999946">Chris Worth</author>
<affiliation confidence="0.9996575">Department of Linguistics The Ohio State University</affiliation>
<email confidence="0.999363">worth@ling.osu.edu</email>
<abstract confidence="0.999663833333333">Linear Categorial Grammar (LinCG) is a sign-based, Curryesque, relational, logical categorial grammar (CG) whose central architecture is based on linear logic. Curryesque grammars separate the abstract combinatorics (tectogrammar) of linguistic expressions from their concrete, audible representations (phenogrammar). Most of these grammars encode linear order in string-based lambda terms, in which there is no obvious way to distinguish right from left. Without some notion of directionality, grammars are unable to differentiate, say, subject and object for purposes of building functorial coordinate structures. We introduce the notion of a phenominator as a way to encode the term structure of a functor separately from its “string support”. This technology is then employed to analyze a range of coordination phenomena typically left unaddressed by Linear Logic-based Curryesque frameworks.</abstract>
<note confidence="0.7167105">1 Overview Flexibility to the notion of constituency in con-</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Haskell B Curry</author>
</authors>
<title>Some Logical Aspects of Grammatical Structure.</title>
<date>1961</date>
<booktitle>Structure of Language and its Mathematical Aspects,</booktitle>
<pages>56--68</pages>
<editor>In Roman. O. Jakobson, editor,</editor>
<publisher>American Mathematical Society.</publisher>
<contexts>
<context position="3016" citStr="Curry (1961)" startWordPosition="477" endWordPosition="478">(3) have more in common than not. Section 1 provides an overview of the data and and central issues surrounding an analysis of coordination in Curryesque grammars. Section 2 introduces the reader to the framework of LinCG, and presents the technical innovations at the heart of this paper. Section 3 gives lexical entries and derivations for the examples in section 1, and section 4 discusses our results and suggests some directions for research in the near future, with references following. 1.1 Curryesque grammars and Linear Categorial Grammar We take as our starting point the Curryesque (after Curry (1961)) tradition of categorial grammars, making particular reference to those originating with Oehrle (1994) and continuing with Abstract Categorial Grammar (ACG) of de Groote (2001), Muskens (2010)’s Lambda Grammar (λG), Kubota and Levine’s Hybrid Type-Logical Categorial Grammar (Kubota and Levine, 2012) and to a lesser extent the Grammatical Framework of Ranta (2004), and others. These dialects 28 Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 28–36, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics of catego</context>
</contexts>
<marker>Curry, 1961</marker>
<rawString>Haskell B. Curry. 1961. Some Logical Aspects of Grammatical Structure. In Roman. O. Jakobson, editor, Structure of Language and its Mathematical Aspects, pages 56–68. American Mathematical Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe de Groote</author>
<author>Sarah Maarek</author>
</authors>
<title>Typetheoretic Extensions of Abstract Categorial Grammars.</title>
<date>2007</date>
<booktitle>Proceedings of Workshop on New Directions in Type-Theoretic Grammars.</booktitle>
<editor>In Reinhard Muskens, editor,</editor>
<marker>de Groote, Maarek, 2007</marker>
<rawString>Philippe de Groote and Sarah Maarek. 2007. Typetheoretic Extensions of Abstract Categorial Grammars. In Reinhard Muskens, editor, Proceedings of Workshop on New Directions in Type-Theoretic Grammars.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe de Groote</author>
</authors>
<title>Towards Abstract Categorial Grammars.</title>
<date>2001</date>
<booktitle>In Association for Computational Linguistics, 39th Annual Meeting and 10th Conference of the European Chapter, Proceedings of the Conference,</booktitle>
<pages>148--155</pages>
<marker>de Groote, 2001</marker>
<rawString>Philippe de Groote. 2001. Towards Abstract Categorial Grammars. In Association for Computational Linguistics, 39th Annual Meeting and 10th Conference of the European Chapter, Proceedings of the Conference, pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dowty</author>
</authors>
<title>Type raising, functional composition, and non-constituent conjunction.</title>
<date>1988</date>
<booktitle>Categorial Grammars and Natural Language Structures, volume 32 of Studies in Linguistics and Philosophy,</booktitle>
<pages>153--197</pages>
<editor>In Richard T. Oehrle, Emmon Bach, and Deirdre Wheeler, editors,</editor>
<publisher>Springer Netherlands.</publisher>
<contexts>
<context position="17653" citStr="Dowty (1988)" startWordPosition="2939" endWordPosition="2940">is fact reflects that the calculus we are dealing with is similar to the associative Lambek Calculus. 6A reviewer suggests that this concept may be related to the “context passing representation” of Hughes (1995), and the association of a nil term with its continuation with respect to contexts is assuredly evocative of the association of the vacuity on a phenominator-indexed type with the continuation of a with respect to a phenominator. 31 tor can be profitably employed to provide exactly such an analysis by adopting and reinterpreting a categorial account along the lines of the one given in Dowty (1988). We formally define vacuities as follows: vacSt→St =def As.s vacτϕ =def (cp E) The reader should note that as a special case of the second clause, we have vacSt = vacStas.s = (As.s E) = E This in turn enables us to define say: saySt =def As.s` sayτ1→τ2 =def Ak : T1 → T2. sayτ2(k vacτ1) say(τ1→τ2)ϕ =def sayτ1→τ2 For an expedient example, we can apply say to our putative lexical entry from earlier, and verify that it will reduce to the string SNIVELED as desired: saySt→St As. s · SNIVELED = Ak : St → St. (saySt(k vacSt)) As. s · SNIVELED = saySt (As. s · SNIVELED vacSt) = saySt (As. s · SNIVELE</context>
<context position="26655" citStr="Dowty (1988)" startWordPosition="4651" endWordPosition="4652">· SNIVELED) = λs. s · WHINED · AND · SNIVELED : (St → St)VP 6. ` JOFFREY : St 7. ` JOFFREY · WHINED · AND · SNIVELED: St 3.4 Right node raising In the end, ‘right node raising’ constructions prove only to be a special case of functor coordination. The key here is the licensing of the ‘rightwardlooking’ functors, which are under the image of the phenominator λvs.v · s. As was the case with the ‘leftward-looking’ functor coordination example in section 3.3, this analysis is essentially the same as the well-known Lambek categorial grammar analysis originating in Steedman (1985) and continuing in Dowty (1988) and Morrill (1994). The difference is that we encode directionality in the phenominator, rather than in the type. Since our system does not include function composition as a rule, but as a theorem, we will need to make use of hypothetical reasoning in order to permute the order of the string arguments in order to construct expressions with the correct structure.8 As was the case with the functor coordination example in section 3.3, applying say to the conjuncts passes them the empty string, reducing them to their string support, as shown here: f : (St → St)RNR ` sa y(St→St)RNR f = saySt→St f </context>
</contexts>
<marker>Dowty, 1988</marker>
<rawString>David Dowty. 1988. Type raising, functional composition, and non-constituent conjunction. In Richard T. Oehrle, Emmon Bach, and Deirdre Wheeler, editors, Categorial Grammars and Natural Language Structures, volume 32 of Studies in Linguistics and Philosophy, pages 153–197. Springer Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hughes</author>
</authors>
<title>The design of a pretty-printing library.</title>
<date>1995</date>
<booktitle>In Advanced Functional Programming,</booktitle>
<pages>53--96</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="17253" citStr="Hughes (1995)" startWordPosition="2871" endWordPosition="2872">arguments. One practical application of this clause is in analyzing so-called “argument cluster coordination”, where this definition will ensure that the argument cluster gets linearized in the correct manner. This analysis is regrettably just outside the scope of the current inquiry, though the notion of the phenomina5Since we believe it is possible to embed Lambek categorial grammars in LinCG, this fact reflects that the calculus we are dealing with is similar to the associative Lambek Calculus. 6A reviewer suggests that this concept may be related to the “context passing representation” of Hughes (1995), and the association of a nil term with its continuation with respect to contexts is assuredly evocative of the association of the vacuity on a phenominator-indexed type with the continuation of a with respect to a phenominator. 31 tor can be profitably employed to provide exactly such an analysis by adopting and reinterpreting a categorial account along the lines of the one given in Dowty (1988). We formally define vacuities as follows: vacSt→St =def As.s vacτϕ =def (cp E) The reader should note that as a special case of the second clause, we have vacSt = vacStas.s = (As.s E) = E This in tur</context>
</contexts>
<marker>Hughes, 1995</marker>
<rawString>John Hughes. 1995. The design of a pretty-printing library. In Advanced Functional Programming, pages 53–96. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Kubota</author>
<author>Robert Levine</author>
</authors>
<title>Gapping as like-category coordination.</title>
<date>2012</date>
<booktitle>Logical Aspects of Computational Linguistics (LACL)</booktitle>
<editor>In D. B´echet and A. Dikovsky, editors,</editor>
<contexts>
<context position="3317" citStr="Kubota and Levine, 2012" startWordPosition="517" endWordPosition="520"> Section 3 gives lexical entries and derivations for the examples in section 1, and section 4 discusses our results and suggests some directions for research in the near future, with references following. 1.1 Curryesque grammars and Linear Categorial Grammar We take as our starting point the Curryesque (after Curry (1961)) tradition of categorial grammars, making particular reference to those originating with Oehrle (1994) and continuing with Abstract Categorial Grammar (ACG) of de Groote (2001), Muskens (2010)’s Lambda Grammar (λG), Kubota and Levine’s Hybrid Type-Logical Categorial Grammar (Kubota and Levine, 2012) and to a lesser extent the Grammatical Framework of Ranta (2004), and others. These dialects 28 Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 28–36, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics of categorial grammar make a distinction between Tectogrammar, or “abstract syntax”, and Phenogrammar, or “concrete syntax”. Tectogrammar is primarily concerned with the structural properties of grammar, among them cooccurrence, case, agreement, tense, and so forth. Phenogrammar is concerned with computing a </context>
<context position="31082" citStr="Kubota and Levine (2012)" startWordPosition="5488" endWordPosition="5491">account here. Given that phenominators provide access to the structure of functional terms which concatenate strings to the right and left of their string support, it is our belief that any Lambek categorial grammar analysis can be recast in LinCG by an algorithmic translation of directional slash types into phenominator-indexed functional phenotypes, and we are currently in the process of evaluating a potential translation algorithm from directional slash types to phenominators. This should in turn provide us with most of the details necessary to describe a system which emulates the HTLCG of Kubota and Levine (2012), which provides analyses of various gapping phenomena, greatly increasing the overall empirical coverage. There are a number of coordination phenomena that require modifications to the tectogrammatical component. We would like to be able to analyze unlike category coordinations like rich and an excellent cook in the manner of Bayer (1996), as well as Morrill (1996), which would require the addition of some variety of sum types in the tectogrammar. Further muddying the waters is so-called “iterated” or “list” coordination, which requires the ability to generate coordinate structures containing</context>
</contexts>
<marker>Kubota, Levine, 2012</marker>
<rawString>Yusuke Kubota and Robert Levine. 2012. Gapping as like-category coordination. In D. B´echet and A. Dikovsky, editors, Logical Aspects of Computational Linguistics (LACL) 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Kubota</author>
</authors>
<title>(In)flexibility of Constituency in Japanese in Multi-Modal Categorial Grammar with Structured Phonology.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>The Ohio State University.</institution>
<contexts>
<context position="8336" citStr="Kubota (2010)" startWordPosition="1315" endWordPosition="1316">le (1995) calls their string support, that is, the string constants which make up the body of a particular functional term, and the linearization structure such functors impose on their arguments. 2 Linear Categorial Grammar (LinCG) Curryesque grammars separate the notion of linear order from the abstract combinatorics of linguis1A noteworthy exception is Ranta’s Grammatical Framework (GF), explored in, e.g. Ranta (2004) and Ranta (2009). GF also makes distinctions between tectogrammar and phenogrammar, though it has a somewhat different conception of each. 2A problem explicitly recognized by Kubota (2010) in section 3.2.1. 29 Γ,x : A ` b : B tic expressions, and as such base their tectogrammars around logics other than bilinear logic; the Grammatical Framework is based on Martin-L¨of type theory, and LinCG and its cousins ACG and λG use linear logic. Linear logic is generally described as being “resource-sensitive”, owing to the lack of the structural rules of weakening and contraction. Resource sensitivity is an attractive notion, theoretically, since it allows us to describe processes of resource production, consumption, and combination in a manner which is agnostic about precisely how resou</context>
</contexts>
<marker>Kubota, 2010</marker>
<rawString>Yusuke Kubota. 2010. (In)flexibility of Constituency in Japanese in Multi-Modal Categorial Grammar with Structured Phonology. Ph.D. thesis, The Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lambek</author>
<author>P J Scott</author>
</authors>
<title>Introduction to higher order categorical logic.</title>
<date>1986</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="18460" citStr="Lambek and Scott (1986)" startWordPosition="3091" endWordPosition="3094"> = E This in turn enables us to define say: saySt =def As.s` sayτ1→τ2 =def Ak : T1 → T2. sayτ2(k vacτ1) say(τ1→τ2)ϕ =def sayτ1→τ2 For an expedient example, we can apply say to our putative lexical entry from earlier, and verify that it will reduce to the string SNIVELED as desired: saySt→St As. s · SNIVELED = Ak : St → St. (saySt(k vacSt)) As. s · SNIVELED = saySt (As. s · SNIVELED vacSt) = saySt (As. s · SNIVELED E) = saySt E · SNIVELED = saySt SNIVELED = As.s SNIVELED = SNIVELED 2.1.2 Subtyping by unary phenominators In order to augment our type theory with the relevant subtypes, we turn to Lambek and Scott (1986), who hold that one way to do subtyping is by defining predicates that amount to the characteristic function of the particular subtype in question, and then ensuring that these predicates meet certain axioms embedding the subtype into the supertype. We will be able to write such predicates using phenominators. A unary phenominator is one which has under its image a function whose string support is a single contiguous string. With this idea in place, we are able to assign subtypes to functional types in the following way. For T a (functional) type, we write Tϕ (with cp a phenominator) as shorth</context>
</contexts>
<marker>Lambek, Scott, 1986</marker>
<rawString>J. Lambek and P.J. Scott. 1986. Introduction to higher order categorical logic. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Martin</author>
</authors>
<title>The Dynamics of Sense and Implicature.</title>
<date>2013</date>
<tech>Ph.D. thesis,</tech>
<institution>The Ohio State University.</institution>
<contexts>
<context position="4489" citStr="Martin (2013)" startWordPosition="687" endWordPosition="688">enogrammar is concerned with computing a pre-phonological representation of what will eventually be produced by the speaker, and encompasses word order, morphology, prosody, and the like. Linear Categorial Grammar (LinCG) is a signbased, Curryesque, relational, logical categorial grammar whose central architecture is based on linear logic. Abbreviatory overlap has been a regrettably persistent problem, and LinCG is the same in essence as the framework varyingly called Linear Grammar (LG) and Pheno-TectoDifferentiated Categorial Grammar (PTDCG), and developed in Smith (2010), Mihalicek (2012), Martin (2013), Pollard and Smith (2012), and Pollard (2013). In LinCG, the syntax-phonology and syntax-semantics interfaces amount to noting that the logics for the phenogrammar, the tectogrammar, and the semantics operate in parallel. This stands in contrast to ‘syntactocentric’ theories of grammar, where syntax is taken to be the fundamental domain within which expressions combine, and then phonology and semantics are ‘read off’ of the syntactic representation. LinCG is conceptually different in that it has relational, rather than functional, interfaces between the three components of the grammar. Since </context>
</contexts>
<marker>Martin, 2013</marker>
<rawString>Scott Martin. 2013. The Dynamics of Sense and Implicature. Ph.D. thesis, The Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vedrana Mihalicek</author>
</authors>
<title>Serbo-Croatian Word Order: A Logical Approach.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>The Ohio State University.</institution>
<contexts>
<context position="4474" citStr="Mihalicek (2012)" startWordPosition="685" endWordPosition="686">, and so forth. Phenogrammar is concerned with computing a pre-phonological representation of what will eventually be produced by the speaker, and encompasses word order, morphology, prosody, and the like. Linear Categorial Grammar (LinCG) is a signbased, Curryesque, relational, logical categorial grammar whose central architecture is based on linear logic. Abbreviatory overlap has been a regrettably persistent problem, and LinCG is the same in essence as the framework varyingly called Linear Grammar (LG) and Pheno-TectoDifferentiated Categorial Grammar (PTDCG), and developed in Smith (2010), Mihalicek (2012), Martin (2013), Pollard and Smith (2012), and Pollard (2013). In LinCG, the syntax-phonology and syntax-semantics interfaces amount to noting that the logics for the phenogrammar, the tectogrammar, and the semantics operate in parallel. This stands in contrast to ‘syntactocentric’ theories of grammar, where syntax is taken to be the fundamental domain within which expressions combine, and then phonology and semantics are ‘read off’ of the syntactic representation. LinCG is conceptually different in that it has relational, rather than functional, interfaces between the three components of the </context>
</contexts>
<marker>Mihalicek, 2012</marker>
<rawString>Vedrana Mihalicek. 2012. Serbo-Croatian Word Order: A Logical Approach. Ph.D. thesis, The Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glyn V Morrill</author>
</authors>
<title>Type Logical Grammar.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="26674" citStr="Morrill (1994)" startWordPosition="4654" endWordPosition="4655"> s · WHINED · AND · SNIVELED : (St → St)VP 6. ` JOFFREY : St 7. ` JOFFREY · WHINED · AND · SNIVELED: St 3.4 Right node raising In the end, ‘right node raising’ constructions prove only to be a special case of functor coordination. The key here is the licensing of the ‘rightwardlooking’ functors, which are under the image of the phenominator λvs.v · s. As was the case with the ‘leftward-looking’ functor coordination example in section 3.3, this analysis is essentially the same as the well-known Lambek categorial grammar analysis originating in Steedman (1985) and continuing in Dowty (1988) and Morrill (1994). The difference is that we encode directionality in the phenominator, rather than in the type. Since our system does not include function composition as a rule, but as a theorem, we will need to make use of hypothetical reasoning in order to permute the order of the string arguments in order to construct expressions with the correct structure.8 As was the case with the functor coordination example in section 3.3, applying say to the conjuncts passes them the empty string, reducing them to their string support, as shown here: f : (St → St)RNR ` sa y(St→St)RNR f = saySt→St f = saySt (f vacSt) =</context>
</contexts>
<marker>Morrill, 1994</marker>
<rawString>Glyn V. Morrill. 1994. Type Logical Grammar. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glyn Morrill</author>
</authors>
<title>Grammar and logic*.</title>
<date>1996</date>
<journal>Theoria,</journal>
<volume>62</volume>
<issue>3</issue>
<contexts>
<context position="31450" citStr="Morrill (1996)" startWordPosition="5548" endWordPosition="5549">the process of evaluating a potential translation algorithm from directional slash types to phenominators. This should in turn provide us with most of the details necessary to describe a system which emulates the HTLCG of Kubota and Levine (2012), which provides analyses of various gapping phenomena, greatly increasing the overall empirical coverage. There are a number of coordination phenomena that require modifications to the tectogrammatical component. We would like to be able to analyze unlike category coordinations like rich and an excellent cook in the manner of Bayer (1996), as well as Morrill (1996), which would require the addition of some variety of sum types in the tectogrammar. Further muddying the waters is so-called “iterated” or “list” coordination, which requires the ability to generate coordinate structures containing a number of conjuncts with no coordinating conjunction, as in Thurston, Kim, and Steve. It is our intent to extend the use of phenominators to analyze intonation as well, and we expect that they can be fruitfully employed to give accounts of focus, association with focus, contrastive topicalization, “in-situ” topicalization, alternative questions, and any number of</context>
</contexts>
<marker>Morrill, 1996</marker>
<rawString>Glyn Morrill. 1996. Grammar and logic*. Theoria, 62(3):260–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Muskens</author>
</authors>
<title>Lambda grammars and the syntax-semantics interface.</title>
<date>2001</date>
<booktitle>In Proceedings of the Thirteenth Amsterdam Colloquium,</booktitle>
<pages>150--155</pages>
<location>Universiteit van Amsterdam.</location>
<contexts>
<context position="12641" citStr="Muskens (2001)" startWordPosition="2075" endWordPosition="2076">hich concatenates the string SNIVELED to the right of s. 2.1.1 Phenominators The center of our analysis of coordination is the notion of a phenominator (short for phenocombinator), a particular variety of typed lambda term. Intuitively, phenominators serve the same purpose for LinCG that bilinear (slash) types do for Lambek categorial grammars. Specifically, 3The exact status of the rule of η-conversion with respect to this framework is currently unclear, and since we do not make use of it, we omit its discussion here. 4although other structures have been proposed, e.g. the node sets found in Muskens (2001). Ax 30 they encode the linearization structure of a functor, that is, where arguments may eventually occur with respect to its string support. To put it another way, a phenominator describes the structure a functor “projects”, in terms of linear order. From a technical standpoint, we would like to define a phenominator as a closed monoidal linear lambda term, i.e. a term containing no constants other than concatenation and the empty string. The idea is that phenominators are the terms of the higher order theory of monoids, and they in some ways describe the abstract “shape” of possible string</context>
</contexts>
<marker>Muskens, 2001</marker>
<rawString>Reinhard Muskens. 2001. Lambda grammars and the syntax-semantics interface. In Proceedings of the Thirteenth Amsterdam Colloquium, pages 150–155. Universiteit van Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Oehrle</author>
</authors>
<title>Term-Labeled Categorial Type Systems. Linguistics and Philosophy,</title>
<date>1994</date>
<pages>17--633</pages>
<contexts>
<context position="3119" citStr="Oehrle (1994)" startWordPosition="490" endWordPosition="491">rrounding an analysis of coordination in Curryesque grammars. Section 2 introduces the reader to the framework of LinCG, and presents the technical innovations at the heart of this paper. Section 3 gives lexical entries and derivations for the examples in section 1, and section 4 discusses our results and suggests some directions for research in the near future, with references following. 1.1 Curryesque grammars and Linear Categorial Grammar We take as our starting point the Curryesque (after Curry (1961)) tradition of categorial grammars, making particular reference to those originating with Oehrle (1994) and continuing with Abstract Categorial Grammar (ACG) of de Groote (2001), Muskens (2010)’s Lambda Grammar (λG), Kubota and Levine’s Hybrid Type-Logical Categorial Grammar (Kubota and Levine, 2012) and to a lesser extent the Grammatical Framework of Ranta (2004), and others. These dialects 28 Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 28–36, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics of categorial grammar make a distinction between Tectogrammar, or “abstract syntax”, and Phenogrammar, or “concr</context>
</contexts>
<marker>Oehrle, 1994</marker>
<rawString>Richard Oehrle. 1994. Term-Labeled Categorial Type Systems. Linguistics and Philosophy, 17:633–678.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dick Oehrle</author>
</authors>
<title>Some 3-dimensional systems of labelled deduction.</title>
<date>1995</date>
<journal>Logic Journal of the IGPL,</journal>
<pages>3--2</pages>
<contexts>
<context position="7732" citStr="Oehrle (1995)" startWordPosition="1225" endWordPosition="1226">ionality, grammars are unable to distinguish between, say, subject and object. This would seem to predict, for example, that As. s · SLAPPED · JOFFREY and As. TYRION · SLAPPED · s would have the same syntactic category (NP —o S in the tectogrammar, and St → St in the phenogrammar), and would thus be compatible under coordination, but this is generally not the case. What we need is a way to examine the structure of a lambda term independently of the specific string constants that comprise it. To put it another way, in order to coordinate functors, we need to be able to distinguish between what Oehrle (1995) calls their string support, that is, the string constants which make up the body of a particular functional term, and the linearization structure such functors impose on their arguments. 2 Linear Categorial Grammar (LinCG) Curryesque grammars separate the notion of linear order from the abstract combinatorics of linguis1A noteworthy exception is Ranta’s Grammatical Framework (GF), explored in, e.g. Ranta (2004) and Ranta (2009). GF also makes distinctions between tectogrammar and phenogrammar, though it has a somewhat different conception of each. 2A problem explicitly recognized by Kubota (2</context>
<context position="14437" citStr="Oehrle (1995)" startWordPosition="2391" endWordPosition="2392">he construction with which they are most commonly associated: VP for verb phrases and intransitive verbs, TV for transitive verbs, DTV for ditransitive verbs, QNP for quantified noun phrases, and RNR for right node raising constructions. Here are examples of some of the most common phenominators we will make use of and the abbreviations we customarily use for them: Phenominator Abbreviation As.s (omitted) Avs.s · v VP Avst.t · v · s TV Avstu.u · v · s · t DTV AvP.(P v) QNP Avs.v · s RNR As indicated previously, the first argument of a phenominator always corresponds to what we refer to (after Oehrle (1995)) as the string support of a particular term. With the first argument dispensed with, we have chosen the argument order of the phenominators out of general concern for what we perceive to be fairly uncontroversial categorial analyses of English grammatical phenomena. That is, transitive verbs take their object arguments first, and then their subject arguments, ditransitives take their first and second object arguments, followed by their subject argument, etc. As long as the arguments in question are immediately adjacent to the string support at each successive application, it is possible to pe</context>
</contexts>
<marker>Oehrle, 1995</marker>
<rawString>Dick Oehrle. 1995. Some 3-dimensional systems of labelled deduction. Logic Journal of the IGPL, 3(2-3):429–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>E Allyn Smith</author>
</authors>
<title>A unified analysis of the same, phrasal comparatives, and superlatives.</title>
<date>2012</date>
<booktitle>In Anca Chereches, editor, Proceedings of SALT 22,</booktitle>
<pages>307--325</pages>
<contexts>
<context position="4515" citStr="Pollard and Smith (2012)" startWordPosition="689" endWordPosition="692">oncerned with computing a pre-phonological representation of what will eventually be produced by the speaker, and encompasses word order, morphology, prosody, and the like. Linear Categorial Grammar (LinCG) is a signbased, Curryesque, relational, logical categorial grammar whose central architecture is based on linear logic. Abbreviatory overlap has been a regrettably persistent problem, and LinCG is the same in essence as the framework varyingly called Linear Grammar (LG) and Pheno-TectoDifferentiated Categorial Grammar (PTDCG), and developed in Smith (2010), Mihalicek (2012), Martin (2013), Pollard and Smith (2012), and Pollard (2013). In LinCG, the syntax-phonology and syntax-semantics interfaces amount to noting that the logics for the phenogrammar, the tectogrammar, and the semantics operate in parallel. This stands in contrast to ‘syntactocentric’ theories of grammar, where syntax is taken to be the fundamental domain within which expressions combine, and then phonology and semantics are ‘read off’ of the syntactic representation. LinCG is conceptually different in that it has relational, rather than functional, interfaces between the three components of the grammar. Since we do not interpret syntac</context>
</contexts>
<marker>Pollard, Smith, 2012</marker>
<rawString>Carl Pollard and E. Allyn Smith. 2012. A unified analysis of the same, phrasal comparatives, and superlatives. In Anca Chereches, editor, Proceedings of SALT 22, pages 307–325. eLanguage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
</authors>
<title>Agnostic Hyperintensional Semantics. Synthese.</title>
<date>2013</date>
<note>to appear.</note>
<contexts>
<context position="4535" citStr="Pollard (2013)" startWordPosition="694" endWordPosition="696">phonological representation of what will eventually be produced by the speaker, and encompasses word order, morphology, prosody, and the like. Linear Categorial Grammar (LinCG) is a signbased, Curryesque, relational, logical categorial grammar whose central architecture is based on linear logic. Abbreviatory overlap has been a regrettably persistent problem, and LinCG is the same in essence as the framework varyingly called Linear Grammar (LG) and Pheno-TectoDifferentiated Categorial Grammar (PTDCG), and developed in Smith (2010), Mihalicek (2012), Martin (2013), Pollard and Smith (2012), and Pollard (2013). In LinCG, the syntax-phonology and syntax-semantics interfaces amount to noting that the logics for the phenogrammar, the tectogrammar, and the semantics operate in parallel. This stands in contrast to ‘syntactocentric’ theories of grammar, where syntax is taken to be the fundamental domain within which expressions combine, and then phonology and semantics are ‘read off’ of the syntactic representation. LinCG is conceptually different in that it has relational, rather than functional, interfaces between the three components of the grammar. Since we do not interpret syntactic types into pheno</context>
</contexts>
<marker>Pollard, 2013</marker>
<rawString>Carl Pollard. 2013. Agnostic Hyperintensional Semantics. Synthese. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aarne Ranta</author>
</authors>
<title>Grammatical Framework: A Type-Theoretical Grammar Formalism.</title>
<date>2004</date>
<journal>Journal of Functional Programming,</journal>
<pages>14--145</pages>
<contexts>
<context position="3382" citStr="Ranta (2004)" startWordPosition="530" endWordPosition="531">1, and section 4 discusses our results and suggests some directions for research in the near future, with references following. 1.1 Curryesque grammars and Linear Categorial Grammar We take as our starting point the Curryesque (after Curry (1961)) tradition of categorial grammars, making particular reference to those originating with Oehrle (1994) and continuing with Abstract Categorial Grammar (ACG) of de Groote (2001), Muskens (2010)’s Lambda Grammar (λG), Kubota and Levine’s Hybrid Type-Logical Categorial Grammar (Kubota and Levine, 2012) and to a lesser extent the Grammatical Framework of Ranta (2004), and others. These dialects 28 Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 28–36, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics of categorial grammar make a distinction between Tectogrammar, or “abstract syntax”, and Phenogrammar, or “concrete syntax”. Tectogrammar is primarily concerned with the structural properties of grammar, among them cooccurrence, case, agreement, tense, and so forth. Phenogrammar is concerned with computing a pre-phonological representation of what will eventually be produc</context>
<context position="8147" citStr="Ranta (2004)" startWordPosition="1287" endWordPosition="1288">f a lambda term independently of the specific string constants that comprise it. To put it another way, in order to coordinate functors, we need to be able to distinguish between what Oehrle (1995) calls their string support, that is, the string constants which make up the body of a particular functional term, and the linearization structure such functors impose on their arguments. 2 Linear Categorial Grammar (LinCG) Curryesque grammars separate the notion of linear order from the abstract combinatorics of linguis1A noteworthy exception is Ranta’s Grammatical Framework (GF), explored in, e.g. Ranta (2004) and Ranta (2009). GF also makes distinctions between tectogrammar and phenogrammar, though it has a somewhat different conception of each. 2A problem explicitly recognized by Kubota (2010) in section 3.2.1. 29 Γ,x : A ` b : B tic expressions, and as such base their tectogrammars around logics other than bilinear logic; the Grammatical Framework is based on Martin-L¨of type theory, and LinCG and its cousins ACG and λG use linear logic. Linear logic is generally described as being “resource-sensitive”, owing to the lack of the structural rules of weakening and contraction. Resource sensitivity </context>
</contexts>
<marker>Ranta, 2004</marker>
<rawString>Aarne Ranta. 2004. Grammatical Framework: A Type-Theoretical Grammar Formalism. Journal of Functional Programming, 14:145–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aarne Ranta</author>
</authors>
<title>The GF resource grammar library.</title>
<date>2009</date>
<journal>Linguistic Issues in Language Technology,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="8164" citStr="Ranta (2009)" startWordPosition="1290" endWordPosition="1291">ndependently of the specific string constants that comprise it. To put it another way, in order to coordinate functors, we need to be able to distinguish between what Oehrle (1995) calls their string support, that is, the string constants which make up the body of a particular functional term, and the linearization structure such functors impose on their arguments. 2 Linear Categorial Grammar (LinCG) Curryesque grammars separate the notion of linear order from the abstract combinatorics of linguis1A noteworthy exception is Ranta’s Grammatical Framework (GF), explored in, e.g. Ranta (2004) and Ranta (2009). GF also makes distinctions between tectogrammar and phenogrammar, though it has a somewhat different conception of each. 2A problem explicitly recognized by Kubota (2010) in section 3.2.1. 29 Γ,x : A ` b : B tic expressions, and as such base their tectogrammars around logics other than bilinear logic; the Grammatical Framework is based on Martin-L¨of type theory, and LinCG and its cousins ACG and λG use linear logic. Linear logic is generally described as being “resource-sensitive”, owing to the lack of the structural rules of weakening and contraction. Resource sensitivity is an attractive </context>
</contexts>
<marker>Ranta, 2009</marker>
<rawString>Aarne Ranta. 2009. The GF resource grammar library. Linguistic Issues in Language Technology, 2(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Allyn Smith</author>
</authors>
<date>2010</date>
<booktitle>Correlational Comparison in English. Ph.D. thesis, The</booktitle>
<institution>Ohio State University.</institution>
<contexts>
<context position="4456" citStr="Smith (2010)" startWordPosition="683" endWordPosition="684">reement, tense, and so forth. Phenogrammar is concerned with computing a pre-phonological representation of what will eventually be produced by the speaker, and encompasses word order, morphology, prosody, and the like. Linear Categorial Grammar (LinCG) is a signbased, Curryesque, relational, logical categorial grammar whose central architecture is based on linear logic. Abbreviatory overlap has been a regrettably persistent problem, and LinCG is the same in essence as the framework varyingly called Linear Grammar (LG) and Pheno-TectoDifferentiated Categorial Grammar (PTDCG), and developed in Smith (2010), Mihalicek (2012), Martin (2013), Pollard and Smith (2012), and Pollard (2013). In LinCG, the syntax-phonology and syntax-semantics interfaces amount to noting that the logics for the phenogrammar, the tectogrammar, and the semantics operate in parallel. This stands in contrast to ‘syntactocentric’ theories of grammar, where syntax is taken to be the fundamental domain within which expressions combine, and then phonology and semantics are ‘read off’ of the syntactic representation. LinCG is conceptually different in that it has relational, rather than functional, interfaces between the three </context>
</contexts>
<marker>Smith, 2010</marker>
<rawString>E. Allyn Smith. 2010. Correlational Comparison in English. Ph.D. thesis, The Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Dependency and co¨ordination in the grammar of dutch and english. Language,</title>
<date>1985</date>
<pages>523--568</pages>
<contexts>
<context position="26624" citStr="Steedman (1985)" startWordPosition="4646" endWordPosition="4647">ED) ... = (λvs.s · v WHINED · AND · SNIVELED) = λs. s · WHINED · AND · SNIVELED : (St → St)VP 6. ` JOFFREY : St 7. ` JOFFREY · WHINED · AND · SNIVELED: St 3.4 Right node raising In the end, ‘right node raising’ constructions prove only to be a special case of functor coordination. The key here is the licensing of the ‘rightwardlooking’ functors, which are under the image of the phenominator λvs.v · s. As was the case with the ‘leftward-looking’ functor coordination example in section 3.3, this analysis is essentially the same as the well-known Lambek categorial grammar analysis originating in Steedman (1985) and continuing in Dowty (1988) and Morrill (1994). The difference is that we encode directionality in the phenominator, rather than in the type. Since our system does not include function composition as a rule, but as a theorem, we will need to make use of hypothetical reasoning in order to permute the order of the string arguments in order to construct expressions with the correct structure.8 As was the case with the functor coordination example in section 3.3, applying say to the conjuncts passes them the empty string, reducing them to their string support, as shown here: f : (St → St)RNR `</context>
</contexts>
<marker>Steedman, 1985</marker>
<rawString>Mark Steedman. 1985. Dependency and co¨ordination in the grammar of dutch and english. Language, pages 523–568.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
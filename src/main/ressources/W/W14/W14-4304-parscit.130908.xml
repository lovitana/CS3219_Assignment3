<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990017">
Situated Language Understanding at 25 Miles per Hour
</title>
<author confidence="0.999027">
Teruhisa Misu, Antoine Raux; Rakesh Gupta
</author>
<affiliation confidence="0.999597">
Honda Research Institute USA
</affiliation>
<address confidence="0.926981">
425 National Avenue
Mountain View, CA 94040
</address>
<email confidence="0.998761">
tmisu@hra.com
</email>
<note confidence="0.88302825">
Ian Lane
Carnegie Mellon University
NASA Ames Research Park
Moffett Field, CA 93085
</note>
<sectionHeader confidence="0.970351" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999934571428571">
In this paper, we address issues in situ-
ated language understanding in a rapidly
changing environment – a moving car.
Specifically, we propose methods for un-
derstanding user queries about specific tar-
get buildings in their surroundings. Unlike
previous studies on physically situated in-
teractions such as interaction with mobile
robots, the task is very sensitive to tim-
ing because the spatial relation between
the car and the target is changing while
the user is speaking. We collected situated
utterances from drivers using our research
system, Townsurfer, which is embedded
in a real vehicle. Based on this data, we
analyze the timing of user queries, spa-
tial relationships between the car and tar-
gets, head pose of the user, and linguis-
tic cues. Optimized on the data, our al-
gorithms improved the target identification
rate by 24.1% absolute.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988565392156863">
Recent advances in sensing technologies have en-
abled researchers to explore applications that re-
quire a clear awareness of the systems’ dynamic
context and physical surroundings. Such appli-
cations include multi-participant conversation sys-
tems (Bohus and Horvitz, 2009) and human-robot
interaction (Tellex et al., 2011; Sugiura et al.,
2011). The general problem of understanding and
interacting with human users in such environments
is referred to as situated interaction.
We address yet another environment, where sit-
uated interactions takes place – a moving car. In
the previous work, we collected over 60 hours of
in-car human-human interactions, where drivers
interact with an expert co-pilot sitting next to them
in the vehicle (Cohen et al., 2014). One of the
* Currently with Lenovo.
insights from the analysis on this corpus is that
drivers frequently use referring expressions about
their surroundings. (e.g. What is that big building
on the right?) Based on this insight, we have de-
veloped Townsurfer (Lane et al., 2012; Misu et
al., 2013), a situated in-car intelligent assistant.
Using geo-location information, the system can
answer user queries/questions that contain object
references about points-of-interest (POIs) in their
surroundings. We use driver (user) face orienta-
tion to understand their queries and provide the re-
quested information about the POI they are look-
ing at. We have previously demonstrated and eval-
uated the system in a simulated environment (Lane
et al., 2012). In this paper, we evaluate its utility
in real driving situations.
Compared to conventional situated dialog tasks,
query understanding in our task is expected to be
more time sensitive, due to the rapidly changing
environment while driving. Typically, a car will
move 10 meters in one second while driving at 25
mi/h. So timing can be a crucial factor. In addi-
tion, it is not well understood what kind of linguis-
tic cues are naturally provided by drivers, and their
contributions to situated language understanding
in such an environment. To the best of our knowl-
edge, this is the first study that tackles the issue of
situated language understanding in rapidly moving
vehicles.
In this paper, we first present an overview of the
Townsurfer in-car spoken dialog system (Section
2). Based on our data collection using the sys-
tem, we analyze user behavior while using the sys-
tem focusing on language understanding (Section
</bodyText>
<listItem confidence="0.9622755">
3). Specifically, we answer the following research
questions about the task and the system through
data collection and analysis:
1. Is timing an important factor of situated lan-
guage understanding?
2. Does head pose play an important role in lan-
guage understanding? Or is spatial distance
information enough?
</listItem>
<page confidence="0.984844">
22
</page>
<note confidence="0.851382">
Proceedings of the SIGDIAL 2014 Conference, pages 22–31,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.990407909090909">
POI identification
Sensors Sensor signal understanding
(situated understanding)
3) POI Posterior
calculation
by Belief tracking
2) POI score
(prior) calculation
1) (Candidate)
POI look-up
Semantic
geo-spatial
database
Understanding
result
(POI with
maximum
posterior)
Speech recognition
Speech
Microphone
Natural language
understanding
Depth sensor
IMU
GPS
Gaze (Head-pose)
estimation
Geo-location
estimation
Gaze
Geo-
location
</figure>
<figureCaption confidence="0.999254">
Figure 1: System overview of Townsurfer
</figureCaption>
<tableCaption confidence="0.953916">
Table 1: Example dialog with Townsurfer
</tableCaption>
<equation confidence="0.400097">
U1: What is that place. (POI in gaze)
</equation>
<bodyText confidence="0.850355105263158">
S1: This is Specialty Cafe, a mid-scale coffee
shop that serves sandwiches.
U2: What is its (POI in dialog history) rating.
S2: The rating of Specialty Cafe is above av-
erage.
U3: How about that one on the left.
(POI located on the left)
S3: This is Roger’s Deli, a low-priced restau-
rant that serves American food.
3. What is the role of linguistic cues in this task?
What kinds of linguistic cues do drivers nat-
urally provide?
Based on the hypothesis obtained from the analy-
sis for these questions, we propose methods to im-
prove situated language understanding (Section 4),
and analyze their contributions based on the col-
lected data (Sections 5 and 6). We then clarify our
research contributions through discussion (Section
7) and comparison with related studies (Section 8).
</bodyText>
<sectionHeader confidence="0.840056" genericHeader="introduction">
2 Architecture and Hardware of
Townsurfer
</sectionHeader>
<bodyText confidence="0.9903825625">
The system uses three main input modalities,
speech, geo-location, and head pose. Speech is
the main input modality of the system. It is used to
trigger interactions with the system. User speech
is recognized, then requested concepts/values are
extracted. Geo-location and head pose informa-
tion are used to understand the target POI of the
user query. An overview of the system with a pro-
cess flow is illustrated in Figure 1 and an exam-
ple dialog with the system is shown in Table 1. A
video of an example dialog is also attached.
In this paper, we address issues in identify-
ing user intended POI, which is a form of ref-
erence resolution using multi-modal information
sources&apos;. The POI identification process consists
of the following three steps (cf. Figure 1). This
is similar to but different from our previous work
on landmark-based destination setting (Ma et al.,
2012).
1) The system lists candidate POIs based on geo-
location at the timing of a driver query. Rela-
tive positions of POIs to the car are also cal-
culated based on geo-location and the head-
ing of the car.
2). Based on spatial linguistic cues in the user
utterance (e.g. to my right, on the left), a
2D scoring function is selected to identify ar-
eas where the target POI is likely to be. This
function takes into account the position of the
POI relative to the car, as well as driver head
pose. Scores for all candidate POIs are cal-
culated.
3) Posterior probabilities of each POI are cal-
culated using the score of step 2 as prior,
and non-spatial linguistic information (e.g.
POI categories, building properties) as obser-
vations. This posterior calculation is com-
puted using our Bayesian belief tracker called
DPOT (Raux and Ma, 2011).
The details are explained in Section 4.
System hardware consists of a 3D depth sen-
sor (Primesense Carmine 1.09), a USB GPS (BU-
353S4), an IMU sensor (3DM-GX3-25) and a
close talk microphone (plantronics Voyage Leg-
&apos;We do not deal with issues in language understanding
related to dialog history and query type. (e.g. General infor-
mation request such as U1 vs request about specific property
of POI such as U2 in Table 1)
</bodyText>
<page confidence="0.996851">
23
</page>
<bodyText confidence="0.998534333333333">
end UC). These consumer grade sensors are in-
stalled in our Honda Pilot experiment car. We
use Point Cloud Library (PCL) for the face direc-
tion estimation. Geo-location is estimated based
on Extended Kalman filter-based algorithm using
GPS and gyro information as input at 1.5 Hz. The
system is implemented based on the Robot Oper-
ating System ROS (Quigley et al., 2009). Each
component is implemented as a node of ROS, and
communications between the nodes are performed
using the standard message passing mechanisms
in ROS.
</bodyText>
<sectionHeader confidence="0.838912" genericHeader="method">
3 Data Collection and Analysis
</sectionHeader>
<subsectionHeader confidence="0.999258">
3.1 Collection Setting
</subsectionHeader>
<bodyText confidence="0.999961212121212">
We collected data using a test route. The route
passes through downtown Mountain View2 and
residential area around Honda Research Institute.
We manually constructed our database containing
250 POIs (businesses such as restaurants, compa-
nies) in this area. Each database entry (POI) has
name, geo-location, category and property infor-
mation explained in Section 3.4. POI geo-location
is represented as a latitude-longitude pair (e.g.
37.4010,-122.0539). Size and shape of buildings
are not taken into account. It takes about 30 min-
utes to drive the route. The major difference be-
tween residential area and downtown is the POI
density. While each POI in downtown has on aver-
age 7.2 other POIs within 50 meters, in residential
area POIs have only 1.9 neighbors. Speed limits
also differ between the two (35 mi/h vs 25 mi/h).
We collected data from 14 subjects. They were
asked to drive the test route and make queries
about surrounding businesses. We showed a demo
video3 of the system to the users before starting the
data collection. We also told them that the objec-
tive is a data collection for a situated spoken dia-
log system, rather than the evaluation of the whole
system. We asked subjects to include the full de-
scription of the target POI within a single utterance
to avoid queries whose understanding requires di-
alog history information4. Although the system
answered based on the baseline strategy explained
in Section 4.1, we asked subjects to ignore the sys-
tem responses.
As a result, we collected 399 queries with a
valid target POI. Queries about businesses that do
</bodyText>
<footnote confidence="0.728847166666667">
2We assumed that a POI is in downtown when it is located
within the rectangle by geo-location coordinates (37.3902, -
122.0827) and (37.3954, -122.0760).
Snot the attached one.
4Understanding including dialog history information is
our future work.
</footnote>
<figureCaption confidence="0.858224">
Figure 2: Parameters used to calculate POI score
(prior)
</figureCaption>
<figure confidence="0.997547666666667">
Y Distance (m)
Distance (m)
Left Right
</figure>
<figureCaption confidence="0.999908">
Figure 3: Target POI positions
</figureCaption>
<bodyText confidence="0.999941857142857">
not exist on our database (typically a vacant store)
were excluded. The data contains 171 queries in
downtown and 228 in residential area. The queries
were transcribed and the user-intended POIs were
manually annotated by confirming the intended
target POI with the subjects after the data collec-
tion based on a video taken during the drive.
</bodyText>
<subsectionHeader confidence="0.999293">
3.2 Analysis of Spatial Relation of POI and
Head Pose
</subsectionHeader>
<bodyText confidence="0.9351785625">
We first analyze the spatial relation between posi-
tion cues (right/left) and the position of the user-
intended target POIs Out of the collected 399
queries, 237 (59.4%) of them contain either right
or left position cue (e.g. What is that on the left?).
The relation between the position cues (cf. Figure
2) and POI positions at start-of-speech timing 5 is
plotted in Figure 3. The X-axis is a lateral distance
(a distance in the direction orthogonal to the head-
ing; a positive value means the right direction) and
the Y-axis is an axial distance (a distance in the
heading direction; a negative value means the POI
is in back of the car. ). The most obvious finding
from the scatter plot is that right and left are pow-
5Specifically, the latest GPS and face direction informa-
tion at that timing is used.
</bodyText>
<figure confidence="0.98783425">
Heading
direction
y
face direction
θ
x
target
direction
POI
• : right
X: left
+: no cue
</figure>
<page confidence="0.994384">
24
</page>
<tableCaption confidence="0.999767">
Table 2: Comparison of average and standard deviation of distance (in meter) of POI form the car
</tableCaption>
<table confidence="0.924367875">
ASR result timing Start-of-speech timing
Position cue Site Ave dist. Std dist. Ave dist. Std dist.
Right/left Downtown 17.5 31.0 31.9 28.3
Residential 22.0 36.3 45.2 36.5
No right/left Downtown 17.4 27.8 31.1 26.5
cue Residential 38.3 45.9 52.3 43.4
y Distance (m)
angular difference (degree) θ
</table>
<figureCaption confidence="0.938842">
Figure 4: Relation between POI positions and
head pose
</figureCaption>
<bodyText confidence="0.999965">
erful cues for the system to identify target POIs.
We can also see that the POI position distribution
has a large standard deviation. This is partly be-
cause the route has multiple sites from downtown
and residential area. Interestingly, while the aver-
age distance to the target POI in downtown is 37.0
meters, that of residential area is 57.4 meters.
We also analyze the relation between face di-
rection and POI positions. Figure 4 plots the re-
lation between the axial distance and the angular
difference 0 (between the user face direction and
the target POI direction) (cf. Figure 2). The scat-
ter plot suggests that the angular differences for
distant target POIs is often small. For close target
POIs the angular differences are larger and have a
large variance6.
</bodyText>
<subsectionHeader confidence="0.999967">
3.3 Analysis of Timing
</subsectionHeader>
<bodyText confidence="0.999627384615384">
Referring expressions such as “the building on the
right” must be resolved with respect to the context
in which the user intended. However, in a moving
car, such a context (i.e. the position of the car and
the situation in the surroundings) can be very dif-
ferent between the time when the user starts speak-
ing the sentence and the time they finish speaking
it. Therefore, situated understanding must be very
time sensitive.
To confirm and investigate this issue, we ana-
lyze the difference in the POI positions between
the time the ASR result is output vs the time the
user actually started speaking. The hypothesis is
</bodyText>
<footnote confidence="0.934696">
6We will discuss the reason for this in Section 6.2.
</footnote>
<tableCaption confidence="0.997076">
Table 3: User-provided linguistic cues
</tableCaption>
<figure confidence="0.843867625">
Category of linguistic cue Percentage
used (%)
Relative position to the car (right/left) 59.4
Business category (e.g. restaurant, cafe) 31.8
Color of the POI (e.g. green, yellow) 12.8
Cuisine (e.g. Chinese, Japanese, Mexican) 8.3
Equipments (e.g. awning, outside seating) 7.2
Relative position to the road (e.g. corner) 6.5
</figure>
<bodyText confidence="0.998556636363636">
that the latter yields a more accurate context in
which to interpret the user sentence. In contrast,
our baseline system uses the more straightforward
approach of resolving expressions using the con-
text at the time of resolution, i.e. whenever the
ASR/NLU has finished processing an utterance
(hereafter “ASR results timing”).
Specifically, we compare the average axial dis-
tance to the target POIs and its standard deviation
between these two timings. Table 2 lists these fig-
ures broken down by position cue types and sites.
The average axial distance from the car to the tar-
get POIs is often small at the ASR result timing,
but the standard deviation is generally small at the
start-of-speech timing. This indicates that the tar-
get POI positions at the start-of-speech timing is
more consistent across users and sentence lengths
than that at the ASR result timing. This result indi-
cates the presence of a better POI likelihood func-
tion using the context (i.e. car position and orien-
tation) at the start-of-speech timing than using the
ASR result timing.
</bodyText>
<subsectionHeader confidence="0.999783">
3.4 Analysis of Linguistic Cues
</subsectionHeader>
<bodyText confidence="0.999970466666667">
We then analyze the linguistic cues provided by
the users. Here, we focus on objective and sta-
ble cues. We exclude subjective cues (e.g. big,
beautiful, colorful) and cues that might change in
a short period of time (e.g. with a woman dressed
in green in front). We have categorized the linguis-
tic cues used to describe the target POIs. Table 3
lists the cue types and the percentage of user utter-
ances containing each cue type.
The cues that the users most often provided con-
cern POI position related to the car (right and left).
Nearly 60% of queries included this type of cue
and every subject provided it at least once. The
second most frequent cue is category of business,
especially in downtown. Users also provided col-
</bodyText>
<page confidence="0.990599">
25
</page>
<bodyText confidence="0.999582">
ors of POIs. Other cues include cuisine, equip-
ments, relative position to the road (e.g. on the
corner).
Another interesting finding from the analysis is
that the users provided more linguistic cues with
increasing candidate POIs in their field of view.
Actually, the users provided 1.51 categories in av-
erage per query in downtown, while they provided
1.03 categories in residential area. (cf. POI den-
sity in Section 3.2: 7.2 vs 1.9) This indicates that
users provide cues considering environment com-
plexity.
</bodyText>
<sectionHeader confidence="0.995795" genericHeader="method">
4 Methods for Situated Language
Understanding
</sectionHeader>
<subsectionHeader confidence="0.998432">
4.1 Baseline Strategy
</subsectionHeader>
<bodyText confidence="0.999995689655172">
We use our previous version (Misu et al., 2013)
as the baseline system for situated language un-
derstanding. The baseline strategy consists of the
following three paragraphs, which correspond to
the process 1)-3) in Section 2 and Figure 1.
The system makes a POI look-up based on the
geo-location information at the time ASR result
is obtained. The search range of candidate POIs
is within the range (relative geo-location of POIs
against the car location) of -50 to 200 meters in
the travelling direction and 100 meters to the left
and 100 meters to the right in the lateral direction.
The ASR result timing is also used to measure the
distances to the candidate POIs.
POI priors are calculated based on the distance
from the car (= axial distance) based on “the closer
to the car the likely” principle. We use a likelihood
function inversely proportional to the distance. We
use position cues simply to remove POIs from a
list of candidates. For example “right” position
cue is used to remove candidate POIs that are lo-
cated on &lt; 0 position in the lateral distance. When
no right/left cue is provided, POIs outside of 45
degrees from the face direction are removed from
the list of candidates.
No linguistic cues except right/left are used to
calculate POI posterior probabilities. So, the sys-
tem selects the POI with the highest prior (POI
score) as the language understanding result.
</bodyText>
<subsectionHeader confidence="0.9972185">
4.2 Strategies Toward Better Situated
Language Understanding
</subsectionHeader>
<bodyText confidence="0.99686225">
To achieve better situated language understanding
(POI identification) based on the findings of the
analysis in Section 3, we modify steps 1)-3) as fol-
lows:
</bodyText>
<footnote confidence="0.457175">
1. Using start-of-speech timing for the POI
prior calculation
</footnote>
<table confidence="0.795177">
Y Distance (m)
Left Right Distance (m)
</table>
<figureCaption confidence="0.960496">
Figure 5: Example GMM fitting
</figureCaption>
<listItem confidence="0.996141333333333">
2. Gaussian mixture model (GMM)-based POI
probability (prior) calculation
3. Linguistic cues for the posterior calculation.
</listItem>
<bodyText confidence="0.999865885714286">
We use the start-of-speech timing instead of the
time ASR result is output. Because the standard
deviations of the POI distances are small (cf. Sec-
tion 3.2), we expect that a better POI probability
score estimation with the POI positions at this tim-
ing in the subsequent processes than the positions
at the ASR result timing. The POI look-up range
is the same as the baseline.
We apply Gaussian mixture model (GMM) with
diagonal covariance matrices over the input pa-
rameter space. The POI probability (prior) is cal-
culated based on these Gaussians. We use two in-
put parameters of the lateral and axial distances for
queries with right/left cue, and three parameters of
the lateral and axial distances and the difference
in degree between the target and head pose direc-
tions for queries without right/left cue. (The effect
of the parameters is discussed later in Section 6.2.)
We empirically set the number of Gaussian com-
ponents to 2. An example GMM fitting to the POI
positions for queries with right and left cues is il-
lustrated in Figures 5. The center of ellipse is the
mean of the Gaussian.
We use the five linguistic cue categories of Sec-
tion 3.4 for the posterior calculation by the belief
tracker. In the following experiments, we use ei-
ther 1 or 0 as a likelihood of natural language un-
derstanding (NLU) observation. The likelihood
for the category value is 1 if a user query (NLU
result) contains the target value, otherwise 0. This
corresponds to a strategy of simply removing can-
didate POIs that do not have the category values
specified by the user. Here, we assume a clean POI
database with all their properties annotated manu-
ally.
</bodyText>
<figure confidence="0.636486">
• : right
X: left
</figure>
<page confidence="0.993395">
26
</page>
<tableCaption confidence="0.999887">
Table 4: Comparison of POI identification rate
</tableCaption>
<table confidence="0.972296125">
Method Success
rate (%)
right/left linguistic cues, 43.1
the-closer-the-likely likelihood,
ASR result timing) (Baseline)
1) Start-of-speech timing 42.9
2) GMM-based likelihood 47.9
3) Linguistic cues 54.6
</table>
<equation confidence="0.56897825">
1) + 2) 50.6
1) + 3) 54.4
2) + 3) 62.2
1) + 2) + 3) 67.2
</equation>
<sectionHeader confidence="0.992772" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999214888888889">
We use manual transcriptions and natural language
understanding results of the user queries to focus
our evaluations on the issues listed in Section 1.
We evaluate the situated language understanding
(POI identification) performance based on cross
validation. We use the data from 13 users to train
GMM parameters and to define a set of possible
linguistic values, and the data from the remaining
user for evaluation. We train the model parameters
of the GMM using the EM algorithm. Knowledge
about the sites (downtown or residential area) is
not used in the training7.
We do not set a threshold for the presentation.
We judge the system successfully understands a
user query when the posterior of the target (user-
intended) POI is the highest. The chance rate,
given by the average of the inverse number of can-
didate POIs in the POI look-up is 10.0%.
</bodyText>
<sectionHeader confidence="0.716371" genericHeader="method">
6 Analysis of the Results
</sectionHeader>
<bodyText confidence="0.992697133333334">
We first analyze the effect of our three methods
described in Section 4.2. The results are listed in
Table 4.
Simply using the POI positions at the start-of-
speech timing instead of those of the ASR result
timing did not lead to an improvement. This re-
sult is reasonable because the distances to target
POIs are often smaller at the ASR result timing
as we showed in Table 2. However, we achieved
a better improvement (7.5% over the baseline) by
combining it with the GMM-based likelihood cal-
culation. The results supports our Section 3.3 hy-
pothesis that the POI position is less dependent
on users/scenes at the start-of-speech timing. The
linguistic cues were the most powerful informa-
</bodyText>
<footnote confidence="0.9845325">
7The performance was better when the knowledge was not
used.
</footnote>
<figureCaption confidence="0.999864">
Figure 6: Breakdown of error causes
</figureCaption>
<bodyText confidence="0.997363666666667">
tion for this task. The improvement over the base-
line was 11.5%. By using these three methods to-
gether, we obtained more than additive improve-
ment of 24.1% in the POI identification rate over
the baseline8. The success rates per site were
60.8% in downtown and 71.9% in residential area.
</bodyText>
<subsectionHeader confidence="0.925852">
6.1 Error Analysis
</subsectionHeader>
<bodyText confidence="0.996683333333333">
To analyze the causes of the remaining errors, we
have categorized the errors into the following four
categories:
</bodyText>
<listItem confidence="0.994634384615385">
1. Ambiguous references: There were multi-
ple POIs that matched the user query. (e.g.
another yellow building sat next to the target)
2. Linguistic cue: The driver used undefined
linguistic cues such subjective expressions or
dynamic references objects (e.g. optometrist,
across the street, colorful)
3. Localization error: Errors in estimating
geo-location or heading of the car.
4. User error: There were errors in the user
descriptions (e.g. user misunderstood the
neighbor POI’s outside seating as the tar-
get’s)
</listItem>
<bodyText confidence="0.992746583333333">
The distribution of error causes is illustrated in
Figure 6. More than half of the errors are due
to reference ambiguity. These errors are expected
to be resolved through clarification dialogs. (e.g.
asking user “Did you mean the one in front or
back?”) Linguistic errors might be partly resolved
by using a better database with detailed category
information. For dynamic references and subjec-
tive cues, use of image processing techniques will
help. Localization errors can be solved by using
high-quality GPS and IMU sensors. User errors
were rare and only made in downtown.
</bodyText>
<subsectionHeader confidence="0.979198">
6.2 Breakdown of Effect of the Spatial
Distance and Head Pose
</subsectionHeader>
<bodyText confidence="0.999899333333333">
We then evaluate the features used for the POI
prior calculation to investigate the effect of the in-
put parameters of the lateral and axial distances
</bodyText>
<footnote confidence="0.861034666666667">
8For reference, the performances of “1) + 2) + 3)” were
62.9%, 67.2%, 66.1%, 67.2%, and 66.2% when the number
of Gaussian components were 1, 2, 3, 4, and 5.
</footnote>
<figure confidence="0.890984">
Confusion
Linguistic cue
Localization error
User error
</figure>
<page confidence="0.994132">
27
</page>
<tableCaption confidence="0.829843">
Table 5: Relation between the parameters used for
the POI identification and success rates (%)
</tableCaption>
<table confidence="0.975326">
query type
right/left no cue
58.6 51.2
59.5 53.7
43.3 44.4
73.8 54.3
57.8 48.1
59.1 54.9
68.4 57.4
</table>
<bodyText confidence="0.9997607">
and the difference in degree between the target
and user face direction angles. Table 5 lists the
relationship between the parameters used for the
GMM-based likelihood calculation and the POI
identification performances9.
The results indicate that the axial distance is
the most important parameter. We got a slight
improvement by using the face direction informa-
tion for the queries without right/left cue, but the
improvement was not significant. On the other
hand, use of face direction information for the
right/left queries clearly degraded the POI iden-
tification performance. We think this is because
the users finished looking at the POI and returned
the face to the front when they started speaking,
thus they explicitly provided right/left information
to the system. However, we believe that using a
long-term trajectory of the user face direction will
contribute to an improve in the POI identification
performance.
</bodyText>
<subsectionHeader confidence="0.9981875">
6.3 Breakdown of the Effect of Linguistic
Cues
</subsectionHeader>
<bodyText confidence="0.9998196">
We then evaluate the effect of the linguistic cues
per category. Table 6 lists the relationship between
the categories used for the posterior calculation
and the success rates. There is a strong correlation
between the frequency of the cues used (cf. Table
3) and their contributions to the improvement in
success rate. For example, business category in-
formation contributed the most, boosting the per-
formance by 8.5%.
Another point we note is that the contribution of
business category and cuisine categories is large.
Because other categories (e.g. color) are not read-
ily available in a public POI database (e.g. Google
Places API, Yelp API), we can obtain reasonable
performance without using a special database or
</bodyText>
<footnote confidence="0.856845">
9Note that, we first determine the function to calculate
POI scores (priors) based on the position cues, then calculate
scores with the selected function.
</footnote>
<tableCaption confidence="0.987086">
Table 6: Effect of linguistic cues
</tableCaption>
<table confidence="0.94952125">
linguistic cue Success
category used rate (%)
No linguistic cues (*) 50.6
(*) + Business category (e.g. cafe) 59.1
(*) + Color of the POI (e.g. green) 57.6
(*) + Cuisine (e.g. Chinese) 54.1
(*) + Equipments (e.g. awning) 53.9
(*) + Relative position (e.g. corner) 51.4
</table>
<bodyText confidence="0.995059285714286">
image processing.
We also found that linguistic cues were espe-
cially effective in downtown. Actually, while the
improvement10 was 20.0% in downtown that for
residential area was 14.4%. This mainly would be
because the users provided more linguistic cues in
downtown considering the difficulty of the task.
</bodyText>
<subsectionHeader confidence="0.999811">
6.4 Using Speech Recognition Results
</subsectionHeader>
<bodyText confidence="0.999994470588235">
We evaluate the degradation by using automatic
speech recognition (ASR) results. We use Google
ASR11 and Julius (Kawahara et al., 2004) speech
recognition system with a language model trained
from 38K example sentences generated from a
grammar. An acoustic model trained from the
WSJ speech corpus is used. Note that they are
not necessarily the best system for this domain.
Google ASR uses a general language model for
dictation and Julius uses a mismatched acoustic
model in terms of the noise condition.
The query success rate was 56.3% for Julius and
60.3% for Google ASR. We got ASR accuracies
of 77.9% and 80.4% respectively. We believe the
performance will improve when N-best hypothe-
ses with confidence scores are used in the posterior
calculating using the belief tracker.
</bodyText>
<sectionHeader confidence="0.997163" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999797272727273">
The main limitation of this work comes from the
small amount of data that we were able to collect.
It is not clear how the results obtained here would
generalize to other sites, POI density, velocities
and sensor performances. Also, results might de-
pend on experimental conditions, such as weather,
hour, season. Hyper-parameters such as the opti-
mal number of Gaussian components might have
to be adapted to different situations. We there-
fore acknowledge that the scenes we experimented
are only a limited cases of daily driving activities.
</bodyText>
<footnote confidence="0.8925335">
101) + 2) vs 1) + 2) + 3).
11Although it is not realistic to use cloud-based speech
recognition system considering the current latency, we use
this as a reference system.
</footnote>
<table confidence="0.943522375">
parameters used
lateral (x) distance
axial (y) distance
face direction
lateral + axial (x + y)
lateral (x) + face direction
axial (y) + face direction
lateral + axial + face
</table>
<page confidence="0.998455">
28
</page>
<bodyText confidence="0.999949833333333">
However, the methods we propose are general and
our findings should be verifiable without loss of
generality by collecting more data and using more
input parameters (e.g. velocity) for the POI prior
calculation.
In addition, much future work remains to realize
a natural interaction with the system, such as tak-
ing into account dialog history and selecting opti-
mal system responses. On the other hand, we be-
lieve this is one of the best platform to investigate
situated interactions. The major topics that we are
going to tackle are:
</bodyText>
<listItem confidence="0.707866">
1. Dialog strategy: Dialog strategy and system
</listItem>
<bodyText confidence="0.807004583333333">
prompt generation for situated environments
are important research topics, especially to
clarify the target when there is ambiguity as
mentioned in Section 6.1. The topic will in-
clude an adaptation of system utterances (en-
trainment) to the user (Hu et al., 2014).
2. Eye tracker: Although we believe head pose
is good enough to estimate user intentions be-
cause we are trained to move the head in driv-
ing schools to look around to confirm safety,
we would like to confirm the difference in
this task between face direction and eye-gaze.
</bodyText>
<listItem confidence="0.967613571428571">
3. POI identification using face direction trajec-
tory: Our analysis showed that the use of face
direction sometimes degrades the POI identi-
fication performance. However, we believe
that using a trajectory of face direction will
change the result.
4. Database: We assumed a clean and perfect
</listItem>
<bodyText confidence="0.992202181818182">
database but we are going to evaluate the per-
formance when noisy database is used. (e.g.
A database based on image recognition re-
sults or user dialog log.)
5. Feedback: Koller et al. (2012) demonstrated
referential resolution is enhanced by giving
gaze information feedback to the user. We
would like to analyze the effect of feedback
with an automotive augmented reality envi-
ronment using our 3D head-up display (Ng-
Thow-Hing et al., 2013).
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999192978723404">
The related studies include a landmark-based nav-
igation that handles landmarks as information for
a dialog. Similar system concepts have been
provided for pedestrian navigation situations (Ja-
narthanam et al., 2013; Hu et al., 2014), they do
not handle a rapidly changing environment.
Several works have used timing to enhance
natural interaction with systems. Rose and
Horvitz (2003) and Raux and Eskenazi (2009)
used timing information to detect user barge-ins.
Studies on incremental speech understanding and
generation (Skantze and Hjalmarsson, 2010; Deth-
lefs et al., 2012) have proved that real-time feed-
back actions have potential benefits for users.
Komatani et al. (2012) used user speech timing
against user’s previous and system’s utterances
to understand the intentions of user utterances.
While the above studies have handled timing fo-
cusing on (para-)linguistic aspect, our work han-
dles timing issues in relation to the user’s physical
surroundings.
Recent advancements in gaze and face direction
estimation have led to better user behavior under-
standing. There are a number of studies that have
analyzed relationship between gaze and user in-
tention, such as user focus (Yonetani et al., 2010),
preference (Kayama et al., 2010), and reference
expression understanding (Koller et al., 2012), be-
tween gaze and turn-taking (Jokinen et al., 2010;
Kawahara, 2012). Nakano et al. (2013) used face
direction for addressee identification. The previ-
ous studies most related to ours are reference res-
olution methods by Chai and Prasov (2010), Iida
et al. (2011) and Kennington et al. (2013). They
confirmed that the system’s reference resolution
performance is enhanced by taking the user’s eye
fixation into account. However, their results are
not directly applied to an interaction in a rapidly
changing environment while driving, where eye
fixations are unusual activities.
Marge and Rudnicky (2010) analyzed the effect
of space and distance for spatial language under-
standing for a human-robot communication. Our
task differs with this because we handle a rapidly
changing environment. We believe we can im-
prove our understanding performance based on
their findings.
</bodyText>
<sectionHeader confidence="0.996142" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999976166666667">
We addressed situated language understanding in
a moving car. We focused on issues in understand-
ing user language of timing, spatial distance, and
linguistic cues. Based on the analysis of the col-
lected user utterances, we proposed methods of us-
ing start-of-speech timing for the POI prior calcu-
lation, GMM-based POI probability (prior) calcu-
lation, and linguistic cues for the posterior calcula-
tion to improve the accuracy of situated language
understanding. The effectiveness of the proposed
methods was confirmed by achieving a significant
improvement in a POI identification task.
</bodyText>
<page confidence="0.9982">
29
</page>
<sectionHeader confidence="0.994541" genericHeader="acknowledgments">
10 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998">
The authors would like to thank Yi Ma at Ohio
State University for his contributions to the devel-
opment of HRItk.
</bodyText>
<sectionHeader confidence="0.990248" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883114583333">
D. Bohus and E. Horvitz. 2009. Models for Multi-
party Engagement in Open-World Dialog. In Proc.
SIGDIAL, pages 225–234.
J. Chai and Z. Prasov. 2010. Fusing eye gaze with
speech recognition hypotheses to resolve exophoric
reference in situated dialogue. In Proc. EMNLP.
D. Cohen, A. Chandrashekaran, I. Lane, and A. Raux.
2014. The hri-cmu corpus of situated in-car interac-
tions. In Proc. IWSDS, pages 201–212.
N. Dethlefs, H. Hastie, V. Rieser, and O. Lemon. 2012.
Optimising incremental dialogue decisions using in-
formation density for interactive systems. In Proc.
EMNLP, pages 82–93.
Z. Hu, G. Halberg, C. Jimenez, and M. Walker. 2014.
Entrainment in pedestrian direction giving: How
many kinds of entrainment? In Proc. IWSDS, pages
90–101.
R. Iida, M. Yasuhara, and T. Tokunaga. 2011. Multi-
modal reference resolution in situated dialogue by
integrating linguistic and extra-linguistic clues. In
Proc. IJCNLP, pages 84–92.
S. Janarthanam, O. Lemon, X. Liu, P. Bartie, W. Mack-
aness, and T. Dalmas. 2013. A multithreaded con-
versational interface for pedestrian navigation and
question answering. In Proc. SIGDIAL, pages 151–
153.
K. Jokinen, M. Nishida, and S. Yamamoto. 2010. On
eye-gaze and turn-taking. In Proc. EGIHMI.
T. Kawahara, A. Lee, K. Takeda, K. Itou, and
K. Shikano. 2004. Recent Progress of Open-Source
LVCSR Engine Julius and Japanese Model Reposi-
tory. In Proc. ICSLP, volume IV.
T. Kawahara. 2012. Multi-modal sensing and analysis
of poster conversations toward smart posterboard. In
Proc. SIGDIAL.
K. Kayama, A. Kobayashi, E. Mizukami, T. Misu,
H. Kashioka, H. Kawai, and S. Nakamura. 2010.
Spoken Dialog System on Plasma Display Panel Es-
timating User’s Interest by Image Processing. In
Proc. 1st International Workshop on Human-Centric
Interfaces for Ambient Intelligence (HCIAmi).
C. Kennington, S. Kousidis, and D. Schlangen. 2013.
Interpreting situated dialogue utterances: an update
model that uses speech, gaze, and gesture informa-
tion. In Proc. SIGDIAL.
A. Koller, K. Garoufi, M. Staudte, and M. Crocker.
2012. Enhancing referential success by tracking
hearer gaze. In Proc. SIGDIAL, pages 30–39.
K. Komatani, A. Hirano, and M. Nakano. 2012. De-
tecting system-directed utterances using dialogue-
level features. In Proc. Interspeech.
I. Lane, Y. Ma, and A. Raux. 2012. AIDAS - Immer-
sive Interaction within Vehicles. In Proc. SLT.
Y. Ma, A. Raux, D. Ramachandran, and R. Gupta.
2012. Landmark-based location belief tracking in
a spoken dialog system. In Proc. SIGDIAL, pages
169–178.
M. Marge and A. Rudnicky. 2010. Comparing Spo-
ken Language Route Instructions for Robots across
Environment Representations. In Proc. SIGDIAL,
pages 157–164.
T. Misu, A. Raux, I. Lane, J. Devassy, and R. Gupta.
2013. Situated multi-modal dialog system in vehi-
cles. In Proc. Gaze in Multimodal Interaction, pages
25–28.
Y. Nakano, N. Baba, H. Huang, and Y. Hayashi.
2013. Implementation and evaluation of a multi-
modal addressee identification mechanism for mul-
tiparty conversation systems. In Proc. ICMI, pages
35–42.
V. Ng-Thow-Hing, K. Bark, L. Beckwith, C. Tran,
R. Bhandari, and S. Sridhar. 2013. User-centered
perspectives for automotive augmented reality. In
Proc. ISMAR.
M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote,
J. Leibs, R. Wheeler, and A. Ng. 2009. ROS:
an open-source Robot Operating System. In Proc.
ICRA Workshop on Open Source Software.
A. Raux and M. Eskenazi. 2009. A Finite-state Turn-
taking Model for Spoken Dialog Systems. In Proc.
HLT/NAACL, pages 629–637.
A. Raux and Y. Ma. 2011. Efficient probabilistic track-
ing of user goal and dialog history for spoken dialog
systems. In Proc. Interspeech, pages 801–804.
R. Rose and H. Kim. 2003. A hybrid barge-in proce-
dure for more reliable turn-taking in human-machine
dialog systems. In Proc. Automatic Speech Recog-
nition and Understanding Workshop (ASRU), pages
198–203.
G. Skantze and A. Hjalmarsson. 2010. Towards incre-
mental speech generation in dialogue systems. In
Proc. SIGDIAL, pages 1–8.
K. Sugiura, N. Iwahashi, H. Kawai, and S. Nakamura.
2011. Situated spoken dialogue with robots using
active learning. Advance Robotics, 25(17):2207–
2232.
</reference>
<page confidence="0.999551">
30
</page>
<tableCaption confidence="0.997058">
Table 7: Example user utterances
</tableCaption>
<bodyText confidence="0.9743588">
- What is that blue restaurant on the right?
- How about this building to my right with outside seating?
- What is that Chinese restaurant on the left?
- Orange building to my right.
- What kind of the restaurant is that on the corner?
- The building on my right at the corner of the street.
- What about the building on my right with woman with a jacket in front
- Do you know how good is this restaurant to the left?
- Townsurfer, there is an interesting bakery what is that?
- Is this restaurant on the right any good?
</bodyText>
<reference confidence="0.9971775">
S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Baner-
jee, S. Teller, and N. Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. In Proc. AAAI.
R. Yonetani, H. Kawashima, T. Hirayama, and T. Mat-
suyama. 2010. Gaze probing: Event-based estima-
tion of objects being focused on. In Proc. ICPR,
pages 101–104.
</reference>
<sectionHeader confidence="0.528656" genericHeader="conclusions">
11 Appendix
</sectionHeader>
<reference confidence="0.98840305">
Test route:
https://www.google.com/maps/
preview/dir/Honda+Research+
Institute,+425+National+Ave+
%23100,+Mountain+View,+CA+
94043/37.4009909,-122.0518957/
37.4052337,-122.0565795/37.
3973374,-122.0595982/37.4004787,
-122.0730021/Wells+Fargo/37.
4001639,-122.0729708/37.3959193,
-122.0539449/37.4009821,-122.
0540093/@37.3999836,-122.
0792529,14z/data=!4m21!4m20!
1m5!1m1!1s0x808fb713c225003d:
0xcf989a0bb230e5c0!2m2!
1d-122.054006!2d37.401016!
1m0!1m0!1m0!1m0!1m5!1m1!1s0x0:
0x86ca9ba8a2f15150!2m2!1d-122.
082546!2d37.388722!1m0!1m0!1m0!
3e0
</reference>
<page confidence="0.99991">
31
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.601426">
<title confidence="0.89444">Situated Language Understanding at 25 Miles per Hour</title>
<author confidence="0.817319">Antoine Misu</author>
<affiliation confidence="0.999904">Honda Research Institute</affiliation>
<address confidence="0.985257">425 National Mountain View, CA</address>
<email confidence="0.999102">tmisu@hra.com</email>
<author confidence="0.935378">Ian</author>
<affiliation confidence="0.965835">Carnegie Mellon NASA Ames Research</affiliation>
<address confidence="0.997602">Moffett Field, CA 93085</address>
<abstract confidence="0.995539318181818">In this paper, we address issues in situated language understanding in a rapidly changing environment – a moving car. Specifically, we propose methods for understanding user queries about specific target buildings in their surroundings. Unlike previous studies on physically situated interactions such as interaction with mobile robots, the task is very sensitive to timing because the spatial relation between the car and the target is changing while the user is speaking. We collected situated utterances from drivers using our research system, Townsurfer, which is embedded in a real vehicle. Based on this data, we analyze the timing of user queries, spatial relationships between the car and targets, head pose of the user, and linguistic cues. Optimized on the data, our algorithms improved the target identification rate by 24.1% absolute.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>E Horvitz</author>
</authors>
<title>Models for Multiparty Engagement in Open-World Dialog.</title>
<date>2009</date>
<booktitle>In Proc. SIGDIAL,</booktitle>
<pages>225--234</pages>
<contexts>
<context position="1406" citStr="Bohus and Horvitz, 2009" startWordPosition="214" endWordPosition="217">from drivers using our research system, Townsurfer, which is embedded in a real vehicle. Based on this data, we analyze the timing of user queries, spatial relationships between the car and targets, head pose of the user, and linguistic cues. Optimized on the data, our algorithms improved the target identification rate by 24.1% absolute. 1 Introduction Recent advances in sensing technologies have enabled researchers to explore applications that require a clear awareness of the systems’ dynamic context and physical surroundings. Such applications include multi-participant conversation systems (Bohus and Horvitz, 2009) and human-robot interaction (Tellex et al., 2011; Sugiura et al., 2011). The general problem of understanding and interacting with human users in such environments is referred to as situated interaction. We address yet another environment, where situated interactions takes place – a moving car. In the previous work, we collected over 60 hours of in-car human-human interactions, where drivers interact with an expert co-pilot sitting next to them in the vehicle (Cohen et al., 2014). One of the * Currently with Lenovo. insights from the analysis on this corpus is that drivers frequently use refe</context>
</contexts>
<marker>Bohus, Horvitz, 2009</marker>
<rawString>D. Bohus and E. Horvitz. 2009. Models for Multiparty Engagement in Open-World Dialog. In Proc. SIGDIAL, pages 225–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chai</author>
<author>Z Prasov</author>
</authors>
<title>Fusing eye gaze with speech recognition hypotheses to resolve exophoric reference in situated dialogue.</title>
<date>2010</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="31210" citStr="Chai and Prasov (2010)" startWordPosition="5127" endWordPosition="5130">ion to the user’s physical surroundings. Recent advancements in gaze and face direction estimation have led to better user behavior understanding. There are a number of studies that have analyzed relationship between gaze and user intention, such as user focus (Yonetani et al., 2010), preference (Kayama et al., 2010), and reference expression understanding (Koller et al., 2012), between gaze and turn-taking (Jokinen et al., 2010; Kawahara, 2012). Nakano et al. (2013) used face direction for addressee identification. The previous studies most related to ours are reference resolution methods by Chai and Prasov (2010), Iida et al. (2011) and Kennington et al. (2013). They confirmed that the system’s reference resolution performance is enhanced by taking the user’s eye fixation into account. However, their results are not directly applied to an interaction in a rapidly changing environment while driving, where eye fixations are unusual activities. Marge and Rudnicky (2010) analyzed the effect of space and distance for spatial language understanding for a human-robot communication. Our task differs with this because we handle a rapidly changing environment. We believe we can improve our understanding perform</context>
</contexts>
<marker>Chai, Prasov, 2010</marker>
<rawString>J. Chai and Z. Prasov. 2010. Fusing eye gaze with speech recognition hypotheses to resolve exophoric reference in situated dialogue. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cohen</author>
<author>A Chandrashekaran</author>
<author>I Lane</author>
<author>A Raux</author>
</authors>
<title>The hri-cmu corpus of situated in-car interactions.</title>
<date>2014</date>
<booktitle>In Proc. IWSDS,</booktitle>
<pages>201--212</pages>
<contexts>
<context position="1891" citStr="Cohen et al., 2014" startWordPosition="290" endWordPosition="293"> dynamic context and physical surroundings. Such applications include multi-participant conversation systems (Bohus and Horvitz, 2009) and human-robot interaction (Tellex et al., 2011; Sugiura et al., 2011). The general problem of understanding and interacting with human users in such environments is referred to as situated interaction. We address yet another environment, where situated interactions takes place – a moving car. In the previous work, we collected over 60 hours of in-car human-human interactions, where drivers interact with an expert co-pilot sitting next to them in the vehicle (Cohen et al., 2014). One of the * Currently with Lenovo. insights from the analysis on this corpus is that drivers frequently use referring expressions about their surroundings. (e.g. What is that big building on the right?) Based on this insight, we have developed Townsurfer (Lane et al., 2012; Misu et al., 2013), a situated in-car intelligent assistant. Using geo-location information, the system can answer user queries/questions that contain object references about points-of-interest (POIs) in their surroundings. We use driver (user) face orientation to understand their queries and provide the requested inform</context>
</contexts>
<marker>Cohen, Chandrashekaran, Lane, Raux, 2014</marker>
<rawString>D. Cohen, A. Chandrashekaran, I. Lane, and A. Raux. 2014. The hri-cmu corpus of situated in-car interactions. In Proc. IWSDS, pages 201–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Dethlefs</author>
<author>H Hastie</author>
<author>V Rieser</author>
<author>O Lemon</author>
</authors>
<title>Optimising incremental dialogue decisions using information density for interactive systems.</title>
<date>2012</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>82--93</pages>
<contexts>
<context position="30243" citStr="Dethlefs et al., 2012" startWordPosition="4976" endWordPosition="4980">ay (NgThow-Hing et al., 2013). 8 Related Work The related studies include a landmark-based navigation that handles landmarks as information for a dialog. Similar system concepts have been provided for pedestrian navigation situations (Janarthanam et al., 2013; Hu et al., 2014), they do not handle a rapidly changing environment. Several works have used timing to enhance natural interaction with systems. Rose and Horvitz (2003) and Raux and Eskenazi (2009) used timing information to detect user barge-ins. Studies on incremental speech understanding and generation (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012) have proved that real-time feedback actions have potential benefits for users. Komatani et al. (2012) used user speech timing against user’s previous and system’s utterances to understand the intentions of user utterances. While the above studies have handled timing focusing on (para-)linguistic aspect, our work handles timing issues in relation to the user’s physical surroundings. Recent advancements in gaze and face direction estimation have led to better user behavior understanding. There are a number of studies that have analyzed relationship between gaze and user intention, such as user </context>
</contexts>
<marker>Dethlefs, Hastie, Rieser, Lemon, 2012</marker>
<rawString>N. Dethlefs, H. Hastie, V. Rieser, and O. Lemon. 2012. Optimising incremental dialogue decisions using information density for interactive systems. In Proc. EMNLP, pages 82–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Hu</author>
<author>G Halberg</author>
<author>C Jimenez</author>
<author>M Walker</author>
</authors>
<title>Entrainment in pedestrian direction giving: How many kinds of entrainment?</title>
<date>2014</date>
<booktitle>In Proc. IWSDS,</booktitle>
<pages>90--101</pages>
<contexts>
<context position="28646" citStr="Hu et al., 2014" startWordPosition="4720" endWordPosition="4723">ch future work remains to realize a natural interaction with the system, such as taking into account dialog history and selecting optimal system responses. On the other hand, we believe this is one of the best platform to investigate situated interactions. The major topics that we are going to tackle are: 1. Dialog strategy: Dialog strategy and system prompt generation for situated environments are important research topics, especially to clarify the target when there is ambiguity as mentioned in Section 6.1. The topic will include an adaptation of system utterances (entrainment) to the user (Hu et al., 2014). 2. Eye tracker: Although we believe head pose is good enough to estimate user intentions because we are trained to move the head in driving schools to look around to confirm safety, we would like to confirm the difference in this task between face direction and eye-gaze. 3. POI identification using face direction trajectory: Our analysis showed that the use of face direction sometimes degrades the POI identification performance. However, we believe that using a trajectory of face direction will change the result. 4. Database: We assumed a clean and perfect database but we are going to evalua</context>
<context position="29898" citStr="Hu et al., 2014" startWordPosition="4926" endWordPosition="4929">se is used. (e.g. A database based on image recognition results or user dialog log.) 5. Feedback: Koller et al. (2012) demonstrated referential resolution is enhanced by giving gaze information feedback to the user. We would like to analyze the effect of feedback with an automotive augmented reality environment using our 3D head-up display (NgThow-Hing et al., 2013). 8 Related Work The related studies include a landmark-based navigation that handles landmarks as information for a dialog. Similar system concepts have been provided for pedestrian navigation situations (Janarthanam et al., 2013; Hu et al., 2014), they do not handle a rapidly changing environment. Several works have used timing to enhance natural interaction with systems. Rose and Horvitz (2003) and Raux and Eskenazi (2009) used timing information to detect user barge-ins. Studies on incremental speech understanding and generation (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012) have proved that real-time feedback actions have potential benefits for users. Komatani et al. (2012) used user speech timing against user’s previous and system’s utterances to understand the intentions of user utterances. While the above studies have ha</context>
</contexts>
<marker>Hu, Halberg, Jimenez, Walker, 2014</marker>
<rawString>Z. Hu, G. Halberg, C. Jimenez, and M. Walker. 2014. Entrainment in pedestrian direction giving: How many kinds of entrainment? In Proc. IWSDS, pages 90–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iida</author>
<author>M Yasuhara</author>
<author>T Tokunaga</author>
</authors>
<title>Multimodal reference resolution in situated dialogue by integrating linguistic and extra-linguistic clues.</title>
<date>2011</date>
<booktitle>In Proc. IJCNLP,</booktitle>
<pages>84--92</pages>
<contexts>
<context position="31230" citStr="Iida et al. (2011)" startWordPosition="5131" endWordPosition="5134">al surroundings. Recent advancements in gaze and face direction estimation have led to better user behavior understanding. There are a number of studies that have analyzed relationship between gaze and user intention, such as user focus (Yonetani et al., 2010), preference (Kayama et al., 2010), and reference expression understanding (Koller et al., 2012), between gaze and turn-taking (Jokinen et al., 2010; Kawahara, 2012). Nakano et al. (2013) used face direction for addressee identification. The previous studies most related to ours are reference resolution methods by Chai and Prasov (2010), Iida et al. (2011) and Kennington et al. (2013). They confirmed that the system’s reference resolution performance is enhanced by taking the user’s eye fixation into account. However, their results are not directly applied to an interaction in a rapidly changing environment while driving, where eye fixations are unusual activities. Marge and Rudnicky (2010) analyzed the effect of space and distance for spatial language understanding for a human-robot communication. Our task differs with this because we handle a rapidly changing environment. We believe we can improve our understanding performance based on their </context>
</contexts>
<marker>Iida, Yasuhara, Tokunaga, 2011</marker>
<rawString>R. Iida, M. Yasuhara, and T. Tokunaga. 2011. Multimodal reference resolution in situated dialogue by integrating linguistic and extra-linguistic clues. In Proc. IJCNLP, pages 84–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
<author>X Liu</author>
<author>P Bartie</author>
<author>W Mackaness</author>
<author>T Dalmas</author>
</authors>
<title>A multithreaded conversational interface for pedestrian navigation and question answering.</title>
<date>2013</date>
<booktitle>In Proc. SIGDIAL,</booktitle>
<pages>151--153</pages>
<contexts>
<context position="29880" citStr="Janarthanam et al., 2013" startWordPosition="4921" endWordPosition="4925">formance when noisy database is used. (e.g. A database based on image recognition results or user dialog log.) 5. Feedback: Koller et al. (2012) demonstrated referential resolution is enhanced by giving gaze information feedback to the user. We would like to analyze the effect of feedback with an automotive augmented reality environment using our 3D head-up display (NgThow-Hing et al., 2013). 8 Related Work The related studies include a landmark-based navigation that handles landmarks as information for a dialog. Similar system concepts have been provided for pedestrian navigation situations (Janarthanam et al., 2013; Hu et al., 2014), they do not handle a rapidly changing environment. Several works have used timing to enhance natural interaction with systems. Rose and Horvitz (2003) and Raux and Eskenazi (2009) used timing information to detect user barge-ins. Studies on incremental speech understanding and generation (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012) have proved that real-time feedback actions have potential benefits for users. Komatani et al. (2012) used user speech timing against user’s previous and system’s utterances to understand the intentions of user utterances. While the abo</context>
</contexts>
<marker>Janarthanam, Lemon, Liu, Bartie, Mackaness, Dalmas, 2013</marker>
<rawString>S. Janarthanam, O. Lemon, X. Liu, P. Bartie, W. Mackaness, and T. Dalmas. 2013. A multithreaded conversational interface for pedestrian navigation and question answering. In Proc. SIGDIAL, pages 151– 153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jokinen</author>
<author>M Nishida</author>
<author>S Yamamoto</author>
</authors>
<title>On eye-gaze and turn-taking.</title>
<date>2010</date>
<booktitle>In Proc. EGIHMI.</booktitle>
<contexts>
<context position="31020" citStr="Jokinen et al., 2010" startWordPosition="5097" endWordPosition="5100">stem’s utterances to understand the intentions of user utterances. While the above studies have handled timing focusing on (para-)linguistic aspect, our work handles timing issues in relation to the user’s physical surroundings. Recent advancements in gaze and face direction estimation have led to better user behavior understanding. There are a number of studies that have analyzed relationship between gaze and user intention, such as user focus (Yonetani et al., 2010), preference (Kayama et al., 2010), and reference expression understanding (Koller et al., 2012), between gaze and turn-taking (Jokinen et al., 2010; Kawahara, 2012). Nakano et al. (2013) used face direction for addressee identification. The previous studies most related to ours are reference resolution methods by Chai and Prasov (2010), Iida et al. (2011) and Kennington et al. (2013). They confirmed that the system’s reference resolution performance is enhanced by taking the user’s eye fixation into account. However, their results are not directly applied to an interaction in a rapidly changing environment while driving, where eye fixations are unusual activities. Marge and Rudnicky (2010) analyzed the effect of space and distance for sp</context>
</contexts>
<marker>Jokinen, Nishida, Yamamoto, 2010</marker>
<rawString>K. Jokinen, M. Nishida, and S. Yamamoto. 2010. On eye-gaze and turn-taking. In Proc. EGIHMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kawahara</author>
<author>A Lee</author>
<author>K Takeda</author>
<author>K Itou</author>
<author>K Shikano</author>
</authors>
<date>2004</date>
<booktitle>Recent Progress of Open-Source LVCSR Engine Julius and Japanese Model Repository. In Proc. ICSLP, volume IV.</booktitle>
<contexts>
<context position="26254" citStr="Kawahara et al., 2004" startWordPosition="4323" endWordPosition="4326">r of the POI (e.g. green) 57.6 (*) + Cuisine (e.g. Chinese) 54.1 (*) + Equipments (e.g. awning) 53.9 (*) + Relative position (e.g. corner) 51.4 image processing. We also found that linguistic cues were especially effective in downtown. Actually, while the improvement10 was 20.0% in downtown that for residential area was 14.4%. This mainly would be because the users provided more linguistic cues in downtown considering the difficulty of the task. 6.4 Using Speech Recognition Results We evaluate the degradation by using automatic speech recognition (ASR) results. We use Google ASR11 and Julius (Kawahara et al., 2004) speech recognition system with a language model trained from 38K example sentences generated from a grammar. An acoustic model trained from the WSJ speech corpus is used. Note that they are not necessarily the best system for this domain. Google ASR uses a general language model for dictation and Julius uses a mismatched acoustic model in terms of the noise condition. The query success rate was 56.3% for Julius and 60.3% for Google ASR. We got ASR accuracies of 77.9% and 80.4% respectively. We believe the performance will improve when N-best hypotheses with confidence scores are used in the p</context>
</contexts>
<marker>Kawahara, Lee, Takeda, Itou, Shikano, 2004</marker>
<rawString>T. Kawahara, A. Lee, K. Takeda, K. Itou, and K. Shikano. 2004. Recent Progress of Open-Source LVCSR Engine Julius and Japanese Model Repository. In Proc. ICSLP, volume IV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kawahara</author>
</authors>
<title>Multi-modal sensing and analysis of poster conversations toward smart posterboard.</title>
<date>2012</date>
<booktitle>In Proc. SIGDIAL.</booktitle>
<contexts>
<context position="31037" citStr="Kawahara, 2012" startWordPosition="5101" endWordPosition="5102">nderstand the intentions of user utterances. While the above studies have handled timing focusing on (para-)linguistic aspect, our work handles timing issues in relation to the user’s physical surroundings. Recent advancements in gaze and face direction estimation have led to better user behavior understanding. There are a number of studies that have analyzed relationship between gaze and user intention, such as user focus (Yonetani et al., 2010), preference (Kayama et al., 2010), and reference expression understanding (Koller et al., 2012), between gaze and turn-taking (Jokinen et al., 2010; Kawahara, 2012). Nakano et al. (2013) used face direction for addressee identification. The previous studies most related to ours are reference resolution methods by Chai and Prasov (2010), Iida et al. (2011) and Kennington et al. (2013). They confirmed that the system’s reference resolution performance is enhanced by taking the user’s eye fixation into account. However, their results are not directly applied to an interaction in a rapidly changing environment while driving, where eye fixations are unusual activities. Marge and Rudnicky (2010) analyzed the effect of space and distance for spatial language un</context>
</contexts>
<marker>Kawahara, 2012</marker>
<rawString>T. Kawahara. 2012. Multi-modal sensing and analysis of poster conversations toward smart posterboard. In Proc. SIGDIAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kayama</author>
<author>A Kobayashi</author>
<author>E Mizukami</author>
<author>T Misu</author>
<author>H Kashioka</author>
<author>H Kawai</author>
<author>S Nakamura</author>
</authors>
<title>Spoken Dialog System on Plasma Display Panel Estimating User’s Interest by Image Processing.</title>
<date>2010</date>
<booktitle>In Proc. 1st International Workshop on Human-Centric Interfaces for Ambient Intelligence (HCIAmi).</booktitle>
<contexts>
<context position="30906" citStr="Kayama et al., 2010" startWordPosition="5080" endWordPosition="5083">s have potential benefits for users. Komatani et al. (2012) used user speech timing against user’s previous and system’s utterances to understand the intentions of user utterances. While the above studies have handled timing focusing on (para-)linguistic aspect, our work handles timing issues in relation to the user’s physical surroundings. Recent advancements in gaze and face direction estimation have led to better user behavior understanding. There are a number of studies that have analyzed relationship between gaze and user intention, such as user focus (Yonetani et al., 2010), preference (Kayama et al., 2010), and reference expression understanding (Koller et al., 2012), between gaze and turn-taking (Jokinen et al., 2010; Kawahara, 2012). Nakano et al. (2013) used face direction for addressee identification. The previous studies most related to ours are reference resolution methods by Chai and Prasov (2010), Iida et al. (2011) and Kennington et al. (2013). They confirmed that the system’s reference resolution performance is enhanced by taking the user’s eye fixation into account. However, their results are not directly applied to an interaction in a rapidly changing environment while driving, wher</context>
</contexts>
<marker>Kayama, Kobayashi, Mizukami, Misu, Kashioka, Kawai, Nakamura, 2010</marker>
<rawString>K. Kayama, A. Kobayashi, E. Mizukami, T. Misu, H. Kashioka, H. Kawai, and S. Nakamura. 2010. Spoken Dialog System on Plasma Display Panel Estimating User’s Interest by Image Processing. In Proc. 1st International Workshop on Human-Centric Interfaces for Ambient Intelligence (HCIAmi).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kennington</author>
<author>S Kousidis</author>
<author>D Schlangen</author>
</authors>
<title>Interpreting situated dialogue utterances: an update model that uses speech, gaze, and gesture information.</title>
<date>2013</date>
<booktitle>In Proc. SIGDIAL.</booktitle>
<contexts>
<context position="31259" citStr="Kennington et al. (2013)" startWordPosition="5136" endWordPosition="5139"> advancements in gaze and face direction estimation have led to better user behavior understanding. There are a number of studies that have analyzed relationship between gaze and user intention, such as user focus (Yonetani et al., 2010), preference (Kayama et al., 2010), and reference expression understanding (Koller et al., 2012), between gaze and turn-taking (Jokinen et al., 2010; Kawahara, 2012). Nakano et al. (2013) used face direction for addressee identification. The previous studies most related to ours are reference resolution methods by Chai and Prasov (2010), Iida et al. (2011) and Kennington et al. (2013). They confirmed that the system’s reference resolution performance is enhanced by taking the user’s eye fixation into account. However, their results are not directly applied to an interaction in a rapidly changing environment while driving, where eye fixations are unusual activities. Marge and Rudnicky (2010) analyzed the effect of space and distance for spatial language understanding for a human-robot communication. Our task differs with this because we handle a rapidly changing environment. We believe we can improve our understanding performance based on their findings. 9 Conclusion We add</context>
</contexts>
<marker>Kennington, Kousidis, Schlangen, 2013</marker>
<rawString>C. Kennington, S. Kousidis, and D. Schlangen. 2013. Interpreting situated dialogue utterances: an update model that uses speech, gaze, and gesture information. In Proc. SIGDIAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Koller</author>
<author>K Garoufi</author>
<author>M Staudte</author>
<author>M Crocker</author>
</authors>
<title>Enhancing referential success by tracking hearer gaze.</title>
<date>2012</date>
<booktitle>In Proc. SIGDIAL,</booktitle>
<pages>30--39</pages>
<contexts>
<context position="29400" citStr="Koller et al. (2012)" startWordPosition="4849" endWordPosition="4852">driving schools to look around to confirm safety, we would like to confirm the difference in this task between face direction and eye-gaze. 3. POI identification using face direction trajectory: Our analysis showed that the use of face direction sometimes degrades the POI identification performance. However, we believe that using a trajectory of face direction will change the result. 4. Database: We assumed a clean and perfect database but we are going to evaluate the performance when noisy database is used. (e.g. A database based on image recognition results or user dialog log.) 5. Feedback: Koller et al. (2012) demonstrated referential resolution is enhanced by giving gaze information feedback to the user. We would like to analyze the effect of feedback with an automotive augmented reality environment using our 3D head-up display (NgThow-Hing et al., 2013). 8 Related Work The related studies include a landmark-based navigation that handles landmarks as information for a dialog. Similar system concepts have been provided for pedestrian navigation situations (Janarthanam et al., 2013; Hu et al., 2014), they do not handle a rapidly changing environment. Several works have used timing to enhance natural</context>
<context position="30968" citStr="Koller et al., 2012" startWordPosition="5088" endWordPosition="5091">ed user speech timing against user’s previous and system’s utterances to understand the intentions of user utterances. While the above studies have handled timing focusing on (para-)linguistic aspect, our work handles timing issues in relation to the user’s physical surroundings. Recent advancements in gaze and face direction estimation have led to better user behavior understanding. There are a number of studies that have analyzed relationship between gaze and user intention, such as user focus (Yonetani et al., 2010), preference (Kayama et al., 2010), and reference expression understanding (Koller et al., 2012), between gaze and turn-taking (Jokinen et al., 2010; Kawahara, 2012). Nakano et al. (2013) used face direction for addressee identification. The previous studies most related to ours are reference resolution methods by Chai and Prasov (2010), Iida et al. (2011) and Kennington et al. (2013). They confirmed that the system’s reference resolution performance is enhanced by taking the user’s eye fixation into account. However, their results are not directly applied to an interaction in a rapidly changing environment while driving, where eye fixations are unusual activities. Marge and Rudnicky (20</context>
</contexts>
<marker>Koller, Garoufi, Staudte, Crocker, 2012</marker>
<rawString>A. Koller, K. Garoufi, M. Staudte, and M. Crocker. 2012. Enhancing referential success by tracking hearer gaze. In Proc. SIGDIAL, pages 30–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Komatani</author>
<author>A Hirano</author>
<author>M Nakano</author>
</authors>
<title>Detecting system-directed utterances using dialoguelevel features.</title>
<date>2012</date>
<booktitle>In Proc. Interspeech.</booktitle>
<contexts>
<context position="30345" citStr="Komatani et al. (2012)" startWordPosition="4993" endWordPosition="4996">that handles landmarks as information for a dialog. Similar system concepts have been provided for pedestrian navigation situations (Janarthanam et al., 2013; Hu et al., 2014), they do not handle a rapidly changing environment. Several works have used timing to enhance natural interaction with systems. Rose and Horvitz (2003) and Raux and Eskenazi (2009) used timing information to detect user barge-ins. Studies on incremental speech understanding and generation (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012) have proved that real-time feedback actions have potential benefits for users. Komatani et al. (2012) used user speech timing against user’s previous and system’s utterances to understand the intentions of user utterances. While the above studies have handled timing focusing on (para-)linguistic aspect, our work handles timing issues in relation to the user’s physical surroundings. Recent advancements in gaze and face direction estimation have led to better user behavior understanding. There are a number of studies that have analyzed relationship between gaze and user intention, such as user focus (Yonetani et al., 2010), preference (Kayama et al., 2010), and reference expression understandin</context>
</contexts>
<marker>Komatani, Hirano, Nakano, 2012</marker>
<rawString>K. Komatani, A. Hirano, and M. Nakano. 2012. Detecting system-directed utterances using dialoguelevel features. In Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Lane</author>
<author>Y Ma</author>
<author>A Raux</author>
</authors>
<title>AIDAS - Immersive Interaction within Vehicles. In</title>
<date>2012</date>
<booktitle>Proc. SLT.</booktitle>
<contexts>
<context position="2167" citStr="Lane et al., 2012" startWordPosition="336" endWordPosition="339">in such environments is referred to as situated interaction. We address yet another environment, where situated interactions takes place – a moving car. In the previous work, we collected over 60 hours of in-car human-human interactions, where drivers interact with an expert co-pilot sitting next to them in the vehicle (Cohen et al., 2014). One of the * Currently with Lenovo. insights from the analysis on this corpus is that drivers frequently use referring expressions about their surroundings. (e.g. What is that big building on the right?) Based on this insight, we have developed Townsurfer (Lane et al., 2012; Misu et al., 2013), a situated in-car intelligent assistant. Using geo-location information, the system can answer user queries/questions that contain object references about points-of-interest (POIs) in their surroundings. We use driver (user) face orientation to understand their queries and provide the requested information about the POI they are looking at. We have previously demonstrated and evaluated the system in a simulated environment (Lane et al., 2012). In this paper, we evaluate its utility in real driving situations. Compared to conventional situated dialog tasks, query understan</context>
</contexts>
<marker>Lane, Ma, Raux, 2012</marker>
<rawString>I. Lane, Y. Ma, and A. Raux. 2012. AIDAS - Immersive Interaction within Vehicles. In Proc. SLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ma</author>
<author>A Raux</author>
<author>D Ramachandran</author>
<author>R Gupta</author>
</authors>
<title>Landmark-based location belief tracking in a spoken dialog system.</title>
<date>2012</date>
<booktitle>In Proc. SIGDIAL,</booktitle>
<pages>169--178</pages>
<contexts>
<context position="6252" citStr="Ma et al., 2012" startWordPosition="984" endWordPosition="987">o-location and head pose information are used to understand the target POI of the user query. An overview of the system with a process flow is illustrated in Figure 1 and an example dialog with the system is shown in Table 1. A video of an example dialog is also attached. In this paper, we address issues in identifying user intended POI, which is a form of reference resolution using multi-modal information sources&apos;. The POI identification process consists of the following three steps (cf. Figure 1). This is similar to but different from our previous work on landmark-based destination setting (Ma et al., 2012). 1) The system lists candidate POIs based on geolocation at the timing of a driver query. Relative positions of POIs to the car are also calculated based on geo-location and the heading of the car. 2). Based on spatial linguistic cues in the user utterance (e.g. to my right, on the left), a 2D scoring function is selected to identify areas where the target POI is likely to be. This function takes into account the position of the POI relative to the car, as well as driver head pose. Scores for all candidate POIs are calculated. 3) Posterior probabilities of each POI are calculated using the sc</context>
</contexts>
<marker>Ma, Raux, Ramachandran, Gupta, 2012</marker>
<rawString>Y. Ma, A. Raux, D. Ramachandran, and R. Gupta. 2012. Landmark-based location belief tracking in a spoken dialog system. In Proc. SIGDIAL, pages 169–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marge</author>
<author>A Rudnicky</author>
</authors>
<title>Comparing Spoken Language Route Instructions for Robots across Environment Representations. In</title>
<date>2010</date>
<booktitle>Proc. SIGDIAL,</booktitle>
<pages>157--164</pages>
<contexts>
<context position="31571" citStr="Marge and Rudnicky (2010)" startWordPosition="5181" endWordPosition="5184"> (Koller et al., 2012), between gaze and turn-taking (Jokinen et al., 2010; Kawahara, 2012). Nakano et al. (2013) used face direction for addressee identification. The previous studies most related to ours are reference resolution methods by Chai and Prasov (2010), Iida et al. (2011) and Kennington et al. (2013). They confirmed that the system’s reference resolution performance is enhanced by taking the user’s eye fixation into account. However, their results are not directly applied to an interaction in a rapidly changing environment while driving, where eye fixations are unusual activities. Marge and Rudnicky (2010) analyzed the effect of space and distance for spatial language understanding for a human-robot communication. Our task differs with this because we handle a rapidly changing environment. We believe we can improve our understanding performance based on their findings. 9 Conclusion We addressed situated language understanding in a moving car. We focused on issues in understanding user language of timing, spatial distance, and linguistic cues. Based on the analysis of the collected user utterances, we proposed methods of using start-of-speech timing for the POI prior calculation, GMM-based POI p</context>
</contexts>
<marker>Marge, Rudnicky, 2010</marker>
<rawString>M. Marge and A. Rudnicky. 2010. Comparing Spoken Language Route Instructions for Robots across Environment Representations. In Proc. SIGDIAL, pages 157–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Misu</author>
<author>A Raux</author>
<author>I Lane</author>
<author>J Devassy</author>
<author>R Gupta</author>
</authors>
<title>Situated multi-modal dialog system in vehicles.</title>
<date>2013</date>
<booktitle>In Proc. Gaze in Multimodal Interaction,</booktitle>
<pages>25--28</pages>
<contexts>
<context position="2187" citStr="Misu et al., 2013" startWordPosition="340" endWordPosition="343">s is referred to as situated interaction. We address yet another environment, where situated interactions takes place – a moving car. In the previous work, we collected over 60 hours of in-car human-human interactions, where drivers interact with an expert co-pilot sitting next to them in the vehicle (Cohen et al., 2014). One of the * Currently with Lenovo. insights from the analysis on this corpus is that drivers frequently use referring expressions about their surroundings. (e.g. What is that big building on the right?) Based on this insight, we have developed Townsurfer (Lane et al., 2012; Misu et al., 2013), a situated in-car intelligent assistant. Using geo-location information, the system can answer user queries/questions that contain object references about points-of-interest (POIs) in their surroundings. We use driver (user) face orientation to understand their queries and provide the requested information about the POI they are looking at. We have previously demonstrated and evaluated the system in a simulated environment (Lane et al., 2012). In this paper, we evaluate its utility in real driving situations. Compared to conventional situated dialog tasks, query understanding in our task is </context>
<context position="16030" citStr="Misu et al., 2013" startWordPosition="2633" endWordPosition="2636">POIs. Other cues include cuisine, equipments, relative position to the road (e.g. on the corner). Another interesting finding from the analysis is that the users provided more linguistic cues with increasing candidate POIs in their field of view. Actually, the users provided 1.51 categories in average per query in downtown, while they provided 1.03 categories in residential area. (cf. POI density in Section 3.2: 7.2 vs 1.9) This indicates that users provide cues considering environment complexity. 4 Methods for Situated Language Understanding 4.1 Baseline Strategy We use our previous version (Misu et al., 2013) as the baseline system for situated language understanding. The baseline strategy consists of the following three paragraphs, which correspond to the process 1)-3) in Section 2 and Figure 1. The system makes a POI look-up based on the geo-location information at the time ASR result is obtained. The search range of candidate POIs is within the range (relative geo-location of POIs against the car location) of -50 to 200 meters in the travelling direction and 100 meters to the left and 100 meters to the right in the lateral direction. The ASR result timing is also used to measure the distances t</context>
</contexts>
<marker>Misu, Raux, Lane, Devassy, Gupta, 2013</marker>
<rawString>T. Misu, A. Raux, I. Lane, J. Devassy, and R. Gupta. 2013. Situated multi-modal dialog system in vehicles. In Proc. Gaze in Multimodal Interaction, pages 25–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Nakano</author>
<author>N Baba</author>
<author>H Huang</author>
<author>Y Hayashi</author>
</authors>
<title>Implementation and evaluation of a multimodal addressee identification mechanism for multiparty conversation systems.</title>
<date>2013</date>
<booktitle>In Proc. ICMI,</booktitle>
<pages>35--42</pages>
<contexts>
<context position="31059" citStr="Nakano et al. (2013)" startWordPosition="5103" endWordPosition="5106">entions of user utterances. While the above studies have handled timing focusing on (para-)linguistic aspect, our work handles timing issues in relation to the user’s physical surroundings. Recent advancements in gaze and face direction estimation have led to better user behavior understanding. There are a number of studies that have analyzed relationship between gaze and user intention, such as user focus (Yonetani et al., 2010), preference (Kayama et al., 2010), and reference expression understanding (Koller et al., 2012), between gaze and turn-taking (Jokinen et al., 2010; Kawahara, 2012). Nakano et al. (2013) used face direction for addressee identification. The previous studies most related to ours are reference resolution methods by Chai and Prasov (2010), Iida et al. (2011) and Kennington et al. (2013). They confirmed that the system’s reference resolution performance is enhanced by taking the user’s eye fixation into account. However, their results are not directly applied to an interaction in a rapidly changing environment while driving, where eye fixations are unusual activities. Marge and Rudnicky (2010) analyzed the effect of space and distance for spatial language understanding for a huma</context>
</contexts>
<marker>Nakano, Baba, Huang, Hayashi, 2013</marker>
<rawString>Y. Nakano, N. Baba, H. Huang, and Y. Hayashi. 2013. Implementation and evaluation of a multimodal addressee identification mechanism for multiparty conversation systems. In Proc. ICMI, pages 35–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng-Thow-Hing</author>
<author>K Bark</author>
<author>L Beckwith</author>
<author>C Tran</author>
<author>R Bhandari</author>
<author>S Sridhar</author>
</authors>
<title>User-centered perspectives for automotive augmented reality.</title>
<date>2013</date>
<booktitle>In Proc. ISMAR.</booktitle>
<marker>Ng-Thow-Hing, Bark, Beckwith, Tran, Bhandari, Sridhar, 2013</marker>
<rawString>V. Ng-Thow-Hing, K. Bark, L. Beckwith, C. Tran, R. Bhandari, and S. Sridhar. 2013. User-centered perspectives for automotive augmented reality. In Proc. ISMAR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Quigley</author>
<author>K Conley</author>
<author>B Gerkey</author>
<author>J Faust</author>
<author>T Foote</author>
<author>J Leibs</author>
<author>R Wheeler</author>
<author>A Ng</author>
</authors>
<title>ROS: an open-source Robot Operating System. In</title>
<date>2009</date>
<booktitle>Proc. ICRA Workshop on Open Source Software.</booktitle>
<contexts>
<context position="7870" citStr="Quigley et al., 2009" startWordPosition="1268" endWordPosition="1271">lose talk microphone (plantronics Voyage Leg&apos;We do not deal with issues in language understanding related to dialog history and query type. (e.g. General information request such as U1 vs request about specific property of POI such as U2 in Table 1) 23 end UC). These consumer grade sensors are installed in our Honda Pilot experiment car. We use Point Cloud Library (PCL) for the face direction estimation. Geo-location is estimated based on Extended Kalman filter-based algorithm using GPS and gyro information as input at 1.5 Hz. The system is implemented based on the Robot Operating System ROS (Quigley et al., 2009). Each component is implemented as a node of ROS, and communications between the nodes are performed using the standard message passing mechanisms in ROS. 3 Data Collection and Analysis 3.1 Collection Setting We collected data using a test route. The route passes through downtown Mountain View2 and residential area around Honda Research Institute. We manually constructed our database containing 250 POIs (businesses such as restaurants, companies) in this area. Each database entry (POI) has name, geo-location, category and property information explained in Section 3.4. POI geo-location is repre</context>
</contexts>
<marker>Quigley, Conley, Gerkey, Faust, Foote, Leibs, Wheeler, Ng, 2009</marker>
<rawString>M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler, and A. Ng. 2009. ROS: an open-source Robot Operating System. In Proc. ICRA Workshop on Open Source Software.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
<author>M Eskenazi</author>
</authors>
<title>A Finite-state Turntaking Model for Spoken Dialog Systems.</title>
<date>2009</date>
<booktitle>In Proc. HLT/NAACL,</booktitle>
<pages>629--637</pages>
<contexts>
<context position="30079" citStr="Raux and Eskenazi (2009)" startWordPosition="4954" endWordPosition="4957">ing gaze information feedback to the user. We would like to analyze the effect of feedback with an automotive augmented reality environment using our 3D head-up display (NgThow-Hing et al., 2013). 8 Related Work The related studies include a landmark-based navigation that handles landmarks as information for a dialog. Similar system concepts have been provided for pedestrian navigation situations (Janarthanam et al., 2013; Hu et al., 2014), they do not handle a rapidly changing environment. Several works have used timing to enhance natural interaction with systems. Rose and Horvitz (2003) and Raux and Eskenazi (2009) used timing information to detect user barge-ins. Studies on incremental speech understanding and generation (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012) have proved that real-time feedback actions have potential benefits for users. Komatani et al. (2012) used user speech timing against user’s previous and system’s utterances to understand the intentions of user utterances. While the above studies have handled timing focusing on (para-)linguistic aspect, our work handles timing issues in relation to the user’s physical surroundings. Recent advancements in gaze and face direction est</context>
</contexts>
<marker>Raux, Eskenazi, 2009</marker>
<rawString>A. Raux and M. Eskenazi. 2009. A Finite-state Turntaking Model for Spoken Dialog Systems. In Proc. HLT/NAACL, pages 629–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
<author>Y Ma</author>
</authors>
<title>Efficient probabilistic tracking of user goal and dialog history for spoken dialog systems.</title>
<date>2011</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>801--804</pages>
<contexts>
<context position="7079" citStr="Raux and Ma, 2011" startWordPosition="1132" endWordPosition="1135">on spatial linguistic cues in the user utterance (e.g. to my right, on the left), a 2D scoring function is selected to identify areas where the target POI is likely to be. This function takes into account the position of the POI relative to the car, as well as driver head pose. Scores for all candidate POIs are calculated. 3) Posterior probabilities of each POI are calculated using the score of step 2 as prior, and non-spatial linguistic information (e.g. POI categories, building properties) as observations. This posterior calculation is computed using our Bayesian belief tracker called DPOT (Raux and Ma, 2011). The details are explained in Section 4. System hardware consists of a 3D depth sensor (Primesense Carmine 1.09), a USB GPS (BU353S4), an IMU sensor (3DM-GX3-25) and a close talk microphone (plantronics Voyage Leg&apos;We do not deal with issues in language understanding related to dialog history and query type. (e.g. General information request such as U1 vs request about specific property of POI such as U2 in Table 1) 23 end UC). These consumer grade sensors are installed in our Honda Pilot experiment car. We use Point Cloud Library (PCL) for the face direction estimation. Geo-location is estima</context>
</contexts>
<marker>Raux, Ma, 2011</marker>
<rawString>A. Raux and Y. Ma. 2011. Efficient probabilistic tracking of user goal and dialog history for spoken dialog systems. In Proc. Interspeech, pages 801–804.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rose</author>
<author>H Kim</author>
</authors>
<title>A hybrid barge-in procedure for more reliable turn-taking in human-machine dialog systems.</title>
<date>2003</date>
<booktitle>In Proc. Automatic Speech Recognition and Understanding Workshop (ASRU),</booktitle>
<pages>198--203</pages>
<marker>Rose, Kim, 2003</marker>
<rawString>R. Rose and H. Kim. 2003. A hybrid barge-in procedure for more reliable turn-taking in human-machine dialog systems. In Proc. Automatic Speech Recognition and Understanding Workshop (ASRU), pages 198–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Skantze</author>
<author>A Hjalmarsson</author>
</authors>
<title>Towards incremental speech generation in dialogue systems.</title>
<date>2010</date>
<booktitle>In Proc. SIGDIAL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="30219" citStr="Skantze and Hjalmarsson, 2010" startWordPosition="4972" endWordPosition="4975">ment using our 3D head-up display (NgThow-Hing et al., 2013). 8 Related Work The related studies include a landmark-based navigation that handles landmarks as information for a dialog. Similar system concepts have been provided for pedestrian navigation situations (Janarthanam et al., 2013; Hu et al., 2014), they do not handle a rapidly changing environment. Several works have used timing to enhance natural interaction with systems. Rose and Horvitz (2003) and Raux and Eskenazi (2009) used timing information to detect user barge-ins. Studies on incremental speech understanding and generation (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012) have proved that real-time feedback actions have potential benefits for users. Komatani et al. (2012) used user speech timing against user’s previous and system’s utterances to understand the intentions of user utterances. While the above studies have handled timing focusing on (para-)linguistic aspect, our work handles timing issues in relation to the user’s physical surroundings. Recent advancements in gaze and face direction estimation have led to better user behavior understanding. There are a number of studies that have analyzed relationship between gaze and user </context>
</contexts>
<marker>Skantze, Hjalmarsson, 2010</marker>
<rawString>G. Skantze and A. Hjalmarsson. 2010. Towards incremental speech generation in dialogue systems. In Proc. SIGDIAL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sugiura</author>
<author>N Iwahashi</author>
<author>H Kawai</author>
<author>S Nakamura</author>
</authors>
<title>Situated spoken dialogue with robots using active learning.</title>
<date>2011</date>
<journal>Advance Robotics,</journal>
<volume>25</volume>
<issue>17</issue>
<pages>2232</pages>
<contexts>
<context position="1478" citStr="Sugiura et al., 2011" startWordPosition="225" endWordPosition="228">real vehicle. Based on this data, we analyze the timing of user queries, spatial relationships between the car and targets, head pose of the user, and linguistic cues. Optimized on the data, our algorithms improved the target identification rate by 24.1% absolute. 1 Introduction Recent advances in sensing technologies have enabled researchers to explore applications that require a clear awareness of the systems’ dynamic context and physical surroundings. Such applications include multi-participant conversation systems (Bohus and Horvitz, 2009) and human-robot interaction (Tellex et al., 2011; Sugiura et al., 2011). The general problem of understanding and interacting with human users in such environments is referred to as situated interaction. We address yet another environment, where situated interactions takes place – a moving car. In the previous work, we collected over 60 hours of in-car human-human interactions, where drivers interact with an expert co-pilot sitting next to them in the vehicle (Cohen et al., 2014). One of the * Currently with Lenovo. insights from the analysis on this corpus is that drivers frequently use referring expressions about their surroundings. (e.g. What is that big build</context>
</contexts>
<marker>Sugiura, Iwahashi, Kawai, Nakamura, 2011</marker>
<rawString>K. Sugiura, N. Iwahashi, H. Kawai, and S. Nakamura. 2011. Situated spoken dialogue with robots using active learning. Advance Robotics, 25(17):2207– 2232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tellex</author>
<author>T Kollar</author>
<author>S Dickerson</author>
<author>M Walter</author>
<author>A Banerjee</author>
<author>S Teller</author>
<author>N Roy</author>
</authors>
<title>Understanding natural language commands for robotic navigation and mobile manipulation. In</title>
<date>2011</date>
<booktitle>Proc. AAAI.</booktitle>
<contexts>
<context position="1455" citStr="Tellex et al., 2011" startWordPosition="221" endWordPosition="224">ich is embedded in a real vehicle. Based on this data, we analyze the timing of user queries, spatial relationships between the car and targets, head pose of the user, and linguistic cues. Optimized on the data, our algorithms improved the target identification rate by 24.1% absolute. 1 Introduction Recent advances in sensing technologies have enabled researchers to explore applications that require a clear awareness of the systems’ dynamic context and physical surroundings. Such applications include multi-participant conversation systems (Bohus and Horvitz, 2009) and human-robot interaction (Tellex et al., 2011; Sugiura et al., 2011). The general problem of understanding and interacting with human users in such environments is referred to as situated interaction. We address yet another environment, where situated interactions takes place – a moving car. In the previous work, we collected over 60 hours of in-car human-human interactions, where drivers interact with an expert co-pilot sitting next to them in the vehicle (Cohen et al., 2014). One of the * Currently with Lenovo. insights from the analysis on this corpus is that drivers frequently use referring expressions about their surroundings. (e.g.</context>
</contexts>
<marker>Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, Roy, 2011</marker>
<rawString>S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller, and N. Roy. 2011. Understanding natural language commands for robotic navigation and mobile manipulation. In Proc. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yonetani</author>
<author>H Kawashima</author>
<author>T Hirayama</author>
<author>T Matsuyama</author>
</authors>
<title>Gaze probing: Event-based estimation of objects being focused on.</title>
<date>2010</date>
<booktitle>In Proc. ICPR,</booktitle>
<pages>101--104</pages>
<contexts>
<context position="30872" citStr="Yonetani et al., 2010" startWordPosition="5075" endWordPosition="5078">roved that real-time feedback actions have potential benefits for users. Komatani et al. (2012) used user speech timing against user’s previous and system’s utterances to understand the intentions of user utterances. While the above studies have handled timing focusing on (para-)linguistic aspect, our work handles timing issues in relation to the user’s physical surroundings. Recent advancements in gaze and face direction estimation have led to better user behavior understanding. There are a number of studies that have analyzed relationship between gaze and user intention, such as user focus (Yonetani et al., 2010), preference (Kayama et al., 2010), and reference expression understanding (Koller et al., 2012), between gaze and turn-taking (Jokinen et al., 2010; Kawahara, 2012). Nakano et al. (2013) used face direction for addressee identification. The previous studies most related to ours are reference resolution methods by Chai and Prasov (2010), Iida et al. (2011) and Kennington et al. (2013). They confirmed that the system’s reference resolution performance is enhanced by taking the user’s eye fixation into account. However, their results are not directly applied to an interaction in a rapidly changi</context>
</contexts>
<marker>Yonetani, Kawashima, Hirayama, Matsuyama, 2010</marker>
<rawString>R. Yonetani, H. Kawashima, T. Hirayama, and T. Matsuyama. 2010. Gaze probing: Event-based estimation of objects being focused on. In Proc. ICPR, pages 101–104.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Test route</author>
</authors>
<pages>37--4052337</pages>
<marker>route, </marker>
<rawString>Test route: 37.4052337,-122.0565795/37. 3973374,-122.0595982/37.4004787, -122.0730021/Wells+Fargo/37. -122.0539449/37.4009821,-122. 0540093/@37.3999836,-122. 1m0!1m0!1m0!1m0!1m5!1m1!1s0x0: 0x86ca9ba8a2f15150!2m2!1d-122.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001288">
<title confidence="0.999365">
Towards Automatic Scoring of Cloze Items by
Selecting Low-Ambiguity Contexts
</title>
<author confidence="0.992878">
Tobias Horsmann, Torsten Zesch
</author>
<affiliation confidence="0.841969666666667">
Language Technology Lab
University of Duisburg-Essen
Germany
</affiliation>
<email confidence="0.576734">
{tobias.horsmann, torsten.zesch}@uni-due.de
</email>
<sectionHeader confidence="0.9972" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999869454545455">
In second language learning, cloze tests (also known as fill-in-the-blank tests) are frequently used for
assessing the learning progress of students. While preparation effort for these tests is low, scoring
needs to be done manually, as there usually is a huge number of correct solutions. In this paper, we
examine whether the ambiguity of cloze items can be lowered to a point where automatic scoring
becomes possible. We utilize the local context of a word to collect evidence of low-ambiguity. We
do that by seeking for collocated word sequences, but also taking structural information on sentence
level into account. We evaluate the effectiveness of our method in a user study on cloze items ranked
by our method. For the top-ranked items (lowest ambiguity) the subjects provide the target word
significantly more often than for the bottom-ranked items (59.9% vs. 36.5%). While this shows the
potential of our method, we did not succeed in fully eliminating ambiguity. Thus, further research is
necessary before fully automatic scoring becomes possible.
</bodyText>
<keyword confidence="0.730815">
Keywords: cloze tests, language proficiency tests, automatic scoring.
</keyword>
<note confidence="0.828251666666667">
Tobias Horsmann and Torsten Zesch 2014. Towards automatic scoring of cloze items by selecting low-ambiguity contexts.
Proceedings of the third workshop on NLP for computer-assisted language learning. NEALT Proceedings Series 22 /
Linköping Electronic Conference Proceedings 107: 33–42.
</note>
<page confidence="0.998442">
33
</page>
<sectionHeader confidence="0.994757" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995665333333333">
Cloze items (Taylor, 1953, 1956; O’Toole and King, 2011) are frequently used to test language
proficiency. A cloze item consists of a sentence with usually one word being blanked. The learner’s
task is to find the correct word for the blank:
</bodyText>
<listItem confidence="0.889787">
(1) He sold his ____yesterday below price.
</listItem>
<bodyText confidence="0.99214808">
As we can see from example (1), blanks can be quite ambiguous, i.e. a very high number of correct
solutions exists. In this example, a wide range of nouns are acceptable solutions including books,
house, or bike, but also daughter or kidney cannot be ruled out in this (quite limited) context. Such
ambiguity is not only a problem for language learners, but even native speakers frequently fail when
facing such a task (Klein-Braley and Raatz, 1982).
If cloze items should be automatically generated and scored, ambiguous items pose a serious
problem, as we only know for sure one correct answer namely the one that was used in the original
sentence. Students might get frustrated if they provide a valid solution that is not recognized by
the system. The same problem affects an alternative solution to the problem: providing a list
of alternative answer options - called distractors (Sumita et al., 2005). Determining whether a
distractor is actually another valid solution is equivalent to the problem described above. Thus,
finding good distractors is still an unsolved problem that attracts a lot of research (Lee and Seneff,
2007; Smith and Avinesh, 2010; Sakaguchi et al., 2013; Zesch and Melamud, 2014). Furthermore,
providing distractors for cloze items also considerably changes the nature of the task, as distractors
are recognition stimuli, i.e. the student recognizes the correct answer rather than having to actively
produce it (Gonzáles, 1996).
Now consider example sentence (2):
(2) I went to the ____today and now I have sand in my shoes.
Most people would come up with beach or maybe desert, while other solutions are highly unlikely,
which means that the blank is less ambiguous than example (1). This leads to our research question,
whether it is possible to find contexts that are specific enough to only allow one correct solution.
Such a setup would dramatically simplify automatic scoring.
We limit the scope of this work to determine low-ambiguity contexts for single-word nouns and
leave other parts of speech for future work. In the next section, we describe how such contexts can
be detected.
</bodyText>
<sectionHeader confidence="0.814261" genericHeader="method">
2 Detecting Low-Ambiguity Contexts
</sectionHeader>
<bodyText confidence="0.99504325">
In order to determine low-ambiguity contexts, we introduce several detectors that collect evidence
of low ambiguity. Figure 1 shows the process chain: First, a target word is chosen for which a
context of low ambiguity shall be found. If a sentence contains the target word, we run all detectors
in parallel and combine their scores to form the final score. We manually determined the perceived
reliability of the detectors and assigned them to three classes strong, medium, and weak. Each of
the classes correspond to a certain weight of the detector in the final score: a medium detector is 5
times as important as a weak one, and a strong detector is 20 times more important than a weak one.
Table 1 provides an overview of the class assignments for detectors. As this is basically a linear
</bodyText>
<page confidence="0.999056">
34
</page>
<figureCaption confidence="0.993149">
Figure 1: Low Ambiguity Context Determination Process Chain
</figureCaption>
<table confidence="0.952667625">
Detector Reliability Weigth
Distributional Thesaurus weak 1
POS Pattern Sliding Window weak 1
Bigram Sliding Window medium 5
Skip-Bigrams medium 5
Hearst-Patterns medium 5
3-5 Gram Sliding Window strong 20
Word Repetition strong 20
</table>
<tableCaption confidence="0.999535">
Table 1: List of detectors with their reliability and assigned weigth
</tableCaption>
<bodyText confidence="0.9773635">
regression with hand-assigned weights, the weights can (and should) be learned if labeled training
data is available.
In the remainder of this section, we describe our detectors in more detail. The detectors are not
mutually exclusive and a word might be detected by several detectors.
</bodyText>
<subsectionHeader confidence="0.973873">
2.1 Collocation-based Detectors
</subsectionHeader>
<bodyText confidence="0.999859588235294">
Subsequently, we will introduce three detectors that are variations of testing a word for being
collocated with one or more nearby words. These detectors are motivated by the research field of
word prediction where an algorithm tries to determine the next word a user might want to type on a
keyboard based on the already entered word sequence (Li and Hirst, 2005; Trnka, 2008; Aliprandi
et al., 2007). We hypothesize that if a word is easy to predict it should also be less ambiguous in that
context. We further expect that longer collocated sequences are easier to predict than shorter ones.
Another way to look at this problem is its relation to language model perplexity (Chen et al., 1998).
Classically, perplexity makes a statement about how well a model predicts test data. In our task,
we try to do the exact opposite. We want to determine a test set on which we would achieve a low
perplexity on our (static) language model.
3-5 Gram Sliding Window This detector tests if the target word and its neighboring words are
collocated, taking into account three, four, and five grams.
We start by moving a 5-gram context-window over the sentence and signal a match if the association
strength of the ngram exceeds the threshold. Otherwise, we move the window one position to the
right and repeat until either a match occurs or the target word became the most left-hand word in the
window. If no match occurs, the window size is decreased and the procedure repeated.
An example sentence that demonstrates the detection of a three word collocation is depicted in
</bodyText>
<page confidence="0.998215">
35
</page>
<figureCaption confidence="0.995401">
Figure 2: Sliding Window: Collocation detection with window size of three
Figure 2. The target word is forest. The window has to be shifted two times until it is over the phrase
forest fire prevention in order to match.
</figureCaption>
<bodyText confidence="0.999693">
In order to quantify word association strength, we use pointwise mutual information (PMI) (Church
and Hanks, 1990) and obtain the required word frequencies from the Google Web1T corpus (Brants
and Franz, 2006). As PMI is only defined for bigrams, sequences of length n ≥ 3 have to be split
into two units that are pseudo-bigrams.1 For example, calculating the PMI for forest fire prevention,
would either lead to a split such as [forest fire] [prevention] or [forest] [fire prevention].
Following Korkontzelos et al. (2008), we use the so called “pessimistic split” to compute PMI by
using the split with the highest likelihood of all possible splits:
</bodyText>
<equation confidence="0.995678">
P(w1,...,wn)
PMIp,s(w1, ..., wn) = lo9 P(w1,...,wi)P(wi+1,...,wn)
</equation>
<bodyText confidence="0.992202217391304">
Using the highest likelihood split decreases the number of false alarms by the detector. This
is a pessimistic, conservative way of calculating PMI (Hartmann et al., 2012) because the best-
split of many actually collocated words will not have co-occurrences high enough to exceed the
threshold.2 Hence, we increase the precision in detection of true collocations, but lower the recall.
In combination with the high threshold, we limit the detected word-sequences to those which are
extremely frequent to occur in daily life. We expect that the frequency and commonness of such a
word sequence will provide the most ideal circumstance to restore a deleted word.
Bigram Sliding Window We also use a bigram sliding window detector, but found that it does
not work as reliable as the 3-5 gram detector. Thus, we assign a medium reliability. Also, PMI
values are usually lower so that we need to set a different threshold (5 instead of 10) in order to ever
trigger the detector.
Skip Bigrams Words that occur frequently together do not need to be directly adjacent, but can
be separated by other words. To detect such cases, this detector calculates the PMI value between
the target word and adjectives or verbs occurring in an n-word window around it. We consider the
four words to the left and to the right of the target word. The words directly adjacent to the target
word are omitted as this case is already handled by the Bigram Sliding Window detector.
For example, we want to detect cases such as:
It aims to be both, empirical, but novel ____. , → research.
The word empirical is clearly referencing to research in this case.
We assign a medium reliability to this detector and used the same PMI threshold of 5 as for the other
bigram detector.
1“pseudo-bigram” because such a bigram does not necessarily contain two words, it might only be one, but also three or
more depending on the length of the word sequence.
</bodyText>
<footnote confidence="0.784938">
2We use a threshold of 10
</footnote>
<page confidence="0.991082">
36
</page>
<subsectionHeader confidence="0.969483">
2.2 POS Pattern Sliding Window
</subsectionHeader>
<bodyText confidence="0.999987285714286">
This detector works like the other sliding window detectors, but relies on part-of-speech (POS)
ngrams instead of token ngrams. We use a list of POS pattern from Arranz et al. (2005) that were
originally used for detecting multiword expressions. We found that the approach did not work well,
as contexts with low and high ambiguity can have the same POS patterns. For example, “balance of
international payment” (NN-IN-JJ-NN) has the same POS pattern as “car with expensive tires” but
is less ambiguous depending on the position of the cloze item. We thus assign a low reliability, but
still think that the detector can provide supporting evidence for other more important detectors.
</bodyText>
<subsectionHeader confidence="0.998772">
2.3 Word Repetition
</subsectionHeader>
<bodyText confidence="0.9972082">
If the deleted word is repeated later in the sentence, it proposes itself as possible candidate for
solving the blank. The detector matches if the target word occurs a second time in the sentence in
any inflection form.
Go through the ____and mark each affected slice., — slices
We found that this is a strong indicator.
</bodyText>
<subsectionHeader confidence="0.988185">
2.4 Distributional Thesaurus
</subsectionHeader>
<bodyText confidence="0.982020583333333">
A sentence may contain many words that are semantically related to the target word, but that do not
appear in a collocation:
The compressor is necessitated by ____which injects fuel into the cylinder — air
In the example, we show semantically related words in bold-face. They help to guide the reader
towards the right choice for the blank.
We detect semantically related words using the distributional thesaurus from Biemann and Riedl
(2013) that was computed over the top 1 million words of the English Google Books Corpus
(Goldberg and Orwant, 2013). We retrieve the fifty highest ranked words that are associated with
our target word in the thesaurus. These words are compared with all words occurring in the sentence.
If two or more associated words are found in the sentence the detector matches.
Unfortunately, the detector fires quite often, even in cases of a rather weak relationship between two
words. We thus assign a low reliability.
</bodyText>
<subsectionHeader confidence="0.989985">
2.5 Hearst Patterns
</subsectionHeader>
<bodyText confidence="0.99878575">
Hearst patterns (Hearst, 1992) are lexico-syntactic pattern like “X such as Y, and Z” that are
frequently used to detect hierarchical relationships between words. We argue that wherever such a
pattern can be found in a sentence, the ambiguity should be reduced as the reader can be guided by
the explicitly stated relationships.
</bodyText>
<footnote confidence="0.378121">
Already small amounts of ____
such as beer or wine can affect your ability to drive. — alcohol
</footnote>
<page confidence="0.99852">
37
</page>
<bodyText confidence="0.999801714285714">
This structure allows the reader to abstract to the more general, deleted word. In the case of beer
and wine, the reader may conclude that the deleted word is alcohol. Thus, if the more general word
is deleted, the list of more specific words allows determining the deleted word. Note that the other
direction is usually more difficult, e.g. ‘fruit — orange’ is harder than ‘orange — fruit’ as there are
many different fruit, but only one more general concept.
We found that the detector works quite well, but produces some false alarms. We thus assign a
medium reliability.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999980761904762">
In order to evaluate our methods, we have to determine how often human subjects are able to
complete the cloze items with the target word. If subjects consistently only give the correct answer,
our method would work perfectly. However, as this might be too optimistic, we will measure how
often the correct word (the originally deleted word) is provided and how many different alternatives
human subjects provide.
In order to create the evaluation dataset, we randomly select sentences from the UkWaC corpus
(Baroni et al., 2009) which contains general-purpose text obtained from uk-domain websites. The
sentences are filtered by their reading-difficulty using the Flesch–Kincaid test (Kincaid et al., 1975):
we remove all sentences requiring more than 10 years of school-education. We then only keep
sentences that contain a noun from a randomly chosen subset of common nouns. We then apply our
method to the remaining sentences obtaining weight scores from 0 to 52, where a higher number
means more evidence (less ambiguity). From this ranking, we select the 25 top-ranked sentences
and the 25 bottom-ranked sentences. The bottom-ranked sentences have a score-range of 11 to 16,
thus, none of the strong detectors contributed to their score. We compiled a cloze test from both
collections (replacing the common noun with a blank) and asked volunteer participants to solve the
items.
Overall, 30 native speakers of English completed the study, which was conducted online. Before
the actual study, participants were shown a detailed manual describing the study. Participants were
asked not to cheat and not to use any search engines or ask other people for second opinions. If they
could not come up with an answer, participants were instructed to move to the next sentence. The
fifty sentences were offered over 4 web pages; each page briefly repeated the manual.
</bodyText>
<sectionHeader confidence="0.999625" genericHeader="evaluation">
4 Results &amp; Discussion
</sectionHeader>
<bodyText confidence="0.999962727272727">
Figure 3 shows the result of the study in terms of how often participants provided the target word.
For the top-ranked items, participants provided the target word on average in 59.9% of the cases,
while the average was only 36.5% in the bottom-ranked group. This clearly shows that our method is
effective in reducing the ambiguity of cloze items. However, we did not succeed in fully eliminating
ambiguity.
The achieved ambiguity reduction can also be measured by considering how many different answers
were provided for solving a cloze item. The top-ranked cloze items had an average of 4.5 different
answers per item including the target word. Thus, three to four alternative answers were always
provided although the frequency for choosing the target word clearly outweighs those of the
alternatives. For the bottom-ranked cloze items, participants provided on average almost twice as
much different answers (8.4).
</bodyText>
<page confidence="0.997655">
38
</page>
<figureCaption confidence="0.741683">
top-ranked bottom-ranked
Figure 3: Ratio of participants providing correct answer for top-ranked and bottom-ranked items
</figureCaption>
<subsectionHeader confidence="0.920324">
4.1 Error Analysis
</subsectionHeader>
<bodyText confidence="0.982107857142857">
Our methods worked well in some cases but also yielded poorly performing cases. We discuss
subsequently examples for both.
Top-ranked (correct) The following sentences are examples where the user study confirmed a
low-ambiguity context with at least 80% of all human subjects providing the target word.
(A) It is important that you spend some ____over the next few weeks to help your
kitten adjust to its new family so please take the time to read our information before
you get your new kitten. → time (96%)
</bodyText>
<listItem confidence="0.649189">
(B) Effects : A trip can take from 20 minutes to an ____to start and usually lasts about
12 hours. → hour (93%)
(C) In 1974 the former girls’ premises were occupied by Orange Hill junior high ____
and the boys ’ by Orange Hill senior high school. → school (86%)
</listItem>
<bodyText confidence="0.997324333333333">
The cloze items in sentence (A-C) is placed within very common phrases. In (A) and (C) the local
word context alone was sufficiently specific to fill the item with its original word. In sentence (B)
we have even more hints: (i) the target word is repeated within the sentence, and (ii) the determiner
“an” right in front of the blank further limits the possible candidate answers.
Top-ranked (wrong) For the following sentences only 20% or less of the participants provided
the target word:
(E) They have various difficulties including learning difficulties, physical difficulties,
emotional and behavioural ____as well as mental health problems. → problems (20%)
(F) All stores offer a wide and varied ____of hot tubs and spas, please call us to arrange
a dip at anytime on 0800 085 8880 or go to the showroom locator to find your nearest
branch. → range (16%)
(G) The ground floor has a well equipped living room, dining ____and kitchen and the
private seating area is only a few steps away, in the garden. → area (13%)
The sentences in the top-ranked group are mainly from matches of collocation-based detectors and
word redundancy. In sentence (E) difficulties (30%) is provided most frequently, but was not used in
</bodyText>
<figure confidence="0.996736416666667">
100
80
60
40
20
0
AVG: 59.9
-23.3
AVG: 36.5
top-ranked
bottom-ranked
% target word
</figure>
<page confidence="0.99848">
39
</page>
<bodyText confidence="0.999015285714286">
the original sentence probably due to stylistic considerations. In (G) room (80%) is selected due
to a similar pattern. In sentence (F) the most frequent answer is selection (63%) instead of range.
Selection is a synonym of range in this context. In order to correctly rank cases like this we need to
take competing candidates into account in the ranking process.
Bottom-ranked (wrong) Two items stand out in the bottom-ranked group, because participants
provided the target word in over 80% of cases so they should actually be in the top-ranked group.
(H) I managed to ride the bike to work with the handlebars pointing in a different ____
to the wheel and almost in tears. — direction (96%)
(I) The disease usually starts in the wrists, hands or feet, and can spread to other joints
and other ____of the body. — parts (83%)
In case of sentence (H) pointing in a different direction missed the threshold, but is actually a quite
strong collocation. In sentence (I), the enumeration of body parts provided the needed context to
determine the deleted word. Our Hearst detector matched, but its weight was not enough to put the
item in the top-ranked group.
Bottom-ranked (correct) The following two sentences were not answered by a single participants
with the target word. Although they belong in the bottom-ranked group, the especially low score
deserves some discussion.
(J) This new edition contains many more illustrations and anecdotes, and two new
chapters on ____’s surviving Bristol Channel pilot cutters and their restoration and
model making of these craft. — today (0%)
(K) Anta Scotland Ltd Specialise in fabrics and ceramics and have various contemporary
versions of traditional ____such as tartans. — designs (0%)
In case of (J), the possessive case of the deleted word confused almost all participants. This blank
remained unanswered quite often (40%). If an answer was provided it was rather a proper noun.
Sentence (K) was most frequently answered with fabrics (40%). The Hearst detector matched, but
we assume that our participants were unfamiliar with the word tartans, and thus could not take
advantage of this structural hint. This illustrates why we only put medium weights on the Hearst
detector. The conditions under which a detector is useful remains difficult to predict.
</bodyText>
<sectionHeader confidence="0.996711" genericHeader="conclusions">
5 Conclusion &amp; Future Work
</sectionHeader>
<bodyText confidence="0.998148875">
In this paper, we discussed methods for determining cloze items with reduced ambiguity. We
introduced seven detectors in order to find such items. We found that our methods are able to
significantly reduce the ambiguity of blanks, but that we could not reach our goal of a single valid
answer per item.
In future work, we want to improve the detectors in order to further reduce ambiguity of selected
sentences. We also need to address the problem that cloze items might become to easy, as e.g. word
repetition is a strong detector, but obviously not very useful when generating a language proficiency
test.
</bodyText>
<page confidence="0.997839">
40
</page>
<sectionHeader confidence="0.98951" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998345142857143">
Aliprandi, C., Carmignani, N., and Mancarella, P. (2007). In International Journal of Computing
and Information Sciences, volume 5, pages 79–85.
Arranz, V., Atserias, J., and Castillo, M. (2005). Multiwords and word sense disambiguation. In
Gelbukh, A., editor, Computational Linguistics and Intelligent Text Processing, volume 3406 of
Lecture Notes in Computer Science, pages 250–262. Springer Berlin Heidelberg.
Baroni, M., Bernardini, S., Ferraresi, A., and Zanchetta, E. (2009). The wacky wide web: A
collection of very large linguistically processed web-crawled corpora. Language Resources and
Evaluation, 3:209–226.
Biemann, C. and Riedl, M. (2013). Text: now in 2d! a framework for lexical expansion with
contextual similarity. Journal of Language Modelling, 1:55–95.
Brants, T. and Franz, A. (2006). Web 1t 5-gram corpus version 1.1. Linguistic Data Consortium.
Chen, S., Beeferman, D., and Rosenfeld, R. (1998). Evaluation Metrics for Language Models.
In DARPA Broadcast News Transcription and Understanding Workshop (BNTUW), Lansdowne,
Virginia, USA.
Church, K. W. and Hanks, P. (1990). Word association norms, mutual information, and lexicography.
Comput. Linguist., 16(1):22–29.
Goldberg, Y. and Orwant, J. (2013). A dataset of syntactic-ngrams over time from a very large
corpus of english books.
Gonzáles, A. B. (1996). Testing english as a foreign language: an overview and some methodolog-
ical considerations. Revista española de lingüística aplicada, 11:71–94.
Hartmann, S., Szarvas, G., and Gurevych, I. (2012). Mining multiword terms from wikipedia. In
Pazienza, M. T. and Stellato, A., editors, Semi-Automatic Ontology Development: Processes and
Resources, pages 226–258. IGI Global, Hershey, PA, USA.
Hearst, M. A. (1992). Automatic acquisition of hyponyms from large text corpora. In In Proceed-
ings of the 14th International Conference on Computational Linguistics, pages 539–545.
Kincaid, P. J., Fishburne, R. P. J., Rogers, R. L., and Chissom, B. S. (1975). Derivation of new
readability formulas (automated readability index, fog cound and flesch reading ease formula) for
navy enlisted personnel. Naval Technical Training Command: Research Branch Report 8-75.
Klein-Braley, C. and Raatz, U. (1982). Der c-test: ein neuer ansatz zur messung von allgemeiner
sprachbeherrschung. AKS-Rundbrief, pages 23–37.
Korkontzelos, I., Klapaftis, I., and Manandhar, S. (2008). Reviewing and evaluating automatic term
recognition techniques. In Nordström, B. and Ranta, A., editors, Advances in Natural Language
Processing, volume 5221 of Lecture Notes in Computer Science, pages 248–259. Springer Berlin
Heidelberg.
Lee, J. and Seneff, S. (2007). Automatic generation of cloze items for prepositions. Interspeech.
</reference>
<page confidence="0.991143">
41
</page>
<reference confidence="0.98651324">
Li, J. and Hirst, G. (2005). Semantic knowledge in word completion. In Proceedings of the 7th
international ACM SIGACCESS conference on Computers and accessibility, Assets ’05, pages
121–128, New York, NY, USA. ACM.
O’Toole, J. M. and King, R. A. R. (2011). The deceptive mean: Conceptual scoring of cloze entries
differentially advantages more able readers. Language Testing.
Sakaguchi, K., Arase, Y., and Komachi, M. (2013). Discriminative approach to fill-in-the-blank
quiz generation for language learners. In Proceedings of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 238–242.
Smith, S. and Avinesh, P. (2010). Gap-fill tests for language learners: Corpus-driven item
generation. In Proceedings of ICON-2010: 8th International Conference on Natural Language
Processing.
Sumita, E., Sugaya, F., and Yamamoto, S. (2005). Measuring non-native speakers’ proficiency of
english by using a test with automatically-generated fill-in-the-blank questions. In Proceedings of
the second workshop on Building Educational Applications Using NLP, EdAppsNLP 05, pages
61–68, Stroudsburg, PA, USA. Association for Computational Linguistics.
Taylor, W. (1953). Cloze procedure: A new tool for measuring readability. Journalism Quarterly,
30:415–433.
Taylor, W. (1956). Recent developments in the use of cloze procedure. Journalism Quarterly,
33:42.
Trnka, K. (2008). Adaptive language modeling for word prediction. In Proceedings of the ACL-08:
HLT Student Research Workshop, pages 61–66, Columbus, Ohio. Association for Computational
Linguistics.
Zesch, T. and Melamud, O. (2014). Automatic Generation of Challenging Distractors Using
Context-Sensitive Inference Rules. In Proceedings of the 9th Workshop on Innovative Use of NLP
for Building Educational Applications at ACL, Baltimore, USA.
</reference>
<page confidence="0.999271">
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.658711">
<title confidence="0.999824">Towards Automatic Scoring of Cloze Items Selecting Low-Ambiguity Contexts</title>
<author confidence="0.999373">Tobias Horsmann</author>
<author confidence="0.999373">Torsten Zesch</author>
<affiliation confidence="0.996517">Language Technology University of Duisburg-Essen</affiliation>
<address confidence="0.991935">Germany</address>
<email confidence="0.996076">tobias.horsmann@uni-due.de</email>
<email confidence="0.996076">torsten.zesch@uni-due.de</email>
<abstract confidence="0.964908875">In second language learning, cloze tests (also known as fill-in-the-blank tests) are frequently used for assessing the learning progress of students. While preparation effort for these tests is low, scoring needs to be done manually, as there usually is a huge number of correct solutions. In this paper, we examine whether the ambiguity of cloze items can be lowered to a point where automatic scoring becomes possible. We utilize the local context of a word to collect evidence of low-ambiguity. We do that by seeking for collocated word sequences, but also taking structural information on sentence level into account. We evaluate the effectiveness of our method in a user study on cloze items ranked by our method. For the top-ranked items (lowest ambiguity) the subjects provide the target word significantly more often than for the bottom-ranked items (59.9% vs. 36.5%). While this shows the potential of our method, we did not succeed in fully eliminating ambiguity. Thus, further research is necessary before fully automatic scoring becomes possible. tests, language proficiency tests, automatic scoring. Tobias Horsmann and Torsten Zesch 2014. Towards automatic scoring of cloze items by selecting low-ambiguity contexts. of the third workshop on NLP for computer-assisted language learning. Proceedings Series 22 / Linköping Electronic Conference Proceedings 107: 33–42.</abstract>
<intro confidence="0.784734">33</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Aliprandi</author>
<author>N Carmignani</author>
<author>P Mancarella</author>
</authors>
<date>2007</date>
<booktitle>In International Journal of Computing and Information Sciences,</booktitle>
<volume>5</volume>
<pages>79--85</pages>
<contexts>
<context position="5974" citStr="Aliprandi et al., 2007" startWordPosition="940" endWordPosition="943">ng data is available. In the remainder of this section, we describe our detectors in more detail. The detectors are not mutually exclusive and a word might be detected by several detectors. 2.1 Collocation-based Detectors Subsequently, we will introduce three detectors that are variations of testing a word for being collocated with one or more nearby words. These detectors are motivated by the research field of word prediction where an algorithm tries to determine the next word a user might want to type on a keyboard based on the already entered word sequence (Li and Hirst, 2005; Trnka, 2008; Aliprandi et al., 2007). We hypothesize that if a word is easy to predict it should also be less ambiguous in that context. We further expect that longer collocated sequences are easier to predict than shorter ones. Another way to look at this problem is its relation to language model perplexity (Chen et al., 1998). Classically, perplexity makes a statement about how well a model predicts test data. In our task, we try to do the exact opposite. We want to determine a test set on which we would achieve a low perplexity on our (static) language model. 3-5 Gram Sliding Window This detector tests if the target word and </context>
</contexts>
<marker>Aliprandi, Carmignani, Mancarella, 2007</marker>
<rawString>Aliprandi, C., Carmignani, N., and Mancarella, P. (2007). In International Journal of Computing and Information Sciences, volume 5, pages 79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Arranz</author>
<author>J Atserias</author>
<author>M Castillo</author>
</authors>
<title>Multiwords and word sense disambiguation.</title>
<date>2005</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>3406</volume>
<pages>250--262</pages>
<editor>In Gelbukh, A., editor,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="10237" citStr="Arranz et al. (2005)" startWordPosition="1665" endWordPosition="1668"> ____. , → research. The word empirical is clearly referencing to research in this case. We assign a medium reliability to this detector and used the same PMI threshold of 5 as for the other bigram detector. 1“pseudo-bigram” because such a bigram does not necessarily contain two words, it might only be one, but also three or more depending on the length of the word sequence. 2We use a threshold of 10 36 2.2 POS Pattern Sliding Window This detector works like the other sliding window detectors, but relies on part-of-speech (POS) ngrams instead of token ngrams. We use a list of POS pattern from Arranz et al. (2005) that were originally used for detecting multiword expressions. We found that the approach did not work well, as contexts with low and high ambiguity can have the same POS patterns. For example, “balance of international payment” (NN-IN-JJ-NN) has the same POS pattern as “car with expensive tires” but is less ambiguous depending on the position of the cloze item. We thus assign a low reliability, but still think that the detector can provide supporting evidence for other more important detectors. 2.3 Word Repetition If the deleted word is repeated later in the sentence, it proposes itself as p</context>
</contexts>
<marker>Arranz, Atserias, Castillo, 2005</marker>
<rawString>Arranz, V., Atserias, J., and Castillo, M. (2005). Multiwords and word sense disambiguation. In Gelbukh, A., editor, Computational Linguistics and Intelligent Text Processing, volume 3406 of Lecture Notes in Computer Science, pages 250–262. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>S Bernardini</author>
<author>A Ferraresi</author>
<author>E Zanchetta</author>
</authors>
<title>The wacky wide web: A collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>3--209</pages>
<contexts>
<context position="13628" citStr="Baroni et al., 2009" startWordPosition="2226" endWordPosition="2229"> but produces some false alarms. We thus assign a medium reliability. 3 Experimental Setup In order to evaluate our methods, we have to determine how often human subjects are able to complete the cloze items with the target word. If subjects consistently only give the correct answer, our method would work perfectly. However, as this might be too optimistic, we will measure how often the correct word (the originally deleted word) is provided and how many different alternatives human subjects provide. In order to create the evaluation dataset, we randomly select sentences from the UkWaC corpus (Baroni et al., 2009) which contains general-purpose text obtained from uk-domain websites. The sentences are filtered by their reading-difficulty using the Flesch–Kincaid test (Kincaid et al., 1975): we remove all sentences requiring more than 10 years of school-education. We then only keep sentences that contain a noun from a randomly chosen subset of common nouns. We then apply our method to the remaining sentences obtaining weight scores from 0 to 52, where a higher number means more evidence (less ambiguity). From this ranking, we select the 25 top-ranked sentences and the 25 bottom-ranked sentences. The bott</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Baroni, M., Bernardini, S., Ferraresi, A., and Zanchetta, E. (2009). The wacky wide web: A collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation, 3:209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Biemann</author>
<author>M Riedl</author>
</authors>
<title>Text: now in 2d! a framework for lexical expansion with contextual similarity.</title>
<date>2013</date>
<journal>Journal of Language Modelling,</journal>
<pages>1--55</pages>
<contexts>
<context position="11551" citStr="Biemann and Riedl (2013)" startWordPosition="1879" endWordPosition="1882">second time in the sentence in any inflection form. Go through the ____and mark each affected slice., — slices We found that this is a strong indicator. 2.4 Distributional Thesaurus A sentence may contain many words that are semantically related to the target word, but that do not appear in a collocation: The compressor is necessitated by ____which injects fuel into the cylinder — air In the example, we show semantically related words in bold-face. They help to guide the reader towards the right choice for the blank. We detect semantically related words using the distributional thesaurus from Biemann and Riedl (2013) that was computed over the top 1 million words of the English Google Books Corpus (Goldberg and Orwant, 2013). We retrieve the fifty highest ranked words that are associated with our target word in the thesaurus. These words are compared with all words occurring in the sentence. If two or more associated words are found in the sentence the detector matches. Unfortunately, the detector fires quite often, even in cases of a rather weak relationship between two words. We thus assign a low reliability. 2.5 Hearst Patterns Hearst patterns (Hearst, 1992) are lexico-syntactic pattern like “X such as</context>
</contexts>
<marker>Biemann, Riedl, 2013</marker>
<rawString>Biemann, C. and Riedl, M. (2013). Text: now in 2d! a framework for lexical expansion with contextual similarity. Journal of Language Modelling, 1:55–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Franz</author>
</authors>
<title>Web 1t 5-gram corpus version 1.1. Linguistic Data Consortium.</title>
<date>2006</date>
<contexts>
<context position="7569" citStr="Brants and Franz, 2006" startWordPosition="1212" endWordPosition="1215">ord in the window. If no match occurs, the window size is decreased and the procedure repeated. An example sentence that demonstrates the detection of a three word collocation is depicted in 35 Figure 2: Sliding Window: Collocation detection with window size of three Figure 2. The target word is forest. The window has to be shifted two times until it is over the phrase forest fire prevention in order to match. In order to quantify word association strength, we use pointwise mutual information (PMI) (Church and Hanks, 1990) and obtain the required word frequencies from the Google Web1T corpus (Brants and Franz, 2006). As PMI is only defined for bigrams, sequences of length n ≥ 3 have to be split into two units that are pseudo-bigrams.1 For example, calculating the PMI for forest fire prevention, would either lead to a split such as [forest fire] [prevention] or [forest] [fire prevention]. Following Korkontzelos et al. (2008), we use the so called “pessimistic split” to compute PMI by using the split with the highest likelihood of all possible splits: P(w1,...,wn) PMIp,s(w1, ..., wn) = lo9 P(w1,...,wi)P(wi+1,...,wn) Using the highest likelihood split decreases the number of false alarms by the detector. Th</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Brants, T. and Franz, A. (2006). Web 1t 5-gram corpus version 1.1. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>D Beeferman</author>
<author>R Rosenfeld</author>
</authors>
<title>Evaluation Metrics for Language Models.</title>
<date>1998</date>
<booktitle>In DARPA Broadcast News Transcription and Understanding Workshop (BNTUW),</booktitle>
<location>Lansdowne, Virginia, USA.</location>
<contexts>
<context position="6267" citStr="Chen et al., 1998" startWordPosition="991" endWordPosition="994">ng a word for being collocated with one or more nearby words. These detectors are motivated by the research field of word prediction where an algorithm tries to determine the next word a user might want to type on a keyboard based on the already entered word sequence (Li and Hirst, 2005; Trnka, 2008; Aliprandi et al., 2007). We hypothesize that if a word is easy to predict it should also be less ambiguous in that context. We further expect that longer collocated sequences are easier to predict than shorter ones. Another way to look at this problem is its relation to language model perplexity (Chen et al., 1998). Classically, perplexity makes a statement about how well a model predicts test data. In our task, we try to do the exact opposite. We want to determine a test set on which we would achieve a low perplexity on our (static) language model. 3-5 Gram Sliding Window This detector tests if the target word and its neighboring words are collocated, taking into account three, four, and five grams. We start by moving a 5-gram context-window over the sentence and signal a match if the association strength of the ngram exceeds the threshold. Otherwise, we move the window one position to the right and re</context>
</contexts>
<marker>Chen, Beeferman, Rosenfeld, 1998</marker>
<rawString>Chen, S., Beeferman, D., and Rosenfeld, R. (1998). Evaluation Metrics for Language Models. In DARPA Broadcast News Transcription and Understanding Workshop (BNTUW), Lansdowne, Virginia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Comput. Linguist.,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="7474" citStr="Church and Hanks, 1990" startWordPosition="1197" endWordPosition="1200">the right and repeat until either a match occurs or the target word became the most left-hand word in the window. If no match occurs, the window size is decreased and the procedure repeated. An example sentence that demonstrates the detection of a three word collocation is depicted in 35 Figure 2: Sliding Window: Collocation detection with window size of three Figure 2. The target word is forest. The window has to be shifted two times until it is over the phrase forest fire prevention in order to match. In order to quantify word association strength, we use pointwise mutual information (PMI) (Church and Hanks, 1990) and obtain the required word frequencies from the Google Web1T corpus (Brants and Franz, 2006). As PMI is only defined for bigrams, sequences of length n ≥ 3 have to be split into two units that are pseudo-bigrams.1 For example, calculating the PMI for forest fire prevention, would either lead to a split such as [forest fire] [prevention] or [forest] [fire prevention]. Following Korkontzelos et al. (2008), we use the so called “pessimistic split” to compute PMI by using the split with the highest likelihood of all possible splits: P(w1,...,wn) PMIp,s(w1, ..., wn) = lo9 P(w1,...,wi)P(wi+1,...,</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Church, K. W. and Hanks, P. (1990). Word association norms, mutual information, and lexicography. Comput. Linguist., 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Goldberg</author>
<author>J Orwant</author>
</authors>
<title>A dataset of syntactic-ngrams over time from a very large corpus of english books.</title>
<date>2013</date>
<contexts>
<context position="11661" citStr="Goldberg and Orwant, 2013" startWordPosition="1898" endWordPosition="1901">es We found that this is a strong indicator. 2.4 Distributional Thesaurus A sentence may contain many words that are semantically related to the target word, but that do not appear in a collocation: The compressor is necessitated by ____which injects fuel into the cylinder — air In the example, we show semantically related words in bold-face. They help to guide the reader towards the right choice for the blank. We detect semantically related words using the distributional thesaurus from Biemann and Riedl (2013) that was computed over the top 1 million words of the English Google Books Corpus (Goldberg and Orwant, 2013). We retrieve the fifty highest ranked words that are associated with our target word in the thesaurus. These words are compared with all words occurring in the sentence. If two or more associated words are found in the sentence the detector matches. Unfortunately, the detector fires quite often, even in cases of a rather weak relationship between two words. We thus assign a low reliability. 2.5 Hearst Patterns Hearst patterns (Hearst, 1992) are lexico-syntactic pattern like “X such as Y, and Z” that are frequently used to detect hierarchical relationships between words. We argue that wherever</context>
</contexts>
<marker>Goldberg, Orwant, 2013</marker>
<rawString>Goldberg, Y. and Orwant, J. (2013). A dataset of syntactic-ngrams over time from a very large corpus of english books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B Gonzáles</author>
</authors>
<title>Testing english as a foreign language: an overview and some methodological considerations. Revista española de lingüística aplicada,</title>
<date>1996</date>
<pages>11--71</pages>
<contexts>
<context position="3398" citStr="Gonzáles, 1996" startWordPosition="519" endWordPosition="520">ative answer options - called distractors (Sumita et al., 2005). Determining whether a distractor is actually another valid solution is equivalent to the problem described above. Thus, finding good distractors is still an unsolved problem that attracts a lot of research (Lee and Seneff, 2007; Smith and Avinesh, 2010; Sakaguchi et al., 2013; Zesch and Melamud, 2014). Furthermore, providing distractors for cloze items also considerably changes the nature of the task, as distractors are recognition stimuli, i.e. the student recognizes the correct answer rather than having to actively produce it (Gonzáles, 1996). Now consider example sentence (2): (2) I went to the ____today and now I have sand in my shoes. Most people would come up with beach or maybe desert, while other solutions are highly unlikely, which means that the blank is less ambiguous than example (1). This leads to our research question, whether it is possible to find contexts that are specific enough to only allow one correct solution. Such a setup would dramatically simplify automatic scoring. We limit the scope of this work to determine low-ambiguity contexts for single-word nouns and leave other parts of speech for future work. In th</context>
</contexts>
<marker>Gonzáles, 1996</marker>
<rawString>Gonzáles, A. B. (1996). Testing english as a foreign language: an overview and some methodological considerations. Revista española de lingüística aplicada, 11:71–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hartmann</author>
<author>G Szarvas</author>
<author>I Gurevych</author>
</authors>
<title>Mining multiword terms from wikipedia.</title>
<date>2012</date>
<booktitle>Semi-Automatic Ontology Development: Processes and Resources,</booktitle>
<pages>226--258</pages>
<editor>In Pazienza, M. T. and Stellato, A., editors,</editor>
<location>Hershey, PA, USA.</location>
<contexts>
<context position="8249" citStr="Hartmann et al., 2012" startWordPosition="1320" endWordPosition="1323">n ≥ 3 have to be split into two units that are pseudo-bigrams.1 For example, calculating the PMI for forest fire prevention, would either lead to a split such as [forest fire] [prevention] or [forest] [fire prevention]. Following Korkontzelos et al. (2008), we use the so called “pessimistic split” to compute PMI by using the split with the highest likelihood of all possible splits: P(w1,...,wn) PMIp,s(w1, ..., wn) = lo9 P(w1,...,wi)P(wi+1,...,wn) Using the highest likelihood split decreases the number of false alarms by the detector. This is a pessimistic, conservative way of calculating PMI (Hartmann et al., 2012) because the bestsplit of many actually collocated words will not have co-occurrences high enough to exceed the threshold.2 Hence, we increase the precision in detection of true collocations, but lower the recall. In combination with the high threshold, we limit the detected word-sequences to those which are extremely frequent to occur in daily life. We expect that the frequency and commonness of such a word sequence will provide the most ideal circumstance to restore a deleted word. Bigram Sliding Window We also use a bigram sliding window detector, but found that it does not work as reliable</context>
</contexts>
<marker>Hartmann, Szarvas, Gurevych, 2012</marker>
<rawString>Hartmann, S., Szarvas, G., and Gurevych, I. (2012). Mining multiword terms from wikipedia. In Pazienza, M. T. and Stellato, A., editors, Semi-Automatic Ontology Development: Processes and Resources, pages 226–258. IGI Global, Hershey, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora. In</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="12106" citStr="Hearst, 1992" startWordPosition="1972" endWordPosition="1973">the distributional thesaurus from Biemann and Riedl (2013) that was computed over the top 1 million words of the English Google Books Corpus (Goldberg and Orwant, 2013). We retrieve the fifty highest ranked words that are associated with our target word in the thesaurus. These words are compared with all words occurring in the sentence. If two or more associated words are found in the sentence the detector matches. Unfortunately, the detector fires quite often, even in cases of a rather weak relationship between two words. We thus assign a low reliability. 2.5 Hearst Patterns Hearst patterns (Hearst, 1992) are lexico-syntactic pattern like “X such as Y, and Z” that are frequently used to detect hierarchical relationships between words. We argue that wherever such a pattern can be found in a sentence, the ambiguity should be reduced as the reader can be guided by the explicitly stated relationships. Already small amounts of ____ such as beer or wine can affect your ability to drive. — alcohol 37 This structure allows the reader to abstract to the more general, deleted word. In the case of beer and wine, the reader may conclude that the deleted word is alcohol. Thus, if the more general word is d</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Hearst, M. A. (1992). Automatic acquisition of hyponyms from large text corpora. In In Proceedings of the 14th International Conference on Computational Linguistics, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Kincaid</author>
<author>R P J Fishburne</author>
<author>R L Rogers</author>
<author>B S Chissom</author>
</authors>
<title>Derivation of new readability formulas (automated readability index, fog cound and flesch reading ease formula) for navy enlisted personnel. Naval Technical Training Command: Research Branch</title>
<date>1975</date>
<tech>Report 8-75.</tech>
<contexts>
<context position="13806" citStr="Kincaid et al., 1975" startWordPosition="2249" endWordPosition="2252">le to complete the cloze items with the target word. If subjects consistently only give the correct answer, our method would work perfectly. However, as this might be too optimistic, we will measure how often the correct word (the originally deleted word) is provided and how many different alternatives human subjects provide. In order to create the evaluation dataset, we randomly select sentences from the UkWaC corpus (Baroni et al., 2009) which contains general-purpose text obtained from uk-domain websites. The sentences are filtered by their reading-difficulty using the Flesch–Kincaid test (Kincaid et al., 1975): we remove all sentences requiring more than 10 years of school-education. We then only keep sentences that contain a noun from a randomly chosen subset of common nouns. We then apply our method to the remaining sentences obtaining weight scores from 0 to 52, where a higher number means more evidence (less ambiguity). From this ranking, we select the 25 top-ranked sentences and the 25 bottom-ranked sentences. The bottom-ranked sentences have a score-range of 11 to 16, thus, none of the strong detectors contributed to their score. We compiled a cloze test from both collections (replacing the c</context>
</contexts>
<marker>Kincaid, Fishburne, Rogers, Chissom, 1975</marker>
<rawString>Kincaid, P. J., Fishburne, R. P. J., Rogers, R. L., and Chissom, B. S. (1975). Derivation of new readability formulas (automated readability index, fog cound and flesch reading ease formula) for navy enlisted personnel. Naval Technical Training Command: Research Branch Report 8-75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Klein-Braley</author>
<author>U Raatz</author>
</authors>
<title>Der c-test: ein neuer ansatz zur messung von allgemeiner sprachbeherrschung. AKS-Rundbrief,</title>
<date>1982</date>
<pages>23--37</pages>
<contexts>
<context position="2389" citStr="Klein-Braley and Raatz, 1982" startWordPosition="360" endWordPosition="363">. A cloze item consists of a sentence with usually one word being blanked. The learner’s task is to find the correct word for the blank: (1) He sold his ____yesterday below price. As we can see from example (1), blanks can be quite ambiguous, i.e. a very high number of correct solutions exists. In this example, a wide range of nouns are acceptable solutions including books, house, or bike, but also daughter or kidney cannot be ruled out in this (quite limited) context. Such ambiguity is not only a problem for language learners, but even native speakers frequently fail when facing such a task (Klein-Braley and Raatz, 1982). If cloze items should be automatically generated and scored, ambiguous items pose a serious problem, as we only know for sure one correct answer namely the one that was used in the original sentence. Students might get frustrated if they provide a valid solution that is not recognized by the system. The same problem affects an alternative solution to the problem: providing a list of alternative answer options - called distractors (Sumita et al., 2005). Determining whether a distractor is actually another valid solution is equivalent to the problem described above. Thus, finding good distract</context>
</contexts>
<marker>Klein-Braley, Raatz, 1982</marker>
<rawString>Klein-Braley, C. and Raatz, U. (1982). Der c-test: ein neuer ansatz zur messung von allgemeiner sprachbeherrschung. AKS-Rundbrief, pages 23–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Korkontzelos</author>
<author>I Klapaftis</author>
<author>S Manandhar</author>
</authors>
<title>Reviewing and evaluating automatic term recognition techniques.</title>
<date>2008</date>
<booktitle>Advances in Natural Language Processing,</booktitle>
<volume>5221</volume>
<pages>248--259</pages>
<editor>In Nordström, B. and Ranta, A., editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="7883" citStr="Korkontzelos et al. (2008)" startWordPosition="1264" endWordPosition="1267">ndow has to be shifted two times until it is over the phrase forest fire prevention in order to match. In order to quantify word association strength, we use pointwise mutual information (PMI) (Church and Hanks, 1990) and obtain the required word frequencies from the Google Web1T corpus (Brants and Franz, 2006). As PMI is only defined for bigrams, sequences of length n ≥ 3 have to be split into two units that are pseudo-bigrams.1 For example, calculating the PMI for forest fire prevention, would either lead to a split such as [forest fire] [prevention] or [forest] [fire prevention]. Following Korkontzelos et al. (2008), we use the so called “pessimistic split” to compute PMI by using the split with the highest likelihood of all possible splits: P(w1,...,wn) PMIp,s(w1, ..., wn) = lo9 P(w1,...,wi)P(wi+1,...,wn) Using the highest likelihood split decreases the number of false alarms by the detector. This is a pessimistic, conservative way of calculating PMI (Hartmann et al., 2012) because the bestsplit of many actually collocated words will not have co-occurrences high enough to exceed the threshold.2 Hence, we increase the precision in detection of true collocations, but lower the recall. In combination with </context>
</contexts>
<marker>Korkontzelos, Klapaftis, Manandhar, 2008</marker>
<rawString>Korkontzelos, I., Klapaftis, I., and Manandhar, S. (2008). Reviewing and evaluating automatic term recognition techniques. In Nordström, B. and Ranta, A., editors, Advances in Natural Language Processing, volume 5221 of Lecture Notes in Computer Science, pages 248–259. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
<author>S Seneff</author>
</authors>
<title>Automatic generation of cloze items for prepositions.</title>
<date>2007</date>
<journal>Interspeech.</journal>
<contexts>
<context position="3075" citStr="Lee and Seneff, 2007" startWordPosition="470" endWordPosition="473">iguous items pose a serious problem, as we only know for sure one correct answer namely the one that was used in the original sentence. Students might get frustrated if they provide a valid solution that is not recognized by the system. The same problem affects an alternative solution to the problem: providing a list of alternative answer options - called distractors (Sumita et al., 2005). Determining whether a distractor is actually another valid solution is equivalent to the problem described above. Thus, finding good distractors is still an unsolved problem that attracts a lot of research (Lee and Seneff, 2007; Smith and Avinesh, 2010; Sakaguchi et al., 2013; Zesch and Melamud, 2014). Furthermore, providing distractors for cloze items also considerably changes the nature of the task, as distractors are recognition stimuli, i.e. the student recognizes the correct answer rather than having to actively produce it (Gonzáles, 1996). Now consider example sentence (2): (2) I went to the ____today and now I have sand in my shoes. Most people would come up with beach or maybe desert, while other solutions are highly unlikely, which means that the blank is less ambiguous than example (1). This leads to our r</context>
</contexts>
<marker>Lee, Seneff, 2007</marker>
<rawString>Lee, J. and Seneff, S. (2007). Automatic generation of cloze items for prepositions. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Li</author>
<author>G Hirst</author>
</authors>
<title>Semantic knowledge in word completion.</title>
<date>2005</date>
<booktitle>In Proceedings of the 7th international ACM SIGACCESS conference on Computers and accessibility, Assets ’05,</booktitle>
<pages>121--128</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5936" citStr="Li and Hirst, 2005" startWordPosition="934" endWordPosition="937">uld) be learned if labeled training data is available. In the remainder of this section, we describe our detectors in more detail. The detectors are not mutually exclusive and a word might be detected by several detectors. 2.1 Collocation-based Detectors Subsequently, we will introduce three detectors that are variations of testing a word for being collocated with one or more nearby words. These detectors are motivated by the research field of word prediction where an algorithm tries to determine the next word a user might want to type on a keyboard based on the already entered word sequence (Li and Hirst, 2005; Trnka, 2008; Aliprandi et al., 2007). We hypothesize that if a word is easy to predict it should also be less ambiguous in that context. We further expect that longer collocated sequences are easier to predict than shorter ones. Another way to look at this problem is its relation to language model perplexity (Chen et al., 1998). Classically, perplexity makes a statement about how well a model predicts test data. In our task, we try to do the exact opposite. We want to determine a test set on which we would achieve a low perplexity on our (static) language model. 3-5 Gram Sliding Window This </context>
</contexts>
<marker>Li, Hirst, 2005</marker>
<rawString>Li, J. and Hirst, G. (2005). Semantic knowledge in word completion. In Proceedings of the 7th international ACM SIGACCESS conference on Computers and accessibility, Assets ’05, pages 121–128, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M O’Toole</author>
<author>R A R King</author>
</authors>
<title>The deceptive mean: Conceptual scoring of cloze entries differentially advantages more able readers. Language Testing.</title>
<date>2011</date>
<marker>O’Toole, King, 2011</marker>
<rawString>O’Toole, J. M. and King, R. A. R. (2011). The deceptive mean: Conceptual scoring of cloze entries differentially advantages more able readers. Language Testing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sakaguchi</author>
<author>Y Arase</author>
<author>M Komachi</author>
</authors>
<title>Discriminative approach to fill-in-the-blank quiz generation for language learners.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>238--242</pages>
<contexts>
<context position="3124" citStr="Sakaguchi et al., 2013" startWordPosition="478" endWordPosition="481"> know for sure one correct answer namely the one that was used in the original sentence. Students might get frustrated if they provide a valid solution that is not recognized by the system. The same problem affects an alternative solution to the problem: providing a list of alternative answer options - called distractors (Sumita et al., 2005). Determining whether a distractor is actually another valid solution is equivalent to the problem described above. Thus, finding good distractors is still an unsolved problem that attracts a lot of research (Lee and Seneff, 2007; Smith and Avinesh, 2010; Sakaguchi et al., 2013; Zesch and Melamud, 2014). Furthermore, providing distractors for cloze items also considerably changes the nature of the task, as distractors are recognition stimuli, i.e. the student recognizes the correct answer rather than having to actively produce it (Gonzáles, 1996). Now consider example sentence (2): (2) I went to the ____today and now I have sand in my shoes. Most people would come up with beach or maybe desert, while other solutions are highly unlikely, which means that the blank is less ambiguous than example (1). This leads to our research question, whether it is possible to find </context>
</contexts>
<marker>Sakaguchi, Arase, Komachi, 2013</marker>
<rawString>Sakaguchi, K., Arase, Y., and Komachi, M. (2013). Discriminative approach to fill-in-the-blank quiz generation for language learners. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 238–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Smith</author>
<author>P Avinesh</author>
</authors>
<title>Gap-fill tests for language learners: Corpus-driven item generation.</title>
<date>2010</date>
<booktitle>In Proceedings of ICON-2010: 8th International Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="3100" citStr="Smith and Avinesh, 2010" startWordPosition="474" endWordPosition="477">rious problem, as we only know for sure one correct answer namely the one that was used in the original sentence. Students might get frustrated if they provide a valid solution that is not recognized by the system. The same problem affects an alternative solution to the problem: providing a list of alternative answer options - called distractors (Sumita et al., 2005). Determining whether a distractor is actually another valid solution is equivalent to the problem described above. Thus, finding good distractors is still an unsolved problem that attracts a lot of research (Lee and Seneff, 2007; Smith and Avinesh, 2010; Sakaguchi et al., 2013; Zesch and Melamud, 2014). Furthermore, providing distractors for cloze items also considerably changes the nature of the task, as distractors are recognition stimuli, i.e. the student recognizes the correct answer rather than having to actively produce it (Gonzáles, 1996). Now consider example sentence (2): (2) I went to the ____today and now I have sand in my shoes. Most people would come up with beach or maybe desert, while other solutions are highly unlikely, which means that the blank is less ambiguous than example (1). This leads to our research question, whether</context>
</contexts>
<marker>Smith, Avinesh, 2010</marker>
<rawString>Smith, S. and Avinesh, P. (2010). Gap-fill tests for language learners: Corpus-driven item generation. In Proceedings of ICON-2010: 8th International Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sumita</author>
<author>F Sugaya</author>
<author>S Yamamoto</author>
</authors>
<title>Measuring non-native speakers’ proficiency of english by using a test with automatically-generated fill-in-the-blank questions.</title>
<date>2005</date>
<booktitle>In Proceedings of the second workshop on Building Educational Applications Using NLP, EdAppsNLP 05,</booktitle>
<pages>61--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2846" citStr="Sumita et al., 2005" startWordPosition="435" endWordPosition="438">ontext. Such ambiguity is not only a problem for language learners, but even native speakers frequently fail when facing such a task (Klein-Braley and Raatz, 1982). If cloze items should be automatically generated and scored, ambiguous items pose a serious problem, as we only know for sure one correct answer namely the one that was used in the original sentence. Students might get frustrated if they provide a valid solution that is not recognized by the system. The same problem affects an alternative solution to the problem: providing a list of alternative answer options - called distractors (Sumita et al., 2005). Determining whether a distractor is actually another valid solution is equivalent to the problem described above. Thus, finding good distractors is still an unsolved problem that attracts a lot of research (Lee and Seneff, 2007; Smith and Avinesh, 2010; Sakaguchi et al., 2013; Zesch and Melamud, 2014). Furthermore, providing distractors for cloze items also considerably changes the nature of the task, as distractors are recognition stimuli, i.e. the student recognizes the correct answer rather than having to actively produce it (Gonzáles, 1996). Now consider example sentence (2): (2) I went </context>
</contexts>
<marker>Sumita, Sugaya, Yamamoto, 2005</marker>
<rawString>Sumita, E., Sugaya, F., and Yamamoto, S. (2005). Measuring non-native speakers’ proficiency of english by using a test with automatically-generated fill-in-the-blank questions. In Proceedings of the second workshop on Building Educational Applications Using NLP, EdAppsNLP 05, pages 61–68, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Taylor</author>
</authors>
<title>Cloze procedure: A new tool for measuring readability.</title>
<date>1953</date>
<journal>Journalism Quarterly,</journal>
<pages>30--415</pages>
<contexts>
<context position="1680" citStr="Taylor, 1953" startWordPosition="242" endWordPosition="243">anked items (59.9% vs. 36.5%). While this shows the potential of our method, we did not succeed in fully eliminating ambiguity. Thus, further research is necessary before fully automatic scoring becomes possible. Keywords: cloze tests, language proficiency tests, automatic scoring. Tobias Horsmann and Torsten Zesch 2014. Towards automatic scoring of cloze items by selecting low-ambiguity contexts. Proceedings of the third workshop on NLP for computer-assisted language learning. NEALT Proceedings Series 22 / Linköping Electronic Conference Proceedings 107: 33–42. 33 1 Introduction Cloze items (Taylor, 1953, 1956; O’Toole and King, 2011) are frequently used to test language proficiency. A cloze item consists of a sentence with usually one word being blanked. The learner’s task is to find the correct word for the blank: (1) He sold his ____yesterday below price. As we can see from example (1), blanks can be quite ambiguous, i.e. a very high number of correct solutions exists. In this example, a wide range of nouns are acceptable solutions including books, house, or bike, but also daughter or kidney cannot be ruled out in this (quite limited) context. Such ambiguity is not only a problem for langu</context>
</contexts>
<marker>Taylor, 1953</marker>
<rawString>Taylor, W. (1953). Cloze procedure: A new tool for measuring readability. Journalism Quarterly, 30:415–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Taylor</author>
</authors>
<title>Recent developments in the use of cloze procedure.</title>
<date>1956</date>
<journal>Journalism Quarterly,</journal>
<pages>33--42</pages>
<marker>Taylor, 1956</marker>
<rawString>Taylor, W. (1956). Recent developments in the use of cloze procedure. Journalism Quarterly, 33:42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Trnka</author>
</authors>
<title>Adaptive language modeling for word prediction.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-08: HLT Student Research Workshop,</booktitle>
<pages>61--66</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="5949" citStr="Trnka, 2008" startWordPosition="938" endWordPosition="939">abeled training data is available. In the remainder of this section, we describe our detectors in more detail. The detectors are not mutually exclusive and a word might be detected by several detectors. 2.1 Collocation-based Detectors Subsequently, we will introduce three detectors that are variations of testing a word for being collocated with one or more nearby words. These detectors are motivated by the research field of word prediction where an algorithm tries to determine the next word a user might want to type on a keyboard based on the already entered word sequence (Li and Hirst, 2005; Trnka, 2008; Aliprandi et al., 2007). We hypothesize that if a word is easy to predict it should also be less ambiguous in that context. We further expect that longer collocated sequences are easier to predict than shorter ones. Another way to look at this problem is its relation to language model perplexity (Chen et al., 1998). Classically, perplexity makes a statement about how well a model predicts test data. In our task, we try to do the exact opposite. We want to determine a test set on which we would achieve a low perplexity on our (static) language model. 3-5 Gram Sliding Window This detector test</context>
</contexts>
<marker>Trnka, 2008</marker>
<rawString>Trnka, K. (2008). Adaptive language modeling for word prediction. In Proceedings of the ACL-08: HLT Student Research Workshop, pages 61–66, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zesch</author>
<author>O Melamud</author>
</authors>
<title>Automatic Generation of Challenging Distractors Using Context-Sensitive Inference Rules.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th Workshop on Innovative Use of NLP for Building Educational Applications at ACL,</booktitle>
<location>Baltimore, USA.</location>
<contexts>
<context position="3150" citStr="Zesch and Melamud, 2014" startWordPosition="482" endWordPosition="485">ct answer namely the one that was used in the original sentence. Students might get frustrated if they provide a valid solution that is not recognized by the system. The same problem affects an alternative solution to the problem: providing a list of alternative answer options - called distractors (Sumita et al., 2005). Determining whether a distractor is actually another valid solution is equivalent to the problem described above. Thus, finding good distractors is still an unsolved problem that attracts a lot of research (Lee and Seneff, 2007; Smith and Avinesh, 2010; Sakaguchi et al., 2013; Zesch and Melamud, 2014). Furthermore, providing distractors for cloze items also considerably changes the nature of the task, as distractors are recognition stimuli, i.e. the student recognizes the correct answer rather than having to actively produce it (Gonzáles, 1996). Now consider example sentence (2): (2) I went to the ____today and now I have sand in my shoes. Most people would come up with beach or maybe desert, while other solutions are highly unlikely, which means that the blank is less ambiguous than example (1). This leads to our research question, whether it is possible to find contexts that are specific</context>
</contexts>
<marker>Zesch, Melamud, 2014</marker>
<rawString>Zesch, T. and Melamud, O. (2014). Automatic Generation of Challenging Distractors Using Context-Sensitive Inference Rules. In Proceedings of the 9th Workshop on Innovative Use of NLP for Building Educational Applications at ACL, Baltimore, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
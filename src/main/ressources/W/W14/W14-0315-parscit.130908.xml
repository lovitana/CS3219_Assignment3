<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.998914">
Speech-Enabled Computer-Aided Translation: A Satisfaction Survey
with Post-Editor Trainees
</title>
<author confidence="0.995657">
Bartolomé Mesa-Lao
</author>
<affiliation confidence="0.993351666666667">
Center for Research and Innovation in Translation and Translation Technology
Department of International Business Communication
Copenhagen Business School, Denmark
</affiliation>
<email confidence="0.998154">
bm.ibc@cbs.dk
</email>
<sectionHeader confidence="0.993881" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902095238095">
The present study has surveyed post-editor
trainees’ views and attitudes before and after the
introduction of speech technology as a front end to
a computer-aided translation workbench. The aim
of the survey was (i) to identify attitudes and
perceptions among post-editor trainees before
performing a post-editing task using automatic
speech recognition (ASR); and (ii) to assess the
degree to which post-editors’ attitudes and
expectations to the use of speech technology
changed after actually using it. The survey was
based on two questionnaires: the first one
administered before the participants performed
with the ASR system and the second one at the end
of the session, once they have actually used ASR
while post-editing machine translation outputs.
Overall, the results suggest that the surveyed post-
editor trainees tended to report a positive view of
ASR in the context of post-editing and they would
consider adopting ASR as an input method for
future post-editing tasks.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939789473684">
In recent years, significant progress has been
made in advancing automatic speech recognition
(ASR) technology. Nowadays it can be found at
the other end of customer-support hotlines, it is
built into operating systems and it is offered as
an alternative text-input method in many mobile
devices. This technology is not only improving at
a steady pace, but is also becoming increasingly
usable and useful.
At the same time, the translation industry is
going through a societal and technological
change in its evolution. In less than ten years, the
industry is considering new tools, workflows and
solutions to service a steadily growing market.
Given the significant improvements in machine
translation (MT) quality and the increasing
demand for translations, post-editing of MT is
becoming a well-accepted practice in the
translation industry, since it has been shown to
allow for larger volumes of translations to be
produced saving time and costs.
Against this background, it seems reasonable
to envisage an era of converge in the future years
where speech technology can make a difference
in the field of translation technologies. As post-
editing services are becoming a common practice
among language service providers and ASR is
gaining momentum, it seems reasonable to
explore the interplay between both fields to
create new business solutions and workflows.
In the context of machine-aided human
translation and human-aided machine translation,
different scenarios have been investigated where
human translators are brought into the loop
interacting with a computer through a variety of
input modalities to improve the efficiency and
accuracy of the translation process (e.g.,
Dragsted et al. 2011, Toselli et al. 2011, Vidal
2006). ASR systems have the potential to
improve the productivity and comfort of
performing computer-based tasks for a wide
variety of users, allowing them to enter both text
and commands into the computer using just their
voice. However, further studies need to be
conducted to build up new knowledge about the
way in which state-of-the-art ASR software can
be applied to one of the most common tasks
translators face nowadays, i.e. post-editing of
MT outputs.
The present study has two related objectives:
First, to report on a satisfaction survey with post-
editor trainees after showing them how to use
ASR in post-editing tasks. Second, based on the
feedback provided by the participants, to assess
the change in users’ expectations and acceptance
of ASR technology as an alternative input
method for their daily work.
</bodyText>
<page confidence="0.987606">
99
</page>
<note confidence="0.4967595">
Workshop on Humans and Computer-assisted Translation, pages 99–103,
Gothenburg, Sweden, 26 April 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.945735" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.9997615">
In this study, we explore the potential of
combining one of the most popular computer-
aided translation workbenches in the market (i.e.
memoQ) with one of the most well-known ASR
packages (i.e. Dragon Naturally Speaking from
Nuance).
</bodyText>
<subsectionHeader confidence="0.959018">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.999977071428572">
Two questionnaires were developed and
deployed as a survey. The survey was divided
into two phases, a prospective phase in which we
surveyed post-editor trainees’ views and
expectations toward ASR and a subsequent
retrospective phase in which actual post-editor’s
experiences and satisfaction with the technology
were surveyed. Participants had to answer a 10-
item questionnaire in the prospective phase and a
7-item questionnaire in the retrospective phase.
These two questionnaires partially overlapped,
allowing us to compare, for each participant, the
answers given before and after the introduction
and use of the target technology.
</bodyText>
<subsectionHeader confidence="0.999396">
2.2 Participants profile
</subsectionHeader>
<bodyText confidence="0.999993">
Participants were recruited through the
Universitat Autònoma de Barcelona (Spain). The
group included 11 females and 4 males, ranging
in age from 22 to 35. All 15 participants had a
full degree in Translation and Interpreting
Studies and were regular users of computer-aided
translation software (mainly memoQ and SDL
Trados Studio). All of them had already
performed MT post-editing tasks as part of their
previous training as translators and, at the
moment of the data collection, they were also
taking a 12-hour course on post-editing as part of
their master’s degree in Translation. None of the
participants had ever user Dragon Naturally
Speaking, but four participants declared to have
tried the speech input options in their mobile
phones to dictate text messages.
</bodyText>
<subsectionHeader confidence="0.996452">
2.3 Procedure
</subsectionHeader>
<bodyText confidence="0.9983758">
Individual sessions occurred at a university
office. In the first part of the session, each
participant had to complete an on-line
questionnaire. This initial survey covered the
following topics:
</bodyText>
<listItem confidence="0.934913090909091">
1. General information about their profile
as translators; including education, years
of experience and employment status.
2. Background in computer-aided trans-
lation software in their daily life as
professional translators.
3. Experience in the field of post-editing
MT outputs and training received.
4. Information about their usage of ASR as
compared to other input methods and, if
applicable, likes and dislike about it.
</listItem>
<bodyText confidence="0.9990034">
In the second part of the session, after the
initial questionnaire was completed, all
participants performed two post-editing tasks
under the following two input conditions (one
each):
</bodyText>
<listItem confidence="0.99901">
• Condition 1: non-ASR input modality, i.e.
keyboard and mouse.
• Condition 2: ASR input modality com-
bined with other non-ASR modalities, i.e.
keyboard and mouse.
</listItem>
<bodyText confidence="0.999891884615385">
The language pair involved in the tasks was
Spanish to English1. Two different texts from the
domain of mobile phone marketing were used to
perform the post-editing tasks under condition 1
and 2. These two texts were imported to a
memoQ project and then fully pre-translated
using MT coming from the Google API plug-in
in memoQ. The order of the two input conditions
and the two texts in each condition were
counterbalanced across participants.
In an attempt to unify post-editing criteria
among participants, all of them were instructed
to follow the same post-editing guidelines aiming
at a final high-quality target text2. In the ASR
input condition, participants also read in hard
copy the most frequent commands in Dragon
Naturally Speaking v.10 that they could use to
post-edit using ASR (Select &lt;w&gt;, Scratch that,
Cut that, etc.). All of them had to do the basic
training tutorial included in the software (5
minutes training on average per participant) in
order to improve the recognition accuracy.
Following the training, participants also had the
chance to practice the dictation of text and
commands before actually performing the two
post-editing tasks.
</bodyText>
<footnote confidence="0.328390111111111">
1 Participants performed from L1 to L2.
2 The post-editing guidelines distributed in hard copy
were: i) Retain as much raw MT as possible; ii) Do
not introduce stylistic changes; iii) Make corrections
only where absolutely necessary, i.e. correct words
and phrases that are clearly wrong, inadequate or
ambiguous according to English grammar; iv) Make
sure there are no mistranslations with regard to the
Spanish source text; v) Publishable quality is expected.
</footnote>
<page confidence="0.971617">
100
</page>
<bodyText confidence="0.99979125">
In the third part of the session, participants
completed a 7-item post-session questionnaire
regarding their opinions about ASR while post-
editing.
</bodyText>
<subsectionHeader confidence="0.912825">
2.4 Data collection and analysis
Survey data
</subsectionHeader>
<bodyText confidence="0.963416878787879">
For questionnaires’ data, responses to
quantitative items were entered into a
spreadsheet and mean responses were calculated
across participants. For a comparison of
responses to different survey items, paired
statistics were used: paired t-test for items coded
as ordinal variables, and chi-square test for items
coded as categorical variables. The
questionnaires did not include open-ended
questions or comments.
Task log files
For task performance data (which is not going to
be elaborated in this paper), computer screen
including audio was recorded using BB
FlashBack Recorder Pro v. 2.8 from Blueberry
Software. With the use of the video recordings, a
time-stamped log of user actions and ASR
system responses was produced for each
participant. Each user action was coded for the
following: (i) input method involved; (ii) for the
post-editing task involving ASR, text entry rate
in the form of text or commands, and (iii), for the
same task, which method of error correction was
used.
Satisfaction data
Responses to the post-session questionnaire were
entered and averaged. We computed an overall
ASR “satisfaction score” for each participant by
summing the responses to the seven items that
related to satisfaction with ASR. We computed a
95 percent confidence interval (CI) for the mean
of the satisfaction score to create bounded
estimated for the satisfaction score.
</bodyText>
<sectionHeader confidence="0.998724" genericHeader="method">
3 Survey results
</sectionHeader>
<subsectionHeader confidence="0.999971">
3.1 Usage of speech input method
</subsectionHeader>
<bodyText confidence="0.998134285714286">
To determine why participants would decide to
use ASR in the future to post-edit, we asked
them to rate the importance of eight different
reasons, on a scale of 1 to 7, with 7 being the
highest in importance. The top reason for
deciding to use ASR was that it would involve
less fatigue (Table 1).
</bodyText>
<table confidence="0.9942161">
Reasons for using speech Mean 95% CI
input method
Less fatigue 5.6* 4.9, 6.4
Speed 5.5* 4.8, 6.3
Ease of use 4.9* 4.7, 5.3
Cool technology 4.7* 4.0, 4.8
Limited alternatives 3.1 2.9, 3.3
Accuracy 2.9 2.1, 3.2
Personal preference 2.7 2.3, 2.9
Others 1 1, 1.2
</table>
<tableCaption confidence="0.92201825">
* Reasons with importance significantly greater than
neutral rating of 4.0 (p &lt; 0.05)
Table 1: Importance of reasons for using automatic
speech recognition (ASR), rated on a scale from 1 to 7.
</tableCaption>
<subsectionHeader confidence="0.999567">
3.2 Usage of non-speech input methods
</subsectionHeader>
<bodyText confidence="0.999641083333333">
Since none of the participants had ever used ASR
to perform any of their translation or post-editing
assignments before, and in order to understand
the relative usage data, we also asked
participants about their reasons for choosing non-
speech input methods (i.e. keyboard and mouse).
For this end, they rated the importance of six
reasons on a scale of 1 to 7, with 7 being most
important. In the introductory questionnaire,
most participants believed that keyboard short-
cuts would be quicker and easier than using
spoken commands (Table 2).
</bodyText>
<table confidence="0.9533326">
Reasons for using non- Mean 95% CI
speech input methods
They are easier 6.5* 5.7, 6.8
Less setup involved 6.1* 5.5, 6.3
Frustration with speech 5.9* 5.2, 6.1
They are faster 3.1 2.7, 3.8
Just for variety 2.0 1.3, 2.8
To rest my voice 1.3 1.1, 2.3
* Reasons with importance significantly greater than
neutral rating of 4.0 (p &lt; 0.05)
</table>
<tableCaption confidence="0.969928">
Table 2: Importance of reasons for choosing non-
speech input methods instead of automatic speech
recognition, rated on a scale from 1 to 7.
</tableCaption>
<bodyText confidence="0.999719875">
Having to train the system (setup involved) in
order to improve recognition accuracy or
donning a headset for dictating was initially
perceived as a barrier for using ASR as the
preferred input method. According to the survey,
participants would also choose other input
methods when ASR performed poorly or not at
all, either in general or for dictating particular
</bodyText>
<page confidence="0.997831">
101
</page>
<bodyText confidence="0.9935882">
commands (e.g., for some participants the
command Cut that was consistently recognized
as Cap that). Less important reasons were the
need to rest one’s voice or to switch methods just
for variety.
</bodyText>
<subsectionHeader confidence="0.693625">
3.3 Opinions about speech and non-speech
input methods
</subsectionHeader>
<bodyText confidence="0.58318">
Participants rated their satisfaction with 10
usability indicators for both ASR and non-ASR
alternatives (Tables 3 and 4).
</bodyText>
<table confidence="0.999610875">
Likes % responding yes
ASR Non-ASR
Ease 85.3 91.9
Speed 74.9 88.6
Less effort 73.9 75.3
Fun 62.3 23.6
Accuracy 52.7 85.3
Trendy 39.5 23.1
</table>
<tableCaption confidence="0.974325">
Table 3: Percentage of participants who liked
particular aspects of the automatic speech recognition
(ASR) system and non-speech input methods.
</tableCaption>
<table confidence="0.999810166666667">
% responding yes
Dislikes ASR Non-ASR
Fixing recognition mistakes 74.5 
Disturbs colleagues 45.9 
Setup involved 36.8 
Fatigue 17.3 12.7
</table>
<tableCaption confidence="0.911539666666667">
Table 4: Percentage of participants who disliked
particular aspects of the automatic speech recognition
(ASR) system and non-speech input methods.
</tableCaption>
<bodyText confidence="0.999811631578947">
ASR for translator-computer interaction
succeeds at easing the task (its most-liked
benefit). Almost 75% liked the speed they
archived with ASR, despite being slower when
compared against non-ASR input methods.
Almost 74% liked the effort required to use ASR,
and only 17.3% found it fatiguing. Participant’s
largest complaint with ASR was related to
recognition accuracy. Only 52.7% liked the
recognition accuracy they achieved and fixing
recognition mistakes ranked as the top dislike at
74.5%. The second most frequent dislike was
potential work environment dissonance or loss of
privacy during use of ASR at 45.9% of
participants.
Ratings show significant differences between
ASR and non-speech input methods, particularly
with regard to accuracy and amusement involved
(Fun item in the questionnaire).
</bodyText>
<subsectionHeader confidence="0.998722">
3.4 Post-session questionnaire results
</subsectionHeader>
<bodyText confidence="0.999169181818182">
To further examine subjective opinions of ASR
in post-editing compared to non-speech input
methods, we asked participants to rate their
agreement to several statements regarding
learnability, ease of use, reliability and fun after
performing the post-editing tasks under the two
conditions. Agreement was rated on a scale of 1
to 7, from “strongly disagree” to “strongly
agree”. Table 5 shows participants’ level of
agreement with the seven statements in the post-
session questionnaire.
</bodyText>
<figure confidence="0.7538805">
Level of
Statement agreement
Mean 95% CI
1. I expected using ASR in post-
editing to be more difficult than it 6.6* 6.5, 6.8
actually is.
2. My performance with the
selection of ASR commands 6.5* 5.4, 6.9
improved by the end of the session.
3. The system correctly recognizes
5.9* 5.5, 6.4
almost every command I dictate.
</figure>
<reference confidence="0.8313674">
4. It is difficult to correct errors 2.9 2.3, 4.1
made by the ASR software.
5. Using ASR in the context of
post-editing can be a frustrating
experience.
6. I can enter text more accurately
with ASR than with any other
method.
7. I was tired by the end of the 1.7 1.2, 2.9
session.
</reference>
<tableCaption confidence="0.801498">
* Agreement significantly greater than neutral rating
of 4.0 (p &lt; 0.05)
Table 5: Participants’ level of agreement to statements
about ASR input method in post-editing tasks.
Ratings are on scale 1 to 7, from “strong disagree” to
“strongly agree”, with 4.0 representing neutral rating.
</tableCaption>
<bodyText confidence="0.9999826">
The results of the post-session questionnaire
show that participants had significantly greater
than neutral agreement (positively) about ASR in
the context of post-editing. Overall they agreed
that it is easier to use ASR for post-editing
purposes than they actually thought. They also
positively agreed that the ASR software was able
to recognize almost every command they
dictated (i.e. Select &lt;w&gt;, Scratch that, etc.) and
acknowledged that their performance when
dictating commands was better as they became
more familiar with the task.
When scores were combined for the seven
statements into an overall satisfaction score, the
average was 73.5 [66.3, 87.4], on a scale of 0 to
</bodyText>
<figure confidence="0.404757">
2.4 1.9, 3.8
2.1 1.7, 2.9
</figure>
<page confidence="0.993611">
102
</page>
<bodyText confidence="0.999880333333333">
1003. Thus, this average is significantly more
positive than neutral. 12 out of the 15 surveyed
participants stated that they will definitely
consider adopting ASR in combination with non-
speech input modalities in their daily practice as
professional translators.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.904952555555556">
The results of the present study show that the
surveyed post-editor trainees tended to report a
very positive view on the use of ASR in the
context of post-editing. In general, findings
suggest that human translators would not regret
the integration of ASR as one of the possible
input methods for performing post-editing tasks.
While many questions regarding effective use
of ASR remain, this study provides some basis
for further efforts to better integrate ASR in the
context of computer-aided translation. Some
specific insights supported by the collected data
are:
• Expectations about ASR were definitely
more positive after having performed with
speech as an input method. Participants
positively agreed that it is easier and more
effective than previously thought.
• Most of the challenges (dislikes) of ASR
when compared to other non-input
methods can be tacked if the user is
provided with both ASR and non-ASR
input methods for them to be used at their
convenience. Participants’ views seem to
indicate that they would use ASR as a
complement rather than a substitute for
non-speech input methods.
</bodyText>
<sectionHeader confidence="0.999607" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9997366">
Post-editor trainees have a positive view of ASR
when combining traditional non-speech input
methods (i.e. keyboard and mouse) with the use
of speech. Acknowledging this up front, an
interesting field for future work is to introduce
proper training on correction strategies. Studies
in this direction could help to investigate how
training post-editors to apply optimal correction
strategies can help them to increase performance
and, consequently, user satisfaction.
</bodyText>
<footnote confidence="0.6545285">
3 A score of 100 represents a strong agreement with
all positive statements and a strong disagreement with
all negative statements, while a score of 50 represents
a neutral response to all statements.
</footnote>
<sectionHeader confidence="0.996132" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999712666666667">
We would like to thank all the participants in this
study for their generous contributions of time,
effort and insights.
</bodyText>
<sectionHeader confidence="0.99641" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998676">
Dragsted, B., Mees, I. M., Gorm Hansen, I. 2011.
Speaking your translation: students’ first encounter
with speech recognition technology, Translation &amp;
Interpreting, Vol 3(1).
Dymetman,M., Brousseau, J., Foster, G., Isabelle, P.,
Normandin, Y., &amp; Plamondon, P. 1994. Towards
an automatic dictation system for translators: the
TransTalk project. Proceedings of the international
conference on spoken language processing (ICSLP
94), 691–694.
Koester, HH. 2004. Usage, performance, and
satisfaction outcomes for experienced users of
automatic speech recognition. Journal of
Rehabilitation Research &amp; Development. Vol 41(5):
739-754.
O’Brien, S. 2012. Translation as human-computer
interaction. Translation Spaces, 1(1), 101-122.
Toselli, A., Vidal, E., Casacuberta, F. 2011.
Multimodal Interactive Pattern Recognition and
Applications. Springer.
Vidal, E., Casacuberta, F., Rodríguez, L., Civera, J.,
Martínez-Hinarejos. C.D. 2006. Computer-Assisted
Translation Using Speech Recognition. IEEE
Transactions on Audio, Speech, and Language
Processing, 14(3): 941-951.
</reference>
<page confidence="0.999298">
103
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.660279">
<title confidence="0.99768">Speech-Enabled Computer-Aided Translation: A Satisfaction with Post-Editor Trainees</title>
<author confidence="0.791742">Bartolomé</author>
<affiliation confidence="0.9604545">Center for Research and Innovation in Translation and Translation Department of International Business</affiliation>
<address confidence="0.967988">Copenhagen Business School, Denmark</address>
<email confidence="0.995859">bm.ibc@cbs.dk</email>
<abstract confidence="0.959527">The present study has surveyed post-editor trainees’ views and attitudes before and after the</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>It is difficult to correct errors 2.9 2.3, 4.1 made by the ASR software. 5. Using ASR in the context of post-editing can be a frustrating experience.</title>
<marker></marker>
<rawString>4. It is difficult to correct errors 2.9 2.3, 4.1 made by the ASR software. 5. Using ASR in the context of post-editing can be a frustrating experience.</rawString>
</citation>
<citation valid="false">
<title>I can enter text more accurately with ASR than with any other method.</title>
<marker></marker>
<rawString>6. I can enter text more accurately with ASR than with any other method.</rawString>
</citation>
<citation valid="false">
<title>I was tired by the end</title>
<journal>of the</journal>
<volume>1</volume>
<pages>session.</pages>
<marker></marker>
<rawString>7. I was tired by the end of the 1.7 1.2, 2.9 session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dragsted</author>
<author>I M Mees</author>
<author>Gorm Hansen</author>
<author>I</author>
</authors>
<title>Speaking your translation: students’ first encounter with speech recognition technology,</title>
<date>2011</date>
<journal>Translation &amp; Interpreting, Vol</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="2999" citStr="Dragsted et al. 2011" startWordPosition="443" endWordPosition="446">fference in the field of translation technologies. As postediting services are becoming a common practice among language service providers and ASR is gaining momentum, it seems reasonable to explore the interplay between both fields to create new business solutions and workflows. In the context of machine-aided human translation and human-aided machine translation, different scenarios have been investigated where human translators are brought into the loop interacting with a computer through a variety of input modalities to improve the efficiency and accuracy of the translation process (e.g., Dragsted et al. 2011, Toselli et al. 2011, Vidal 2006). ASR systems have the potential to improve the productivity and comfort of performing computer-based tasks for a wide variety of users, allowing them to enter both text and commands into the computer using just their voice. However, further studies need to be conducted to build up new knowledge about the way in which state-of-the-art ASR software can be applied to one of the most common tasks translators face nowadays, i.e. post-editing of MT outputs. The present study has two related objectives: First, to report on a satisfaction survey with posteditor train</context>
</contexts>
<marker>Dragsted, Mees, Hansen, I, 2011</marker>
<rawString>Dragsted, B., Mees, I. M., Gorm Hansen, I. 2011. Speaking your translation: students’ first encounter with speech recognition technology, Translation &amp; Interpreting, Vol 3(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dymetman</author>
<author>J Brousseau</author>
<author>G Foster</author>
<author>P Isabelle</author>
<author>Y Normandin</author>
<author>P Plamondon</author>
</authors>
<title>Towards an automatic dictation system for translators: the TransTalk project.</title>
<date>1994</date>
<booktitle>Proceedings of the international conference on spoken language processing (ICSLP 94),</booktitle>
<pages>691--694</pages>
<marker>Dymetman, Brousseau, Foster, Isabelle, Normandin, Plamondon, 1994</marker>
<rawString>Dymetman,M., Brousseau, J., Foster, G., Isabelle, P., Normandin, Y., &amp; Plamondon, P. 1994. Towards an automatic dictation system for translators: the TransTalk project. Proceedings of the international conference on spoken language processing (ICSLP 94), 691–694.</rawString>
</citation>
<citation valid="true">
<authors>
<author>HH Koester</author>
</authors>
<title>Usage, performance, and satisfaction outcomes for experienced users of automatic speech recognition.</title>
<date>2004</date>
<journal>Journal of Rehabilitation Research &amp; Development. Vol</journal>
<volume>41</volume>
<issue>5</issue>
<pages>739--754</pages>
<marker>Koester, 2004</marker>
<rawString>Koester, HH. 2004. Usage, performance, and satisfaction outcomes for experienced users of automatic speech recognition. Journal of Rehabilitation Research &amp; Development. Vol 41(5): 739-754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S O’Brien</author>
</authors>
<title>Translation as human-computer interaction.</title>
<date>2012</date>
<journal>Translation Spaces,</journal>
<volume>1</volume>
<issue>1</issue>
<pages>101--122</pages>
<marker>O’Brien, 2012</marker>
<rawString>O’Brien, S. 2012. Translation as human-computer interaction. Translation Spaces, 1(1), 101-122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Toselli</author>
<author>E Vidal</author>
<author>F Casacuberta</author>
</authors>
<title>Multimodal Interactive Pattern Recognition and Applications.</title>
<date>2011</date>
<publisher>Springer.</publisher>
<contexts>
<context position="3020" citStr="Toselli et al. 2011" startWordPosition="447" endWordPosition="450">of translation technologies. As postediting services are becoming a common practice among language service providers and ASR is gaining momentum, it seems reasonable to explore the interplay between both fields to create new business solutions and workflows. In the context of machine-aided human translation and human-aided machine translation, different scenarios have been investigated where human translators are brought into the loop interacting with a computer through a variety of input modalities to improve the efficiency and accuracy of the translation process (e.g., Dragsted et al. 2011, Toselli et al. 2011, Vidal 2006). ASR systems have the potential to improve the productivity and comfort of performing computer-based tasks for a wide variety of users, allowing them to enter both text and commands into the computer using just their voice. However, further studies need to be conducted to build up new knowledge about the way in which state-of-the-art ASR software can be applied to one of the most common tasks translators face nowadays, i.e. post-editing of MT outputs. The present study has two related objectives: First, to report on a satisfaction survey with posteditor trainees after showing the</context>
</contexts>
<marker>Toselli, Vidal, Casacuberta, 2011</marker>
<rawString>Toselli, A., Vidal, E., Casacuberta, F. 2011. Multimodal Interactive Pattern Recognition and Applications. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D</author>
</authors>
<title>Computer-Assisted Translation Using Speech Recognition.</title>
<date>2006</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>14</volume>
<issue>3</issue>
<pages>941--951</pages>
<marker>D, 2006</marker>
<rawString>Vidal, E., Casacuberta, F., Rodríguez, L., Civera, J., Martínez-Hinarejos. C.D. 2006. Computer-Assisted Translation Using Speech Recognition. IEEE Transactions on Audio, Speech, and Language Processing, 14(3): 941-951.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
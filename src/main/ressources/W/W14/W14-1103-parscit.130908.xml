<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000085">
<title confidence="0.9314455">
The impact of near domain transfer on biomedical named entity
recognition
</title>
<author confidence="0.9988">
Nigel Collier* Ferdinand Paster
</author>
<affiliation confidence="0.979396">
European Bioinformatics Institute University of Applied Sciences
Hinxton, Cambridge, UK, and Upper Austria
National Institute of Informatics, Tokyo, Japan Hagenberg Campus, Austria
</affiliation>
<author confidence="0.929712">
Mai-vu Tran
</author>
<affiliation confidence="0.805854">
University of Engineering and Technology - VNU
Hanoi, Vietnam
</affiliation>
<sectionHeader confidence="0.985486" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999924103448276">
Current research in fully supervised
biomedical named entity recognition
(bioNER) is often conducted in a setting
of low sample sizes. Whilst experi-
mental results show strong performance
in-domain it has been recognised that
quality suffers when models are applied to
heterogeneous text collections. However
the causal factors have until now been
uncertain. In this paper we describe a con-
trolled experiment into near domain bias
for two Medline corpora on hereditary
diseases. Five strategies are employed
for mitigating the impact of near domain
transference including simple transfer-
ence, pooling, stacking, class re-labeling
and feature augmentation. We measure
their effect on f-score performance against
an in domain baseline. Stacking and
feature augmentation mitigate f-score loss
but do not necessarily result in superior
performance except for selected classes.
Simple pooling of data across domains
failed to exploit size effects for most
classes. We conclude that we can expect
lower performance and higher annotation
costs if we do not adequately compensate
for the distributional dissimilarities of
domains during learning.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987293326530612">
Model and feature selection are important exper-
imental tasks in supervised machine learning for
suggesting approaches that will generalise well on
real world data. Research in biomedical named en-
tity recognition (bioNER) often displays two fea-
tures: (1) small samples of labeled data, and (2)
an implicit assumption that the future data will be
∗collier@ebi.ac.uk
drawn from a similar distribution to the labeled
data and hence that minimising expected predic-
tion error on held out data will minimise actual fu-
ture loss. Since expert labeling is time consuming
and expensive, labeled data sets tend to be rela-
tively small, e.g. (Kim et al., 2003; Tanabe et al.,
2005; Pyysalo et al., 2007), in the region of a few
hundred or thousand Medline abstracts. Despite
the danger of intrinsic idiosyncracies such corpora
are often used to demonstrate putative prediction
error across the heterogeneous collection of 22
million Medline abstracts. Once this assumption
is made explicit it is of interest to both researchers
and users that the implications and limitations of
such experimental settings are explored.
Cross domain studies have indicated an ad-
vantage for mechanisms that compensate for do-
main bias. For fully supervised learning, which
is the scenario we explore here, recent methods
include: feature augmentation (Daum´e III, 2007;
Arnold et al., 2008; McClosky et al., 2010), in-
stance weighting (Jiang and Zhai, 2007; Foster
et al., 2010), schema harmonisation (Wang et al.,
2010) and semi-supervised/lightly supervised ap-
proaches (Sagae and Tsujii, 2007; Liu et al., 2011;
Pan et al., 2013). More generally there is a wide
body of work in transfer learning (also known as
domain adaptation) that tries to handle discrep-
ancies between training and testing distributions
(Pan and Yang, 2010).
As an illustration of near domain bias consider
the list of high frequency named entities in Ta-
ble 1 drawn from two sub-domains in the research
literature of hereditary diseases. A domain ex-
pert in hereditary diseases would have no diffi-
culty in dividing them into two non-overlapping
sets corresponding to the two near domains with
one term t5 patients shared by both: {t�,t�,t�,t�}
and {t2,t�,t4,t�,t�0}.
Previous studies have shown what happens
when you radically change the domain and/or the
</bodyText>
<page confidence="0.992766">
11
</page>
<note confidence="0.933037666666667">
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 11–20,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
t1 rheumatoid t6 human leukocyte
arthritis antigen
t2 lupus t7 coronary heart
erythematosus disease
t3 leopard syndrome t8 type 1 diabetes
t4 Omapatrilat t9 T1D
t5 patients t10 hypertension
</note>
<tableCaption confidence="0.97197">
Table 1: High frequency entities in the hereditory
</tableCaption>
<bodyText confidence="0.951484076923077">
disease literature for auto-immune and cardio-
vascular diseases.
annotation schema, e.g. from newswire to Med-
line or Web pages. But what happens when the
annotation schema, the annotator and the primary
domain stay the same? Although the notion of
domain is difficult to formalise in the context of
research literature, this study explores the con-
dition where the variable factor is a shift to a
near domain of literature as defined by biocura-
tors and illustrated in the previous example. Our
contribution to biomedical named entity recogni-
tion (bioNER) is in five areas:
</bodyText>
<listItem confidence="0.998945083333333">
1. We compare four data combination strate-
gies for mitigating the impact of near domain
transference and measure their effect on f-
score performance against an in domain base-
line.
2. We provide additional evidence for the effec-
tiveness of (Daum´e III, 2007)’s frustratingly
simple strategy which provides both general
and domain-specific features; in effect a joint
learning model.
3. Expectedly, but not trivially, we show that
a general loss of f-score occurs on bioNER
when transfering to near domains. This loss
is not uniform across all classes. We provide
class-by-class drill down analysis to the un-
derlying causal factors which make some en-
tities more robust to near domain transference
in biomedicine than others.
4. Our results challenge the notion that pool-
ing small corpora, even when guideline dif-
ferences are reconciled, leads to improved
f-score performance (Wang et al., 2010;
Wagholikar et al., 2013).
5. In addition to the usual biomedical entity
</listItem>
<bodyText confidence="0.97409775">
types we introduce the class of phenotypes
which are valued as indicators of genetic mal-
function and characteristic of diseases. The
phenotype class incorporates a complex de-
pendency between classes, notably anatomi-
cal entities and genes.
This paper is organised as follows: Section 2
describes related work in cross domain transfer
for biomedical NER, Section 3 discusses our ap-
proach including the two data sets used in our ex-
periments, CRF model, feature choices and evalu-
ation framework. In Section 4 we outline our ex-
perimental design. Finally in Section 5 we com-
pare the performance of six data selection strate-
gies that try to maximise f-score performance on
domain entity classes in the target corpus.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999960382352941">
It is surprising that there exists, to the best of our
knowledge, no controlled study that has shed light
on the issue of near domain transfer for bioNER
in a straightforward manner. The closest approach
to our investigation in the biomedical domain is
(Wang et al., 2009). Wang et al. explore potential
sources of incompatibility across major bioNER
corpora with different annotation schema (GENIA
- 2000 Medline abstracts, GENETAG - approx-
imately 20,000 Medline sentences and AIMed -
225 Medline abstracts). They focus exclusively
on protein name recognition and observe a drop in
performance of 12% f-score when combining data
from different corpora. Various reasons are put
forwards such as differences in entity boundary
conventions, the scope of the entity class defini-
tions, distributional properties of the entity classes
and the degree of overlap between corpora.
A follow up study by the authors (Wang et
al., 2010) looked at increasing compatibility be-
tween the GENIA and GENETAG corpora by re-
organising the annotation schema to unify pro-
tein, DNA and RNA NER under a new label GGP
(Gene and Gene Product). However the best per-
formance from the coarse grained annotations still
do not improve on the intra-corpus data.
In earlier work, (Tsai et al., 2006) looked at
schema differences between the JNLPBA corpus
of 2000 Medline abstracts (Kim et al., 2004) and
the BioCreative corpus of 15,000 Medline sen-
tences (Yeh et al., 2005) and tried to harmonise
matching criteria. They demonstrated that relax-
ing the boundary matching criteria was helpful in
maximising the cross-domain performance.
</bodyText>
<page confidence="0.997554">
12
</page>
<bodyText confidence="0.9999755">
In the clinical domain (Wagholikar et al.,
2013), explore the effect of harmonising annota-
tion guidelines on the 2010 i2b2 challenge with
Mayo Clinic Rochester (MCR) electronic patient
records. They concluded that the effectiveness of
pooling - i.e. merging of corpora by ensuring a
common format and harmonised semantics - is de-
pendent on several factors including compatibility
between the annotation schema and differences in
size. Again they noticed that simple pooling re-
sulted in a loss of f-score, 12% for MCR and 4%
for i2b2. They concluded that the asymmetry was
likely due to size effects of the corpora, i.e. MCR
being smaller suffered a greater loss due to the
classifier being biased towards i2b2.
Due to the formulation of these studies and their
limited scope it has previously been difficult to un-
derstand the precise causual factors affecting per-
formance. Our study sheds light on the expected
level of loss under different combination strategies
and more importantly highlights the non-uniform
nature of that loss.
</bodyText>
<sectionHeader confidence="0.993084" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.9997725">
We assume two small labeled data sets DS =
di..dsn and DT = dt1..dtr,.t. dz = (xi C X, yi C
Y ) is drawn from an unknown distribution P3
and represents the source document examples.
Similarly,dti = (xi C X, yi C Y ) is also drawn
from an unknown distribution Pt and represents
the target document examples. We assume that
DS has N examples and DT has M examples
where N Pz� M. xi represents a covariate or fea-
ture vector and yi is a target or label that can take
multiple discrete values. We have a learning al-
gorithm that learns a function h : X —* Y with
minimal loss on the portion of DT used for test-
ing. Any combination of DS and DT which are
not used in testing can be used to learn h. Our task
is to explore various strategies for data selection
and re-factoring labels/features in order to max-
imise held out performance.
</bodyText>
<subsectionHeader confidence="0.995044">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.988702927272727">
In this paper we aim to empirically test domain
transferrence for bioNER under the condition that
the test and training data are relatively small and
drawn from near domains, i.e. from studies on
different types of heritable diseases. To do this
we selected Medline abstracts from PubMed that
were cited by biocuration experts in the canon-
ical database on heritable diseases, the Online
Mendelian Inheritance of Man (OMIM) (Hamosh
et al., 2005). We selected auto-immune diseases
and cardio-vascular diseases for our two corpora
which we denote as C1 and C2 respectively. By
comparing performance of a single model, a single
annotator and a single annotation scheme with a
range of sampling techniques we hope to quantify
the effects of domain transferrence in isolation.
The target classes for the entities are as follows:
ANA Anatomical structures in the body. e.g.
liver, heart.
CHE A chemical or drug. e.g. pristane, his-
tamine, S-nitrosoglutathione.
DIS Diseases. e.g. end stage renal disease, mitral
valve prolapse.
GGP Genes and gene products. e.g. KLKB1
gene, highly penetrant recessive major gene.
PHE Phenotype entities describing observable
and measurable characteristic of an organism.
e.g. cardiovascular abnormalities, abundant
ragged-red fibers, elevated IgE levels.
ORG A living organism. e.g.first-degree rela-
tives, mice.
The two corpora were annotated by a single
experienced annotator who had participated in
the GENIA entity and event corpus annotation.
We developed detailed guidelines for single span
none-nested entities before conducting a training
and feedback session. Feedback was conducted
over two weeks by email and direct meetings with
the annotator and then annotation took approxi-
mately two months. The characteristics of the two
corpora are shown in Table 2. Because annotation
was carried out by only one person we do not pro-
vide inter-annotator scores.
Importantly, we note four points at this stage:
(1) We incorporate a new named entity type, phe-
notype, which is aligned with investigations into
heritable diseases. Semantically it is interesting
because phenotypes annotated in the auto-immune
literature pertain more often to sub-cellular pro-
cesses and those in the cardiovascular domain per-
tain more often to cells, tissues and organs; (2)
It can be seen that two NE classes fall well be-
low 500 instances - what we might arbitarily con-
sider the necessary level of support for high lev-
els of performance. These are ANA and CHE;
</bodyText>
<page confidence="0.998163">
13
</page>
<table confidence="0.99913225">
C1 C2 a b
Abstracts 110 80 - -
Tokens 27,421 26,578 - -
Av.length 32.57 29.93 - -
ANA 194 195 0.33 0.26
(138) (133)
CHE 44 147 0.08 0.07
(33) (75)
DIS 892 955 0.39 0.27
(282) (442)
GGP 1663 754 0.41 0.45
(928) (511)
ORG 799 770 0.56 0.67
(429) (323)
PHE 507 1430 0.52 0.33
(423) (1113)
</table>
<tableCaption confidence="0.986724">
Table 2: Characteristics of the C1 auto-immune
</tableCaption>
<bodyText confidence="0.987039875">
and C2 cardiovascular corpora: number of ab-
stracts, number of tokens, average sentence length,
frequency of each entity type. Figures in parenthe-
ses represent counts after removing duplication. a:
probability that a word in an entity class X in C1
is also a word in entity class X in C2. b: probabil-
ity that a word in an entity class X in C2 is also a
word in entity class X in C1
</bodyText>
<listItem confidence="0.561234">
(3) We calculated from Table 2 the average num-
ber of mentions for each entity form by class and
noted that this is relatively stable across corpora,
except for DIS which has less variation in C2 than
C1 and CHE which has more variation in C2 than
C1. When combining evidence from both cor-
pora the approximate order of type/token ratio are
PHE &lt; ANA &lt; CHE, GGP &lt; ORG &lt; DIS
</listItem>
<bodyText confidence="0.999521">
indicating that on average PHE entities have the
greatest variation. Average entity lengths in to-
kens (not shown) indicate that PHE are signifi-
cantly longer than other entity mentions; and (4)
We calculated the probability that a word token in
an entity class from one corpus would appear in
an instance of the same entity class in the other
corpus, reported as columns a and b. Although the
probability of an exact match in instances between
entities in the two corpora is generally quite low
(below 20% - data not shown) there appears to be
significant vocabulary overlap in most classes ex-
cept for chemicals.
</bodyText>
<subsectionHeader confidence="0.999791">
3.2 Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.9999396">
As in (Finkel and Manning, 2009) we apply our
approach to a linear chain conditional random field
(CRF) model (Lafferty et al., 2001; McCallum
and Wei, 2003; Settles, 2004; Doan et al., 2012)
using the Mallet toolkits with default parameters.
CRFs have been shown consistently to be among
the highest performing bioNER learners. The data
selection strategies employed here though are neu-
tral and could have been applied to any other fully
supervised learner model.
</bodyText>
<subsectionHeader confidence="0.967742">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.99989772972973">
We made use of a wide range of features, both
conventional features such as word or part of
speech, as well as gazetteers derived from ex-
ternal classification schemes that have been hand
crafted by experts. These are shown in Ta-
ble 3. Previous studies such as (Ratinov and
Roth, 2009) have noted that domain gazetteer
features play a critical role in aiding classifi-
cation. In order to show realistic model be-
haviour consistent with state-of-the-art techniques
we have included gazetteers derived from: the Hu-
man Phenotype Ontology (HPO: 15,800 terms),
the Mammalian Phenotype Ontology (MP: 23,700
terms), the Phenotypic Attribute and Trait On-
tology (PATO: 2,200 synonyms), the Brenda
Tissue Ontology (BTO: 9,600 synonyms), the
Foundation Model of Anatomy (FMA: 120,000
terms), National Library of Medicine gene list
(NLM: 9 million terms), UMLS disease terms
(UMLS: 275,000 terms), Jochem chemical terms
(JOCHEM: 320,000 terms).
The feature set is quite large and therefore there
is a danger that the learner will be hindered. For
feature selection, we conducted baseline test runs
under the same experimental conditions as those
reported here using a grid search on features F1
to F11 and found that f-score performance was
uniformly lower when removing any feature (data
not shown but available as supplementary material
from the first author).
In order to characterise the contribution each
feature is making in label prediction we wanted to
provide a measure of similarity between the fea-
ture and the class label probability distributions.
Here we use the Gain Ratio (GR) to estimate intra-
corpus class prediction performance by each fea-
ture. GR was used as a splitting function in C4.5
</bodyText>
<equation confidence="0.551922">
lhttp://mallet.cs.umass.edu/
14
(Quinlan, 1993) and is defined as
GR(C, F) = IG(C, F)/H(F) (1)
</equation>
<bodyText confidence="0.995519666666667">
where C represents a class label and F repre-
sents a feature type. IG is information gain and
defined as,
</bodyText>
<equation confidence="0.99800675">
IG(C, F) = H(C) − H(C|X) (2)
H is entropy and defined for feature types as,
H(F) = − ∑n p(fi)lo92(p(fi)) (3)
i=1
</equation>
<bodyText confidence="0.9550944">
for n feature types fi E F. Further informa-
tion can be found in (Quinlan, 1993). GR is used
in C4.5 in preference to IG because of its ability
to normalise for the biases in IG. Generally this
results in GR having greater predictive accuracy
than IR since it takes into account the number of
feature values. Note that GR is undefined when
the denominator is zero.
Several points emerge from looking at GR and
IG values in Table 3:
</bodyText>
<listItem confidence="0.721439">
•
</listItem>
<bodyText confidence="0.978650222222222">
C1 (auto-immune) and C2 (cardio-vascular)
have about the same information gain con-
tribution from most features but C1 seems
to benefit more from GENIA named entity
tagging, Human Phenotype Ontology (HPO),
Foundation Model of Anatomy (FMA) and
Gene Ontology (GO) terms whereas C2 ben-
efits more from the UMLS diseases and
ChEBI terms.
</bodyText>
<listItem confidence="0.9932315">
• GO, containing terms about genetic pro-
cesses, has a higher GR in C1 than C2. This
supports what we already expected - that
auto-immune diseases contain a higher pro-
portion of information about genetic process
phenotypes than cardiovascular.
• The GENIA POS tags seem to provide a
slightly higher GR in C2 than in C1.
• Despite its large size, UMLS has a smaller
GR on both corpora compared to some other
resources like HPO or GO or MA. This is de-
spite its high IG value.
</listItem>
<subsectionHeader confidence="0.829568">
3.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.988396269230769">
Traditional re-sampling using k-fold cross valida-
tion (k-CV) divides the n labelled documents into
k disjoint subsets of approximately equal size des-
ignated as Di for i = 1,.., k. The NER learner
is trained successively on k − 1 folds from D and
tested on a held out fold over k iterations. In or-
der to preserve independence between contexts in
training and held out data we assume here that the
unit of division is the document, i.e. a single Med-
line abstract. Estimated prediction error is calcu-
lated based on the learner’s labels on the k held
out folds. Whilst k-CV is known to be nearly un-
biased it is a highly variable estimator. Several
studies have looked at k-CV for small sample sets.
For example, (Braga-Neto and Dougherty, 2004)
found on classifier experiments for small microar-
ray samples (20 &lt;= n &lt;= 120) that whilst k-
CV showed low bias they suffered from excessive
variance compared to bootstrap or resubstitution
estimators.
One cause of variance has been identified as
within-block and between-block training errors
arising from the disproportionate effects of a sin-
gle abstract appearing in the training set of many
folds. In order to reduce this effect Monte Carlo
cross validation was used (also called CV with rep-
etition). 100 iterations were used to randomly re-
order the documents in the corpora before 10-fold
CV sampling was run (cv10r100). Sampling of
documents is done without replacement so that the
independence between training and testing sets are
maintained. Stratification was not applied. Mi-
cro averaged f-scores for labeling accuracy were
calculated based on the 1000 test folds for each
model. Evaluation was done in both directions
(training and testing) for each corpus C1 and C2
to show any asymmetrical effects. To minimse the
time taken for each experiment a cluster computer
was used with 48 nodes.
The matching criteria we employ is the exact
match - i.e. the span of the system labeling and
the held out data labels should be exactly the same.
Although this is not a necessary criteria for some
applications such as database curation we used it
here as it is widely applied in shared evaluations
and shows the clearest effects of modeling choice.
We evaluate using the named entity precision,
recall and F-score calculated using the CoNLL
2003 Perl script. This was calculated as,
f − score = (2 x precision x recall) (4)
(precision + recall)
where,
</bodyText>
<page confidence="0.99599">
15
</page>
<table confidence="0.999896148148148">
Feature IG(C1, Fi) GR(C1, Fi) IG(C2, Fi) GR(C2, Fi)
F1 Word 1.17 0.13 1.20 0.13
F2 Lemma 1.15 0.13 1.18 0.13
F3 POS tag 0.36 0.09 1.18 0.13
F4 Chunk tag 0.22 0.12 0.26 0.10
F5 GENIA NEa 0.20 0.35 0.14 0.27
F6 Orthography 0.15 0.08 0.16 0.08
F7 Domain prefix 0.11 0.11 0.11 0.10
F8 Domain suffix 0.08 0.11 0.08 0.11
F9 Word length 0.13 0.05 0.16 0.06
F10 Parenthesis 0.04 0.20 0.04 0.23
F11 Abbreviation 0.08 0.22 0.06 0.24
F12 HPOb 0.07 0.41 0.09 0.33
F13 MPc 0.03 0.33 0.06 0.33
F14 PATOd 0.01 0.03 0.02 0.04
F15 BTOe 0.03 0.32 0.03 0.29
F16 FMAf 0.05 0.28 0.05 0.23
F17 MAg 0.02 0.31 0.02 0.29
F18 PROh 0.02 0.12 0.03 0.15
F19 ChEBIi 0.01 0.15 0.03 0.20
F20 JOCHEMj 0.01 0.15 0.01 0.14
F21 NCBIk 0.01 0.14 0.01 0.14
F22 UMLSl disease 0.01 0.14 0.03 0.24
F23 NCBI gene 0.02 0.18 0.02 0.19
F24 GOm 0.13 0.38 0.05 0.28
F25 UMLSn 0.48 0.12 0.52 0.11
F26 45CLUSTERSo 0.50 0.10 0.47 0.10
</table>
<tableCaption confidence="0.739062">
Table 3: Features used in the experiments. aThe GENIA named entity tagger (Kim et al., 2003),
b(Robinson et al., 2008), c(Smith et al., 2004), d(Gkoutos et al., 2005), e(Gremse et al., 2011), f(Rosse
</tableCaption>
<bodyText confidence="0.84719775">
and Mejino, 2003), g(Hayamizu et al., 2005), h(Natale et al., 2011) , i(Degtyarenko et al., 2008), j(Hettne
et al., 2009),k(Federhen, 2012),l(Lindberg et al., 1993),m(Gene Ontology Consortium, 2000),n133 cat-
egories from the UMLS,o45 cluster classes derived by Richard Socher and Christoph Manning PubMed
available at http://nlp.stanford.edu/software/bionlp2011-distsim-clusters-v1.tar.gz
</bodyText>
<equation confidence="0.996543">
precision = TP/(TP + FP) (5)
and,
recall = TP/(TP + FN) (6)
</equation>
<bodyText confidence="0.999979714285714">
A true positive (TP) is a gold standard NE
tagged by the system as an NE. A true negative
(TN) is a gold standard none-NE tagged by the
system as a none-NE. A false positive (FP) is a
gold standard none-NE tagged by the system as
an NE. Evaluation is based on correctly marked
whole entities rather than tokens.
</bodyText>
<sectionHeader confidence="0.997363" genericHeader="method">
4 Experimental design
</sectionHeader>
<bodyText confidence="0.9986734">
In this section we present the experimental condi-
tions we used, starting with a description of the
models which we designate M1 to M6 and de-
scribe below. All methods made use of 100 iter-
ations of Monte Carlo 10-fold cross validation.
</bodyText>
<listItem confidence="0.996152">
M1: IN DOMAIN We trained and tested on only
the data for the source domain. This methods
forms our baseline and represents the stan-
dard experimental setting.
M2: OUT DOMAIN We trained on the source
domain and tested on the target domain. This
method shows expected loss on near domain
transferrence and represents the standard op-
erational setting for users.
</listItem>
<page confidence="0.970584">
16
</page>
<bodyText confidence="0.802810333333333">
M3: MIX-IN We trained on 100% of the source
domain and unified this with 90% of the
folded in target domain data, leaving 10%
for testing. This method reflects the pooling
technique typically employed in corpus con-
struction for bioNER.
</bodyText>
<listItem confidence="0.892029347826087">
M4: STACK We trained a CRF model on 100%
of the source domain and stacked it with
another CRF trained on 90% of the folded
in target domain data. Stacking employs a
meta-classifier and is a popular method for
constructing high performance ensembles of
classifiers (Ekbal and Saha, 2013). In this
case we collected the output labels from the
source domain-trained CRF on target sen-
tences and added them as features for the tar-
get domain trained CRF.
M5: BINARY CLASS We re-labeled the com-
plex class PHE as PHE-C1 in C1 and PHE-
C2 in C2 and repeated M3. Afterwards we
recombined PHE-C1 and PHE-C2 into PHE.
M6: FRUSTRATINGLY SIMPLE We fol-
lowed the feature augmentation approach of
(Daum´e III, 2007). This method effectively
provides a joint learning model on C1 and
C2 by splitting each feature into three parts:
one for sharing cross domain values and one
for each domain specific value. We evaluated
using the same regime as M3.
</listItem>
<sectionHeader confidence="0.966589" genericHeader="evaluation">
5 Experimental results and discussion
</sectionHeader>
<bodyText confidence="0.999930842857143">
In Table 4 we show f-score performance from near
biomedical domains with our six strategies. This
section now tries to draw together an interpretation
for the performance trends that we see and to drill
down to some of the causal factors.
Held out tests performed in-domain (M1) on
both corpora C1 and C2 indicate a relatively high
level of performance, conservatively in line with
state-of-the-art estimates. The broad trend in per-
formance is for entity classes with more instances
to out perform others with lower numbers. The
class which most obviously breaks this trend is
the complex entity type of PHE. To understand
this consider that PHE is defined as an observable
property on an organism and as such tends to be
formed from a quality such as malformed that de-
scribes a structural entity such as valve. To see
closer what is happening we looked at the confu-
sion matrices for M1 on both corpora. For both
C1 and C2 we observed that a substantial pro-
portion of words inside PHE sequences were con-
fused with GGP, DIS or ANA entities. Similarly
a high proportion of words inside ANA sequences
were confused with PHE entities. This indicates
that dependencies within complex biomedical en-
tities like PHE might better be modeled explicitly
using tree-structures in a manner similar to events
rather than using n-gram relations.
In the M2 out of domain experiments we see
a generally severe loss of f-score performance
across most classes. Training on C2 and testing
on C1 results in a 19.1% loss (F1 69.9 to 50.8)
and training on C1 and testing on C2 results in
a 11.9% loss overall (F1 58.5 to 46.6). The re-
sults agree with Wang et al.’s experience on het-
erogeneous Medline corpora and extend the upper
limit on all-class loss due to domain transferrence
to 19%. The only NE class where we see a sym-
metric benefit from pooling entities in M3 is for
ORG (F1 68.4 to 72.2, F1 73.2 to 77.4). Intrigu-
ingly the data from Tables 2 and 4 hint at a correla-
tion between the success of M3 pooling for ORG
and broad cross-domain compatibility on the vo-
cabulary (over 50% of ORG vocabulary is shared
across corpora). However this is not supported
in the low sharing case for CHE where we see
increased performance from pooling (F1 31.3 to
38.7) when the target is C2 but decreased perfor-
mance when the target is C1 (F1 29.5 to 20.0).
When we look at the pooling method (M3) and
compare to the in-domain method (M1) no obvi-
ous size effect occurs for the number of entities
in each class. To see this we can examine entity
classes with an imbalanced number of instances
in C1 and C2 such as CHE, GGP and PHE. Con-
sider the following three cases: (1) Adding 147
instances of CHE from C2 to 44 instances from
C1 is associated with CHE performance dropping
from M1:29.5 to M3:20.0 when tested on C1; (2)
Similarly adding 1430 instances of PHE from C2
to 507 instances from C1 is associated with PHE
performance dropping from 46.0 in M1 to 39.7 in
M3 when tested on C1; (3) But adding 1663 in-
stances of GGP from C1 to 754 from C2 is asso-
ciated with GGP rising from 57.2 in M1 to 61.1 in
M3. If simply pooling more entities was impor-
tant to improved f-score we would expect to see a
clearer pattern of improvement but we do not.
The overall pooling loss for all classes on M3
is within 3% in both directions and within the
</bodyText>
<page confidence="0.997912">
17
</page>
<table confidence="0.998288692307692">
Model Target ANA CHE DIS GGP PHE ORG ALL
M1 C1 57.1 29.5 80.4 74.0 46.0 68.4 69.9
M2 C1 34.3 26.9 57.7 55.6 26.9 64.0 50.8
M3 C1 50.8 20.0 77.9 71.7 39.7 72.2 67.3
M4 C1 56.3 17.4 79.0 74.1 44.1 70.8 69.8
M5 C1 56.7 29.6 77.3 72.7 41.5 72.8 68.3
M6 C1 57.1 27.7 79.0 73.4 44.9 69.9 69.5
M1 C2 37.2 31.3 72.9 57.2 46.5 73.2 58.5
M2 C2 21.2 20.2 57.0 52.3 24.4 68.5 46.6
M3 C2 36.8 38.7 72.3 61.1 44.0 77.4 59.7
M4 C2 34.8 34.4 72.5 57.5 45.9 74.7 58.5
M5 C2 34.1 41.6 73.6 58.9 43.2 78.5 59.6
M6 C2 39.9 35.0 73.3 56.4 46.6 75.0 59.1
</table>
<tableCaption confidence="0.707389">
Table 4: Named entity recognition f-scores using Methods 1 to 6. All methods were tested using 100
iterations of Monte Carlo 10-fold cross validation. Figures in bold show best in class scores. Figures in
italics show scores above the M1 baseline.
</tableCaption>
<bodyText confidence="0.999830763157895">
bounds observed by (Wang et al., 2009) and
(Wagholikar et al., 2013) for their pooling of het-
erogeneous Medline corpora. Except for the ORG
class which we higlighted above, we might cau-
tiously quantify the loss of pooled entity mentions
as being in the range up to 9.5% for CHE but more
typically below 4%. The majority of the differ-
ences they observed - which are not present in our
data - are most likely due to concept definition dif-
ferences and annotation conventions.
In contrast to our expectations the M4 experi-
ments showed very mild benefits for stacking and
these were mixed across entity types. M4 tests
on C2 showed no general improvement but some
improvement in CHE and ORG. M4 tests on C1
resulted again in no overall improvement except
for some gain for ORG, supporting our hypothesis
that there is greater compatibility in ORG across
domains.
The M5 approach of splitting the PHE labels for
the two corpora resulted in a noticable improve-
ment over M3 on the C1 test but unfortunately this
was not sustained when testing on C2.
It is striking that in the M6 experiments the fea-
ture augmentation method only just meets the in-
domain f-score on C1 and mildly exceeds it on C2.
One explanation is that the corpora are so small
that a richer feature set has only marginal effects
on performance. Table 3 certainly indicates that
many of the features have low predictive capac-
ity (gain ratio values below 0.1) in an intra-corpus
setting but this is not the case for others such as
GENIA NE tags or HPO gazzetteer terms.
Overall when we average the f-scores across
models for C1 and C2 we see that there is a
marginal benefit to the M1, M4 and M6 strategies
over M3 and M5 with M2 suffering the greatest
loss in performance.
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999628115384615">
In this paper we have provided evidence that trans-
ference even to closely related domains in biomed-
ical NER incurs a severe loss in f-score. We
have demonstrated empirically that strategies that
make use of multi-domain corpora such as stack-
ing learners and feature augmentation mitigate the
accuracy loss but do not necessarily result in supe-
rior performance except for selected classes such
as organisms where there appears to be broad
terminology consensus. Simple pooling of data
across domains failed to exploit size effects espe-
cially for the complex class of phenotypes. The
list of strategies employed has not been exhaus-
tive and it is possible that others such as feature
hierarchies (Arnold et al., 2008) might yield better
results.
BioNER is complicated by various factors such
as descriptive names, polysemous terms, conjuc-
tions, nested constructions and a high quantity of
abbreviations. We have shown that performance is
also held back by not considering document level
properties related to domain such as topicality. We
can expect lower performance and higher annota-
tion costs if we do not adequately allow for the dis-
tributional dissimilarities of domains during learn-
ing, even in closely related topical settings.
</bodyText>
<page confidence="0.998659">
18
</page>
<sectionHeader confidence="0.999214" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999912428571428">
The authors gratefully acknowledge the many
helpful comments from the anonymous review-
ers of this paper. Nigel Collier’s research is
supported by the European Commission through
the Marie Curie International Incoming Fellow-
ship (IIF) programme (Project: Phenominer, Ref:
301806).
</bodyText>
<sectionHeader confidence="0.998911" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999501729166667">
A. Arnold, N. Nallapati, and W. Cohen. 2008. Exploit-
ing feature hierarchy for transfer learning in named
entity recognition. In Annual meeting of the Asso-
ciation for Computational Linguistics (ACL 2008),
pages 245–253.
U. Braga-Neto and E. Dougherty. 2004. Is cross-
validation valid for small-sample microarray classi-
fication? Bioinformatics, 20(3):374–380.
H. Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Annual meeting of the Association for
Computational Linguistics (ACL 2007), pages 256–
263.
K. Degtyarenko, P. de Matos, M. Ennis, J. Hastings,
M. Zbinden, A. McNaught, R. Alc´antara, M. Dar-
sow, M. Guedj, and M. Ashburner. 2008. ChEBI:
a database and ontology for chemical entities of bi-
ological interest. Nucleic acids research, 36(suppl
1):D344–D350.
S. Doan, N. Collier, H. Xu, P. Duy, and T. Phuong.
2012. Recognition of medication information from
discharge summaries using ensembles of classifiers.
BMC Medical Informatics and Decision Making,
12(1):36.
A. Ekbal and S. Saha. 2013. Stacked ensemble cou-
pled with feature selection for biomedical entity ex-
traction. Knowledge-Based Systems.
S. Federhen. 2012. The NCBI taxonomy database.
Nucleic acids research, 40(D1):D136–D143.
J. Finkel and C. Manning. 2009. Hierarchical bayesian
domain adaptation. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 602–610.
G. Foster, C. Goutte, and R. Kuhn. 2010. Discrim-
inative instance weighting for domain adaptation in
statistical machine translation. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2010), pages 451–
459.
Gene Ontology Consortium. 2000. Gene ontology:
tool for the unification of biology. Nature Genetics,
25:19–29.
G. Gkoutos, E. Green, A. Mallon, J. Hancock, and
D. Davidson. 2005. Using ontologies to describe
mouse phenotypes. Genome Biology, 6:R8.
M. Gremse, A. Chang, I. Schomburg, A. Grote,
M. Scheer, C. Ebeling, and D. Schomburg. 2011.
The BRENDA tissue ontology (BTO): the first
all-integrating ontology of all organisms for en-
zyme sources. Nucleic Acids Research, 39(suppl
1):D507–D513.
A. Hamosh, A. F. Scott, J. S. Amberger, and C. A. Boc-
chini. 2005. Online mendelian inheritance of man
(OMIM), a knowledgebase of human genes and ge-
netic disorders. Nucleic Acids Research, 33(suppl
1):D514–D517.
T. Hayamizu, M. Mangan, J. Corradi, J. Kadin,
M. Ringwald, et al. 2005. The adult mouse anatom-
ical dictionary: a tool for annotating and integrating
data. Genome Biol, 6(3):R29.
K. Hettne, R. Stierum, M. Schuemie, P. Hendriksen,
B. Schijvenaars, E. van Mulligen, J. Kleinjans, and
J. Kors. 2009. A dictionary to identify small
molecules and drugs in free text. Bioinformatics,
25(22):2983–2991.
J. Jiang and C. Zhai. 2007. Instance weighting for
domain adaptation in NLP. In Annual meeting of
the Association for Computational Linguistics (ACL
2007), volume 2007, page 22.
J. D. Kim, T. Ohta, Y. Tateishi, and J. Tsujii. 2003.
GENIA corpus - a semantically annotated corpus for
bio-textmining. Bioinformatics, 19(Suppl.1):180–
182.
J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recog-
nition task at JNLPBA. In N. Collier, P. Ruch,
and A. Nazarenko, editors, Proceedings of the In-
ternational Joint Workshop on Natural Language
Processing in Biomedicine and its Applications
(JNLPBA), Geneva, Switzerland, pages 70–75, Au-
gust 28–29. held in conjunction with COL-
ING’2004.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of the Eighteenth International Conference on
Machine Learning, Massachusetts, USA, pages 282–
289, June 28th – July 1st.
Donald A.B. Lindberg, L. Humphreys, Betsy, and
T. McCray, Alexa. 1993. The unified medical lan-
guage system. Methods of Information in Medicine,
32:281–291.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing named entities in tweets.
In Annual meeting of the Association for Computa-
tional Linguistics (ACL 2011), pages 359–367.
</reference>
<page confidence="0.984074">
19
</page>
<reference confidence="0.999501297619047">
A. McCallum and L. Wei. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proc. Seventh Conference on Natural language
learning at HLT-NAACL 2003 - Volume 4, CONLL
’03, pages 188–191.
D. McClosky, E. Charniak, and M. Johnson. 2010. Au-
tomatic domain adaptation for parsing. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 28–36.
Association for Computational Linguistics.
D. Natale, C. Arighi, W. Barker, J. Blake, C. Bult,
M. Caudy, H. Drabkin, P. DEustachio, A. Evsikov,
H. Huang, et al. 2011. The protein ontology: a
structured representation of protein forms and com-
plexes. Nucleic acids research, 39(suppl 1):D539–
D545.
S. Pan and Q. Yang. 2010. A survey on transfer learn-
ing. Knowledge and Data Engineering, IEEE Trans-
actions on, 22(10):1345–1359.
S. Pan, Z. Toh, and J. Su. 2013. Transfer joint em-
bedding for cross-domain named entity recognition.
ACM Transactions on Information Systems (TOIS),
31(2):7.
S. Pyysalo, F. Ginter, J. Heimonen, J. Bj¨orne,
J. Boberg, J. J¨arvinen, and T. Salakoski. 2007.
Bioinfer: a corpus for information extraction in the
biomedical domain. BMC bioinformatics, 8(1):50.
J. Quinlan. 1993. C4. 5: programs for machine learn-
ing, volume 1. Morgan kaufmann.
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning (CoNLL), pages
147–155.
P. N. Robinson, S. Kohler, S. Bauer, D. Seelow,
D. Horn, and S. Mundlos. 2008. The human pheno-
type ontology: a tool for annotating and analyzing
human hereditary disease. The American Journal of
Human Genetics, 83(5):610–615.
C. Rosse and J. L. V. Mejino. 2003. A reference on-
tology for bioinformatics: the Foundational Model
of Anatomy. Journal of Biomedical Informatics,
36(6):478–500, December. PMID: 14759820.
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with lr models and parser en-
sembles. In Conference on Empirical Methods in
Natural Language Processing Conference on Com-
putational Natural Language Learning (EMNLP-
CoNLL), volume 2007, pages 1044–1050.
B. Settles. 2004. Biomedical named entity recognition
using conditional random fields. In Proceedings of
the International Joint Workshop on Natural Lan-
guage Processing in Biomedicine and its Applica-
tions (JNLPBA) at COLING’2004, Geneva, Switzer-
land, pages 104–107, August 28–29.
C. L. Smith, C. W. Goldsmith, and J. T. Eppig. 2004.
The mammalian phenotype ontology as a tool for an-
notating, analyzing and comparing phenotypic infor-
mation. Genome Biology, 6:R7.
L. Tanabe, N. Xie, L. H. Thom, W. Matten, and W. J.
Wilbur. 2005. GENETAG: a tagged corpus for
gene/protein named entity recognition. BMC Bioin-
formatics, 6(Suppl 1):S3.
R. Tsai, S. Wu, W. Chou, Y. Lin, D. He, J. Hsiang,
T. Sung, and W. Hsu. 2006. Various criteria in the
evaluation of biomedical named entity recognition.
BMC bioinformatics, 7(1):92.
K. Wagholikar, M. Torii, S. Jonnalagadda, H. Liu, et al.
2013. Pooling annotated corpora for clinical con-
cept extraction. J. Biomedical Semantics, 4:3.
Y. Wang, J. Kim, R. Sætre, S. Pyysalo, and J. Tsujii.
2009. Investigating heterogeneous protein annota-
tions toward cross-corpora utilization. BMC bioin-
formatics, 10(1):403.
Y. Wang, J. Kim, R. Sætre, S Pyysalo, T. Ohta, and
J. Tsujii. 2010. Improving the inter-corpora com-
patibility for protein annotations. Journal of bioin-
formatics and computational biology, 8(05):901–
916.
A. Yeh, A. Morgan, M. Colosimo, and L. Hirschman.
2005. Biocreative task 1a: gene mention finding
evaluation. BMC bioinformatics, 6(Suppl 1):S2.
</reference>
<page confidence="0.99489">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.317372">
<title confidence="0.755575166666667">The impact of near domain transfer on biomedical named entity recognition Paster European Bioinformatics Institute University of Applied Sciences Hinxton, Cambridge, UK, and Upper National Institute of Informatics, Tokyo, Japan Hagenberg Campus, Austria</title>
<author confidence="0.942477">Mai-vu Tran</author>
<affiliation confidence="0.999624">University of Engineering and Technology -</affiliation>
<address confidence="0.956906">Hanoi, Vietnam</address>
<abstract confidence="0.999711233333333">Current research in fully supervised biomedical named entity recognition (bioNER) is often conducted in a setting of low sample sizes. Whilst experimental results show strong performance in-domain it has been recognised that quality suffers when models are applied to heterogeneous text collections. However the causal factors have until now been uncertain. In this paper we describe a controlled experiment into near domain bias for two Medline corpora on hereditary diseases. Five strategies are employed for mitigating the impact of near domain transference including simple transference, pooling, stacking, class re-labeling and feature augmentation. We measure their effect on f-score performance against an in domain baseline. Stacking and feature augmentation mitigate f-score loss but do not necessarily result in superior performance except for selected classes. Simple pooling of data across domains failed to exploit size effects for most classes. We conclude that we can expect lower performance and higher annotation costs if we do not adequately compensate for the distributional dissimilarities of domains during learning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Arnold</author>
<author>N Nallapati</author>
<author>W Cohen</author>
</authors>
<title>Exploiting feature hierarchy for transfer learning in named entity recognition.</title>
<date>2008</date>
<booktitle>In Annual meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>245--253</pages>
<contexts>
<context position="2879" citStr="Arnold et al., 2008" startWordPosition="426" endWordPosition="429">line abstracts. Despite the danger of intrinsic idiosyncracies such corpora are often used to demonstrate putative prediction error across the heterogeneous collection of 22 million Medline abstracts. Once this assumption is made explicit it is of interest to both researchers and users that the implications and limitations of such experimental settings are explored. Cross domain studies have indicated an advantage for mechanisms that compensate for domain bias. For fully supervised learning, which is the scenario we explore here, recent methods include: feature augmentation (Daum´e III, 2007; Arnold et al., 2008; McClosky et al., 2010), instance weighting (Jiang and Zhai, 2007; Foster et al., 2010), schema harmonisation (Wang et al., 2010) and semi-supervised/lightly supervised approaches (Sagae and Tsujii, 2007; Liu et al., 2011; Pan et al., 2013). More generally there is a wide body of work in transfer learning (also known as domain adaptation) that tries to handle discrepancies between training and testing distributions (Pan and Yang, 2010). As an illustration of near domain bias consider the list of high frequency named entities in Table 1 drawn from two sub-domains in the research literature of </context>
<context position="30565" citStr="Arnold et al., 2008" startWordPosition="5168" endWordPosition="5171">iomedical NER incurs a severe loss in f-score. We have demonstrated empirically that strategies that make use of multi-domain corpora such as stacking learners and feature augmentation mitigate the accuracy loss but do not necessarily result in superior performance except for selected classes such as organisms where there appears to be broad terminology consensus. Simple pooling of data across domains failed to exploit size effects especially for the complex class of phenotypes. The list of strategies employed has not been exhaustive and it is possible that others such as feature hierarchies (Arnold et al., 2008) might yield better results. BioNER is complicated by various factors such as descriptive names, polysemous terms, conjuctions, nested constructions and a high quantity of abbreviations. We have shown that performance is also held back by not considering document level properties related to domain such as topicality. We can expect lower performance and higher annotation costs if we do not adequately allow for the distributional dissimilarities of domains during learning, even in closely related topical settings. 18 Acknowledgments The authors gratefully acknowledge the many helpful comments fr</context>
</contexts>
<marker>Arnold, Nallapati, Cohen, 2008</marker>
<rawString>A. Arnold, N. Nallapati, and W. Cohen. 2008. Exploiting feature hierarchy for transfer learning in named entity recognition. In Annual meeting of the Association for Computational Linguistics (ACL 2008), pages 245–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Braga-Neto</author>
<author>E Dougherty</author>
</authors>
<title>Is crossvalidation valid for small-sample microarray classification?</title>
<date>2004</date>
<journal>Bioinformatics,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="18695" citStr="Braga-Neto and Dougherty, 2004" startWordPosition="3068" endWordPosition="3071"> subsets of approximately equal size designated as Di for i = 1,.., k. The NER learner is trained successively on k − 1 folds from D and tested on a held out fold over k iterations. In order to preserve independence between contexts in training and held out data we assume here that the unit of division is the document, i.e. a single Medline abstract. Estimated prediction error is calculated based on the learner’s labels on the k held out folds. Whilst k-CV is known to be nearly unbiased it is a highly variable estimator. Several studies have looked at k-CV for small sample sets. For example, (Braga-Neto and Dougherty, 2004) found on classifier experiments for small microarray samples (20 &lt;= n &lt;= 120) that whilst kCV showed low bias they suffered from excessive variance compared to bootstrap or resubstitution estimators. One cause of variance has been identified as within-block and between-block training errors arising from the disproportionate effects of a single abstract appearing in the training set of many folds. In order to reduce this effect Monte Carlo cross validation was used (also called CV with repetition). 100 iterations were used to randomly reorder the documents in the corpora before 10-fold CV samp</context>
</contexts>
<marker>Braga-Neto, Dougherty, 2004</marker>
<rawString>U. Braga-Neto and E. Dougherty. 2004. Is crossvalidation valid for small-sample microarray classification? Bioinformatics, 20(3):374–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Annual meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>256--263</pages>
<marker>Daum´e, 2007</marker>
<rawString>H. Daum´e III. 2007. Frustratingly easy domain adaptation. In Annual meeting of the Association for Computational Linguistics (ACL 2007), pages 256– 263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Degtyarenko</author>
<author>P de Matos</author>
<author>M Ennis</author>
<author>J Hastings</author>
<author>M Zbinden</author>
<author>A McNaught</author>
<author>R Alc´antara</author>
<author>M Darsow</author>
<author>M Guedj</author>
<author>M Ashburner</author>
</authors>
<title>ChEBI: a database and ontology for chemical entities of biological interest. Nucleic acids research, 36(suppl 1):D344–D350.</title>
<date>2008</date>
<marker>Degtyarenko, de Matos, Ennis, Hastings, Zbinden, McNaught, Alc´antara, Darsow, Guedj, Ashburner, 2008</marker>
<rawString>K. Degtyarenko, P. de Matos, M. Ennis, J. Hastings, M. Zbinden, A. McNaught, R. Alc´antara, M. Darsow, M. Guedj, and M. Ashburner. 2008. ChEBI: a database and ontology for chemical entities of biological interest. Nucleic acids research, 36(suppl 1):D344–D350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Doan</author>
<author>N Collier</author>
<author>H Xu</author>
<author>P Duy</author>
<author>T Phuong</author>
</authors>
<title>Recognition of medication information from discharge summaries using ensembles of classifiers.</title>
<date>2012</date>
<journal>BMC Medical Informatics and Decision Making,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="14410" citStr="Doan et al., 2012" startWordPosition="2340" endWordPosition="2343">ability that a word token in an entity class from one corpus would appear in an instance of the same entity class in the other corpus, reported as columns a and b. Although the probability of an exact match in instances between entities in the two corpora is generally quite low (below 20% - data not shown) there appears to be significant vocabulary overlap in most classes except for chemicals. 3.2 Conditional Random Fields As in (Finkel and Manning, 2009) we apply our approach to a linear chain conditional random field (CRF) model (Lafferty et al., 2001; McCallum and Wei, 2003; Settles, 2004; Doan et al., 2012) using the Mallet toolkits with default parameters. CRFs have been shown consistently to be among the highest performing bioNER learners. The data selection strategies employed here though are neutral and could have been applied to any other fully supervised learner model. 3.3 Features We made use of a wide range of features, both conventional features such as word or part of speech, as well as gazetteers derived from external classification schemes that have been hand crafted by experts. These are shown in Table 3. Previous studies such as (Ratinov and Roth, 2009) have noted that domain gazet</context>
</contexts>
<marker>Doan, Collier, Xu, Duy, Phuong, 2012</marker>
<rawString>S. Doan, N. Collier, H. Xu, P. Duy, and T. Phuong. 2012. Recognition of medication information from discharge summaries using ensembles of classifiers. BMC Medical Informatics and Decision Making, 12(1):36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ekbal</author>
<author>S Saha</author>
</authors>
<title>Stacked ensemble coupled with feature selection for biomedical entity extraction. Knowledge-Based Systems.</title>
<date>2013</date>
<contexts>
<context position="23334" citStr="Ekbal and Saha, 2013" startWordPosition="3861" endWordPosition="3864">ed loss on near domain transferrence and represents the standard operational setting for users. 16 M3: MIX-IN We trained on 100% of the source domain and unified this with 90% of the folded in target domain data, leaving 10% for testing. This method reflects the pooling technique typically employed in corpus construction for bioNER. M4: STACK We trained a CRF model on 100% of the source domain and stacked it with another CRF trained on 90% of the folded in target domain data. Stacking employs a meta-classifier and is a popular method for constructing high performance ensembles of classifiers (Ekbal and Saha, 2013). In this case we collected the output labels from the source domain-trained CRF on target sentences and added them as features for the target domain trained CRF. M5: BINARY CLASS We re-labeled the complex class PHE as PHE-C1 in C1 and PHEC2 in C2 and repeated M3. Afterwards we recombined PHE-C1 and PHE-C2 into PHE. M6: FRUSTRATINGLY SIMPLE We followed the feature augmentation approach of (Daum´e III, 2007). This method effectively provides a joint learning model on C1 and C2 by splitting each feature into three parts: one for sharing cross domain values and one for each domain specific value.</context>
</contexts>
<marker>Ekbal, Saha, 2013</marker>
<rawString>A. Ekbal and S. Saha. 2013. Stacked ensemble coupled with feature selection for biomedical entity extraction. Knowledge-Based Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Federhen</author>
</authors>
<title>The NCBI taxonomy database. Nucleic acids research,</title>
<date>2012</date>
<pages>40--1</pages>
<contexts>
<context position="21574" citStr="Federhen, 2012" startWordPosition="3572" endWordPosition="3573">2 0.12 0.03 0.15 F19 ChEBIi 0.01 0.15 0.03 0.20 F20 JOCHEMj 0.01 0.15 0.01 0.14 F21 NCBIk 0.01 0.14 0.01 0.14 F22 UMLSl disease 0.01 0.14 0.03 0.24 F23 NCBI gene 0.02 0.18 0.02 0.19 F24 GOm 0.13 0.38 0.05 0.28 F25 UMLSn 0.48 0.12 0.52 0.11 F26 45CLUSTERSo 0.50 0.10 0.47 0.10 Table 3: Features used in the experiments. aThe GENIA named entity tagger (Kim et al., 2003), b(Robinson et al., 2008), c(Smith et al., 2004), d(Gkoutos et al., 2005), e(Gremse et al., 2011), f(Rosse and Mejino, 2003), g(Hayamizu et al., 2005), h(Natale et al., 2011) , i(Degtyarenko et al., 2008), j(Hettne et al., 2009),k(Federhen, 2012),l(Lindberg et al., 1993),m(Gene Ontology Consortium, 2000),n133 categories from the UMLS,o45 cluster classes derived by Richard Socher and Christoph Manning PubMed available at http://nlp.stanford.edu/software/bionlp2011-distsim-clusters-v1.tar.gz precision = TP/(TP + FP) (5) and, recall = TP/(TP + FN) (6) A true positive (TP) is a gold standard NE tagged by the system as an NE. A true negative (TN) is a gold standard none-NE tagged by the system as a none-NE. A false positive (FP) is a gold standard none-NE tagged by the system as an NE. Evaluation is based on correctly marked whole entities</context>
</contexts>
<marker>Federhen, 2012</marker>
<rawString>S. Federhen. 2012. The NCBI taxonomy database. Nucleic acids research, 40(D1):D136–D143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Finkel</author>
<author>C Manning</author>
</authors>
<title>Hierarchical bayesian domain adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>602--610</pages>
<contexts>
<context position="14251" citStr="Finkel and Manning, 2009" startWordPosition="2313" endWordPosition="2316">greatest variation. Average entity lengths in tokens (not shown) indicate that PHE are significantly longer than other entity mentions; and (4) We calculated the probability that a word token in an entity class from one corpus would appear in an instance of the same entity class in the other corpus, reported as columns a and b. Although the probability of an exact match in instances between entities in the two corpora is generally quite low (below 20% - data not shown) there appears to be significant vocabulary overlap in most classes except for chemicals. 3.2 Conditional Random Fields As in (Finkel and Manning, 2009) we apply our approach to a linear chain conditional random field (CRF) model (Lafferty et al., 2001; McCallum and Wei, 2003; Settles, 2004; Doan et al., 2012) using the Mallet toolkits with default parameters. CRFs have been shown consistently to be among the highest performing bioNER learners. The data selection strategies employed here though are neutral and could have been applied to any other fully supervised learner model. 3.3 Features We made use of a wide range of features, both conventional features such as word or part of speech, as well as gazetteers derived from external classifica</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>J. Finkel and C. Manning. 2009. Hierarchical bayesian domain adaptation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 602–610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Foster</author>
<author>C Goutte</author>
<author>R Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010),</booktitle>
<pages>451--459</pages>
<contexts>
<context position="2967" citStr="Foster et al., 2010" startWordPosition="441" endWordPosition="444">sed to demonstrate putative prediction error across the heterogeneous collection of 22 million Medline abstracts. Once this assumption is made explicit it is of interest to both researchers and users that the implications and limitations of such experimental settings are explored. Cross domain studies have indicated an advantage for mechanisms that compensate for domain bias. For fully supervised learning, which is the scenario we explore here, recent methods include: feature augmentation (Daum´e III, 2007; Arnold et al., 2008; McClosky et al., 2010), instance weighting (Jiang and Zhai, 2007; Foster et al., 2010), schema harmonisation (Wang et al., 2010) and semi-supervised/lightly supervised approaches (Sagae and Tsujii, 2007; Liu et al., 2011; Pan et al., 2013). More generally there is a wide body of work in transfer learning (also known as domain adaptation) that tries to handle discrepancies between training and testing distributions (Pan and Yang, 2010). As an illustration of near domain bias consider the list of high frequency named entities in Table 1 drawn from two sub-domains in the research literature of hereditary diseases. A domain expert in hereditary diseases would have no difficulty in </context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>G. Foster, C. Goutte, and R. Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010), pages 451– 459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gene Ontology Consortium</author>
</authors>
<title>Gene ontology: tool for the unification of biology.</title>
<date>2000</date>
<journal>Nature Genetics,</journal>
<pages>25--19</pages>
<contexts>
<context position="21633" citStr="Consortium, 2000" startWordPosition="3578" endWordPosition="3579">Mj 0.01 0.15 0.01 0.14 F21 NCBIk 0.01 0.14 0.01 0.14 F22 UMLSl disease 0.01 0.14 0.03 0.24 F23 NCBI gene 0.02 0.18 0.02 0.19 F24 GOm 0.13 0.38 0.05 0.28 F25 UMLSn 0.48 0.12 0.52 0.11 F26 45CLUSTERSo 0.50 0.10 0.47 0.10 Table 3: Features used in the experiments. aThe GENIA named entity tagger (Kim et al., 2003), b(Robinson et al., 2008), c(Smith et al., 2004), d(Gkoutos et al., 2005), e(Gremse et al., 2011), f(Rosse and Mejino, 2003), g(Hayamizu et al., 2005), h(Natale et al., 2011) , i(Degtyarenko et al., 2008), j(Hettne et al., 2009),k(Federhen, 2012),l(Lindberg et al., 1993),m(Gene Ontology Consortium, 2000),n133 categories from the UMLS,o45 cluster classes derived by Richard Socher and Christoph Manning PubMed available at http://nlp.stanford.edu/software/bionlp2011-distsim-clusters-v1.tar.gz precision = TP/(TP + FP) (5) and, recall = TP/(TP + FN) (6) A true positive (TP) is a gold standard NE tagged by the system as an NE. A true negative (TN) is a gold standard none-NE tagged by the system as a none-NE. A false positive (FP) is a gold standard none-NE tagged by the system as an NE. Evaluation is based on correctly marked whole entities rather than tokens. 4 Experimental design In this section </context>
</contexts>
<marker>Consortium, 2000</marker>
<rawString>Gene Ontology Consortium. 2000. Gene ontology: tool for the unification of biology. Nature Genetics, 25:19–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gkoutos</author>
<author>E Green</author>
<author>A Mallon</author>
<author>J Hancock</author>
<author>D Davidson</author>
</authors>
<title>Using ontologies to describe mouse phenotypes. Genome Biology,</title>
<date>2005</date>
<contexts>
<context position="21401" citStr="Gkoutos et al., 2005" startWordPosition="3544" endWordPosition="3547">Ob 0.07 0.41 0.09 0.33 F13 MPc 0.03 0.33 0.06 0.33 F14 PATOd 0.01 0.03 0.02 0.04 F15 BTOe 0.03 0.32 0.03 0.29 F16 FMAf 0.05 0.28 0.05 0.23 F17 MAg 0.02 0.31 0.02 0.29 F18 PROh 0.02 0.12 0.03 0.15 F19 ChEBIi 0.01 0.15 0.03 0.20 F20 JOCHEMj 0.01 0.15 0.01 0.14 F21 NCBIk 0.01 0.14 0.01 0.14 F22 UMLSl disease 0.01 0.14 0.03 0.24 F23 NCBI gene 0.02 0.18 0.02 0.19 F24 GOm 0.13 0.38 0.05 0.28 F25 UMLSn 0.48 0.12 0.52 0.11 F26 45CLUSTERSo 0.50 0.10 0.47 0.10 Table 3: Features used in the experiments. aThe GENIA named entity tagger (Kim et al., 2003), b(Robinson et al., 2008), c(Smith et al., 2004), d(Gkoutos et al., 2005), e(Gremse et al., 2011), f(Rosse and Mejino, 2003), g(Hayamizu et al., 2005), h(Natale et al., 2011) , i(Degtyarenko et al., 2008), j(Hettne et al., 2009),k(Federhen, 2012),l(Lindberg et al., 1993),m(Gene Ontology Consortium, 2000),n133 categories from the UMLS,o45 cluster classes derived by Richard Socher and Christoph Manning PubMed available at http://nlp.stanford.edu/software/bionlp2011-distsim-clusters-v1.tar.gz precision = TP/(TP + FP) (5) and, recall = TP/(TP + FN) (6) A true positive (TP) is a gold standard NE tagged by the system as an NE. A true negative (TN) is a gold standard none</context>
</contexts>
<marker>Gkoutos, Green, Mallon, Hancock, Davidson, 2005</marker>
<rawString>G. Gkoutos, E. Green, A. Mallon, J. Hancock, and D. Davidson. 2005. Using ontologies to describe mouse phenotypes. Genome Biology, 6:R8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gremse</author>
<author>A Chang</author>
<author>I Schomburg</author>
<author>A Grote</author>
<author>M Scheer</author>
<author>C Ebeling</author>
<author>D Schomburg</author>
</authors>
<title>The BRENDA tissue ontology (BTO): the first all-integrating ontology of all organisms for enzyme sources.</title>
<date>2011</date>
<journal>Nucleic Acids Research,</journal>
<volume>39</volume>
<pages>1--507</pages>
<contexts>
<context position="21425" citStr="Gremse et al., 2011" startWordPosition="3548" endWordPosition="3551">3 MPc 0.03 0.33 0.06 0.33 F14 PATOd 0.01 0.03 0.02 0.04 F15 BTOe 0.03 0.32 0.03 0.29 F16 FMAf 0.05 0.28 0.05 0.23 F17 MAg 0.02 0.31 0.02 0.29 F18 PROh 0.02 0.12 0.03 0.15 F19 ChEBIi 0.01 0.15 0.03 0.20 F20 JOCHEMj 0.01 0.15 0.01 0.14 F21 NCBIk 0.01 0.14 0.01 0.14 F22 UMLSl disease 0.01 0.14 0.03 0.24 F23 NCBI gene 0.02 0.18 0.02 0.19 F24 GOm 0.13 0.38 0.05 0.28 F25 UMLSn 0.48 0.12 0.52 0.11 F26 45CLUSTERSo 0.50 0.10 0.47 0.10 Table 3: Features used in the experiments. aThe GENIA named entity tagger (Kim et al., 2003), b(Robinson et al., 2008), c(Smith et al., 2004), d(Gkoutos et al., 2005), e(Gremse et al., 2011), f(Rosse and Mejino, 2003), g(Hayamizu et al., 2005), h(Natale et al., 2011) , i(Degtyarenko et al., 2008), j(Hettne et al., 2009),k(Federhen, 2012),l(Lindberg et al., 1993),m(Gene Ontology Consortium, 2000),n133 categories from the UMLS,o45 cluster classes derived by Richard Socher and Christoph Manning PubMed available at http://nlp.stanford.edu/software/bionlp2011-distsim-clusters-v1.tar.gz precision = TP/(TP + FP) (5) and, recall = TP/(TP + FN) (6) A true positive (TP) is a gold standard NE tagged by the system as an NE. A true negative (TN) is a gold standard none-NE tagged by the system</context>
</contexts>
<marker>Gremse, Chang, Schomburg, Grote, Scheer, Ebeling, Schomburg, 2011</marker>
<rawString>M. Gremse, A. Chang, I. Schomburg, A. Grote, M. Scheer, C. Ebeling, and D. Schomburg. 2011. The BRENDA tissue ontology (BTO): the first all-integrating ontology of all organisms for enzyme sources. Nucleic Acids Research, 39(suppl 1):D507–D513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hamosh</author>
<author>A F Scott</author>
<author>J S Amberger</author>
<author>C A Bocchini</author>
</authors>
<title>Online mendelian inheritance of man (OMIM), a knowledgebase of human genes and genetic disorders.</title>
<date>2005</date>
<journal>Nucleic Acids Research,</journal>
<volume>33</volume>
<pages>1--514</pages>
<contexts>
<context position="10466" citStr="Hamosh et al., 2005" startWordPosition="1671" endWordPosition="1674">sting can be used to learn h. Our task is to explore various strategies for data selection and re-factoring labels/features in order to maximise held out performance. 3.1 Data In this paper we aim to empirically test domain transferrence for bioNER under the condition that the test and training data are relatively small and drawn from near domains, i.e. from studies on different types of heritable diseases. To do this we selected Medline abstracts from PubMed that were cited by biocuration experts in the canonical database on heritable diseases, the Online Mendelian Inheritance of Man (OMIM) (Hamosh et al., 2005). We selected auto-immune diseases and cardio-vascular diseases for our two corpora which we denote as C1 and C2 respectively. By comparing performance of a single model, a single annotator and a single annotation scheme with a range of sampling techniques we hope to quantify the effects of domain transferrence in isolation. The target classes for the entities are as follows: ANA Anatomical structures in the body. e.g. liver, heart. CHE A chemical or drug. e.g. pristane, histamine, S-nitrosoglutathione. DIS Diseases. e.g. end stage renal disease, mitral valve prolapse. GGP Genes and gene produ</context>
</contexts>
<marker>Hamosh, Scott, Amberger, Bocchini, 2005</marker>
<rawString>A. Hamosh, A. F. Scott, J. S. Amberger, and C. A. Bocchini. 2005. Online mendelian inheritance of man (OMIM), a knowledgebase of human genes and genetic disorders. Nucleic Acids Research, 33(suppl 1):D514–D517.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hayamizu</author>
<author>M Mangan</author>
<author>J Corradi</author>
<author>J Kadin</author>
<author>M Ringwald</author>
</authors>
<title>The adult mouse anatomical dictionary: a tool for annotating and integrating data.</title>
<date>2005</date>
<journal>Genome Biol,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="21478" citStr="Hayamizu et al., 2005" startWordPosition="3556" endWordPosition="3559">0.04 F15 BTOe 0.03 0.32 0.03 0.29 F16 FMAf 0.05 0.28 0.05 0.23 F17 MAg 0.02 0.31 0.02 0.29 F18 PROh 0.02 0.12 0.03 0.15 F19 ChEBIi 0.01 0.15 0.03 0.20 F20 JOCHEMj 0.01 0.15 0.01 0.14 F21 NCBIk 0.01 0.14 0.01 0.14 F22 UMLSl disease 0.01 0.14 0.03 0.24 F23 NCBI gene 0.02 0.18 0.02 0.19 F24 GOm 0.13 0.38 0.05 0.28 F25 UMLSn 0.48 0.12 0.52 0.11 F26 45CLUSTERSo 0.50 0.10 0.47 0.10 Table 3: Features used in the experiments. aThe GENIA named entity tagger (Kim et al., 2003), b(Robinson et al., 2008), c(Smith et al., 2004), d(Gkoutos et al., 2005), e(Gremse et al., 2011), f(Rosse and Mejino, 2003), g(Hayamizu et al., 2005), h(Natale et al., 2011) , i(Degtyarenko et al., 2008), j(Hettne et al., 2009),k(Federhen, 2012),l(Lindberg et al., 1993),m(Gene Ontology Consortium, 2000),n133 categories from the UMLS,o45 cluster classes derived by Richard Socher and Christoph Manning PubMed available at http://nlp.stanford.edu/software/bionlp2011-distsim-clusters-v1.tar.gz precision = TP/(TP + FP) (5) and, recall = TP/(TP + FN) (6) A true positive (TP) is a gold standard NE tagged by the system as an NE. A true negative (TN) is a gold standard none-NE tagged by the system as a none-NE. A false positive (FP) is a gold standa</context>
</contexts>
<marker>Hayamizu, Mangan, Corradi, Kadin, Ringwald, 2005</marker>
<rawString>T. Hayamizu, M. Mangan, J. Corradi, J. Kadin, M. Ringwald, et al. 2005. The adult mouse anatomical dictionary: a tool for annotating and integrating data. Genome Biol, 6(3):R29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hettne</author>
<author>R Stierum</author>
<author>M Schuemie</author>
<author>P Hendriksen</author>
<author>B Schijvenaars</author>
<author>E van Mulligen</author>
<author>J Kleinjans</author>
<author>J Kors</author>
</authors>
<title>A dictionary to identify small molecules and drugs in free text.</title>
<date>2009</date>
<journal>Bioinformatics,</journal>
<volume>25</volume>
<issue>22</issue>
<marker>Hettne, Stierum, Schuemie, Hendriksen, Schijvenaars, van Mulligen, Kleinjans, Kors, 2009</marker>
<rawString>K. Hettne, R. Stierum, M. Schuemie, P. Hendriksen, B. Schijvenaars, E. van Mulligen, J. Kleinjans, and J. Kors. 2009. A dictionary to identify small molecules and drugs in free text. Bioinformatics, 25(22):2983–2991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>C Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in NLP.</title>
<date>2007</date>
<booktitle>In Annual meeting of the Association for Computational Linguistics (ACL</booktitle>
<volume>volume</volume>
<pages>22</pages>
<contexts>
<context position="2945" citStr="Jiang and Zhai, 2007" startWordPosition="437" endWordPosition="440">ch corpora are often used to demonstrate putative prediction error across the heterogeneous collection of 22 million Medline abstracts. Once this assumption is made explicit it is of interest to both researchers and users that the implications and limitations of such experimental settings are explored. Cross domain studies have indicated an advantage for mechanisms that compensate for domain bias. For fully supervised learning, which is the scenario we explore here, recent methods include: feature augmentation (Daum´e III, 2007; Arnold et al., 2008; McClosky et al., 2010), instance weighting (Jiang and Zhai, 2007; Foster et al., 2010), schema harmonisation (Wang et al., 2010) and semi-supervised/lightly supervised approaches (Sagae and Tsujii, 2007; Liu et al., 2011; Pan et al., 2013). More generally there is a wide body of work in transfer learning (also known as domain adaptation) that tries to handle discrepancies between training and testing distributions (Pan and Yang, 2010). As an illustration of near domain bias consider the list of high frequency named entities in Table 1 drawn from two sub-domains in the research literature of hereditary diseases. A domain expert in hereditary diseases would </context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>J. Jiang and C. Zhai. 2007. Instance weighting for domain adaptation in NLP. In Annual meeting of the Association for Computational Linguistics (ACL 2007), volume 2007, page 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Kim</author>
<author>T Ohta</author>
<author>Y Tateishi</author>
<author>J Tsujii</author>
</authors>
<title>GENIA corpus - a semantically annotated corpus for bio-textmining. Bioinformatics,</title>
<date>2003</date>
<pages>182</pages>
<contexts>
<context position="2168" citStr="Kim et al., 2003" startWordPosition="317" endWordPosition="320">re important experimental tasks in supervised machine learning for suggesting approaches that will generalise well on real world data. Research in biomedical named entity recognition (bioNER) often displays two features: (1) small samples of labeled data, and (2) an implicit assumption that the future data will be ∗collier@ebi.ac.uk drawn from a similar distribution to the labeled data and hence that minimising expected prediction error on held out data will minimise actual future loss. Since expert labeling is time consuming and expensive, labeled data sets tend to be relatively small, e.g. (Kim et al., 2003; Tanabe et al., 2005; Pyysalo et al., 2007), in the region of a few hundred or thousand Medline abstracts. Despite the danger of intrinsic idiosyncracies such corpora are often used to demonstrate putative prediction error across the heterogeneous collection of 22 million Medline abstracts. Once this assumption is made explicit it is of interest to both researchers and users that the implications and limitations of such experimental settings are explored. Cross domain studies have indicated an advantage for mechanisms that compensate for domain bias. For fully supervised learning, which is th</context>
<context position="21327" citStr="Kim et al., 2003" startWordPosition="3532" endWordPosition="3535">thesis 0.04 0.20 0.04 0.23 F11 Abbreviation 0.08 0.22 0.06 0.24 F12 HPOb 0.07 0.41 0.09 0.33 F13 MPc 0.03 0.33 0.06 0.33 F14 PATOd 0.01 0.03 0.02 0.04 F15 BTOe 0.03 0.32 0.03 0.29 F16 FMAf 0.05 0.28 0.05 0.23 F17 MAg 0.02 0.31 0.02 0.29 F18 PROh 0.02 0.12 0.03 0.15 F19 ChEBIi 0.01 0.15 0.03 0.20 F20 JOCHEMj 0.01 0.15 0.01 0.14 F21 NCBIk 0.01 0.14 0.01 0.14 F22 UMLSl disease 0.01 0.14 0.03 0.24 F23 NCBI gene 0.02 0.18 0.02 0.19 F24 GOm 0.13 0.38 0.05 0.28 F25 UMLSn 0.48 0.12 0.52 0.11 F26 45CLUSTERSo 0.50 0.10 0.47 0.10 Table 3: Features used in the experiments. aThe GENIA named entity tagger (Kim et al., 2003), b(Robinson et al., 2008), c(Smith et al., 2004), d(Gkoutos et al., 2005), e(Gremse et al., 2011), f(Rosse and Mejino, 2003), g(Hayamizu et al., 2005), h(Natale et al., 2011) , i(Degtyarenko et al., 2008), j(Hettne et al., 2009),k(Federhen, 2012),l(Lindberg et al., 1993),m(Gene Ontology Consortium, 2000),n133 categories from the UMLS,o45 cluster classes derived by Richard Socher and Christoph Manning PubMed available at http://nlp.stanford.edu/software/bionlp2011-distsim-clusters-v1.tar.gz precision = TP/(TP + FP) (5) and, recall = TP/(TP + FN) (6) A true positive (TP) is a gold standard NE t</context>
</contexts>
<marker>Kim, Ohta, Tateishi, Tsujii, 2003</marker>
<rawString>J. D. Kim, T. Ohta, Y. Tateishi, and J. Tsujii. 2003. GENIA corpus - a semantically annotated corpus for bio-textmining. Bioinformatics, 19(Suppl.1):180– 182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
<author>T Ohta</author>
<author>Y Tsuruoka</author>
<author>Y Tateisi</author>
<author>N Collier</author>
</authors>
<title>Introduction to the bio-entity recognition task at JNLPBA.</title>
<date>2004</date>
<booktitle>Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA), Geneva, Switzerland,</booktitle>
<pages>70--75</pages>
<editor>In N. Collier, P. Ruch, and A. Nazarenko, editors,</editor>
<contexts>
<context position="7901" citStr="Kim et al., 2004" startWordPosition="1231" endWordPosition="1234">of the entity class definitions, distributional properties of the entity classes and the degree of overlap between corpora. A follow up study by the authors (Wang et al., 2010) looked at increasing compatibility between the GENIA and GENETAG corpora by reorganising the annotation schema to unify protein, DNA and RNA NER under a new label GGP (Gene and Gene Product). However the best performance from the coarse grained annotations still do not improve on the intra-corpus data. In earlier work, (Tsai et al., 2006) looked at schema differences between the JNLPBA corpus of 2000 Medline abstracts (Kim et al., 2004) and the BioCreative corpus of 15,000 Medline sentences (Yeh et al., 2005) and tried to harmonise matching criteria. They demonstrated that relaxing the boundary matching criteria was helpful in maximising the cross-domain performance. 12 In the clinical domain (Wagholikar et al., 2013), explore the effect of harmonising annotation guidelines on the 2010 i2b2 challenge with Mayo Clinic Rochester (MCR) electronic patient records. They concluded that the effectiveness of pooling - i.e. merging of corpora by ensuring a common format and harmonised semantics - is dependent on several factors inclu</context>
</contexts>
<marker>Kim, Ohta, Tsuruoka, Tateisi, Collier, 2004</marker>
<rawString>J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier. 2004. Introduction to the bio-entity recognition task at JNLPBA. In N. Collier, P. Ruch, and A. Nazarenko, editors, Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA), Geneva, Switzerland, pages 70–75, August 28–29. held in conjunction with COLING’2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<location>Massachusetts, USA,</location>
<contexts>
<context position="14351" citStr="Lafferty et al., 2001" startWordPosition="2330" endWordPosition="2333">ger than other entity mentions; and (4) We calculated the probability that a word token in an entity class from one corpus would appear in an instance of the same entity class in the other corpus, reported as columns a and b. Although the probability of an exact match in instances between entities in the two corpora is generally quite low (below 20% - data not shown) there appears to be significant vocabulary overlap in most classes except for chemicals. 3.2 Conditional Random Fields As in (Finkel and Manning, 2009) we apply our approach to a linear chain conditional random field (CRF) model (Lafferty et al., 2001; McCallum and Wei, 2003; Settles, 2004; Doan et al., 2012) using the Mallet toolkits with default parameters. CRFs have been shown consistently to be among the highest performing bioNER learners. The data selection strategies employed here though are neutral and could have been applied to any other fully supervised learner model. 3.3 Features We made use of a wide range of features, both conventional features such as word or part of speech, as well as gazetteers derived from external classification schemes that have been hand crafted by experts. These are shown in Table 3. Previous studies su</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, Massachusetts, USA, pages 282– 289, June 28th – July 1st.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald A B Lindberg</author>
<author>L Humphreys</author>
<author>Betsy</author>
<author>T McCray</author>
<author>Alexa</author>
</authors>
<title>The unified medical language system.</title>
<date>1993</date>
<booktitle>Methods of Information in Medicine,</booktitle>
<pages>32--281</pages>
<contexts>
<context position="21599" citStr="Lindberg et al., 1993" startWordPosition="3573" endWordPosition="3576">19 ChEBIi 0.01 0.15 0.03 0.20 F20 JOCHEMj 0.01 0.15 0.01 0.14 F21 NCBIk 0.01 0.14 0.01 0.14 F22 UMLSl disease 0.01 0.14 0.03 0.24 F23 NCBI gene 0.02 0.18 0.02 0.19 F24 GOm 0.13 0.38 0.05 0.28 F25 UMLSn 0.48 0.12 0.52 0.11 F26 45CLUSTERSo 0.50 0.10 0.47 0.10 Table 3: Features used in the experiments. aThe GENIA named entity tagger (Kim et al., 2003), b(Robinson et al., 2008), c(Smith et al., 2004), d(Gkoutos et al., 2005), e(Gremse et al., 2011), f(Rosse and Mejino, 2003), g(Hayamizu et al., 2005), h(Natale et al., 2011) , i(Degtyarenko et al., 2008), j(Hettne et al., 2009),k(Federhen, 2012),l(Lindberg et al., 1993),m(Gene Ontology Consortium, 2000),n133 categories from the UMLS,o45 cluster classes derived by Richard Socher and Christoph Manning PubMed available at http://nlp.stanford.edu/software/bionlp2011-distsim-clusters-v1.tar.gz precision = TP/(TP + FP) (5) and, recall = TP/(TP + FN) (6) A true positive (TP) is a gold standard NE tagged by the system as an NE. A true negative (TN) is a gold standard none-NE tagged by the system as a none-NE. A false positive (FP) is a gold standard none-NE tagged by the system as an NE. Evaluation is based on correctly marked whole entities rather than tokens. 4 Ex</context>
</contexts>
<marker>Lindberg, Humphreys, Betsy, McCray, Alexa, 1993</marker>
<rawString>Donald A.B. Lindberg, L. Humphreys, Betsy, and T. McCray, Alexa. 1993. The unified medical language system. Methods of Information in Medicine, 32:281–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Shaodian Zhang</author>
<author>Furu Wei</author>
<author>Ming Zhou</author>
</authors>
<title>Recognizing named entities in tweets.</title>
<date>2011</date>
<booktitle>In Annual meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>359--367</pages>
<contexts>
<context position="3101" citStr="Liu et al., 2011" startWordPosition="460" endWordPosition="463">made explicit it is of interest to both researchers and users that the implications and limitations of such experimental settings are explored. Cross domain studies have indicated an advantage for mechanisms that compensate for domain bias. For fully supervised learning, which is the scenario we explore here, recent methods include: feature augmentation (Daum´e III, 2007; Arnold et al., 2008; McClosky et al., 2010), instance weighting (Jiang and Zhai, 2007; Foster et al., 2010), schema harmonisation (Wang et al., 2010) and semi-supervised/lightly supervised approaches (Sagae and Tsujii, 2007; Liu et al., 2011; Pan et al., 2013). More generally there is a wide body of work in transfer learning (also known as domain adaptation) that tries to handle discrepancies between training and testing distributions (Pan and Yang, 2010). As an illustration of near domain bias consider the list of high frequency named entities in Table 1 drawn from two sub-domains in the research literature of hereditary diseases. A domain expert in hereditary diseases would have no difficulty in dividing them into two non-overlapping sets corresponding to the two near domains with one term t5 patients shared by both: {t�,t�,t�,</context>
</contexts>
<marker>Liu, Zhang, Wei, Zhou, 2011</marker>
<rawString>Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming Zhou. 2011. Recognizing named entities in tweets. In Annual meeting of the Association for Computational Linguistics (ACL 2011), pages 359–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>L Wei</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In Proc. Seventh Conference on Natural language learning at HLT-NAACL 2003 - Volume 4, CONLL ’03,</booktitle>
<pages>188--191</pages>
<contexts>
<context position="14375" citStr="McCallum and Wei, 2003" startWordPosition="2334" endWordPosition="2337">entions; and (4) We calculated the probability that a word token in an entity class from one corpus would appear in an instance of the same entity class in the other corpus, reported as columns a and b. Although the probability of an exact match in instances between entities in the two corpora is generally quite low (below 20% - data not shown) there appears to be significant vocabulary overlap in most classes except for chemicals. 3.2 Conditional Random Fields As in (Finkel and Manning, 2009) we apply our approach to a linear chain conditional random field (CRF) model (Lafferty et al., 2001; McCallum and Wei, 2003; Settles, 2004; Doan et al., 2012) using the Mallet toolkits with default parameters. CRFs have been shown consistently to be among the highest performing bioNER learners. The data selection strategies employed here though are neutral and could have been applied to any other fully supervised learner model. 3.3 Features We made use of a wide range of features, both conventional features such as word or part of speech, as well as gazetteers derived from external classification schemes that have been hand crafted by experts. These are shown in Table 3. Previous studies such as (Ratinov and Roth,</context>
</contexts>
<marker>McCallum, Wei, 2003</marker>
<rawString>A. McCallum and L. Wei. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In Proc. Seventh Conference on Natural language learning at HLT-NAACL 2003 - Volume 4, CONLL ’03, pages 188–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Automatic domain adaptation for parsing. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>28--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2903" citStr="McClosky et al., 2010" startWordPosition="430" endWordPosition="433">te the danger of intrinsic idiosyncracies such corpora are often used to demonstrate putative prediction error across the heterogeneous collection of 22 million Medline abstracts. Once this assumption is made explicit it is of interest to both researchers and users that the implications and limitations of such experimental settings are explored. Cross domain studies have indicated an advantage for mechanisms that compensate for domain bias. For fully supervised learning, which is the scenario we explore here, recent methods include: feature augmentation (Daum´e III, 2007; Arnold et al., 2008; McClosky et al., 2010), instance weighting (Jiang and Zhai, 2007; Foster et al., 2010), schema harmonisation (Wang et al., 2010) and semi-supervised/lightly supervised approaches (Sagae and Tsujii, 2007; Liu et al., 2011; Pan et al., 2013). More generally there is a wide body of work in transfer learning (also known as domain adaptation) that tries to handle discrepancies between training and testing distributions (Pan and Yang, 2010). As an illustration of near domain bias consider the list of high frequency named entities in Table 1 drawn from two sub-domains in the research literature of hereditary diseases. A d</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2010</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2010. Automatic domain adaptation for parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 28–36. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Natale</author>
<author>C Arighi</author>
<author>W Barker</author>
<author>J Blake</author>
<author>C Bult</author>
<author>M Caudy</author>
<author>H Drabkin</author>
<author>P DEustachio</author>
<author>A Evsikov</author>
<author>H Huang</author>
</authors>
<title>The protein ontology: a structured representation of protein forms and complexes. Nucleic acids research, 39(suppl 1):D539– D545.</title>
<date>2011</date>
<contexts>
<context position="21502" citStr="Natale et al., 2011" startWordPosition="3560" endWordPosition="3563">03 0.29 F16 FMAf 0.05 0.28 0.05 0.23 F17 MAg 0.02 0.31 0.02 0.29 F18 PROh 0.02 0.12 0.03 0.15 F19 ChEBIi 0.01 0.15 0.03 0.20 F20 JOCHEMj 0.01 0.15 0.01 0.14 F21 NCBIk 0.01 0.14 0.01 0.14 F22 UMLSl disease 0.01 0.14 0.03 0.24 F23 NCBI gene 0.02 0.18 0.02 0.19 F24 GOm 0.13 0.38 0.05 0.28 F25 UMLSn 0.48 0.12 0.52 0.11 F26 45CLUSTERSo 0.50 0.10 0.47 0.10 Table 3: Features used in the experiments. aThe GENIA named entity tagger (Kim et al., 2003), b(Robinson et al., 2008), c(Smith et al., 2004), d(Gkoutos et al., 2005), e(Gremse et al., 2011), f(Rosse and Mejino, 2003), g(Hayamizu et al., 2005), h(Natale et al., 2011) , i(Degtyarenko et al., 2008), j(Hettne et al., 2009),k(Federhen, 2012),l(Lindberg et al., 1993),m(Gene Ontology Consortium, 2000),n133 categories from the UMLS,o45 cluster classes derived by Richard Socher and Christoph Manning PubMed available at http://nlp.stanford.edu/software/bionlp2011-distsim-clusters-v1.tar.gz precision = TP/(TP + FP) (5) and, recall = TP/(TP + FN) (6) A true positive (TP) is a gold standard NE tagged by the system as an NE. A true negative (TN) is a gold standard none-NE tagged by the system as a none-NE. A false positive (FP) is a gold standard none-NE tagged by the</context>
</contexts>
<marker>Natale, Arighi, Barker, Blake, Bult, Caudy, Drabkin, DEustachio, Evsikov, Huang, 2011</marker>
<rawString>D. Natale, C. Arighi, W. Barker, J. Blake, C. Bult, M. Caudy, H. Drabkin, P. DEustachio, A. Evsikov, H. Huang, et al. 2011. The protein ontology: a structured representation of protein forms and complexes. Nucleic acids research, 39(suppl 1):D539– D545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pan</author>
<author>Q Yang</author>
</authors>
<title>A survey on transfer learning.</title>
<date>2010</date>
<journal>Knowledge and Data Engineering, IEEE Transactions on,</journal>
<volume>22</volume>
<issue>10</issue>
<contexts>
<context position="3319" citStr="Pan and Yang, 2010" startWordPosition="496" endWordPosition="499">mpensate for domain bias. For fully supervised learning, which is the scenario we explore here, recent methods include: feature augmentation (Daum´e III, 2007; Arnold et al., 2008; McClosky et al., 2010), instance weighting (Jiang and Zhai, 2007; Foster et al., 2010), schema harmonisation (Wang et al., 2010) and semi-supervised/lightly supervised approaches (Sagae and Tsujii, 2007; Liu et al., 2011; Pan et al., 2013). More generally there is a wide body of work in transfer learning (also known as domain adaptation) that tries to handle discrepancies between training and testing distributions (Pan and Yang, 2010). As an illustration of near domain bias consider the list of high frequency named entities in Table 1 drawn from two sub-domains in the research literature of hereditary diseases. A domain expert in hereditary diseases would have no difficulty in dividing them into two non-overlapping sets corresponding to the two near domains with one term t5 patients shared by both: {t�,t�,t�,t�} and {t2,t�,t4,t�,t�0}. Previous studies have shown what happens when you radically change the domain and/or the 11 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi</context>
</contexts>
<marker>Pan, Yang, 2010</marker>
<rawString>S. Pan and Q. Yang. 2010. A survey on transfer learning. Knowledge and Data Engineering, IEEE Transactions on, 22(10):1345–1359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pan</author>
<author>Z Toh</author>
<author>J Su</author>
</authors>
<title>Transfer joint embedding for cross-domain named entity recognition.</title>
<date>2013</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="3120" citStr="Pan et al., 2013" startWordPosition="464" endWordPosition="467">s of interest to both researchers and users that the implications and limitations of such experimental settings are explored. Cross domain studies have indicated an advantage for mechanisms that compensate for domain bias. For fully supervised learning, which is the scenario we explore here, recent methods include: feature augmentation (Daum´e III, 2007; Arnold et al., 2008; McClosky et al., 2010), instance weighting (Jiang and Zhai, 2007; Foster et al., 2010), schema harmonisation (Wang et al., 2010) and semi-supervised/lightly supervised approaches (Sagae and Tsujii, 2007; Liu et al., 2011; Pan et al., 2013). More generally there is a wide body of work in transfer learning (also known as domain adaptation) that tries to handle discrepancies between training and testing distributions (Pan and Yang, 2010). As an illustration of near domain bias consider the list of high frequency named entities in Table 1 drawn from two sub-domains in the research literature of hereditary diseases. A domain expert in hereditary diseases would have no difficulty in dividing them into two non-overlapping sets corresponding to the two near domains with one term t5 patients shared by both: {t�,t�,t�,t�} and {t2,t�,t4,t</context>
</contexts>
<marker>Pan, Toh, Su, 2013</marker>
<rawString>S. Pan, Z. Toh, and J. Su. 2013. Transfer joint embedding for cross-domain named entity recognition. ACM Transactions on Information Systems (TOIS), 31(2):7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pyysalo</author>
<author>F Ginter</author>
<author>J Heimonen</author>
<author>J Bj¨orne</author>
<author>J Boberg</author>
<author>J J¨arvinen</author>
<author>T Salakoski</author>
</authors>
<title>Bioinfer: a corpus for information extraction in the biomedical domain.</title>
<date>2007</date>
<journal>BMC bioinformatics,</journal>
<volume>8</volume>
<issue>1</issue>
<marker>Pyysalo, Ginter, Heimonen, Bj¨orne, Boberg, J¨arvinen, Salakoski, 2007</marker>
<rawString>S. Pyysalo, F. Ginter, J. Heimonen, J. Bj¨orne, J. Boberg, J. J¨arvinen, and T. Salakoski. 2007. Bioinfer: a corpus for information extraction in the biomedical domain. BMC bioinformatics, 8(1):50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Quinlan</author>
</authors>
<title>C4. 5: programs for machine learning, volume 1.</title>
<date>1993</date>
<publisher>Morgan kaufmann.</publisher>
<contexts>
<context position="16435" citStr="Quinlan, 1993" startWordPosition="2661" endWordPosition="2662"> those reported here using a grid search on features F1 to F11 and found that f-score performance was uniformly lower when removing any feature (data not shown but available as supplementary material from the first author). In order to characterise the contribution each feature is making in label prediction we wanted to provide a measure of similarity between the feature and the class label probability distributions. Here we use the Gain Ratio (GR) to estimate intracorpus class prediction performance by each feature. GR was used as a splitting function in C4.5 lhttp://mallet.cs.umass.edu/ 14 (Quinlan, 1993) and is defined as GR(C, F) = IG(C, F)/H(F) (1) where C represents a class label and F represents a feature type. IG is information gain and defined as, IG(C, F) = H(C) − H(C|X) (2) H is entropy and defined for feature types as, H(F) = − ∑n p(fi)lo92(p(fi)) (3) i=1 for n feature types fi E F. Further information can be found in (Quinlan, 1993). GR is used in C4.5 in preference to IG because of its ability to normalise for the biases in IG. Generally this results in GR having greater predictive accuracy than IR since it takes into account the number of feature values. Note that GR is undefined </context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Quinlan. 1993. C4. 5: programs for machine learning, volume 1. Morgan kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>147--155</pages>
<contexts>
<context position="14981" citStr="Ratinov and Roth, 2009" startWordPosition="2435" endWordPosition="2438">lum and Wei, 2003; Settles, 2004; Doan et al., 2012) using the Mallet toolkits with default parameters. CRFs have been shown consistently to be among the highest performing bioNER learners. The data selection strategies employed here though are neutral and could have been applied to any other fully supervised learner model. 3.3 Features We made use of a wide range of features, both conventional features such as word or part of speech, as well as gazetteers derived from external classification schemes that have been hand crafted by experts. These are shown in Table 3. Previous studies such as (Ratinov and Roth, 2009) have noted that domain gazetteer features play a critical role in aiding classification. In order to show realistic model behaviour consistent with state-of-the-art techniques we have included gazetteers derived from: the Human Phenotype Ontology (HPO: 15,800 terms), the Mammalian Phenotype Ontology (MP: 23,700 terms), the Phenotypic Attribute and Trait Ontology (PATO: 2,200 synonyms), the Brenda Tissue Ontology (BTO: 9,600 synonyms), the Foundation Model of Anatomy (FMA: 120,000 terms), National Library of Medicine gene list (NLM: 9 million terms), UMLS disease terms (UMLS: 275,000 terms), J</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>L. Ratinov and D. Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P N Robinson</author>
<author>S Kohler</author>
<author>S Bauer</author>
<author>D Seelow</author>
<author>D Horn</author>
<author>S Mundlos</author>
</authors>
<title>The human phenotype ontology: a tool for annotating and analyzing human hereditary disease.</title>
<date>2008</date>
<journal>The American Journal of Human Genetics,</journal>
<volume>83</volume>
<issue>5</issue>
<contexts>
<context position="21353" citStr="Robinson et al., 2008" startWordPosition="3536" endWordPosition="3539"> 0.23 F11 Abbreviation 0.08 0.22 0.06 0.24 F12 HPOb 0.07 0.41 0.09 0.33 F13 MPc 0.03 0.33 0.06 0.33 F14 PATOd 0.01 0.03 0.02 0.04 F15 BTOe 0.03 0.32 0.03 0.29 F16 FMAf 0.05 0.28 0.05 0.23 F17 MAg 0.02 0.31 0.02 0.29 F18 PROh 0.02 0.12 0.03 0.15 F19 ChEBIi 0.01 0.15 0.03 0.20 F20 JOCHEMj 0.01 0.15 0.01 0.14 F21 NCBIk 0.01 0.14 0.01 0.14 F22 UMLSl disease 0.01 0.14 0.03 0.24 F23 NCBI gene 0.02 0.18 0.02 0.19 F24 GOm 0.13 0.38 0.05 0.28 F25 UMLSn 0.48 0.12 0.52 0.11 F26 45CLUSTERSo 0.50 0.10 0.47 0.10 Table 3: Features used in the experiments. aThe GENIA named entity tagger (Kim et al., 2003), b(Robinson et al., 2008), c(Smith et al., 2004), d(Gkoutos et al., 2005), e(Gremse et al., 2011), f(Rosse and Mejino, 2003), g(Hayamizu et al., 2005), h(Natale et al., 2011) , i(Degtyarenko et al., 2008), j(Hettne et al., 2009),k(Federhen, 2012),l(Lindberg et al., 1993),m(Gene Ontology Consortium, 2000),n133 categories from the UMLS,o45 cluster classes derived by Richard Socher and Christoph Manning PubMed available at http://nlp.stanford.edu/software/bionlp2011-distsim-clusters-v1.tar.gz precision = TP/(TP + FP) (5) and, recall = TP/(TP + FN) (6) A true positive (TP) is a gold standard NE tagged by the system as an </context>
</contexts>
<marker>Robinson, Kohler, Bauer, Seelow, Horn, Mundlos, 2008</marker>
<rawString>P. N. Robinson, S. Kohler, S. Bauer, D. Seelow, D. Horn, and S. Mundlos. 2008. The human phenotype ontology: a tool for annotating and analyzing human hereditary disease. The American Journal of Human Genetics, 83(5):610–615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Rosse</author>
<author>J L V Mejino</author>
</authors>
<title>A reference ontology for bioinformatics: the Foundational Model of Anatomy.</title>
<date>2003</date>
<journal>Journal of Biomedical Informatics,</journal>
<tech>PMID:</tech>
<volume>36</volume>
<issue>6</issue>
<pages>14759820</pages>
<contexts>
<context position="21452" citStr="Rosse and Mejino, 2003" startWordPosition="3552" endWordPosition="3555">3 F14 PATOd 0.01 0.03 0.02 0.04 F15 BTOe 0.03 0.32 0.03 0.29 F16 FMAf 0.05 0.28 0.05 0.23 F17 MAg 0.02 0.31 0.02 0.29 F18 PROh 0.02 0.12 0.03 0.15 F19 ChEBIi 0.01 0.15 0.03 0.20 F20 JOCHEMj 0.01 0.15 0.01 0.14 F21 NCBIk 0.01 0.14 0.01 0.14 F22 UMLSl disease 0.01 0.14 0.03 0.24 F23 NCBI gene 0.02 0.18 0.02 0.19 F24 GOm 0.13 0.38 0.05 0.28 F25 UMLSn 0.48 0.12 0.52 0.11 F26 45CLUSTERSo 0.50 0.10 0.47 0.10 Table 3: Features used in the experiments. aThe GENIA named entity tagger (Kim et al., 2003), b(Robinson et al., 2008), c(Smith et al., 2004), d(Gkoutos et al., 2005), e(Gremse et al., 2011), f(Rosse and Mejino, 2003), g(Hayamizu et al., 2005), h(Natale et al., 2011) , i(Degtyarenko et al., 2008), j(Hettne et al., 2009),k(Federhen, 2012),l(Lindberg et al., 1993),m(Gene Ontology Consortium, 2000),n133 categories from the UMLS,o45 cluster classes derived by Richard Socher and Christoph Manning PubMed available at http://nlp.stanford.edu/software/bionlp2011-distsim-clusters-v1.tar.gz precision = TP/(TP + FP) (5) and, recall = TP/(TP + FN) (6) A true positive (TP) is a gold standard NE tagged by the system as an NE. A true negative (TN) is a gold standard none-NE tagged by the system as a none-NE. A false posi</context>
</contexts>
<marker>Rosse, Mejino, 2003</marker>
<rawString>C. Rosse and J. L. V. Mejino. 2003. A reference ontology for bioinformatics: the Foundational Model of Anatomy. Journal of Biomedical Informatics, 36(6):478–500, December. PMID: 14759820.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>J Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with lr models and parser ensembles.</title>
<date>2007</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing Conference on Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<volume>volume</volume>
<pages>1044--1050</pages>
<contexts>
<context position="3083" citStr="Sagae and Tsujii, 2007" startWordPosition="456" endWordPosition="459">Once this assumption is made explicit it is of interest to both researchers and users that the implications and limitations of such experimental settings are explored. Cross domain studies have indicated an advantage for mechanisms that compensate for domain bias. For fully supervised learning, which is the scenario we explore here, recent methods include: feature augmentation (Daum´e III, 2007; Arnold et al., 2008; McClosky et al., 2010), instance weighting (Jiang and Zhai, 2007; Foster et al., 2010), schema harmonisation (Wang et al., 2010) and semi-supervised/lightly supervised approaches (Sagae and Tsujii, 2007; Liu et al., 2011; Pan et al., 2013). More generally there is a wide body of work in transfer learning (also known as domain adaptation) that tries to handle discrepancies between training and testing distributions (Pan and Yang, 2010). As an illustration of near domain bias consider the list of high frequency named entities in Table 1 drawn from two sub-domains in the research literature of hereditary diseases. A domain expert in hereditary diseases would have no difficulty in dividing them into two non-overlapping sets corresponding to the two near domains with one term t5 patients shared b</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>K. Sagae and J. Tsujii. 2007. Dependency parsing and domain adaptation with lr models and parser ensembles. In Conference on Empirical Methods in Natural Language Processing Conference on Computational Natural Language Learning (EMNLPCoNLL), volume 2007, pages 1044–1050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Settles</author>
</authors>
<title>Biomedical named entity recognition using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA) at COLING’2004,</booktitle>
<pages>104--107</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="14390" citStr="Settles, 2004" startWordPosition="2338" endWordPosition="2339">ulated the probability that a word token in an entity class from one corpus would appear in an instance of the same entity class in the other corpus, reported as columns a and b. Although the probability of an exact match in instances between entities in the two corpora is generally quite low (below 20% - data not shown) there appears to be significant vocabulary overlap in most classes except for chemicals. 3.2 Conditional Random Fields As in (Finkel and Manning, 2009) we apply our approach to a linear chain conditional random field (CRF) model (Lafferty et al., 2001; McCallum and Wei, 2003; Settles, 2004; Doan et al., 2012) using the Mallet toolkits with default parameters. CRFs have been shown consistently to be among the highest performing bioNER learners. The data selection strategies employed here though are neutral and could have been applied to any other fully supervised learner model. 3.3 Features We made use of a wide range of features, both conventional features such as word or part of speech, as well as gazetteers derived from external classification schemes that have been hand crafted by experts. These are shown in Table 3. Previous studies such as (Ratinov and Roth, 2009) have not</context>
</contexts>
<marker>Settles, 2004</marker>
<rawString>B. Settles. 2004. Biomedical named entity recognition using conditional random fields. In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA) at COLING’2004, Geneva, Switzerland, pages 104–107, August 28–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Smith</author>
<author>C W Goldsmith</author>
<author>J T Eppig</author>
</authors>
<title>The mammalian phenotype ontology as a tool for annotating, analyzing and comparing phenotypic information. Genome Biology,</title>
<date>2004</date>
<contexts>
<context position="21376" citStr="Smith et al., 2004" startWordPosition="3540" endWordPosition="3543">8 0.22 0.06 0.24 F12 HPOb 0.07 0.41 0.09 0.33 F13 MPc 0.03 0.33 0.06 0.33 F14 PATOd 0.01 0.03 0.02 0.04 F15 BTOe 0.03 0.32 0.03 0.29 F16 FMAf 0.05 0.28 0.05 0.23 F17 MAg 0.02 0.31 0.02 0.29 F18 PROh 0.02 0.12 0.03 0.15 F19 ChEBIi 0.01 0.15 0.03 0.20 F20 JOCHEMj 0.01 0.15 0.01 0.14 F21 NCBIk 0.01 0.14 0.01 0.14 F22 UMLSl disease 0.01 0.14 0.03 0.24 F23 NCBI gene 0.02 0.18 0.02 0.19 F24 GOm 0.13 0.38 0.05 0.28 F25 UMLSn 0.48 0.12 0.52 0.11 F26 45CLUSTERSo 0.50 0.10 0.47 0.10 Table 3: Features used in the experiments. aThe GENIA named entity tagger (Kim et al., 2003), b(Robinson et al., 2008), c(Smith et al., 2004), d(Gkoutos et al., 2005), e(Gremse et al., 2011), f(Rosse and Mejino, 2003), g(Hayamizu et al., 2005), h(Natale et al., 2011) , i(Degtyarenko et al., 2008), j(Hettne et al., 2009),k(Federhen, 2012),l(Lindberg et al., 1993),m(Gene Ontology Consortium, 2000),n133 categories from the UMLS,o45 cluster classes derived by Richard Socher and Christoph Manning PubMed available at http://nlp.stanford.edu/software/bionlp2011-distsim-clusters-v1.tar.gz precision = TP/(TP + FP) (5) and, recall = TP/(TP + FN) (6) A true positive (TP) is a gold standard NE tagged by the system as an NE. A true negative (TN</context>
</contexts>
<marker>Smith, Goldsmith, Eppig, 2004</marker>
<rawString>C. L. Smith, C. W. Goldsmith, and J. T. Eppig. 2004. The mammalian phenotype ontology as a tool for annotating, analyzing and comparing phenotypic information. Genome Biology, 6:R7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tanabe</author>
<author>N Xie</author>
<author>L H Thom</author>
<author>W Matten</author>
<author>W J Wilbur</author>
</authors>
<title>GENETAG: a tagged corpus for gene/protein named entity recognition.</title>
<date>2005</date>
<journal>BMC Bioinformatics,</journal>
<volume>6</volume>
<pages>1--3</pages>
<contexts>
<context position="2189" citStr="Tanabe et al., 2005" startWordPosition="321" endWordPosition="324">imental tasks in supervised machine learning for suggesting approaches that will generalise well on real world data. Research in biomedical named entity recognition (bioNER) often displays two features: (1) small samples of labeled data, and (2) an implicit assumption that the future data will be ∗collier@ebi.ac.uk drawn from a similar distribution to the labeled data and hence that minimising expected prediction error on held out data will minimise actual future loss. Since expert labeling is time consuming and expensive, labeled data sets tend to be relatively small, e.g. (Kim et al., 2003; Tanabe et al., 2005; Pyysalo et al., 2007), in the region of a few hundred or thousand Medline abstracts. Despite the danger of intrinsic idiosyncracies such corpora are often used to demonstrate putative prediction error across the heterogeneous collection of 22 million Medline abstracts. Once this assumption is made explicit it is of interest to both researchers and users that the implications and limitations of such experimental settings are explored. Cross domain studies have indicated an advantage for mechanisms that compensate for domain bias. For fully supervised learning, which is the scenario we explore</context>
</contexts>
<marker>Tanabe, Xie, Thom, Matten, Wilbur, 2005</marker>
<rawString>L. Tanabe, N. Xie, L. H. Thom, W. Matten, and W. J. Wilbur. 2005. GENETAG: a tagged corpus for gene/protein named entity recognition. BMC Bioinformatics, 6(Suppl 1):S3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tsai</author>
<author>S Wu</author>
<author>W Chou</author>
<author>Y Lin</author>
<author>D He</author>
<author>J Hsiang</author>
<author>T Sung</author>
<author>W Hsu</author>
</authors>
<title>Various criteria in the evaluation of biomedical named entity recognition.</title>
<date>2006</date>
<journal>BMC bioinformatics,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="7801" citStr="Tsai et al., 2006" startWordPosition="1215" endWordPosition="1218">pora. Various reasons are put forwards such as differences in entity boundary conventions, the scope of the entity class definitions, distributional properties of the entity classes and the degree of overlap between corpora. A follow up study by the authors (Wang et al., 2010) looked at increasing compatibility between the GENIA and GENETAG corpora by reorganising the annotation schema to unify protein, DNA and RNA NER under a new label GGP (Gene and Gene Product). However the best performance from the coarse grained annotations still do not improve on the intra-corpus data. In earlier work, (Tsai et al., 2006) looked at schema differences between the JNLPBA corpus of 2000 Medline abstracts (Kim et al., 2004) and the BioCreative corpus of 15,000 Medline sentences (Yeh et al., 2005) and tried to harmonise matching criteria. They demonstrated that relaxing the boundary matching criteria was helpful in maximising the cross-domain performance. 12 In the clinical domain (Wagholikar et al., 2013), explore the effect of harmonising annotation guidelines on the 2010 i2b2 challenge with Mayo Clinic Rochester (MCR) electronic patient records. They concluded that the effectiveness of pooling - i.e. merging of </context>
</contexts>
<marker>Tsai, Wu, Chou, Lin, He, Hsiang, Sung, Hsu, 2006</marker>
<rawString>R. Tsai, S. Wu, W. Chou, Y. Lin, D. He, J. Hsiang, T. Sung, and W. Hsu. 2006. Various criteria in the evaluation of biomedical named entity recognition. BMC bioinformatics, 7(1):92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wagholikar</author>
<author>M Torii</author>
<author>S Jonnalagadda</author>
<author>H Liu</author>
</authors>
<title>Pooling annotated corpora for clinical concept extraction.</title>
<date>2013</date>
<journal>J. Biomedical Semantics,</journal>
<volume>4</volume>
<contexts>
<context position="5762" citStr="Wagholikar et al., 2013" startWordPosition="882" endWordPosition="885">ch provides both general and domain-specific features; in effect a joint learning model. 3. Expectedly, but not trivially, we show that a general loss of f-score occurs on bioNER when transfering to near domains. This loss is not uniform across all classes. We provide class-by-class drill down analysis to the underlying causal factors which make some entities more robust to near domain transference in biomedicine than others. 4. Our results challenge the notion that pooling small corpora, even when guideline differences are reconciled, leads to improved f-score performance (Wang et al., 2010; Wagholikar et al., 2013). 5. In addition to the usual biomedical entity types we introduce the class of phenotypes which are valued as indicators of genetic malfunction and characteristic of diseases. The phenotype class incorporates a complex dependency between classes, notably anatomical entities and genes. This paper is organised as follows: Section 2 describes related work in cross domain transfer for biomedical NER, Section 3 discusses our approach including the two data sets used in our experiments, CRF model, feature choices and evaluation framework. In Section 4 we outline our experimental design. Finally in </context>
<context position="8188" citStr="Wagholikar et al., 2013" startWordPosition="1275" endWordPosition="1278"> schema to unify protein, DNA and RNA NER under a new label GGP (Gene and Gene Product). However the best performance from the coarse grained annotations still do not improve on the intra-corpus data. In earlier work, (Tsai et al., 2006) looked at schema differences between the JNLPBA corpus of 2000 Medline abstracts (Kim et al., 2004) and the BioCreative corpus of 15,000 Medline sentences (Yeh et al., 2005) and tried to harmonise matching criteria. They demonstrated that relaxing the boundary matching criteria was helpful in maximising the cross-domain performance. 12 In the clinical domain (Wagholikar et al., 2013), explore the effect of harmonising annotation guidelines on the 2010 i2b2 challenge with Mayo Clinic Rochester (MCR) electronic patient records. They concluded that the effectiveness of pooling - i.e. merging of corpora by ensuring a common format and harmonised semantics - is dependent on several factors including compatibility between the annotation schema and differences in size. Again they noticed that simple pooling resulted in a loss of f-score, 12% for MCR and 4% for i2b2. They concluded that the asymmetry was likely due to size effects of the corpora, i.e. MCR being smaller suffered a</context>
<context position="28174" citStr="Wagholikar et al., 2013" startWordPosition="4752" endWordPosition="4755"> 72.7 41.5 72.8 68.3 M6 C1 57.1 27.7 79.0 73.4 44.9 69.9 69.5 M1 C2 37.2 31.3 72.9 57.2 46.5 73.2 58.5 M2 C2 21.2 20.2 57.0 52.3 24.4 68.5 46.6 M3 C2 36.8 38.7 72.3 61.1 44.0 77.4 59.7 M4 C2 34.8 34.4 72.5 57.5 45.9 74.7 58.5 M5 C2 34.1 41.6 73.6 58.9 43.2 78.5 59.6 M6 C2 39.9 35.0 73.3 56.4 46.6 75.0 59.1 Table 4: Named entity recognition f-scores using Methods 1 to 6. All methods were tested using 100 iterations of Monte Carlo 10-fold cross validation. Figures in bold show best in class scores. Figures in italics show scores above the M1 baseline. bounds observed by (Wang et al., 2009) and (Wagholikar et al., 2013) for their pooling of heterogeneous Medline corpora. Except for the ORG class which we higlighted above, we might cautiously quantify the loss of pooled entity mentions as being in the range up to 9.5% for CHE but more typically below 4%. The majority of the differences they observed - which are not present in our data - are most likely due to concept definition differences and annotation conventions. In contrast to our expectations the M4 experiments showed very mild benefits for stacking and these were mixed across entity types. M4 tests on C2 showed no general improvement but some improveme</context>
</contexts>
<marker>Wagholikar, Torii, Jonnalagadda, Liu, 2013</marker>
<rawString>K. Wagholikar, M. Torii, S. Jonnalagadda, H. Liu, et al. 2013. Pooling annotated corpora for clinical concept extraction. J. Biomedical Semantics, 4:3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>J Kim</author>
<author>R Sætre</author>
<author>S Pyysalo</author>
<author>J Tsujii</author>
</authors>
<title>Investigating heterogeneous protein annotations toward cross-corpora utilization.</title>
<date>2009</date>
<journal>BMC bioinformatics,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="6807" citStr="Wang et al., 2009" startWordPosition="1056" endWordPosition="1059">proach including the two data sets used in our experiments, CRF model, feature choices and evaluation framework. In Section 4 we outline our experimental design. Finally in Section 5 we compare the performance of six data selection strategies that try to maximise f-score performance on domain entity classes in the target corpus. 2 Related work It is surprising that there exists, to the best of our knowledge, no controlled study that has shed light on the issue of near domain transfer for bioNER in a straightforward manner. The closest approach to our investigation in the biomedical domain is (Wang et al., 2009). Wang et al. explore potential sources of incompatibility across major bioNER corpora with different annotation schema (GENIA - 2000 Medline abstracts, GENETAG - approximately 20,000 Medline sentences and AIMed - 225 Medline abstracts). They focus exclusively on protein name recognition and observe a drop in performance of 12% f-score when combining data from different corpora. Various reasons are put forwards such as differences in entity boundary conventions, the scope of the entity class definitions, distributional properties of the entity classes and the degree of overlap between corpora.</context>
<context position="28144" citStr="Wang et al., 2009" startWordPosition="4747" endWordPosition="4750">9.8 M5 C1 56.7 29.6 77.3 72.7 41.5 72.8 68.3 M6 C1 57.1 27.7 79.0 73.4 44.9 69.9 69.5 M1 C2 37.2 31.3 72.9 57.2 46.5 73.2 58.5 M2 C2 21.2 20.2 57.0 52.3 24.4 68.5 46.6 M3 C2 36.8 38.7 72.3 61.1 44.0 77.4 59.7 M4 C2 34.8 34.4 72.5 57.5 45.9 74.7 58.5 M5 C2 34.1 41.6 73.6 58.9 43.2 78.5 59.6 M6 C2 39.9 35.0 73.3 56.4 46.6 75.0 59.1 Table 4: Named entity recognition f-scores using Methods 1 to 6. All methods were tested using 100 iterations of Monte Carlo 10-fold cross validation. Figures in bold show best in class scores. Figures in italics show scores above the M1 baseline. bounds observed by (Wang et al., 2009) and (Wagholikar et al., 2013) for their pooling of heterogeneous Medline corpora. Except for the ORG class which we higlighted above, we might cautiously quantify the loss of pooled entity mentions as being in the range up to 9.5% for CHE but more typically below 4%. The majority of the differences they observed - which are not present in our data - are most likely due to concept definition differences and annotation conventions. In contrast to our expectations the M4 experiments showed very mild benefits for stacking and these were mixed across entity types. M4 tests on C2 showed no general </context>
</contexts>
<marker>Wang, Kim, Sætre, Pyysalo, Tsujii, 2009</marker>
<rawString>Y. Wang, J. Kim, R. Sætre, S. Pyysalo, and J. Tsujii. 2009. Investigating heterogeneous protein annotations toward cross-corpora utilization. BMC bioinformatics, 10(1):403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>J Kim</author>
<author>R Sætre</author>
<author>S Pyysalo</author>
<author>T Ohta</author>
<author>J Tsujii</author>
</authors>
<title>Improving the inter-corpora compatibility for protein annotations.</title>
<date>2010</date>
<journal>Journal of bioinformatics and computational biology,</journal>
<volume>8</volume>
<issue>05</issue>
<pages>916</pages>
<contexts>
<context position="3009" citStr="Wang et al., 2010" startWordPosition="447" endWordPosition="450"> across the heterogeneous collection of 22 million Medline abstracts. Once this assumption is made explicit it is of interest to both researchers and users that the implications and limitations of such experimental settings are explored. Cross domain studies have indicated an advantage for mechanisms that compensate for domain bias. For fully supervised learning, which is the scenario we explore here, recent methods include: feature augmentation (Daum´e III, 2007; Arnold et al., 2008; McClosky et al., 2010), instance weighting (Jiang and Zhai, 2007; Foster et al., 2010), schema harmonisation (Wang et al., 2010) and semi-supervised/lightly supervised approaches (Sagae and Tsujii, 2007; Liu et al., 2011; Pan et al., 2013). More generally there is a wide body of work in transfer learning (also known as domain adaptation) that tries to handle discrepancies between training and testing distributions (Pan and Yang, 2010). As an illustration of near domain bias consider the list of high frequency named entities in Table 1 drawn from two sub-domains in the research literature of hereditary diseases. A domain expert in hereditary diseases would have no difficulty in dividing them into two non-overlapping set</context>
<context position="5736" citStr="Wang et al., 2010" startWordPosition="878" endWordPosition="881">simple strategy which provides both general and domain-specific features; in effect a joint learning model. 3. Expectedly, but not trivially, we show that a general loss of f-score occurs on bioNER when transfering to near domains. This loss is not uniform across all classes. We provide class-by-class drill down analysis to the underlying causal factors which make some entities more robust to near domain transference in biomedicine than others. 4. Our results challenge the notion that pooling small corpora, even when guideline differences are reconciled, leads to improved f-score performance (Wang et al., 2010; Wagholikar et al., 2013). 5. In addition to the usual biomedical entity types we introduce the class of phenotypes which are valued as indicators of genetic malfunction and characteristic of diseases. The phenotype class incorporates a complex dependency between classes, notably anatomical entities and genes. This paper is organised as follows: Section 2 describes related work in cross domain transfer for biomedical NER, Section 3 discusses our approach including the two data sets used in our experiments, CRF model, feature choices and evaluation framework. In Section 4 we outline our experi</context>
<context position="7460" citStr="Wang et al., 2010" startWordPosition="1156" endWordPosition="1159">rces of incompatibility across major bioNER corpora with different annotation schema (GENIA - 2000 Medline abstracts, GENETAG - approximately 20,000 Medline sentences and AIMed - 225 Medline abstracts). They focus exclusively on protein name recognition and observe a drop in performance of 12% f-score when combining data from different corpora. Various reasons are put forwards such as differences in entity boundary conventions, the scope of the entity class definitions, distributional properties of the entity classes and the degree of overlap between corpora. A follow up study by the authors (Wang et al., 2010) looked at increasing compatibility between the GENIA and GENETAG corpora by reorganising the annotation schema to unify protein, DNA and RNA NER under a new label GGP (Gene and Gene Product). However the best performance from the coarse grained annotations still do not improve on the intra-corpus data. In earlier work, (Tsai et al., 2006) looked at schema differences between the JNLPBA corpus of 2000 Medline abstracts (Kim et al., 2004) and the BioCreative corpus of 15,000 Medline sentences (Yeh et al., 2005) and tried to harmonise matching criteria. They demonstrated that relaxing the bounda</context>
</contexts>
<marker>Wang, Kim, Sætre, Pyysalo, Ohta, Tsujii, 2010</marker>
<rawString>Y. Wang, J. Kim, R. Sætre, S Pyysalo, T. Ohta, and J. Tsujii. 2010. Improving the inter-corpora compatibility for protein annotations. Journal of bioinformatics and computational biology, 8(05):901– 916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yeh</author>
<author>A Morgan</author>
<author>M Colosimo</author>
<author>L Hirschman</author>
</authors>
<title>Biocreative task 1a: gene mention finding evaluation. BMC bioinformatics, 6(Suppl 1):S2.</title>
<date>2005</date>
<contexts>
<context position="7975" citStr="Yeh et al., 2005" startWordPosition="1244" endWordPosition="1247">lasses and the degree of overlap between corpora. A follow up study by the authors (Wang et al., 2010) looked at increasing compatibility between the GENIA and GENETAG corpora by reorganising the annotation schema to unify protein, DNA and RNA NER under a new label GGP (Gene and Gene Product). However the best performance from the coarse grained annotations still do not improve on the intra-corpus data. In earlier work, (Tsai et al., 2006) looked at schema differences between the JNLPBA corpus of 2000 Medline abstracts (Kim et al., 2004) and the BioCreative corpus of 15,000 Medline sentences (Yeh et al., 2005) and tried to harmonise matching criteria. They demonstrated that relaxing the boundary matching criteria was helpful in maximising the cross-domain performance. 12 In the clinical domain (Wagholikar et al., 2013), explore the effect of harmonising annotation guidelines on the 2010 i2b2 challenge with Mayo Clinic Rochester (MCR) electronic patient records. They concluded that the effectiveness of pooling - i.e. merging of corpora by ensuring a common format and harmonised semantics - is dependent on several factors including compatibility between the annotation schema and differences in size. </context>
</contexts>
<marker>Yeh, Morgan, Colosimo, Hirschman, 2005</marker>
<rawString>A. Yeh, A. Morgan, M. Colosimo, and L. Hirschman. 2005. Biocreative task 1a: gene mention finding evaluation. BMC bioinformatics, 6(Suppl 1):S2.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
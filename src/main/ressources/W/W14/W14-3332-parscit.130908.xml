<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003649">
<title confidence="0.994429">
Experiments in Medical Translation Shared Task at WMT 2014
</title>
<author confidence="0.984679">
Jian Zhang, Xiaofeng Wu,
Iacer Calixto, Ali Hosseinzadeh Vahid, Xiaojun Zhang,
Andy Way, Qun Liu
</author>
<affiliation confidence="0.988398666666667">
The CNGL Centre for Global Intelligent Content
School of Computing
Dublin City University, Ireland
</affiliation>
<email confidence="0.613692333333333">
{zhangj,xiaofengwu,
icalixto,avahid,xzhang,
away,qliu}@computing.dcu.ie
</email>
<sectionHeader confidence="0.998207" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999342">
This paper describes Dublin City Uni-
versity’s (DCU) submission to the WMT
2014 Medical Summary task. We re-
port our results on the test data set in
the French to English translation direction.
We also report statistics collected from the
corpora used to train our translation sys-
tem. We conducted our experiment on the
Moses 1.0 phrase-based translation system
framework. We performed a variety of ex-
periments on translation models, reorder-
ing models, operation sequence model and
language model. We also experimented
with data selection and removal the length
constraint for phrase-pair extraction.
</bodyText>
<sectionHeader confidence="0.993442" genericHeader="keywords">
1 System Description
</sectionHeader>
<subsectionHeader confidence="0.999625">
1.1 Training Data Statistics and Preparation
</subsectionHeader>
<bodyText confidence="0.998974055555556">
The training corpora provided to the medical
translation shared task can be divided into 3 cat-
egories:
Medical in-domain corpora: these corpora
contain documents, patents, articles, terminology
lists, and titles that are representative of the same
medical domain as the development and test data
sets (Table 1, second column).
Medical out-of-domain corpora: these cor-
pora also contain medical documents, patents, ar-
ticles, terminologies lists and titles, but describe
a different domain from the development and test
data sets (Table 1, third column).
General domain corpora: these corpora con-
sist of general-domain text (WMT 2014 general
translation subtask corpora), and encompass vari-
ous domains. (We did not use these corpora in our
system).
</bodyText>
<table confidence="0.992404090909091">
Corpus In-domain Out-of-domain
parallel sentence parallel sentence
number number
EMEA 1,092,568 0
COPPA 664,658 2,841,849
PatTR-title 408,502 2,096,270
PatTR-abstract 688,147 3,009,523
PatTR-claims 1,105,230 5,861,621
UMLS 85,705 0
Wikipedia 8,448 0
TOTAL 4,053,258 13,809,263
</table>
<tableCaption confidence="0.9694505">
Table 1: WMT 2014 Medical Translation shared
task parallel training data before preprocessing.
</tableCaption>
<bodyText confidence="0.997965882352941">
Within all the provided training corpora from
WMT 2014, 70.72% of the medical in domain
bilingual sentences, and 100% of the medical
out-of-domain bilingual sentences were obtained
from patent document collections. Motivated by
these percentages, we view the WMT 2014 med-
ical translation shared task as similar to training
a patent-specific translation system. The mono-
lingual corpora are taken from 9 different cor-
pora collections, and there is no clear demarca-
tion of the in/out-of-domain boundaries (except
the PatTR collection). Our method of differenti-
ating between the in/out-of-domain monolingual
corpora is that only English sentences from the
third column of Table 1, and the patent descrip-
tion documents from PatTR collection, are out-of-
domain monolingual corpora. All other English
</bodyText>
<page confidence="0.921854">
260
</page>
<bodyText confidence="0.908086214285714">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 260–265,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
sentences are treated as an in-domain monolingual
resource.
A patent document usually comprises title, ab-
stract, claims and description fields. The docu-
ments often use its unique formatting and con-
tain linguistic idiosyncrasies, which distinguish
patent-specific translation systems from general
translation systems, in both training and transla-
tion phases (Ceaus¸u et al., 2011). We have also
found that some common writing styles are con-
stantly used, especially for long sentences. For
example, a typical patent claim begins with
</bodyText>
<subsectionHeader confidence="0.849755">
Method of [X], which comprising:
</subsectionHeader>
<bodyText confidence="0.999988890909091">
followed by a numbered list. The abstract
field normally contains one paragraph only, but
with multiple sentences. Those long sentences
are necessarily filtered out to facilitate efficient
word alignment, using a tool such as GIZA++
(Och, 2003) word aligner with the default param-
eter settings. However, because statistical ma-
chine translation depends on the training data to
estimate translation probability, more high qual-
ity training data often leads a better translation re-
sult. One possible method of including long sen-
tences into the training cycle is to change the word
aligner’s parameter settings to handle longer sen-
tences; however, aligning long sentences is time
consuming. Our solution is to capture the styled
long sentences and attempt to split them on both
source and target side simultaneously according
to the numbered list or sentence boundary indica-
tions. If the sentence number after splitting are
matching in both source and target sides, and each
sentence pair is within the token length ratio of
3, we assume the split attempt is successful, oth-
erwise the sentences are kept unchanged and will
be filtered out eventually. We applied our splitting
attempt approach on the patent documents at the
data preparation step which consequently results
in 19.35% and 7.1% increase in the number of
sentence pairs compared with the original medical
in-domain (from 4053258 to 4837382) and over-
all medical (from 17862521 to 19124142) datasets
respectively.
Another finding from the training corpora is that
the titles of the patent documents are often capital-
ized in the training corpora. Since we are training
a true-cased translation system, and the transla-
tion inputs contain non-title sentences, capitalized
training sentences will contribute biased weights
to our true-case model. We addressed this issue by
creating a lowercase version of the title corpora,
then we trained our true-case model with the low-
ercased titles corpora and other non-title corpora.
We also included the lowercased title corpora in
the translation system training.
We tokenized the training corpora using the
tokenizer script distributed in the Moses 1.0
framework with additional patent document non-
breaking preferences observed during data prepa-
ration, such as Figs and FIGS etc., and a modified
aggressive setting (split hyphen character in all
cases). Other data preparation steps included char-
acter normalization, character/token based foreign
language detection, HTML/XML tag removal,
case insensitive duplication removal, longer sen-
tence removal (2-80, length ratio 9), resulting in
the preprocessed data shown in Table 2.
</bodyText>
<table confidence="0.996303727272727">
Corpus In-domain Out-of-domain
parallel sentence parallel sentence
number number
EMEA 273,532 0
COPPA 1,374,371 6,075,599
PatTR-title 63,856 3,457,164
PatTR-abstract 599,435 2,595,515
PatTR-claims 876,603 4,244,324
UMLS 85,683 0
Wikipedia 8,438 0
TOTAL 3,956,478 16,372,602
</table>
<tableCaption confidence="0.853805">
Table 2: WMT 2014 Medical Translation shared
task parallel training data after preprocessing
steps.
</tableCaption>
<subsectionHeader confidence="0.997173">
1.2 Training Data Selection
</subsectionHeader>
<bodyText confidence="0.999959705882353">
It is an open secret that high quality and large
quantity of the parallel corpus are the two most
important factors for a high-quality SMT system.
These factors assist the word aligner in producing
a precise alignment model, which in turn brings
benefits to the other SMT training steps.
The quantity factor also helps the SMT system
to cover more translation input variations. In order
to efficiently use the training corpora listed in Ta-
ble 2, we explored some data selection methodolo-
gies. We used the feature decay algorithm (Bicici
et al., 2014) to select the training instances trans-
ductively, using the source side of the test set. We
built systems with the pre-defined selection pro-
portions in token number, 1/64, 1/32, 1/16, 1/8,
1/2, 3/4 and 1 of all the in-domain medical train-
ing data, then searched for the best performing
</bodyText>
<page confidence="0.983642">
261
</page>
<bodyText confidence="0.999615272727273">
system using the test data set as our baseline (Ta-
ble 3). For the purpose of making the potential
baseline systems comparable, instance selection
was employed after word alignment using word
aligner MGIZA++ (Gao and Vogel, 2008) on all
the available data. The transductive learning uses
features extracted from the source data of the de-
velopment set with the default feature decay algo-
rithm weight settings. All of systems were trained
using the default phrase-based training parameter
settings of Moses 1.0 framework, with additional
msd-bidirectional-fe reordering model (Koehn et
al., 2005). We extract phrase pairs based on grow-
diag-final-and (Koehn et al., 2003) heuristics.
The language model was created with open source
IRSTLM toolkit (Federico et al., 2008) using all
the English in-domain data (monolingual and par-
allel). We used 5-gram with modied Kneser-Ney
smoothing (Kneser and Ney, 1995). The tuning
step used minimum error rate training (MERT)
(Och, 2003). The performance was measured by
the test data set in case insensitive BLEU score.
</bodyText>
<table confidence="0.9832028">
Proportions Test set
case insensitive BLEU
1/64 0.4374
1/32 0.4409
1/16 0.4370
1/8 0.4419
1/4 0.4390
1/2 0.4399
3/4 0.4397
1 0.4260
</table>
<tableCaption confidence="0.997941">
Table 3: Feature decay algorithm transductive
</tableCaption>
<bodyText confidence="0.963069111111111">
learning selection on all in-domain data using ex-
tracted features from the source side of the test
data set. We choose system uses 1/8 proportions
of the in-domain data as our baseline system.
Our results show that the system trained with
1/8 proportion of the in-domain medical training
data (398,098 sentence pairs) selected by FDA
outperformed the others. We chose this system as
our baseline system.
</bodyText>
<sectionHeader confidence="0.999878" genericHeader="introduction">
2 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996029">
2.1 Maximum Phrase Length
</subsectionHeader>
<bodyText confidence="0.998227818181818">
While extracting phrase pairs, collecting longer
phrases is not guaranteed to produce a better qual-
ity phrase table than the shorter settings, even
setting the maximum phrase length to three can
achieve top performance (Koehn et al., 2003).
We take this WMT 2014 opportunity to study the
capability of long phrase lengths ( &gt;=10 ). We
trained translation models with phrase length set-
ting from 10 to 15, employed them to our base-
line system and compared the performance with
the default setting (length = 7).
</bodyText>
<table confidence="0.9997857">
Phrase Length Phrase Table Test set
Entries case insensitive
BLEU
7 (Baseline) 19.31 0.4419
10 29.67 0.4400
11 32.87 0.4416
12 35.95 0.4444*
13 38.91 0.4448*
14 41.75 0.4444*
15 44.47 0.4362
</table>
<tableCaption confidence="0.989802">
Table 4: -max-phrase-length setting experiment,
</tableCaption>
<bodyText confidence="0.999568666666667">
where phrase table entries is in millions. * indi-
actes statistically significant improvement at the p
= 0.05 level.1
As stated in (Koehn et al., 2003) and expected,
the size of the phrase table is linear with respect to
the maximum phrase length restriction. Surpris-
ingly, we also found the performance can still im-
prove after the default length setting, until a peak
point (Table 4).
It is also interesting to see the effect for each
sentence in the test set when the default phrase
length setting in Moses framework is changed. We
first evaluated the sentence level BLEU scores for
the systems listed in Table 4, then compared them
with our baseline system sentence level BLEU
scores and categorised the compared results into
increased, decreased or unaffected groups (Fig-
ure 1). We found that system with -max-phrase-
length set to 12 is influenced the least (158, 118
and 724 sentences have BLEU score increased,
decreased and unaffected respectively) and with
-max-phrase-length sets to 10 is influenced the
most (261, 257 and 482 sentences have BLEU
score increased, decreased and unaffected respec-
tively).
We then looked into the decoding phase and
tried to discover the actual phrase length that was
used to generate the translation outputs. We ex-
posed the translation segmentations by trigger-
ing the -report-segmentation decoding parameter
</bodyText>
<footnote confidence="0.9903665">
1The same notation is used for the rest of the tables in this
paper
</footnote>
<page confidence="0.993631">
262
</page>
<figureCaption confidence="0.799062333333333">
-max-phrase-length from 10 to 15
Figure 1: Sentence level BLEU score affects when
enlarge -max-phrase-length
</figureCaption>
<figure confidence="0.882337">
pl=1 pl=2 pl=3 pl=4 pl=5 pl=6 pl=7 pl=8
baseline and -max-phrase-length from 10 to 15
</figure>
<figureCaption confidence="0.9628795">
Figure 2: Phrase length (pl) distribution used in
decoding
</figureCaption>
<bodyText confidence="0.9999502">
in the Moses framework and computed the per-
centage of different phrases used according to the
phrase token number (Figure 2). The transla-
tion is mostly generated from short source phrases
(length&lt;4) in all the systems during decoding,
which we think is the reason that setting phrase ex-
traction to length 3 can achieve top performance.
We did not carry out more experiments in this
case, as we think there is no absolute maximum
phrase length setting which can fit into all experi-
ments and such experiments depend on many fac-
tors, such as the similarity between the training
corpus and then testing data. The choice to set
-max-phrase-length to 13 is purely directed by the
BLEU score shown in Table 4.
</bodyText>
<subsectionHeader confidence="0.997244">
2.2 Reordering Models
</subsectionHeader>
<bodyText confidence="0.999853321428572">
Ceaus¸u et al. (2011) also found that long-range re-
ordering is one of the characteristics of patent doc-
uments; however, long-range reordering increases
the difficulty of SMT training and decoding. We
experimented two approaches to address this chal-
lenge. Apart from the msd-bidirectional-fe lexi-
cal reordering model (Koehn et al., 2005) in our
baseline system, the phrase-based orientation and
hierarchical orientation reordering models (Gal-
ley and Manning, 2008) can capture long distance
dependencies. The phrase-based orientation re-
ordering model is similar to the lexical reordering
approach, the only difference between these two
models is the phrase-based reordering model per-
forms reordering only on the phrase level, but the
hierarchical reordering model does not have such
constraint - it does not require phrases to be ad-
jacent. OSM (Durrani, 2011) (Durrani, 2013b)
is a sequence model integrating the N-gram-based
translation model and reordering model. It de-
fines three operations for reordering and consid-
ers all reordering possibilities within a fixed win-
dow while searching. We experimented with both
reordering models, and found that the system de-
fined with three reordering models performs bet-
ter (Table 5) than OSM. We then tried to use both
OSM and the reordering models together, which
produced the best system at this point.
</bodyText>
<table confidence="0.995245833333333">
Systems Test set
case insensitive BLEU
Baseline + 13 0.4448
+ OSM 0.4472
+ pho-ho 0.4551*
+ pho-ho + OSM 0.4561*
</table>
<tableCaption confidence="0.999654">
Table 5: Reordering Model or/and OSM results
</tableCaption>
<subsectionHeader confidence="0.997146">
2.3 Two Translation Models
</subsectionHeader>
<bodyText confidence="0.957102875">
The back-off model aims to produce translations
for the unknown words or unknown phrases in the
primary translation table by yielding the phrase ta-
ble translation probability from primary transla-
tion table to the back-off table, as in (Koehn et
al., 2012a)
pBO(e|f) = { p1(e|f) if p2(e|f) otherwise t1(f) &gt; 0
Moreover, we look at using the back off model
</bodyText>
<figure confidence="0.998867066666667">
10 11 12 13 14 15
750
700
650
600
550
500
450
400
350
300
250
200
increased decreased unaffected
7 10 11 12 13 14 15
Source phrase used in decoding (%)
5 · 10−2
5 · 10−4
0.55
0.45
0.35
0.25
0.15
0.6
0.5
0.4
0.3
0.2
0.1
test set (1000 sentences)
</figure>
<page confidence="0.998281">
263
</page>
<bodyText confidence="0.999983230769231">
as a domain adaptation approach, which is to con-
strain the translation options within the target do-
main unless no options can be found, in which
case the translation will be selected from the back-
off model.
Phrase table fill-up (Bisazza et al., 2011) is a
very similar approach with back-off models, it col-
lects and uses the phrase pairs from the out-of-
domain phrase table only when the input is un-
available at the in-domain phrase table. It merges
the in-domain and out-of-domain translation mod-
els into one, where the scores are taken from more
reliable source. To distinguish the source of a
phrase pair entry, fill-up assigns a binary value as
an additional feature at the merged phrase table.
We trained our out-of-domain translation model
separately using all of the out-of-domain medi-
cal data listed at Table 2 with the same parame-
ter settings as our baseline system, then employed
Moses’s back-off model feature to pass the pri-
mary and back-off translation models to the de-
coder at tuning and translation time. The fill-up
tool was sourced from (Bisazza et al., 2011) at
Moses’s distribution. Our experiment results (Ta-
ble 6) show that the fill-up approach performed
better than the back-off model approach.
</bodyText>
<table confidence="0.9936938">
Systems Test set
case insensitive BLEU
Baseline + 13 + pho-ho + OSM 0.4561
Back-off 0.4573
Fill-up 0.4599*
</table>
<tableCaption confidence="0.992716">
Table 6: Back-off and fill-up experiment results
</tableCaption>
<subsectionHeader confidence="0.905301">
2.4 Language Model
</subsectionHeader>
<bodyText confidence="0.9997942">
Until now, we have reported our results using a
language model trained with all in-domain medi-
cal data only. We also took the similar approach
to (Koehn et al., 2007) and carried out language
model experiments. We trained our out-of-domain
language model with all the out-of-domain En-
glish sentences mentioned in section 1.1, then in-
terpolated the in-domain and out-of-domain lan-
guage model by optimizing the perplexity to the
development data set. We received a similar pic-
ture to (Koehn et al., 2007), where the language
model trained with only in-domain data performed
the best (Table 7).
Our final submission for WMT 2014 Medical
Translation shared task is the * system at Table 7.
</bodyText>
<table confidence="0.997068833333333">
Systems Test set
case insensitive BLEU
Baseline + 13 + pho-ho
+ OSM + Fill-up* 0.4599
out-of-domain LM 0.4461
interpolated LM 0.4592
</table>
<tableCaption confidence="0.999413">
Table 7: Language model experiment results
</tableCaption>
<sectionHeader confidence="0.998532" genericHeader="conclusions">
3 Conclusion
</sectionHeader>
<bodyText confidence="0.999930583333333">
In this paper, we report our results on the WMT
2014 in the French to English translation direc-
tion. We shared our statistics for the bilingual
corpora used to train our translation system. All
systems were trained using the open source Moses
1.0 translation framework. Based on the feature
set of Moses phrased-based translation system, we
carried out our experiments on translation models,
reordering models, operation sequence model and
language model. We also experimented on data
selection and releasing the length restriction while
extracting phrase pairs.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="acknowledgments">
4 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999642166666667">
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
the Centre for Next Generation Localisation
(www.cngl.ie) at Dublin City University. We
would also like to acknowledge Ergun Bicici who
gives suggestions at the data selection approach.
</bodyText>
<sectionHeader confidence="0.998963" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99599535">
Alexandru Ceaus¸u, John Tinsley, Jian Zhang and Andy
Way. 2011. Experiments on domain adaptation for
patent machine translation in the PLuTO project,
The 15th conference of the European Association
for Machine Translation, Leuven, Belgium.
Arianna Bisazza, Nick Ruiz, and Marcello Fed-
erico. 2011. Fill-up versus Interpolation Meth-
ods for Phrase-based SMT Adaptation., In Interna-
tional Workshop on Spoken Language Translation
(IWSLT), San Francisco, CA.
Durrani, N., Schmid, H., and Fraser, A. 2011. A
Joint Sequence Translation Model with Integrated
Reordering., The 49th Annual Meeting of the As-
sociation for Computational Linguistics, Portland,
Oregon, USA.
Durrani, N., Fraser, A., Schmid, H., Hoang, H., and
Koehn, P. 2013b. Can Markov Models Over Min-
imal Translation Units Help Phrase-Based SMT,
The 51th Annual Meeting of the Association for
Computational Linguistics, Sofia, Bulgaria.
</reference>
<page confidence="0.976648">
264
</page>
<reference confidence="0.999728320754717">
Ergun Bicici and Deniz Yuret. 2014. Optimizing In-
stance Selection for Statistical Machine Translation
with Feature Decay Algorithms, IEEE/ACM Trans-
actions On Audio, Speech, and Language Process-
ing (TASLP).
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models,
Computational Linguistics, 29(1):1951.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation, The 41th Annual
Meeting of the Association for Computational Lin-
guistics, Sapporo, Japan.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models, Interspeech,
Brisbane, Australia.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. , The 2008 Conference on Empirical Meth-
ods in Natural Language Processing,pages 848856,
Honolulu, Hawaii, October. Association for Compu-
tational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool, In Software Engi-
neering, Testing, and Quality Assurance for Natu-
ral Language Processing, SETQA-NLP 2008, pages
49-57, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation,
International Workshop on Spoken Language Trans-
lation.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in Domain Adaptation for Statistical Machine
Translation, The Second Workshop on Statistical
Machine Translation, pages 224227, Prague.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2003. Statistical phrase-based trans-
lation, 2003 Conference of the North American
Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages
4854, Edmonton, Canada.
Philipp Koehn, and Barry Haddow. 2012. Interpolated
backoff for factored translation models., The 10th
Conference of the Association for Machine Transla-
tion in the Americas (AMTA).
Reinhard Kneser and Hermann Ney 1995. Improved
backing-offfor m-gram language modeling., IEEE
International Conference on Acoustics, Speech and
Signal Processing, pages 181184.
</reference>
<page confidence="0.998454">
265
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.454288">Experiments in Medical Translation Shared Task at WMT 2014</title>
<author confidence="0.722392666666667">Jian Zhang</author>
<author confidence="0.722392666666667">Xiaofeng Wu</author>
<author confidence="0.722392666666667">Iacer Calixto</author>
<author confidence="0.722392666666667">Ali Hosseinzadeh Vahid</author>
<author confidence="0.722392666666667">Xiaojun Andy Way</author>
<author confidence="0.722392666666667">Qun Liu</author>
<affiliation confidence="0.991309">The CNGL Centre for Global Intelligent School of Dublin City University,</affiliation>
<abstract confidence="0.940535026315789">This paper describes Dublin City University’s (DCU) submission to the WMT 2014 Medical Summary task. We report our results on the test data set in the French to English translation direction. We also report statistics collected from the corpora used to train our translation system. We conducted our experiment on the Moses 1.0 phrase-based translation system framework. We performed a variety of experiments on translation models, reordering models, operation sequence model and language model. We also experimented with data selection and removal the length constraint for phrase-pair extraction. 1 System Description 1.1 Training Data Statistics and Preparation The training corpora provided to the medical translation shared task can be divided into 3 categories: in-domain corpora: corpora contain documents, patents, articles, terminology lists, and titles that are representative of the same medical domain as the development and test data sets (Table 1, second column). out-of-domain corpora: corpora also contain medical documents, patents, articles, terminologies lists and titles, but describe a different domain from the development and test data sets (Table 1, third column). domain corpora: corpora consist of general-domain text (WMT 2014 general translation subtask corpora), and encompass various domains. (We did not use these corpora in our system). Corpus In-domain parallel sentence number Out-of-domain parallel sentence number EMEA 1,092,568 0 COPPA 664,658 2,841,849 PatTR-title 408,502 2,096,270 PatTR-abstract 688,147 3,009,523 PatTR-claims 1,105,230 5,861,621 UMLS 85,705 0 Wikipedia 8,448 0 TOTAL 4,053,258 13,809,263 Table 1: WMT 2014 Medical Translation shared task parallel training data before preprocessing. Within all the provided training corpora from WMT 2014, 70.72% of the medical in domain bilingual sentences, and 100% of the medical out-of-domain bilingual sentences were obtained from patent document collections. Motivated by these percentages, we view the WMT 2014 medical translation shared task as similar to training a patent-specific translation system. The monolingual corpora are taken from 9 different corpora collections, and there is no clear demarcation of the in/out-of-domain boundaries (except the PatTR collection). Our method of differentiating between the in/out-of-domain monolingual corpora is that only English sentences from the third column of Table 1, and the patent description documents from PatTR collection, are out-ofdomain monolingual corpora. All other English 260 of the Ninth Workshop on Statistical Machine pages Maryland USA, June 26–27, 2014. Association for Computational Linguistics sentences are treated as an in-domain monolingual resource. A patent document usually comprises title, abstract, claims and description fields. The documents often use its unique formatting and contain linguistic idiosyncrasies, which distinguish patent-specific translation systems from general translation systems, in both training and translaphases et al., 2011). We have also found that some common writing styles are constantly used, especially for long sentences. For example, a typical patent claim begins with Method of [X], which comprising: followed by a numbered list. The abstract field normally contains one paragraph only, but with multiple sentences. Those long sentences are necessarily filtered out to facilitate efficient word alignment, using a tool such as GIZA++ (Och, 2003) word aligner with the default parameter settings. However, because statistical machine translation depends on the training data to estimate translation probability, more high quality training data often leads a better translation result. One possible method of including long sentences into the training cycle is to change the word aligner’s parameter settings to handle longer sentences; however, aligning long sentences is time consuming. Our solution is to capture the styled long sentences and attempt to split them on both source and target side simultaneously according to the numbered list or sentence boundary indications. If the sentence number after splitting are matching in both source and target sides, and each sentence pair is within the token length ratio of 3, we assume the split attempt is successful, otherwise the sentences are kept unchanged and will be filtered out eventually. We applied our splitting attempt approach on the patent documents at the data preparation step which consequently results in 19.35% and 7.1% increase in the number of sentence pairs compared with the original medical in-domain (from 4053258 to 4837382) and overall medical (from 17862521 to 19124142) datasets respectively. Another finding from the training corpora is that the titles of the patent documents are often capitalized in the training corpora. Since we are training a true-cased translation system, and the translation inputs contain non-title sentences, capitalized training sentences will contribute biased weights to our true-case model. We addressed this issue by creating a lowercase version of the title corpora, then we trained our true-case model with the lowercased titles corpora and other non-title corpora. We also included the lowercased title corpora in the translation system training. We tokenized the training corpora using the tokenizer script distributed in the Moses 1.0 framework with additional patent document nonbreaking preferences observed during data preparation, such as Figs and FIGS etc., and a modified aggressive setting (split hyphen character in all cases). Other data preparation steps included character normalization, character/token based foreign language detection, HTML/XML tag removal, case insensitive duplication removal, longer sentence removal (2-80, length ratio 9), resulting in the preprocessed data shown in Table 2. Corpus In-domain parallel sentence number Out-of-domain parallel sentence number EMEA 273,532 0 COPPA 1,374,371 6,075,599 PatTR-title 63,856 3,457,164 PatTR-abstract 599,435 2,595,515 PatTR-claims 876,603 4,244,324 UMLS 85,683 0 Wikipedia 8,438 0 TOTAL 3,956,478 16,372,602 Table 2: WMT 2014 Medical Translation shared task parallel training data after preprocessing steps. 1.2 Training Data Selection It is an open secret that high quality and large quantity of the parallel corpus are the two most important factors for a high-quality SMT system. These factors assist the word aligner in producing a precise alignment model, which in turn brings benefits to the other SMT training steps. The quantity factor also helps the SMT system to cover more translation input variations. In order to efficiently use the training corpora listed in Table 2, we explored some data selection methodologies. We used the feature decay algorithm (Bicici et al., 2014) to select the training instances transductively, using the source side of the test set. We built systems with the pre-defined selection proportions in token number, 1/64, 1/32, 1/16, 1/8, 1/2, 3/4 and 1 of all the in-domain medical training data, then searched for the best performing 261 system using the test data set as our baseline (Table 3). For the purpose of making the potential baseline systems comparable, instance selection was employed after word alignment using word aligner MGIZA++ (Gao and Vogel, 2008) on all the available data. The transductive learning uses features extracted from the source data of the development set with the default feature decay algorithm weight settings. All of systems were trained using the default phrase-based training parameter settings of Moses 1.0 framework, with additional msd-bidirectional-fe reordering model (Koehn et al., 2005). We extract phrase pairs based on growdiag-final-and (Koehn et al., 2003) heuristics. The language model was created with open source IRSTLM toolkit (Federico et al., 2008) using all the English in-domain data (monolingual and parallel). We used 5-gram with modied Kneser-Ney smoothing (Kneser and Ney, 1995). The tuning step used minimum error rate training (MERT) (Och, 2003). The performance was measured by the test data set in case insensitive BLEU score. Proportions Test set case insensitive BLEU</abstract>
<phone confidence="0.778863375">1/64 0.4374 1/32 0.4409 1/16 0.4370 1/8 0.4419 1/4 0.4390 1/2 0.4399 3/4 0.4397 1 0.4260</phone>
<abstract confidence="0.99645368">Table 3: Feature decay algorithm transductive learning selection on all in-domain data using extracted features from the source side of the test data set. We choose system uses 1/8 proportions of the in-domain data as our baseline system. Our results show that the system trained with 1/8 proportion of the in-domain medical training data (398,098 sentence pairs) selected by FDA outperformed the others. We chose this system as our baseline system. 2 Experiments 2.1 Maximum Phrase Length While extracting phrase pairs, collecting longer phrases is not guaranteed to produce a better quality phrase table than the shorter settings, even setting the maximum phrase length to three can achieve top performance (Koehn et al., 2003). We take this WMT 2014 opportunity to study the of long phrase lengths ( ). We trained translation models with phrase length setting from 10 to 15, employed them to our baseline system and compared the performance with the default setting (length = 7). Phrase Length Phrase Table Entries Test set case insensitive BLEU</abstract>
<phone confidence="0.601536666666667">7 (Baseline) 19.31 0.4419 10 29.67 0.4400 11 32.87 0.4416</phone>
<address confidence="0.635512666666667">12 35.95 0.4444* 13 38.91 0.4448* 14 41.75 0.4444*</address>
<phone confidence="0.633146">15 44.47 0.4362</phone>
<abstract confidence="0.998734861386138">Table 4: -max-phrase-length setting experiment, where phrase table entries is in millions. * indistatistically significant improvement at the 0.05 As stated in (Koehn et al., 2003) and expected, the size of the phrase table is linear with respect to the maximum phrase length restriction. Surprisingly, we also found the performance can still improve after the default length setting, until a peak point (Table 4). It is also interesting to see the effect for each sentence in the test set when the default phrase length setting in Moses framework is changed. We first evaluated the sentence level BLEU scores for the systems listed in Table 4, then compared them with our baseline system sentence level BLEU scores and categorised the compared results into increased, decreased or unaffected groups (Figure 1). We found that system with -max-phraselength set to 12 is influenced the least (158, 118 and 724 sentences have BLEU score increased, decreased and unaffected respectively) and with -max-phrase-length sets to 10 is influenced the most (261, 257 and 482 sentences have BLEU score increased, decreased and unaffected respectively). We then looked into the decoding phase and tried to discover the actual phrase length that was used to generate the translation outputs. We exposed the translation segmentations by triggering the -report-segmentation decoding parameter same notation is used for the rest of the tables in this paper 262 -max-phrase-length from 10 to 15 Figure 1: Sentence level BLEU score affects when enlarge -max-phrase-length pl=1 pl=2 pl=3 pl=4 pl=5 pl=6 pl=7 pl=8 baseline and -max-phrase-length from 10 to 15 Figure 2: Phrase length (pl) distribution used in decoding in the Moses framework and computed the percentage of different phrases used according to the phrase token number (Figure 2). The translation is mostly generated from short source phrases in all the systems during decoding, which we think is the reason that setting phrase extraction to length 3 can achieve top performance. We did not carry out more experiments in this case, as we think there is no absolute maximum phrase length setting which can fit into all experiments and such experiments depend on many factors, such as the similarity between the training corpus and then testing data. The choice to set -max-phrase-length to 13 is purely directed by the BLEU score shown in Table 4. 2.2 Reordering Models et al. (2011) also found that long-range reordering is one of the characteristics of patent documents; however, long-range reordering increases the difficulty of SMT training and decoding. We experimented two approaches to address this challenge. Apart from the msd-bidirectional-fe lexical reordering model (Koehn et al., 2005) in our baseline system, the phrase-based orientation and hierarchical orientation reordering models (Galley and Manning, 2008) can capture long distance dependencies. The phrase-based orientation reordering model is similar to the lexical reordering approach, the only difference between these two models is the phrase-based reordering model performs reordering only on the phrase level, but the hierarchical reordering model does not have such constraint it does not require phrases to be adjacent. OSM (Durrani, 2011) (Durrani, 2013b) is a sequence model integrating the N-gram-based translation model and reordering model. It defines three operations for reordering and considers all reordering possibilities within a fixed window while searching. We experimented with both reordering models, and found that the system defined with three reordering models performs better (Table 5) than OSM. We then tried to use both OSM and the reordering models together, which produced the best system at this point. Systems Test set case insensitive BLEU Baseline + 13 0.4448 + OSM 0.4472 + pho-ho 0.4551* + pho-ho + OSM 0.4561* Table 5: Reordering Model or/and OSM results 2.3 Two Translation Models The back-off model aims to produce translations for the unknown words or unknown phrases in the primary translation table by yielding the phrase table translation probability from primary translation table to the back-off table, as in (Koehn et al., 2012a) = Moreover, we look at using the back off model</abstract>
<phone confidence="0.341078">10 11 12 13 14 15</phone>
<address confidence="0.8280825">750 700 650 600 550 500 450 400 350 300 250 200</address>
<abstract confidence="0.9920391125">increased decreased unaffected 7 10 11 12 13 14 15 Source phrase used in decoding (%) test set (1000 sentences) 263 as a domain adaptation approach, which is to constrain the translation options within the target domain unless no options can be found, in which case the translation will be selected from the backoff model. Phrase table fill-up (Bisazza et al., 2011) is a very similar approach with back-off models, it collects and uses the phrase pairs from the out-ofdomain phrase table only when the input is unavailable at the in-domain phrase table. It merges the in-domain and out-of-domain translation models into one, where the scores are taken from more reliable source. To distinguish the source of a phrase pair entry, fill-up assigns a binary value as an additional feature at the merged phrase table. We trained our out-of-domain translation model separately using all of the out-of-domain medical data listed at Table 2 with the same parameter settings as our baseline system, then employed Moses’s back-off model feature to pass the primary and back-off translation models to the decoder at tuning and translation time. The fill-up tool was sourced from (Bisazza et al., 2011) at Moses’s distribution. Our experiment results (Table 6) show that the fill-up approach performed better than the back-off model approach. Systems Test set case insensitive BLEU Baseline + 13 + pho-ho + OSM 0.4561 Back-off 0.4573 Fill-up 0.4599* Table 6: Back-off and fill-up experiment results 2.4 Language Model Until now, we have reported our results using a language model trained with all in-domain medical data only. We also took the similar approach to (Koehn et al., 2007) and carried out language model experiments. We trained our out-of-domain language model with all the out-of-domain English sentences mentioned in section 1.1, then interpolated the in-domain and out-of-domain language model by optimizing the perplexity to the development data set. We received a similar picture to (Koehn et al., 2007), where the language model trained with only in-domain data performed the best (Table 7). Our final submission for WMT 2014 Medical Translation shared task is the * system at Table 7. Systems Test set case insensitive BLEU Baseline + 13 + pho-ho + OSM + Fill-up* 0.4599 out-of-domain LM 0.4461 interpolated LM 0.4592 Table 7: Language model experiment results 3 Conclusion In this paper, we report our results on the WMT 2014 in the French to English translation direction. We shared our statistics for the bilingual corpora used to train our translation system. All systems were trained using the open source Moses 1.0 translation framework. Based on the feature set of Moses phrased-based translation system, we carried out our experiments on translation models, reordering models, operation sequence model and language model. We also experimented on data selection and releasing the length restriction while extracting phrase pairs. 4 Acknowledgements This research is supported by the Science Foundation Ireland (Grant 12/CE/I2267) as part of the Centre for Next Generation Localisation (www.cngl.ie) at Dublin City University. We would also like to acknowledge Ergun Bicici who gives suggestions at the data selection approach.</abstract>
<note confidence="0.694177928571429">References John Tinsley, Jian Zhang and Andy 2011. on domain adaptation for machine translation in the PLuTO The 15th conference of the European Association for Machine Translation, Leuven, Belgium. Arianna Bisazza, Nick Ruiz, and Marcello Fed- 2011. versus Interpolation Methfor Phrase-based SMT In International Workshop on Spoken Language Translation (IWSLT), San Francisco, CA. N., Schmid, H., and Fraser, A. 2011. Joint Sequence Translation Model with Integrated The 49th Annual Meeting of the As-</note>
<affiliation confidence="0.833308">sociation for Computational Linguistics, Portland,</affiliation>
<address confidence="0.780149">Oregon, USA.</address>
<note confidence="0.925476714285714">Durrani, N., Fraser, A., Schmid, H., Hoang, H., and P. 2013b. Markov Models Over Min- Translation Units Help Phrase-Based The 51th Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria. 264 Bicici and Deniz Yuret. 2014. In-</note>
<title confidence="0.85964725">stance Selection for Statistical Machine Translation Feature Decay IEEE/ACM Transactions On Audio, Speech, and Language Processing (TASLP).</title>
<author confidence="0.56768">systematic</author>
<abstract confidence="0.930033">of various statistical alignment Computational Linguistics, 29(1):1951. Josef Och. 2003. error rate training statistical machine The 41th Annual</abstract>
<affiliation confidence="0.866174">Meeting of the Association for Computational Lin-</affiliation>
<address confidence="0.788008">guistics, Sapporo, Japan.</address>
<author confidence="0.851536">Marcello Federico</author>
<author confidence="0.851536">Nicola Bertoldi</author>
<author confidence="0.851536">Mauro Cet-</author>
<abstract confidence="0.3134015">2008. an open source toolkit for large scale language Interspeech,</abstract>
<address confidence="0.845796">Brisbane, Australia.</address>
<note confidence="0.881699590909091">Galley and Christopher D. Manning. 2008. simple and effective hierarchical phrase reordering The 2008 Conference on Empirical Methods in Natural Language Processing,pages 848856, Honolulu, Hawaii, October. Association for Computational Linguistics. Gao and Stephan Vogel. 2008. implemenof word alignment In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, SETQA-NLP 2008, pages 49-57, Stroudsburg, PA, USA. Association for Computational Linguistics. Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne and Talbot. 2005. System Description the 2005 IWSLT Speech Translation International Workshop on Spoken Language Translation. Koehn and Josh Schroeder. 2007. Experiments in Domain Adaptation for Statistical Machine The Second Workshop on Statistical Machine Translation, pages 224227, Prague.</note>
<author confidence="0.523902">Philipp Koehn</author>
<author confidence="0.523902">Amittai Axelrod</author>
<author confidence="0.523902">Alexandra Birch Mayne</author>
<author confidence="0.523902">Chris Callison-Burch</author>
<author confidence="0.523902">Miles Osborne</author>
<note confidence="0.9402295">Talbot. 2003. phrase-based trans- 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 4854, Edmonton, Canada. Koehn, and Barry Haddow. 2012. for factored translation The 10th Conference of the Association for Machine Translation in the Americas (AMTA). Kneser and Hermann Ney 1995. m-gram language IEEE International Conference on Acoustics, Speech and Signal Processing, pages 181184. 265</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandru Ceaus¸u</author>
<author>John Tinsley</author>
<author>Jian Zhang</author>
<author>Andy Way</author>
</authors>
<title>Experiments on domain adaptation for patent machine translation in the PLuTO project,</title>
<date>2011</date>
<booktitle>The 15th conference of the European Association for Machine Translation,</booktitle>
<location>Leuven, Belgium.</location>
<marker>Ceaus¸u, Tinsley, Zhang, Way, 2011</marker>
<rawString>Alexandru Ceaus¸u, John Tinsley, Jian Zhang and Andy Way. 2011. Experiments on domain adaptation for patent machine translation in the PLuTO project, The 15th conference of the European Association for Machine Translation, Leuven, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Nick Ruiz</author>
<author>Marcello Federico</author>
</authors>
<title>Fill-up versus Interpolation Methods for Phrase-based SMT Adaptation.,</title>
<date>2011</date>
<booktitle>In International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="14732" citStr="Bisazza et al., 2011" startWordPosition="2304" endWordPosition="2307">n (Koehn et al., 2012a) pBO(e|f) = { p1(e|f) if p2(e|f) otherwise t1(f) &gt; 0 Moreover, we look at using the back off model 10 11 12 13 14 15 750 700 650 600 550 500 450 400 350 300 250 200 increased decreased unaffected 7 10 11 12 13 14 15 Source phrase used in decoding (%) 5 · 10−2 5 · 10−4 0.55 0.45 0.35 0.25 0.15 0.6 0.5 0.4 0.3 0.2 0.1 test set (1000 sentences) 263 as a domain adaptation approach, which is to constrain the translation options within the target domain unless no options can be found, in which case the translation will be selected from the backoff model. Phrase table fill-up (Bisazza et al., 2011) is a very similar approach with back-off models, it collects and uses the phrase pairs from the out-ofdomain phrase table only when the input is unavailable at the in-domain phrase table. It merges the in-domain and out-of-domain translation models into one, where the scores are taken from more reliable source. To distinguish the source of a phrase pair entry, fill-up assigns a binary value as an additional feature at the merged phrase table. We trained our out-of-domain translation model separately using all of the out-of-domain medical data listed at Table 2 with the same parameter settings</context>
</contexts>
<marker>Bisazza, Ruiz, Federico, 2011</marker>
<rawString>Arianna Bisazza, Nick Ruiz, and Marcello Federico. 2011. Fill-up versus Interpolation Methods for Phrase-based SMT Adaptation., In International Workshop on Spoken Language Translation (IWSLT), San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Durrani</author>
<author>H Schmid</author>
<author>A Fraser</author>
</authors>
<title>A Joint Sequence Translation Model with Integrated Reordering.,</title>
<date>2011</date>
<booktitle>The 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Portland, Oregon, USA.</location>
<marker>Durrani, Schmid, Fraser, 2011</marker>
<rawString>Durrani, N., Schmid, H., and Fraser, A. 2011. A Joint Sequence Translation Model with Integrated Reordering., The 49th Annual Meeting of the Association for Computational Linguistics, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Durrani</author>
<author>A Fraser</author>
<author>H Schmid</author>
<author>H Hoang</author>
<author>P Koehn</author>
</authors>
<title>Can Markov Models Over Minimal Translation Units Help Phrase-Based</title>
<date>2013</date>
<booktitle>SMT, The 51th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sofia, Bulgaria.</location>
<marker>Durrani, Fraser, Schmid, Hoang, Koehn, 2013</marker>
<rawString>Durrani, N., Fraser, A., Schmid, H., Hoang, H., and Koehn, P. 2013b. Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT, The 51th Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bicici</author>
<author>Deniz Yuret</author>
</authors>
<title>Optimizing Instance Selection for Statistical Machine Translation with Feature Decay Algorithms,</title>
<date>2014</date>
<journal>IEEE/ACM Transactions On Audio, Speech, and Language Processing (TASLP).</journal>
<marker>Bicici, Yuret, 2014</marker>
<rawString>Ergun Bicici and Deniz Yuret. 2014. Optimizing Instance Selection for Statistical Machine Translation with Feature Decay Algorithms, IEEE/ACM Transactions On Audio, Speech, and Language Processing (TASLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models,</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models, Computational Linguistics, 29(1):1951.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation,</title>
<date>2003</date>
<booktitle>The 41th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="3902" citStr="Och, 2003" startWordPosition="562" endWordPosition="563">in linguistic idiosyncrasies, which distinguish patent-specific translation systems from general translation systems, in both training and translation phases (Ceaus¸u et al., 2011). We have also found that some common writing styles are constantly used, especially for long sentences. For example, a typical patent claim begins with Method of [X], which comprising: followed by a numbered list. The abstract field normally contains one paragraph only, but with multiple sentences. Those long sentences are necessarily filtered out to facilitate efficient word alignment, using a tool such as GIZA++ (Och, 2003) word aligner with the default parameter settings. However, because statistical machine translation depends on the training data to estimate translation probability, more high quality training data often leads a better translation result. One possible method of including long sentences into the training cycle is to change the word aligner’s parameter settings to handle longer sentences; however, aligning long sentences is time consuming. Our solution is to capture the styled long sentences and attempt to split them on both source and target side simultaneously according to the numbered list or</context>
<context position="8495" citStr="Och, 2003" startWordPosition="1271" endWordPosition="1272">fault feature decay algorithm weight settings. All of systems were trained using the default phrase-based training parameter settings of Moses 1.0 framework, with additional msd-bidirectional-fe reordering model (Koehn et al., 2005). We extract phrase pairs based on growdiag-final-and (Koehn et al., 2003) heuristics. The language model was created with open source IRSTLM toolkit (Federico et al., 2008) using all the English in-domain data (monolingual and parallel). We used 5-gram with modied Kneser-Ney smoothing (Kneser and Ney, 1995). The tuning step used minimum error rate training (MERT) (Och, 2003). The performance was measured by the test data set in case insensitive BLEU score. Proportions Test set case insensitive BLEU 1/64 0.4374 1/32 0.4409 1/16 0.4370 1/8 0.4419 1/4 0.4390 1/2 0.4399 3/4 0.4397 1 0.4260 Table 3: Feature decay algorithm transductive learning selection on all in-domain data using extracted features from the source side of the test data set. We choose system uses 1/8 proportions of the in-domain data as our baseline system. Our results show that the system trained with 1/8 proportion of the in-domain medical training data (398,098 sentence pairs) selected by FDA outp</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation, The 41th Annual Meeting of the Association for Computational Linguistics, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
</authors>
<title>IRSTLM: an open source toolkit for handling large scale language models, Interspeech,</title>
<date>2008</date>
<location>Brisbane, Australia.</location>
<contexts>
<context position="8290" citStr="Federico et al., 2008" startWordPosition="1237" endWordPosition="1240">n was employed after word alignment using word aligner MGIZA++ (Gao and Vogel, 2008) on all the available data. The transductive learning uses features extracted from the source data of the development set with the default feature decay algorithm weight settings. All of systems were trained using the default phrase-based training parameter settings of Moses 1.0 framework, with additional msd-bidirectional-fe reordering model (Koehn et al., 2005). We extract phrase pairs based on growdiag-final-and (Koehn et al., 2003) heuristics. The language model was created with open source IRSTLM toolkit (Federico et al., 2008) using all the English in-domain data (monolingual and parallel). We used 5-gram with modied Kneser-Ney smoothing (Kneser and Ney, 1995). The tuning step used minimum error rate training (MERT) (Och, 2003). The performance was measured by the test data set in case insensitive BLEU score. Proportions Test set case insensitive BLEU 1/64 0.4374 1/32 0.4409 1/16 0.4370 1/8 0.4419 1/4 0.4390 1/2 0.4399 3/4 0.4397 1 0.4260 Table 3: Feature decay algorithm transductive learning selection on all in-domain data using extracted features from the source side of the test data set. We choose system uses 1/</context>
</contexts>
<marker>Federico, Bertoldi, Cettolo, 2008</marker>
<rawString>Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. IRSTLM: an open source toolkit for handling large scale language models, Interspeech, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>The 2008 Conference on Empirical Methods in Natural Language Processing,pages 848856,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="12810" citStr="Galley and Manning, 2008" startWordPosition="1971" endWordPosition="1975">he training corpus and then testing data. The choice to set -max-phrase-length to 13 is purely directed by the BLEU score shown in Table 4. 2.2 Reordering Models Ceaus¸u et al. (2011) also found that long-range reordering is one of the characteristics of patent documents; however, long-range reordering increases the difficulty of SMT training and decoding. We experimented two approaches to address this challenge. Apart from the msd-bidirectional-fe lexical reordering model (Koehn et al., 2005) in our baseline system, the phrase-based orientation and hierarchical orientation reordering models (Galley and Manning, 2008) can capture long distance dependencies. The phrase-based orientation reordering model is similar to the lexical reordering approach, the only difference between these two models is the phrase-based reordering model performs reordering only on the phrase level, but the hierarchical reordering model does not have such constraint - it does not require phrases to be adjacent. OSM (Durrani, 2011) (Durrani, 2013b) is a sequence model integrating the N-gram-based translation model and reordering model. It defines three operations for reordering and considers all reordering possibilities within a fix</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. , The 2008 Conference on Empirical Methods in Natural Language Processing,pages 848856, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Parallel implementations of word alignment tool,</title>
<date>2008</date>
<booktitle>In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, SETQA-NLP</booktitle>
<pages>49--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7752" citStr="Gao and Vogel, 2008" startWordPosition="1156" endWordPosition="1159"> explored some data selection methodologies. We used the feature decay algorithm (Bicici et al., 2014) to select the training instances transductively, using the source side of the test set. We built systems with the pre-defined selection proportions in token number, 1/64, 1/32, 1/16, 1/8, 1/2, 3/4 and 1 of all the in-domain medical training data, then searched for the best performing 261 system using the test data set as our baseline (Table 3). For the purpose of making the potential baseline systems comparable, instance selection was employed after word alignment using word aligner MGIZA++ (Gao and Vogel, 2008) on all the available data. The transductive learning uses features extracted from the source data of the development set with the default feature decay algorithm weight settings. All of systems were trained using the default phrase-based training parameter settings of Moses 1.0 framework, with additional msd-bidirectional-fe reordering model (Koehn et al., 2005). We extract phrase pairs based on growdiag-final-and (Koehn et al., 2003) heuristics. The language model was created with open source IRSTLM toolkit (Federico et al., 2008) using all the English in-domain data (monolingual and paralle</context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool, In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, SETQA-NLP 2008, pages 49-57, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<date>2005</date>
<booktitle>Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation, International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="8117" citStr="Koehn et al., 2005" startWordPosition="1210" endWordPosition="1213">r the best performing 261 system using the test data set as our baseline (Table 3). For the purpose of making the potential baseline systems comparable, instance selection was employed after word alignment using word aligner MGIZA++ (Gao and Vogel, 2008) on all the available data. The transductive learning uses features extracted from the source data of the development set with the default feature decay algorithm weight settings. All of systems were trained using the default phrase-based training parameter settings of Moses 1.0 framework, with additional msd-bidirectional-fe reordering model (Koehn et al., 2005). We extract phrase pairs based on growdiag-final-and (Koehn et al., 2003) heuristics. The language model was created with open source IRSTLM toolkit (Federico et al., 2008) using all the English in-domain data (monolingual and parallel). We used 5-gram with modied Kneser-Ney smoothing (Kneser and Ney, 1995). The tuning step used minimum error rate training (MERT) (Och, 2003). The performance was measured by the test data set in case insensitive BLEU score. Proportions Test set case insensitive BLEU 1/64 0.4374 1/32 0.4409 1/16 0.4370 1/8 0.4419 1/4 0.4390 1/2 0.4399 3/4 0.4397 1 0.4260 Table </context>
<context position="12683" citStr="Koehn et al., 2005" startWordPosition="1955" endWordPosition="1958"> setting which can fit into all experiments and such experiments depend on many factors, such as the similarity between the training corpus and then testing data. The choice to set -max-phrase-length to 13 is purely directed by the BLEU score shown in Table 4. 2.2 Reordering Models Ceaus¸u et al. (2011) also found that long-range reordering is one of the characteristics of patent documents; however, long-range reordering increases the difficulty of SMT training and decoding. We experimented two approaches to address this challenge. Apart from the msd-bidirectional-fe lexical reordering model (Koehn et al., 2005) in our baseline system, the phrase-based orientation and hierarchical orientation reordering models (Galley and Manning, 2008) can capture long distance dependencies. The phrase-based orientation reordering model is similar to the lexical reordering approach, the only difference between these two models is the phrase-based reordering model performs reordering only on the phrase level, but the hierarchical reordering model does not have such constraint - it does not require phrases to be adjacent. OSM (Durrani, 2011) (Durrani, 2013b) is a sequence model integrating the N-gram-based translation</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation, International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<date>2007</date>
<booktitle>Experiments in Domain Adaptation for Statistical Machine Translation, The Second Workshop on Statistical Machine Translation,</booktitle>
<pages>224227</pages>
<location>Prague.</location>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in Domain Adaptation for Statistical Machine Translation, The Second Workshop on Statistical Machine Translation, pages 224227, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Statistical phrase-based translation,</title>
<date>2003</date>
<booktitle>Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>4854</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="8191" citStr="Koehn et al., 2003" startWordPosition="1222" endWordPosition="1225">Table 3). For the purpose of making the potential baseline systems comparable, instance selection was employed after word alignment using word aligner MGIZA++ (Gao and Vogel, 2008) on all the available data. The transductive learning uses features extracted from the source data of the development set with the default feature decay algorithm weight settings. All of systems were trained using the default phrase-based training parameter settings of Moses 1.0 framework, with additional msd-bidirectional-fe reordering model (Koehn et al., 2005). We extract phrase pairs based on growdiag-final-and (Koehn et al., 2003) heuristics. The language model was created with open source IRSTLM toolkit (Federico et al., 2008) using all the English in-domain data (monolingual and parallel). We used 5-gram with modied Kneser-Ney smoothing (Kneser and Ney, 1995). The tuning step used minimum error rate training (MERT) (Och, 2003). The performance was measured by the test data set in case insensitive BLEU score. Proportions Test set case insensitive BLEU 1/64 0.4374 1/32 0.4409 1/16 0.4370 1/8 0.4419 1/4 0.4390 1/2 0.4399 3/4 0.4397 1 0.4260 Table 3: Feature decay algorithm transductive learning selection on all in-domai</context>
<context position="9440" citStr="Koehn et al., 2003" startWordPosition="1421" endWordPosition="1424">s from the source side of the test data set. We choose system uses 1/8 proportions of the in-domain data as our baseline system. Our results show that the system trained with 1/8 proportion of the in-domain medical training data (398,098 sentence pairs) selected by FDA outperformed the others. We chose this system as our baseline system. 2 Experiments 2.1 Maximum Phrase Length While extracting phrase pairs, collecting longer phrases is not guaranteed to produce a better quality phrase table than the shorter settings, even setting the maximum phrase length to three can achieve top performance (Koehn et al., 2003). We take this WMT 2014 opportunity to study the capability of long phrase lengths ( &gt;=10 ). We trained translation models with phrase length setting from 10 to 15, employed them to our baseline system and compared the performance with the default setting (length = 7). Phrase Length Phrase Table Test set Entries case insensitive BLEU 7 (Baseline) 19.31 0.4419 10 29.67 0.4400 11 32.87 0.4416 12 35.95 0.4444* 13 38.91 0.4448* 14 41.75 0.4444* 15 44.47 0.4362 Table 4: -max-phrase-length setting experiment, where phrase table entries is in millions. * indiactes statistically significant improvemen</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2003</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne and David Talbot. 2003. Statistical phrase-based translation, 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 4854, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Barry Haddow</author>
</authors>
<title>Interpolated backoff for factored translation models.,</title>
<date>2012</date>
<booktitle>The 10th Conference of the Association for Machine Translation in the Americas (AMTA).</booktitle>
<marker>Koehn, Haddow, 2012</marker>
<rawString>Philipp Koehn, and Barry Haddow. 2012. Interpolated backoff for factored translation models., The 10th Conference of the Association for Machine Translation in the Americas (AMTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-offfor m-gram language modeling.,</title>
<date>1995</date>
<booktitle>IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>181184</pages>
<contexts>
<context position="8426" citStr="Kneser and Ney, 1995" startWordPosition="1258" endWordPosition="1261"> uses features extracted from the source data of the development set with the default feature decay algorithm weight settings. All of systems were trained using the default phrase-based training parameter settings of Moses 1.0 framework, with additional msd-bidirectional-fe reordering model (Koehn et al., 2005). We extract phrase pairs based on growdiag-final-and (Koehn et al., 2003) heuristics. The language model was created with open source IRSTLM toolkit (Federico et al., 2008) using all the English in-domain data (monolingual and parallel). We used 5-gram with modied Kneser-Ney smoothing (Kneser and Ney, 1995). The tuning step used minimum error rate training (MERT) (Och, 2003). The performance was measured by the test data set in case insensitive BLEU score. Proportions Test set case insensitive BLEU 1/64 0.4374 1/32 0.4409 1/16 0.4370 1/8 0.4419 1/4 0.4390 1/2 0.4399 3/4 0.4397 1 0.4260 Table 3: Feature decay algorithm transductive learning selection on all in-domain data using extracted features from the source side of the test data set. We choose system uses 1/8 proportions of the in-domain data as our baseline system. Our results show that the system trained with 1/8 proportion of the in-domai</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney 1995. Improved backing-offfor m-gram language modeling., IEEE International Conference on Acoustics, Speech and Signal Processing, pages 181184.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
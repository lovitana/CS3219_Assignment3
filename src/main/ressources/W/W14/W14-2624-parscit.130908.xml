<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016985">
<title confidence="0.9945595">
The Use of Text Similarity and Sentiment Analysis to Examine Ra-
tionales in the Large-Scale Online Deliberations
</title>
<author confidence="0.997544">
Wanting Mao
</author>
<affiliation confidence="0.961391">
Department of Computer
Science
The University of Western
Ontario
</affiliation>
<address confidence="0.69202">
London, ON, Canada
</address>
<email confidence="0.99622">
fiona.wt.mao@gmail.com
</email>
<author confidence="0.984492">
Lu Xiao
</author>
<affiliation confidence="0.94261775">
Faculty of Information &amp;
Media Studies
The University of Western
Ontario
</affiliation>
<address confidence="0.690171">
London, ON, Canada
</address>
<email confidence="0.992063">
lxiao24@uwo.ca
</email>
<author confidence="0.996513">
Robert Mercer
</author>
<affiliation confidence="0.961317">
Department of Computer
Science
The University of Western
Ontario
</affiliation>
<address confidence="0.692859">
London, ON, Canada
</address>
<email confidence="0.99769">
mercer@csd.uwo.ca
</email>
<sectionHeader confidence="0.995629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938153846154">
To overcome the increasingly time con-
suming and potentially challenging iden-
tification of key points and the associated
rationales in large-scale online delibera-
tions, we propose a computational lin-
guistics method that has the potential of
facilitating this process of reading and
evaluating the text. Our approach is novel
in how we determine the sentiment of a
rationale at the sentence level and in that
it includes a text similarity measure and
sentence-level sentiment analysis to
achieve this goal.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985982142857">
In an online deliberation situation where users
join in and offer their opinions or suggestions,
they are expected to provide the rationales that
justify their standpoints in the deliberation. In
the final decision making process, one expected-
ly needs to read through the content and weigh
different key points and related rationales. Wik-
ipedia Article for Deletion (AfD) deliberations
represent one such example. In the Wikipedia
community, any member can propose to delete
an existing Wikipedia article. After an article is
proposed to delete, a deliberation topic about the
article is opened in the AfD forum. The commu-
nity members can express their opinions (e.g., to
keep or to delete the article) and provide their
rationales within the specified time period. After
that, a community member (often a Wikipedia
administrator) closes the deliberation by making
the final decision. Researchers have analyzed the
Wikipedia AfD forum and have demonstrated
that it presents a successful example of large-
scale online deliberation by allowing many peo-
ple to participate equally, encouraging people to
deliberate, and producing rational and meaning-
ful rationales (e.g., Schneider et al., 2012; Xiao
&amp; Askin, 2014). Wikipedia policy requires that
the final decision about the article should be
made based on the discussed rationales instead of
the count of opinion votes. In practice many
Wikipedia members who close the deliberations
follow this policy, which implies the potential
problem of representing the diverse rationales
and identifying the influential ones in this con-
text.
Generating the final decision of a large scale
online deliberation can become a daunting task,
as the amount of opinions and rationales in the
deliberation content increases significantly. To
facilitate this decision making process in large-
scale online deliberations, we have developed a
method that uses an existing text-to-text similari-
ty measure and our developed sentence-level
sentiment analysis algorithm to address this issue.
Specifically, we first group participants’ opinions
according to the similarity measure, then we
identify the positive, neutral, and negative senti-
ments suggested by the participants&apos; rationales in
each group, and finally we choose a representa-
tive rationale from each sentiment category in a
group. With our method the diverse opinions and
rationales are presented to the final decision
maker through a representative set of the ration-
ales, reducing the redundant information from
the deliberation content so as to make the process
of reading and evaluating the deliberation con-
tent more efficient.
</bodyText>
<page confidence="0.975916">
147
</page>
<bodyText confidence="0.722518">
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 147–153,
Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="introduction">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.993209">
2.1 Text Similarity
</subsectionHeader>
<bodyText confidence="0.9999011">
Recognizing the relation between texts (e.g.,
sentence to sentence, paragraph to paragraph)
could help people better understand the context.
Text similarity can be interpreted as similarity
between sentences, paragraphs, documents, etc.
It has been used in various aspects in NLP such
as information retrieval, text classification, and
automatic evaluation. The most fundamental part
is word similarity. We consider words to be simi-
lar in the following conditions: synonyms, anto-
nyms, similar concept (e.g., red, green), similar
context (e.g., doctor, hospital), and hypo-
nym/hypernym relation (e.g., dog, pet).
WordNet, a word-to-word similarity library
was developed by Pedersen et al. (2004), and has
been widely used to compute the similarity at a
coarser granularity (e.g., sentence-to-sentence
similarity). Various methods to deal with text
similarity have been proposed over the past dec-
ades. Mihalcea et al. (2006) proposed a greedy
method to calculate the similarity score between
two texts T1 and T2. Basically for each word in
T1 (T2), the maximum similarity score to any
word in T2 (T1) is used. The WordNet similarity
can be used for assigning similarity scores be-
tween every pair of words in the two texts.
Rus and Lintean (2012) proposed an optimal
method to compute text similarity based on
word-to-word similarity. It is similar to the opti-
mal assignment problem. Given a weighted
complete bipartite graph (G = X È Y; X × Y),
with weight w(xy) on edge xy, we need to find a
matching from X to Y with a maximum total
weight. Their results showed that the optimal
method outperformed the greedy method in
terms of accuracy and kappa statistics.
Other statistics-based algorithms are also de-
veloped to measure text similarity, e.g., the use
of the Latent Dirichlet Allocation (LDA) model
(Rus et al., 2013).
</bodyText>
<subsectionHeader confidence="0.999821">
2.2 Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.999002625">
Sentiment analysis is meant to determine the
polarity of a certain text, which can be positive,
negative or neutral. Related academia and indus-
tries have been extensively investigating senti-
ment analysis methods over the last decade.
While most of the early work in sentiment analy-
sis is aimed at analyzing the polarity of customer
reviews (e.g., Kim and Hovy, 2004; Hu and Liu,
2004; Turney, 2002), there is a proliferation in
analyzing social media text (e.g., Balahur, 2013;
Liebrecht et al., 2013; Bakliwal et al., 2012;
Montejo-Raez et al., 2012) and online discus-
sions (e.g., Sood et al., 2012a, 2012b).
Researchers have used a variety of approach-
es to detect the sentiment polarity of the given
text. For example, in Kim and Hovy&apos;s system
(2004) the sentiment region of the opinion is
identified based on the extracted opinion holders
and topics. The system combines the sentiments
of the sentiment region and the polarity of the
words to determine the polarity of the given text.
In Li and Wu’s (2010) study, they interpreted
the article as a sequence of key words and calcu-
lated the sentiment score of each key word based
on the dictionary and its privative and modifier
near it. In the analysis of the tweets, Balahur
(2013) replaced the sentiment words and modifi-
ers by sentiment labels (positive, negative, high
positive and high negative) or modification la-
bels (negator, intensifier or diminisher), and then
applied Support Vector Machine Sequential Min-
imal Optimization (SVM SMO) to classify three
different data sets.
Online discussions may have inappropriate
use of language in some cases, which affects the
online community management negatively. Sood
et al. (2012a) proposed a multistep classifier by
combining valence analysis and a SVM to detect
insults and classify the insult object.
Researchers have also looked at the use of de-
pendency tree-based method for sentiment classi-
fication. For instance, Nakagawa et al. (2010)
used a probabilistic model of the information
garnered from the dependency tree to determine
the sentiment of a sentence. Rentoumi et al.
(2010) combines word sense disambiguation, a
rule-based system, and Hidden Markov Models
(HMMs) to deal with figurative language (e.g.
record-shattering day) in sentiment analysis.
Moilanen and Pulman (2007) presented a com-
positional model for three-class (positive, nega-
tive, and neutral) phrase-level and sentence-level
sentiment analysis. In their algorithm, each bi-
nary combination of a Head and Complement
had a rule that determined which of the Head and
Complement polarities dominated. In exceptional
cases the rule inverts the polarity of the subordi-
nate.
Socher et al. (2013) developed a Recursive
Neural Tensor Network (RNTN) model. The au-
thors showed that the accuracy obtained by
RNTN outperformed a standard recursive neural
network (RNN), matrix-vector RNN (MV-RNN),
Naive Bayes (NB) and SVM. The advantage of
</bodyText>
<page confidence="0.991515">
148
</page>
<bodyText confidence="0.989599213114754">
RNTN is especially evident when compared with
the methods that only use bag of words (NB and
SVM). This indicates the importance of using
parse trees during sentiment analysis.
3 A Method for Identifying Representa-
tive Rationales in Online Delibera-
tions
Our observation of the Wikipedia AfD forum
suggests that one topic (e.g., notability) can ap-
pear multiple times in different rationales by dif-
ferent users. For example, two users’ comments
–“Could be redirected to OpenWMA, the content
of which isn&apos;t all that different from this article”
and “Redirected to OpenWMA as suggested”–are
considered redundant.
The redundant information itself does not add
a new perspective to final decision making. On
the other hand, sometime the information about
the same type of rationale represents different
opinions about it. Here is one such example from
an article’s deletion discussion: “redirecting the
page to the lead actors future projects section
will be cool” and “I don&apos;t think it is wise to redi-
rect to the original film”.
To make the final decision making process
more efficient, compared to human reading of all
the deliberation content, we have developed a
method that includes a text-to-text similarity
measure and a sentence-level sentiment analysis
algorithm. Specifically, we use text similarity to
group the rationales according to the aspects they
reflect so we can select some rationales from
each aspect group instead of all of them. We note
that although the rationales are redundant in
showing the same aspect, the redundancy implies
the importance of the aspect in the deliberation
since they are used multiple times by users in
justifying their opinions. So in our method, we
record the number of members that proposed the
same aspect assuming that this would indicate
the level of importance of the aspect to some ex-
tent. .
With the rationales grouped according to the
aspects that they involve (e.g., notability, credi-
bility, etc), our method examines the sentiment
polarity of each rationale in a group to further
examine whether the rationale is positive or neg-
ative (e.g., the article is notable or not), or is neu-
tral about the aspect. Then we can identify the
representative rationales of an opinion by choos-
ing those that have the highest similarity score in
a group. In sum, the text-to-text similarity
measure combined with our sentence-level sen-
timent analysis algorithm helps us identify the
representative rationales of diverse opinions in
an online deliberation. An overview of our meth-
od is shown in Figure 1.
We applied our method in analyzing Wikipe-
dia Article for Deletion (AfD) deliberation con-
tent. Next we discuss how this method is used to
analyze the content.
</bodyText>
<figureCaption confidence="0.95712">
Figure 1. An overview of our method for identifying representative rationales from large-scale online delibera-
tion
</figureCaption>
<subsectionHeader confidence="0.998865">
3.1 Text-to-Text Similarity Measure
</subsectionHeader>
<bodyText confidence="0.9999778">
In our study, we used SEMILAR, a semantic
similarity toolkit (Rus et al., 2013), to measure
We tested three similarity approaches provided
in SEMILAR: optimum method based on Word-
Net, similarity based on Latent Semantic Analy-
sis (LSA) and similarity based on Latent Di-
richlet Analysis (LDA). We first extracted 80
pairs of sentences from the Wikipedia AfD fo-
rum and manually annotated them as similar or
not. We then used these annotated results in
measuring the accuracy of the three SEMILAR
approaches. SEMILAR assigns a similarity score
to each pair of sentences ranging from 0 to 1. To
evaluate the accuracy of the three approaches, we
identified a threshold to divide the result into two
groups (i.e., similar and not similar). To do so,
we computed the accuracy for 101 thresholds
ranging from 0.00 to 1.00 with an interval of
0.01 to find the highest accuracy. Through this
approach, we identified that the WordNet-based
</bodyText>
<page confidence="0.99597">
149
</page>
<bodyText confidence="0.999922">
optimum method achieved the best accuracy of
76.3% at threshold 0.13. The other two methods
achieved similar accuracy (76.3% and 75% re-
spectively) but took more than double the time to
process. Therefore, we chose the WordNet-based
optimum method.
With this method, we have a similarity matrix
that shows the similarity score between every
pair of sentences in the discussion. We transform
the similarity matrix to a dissimilarity matrix by
transforming the similarity score x for two sen-
tences to the distance between the sentences 1/x.
Then we used hierarchical clustering (Kaufman
and Rousseeuw, 2009) to cluster the sentences
into groups. To do so, we set the maximum al-
lowed distance between two similar sentences to
be 8 (i.e., the similarity score would be 0.125),
and used the agglomerative approach to form the
clusters. As a consequence, the sentences in the
same group are related to a common theme.
</bodyText>
<subsectionHeader confidence="0.999493">
3.2 Sentence-Level Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.99993109375">
In our sentiment analysis algorithm, each word in
a sentence is assigned a prior polarity based on
an adapted MPQA Subjectivity Lexicon
(Pedersen et al., 2004). Compared to the original
Lexicon, this adapted one includes additional
sentiment words that are important for the Wik-
ipedia’s AfD discussions (e.g., notable). Then,
using the syntactic and dependency trees of the
sentence, the algorithm calculates each word’s
current polarity score which can be affected by
its children’s polarity scores. Through this ap-
proach, the root’s current polarity score becomes
the sentence’s polarity score.
The children’s polarity scores can affect the
parent’s prior polarity score positively or nega-
tively. The positive or neutral effect of the chil-
dren’s polarity scores is reflected through sum-
ming the children’s polarity scores and then ad-
ding the sum to the parent’s polarity score. The
negative effect is reflected through summing the
children’s polarity scores and then multiplying
the sum to the parent’s polarity score. Because
our algorithm only considers three sentiment sit-
uations: negative, positive, and neutral, it is the
negation of the parent’s prior polarity that affects
the accuracy of our algorithm the most. There-
fore, the core of our algorithm is a recursive
method that examines different negation situa-
tions in the input sentence, starting from the leaf
node of the sentence’s dependency tree. We use
this tree structure because it helps us detect the
most of the negation situations:
</bodyText>
<listItem confidence="0.993169666666667">
1. I agree that the place is notable.
2. I don’t agree that the place is notable.
(Local Negation)
3. I disagree that the place is notable. (Pred-
icate Negation)
4. Neither one of us agrees that the place is
notable. (Subject Negation)
5. It is a violation of notability. (Preposition
Negation)
</listItem>
<bodyText confidence="0.999900034482759">
However, there is one negation situation that
cannot be detected from the syntactic structure of
the sentence. For example, in the sentence “the
place is of indeterminable notability”, notability
is a positive word, but as it is modified by a neg-
ative word indeterminable the phrase becomes
negative. This negation case is called modifier
negation. A negative modifier might also negate
a negative word, such as little damage, never fail.
However a negative modifier does not always
negate the polarity of the phrase determined by
the polarity of the related word. Instead, the
phrase remains its prior polarity, e.g., terribly
allergic.
It is also worth noticing that context affects
the phrase polarity. Consider the phrase original
research in our study context – the Wikipedia
AfD forum. Because articles reporting original
research violate Wikipedia’s neutrality policy,
the phrase original research in the deletion dis-
cussions should be considered to be negative.
As there is no straightforward way of deter-
mining whether or not a modifier negates the
polarity of the word being modified, we decided
to use machine learning methods to help classify
the modifier negation cases. We considered the
following modifier phrases in the study and at
least one word in the phrase has to be a sentiment
word:
</bodyText>
<listItem confidence="0.9999864">
• Noun modified by adjective
• Noun modified by noun
• Adjective modified by adverb
• Adverb modified by adverb
• Verb modified by adverb
</listItem>
<bodyText confidence="0.999550307692308">
We used six attributes to describe a two-word
phrase: first word token, second word token, first
word polarity, second word polarity, first word
part-of-speech (POS), and second word POS.
The machine learning algorithm is expected to
predict the polarity of a word pair given these six
attributes of the pair. To build our machine learn-
ing model, we obtained 961 two-word phrases
from the AfD forum and annotated their polari-
ties manually. They all follow the modifier nega-
tion combinations discussed earlier and at least
one of the two words is a sentiment word. The
selected phrases are balanced in terms of the
</bodyText>
<page confidence="0.995083">
150
</page>
<bodyText confidence="0.999091916666667">
number of positive, negative, and neutral cases
represented in the data set. We then used Weka
(Hall et al., 2009) to evaluate the performance of
three machine learning algorithms with 10-fold
cross validation: Naive Bayes, k-nearest neigh-
bor (KNN) and decision tree. The results showed
that the accuracy produced by KNN is the high-
est among the three methods. We further identi-
fied that when the k value is 1, the KNN perfor-
mance is the best. Thus we selected the KNN
method in detecting modifier negation in our
method.
</bodyText>
<figureCaption confidence="0.8717078">
Figure 2 shows the calculated polarity score
for the sentence “Neither one of us agrees that
the place is notable”.
Figure 2. Polarity score on every node of the sen--‐
tence’ dependency structure
</figureCaption>
<bodyText confidence="0.999934933333333">
As shown in the figure, there are two positive
words agree and notable and one negative word
neither. If we simply use a bag of words ap-
proach and add the polarity scores together, we
would get a result of positive. However, the neg-
ative word neither, being part of the subject,
plays a dominant role in this sentence. Our algo-
rithm is able to detect that negation influence: the
root node is a verb and not neutral, so its current
polarity score is the product of its prior polarity
+1 multiplied by that of the node notable, which
is also +1. Then because of the subject negation,
the final polarity score of the root node is the
multiplication of its current polarity score by the
polarity score of the subject node, which is -1.
</bodyText>
<sectionHeader confidence="0.92773" genericHeader="related work">
4 Evaluation and Discussion
</sectionHeader>
<bodyText confidence="0.999951482758621">
To evaluate the performance of our sentiment
polarity prediction algorithm, we randomly se-
lected 236 sentences from the Wikipedia AfD
forum and manually annotated their sentiment
polarity. 83 sentences are annotated as positive,
102 as negative and 51 as neutral. With our algo-
rithm that includes the machine learning process
to detect modifier negations, the accuracy is
60.2%. In Socher et al.’s (2013) evaluation of
their algorithm, 5-class (very negative, negative,
neutral, positive, very positive) and 2-class (neg-
ative, positive) predictions of sentence-level sen-
timent analysis reached an accuracy of 45.7%
and 85.4% respectively. We anticipate that the
accuracy of their algorithm for 3-class prediction
would be around 60%.
For sentence-level sentiment analysis, Moil-
anen and Pulman’s algorithm obtained an accu-
racy of 65.6%. Our algorithm differs from Moil-
anen and Pulman in two ways: (1) the node-
based computation is more general, i.e. for verbs,
prepositions, and subjects it is a simple combina-
tion (multiplication or addition) of the subordi-
nate nodes&apos; polarities, and for local negation it is
an inversion of the subordinate polarity; (2) a
trained classifier serves two functions: it fulfills
the role of determining the contextual informa-
tion and it determines whether a modifier chang-
es the polarity of what it modifies. .
</bodyText>
<sectionHeader confidence="0.996543" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999947935483871">
Deliberation is a method of logical communica-
tion that rationalizes the process of reaching a
decision. To reach the decision, people often
need to weigh different opinions and rationales
expressed in the deliberation. Given the prolifer-
ation of online platforms and communities for
collective decision making and knowledge crea-
tion, online deliberation is becoming an increas-
ingly important and common approach of engag-
ing large numbers of people to participate in the
decision making processes. One foreseen issue in
such a context is the daunting tasks of reading
through all the deliberation content, and identify-
ing and evaluating diverse key points and related
rationales.
Our study is interested in addressing the issue
through a computational linguistic approach. We
developed an approach that combines a text-to-
text similarity technique with a sentence-level
sentiment analysis method. The deliberation con-
tent is first divided into groups based on the
similarity of texts, then within each group we use
a recursive algorithm to examine the sentiment
polarity of each sentence according to the identi-
fied similar topic to further classify the sentences
into three groups: positive, neutral, and negative.
Although not discussed in this paper, it is a sim-
ple step to identify the representative rationales
of diverse opinions by choosing those that have
the highest similarity score in each polarity
group.
</bodyText>
<page confidence="0.997612">
151
</page>
<sectionHeader confidence="0.93955" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.492546333333333">
This project is partially supported by the Discovery
program of The Natural Sciences and Engineering
Research Council of Canada (NSERC).
</bodyText>
<sectionHeader confidence="0.969352" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999846718446602">
Akshat Bakliwal, Piyush Arora, Senthil Madhappan-
Nikhil Kapre, Mukesh Singh and Vasudeva Varma,
Mining Sentiments from Tweets, Proceedings of
the 3rd Workshop on Computational Approaches
to Subjectivity and Sentiment Analysis. 11–18, Je-
ju, Republic of Korea, 2012
Alexandra Balahur. Sentiment Analysis in Social Me-
dia Texts. WASSA 2013, page 120. 2013.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
The Weka data mining software: an Update. ACM
SIGKDD Explorations Newsletter, 11(1):10–18,
2009.
Minqing Hu and Bing Liu. Mining and summarizing
customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on
Knowledge discovery and data mining, pages 168–
177. ACM, 2004.
Leonard Kaufman and Peter J Rousseeuw. Finding
groups in data: an introduction to cluster analysis,
volume 344. John Wiley &amp; Sons, 2009.
Soo-Min Kim and Eduard Hovy. Determining the
sentiment of opinions. In Proceedings of the 20th
international conference on Computational Lin-
guistics, page 1367. Association for Computational
Linguistics, 2004.
Nan Li, and Desheng Dash Wu. Using text mining
and sentiment analysis for online forums hotspot
detection and forecast. Decision Support Sys-
tems 48(2):354-368. 2010.
Christine Liebrecht, Florian Kunneman, and Antal
van den Bosch (2013). The perfect solution for de-
tecting sarcasm in tweets# not, Proceedings of the
4th Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis,
29–37, Atlanta, Georgia, 2013.
Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI,
volume 6, pages 775–780, 2006.
Karo Moilanen and Stephen Pulman. Sentiment com-
position. In In Proceedings of Recent Advances in
Natural Language Processing, pages 378 – 382,
2007.
Arturo Montejo-Raez, Eugenio Martiõnez-Camara, M.
Teresa Martin-Valdivia and L.Alfonso Urena-
Lopez, Random Walk Weighting over SentiWord-
Net for Sentiment Polarity Detection on Twitter,
Proceedings of the 3rd Workshop on Computation-
al Approaches to Subjectivity and Sentiment Anal-
ysis. 11–18, Jeju, Republic of Korea, 2012.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kuro-
hashi. Dependency tree-based sentiment classifica-
tion using CRFs with hidden variables, In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 786-794,
2010.
Ted Pedersen, Siddharth Patwardhan, and Jason
Michelizzi. Wordnet:: Similarity: measuring the re-
latedness of concepts. In Demonstration Papers at
HLT-NAACL 2004, pages 38–41. Association for
Computational Linguistics, 2004.
Vassiliki Rentoumi, Stefanos Petrakis, Manfred Klen-
ner, George A. Vouros, and Vangelis Karkaletsis.
United we stand: Improving sentiment analysis by
joining machine learning and rule based methods.
In Proceedings of the Seventh conference on Inter-
national Language Resources and Evaluation
(LREC’10), Valletta, Malta. pages 1089 – 1094,
2010.
Vasile Rus and Mihai Lintean. A comparison of
greedy and optimal assessment of natural language
student input using word-to-word similarity met-
rics. In Proceedings of the Seventh Workshop on
Building Educational Applications Using NLP,
pages 157–162. Association for Computational
Linguistics, 2012.
Vasile Rus, Mihai Lintean, Rajendra Banjade, Nobal
Niraula, and Dan Stefanescu. Semilar: The seman-
tic similarity toolkit. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
163–168. 2013.
Jodi Schneider, Alexandre Passant, and Stefan Decker.
Deletion discussions in Wikipedia: Decision fac-
tors and outcomes. In Proceedings of the Eighth
Annual International Symposium on Wikis and
Open Collaboration, page 17. ACM, 2012.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models for
semantic compositionality over a sentiment tree-
bank. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing
(EMNLP), pages 1631–1642, 2013.
Sara Owsley Sood, Elizabeth F. Churchill, and Judd
Antin. Automatic identification of personal insults
on social news sites. J. Am. Soc. Inf. Sci., 63: 270–
285. 2012a.
Sara Sood, Judd Antin, and Elizabeth Churchill. 2012.
Profanity use in online communities. In Proceed-
ings of the SIGCHI Conference on Human Factors
</reference>
<page confidence="0.977975">
152
</page>
<reference confidence="0.997965916666667">
in Computing Systems (CHI &apos;12). ACM, New York,
NY, USA, 1481-1490, 2012b.
Peter D Turney. Thumbs up or thumbs down?: seman-
tic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 417–424. Association for Computa-
tional Linguistics, 2002.
Lu Xiao and Nicole Askin. What influences online
deliberation? A Wikipedia study. Journal of the
Association for Information Science and Technol-
ogy, 65, pages 898–910, 2014
</reference>
<page confidence="0.99923">
153
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.073766">
<title confidence="0.987656">Use of Text Similarity and Sentiment Analysis to Examine tionales in the Large-Scale Online Deliberations</title>
<author confidence="0.576303">Wanting</author>
<affiliation confidence="0.870515">Department of The University of London, ON,</affiliation>
<email confidence="0.998415">fiona.wt.mao@gmail.com</email>
<author confidence="0.941477">Lu</author>
<affiliation confidence="0.951468">Faculty of Information Media The University of</affiliation>
<address confidence="0.47851">London, ON,</address>
<email confidence="0.977649">lxiao24@uwo.ca</email>
<author confidence="0.973152">Robert</author>
<affiliation confidence="0.835228666666667">Department of The University of London, ON,</affiliation>
<email confidence="0.987338">mercer@csd.uwo.ca</email>
<abstract confidence="0.999523142857143">To overcome the increasingly time consuming and potentially challenging identification of key points and the associated rationales in large-scale online deliberations, we propose a computational linguistics method that has the potential of facilitating this process of reading and evaluating the text. Our approach is novel in how we determine the sentiment of a rationale at the sentence level and in that it includes a text similarity measure and sentence-level sentiment analysis to achieve this goal.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Akshat Bakliwal</author>
<author>Piyush Arora</author>
</authors>
<title>Senthil MadhappanNikhil Kapre, Mukesh Singh and Vasudeva Varma, Mining Sentiments from Tweets,</title>
<date>2012</date>
<booktitle>Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis. 11–18, Jeju,</booktitle>
<location>Republic of</location>
<marker>Bakliwal, Arora, 2012</marker>
<rawString>Akshat Bakliwal, Piyush Arora, Senthil MadhappanNikhil Kapre, Mukesh Singh and Vasudeva Varma, Mining Sentiments from Tweets, Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis. 11–18, Jeju, Republic of Korea, 2012</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Balahur</author>
</authors>
<title>Sentiment Analysis in Social Media Texts.</title>
<date>2013</date>
<booktitle>WASSA 2013,</booktitle>
<pages>120</pages>
<contexts>
<context position="6183" citStr="Balahur, 2013" startWordPosition="948" endWordPosition="949">oped to measure text similarity, e.g., the use of the Latent Dirichlet Allocation (LDA) model (Rus et al., 2013). 2.2 Sentiment Analysis Sentiment analysis is meant to determine the polarity of a certain text, which can be positive, negative or neutral. Related academia and industries have been extensively investigating sentiment analysis methods over the last decade. While most of the early work in sentiment analysis is aimed at analyzing the polarity of customer reviews (e.g., Kim and Hovy, 2004; Hu and Liu, 2004; Turney, 2002), there is a proliferation in analyzing social media text (e.g., Balahur, 2013; Liebrecht et al., 2013; Bakliwal et al., 2012; Montejo-Raez et al., 2012) and online discussions (e.g., Sood et al., 2012a, 2012b). Researchers have used a variety of approaches to detect the sentiment polarity of the given text. For example, in Kim and Hovy&apos;s system (2004) the sentiment region of the opinion is identified based on the extracted opinion holders and topics. The system combines the sentiments of the sentiment region and the polarity of the words to determine the polarity of the given text. In Li and Wu’s (2010) study, they interpreted the article as a sequence of key words and</context>
</contexts>
<marker>Balahur, 2013</marker>
<rawString>Alexandra Balahur. Sentiment Analysis in Social Media Texts. WASSA 2013, page 120. 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The Weka data mining software: an Update.</title>
<date>2009</date>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="17322" citStr="Hall et al., 2009" startWordPosition="2740" endWordPosition="2743">cond word polarity, first word part-of-speech (POS), and second word POS. The machine learning algorithm is expected to predict the polarity of a word pair given these six attributes of the pair. To build our machine learning model, we obtained 961 two-word phrases from the AfD forum and annotated their polarities manually. They all follow the modifier negation combinations discussed earlier and at least one of the two words is a sentiment word. The selected phrases are balanced in terms of the 150 number of positive, negative, and neutral cases represented in the data set. We then used Weka (Hall et al., 2009) to evaluate the performance of three machine learning algorithms with 10-fold cross validation: Naive Bayes, k-nearest neighbor (KNN) and decision tree. The results showed that the accuracy produced by KNN is the highest among the three methods. We further identified that when the k value is 1, the KNN performance is the best. Thus we selected the KNN method in detecting modifier negation in our method. Figure 2 shows the calculated polarity score for the sentence “Neither one of us agrees that the place is notable”. Figure 2. Polarity score on every node of the sen--‐ tence’ dependency struc</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. The Weka data mining software: an Update. ACM SIGKDD Explorations Newsletter, 11(1):10–18, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="6090" citStr="Hu and Liu, 2004" startWordPosition="932" endWordPosition="935">thod in terms of accuracy and kappa statistics. Other statistics-based algorithms are also developed to measure text similarity, e.g., the use of the Latent Dirichlet Allocation (LDA) model (Rus et al., 2013). 2.2 Sentiment Analysis Sentiment analysis is meant to determine the polarity of a certain text, which can be positive, negative or neutral. Related academia and industries have been extensively investigating sentiment analysis methods over the last decade. While most of the early work in sentiment analysis is aimed at analyzing the polarity of customer reviews (e.g., Kim and Hovy, 2004; Hu and Liu, 2004; Turney, 2002), there is a proliferation in analyzing social media text (e.g., Balahur, 2013; Liebrecht et al., 2013; Bakliwal et al., 2012; Montejo-Raez et al., 2012) and online discussions (e.g., Sood et al., 2012a, 2012b). Researchers have used a variety of approaches to detect the sentiment polarity of the given text. For example, in Kim and Hovy&apos;s system (2004) the sentiment region of the opinion is identified based on the extracted opinion holders and topics. The system combines the sentiments of the sentiment region and the polarity of the words to determine the polarity of the given t</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge discovery and data mining, pages 168– 177. ACM, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Kaufman</author>
<author>Peter J Rousseeuw</author>
</authors>
<title>Finding groups in data: an introduction to cluster analysis, volume 344.</title>
<date>2009</date>
<publisher>John Wiley &amp; Sons,</publisher>
<contexts>
<context position="13005" citStr="Kaufman and Rousseeuw, 2009" startWordPosition="2037" endWordPosition="2040">t the WordNet-based 149 optimum method achieved the best accuracy of 76.3% at threshold 0.13. The other two methods achieved similar accuracy (76.3% and 75% respectively) but took more than double the time to process. Therefore, we chose the WordNet-based optimum method. With this method, we have a similarity matrix that shows the similarity score between every pair of sentences in the discussion. We transform the similarity matrix to a dissimilarity matrix by transforming the similarity score x for two sentences to the distance between the sentences 1/x. Then we used hierarchical clustering (Kaufman and Rousseeuw, 2009) to cluster the sentences into groups. To do so, we set the maximum allowed distance between two similar sentences to be 8 (i.e., the similarity score would be 0.125), and used the agglomerative approach to form the clusters. As a consequence, the sentences in the same group are related to a common theme. 3.2 Sentence-Level Sentiment Analysis In our sentiment analysis algorithm, each word in a sentence is assigned a prior polarity based on an adapted MPQA Subjectivity Lexicon (Pedersen et al., 2004). Compared to the original Lexicon, this adapted one includes additional sentiment words that ar</context>
</contexts>
<marker>Kaufman, Rousseeuw, 2009</marker>
<rawString>Leonard Kaufman and Peter J Rousseeuw. Finding groups in data: an introduction to cluster analysis, volume 344. John Wiley &amp; Sons, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>1367</pages>
<contexts>
<context position="6072" citStr="Kim and Hovy, 2004" startWordPosition="928" endWordPosition="931">formed the greedy method in terms of accuracy and kappa statistics. Other statistics-based algorithms are also developed to measure text similarity, e.g., the use of the Latent Dirichlet Allocation (LDA) model (Rus et al., 2013). 2.2 Sentiment Analysis Sentiment analysis is meant to determine the polarity of a certain text, which can be positive, negative or neutral. Related academia and industries have been extensively investigating sentiment analysis methods over the last decade. While most of the early work in sentiment analysis is aimed at analyzing the polarity of customer reviews (e.g., Kim and Hovy, 2004; Hu and Liu, 2004; Turney, 2002), there is a proliferation in analyzing social media text (e.g., Balahur, 2013; Liebrecht et al., 2013; Bakliwal et al., 2012; Montejo-Raez et al., 2012) and online discussions (e.g., Sood et al., 2012a, 2012b). Researchers have used a variety of approaches to detect the sentiment polarity of the given text. For example, in Kim and Hovy&apos;s system (2004) the sentiment region of the opinion is identified based on the extracted opinion holders and topics. The system combines the sentiments of the sentiment region and the polarity of the words to determine the polar</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-Min Kim and Eduard Hovy. Determining the sentiment of opinions. In Proceedings of the 20th international conference on Computational Linguistics, page 1367. Association for Computational Linguistics, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Li</author>
<author>Desheng Dash Wu</author>
</authors>
<title>Using text mining and sentiment analysis for online forums hotspot detection and forecast. Decision Support Systems 48(2):354-368.</title>
<date>2010</date>
<marker>Li, Wu, 2010</marker>
<rawString>Nan Li, and Desheng Dash Wu. Using text mining and sentiment analysis for online forums hotspot detection and forecast. Decision Support Systems 48(2):354-368. 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Liebrecht</author>
<author>Florian Kunneman</author>
<author>Antal van den Bosch</author>
</authors>
<title>The perfect solution for detecting sarcasm in tweets# not,</title>
<date>2013</date>
<booktitle>Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis,</booktitle>
<location>29–37, Atlanta, Georgia,</location>
<marker>Liebrecht, Kunneman, van den Bosch, 2013</marker>
<rawString>Christine Liebrecht, Florian Kunneman, and Antal van den Bosch (2013). The perfect solution for detecting sarcasm in tweets# not, Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, 29–37, Atlanta, Georgia, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In AAAI,</booktitle>
<volume>6</volume>
<pages>775--780</pages>
<contexts>
<context position="4784" citStr="Mihalcea et al. (2006)" startWordPosition="708" endWordPosition="711"> retrieval, text classification, and automatic evaluation. The most fundamental part is word similarity. We consider words to be similar in the following conditions: synonyms, antonyms, similar concept (e.g., red, green), similar context (e.g., doctor, hospital), and hyponym/hypernym relation (e.g., dog, pet). WordNet, a word-to-word similarity library was developed by Pedersen et al. (2004), and has been widely used to compute the similarity at a coarser granularity (e.g., sentence-to-sentence similarity). Various methods to deal with text similarity have been proposed over the past decades. Mihalcea et al. (2006) proposed a greedy method to calculate the similarity score between two texts T1 and T2. Basically for each word in T1 (T2), the maximum similarity score to any word in T2 (T1) is used. The WordNet similarity can be used for assigning similarity scores between every pair of words in the two texts. Rus and Lintean (2012) proposed an optimal method to compute text similarity based on word-to-word similarity. It is similar to the optimal assignment problem. Given a weighted complete bipartite graph (G = X È Y; X × Y), with weight w(xy) on edge xy, we need to find a matching from X to Y with a max</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. Corpus-based and knowledge-based measures of text semantic similarity. In AAAI, volume 6, pages 775–780, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karo Moilanen</author>
<author>Stephen Pulman</author>
</authors>
<title>Sentiment composition. In</title>
<date>2007</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing,</booktitle>
<pages>378--382</pages>
<contexts>
<context position="7998" citStr="Moilanen and Pulman (2007)" startWordPosition="1233" endWordPosition="1236">Sood et al. (2012a) proposed a multistep classifier by combining valence analysis and a SVM to detect insults and classify the insult object. Researchers have also looked at the use of dependency tree-based method for sentiment classification. For instance, Nakagawa et al. (2010) used a probabilistic model of the information garnered from the dependency tree to determine the sentiment of a sentence. Rentoumi et al. (2010) combines word sense disambiguation, a rule-based system, and Hidden Markov Models (HMMs) to deal with figurative language (e.g. record-shattering day) in sentiment analysis. Moilanen and Pulman (2007) presented a compositional model for three-class (positive, negative, and neutral) phrase-level and sentence-level sentiment analysis. In their algorithm, each binary combination of a Head and Complement had a rule that determined which of the Head and Complement polarities dominated. In exceptional cases the rule inverts the polarity of the subordinate. Socher et al. (2013) developed a Recursive Neural Tensor Network (RNTN) model. The authors showed that the accuracy obtained by RNTN outperformed a standard recursive neural network (RNN), matrix-vector RNN (MV-RNN), Naive Bayes (NB) and SVM. </context>
</contexts>
<marker>Moilanen, Pulman, 2007</marker>
<rawString>Karo Moilanen and Stephen Pulman. Sentiment composition. In In Proceedings of Recent Advances in Natural Language Processing, pages 378 – 382, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arturo Montejo-Raez</author>
<author>Eugenio Martiõnez-Camara</author>
<author>M</author>
</authors>
<title>Teresa Martin-Valdivia and L.Alfonso UrenaLopez, Random Walk Weighting over SentiWordNet for Sentiment Polarity Detection on Twitter,</title>
<date>2012</date>
<booktitle>Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis. 11–18, Jeju,</booktitle>
<location>Republic of</location>
<contexts>
<context position="6258" citStr="Montejo-Raez et al., 2012" startWordPosition="958" endWordPosition="961">irichlet Allocation (LDA) model (Rus et al., 2013). 2.2 Sentiment Analysis Sentiment analysis is meant to determine the polarity of a certain text, which can be positive, negative or neutral. Related academia and industries have been extensively investigating sentiment analysis methods over the last decade. While most of the early work in sentiment analysis is aimed at analyzing the polarity of customer reviews (e.g., Kim and Hovy, 2004; Hu and Liu, 2004; Turney, 2002), there is a proliferation in analyzing social media text (e.g., Balahur, 2013; Liebrecht et al., 2013; Bakliwal et al., 2012; Montejo-Raez et al., 2012) and online discussions (e.g., Sood et al., 2012a, 2012b). Researchers have used a variety of approaches to detect the sentiment polarity of the given text. For example, in Kim and Hovy&apos;s system (2004) the sentiment region of the opinion is identified based on the extracted opinion holders and topics. The system combines the sentiments of the sentiment region and the polarity of the words to determine the polarity of the given text. In Li and Wu’s (2010) study, they interpreted the article as a sequence of key words and calculated the sentiment score of each key word based on the dictionary an</context>
</contexts>
<marker>Montejo-Raez, Martiõnez-Camara, M, 2012</marker>
<rawString>Arturo Montejo-Raez, Eugenio Martiõnez-Camara, M. Teresa Martin-Valdivia and L.Alfonso UrenaLopez, Random Walk Weighting over SentiWordNet for Sentiment Polarity Detection on Twitter, Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis. 11–18, Jeju, Republic of Korea, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kentaro Inui</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Dependency tree-based sentiment classification using CRFs with hidden variables,</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>786--794</pages>
<contexts>
<context position="7652" citStr="Nakagawa et al. (2010)" startWordPosition="1183" endWordPosition="1186"> positive and high negative) or modification labels (negator, intensifier or diminisher), and then applied Support Vector Machine Sequential Minimal Optimization (SVM SMO) to classify three different data sets. Online discussions may have inappropriate use of language in some cases, which affects the online community management negatively. Sood et al. (2012a) proposed a multistep classifier by combining valence analysis and a SVM to detect insults and classify the insult object. Researchers have also looked at the use of dependency tree-based method for sentiment classification. For instance, Nakagawa et al. (2010) used a probabilistic model of the information garnered from the dependency tree to determine the sentiment of a sentence. Rentoumi et al. (2010) combines word sense disambiguation, a rule-based system, and Hidden Markov Models (HMMs) to deal with figurative language (e.g. record-shattering day) in sentiment analysis. Moilanen and Pulman (2007) presented a compositional model for three-class (positive, negative, and neutral) phrase-level and sentence-level sentiment analysis. In their algorithm, each binary combination of a Head and Complement had a rule that determined which of the Head and C</context>
</contexts>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. Dependency tree-based sentiment classification using CRFs with hidden variables, In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 786-794, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet:: Similarity: measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Demonstration Papers at HLT-NAACL 2004,</booktitle>
<pages>38--41</pages>
<contexts>
<context position="4556" citStr="Pedersen et al. (2004)" startWordPosition="673" endWordPosition="676">agraph to paragraph) could help people better understand the context. Text similarity can be interpreted as similarity between sentences, paragraphs, documents, etc. It has been used in various aspects in NLP such as information retrieval, text classification, and automatic evaluation. The most fundamental part is word similarity. We consider words to be similar in the following conditions: synonyms, antonyms, similar concept (e.g., red, green), similar context (e.g., doctor, hospital), and hyponym/hypernym relation (e.g., dog, pet). WordNet, a word-to-word similarity library was developed by Pedersen et al. (2004), and has been widely used to compute the similarity at a coarser granularity (e.g., sentence-to-sentence similarity). Various methods to deal with text similarity have been proposed over the past decades. Mihalcea et al. (2006) proposed a greedy method to calculate the similarity score between two texts T1 and T2. Basically for each word in T1 (T2), the maximum similarity score to any word in T2 (T1) is used. The WordNet similarity can be used for assigning similarity scores between every pair of words in the two texts. Rus and Lintean (2012) proposed an optimal method to compute text similar</context>
<context position="13509" citStr="Pedersen et al., 2004" startWordPosition="2121" endWordPosition="2124">entences to the distance between the sentences 1/x. Then we used hierarchical clustering (Kaufman and Rousseeuw, 2009) to cluster the sentences into groups. To do so, we set the maximum allowed distance between two similar sentences to be 8 (i.e., the similarity score would be 0.125), and used the agglomerative approach to form the clusters. As a consequence, the sentences in the same group are related to a common theme. 3.2 Sentence-Level Sentiment Analysis In our sentiment analysis algorithm, each word in a sentence is assigned a prior polarity based on an adapted MPQA Subjectivity Lexicon (Pedersen et al., 2004). Compared to the original Lexicon, this adapted one includes additional sentiment words that are important for the Wikipedia’s AfD discussions (e.g., notable). Then, using the syntactic and dependency trees of the sentence, the algorithm calculates each word’s current polarity score which can be affected by its children’s polarity scores. Through this approach, the root’s current polarity score becomes the sentence’s polarity score. The children’s polarity scores can affect the parent’s prior polarity score positively or negatively. The positive or neutral effect of the children’s polarity sc</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. Wordnet:: Similarity: measuring the relatedness of concepts. In Demonstration Papers at HLT-NAACL 2004, pages 38–41. Association for Computational Linguistics, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vassiliki Rentoumi</author>
<author>Stefanos Petrakis</author>
<author>Manfred Klenner</author>
<author>George A Vouros</author>
<author>Vangelis Karkaletsis</author>
</authors>
<title>United we stand: Improving sentiment analysis by joining machine learning and rule based methods.</title>
<date></date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<pages>1089--1094</pages>
<location>Valletta,</location>
<marker>Rentoumi, Petrakis, Klenner, Vouros, Karkaletsis, </marker>
<rawString>Vassiliki Rentoumi, Stefanos Petrakis, Manfred Klenner, George A. Vouros, and Vangelis Karkaletsis. United we stand: Improving sentiment analysis by joining machine learning and rule based methods. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta. pages 1089 – 1094, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
<author>Mihai Lintean</author>
</authors>
<title>A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>157--162</pages>
<contexts>
<context position="5105" citStr="Rus and Lintean (2012)" startWordPosition="766" endWordPosition="769">a word-to-word similarity library was developed by Pedersen et al. (2004), and has been widely used to compute the similarity at a coarser granularity (e.g., sentence-to-sentence similarity). Various methods to deal with text similarity have been proposed over the past decades. Mihalcea et al. (2006) proposed a greedy method to calculate the similarity score between two texts T1 and T2. Basically for each word in T1 (T2), the maximum similarity score to any word in T2 (T1) is used. The WordNet similarity can be used for assigning similarity scores between every pair of words in the two texts. Rus and Lintean (2012) proposed an optimal method to compute text similarity based on word-to-word similarity. It is similar to the optimal assignment problem. Given a weighted complete bipartite graph (G = X È Y; X × Y), with weight w(xy) on edge xy, we need to find a matching from X to Y with a maximum total weight. Their results showed that the optimal method outperformed the greedy method in terms of accuracy and kappa statistics. Other statistics-based algorithms are also developed to measure text similarity, e.g., the use of the Latent Dirichlet Allocation (LDA) model (Rus et al., 2013). 2.2 Sentiment Analysi</context>
</contexts>
<marker>Rus, Lintean, 2012</marker>
<rawString>Vasile Rus and Mihai Lintean. A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 157–162. Association for Computational Linguistics, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
<author>Mihai Lintean</author>
<author>Rajendra Banjade</author>
<author>Nobal Niraula</author>
<author>Dan Stefanescu</author>
</authors>
<title>Semilar: The semantic similarity toolkit.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>163--168</pages>
<contexts>
<context position="5682" citStr="Rus et al., 2013" startWordPosition="865" endWordPosition="868">in the two texts. Rus and Lintean (2012) proposed an optimal method to compute text similarity based on word-to-word similarity. It is similar to the optimal assignment problem. Given a weighted complete bipartite graph (G = X È Y; X × Y), with weight w(xy) on edge xy, we need to find a matching from X to Y with a maximum total weight. Their results showed that the optimal method outperformed the greedy method in terms of accuracy and kappa statistics. Other statistics-based algorithms are also developed to measure text similarity, e.g., the use of the Latent Dirichlet Allocation (LDA) model (Rus et al., 2013). 2.2 Sentiment Analysis Sentiment analysis is meant to determine the polarity of a certain text, which can be positive, negative or neutral. Related academia and industries have been extensively investigating sentiment analysis methods over the last decade. While most of the early work in sentiment analysis is aimed at analyzing the polarity of customer reviews (e.g., Kim and Hovy, 2004; Hu and Liu, 2004; Turney, 2002), there is a proliferation in analyzing social media text (e.g., Balahur, 2013; Liebrecht et al., 2013; Bakliwal et al., 2012; Montejo-Raez et al., 2012) and online discussions </context>
<context position="11548" citStr="Rus et al., 2013" startWordPosition="1800" endWordPosition="1803">text similarity measure combined with our sentence-level sentiment analysis algorithm helps us identify the representative rationales of diverse opinions in an online deliberation. An overview of our method is shown in Figure 1. We applied our method in analyzing Wikipedia Article for Deletion (AfD) deliberation content. Next we discuss how this method is used to analyze the content. Figure 1. An overview of our method for identifying representative rationales from large-scale online deliberation 3.1 Text-to-Text Similarity Measure In our study, we used SEMILAR, a semantic similarity toolkit (Rus et al., 2013), to measure We tested three similarity approaches provided in SEMILAR: optimum method based on WordNet, similarity based on Latent Semantic Analysis (LSA) and similarity based on Latent Dirichlet Analysis (LDA). We first extracted 80 pairs of sentences from the Wikipedia AfD forum and manually annotated them as similar or not. We then used these annotated results in measuring the accuracy of the three SEMILAR approaches. SEMILAR assigns a similarity score to each pair of sentences ranging from 0 to 1. To evaluate the accuracy of the three approaches, we identified a threshold to divide the re</context>
</contexts>
<marker>Rus, Lintean, Banjade, Niraula, Stefanescu, 2013</marker>
<rawString>Vasile Rus, Mihai Lintean, Rajendra Banjade, Nobal Niraula, and Dan Stefanescu. Semilar: The semantic similarity toolkit. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 163–168. 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jodi Schneider</author>
<author>Alexandre Passant</author>
<author>Stefan Decker</author>
</authors>
<title>Deletion discussions in Wikipedia: Decision factors and outcomes.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eighth Annual International Symposium on Wikis and Open Collaboration,</booktitle>
<pages>17</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="2180" citStr="Schneider et al., 2012" startWordPosition="325" endWordPosition="328">about the article is opened in the AfD forum. The community members can express their opinions (e.g., to keep or to delete the article) and provide their rationales within the specified time period. After that, a community member (often a Wikipedia administrator) closes the deliberation by making the final decision. Researchers have analyzed the Wikipedia AfD forum and have demonstrated that it presents a successful example of largescale online deliberation by allowing many people to participate equally, encouraging people to deliberate, and producing rational and meaningful rationales (e.g., Schneider et al., 2012; Xiao &amp; Askin, 2014). Wikipedia policy requires that the final decision about the article should be made based on the discussed rationales instead of the count of opinion votes. In practice many Wikipedia members who close the deliberations follow this policy, which implies the potential problem of representing the diverse rationales and identifying the influential ones in this context. Generating the final decision of a large scale online deliberation can become a daunting task, as the amount of opinions and rationales in the deliberation content increases significantly. To facilitate this d</context>
</contexts>
<marker>Schneider, Passant, Decker, 2012</marker>
<rawString>Jodi Schneider, Alexandre Passant, and Stefan Decker. Deletion discussions in Wikipedia: Decision factors and outcomes. In Proceedings of the Eighth Annual International Symposium on Wikis and Open Collaboration, page 17. ACM, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="8375" citStr="Socher et al. (2013)" startWordPosition="1291" endWordPosition="1294">ment of a sentence. Rentoumi et al. (2010) combines word sense disambiguation, a rule-based system, and Hidden Markov Models (HMMs) to deal with figurative language (e.g. record-shattering day) in sentiment analysis. Moilanen and Pulman (2007) presented a compositional model for three-class (positive, negative, and neutral) phrase-level and sentence-level sentiment analysis. In their algorithm, each binary combination of a Head and Complement had a rule that determined which of the Head and Complement polarities dominated. In exceptional cases the rule inverts the polarity of the subordinate. Socher et al. (2013) developed a Recursive Neural Tensor Network (RNTN) model. The authors showed that the accuracy obtained by RNTN outperformed a standard recursive neural network (RNN), matrix-vector RNN (MV-RNN), Naive Bayes (NB) and SVM. The advantage of 148 RNTN is especially evident when compared with the methods that only use bag of words (NB and SVM). This indicates the importance of using parse trees during sentiment analysis. 3 A Method for Identifying Representative Rationales in Online Deliberations Our observation of the Wikipedia AfD forum suggests that one topic (e.g., notability) can appear multi</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642, 2013.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Sara Owsley Sood</author>
<author>Elizabeth F Churchill</author>
<author>Judd Antin</author>
</authors>
<title>Automatic identification of personal insults on social news sites.</title>
<journal>J. Am. Soc. Inf. Sci.,</journal>
<volume>63</volume>
<pages>270--285</pages>
<marker>Sood, Churchill, Antin, </marker>
<rawString>Sara Owsley Sood, Elizabeth F. Churchill, and Judd Antin. Automatic identification of personal insults on social news sites. J. Am. Soc. Inf. Sci., 63: 270– 285. 2012a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Sood</author>
<author>Judd Antin</author>
<author>Elizabeth Churchill</author>
</authors>
<title>Profanity use in online communities.</title>
<date>2012</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;12).</booktitle>
<publisher>ACM,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="6306" citStr="Sood et al., 2012" startWordPosition="967" endWordPosition="970">entiment Analysis Sentiment analysis is meant to determine the polarity of a certain text, which can be positive, negative or neutral. Related academia and industries have been extensively investigating sentiment analysis methods over the last decade. While most of the early work in sentiment analysis is aimed at analyzing the polarity of customer reviews (e.g., Kim and Hovy, 2004; Hu and Liu, 2004; Turney, 2002), there is a proliferation in analyzing social media text (e.g., Balahur, 2013; Liebrecht et al., 2013; Bakliwal et al., 2012; Montejo-Raez et al., 2012) and online discussions (e.g., Sood et al., 2012a, 2012b). Researchers have used a variety of approaches to detect the sentiment polarity of the given text. For example, in Kim and Hovy&apos;s system (2004) the sentiment region of the opinion is identified based on the extracted opinion holders and topics. The system combines the sentiments of the sentiment region and the polarity of the words to determine the polarity of the given text. In Li and Wu’s (2010) study, they interpreted the article as a sequence of key words and calculated the sentiment score of each key word based on the dictionary and its privative and modifier near it. In the ana</context>
</contexts>
<marker>Sood, Antin, Churchill, 2012</marker>
<rawString>Sara Sood, Judd Antin, and Elizabeth Churchill. 2012. Profanity use in online communities. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;12). ACM, New York, NY, USA, 1481-1490, 2012b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="6105" citStr="Turney, 2002" startWordPosition="936" endWordPosition="937">ccuracy and kappa statistics. Other statistics-based algorithms are also developed to measure text similarity, e.g., the use of the Latent Dirichlet Allocation (LDA) model (Rus et al., 2013). 2.2 Sentiment Analysis Sentiment analysis is meant to determine the polarity of a certain text, which can be positive, negative or neutral. Related academia and industries have been extensively investigating sentiment analysis methods over the last decade. While most of the early work in sentiment analysis is aimed at analyzing the polarity of customer reviews (e.g., Kim and Hovy, 2004; Hu and Liu, 2004; Turney, 2002), there is a proliferation in analyzing social media text (e.g., Balahur, 2013; Liebrecht et al., 2013; Bakliwal et al., 2012; Montejo-Raez et al., 2012) and online discussions (e.g., Sood et al., 2012a, 2012b). Researchers have used a variety of approaches to detect the sentiment polarity of the given text. For example, in Kim and Hovy&apos;s system (2004) the sentiment region of the opinion is identified based on the extracted opinion holders and topics. The system combines the sentiments of the sentiment region and the polarity of the words to determine the polarity of the given text. In Li and </context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D Turney. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 417–424. Association for Computational Linguistics, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu Xiao</author>
<author>Nicole Askin</author>
</authors>
<title>What influences online deliberation? A Wikipedia study.</title>
<date>2014</date>
<journal>Journal of the Association for Information Science and Technology,</journal>
<volume>65</volume>
<pages>898--910</pages>
<contexts>
<context position="2201" citStr="Xiao &amp; Askin, 2014" startWordPosition="329" endWordPosition="332">ned in the AfD forum. The community members can express their opinions (e.g., to keep or to delete the article) and provide their rationales within the specified time period. After that, a community member (often a Wikipedia administrator) closes the deliberation by making the final decision. Researchers have analyzed the Wikipedia AfD forum and have demonstrated that it presents a successful example of largescale online deliberation by allowing many people to participate equally, encouraging people to deliberate, and producing rational and meaningful rationales (e.g., Schneider et al., 2012; Xiao &amp; Askin, 2014). Wikipedia policy requires that the final decision about the article should be made based on the discussed rationales instead of the count of opinion votes. In practice many Wikipedia members who close the deliberations follow this policy, which implies the potential problem of representing the diverse rationales and identifying the influential ones in this context. Generating the final decision of a large scale online deliberation can become a daunting task, as the amount of opinions and rationales in the deliberation content increases significantly. To facilitate this decision making proces</context>
</contexts>
<marker>Xiao, Askin, 2014</marker>
<rawString>Lu Xiao and Nicole Askin. What influences online deliberation? A Wikipedia study. Journal of the Association for Information Science and Technology, 65, pages 898–910, 2014</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
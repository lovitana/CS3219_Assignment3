<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000265">
<note confidence="0.83733">
BEER: BEtter Evaluation as Ranking
Miloˇs Stanojevi´c Khalil Sima’an
</note>
<sectionHeader confidence="0.290101" genericHeader="abstract">
ILLC ILLC
</sectionHeader>
<affiliation confidence="0.961824">
University of Amsterdam University of Amsterdam
</affiliation>
<email confidence="0.987066">
mstanojevic@uva.nl k.simaan@uva.nl
</email>
<sectionHeader confidence="0.99451" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.9999889">
We present the UvA-ILLC submission of
the BEER metric to WMT 14 metrics task.
BEER is a sentence level metric that can
incorporate a large number of features
combined in a linear model. Novel con-
tributions are (1) efficient tuning of a large
number of features for maximizing corre-
lation with human system ranking, and (2)
novel features that give smoother sentence
level scores.
</bodyText>
<sectionHeader confidence="0.998424" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99966">
The quality of sentence level (also called segment
level) evaluation metrics in machine translation is
often considered inferior to the quality of corpus
(or system) level metrics. Yet, a sentence level
metrics has important advantages as it:
</bodyText>
<listItem confidence="0.991068666666667">
1. provides an informative score to individual
translations
2. is assumed by MT tuning algorithms (Hop-
kins and May, 2011).
3. facilitates easier statistical testing using sign
test or t-test (Collins et al., 2005)
</listItem>
<bodyText confidence="0.998955633333333">
We think that the root cause for most of the diffi-
culty in creating a good sentence level metric is the
sparseness of the features often used. Consider the
n-gram counting metrics (BLEU (Papineni et al.,
2002)): counts of higher order n-grams are usu-
ally rather small, if not zero, when counted at the
individual sentence level. Metrics based on such
counts are brittle at the sentence level even when
they might be good at the corpus level. Ideally we
should have features of varying granularity that we
can optimize on the actual evaluation task: relative
ranking of system outputs.
Therefore, in this paper we explore two kinds of
less sparse features:
Character n-grams are features at the sub-word
level that provide evidence for translation ad-
equacy - for example whether the stem is cor-
rectly translated,
Abstract ordering patterns found in tree factor-
izations of permutations into Permutation
Trees (PETs) (Zhang and Gildea, 2007), in-
cluding non-lexical alignment patterns.
The BEER metric combines features of both kinds
(presented in Section 2).
With the growing number of adequacy and or-
dering features we need a model that facilitates ef-
ficient training. We would like to train for opti-
mal Kendall τ correlation with rankings by human
evaluators. The models in the literature tackle this
problem by
</bodyText>
<listItem confidence="0.965665857142857">
1. training for another similar objective – e.g.,
tuning for absolute adequacy and fluency
scores instead on rankings, or
2. training for rankings directly but with meta-
heuristic approaches like hill-climbing, or
3. training for pairwise rankings using learning-
to-rank techniques
</listItem>
<bodyText confidence="0.999060142857143">
Approach (1) has two disadvantages. One is the
inconsistency between the training and the testing
objectives. The other, is that absolute rankings are
not reliable enough because humans are better at
giving relative than absolute judgments (see WMT
manual evaluations (Callison-Burch et al., 2007)).
Approach (2) does not allow integrating a large
number of features which makes it less attractive.
Approach (3) allows integration of a large num-
ber of features whose weights could be determined
in an elegant machine learning framework. The
output of learning in this approach can be either a
function that ranks all hypotheses directly (global
ranking model) or a function that assigns a score
</bodyText>
<page confidence="0.97897">
414
</page>
<bodyText confidence="0.882663636363636">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 414–419,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
to each hypothesis individually which can be used
for ranking (local ranking model) (Li, 2011). Lo-
cal ranking models are preferable because they
provide absolute distance between hypotheses like
most existing evaluation metrics.
In this paper we follow the learning-to-rank ap-
proach which produces a local ranking model in a
similar way to PRO MT systems tuning (Hopkins
and May, 2011).
</bodyText>
<sectionHeader confidence="0.970735" genericHeader="method">
2 Model
</sectionHeader>
<bodyText confidence="0.999965666666667">
Our model is a fairly simple linear interpolation of
feature functions, which is easy to train and simple
to interpret. The model determines the similarity
of the hypothesis h to the reference translation r
by assigning a weight wi to each feature Oi(h, r).
The linear scoring function is given by:
</bodyText>
<equation confidence="0.990571">
�score(h,r) = wi × Oi(h,r) = w�· φ�
i
</equation>
<subsectionHeader confidence="0.995016">
2.1 Adequacy features
</subsectionHeader>
<bodyText confidence="0.9954405">
The features used are precision P, recall R and
F1-score F for different counts:
</bodyText>
<equation confidence="0.392728333333333">
Pfunction, Rfunction, Ffunction on matched func-
tion words
Pcontent, Rcontent, Fcontent on matched content
words (all non-function words)
Pall, Rall, Fall on matched words of any type
Pchar n−gram, Rchar n−gram, Fchar n−gram
</equation>
<bodyText confidence="0.998388944444444">
matching of the character n-grams
By differentiating function and non-function
words we might have a better estimate of which
words are more important and which are less. The
last, but as we will see later the most important,
adequacy feature is matching character n-grams,
originally proposed in (Yang et al., 2013). This
can reward some translations even if they did not
get the morphology completely right. Many met-
rics solve this problem by using stemmers, but us-
ing features based on character n-grams is more
robust since it does not depend on the quality
of the stemmer. For character level n-grams we
can afford higher-order n-grams with less risk of
sparse counts as on word n-grams. In our exper-
iments we used character n-grams for size up to
6 which makes the total number of all adequacy
features 27.
</bodyText>
<subsectionHeader confidence="0.999796">
2.2 Ordering features
</subsectionHeader>
<bodyText confidence="0.999782636363637">
To evaluate word order we follow (Isozaki et al.,
2010; Birch and Osborne, 2010) in representing
reordering as a permutation and then measuring
the distance to the ideal monotone permutation.
Here we take one feature from previous work –
Kendall τ distance from the monotone permuta-
tion. This metrics on the permutation level has
been shown to have high correlation with human
judgment on language pairs with very different
word order.
Additionally, we add novel features with an
even less sparse view of word order by exploiting
hierarchical structure that exists in permutations
(Zhang and Gildea, 2007). The trees that represent
this structure are called PETs (PErmutation Trees
– see the next subsection). Metrics defined over
PETs usually have a better estimate of long dis-
tance reorderings (Stanojevi´c and Sima’an, 2013).
Here we use simple versions of these metrics:
Δcount the ratio between the number of different
permutation trees (PETs) (Zhang and Gildea,
2007) that could be built for the given per-
mutation over the number of trees that could
be built if permutation was completely mono-
tone (there is a perfect word order).
Δ[ ] ratio of the number of monotone nodes in
a PET to the maximum possible number of
nodes – the lenght of the sentence n.
Δ&lt;&gt; ratio of the number of inverted nodes to n
Δ=4 ratio of the number of nodes with branching
factor 4 to n
Δ&gt;4 ratio of the number of nodes with branching
factor bigger than 4 to n
</bodyText>
<subsectionHeader confidence="0.999385">
2.3 Why features based on PETs?
</subsectionHeader>
<bodyText confidence="0.999934583333333">
PETs are recursive factorizations of permutations
into their minimal units. We refer the reader to
(Zhang and Gildea, 2007) for formal treatment of
PETs and efficient algorithms for their construc-
tion. Here we present them informally to exploit
them for presenting novel ordering metrics.
A PET is a tree structure with the nodes deco-
rated with operators (like in ITG) that are them-
selves permutations that cannot be factorized any
further into contiguous sub-parts (called opera-
tors). As an example, see the PET in Figure 1a.
This PET has one 4-branching node, one inverted
</bodyText>
<page confidence="0.935729">
415
</page>
<equation confidence="0.93758">
h2, 4,1, 3i h2, 1i
h1,2i
1
h2, 1i
2
h2, 1i
2 h2, 1i
1 3
h2,1i
h2,1i
</equation>
<figure confidence="0.71043475">
4
h1, 2i
5 6 2 1 4 3 4 3
(a) Complex PET (b) PET with inversions (c) Fully inverted PET
</figure>
<figureCaption confidence="0.999974">
Figure 1: Examples of PETs
</figureCaption>
<bodyText confidence="0.999921304347826">
node and one monotone. The nodes are decorated
by operators that stand for a permutation of the
direct children of the node.
PETs have two important properties that make
them attractive for observing ordering: firstly, the
PET operators show the minimal units of ordering
that constitute the permutation itself, and secondly
the higher level operators capture hidden patterns
of ordering that cannot be observed without fac-
torization. Statistics over patterns of ordering us-
ing PETs are non-lexical and hence far less sparse
than word or character n-gram statistics.
In PETs, the minimal operators on the node
stand for ordering that cannot be broken down any
further. The binary monotone operator is the sim-
plest, binary inverted is the second in line, fol-
lowed by operators of length four like h2, 4, 1, 3i
(Wu, 1997), and then operators longer than four.
The larger the branching factor under a PET node
(the length of the operator on that node) the more
complex the ordering. Hence, we devise possi-
ble branching feature functions over the operator
length for the nodes in PETs:
</bodyText>
<listItem confidence="0.999259">
• factor 2 - with two features: Δ[ ]and Δ&lt;&gt;
(there are no nodes with factor 3 (Wu, 1997))
• factor 4 - feature Δ=4
• factor bigger than 4 - feature Δ&gt;4
</listItem>
<bodyText confidence="0.99938615">
All of the mentioned PETs node features, except
Δ[ ]and Δcount, signify the wrong word order but
of different magnitude. Ideally all nodes in a PET
would be binary monotone, but when that is not
the case we are able to quantify how far we are
from that ideal binary monotone PET.
In contrast with word n-grams used in other
metrics, counts over PET operators are far less
sparse on the sentence level and could be more
reliable. Consider permutations 2143 and 4321
and their corresponding PETs in Figure 1b and
1c. None of them has any exact n-gram matched
(we ignore unigrams now). But, it is clear that
2143 is somewhat better since it has at least some
words in more or less the right order. These “ab-
stract n-grams” pertaining to correct ordering of
full phrases could be counted using Δ[ ]which
would recognize that on top of the PET in 1b there
is the monotone node unlike the PET in 1c which
has no monotone nodes at all.
</bodyText>
<sectionHeader confidence="0.998094" genericHeader="method">
3 Tuning for human judgment
</sectionHeader>
<bodyText confidence="0.994277">
The task of correlation with human judgment on
the sentence level is usually posed in the following
way (Mach´aˇcek and Bojar, 2013):
</bodyText>
<listItem confidence="0.996732142857143">
• Translate all source sentences using the avail-
able machine translation systems
• Let human evaluators rank them by quality
compared to the reference translation
• Each evaluation metric should do the same
task of ranking the hypothesis translations
• The metric with higher Kendall T correlation
</listItem>
<bodyText confidence="0.966657375">
with human judgment is considered better
Let us take any pair of hypotheses that have the
same reference r where one is better (hgood) than
the other one (hbad) as judged by human evaluator.
In order for our metric to give the same ranking as
human judges do, it needs to give the higher score
to the hgood hypothesis. Given that our model is
linear we can derive:
</bodyText>
<equation confidence="0.9979392">
score(hgood, r) &gt; score(hbad, r) ⇔
w~· ~φgood &gt; w~ · ~φbad ⇔
w~· ~φgood − w~ · ~φbad &gt; 0 ⇔
w~ · (~φgood − ~φbad) &gt; 0
w~ · (~φbad − ~φgood) C 0
</equation>
<bodyText confidence="0.999919">
The most important part here are the last two
equations. Using them we formulate ranking prob-
lem as a problem of binary classification: the pos-
itive training instance would have feature values
</bodyText>
<page confidence="0.996219">
416
</page>
<bodyText confidence="0.904944">
~φgood − ~φbad and the negative training instance
would have feature values ~φbad− ~φgood. This trick
was used in PRO (Hopkins and May, 2011) but for
the different task:
</bodyText>
<listItem confidence="0.999243">
• tuning the model of the SMT system
• objective function was an evaluation metric
</listItem>
<bodyText confidence="0.999729777777778">
Given this formulation of the training instances
we can train the classifier using pairs of hypothe-
ses. Note that even though it uses pairs of hypothe-
ses for training in the evaluation time it uses only
one hypothesis – it does not require the pair of hy-
potheses to compare them. The score of the classi-
fier is interpreted as confidence that the hypothesis
is a good translation. This differs from the major-
ity of earlier work which we explain in Section 6.
</bodyText>
<sectionHeader confidence="0.994315" genericHeader="method">
4 Experiments on WMT12 data
</sectionHeader>
<bodyText confidence="0.999935">
We conducted experiments for the metric which
in total has 33 features (27 for adequacy and 6
for word order). Some of the features in the
metric depend on external sources of informa-
tion. For function words we use listings that are
created for many languages and are distributed
with METEOR toolkit (Denkowski and Lavie,
2011). The permutations are extracted using ME-
TEOR aligner which does fuzzy matching using
resources such as WordNet, paraphrase tables and
stemmers. METEOR is not used for any scoring,
but only for aligning hypothesis and reference.
For training we used the data from WMT13 hu-
man evaluation of the systems (Mach´aˇcek and Bo-
jar, 2013). Before evaluation, all data was low-
ercased and tokenized. After preprocessing, we
extract training examples for our binary classifier.
The number of non-tied human judgments per lan-
guage pair are shown in Table 1. Each human
judgment produces two training instances : one
positive and one negative. For learning we use
regression implementation in the Vowpal Wabbit
toolkit 1.
Tuned metric is tested on the human evaluated
data from WMT12 (Callison-Burch et al., 2012)
for correlation with the human judgment. As base-
line we used one of the best ranked metrics on the
sentence level evaluations from previous WMT
tasks – METEOR (Denkowski and Lavie, 2011).
The results are presented in the Table 2. The pre-
sented results are computed using definition of
</bodyText>
<footnote confidence="0.962445">
1https://github.com/JohnLangford/
vowpal_wabbit
</footnote>
<table confidence="0.998942818181818">
language pair #comparisons
cs-en 85469
de-en 128668
es-en 67832
fr-en 80741
ru-en 151422
en-cs 102842
en-de 77286
en-es 60464
en-fr 100783
en-ru 87323
</table>
<tableCaption confidence="0.996445">
Table 1: Number of human judgments in WMT13
</tableCaption>
<table confidence="0.999664727272727">
language BEER BEER METEOR
pair with without
paraphrases paraphrases
en-cs 0.194 0.190 0.152
en-fr 0.257 0.250 0.262
en-de 0.228 0.217 0.180
en-es 0.227 0.235 0.201
cs-en 0.215 0.213 0.205
fr-en 0.270 0.254 0.249
de-en 0.290 0.271 0.273
es-en 0.267 0.249 0.247
</table>
<tableCaption confidence="0.999603">
Table 2: Kendall τ correleation on WMT12 data
</tableCaption>
<bodyText confidence="0.992604833333333">
Kendall τ from the WMT12 (Callison-Burch et
al., 2012) so the scores could be compared with
other metrics on the same dataset that were re-
ported in the proceedings of that year (Callison-
Burch et al., 2012).
The results show that BEER with and without
paraphrase support outperforms METEOR (and
almost all other metrics on WMT12 metrics task)
on the majority of language pairs. Paraphrase sup-
port matters mostly when the target language is
English, but even in language pairs where it does
not help significantly it can be useful.
</bodyText>
<sectionHeader confidence="0.916673" genericHeader="method">
5 WMT14 evaluation task results
</sectionHeader>
<bodyText confidence="0.9999495">
In Table 4 and Table 3 you can see the results of
top 5 ranked metrics on the segment level evalua-
tion task of WMT14. In 5 out of 10 language pairs
BEER was ranked the first, on 4 the second best
and on one third best metric. The cases where it
failed to win the first place are:
</bodyText>
<listItem confidence="0.57196875">
• against DISCOTK-PARTY-TUNED on * - En-
glish except Hindi-English. DISCOTK-
PARTY-TUNED participated only in evalua-
tion of English which suggests that it uses
some language specific components which is
not the case with the current version of BEER
• against METEOR and AMBER on English-
Hindi. The reason for this is simply that we
</listItem>
<page confidence="0.991311">
417
</page>
<table confidence="0.99995">
Direction fr-en de-en hi-en cs-en ru-en
DISCOTK-PARTY-TUNED .433 .381 .434 .328 .364
BEER .417 .337 .438 .284 .337
REDCOMBSENT .406 .338 .417 .284 .343
REDCOMBSYSSENT .408 .338 .416 .282 .343
METEOR .406 .334 .420 .282 .337
Direction en-fr en-de en-hi en-cs en-ru
BEER .295 .258 .250 .344 .440
METEOR .278 .233 .264 .318 .427
AMBER .261 .224 .286 .302 .397
BLEU-NRC .257 .193 .234 .297 .391
APAC .255 .201 .203 .292 .388
</table>
<tableCaption confidence="0.99277125">
Table 3: Kendall τ correlations on the WMT14 hu-
man judgements when translating out of English.
Table 4: Kendall τ correlations on the WMT14
human judgements when translating into English.
</tableCaption>
<bodyText confidence="0.999624428571429">
did not have the data to tune our metric for
Hindi. Even by treating Hindi as English we
manage to get high in the rankings for this
language.
From metrics that participated in all language
pairs on the sentence level on average BEER has
the best correlation with the human judgment.
</bodyText>
<sectionHeader confidence="0.999844" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.999808975609756">
The main contribution of our metric is a linear
combination of features with far less sparse statis-
tics than earlier work. In particular, we employ
novel ordering features over PETs, a range of char-
acter n-gram features for adequancy, and direct
tuning for human ranking.
There are in the literature three main approaches
for tuning the machine translation metrics.
Approach 1 SPEDE (Wang and Manning, 2012),
metric of (Specia and Gim´enez, 2010),
ROSE-reg (Song and Cohn, 2011), ABS met-
ric of (Pad´o et al., 2009) and many oth-
ers train their regression models on the data
that has absolute scores for adequacy, fluency
or post-editing and then test on the ranking
problem. This is sometimes called pointwise
approach to learning-to-rank. In contrast our
metric is trained for ranking and tested on
ranking.
Approach 2 METEOR is tuned for the ranking
and tested on the ranking like our metric but
the tuning method is different. METEOR has
a non-linear model which is hard to tune with
gradient based methods so instead they tune
their parameters by hill-climbing (Lavie and
Agarwal, 2008). This not only reduces the
number of features that could be used but also
restricts the fine tuning of the existing small
number of parameters.
Approach 3 Some methods, like ours, allow
training of a large number of parameters for
ranking. Global ranking models that di-
rectly rank hypotheses are used in ROSE-
rank (Song and Cohn, 2011) and PAIR met-
ric of (Pad´o et al., 2009). Our work is more
similar to the training method for local rank-
ing models that give score directly (as it is
usually expected from an evaluation metric)
which was originally proposed in (Ye et al.,
2007) and later applied in (Duh, 2008) and
(Yang et al., 2013).
</bodyText>
<sectionHeader confidence="0.973782" genericHeader="conclusions">
7 Conclusion and future plans
</sectionHeader>
<bodyText confidence="0.999995625">
We have shown the advantages of combining
many simple features in a tunable linear model
of MT evaluation metric. Unlike majority of the
previous work we create a framework for training
large number of features on human rankings and at
the same time as a result of tuning produce a score
based metric which does not require two (or more)
hypotheses for comparison. The features that we
used are selected for reducing sparseness on the
sentence level. Together the smooth features and
the learning algorithm produce the metric that has
a very high correlation with human judgment.
For future research we plan to investigate some
more linguistically inspired features and also ex-
plore how this metric could be tuned for better tun-
ing of statistical machine translation systems.
</bodyText>
<sectionHeader confidence="0.997182" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.918466">
This work is supported by STW grant nr. 12271
and NWO VICI grant nr. 277-89-002.
</bodyText>
<sectionHeader confidence="0.997898" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.97589775">
Alexandra Birch and Miles Osborne. 2010. LRscore
for Evaluating Lexical and Reordering Quality in
MT. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 327–332, Uppsala, Sweden, July. Association
for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
</reference>
<page confidence="0.989318">
418
</page>
<reference confidence="0.997559309090909">
(Meta-) Evaluation of Machine Translation. In
Proceedings of the Second Workshop on Statistical
Machine Translation, StatMT ’07, pages 136–158,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10–51, Montr´eal, Canada, June. Association for
Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Ku&amp;quot;cerov´a.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, ACL ’05, pages 531–540, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
Kevin Duh. 2008. Ranking vs. Regression in Ma-
chine Translation Evaluation. In Proceedings of the
Third Workshop on Statistical Machine Translation,
StatMT ’08, pages 191–194, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
Ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352–1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
Evaluation of Translation Quality for Distant Lan-
guage Pairs. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’10, pages 944–952, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Alon Lavie and Abhaya Agarwal. 2008. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
Proceedings of the ACL 2008 Workshop on Statisti-
cal Machine Translation.
Hang Li. 2011. Learning to Rank for Information Re-
trieval and Natural Language Processing. Synthesis
Lectures on Human Language Technologies. Mor-
gan &amp; Claypool Publishers.
Matous&amp;quot; Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2013. Results
of the WMT13 Metrics Shared Task. In Proceed-
ings of the Eighth Workshop on Statistical Machine
Translation, pages 45–51, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Sebastian Pad´o, Michel Galley, Dan Jurafsky, and
Christopher D. Manning. 2009. Textual Entail-
ment Features for Machine Translation Evaluation.
In Proceedings of the Fourth Workshop on Statis-
tical Machine Translation, StatMT ’09, pages 37–
41, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Xingyi Song and Trevor Cohn. 2011. Regression and
Ranking based Optimisation for Sentence Level MT
Evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 123–129,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Lucia Specia and Jes´us Gim´enez. 2010. Combining
Confidence Estimation and Reference-based Metrics
for Segment-level MT Evaluation. In Ninth Confer-
ence of the Association for Machine Translation in
the Americas, AMTA-2010, Denver, Colorado.
Milo&amp;quot;s Stanojevi´c and Khalil Sima’an. 2013. Eval-
uating Long Range Reordering with Permutation-
Forests. In ILLC Prepublication Series, PP-2013-
14. University of Amsterdam.
Mengqiu Wang and Christopher D. Manning. 2012.
SPEDE: Probabilistic Edit Distance Metrics for MT
Evaluation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, WMT ’12,
pages 76–83, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377–403.
Muyun Yang, Junguo Zhu, Sheng Li, and Tiejun Zhao.
2013. Fusion of Word and Letter Based Metrics
for Automatic MT Evaluation. In Proceedings of
the Twenty-Third International Joint Conference on
Artificial Intelligence, IJCAI’13, pages 2204–2210.
AAAI Press.
Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sen-
tence Level Machine Translation Evaluation As a
Ranking Problem: One Step Aside from BLEU. In
Proceedings of the Second Workshop on Statistical
Machine Translation, StatMT ’07, pages 240–247,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Hao Zhang and Daniel Gildea. 2007. Factorization of
synchronous context-free grammars in linear time.
In In NAACL Workshop on Syntax and Structure in
Statistical Translation (SSST.
</reference>
<page confidence="0.998665">
419
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.948059">
<title confidence="0.999121">BEtter Evaluation as Ranking</title>
<author confidence="0.998163">Miloˇs Stanojevi´c Khalil Sima’an</author>
<affiliation confidence="0.999183">ILLC ILLC University of Amsterdam University of Amsterdam</affiliation>
<email confidence="0.965264">mstanojevic@uva.nlk.simaan@uva.nl</email>
<abstract confidence="0.998736727272727">We present the UvA-ILLC submission of to WMT 14 metrics task. a sentence level metric that can incorporate a large number of features combined in a linear model. Novel contributions are (1) efficient tuning of a large number of features for maximizing correwith system and (2) novel features that give smoother sentence level scores.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
</authors>
<title>LRscore for Evaluating Lexical and Reordering Quality in MT.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>327--332</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5464" citStr="Birch and Osborne, 2010" startWordPosition="871" endWordPosition="874">ed in (Yang et al., 2013). This can reward some translations even if they did not get the morphology completely right. Many metrics solve this problem by using stemmers, but using features based on character n-grams is more robust since it does not depend on the quality of the stemmer. For character level n-grams we can afford higher-order n-grams with less risk of sparse counts as on word n-grams. In our experiments we used character n-grams for size up to 6 which makes the total number of all adequacy features 27. 2.2 Ordering features To evaluate word order we follow (Isozaki et al., 2010; Birch and Osborne, 2010) in representing reordering as a permutation and then measuring the distance to the ideal monotone permutation. Here we take one feature from previous work – Kendall τ distance from the monotone permutation. This metrics on the permutation level has been shown to have high correlation with human judgment on language pairs with very different word order. Additionally, we add novel features with an even less sparse view of word order by exploiting hierarchical structure that exists in permutations (Zhang and Gildea, 2007). The trees that represent this structure are called PETs (PErmutation Tree</context>
</contexts>
<marker>Birch, Osborne, 2010</marker>
<rawString>Alexandra Birch and Miles Osborne. 2010. LRscore for Evaluating Lexical and Reordering Quality in MT. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 327–332, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<date>2007</date>
<contexts>
<context position="2911" citStr="Callison-Burch et al., 2007" startWordPosition="454" endWordPosition="457">he models in the literature tackle this problem by 1. training for another similar objective – e.g., tuning for absolute adequacy and fluency scores instead on rankings, or 2. training for rankings directly but with metaheuristic approaches like hill-climbing, or 3. training for pairwise rankings using learningto-rank techniques Approach (1) has two disadvantages. One is the inconsistency between the training and the testing objectives. The other, is that absolute rankings are not reliable enough because humans are better at giving relative than absolute judgments (see WMT manual evaluations (Callison-Burch et al., 2007)). Approach (2) does not allow integrating a large number of features which makes it less attractive. Approach (3) allows integration of a large number of features whose weights could be determined in an elegant machine learning framework. The output of learning in this approach can be either a function that ranks all hypotheses directly (global ranking model) or a function that assigns a score 414 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 414–419, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics to each hypothesis in</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007.</rawString>
</citation>
<citation valid="false">
<title>Evaluation of Machine Translation.</title>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07,</booktitle>
<pages>136--158</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker></marker>
<rawString>(Meta-) Evaluation of Machine Translation. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 136–158, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="12816" citStr="Callison-Burch et al., 2012" startWordPosition="2163" endWordPosition="2166">scoring, but only for aligning hypothesis and reference. For training we used the data from WMT13 human evaluation of the systems (Mach´aˇcek and Bojar, 2013). Before evaluation, all data was lowercased and tokenized. After preprocessing, we extract training examples for our binary classifier. The number of non-tied human judgments per language pair are shown in Table 1. Each human judgment produces two training instances : one positive and one negative. For learning we use regression implementation in the Vowpal Wabbit toolkit 1. Tuned metric is tested on the human evaluated data from WMT12 (Callison-Burch et al., 2012) for correlation with the human judgment. As baseline we used one of the best ranked metrics on the sentence level evaluations from previous WMT tasks – METEOR (Denkowski and Lavie, 2011). The results are presented in the Table 2. The presented results are computed using definition of 1https://github.com/JohnLangford/ vowpal_wabbit language pair #comparisons cs-en 85469 de-en 128668 es-en 67832 fr-en 80741 ru-en 151422 en-cs 102842 en-de 77286 en-es 60464 en-fr 100783 en-ru 87323 Table 1: Number of human judgments in WMT13 language BEER BEER METEOR pair with without paraphrases paraphrases en-</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerov´a</author>
</authors>
<title>Clause Restructuring for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>531--540</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Collins, Koehn, Kucerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Ku&amp;quot;cerov´a. 2005. Clause Restructuring for Statistical Machine Translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 531–540, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="12016" citStr="Denkowski and Lavie, 2011" startWordPosition="2035" endWordPosition="2038">me it uses only one hypothesis – it does not require the pair of hypotheses to compare them. The score of the classifier is interpreted as confidence that the hypothesis is a good translation. This differs from the majority of earlier work which we explain in Section 6. 4 Experiments on WMT12 data We conducted experiments for the metric which in total has 33 features (27 for adequacy and 6 for word order). Some of the features in the metric depend on external sources of information. For function words we use listings that are created for many languages and are distributed with METEOR toolkit (Denkowski and Lavie, 2011). The permutations are extracted using METEOR aligner which does fuzzy matching using resources such as WordNet, paraphrase tables and stemmers. METEOR is not used for any scoring, but only for aligning hypothesis and reference. For training we used the data from WMT13 human evaluation of the systems (Mach´aˇcek and Bojar, 2013). Before evaluation, all data was lowercased and tokenized. After preprocessing, we extract training examples for our binary classifier. The number of non-tied human judgments per language pair are shown in Table 1. Each human judgment produces two training instances : </context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
</authors>
<title>Ranking vs. Regression in Machine Translation Evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation, StatMT ’08,</booktitle>
<pages>191--194</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17434" citStr="Duh, 2008" startWordPosition="2951" endWordPosition="2952">This not only reduces the number of features that could be used but also restricts the fine tuning of the existing small number of parameters. Approach 3 Some methods, like ours, allow training of a large number of parameters for ranking. Global ranking models that directly rank hypotheses are used in ROSErank (Song and Cohn, 2011) and PAIR metric of (Pad´o et al., 2009). Our work is more similar to the training method for local ranking models that give score directly (as it is usually expected from an evaluation metric) which was originally proposed in (Ye et al., 2007) and later applied in (Duh, 2008) and (Yang et al., 2013). 7 Conclusion and future plans We have shown the advantages of combining many simple features in a tunable linear model of MT evaluation metric. Unlike majority of the previous work we create a framework for training large number of features on human rankings and at the same time as a result of tuning produce a score based metric which does not require two (or more) hypotheses for comparison. The features that we used are selected for reducing sparseness on the sentence level. Together the smooth features and the learning algorithm produce the metric that has a very hi</context>
</contexts>
<marker>Duh, 2008</marker>
<rawString>Kevin Duh. 2008. Ranking vs. Regression in Machine Translation Evaluation. In Proceedings of the Third Workshop on Statistical Machine Translation, StatMT ’08, pages 191–194, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as Ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="930" citStr="Hopkins and May, 2011" startWordPosition="138" endWordPosition="142">e number of features combined in a linear model. Novel contributions are (1) efficient tuning of a large number of features for maximizing correlation with human system ranking, and (2) novel features that give smoother sentence level scores. 1 Introduction The quality of sentence level (also called segment level) evaluation metrics in machine translation is often considered inferior to the quality of corpus (or system) level metrics. Yet, a sentence level metrics has important advantages as it: 1. provides an informative score to individual translations 2. is assumed by MT tuning algorithms (Hopkins and May, 2011). 3. facilitates easier statistical testing using sign test or t-test (Collins et al., 2005) We think that the root cause for most of the difficulty in creating a good sentence level metric is the sparseness of the features often used. Consider the n-gram counting metrics (BLEU (Papineni et al., 2002)): counts of higher order n-grams are usually rather small, if not zero, when counted at the individual sentence level. Metrics based on such counts are brittle at the sentence level even when they might be good at the corpus level. Ideally we should have features of varying granularity that we ca</context>
<context position="3875" citStr="Hopkins and May, 2011" startWordPosition="603" endWordPosition="606">lobal ranking model) or a function that assigns a score 414 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 414–419, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics to each hypothesis individually which can be used for ranking (local ranking model) (Li, 2011). Local ranking models are preferable because they provide absolute distance between hypotheses like most existing evaluation metrics. In this paper we follow the learning-to-rank approach which produces a local ranking model in a similar way to PRO MT systems tuning (Hopkins and May, 2011). 2 Model Our model is a fairly simple linear interpolation of feature functions, which is easy to train and simple to interpret. The model determines the similarity of the hypothesis h to the reference translation r by assigning a weight wi to each feature Oi(h, r). The linear scoring function is given by: �score(h,r) = wi × Oi(h,r) = w�· φ� i 2.1 Adequacy features The features used are precision P, recall R and F1-score F for different counts: Pfunction, Rfunction, Ffunction on matched function words Pcontent, Rcontent, Fcontent on matched content words (all non-function words) Pall, Rall, F</context>
<context position="11091" citStr="Hopkins and May, 2011" startWordPosition="1871" endWordPosition="1874">he same ranking as human judges do, it needs to give the higher score to the hgood hypothesis. Given that our model is linear we can derive: score(hgood, r) &gt; score(hbad, r) ⇔ w~· ~φgood &gt; w~ · ~φbad ⇔ w~· ~φgood − w~ · ~φbad &gt; 0 ⇔ w~ · (~φgood − ~φbad) &gt; 0 w~ · (~φbad − ~φgood) C 0 The most important part here are the last two equations. Using them we formulate ranking problem as a problem of binary classification: the positive training instance would have feature values 416 ~φgood − ~φbad and the negative training instance would have feature values ~φbad− ~φgood. This trick was used in PRO (Hopkins and May, 2011) but for the different task: • tuning the model of the SMT system • objective function was an evaluation metric Given this formulation of the training instances we can train the classifier using pairs of hypotheses. Note that even though it uses pairs of hypotheses for training in the evaluation time it uses only one hypothesis – it does not require the pair of hypotheses to compare them. The score of the classifier is interpreted as confidence that the hypothesis is a good translation. This differs from the majority of earlier work which we explain in Section 6. 4 Experiments on WMT12 data We</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as Ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Tsutomu Hirao</author>
<author>Kevin Duh</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
</authors>
<title>Automatic Evaluation of Translation Quality for Distant Language Pairs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>944--952</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5438" citStr="Isozaki et al., 2010" startWordPosition="867" endWordPosition="870">ams, originally proposed in (Yang et al., 2013). This can reward some translations even if they did not get the morphology completely right. Many metrics solve this problem by using stemmers, but using features based on character n-grams is more robust since it does not depend on the quality of the stemmer. For character level n-grams we can afford higher-order n-grams with less risk of sparse counts as on word n-grams. In our experiments we used character n-grams for size up to 6 which makes the total number of all adequacy features 27. 2.2 Ordering features To evaluate word order we follow (Isozaki et al., 2010; Birch and Osborne, 2010) in representing reordering as a permutation and then measuring the distance to the ideal monotone permutation. Here we take one feature from previous work – Kendall τ distance from the monotone permutation. This metrics on the permutation level has been shown to have high correlation with human judgment on language pairs with very different word order. Additionally, we add novel features with an even less sparse view of word order by exploiting hierarchical structure that exists in permutations (Zhang and Gildea, 2007). The trees that represent this structure are cal</context>
</contexts>
<marker>Isozaki, Hirao, Duh, Sudoh, Tsukada, 2010</marker>
<rawString>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010. Automatic Evaluation of Translation Quality for Distant Language Pairs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 944–952, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL 2008 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="16822" citStr="Lavie and Agarwal, 2008" startWordPosition="2840" endWordPosition="2843">d Cohn, 2011), ABS metric of (Pad´o et al., 2009) and many others train their regression models on the data that has absolute scores for adequacy, fluency or post-editing and then test on the ranking problem. This is sometimes called pointwise approach to learning-to-rank. In contrast our metric is trained for ranking and tested on ranking. Approach 2 METEOR is tuned for the ranking and tested on the ranking like our metric but the tuning method is different. METEOR has a non-linear model which is hard to tune with gradient based methods so instead they tune their parameters by hill-climbing (Lavie and Agarwal, 2008). This not only reduces the number of features that could be used but also restricts the fine tuning of the existing small number of parameters. Approach 3 Some methods, like ours, allow training of a large number of parameters for ranking. Global ranking models that directly rank hypotheses are used in ROSErank (Song and Cohn, 2011) and PAIR metric of (Pad´o et al., 2009). Our work is more similar to the training method for local ranking models that give score directly (as it is usually expected from an evaluation metric) which was originally proposed in (Ye et al., 2007) and later applied in</context>
</contexts>
<marker>Lavie, Agarwal, 2008</marker>
<rawString>Alon Lavie and Abhaya Agarwal. 2008. METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments. In Proceedings of the ACL 2008 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
</authors>
<title>Learning to Rank for Information Retrieval and Natural Language Processing. Synthesis Lectures on Human Language Technologies.</title>
<date>2011</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="3584" citStr="Li, 2011" startWordPosition="559" endWordPosition="560">tures which makes it less attractive. Approach (3) allows integration of a large number of features whose weights could be determined in an elegant machine learning framework. The output of learning in this approach can be either a function that ranks all hypotheses directly (global ranking model) or a function that assigns a score 414 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 414–419, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics to each hypothesis individually which can be used for ranking (local ranking model) (Li, 2011). Local ranking models are preferable because they provide absolute distance between hypotheses like most existing evaluation metrics. In this paper we follow the learning-to-rank approach which produces a local ranking model in a similar way to PRO MT systems tuning (Hopkins and May, 2011). 2 Model Our model is a fairly simple linear interpolation of feature functions, which is easy to train and simple to interpret. The model determines the similarity of the hypothesis h to the reference translation r by assigning a weight wi to each feature Oi(h, r). The linear scoring function is given by: </context>
</contexts>
<marker>Li, 2011</marker>
<rawString>Hang Li. 2011. Learning to Rank for Information Retrieval and Natural Language Processing. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matous Mach´acek</author>
<author>Ondrej Bojar</author>
</authors>
<title>Results of the WMT13 Metrics Shared Task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>45--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Mach´acek, Bojar, 2013</marker>
<rawString>Matous&amp;quot; Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2013. Results of the WMT13 Metrics Shared Task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 45–51, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Michel Galley</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Textual Entailment Features for Machine Translation Evaluation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09,</booktitle>
<pages>37--41</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Pad´o, Galley, Jurafsky, Manning, 2009</marker>
<rawString>Sebastian Pad´o, Michel Galley, Dan Jurafsky, and Christopher D. Manning. 2009. Textual Entailment Features for Machine Translation Evaluation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09, pages 37– 41, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1232" citStr="Papineni et al., 2002" startWordPosition="190" endWordPosition="193">egment level) evaluation metrics in machine translation is often considered inferior to the quality of corpus (or system) level metrics. Yet, a sentence level metrics has important advantages as it: 1. provides an informative score to individual translations 2. is assumed by MT tuning algorithms (Hopkins and May, 2011). 3. facilitates easier statistical testing using sign test or t-test (Collins et al., 2005) We think that the root cause for most of the difficulty in creating a good sentence level metric is the sparseness of the features often used. Consider the n-gram counting metrics (BLEU (Papineni et al., 2002)): counts of higher order n-grams are usually rather small, if not zero, when counted at the individual sentence level. Metrics based on such counts are brittle at the sentence level even when they might be good at the corpus level. Ideally we should have features of varying granularity that we can optimize on the actual evaluation task: relative ranking of system outputs. Therefore, in this paper we explore two kinds of less sparse features: Character n-grams are features at the sub-word level that provide evidence for translation adequacy - for example whether the stem is correctly translate</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xingyi Song</author>
<author>Trevor Cohn</author>
</authors>
<title>Regression and Ranking based Optimisation for Sentence Level MT Evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>123--129</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="16211" citStr="Song and Cohn, 2011" startWordPosition="2737" endWordPosition="2740">hat participated in all language pairs on the sentence level on average BEER has the best correlation with the human judgment. 6 Related work The main contribution of our metric is a linear combination of features with far less sparse statistics than earlier work. In particular, we employ novel ordering features over PETs, a range of character n-gram features for adequancy, and direct tuning for human ranking. There are in the literature three main approaches for tuning the machine translation metrics. Approach 1 SPEDE (Wang and Manning, 2012), metric of (Specia and Gim´enez, 2010), ROSE-reg (Song and Cohn, 2011), ABS metric of (Pad´o et al., 2009) and many others train their regression models on the data that has absolute scores for adequacy, fluency or post-editing and then test on the ranking problem. This is sometimes called pointwise approach to learning-to-rank. In contrast our metric is trained for ranking and tested on ranking. Approach 2 METEOR is tuned for the ranking and tested on the ranking like our metric but the tuning method is different. METEOR has a non-linear model which is hard to tune with gradient based methods so instead they tune their parameters by hill-climbing (Lavie and Aga</context>
</contexts>
<marker>Song, Cohn, 2011</marker>
<rawString>Xingyi Song and Trevor Cohn. 2011. Regression and Ranking based Optimisation for Sentence Level MT Evaluation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 123–129, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Jes´us Gim´enez</author>
</authors>
<title>Combining Confidence Estimation and Reference-based Metrics for Segment-level MT Evaluation.</title>
<date>2010</date>
<booktitle>In Ninth Conference of the Association for Machine Translation in the Americas, AMTA-2010,</booktitle>
<location>Denver, Colorado.</location>
<marker>Specia, Gim´enez, 2010</marker>
<rawString>Lucia Specia and Jes´us Gim´enez. 2010. Combining Confidence Estimation and Reference-based Metrics for Segment-level MT Evaluation. In Ninth Conference of the Association for Machine Translation in the Americas, AMTA-2010, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milos Stanojevi´c</author>
<author>Khalil Sima’an</author>
</authors>
<title>Evaluating Long Range Reordering with PermutationForests.</title>
<date>2013</date>
<booktitle>In ILLC Prepublication Series,</booktitle>
<pages>2013--14</pages>
<institution>University of Amsterdam.</institution>
<marker>Stanojevi´c, Sima’an, 2013</marker>
<rawString>Milo&amp;quot;s Stanojevi´c and Khalil Sima’an. 2013. Evaluating Long Range Reordering with PermutationForests. In ILLC Prepublication Series, PP-2013-14. University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>SPEDE: Probabilistic Edit Distance Metrics for MT Evaluation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ’12,</booktitle>
<pages>76--83</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="16140" citStr="Wang and Manning, 2012" startWordPosition="2726" endWordPosition="2729">sh we manage to get high in the rankings for this language. From metrics that participated in all language pairs on the sentence level on average BEER has the best correlation with the human judgment. 6 Related work The main contribution of our metric is a linear combination of features with far less sparse statistics than earlier work. In particular, we employ novel ordering features over PETs, a range of character n-gram features for adequancy, and direct tuning for human ranking. There are in the literature three main approaches for tuning the machine translation metrics. Approach 1 SPEDE (Wang and Manning, 2012), metric of (Specia and Gim´enez, 2010), ROSE-reg (Song and Cohn, 2011), ABS metric of (Pad´o et al., 2009) and many others train their regression models on the data that has absolute scores for adequacy, fluency or post-editing and then test on the ranking problem. This is sometimes called pointwise approach to learning-to-rank. In contrast our metric is trained for ranking and tested on ranking. Approach 2 METEOR is tuned for the ranking and tested on the ranking like our metric but the tuning method is different. METEOR has a non-linear model which is hard to tune with gradient based method</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Mengqiu Wang and Christopher D. Manning. 2012. SPEDE: Probabilistic Edit Distance Metrics for MT Evaluation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ’12, pages 76–83, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="8441" citStr="Wu, 1997" startWordPosition="1389" endWordPosition="1390">firstly, the PET operators show the minimal units of ordering that constitute the permutation itself, and secondly the higher level operators capture hidden patterns of ordering that cannot be observed without factorization. Statistics over patterns of ordering using PETs are non-lexical and hence far less sparse than word or character n-gram statistics. In PETs, the minimal operators on the node stand for ordering that cannot be broken down any further. The binary monotone operator is the simplest, binary inverted is the second in line, followed by operators of length four like h2, 4, 1, 3i (Wu, 1997), and then operators longer than four. The larger the branching factor under a PET node (the length of the operator on that node) the more complex the ordering. Hence, we devise possible branching feature functions over the operator length for the nodes in PETs: • factor 2 - with two features: Δ[ ]and Δ&lt;&gt; (there are no nodes with factor 3 (Wu, 1997)) • factor 4 - feature Δ=4 • factor bigger than 4 - feature Δ&gt;4 All of the mentioned PETs node features, except Δ[ ]and Δcount, signify the wrong word order but of different magnitude. Ideally all nodes in a PET would be binary monotone, but when th</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muyun Yang</author>
<author>Junguo Zhu</author>
<author>Sheng Li</author>
<author>Tiejun Zhao</author>
</authors>
<title>Fusion of Word and Letter Based Metrics for Automatic MT Evaluation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI’13,</booktitle>
<pages>2204--2210</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="4865" citStr="Yang et al., 2013" startWordPosition="766" endWordPosition="769"> features used are precision P, recall R and F1-score F for different counts: Pfunction, Rfunction, Ffunction on matched function words Pcontent, Rcontent, Fcontent on matched content words (all non-function words) Pall, Rall, Fall on matched words of any type Pchar n−gram, Rchar n−gram, Fchar n−gram matching of the character n-grams By differentiating function and non-function words we might have a better estimate of which words are more important and which are less. The last, but as we will see later the most important, adequacy feature is matching character n-grams, originally proposed in (Yang et al., 2013). This can reward some translations even if they did not get the morphology completely right. Many metrics solve this problem by using stemmers, but using features based on character n-grams is more robust since it does not depend on the quality of the stemmer. For character level n-grams we can afford higher-order n-grams with less risk of sparse counts as on word n-grams. In our experiments we used character n-grams for size up to 6 which makes the total number of all adequacy features 27. 2.2 Ordering features To evaluate word order we follow (Isozaki et al., 2010; Birch and Osborne, 2010) </context>
<context position="17458" citStr="Yang et al., 2013" startWordPosition="2954" endWordPosition="2957">duces the number of features that could be used but also restricts the fine tuning of the existing small number of parameters. Approach 3 Some methods, like ours, allow training of a large number of parameters for ranking. Global ranking models that directly rank hypotheses are used in ROSErank (Song and Cohn, 2011) and PAIR metric of (Pad´o et al., 2009). Our work is more similar to the training method for local ranking models that give score directly (as it is usually expected from an evaluation metric) which was originally proposed in (Ye et al., 2007) and later applied in (Duh, 2008) and (Yang et al., 2013). 7 Conclusion and future plans We have shown the advantages of combining many simple features in a tunable linear model of MT evaluation metric. Unlike majority of the previous work we create a framework for training large number of features on human rankings and at the same time as a result of tuning produce a score based metric which does not require two (or more) hypotheses for comparison. The features that we used are selected for reducing sparseness on the sentence level. Together the smooth features and the learning algorithm produce the metric that has a very high correlation with huma</context>
</contexts>
<marker>Yang, Zhu, Li, Zhao, 2013</marker>
<rawString>Muyun Yang, Junguo Zhu, Sheng Li, and Tiejun Zhao. 2013. Fusion of Word and Letter Based Metrics for Automatic MT Evaluation. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI’13, pages 2204–2210. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Ye</author>
<author>Ming Zhou</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Sentence Level Machine Translation Evaluation As a Ranking Problem: One Step Aside from BLEU.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07,</booktitle>
<pages>240--247</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17401" citStr="Ye et al., 2007" startWordPosition="2943" endWordPosition="2946">ll-climbing (Lavie and Agarwal, 2008). This not only reduces the number of features that could be used but also restricts the fine tuning of the existing small number of parameters. Approach 3 Some methods, like ours, allow training of a large number of parameters for ranking. Global ranking models that directly rank hypotheses are used in ROSErank (Song and Cohn, 2011) and PAIR metric of (Pad´o et al., 2009). Our work is more similar to the training method for local ranking models that give score directly (as it is usually expected from an evaluation metric) which was originally proposed in (Ye et al., 2007) and later applied in (Duh, 2008) and (Yang et al., 2013). 7 Conclusion and future plans We have shown the advantages of combining many simple features in a tunable linear model of MT evaluation metric. Unlike majority of the previous work we create a framework for training large number of features on human rankings and at the same time as a result of tuning produce a score based metric which does not require two (or more) hypotheses for comparison. The features that we used are selected for reducing sparseness on the sentence level. Together the smooth features and the learning algorithm prod</context>
</contexts>
<marker>Ye, Zhou, Lin, 2007</marker>
<rawString>Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sentence Level Machine Translation Evaluation As a Ranking Problem: One Step Aside from BLEU. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 240–247, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Factorization of synchronous context-free grammars in linear time.</title>
<date>2007</date>
<booktitle>In In NAACL Workshop on Syntax and Structure in Statistical Translation (SSST.</booktitle>
<contexts>
<context position="1961" citStr="Zhang and Gildea, 2007" startWordPosition="308" endWordPosition="311">ntence level. Metrics based on such counts are brittle at the sentence level even when they might be good at the corpus level. Ideally we should have features of varying granularity that we can optimize on the actual evaluation task: relative ranking of system outputs. Therefore, in this paper we explore two kinds of less sparse features: Character n-grams are features at the sub-word level that provide evidence for translation adequacy - for example whether the stem is correctly translated, Abstract ordering patterns found in tree factorizations of permutations into Permutation Trees (PETs) (Zhang and Gildea, 2007), including non-lexical alignment patterns. The BEER metric combines features of both kinds (presented in Section 2). With the growing number of adequacy and ordering features we need a model that facilitates efficient training. We would like to train for optimal Kendall τ correlation with rankings by human evaluators. The models in the literature tackle this problem by 1. training for another similar objective – e.g., tuning for absolute adequacy and fluency scores instead on rankings, or 2. training for rankings directly but with metaheuristic approaches like hill-climbing, or 3. training fo</context>
<context position="5989" citStr="Zhang and Gildea, 2007" startWordPosition="954" endWordPosition="957">Ordering features To evaluate word order we follow (Isozaki et al., 2010; Birch and Osborne, 2010) in representing reordering as a permutation and then measuring the distance to the ideal monotone permutation. Here we take one feature from previous work – Kendall τ distance from the monotone permutation. This metrics on the permutation level has been shown to have high correlation with human judgment on language pairs with very different word order. Additionally, we add novel features with an even less sparse view of word order by exploiting hierarchical structure that exists in permutations (Zhang and Gildea, 2007). The trees that represent this structure are called PETs (PErmutation Trees – see the next subsection). Metrics defined over PETs usually have a better estimate of long distance reorderings (Stanojevi´c and Sima’an, 2013). Here we use simple versions of these metrics: Δcount the ratio between the number of different permutation trees (PETs) (Zhang and Gildea, 2007) that could be built for the given permutation over the number of trees that could be built if permutation was completely monotone (there is a perfect word order). Δ[ ] ratio of the number of monotone nodes in a PET to the maximum p</context>
</contexts>
<marker>Zhang, Gildea, 2007</marker>
<rawString>Hao Zhang and Daniel Gildea. 2007. Factorization of synchronous context-free grammars in linear time. In In NAACL Workshop on Syntax and Structure in Statistical Translation (SSST.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
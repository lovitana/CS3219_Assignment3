<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010867">
<title confidence="0.98761">
Towards Social Event Detection and Contextualisation for Journalists
</title>
<author confidence="0.98748">
Prashant Khare
</author>
<affiliation confidence="0.993909">
Insight Centre for Data Analytics
National University of Ireland,
</affiliation>
<address confidence="0.589361">
Galway, Ireland
</address>
<email confidence="0.611185">
prashant.khare@insight-
centre.org
</email>
<author confidence="0.905495">
Bahareh Rahmanzadeh Heravi
</author>
<affiliation confidence="0.966947">
Insight Centre for Data Analytics
National University of Ireland,
</affiliation>
<address confidence="0.500385">
Galway, Ireland
</address>
<email confidence="0.441482">
bahareh.heravi@insight-
centre.org
</email>
<sectionHeader confidence="0.971811" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997776">
Social media platforms have become an important source of information in course of a break-
ing news event, such as natural calamity, political uproar, etc. News organisations and journal-
ists are increasingly realising the value of information being propagated via social media.
However, the sheer volume of the data produced on social media is overwhelming and manual
inspection of this streaming data for finding, aggregation, and contextualising emerging event
in a short time span is a day-to-day challenge by journalists and media organisations. It high-
lights the need for better tools and methods to help them utilise this user generated information
for news production. This paper addresses the above problem for journalists by proposing an
event detection and contextualisation framework that receives an input stream of social media
data and generates the likely events in the form of clusters along with a certain context.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99626296">
Social media platforms have evolved to being more than just a user-to-user interaction channel, and
play a prominent role in real-time information sharing. In many cases the real life ‘events’ are now
shared and broadcast on the social media platforms, by normal citizens, and not professional journal-
ists. This has turned the former consumer [only] of the news into [also] a broadcaster of the news, and
thus the social media platforms into an invaluable source of newsworthy information. The news organ-
isations are now more and more interested in gathering real-time information (such as breaking news,
images, videos) by means of monitoring and harvesting the user-generated content (UGC). Survey
results reveal that journalists are increasingly using social media platforms for their professional activ-
ities. For example surveys reveal that 96% of journalists in the UK and use feeds from social media in
their work on a daily basis (Cision, 2013), 99% of Irish journalists use social media as a source of in-
formation in their work (Heravi et al., 2014), and 51% of journalists globally leverage microblogs to
consume feeds for news and stories (Oriella, 2013). With the increasing usage of social media in the
journalistic processes, it is critical for journalists to be able to filter the social streams to discover
breaking news, and then analyse, aggregate, contextualise, and verify them in timely manner.
The concept of Social Semantic Journalism, introduced by Heravi et al. (2012), targets the above
problems encountered by the media organisations. The Social Semantic Journalism framework (Heravi
and McGinnis, 2013) utilises the social and semantic web technologies, and provides an integrated
view for enhancing newsworthy information discovery, filtering, aggregation, verification and publi-
cation. While there is considerable work done to retrieve information from various sources of data
(such as text) by various means, there is a paucity of tools available for detecting events from social
media data and extracting relevant information about such events in the real time. Building upon the
ideas of Social Semantic Journalism, to aid journalists in utilising UGC in an efficient manner, this
paper proposes a framework that implements an event detection pipeline, which clusters the data into
different events, and determines the context of the events based on entities (mentions particular to any
person, place, event, or thing) related to the events. The information that flows on the social media is
</bodyText>
<footnote confidence="0.530824">
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
</footnote>
<page confidence="0.981282">
54
</page>
<note confidence="0.7448325">
Proceedings of the AHA! Workshop on Information Discovery in Text, pages 54–59,
Dublin, Ireland, August 23 2014.
</note>
<bodyText confidence="0.994736166666667">
often via textual medium, and therefore in this proposed framework, we leverage text mining and Nat-
ural Language Processing (NLP) technologies to extract the information.
The remainder of this paper is organised as follows. Section 2 provides a background to the problem
and briefly reviews related work. Section 3 presents our proposed Event Detection and Contextualisa-
tion framework and gives a detailed overview of its components and phases. Section 4 concludes the
paper and discusses directions for future research.
</bodyText>
<sectionHeader confidence="0.657134" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.999965054054054">
Identifying new events, in the form of news from the data, is an area of interest for researchers for a
long time. Topic detection and tracking (TDT) (Allan, 2002) focuses on breaking down a streaming
text from newswire into smaller cohesive news pieces and determining if something has not been ear-
lier reported. An event detection cycle is seen as a subtask within TDT (Allan, 2002). The data from
social media platforms, such as Twitter, is quite voluminous and the streaming nature of this data war-
rants the usage of streaming algorithm models, where the data arrives in a chronological order (Mu-
thukrishnan, 2005). The social media data is further processed in a bounded space and time, i.e. as
every entry arrives it gets processed. Traditional approaches for identifying new information (an
event) were to compare each new entry in the data with the previously arrived entries. Petrovic et al.
(2010) investigated ways to identify tweets that first report the occurrence of an event by clustering
mechanism to identify nearest neighbours in the textual data. This work has motivated many other
contemporary research works to head in a related direction
Osborne et al. (2012) used the approach by Petrovic et al. (2010) as a baseline and investigated the
ways to improve the event detection mechanism on Twitter data, by matching the frequency of newly
occurring events from tweets with the activity (number of visits on a page) of the corresponding pages
of entities from Wikipedia and analysed if there was a similar pattern observed while determining an
event. Parikh and Karlapalem (2013), also considering frequency based analysis, developed an event
detection system that extracts events from tweets by examining frequencies in the temporal blocks of
streaming data.
Natural Language Processing (NLP) techniques can be leveraged in detecting events from
voluminous social media data. Events are associated with entities and NLP techniques can be applied
to extract the entities that are mentioned in the text that defines an event. To perform Named Entity
Recognition (NER) on tweets Ritter et. al. (2011) redeveloped the taggers and segmenters of Stanford
NLP library1. Ritter et al. (2012) extending the above work created an application Twical, that
extracted an open domain calendar for events that were shared on Twitter.
For an event detection system, it is also crucial to determine the context of a piece of
text/information. The contextualisation is answering the question ‘what is this about?’ and one of the
ways to answer it could be by aggregating information from knowledge base such as Wikipedia
(SanJuan et al., 2012). A potential context of the content can likely be inferred by extracting set of top-
ics that bound the text. Hulpus et al. (2013) proposed an approach by linking the topics inherent to a
text with the concepts in DBpedia2 and thereby automatically extracting the topic labels from the cor-
pus. Meij et al. (2012) extracted underlying concepts of a text from a large knowledge base of Wik-
ipedia articles by applying a supervised learning using a Naive Bayes (NB), Support Vector Machines
(SVM), and a C4.5 decision tree classifier. Large knowledge bases, such as YAGO3, are also used
(Hoffart et al., 2013) to explore the inherent relationship between entities and disambiguate them to
derive the context. Taking insights from various research works briefed above, we aim to construct a
framework that is inspired by ideas from different works in the next section.
</bodyText>
<sectionHeader confidence="0.987389" genericHeader="method">
3 The Event Detection &amp; Contextualisation Framework
</sectionHeader>
<bodyText confidence="0.999730666666667">
There are various approaches to extract the information from data, by means of clustering, entity
extraction and contextualisation, yet there is no observed pipeline that incorporates different methods
and brings them under one framework so as to to generate insights from streaming social media data.
</bodyText>
<footnote confidence="0.999226333333333">
1 http://nlp.stanford.edu/software/corenlp.shtml
2 http://dbpedia.org/About
3 http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/
</footnote>
<page confidence="0.9987">
55
</page>
<bodyText confidence="0.996276571428571">
We aim to address this gap, by proposing a framework that performs the aforementioned
functionalities under one system. A complete illustration of the framework is visualised in the Figure 1
(further explained in detail). It is a pipeline that incorporates several components, each followed by
another phase that uses the output from the previous one. The data could potentially come from
various social media APIs; however we have focused on data collected from Twitter streaming API4.
Following sections explain the different phases, in order, that process the input streaming data: index-
ing and clustering, entity recognition, and entity disambiguation and contextualisation.
</bodyText>
<figureCaption confidence="0.996138">
Figure 1. Event detection and Contextualisation framework
</figureCaption>
<subsectionHeader confidence="0.979401">
3.1 Indexing and Clustering
</subsectionHeader>
<bodyText confidence="0.999930055555556">
This phase is aimed at pre-processing the data, breaking the data into set of keywords and generating
an index that maps words against their corresponding document. Once an index is created, the data is
clustered as sets of word vectors occurring together prominently. These clusters tend to represent the
events that exist in the data.
Indexing: The data is indexed in this sub-phase. The incoming data stream is stored and then the
divided in slabs of time windows (say of 10 minutes each). This is done to analyse the data based on
regular time intervals, which may result in inferring only the highly dominating events/clusters present
in the data. An index, between terms and corresponding documents (that initially contained those
terms), is generated for this slab of data using standard libraries such as Lucene5 and Solr6 (built over
Lucene).
Clustering: In this sub-phase we derive the preliminary clusters of the data, which are likely to reflect
the most related content within the data slab that was earlier created. Examples of the clustering algo-
rithms that can be hired to cluster the data are k-means, PLSA (Hofmann, 1999) and LDA (Blei et al.,
2003). After the clusters are formed, the terms with high weight in the clusters are taken to query the
index for retrieving the most relevant documents based threshold relevance score. The relevance score
is derived from term frequency and inverse document frequency (tf-idf) (Manning et al., 2008) value
and accordingly the documents are retrieved. The text from those top scored documents can now be
extracted and merged into one string, hereafter called event string, which tends to represent the infor-
</bodyText>
<footnote confidence="0.99947">
4 https://dev.twitter.com/docs/api/streaming
5 http://lucene.apache.org/
6 http://lucene.apache.org/solr/
</footnote>
<page confidence="0.996784">
56
</page>
<bodyText confidence="0.997461">
mation stored against a particular cluster or event. This event string is further used for NER and dis-
ambiguation.
</bodyText>
<subsectionHeader confidence="0.996048">
3.2 Entity Recognition
</subsectionHeader>
<bodyText confidence="0.989175714285714">
The event string, derived above, is further annotated for its entities by applying Named Entity Recog-
nition techniques. NER is an information extraction task to extract key elements, hereafter referred to
as Entities, from a text and categorise them into person, location, organisation, etc. In this work we
rely on libraries such as Stanford NER (Finkel et al., 2005) or other wrappers to this library, which
implement it to extract named entities. However, there are other libraries available for this purpose, for
instance, Open NLP7, Open Calais8, etc. A detailed explanation of the NER models is given in the re-
search work by Sang and Meulder (2003) and Finkel et al. (2005).
For each mention in the string there can be multiple candidate entities which further need to be
disambiguated. An explanation of it could be given with an example, such as in “David was playing
for Manchester United when Victoria gave her auditions. Victoria later became part of band Spice
Girls”: how could it be determined whether Victoria is a person (particularly Victoria Beckham) and
not Victoria- a place or Queen Victoria, and David implies David Beckham and not David - a figure in
religious text/history. Establishing such a mapping between mention and most relevant entity is termed
as named entity disambiguation process.
</bodyText>
<subsectionHeader confidence="0.997924">
3.3 Entity Disambiguation and Contextualisation
</subsectionHeader>
<bodyText confidence="0.999990555555555">
In the entity disambiguation and contextualisation phase, initially an input text (web page, language
paragraph, sentence, article) is resolved into various mentions of entities (surface mentions- that
means its just a mention with no associated knowledge) by matching all the potential candidate entities
with the surface mentions. For this purpose Stanford NER tagger is used. For each mention (a poten-
tial entity) knowledge sources such as DBpedia and/or Yago (Hoffart et al., 2013; Hoffart et al., 2011)
are harvested to extract potential entity mentions. Each mention will then be mapped for numerous
potential entity candidates. After extracting the candidate entities, a relevance score can be assigned to
each based on features such as a prior for candidate entity popularity, mutual information (similarity
between key-phrase or query string and description of the entity), syntax based similarity (Thater et
al., 2010), entity-entity coherence (quantifying the number of similar incoming links on a knowledge
base as Wikipedia). Milne and Witten (2008) extended few similarity measures defined by Bunescu
and Pasca (2006), which compared the context of a given text to the entities mention in Wikipedia.
Considering the above features, a graph of mentions and candidate entities, with the edges as
weights, can be generated. Each node will have a certain weight on its edge, a greedy approach can be
adopted to iteratively remove the low weight nodes to disambiguate the entities (Hoffart et al., 2011).
This approach will result in disambiguated entities (to a high degree) for each surface mentions of the
input text and represent entities according to the context of the input text. After the disambiguation of
the entities, a knowledge resource can be hired to query for generating a brief description about the
prominent entities (such as their abstract/description and type), and thereby contextualising the whole
input text with a bag of entities and their corresponding description.
The overall framework describes a mechanism to design a tool that can process input streaming data
into set of clusters that reflect events and assists in visualising the context of those events. This
framework is considered to enhance event detection approaches by enriching the events with their rel-
evant information being extracted from knowledge resources. While some of the state of the art tech-
niques and tools incorporated in this framework have been proposed and/or utilised in other domains,
the proposed framework is a novel end-to-end pipeline specifically designed for the news industry and
for breaking event detection and contextualisation.
</bodyText>
<sectionHeader confidence="0.988288" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9987215">
This paper presents a framework, which aims at assisting journalists in dealing with the ever- flooding
UGC to detect the upcoming/breaking events. Various surveys (Oriella, 2013; Cision, 2013; Heravi et.
</bodyText>
<footnote confidence="0.99802">
7 https://opennlp.apache.org/
8 http://www.opencalais.com/
</footnote>
<page confidence="0.998953">
57
</page>
<bodyText confidence="0.999977111111111">
al., 2014) highlight the growing need for specialised tools to allow journalists utilise the user-
generated for news production and storytelling. The proposed framework is believed to be an im-
portant step forward in addressing the challenges encountered by journalists in leveraging the social
media content for emerging event detection and event contextualisation in the process of news produc-
tion. The emerging events can now be visualised without needing to manually assess the frequency of
any particular information propagation on social media and also generate the context of the infor-
mation at the same time.
An early phase test was performed on the proposed pipeline so as to assess the viability of the
framework. The framework was simulated with a sample data constituting of tweets from three differ-
ent known events and it reflected encouraging results with respect to the viability of the underlying
processes and the framework as a whole. The framework successfully clustered the sample data, using
k-means algorithm, into unique clusters and the entity disambiguation phase, implemented using AI-
DA framework (Hoffart et al., 2011), yielded relevant entities. An end-to-end evaluation of the pipe-
line, however, is yet to be performed to analyse the results of every phase, and the pipeline as a whole.
There are foreseen challenges such as noise filtering, the non-lexical nature of the data, and the veri-
ty of the content. The data from social media contains an enormous amount of noise (such as random
tweets posted by users which do not have a relevance with the event and may yet contain the filtering
keywords) in exhaustive social media streams when it comes to filtering the content specific to certain
events/topics and that could certainly affect the outcome of the event clusters. Apart from noise, often
the language used on social media is non-lexical and non-syntactic in nature because users compro-
mise with the language rules to share more information in limited space (e.g. Twitter allows only 140
characters) hence leveraging the NLP techniques may not result in most efficient results.
The above challenges require a thorough investigation of the current state of the research and as a
future work we aim to address 1) perform an end-to-end evaluation on the pipeline and 2) address the
above challenges by exploring how information extraction techniques can be customised for syntacti-
cally and lexically inefficient data and thereby refine the information gathering processes for journal-
ists.
</bodyText>
<sectionHeader confidence="0.959962" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.986173">
This publication has emanated from research conducted with the financial support of Science
Foundation Ireland (SFI) under Grant Numbers 12/TIDA/I2389, SFI/12/RC/2289 and 13/TIDA/I2743.
</bodyText>
<sectionHeader confidence="0.998566" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999303058823529">
Allan, J. 2002. Introduction to topic detection and tracking. In Topic detection and tracking (pp. 1-16). Springer
US.
Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. 2003. Latent dirichlet allocation. the Journal of machine Learning
research, 3, 993-1022.
Bunescu, R. C., &amp; Pasca, M. 2006. Using Encyclopedic Knowledge for Named entity Disambiguation. In EACL
(Vol. 6, pp. 9-16).
Cision. 2013. Social Journalism Study 2013. Report by Cision &amp; Canterbury Christ Church University (UK).
http://www.cision.com/uk/wp-content/uploads/2014/05/Social-Journalism-Study-2013.pdf visited July
13th,2014
Finkel, J. R., Grenager, T., &amp; Manning, C. 2005, June. Incorporating non-local information into information
extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for
Computational Linguistics (pp. 363-370). Association for Computational Linguistics.
Heravi, B. R., Boran, M., &amp; Breslin, J. 2012. Towards Social Semantic Journalism. In Sixth International AAAI
Conference on Weblogs and Social Media.
Heravi, B. R., &amp; McGinnis, J. 2013. A Framework for Social Semantic Journalism. In First International IFIP
Working Conference on Value-Driven Social &amp; Semantic Collective Intelligence (VaSCo), at ACM Web
Science.
</reference>
<page confidence="0.993281">
58
</page>
<reference confidence="0.998709333333333">
Heravi, B. R., Harrower, N., Boran, M. 2014. Social Journalism Survey: First National Study on Irish Journalists&apos;
use of Social Media. HuJo, Insight Centre for Data Analytics, National University of Ireland, Galway
(forthcoming 20 July 2014).
Hoffart, J., Yosef, M. A., Bordino, I., Fürstenau, H., Pinkal, M., Spaniol, M., Taneva, B., Thater, S., Weikum, G.
2011. Robust disambiguation of named entities in text. InProceedings of the Conference on Empirical
Methods in Natural Language Processing (pp. 782-792). Association for Computational Linguistics.
Hofmann, T. 1999. Probabilistic latent semantic analysis. InProceedings of the Fifteenth conference on
Uncertainty in artificial intelligence(pp. 289-296). Morgan Kaufmann Publishers Inc.
Hoffart, J., Suchanek, F. M., Berberich, K., &amp; Weikum, G. 2013. YAGO2: A spatially and temporally enhanced
knowledge base from Wikipedia. Artificial Intelligence, 194, 28-61.
Hulpus, I., Hayes, C., Karnstedt, M., &amp; Greene, D. 2013. Unsupervised graph-based topic labelling using
DBpedia. In Proceedings of the sixth ACM international conference on Web search and data mining (pp. 465-
474). ACM.
Manning, C. D., Raghavan, P., &amp; Schütze, H. 2008. Introduction to information retrieval (Vol. 1, p. 6).
Cambridge: Cambridge university press.
Meij, E., Weerkamp, W., &amp; de Rijke, M. 2012. Adding semantics to microblog posts. In Proceedings of the fifth
ACM international conference on Web search and data mining (pp. 563-572). ACM.
Milne, D., &amp; Witten, I. H. 2008. Learning to link with wikipedia. In Proceedings of the 17th ACM conference on
Information and knowledge management (pp. 509-518). ACM.
Muthukrishnan, S. 2005. Data streams: Algorithms and applications. Now Publishers Inc.
Oriella. 2013. The New Normal for News: Have global media Changed forever Oriella PR Network Global
Digital Journalism Study 2013. Available from
http://www.oriellaprnetwork.com/sites/default/files/research/Brands2Life_ODJS_v4.pdf visited July
13th,2014
Osborne, M., Petrovic, S., McCreadie, R., Macdonald, C., &amp; Ounis, I. 2012. Bieber no more: First story
detection using Twitter and Wikipedia. InProceedings of the Workshop on Time-aware Information Access.
TAIA (Vol. 12).
Parikh, R., &amp; Karlapalem, K. 2013. Et: events from tweets. InProceedings of the 22nd international conference
on World Wide Web companion (pp. 613-620). International World Wide Web Conferences Steering
Committee.
Petrović, S., Osborne, M., &amp; Lavrenko, V. 2010. Streaming first story detection with application to twitter. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the
Association for Computational Linguistics (pp. 181-189). Association for Computational Linguistics.
Ritter, A., Clark, S., &amp; Etzioni, O. 2011. Named entity recognition in tweets: an experimental study. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing (pp. 1524-1534).
Association for Computational Linguistics.
Ritter, A., Etzioni, O., &amp; Clark, S. 2012. Open domain event extraction from twitter. In Proceedings of the 18th
ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1104-1112). ACM.
Thater, S., Fürstenau, H., &amp; Pinkal, M. 2010. Contextualizing semantic representations using syntactically
enriched vector models. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics (pp. 948-957). Association for Computational Linguistics.
Tjong Kim Sang, E. F., &amp; De Meulder, F. 2003. Introduction to the CoNLL-2003 shared task: Language
independent named entity recognition. InProceedings of the seventh conference on Natural language learning
at HLT-NAACL 2003-Volume 4 (pp. 142-147). Association for Computational Linguistics.
Zaki, M. J., &amp; Meira Jr, W. 2011. Fundamentals of data mining algorithms.
</reference>
<page confidence="0.99926">
59
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.748809">
<title confidence="0.999917">Towards Social Event Detection and Contextualisation for Journalists</title>
<author confidence="0.985809">Prashant Khare</author>
<affiliation confidence="0.996272">Insight Centre for Data Analytics National University of Ireland,</affiliation>
<address confidence="0.998757">Galway, Ireland</address>
<email confidence="0.938197">prashant.khare@insightcentre.org</email>
<author confidence="0.965847">Bahareh Rahmanzadeh Heravi</author>
<affiliation confidence="0.993736">Insight Centre for Data Analytics National University of Ireland,</affiliation>
<address confidence="0.998925">Galway, Ireland</address>
<email confidence="0.967246">bahareh.heravi@insightcentre.org</email>
<abstract confidence="0.996279181818182">Social media platforms have become an important source of information in course of a breaking news event, such as natural calamity, political uproar, etc. News organisations and journalists are increasingly realising the value of information being propagated via social media. However, the sheer volume of the data produced on social media is overwhelming and manual inspection of this streaming data for finding, aggregation, and contextualising emerging event in a short time span is a day-to-day challenge by journalists and media organisations. It highlights the need for better tools and methods to help them utilise this user generated information for news production. This paper addresses the above problem for journalists by proposing an event detection and contextualisation framework that receives an input stream of social media data and generates the likely events in the form of clusters along with a certain context.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
</authors>
<title>Introduction to topic detection and tracking.</title>
<date>2002</date>
<booktitle>In Topic detection and tracking</booktitle>
<pages>1--16</pages>
<publisher>Springer US.</publisher>
<contexts>
<context position="4862" citStr="Allan, 2002" startWordPosition="731" endWordPosition="732">l Language Processing (NLP) technologies to extract the information. The remainder of this paper is organised as follows. Section 2 provides a background to the problem and briefly reviews related work. Section 3 presents our proposed Event Detection and Contextualisation framework and gives a detailed overview of its components and phases. Section 4 concludes the paper and discusses directions for future research. 2 Background and Related Work Identifying new events, in the form of news from the data, is an area of interest for researchers for a long time. Topic detection and tracking (TDT) (Allan, 2002) focuses on breaking down a streaming text from newswire into smaller cohesive news pieces and determining if something has not been earlier reported. An event detection cycle is seen as a subtask within TDT (Allan, 2002). The data from social media platforms, such as Twitter, is quite voluminous and the streaming nature of this data warrants the usage of streaming algorithm models, where the data arrives in a chronological order (Muthukrishnan, 2005). The social media data is further processed in a bounded space and time, i.e. as every entry arrives it gets processed. Traditional approaches f</context>
</contexts>
<marker>Allan, 2002</marker>
<rawString>Allan, J. 2002. Introduction to topic detection and tracking. In Topic detection and tracking (pp. 1-16). Springer US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<volume>3</volume>
<pages>993--1022</pages>
<contexts>
<context position="10671" citStr="Blei et al., 2003" startWordPosition="1632" endWordPosition="1635">ntervals, which may result in inferring only the highly dominating events/clusters present in the data. An index, between terms and corresponding documents (that initially contained those terms), is generated for this slab of data using standard libraries such as Lucene5 and Solr6 (built over Lucene). Clustering: In this sub-phase we derive the preliminary clusters of the data, which are likely to reflect the most related content within the data slab that was earlier created. Examples of the clustering algorithms that can be hired to cluster the data are k-means, PLSA (Hofmann, 1999) and LDA (Blei et al., 2003). After the clusters are formed, the terms with high weight in the clusters are taken to query the index for retrieving the most relevant documents based threshold relevance score. The relevance score is derived from term frequency and inverse document frequency (tf-idf) (Manning et al., 2008) value and accordingly the documents are retrieved. The text from those top scored documents can now be extracted and merged into one string, hereafter called event string, which tends to represent the infor4 https://dev.twitter.com/docs/api/streaming 5 http://lucene.apache.org/ 6 http://lucene.apache.org</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3, 993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Bunescu</author>
<author>M Pasca</author>
</authors>
<title>Using Encyclopedic Knowledge for Named entity Disambiguation.</title>
<date>2006</date>
<journal>In EACL</journal>
<volume>6</volume>
<pages>9--16</pages>
<contexts>
<context position="13915" citStr="Bunescu and Pasca (2006)" startWordPosition="2127" endWordPosition="2130"> are harvested to extract potential entity mentions. Each mention will then be mapped for numerous potential entity candidates. After extracting the candidate entities, a relevance score can be assigned to each based on features such as a prior for candidate entity popularity, mutual information (similarity between key-phrase or query string and description of the entity), syntax based similarity (Thater et al., 2010), entity-entity coherence (quantifying the number of similar incoming links on a knowledge base as Wikipedia). Milne and Witten (2008) extended few similarity measures defined by Bunescu and Pasca (2006), which compared the context of a given text to the entities mention in Wikipedia. Considering the above features, a graph of mentions and candidate entities, with the edges as weights, can be generated. Each node will have a certain weight on its edge, a greedy approach can be adopted to iteratively remove the low weight nodes to disambiguate the entities (Hoffart et al., 2011). This approach will result in disambiguated entities (to a high degree) for each surface mentions of the input text and represent entities according to the context of the input text. After the disambiguation of the ent</context>
</contexts>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>Bunescu, R. C., &amp; Pasca, M. 2006. Using Encyclopedic Knowledge for Named entity Disambiguation. In EACL (Vol. 6, pp. 9-16).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cision</author>
</authors>
<title>Social Journalism Study 2013. Report by Cision &amp;</title>
<date>2013</date>
<institution>Canterbury Christ Church University</institution>
<note>(UK). http://www.cision.com/uk/wp-content/uploads/2014/05/Social-Journalism-Study-2013.pdf visited</note>
<contexts>
<context position="2247" citStr="Cision, 2013" startWordPosition="334" endWordPosition="335">umer [only] of the news into [also] a broadcaster of the news, and thus the social media platforms into an invaluable source of newsworthy information. The news organisations are now more and more interested in gathering real-time information (such as breaking news, images, videos) by means of monitoring and harvesting the user-generated content (UGC). Survey results reveal that journalists are increasingly using social media platforms for their professional activities. For example surveys reveal that 96% of journalists in the UK and use feeds from social media in their work on a daily basis (Cision, 2013), 99% of Irish journalists use social media as a source of information in their work (Heravi et al., 2014), and 51% of journalists globally leverage microblogs to consume feeds for news and stories (Oriella, 2013). With the increasing usage of social media in the journalistic processes, it is critical for journalists to be able to filter the social streams to discover breaking news, and then analyse, aggregate, contextualise, and verify them in timely manner. The concept of Social Semantic Journalism, introduced by Heravi et al. (2012), targets the above problems encountered by the media organ</context>
<context position="15661" citStr="Cision, 2013" startWordPosition="2401" endWordPosition="2402">y enriching the events with their relevant information being extracted from knowledge resources. While some of the state of the art techniques and tools incorporated in this framework have been proposed and/or utilised in other domains, the proposed framework is a novel end-to-end pipeline specifically designed for the news industry and for breaking event detection and contextualisation. 4 Conclusion and Future Work This paper presents a framework, which aims at assisting journalists in dealing with the ever- flooding UGC to detect the upcoming/breaking events. Various surveys (Oriella, 2013; Cision, 2013; Heravi et. 7 https://opennlp.apache.org/ 8 http://www.opencalais.com/ 57 al., 2014) highlight the growing need for specialised tools to allow journalists utilise the usergenerated for news production and storytelling. The proposed framework is believed to be an important step forward in addressing the challenges encountered by journalists in leveraging the social media content for emerging event detection and event contextualisation in the process of news production. The emerging events can now be visualised without needing to manually assess the frequency of any particular information propa</context>
</contexts>
<marker>Cision, 2013</marker>
<rawString>Cision. 2013. Social Journalism Study 2013. Report by Cision &amp; Canterbury Christ Church University (UK). http://www.cision.com/uk/wp-content/uploads/2014/05/Social-Journalism-Study-2013.pdf visited July 13th,2014</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</booktitle>
<pages>363--370</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="11787" citStr="Finkel et al., 2005" startWordPosition="1799" endWordPosition="1802">infor4 https://dev.twitter.com/docs/api/streaming 5 http://lucene.apache.org/ 6 http://lucene.apache.org/solr/ 56 mation stored against a particular cluster or event. This event string is further used for NER and disambiguation. 3.2 Entity Recognition The event string, derived above, is further annotated for its entities by applying Named Entity Recognition techniques. NER is an information extraction task to extract key elements, hereafter referred to as Entities, from a text and categorise them into person, location, organisation, etc. In this work we rely on libraries such as Stanford NER (Finkel et al., 2005) or other wrappers to this library, which implement it to extract named entities. However, there are other libraries available for this purpose, for instance, Open NLP7, Open Calais8, etc. A detailed explanation of the NER models is given in the research work by Sang and Meulder (2003) and Finkel et al. (2005). For each mention in the string there can be multiple candidate entities which further need to be disambiguated. An explanation of it could be given with an example, such as in “David was playing for Manchester United when Victoria gave her auditions. Victoria later became part of band S</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Finkel, J. R., Grenager, T., &amp; Manning, C. 2005, June. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (pp. 363-370). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B R Heravi</author>
<author>M Boran</author>
<author>J Breslin</author>
</authors>
<title>Towards Social Semantic Journalism.</title>
<date>2012</date>
<booktitle>In Sixth International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="2788" citStr="Heravi et al. (2012)" startWordPosition="419" endWordPosition="422">K and use feeds from social media in their work on a daily basis (Cision, 2013), 99% of Irish journalists use social media as a source of information in their work (Heravi et al., 2014), and 51% of journalists globally leverage microblogs to consume feeds for news and stories (Oriella, 2013). With the increasing usage of social media in the journalistic processes, it is critical for journalists to be able to filter the social streams to discover breaking news, and then analyse, aggregate, contextualise, and verify them in timely manner. The concept of Social Semantic Journalism, introduced by Heravi et al. (2012), targets the above problems encountered by the media organisations. The Social Semantic Journalism framework (Heravi and McGinnis, 2013) utilises the social and semantic web technologies, and provides an integrated view for enhancing newsworthy information discovery, filtering, aggregation, verification and publication. While there is considerable work done to retrieve information from various sources of data (such as text) by various means, there is a paucity of tools available for detecting events from social media data and extracting relevant information about such events in the real time.</context>
</contexts>
<marker>Heravi, Boran, Breslin, 2012</marker>
<rawString>Heravi, B. R., Boran, M., &amp; Breslin, J. 2012. Towards Social Semantic Journalism. In Sixth International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B R Heravi</author>
<author>J McGinnis</author>
</authors>
<title>A Framework for Social Semantic Journalism.</title>
<date>2013</date>
<booktitle>In First International IFIP Working Conference on Value-Driven Social &amp; Semantic Collective Intelligence (VaSCo), at</booktitle>
<publisher>ACM Web Science.</publisher>
<contexts>
<context position="2925" citStr="Heravi and McGinnis, 2013" startWordPosition="437" endWordPosition="440">rce of information in their work (Heravi et al., 2014), and 51% of journalists globally leverage microblogs to consume feeds for news and stories (Oriella, 2013). With the increasing usage of social media in the journalistic processes, it is critical for journalists to be able to filter the social streams to discover breaking news, and then analyse, aggregate, contextualise, and verify them in timely manner. The concept of Social Semantic Journalism, introduced by Heravi et al. (2012), targets the above problems encountered by the media organisations. The Social Semantic Journalism framework (Heravi and McGinnis, 2013) utilises the social and semantic web technologies, and provides an integrated view for enhancing newsworthy information discovery, filtering, aggregation, verification and publication. While there is considerable work done to retrieve information from various sources of data (such as text) by various means, there is a paucity of tools available for detecting events from social media data and extracting relevant information about such events in the real time. Building upon the ideas of Social Semantic Journalism, to aid journalists in utilising UGC in an efficient manner, this paper proposes a</context>
</contexts>
<marker>Heravi, McGinnis, 2013</marker>
<rawString>Heravi, B. R., &amp; McGinnis, J. 2013. A Framework for Social Semantic Journalism. In First International IFIP Working Conference on Value-Driven Social &amp; Semantic Collective Intelligence (VaSCo), at ACM Web Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B R Heravi</author>
<author>N Harrower</author>
<author>M Boran</author>
</authors>
<title>Social Journalism Survey: First National Study on Irish Journalists&apos; use of Social Media.</title>
<date>2014</date>
<tech>HuJo,</tech>
<institution>Insight Centre for Data Analytics, National University of Ireland,</institution>
<location>Galway</location>
<contexts>
<context position="2353" citStr="Heravi et al., 2014" startWordPosition="352" endWordPosition="355">into an invaluable source of newsworthy information. The news organisations are now more and more interested in gathering real-time information (such as breaking news, images, videos) by means of monitoring and harvesting the user-generated content (UGC). Survey results reveal that journalists are increasingly using social media platforms for their professional activities. For example surveys reveal that 96% of journalists in the UK and use feeds from social media in their work on a daily basis (Cision, 2013), 99% of Irish journalists use social media as a source of information in their work (Heravi et al., 2014), and 51% of journalists globally leverage microblogs to consume feeds for news and stories (Oriella, 2013). With the increasing usage of social media in the journalistic processes, it is critical for journalists to be able to filter the social streams to discover breaking news, and then analyse, aggregate, contextualise, and verify them in timely manner. The concept of Social Semantic Journalism, introduced by Heravi et al. (2012), targets the above problems encountered by the media organisations. The Social Semantic Journalism framework (Heravi and McGinnis, 2013) utilises the social and sem</context>
</contexts>
<marker>Heravi, Harrower, Boran, 2014</marker>
<rawString>Heravi, B. R., Harrower, N., Boran, M. 2014. Social Journalism Survey: First National Study on Irish Journalists&apos; use of Social Media. HuJo, Insight Centre for Data Analytics, National University of Ireland, Galway (forthcoming 20 July 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hoffart</author>
<author>M A Yosef</author>
<author>I Bordino</author>
<author>H Fürstenau</author>
<author>M Pinkal</author>
<author>M Spaniol</author>
<author>B Taneva</author>
<author>S Thater</author>
<author>G Weikum</author>
</authors>
<title>Robust disambiguation of named entities</title>
<date>2011</date>
<booktitle>in text. InProceedings of the Conference on Empirical Methods in Natural Language Processing</booktitle>
<pages>782--792</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="13291" citStr="Hoffart et al., 2011" startWordPosition="2036" endWordPosition="2039">relevant entity is termed as named entity disambiguation process. 3.3 Entity Disambiguation and Contextualisation In the entity disambiguation and contextualisation phase, initially an input text (web page, language paragraph, sentence, article) is resolved into various mentions of entities (surface mentions- that means its just a mention with no associated knowledge) by matching all the potential candidate entities with the surface mentions. For this purpose Stanford NER tagger is used. For each mention (a potential entity) knowledge sources such as DBpedia and/or Yago (Hoffart et al., 2013; Hoffart et al., 2011) are harvested to extract potential entity mentions. Each mention will then be mapped for numerous potential entity candidates. After extracting the candidate entities, a relevance score can be assigned to each based on features such as a prior for candidate entity popularity, mutual information (similarity between key-phrase or query string and description of the entity), syntax based similarity (Thater et al., 2010), entity-entity coherence (quantifying the number of similar incoming links on a knowledge base as Wikipedia). Milne and Witten (2008) extended few similarity measures defined by </context>
<context position="16879" citStr="Hoffart et al., 2011" startWordPosition="2584" endWordPosition="2587">opagation on social media and also generate the context of the information at the same time. An early phase test was performed on the proposed pipeline so as to assess the viability of the framework. The framework was simulated with a sample data constituting of tweets from three different known events and it reflected encouraging results with respect to the viability of the underlying processes and the framework as a whole. The framework successfully clustered the sample data, using k-means algorithm, into unique clusters and the entity disambiguation phase, implemented using AIDA framework (Hoffart et al., 2011), yielded relevant entities. An end-to-end evaluation of the pipeline, however, is yet to be performed to analyse the results of every phase, and the pipeline as a whole. There are foreseen challenges such as noise filtering, the non-lexical nature of the data, and the verity of the content. The data from social media contains an enormous amount of noise (such as random tweets posted by users which do not have a relevance with the event and may yet contain the filtering keywords) in exhaustive social media streams when it comes to filtering the content specific to certain events/topics and tha</context>
</contexts>
<marker>Hoffart, Yosef, Bordino, Fürstenau, Pinkal, Spaniol, Taneva, Thater, Weikum, 2011</marker>
<rawString>Hoffart, J., Yosef, M. A., Bordino, I., Fürstenau, H., Pinkal, M., Spaniol, M., Taneva, B., Thater, S., Weikum, G. 2011. Robust disambiguation of named entities in text. InProceedings of the Conference on Empirical Methods in Natural Language Processing (pp. 782-792). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Probabilistic latent semantic analysis.</title>
<date>1999</date>
<booktitle>InProceedings of the Fifteenth conference on Uncertainty in artificial intelligence(pp.</booktitle>
<pages>289--296</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="10643" citStr="Hofmann, 1999" startWordPosition="1628" endWordPosition="1629"> based on regular time intervals, which may result in inferring only the highly dominating events/clusters present in the data. An index, between terms and corresponding documents (that initially contained those terms), is generated for this slab of data using standard libraries such as Lucene5 and Solr6 (built over Lucene). Clustering: In this sub-phase we derive the preliminary clusters of the data, which are likely to reflect the most related content within the data slab that was earlier created. Examples of the clustering algorithms that can be hired to cluster the data are k-means, PLSA (Hofmann, 1999) and LDA (Blei et al., 2003). After the clusters are formed, the terms with high weight in the clusters are taken to query the index for retrieving the most relevant documents based threshold relevance score. The relevance score is derived from term frequency and inverse document frequency (tf-idf) (Manning et al., 2008) value and accordingly the documents are retrieved. The text from those top scored documents can now be extracted and merged into one string, hereafter called event string, which tends to represent the infor4 https://dev.twitter.com/docs/api/streaming 5 http://lucene.apache.org</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Hofmann, T. 1999. Probabilistic latent semantic analysis. InProceedings of the Fifteenth conference on Uncertainty in artificial intelligence(pp. 289-296). Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hoffart</author>
<author>F M Suchanek</author>
<author>K Berberich</author>
<author>G Weikum</author>
</authors>
<title>YAGO2: A spatially and temporally enhanced knowledge base from Wikipedia.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<volume>194</volume>
<pages>28--61</pages>
<contexts>
<context position="7948" citStr="Hoffart et al., 2013" startWordPosition="1228" endWordPosition="1231">uan et al., 2012). A potential context of the content can likely be inferred by extracting set of topics that bound the text. Hulpus et al. (2013) proposed an approach by linking the topics inherent to a text with the concepts in DBpedia2 and thereby automatically extracting the topic labels from the corpus. Meij et al. (2012) extracted underlying concepts of a text from a large knowledge base of Wikipedia articles by applying a supervised learning using a Naive Bayes (NB), Support Vector Machines (SVM), and a C4.5 decision tree classifier. Large knowledge bases, such as YAGO3, are also used (Hoffart et al., 2013) to explore the inherent relationship between entities and disambiguate them to derive the context. Taking insights from various research works briefed above, we aim to construct a framework that is inspired by ideas from different works in the next section. 3 The Event Detection &amp; Contextualisation Framework There are various approaches to extract the information from data, by means of clustering, entity extraction and contextualisation, yet there is no observed pipeline that incorporates different methods and brings them under one framework so as to to generate insights from streaming social</context>
<context position="13268" citStr="Hoffart et al., 2013" startWordPosition="2032" endWordPosition="2035">ween mention and most relevant entity is termed as named entity disambiguation process. 3.3 Entity Disambiguation and Contextualisation In the entity disambiguation and contextualisation phase, initially an input text (web page, language paragraph, sentence, article) is resolved into various mentions of entities (surface mentions- that means its just a mention with no associated knowledge) by matching all the potential candidate entities with the surface mentions. For this purpose Stanford NER tagger is used. For each mention (a potential entity) knowledge sources such as DBpedia and/or Yago (Hoffart et al., 2013; Hoffart et al., 2011) are harvested to extract potential entity mentions. Each mention will then be mapped for numerous potential entity candidates. After extracting the candidate entities, a relevance score can be assigned to each based on features such as a prior for candidate entity popularity, mutual information (similarity between key-phrase or query string and description of the entity), syntax based similarity (Thater et al., 2010), entity-entity coherence (quantifying the number of similar incoming links on a knowledge base as Wikipedia). Milne and Witten (2008) extended few similari</context>
</contexts>
<marker>Hoffart, Suchanek, Berberich, Weikum, 2013</marker>
<rawString>Hoffart, J., Suchanek, F. M., Berberich, K., &amp; Weikum, G. 2013. YAGO2: A spatially and temporally enhanced knowledge base from Wikipedia. Artificial Intelligence, 194, 28-61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Hulpus</author>
<author>C Hayes</author>
<author>M Karnstedt</author>
<author>D Greene</author>
</authors>
<title>Unsupervised graph-based topic labelling using DBpedia.</title>
<date>2013</date>
<booktitle>In Proceedings of the sixth ACM international conference on Web search and data mining</booktitle>
<pages>465--474</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7473" citStr="Hulpus et al. (2013)" startWordPosition="1149" endWordPosition="1152">rs of Stanford NLP library1. Ritter et al. (2012) extending the above work created an application Twical, that extracted an open domain calendar for events that were shared on Twitter. For an event detection system, it is also crucial to determine the context of a piece of text/information. The contextualisation is answering the question ‘what is this about?’ and one of the ways to answer it could be by aggregating information from knowledge base such as Wikipedia (SanJuan et al., 2012). A potential context of the content can likely be inferred by extracting set of topics that bound the text. Hulpus et al. (2013) proposed an approach by linking the topics inherent to a text with the concepts in DBpedia2 and thereby automatically extracting the topic labels from the corpus. Meij et al. (2012) extracted underlying concepts of a text from a large knowledge base of Wikipedia articles by applying a supervised learning using a Naive Bayes (NB), Support Vector Machines (SVM), and a C4.5 decision tree classifier. Large knowledge bases, such as YAGO3, are also used (Hoffart et al., 2013) to explore the inherent relationship between entities and disambiguate them to derive the context. Taking insights from vari</context>
</contexts>
<marker>Hulpus, Hayes, Karnstedt, Greene, 2013</marker>
<rawString>Hulpus, I., Hayes, C., Karnstedt, M., &amp; Greene, D. 2013. Unsupervised graph-based topic labelling using DBpedia. In Proceedings of the sixth ACM international conference on Web search and data mining (pp. 465-474). ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>P Raghavan</author>
<author>H Schütze</author>
</authors>
<title>Introduction to information retrieval</title>
<date>2008</date>
<volume>1</volume>
<pages>6</pages>
<location>Cambridge: Cambridge university</location>
<note>press.</note>
<contexts>
<context position="10965" citStr="Manning et al., 2008" startWordPosition="1678" endWordPosition="1681">er Lucene). Clustering: In this sub-phase we derive the preliminary clusters of the data, which are likely to reflect the most related content within the data slab that was earlier created. Examples of the clustering algorithms that can be hired to cluster the data are k-means, PLSA (Hofmann, 1999) and LDA (Blei et al., 2003). After the clusters are formed, the terms with high weight in the clusters are taken to query the index for retrieving the most relevant documents based threshold relevance score. The relevance score is derived from term frequency and inverse document frequency (tf-idf) (Manning et al., 2008) value and accordingly the documents are retrieved. The text from those top scored documents can now be extracted and merged into one string, hereafter called event string, which tends to represent the infor4 https://dev.twitter.com/docs/api/streaming 5 http://lucene.apache.org/ 6 http://lucene.apache.org/solr/ 56 mation stored against a particular cluster or event. This event string is further used for NER and disambiguation. 3.2 Entity Recognition The event string, derived above, is further annotated for its entities by applying Named Entity Recognition techniques. NER is an information extr</context>
</contexts>
<marker>Manning, Raghavan, Schütze, 2008</marker>
<rawString>Manning, C. D., Raghavan, P., &amp; Schütze, H. 2008. Introduction to information retrieval (Vol. 1, p. 6). Cambridge: Cambridge university press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Meij</author>
<author>W Weerkamp</author>
<author>M de Rijke</author>
</authors>
<title>Adding semantics to microblog posts.</title>
<date>2012</date>
<booktitle>In Proceedings of the fifth ACM international conference on Web search and data mining</booktitle>
<pages>563--572</pages>
<publisher>ACM.</publisher>
<marker>Meij, Weerkamp, de Rijke, 2012</marker>
<rawString>Meij, E., Weerkamp, W., &amp; de Rijke, M. 2012. Adding semantics to microblog posts. In Proceedings of the fifth ACM international conference on Web search and data mining (pp. 563-572). ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milne</author>
<author>I H Witten</author>
</authors>
<title>Learning to link with wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th ACM conference on Information and knowledge management</booktitle>
<pages>509--518</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="13846" citStr="Milne and Witten (2008)" startWordPosition="2117" endWordPosition="2120"> as DBpedia and/or Yago (Hoffart et al., 2013; Hoffart et al., 2011) are harvested to extract potential entity mentions. Each mention will then be mapped for numerous potential entity candidates. After extracting the candidate entities, a relevance score can be assigned to each based on features such as a prior for candidate entity popularity, mutual information (similarity between key-phrase or query string and description of the entity), syntax based similarity (Thater et al., 2010), entity-entity coherence (quantifying the number of similar incoming links on a knowledge base as Wikipedia). Milne and Witten (2008) extended few similarity measures defined by Bunescu and Pasca (2006), which compared the context of a given text to the entities mention in Wikipedia. Considering the above features, a graph of mentions and candidate entities, with the edges as weights, can be generated. Each node will have a certain weight on its edge, a greedy approach can be adopted to iteratively remove the low weight nodes to disambiguate the entities (Hoffart et al., 2011). This approach will result in disambiguated entities (to a high degree) for each surface mentions of the input text and represent entities according </context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>Milne, D., &amp; Witten, I. H. 2008. Learning to link with wikipedia. In Proceedings of the 17th ACM conference on Information and knowledge management (pp. 509-518). ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Muthukrishnan</author>
</authors>
<title>Data streams: Algorithms and applications. Now Publishers Inc. Oriella.</title>
<date>2005</date>
<contexts>
<context position="5317" citStr="Muthukrishnan, 2005" startWordPosition="805" endWordPosition="807">k Identifying new events, in the form of news from the data, is an area of interest for researchers for a long time. Topic detection and tracking (TDT) (Allan, 2002) focuses on breaking down a streaming text from newswire into smaller cohesive news pieces and determining if something has not been earlier reported. An event detection cycle is seen as a subtask within TDT (Allan, 2002). The data from social media platforms, such as Twitter, is quite voluminous and the streaming nature of this data warrants the usage of streaming algorithm models, where the data arrives in a chronological order (Muthukrishnan, 2005). The social media data is further processed in a bounded space and time, i.e. as every entry arrives it gets processed. Traditional approaches for identifying new information (an event) were to compare each new entry in the data with the previously arrived entries. Petrovic et al. (2010) investigated ways to identify tweets that first report the occurrence of an event by clustering mechanism to identify nearest neighbours in the textual data. This work has motivated many other contemporary research works to head in a related direction Osborne et al. (2012) used the approach by Petrovic et al.</context>
</contexts>
<marker>Muthukrishnan, 2005</marker>
<rawString>Muthukrishnan, S. 2005. Data streams: Algorithms and applications. Now Publishers Inc. Oriella. 2013. The New Normal for News: Have global media Changed forever Oriella PR Network Global</rawString>
</citation>
<citation valid="true">
<authors>
<author>Digital Journalism Study</author>
</authors>
<date>2013</date>
<note>Available from http://www.oriellaprnetwork.com/sites/default/files/research/Brands2Life_ODJS_v4.pdf visited July 13th,2014</note>
<marker>Study, 2013</marker>
<rawString>Digital Journalism Study 2013. Available from http://www.oriellaprnetwork.com/sites/default/files/research/Brands2Life_ODJS_v4.pdf visited July 13th,2014</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Osborne</author>
<author>S Petrovic</author>
<author>R McCreadie</author>
<author>C Macdonald</author>
<author>I Ounis</author>
</authors>
<title>Bieber no more: First story detection using Twitter and Wikipedia. InProceedings of the Workshop on Time-aware Information Access.</title>
<date>2012</date>
<journal>TAIA (Vol.</journal>
<volume>12</volume>
<contexts>
<context position="5880" citStr="Osborne et al. (2012)" startWordPosition="894" endWordPosition="897">ata arrives in a chronological order (Muthukrishnan, 2005). The social media data is further processed in a bounded space and time, i.e. as every entry arrives it gets processed. Traditional approaches for identifying new information (an event) were to compare each new entry in the data with the previously arrived entries. Petrovic et al. (2010) investigated ways to identify tweets that first report the occurrence of an event by clustering mechanism to identify nearest neighbours in the textual data. This work has motivated many other contemporary research works to head in a related direction Osborne et al. (2012) used the approach by Petrovic et al. (2010) as a baseline and investigated the ways to improve the event detection mechanism on Twitter data, by matching the frequency of newly occurring events from tweets with the activity (number of visits on a page) of the corresponding pages of entities from Wikipedia and analysed if there was a similar pattern observed while determining an event. Parikh and Karlapalem (2013), also considering frequency based analysis, developed an event detection system that extracts events from tweets by examining frequencies in the temporal blocks of streaming data. Na</context>
</contexts>
<marker>Osborne, Petrovic, McCreadie, Macdonald, Ounis, 2012</marker>
<rawString>Osborne, M., Petrovic, S., McCreadie, R., Macdonald, C., &amp; Ounis, I. 2012. Bieber no more: First story detection using Twitter and Wikipedia. InProceedings of the Workshop on Time-aware Information Access. TAIA (Vol. 12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Parikh</author>
<author>K Karlapalem</author>
</authors>
<title>Et: events from tweets.</title>
<date>2013</date>
<booktitle>InProceedings of the 22nd international conference on World Wide Web companion</booktitle>
<pages>613--620</pages>
<institution>International World Wide Web Conferences Steering Committee.</institution>
<contexts>
<context position="6297" citStr="Parikh and Karlapalem (2013)" startWordPosition="962" endWordPosition="965">urrence of an event by clustering mechanism to identify nearest neighbours in the textual data. This work has motivated many other contemporary research works to head in a related direction Osborne et al. (2012) used the approach by Petrovic et al. (2010) as a baseline and investigated the ways to improve the event detection mechanism on Twitter data, by matching the frequency of newly occurring events from tweets with the activity (number of visits on a page) of the corresponding pages of entities from Wikipedia and analysed if there was a similar pattern observed while determining an event. Parikh and Karlapalem (2013), also considering frequency based analysis, developed an event detection system that extracts events from tweets by examining frequencies in the temporal blocks of streaming data. Natural Language Processing (NLP) techniques can be leveraged in detecting events from voluminous social media data. Events are associated with entities and NLP techniques can be applied to extract the entities that are mentioned in the text that defines an event. To perform Named Entity Recognition (NER) on tweets Ritter et. al. (2011) redeveloped the taggers and segmenters of Stanford NLP library1. Ritter et al. (</context>
</contexts>
<marker>Parikh, Karlapalem, 2013</marker>
<rawString>Parikh, R., &amp; Karlapalem, K. 2013. Et: events from tweets. InProceedings of the 22nd international conference on World Wide Web companion (pp. 613-620). International World Wide Web Conferences Steering Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrović</author>
<author>M Osborne</author>
<author>V Lavrenko</author>
</authors>
<title>Streaming first story detection with application to twitter.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</booktitle>
<pages>181--189</pages>
<institution>Association for Computational Linguistics.</institution>
<marker>Petrović, Osborne, Lavrenko, 2010</marker>
<rawString>Petrović, S., Osborne, M., &amp; Lavrenko, V. 2010. Streaming first story detection with application to twitter. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (pp. 181-189). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritter</author>
<author>S Clark</author>
<author>O Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing</booktitle>
<pages>1524--1534</pages>
<institution>Association for Computational Linguistics.</institution>
<marker>Ritter, Clark, Etzioni, 2011</marker>
<rawString>Ritter, A., Clark, S., &amp; Etzioni, O. 2011. Named entity recognition in tweets: an experimental study. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (pp. 1524-1534). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritter</author>
<author>O Etzioni</author>
<author>S Clark</author>
</authors>
<title>Open domain event extraction from twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</booktitle>
<pages>1104--1112</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6902" citStr="Ritter et al. (2012)" startWordPosition="1053" endWordPosition="1056">rlapalem (2013), also considering frequency based analysis, developed an event detection system that extracts events from tweets by examining frequencies in the temporal blocks of streaming data. Natural Language Processing (NLP) techniques can be leveraged in detecting events from voluminous social media data. Events are associated with entities and NLP techniques can be applied to extract the entities that are mentioned in the text that defines an event. To perform Named Entity Recognition (NER) on tweets Ritter et. al. (2011) redeveloped the taggers and segmenters of Stanford NLP library1. Ritter et al. (2012) extending the above work created an application Twical, that extracted an open domain calendar for events that were shared on Twitter. For an event detection system, it is also crucial to determine the context of a piece of text/information. The contextualisation is answering the question ‘what is this about?’ and one of the ways to answer it could be by aggregating information from knowledge base such as Wikipedia (SanJuan et al., 2012). A potential context of the content can likely be inferred by extracting set of topics that bound the text. Hulpus et al. (2013) proposed an approach by link</context>
</contexts>
<marker>Ritter, Etzioni, Clark, 2012</marker>
<rawString>Ritter, A., Etzioni, O., &amp; Clark, S. 2012. Open domain event extraction from twitter. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1104-1112). ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Thater</author>
<author>H Fürstenau</author>
<author>M Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>948--957</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="13712" citStr="Thater et al., 2010" startWordPosition="2098" endWordPosition="2101">th the surface mentions. For this purpose Stanford NER tagger is used. For each mention (a potential entity) knowledge sources such as DBpedia and/or Yago (Hoffart et al., 2013; Hoffart et al., 2011) are harvested to extract potential entity mentions. Each mention will then be mapped for numerous potential entity candidates. After extracting the candidate entities, a relevance score can be assigned to each based on features such as a prior for candidate entity popularity, mutual information (similarity between key-phrase or query string and description of the entity), syntax based similarity (Thater et al., 2010), entity-entity coherence (quantifying the number of similar incoming links on a knowledge base as Wikipedia). Milne and Witten (2008) extended few similarity measures defined by Bunescu and Pasca (2006), which compared the context of a given text to the entities mention in Wikipedia. Considering the above features, a graph of mentions and candidate entities, with the edges as weights, can be generated. Each node will have a certain weight on its edge, a greedy approach can be adopted to iteratively remove the low weight nodes to disambiguate the entities (Hoffart et al., 2011). This approach </context>
</contexts>
<marker>Thater, Fürstenau, Pinkal, 2010</marker>
<rawString>Thater, S., Fürstenau, H., &amp; Pinkal, M. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (pp. 948-957). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tjong Kim Sang</author>
<author>E F</author>
<author>F De Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 shared task: Language independent named entity recognition.</title>
<date>2003</date>
<booktitle>InProceedings of the seventh conference on Natural language learning at HLT-NAACL</booktitle>
<volume>2003</volume>
<pages>142--147</pages>
<institution>Association for Computational Linguistics.</institution>
<marker>Sang, F, De Meulder, 2003</marker>
<rawString>Tjong Kim Sang, E. F., &amp; De Meulder, F. 2003. Introduction to the CoNLL-2003 shared task: Language independent named entity recognition. InProceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4 (pp. 142-147). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Zaki</author>
<author>Meira Jr</author>
<author>W</author>
</authors>
<title>Fundamentals of data mining algorithms.</title>
<date>2011</date>
<marker>Zaki, Jr, W, 2011</marker>
<rawString>Zaki, M. J., &amp; Meira Jr, W. 2011. Fundamentals of data mining algorithms.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
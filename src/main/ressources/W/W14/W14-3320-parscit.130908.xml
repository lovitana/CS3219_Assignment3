<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001711">
<title confidence="0.994894">
The UA-Prompsit hybrid machine translation system for the 2014
Workshop on Statistical Machine Translation
</title>
<author confidence="0.994617">
Victor M. S´anchez-Cartagena,∗ ‡ Juan Antonio P´erez-Ortiz,∗ Felipe S´anchez-Martinez∗
</author>
<affiliation confidence="0.749959666666667">
∗Dep. de Llenguatges i Sistemes Inform`atics,
Universitat d’Alacant, E-03071, Alacant, Spain
‡Prompsit Language Engineering,
</affiliation>
<address confidence="0.792487">
Av. Universitat, s/n. Edifici Quorum III, E-03202, Elx, Spain
</address>
<email confidence="0.998714">
{vmsanchez,japerez,fsanchez}@dlsi.ua.es
</email>
<sectionHeader confidence="0.997388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999865058823529">
This paper describes the system jointly de-
veloped by members of the Departament
de Llenguatges i Sistemes Inform`atics
at Universitat d’Alacant and the Promp-
sit Language Engineering company for
the shared translation task of the 2014
Workshop on Statistical Machine Trans-
lation. We present a phrase-based sta-
tistical machine translation system whose
phrase table is enriched with information
obtained from dictionaries and shallow-
transfer rules like those used in rule-based
machine translation. The novelty of our
approach lies in the fact that the transfer
rules used were not written by humans, but
automatically inferred from a parallel cor-
pus.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968732142857">
This paper describes the system jointly submitted
by the Departament de Llenguatges i Sistemes In-
form`atics at Universitat d’Alacant and the Promp-
sit Language Engineering company to the shared
translation task of the ACL 2014 Ninth Workshop
on Statistical Machine Translation (WMT 2014).
We participated in the English–French translation
task with a hybrid system that combines, in a
phrase-based statistical machine translation (PB-
SMT) system, bilingual phrases obtained from par-
allel corpora in the usual way (Koehn, 2010, ch.
5), and also bilingual phrases obtained from the
existing dictionaries in the Apertium rule-based
machine translation (RBMT) platform (Forcada et
al., 2011) and a number of shallow-transfer ma-
chine translation rules automatically inferred from
a small subset of the training corpus.
Among the different approaches for adding lin-
guistic information to SMT systems (Costa-Juss`a
and Farr´us, 2014), we followed the path we started
with our submission to the Spanish–English WMT
2011 shared translation task (S´anchez-Cartagena
et al., 2011b) which consisted of enriching the
phrase table of a PBSMT system with phrase pairs
generated using the dictionaries and rules in the
Apertium (Forcada et al., 2011) Spanish–English
RBMT system; our approach was one of the win-
ners1 (together with two online SMT systems that
were not submitted for the task but were included
in the evaluation by the organisers and a system by
Systran) in the pairwise manual evaluation of the
English–Spanish translation task (Callison-Burch
et al., 2011). In this submission, however, we
only borrow the dictionaries from the Apertium
English–French RBMT system and use them to au-
tomatically infer the rules from a parallel corpus.
We therefore avoid the need for human-written
rules, which are usually written by trained experts,
and explore a novel way to add morphological
information to PBSMT. The rules inferred from
corpora and used to enlarge the phrase table are
shallow-transfer rules that build their output with
the help of the bilingual dictionary and work on
flat intermediate representations (see section 3.1);
no syntactic parsing is consequently required.
The rest of the paper is organised as follows.
The following section outlines related hybrid ap-
proaches. Section 3 formally defines the RBMT
paradigm and summarises the method followed
to automatically infer the shallow-transfer rules,
whereas the enrichment of the phrase table is de-
scribed in section 4. Sections 5 and 6 describe, re-
spectively, the resources we used to build our sub-
mission and the results achieved for the English–
French language pair. The paper ends with some
concluding remarks.
</bodyText>
<sectionHeader confidence="0.999555" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.806915">
Linguistic data from RBMT systems have already
been used to enrich SMT systems (Tyers, 2009;
Schwenk et al., 2009; Eisele et al., 2008; S´anchez-
Cartagena et al., 2011a). We have already proved
</bodyText>
<footnote confidence="0.940816">
1No other system was found statistically significantly bet-
ter using the sign test at p ≤ 0.10.
</footnote>
<page confidence="0.867398">
178
</page>
<note confidence="0.725788">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 178–185,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999800704545455">
that using hand-written rules and dictionaries from
RBMT yields better results than using only dictio-
naries (S´anchez-Cartagena et al., 2011a).
However, in the approach we present in this pa-
per, rules are automatically inferred from a paral-
lel corpus after converting it into the intermedi-
ate representation used by the Apertium RBMT
platform (see section 3.3). It can be therefore
seen as a novel method to add morphological in-
formation to SMT, as factored translation models
do (Koehn and Hoang, 2007; Graham and van
Genabith, 2010). Unlike factored models, we do
not estimate independent statistical models for the
translation of the different factors (lemmas, lexi-
cal categories, morphological inflection attributes,
etc.) and for the generation of the final surface
forms. Instead, we first infer a set of rules that deal
with the grammatical divergences between the lan-
guages involved by performing operations such as
reorderings, gender and number agreements, etc.
Afterwards, we add synthetic phrase pairs gener-
ated from these rules and the Apertium dictionar-
ies to the data from which the well-known, classi-
cal PBSMT models (Koehn, 2010) are estimated.
The rules in our approach operate on the source-
language (SL) morphological attributes of the in-
put words and on the target-language (TL) mor-
phological attributes of their translation according
to a bilingual dictionary. In addition, they do no
contain probabilities or scores, thus they increase
the predictability of the output and can be easily
corrected by humans. This fact also represents a
significant difference with the probabilistic rules
used by certain approaches that aim at improving
the grammaticality of the SMT output (Riezler and
Maxwell III, 2006; Bojar and Hajiˇc, 2008).
With respect to the rule inference approach,
other approaches such as those by S´anchez-
Mart´ınez and Forcada (2009) and Caseli et al.
(2006) can be found in literature; however, our ap-
proach is the first strategy for shallow-transfer rule
inference which generalises to unseen combina-
tions of morphological inflection attributes in the
training corpus (S´anchez-Cartagena et al., 2014).
</bodyText>
<sectionHeader confidence="0.991572" genericHeader="method">
3 Inferring shallow-transfer rules from
parallel corpora
</sectionHeader>
<subsectionHeader confidence="0.9917635">
3.1 Shallow-transfer rule-based machine
translation
</subsectionHeader>
<bodyText confidence="0.9996939">
The RBMT process can be split into three different
steps (Hutchins and Somers, 1992): (i) analysis of
the SL text to build an SL intermediate represen-
tation; (ii) transfer from that SL intermediate rep-
resentation into a TL intermediate representation;
and (iii) generation of the final translation from the
TL intermediate representation.
Shallow-transfer RBMT systems use relatively
simple intermediate representations, which are
based on lexical forms consisting of lemma, part
of speech and morphological inflection informa-
tion of the words, and apply simple shallow-
transfer rules that operate on sequences of lexical
forms: this kind of systems do not perform full
parsing. For instance, for translating the English
sentence I like Pierre’s house into French with
the Apertium shallow-transfer RBMT platform we
have used to build our submission, the following
steps are carried out. First, the sentence is anal-
ysed as the following sequence of lexical forms:
</bodyText>
<equation confidence="0.7991902">
IPRN-p:1.num:sg
like VB-t:pres.p:ǫ:num:ǫ
Pierre PN
’s POS
house N-gen:ǫ.num:sg
</equation>
<bodyText confidence="0.999530888888889">
This sequence is made up of a personal pronoun
(PRN) in first person (p:1) singular (num:sg)
with lemma I, the verb (VB) like in present tense
(t:pres), a proper noun (PN) with lemma Pierre,
the possessive ending (POS), and a noun (N) in sin-
gular with lemma house. Some morphological in-
flection attributes have an empty value ǫ because
they do not apply to the corresponding language.
Then, structural transfer rules are applied to ob-
tain the TL intermediate representation with the
help of the bilingual dictionary, which provides
the individual translation of each SL lexical form
(including its morphological information). In this
case, two rules are applied: the first one makes the
verb to agree with the personal pronoun, while the
second one translates the English possessive con-
struction into French. The resulting sequence of
TL lexical forms is:
</bodyText>
<figure confidence="0.583099833333333">
JePRN-p:1.num:sg
aime VB-t:pres.p:1:num:sg
leDT-gen:f.num:sg
maison N-gen:f.num:sg
de PR
Pierre PN
</figure>
<bodyText confidence="0.987068166666667">
Note that a preposition (PR) with lemma de and a
determiner (DT) with lemma le and the same gen-
der and number as the common noun have been
added by the rule. Finally, the translation into TL
is generated from the TL lexical forms: J’aime la
maison de Pierre.
</bodyText>
<page confidence="0.936686">
179
</page>
<equation confidence="0.999717">
t1 : le DT-gen:$3t .num:$3s
s1 : PN
s2 : POS
t2 : N-gen:$3t .num:$3s
s3 : N-gen:*.num:*
t3 : de PR
t4 : PN
</equation>
<figureCaption confidence="0.998572">
Figure 1: Shallow-transfer rule for the translation of the English Saxon genitive construction into French.
</figureCaption>
<subsectionHeader confidence="0.906741">
3.2 A rule formalism suitable for rule
inference
</subsectionHeader>
<bodyText confidence="0.999917882352942">
Figure 1 shows the second rule applied in the
example from the previous section encoded with
the formalism we have defined for rule infer-
ence (S´anchez-Cartagena et al., 2014). Each rule
contains a sequence of SL word classes (depicted
as the sequence of boxes at the top of the figure)
and TL word classes (the sequence of boxes be-
low them). The sequence of SL word classes de-
fines the set of sequences of lexical forms which
will match the rule. Each SL word class si defines
the conditions that must be met by the i-th lexical
form matching the rule and contains an optional
lemma (no lemma means that any SL lemma is al-
lowed), a lexical category and a set of morpholog-
ical inflection attributes and their expected values.
A wildcard (asterisk) as the value of a morpholog-
ical inflection attribute means that it matches any
possible value. Thus, the rule from the example
matches any proper noun followed by a possessive
ending and a noun, regardless of its gender and
number.
As regards the TL word classes, they contain
the same elements as the SL word classes and de-
fine the output of the rule. An empty lemma in a
TL word class means that it is obtained by looking
up in the bilingual dictionary the SL lexical form
matching the aligned SL word class (alignments
are represented as lines connecting SL and TL
word classes). The reference value $i smeans that
the value of a morphological inflection attribute is
copied from the SL lexical form matching the i-th
SL word class, while the reference value $it means
that the value is taken from the TL lexical form ob-
tained after looking up in the bilingual dictionary
the aforementioned SL lexical form. The rule de-
picted in Figure 1 generates a sequence of four TL
lexical forms. The first one is a determiner whose
lemma is le, its gender is obtained from the gender
of the TL lexical form resulting after looking up in
the bilingual dictionary the third matching SL lex-
ical form ($3t), that is, the common noun, while its
number is directly obtained from the same SL lexi-
cal form before dictionary look-up ($3s). Although
they have not been used in this example, explicit
values can be used in the morphological inflection
attributes of the SL and TL word classes, thus re-
stricting the SL lexical forms to which the rule can
be applied to those having the values in the corre-
sponding SL word classes,2 and explicitly stating
the value that the TL lexical forms produced by
the rule will have, respectively.
</bodyText>
<subsectionHeader confidence="0.99622">
3.3 Rule inference algorithm
</subsectionHeader>
<bodyText confidence="0.936091060606061">
The set of rules that will be used to generate the
phrase pairs that will be integrated into the PB-
SMT system’s phrase table, encoded with the for-
malism presented in the previous section, are ob-
tained from the parallel corpus by applying the
steps described in this section. They are a subset
of the steps followed by S´anchez-Cartagena et al.
(2014) to infer shallow-transfer rules to be used in
Apertium from small parallel corpora.
First, both sides of the parallel corpus are mor-
phologically analysed and converted into the inter-
mediate representations used by Apertium. Word
alignments are then obtained by symmetrising
(using the refined intersection method proposed
by Och and Ney (2003)) the set of alignments
provided by GIZA++ (Och and Ney, 2003) when
it is run on both translations directions. After-
wards, the bilingual phrase pairs compatible with
the alignments are extracted as it is usually done
in SMT (Koehn, 2010, Sec. 5.2.3), and those that
are not compatible with the bilingual dictionary of
the Apertium English–French RBMT system3 or
2In addition to that criterion, our formalism also permits
restricting the application of a rule to the SL lexical forms
that, after being looked up in the bilingual dictionary, the
TL lexical forms obtained from them have specific morpho-
logical inflection attribute values (S´anchez-Cartagena et al.,
2014) although no restrictions of this type are imposed in the
rule depicted in Figure 1.
3If the words that belong to open lexical categories (those
that carry the meaning of the sentence: nouns, verbs, adjec-
tives, etc.) are aligned with other words that do not match
the translation present in the bilingual dictionary, the rule in-
</bodyText>
<page confidence="0.988975">
180
</page>
<bodyText confidence="0.999695181818182">
contain punctuation marks or unknown words are
discarded. Finally, from each bilingual phrase pair,
all the possible rules which correctly reproduce it
—when the rule is applied to the SL side of the
phrase pair, its TL side is obtained— are gener-
ated as follows. First, a very specific rule, which
matches only the SL phrase in the bilingual phrase
pair is generated; more general rules are then cre-
ated by modifying this initial rule. The modifica-
tions to the initial rule consist of removing lem-
mas from the SL and TL word classes, introduc-
ing wildcard values in the morphological inflec-
tion attributes of the SL word classes and adding
reference values in the morphological inflection at-
tributes of the TL word classes. The result of this
process is a huge set of rules with different levels
of generalisation. Obviously, not all the rules in
this set will be used: the best ones are automati-
cally selected by considering all the rules obtained
from the different bilingual phrase pairs extracted
from the corpus and finding the minimum set of
rules that meets the following two conditions:
</bodyText>
<listItem confidence="0.985733571428571">
1. Each bilingual phrase pair is correctly repro-
duced by at least one rule.
2. If a rule matches the SL side of bilingual
phrase pair but does not correctly reproduce
its TL side, there is another rule that is more
specific (i.e. less general) than it, and cor-
rectly reproduces its TL side.
</listItem>
<bodyText confidence="0.892435904761904">
This minimisation problem is formulated as an in-
teger linear programming4 problem (Garfinkel and
Nemhauser, 1972) and solved using the branch
and cut algorithm (Xu et al., 2009).
From the small subset of the huge initial rules
obtained by solving the minimisation problem, the
rules whose effect can be achieved by combining
shorter rules or by translating all or some of the
words in isolation (i.e. word for word) are re-
moved. In this way, the number of rules is further
reduced and long rules, which are more prone to
overgeneralisation because they are inferred from
fewer bilingual phrase pairs, are discarded.5
ference algorithm is likely to infer many very specific rules
that try to correct that lexical mismatch. Since the aim of
our approach is learning general rules that deal with the
grammatical divergences between languages, the bilingual
phrases that contain the aforementioned alignments are dis-
carded. Words from closed lexical categories, that usually
suffer deeper changes when the sentence is translated to a dif-
ferent language, are not subject to this restriction.
</bodyText>
<footnote confidence="0.86881275">
4An integer linear programming problem involves the op-
timisation (maximisation or minimisation) of a linear objec-
tive function subject to linear inequality constraints.
5Although longer rules contain more context information,
</footnote>
<sectionHeader confidence="0.8723235" genericHeader="method">
4 Enhancing phrase-based SMT with
shallow-transfer linguistic resources
</sectionHeader>
<bodyText confidence="0.99941374509804">
The set of shallow-transfer rules inferred from the
parallel corpus are integrated in the PBSMT sys-
tem, together with the RBMT dictionaries, using
the same method we used for our WMT 2011
shared translation task subsmission (S´anchez-
Cartagena et al., 2011b). However, it is important
to stress that, until now, this strategy had only been
tested when the rules to be integrated were hand-
written and not automatically obtained from cor-
pora.
Our strategy involves adding to the phrase ta-
ble of the PBSMT system all the bilingual phrase
pairs which either match a shallow-transfer rule or
an entry in the bilingual dictionary. Generating the
set of bilingual phrase pairs which match bilingual
dictionary entries is straightforward. First, all the
SL surface forms that are recognised by Apertium
and their corresponding lexical forms are gener-
ated. Then, these SL lexical forms are translated
using the bilingual dictionary, and finally their TL
surface forms are generated.
Bilingual phrase pairs which match structural
transfer rules are generated in a similar way. First,
the SL sentences to be translated are analysed with
Apertium to get their SL lexical forms, and then
the sequences of lexical forms that match a struc-
tural transfer rule are translated with that rule and
passed through the rest of the Apertium pipeline
in order to get their translations. If a sequence
of SL lexical forms is matched by more than one
structural transfer rule, it will be used to generate
as many bilingual phrase pairs as different rules
it matches. This differs from the way in which
Apertium translates, as it only applies the longest
rule. Note also that the test set is used to guide the
phrase extraction in order to avoid the generation
of an unmanageable set of phrase pairs.
We add these bilingual phrase pairs directly to
the phrase table, rather than adding them to the
training corpus and relying on the phrase extrac-
tion algorithm (Koehn, 2010, sec. 5.2.3), in order
to avoid splitting the multi-word expressions pro-
vided by Apertium into smaller phrases (Schwenk
et al., 2009, sec. 2). The bilingual phrase pairs
are added only once to the list of corpus-extracted
phrase pairs, and then the phrase translation prob-
abilities are computed by relative frequency as
usual (Koehn, 2010, sec. 5.2.5). A boolean feature
for our rule inferring algorithm there are fewer bilingual
phrases from which to infer them, and consequently fewer
evidence from which to extract the right reference attributes.
</bodyText>
<page confidence="0.997075">
181
</page>
<bodyText confidence="0.99766225">
function to flag bilingual phrase pairs obtained
from the RBMT resources is added to the phrase
table in order to conveniently weight the synthetic
RBMT phrase pairs.
</bodyText>
<sectionHeader confidence="0.970234" genericHeader="method">
5 System training
</sectionHeader>
<bodyText confidence="0.999929071428571">
We built a baseline PBSMT Moses (Koehn et
al., 2007) system6 from a subset of the paral-
lel corpora distributed as part of the WMT 2014
shared translation task, namely Europarl (Koehn,
2005), News Commentary and Common Crawl,
and a subset of the French monolingual corpora,
namely Common Crawl, Europarl, News Com-
mentary and News Crawl. The language model
was built with the KenLM language modelling
toolkit (Heafield et al., 2013), which was used
to train a 5-gram language model using inter-
polated Kneser-Ney discounting (Goodman and
Chen, 1998). Word alignments were computed
by means of GIZA++ (Och and Ney, 2003). The
weights of the different feature functions were op-
timised by means of minimum error rate train-
ing (Och, 2003) on the 2013 WMT test set.7
The phrase table of this baseline system was
then enriched with phrase pairs generated from
rules automatically inferred from the concatena-
tion of the test corpora distributed for the WMT
2008–2012 shared translation tasks, and from the
English–French bilingual dictionary in the Aper-
tium platform.8 Since the minimisation problem
which needs to be solved in order to obtain the
rules is very time-consuming, we chose a small
rule inference corpus similar to this year’s test set.
The bilingual dictionary, which contains mappings
between SL and TL lemmas, consists of 13 088 en-
tries and is quite small compared to the Spanish–
English bilingual dictionary we used in our sub-
mission to WMT 2011 (S´anchez-Cartagena et al.,
2011b), which consisted of 326 228 bilingual en-
tries. This is because the English–French Aper-
tium linguistic resources were automatically built
by crossing data from other existing language
pairs.
Table 1 summarises the data about the corpora
used to build our submission, both for the PBSMT
baseline system and for the rules used to enrich its
phrase table.
The corpus used to automatically infer the rules
</bodyText>
<footnote confidence="0.9856092">
6No factored models were used.
7The corpora can be downloaded from http://www.
statmt.org/wmt14/translation-task.html.
8https://svn.code.sf.net/p/apertium/
svn/incubator/apertium-en-fr
</footnote>
<table confidence="0.999682428571429">
Task Corpus Sentences
Europarl 2 007 723
News Commentary 183 251
Translation model Common Crawl 3 244 152
Total 5 435 126
Total clean 4 196 987
Common Crawl 3 244 152
Language model Europarl 2 190 579
News Commentary 227 013
News Crawl 30 451 749
Total 36 113 493
Rule inference newstest 2008–2012 13 071
Tuning newstest2013 3 000
Test newstest2014 3 003
</table>
<tableCaption confidence="0.6475845">
Table 1: Size of the corpora used in the experi-
ments. The bilingual training corpora was cleaned
up to remove empty parallel sentences and those
containing more than 40 tokens.
</tableCaption>
<bodyText confidence="0.999959944444444">
was split into two parts: the larger one (4/5 of
the corpus) was used for actual rule inference as
described in section 3.3; the remaining corpus
was used as a development corpus as explained
next. For each rule z, first the proportion r(z) of
bilingual phrase pairs correctly reproduced by the
rule divided by the number of bilingual phrases
it matches is computed. Rules whose proportion
r(z) is lower than a threshold value 6 are then
discarded before solving the minimisation prob-
lem. The value of 6 is chosen so that it maximises,
on the development corpus, the BLEU score (Pap-
ineni et al., 2002) obtained by an Apertium-based
system which uses the inferred rules; in our sub-
mission 6 = 0.15. In addition, rules that do not
correctly reproduce at least 100 bilingual phrase
pairs were also discarded in order to make the min-
imisation problem computationally feasible.
</bodyText>
<sectionHeader confidence="0.999109" genericHeader="evaluation">
6 Results and discussion
</sectionHeader>
<bodyText confidence="0.9998413125">
Table 2 reports the translation performance as
measured by BLEU (Papineni et al., 2002),
TER (Snover et al., 2006) and METEOR (Baner-
jee and Lavie, 2005) achieved by the baseline PB-
SMT, our submission (UA-Prompsit), Apertium
when it uses the set of inferred rules, and Aper-
tium when it uses no rules at all (word-for-word
translation). The size of the phrase table and the
amount of unknown words in the test set are also
reported when applicable.
According to the three evaluation metrics, the
translation performance of our submission is very
close to that of the PBSMT baseline (slightly bet-
ter according to BLEU and TER, and slightly
worse according to METEOR). The difference be-
tween both systems computed by paired bootstrap
</bodyText>
<page confidence="0.992156">
182
</page>
<table confidence="0.9995582">
system BLEU TER METEOR # of unknown words phrase table size
baseline 0.3232 0.5807 0.5441 870 100 530 734
UA-Prompsit 0.3258 0.5781 0.5432 861 100 585 182
Apertium-rules 0.0995 0.7767 0.3168 4 743 -
Apertium-word-for-word 0.0631 0.8368 0.2617 4 743 -
</table>
<tableCaption confidence="0.991254">
Table 2: Case-insensitive BLEU, TER, and METEOR scores obtained, on the newstest2014 test set, by
</tableCaption>
<bodyText confidence="0.965546787234043">
the baseline PBSMT system (baseline), the hybrid system submitted to the WMT 2014 shared translation
task (UA-Prompsit), Apertium when it uses the set of inferred rules (Apertium-rules), and Apertium
when it uses no rules at all (Apertium-word-for-word). The number of unknown words and the size of
the phrase table are also reported when applicable.
resampling (Koehn, 2004) is not statistically sig-
nificant for any of the three evaluation metrics
(1000 iterations, p = 0.05).
An inspection of the 86 rules inferred shows
that they encode some of the transformations that
one would expect from a set of English–French
rules, such as gender and number agreements be-
tween nouns, determiners and adjectives, prepo-
sition changes, and the introduction of the aux-
iliary verb avoir for the past tense. In addition,
the improvement over word-for-word translation
achieved when they are used by Apertium is statis-
tically significant for the three evaluation metrics.
One of the reasons for not improving the base-
line PBMT system might be the small coverage
of the Apertium dictionaries. As already men-
tioned in the previous section, the English–French
bilingual dictionary has a low number of entries
compared to more mature language pairs in Aper-
tium which have around 20 times more bilingual
entries. Table 1 shows some effects of such a
small dictionary: the number of unknown words
for the Apertium-based system is really high, and
with regards to UA-Prompsit, its coverage barely
increases when compared to the PBSMT baseline.
We plan to test the approach presented in this paper
with language pairs for which more mature dictio-
naries are available in the Apertium project.
In addition to this, due to the tight schedule, we
had to remove the rules not reproducing at least
100 bilingual phrase pairs in order to solve the min-
imisation problem in a short amount of time. This
has clearly reduced the amount of rules inferred
and prevented some useful information present in
the parallel corpus from being incorporated in the
form of rules. For instance, no rule matching a
sequence longer than 3 lexical forms has been ex-
tracted (long bilingual phrases are less frequent
than short ones). Future research directions for
alleviating this problem include setting the mini-
mum number of reproduced bilingual phrases in-
dependently for each sequence of SL lexical cate-
gories (S´anchez-Cartagena et al., 2014).
</bodyText>
<sectionHeader confidence="0.985723" genericHeader="conclusions">
7 Concluding remarks
</sectionHeader>
<bodyText confidence="0.999982692307692">
We have presented the MT system submitted
jointly by the Departament de Llenguatges i Sis-
temes Inform`atics at Universitat d’Alacant and
Prompsit Language Engineering to the WMT
2014 shared translation task. We developed a
hybrid system for the English–French language
pair which enriches the phrase table of a stan-
dard PBSMT system with phrase pairs generated
from the Apertium RBMT dictionaries and a set of
shallow-transfer rules automatically inferred from
a parallel corpus, also with the help of the dic-
tionaries. This submission aims at solving one
strong limitation of a previous submission of our
team (S´anchez-Cartagena et al., 2011b): the need
for a hand-crafted set of shallow-transfer rules,
which can only be written by people with a deep
knowledge of the languages involved. Our ap-
proach outperforms a standard PBSMT system
built from the same data by a small, non statisti-
cally significant margin, according to two of the
three evaluation metrics used. The low coverage
of the dictionaries used and the aggressive pruning
carried out when solving the minimisation prob-
lem needed to infer the rules are probably the rea-
sons behind such a small improvement over the
baseline.
</bodyText>
<sectionHeader confidence="0.996489" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.993449111111111">
Work funded by Universitat d’Alacant through
project GRE11-20, by the Spanish Ministry
of Economy and Competitiveness through
projects TIN2009-14009-C02-01 and TIN2012-
32615, by Generalitat Valenciana through grant
ACIF/2010/174 (VALi+d programme), and by
the European Union Seventh Framework Pro-
gramme FP7/2007-2013 under grant agreement
PIAP-GA-2012-324414 (Abu-MaTran).
</bodyText>
<page confidence="0.998832">
183
</page>
<sectionHeader confidence="0.996333" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999419435185186">
S. Banerjee and A. Lavie. 2005. Meteor: An auto-
matic metric for mt evaluation with improved corre-
lation with human judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summa-
rization, pages 65–72.
O. Bojar and J. Hajiˇc. 2008. Phrase-based and deep
syntactic English-to-Czech statistical machine trans-
lation. In Proceedings of the third Workshop on Sta-
tistical Machine translation, pages 143–146. Associ-
ation for Computational Linguistics.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 workshop on statisti-
cal machine translation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
22–64, Edinburgh, Scotland, July. Association for
Computational Linguistics.
H. M. Caseli, M. G. V. Nunes, and M. L. Forcada. 2006.
Automatic induction of bilingual resources from
aligned parallel corpora: application to shallow-
transfer machine translation. Machine Translation,
20(4):227–245. Published in 2008.
M. R. Costa-Juss`a and M. Farr´us. 2014. Statistical
machine translation enhancements through linguis-
tic levels: A survey. ACM Comput. Surv., 46(3).
A. Eisele, C. Federmann, H. Saint-Amand, M. Jelling-
haus, T. Herrmann, and Y. Chen. 2008. Us-
ing Moses to integrate multiple rule-based machine
translation engines into a hybrid system. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 179–182, Columbus, Ohio.
M. L. Forcada, M. Ginestf-Rosell, J. Nordfalk,
J. O’Regan, S. Ortiz-Rojas, J. A. P´erez-Ortiz,
G. Ramfrez-S´anchez F. S´anchez-Martfnez, and F. M.
Tyers. 2011. Apertium: a free/open-source platform
for rule-based machine translation. Machine Trans-
lation, 25(2):127–144. Special Issue: Free/Open-
Source Machine Translation.
R. S. Garfinkel and G. L. Nemhauser. 1972. Integer
programming, volume 4. Wiley New York.
J. Goodman and S. F. Chen. 1998. An empirical
study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Harvard Univer-
sity, August.
Y. Graham and J. van Genabith. 2010. Factor tem-
plates for factored machine translation models. In
IWSLT 2010 : 7th International Workshop on Spo-
ken Language Translation, pages 275–283.
K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013. Scalable modified Kneser-Ney language
model estimation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics, pages 690–696, Sofia, Bulgaria, August.
W. J. Hutchins and H. L. Somers. 1992. An introduc-
tion to machine translation, volume 362. Academic
Press New York.
P. Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868–876,
Prague.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Annual Meeting of the Association for Computa-
tional Linguistics (ACL), demonstration session.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, volume 4, pages 388–395.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In Proceedings of the Tenth
Machine Translation Summit, pages 12–16, Phuket,
Thailand, September.
P. Koehn. 2010. Statistical Machine Translation. Cam-
bridge University Press.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29:19–51, March.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics, pages 160–167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the ACL, pages 311–318.
S. Riezler and J. T. Maxwell III. 2006. Grammati-
cal machine translation. In Proceedings of the main
conference on Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion of Computational Linguistics, pages 248–255.
Association for Computational Linguistics.
V. M. S´anchez-Cartagena, F. S´anchez-Martfnez, and
J. A. P´erez-Ortiz. 2011a. Integrating shallow-
transfer rules into phrase-based statistical machine
translation. In Proceedings of the XIII Machine
Translation Summit, pages 562–569, Xiamen, China,
September.
V. M. S´anchez-Cartagena, F. S´anchez-Martfnez, and
J. A. P´erez-Ortiz. 2011b. The Universitat d’Alacant
hybrid machine translation system for wmt 2011. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 457–463, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.987255">
184
</page>
<reference confidence="0.999823064516129">
V. M. S´anchez-Cartagena, J. A. P´erez-Ortiz, and
F. S´anchez-Martinez. 2014. A generalised align-
ment template formalism and its application to the
inference of shallow-transfer machine translation
rules from scarce bilingual corpora. Computer
Speech and Language. Submitted to the Special Is-
sue on Hybrid Machine Translation.
F. S´anchez-Martinez and M. L. Forcada. 2009. Infer-
ring shallow-transfer machine translation rules from
small parallel corpora. Journal of Artificial Intelli-
gence Research, 34(1):605–635.
H. Schwenk, S. Abdul-Rauf, L. Barrault, and J. Senel-
lart. 2009. SMT and SPE Machine Transla-
tion Systems for WMT’09. In Proceedings of the
Fourth Workshop on Statistical Machine Translation,
StatMT ’09, pages 130–134, Stroudsburg, PA, USA.
Association for Computational Linguistics.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In In Proceedings
of Association for Machine Translation in the Amer-
icas, pages 223–231.
F. M. Tyers. 2009. Rule-based augmentation of train-
ing data in Breton–French statistical machine trans-
lation. In Proceedings of the 13th Annual Confer-
ence of the European Association of Machine Trans-
lation, pages 213–217.
Y. Xu, T. K. Ralphs, L. Lad´anyi, and M. J. Saltzman.
2009. Computational experience with a software
framework for parallel integer programming. IN-
FORMS Journal on Computing, 21(3):383–397.
</reference>
<page confidence="0.998845">
185
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.611227">
<title confidence="0.997484">The UA-Prompsit hybrid machine translation system for the Workshop on Statistical Machine Translation</title>
<author confidence="0.999806">Antonio Felipe</author>
<affiliation confidence="0.930984666666667">de Llenguatges i Sistemes Universitat d’Alacant, E-03071, Alacant, Spain Language Engineering,</affiliation>
<address confidence="0.938874">Av. Universitat, s/n. Edifici Quorum III, E-03202, Elx, Spain</address>
<abstract confidence="0.985964888888889">This paper describes the system jointly developed by members of the Departament de Llenguatges i Sistemes Inform`atics at Universitat d’Alacant and the Prompsit Language Engineering company for the shared translation task of the 2014 Workshop on Statistical Machine Translation. We present a phrase-based statistical machine translation system whose phrase table is enriched with information obtained from dictionaries and shallowtransfer rules like those used in rule-based machine translation. The novelty of our approach lies in the fact that the transfer rules used were not written by humans, but automatically inferred from a parallel corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>A Lavie</author>
</authors>
<title>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="22329" citStr="Banerjee and Lavie, 2005" startWordPosition="3589" endWordPosition="3593">e 6 are then discarded before solving the minimisation problem. The value of 6 is chosen so that it maximises, on the development corpus, the BLEU score (Papineni et al., 2002) obtained by an Apertium-based system which uses the inferred rules; in our submission 6 = 0.15. In addition, rules that do not correctly reproduce at least 100 bilingual phrase pairs were also discarded in order to make the minimisation problem computationally feasible. 6 Results and discussion Table 2 reports the translation performance as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) achieved by the baseline PBSMT, our submission (UA-Prompsit), Apertium when it uses the set of inferred rules, and Apertium when it uses no rules at all (word-for-word translation). The size of the phrase table and the amount of unknown words in the test set are also reported when applicable. According to the three evaluation metrics, the translation performance of our submission is very close to that of the PBSMT baseline (slightly better according to BLEU and TER, and slightly worse according to METEOR). The difference between both systems computed by paired bootstrap 182 system BLEU TER ME</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>S. Banerjee and A. Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>J Hajiˇc</author>
</authors>
<title>Phrase-based and deep syntactic English-to-Czech statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the third Workshop on Statistical Machine translation,</booktitle>
<pages>143--146</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Bojar, Hajiˇc, 2008</marker>
<rawString>O. Bojar and J. Hajiˇc. 2008. Phrase-based and deep syntactic English-to-Czech statistical machine translation. In Proceedings of the third Workshop on Statistical Machine translation, pages 143–146. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>O Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>22--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="2650" citStr="Callison-Burch et al., 2011" startWordPosition="381" endWordPosition="384"> followed the path we started with our submission to the Spanish–English WMT 2011 shared translation task (S´anchez-Cartagena et al., 2011b) which consisted of enriching the phrase table of a PBSMT system with phrase pairs generated using the dictionaries and rules in the Apertium (Forcada et al., 2011) Spanish–English RBMT system; our approach was one of the winners1 (together with two online SMT systems that were not submitted for the task but were included in the evaluation by the organisers and a system by Systran) in the pairwise manual evaluation of the English–Spanish translation task (Callison-Burch et al., 2011). In this submission, however, we only borrow the dictionaries from the Apertium English–French RBMT system and use them to automatically infer the rules from a parallel corpus. We therefore avoid the need for human-written rules, which are usually written by trained experts, and explore a novel way to add morphological information to PBSMT. The rules inferred from corpora and used to enlarge the phrase table are shallow-transfer rules that build their output with the help of the bilingual dictionary and work on flat intermediate representations (see section 3.1); no syntactic parsing is conse</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22–64, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H M Caseli</author>
<author>M G V Nunes</author>
<author>M L Forcada</author>
</authors>
<title>Automatic induction of bilingual resources from aligned parallel corpora: application to shallowtransfer machine translation.</title>
<date>2006</date>
<journal>Machine Translation,</journal>
<volume>20</volume>
<issue>4</issue>
<note>Published in</note>
<contexts>
<context position="6147" citStr="Caseli et al. (2006)" startWordPosition="928" endWordPosition="931"> the target-language (TL) morphological attributes of their translation according to a bilingual dictionary. In addition, they do no contain probabilities or scores, thus they increase the predictability of the output and can be easily corrected by humans. This fact also represents a significant difference with the probabilistic rules used by certain approaches that aim at improving the grammaticality of the SMT output (Riezler and Maxwell III, 2006; Bojar and Hajiˇc, 2008). With respect to the rule inference approach, other approaches such as those by S´anchezMart´ınez and Forcada (2009) and Caseli et al. (2006) can be found in literature; however, our approach is the first strategy for shallow-transfer rule inference which generalises to unseen combinations of morphological inflection attributes in the training corpus (S´anchez-Cartagena et al., 2014). 3 Inferring shallow-transfer rules from parallel corpora 3.1 Shallow-transfer rule-based machine translation The RBMT process can be split into three different steps (Hutchins and Somers, 1992): (i) analysis of the SL text to build an SL intermediate representation; (ii) transfer from that SL intermediate representation into a TL intermediate represen</context>
</contexts>
<marker>Caseli, Nunes, Forcada, 2006</marker>
<rawString>H. M. Caseli, M. G. V. Nunes, and M. L. Forcada. 2006. Automatic induction of bilingual resources from aligned parallel corpora: application to shallowtransfer machine translation. Machine Translation, 20(4):227–245. Published in 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Costa-Juss`a</author>
<author>M Farr´us</author>
</authors>
<title>Statistical machine translation enhancements through linguistic levels: A survey.</title>
<date>2014</date>
<journal>ACM Comput. Surv.,</journal>
<volume>46</volume>
<issue>3</issue>
<marker>Costa-Juss`a, Farr´us, 2014</marker>
<rawString>M. R. Costa-Juss`a and M. Farr´us. 2014. Statistical machine translation enhancements through linguistic levels: A survey. ACM Comput. Surv., 46(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Eisele</author>
<author>C Federmann</author>
<author>H Saint-Amand</author>
<author>M Jellinghaus</author>
<author>T Herrmann</author>
<author>Y Chen</author>
</authors>
<title>Using Moses to integrate multiple rule-based machine translation engines into a hybrid system.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>179--182</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="3919" citStr="Eisele et al., 2008" startWordPosition="583" endWordPosition="586">ed as follows. The following section outlines related hybrid approaches. Section 3 formally defines the RBMT paradigm and summarises the method followed to automatically infer the shallow-transfer rules, whereas the enrichment of the phrase table is described in section 4. Sections 5 and 6 describe, respectively, the resources we used to build our submission and the results achieved for the English– French language pair. The paper ends with some concluding remarks. 2 Related work Linguistic data from RBMT systems have already been used to enrich SMT systems (Tyers, 2009; Schwenk et al., 2009; Eisele et al., 2008; S´anchezCartagena et al., 2011a). We have already proved 1No other system was found statistically significantly better using the sign test at p ≤ 0.10. 178 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 178–185, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics that using hand-written rules and dictionaries from RBMT yields better results than using only dictionaries (S´anchez-Cartagena et al., 2011a). However, in the approach we present in this paper, rules are automatically inferred from a parallel corpus after converti</context>
</contexts>
<marker>Eisele, Federmann, Saint-Amand, Jellinghaus, Herrmann, Chen, 2008</marker>
<rawString>A. Eisele, C. Federmann, H. Saint-Amand, M. Jellinghaus, T. Herrmann, and Y. Chen. 2008. Using Moses to integrate multiple rule-based machine translation engines into a hybrid system. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 179–182, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Forcada</author>
<author>M Ginestf-Rosell</author>
<author>J Nordfalk</author>
<author>J O’Regan</author>
<author>S Ortiz-Rojas</author>
<author>J A P´erez-Ortiz</author>
<author>G Ramfrez-S´anchez F S´anchez-Martfnez</author>
<author>F M Tyers</author>
</authors>
<title>Apertium: a free/open-source platform for rule-based machine translation. Machine Translation, 25(2):127–144. Special Issue: Free/OpenSource Machine Translation.</title>
<date>2011</date>
<marker>Forcada, Ginestf-Rosell, Nordfalk, O’Regan, Ortiz-Rojas, P´erez-Ortiz, S´anchez-Martfnez, Tyers, 2011</marker>
<rawString>M. L. Forcada, M. Ginestf-Rosell, J. Nordfalk, J. O’Regan, S. Ortiz-Rojas, J. A. P´erez-Ortiz, G. Ramfrez-S´anchez F. S´anchez-Martfnez, and F. M. Tyers. 2011. Apertium: a free/open-source platform for rule-based machine translation. Machine Translation, 25(2):127–144. Special Issue: Free/OpenSource Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Garfinkel</author>
<author>G L Nemhauser</author>
</authors>
<date>1972</date>
<booktitle>Integer programming,</booktitle>
<volume>4</volume>
<publisher>Wiley</publisher>
<location>New York.</location>
<contexts>
<context position="14715" citStr="Garfinkel and Nemhauser, 1972" startWordPosition="2356" endWordPosition="2359"> the best ones are automatically selected by considering all the rules obtained from the different bilingual phrase pairs extracted from the corpus and finding the minimum set of rules that meets the following two conditions: 1. Each bilingual phrase pair is correctly reproduced by at least one rule. 2. If a rule matches the SL side of bilingual phrase pair but does not correctly reproduce its TL side, there is another rule that is more specific (i.e. less general) than it, and correctly reproduces its TL side. This minimisation problem is formulated as an integer linear programming4 problem (Garfinkel and Nemhauser, 1972) and solved using the branch and cut algorithm (Xu et al., 2009). From the small subset of the huge initial rules obtained by solving the minimisation problem, the rules whose effect can be achieved by combining shorter rules or by translating all or some of the words in isolation (i.e. word for word) are removed. In this way, the number of rules is further reduced and long rules, which are more prone to overgeneralisation because they are inferred from fewer bilingual phrase pairs, are discarded.5 ference algorithm is likely to infer many very specific rules that try to correct that lexical m</context>
</contexts>
<marker>Garfinkel, Nemhauser, 1972</marker>
<rawString>R. S. Garfinkel and G. L. Nemhauser. 1972. Integer programming, volume 4. Wiley New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
<author>S F Chen</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University,</institution>
<contexts>
<context position="19218" citStr="Goodman and Chen, 1998" startWordPosition="3081" endWordPosition="3084">er to conveniently weight the synthetic RBMT phrase pairs. 5 System training We built a baseline PBSMT Moses (Koehn et al., 2007) system6 from a subset of the parallel corpora distributed as part of the WMT 2014 shared translation task, namely Europarl (Koehn, 2005), News Commentary and Common Crawl, and a subset of the French monolingual corpora, namely Common Crawl, Europarl, News Commentary and News Crawl. The language model was built with the KenLM language modelling toolkit (Heafield et al., 2013), which was used to train a 5-gram language model using interpolated Kneser-Ney discounting (Goodman and Chen, 1998). Word alignments were computed by means of GIZA++ (Och and Ney, 2003). The weights of the different feature functions were optimised by means of minimum error rate training (Och, 2003) on the 2013 WMT test set.7 The phrase table of this baseline system was then enriched with phrase pairs generated from rules automatically inferred from the concatenation of the test corpora distributed for the WMT 2008–2012 shared translation tasks, and from the English–French bilingual dictionary in the Apertium platform.8 Since the minimisation problem which needs to be solved in order to obtain the rules is</context>
</contexts>
<marker>Goodman, Chen, 1998</marker>
<rawString>J. Goodman and S. F. Chen. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Graham</author>
<author>J van Genabith</author>
</authors>
<title>Factor templates for factored machine translation models.</title>
<date>2010</date>
<booktitle>In IWSLT 2010 : 7th International Workshop on Spoken Language Translation,</booktitle>
<pages>275--283</pages>
<marker>Graham, van Genabith, 2010</marker>
<rawString>Y. Graham and J. van Genabith. 2010. Factor templates for factored machine translation models. In IWSLT 2010 : 7th International Workshop on Spoken Language Translation, pages 275–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Heafield</author>
<author>I Pouzyrevsky</author>
<author>J H Clark</author>
<author>P Koehn</author>
</authors>
<title>Scalable modified Kneser-Ney language model estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>690--696</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="19102" citStr="Heafield et al., 2013" startWordPosition="3063" endWordPosition="3066">s. 181 function to flag bilingual phrase pairs obtained from the RBMT resources is added to the phrase table in order to conveniently weight the synthetic RBMT phrase pairs. 5 System training We built a baseline PBSMT Moses (Koehn et al., 2007) system6 from a subset of the parallel corpora distributed as part of the WMT 2014 shared translation task, namely Europarl (Koehn, 2005), News Commentary and Common Crawl, and a subset of the French monolingual corpora, namely Common Crawl, Europarl, News Commentary and News Crawl. The language model was built with the KenLM language modelling toolkit (Heafield et al., 2013), which was used to train a 5-gram language model using interpolated Kneser-Ney discounting (Goodman and Chen, 1998). Word alignments were computed by means of GIZA++ (Och and Ney, 2003). The weights of the different feature functions were optimised by means of minimum error rate training (Och, 2003) on the 2013 WMT test set.7 The phrase table of this baseline system was then enriched with phrase pairs generated from rules automatically inferred from the concatenation of the test corpora distributed for the WMT 2008–2012 shared translation tasks, and from the English–French bilingual dictionar</context>
</contexts>
<marker>Heafield, Pouzyrevsky, Clark, Koehn, 2013</marker>
<rawString>K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn. 2013. Scalable modified Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690–696, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Hutchins</author>
<author>H L Somers</author>
</authors>
<title>An introduction to machine translation, volume 362.</title>
<date>1992</date>
<publisher>Academic Press</publisher>
<location>New York.</location>
<contexts>
<context position="6587" citStr="Hutchins and Somers, 1992" startWordPosition="989" endWordPosition="992">d Maxwell III, 2006; Bojar and Hajiˇc, 2008). With respect to the rule inference approach, other approaches such as those by S´anchezMart´ınez and Forcada (2009) and Caseli et al. (2006) can be found in literature; however, our approach is the first strategy for shallow-transfer rule inference which generalises to unseen combinations of morphological inflection attributes in the training corpus (S´anchez-Cartagena et al., 2014). 3 Inferring shallow-transfer rules from parallel corpora 3.1 Shallow-transfer rule-based machine translation The RBMT process can be split into three different steps (Hutchins and Somers, 1992): (i) analysis of the SL text to build an SL intermediate representation; (ii) transfer from that SL intermediate representation into a TL intermediate representation; and (iii) generation of the final translation from the TL intermediate representation. Shallow-transfer RBMT systems use relatively simple intermediate representations, which are based on lexical forms consisting of lemma, part of speech and morphological inflection information of the words, and apply simple shallowtransfer rules that operate on sequences of lexical forms: this kind of systems do not perform full parsing. For in</context>
</contexts>
<marker>Hutchins, Somers, 1992</marker>
<rawString>W. J. Hutchins and H. L. Somers. 1992. An introduction to machine translation, volume 362. Academic Press New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>868--876</pages>
<location>Prague.</location>
<contexts>
<context position="4756" citStr="Koehn and Hoang, 2007" startWordPosition="713" endWordPosition="716">ranslation, pages 178–185, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics that using hand-written rules and dictionaries from RBMT yields better results than using only dictionaries (S´anchez-Cartagena et al., 2011a). However, in the approach we present in this paper, rules are automatically inferred from a parallel corpus after converting it into the intermediate representation used by the Apertium RBMT platform (see section 3.3). It can be therefore seen as a novel method to add morphological information to SMT, as factored translation models do (Koehn and Hoang, 2007; Graham and van Genabith, 2010). Unlike factored models, we do not estimate independent statistical models for the translation of the different factors (lemmas, lexical categories, morphological inflection attributes, etc.) and for the generation of the final surface forms. Instead, we first infer a set of rules that deal with the grammatical divergences between the languages involved by performing operations such as reorderings, gender and number agreements, etc. Afterwards, we add synthetic phrase pairs generated from these rules and the Apertium dictionaries to the data from which the well</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>P. Koehn and Hieu Hoang. 2007. Factored translation models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 868–876, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session.</booktitle>
<contexts>
<context position="18724" citStr="Koehn et al., 2007" startWordPosition="3002" endWordPosition="3005">e added only once to the list of corpus-extracted phrase pairs, and then the phrase translation probabilities are computed by relative frequency as usual (Koehn, 2010, sec. 5.2.5). A boolean feature for our rule inferring algorithm there are fewer bilingual phrases from which to infer them, and consequently fewer evidence from which to extract the right reference attributes. 181 function to flag bilingual phrase pairs obtained from the RBMT resources is added to the phrase table in order to conveniently weight the synthetic RBMT phrase pairs. 5 System training We built a baseline PBSMT Moses (Koehn et al., 2007) system6 from a subset of the parallel corpora distributed as part of the WMT 2014 shared translation task, namely Europarl (Koehn, 2005), News Commentary and Common Crawl, and a subset of the French monolingual corpora, namely Common Crawl, Europarl, News Commentary and News Crawl. The language model was built with the KenLM language modelling toolkit (Heafield et al., 2013), which was used to train a 5-gram language model using interpolated Kneser-Ney discounting (Goodman and Chen, 1998). Word alignments were computed by means of GIZA++ (Och and Ney, 2003). The weights of the different featu</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, et al. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<volume>4</volume>
<pages>388--395</pages>
<contexts>
<context position="23635" citStr="Koehn, 2004" startWordPosition="3802" endWordPosition="3803">mpsit 0.3258 0.5781 0.5432 861 100 585 182 Apertium-rules 0.0995 0.7767 0.3168 4 743 - Apertium-word-for-word 0.0631 0.8368 0.2617 4 743 - Table 2: Case-insensitive BLEU, TER, and METEOR scores obtained, on the newstest2014 test set, by the baseline PBSMT system (baseline), the hybrid system submitted to the WMT 2014 shared translation task (UA-Prompsit), Apertium when it uses the set of inferred rules (Apertium-rules), and Apertium when it uses no rules at all (Apertium-word-for-word). The number of unknown words and the size of the phrase table are also reported when applicable. resampling (Koehn, 2004) is not statistically significant for any of the three evaluation metrics (1000 iterations, p = 0.05). An inspection of the 86 rules inferred shows that they encode some of the transformations that one would expect from a set of English–French rules, such as gender and number agreements between nouns, determiners and adjectives, preposition changes, and the introduction of the auxiliary verb avoir for the past tense. In addition, the improvement over word-for-word translation achieved when they are used by Apertium is statistically significant for the three evaluation metrics. One of the reaso</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, volume 4, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth Machine Translation Summit,</booktitle>
<pages>12--16</pages>
<location>Phuket, Thailand,</location>
<contexts>
<context position="18861" citStr="Koehn, 2005" startWordPosition="3027" endWordPosition="3028"> as usual (Koehn, 2010, sec. 5.2.5). A boolean feature for our rule inferring algorithm there are fewer bilingual phrases from which to infer them, and consequently fewer evidence from which to extract the right reference attributes. 181 function to flag bilingual phrase pairs obtained from the RBMT resources is added to the phrase table in order to conveniently weight the synthetic RBMT phrase pairs. 5 System training We built a baseline PBSMT Moses (Koehn et al., 2007) system6 from a subset of the parallel corpora distributed as part of the WMT 2014 shared translation task, namely Europarl (Koehn, 2005), News Commentary and Common Crawl, and a subset of the French monolingual corpora, namely Common Crawl, Europarl, News Commentary and News Crawl. The language model was built with the KenLM language modelling toolkit (Heafield et al., 2013), which was used to train a 5-gram language model using interpolated Kneser-Ney discounting (Goodman and Chen, 1998). Word alignments were computed by means of GIZA++ (Och and Ney, 2003). The weights of the different feature functions were optimised by means of minimum error rate training (Och, 2003) on the 2013 WMT test set.7 The phrase table of this basel</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Tenth Machine Translation Summit, pages 12–16, Phuket, Thailand, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1617" citStr="Koehn, 2010" startWordPosition="225" endWordPosition="226">ot written by humans, but automatically inferred from a parallel corpus. 1 Introduction This paper describes the system jointly submitted by the Departament de Llenguatges i Sistemes Inform`atics at Universitat d’Alacant and the Prompsit Language Engineering company to the shared translation task of the ACL 2014 Ninth Workshop on Statistical Machine Translation (WMT 2014). We participated in the English–French translation task with a hybrid system that combines, in a phrase-based statistical machine translation (PBSMT) system, bilingual phrases obtained from parallel corpora in the usual way (Koehn, 2010, ch. 5), and also bilingual phrases obtained from the existing dictionaries in the Apertium rule-based machine translation (RBMT) platform (Forcada et al., 2011) and a number of shallow-transfer machine translation rules automatically inferred from a small subset of the training corpus. Among the different approaches for adding linguistic information to SMT systems (Costa-Juss`a and Farr´us, 2014), we followed the path we started with our submission to the Spanish–English WMT 2011 shared translation task (S´anchez-Cartagena et al., 2011b) which consisted of enriching the phrase table of a PBS</context>
<context position="5400" citStr="Koehn, 2010" startWordPosition="813" endWordPosition="814">. Unlike factored models, we do not estimate independent statistical models for the translation of the different factors (lemmas, lexical categories, morphological inflection attributes, etc.) and for the generation of the final surface forms. Instead, we first infer a set of rules that deal with the grammatical divergences between the languages involved by performing operations such as reorderings, gender and number agreements, etc. Afterwards, we add synthetic phrase pairs generated from these rules and the Apertium dictionaries to the data from which the well-known, classical PBSMT models (Koehn, 2010) are estimated. The rules in our approach operate on the sourcelanguage (SL) morphological attributes of the input words and on the target-language (TL) morphological attributes of their translation according to a bilingual dictionary. In addition, they do no contain probabilities or scores, thus they increase the predictability of the output and can be easily corrected by humans. This fact also represents a significant difference with the probabilistic rules used by certain approaches that aim at improving the grammaticality of the SMT output (Riezler and Maxwell III, 2006; Bojar and Hajiˇc, </context>
<context position="12451" citStr="Koehn, 2010" startWordPosition="1978" endWordPosition="1979">´anchez-Cartagena et al. (2014) to infer shallow-transfer rules to be used in Apertium from small parallel corpora. First, both sides of the parallel corpus are morphologically analysed and converted into the intermediate representations used by Apertium. Word alignments are then obtained by symmetrising (using the refined intersection method proposed by Och and Ney (2003)) the set of alignments provided by GIZA++ (Och and Ney, 2003) when it is run on both translations directions. Afterwards, the bilingual phrase pairs compatible with the alignments are extracted as it is usually done in SMT (Koehn, 2010, Sec. 5.2.3), and those that are not compatible with the bilingual dictionary of the Apertium English–French RBMT system3 or 2In addition to that criterion, our formalism also permits restricting the application of a rule to the SL lexical forms that, after being looked up in the bilingual dictionary, the TL lexical forms obtained from them have specific morphological inflection attribute values (S´anchez-Cartagena et al., 2014) although no restrictions of this type are imposed in the rule depicted in Figure 1. 3If the words that belong to open lexical categories (those that carry the meaning</context>
<context position="17932" citStr="Koehn, 2010" startWordPosition="2877" endWordPosition="2878"> to get their translations. If a sequence of SL lexical forms is matched by more than one structural transfer rule, it will be used to generate as many bilingual phrase pairs as different rules it matches. This differs from the way in which Apertium translates, as it only applies the longest rule. Note also that the test set is used to guide the phrase extraction in order to avoid the generation of an unmanageable set of phrase pairs. We add these bilingual phrase pairs directly to the phrase table, rather than adding them to the training corpus and relying on the phrase extraction algorithm (Koehn, 2010, sec. 5.2.3), in order to avoid splitting the multi-word expressions provided by Apertium into smaller phrases (Schwenk et al., 2009, sec. 2). The bilingual phrase pairs are added only once to the list of corpus-extracted phrase pairs, and then the phrase translation probabilities are computed by relative frequency as usual (Koehn, 2010, sec. 5.2.5). A boolean feature for our rule inferring algorithm there are fewer bilingual phrases from which to infer them, and consequently fewer evidence from which to extract the right reference attributes. 181 function to flag bilingual phrase pairs obtai</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>P. Koehn. 2010. Statistical Machine Translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--19</pages>
<contexts>
<context position="12215" citStr="Och and Ney (2003)" startWordPosition="1936" endWordPosition="1939"> integrated into the PBSMT system’s phrase table, encoded with the formalism presented in the previous section, are obtained from the parallel corpus by applying the steps described in this section. They are a subset of the steps followed by S´anchez-Cartagena et al. (2014) to infer shallow-transfer rules to be used in Apertium from small parallel corpora. First, both sides of the parallel corpus are morphologically analysed and converted into the intermediate representations used by Apertium. Word alignments are then obtained by symmetrising (using the refined intersection method proposed by Och and Ney (2003)) the set of alignments provided by GIZA++ (Och and Ney, 2003) when it is run on both translations directions. Afterwards, the bilingual phrase pairs compatible with the alignments are extracted as it is usually done in SMT (Koehn, 2010, Sec. 5.2.3), and those that are not compatible with the bilingual dictionary of the Apertium English–French RBMT system3 or 2In addition to that criterion, our formalism also permits restricting the application of a rule to the SL lexical forms that, after being looked up in the bilingual dictionary, the TL lexical forms obtained from them have specific morpho</context>
<context position="19288" citStr="Och and Ney, 2003" startWordPosition="3093" endWordPosition="3096">g We built a baseline PBSMT Moses (Koehn et al., 2007) system6 from a subset of the parallel corpora distributed as part of the WMT 2014 shared translation task, namely Europarl (Koehn, 2005), News Commentary and Common Crawl, and a subset of the French monolingual corpora, namely Common Crawl, Europarl, News Commentary and News Crawl. The language model was built with the KenLM language modelling toolkit (Heafield et al., 2013), which was used to train a 5-gram language model using interpolated Kneser-Ney discounting (Goodman and Chen, 1998). Word alignments were computed by means of GIZA++ (Och and Ney, 2003). The weights of the different feature functions were optimised by means of minimum error rate training (Och, 2003) on the 2013 WMT test set.7 The phrase table of this baseline system was then enriched with phrase pairs generated from rules automatically inferred from the concatenation of the test corpora distributed for the WMT 2008–2012 shared translation tasks, and from the English–French bilingual dictionary in the Apertium platform.8 Since the minimisation problem which needs to be solved in order to obtain the rules is very time-consuming, we chose a small rule inference corpus similar t</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29:19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="19403" citStr="Och, 2003" startWordPosition="3115" endWordPosition="3116"> the WMT 2014 shared translation task, namely Europarl (Koehn, 2005), News Commentary and Common Crawl, and a subset of the French monolingual corpora, namely Common Crawl, Europarl, News Commentary and News Crawl. The language model was built with the KenLM language modelling toolkit (Heafield et al., 2013), which was used to train a 5-gram language model using interpolated Kneser-Ney discounting (Goodman and Chen, 1998). Word alignments were computed by means of GIZA++ (Och and Ney, 2003). The weights of the different feature functions were optimised by means of minimum error rate training (Och, 2003) on the 2013 WMT test set.7 The phrase table of this baseline system was then enriched with phrase pairs generated from rules automatically inferred from the concatenation of the test corpora distributed for the WMT 2008–2012 shared translation tasks, and from the English–French bilingual dictionary in the Apertium platform.8 Since the minimisation problem which needs to be solved in order to obtain the rules is very time-consuming, we chose a small rule inference corpus similar to this year’s test set. The bilingual dictionary, which contains mappings between SL and TL lemmas, consists of 13 </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="21880" citStr="Papineni et al., 2002" startWordPosition="3515" endWordPosition="3519">more than 40 tokens. was split into two parts: the larger one (4/5 of the corpus) was used for actual rule inference as described in section 3.3; the remaining corpus was used as a development corpus as explained next. For each rule z, first the proportion r(z) of bilingual phrase pairs correctly reproduced by the rule divided by the number of bilingual phrases it matches is computed. Rules whose proportion r(z) is lower than a threshold value 6 are then discarded before solving the minimisation problem. The value of 6 is chosen so that it maximises, on the development corpus, the BLEU score (Papineni et al., 2002) obtained by an Apertium-based system which uses the inferred rules; in our submission 6 = 0.15. In addition, rules that do not correctly reproduce at least 100 bilingual phrase pairs were also discarded in order to make the minimisation problem computationally feasible. 6 Results and discussion Table 2 reports the translation performance as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) achieved by the baseline PBSMT, our submission (UA-Prompsit), Apertium when it uses the set of inferred rules, and Apertium when it uses no rules at a</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>J T Maxwell</author>
</authors>
<title>Grammatical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>248--255</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Riezler, Maxwell, 2006</marker>
<rawString>S. Riezler and J. T. Maxwell III. 2006. Grammatical machine translation. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 248–255. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V M S´anchez-Cartagena</author>
<author>F S´anchez-Martfnez</author>
<author>J A P´erez-Ortiz</author>
</authors>
<title>Integrating shallowtransfer rules into phrase-based statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the XIII Machine Translation Summit,</booktitle>
<pages>562--569</pages>
<location>Xiamen, China,</location>
<marker>S´anchez-Cartagena, S´anchez-Martfnez, P´erez-Ortiz, 2011</marker>
<rawString>V. M. S´anchez-Cartagena, F. S´anchez-Martfnez, and J. A. P´erez-Ortiz. 2011a. Integrating shallowtransfer rules into phrase-based statistical machine translation. In Proceedings of the XIII Machine Translation Summit, pages 562–569, Xiamen, China, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V M S´anchez-Cartagena</author>
<author>F S´anchez-Martfnez</author>
<author>J A P´erez-Ortiz</author>
</authors>
<title>The Universitat d’Alacant hybrid machine translation system for wmt 2011.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>457--463</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>S´anchez-Cartagena, S´anchez-Martfnez, P´erez-Ortiz, 2011</marker>
<rawString>V. M. S´anchez-Cartagena, F. S´anchez-Martfnez, and J. A. P´erez-Ortiz. 2011b. The Universitat d’Alacant hybrid machine translation system for wmt 2011. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 457–463, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V M S´anchez-Cartagena</author>
<author>J A P´erez-Ortiz</author>
<author>F S´anchez-Martinez</author>
</authors>
<title>A generalised alignment template formalism and its application to the inference of shallow-transfer machine translation rules from scarce bilingual corpora. Computer Speech and Language. Submitted to the Special Issue on Hybrid Machine Translation.</title>
<date>2014</date>
<marker>S´anchez-Cartagena, P´erez-Ortiz, S´anchez-Martinez, 2014</marker>
<rawString>V. M. S´anchez-Cartagena, J. A. P´erez-Ortiz, and F. S´anchez-Martinez. 2014. A generalised alignment template formalism and its application to the inference of shallow-transfer machine translation rules from scarce bilingual corpora. Computer Speech and Language. Submitted to the Special Issue on Hybrid Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F S´anchez-Martinez</author>
<author>M L Forcada</author>
</authors>
<title>Inferring shallow-transfer machine translation rules from small parallel corpora.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>34</volume>
<issue>1</issue>
<marker>S´anchez-Martinez, Forcada, 2009</marker>
<rawString>F. S´anchez-Martinez and M. L. Forcada. 2009. Inferring shallow-transfer machine translation rules from small parallel corpora. Journal of Artificial Intelligence Research, 34(1):605–635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
<author>S Abdul-Rauf</author>
<author>L Barrault</author>
<author>J Senellart</author>
</authors>
<title>SMT and SPE Machine Translation Systems for WMT’09.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09,</booktitle>
<pages>130--134</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3898" citStr="Schwenk et al., 2009" startWordPosition="579" endWordPosition="582">f the paper is organised as follows. The following section outlines related hybrid approaches. Section 3 formally defines the RBMT paradigm and summarises the method followed to automatically infer the shallow-transfer rules, whereas the enrichment of the phrase table is described in section 4. Sections 5 and 6 describe, respectively, the resources we used to build our submission and the results achieved for the English– French language pair. The paper ends with some concluding remarks. 2 Related work Linguistic data from RBMT systems have already been used to enrich SMT systems (Tyers, 2009; Schwenk et al., 2009; Eisele et al., 2008; S´anchezCartagena et al., 2011a). We have already proved 1No other system was found statistically significantly better using the sign test at p ≤ 0.10. 178 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 178–185, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics that using hand-written rules and dictionaries from RBMT yields better results than using only dictionaries (S´anchez-Cartagena et al., 2011a). However, in the approach we present in this paper, rules are automatically inferred from a parallel </context>
<context position="18065" citStr="Schwenk et al., 2009" startWordPosition="2896" endWordPosition="2899">be used to generate as many bilingual phrase pairs as different rules it matches. This differs from the way in which Apertium translates, as it only applies the longest rule. Note also that the test set is used to guide the phrase extraction in order to avoid the generation of an unmanageable set of phrase pairs. We add these bilingual phrase pairs directly to the phrase table, rather than adding them to the training corpus and relying on the phrase extraction algorithm (Koehn, 2010, sec. 5.2.3), in order to avoid splitting the multi-word expressions provided by Apertium into smaller phrases (Schwenk et al., 2009, sec. 2). The bilingual phrase pairs are added only once to the list of corpus-extracted phrase pairs, and then the phrase translation probabilities are computed by relative frequency as usual (Koehn, 2010, sec. 5.2.5). A boolean feature for our rule inferring algorithm there are fewer bilingual phrases from which to infer them, and consequently fewer evidence from which to extract the right reference attributes. 181 function to flag bilingual phrase pairs obtained from the RBMT resources is added to the phrase table in order to conveniently weight the synthetic RBMT phrase pairs. 5 System tr</context>
</contexts>
<marker>Schwenk, Abdul-Rauf, Barrault, Senellart, 2009</marker>
<rawString>H. Schwenk, S. Abdul-Rauf, L. Barrault, and J. Senellart. 2009. SMT and SPE Machine Translation Systems for WMT’09. In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09, pages 130–134, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation. In</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="22291" citStr="Snover et al., 2006" startWordPosition="3583" endWordPosition="3586">z) is lower than a threshold value 6 are then discarded before solving the minimisation problem. The value of 6 is chosen so that it maximises, on the development corpus, the BLEU score (Papineni et al., 2002) obtained by an Apertium-based system which uses the inferred rules; in our submission 6 = 0.15. In addition, rules that do not correctly reproduce at least 100 bilingual phrase pairs were also discarded in order to make the minimisation problem computationally feasible. 6 Results and discussion Table 2 reports the translation performance as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) achieved by the baseline PBSMT, our submission (UA-Prompsit), Apertium when it uses the set of inferred rules, and Apertium when it uses no rules at all (word-for-word translation). The size of the phrase table and the amount of unknown words in the test set are also reported when applicable. According to the three evaluation metrics, the translation performance of our submission is very close to that of the PBSMT baseline (slightly better according to BLEU and TER, and slightly worse according to METEOR). The difference between both systems computed by p</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. In In Proceedings of Association for Machine Translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Tyers</author>
</authors>
<title>Rule-based augmentation of training data in Breton–French statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Annual Conference of the European Association of Machine Translation,</booktitle>
<pages>213--217</pages>
<contexts>
<context position="3876" citStr="Tyers, 2009" startWordPosition="577" endWordPosition="578">d. The rest of the paper is organised as follows. The following section outlines related hybrid approaches. Section 3 formally defines the RBMT paradigm and summarises the method followed to automatically infer the shallow-transfer rules, whereas the enrichment of the phrase table is described in section 4. Sections 5 and 6 describe, respectively, the resources we used to build our submission and the results achieved for the English– French language pair. The paper ends with some concluding remarks. 2 Related work Linguistic data from RBMT systems have already been used to enrich SMT systems (Tyers, 2009; Schwenk et al., 2009; Eisele et al., 2008; S´anchezCartagena et al., 2011a). We have already proved 1No other system was found statistically significantly better using the sign test at p ≤ 0.10. 178 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 178–185, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics that using hand-written rules and dictionaries from RBMT yields better results than using only dictionaries (S´anchez-Cartagena et al., 2011a). However, in the approach we present in this paper, rules are automatically inf</context>
</contexts>
<marker>Tyers, 2009</marker>
<rawString>F. M. Tyers. 2009. Rule-based augmentation of training data in Breton–French statistical machine translation. In Proceedings of the 13th Annual Conference of the European Association of Machine Translation, pages 213–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Xu</author>
<author>T K Ralphs</author>
<author>L Lad´anyi</author>
<author>M J Saltzman</author>
</authors>
<title>Computational experience with a software framework for parallel integer programming.</title>
<date>2009</date>
<journal>INFORMS Journal on Computing,</journal>
<volume>21</volume>
<issue>3</issue>
<marker>Xu, Ralphs, Lad´anyi, Saltzman, 2009</marker>
<rawString>Y. Xu, T. K. Ralphs, L. Lad´anyi, and M. J. Saltzman. 2009. Computational experience with a software framework for parallel integer programming. INFORMS Journal on Computing, 21(3):383–397.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003386">
<title confidence="0.7207845">
The Enrollment Effect: A Study of Amazon’s Vine Program
Dinesh Puranam
</title>
<author confidence="0.59949">
Samuel Curtis Johnson
</author>
<affiliation confidence="0.923833">
Graduate School of Management
Cornell University
</affiliation>
<email confidence="0.997633">
dp457@cornell.edu
</email>
<sectionHeader confidence="0.993874" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99989247826087">
Do rewards from retailers such as free
products and recognition in the form of
status badges1 influence the recipient’s be-
havior? We present a novel application
of natural language processing to detect
differences in consumer behavior due to
such rewards. Specifically, we investigate
the “Enrollment” effect, i.e. whether re-
ceiving products for free affect how con-
sumer reviews are written. Using data
from Amazon’s Vine program, we con-
duct a detailed analysis to detect stylis-
tic differences in product reviews written
by reviewers before and after enrollment
in the Vine program. Our analysis sug-
gests that the “Enrollment” effect exists.
Further, we are able to characterize the
effect on syntactic and semantic dimen-
sions. This work has implications for re-
searchers, firms and consumer advocates
studying the influence of user-generated
content as these changes in style could po-
tentially influence consumer decisions.
</bodyText>
<sectionHeader confidence="0.999127" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999783555555556">
In 2007 Amazon introduced its Vine program2.
According to Amazon, “Amazon invites cus-
tomers to become Vine Voices based on their re-
viewer rank, which is a reflection of the quality
and helpfulness of their reviews as judged by other
Amazon customers. Amazon provides Vine mem-
bers with free products that have been submitted
to the program by participating vendors. Vine re-
views are the “independent opinions of the Vine
</bodyText>
<footnote confidence="0.92152275">
1A status badge is a special identification usually placed
next to a username in online content.
2http://blog.librarything.com/main/2007/08/amazon-
vine-and-early-reviewers/
</footnote>
<author confidence="0.900234">
Claire Cardie
</author>
<affiliation confidence="0.99726">
Department of Computer Science
Department of Information Science
Cornell University
</affiliation>
<email confidence="0.991289">
cardie@cs.cornell.edu
</email>
<bodyText confidence="0.999597863636364">
Voices.”3 There could be potential concerns as to
whether this enrollment affects the way reviews
are written, introducing, for example, a positive
bias.4
In this work, we investigate whether enroll-
ment in the Vine program results in changes in
the linguistic style used in reviews. We investi-
gate this by looking at reviews by individuals be-
fore and after enrollment in the program. Follow-
ing Feng et al. (2012) and Bergsma et al. (2012),
we conduct a stylometric analysis using a number
of syntactic and semantic features to detect differ-
ences in style. We believe that detecting changes
in consumer behavior due to intervention by a firm
is a novel natural language processing task. Our
approach offers a framework for analyzing text to
detect these changes. This work is relevant for
social scientists and consumer advocates as re-
search suggests that product reviews are influen-
tial (Chevalier and Mayzlin, 2006) and changes in
style could potentially influence consumer deci-
sions.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.995608833333333">
Our work lies at the intersection of research in
four broad areas — Product Reviews, Product
Sampling, Status and Stylometry.
Product Reviews Product reviews have re-
ceived considerable attention in multiple disci-
plines including Marketing, Computer Science
and Information Science. Research has addressed
questions such as the influence of product reviews
on product sales and on brands (Gopinath et al.
(2014); Chevalier and Mayzlin (2006)), detection
of deceptive reviews (Ott et al., 2011) and senti-
ment summarization (Titov and McDonald, 2008).
</bodyText>
<footnote confidence="0.950671">
3http://www.amazon.com/gp/vine/help, words italicized
by authors.
4http://www.npr.org/blogs/money/2013/10/29/241372607/top-
reviewers-on-amazon-get-tons-of-free-stuff.
</footnote>
<page confidence="0.986968">
17
</page>
<note confidence="0.6249795">
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 17–27,
Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999728756756757">
This list is by no means comprehensive, but it is
indicative of the extensive work in this domain.
Product Sampling Here, consumers receive
products for free — as a marketing tactic. This
is also a well-studied phenomenon. Research in
this area has indicated that consumers value free
products (Shampanier et al. (2007); Palmeira and
Srivastava (2013)); that product sampling affects
brand sales (Bawa and Shoemaker, 2004) and that
sampling influences consumer behavior (Wadhwa
et al., 2008).
Status Research shows that status can influ-
ence writing style. Danescu-Niculescu-Mizil et al.
(2012) study discussions among Wikipedia edi-
tors and transcripts of oral arguments before the
U.S. Supreme Court and show how variations
in linguistic style can provide information about
power differences within social groups.
Stylometry focuses on the recognition of style
elements to identify authors (Rosen-Zvi et al.,
2004), detect genders and even determine the
venue where an academic paper was presented
(Bergsma et al., 2012).
Our work draws from each of these research
areas and in turn hopes to make a contribution to
each in return. Our primary objective is to es-
tablish a framework to detect behavioral change
due to a decision by a firm (in this case enroll-
ment to the Vine program characterized by free
products and Vine membership status) by analyz-
ing product reviews. Further, we hope to under-
stand the dimensions on which this behavior may
have changed. Consequently, we pursue a novel
stylometric task. This type of work is especially
important when the traditional numerical measure
(rating) suggests there is no difference in the re-
view pre and post-enrollment (see Section 4).
</bodyText>
<sectionHeader confidence="0.955262" genericHeader="method">
3 Data &amp; Pre-processing Steps
</sectionHeader>
<bodyText confidence="0.999755375">
We gathered all reviews by the top 10,000 review-
ers ranked by Amazon as of September, 2012.
These rankings are partly driven by helpfulness
and recency of reviews5. The data collected in-
cludes the review text, review title, rating as-
signed, date posted, product URL, product price,
whether the reviewed product was received for
free via the Vine program (also referred to as
</bodyText>
<footnote confidence="0.7916085">
5http://www.amazon.com/review/guidelines/top-
reviewers.html/
</footnote>
<bodyText confidence="0.991377">
“Vine Review”), “helpfulness” votes and badges
received by the reviewer.
We collected a total of 2,464,141 reviews of
which 282,913 reviews were for products received
for free via the Vine program. These reviews cov-
ered a total of 9,982 reviewers6 of which 3,566
were members of the Vine program. Approxi-
mately half the reviews belonged to Vine mem-
bers. We eliminated reviews that did not have a
rating. We further excluded reviews where the re-
view text was less than 20 words in length. We
were left with 1,189,704 reviews by Vine mem-
bers.
The date of enrollment to the Vine program
for each reviewer is not explicitly available. We
infer the date of enrollment in the following man-
ner. We sort in ascending order all the “Vine
Reviews” for each reviewer by posted date. We
assume the earliest posted date for a “Vine re-
view” is the enrollment date. This is an important
assumption, as potentially reviewers could have
moved in and out of the program at varying points
of time. Reviewers can be moved out of the pro-
gram for reasons such as not posting a “Vine Re-
view” within 30 days of receipt of the product. In
our data set we found 47,510 “Vine Reviews” by
163 reviewers who were not actively on the Vine
program 7. We can view these reviewers as having
been dropped from the Vine program. Given the
small volume of this type of reviews and review-
ers, our assumption on date of enrollment appears
reasonable.
</bodyText>
<table confidence="0.999273142857143">
Member Free/ Enrollment Review
Type Paid Timing Count
Non Vine Paid NA 1,169,561
Non Vine Free NA 47,510
Vine Paid Post 452,729
Vine Paid Pre 503,688
Vine Free Post 233,287
</table>
<tableCaption confidence="0.997852">
Table 1: Data Summary
</tableCaption>
<sectionHeader confidence="0.994726" genericHeader="method">
4 Enrollment Effect
</sectionHeader>
<bodyText confidence="0.997738666666667">
This research seeks to answer the question: does
enrollment in the Vine program change the writ-
ing styles of reviewers. One naive theory is that
</bodyText>
<footnote confidence="0.9988794">
6During the crawling, ranks changed resulting in fewer
than 10,000 reviewers in our data set.
7As these reviewers were not enrolled to Amazon’s Vine
Program as of September, 2012, they are excluded from our
analysis.
</footnote>
<page confidence="0.999471">
18
</page>
<bodyText confidence="0.999906">
perhaps receiving products for free and receiving
status badges will result in Vine members post-
ing more positive reviews. Interestingly, the av-
erage rating for reviews by Vine members posted
before enrollment is 4.22 and after enrollment is
4.21 and this difference is not statistically signif-
icant. In contrast, the length of reviews signifi-
cantly increased from 251 words prior to enroll-
ment to 306 words post-enrollment. Natural lan-
guage techniques are the only option to further
investigate possible effects of enrollment. Con-
sequently we focus on the review text posted by
Vine members.
</bodyText>
<subsectionHeader confidence="0.986003">
4.1 Approach
</subsectionHeader>
<bodyText confidence="0.9995604">
Following Ashok et al. (2013) and Bergsma et al.
(2012) we construct features that represent writ-
ing style from each review (discussed in more de-
tail in the next section). We incorporate these fea-
tures in a classification algorithm that attempts to
classify each review as having been written pre or
post-enrollment to the Vine program. We report
whether the difference in accuracy for this clas-
sifier vs. a majority vote classification is statisti-
cally significant or not. In order to detect differ-
ences in style pre and post-enrollment, we need to
address certain confounding factors — Reviewer
Specificity , Product Specificity and Time Speci-
ficity.
Reviewer Specificity It may be possible that
certain users post more reviews post-enrollment
than pre-enrollment. Consequently the classifier
may simply end up learning the differences in
style between reviewers. To avoid this, we con-
struct a balanced sample where we randomly se-
lect 25 reviews for each reviewer prior to and post-
enrollment (see Table 2). This also sets our base-
line accuracy at 50%.
Product Specificity As the program started in
2007, the post-enrollment reviews are likely to
predominantly contain products released in after
2007. This might result in the classifier simply
learning the differences between products (say I
Phone vs Palm). Given our focus on style, we
do not use word tokens as such - thus avoiding
the use of product specific features. However, for
some products, the product specific details may
result in the use of specific syntactic structures.
We assume this is not a significant contributor to
the prediction performance. A post-hoc analysis
of the top features supports this assumption. A
second source of change in writing style could be
due to simply whether the product was bought or
received for free. We exclude “Vine Reviews” 8 to
eliminate this confounding factor.
Time Specificity A similar concern as Product
Specificity exists for date references. By focusing
on syntactic and semantic style, we avoid the use
of time specific features.
Another concern is that perhaps post enroll-
ment, reviewers receive writing guidelines from
Amazon. This does not appear to be the case, as
the writing guidelines 9 appear to be for all mem-
bers.We now turn to the extraction of style fea-
tures.
</bodyText>
<table confidence="0.9816895">
Data Type Number of Number of
Reviews Reviewers
Training 113,250 2,265
Test 2,500 50
</table>
<tableCaption confidence="0.996771">
Table 2: Experiment Data
</tableCaption>
<subsectionHeader confidence="0.994215">
4.2 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.99992325">
We consider three different features — “Bag of
words/ unigrams”, “Parse Tree Based Features”
and an umbrella category consisting of genre and
semantic features (see Section 4.2.3).
</bodyText>
<subsectionHeader confidence="0.971572">
4.2.1 Bag of Words
</subsectionHeader>
<bodyText confidence="0.999943">
Bag of Words/Unigrams (UNIGRAMS) Uni-
grams have often been found to be effective pre-
dictive features (Joachims, 2001). In our context,
this serves as a competitive baseline for the clas-
sification task.
</bodyText>
<subsectionHeader confidence="0.744103">
4.2.2 Parse Tree Based Features
</subsectionHeader>
<bodyText confidence="0.999389125">
Following Feng et al. (2012) and Ashok et al.
(2013) we use Probabilistic Context Free Gram-
mar (PCFG) to construct a parse tree for each sen-
tence. We then generate features from this parse
tree and aggregate features to a review level.
All Production Rules (Γ) This set of features
include all production rule features for each re-
view, including the leaves of the parse tree for
</bodyText>
<footnote confidence="0.95992825">
8Reviews where product was received for free via the
Vine program.
9http://www.amazon.com/gp/community-help/customer-
reviews-guidelines
</footnote>
<page confidence="0.998977">
19
</page>
<bodyText confidence="0.997500393939394">
each sentence in the review. This effectively rep-
resents a combination of production rules and un-
igrams as features and represents an additional
competitive baseline.
Non Terminal Production Rules (ΓN) This ex-
cludes the leaves and hence restricts the feature
set to non-terminal production rules. This allows
us to investigate purely syntactic features from the
text.
Phrasal/ Clausal Nodes (PHR/CLSL) We also
investigate features that incorporate phrasal or
clausal nodes of the parse trees. Please see Table
5 and Table 6 for examples of these features.
Parse Tree Measures (PTM) We construct a
set of measures for each sentence based on the
parse tree. These measures are maximum height
of parse tree, maximum width of the parse tree and
the number of sentences in each review.
Latent Dirichlet Allocation (LDA) We also
apply Latent Dirichlet Allocation (Blei et al.,
2003) to the production rules extracted from the
Probabilistic Context Free Grammar. We use the
topics generated as features in our prediction task.
Our objective was to determine whether certain
co-occurring production rules offered better clas-
sification accuracy. Our implementation includes
hyper-parameter optimization via maximum like-
lihood. The number of topics is selected by maxi-
mizing the pairwise cosine distance amongst top-
ics. We used the Stanford Parser (Klein and Man-
ning, 2003) to parse each of the reviews and the
Natural Language Toolkit (NLTK) (Bird et al.,
2009) to post process the results.
</bodyText>
<subsectionHeader confidence="0.710875">
4.2.3 Genre and Semantic Features
</subsectionHeader>
<bodyText confidence="0.999869567567568">
Style Metrics (STYLE) This includes three dis-
tinct types of metrics. Character Based - This
includes counts of uppercased letters, number of
letters, number of spaces and number of vow-
els. Word Based - This includes measures such as
number of short words (3 characters or less ), long
words (8 characters or less), average word length
and number of different words. Syntax Based -
This includes measures such as number of peri-
ods, commas, common conjunctions, interroga-
tives, prepositions, pronouns and verbs.
Parts of Speech (POS) features have often been
surprisingly effective in tasks such as predicting
deception (Ott et al., 2011). Consequently we test
this feature set as well.
Domain-independent Dictionary We make
use of the Linguistic Inquiry and Word Count
(LIWC) categorization (Tausczik and Pen-
nebaker, 2010). One key advantage of this
categorization is that it is domain independent
and emphasizes psycho-linguistic cues. We run
two variants of this set of features. The first
(LIWC ALL) includes all the categories — both
sub-ordinate and super-ordinate categories. The
second (LIWC SUB CATEG.) only includes the
sub-ordinate categories, thus ensuring the features
are mutually exclusive.
Subjectivity Measures (OPINION) We measure
number of subjective, objective and other (neither
subjective nor objective) sentences in each review.
We use the “OpinionFinder System” (Wiebe et al.,
2005) to classify each sentence with these mea-
sures. We aggregate the count of subjective, ob-
jective and other sentences at the review level and
use these aggregates as features.10 We also re-
port results on experiments where multiple feature
types are included simultaneously in the model.
</bodyText>
<sectionHeader confidence="0.996762" genericHeader="method">
5 Experimental Methodology
</sectionHeader>
<bodyText confidence="0.999978">
All experiments use the Fan et al. (2008) im-
plementation of linear Support Vector Machines
(Vapnik, 1998). The linear specification allows
us to infer feature importance. We learn the
penalty parameter via grid search using 5 fold
cross-validation and report performance on a held-
out balanced sample of reviews from 50 randomly
selected users (all of whom were excluded from
the training set) from the group of reviewers with
at least 25 reviews in pre and post enrollment peri-
ods. While reporting the results, for some features
we report the threshold (Thr) value set to exclude
the least frequent features. These thresholds were
also learned via the 5 fold cross validation pro-
cess. Finally, text features can be binarized, mean
centered and/or normalized. Each of these options
were also selected via 5 fold cross validation.
</bodyText>
<sectionHeader confidence="0.999593" genericHeader="method">
6 Results &amp; Analysis
</sectionHeader>
<bodyText confidence="0.9896625">
All of the feature sets perform statistically better11
than a majority vote (50%).
Baselines Unsurprisingly, the feature set con-
taining all production rules (Γ) yields the best ac-
</bodyText>
<footnote confidence="0.904025">
10One drawback is that the classifiers are trained on sen-
tences from the MPQA corpus. Domain specificity is likely
to yield poorer classification performance on our data.
11as indicated by a paired t-test at p=0.05 on the held out
sample
</footnote>
<page confidence="0.945506">
20
</page>
<table confidence="0.99993203125">
Baselines
Style Features Feature Accuracy
Count
UNIGRAMS 796,826 60.9 %
r (Thr =50) 29,362 62.0 %
By Feature Type
Style Features Feature Accuracy
Count
rN (Thr=200) 2,730 59.2 %
PHR/CLSL 23 57.4 %
PTM 3 55.8 %
LDA 200 54.0 %
STYLE 26 57.6 %
POS 45 57.5 %
LIWC ALL 76 59.8 %
LIWC SUB CATEG. 67 60.3 %
OPINION 3 56.3 %
Feature Combinations
Style Features Feature Accuracy
Count
rN (THR=200) + STYLE 2,756 57.9 %
rN (THR=200) + OPINION 2,733 56.2 %
PHR/CLSL + OPINION 26 58.0 %
PHR/CLSL + STYLE 49 57.5 %
LIWC + STYLE 93 60.2 %
LIWC + PHR/CLSL 90 60.2 %
LIWC + rN (Thr=200) 2,797 59.1 %
LIWC + OPINION 70 60.3 %
PTM + OPINION 6 57.2 %
STYLE + OPINION 29 58.7 %
STYLE + PTM 29 57.4 %
LIWC +STYLE+PHR/CLSL 116 60.1 %
</table>
<tableCaption confidence="0.999591">
Table 3: Experiment Results
</tableCaption>
<bodyText confidence="0.998391698630137">
curacy (62.0 %). Unfortunately, as expected, the
top features all included terminal production rules
that signal time or product specificity. For ex-
ample in the pre-enrollment reviews the top 10
features for F include NNP —* ‘Update’, CD —*
‘2006’, NNP —* ‘XP’ and NNP —* ‘Palm’. In
the post-enrollment reviews the top 10 features in-
clude CD —* ‘2012’,CD —* ‘2011’, NN —* ‘iPad’
and NN —* ‘iPhone’. We observe the same issue
with the UNIGRAMS feature set. This supports our
contention that the analysis should restrict itself
to style and domain-independent features. The
best performing style feature set is LIWC SUB
CATEG. followed by Non Terminal Production
Rules (FN). OPINION is the most parsimonious
feature set that performs significantly better than
a majority vote.
Non Terminal Production Rules (FN) Table 7
presents the top Non Terminal Production Rules.
We observe the following: First, pre-enrollment
reviews have noun phrases(NP) that contain fewer
leaf nodes than in the post-enrollment reviews.
This appears to be due to the inclusion of de-
terminers (DT), adjectives (JJ), comparative ad-
jectives (JJR), personal pronouns (PRP $) or
simply more nouns (NN). This might indicate
that topics are discussed with more specifics
in post-enrollment reviews. Second, clauses(S)
begin with action oriented verb phrases (VP)
in the pre-enrollment reviews. In contrast in
the post-enrollment reviews clauses connect two
clauses using coordinating conjunctions(CC) or
prepositions(IN). One possibility is that review-
ers are offering more detail/concepts per sen-
tence (where each clause is a detail/concept) in
the post-enrollment reviews. Finally, we ob-
serve that pre-enrollment reviews include adjec-
tival phrases (ADJP) connect to superlative ad-
verbs (RBS)which convey certainty. We will re-
visit this finding when we review the results from
the LIWC model below.
Phrasal/Clausal (PHR./CLSL.) Tables 5 and 6
suggest that post-enrollment reviews emphasize
information using descriptive phrases — adjecti-
val phrases (ADJP) and adverbial phrases (ADVP)
— and quantifier phrases (QP). Pre-enrollment re-
views appear to have more complex clause struc-
tures (SBAR, SINV, SQ, SBARQ - see table 5 for
definitions).
Parse Tree Metrics (PTM) The three features
used are number of sentences, maximum height
of parse tree and the maximum width of the parse
tree, listed here in descending order of importance
for the post-enrollment reviews. As mentioned
earlier in section 4 the average review length is
higher in the post-enrollment reviews so the find-
ing that the number of sentences predict post-
enrollment reviews is consistent. Maximum tree
width predicts the pre-enrollment reviews. This
flat structure indicates a more complex communi-
cation structure.
Latent Dirichlet Allocation (LDA) This
model did not perform very well, being statis-
tically marginally better than majority vote. As
mentioned before, we selected the number of
topics by maximizing the average cosine distance
amongst topics. Even with 200 topics, this
measure was 0.39, suggesting that the topics were
themselves not well separated. In the limit, each
topic would be a non-terminal production rule.
This is the same as Non Terminal Production
Rules (FN) feature set discussed earlier in this
section.
</bodyText>
<page confidence="0.997885">
21
</page>
<subsectionHeader confidence="0.86402">
Predicts PRE Enrollment
</subsectionHeader>
<bodyText confidence="0.99967025">
‘number of different words’, ‘uppercase’, ‘alphas’,
‘vowels’ , ‘short words’, ‘words per sentence’, ‘to be
words’ , ‘punctuation symbols’, ‘long words’, ‘common
prepositions’
</bodyText>
<subsectionHeader confidence="0.918932">
Predicts POST Enrollment
</subsectionHeader>
<bodyText confidence="0.974412333333333">
‘average word length’, ‘spaces’, ‘verbs are’, ‘chars per
sentence’ , ‘verbs be’, ‘common conjunctions’, ‘verbs
were’, ‘personal pronouns’ , ‘verbs was’, ‘verbs am’
</bodyText>
<tableCaption confidence="0.980088">
Table 4: Style Metrics: Top Features
</tableCaption>
<bodyText confidence="0.999213411111112">
Style (STYLE) Table 4 presents the top features
for this feature set. The features suggest that
reviewers used a more varied vocabulary (num-
ber of different words), more words per sentence
(words per sentence) and more long words (long
words) in pre-enrollment than in post-enrollment
reviews. This might indicate that sentences in
the pre-enrollment reviews were longer and more
complex. Interestingly, the average word length
did go up in the post-enrollment reviews as did
the characters per sentence. In addition, more per-
sonal pronouns and conjunctions are used — a
finding replicated in the model using LIWC fea-
tures (see below).
Parts of Speech (POS) The top features for
post-enrollment are commas, periods, compara-
tive adjectives, verb phrases and coordinating con-
junctions. The top features for pre-enrollment are
nouns, noun phrases, determiners , prepositions
and superlative adverbs. These results are more
difficult to interpret though the use of comparative
adjectives suggest more comparisons between dif-
ferent objects in the post enrollment reviews.
LIWC SUB CATEG. The top 10 LIWC fea-
tures are shown in Table 8. LIWC features are cat-
egories that are contained in broader categories.
For example POSEMO (see Table 8, first feature for
“Predicts POST enrollment”) refers to the class of
positive emotion words. POSEMO itself is con-
tained in a category called “Affective Features”
which in turn is classified as a Psychological Pro-
cess (abbreviated to Pscyh.). The analysis of
the categories of features is in itself interesting.
Psych./ Cognitive Features occur higher up in fea-
tures predictive of pre-enrollment reviews than in
the features predictive of post-enrollment reviews.
“Psych./ Affective Features” occurs as a top fea-
ture for the post-enrollment reviews. The ac-
tual feature from the “Psych./ Affective Features”
category is POSEMO suggesting that the positive
emotion is more strongly conveyed in the post-
enrollment reviews than in the pre-enrollment re-
views. Interestingly the corresponding negative
feature NEGEMO is in the top 10 features predict-
ing the pre-enrollment reviews. This is especially
intriguing since the average rating for reviews in
the pre and post-enrollment reviews is the same
(see 4). We were concerned that possibly our sam-
pling had induced a bias in the ratings. But the av-
erage ratings in our sample are 4.18 and 4.19 pre
and post-enrollment respectively (difference is not
statistically significant).
FUNCTION WORDS occur extensively in the
post-enrollment reviews. We also observe that
inclusive (INCL) and exclusive (EXCL) terms are
used more in the post-enrollment reviews. Its pos-
sible that reviewers are seeking to be more bal-
anced. Products are described in personal (I),
perceptual (FEEL) and relativistic (SPACE) terms.
Pre-enrollment reviews discuss personal concerns
(LEISURE, RELIG) , indicate a level of certainty
(CERTAIN) and opinions are presented in terms of
thought process (INSIGHT). Interestingly, the pre-
enrollment reviews address the reader (YOU).
Opinions (OPINION) Features predicting post-
enrollment are number of objective sentences,
number of subjective sentences and finally num-
ber of other (neither subjective nor objective) sen-
tences. This suggests that reviewers try to write
somewhat more objectively in the post-enrollment
reviews.
Feature Combinations With the exception of
the combinations STYLE + OPINION, PHR/CLSL
+OPINION and PTM + OPINION which improve
on either feature set used alone, none of the
other combinations improved performance over
all component feature sets modeled individually.
Overall, none of the combinations improved over
LIWC SUB CATEG. Hence we do not delve fur-
ther into features from these models.
Summary Overall pre-enrollment reviews are
more complex (complex clauses, wide parse trees,
varied vocabulary, more words per sentence), have
fewer concepts per sentence, contain negative
emotions, addresses the reader directly and are
more certain. Post-enrollment reviews are longer,
more descriptive, contain comparisons, contain
quantifiers, have more positive emotion and de-
scribe the product experience in physical and per-
sonal terms.
</bodyText>
<page confidence="0.997697">
22
</page>
<table confidence="0.991249663551402">
Predicts PRE Enrollment
1 NP (Noun Phrase) 6 LST (List marker.
Includes surrounding
punctuation)
EXAMPLE EXAMPLE
NP (3)
DT NN
another person
2 SBAR (Clause introduced by 7 VP (Verb Phrase)
a (possibly empty) subordinat-
ing conjunction)
EXAMPLE EXAMPLE
SBAR VP
IN S VBN NP
If NP VP loved DT NN
(...) (...) another person
3 SQ ( Inverted yes/no ques- 8 PRN (Parenthetical)
tion, or main clause of a wh-
question, following the wh-
phrase in SBARQ)
EXAMPLE EXAMPLE
SQ (p. 73)
VBZ NP VP
does PRP VB
it matter
4 NAC (Not a Constituent; used 9 SINV ( Inverted
to show the scope of certain declarative sentence,
prenominal modifiers within an i.e. one in which the
NP) subject follows the
tensed verb or modal)
EXAMPLE EXAMPLE
NAC SINV
“ PRP; JJ NN ” CC VBD NP VP .
“ My Oh My ” Nor did PRP (...) .
it
5 SBARQ (Direct question in- 10 NX (Used within
troduced by a wh-word or a wh- certain complex NPs to
phrase) mark the head of the
NP)
EXAMPLE EXAMPLE
SBARQ
SQ .
VP ?
VBG
th
23
Predicts PRE Enrollment
Feature Examples
ROOT → S (1) And nearly every sin-
gle item seemed cute and
usable to me. (2) Look
closely, (...) overwhelming
personal and cultural up-
heaval.
NP → NNP NNP (1) Tim Bess (2) Jennifer
Fitch
PP → IN NP (1) for its psychological and
emotional richness (2) of
loyalty
NP → DT NN (1) the price (2) a book
NP → NNP POS (1) Frost ’s (2) Clough ’s
ADJP → RBS JJ (1) most assuredly (2) most
entertaining
WHNP → WP (1) who (2) what
NP → NNP (1) Blessed (2) India
PP → TO NP (1) to the crime (2) to me
S → VP (1) linking Pye to the crime
scene (2) Gripping due to
(...)
Predicts POST Enrollment
Feature Examples
S → S , IN S . (1) It is functionally the
same as Apple’s 10 watt
charger which outputs 2.1
A , so it is also suitable for
charging the iPad. (2) It has
3 levels of trays that spread
as you open the box, so you
can easily access contents
in all trays.
S → IN NP VP . (1) So I don’t think the
investment in graphics (...)
enjoyability in the game.
(2) So we decided to try it
again this year.
ROOT → NP (1) Some kind of (...) disor-
der ? (2) Proper Alignment
and Posture; This segment
(...) .
S → S CC S . (1) Mage and Takumo (...)
but lacking in depth.(2) The
light feature is great and it
powers off (...).
NP → PRP$ NNP NN (1) your Alpine yodeling
(2) my MacBook Pro
S → VP . (1) Enough negativity. (2)
Suffice it to say that (...) .
NP → DT JJR NN (1) a better future (2) a
slower flow
NP → DT JJ , JJ NN (1) an immediate , visceral
reaction (2)a roots-based,
singer-songwriter effort
NP → DT NNP NNP NNP (1) the Post-Total Body
NNP Weight Training (2) The
Gunfighter DVD Gregory
Peck
WHADVP → WRB RB (1) How far (2) how well
</table>
<tableCaption confidence="0.996247">
Table 7: ΓN : Top Features (PCFG Non Terminal)
</tableCaption>
<table confidence="0.999949805555556">
Predicts PRE Enrollment
Feature Category Examples
leisure Personal Concerns Cook, chat, movie
verb Function words Walk, want, see
certain Psych./Cognitive always, never
Processes
insight Psych./Cognitive think, know, con-
Processes sider
negemo Psych./Affective Pro- Hurt, ugly, nasty
cesses
exclam Exclamation !
period Period .
you Function words 2&amp;quot;d person , you,
your
preps Function words to, with, above
relig Personal Concerns 2&amp;quot;d synagogue, sa-
cred
Predicts POST Enrollment
Feature Category Examples
posemo Psych./Affective Pro- Love, nice, sweet
cesses
article Function words a, an, the
i Function words 1st person singular.
space Psych./Relativity Down, in, thin
ingest Psych./Biological Pro- Dish, eat, pizza
cesses
ipron Function words Impersonal Pro-
nouns, it its ,
those
incl Psych./Cognitive Inclusive, and, with
Processes ,include
conj Function words and, but, whereas
excl Psych./Cognitive Exclusive but,
Processes without, exclude
feel Psych./Perceptual Pro- feels, touch
cesses
</table>
<tableCaption confidence="0.99915">
Table 8: LIWC Sub Category: Top Features
</tableCaption>
<bodyText confidence="0.998538166666667">
These reviews are are specific, balanced and
contain more objective sentences as well.
Discussion on Readability One possibility is
that the “Enrollment” effect leads to reviewers
writing more readable reviews. To test this hy-
pothesis we performed a paired t-test between
readability scores for pre and post-enrollment re-
views. Table 9 suggests that indeed this is the
case. Flesch Reading Ease is the only measure
where a higher score indicates simpler text. For
the rest of the measures a higher score implies
more complex text. All of the measures are within
the average readability range and the magnitude
of the differences are small. Nevertheless, these
differences are statistically significant 12 with one
exception lending support to the idea that “Enroll-
ment” effect might lead to reviewers writing more
readable reviews.
</bodyText>
<footnote confidence="0.9916705">
12The cell size for each class is 57,875, making the modest
difference in magnitude statistically significant.
</footnote>
<page confidence="0.995874">
24
</page>
<table confidence="0.99966605882353">
Reading Pre Post t
Measure /Cite Mean Mean Value
ARI /(Senter and Smith, 9.16 9.15 (0.45)
1967)
Coleman Liau /(Coleman 8.76 8.68 (6.39)*
and Liau, 1975)
Flesch Kincaid /(Kincaid 8.75 8.71 (2.19)*
et al., 1975)
Flesch Reading Ease /(Kin- 65.63 66.18 6.61*
caid et al., 1975)
Gunning Fog /(Gunning, 11.75 11.70 (2.18)*
1952)
LIX /(Anderson, 1983) 38.24 38.07 (2.89)*
RIX /(Anderson, 1983) 3.74 3.71 (3.05)*
SMOG /(McLaughlin, 10.59 10.56 (2.56)*
1969)
* Significant at 5% level
</table>
<tableCaption confidence="0.998061">
Table 9: Readability Measures
</tableCaption>
<bodyText confidence="0.999796">
)suggest that there are changes right upto the en-
rollment dateand some levelling out in the post en-
rollment period, providing some evidence against
this hypothesis.
</bodyText>
<sectionHeader confidence="0.999687" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.99996384">
So far we have ignored the possibility that writ-
ing styles of reviewers may simply continuously
evolve with experience and we are simply detect-
ing a difference due to this underlying trend. 13
To address this question we investigated the sub-
periods within the the pre and post enrollment pe-
riods.
We split the post enrollment period (i.e. from
date of enrollment to the date the most recent re-
view was posted) further into two equal time pe-
riods for each reviewer. As before, we learn a
classifier to discriminate between the sub periods.
Interestingly the classifier performed the same as
chance at p=0.05 (Test Accuracy= 51.0%).14 15
However a similar analysis in the pre-enrollment
period results in a test set accuracy of 63.3% (sig-
nificant at p=0.05). So there is a change in writing
style within the pre-enrollment period, but there is
no continued change post-enrollment. This is not
consistent with the continuous style evolution hy-
pothesis. One account would be that Amazon en-
rolls reviewers whose styles have stabilized. This
remains a possibility as Amazon actively selects
the members (and we are not aware of the specific
rules used by Amazon). The trends (see Figure 1
</bodyText>
<footnote confidence="0.812417909090909">
13Ideally, if a) the enrollment date had been the same for
all reviewers and b) the enrollment was random, we would
have a clean experimental framework to detect whether a
similar trend exists for non-vine reviewers. Unfortunately,
this is not the case.
14We report the results only on POS for conciseness. The
other feature sets performed similarly.
15As before the test sample includes 50 users. However we
sampled only 10 reviews in each sub period. Corresponding
down sampled performance for Pre vs Post enrollment accu-
racy is 57.5% (significant at p=0.05)using POS features.
</footnote>
<figureCaption confidence="0.987576">
Figure 1: Feature Trends
</figureCaption>
<table confidence="0.999538125">
Train Test Accuracy
Size Size
Within Pre- 44,800 1000 63.3%
Enrollment
Within Post- 59,250 1000 51.0%
Enrollment
Pre vs Post Enroll. 53,840 1000 57.5%
Down Sampled
</table>
<tableCaption confidence="0.994789">
Table 10: Sub Period Results
</tableCaption>
<sectionHeader confidence="0.993" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999972176470588">
We view this work as a first step toward inves-
tigating this phenomenon further. In particular,
we plan to test the robustness of our results w.r.t.
product specificity, to investigate stylistic differ-
ences (a) between reviews for purchased products
versus for products received for free amongst Vine
members and (b) between reviews by Vine review-
ers and non-Vine reviewers. Another line of in-
quiry involves decomposing the “Enrollment” ef-
fect into a reputation/status effect (the influence of
the status badge - Vine membership) and a product
sampling effect (the influence of receiving goods
for free). Finally, investigating the temporal dy-
namics of style for these reviewers might prove in-
teresting as would determining whether these sub-
tle differences in style affect the readers and influ-
ence purchase decisions.
</bodyText>
<sectionHeader confidence="0.996769" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.997432">
We would like to thank our reviewers for insight-
ful comments that we sought to address here. We
would also like to thank Myle Ott for generously
sharing the data.
</bodyText>
<page confidence="0.997394">
25
</page>
<sectionHeader confidence="0.990227" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99972696875">
Jonathan Anderson. Lix and rix: Variations on a
little-known readability index. Journal of Read-
ing, pages 490–496, 1983.
Vikas Ganjigunte Ashok, Song Feng, and Yejin
Choi. Success with style: Using writing style
to predict the success of novels. Poetry, 580(9):
70, 2013.
Kapil Bawa and Robert Shoemaker. The ef-
fects of free sample promotions on incremen-
tal brand sales. Marketing Science, 23(3):345–
363, 2004.
Shane Bergsma, Matt Post, and David Yarowsky.
Stylometric analysis of scientific articles. In
Proceedings of the 2012 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies, pages 327–337. Association for
Computational Linguistics, 2012.
Steven Bird, Ewan Klein, and Edward Loper.
Natural Language Processing with Python.
O’Reilly Media, 2009.
David M Blei, Andrew Y Ng, and Michael I Jor-
dan. Latent dirichlet allocation. the Journal of
machine Learning research, 3:993–1022, 2003.
Judith A Chevalier and Dina Mayzlin. The ef-
fect of word of mouth on sales: Online book
reviews. Journal of marketing research, 43(3):
345–354, 2006.
Meri Coleman and TL Liau. A computer readabil-
ity formula designed for machine scoring. Jour-
nal of Applied Psychology, 60(2):283, 1975.
Cristian Danescu-Niculescu-Mizil, Lillian Lee,
Bo Pang, and Jon Kleinberg. Echoes of power:
Language effects and power differences in so-
cial interaction. In Proceedings of the 21st
international conference on World Wide Web,
pages 699–708. ACM, 2012.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. Liblin-
ear: A library for large linear classification.
The Journal of Machine Learning Research, 9:
1871–1874, 2008.
Song Feng, Ritwik Banerjee, and Yejin Choi.
Characterizing stylistic elements in syntactic
structure. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural
Language Learning, pages 1522–1533. Associ-
ation for Computational Linguistics, 2012.
Shyam Gopinath, Jacquelyn S Thomas, and Lak-
shman Krishnamurthi. Investigating the rela-
tionship between the content of online word
of mouth, advertising, and brand performance.
Marketing Science, 2014.
Robert Gunning. Technique of clear writing.
1952.
Thorsten Joachims. A statistical learning learn-
ing model of text classification for support vec-
tor machines. In Proceedings of the 24th an-
nual international ACM SIGIR conference on
Research and development in information re-
trieval, pages 128–136. ACM, 2001.
J Peter Kincaid, Robert P Fishburne Jr, Richard L
Rogers, and Brad S Chissom. Derivation of new
readability formulas (automated readability in-
dex, fog count and flesch reading ease formula)
for navy enlisted personnel. Technical report,
DTIC Document, 1975.
Dan Klein and Christopher D Manning. Accurate
unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Com-
putational Linguistics-Volume 1, pages 423–
430. Association for Computational Linguis-
tics, 2003.
G Harry McLaughlin. Smog grading: A new read-
ability formula. Journal of reading, 12(8):639–
646, 1969.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T
Hancock. Finding deceptive opinion spam by
any stretch of the imagination. In Proceed-
ings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human
Language Technologies-Volume 1, pages 309–
319. Association for Computational Linguis-
tics, 2011.
Mauricio M Palmeira and Joydeep Srivastava.
Free offers cheap product: A selective acces-
sibility account on the valuation of free offers.
Journal of Consumer Research, 40(4):644–656,
2013.
Michal Rosen-Zvi, Thomas Griffiths, Mark
Steyvers, and Padhraic Smyth. The author-
topic model for authors and documents. In Pro-
ceedings of the 20th conference on Uncertainty
in artificial intelligence, pages 487–494. AUAI
Press, 2004.
</reference>
<page confidence="0.950063">
26
</page>
<reference confidence="0.993288642857143">
RJ Senter and EA Smith. Automated readabil-
ity index. Technical report, DTIC Document,
1967.
Kristina Shampanier, Nina Mazar, and Dan
Ariely. Zero as a special price: The true value of
free products. Marketing Science, 26(6):742–
757, 2007.
Yla R Tausczik and James W Pennebaker. The
psychological meaning of words: Liwc and
computerized text analysis methods. Journal of
Language and Social Psychology, 29(1):24–54,
2010.
Ivan Titov and Ryan McDonald. A joint model
of text and aspect ratings for sentiment sum-
marization. In Proceedings of ACL-08: HLT,
pages 308–316, Columbus, Ohio, June 2008.
Association for Computational Linguistics.
Vladimir N. Vapnik. Statistical Learning Theory.
Wiley-Interscience, 1998.
Monica Wadhwa, Baba Shiv, and Stephen M
Nowlis. A bite to whet the reward appetite:
The influence of sampling on reward-seeking
behaviors. Journal of Marketing Research, 45
(4):403–413, 2008.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
Annotating expressions of opinions and emo-
tions in language. Language resources and
evaluation, 39(2-3):165–210, 2005.
</reference>
<page confidence="0.998789">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.818511">
<title confidence="0.9831275">The Enrollment Effect: A Study of Amazon’s Vine Program Dinesh</title>
<author confidence="0.991746">Samuel Curtis</author>
<affiliation confidence="0.916392">Graduate School of Cornell</affiliation>
<email confidence="0.98582">dp457@cornell.edu</email>
<abstract confidence="0.999167208333334">Do rewards from retailers such as free products and recognition in the form of influence the recipient’s behavior? We present a novel application of natural language processing to detect differences in consumer behavior due to such rewards. Specifically, we investigate the “Enrollment” effect, i.e. whether receiving products for free affect how consumer reviews are written. Using data from Amazon’s Vine program, we conduct a detailed analysis to detect stylistic differences in product reviews written by reviewers before and after enrollment in the Vine program. Our analysis suggests that the “Enrollment” effect exists. Further, we are able to characterize the effect on syntactic and semantic dimensions. This work has implications for researchers, firms and consumer advocates studying the influence of user-generated content as these changes in style could potentially influence consumer decisions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jonathan Anderson</author>
</authors>
<title>Lix and rix: Variations on a little-known readability index.</title>
<date>1983</date>
<journal>Journal of Reading,</journal>
<pages>490--496</pages>
<contexts>
<context position="29946" citStr="Anderson, 1983" startWordPosition="4807" endWordPosition="4808"> statistically significant 12 with one exception lending support to the idea that “Enrollment” effect might lead to reviewers writing more readable reviews. 12The cell size for each class is 57,875, making the modest difference in magnitude statistically significant. 24 Reading Pre Post t Measure /Cite Mean Mean Value ARI /(Senter and Smith, 9.16 9.15 (0.45) 1967) Coleman Liau /(Coleman 8.76 8.68 (6.39)* and Liau, 1975) Flesch Kincaid /(Kincaid 8.75 8.71 (2.19)* et al., 1975) Flesch Reading Ease /(Kin- 65.63 66.18 6.61* caid et al., 1975) Gunning Fog /(Gunning, 11.75 11.70 (2.18)* 1952) LIX /(Anderson, 1983) 38.24 38.07 (2.89)* RIX /(Anderson, 1983) 3.74 3.71 (3.05)* SMOG /(McLaughlin, 10.59 10.56 (2.56)* 1969) * Significant at 5% level Table 9: Readability Measures )suggest that there are changes right upto the enrollment dateand some levelling out in the post enrollment period, providing some evidence against this hypothesis. 7 Discussion So far we have ignored the possibility that writing styles of reviewers may simply continuously evolve with experience and we are simply detecting a difference due to this underlying trend. 13 To address this question we investigated the subperiods within the </context>
</contexts>
<marker>Anderson, 1983</marker>
<rawString>Jonathan Anderson. Lix and rix: Variations on a little-known readability index. Journal of Reading, pages 490–496, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Ganjigunte Ashok</author>
<author>Song Feng</author>
<author>Yejin Choi</author>
</authors>
<title>Success with style: Using writing style to predict the success of novels.</title>
<date>2013</date>
<journal>Poetry,</journal>
<volume>580</volume>
<issue>9</issue>
<contexts>
<context position="8512" citStr="Ashok et al. (2013)" startWordPosition="1331" endWordPosition="1334">g products for free and receiving status badges will result in Vine members posting more positive reviews. Interestingly, the average rating for reviews by Vine members posted before enrollment is 4.22 and after enrollment is 4.21 and this difference is not statistically significant. In contrast, the length of reviews significantly increased from 251 words prior to enrollment to 306 words post-enrollment. Natural language techniques are the only option to further investigate possible effects of enrollment. Consequently we focus on the review text posted by Vine members. 4.1 Approach Following Ashok et al. (2013) and Bergsma et al. (2012) we construct features that represent writing style from each review (discussed in more detail in the next section). We incorporate these features in a classification algorithm that attempts to classify each review as having been written pre or post-enrollment to the Vine program. We report whether the difference in accuracy for this classifier vs. a majority vote classification is statistically significant or not. In order to detect differences in style pre and post-enrollment, we need to address certain confounding factors — Reviewer Specificity , Product Specificit</context>
<context position="11386" citStr="Ashok et al. (2013)" startWordPosition="1797" endWordPosition="1800"> Data Type Number of Number of Reviews Reviewers Training 113,250 2,265 Test 2,500 50 Table 2: Experiment Data 4.2 Feature Extraction We consider three different features — “Bag of words/ unigrams”, “Parse Tree Based Features” and an umbrella category consisting of genre and semantic features (see Section 4.2.3). 4.2.1 Bag of Words Bag of Words/Unigrams (UNIGRAMS) Unigrams have often been found to be effective predictive features (Joachims, 2001). In our context, this serves as a competitive baseline for the classification task. 4.2.2 Parse Tree Based Features Following Feng et al. (2012) and Ashok et al. (2013) we use Probabilistic Context Free Grammar (PCFG) to construct a parse tree for each sentence. We then generate features from this parse tree and aggregate features to a review level. All Production Rules (Γ) This set of features include all production rule features for each review, including the leaves of the parse tree for 8Reviews where product was received for free via the Vine program. 9http://www.amazon.com/gp/community-help/customerreviews-guidelines 19 each sentence in the review. This effectively represents a combination of production rules and unigrams as features and represents an a</context>
</contexts>
<marker>Ashok, Feng, Choi, 2013</marker>
<rawString>Vikas Ganjigunte Ashok, Song Feng, and Yejin Choi. Success with style: Using writing style to predict the success of novels. Poetry, 580(9): 70, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kapil Bawa</author>
<author>Robert Shoemaker</author>
</authors>
<title>The effects of free sample promotions on incremental brand sales.</title>
<date>2004</date>
<journal>Marketing Science,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="4156" citStr="Bawa and Shoemaker, 2004" startWordPosition="608" endWordPosition="611">uff. 17 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 17–27, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics This list is by no means comprehensive, but it is indicative of the extensive work in this domain. Product Sampling Here, consumers receive products for free — as a marketing tactic. This is also a well-studied phenomenon. Research in this area has indicated that consumers value free products (Shampanier et al. (2007); Palmeira and Srivastava (2013)); that product sampling affects brand sales (Bawa and Shoemaker, 2004) and that sampling influences consumer behavior (Wadhwa et al., 2008). Status Research shows that status can influence writing style. Danescu-Niculescu-Mizil et al. (2012) study discussions among Wikipedia editors and transcripts of oral arguments before the U.S. Supreme Court and show how variations in linguistic style can provide information about power differences within social groups. Stylometry focuses on the recognition of style elements to identify authors (Rosen-Zvi et al., 2004), detect genders and even determine the venue where an academic paper was presented (Bergsma et al., 2012). </context>
</contexts>
<marker>Bawa, Shoemaker, 2004</marker>
<rawString>Kapil Bawa and Robert Shoemaker. The effects of free sample promotions on incremental brand sales. Marketing Science, 23(3):345– 363, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Matt Post</author>
<author>David Yarowsky</author>
</authors>
<title>Stylometric analysis of scientific articles.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>327--337</pages>
<contexts>
<context position="2258" citStr="Bergsma et al. (2012)" startWordPosition="337" endWordPosition="340">.librarything.com/main/2007/08/amazonvine-and-early-reviewers/ Claire Cardie Department of Computer Science Department of Information Science Cornell University cardie@cs.cornell.edu Voices.”3 There could be potential concerns as to whether this enrollment affects the way reviews are written, introducing, for example, a positive bias.4 In this work, we investigate whether enrollment in the Vine program results in changes in the linguistic style used in reviews. We investigate this by looking at reviews by individuals before and after enrollment in the program. Following Feng et al. (2012) and Bergsma et al. (2012), we conduct a stylometric analysis using a number of syntactic and semantic features to detect differences in style. We believe that detecting changes in consumer behavior due to intervention by a firm is a novel natural language processing task. Our approach offers a framework for analyzing text to detect these changes. This work is relevant for social scientists and consumer advocates as research suggests that product reviews are influential (Chevalier and Mayzlin, 2006) and changes in style could potentially influence consumer decisions. 2 Related Work Our work lies at the intersection of </context>
<context position="4754" citStr="Bergsma et al., 2012" startWordPosition="696" endWordPosition="699">wa and Shoemaker, 2004) and that sampling influences consumer behavior (Wadhwa et al., 2008). Status Research shows that status can influence writing style. Danescu-Niculescu-Mizil et al. (2012) study discussions among Wikipedia editors and transcripts of oral arguments before the U.S. Supreme Court and show how variations in linguistic style can provide information about power differences within social groups. Stylometry focuses on the recognition of style elements to identify authors (Rosen-Zvi et al., 2004), detect genders and even determine the venue where an academic paper was presented (Bergsma et al., 2012). Our work draws from each of these research areas and in turn hopes to make a contribution to each in return. Our primary objective is to establish a framework to detect behavioral change due to a decision by a firm (in this case enrollment to the Vine program characterized by free products and Vine membership status) by analyzing product reviews. Further, we hope to understand the dimensions on which this behavior may have changed. Consequently, we pursue a novel stylometric task. This type of work is especially important when the traditional numerical measure (rating) suggests there is no d</context>
<context position="8538" citStr="Bergsma et al. (2012)" startWordPosition="1336" endWordPosition="1339">receiving status badges will result in Vine members posting more positive reviews. Interestingly, the average rating for reviews by Vine members posted before enrollment is 4.22 and after enrollment is 4.21 and this difference is not statistically significant. In contrast, the length of reviews significantly increased from 251 words prior to enrollment to 306 words post-enrollment. Natural language techniques are the only option to further investigate possible effects of enrollment. Consequently we focus on the review text posted by Vine members. 4.1 Approach Following Ashok et al. (2013) and Bergsma et al. (2012) we construct features that represent writing style from each review (discussed in more detail in the next section). We incorporate these features in a classification algorithm that attempts to classify each review as having been written pre or post-enrollment to the Vine program. We report whether the difference in accuracy for this classifier vs. a majority vote classification is statistically significant or not. In order to detect differences in style pre and post-enrollment, we need to address certain confounding factors — Reviewer Specificity , Product Specificity and Time Specificity. Re</context>
</contexts>
<marker>Bergsma, Post, Yarowsky, 2012</marker>
<rawString>Shane Bergsma, Matt Post, and David Yarowsky. Stylometric analysis of scientific articles. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 327–337. Association for Computational Linguistics, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<title>Natural Language Processing with Python. O’Reilly</title>
<date>2009</date>
<location>Media,</location>
<contexts>
<context position="13303" citStr="Bird et al., 2009" startWordPosition="2096" endWordPosition="2099">t Dirichlet Allocation (Blei et al., 2003) to the production rules extracted from the Probabilistic Context Free Grammar. We use the topics generated as features in our prediction task. Our objective was to determine whether certain co-occurring production rules offered better classification accuracy. Our implementation includes hyper-parameter optimization via maximum likelihood. The number of topics is selected by maximizing the pairwise cosine distance amongst topics. We used the Stanford Parser (Klein and Manning, 2003) to parse each of the reviews and the Natural Language Toolkit (NLTK) (Bird et al., 2009) to post process the results. 4.2.3 Genre and Semantic Features Style Metrics (STYLE) This includes three distinct types of metrics. Character Based - This includes counts of uppercased letters, number of letters, number of spaces and number of vowels. Word Based - This includes measures such as number of short words (3 characters or less ), long words (8 characters or less), average word length and number of different words. Syntax Based - This includes measures such as number of periods, commas, common conjunctions, interrogatives, prepositions, pronouns and verbs. Parts of Speech (POS) feat</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. Natural Language Processing with Python. O’Reilly Media, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<volume>3</volume>
<contexts>
<context position="12727" citStr="Blei et al., 2003" startWordPosition="2008" endWordPosition="2011"> set to non-terminal production rules. This allows us to investigate purely syntactic features from the text. Phrasal/ Clausal Nodes (PHR/CLSL) We also investigate features that incorporate phrasal or clausal nodes of the parse trees. Please see Table 5 and Table 6 for examples of these features. Parse Tree Measures (PTM) We construct a set of measures for each sentence based on the parse tree. These measures are maximum height of parse tree, maximum width of the parse tree and the number of sentences in each review. Latent Dirichlet Allocation (LDA) We also apply Latent Dirichlet Allocation (Blei et al., 2003) to the production rules extracted from the Probabilistic Context Free Grammar. We use the topics generated as features in our prediction task. Our objective was to determine whether certain co-occurring production rules offered better classification accuracy. Our implementation includes hyper-parameter optimization via maximum likelihood. The number of topics is selected by maximizing the pairwise cosine distance amongst topics. We used the Stanford Parser (Klein and Manning, 2003) to parse each of the reviews and the Natural Language Toolkit (NLTK) (Bird et al., 2009) to post process the res</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith A Chevalier</author>
<author>Dina Mayzlin</author>
</authors>
<title>The effect of word of mouth on sales: Online book reviews.</title>
<date>2006</date>
<journal>Journal of marketing research,</journal>
<volume>43</volume>
<issue>3</issue>
<pages>345--354</pages>
<contexts>
<context position="2736" citStr="Chevalier and Mayzlin, 2006" startWordPosition="413" endWordPosition="416">vestigate this by looking at reviews by individuals before and after enrollment in the program. Following Feng et al. (2012) and Bergsma et al. (2012), we conduct a stylometric analysis using a number of syntactic and semantic features to detect differences in style. We believe that detecting changes in consumer behavior due to intervention by a firm is a novel natural language processing task. Our approach offers a framework for analyzing text to detect these changes. This work is relevant for social scientists and consumer advocates as research suggests that product reviews are influential (Chevalier and Mayzlin, 2006) and changes in style could potentially influence consumer decisions. 2 Related Work Our work lies at the intersection of research in four broad areas — Product Reviews, Product Sampling, Status and Stylometry. Product Reviews Product reviews have received considerable attention in multiple disciplines including Marketing, Computer Science and Information Science. Research has addressed questions such as the influence of product reviews on product sales and on brands (Gopinath et al. (2014); Chevalier and Mayzlin (2006)), detection of deceptive reviews (Ott et al., 2011) and sentiment summariz</context>
</contexts>
<marker>Chevalier, Mayzlin, 2006</marker>
<rawString>Judith A Chevalier and Dina Mayzlin. The effect of word of mouth on sales: Online book reviews. Journal of marketing research, 43(3): 345–354, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meri Coleman</author>
<author>TL Liau</author>
</authors>
<title>A computer readability formula designed for machine scoring.</title>
<date>1975</date>
<journal>Journal of Applied Psychology,</journal>
<volume>60</volume>
<issue>2</issue>
<marker>Coleman, Liau, 1975</marker>
<rawString>Meri Coleman and TL Liau. A computer readability formula designed for machine scoring. Journal of Applied Psychology, 60(2):283, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Lillian Lee</author>
<author>Bo Pang</author>
<author>Jon Kleinberg</author>
</authors>
<title>Echoes of power: Language effects and power differences in social interaction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference on World Wide Web,</booktitle>
<pages>699--708</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="4327" citStr="Danescu-Niculescu-Mizil et al. (2012)" startWordPosition="632" endWordPosition="635">4 Association for Computational Linguistics This list is by no means comprehensive, but it is indicative of the extensive work in this domain. Product Sampling Here, consumers receive products for free — as a marketing tactic. This is also a well-studied phenomenon. Research in this area has indicated that consumers value free products (Shampanier et al. (2007); Palmeira and Srivastava (2013)); that product sampling affects brand sales (Bawa and Shoemaker, 2004) and that sampling influences consumer behavior (Wadhwa et al., 2008). Status Research shows that status can influence writing style. Danescu-Niculescu-Mizil et al. (2012) study discussions among Wikipedia editors and transcripts of oral arguments before the U.S. Supreme Court and show how variations in linguistic style can provide information about power differences within social groups. Stylometry focuses on the recognition of style elements to identify authors (Rosen-Zvi et al., 2004), detect genders and even determine the venue where an academic paper was presented (Bergsma et al., 2012). Our work draws from each of these research areas and in turn hopes to make a contribution to each in return. Our primary objective is to establish a framework to detect be</context>
</contexts>
<marker>Danescu-Niculescu-Mizil, Lee, Pang, Kleinberg, 2012</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Lillian Lee, Bo Pang, and Jon Kleinberg. Echoes of power: Language effects and power differences in social interaction. In Proceedings of the 21st international conference on World Wide Web, pages 699–708. ACM, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>9</volume>
<pages>1871--1874</pages>
<contexts>
<context position="15124" citStr="Fan et al. (2008)" startWordPosition="2377" endWordPosition="2380"> categories, thus ensuring the features are mutually exclusive. Subjectivity Measures (OPINION) We measure number of subjective, objective and other (neither subjective nor objective) sentences in each review. We use the “OpinionFinder System” (Wiebe et al., 2005) to classify each sentence with these measures. We aggregate the count of subjective, objective and other sentences at the review level and use these aggregates as features.10 We also report results on experiments where multiple feature types are included simultaneously in the model. 5 Experimental Methodology All experiments use the Fan et al. (2008) implementation of linear Support Vector Machines (Vapnik, 1998). The linear specification allows us to infer feature importance. We learn the penalty parameter via grid search using 5 fold cross-validation and report performance on a heldout balanced sample of reviews from 50 randomly selected users (all of whom were excluded from the training set) from the group of reviewers with at least 25 reviews in pre and post enrollment periods. While reporting the results, for some features we report the threshold (Thr) value set to exclude the least frequent features. These thresholds were also learn</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9: 1871–1874, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Feng</author>
<author>Ritwik Banerjee</author>
<author>Yejin Choi</author>
</authors>
<title>Characterizing stylistic elements in syntactic structure.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1522--1533</pages>
<contexts>
<context position="2232" citStr="Feng et al. (2012)" startWordPosition="332" endWordPosition="335">e content. 2http://blog.librarything.com/main/2007/08/amazonvine-and-early-reviewers/ Claire Cardie Department of Computer Science Department of Information Science Cornell University cardie@cs.cornell.edu Voices.”3 There could be potential concerns as to whether this enrollment affects the way reviews are written, introducing, for example, a positive bias.4 In this work, we investigate whether enrollment in the Vine program results in changes in the linguistic style used in reviews. We investigate this by looking at reviews by individuals before and after enrollment in the program. Following Feng et al. (2012) and Bergsma et al. (2012), we conduct a stylometric analysis using a number of syntactic and semantic features to detect differences in style. We believe that detecting changes in consumer behavior due to intervention by a firm is a novel natural language processing task. Our approach offers a framework for analyzing text to detect these changes. This work is relevant for social scientists and consumer advocates as research suggests that product reviews are influential (Chevalier and Mayzlin, 2006) and changes in style could potentially influence consumer decisions. 2 Related Work Our work li</context>
<context position="11362" citStr="Feng et al. (2012)" startWordPosition="1792" endWordPosition="1795">tion of style features. Data Type Number of Number of Reviews Reviewers Training 113,250 2,265 Test 2,500 50 Table 2: Experiment Data 4.2 Feature Extraction We consider three different features — “Bag of words/ unigrams”, “Parse Tree Based Features” and an umbrella category consisting of genre and semantic features (see Section 4.2.3). 4.2.1 Bag of Words Bag of Words/Unigrams (UNIGRAMS) Unigrams have often been found to be effective predictive features (Joachims, 2001). In our context, this serves as a competitive baseline for the classification task. 4.2.2 Parse Tree Based Features Following Feng et al. (2012) and Ashok et al. (2013) we use Probabilistic Context Free Grammar (PCFG) to construct a parse tree for each sentence. We then generate features from this parse tree and aggregate features to a review level. All Production Rules (Γ) This set of features include all production rule features for each review, including the leaves of the parse tree for 8Reviews where product was received for free via the Vine program. 9http://www.amazon.com/gp/community-help/customerreviews-guidelines 19 each sentence in the review. This effectively represents a combination of production rules and unigrams as feat</context>
</contexts>
<marker>Feng, Banerjee, Choi, 2012</marker>
<rawString>Song Feng, Ritwik Banerjee, and Yejin Choi. Characterizing stylistic elements in syntactic structure. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1522–1533. Association for Computational Linguistics, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shyam Gopinath</author>
<author>Jacquelyn S Thomas</author>
<author>Lakshman Krishnamurthi</author>
</authors>
<title>Investigating the relationship between the content of online word of mouth, advertising, and brand performance. Marketing Science,</title>
<date>2014</date>
<contexts>
<context position="3231" citStr="Gopinath et al. (2014)" startWordPosition="487" endWordPosition="490">ocial scientists and consumer advocates as research suggests that product reviews are influential (Chevalier and Mayzlin, 2006) and changes in style could potentially influence consumer decisions. 2 Related Work Our work lies at the intersection of research in four broad areas — Product Reviews, Product Sampling, Status and Stylometry. Product Reviews Product reviews have received considerable attention in multiple disciplines including Marketing, Computer Science and Information Science. Research has addressed questions such as the influence of product reviews on product sales and on brands (Gopinath et al. (2014); Chevalier and Mayzlin (2006)), detection of deceptive reviews (Ott et al., 2011) and sentiment summarization (Titov and McDonald, 2008). 3http://www.amazon.com/gp/vine/help, words italicized by authors. 4http://www.npr.org/blogs/money/2013/10/29/241372607/topreviewers-on-amazon-get-tons-of-free-stuff. 17 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 17–27, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics This list is by no means comprehensive, but it is indicative of the extensive work in this domain</context>
</contexts>
<marker>Gopinath, Thomas, Krishnamurthi, 2014</marker>
<rawString>Shyam Gopinath, Jacquelyn S Thomas, and Lakshman Krishnamurthi. Investigating the relationship between the content of online word of mouth, advertising, and brand performance. Marketing Science, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Gunning</author>
</authors>
<title>Technique of clear writing.</title>
<date>1952</date>
<marker>Gunning, 1952</marker>
<rawString>Robert Gunning. Technique of clear writing. 1952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>A statistical learning learning model of text classification for support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>128--136</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="11217" citStr="Joachims, 2001" startWordPosition="1770" endWordPosition="1771">uidelines from Amazon. This does not appear to be the case, as the writing guidelines 9 appear to be for all members.We now turn to the extraction of style features. Data Type Number of Number of Reviews Reviewers Training 113,250 2,265 Test 2,500 50 Table 2: Experiment Data 4.2 Feature Extraction We consider three different features — “Bag of words/ unigrams”, “Parse Tree Based Features” and an umbrella category consisting of genre and semantic features (see Section 4.2.3). 4.2.1 Bag of Words Bag of Words/Unigrams (UNIGRAMS) Unigrams have often been found to be effective predictive features (Joachims, 2001). In our context, this serves as a competitive baseline for the classification task. 4.2.2 Parse Tree Based Features Following Feng et al. (2012) and Ashok et al. (2013) we use Probabilistic Context Free Grammar (PCFG) to construct a parse tree for each sentence. We then generate features from this parse tree and aggregate features to a review level. All Production Rules (Γ) This set of features include all production rule features for each review, including the leaves of the parse tree for 8Reviews where product was received for free via the Vine program. 9http://www.amazon.com/gp/community-h</context>
</contexts>
<marker>Joachims, 2001</marker>
<rawString>Thorsten Joachims. A statistical learning learning model of text classification for support vector machines. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 128–136. ACM, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Peter Kincaid</author>
<author>Robert P Fishburne Jr</author>
<author>Richard L Rogers</author>
<author>Brad S Chissom</author>
</authors>
<title>Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel.</title>
<date>1975</date>
<tech>Technical report, DTIC Document,</tech>
<marker>Kincaid, Jr, Rogers, Chissom, 1975</marker>
<rawString>J Peter Kincaid, Robert P Fishburne Jr, Richard L Rogers, and Brad S Chissom. Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. Technical report, DTIC Document, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="13214" citStr="Klein and Manning, 2003" startWordPosition="2079" endWordPosition="2083">d the number of sentences in each review. Latent Dirichlet Allocation (LDA) We also apply Latent Dirichlet Allocation (Blei et al., 2003) to the production rules extracted from the Probabilistic Context Free Grammar. We use the topics generated as features in our prediction task. Our objective was to determine whether certain co-occurring production rules offered better classification accuracy. Our implementation includes hyper-parameter optimization via maximum likelihood. The number of topics is selected by maximizing the pairwise cosine distance amongst topics. We used the Stanford Parser (Klein and Manning, 2003) to parse each of the reviews and the Natural Language Toolkit (NLTK) (Bird et al., 2009) to post process the results. 4.2.3 Genre and Semantic Features Style Metrics (STYLE) This includes three distinct types of metrics. Character Based - This includes counts of uppercased letters, number of letters, number of spaces and number of vowels. Word Based - This includes measures such as number of short words (3 characters or less ), long words (8 characters or less), average word length and number of different words. Syntax Based - This includes measures such as number of periods, commas, common c</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423– 430. Association for Computational Linguistics, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Harry McLaughlin</author>
</authors>
<title>Smog grading: A new readability formula.</title>
<date>1969</date>
<journal>Journal of reading,</journal>
<volume>12</volume>
<issue>8</issue>
<pages>646</pages>
<marker>McLaughlin, 1969</marker>
<rawString>G Harry McLaughlin. Smog grading: A new readability formula. Journal of reading, 12(8):639– 646, 1969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myle Ott</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Jeffrey T Hancock</author>
</authors>
<title>Finding deceptive opinion spam by any stretch of the imagination.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>309--319</pages>
<contexts>
<context position="3313" citStr="Ott et al., 2011" startWordPosition="499" endWordPosition="502">nfluential (Chevalier and Mayzlin, 2006) and changes in style could potentially influence consumer decisions. 2 Related Work Our work lies at the intersection of research in four broad areas — Product Reviews, Product Sampling, Status and Stylometry. Product Reviews Product reviews have received considerable attention in multiple disciplines including Marketing, Computer Science and Information Science. Research has addressed questions such as the influence of product reviews on product sales and on brands (Gopinath et al. (2014); Chevalier and Mayzlin (2006)), detection of deceptive reviews (Ott et al., 2011) and sentiment summarization (Titov and McDonald, 2008). 3http://www.amazon.com/gp/vine/help, words italicized by authors. 4http://www.npr.org/blogs/money/2013/10/29/241372607/topreviewers-on-amazon-get-tons-of-free-stuff. 17 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 17–27, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics This list is by no means comprehensive, but it is indicative of the extensive work in this domain. Product Sampling Here, consumers receive products for free — as a marketing tact</context>
<context position="14003" citStr="Ott et al., 2011" startWordPosition="2209" endWordPosition="2212">E) This includes three distinct types of metrics. Character Based - This includes counts of uppercased letters, number of letters, number of spaces and number of vowels. Word Based - This includes measures such as number of short words (3 characters or less ), long words (8 characters or less), average word length and number of different words. Syntax Based - This includes measures such as number of periods, commas, common conjunctions, interrogatives, prepositions, pronouns and verbs. Parts of Speech (POS) features have often been surprisingly effective in tasks such as predicting deception (Ott et al., 2011). Consequently we test this feature set as well. Domain-independent Dictionary We make use of the Linguistic Inquiry and Word Count (LIWC) categorization (Tausczik and Pennebaker, 2010). One key advantage of this categorization is that it is domain independent and emphasizes psycho-linguistic cues. We run two variants of this set of features. The first (LIWC ALL) includes all the categories — both sub-ordinate and super-ordinate categories. The second (LIWC SUB CATEG.) only includes the sub-ordinate categories, thus ensuring the features are mutually exclusive. Subjectivity Measures (OPINION) </context>
</contexts>
<marker>Ott, Choi, Cardie, Hancock, 2011</marker>
<rawString>Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T Hancock. Finding deceptive opinion spam by any stretch of the imagination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 309– 319. Association for Computational Linguistics, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauricio M Palmeira</author>
<author>Joydeep Srivastava</author>
</authors>
<title>Free offers cheap product: A selective accessibility account on the valuation of free offers.</title>
<date>2013</date>
<journal>Journal of Consumer Research,</journal>
<volume>40</volume>
<issue>4</issue>
<contexts>
<context position="4085" citStr="Palmeira and Srivastava (2013)" startWordPosition="598" endWordPosition="601">/blogs/money/2013/10/29/241372607/topreviewers-on-amazon-get-tons-of-free-stuff. 17 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 17–27, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics This list is by no means comprehensive, but it is indicative of the extensive work in this domain. Product Sampling Here, consumers receive products for free — as a marketing tactic. This is also a well-studied phenomenon. Research in this area has indicated that consumers value free products (Shampanier et al. (2007); Palmeira and Srivastava (2013)); that product sampling affects brand sales (Bawa and Shoemaker, 2004) and that sampling influences consumer behavior (Wadhwa et al., 2008). Status Research shows that status can influence writing style. Danescu-Niculescu-Mizil et al. (2012) study discussions among Wikipedia editors and transcripts of oral arguments before the U.S. Supreme Court and show how variations in linguistic style can provide information about power differences within social groups. Stylometry focuses on the recognition of style elements to identify authors (Rosen-Zvi et al., 2004), detect genders and even determine t</context>
</contexts>
<marker>Palmeira, Srivastava, 2013</marker>
<rawString>Mauricio M Palmeira and Joydeep Srivastava. Free offers cheap product: A selective accessibility account on the valuation of free offers. Journal of Consumer Research, 40(4):644–656, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michal Rosen-Zvi</author>
<author>Thomas Griffiths</author>
<author>Mark Steyvers</author>
<author>Padhraic Smyth</author>
</authors>
<title>The authortopic model for authors and documents.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th conference on Uncertainty in artificial intelligence,</booktitle>
<pages>487--494</pages>
<publisher>AUAI Press,</publisher>
<contexts>
<context position="4648" citStr="Rosen-Zvi et al., 2004" startWordPosition="679" endWordPosition="682">ts (Shampanier et al. (2007); Palmeira and Srivastava (2013)); that product sampling affects brand sales (Bawa and Shoemaker, 2004) and that sampling influences consumer behavior (Wadhwa et al., 2008). Status Research shows that status can influence writing style. Danescu-Niculescu-Mizil et al. (2012) study discussions among Wikipedia editors and transcripts of oral arguments before the U.S. Supreme Court and show how variations in linguistic style can provide information about power differences within social groups. Stylometry focuses on the recognition of style elements to identify authors (Rosen-Zvi et al., 2004), detect genders and even determine the venue where an academic paper was presented (Bergsma et al., 2012). Our work draws from each of these research areas and in turn hopes to make a contribution to each in return. Our primary objective is to establish a framework to detect behavioral change due to a decision by a firm (in this case enrollment to the Vine program characterized by free products and Vine membership status) by analyzing product reviews. Further, we hope to understand the dimensions on which this behavior may have changed. Consequently, we pursue a novel stylometric task. This t</context>
</contexts>
<marker>Rosen-Zvi, Griffiths, Steyvers, Smyth, 2004</marker>
<rawString>Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and Padhraic Smyth. The authortopic model for authors and documents. In Proceedings of the 20th conference on Uncertainty in artificial intelligence, pages 487–494. AUAI Press, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RJ Senter</author>
<author>EA Smith</author>
</authors>
<title>Automated readability index.</title>
<date>1967</date>
<tech>Technical report, DTIC Document,</tech>
<marker>Senter, Smith, 1967</marker>
<rawString>RJ Senter and EA Smith. Automated readability index. Technical report, DTIC Document, 1967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Shampanier</author>
<author>Nina Mazar</author>
<author>Dan Ariely</author>
</authors>
<title>Zero as a special price: The true value of free products.</title>
<date>2007</date>
<journal>Marketing Science,</journal>
<volume>26</volume>
<issue>6</issue>
<pages>757</pages>
<contexts>
<context position="4053" citStr="Shampanier et al. (2007)" startWordPosition="594" endWordPosition="597">thors. 4http://www.npr.org/blogs/money/2013/10/29/241372607/topreviewers-on-amazon-get-tons-of-free-stuff. 17 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 17–27, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics This list is by no means comprehensive, but it is indicative of the extensive work in this domain. Product Sampling Here, consumers receive products for free — as a marketing tactic. This is also a well-studied phenomenon. Research in this area has indicated that consumers value free products (Shampanier et al. (2007); Palmeira and Srivastava (2013)); that product sampling affects brand sales (Bawa and Shoemaker, 2004) and that sampling influences consumer behavior (Wadhwa et al., 2008). Status Research shows that status can influence writing style. Danescu-Niculescu-Mizil et al. (2012) study discussions among Wikipedia editors and transcripts of oral arguments before the U.S. Supreme Court and show how variations in linguistic style can provide information about power differences within social groups. Stylometry focuses on the recognition of style elements to identify authors (Rosen-Zvi et al., 2004), det</context>
</contexts>
<marker>Shampanier, Mazar, Ariely, 2007</marker>
<rawString>Kristina Shampanier, Nina Mazar, and Dan Ariely. Zero as a special price: The true value of free products. Marketing Science, 26(6):742– 757, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yla R Tausczik</author>
<author>James W Pennebaker</author>
</authors>
<title>The psychological meaning of words: Liwc and computerized text analysis methods.</title>
<date>2010</date>
<journal>Journal of Language and Social Psychology,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="14188" citStr="Tausczik and Pennebaker, 2010" startWordPosition="2235" endWordPosition="2239">rd Based - This includes measures such as number of short words (3 characters or less ), long words (8 characters or less), average word length and number of different words. Syntax Based - This includes measures such as number of periods, commas, common conjunctions, interrogatives, prepositions, pronouns and verbs. Parts of Speech (POS) features have often been surprisingly effective in tasks such as predicting deception (Ott et al., 2011). Consequently we test this feature set as well. Domain-independent Dictionary We make use of the Linguistic Inquiry and Word Count (LIWC) categorization (Tausczik and Pennebaker, 2010). One key advantage of this categorization is that it is domain independent and emphasizes psycho-linguistic cues. We run two variants of this set of features. The first (LIWC ALL) includes all the categories — both sub-ordinate and super-ordinate categories. The second (LIWC SUB CATEG.) only includes the sub-ordinate categories, thus ensuring the features are mutually exclusive. Subjectivity Measures (OPINION) We measure number of subjective, objective and other (neither subjective nor objective) sentences in each review. We use the “OpinionFinder System” (Wiebe et al., 2005) to classify each</context>
</contexts>
<marker>Tausczik, Pennebaker, 2010</marker>
<rawString>Yla R Tausczik and James W Pennebaker. The psychological meaning of words: Liwc and computerized text analysis methods. Journal of Language and Social Psychology, 29(1):24–54, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>A joint model of text and aspect ratings for sentiment summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>308--316</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3368" citStr="Titov and McDonald, 2008" startWordPosition="507" endWordPosition="510">anges in style could potentially influence consumer decisions. 2 Related Work Our work lies at the intersection of research in four broad areas — Product Reviews, Product Sampling, Status and Stylometry. Product Reviews Product reviews have received considerable attention in multiple disciplines including Marketing, Computer Science and Information Science. Research has addressed questions such as the influence of product reviews on product sales and on brands (Gopinath et al. (2014); Chevalier and Mayzlin (2006)), detection of deceptive reviews (Ott et al., 2011) and sentiment summarization (Titov and McDonald, 2008). 3http://www.amazon.com/gp/vine/help, words italicized by authors. 4http://www.npr.org/blogs/money/2013/10/29/241372607/topreviewers-on-amazon-get-tons-of-free-stuff. 17 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 17–27, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics This list is by no means comprehensive, but it is indicative of the extensive work in this domain. Product Sampling Here, consumers receive products for free — as a marketing tactic. This is also a well-studied phenomenon. Research in</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. A joint model of text and aspect ratings for sentiment summarization. In Proceedings of ACL-08: HLT, pages 308–316, Columbus, Ohio, June 2008. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<location>Wiley-Interscience,</location>
<contexts>
<context position="15188" citStr="Vapnik, 1998" startWordPosition="2388" endWordPosition="2389">ectivity Measures (OPINION) We measure number of subjective, objective and other (neither subjective nor objective) sentences in each review. We use the “OpinionFinder System” (Wiebe et al., 2005) to classify each sentence with these measures. We aggregate the count of subjective, objective and other sentences at the review level and use these aggregates as features.10 We also report results on experiments where multiple feature types are included simultaneously in the model. 5 Experimental Methodology All experiments use the Fan et al. (2008) implementation of linear Support Vector Machines (Vapnik, 1998). The linear specification allows us to infer feature importance. We learn the penalty parameter via grid search using 5 fold cross-validation and report performance on a heldout balanced sample of reviews from 50 randomly selected users (all of whom were excluded from the training set) from the group of reviewers with at least 25 reviews in pre and post enrollment periods. While reporting the results, for some features we report the threshold (Thr) value set to exclude the least frequent features. These thresholds were also learned via the 5 fold cross validation process. Finally, text featur</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monica Wadhwa</author>
<author>Baba Shiv</author>
<author>M Stephen</author>
</authors>
<title>Nowlis. A bite to whet the reward appetite: The influence of sampling on reward-seeking behaviors.</title>
<date>2008</date>
<journal>Journal of Marketing Research,</journal>
<volume>45</volume>
<pages>4--403</pages>
<contexts>
<context position="4225" citStr="Wadhwa et al., 2008" startWordPosition="618" endWordPosition="621">Attributes in Social Media, pages 17–27, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics This list is by no means comprehensive, but it is indicative of the extensive work in this domain. Product Sampling Here, consumers receive products for free — as a marketing tactic. This is also a well-studied phenomenon. Research in this area has indicated that consumers value free products (Shampanier et al. (2007); Palmeira and Srivastava (2013)); that product sampling affects brand sales (Bawa and Shoemaker, 2004) and that sampling influences consumer behavior (Wadhwa et al., 2008). Status Research shows that status can influence writing style. Danescu-Niculescu-Mizil et al. (2012) study discussions among Wikipedia editors and transcripts of oral arguments before the U.S. Supreme Court and show how variations in linguistic style can provide information about power differences within social groups. Stylometry focuses on the recognition of style elements to identify authors (Rosen-Zvi et al., 2004), detect genders and even determine the venue where an academic paper was presented (Bergsma et al., 2012). Our work draws from each of these research areas and in turn hopes to</context>
</contexts>
<marker>Wadhwa, Shiv, Stephen, 2008</marker>
<rawString>Monica Wadhwa, Baba Shiv, and Stephen M Nowlis. A bite to whet the reward appetite: The influence of sampling on reward-seeking behaviors. Journal of Marketing Research, 45 (4):403–413, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language resources and evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="14771" citStr="Wiebe et al., 2005" startWordPosition="2320" endWordPosition="2323">tion (Tausczik and Pennebaker, 2010). One key advantage of this categorization is that it is domain independent and emphasizes psycho-linguistic cues. We run two variants of this set of features. The first (LIWC ALL) includes all the categories — both sub-ordinate and super-ordinate categories. The second (LIWC SUB CATEG.) only includes the sub-ordinate categories, thus ensuring the features are mutually exclusive. Subjectivity Measures (OPINION) We measure number of subjective, objective and other (neither subjective nor objective) sentences in each review. We use the “OpinionFinder System” (Wiebe et al., 2005) to classify each sentence with these measures. We aggregate the count of subjective, objective and other sentences at the review level and use these aggregates as features.10 We also report results on experiments where multiple feature types are included simultaneously in the model. 5 Experimental Methodology All experiments use the Fan et al. (2008) implementation of linear Support Vector Machines (Vapnik, 1998). The linear specification allows us to infer feature importance. We learn the penalty parameter via grid search using 5 fold cross-validation and report performance on a heldout bala</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2-3):165–210, 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000104">
<title confidence="0.7209">
Focused Entailment Graphs for Open IE Propositions
</title>
<author confidence="0.526672">
Omer Levy† Ido Dagan† Jacob Goldberger§
</author>
<affiliation confidence="0.4736435">
† Computer Science Department § Faculty of Engineering
Bar-Ilan University
</affiliation>
<address confidence="0.432966">
Ramat-Gan, Israel
</address>
<email confidence="0.804501">
{omerlevy,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
</email>
<sectionHeader confidence="0.990044" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999021888888889">
Open IE methods extract structured propo-
sitions from text. However, these propo-
sitions are neither consolidated nor gen-
eralized, and querying them may lead
to insufficient or redundant information.
This work suggests an approach to or-
ganize open IE propositions using entail-
ment graphs. The entailment relation uni-
fies equivalent propositions and induces a
specific-to-general structure. We create a
large dataset of gold-standard proposition
entailment graphs, and provide a novel
algorithm for automatically constructing
them. Our analysis shows that predicate
entailment is extremely context-sensitive,
and that current lexical-semantic resources
do not capture many of the lexical infer-
ences induced by proposition entailment.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999657166666667">
Open information extraction (open IE) extracts
natural language propositions from text without
pre-defined schemas as in supervised relation ex-
traction (Etzioni et al., 2008). These proposi-
tions represent predicate-argument structures as
tuples of natural language strings. Open IE en-
ables knowledge search by aggregating billions of
propositions from the web1. It may also be per-
ceived as capturing an unsupervised knowledge
representation schema, complementing supervised
knowledge bases such as Freebase (Bollacker et
al., 2008), as suggested by Riedel et al (2013).
However, language variability obstructs open IE
from becoming a viable knowledge representation
framework. As it does not consolidate natural lan-
guage expressions, querying a database of open IE
propositions may lead to either insufficient or re-
dundant information. As an illustrative example,
</bodyText>
<footnote confidence="0.738131">
1See demo: openie.cs.washington.edu
</footnote>
<bodyText confidence="0.999951634146342">
querying the demo (footnote 1) for the generally
equivalent relieves headache or treats headache
returns two different lists of entities; out of the top
few results, the only answers these queries seem
to agree on are caffeine and sex. This is a major
drawback relative to supervised knowledge rep-
resentations, which map natural language expres-
sions to structured formal representations, such as
treatments in Freebase.
In this work, we investigate an approach for or-
ganizing and consolidating open IE propositions
using the novel notion of proposition entailment
graphs (see Figure 1) – graphs in which each
node represents a proposition and each directed
edge reflects an entailment relation, in the spirit
of textual entailment (Dagan et al., 2013). En-
tailment provides an effective structure for ag-
gregating natural-language based information; it
merges semantically equivalent propositions into
cliques, and induces specification-generalization
edges between them. For example, (aspirin, elim-
inate, headache) entails, and is more specific than,
(headache, respond to, painkiller).
We thus propose the task of constructing an
entailment graph over a set of open IE proposi-
tions (Section 3), which is closely related to Be-
rant et al’s work (2012) who introduced predicate
entailment graphs. In contrast, our work explores
propositions, which are essentially predicates in-
stantiated with arguments, and thus semantically
richer. We provide a dataset of 30 such graphs,
which represent 1.5 million pairwise entailment
decisions between propositions (Section 4).
To approach this task, we extend the state-of-
the-art method for building entailment graphs (Be-
rant et al., 2012) from predicates to complete
propositions. Both Snow et al (2006) and Berant et
al used WordNet as distant supervision when train-
ing a local pairwise model of lexical entailment.
However, analyzing our data revealed that the lex-
ical inferences captured in WordNet are quite dif-
</bodyText>
<page confidence="0.991728">
87
</page>
<note confidence="0.8199415">
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 87–97,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.99921">
Figure 1: An excerpt from a proposition entailment graph focused on the topic headache. The dashed boundaries in the figure
denote cliques, meaning that all propositions within them are equivalent.
</figureCaption>
<bodyText confidence="0.99988034375">
ferent from the real lexical inferences induced by
proposition entailment, making WordNet a mis-
leading form of supervision. We therefore employ
direct proposition-level supervision, and design a
probabilistic model that captures the underlying
lexical-component inferences (Section 5). We ex-
plore a variety of natural extensions to prior art as
baselines (Section 6) and show that our model out-
performs them (Section 7).
While our model increases performance on this
task, there is still much room for improvement. A
deeper analysis (Section 8) shows that common
lexical-semantic resources, on which we rely as
well, are either too noisy or provide inadequate re-
call regarding lexical entailment. In particular, we
find that predicate inference within propositions
often goes beyond inference between the predi-
cates’ linguistic meanings. While pneumonia re-
quires antibiotics and pneumonia is treated by an-
tibiotics mean the same, the inherent meanings of
require and treat are different. These inferences
pertain to specific world knowledge, and warrant
future research.
Our work also contributes to textual entailment
research. First, we extend entailment graphs to
complete propositions. Secondly, we investigate
an intermediate problem of recognizing entail-
ment between language-based predicate-argument
tuples. Though this problem is simpler than
sentence-level entailment, it does capture entail-
ment of complete statements, which proves to be
quite challenging indeed.
</bodyText>
<sectionHeader confidence="0.991798" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9867845">
Our work builds upon two major research threads:
open IE, and entailment graphs.
</bodyText>
<subsectionHeader confidence="0.91351">
2.1 Open Information Extraction
</subsectionHeader>
<bodyText confidence="0.99989775">
Research in open IE (Etzioni et al., 2008) has fo-
cused on transforming text to predicate-argument
tuples (propositions). The general approach is to
learn proposition extraction patterns, and use them
to create tuples while denoting extraction confi-
dence. Various methods differ in the type of pat-
terns they acquire. For instance, (Banko et al.,
2007) and (Fader et al., 2011) used surface pat-
terns, while (Mausam et al., 2012) and (Xu et al.,
2013) used syntactic dependencies.
Yates and Etzioni (2009) tried to mitigate the
issue of language variability (as exemplified in
the introduction) by clustering synonymous predi-
cates and arguments. While these clusters do con-
tain semantically related items, they do not neces-
sarily reflect equivalence or implication. For ex-
ample, coffee, tea, and caffeine may all appear
in one cluster, but coffee does not imply tea; on
the other hand, separating any element from this
cluster removes a valid implication. Entailment,
however, can capture the fact that both beverages
imply caffeine, but not one another. Also related,
Riedel et al (2013) try to generalize over open IE
extractions by combining knowledge from Free-
base and globally predicting which unobserved
propositions are true. In contrast, our work identi-
fies inference relations between concrete pairs of
observed propositions.
</bodyText>
<subsectionHeader confidence="0.998658">
2.2 Entailment Graphs of Words and Phrases
</subsectionHeader>
<bodyText confidence="0.999698833333333">
Previous work focused on entailment graphs or
similar structures at the sub-propositional level.
In these graphs, each node represents a natu-
ral language word or phrase, and each directed
edge an entailment (or generalization) relation.
Snow et al (2006) created a taxonomy of sense-
</bodyText>
<page confidence="0.99806">
88
</page>
<bodyText confidence="0.999935619047619">
disambiguated nouns and their hyponymy rela-
tions. Berant et al (2012) constructed entailment
graphs of predicate templates. Recently, Mehdad
et al (2013) built an entailment graph of noun
phrases and partial sentences for topic labeling.
The notion of proposition entailment graphs, how-
ever, is novel. This distinction is critical, be-
cause apparently, entailment in the context of spe-
cific propositions does not behave like context-
oblivious lexical entailment (see Section 8).
Berant et al’s work was implemented in Adler
et al’s (2012) text exploration demo, which instan-
tiated manually-annotated predicate entailment
graphs with arguments, and used an additional
lexical resource to determine argument entail-
ment. The combined graphs of predicate and argu-
ment entailments induced a proposition entailment
graph, which could then be explored in a faceted-
search scheme. Our work goes beyond, and at-
tempts to build entailment graphs of propositions
automatically.
</bodyText>
<subsectionHeader confidence="0.91511">
2.2.1 Berant et al’s Algorithm for Predicate
Entailment Graph Construction
</subsectionHeader>
<bodyText confidence="0.998664342857143">
We present Berant et al’s algorithm in detail, as we
rely on it later on. Given a set of predicates {i}1..n
as input (constituting the graph nodes), it returns
a set of entailment decisions (i, j), which become
the directed edges of the entailment graph. The
method works in two phases: (1) local estimation,
and (2) global optimization.
The local estimation model considers every po-
tential edge (i, j) and estimates the probability pij
that this edge indeed exists, i.e. that i entails j.
Each predicate pair is represented with distribu-
tional similarity features, providing some indica-
tion of whether i entails j. The estimator then uses
logistic regression (or a linear SVM) over those
features to predict the probability of entailment. It
is trained with distant supervision from WordNet,
employing synonyms, hypernyms, and (WordNet)
entailments as positive examples, and antonyms,
hyponyms, and cohyponyms as negative.
The global optimization phase then searches
for the most probable transitive entailment graph,
given the local probability estimations. It does so
with an integer linear program (ILP), where each
pair of predicates is represented by a binary vari-
able xij, denoting whether there is an entailment
edge from i to j. The objective function corre-
sponds to the log likelihood of the assignment:
Ei�j xij (log (1-Pij) + log (1−7π )). The prior
term π is the probability of a random pair of pred-
icates to be in an entailment relation, and can be
estimated in advance. The ILP solver searches
for the optimal assignment that maximizes the ob-
jective function under transitivity constraints, ex-
pressed as linear constraints Vi,j,k xij + xjk −
xik G 1.
</bodyText>
<sectionHeader confidence="0.98739" genericHeader="method">
3 Task Definition
</sectionHeader>
<bodyText confidence="0.97106025">
A proposition entailment graph is a directed graph
where each node is a proposition si (s for sen-
tence) and each edge (si, sj) represents an en-
tailment relation from si to sj. A proposi-
tion si is a predicate-argument structure si =
(pi, a1i , a2i , ..., ami � with one predicate pi and its
i
arguments. A proposition-level entailment (si, sj)
holds if the verbalization of si implies sj, accord-
ing to the definition of textual entailment (Dagan
et al., 2013); i.e. if humans reading si would typi-
cally infer that sj is most likely true. Given a set of
propositions (graph nodes), the task of construct-
ing a proposition entailment graph is to recognize
all the entailments among the propositions, i.e.
deciding which directional edges connect which
pairs of nodes.
In this paper, we consider the narrower task
of constructing focused proposition entailment
graphs, following Berant et al’s methodology
in creating focused predicate entailment graphs.
First, all predicates are binary (have two argu-
ments) and are denoted si = (a1i, pi, a2). Sec-
i
ondly, we assume that the propositions were re-
trieved by querying for a particular concept; out
of the two arguments, one argument t (topic) is
common to all the propositions in a single graph.
We denote the non-topic argument as ai. Figure 1
presents an example of an informative entailment
graph focused on the topic headache.
Though confined, this setting still challenges
the state-of-the-art in textual entailment (see Sec-
tion 7). Moreover, these restrictions facilitate
piece-wise investigation of the entailment problem
(see Section 8).
</bodyText>
<sectionHeader confidence="0.998187" genericHeader="method">
4 Dataset
</sectionHeader>
<bodyText confidence="0.9997008">
To construct our dataset of open IE extractions, we
found Google’s syntactic ngrams (Goldberg and
Orwant, 2013) as a useful source of high-quality
propositions. Based on a corpus of 3.5 million En-
glish books, it aggregates every syntactic ngram
</bodyText>
<page confidence="0.998765">
89
</page>
<bodyText confidence="0.986891737704918">
– subtree of a dependency parse – with at most
4 dependency arcs. The resource contains only
tree fragments that appeared at least 10 times in
the corpus, filtering out many low-quality syntac-
tic ngrams.
We extracted the syntactic ngrams that reflect
propositions, i.e. subject-verb-object fragments
where object modifies the verb with either dobj
or pobj. Prepositions in pobj were concatenated
to the verb (e.g. use with). In addition, both sub-
ject and object must each be a noun phrase con-
taining two tokens at most, which are either nouns
or adjectives. Each token in the extracted frag-
ments was then lemmatized using WordNet. After
lemmatization, we grouped all identical proposi-
tions and aggregated their counts. Approximately
68 million propositions were collected.
We chose 30 topics from the healthcare domain
(such as influenza, hiv, and penicillin). For each
topic, we collected the set of propositions con-
taining it, and manually filtered noisy extractions.
This yielded 30 high-quality sets of 5,714 propo-
sitions in total, where each set becomes the set of
nodes in a separate focused entailment graph. The
graphs range from 55 propositions (scurvy) to 562
(headache), with an average of over 190 proposi-
tions per graph. Summing the number of propo-
sition pairs within each graph amounts to a total
of 1.5 million potential entailment edges, which
makes it by far the largest annotated textual entail-
ment dataset to date.
We used a semi-automatic annotation process,
which dramatically narrows down the number of
manual decisions, and hence, the required anno-
tation time. In short, the annotators are given a
series of small clustering tasks before annotating
entailment between those clusters.2
The annotation process was carried out by two
native English speakers, with the aid of encyclope-
dic knowledge for unfamiliar medical terms. The
agreement on a subset of five randomly sampled
graphs was κ = 0.77. Annotating a single graph
took about an hour and a half on average.
Positive entailment judgements constituted only
8.4% of potential edges, and were found to be
100% transitive. We observe that in nearly all of
those cases, a natural alignment between entail-
ing components occurs: predicates align with each
other, the topic is shared, and the remaining non-
2The annotated dataset is publicly available on the first
author’s website.
topic argument aligns with its counterpart. Con-
sider the topic arthritis and the entailing proposi-
tion pair (arthritis, cause, pain)→(symptom, as-
sociate with, arthritis); cause→associate with,
while pain→symptom. Rarely, some mis-
alignments do occur; for instance (vaccine,
protects, body)→(vaccine, provides, protection).
However, it is almost always the case that proposi-
tions entail if and only if their aligned lexical com-
ponents entail as well.
</bodyText>
<sectionHeader confidence="0.992078" genericHeader="method">
5 Algorithm
</sectionHeader>
<bodyText confidence="0.999961407407407">
In this section, we extend Berant et al’s algorithm
(2012) to construct entailment graphs of proposi-
tions. As described in Section 2.2.1, their method
first performs local estimation of predicate entail-
ment and then global optimization. We modify the
local estimation phase to estimate proposition en-
tailment instead, and then apply the same global
optimization in the second phase.
In Section 4, we observed the alignment-based
relationship between proposition and lexical en-
tailment. We leverage this observation to predict
proposition entailment with lexical entailment fea-
tures (as Berant et al), using the Component En-
tailment Conjunction (CEC) model in Section 5.1.
Following Snow et al (2006) and Berant et
al, we could train CEC using distant supervision
from WordNet. In fact, we did try this approach
(presented as baseline methods, Section 6) and
found that it performed poorly. Furthermore, our
analysis (Section 8) suggests that WordNet rela-
tions do not adequately capture the lexical infer-
ences induced by proposition-level entailment. In-
stead, we use a more realistic signal to train CEC –
direct supervision from the annotated dataset. Sec-
tion 5.2 describes how we propagate proposition-
level entailment annotations to the latent lexical
components.
</bodyText>
<subsectionHeader confidence="0.99629">
5.1 Component Entailment Conjunction
</subsectionHeader>
<bodyText confidence="0.9999281">
CEC assumes that proposition-level entailment
is the result of entailment within each pair of
aligned components, i.e. a pair of propositions
entail if and only if both their predicate and ar-
gument pairs entail. This assumption stems from
our observation of alignment in Section 4. Fur-
thermore, CEC leverages this interdependence to
learn separate predicate-entailment and argument-
entailment features through proposition-level su-
pervision.
</bodyText>
<page confidence="0.988818">
90
</page>
<bodyText confidence="0.9750097">
Formally, for every ordered pair of propositions
(i, j) we denote proposition entailment as a binary
random variable xs ij and predicate and argument
entailments as xp ijand xaij, respectively. In our
setting, proposition entailment (xs ij) is observed,
but component entailments (xpij, xa ij) are hidden.
We use logistic regression, with features φpij and
parameter wp, as a probabilistic model of predi-
cate entailment (and so for arguments with φa ijand
wa):
</bodyText>
<equation confidence="0.999762">
pij = P(xpj = 1 φpj;wp)
= σ(φpj · wp)
aij = P(xj = 1ij;wa)= σ( φaij · wa) φa (1)
</equation>
<bodyText confidence="0.9952905">
where σ is the sigmoid σ (z) = 1
1+e−z . We then
define proposition entailment as the conjunction of
its binary components: xs ij = xpij nxa ij. Therefore,
the probability of proposition entailment given the
component features is:
</bodyText>
<equation confidence="0.9647544">
sij = P(xj = 1 �j,0j; wp, wa)
= P(xipj = 1,xa.Z3 �
=1 j,qj; wp, wa)
= P(xPj = 1 φip j;wp)· P(xa.Z3=1 qj;wa)
= pij · aij
</equation>
<bodyText confidence="0.997159">
The proposition entailment probability is thus the
product of component entailment probabilities.
</bodyText>
<equation confidence="0.71025">
{ }
</equation>
<bodyText confidence="0.773801">
Given the proposition-level information xs ,
</bodyText>
<equation confidence="0.86957025">
ij
the log-likelihood is:
`(wp wa)=E i j log P(xsij φipj,φaij; wp, wa)=
E
</equation>
<bodyText confidence="0.493704">
i#j (xsi log (pijaij) + (1 − x j) log (1 − pijaij))
</bodyText>
<subsectionHeader confidence="0.999513">
5.2 Learning Component Models
</subsectionHeader>
<bodyText confidence="0.976966411764706">
We wish to learn the model’s parameters (wp, wa).
Our approach uses direct proposition-level super-
vision from our annotated dataset to train the com-
ponent logistic regression models. Since compo-
nent entailment (xpij, xaij) is not observed in the
data, we apply the iterative EM algorithm (Demp-
ster et al., 1977). In the E-step we estimate their
probabilities from proposition-level labels (xsij),
and in the M-step we use those estimates as “soft”
labels to learn the component-level model param-
eters (wp, wa).
E-Step During the E-step in iteration t + 1,
we compute the probability of component entail-
ments given the proposition entailment informa-
tion, based on the parameters at iteration t (wpt ,
wat
). The predicate probabilities w�oore given by:
</bodyText>
<equation confidence="0.978283">
cp
ij = P (xp
ij = 1
ij ij;wpt ,wt) (2)
</equation>
<bodyText confidence="0.791543">
and are computed with Bayes’ law:
</bodyText>
<table confidence="0.9058915">
� 1 if xsij = 1
�cpij = � pt ij(1−at ij) (3)
if xs ij = 0
1−pt ijat ij
</table>
<bodyText confidence="0.99946515">
where ptij is computed as in Equations 1, with the
parameters at iteration t (wpt ). Argument entail-
ment probabilities (caij) are computed analogously.
M-Step In the M-step, we compute new values
for the parameters (wpt+1, wat+1). In our case, there
is no closed-form formula for updating the param-
eters. Instead, at each iteration, we solve a sepa-
rate logistic regression for each component. While
we have each component model’s features (φpij,
assuming predicates for notation), we do not ob-
serve the component-level entailment labels (xpij);
instead, we obtain their probabilities (cpij) from the
expectation step.
To learn the parameters (wpt+1, wat+1) from the
component entailment probabilities (cpij), we em-
ploy a weighted variant of logistic regression, that
can utilize “soft” class labels (i.e. a probability
distribution over {0,1}). To solve such a logistic
regression (e.g. for wpt+1), we maximize the log-
likelihood:
</bodyText>
<equation confidence="0.865317">
` �wp � =
t+1
Eij (cipj log (P (xipj = 1 φipj; wtp+1)) + (1 − cipj) log (P (xipj = 0 φipj; et+1)) )
</equation>
<bodyText confidence="0.994353">
For optimization, we calculate the derivative, and
use gradient ascent to update wpt+1:
</bodyText>
<equation confidence="0.979409333333333">
Δwp t+1 = ∂`(wp t+1)
∂wp t+1 =
nj(cipj − P(xipj = 1 φipj;wtp+1))φipj
</equation>
<bodyText confidence="0.996613">
This optimization is concave, and therefore the
unique global maximum can be efficiently ob-
tained.
</bodyText>
<subsectionHeader confidence="0.900129">
5.3 Features
</subsectionHeader>
<bodyText confidence="0.99975875">
Similar to Berant et al, we used three types of fea-
tures to describe both predicate pairs (φpij) and ar-
gument pairs (φaij): distributional similarities, lex-
ical resources, and string distances.
</bodyText>
<page confidence="0.99474">
91
</page>
<bodyText confidence="0.999965176470588">
We used the entire database of 68 million ex-
tracted propositions (see Section 4) to create a
word-context matrix; context was defined as other
words that appeared in the same proposition, and
each word was represented as (string, role), role
being the location within the proposition, either
a1, p, or a2. The matrix was then normalized with
pointwise mutual information (Church and Hanks,
1990). We used various metrics to measure dif-
ferent types of similarities between each compo-
nent pair, including: cosine similarity, Lin’s sim-
ilarity (1998), inclusion (Weeds and Weir, 2003),
average precision, and balanced average precision
(Kotlerman et al., 2010). Weed’s and Kotlerman’s
metrics are directional (asymmetric) and indicate
the direction of a potential entailment relation.
These features were used for both predicates and
arguments. In addition, we used Melamud et al’s
(2013) method to learn a context-sensitive model
of predicate entailment, which estimates predicate
similarity in the context of the given arguments.
We leveraged the Unified Medical Language
System (UMLS) to check argument entailment,
using the parent and synonym relations. A single
feature indicated whether such a connection ex-
ists. We also used WordNet relations as features,
specifically: synonyms, hypernyms, entailments,
hyponyms, cohyponyms, antonyms. Each Word-
Net relation constituted a different feature for both
predicates and arguments.
Finally, we added a string equality feature and a
Levenshtein distance feature (Levenshtein, 1966)
for different spellings of the same word to both
predicate and argument feature vectors.
</bodyText>
<sectionHeader confidence="0.99988" genericHeader="method">
6 Baseline Methods
</sectionHeader>
<bodyText confidence="0.997306071428571">
We consider four algorithms that naturally ex-
tend the state-of-the-art to propositions, while us-
ing distant supervision (from WordNet). Since
CEC uses direct supervision, we also examined
another (simpler) directly-supervised algorithm.
As a naive unsupervised baseline, we use Argu-
ment Equality, which returns “entailing” if the ar-
gument pair is identical. Predicate Equality is de-
fined similarly for predicates.
Component-Level Distant Supervision The
following methods use distant supervision from
WordNet (as in Berant et al’s work, Section 2.2.1)
to explicitly train component-level entailment esti-
mators. Specifically, we train a logistic regression
model for each component as specified in Equa-
tions 1 in Section 5.1. We present four methods,
which differ in the way they obtain global graph-
level entailment decisions for propositions, based
on the local component entailment estimates (pij,
aij in Section 5.1).
The first method, Opt(Arg n Pred), uses the
product of both component models to estimate lo-
cal proposition-level entailment: sij = pij · aij.
The global set of proposition entailments is then
determined using Berant et al’s global optimiza-
tion, according to the proposition-level scores sij.
Note that this method is identical to CEC dur-
ing inference, but differs in the way the local es-
timators are learned (with component-level super-
vision from WordNet).
An alternative is Opt(Arg) n Opt(Pred). It
first obtains local probabilities (pij, aij) for each
component as in Opt(Arg n Pred), but then em-
ploys component-level global optimization (tran-
sitivity enforcement), yielding two sets of entail-
ment decisions, xp ijand xaij. Proposition entail-
ment is then determined by the conjunction xs ij =
xp ijn xaij, as in (Adler et al., 2012).
Finally, Opt(Arg) ignores the predicate com-
ponent. Instead, it uses only the argument en-
tailment graph (as produced by Opt(Arg) n
Opt(Pred)) to decide on proposition entailment;
i.e. a pair of propositions entail if and only if their
arguments entail. Opt(Pred) is defined analo-
gously.
Proposition-Level Direct Supervision A sim-
pler alternative to CEC that also employs
proposition-level supervision is Joint Features,
which concatenates the component level features
into a unified feature vector: φsij = φpij ® φaij. We
then couple them with the gold-standard annota-
tions xs ij to create a training set for a single logistic
regression. We use the trained logistic regression
to estimate the local probability of proposition en-
tailment, and then perform global optimization to
construct the entailment graph.
</bodyText>
<sectionHeader confidence="0.992447" genericHeader="method">
7 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.99961125">
We evaluate the models in Sections 5 &amp; 6 on the
30 annotated entailment graphs presented in Sec-
tion 4. During testing, each graph was evaluated
separately. The results presented in this section
are all micro-averages, though macro-averages
were also computed and found to reflect the same
trends. Models trained with distant supervision
were evaluated on all graphs. For directly super-
</bodyText>
<page confidence="0.990808">
92
</page>
<bodyText confidence="0.999877142857143">
vised methods, we used 2 x 6-fold cross valida-
tion (25 training graphs per fold). In this scenario,
each graph induced a set of labeled examples –
its edges being positive examples, and the miss-
ing potential edges being negative ones – and the
union of these sets was used as the training set of
that cross-validation fold.
</bodyText>
<subsectionHeader confidence="0.515573">
7.1 Results
</subsectionHeader>
<bodyText confidence="0.999649027027027">
Table 1 compares the performance of CEC with
that of the baseline methods.
While Joint Features and CEC share exactly the
same features, CEC exploits the inherent conjunc-
tion between predicate and argument entailments
(as observed in Section 4 and modeled in Sec-
tion 5.1), and forces both components to decide on
entailment separately. This differs from the sim-
pler log-linear model (Joint Features) where, for
example, a very strong predicate entailment fea-
ture might override the overall proposition-level
decision, even if there was no strong indication
of argument entailment. As a result, CEC dom-
inates Joint Features in both precision and recall.
The F1 difference between these methods is sta-
tistically significant with McNemar’s test (1947)
with p « 0.01. Specifically, CEC corrected Joint
Features 7621 times, while the opposite occurred
only 4048 times.
CEC also yields relatively high precision
and recall. While it has 2% less recall than
Opt(Arg) (the highest-recall baseline), it sur-
passes Opt(Arg)’s precision by 14%. Along with
a similar comparison to Argument Equality (the
highest precision baseline), CEC notably outper-
forms all baselines.
It is also evident that both directly super-
vised methods outperform the distantly super-
vised methods. Our analysis (Section 8.1) shows
that WordNet lacks significant coverage, and may
therefore be a problematic source of supervision.
Perhaps the most surprising result is the com-
plete failure of WordNet-supervised methods that
consider predicate information. A deeper analy-
sis (Section 8.2) shows that predicate inference is
highly context-sensitive, and deviates beyond the
lexical inferences provided by WordNet.
</bodyText>
<subsectionHeader confidence="0.998971">
7.2 Learning Curve
</subsectionHeader>
<bodyText confidence="0.998285666666667">
We measure the supervision needed to train the di-
rectly supervised models by their learning curves
(Figure 2). Each point is the average F1 score
</bodyText>
<table confidence="0.999901357142857">
Supervision Method Prec. Rec. F1
None Argument 81.6% 42.2% 55.6%
Equality
Predicate 9.3% 1.5% 2.6%
Equality
Component Opt(Arg 73.8% 3.8% 7.2%
(WordNet) ∧ Pred)
Opt(Arg) ∧ 72.3% 3.2% 6.0%
Opt(Pred)
Opt(Arg) 64.6% 55.4% 59.7%
Opt(Pred) 11.0% 6.2% 8.0%
Proposition Joint 76.3% 51.7% 61.6%
(Annotated) Features
CEC 78.7% 53.5% 63.7%
</table>
<tableCaption confidence="0.999486">
Table 1: Performance on gold-standard (micro averaged).
</tableCaption>
<figureCaption confidence="0.980216">
Figure 2: Learning curve of directly supervised methods.
</figureCaption>
<bodyText confidence="0.998707857142857">
across 12 cross-validation folds; e.g. for 10 train-
ing graphs, we used 4 x 3-fold cross validation.
Even 5 training graphs (a day’s worth of annota-
tion) are enough for CEC to perform on-par with
the best distantly supervised method, and with 15
training graphs it outperforms every baseline, in-
cluding Joint Features trained with 25 graphs.
</bodyText>
<subsectionHeader confidence="0.999656">
7.3 Effects of Global Optimization
</subsectionHeader>
<bodyText confidence="0.999367214285714">
We evaluate the effects of enforcing transitivity by
considering CEC with and without the global op-
timization phase. Table 2 shows how many entail-
ment edges were added (and removed) by enforc-
ing transitivity, and measures how many of those
modifications were correct. Apparently, transi-
tivity’s greatest effect is the removal of incorrect
entailment edges. The same phenomenon was
also observed in the work on predicate entailment
graphs (Berant et al., 2012). Overall, transitivity
made 4,848 correct modifications out of 6,734 in
total. A x2 test reveals that the positive contribu-
tion of enforcing transitivity is indeed statistically
significant (p « 0.01).
</bodyText>
<page confidence="0.996678">
93
</page>
<table confidence="0.8298155">
Gold Global Opt Global Opt
Standard Added Edge Removed Edge
Edge Exists 1150 482
No Edge 1404 3698
</table>
<tableCaption confidence="0.984837">
Table 2: The modifications made by enforcing transitivity
w.r.t. the gold standard. 55% of the edges added by enforcing
transitivity are incorrect, but it removed even more incorrect
edges, improving the overall performance.
</tableCaption>
<subsectionHeader confidence="0.769527">
8 Analysis of Lexical Inference
</subsectionHeader>
<bodyText confidence="0.999975125">
Although CEC had a statistically-significant im-
provement upon the baselines, its absolute perfor-
mance leaves much room for improvement. We
hypothesize that the lexical entailment features we
used, following state-of-the-art lexical entailment
modeling, do not capture many of the actual lexi-
cal inferences induced by proposition entailment.
We demonstrate that this is indeed the case.
</bodyText>
<subsectionHeader confidence="0.997678">
8.1 Argument Entailment
</subsectionHeader>
<bodyText confidence="0.999863333333333">
To isolate the effect of different features on pre-
dicting argument entailment, we collected all
proposition pairs that shared exactly the same
predicate and topic, and thus differed in only their
“free” argument. This yielded 20,336 aligned ar-
gument pairs, whose entailment annotations are
equal to the corresponding proposition-entailment
annotation in the dataset.
Using WordNet synonyms and hypernyms to
predict entailment yielded a precision of about
88%, at 40% recall. Though relatively precise,
WordNet’s coverage is limited, and misses many
inferences. We describe three typical types of in-
ferences that were absent from WordNet.
The first type constitutes of widely-
used paraphrases such as peopleHpersons,
womanHfemale, and painHache. These may be
seen as weaker types of synonyms, which may
have nuances, but are typically interchangeable.
Another type is metonymy, in which a concept
is not referred to by its own name, but by that of
an associated concept. This is very common in
our healthcare dataset, where a disease is often re-
ferred to by its underlying pathogen and vice-versa
(e.g. pneumoniaHpneumococcus).
The third type of missing inferences is causal-
ity. Many instances of metonymy (such as the
disease-pathogen example) may be seen as causal-
ity as well. Other examples can be drug and ef-
fect (laxative—*diarrhea) or condition and symp-
tom (influenza—*fever).
WordNet’s lack of such common-sense infer-
ences, which are abundant in our proposition en-
tailment dataset, might make WordNet a problem-
atic source of distant supervision. The fact that
60% of the entailing examples in our dataset are
labeled by WordNet as non-entailing, means that
for each truly positive training example, there is a
higher chance that it will have a negative label.
Distributional similarity is commonly used to
capture such missing inferences and complement
WordNet-like resources. On this dataset, how-
ever, it failed to do so. One of the more in-
dicative similarity measures, inclusion (Weeds and
Weir, 2003), yielded only 27% precision at 40%
recall when tuning a threshold to optimize Fl. In-
creasing precision caused a dramatic drop in re-
call: 50% precision limited recall to 3.2%. Other
similarity measures performed similarly or worse.
It seems that current methods of distributional
word similarity also capture relations quite differ-
ent from inference, such as cohyponyms and do-
main relatedness, and might be less suitable for
modeling lexical entailment on their own.
</bodyText>
<subsectionHeader confidence="0.996079">
8.2 Context-Sensitive Predicate Entailment
</subsectionHeader>
<bodyText confidence="0.999966296296296">
The proposition-level entailment annotation in-
duces an entailment relation between the predi-
cates, which holds in the particular context of the
proposition pair. We wish to understand the na-
ture of this predicate-level entailment, and how it
compares to classic lexical inference as portrayed
in the lexical semantics literature. To that end, we
collected all the entailing proposition pairs with
equal arguments, and extracted the corresponding
predicate pairs (which, assuming alignment, are
necessarily entailing in that context). This list con-
tains 52,560 predicate pairs.
In our first analysis, we explored which Word-
Net relations correlate with predicate entailment,
by checking how well each relation covers the set
of entailed predicate pairs. Synonyms and hyper-
nyms, which are considered positive entailment
indicators, covered only about 8% each. Sur-
prisingly, the hyponym and cohyponym relations
(which are considered negative entailment indica-
tors) covered over 9% and 14%, respectively. Ta-
ble 3 shows the exact details.
It seems that WordNet relations are hardly cor-
related with the context-sensitive predicate-level
entailments in our dataset, and that the classic in-
terpretation of WordNet relations with respect to
entailment does not hold in practice, where en-
</bodyText>
<page confidence="0.999418">
94
</page>
<tableCaption confidence="0.91326875">
Table 3: The portion of positive predicate entailments cov-
ered by each WordNet relation. WordNet relations are di-
vided according to their common interpretations with respect
to lexical entailment.
</tableCaption>
<bodyText confidence="0.999955016129033">
tailments are judged in the context of concrete
propositions. In fact, negative indicators in Word-
Net seem to cover more predicate entailments
than positive ones. This explains the failure of
WordNet-supervised methods with predicate en-
tailment features (Section 7.1).
Since we do not expect WordNet to cover all
shades of entailment, we conducted a manual anal-
ysis as well. 100 entailing predicate pairs were
randomly sampled, and manually annotated for
lexical-level entailment, without seeing their argu-
ments. To compensate for the lack of context, we
guided the annotators to assume a general health-
care scenario, and use a more lenient interpretation
of textual entailment (biased towards positive en-
tailment decisions). Nevertheless, only 56% of the
predicate pairs were labeled as entailing, indicat-
ing that the context-sensitive predicate inferences
captured in our dataset can be quite different from
generic predicate inferences.
We suggest that this phenomenon goes one step
beyond what the current literature considers as
context-sensitive entailment, and that it is more
specific than determining an appropriate lexical
sense. To demonstrate, we present four such
predicate-entailment phenomena.
First, there are cases in which an appropriate
lexical sense could exist in principle, but it is too
specific to be practically covered by a manual re-
source. For example, cures cancer→kills cancer,
but the appropriate sense for kill (cause to cease
existing) does not exist, and in turn, neither does
the hypernymy relation from cure to kill. It is hard
to expect these kinds of obscure senses or relation-
ships to comprehensively appear in a manually-
constructed resource.
In many cases, such a specific sense does not
exist. For example, (pneumonia, require, antibi-
otic)→(pneumonia, treated by, antibiotics), but re-
quire does not have a general sense which means
treat by. The inference in this example does not
stem from the linguistic meaning of each predi-
cate, but rather from the real-world situation their
encapsulating propositions describe.
Another aspect of predicate entailment that
may change when considering propositional con-
text is the direction of inference. For instance,
cause trigger. While it may be the case that trig-
ger entails cause, the converse is not necessarily
true since cause is far more general. However,
when considering (caffeine, cause, headache) and
(caffeine, trigger, headache), both propositions de-
scribe the same real-world situation, and thus both
propositions are mutually entailing. In this con-
text, cause does indeed entail trigger as well.
Finally, figures of speech (such as metaphors)
are abundant and diverse. Though it may not be
so common to read about a drug that “banishes”
headaches, most readers would understand the un-
derlying meaning. These phenomena exceed the
current scope of lexical-semantic resources such
as WordNet, and require world knowledge.
</bodyText>
<sectionHeader confidence="0.994833" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999989529411765">
This paper proposes a novel approach, based on
entailment graphs, for consolidating information
extracted from large corpora. We define the prob-
lem of building proposition entailment graphs, and
provide a large annotated dataset. We also present
the CEC model, which models the connection be-
tween proposition entailment and lexical entail-
ment. Although it outperforms the state-of-the-
art, its performance is not ideal because it relies on
inadequate lexical-semantic resources that do not
capture the common-sense and context-sensitive
inferences which are inherent in proposition en-
tailment. In future work, we intend to further in-
vestigate lexical entailment as induced by proposi-
tion entailment, and hope to develop richer meth-
ods of lexical inference that address the phenom-
ena exhibited in this setting.
</bodyText>
<sectionHeader confidence="0.996225" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.984573857142857">
This work has been supported by the Israeli Min-
istry of Science and Technology grant 3-8705, the
Israel Science Foundation grant 880/12, and the
European Communitys Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 287923 (EXCITEMENT). We would like to
thank our reviewers for their insightful comments.
</bodyText>
<figure confidence="0.999071571428571">
7.85%
Synonyms
5.62%
Direct Hypernyms
Positive
3.14%
Indirect Hypernyms
0.33%
Entailment
0.31%
Antonyms
5.74%
Direct Hyponyms
Negative
3.51%
Indirect Hyponyms
14.30%
Cohyponyms
Interpretation
Coverage
WordNet Relation
</figure>
<page confidence="0.990825">
95
</page>
<sectionHeader confidence="0.982653" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999831581818182">
Meni Adler, Jonathan Berant, and Ido Dagan. 2012.
Entailment-based text exploration with application
to the health-care domain. In Proceedings of the
System Demonstrations of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2012), pages 79–84.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In IJ-
CAI, volume 7, pages 2670–2676.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73–111.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247–1250. ACM.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22–29.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzotto. 2013. Recognizing textual entail-
ment: Models and applications. Synthesis Lectures
on Human Language Technologies, 6(4):1–220.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), pages 1–
38.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S Weld. 2008. Open information extrac-
tion from the Web. Communications of the ACM,
51(12):68–74.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1535–1545, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of english books. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 1: Proceedings of the Main Conference and
the Shared Task: Semantic Textual Similarity, pages
241–247, Atlanta, Georgia, USA, June. Association
for Computational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359–389.
Vladimir I. Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet Physics Doklady, volume 10, page 707.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics, Volume 2, pages 768–774,
Montreal, Quebec, Canada, August. Association for
Computational Linguistics.
Mausam, Michael Schmitz, Stephen Soderland, Robert
Bart, and Oren Etzioni. 2012. Open language learn-
ing for information extraction. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 523–534, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12(2):153–157.
Yashar Mehdad, Giuseppe Carenini, Raymond T. Ng,
and Shafiq Joty. 2013. Towards topic labeling
with phrase entailment and aggregation. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
179–189, Atlanta, Georgia, June. Association for
Computational Linguistics.
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013. A two level
model for context sensitive inference rules. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1331–1340, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 74–84, Atlanta, Georgia, June. Association
for Computational Linguistics.
Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Com-
putational Linguistics (ACL-COLING 2006), pages
801–808.
Julie Weeds and David Weir. 2003. A general
framework for distributional similarity. In Michael
Collins and Mark Steedman, editors, Proceedings of
the 2003 Conference on Empirical Methods in Nat-
ural Language Processing, pages 81–88.
</reference>
<page confidence="0.972318">
96
</page>
<reference confidence="0.99877175">
Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel,
and Denilson Barbosa. 2013. Open information
extraction with tree kernels. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 868–877, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research, 34(1):255.
</reference>
<page confidence="0.999693">
97
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.188816">
<title confidence="0.844830666666667">Focused Entailment Graphs for Open IE Propositions Science Department of Bar-Ilan</title>
<author confidence="0.244873">Ramat-Gan</author>
<abstract confidence="0.993012473684211">Open IE methods extract structured propositions from text. However, these propositions are neither consolidated nor generalized, and querying them may lead to insufficient or redundant information. This work suggests an approach to organize open IE propositions using entailment graphs. The entailment relation unifies equivalent propositions and induces a specific-to-general structure. We create a large dataset of gold-standard proposition entailment graphs, and provide a novel algorithm for automatically constructing them. Our analysis shows that predicate entailment is extremely context-sensitive, and that current lexical-semantic resources do not capture many of the lexical inferences induced by proposition entailment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Meni Adler</author>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
</authors>
<title>Entailment-based text exploration with application to the health-care domain.</title>
<date>2012</date>
<booktitle>In Proceedings of the System Demonstrations of the 50th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>79--84</pages>
<contexts>
<context position="23587" citStr="Adler et al., 2012" startWordPosition="3663" endWordPosition="3666"> et al’s global optimization, according to the proposition-level scores sij. Note that this method is identical to CEC during inference, but differs in the way the local estimators are learned (with component-level supervision from WordNet). An alternative is Opt(Arg) n Opt(Pred). It first obtains local probabilities (pij, aij) for each component as in Opt(Arg n Pred), but then employs component-level global optimization (transitivity enforcement), yielding two sets of entailment decisions, xp ijand xaij. Proposition entailment is then determined by the conjunction xs ij = xp ijn xaij, as in (Adler et al., 2012). Finally, Opt(Arg) ignores the predicate component. Instead, it uses only the argument entailment graph (as produced by Opt(Arg) n Opt(Pred)) to decide on proposition entailment; i.e. a pair of propositions entail if and only if their arguments entail. Opt(Pred) is defined analogously. Proposition-Level Direct Supervision A simpler alternative to CEC that also employs proposition-level supervision is Joint Features, which concatenates the component level features into a unified feature vector: φsij = φpij ® φaij. We then couple them with the gold-standard annotations xs ij to create a trainin</context>
</contexts>
<marker>Adler, Berant, Dagan, 2012</marker>
<rawString>Meni Adler, Jonathan Berant, and Ido Dagan. 2012. Entailment-based text exploration with application to the health-care domain. In Proceedings of the System Demonstrations of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 79–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matthew Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web. In</title>
<date>2007</date>
<booktitle>IJCAI,</booktitle>
<volume>7</volume>
<pages>2670--2676</pages>
<contexts>
<context position="6170" citStr="Banko et al., 2007" startWordPosition="888" endWordPosition="891">lem is simpler than sentence-level entailment, it does capture entailment of complete statements, which proves to be quite challenging indeed. 2 Background Our work builds upon two major research threads: open IE, and entailment graphs. 2.1 Open Information Extraction Research in open IE (Etzioni et al., 2008) has focused on transforming text to predicate-argument tuples (propositions). The general approach is to learn proposition extraction patterns, and use them to create tuples while denoting extraction confidence. Various methods differ in the type of patterns they acquire. For instance, (Banko et al., 2007) and (Fader et al., 2011) used surface patterns, while (Mausam et al., 2012) and (Xu et al., 2013) used syntactic dependencies. Yates and Etzioni (2009) tried to mitigate the issue of language variability (as exemplified in the introduction) by clustering synonymous predicates and arguments. While these clusters do contain semantically related items, they do not necessarily reflect equivalence or implication. For example, coffee, tea, and caffeine may all appear in one cluster, but coffee does not imply tea; on the other hand, separating any element from this cluster removes a valid implicatio</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In IJCAI, volume 7, pages 2670–2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Learning entailment relations by global graph structure optimization.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="3561" citStr="Berant et al., 2012" startWordPosition="503" endWordPosition="507">ache, respond to, painkiller). We thus propose the task of constructing an entailment graph over a set of open IE propositions (Section 3), which is closely related to Berant et al’s work (2012) who introduced predicate entailment graphs. In contrast, our work explores propositions, which are essentially predicates instantiated with arguments, and thus semantically richer. We provide a dataset of 30 such graphs, which represent 1.5 million pairwise entailment decisions between propositions (Section 4). To approach this task, we extend the state-ofthe-art method for building entailment graphs (Berant et al., 2012) from predicates to complete propositions. Both Snow et al (2006) and Berant et al used WordNet as distant supervision when training a local pairwise model of lexical entailment. However, analyzing our data revealed that the lexical inferences captured in WordNet are quite dif87 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 87–97, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics Figure 1: An excerpt from a proposition entailment graph focused on the topic headache. The dashed boundaries in the figure denote cliques,</context>
<context position="7549" citStr="Berant et al (2012)" startWordPosition="1104" endWordPosition="1107">pen IE extractions by combining knowledge from Freebase and globally predicting which unobserved propositions are true. In contrast, our work identifies inference relations between concrete pairs of observed propositions. 2.2 Entailment Graphs of Words and Phrases Previous work focused on entailment graphs or similar structures at the sub-propositional level. In these graphs, each node represents a natural language word or phrase, and each directed edge an entailment (or generalization) relation. Snow et al (2006) created a taxonomy of sense88 disambiguated nouns and their hyponymy relations. Berant et al (2012) constructed entailment graphs of predicate templates. Recently, Mehdad et al (2013) built an entailment graph of noun phrases and partial sentences for topic labeling. The notion of proposition entailment graphs, however, is novel. This distinction is critical, because apparently, entailment in the context of specific propositions does not behave like contextoblivious lexical entailment (see Section 8). Berant et al’s work was implemented in Adler et al’s (2012) text exploration demo, which instantiated manually-annotated predicate entailment graphs with arguments, and used an additional lexi</context>
<context position="28263" citStr="Berant et al., 2012" startWordPosition="4391" endWordPosition="4394">upervised method, and with 15 training graphs it outperforms every baseline, including Joint Features trained with 25 graphs. 7.3 Effects of Global Optimization We evaluate the effects of enforcing transitivity by considering CEC with and without the global optimization phase. Table 2 shows how many entailment edges were added (and removed) by enforcing transitivity, and measures how many of those modifications were correct. Apparently, transitivity’s greatest effect is the removal of incorrect entailment edges. The same phenomenon was also observed in the work on predicate entailment graphs (Berant et al., 2012). Overall, transitivity made 4,848 correct modifications out of 6,734 in total. A x2 test reveals that the positive contribution of enforcing transitivity is indeed statistically significant (p « 0.01). 93 Gold Global Opt Global Opt Standard Added Edge Removed Edge Edge Exists 1150 482 No Edge 1404 3698 Table 2: The modifications made by enforcing transitivity w.r.t. the gold standard. 55% of the edges added by enforcing transitivity are incorrect, but it removed even more incorrect edges, improving the overall performance. 8 Analysis of Lexical Inference Although CEC had a statistically-signi</context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2012</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2012. Learning entailment relations by global graph structure optimization. Computational Linguistics, 38(1):73–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,</booktitle>
<pages>1247--1250</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1517" citStr="Bollacker et al., 2008" startWordPosition="201" endWordPosition="204">ot capture many of the lexical inferences induced by proposition entailment. 1 Introduction Open information extraction (open IE) extracts natural language propositions from text without pre-defined schemas as in supervised relation extraction (Etzioni et al., 2008). These propositions represent predicate-argument structures as tuples of natural language strings. Open IE enables knowledge search by aggregating billions of propositions from the web1. It may also be perceived as capturing an unsupervised knowledge representation schema, complementing supervised knowledge bases such as Freebase (Bollacker et al., 2008), as suggested by Riedel et al (2013). However, language variability obstructs open IE from becoming a viable knowledge representation framework. As it does not consolidate natural language expressions, querying a database of open IE propositions may lead to either insufficient or redundant information. As an illustrative example, 1See demo: openie.cs.washington.edu querying the demo (footnote 1) for the generally equivalent relieves headache or treats headache returns two different lists of entities; out of the top few results, the only answers these queries seem to agree on are caffeine and </context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247–1250. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="20593" citStr="Church and Hanks, 1990" startWordPosition="3216" endWordPosition="3219">ciently obtained. 5.3 Features Similar to Berant et al, we used three types of features to describe both predicate pairs (φpij) and argument pairs (φaij): distributional similarities, lexical resources, and string distances. 91 We used the entire database of 68 million extracted propositions (see Section 4) to create a word-context matrix; context was defined as other words that appeared in the same proposition, and each word was represented as (string, role), role being the location within the proposition, either a1, p, or a2. The matrix was then normalized with pointwise mutual information (Church and Hanks, 1990). We used various metrics to measure different types of similarities between each component pair, including: cosine similarity, Lin’s similarity (1998), inclusion (Weeds and Weir, 2003), average precision, and balanced average precision (Kotlerman et al., 2010). Weed’s and Kotlerman’s metrics are directional (asymmetric) and indicate the direction of a potential entailment relation. These features were used for both predicates and arguments. In addition, we used Melamud et al’s (2013) method to learn a context-sensitive model of predicate entailment, which estimates predicate similarity in the</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Dan Roth</author>
<author>Mark Sammons</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Recognizing textual entailment: Models and applications. Synthesis Lectures on Human Language Technologies,</title>
<date>2013</date>
<pages>6--4</pages>
<contexts>
<context position="2637" citStr="Dagan et al., 2013" startWordPosition="371" endWordPosition="374">ties; out of the top few results, the only answers these queries seem to agree on are caffeine and sex. This is a major drawback relative to supervised knowledge representations, which map natural language expressions to structured formal representations, such as treatments in Freebase. In this work, we investigate an approach for organizing and consolidating open IE propositions using the novel notion of proposition entailment graphs (see Figure 1) – graphs in which each node represents a proposition and each directed edge reflects an entailment relation, in the spirit of textual entailment (Dagan et al., 2013). Entailment provides an effective structure for aggregating natural-language based information; it merges semantically equivalent propositions into cliques, and induces specification-generalization edges between them. For example, (aspirin, eliminate, headache) entails, and is more specific than, (headache, respond to, painkiller). We thus propose the task of constructing an entailment graph over a set of open IE propositions (Section 3), which is closely related to Berant et al’s work (2012) who introduced predicate entailment graphs. In contrast, our work explores propositions, which are es</context>
<context position="10660" citStr="Dagan et al., 2013" startWordPosition="1606" endWordPosition="1609">l assignment that maximizes the objective function under transitivity constraints, expressed as linear constraints Vi,j,k xij + xjk − xik G 1. 3 Task Definition A proposition entailment graph is a directed graph where each node is a proposition si (s for sentence) and each edge (si, sj) represents an entailment relation from si to sj. A proposition si is a predicate-argument structure si = (pi, a1i , a2i , ..., ami � with one predicate pi and its i arguments. A proposition-level entailment (si, sj) holds if the verbalization of si implies sj, according to the definition of textual entailment (Dagan et al., 2013); i.e. if humans reading si would typically infer that sj is most likely true. Given a set of propositions (graph nodes), the task of constructing a proposition entailment graph is to recognize all the entailments among the propositions, i.e. deciding which directional edges connect which pairs of nodes. In this paper, we consider the narrower task of constructing focused proposition entailment graphs, following Berant et al’s methodology in creating focused predicate entailment graphs. First, all predicates are binary (have two arguments) and are denoted si = (a1i, pi, a2). Seci ondly, we ass</context>
</contexts>
<marker>Dagan, Roth, Sammons, Zanzotto, 2013</marker>
<rawString>Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing textual entailment: Models and applications. Synthesis Lectures on Human Language Technologies, 6(4):1–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<pages>1--38</pages>
<contexts>
<context position="18116" citStr="Dempster et al., 1977" startWordPosition="2796" endWordPosition="2800">j · aij The proposition entailment probability is thus the product of component entailment probabilities. { } Given the proposition-level information xs , ij the log-likelihood is: `(wp wa)=E i j log P(xsij φipj,φaij; wp, wa)= E i#j (xsi log (pijaij) + (1 − x j) log (1 − pijaij)) 5.2 Learning Component Models We wish to learn the model’s parameters (wp, wa). Our approach uses direct proposition-level supervision from our annotated dataset to train the component logistic regression models. Since component entailment (xpij, xaij) is not observed in the data, we apply the iterative EM algorithm (Dempster et al., 1977). In the E-step we estimate their probabilities from proposition-level labels (xsij), and in the M-step we use those estimates as “soft” labels to learn the component-level model parameters (wp, wa). E-Step During the E-step in iteration t + 1, we compute the probability of component entailments given the proposition entailment information, based on the parameters at iteration t (wpt , wat ). The predicate probabilities w�oore given by: cp ij = P (xp ij = 1 ij ij;wpt ,wt) (2) and are computed with Bayes’ law: � 1 if xsij = 1 �cpij = � pt ij(1−at ij) (3) if xs ij = 0 1−pt ijat ij where ptij is </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society. Series B (Methodological), pages 1– 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michele Banko</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction from the Web.</title>
<date>2008</date>
<journal>Communications of the ACM,</journal>
<volume>51</volume>
<issue>12</issue>
<contexts>
<context position="1160" citStr="Etzioni et al., 2008" startWordPosition="151" endWordPosition="154">nt relation unifies equivalent propositions and induces a specific-to-general structure. We create a large dataset of gold-standard proposition entailment graphs, and provide a novel algorithm for automatically constructing them. Our analysis shows that predicate entailment is extremely context-sensitive, and that current lexical-semantic resources do not capture many of the lexical inferences induced by proposition entailment. 1 Introduction Open information extraction (open IE) extracts natural language propositions from text without pre-defined schemas as in supervised relation extraction (Etzioni et al., 2008). These propositions represent predicate-argument structures as tuples of natural language strings. Open IE enables knowledge search by aggregating billions of propositions from the web1. It may also be perceived as capturing an unsupervised knowledge representation schema, complementing supervised knowledge bases such as Freebase (Bollacker et al., 2008), as suggested by Riedel et al (2013). However, language variability obstructs open IE from becoming a viable knowledge representation framework. As it does not consolidate natural language expressions, querying a database of open IE propositi</context>
<context position="5862" citStr="Etzioni et al., 2008" startWordPosition="841" endWordPosition="844">fic world knowledge, and warrant future research. Our work also contributes to textual entailment research. First, we extend entailment graphs to complete propositions. Secondly, we investigate an intermediate problem of recognizing entailment between language-based predicate-argument tuples. Though this problem is simpler than sentence-level entailment, it does capture entailment of complete statements, which proves to be quite challenging indeed. 2 Background Our work builds upon two major research threads: open IE, and entailment graphs. 2.1 Open Information Extraction Research in open IE (Etzioni et al., 2008) has focused on transforming text to predicate-argument tuples (propositions). The general approach is to learn proposition extraction patterns, and use them to create tuples while denoting extraction confidence. Various methods differ in the type of patterns they acquire. For instance, (Banko et al., 2007) and (Fader et al., 2011) used surface patterns, while (Mausam et al., 2012) and (Xu et al., 2013) used syntactic dependencies. Yates and Etzioni (2009) tried to mitigate the issue of language variability (as exemplified in the introduction) by clustering synonymous predicates and arguments.</context>
</contexts>
<marker>Etzioni, Banko, Soderland, Weld, 2008</marker>
<rawString>Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extraction from the Web. Communications of the ACM, 51(12):68–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1535--1545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="6195" citStr="Fader et al., 2011" startWordPosition="893" endWordPosition="896">nce-level entailment, it does capture entailment of complete statements, which proves to be quite challenging indeed. 2 Background Our work builds upon two major research threads: open IE, and entailment graphs. 2.1 Open Information Extraction Research in open IE (Etzioni et al., 2008) has focused on transforming text to predicate-argument tuples (propositions). The general approach is to learn proposition extraction patterns, and use them to create tuples while denoting extraction confidence. Various methods differ in the type of patterns they acquire. For instance, (Banko et al., 2007) and (Fader et al., 2011) used surface patterns, while (Mausam et al., 2012) and (Xu et al., 2013) used syntactic dependencies. Yates and Etzioni (2009) tried to mitigate the issue of language variability (as exemplified in the introduction) by clustering synonymous predicates and arguments. While these clusters do contain semantically related items, they do not necessarily reflect equivalence or implication. For example, coffee, tea, and caffeine may all appear in one cluster, but coffee does not imply tea; on the other hand, separating any element from this cluster removes a valid implication. Entailment, however, c</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1535–1545, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Jon Orwant</author>
</authors>
<title>A dataset of syntactic-ngrams over time from a very large corpus of english books.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity,</booktitle>
<pages>241--247</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="11912" citStr="Goldberg and Orwant, 2013" startWordPosition="1803" endWordPosition="1806">were retrieved by querying for a particular concept; out of the two arguments, one argument t (topic) is common to all the propositions in a single graph. We denote the non-topic argument as ai. Figure 1 presents an example of an informative entailment graph focused on the topic headache. Though confined, this setting still challenges the state-of-the-art in textual entailment (see Section 7). Moreover, these restrictions facilitate piece-wise investigation of the entailment problem (see Section 8). 4 Dataset To construct our dataset of open IE extractions, we found Google’s syntactic ngrams (Goldberg and Orwant, 2013) as a useful source of high-quality propositions. Based on a corpus of 3.5 million English books, it aggregates every syntactic ngram 89 – subtree of a dependency parse – with at most 4 dependency arcs. The resource contains only tree fragments that appeared at least 10 times in the corpus, filtering out many low-quality syntactic ngrams. We extracted the syntactic ngrams that reflect propositions, i.e. subject-verb-object fragments where object modifies the verb with either dobj or pobj. Prepositions in pobj were concatenated to the verb (e.g. use with). In addition, both subject and object m</context>
</contexts>
<marker>Goldberg, Orwant, 2013</marker>
<rawString>Yoav Goldberg and Jon Orwant. 2013. A dataset of syntactic-ngrams over time from a very large corpus of english books. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 241–247, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="20854" citStr="Kotlerman et al., 2010" startWordPosition="3254" endWordPosition="3257"> million extracted propositions (see Section 4) to create a word-context matrix; context was defined as other words that appeared in the same proposition, and each word was represented as (string, role), role being the location within the proposition, either a1, p, or a2. The matrix was then normalized with pointwise mutual information (Church and Hanks, 1990). We used various metrics to measure different types of similarities between each component pair, including: cosine similarity, Lin’s similarity (1998), inclusion (Weeds and Weir, 2003), average precision, and balanced average precision (Kotlerman et al., 2010). Weed’s and Kotlerman’s metrics are directional (asymmetric) and indicate the direction of a potential entailment relation. These features were used for both predicates and arguments. In addition, we used Melamud et al’s (2013) method to learn a context-sensitive model of predicate entailment, which estimates predicate similarity in the context of the given arguments. We leveraged the Unified Medical Language System (UMLS) to check argument entailment, using the parent and synonym relations. A single feature indicated whether such a connection exists. We also used WordNet relations as feature</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(4):359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<booktitle>In Soviet Physics Doklady,</booktitle>
<volume>10</volume>
<pages>707</pages>
<contexts>
<context position="21724" citStr="Levenshtein, 1966" startWordPosition="3378" endWordPosition="3379">xt-sensitive model of predicate entailment, which estimates predicate similarity in the context of the given arguments. We leveraged the Unified Medical Language System (UMLS) to check argument entailment, using the parent and synonym relations. A single feature indicated whether such a connection exists. We also used WordNet relations as features, specifically: synonyms, hypernyms, entailments, hyponyms, cohyponyms, antonyms. Each WordNet relation constituted a different feature for both predicates and arguments. Finally, we added a string equality feature and a Levenshtein distance feature (Levenshtein, 1966) for different spellings of the same word to both predicate and argument feature vectors. 6 Baseline Methods We consider four algorithms that naturally extend the state-of-the-art to propositions, while using distant supervision (from WordNet). Since CEC uses direct supervision, we also examined another (simpler) directly-supervised algorithm. As a naive unsupervised baseline, we use Argument Equality, which returns “entailing” if the argument pair is identical. Predicate Equality is defined similarly for predicates. Component-Level Distant Supervision The following methods use distant supervi</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. In Soviet Physics Doklady, volume 10, page 707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2,</booktitle>
<pages>768--774</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montreal, Quebec, Canada,</location>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2, pages 768–774, Montreal, Quebec, Canada, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Schmitz Mausam</author>
<author>Stephen Soderland</author>
<author>Robert Bart</author>
<author>Oren Etzioni</author>
</authors>
<title>Open language learning for information extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>523--534</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="6246" citStr="Mausam et al., 2012" startWordPosition="902" endWordPosition="905">f complete statements, which proves to be quite challenging indeed. 2 Background Our work builds upon two major research threads: open IE, and entailment graphs. 2.1 Open Information Extraction Research in open IE (Etzioni et al., 2008) has focused on transforming text to predicate-argument tuples (propositions). The general approach is to learn proposition extraction patterns, and use them to create tuples while denoting extraction confidence. Various methods differ in the type of patterns they acquire. For instance, (Banko et al., 2007) and (Fader et al., 2011) used surface patterns, while (Mausam et al., 2012) and (Xu et al., 2013) used syntactic dependencies. Yates and Etzioni (2009) tried to mitigate the issue of language variability (as exemplified in the introduction) by clustering synonymous predicates and arguments. While these clusters do contain semantically related items, they do not necessarily reflect equivalence or implication. For example, coffee, tea, and caffeine may all appear in one cluster, but coffee does not imply tea; on the other hand, separating any element from this cluster removes a valid implication. Entailment, however, can capture the fact that both beverages imply caffe</context>
</contexts>
<marker>Mausam, Soderland, Bart, Etzioni, 2012</marker>
<rawString>Mausam, Michael Schmitz, Stephen Soderland, Robert Bart, and Oren Etzioni. 2012. Open language learning for information extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 523–534, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quinn McNemar</author>
</authors>
<title>Note on the sampling error of the difference between correlated proportions or percentages.</title>
<date>1947</date>
<journal>Psychometrika,</journal>
<volume>12</volume>
<issue>2</issue>
<marker>McNemar, 1947</marker>
<rawString>Quinn McNemar. 1947. Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika, 12(2):153–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
<author>Shafiq Joty</author>
</authors>
<title>Towards topic labeling with phrase entailment and aggregation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>179--189</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="7633" citStr="Mehdad et al (2013)" startWordPosition="1115" endWordPosition="1118">h unobserved propositions are true. In contrast, our work identifies inference relations between concrete pairs of observed propositions. 2.2 Entailment Graphs of Words and Phrases Previous work focused on entailment graphs or similar structures at the sub-propositional level. In these graphs, each node represents a natural language word or phrase, and each directed edge an entailment (or generalization) relation. Snow et al (2006) created a taxonomy of sense88 disambiguated nouns and their hyponymy relations. Berant et al (2012) constructed entailment graphs of predicate templates. Recently, Mehdad et al (2013) built an entailment graph of noun phrases and partial sentences for topic labeling. The notion of proposition entailment graphs, however, is novel. This distinction is critical, because apparently, entailment in the context of specific propositions does not behave like contextoblivious lexical entailment (see Section 8). Berant et al’s work was implemented in Adler et al’s (2012) text exploration demo, which instantiated manually-annotated predicate entailment graphs with arguments, and used an additional lexical resource to determine argument entailment. The combined graphs of predicate and </context>
</contexts>
<marker>Mehdad, Carenini, Ng, Joty, 2013</marker>
<rawString>Yashar Mehdad, Giuseppe Carenini, Raymond T. Ng, and Shafiq Joty. 2013. Towards topic labeling with phrase entailment and aggregation. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 179–189, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Melamud</author>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
<author>Idan Szpektor</author>
</authors>
<title>A two level model for context sensitive inference rules.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1331--1340</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Melamud, Berant, Dagan, Goldberger, Szpektor, 2013</marker>
<rawString>Oren Melamud, Jonathan Berant, Ido Dagan, Jacob Goldberger, and Idan Szpektor. 2013. A two level model for context sensitive inference rules. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1331–1340, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
<author>Benjamin M Marlin</author>
</authors>
<title>Relation extraction with matrix factorization and universal schemas.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>74--84</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="1554" citStr="Riedel et al (2013)" startWordPosition="208" endWordPosition="211"> induced by proposition entailment. 1 Introduction Open information extraction (open IE) extracts natural language propositions from text without pre-defined schemas as in supervised relation extraction (Etzioni et al., 2008). These propositions represent predicate-argument structures as tuples of natural language strings. Open IE enables knowledge search by aggregating billions of propositions from the web1. It may also be perceived as capturing an unsupervised knowledge representation schema, complementing supervised knowledge bases such as Freebase (Bollacker et al., 2008), as suggested by Riedel et al (2013). However, language variability obstructs open IE from becoming a viable knowledge representation framework. As it does not consolidate natural language expressions, querying a database of open IE propositions may lead to either insufficient or redundant information. As an illustrative example, 1See demo: openie.cs.washington.edu querying the demo (footnote 1) for the generally equivalent relieves headache or treats headache returns two different lists of entities; out of the top few results, the only answers these queries seem to agree on are caffeine and sex. This is a major drawback relativ</context>
<context position="6905" citStr="Riedel et al (2013)" startWordPosition="1006" endWordPosition="1009">pendencies. Yates and Etzioni (2009) tried to mitigate the issue of language variability (as exemplified in the introduction) by clustering synonymous predicates and arguments. While these clusters do contain semantically related items, they do not necessarily reflect equivalence or implication. For example, coffee, tea, and caffeine may all appear in one cluster, but coffee does not imply tea; on the other hand, separating any element from this cluster removes a valid implication. Entailment, however, can capture the fact that both beverages imply caffeine, but not one another. Also related, Riedel et al (2013) try to generalize over open IE extractions by combining knowledge from Freebase and globally predicting which unobserved propositions are true. In contrast, our work identifies inference relations between concrete pairs of observed propositions. 2.2 Entailment Graphs of Words and Phrases Previous work focused on entailment graphs or similar structures at the sub-propositional level. In these graphs, each node represents a natural language word or phrase, and each directed edge an entailment (or generalization) relation. Snow et al (2006) created a taxonomy of sense88 disambiguated nouns and t</context>
</contexts>
<marker>Riedel, Yao, McCallum, Marlin, 2013</marker>
<rawString>Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M. Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 74–84, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (ACL-COLING</booktitle>
<pages>801--808</pages>
<contexts>
<context position="3626" citStr="Snow et al (2006)" startWordPosition="514" endWordPosition="517">ing an entailment graph over a set of open IE propositions (Section 3), which is closely related to Berant et al’s work (2012) who introduced predicate entailment graphs. In contrast, our work explores propositions, which are essentially predicates instantiated with arguments, and thus semantically richer. We provide a dataset of 30 such graphs, which represent 1.5 million pairwise entailment decisions between propositions (Section 4). To approach this task, we extend the state-ofthe-art method for building entailment graphs (Berant et al., 2012) from predicates to complete propositions. Both Snow et al (2006) and Berant et al used WordNet as distant supervision when training a local pairwise model of lexical entailment. However, analyzing our data revealed that the lexical inferences captured in WordNet are quite dif87 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 87–97, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics Figure 1: An excerpt from a proposition entailment graph focused on the topic headache. The dashed boundaries in the figure denote cliques, meaning that all propositions within them are equivalent. ferent</context>
<context position="7449" citStr="Snow et al (2006)" startWordPosition="1087" endWordPosition="1090">es imply caffeine, but not one another. Also related, Riedel et al (2013) try to generalize over open IE extractions by combining knowledge from Freebase and globally predicting which unobserved propositions are true. In contrast, our work identifies inference relations between concrete pairs of observed propositions. 2.2 Entailment Graphs of Words and Phrases Previous work focused on entailment graphs or similar structures at the sub-propositional level. In these graphs, each node represents a natural language word or phrase, and each directed edge an entailment (or generalization) relation. Snow et al (2006) created a taxonomy of sense88 disambiguated nouns and their hyponymy relations. Berant et al (2012) constructed entailment graphs of predicate templates. Recently, Mehdad et al (2013) built an entailment graph of noun phrases and partial sentences for topic labeling. The notion of proposition entailment graphs, however, is novel. This distinction is critical, because apparently, entailment in the context of specific propositions does not behave like contextoblivious lexical entailment (see Section 8). Berant et al’s work was implemented in Adler et al’s (2012) text exploration demo, which ins</context>
<context position="15563" citStr="Snow et al (2006)" startWordPosition="2376" endWordPosition="2379"> graphs of propositions. As described in Section 2.2.1, their method first performs local estimation of predicate entailment and then global optimization. We modify the local estimation phase to estimate proposition entailment instead, and then apply the same global optimization in the second phase. In Section 4, we observed the alignment-based relationship between proposition and lexical entailment. We leverage this observation to predict proposition entailment with lexical entailment features (as Berant et al), using the Component Entailment Conjunction (CEC) model in Section 5.1. Following Snow et al (2006) and Berant et al, we could train CEC using distant supervision from WordNet. In fact, we did try this approach (presented as baseline methods, Section 6) and found that it performed poorly. Furthermore, our analysis (Section 8) suggests that WordNet relations do not adequately capture the lexical inferences induced by proposition-level entailment. Instead, we use a more realistic signal to train CEC – direct supervision from the annotated dataset. Section 5.2 describes how we propagate propositionlevel entailment annotations to the latent lexical components. 5.1 Component Entailment Conjuncti</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (ACL-COLING 2006), pages 801–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>A general framework for distributional similarity.</title>
<date>2003</date>
<booktitle>Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>81--88</pages>
<editor>In Michael Collins and Mark Steedman, editors,</editor>
<contexts>
<context position="20778" citStr="Weeds and Weir, 2003" startWordPosition="3244" endWordPosition="3247">ical resources, and string distances. 91 We used the entire database of 68 million extracted propositions (see Section 4) to create a word-context matrix; context was defined as other words that appeared in the same proposition, and each word was represented as (string, role), role being the location within the proposition, either a1, p, or a2. The matrix was then normalized with pointwise mutual information (Church and Hanks, 1990). We used various metrics to measure different types of similarities between each component pair, including: cosine similarity, Lin’s similarity (1998), inclusion (Weeds and Weir, 2003), average precision, and balanced average precision (Kotlerman et al., 2010). Weed’s and Kotlerman’s metrics are directional (asymmetric) and indicate the direction of a potential entailment relation. These features were used for both predicates and arguments. In addition, we used Melamud et al’s (2013) method to learn a context-sensitive model of predicate entailment, which estimates predicate similarity in the context of the given arguments. We leveraged the Unified Medical Language System (UMLS) to check argument entailment, using the parent and synonym relations. A single feature indicated</context>
<context position="31243" citStr="Weeds and Weir, 2003" startWordPosition="4849" endWordPosition="4852">t’s lack of such common-sense inferences, which are abundant in our proposition entailment dataset, might make WordNet a problematic source of distant supervision. The fact that 60% of the entailing examples in our dataset are labeled by WordNet as non-entailing, means that for each truly positive training example, there is a higher chance that it will have a negative label. Distributional similarity is commonly used to capture such missing inferences and complement WordNet-like resources. On this dataset, however, it failed to do so. One of the more indicative similarity measures, inclusion (Weeds and Weir, 2003), yielded only 27% precision at 40% recall when tuning a threshold to optimize Fl. Increasing precision caused a dramatic drop in recall: 50% precision limited recall to 3.2%. Other similarity measures performed similarly or worse. It seems that current methods of distributional word similarity also capture relations quite different from inference, such as cohyponyms and domain relatedness, and might be less suitable for modeling lexical entailment on their own. 8.2 Context-Sensitive Predicate Entailment The proposition-level entailment annotation induces an entailment relation between the pre</context>
</contexts>
<marker>Weeds, Weir, 2003</marker>
<rawString>Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Michael Collins and Mark Steedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Xu</author>
<author>Mi-Young Kim</author>
<author>Kevin Quinn</author>
<author>Randy Goebel</author>
<author>Denilson Barbosa</author>
</authors>
<title>Open information extraction with tree kernels.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>868--877</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="6268" citStr="Xu et al., 2013" startWordPosition="907" endWordPosition="910">ch proves to be quite challenging indeed. 2 Background Our work builds upon two major research threads: open IE, and entailment graphs. 2.1 Open Information Extraction Research in open IE (Etzioni et al., 2008) has focused on transforming text to predicate-argument tuples (propositions). The general approach is to learn proposition extraction patterns, and use them to create tuples while denoting extraction confidence. Various methods differ in the type of patterns they acquire. For instance, (Banko et al., 2007) and (Fader et al., 2011) used surface patterns, while (Mausam et al., 2012) and (Xu et al., 2013) used syntactic dependencies. Yates and Etzioni (2009) tried to mitigate the issue of language variability (as exemplified in the introduction) by clustering synonymous predicates and arguments. While these clusters do contain semantically related items, they do not necessarily reflect equivalence or implication. For example, coffee, tea, and caffeine may all appear in one cluster, but coffee does not imply tea; on the other hand, separating any element from this cluster removes a valid implication. Entailment, however, can capture the fact that both beverages imply caffeine, but not one anoth</context>
</contexts>
<marker>Xu, Kim, Quinn, Goebel, Barbosa, 2013</marker>
<rawString>Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel, and Denilson Barbosa. 2013. Open information extraction with tree kernels. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 868–877, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Oren Etzioni</author>
</authors>
<title>Unsupervised methods for determining object and relation synonyms on the web.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="6322" citStr="Yates and Etzioni (2009)" startWordPosition="914" endWordPosition="917">ckground Our work builds upon two major research threads: open IE, and entailment graphs. 2.1 Open Information Extraction Research in open IE (Etzioni et al., 2008) has focused on transforming text to predicate-argument tuples (propositions). The general approach is to learn proposition extraction patterns, and use them to create tuples while denoting extraction confidence. Various methods differ in the type of patterns they acquire. For instance, (Banko et al., 2007) and (Fader et al., 2011) used surface patterns, while (Mausam et al., 2012) and (Xu et al., 2013) used syntactic dependencies. Yates and Etzioni (2009) tried to mitigate the issue of language variability (as exemplified in the introduction) by clustering synonymous predicates and arguments. While these clusters do contain semantically related items, they do not necessarily reflect equivalence or implication. For example, coffee, tea, and caffeine may all appear in one cluster, but coffee does not imply tea; on the other hand, separating any element from this cluster removes a valid implication. Entailment, however, can capture the fact that both beverages imply caffeine, but not one another. Also related, Riedel et al (2013) try to generaliz</context>
</contexts>
<marker>Yates, Etzioni, 2009</marker>
<rawString>Alexander Yates and Oren Etzioni. 2009. Unsupervised methods for determining object and relation synonyms on the web. Journal of Artificial Intelligence Research, 34(1):255.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
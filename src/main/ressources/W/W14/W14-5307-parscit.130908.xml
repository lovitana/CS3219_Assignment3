<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020867">
<title confidence="0.998894">
A Report on the DSL Shared Task 2014
</title>
<author confidence="0.99913">
Marcos Zampieri1, Liling Tan2, Nikola Ljubeˇsi´c3, J¨org Tiedemann4
</author>
<affiliation confidence="0.844071">
Saarland University, Germany1,2
University of Zagreb, Croatia3
Uppsala University, Sweden4
marcos.zampieri@uni-saarland.de, liling.tan@uni-saarland.de
</affiliation>
<email confidence="0.982391">
jorg.tiedemann@lingfil.uu.se, nljubesi@ffzg.hr
</email>
<sectionHeader confidence="0.993324" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99990475">
This paper summarizes the methods, results and findings of the Discriminating between Similar
Languages (DSL) shared task 2014. The shared task provided data from 13 different languages
and varieties divided into 6 groups. Participants were required to train their systems to discrimi-
nate between languages on a training and development set containing 20,000 sentences from each
language (closed submission) and/or any other dataset (open submission). One month later, a test
set containing 1,000 unidentified instances per language was released for evaluation. The DSL
shared task received 22 inscriptions and 8 final submissions. The best system obtained 95.7%
average accuracy.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99985332">
Discriminating between similar languages is one of the bottlenecks of state-of-the-art language iden-
tification systems. Although in recent years systems have been trained to discriminate between more
languages1, they still struggle to discriminate between similar languages such as Croatian and Serbian or
Malay and Indonesian.
From an NLP point of view, the difficulty systems face when discriminating between closely related
languages is similar to the problem of discriminating between standard national language varieties (e.g.
American English and British English or Brazilian Portuguese and European Portuguese), henceforth
varieties. Recent studies show that language varieties can be discriminated automatically using words or
characters as features (Zampieri and Gebre, 2012; Lui and Cook, 2013) . However, due to performance
limitations, state-of-the-art general-purpose language identification systems do not distinguish texts from
different national varieties, modelling pluricentric languages as unique classes.
To evaluate how state-of-the-art systems perform in identifying similar languages and varieties, we
decided to organize the Discriminating between Similar Languages (DSL)2 shared task. This shared task
was organized within the scope of the workshop on Applying NLP Tools to Similar Languages, Varieties
and Dialects (VarDial) in the 2014 edition of COLING.
The motivation behind the DSL shared task is two-fold. Firstly, we have observed an increase of
interest in the topic. This is reflected by a number of papers that have been published about this task in
recent years starting with Ranaivo-Malanc¸on (2006) for Malay and Indonesian and Ljubeˇsi´c et al. (2007)
for South Slavic languages. In the DSL shared task we tried to include (depending on the availability of
data) languages that have been studied in previous experiments, such as Croatian, English, Indonesian,
Malay, Portuguese and Spanish.
The second aspect that motivated us to organize this shared task is that, to our knowledge, no shared
task focusing on the discrimination of similar languages has been organized previously. The most sim-
ilar shared tasks to DSL are the DEFT 2010 shared task (Grouin et al., 2010), in which systems were
required to classify French journalistic texts with respect to their geographical location as well as the
</bodyText>
<footnote confidence="0.882345">
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1Brown (2013) reports results on a system trained to recognize more than 1,100 languages
2http://corporavm.uni-koeln.de/vardial/sharedtask.html
</footnote>
<page confidence="0.976586">
58
</page>
<note confidence="0.9854105">
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 58–67,
Dublin, Ireland, August 23 2014.
</note>
<bodyText confidence="0.997591333333333">
decade in which they were published. Other related shared tasks include the ALTW 2010 multilingual
language identification shared task, a general-purpose language identification task containing data from
74 languages (Baldwin and Lui, 2010) and finally the Native Language Identification (NLI) shared task
(Tetreault et al., 2013) where participants were provided English essays written by foreign students of 11
different mother tongues (Blanchard et al., 2013). Participants had to train their systems to identify the
native language of the writer of each text.
</bodyText>
<sectionHeader confidence="0.999297" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999978806451613">
Among the first studies to investigate the question of discriminating between similar languages is the
study published by Ranaivo-Malanc¸on (2006). The author presents a semi-supervised model to dis-
tinguish between Indonesian and Malay, two closely related languages from the Austronesian family
represented in the DSL shared task. The study uses the frequency and rank of character trigrams derived
from the most frequent words in each language, lists of exclusive words, and the format of numbers
(Malay uses decimal points whereas Indonesian uses commas). The author compares the performance
of this method with the performance obtained by TextCat (Cavnar and Trenkle, 1994).
Ljubeˇsi´c et al. (2007) proposed a computational model for the identification of Croatian texts in
comparison to Slovene and Serbian, reporting 99% recall and precision in three processing stages. The
approach includes a ‘black list’, which increases the performance of the algorithm. Tiedemann and
Ljubeˇsi´c (2012) improved this method and applied it to Bosnian, Croatian and Serbian texts. The study
reports significantly higher performance compared to general purpose language identification methods.
The methods applied to discriminate between texts from different language varieties and dialects are
similar to those applied to similar languages3. One of the methods proposed to identify language varieties
is by Huang and Lee (2008). This study presented a bag-of-words approach to classify Chinese texts from
the mainland and Taiwan with results of up to 92% accuracy.
Another study that focused on language varieties is the one published by Zampieri and Gebre (2012).
In this study, the authors proposed a log-likelihood estimation method along with Laplace smoothing to
identify two varieties of Portuguese (Brazilian and European). Their approach was trained and tested in
a binary setting using journalistic texts with accuracy results above 99.5% for character n-grams. The
algorithm was later adapted to classify Spanish texts using not only the classical word and character
n-grams but also POS distribution (Zampieri et al., 2013).
The aforementioned study by Lui and Cook (2013) investigates computational methods to discriminate
between texts from three different English varieties (Canadian, Australian and British) across different
domains. The authors state that the results obtained suggest that each variety contains characteristics that
are consistent across multiple domains, which enables algorithms to distinguish them regardless of the
data source.
Zaidan and Callison-Burch (2013) propose computational methods for the identification of Arabic
language varieties4 using character and word n-grams. The authors built their own dataset using crowd-
sourcing and investigated annotators’ behaviour, agreement and performance when manually tagging
instances with the correct label (variety).
</bodyText>
<sectionHeader confidence="0.997678" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.999369166666667">
In the following subsections we will describe the methodology adopted for the DSL shared task. Due to
the lack of comparable resources, the first decision we had to take was to create a dataset that could be
used in the shared task and also redistributed to be used in other experiments. We opted for the creation
of a corpus collection based on existing datasets as discussed in 3.1 (Tan et al., 2014).
Groups interested in participating in the DSL shared task had to register themselves in the shared
task website to receive the training and test data. Each group could participate in one or two types of
</bodyText>
<footnote confidence="0.999553">
3In the DSL shared task and in this paper we did not distinguish between language varieties and similar languages. More
on this discussion can be found in Clyne (1992) and Chamber and Trudgill (1998).
4Zaidan and Callison-Burch (2013) use the terms ‘varieties’ and ‘dialects’ interchangeably whereas Lui and Cook (2013)
use the term ‘national dialect’ to refer to what previous work describes as ‘national variety’.
</footnote>
<page confidence="0.999139">
59
</page>
<bodyText confidence="0.827486">
submission as follows:
</bodyText>
<listItem confidence="0.99997">
• Closed Submission: Using only the DSL corpus collection for training.
• Open Submission: Using any other dataset including or not the DSL collection for training.
</listItem>
<bodyText confidence="0.997665857142857">
In the open submission we did not make any distinction between systems using the DSL corpus col-
lection and those that did not. This is different from the types of submissions for the NLI shared task
2013. The NLI shared task offered proposed two types of open submissions: open submission 1 - any
dataset including the aforementioned TOEFL11 dataset (Blanchard et al., 2013) and open submission 2
- any dataset excluding TOEFL11.
For each of these submission types, participants were allowed to submit up to three runs, resulting in
a maximum of six runs in total (three closed submissions and three open submissions).
</bodyText>
<subsectionHeader confidence="0.99704">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.99977725">
As previously mentioned, we decided to compile our own dataset for the shared task. The dataset was
entitled DSL corpus collection and its compilation was motivated by the absence of a resource that
allowed us to evaluate systems on discriminating similar languages. The methods behind the compilation
of this collection and the preliminary baseline experiments are described in Tan et al. (2014).
The DSL corpus collection consists of 18,000 randomly sampled training sentences, 2,000 develop-
ment sentences and 1,000 test sentences for each language (or variety) containing at least 20 tokens5 each.
The languages are presented in table 1 with their ISO 639-1 language codes6. For language varieties the
country code is appended to the ISO code (e.g. en-GB refers to the British variety of English).
</bodyText>
<table confidence="0.998061214285714">
Group Language/Variety Code
A Bosnian bs
Croatian hr
Serbian sr
B Indonesian id
Malay my
C Czech cz
Slovak sk
D Brazilian Portuguese pt-BR
European Portuguese pt-PT
E Argentine Spanish es-AR
Castilian Spanish es-ES
F British English en-GB
American English en-US
</table>
<tableCaption confidence="0.999854">
Table 1: Language Groups - DSL 2014 Shared Task
</tableCaption>
<bodyText confidence="0.999314222222222">
For this collection, randomly sampled sentences from journalistic corpora (and corpora collections) were
selected for each of the 13 classes. Journalistic corpora were preferred because they represent standard
language, which is an important factor to be considered when working with language varieties. Other data
sources (e.g. Wikipedia) do not make any distinction between language varieties and they are therefore
not suitable for the purpose of the shared task. A number of studies mentioned in the related work section
use journalistic texts for similar reasons (Huang and Lee, 2008; Grouin et al., 2010; Zampieri and Gebre,
2012)
Given what has been said in this section, we consider the collection to be a suitable comparable corpora
from this task, which was compiled to avoid bias in classification towards source, register and topics. The
</bodyText>
<footnote confidence="0.9998635">
5We considered a token as orthographic units delimited by white spaces.
6http://www.loc.gov/standards/iso639-2/php/English_list.php
</footnote>
<page confidence="0.998145">
60
</page>
<bodyText confidence="0.999153">
DSL corpus collection was distributed in tab delimited format; the first column contains a sentence in
the language/variety, the second column states its group and the last column refers to its language code.7
</bodyText>
<subsectionHeader confidence="0.969551">
3.1.1 Problems with Group F
</subsectionHeader>
<bodyText confidence="0.999678411764706">
There are no major problems to report regarding the organization of the shared task nor with the com-
pilation of the DSL corpus collection apart from some issues in the Group F data. The organizers and
a couple of teams participating in the shared task observed very poor performance when distinguishing
instances from group F (British English - American British). For example, the baseline experiments
described in Tan et al. (2014) report a very low 0.59 F-measure for Group F (the lowest score) and 0.84
for Group E (the second lowest score). Some of the teams asked human annotators to try to distinguish
the sentences manually and they concluded that some instances were probably misclassified.
We decided to look more carefully at the data and noticed that the instances were originally tagged
based on the websites (newspapers) that they were retrieved from and not the country of the original
publication. There are, however, many cases of cross citation and republication of texts that the original
data sources did not take into account (e.g. British texts that were later republished by an American
website). As the DSL is a corpus collection and manually checking all 20,000 training and development
instances per language was not feasible, we assumed that the original sources8 from which the texts were
retrieved provided the correct country of origin. The assumption was correct for all language groups but
English.
To illustrate the issues above we present next some misclassified examples. Two particular cases raised
by the UMich team are the following:
</bodyText>
<listItem confidence="0.845558">
(1) I think they can afford to give North another innings and some time in Shield cricket and take
another middle order batsman. (en-US)
(2) ATHENS, Ohio (AP) Albuquerque will continue its four-game series in Nashville Thursday night
when it takes on the Sounds behind starter Les Walrond (3-4, 4.50) against Gary Glover, who is
making his first Triple-A start after coming down from Milwaukee. (en-GB)
</listItem>
<bodyText confidence="0.999711857142857">
Example number one was tagged as American English because it was retrieved from the online edition
of The New York Times but it was in fact first published in Australia. The second example is a text
published by Associated Press describing an event that took place in Ohio, United States, but it was
tagged as British English because it was retrieved by the UK Yahoo! sports section.
Our solution was to exclude the language group F from the final scores and perform a manual check in
all its 1,000 test instances9, thus giving the chance to participants to train their algorithms on other data
sources (open submission).
</bodyText>
<subsectionHeader confidence="0.997262">
3.2 Schedule
</subsectionHeader>
<bodyText confidence="0.99738">
The DSL shared task spanned from March 20th when the training set was released, to June 6th when
participants could submit a paper (up to 10 pages) describing their system. We provided one month
between the release of the training and the test set. The schedule of the DSL shared task 2014 can be
seen below.
</bodyText>
<table confidence="0.998718">
Event Date
Training Set Release March 20th, 2014
Test Set Release April 21st, 2014
Submissions Due April 23rd, 2014
Results Announced April 30th, 2014
Paper Submission June 6th, 2014
</table>
<tableCaption confidence="0.995094">
Table 2: DSL 2014 Shared Task Schedule
</tableCaption>
<footnote confidence="0.999702666666667">
7To obtain the data please visit: https://bitbucket.org/alvations/dslsharedtask2014
8See Tan et al. (2014) for a complete description of the data sources of the DSL corpus collection.
9Our manual check suggests that about 25% of the instances in the English dataset was likely to have been misclassified.
</footnote>
<page confidence="0.998668">
61
</page>
<sectionHeader confidence="0.99991" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999936222222222">
This section summarises the results obtained by all participants of the shared task who submitted final
results.10 The DSL shared task included 22 enrolled teams from different countries (e.g. Australia,
Estonia, Holland, Germany, United Kingdom and United States). From the 22 enrolled teams, eight of
them submitted their final results. Most of the groups opted to exclusively use the DSL corpus collection
and therefore participated solely in the closed submission track. Two of them compiled comparable
datasets and also participated in the open submission.
Given that the dataset contained misclassified instances, group F (English) was not taken into account
to compute the final shared task scores. In the next subsections we report results in terms of macro-
average F-measure and accuracy.
</bodyText>
<subsectionHeader confidence="0.95232">
4.1 Closed Submission
</subsectionHeader>
<bodyText confidence="0.844922">
Table 3 presents the best F-measure and Accuracy results obtained by the eight teams that submitted their
results for the closed submission track ordered by accuracy.
</bodyText>
<table confidence="0.999425444444445">
Team Macro-avg F-score Overall Accuracy
NRC-CNRC 0.957 0.957
RAE 0.947 0.947
UMich 0.932 0.932
UniMelb-NLP 0.918 0.918
QMUL 0.925 0.906
LIRA 0.721 0.766
UDE 0.657 0.681
CLCG 0.405 0.453
</table>
<tableCaption confidence="0.997687">
Table 3: Open Submission - Results
</tableCaption>
<bodyText confidence="0.999967444444445">
In the closed submissions, we observed a group of five teams whose systems (best runs) obtained re-
sults over 90% accuracy. This is comparable to what is described in the state-of-the-art literature for
discriminating similar languages and language varieties (Tiedemann and Ljubeˇsi´c, 2012; Lui and Cook,
2013). These five teams submitted system descriptions that allowed us to look in more detail at successful
approaches for this task. System descriptions will be discussed in section 5.
Three of the eight teams obtained substantially lower scores, from 45.33% to 76.64% accuracy. These
three groups unfortunately did not submit system description papers. From our point of view, this would
create an interesting opportunity to look more carefully at the weaknesses of approaches that did not
obtain good results in this task.
</bodyText>
<subsectionHeader confidence="0.958456">
4.2 Open Submission
</subsectionHeader>
<bodyText confidence="0.991251">
Only two systems submitted results for the open submission track and their F-measure and Accuracy
results are presented in table 3.
</bodyText>
<table confidence="0.984860666666667">
Team Macro-avg F-score Overall Accuracy
UniMelb-NLP 0.878 0.880
UMich 0.858 0.859
</table>
<tableCaption confidence="0.998259">
Table 4: Closed Submission - Results
</tableCaption>
<bodyText confidence="0.9985515">
The UniMelb-NLP (Lui et al., 2014) group used data from different corpora such as the BNC, EU-
ROPARL and Open Subtitles whereas UMich (King et al., 2014) compiled journalistic corpora from dif-
ferent sizes for each language ranging from 695,597 tokens for Malay to 20,288,294 tokens for British
English.
</bodyText>
<footnote confidence="0.822787">
10Visit https://bitbucket.org/alvations/dslsharedtask2014/downloads/dsl-results.html
for more detail on the shared task results or at the aforementioned DSL shared task website.
</footnote>
<page confidence="0.999148">
62
</page>
<bodyText confidence="0.999910166666667">
Comparing the results of the closed to the open submissions, we observed that the UniMelb-NLP sub-
mission was outperformed by UMich system by about 1.5% accuracy in the closed submission, but in
the open submission they scored 2.1% better than UMich. This difference can be explained by investi-
gating these two factors: 1) the quality and amount of the collected training data; 2) the robustness of
the method to obtain correct predictions across different datasets and domains as previously discussed
by Lui and Cook (2013) for English varieties.
</bodyText>
<subsectionHeader confidence="0.99287">
4.3 Accuracy per Language Group
</subsectionHeader>
<bodyText confidence="0.973419333333333">
In this subsection we look more carefully at the performance of systems in discriminating each class
within groups A to E. Table 5 presents the accuracy scores obtained per language group for each team
sorted alphabetically. The best score per group is displayed in bold.
</bodyText>
<table confidence="0.995367166666667">
CLCG LIRA NRC-CNRC QMUL RAE UDE UMich UniMelb-NLP
A 0.338 0.333 0.936 0.879 0.919 0.785 0.919 0.915
B 0.503 0.982 0.996 0.935 0.994 0.892 0.992 0.972
C 0.500 1.000 1.000 0.962 1.000 0.493 0.999 1.000
D 0.496 0.892 0.956 0.905 0.948 0.493 0.926 0.896
E 0.503 0.843 0.910 0.865 0.888 0.694 0.876 0.807
</table>
<tableCaption confidence="0.99939">
Table 5: Language Groups A to E - Accuracy Results
</tableCaption>
<bodyText confidence="0.813038625">
The top 5 systems plus the LIRA team obtained very good results for groups B (Malay and Indonesian)
and C (Czech and Slovak). Four out of eight systems obtained perfect performance when discriminating
Czech and Slovak texts. Perfect performance was not achieved by any of the systems when distinguishing
Malay from Indonesian texts, but even so, results were fairly high and the best result was 99.6% accuracy
obtained by the NRC-CNRC group. The perfect results obtained by four groups when distinguishing
texts from group C suggest that Czech and Slovak texts are not as similar as we assumed before the
shared task, and that they therefore possess strong systemic and/or orthographic differences that allow
well-trained classifiers to perform perfectly. Figure 1 presents the accuracy results of the top 5 groups.
</bodyText>
<figureCaption confidence="0.998649">
Figure 1: Language Groups A to E Accuracy - Top 5 Systems
</figureCaption>
<bodyText confidence="0.997424333333333">
Distinguishing between languages from group A (Bosnian, Croatian and Serbian), the only group con-
taining 3 languages, proved to be a challenging task as discussed in previous research (Ljubeˇsi´c et al.,
2007; Tiedemann and Ljubeˇsi´c, 2012). The best result was again obtained by the NRC-CNRC group
with 93.5% accuracy. The groups containing texts written in different language varieties, namely D (Por-
tuguese) and E (Spanish) were the most difficult to discriminate, particularly the Spanish varieties. These
results also corroborate the findings of previous studies (Zampieri et al., 2013).
</bodyText>
<page confidence="0.998845">
63
</page>
<bodyText confidence="0.999952333333333">
The QMUL system that was the 5th best system in the closed submission track did not outperform any
of the other top 5 systems in groups A, B or C. However, the system did better when distinguishing texts
from the two most difficult language groups (D and E), outperforming the UniMelb-NLP submission on
two occasions. The simplicity of the approach proposed by the QMUL, which the author describes as
‘a simple baseline’ (Purver, 2014) may be an explanation for the regular performance across different
language groups.
</bodyText>
<subsectionHeader confidence="0.78211">
4.4 Results Group F
</subsectionHeader>
<bodyText confidence="0.9997166">
To document the problems in the group F (British and American English) dataset we included the results
of both the open and closed submissions for this language group in table 6. As previously mentioned,
submitting group F results was optional and we did not include these results in the final shared task
results. Six out of eight systems decided to submit their predictions as closed submissions and the two
groups participating in the open submission track also submitted their group F results.
</bodyText>
<table confidence="0.999833444444445">
Team F-score Accuracy Type
UMich 0.639 0.639 Open
UniMelb- NLP 0.581 0.583 Open
NRC-CNRC 0.522 0.524 Closed
LIRA 0.450 0.493 Closed
RAE 0.451 0.481 Closed
UMich 0.463 0.464 Closed
UDE 0.451 0.451 Closed
UniMelb-NLP 0.435 0.435 Closed
</table>
<tableCaption confidence="0.999096">
Table 6: Group F - Accuracy Results
</tableCaption>
<bodyText confidence="0.999933090909091">
The results confirm the problems in the DSL dataset discussed in section 3.1.1. After a careful manual
check of the 1,000 test instances, open submissions scores were still substantially lower than the other
groups: 69.9% and 58.3% accuracy. Closed submissions proved to be impossible and only one of the six
systems scored slightly above the 50% baseline.
It should be investigated more carefully in future research whether the poor results for group F reflect
only the problems in the dataset or also the actual difficulty in discriminating between these two varieties
of English. Moderate differences in orthography (e.g. neighbour (UK) and neighbor (US)) as well
as lexical choices (e.g. rubbish (UK) and garbage (US) or trousers (UK) and pants (US)) are present
in texts from these two varieties and these can be informative features for algorithms to discriminate
between them. Discriminating between other English varieties already proved to be a challenging yet
feasible task in previous research (Lui and Cook, 2013).
</bodyText>
<sectionHeader confidence="0.98217" genericHeader="method">
5 System Descriptions
</sectionHeader>
<bodyText confidence="0.999934833333333">
All eight systems that submitted their final results to the shared task were invited to submit papers de-
scribing their systems and the top 5 systems in the closed track submitted their papers, namely: NRC-
CNRC, RAE, UMich, UniMelb-NLP and QMUL.
The best scores were obtained by the NRC-CNRC (Goutte et al., 2014) team which proposed a two-
step approach to predict first the language group than the language of each instance. The language group
was predicted in a 6-way classification using a probabilistic model similar to a Naive Bayes classifier,
and later the method applied SVM classifiers to discriminate within each group: binary for groups B-F
and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian).
An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called
‘white lists’ inspired by the ‘blacklist’ classifier (Tiedemann and Ljubeˇsi´c, 2012). These lists are word
lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc¸on (2006)
proposed to discriminate between Malay and Indonesian.
</bodyText>
<page confidence="0.998601">
64
</page>
<bodyText confidence="0.99947335">
Two groups used Information Gain (IG) to select the best features for classification, namely UMich
(King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit
open submissions. The UniMelb-NLP team tried different classification methods and features (including
delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf
general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has
been widely used for general-purpose language identification and its performance is regarded superior
to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hier-
archically firstly identifying the language group that a sentence belongs to and subsequently the specific
language, achieving performance comparable to the state-of-the-art, but still slightly below the other
three systems.
The QMUL team (Purver, 2014) proposed a linear SVM classifier using words and characters as fea-
tures. The author investigated the influence of the cost parameter c (from 1.0 to 100.0), in the classifiers’
performance. The cost parameter c is responsible for the trade-off between maximum margin and clas-
sification errors. According to the system description the optimal parameter for this task lies between
30.0 and 50.0. Purver (2014) also notes that the linear SVM classifier performs well with word uni-gram
language models in comparison to methods using character n-grams. This observation corroborates the
findings of previous experiments that rely on words as important features to distinguish similar languages
and varieties (Huang and Lee, 2008; Zampieri, 2013)
The features and algorithms presented so far, as well as the system paper descriptions, are summarised
in table7.11
</bodyText>
<table confidence="0.999844666666667">
Team Algorithm Features System Paper
NRC-CNRC Prob. Class. and Linear SVM Words 1-2, Char. 2-6 (Goutte et al., 2014)
RAE MaxEnt Words 1-2, Char. 1-5, ‘Whitelist’ (Porta and Sancho, 2014)
UMich Naive Bayes Words 1-2, Char. 2-6 (IG Feat. Selection) (King et al., 2014)
UniMelb-NLP langid.py Words, Char., POS (IG Feat. Selection) (Lui et al., 2014)
QMUL Linear SVM Words 1, Char. 1-3 (Purver, 2014)
</table>
<tableCaption confidence="0.999384">
Table 7: Top 5 Systems - Features and Algorithms at a Glance
</tableCaption>
<sectionHeader confidence="0.997569" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999931210526316">
Shared tasks are an interesting way of comparing algorithms, computational methods and features using
the same dataset. Given what has been presented in this paper, we believe that the DSL shared task filled
an important gap in language identification and will allow other researchers to look in more detail at the
problem of discriminating similar languages. Accurate methods for discriminating similar languages can
help to improve performance not only in language identification but also in a number of NLP tasks and
applications such as part-of-speech-tagging, spell checking and machine translation.
The best system obtained 95.71% accuracy and F-measure for a set of 11 languages and varieties
divided into 5 groups (A to E), using only the DSL corpus collection. Systems that performed best
modelled their algorithms to perform two-step predictions: first the language group, then the actual class
and used characters and words as features. As we regard the corpus to be a balanced sample of the
news domain, the results obtained confirm the assumption that similar languages and varieties possess
systemic characteristics that can be modelled by algorithms in order to distinguish languages from other
similar languages or varieties using lexical or orthographical features.
Another lesson learned from this shared task is regarding the compilation of group F (English) data.
Researchers, including us, often rely on previously annotated meta-data which sometimes may contain
inaccurate information and errors. Corpus collection for this purpose should be thoroughly checked
(manually if possible). The issues with the group F might have discouraged some of the participants to
continue in the shared task (particularly those who were interested only in the discrimination of English
varieties).
</bodyText>
<footnote confidence="0.7495845">
11UniMelb-NLP experimented different methods in their 6 runs. In this report we commented on the algorithm that achieved
the best performance.
</footnote>
<page confidence="0.998634">
65
</page>
<subsectionHeader confidence="0.985719">
6.1 Future Perspectives
</subsectionHeader>
<bodyText confidence="0.999995125">
The shared task was a very fruitful and positive experience for the organizers. We would like to organize
a second edition of the shared task containing, for example, new language groups for which we could
not find suitable corpora before the 2014 edition. This includes, most notably, the cases of Dutch and
Flemish or the varieties of French and German which could not be included in the DSL shared task due
to the lack of available data.
The DSL corpus collection is freely available and can be used as a gold standard for language iden-
tification or to train algorithms for other NLP tasks involving similar languages. We would like to use
the dataset to investigate, for example, lexical variation between similar languages and varieties as pro-
posed by Piersman et al. (2010) and Soares da Silva (2010) or syntactic variation using annotated data
as discussed in Anstein (2013).
At present, we are investigating the influence of the length of texts in the discrimination of similar
languages. It is a well known fact that the longer texts are, the more likely they are to contain features
that allow algorithms to identify their language. However, this variable was not explored within the
scope of the DSL shared task and we are using the DSL dataset and the results for this purpose. Another
direction that our work may take is the linguistic analysis of the most informative features in classification
as was done recently by Diwersy et al. (2014).
</bodyText>
<sectionHeader confidence="0.996934" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998847333333333">
The authors would like to thank all participants of the DSL shared task for their comments and sugges-
tions throughout the organization of this shared task. We would also like to thank Joel Tetreault and
Binyam Gebrekidan Gebre for their valuable feedback on this report.
</bodyText>
<sectionHeader confidence="0.998094" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999347956521739">
Stefanie Anstein. 2013. Computational approaches to the comparison of regional variety corpora : prototyping a
semi-automatic system for German. Ph.D. thesis, University of Stuttgart.
Timothy Baldwin and Marco Lui. 2010. Multilingual language identification: ALTW 2010 shared task data. In
Proceedings of Australasian Language Technology Association Workshop, pages 4–7.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Educational Testing Service.
Ralf Brown. 2013. Selecting and weighting n-grams to identify 1100 languages. In Proceedings of the 16th In-
ternational Conference on Text Speech and Dialogue (TSD2013), Lecture Notes in Artificial Intelligence (LNAI
8082), pages 519–526, Pilsen, Czech Republic. Springer.
William Cavnar and John Trenkle. 1994. N-gram-based text catogorization. 3rd Symposium on Document Analy-
sis and Information Retrieval (SDAIR-94).
Jack Chambers and Peter Trudgill. 1998. Dialectology (2nd Edition). Cambridge University Press.
Michael Clyne. 1992. Pluricentric Languages: Different Norms in Different Nations. CRC Press.
Sascha Diwersy, Stefan Evert, and Stella Neumann. 2014. A semi-supervised multivariate approach to the study
of language variation. Linguistic Variation in Text and Speech, within and across Languages.
Cyril Goutte, Serge L´eger, and Marine Carpuat. 2014. The NRC system for discriminating similar languages. In
Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial),
Dublin, Ireland.
Cyril Grouin, Dominic Forest, Lyne Da Sylva, Patrick Paroubek, and Pierre Zweigenbaum. 2010. Pr´esentation et
r´esultats du d´efi fouille de texte DEFT2010 o`u et quand un article de presse a-t-il ´et´e ´ecrit? Actes du sixi`eme
D ´Efi Fouille de Textes.
Chu-ren Huang and Lung-hao Lee. 2008. Contrastive approach towards text source classification based on top-
bag-of-word similarity. In Proceedings of PACLIC 2008, pages 404–410.
</reference>
<page confidence="0.854126">
66
</page>
<reference confidence="0.999550268292683">
Ben King, Dragomir Radev, and Steven Abney. 2014. Experiments in sentence language identification with
groups of similar languages. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages,
Varieties and Dialects (VarDial), Dublin, Ireland.
Nikola Ljube&amp;quot;si´c, Nives Mikelic, and Damir Boras. 2007. Language identification: How to distinguish similar
languages? In Proceedings of the 29th International Conference on Information Technology Interfaces.
Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings
of the 50th Meeting of the ACL.
Marco Lui and Paul Cook. 2013. Classifying English documents by national dialect. In Proceedings of Aus-
tralasian Language Tchnology Workshop, pages 5–15.
Marco Lui, Ned Letcher, Oliver Adams, Long Duong, Paul Cook, and Timothy Baldwin. 2014. Exploring methods
and resources for discriminating similar languages. In Proceedings of the 1st Workshop on Applying NLP Tools
to Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.
Yves Piersman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical variation
between language varieties. Natural Language Engineering, 16:469–491.
Jordi Porta and Jos´e-Luis Sancho. 2014. Using maximum entropy models to discriminate between similar lan-
guages and varieties. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties
and Dialects (VarDial), Dublin, Ireland.
Matthew Purver. 2014. A simple baseline for discriminating similar language. In Proceedings of the 1st Workshop
on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.
Bali Ranaivo-Malanc¸on. 2006. Automatic identification of close languages - case study: Malay and Indonesian.
ECTI Transactions on Computer and Information Technology, 2:126–134.
Augusto Soares da Silva. 2010. Measuring and parameterizing lexical convergence and divergence between
European and Brazilian Portuguese: endo/exogeneousness and foreign and normative influence. Advances in
Cognitive Sociolinguistics.
Liling Tan, Marcos Zampieri, Nikola Ljube&amp;quot;si´c, and J¨org Tiedemann. 2014. Merging comparable data sources
for the discrimination of similar languages: The DSL corpus collection. In Proceedings of The Workshop on
Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013. A report on the first native language identification shared
task. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,
Atlanta, GA, USA, June. Association for Computational Linguistics.
J¨org Tiedemann and Nikola Ljube&amp;quot;si´c. 2012. Efficient discrimination between closely related languages. In
Proceedings of COLING 2012, pages 2619–2634, Mumbai, India.
Omar F Zaidan and Chris Callison-Burch. 2013. Arabic dialect identification. Computational Linguistics.
Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic identification of language varieties: The case
of Portuguese. In Proceedings of KONVENS2012, pages 233–237, Vienna, Austria.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and POS
distribution for the identification of Spanish varieties. In Proceedings of TALN2013, pages 580–587, Sable
d’Olonne, France.
Marcos Zampieri. 2013. Using bag-of-words to distinguish similar languages: How efficient are they?
In Proceedings of the 14th IEEE International Symposium on Computational Intelligence and Informatics
(CINTI2013), pages 37–41, Budapest, Hungary.
</reference>
<page confidence="0.999504">
67
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.235214">
<title confidence="0.400966">A Report on the DSL Shared Task 2014</title>
<author confidence="0.970602">Liling Nikola J¨org</author>
<affiliation confidence="0.945755666666667">University, of Zagreb, University,</affiliation>
<email confidence="0.8636905">marcos.zampieri@uni-saarland.de,jorg.tiedemann@lingfil.uu.se,nljubesi@ffzg.hr</email>
<abstract confidence="0.995872111111111">This paper summarizes the methods, results and findings of the Discriminating between Similar Languages (DSL) shared task 2014. The shared task provided data from 13 different languages and varieties divided into 6 groups. Participants were required to train their systems to discriminate between languages on a training and development set containing 20,000 sentences from each language (closed submission) and/or any other dataset (open submission). One month later, a test set containing 1,000 unidentified instances per language was released for evaluation. The DSL shared task received 22 inscriptions and 8 final submissions. The best system obtained 95.7% average accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefanie Anstein</author>
</authors>
<title>Computational approaches to the comparison of regional variety corpora : prototyping a semi-automatic system for German.</title>
<date>2013</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Stuttgart.</institution>
<contexts>
<context position="29047" citStr="Anstein (2013)" startWordPosition="4476" endWordPosition="4477">des, most notably, the cases of Dutch and Flemish or the varieties of French and German which could not be included in the DSL shared task due to the lack of available data. The DSL corpus collection is freely available and can be used as a gold standard for language identification or to train algorithms for other NLP tasks involving similar languages. We would like to use the dataset to investigate, for example, lexical variation between similar languages and varieties as proposed by Piersman et al. (2010) and Soares da Silva (2010) or syntactic variation using annotated data as discussed in Anstein (2013). At present, we are investigating the influence of the length of texts in the discrimination of similar languages. It is a well known fact that the longer texts are, the more likely they are to contain features that allow algorithms to identify their language. However, this variable was not explored within the scope of the DSL shared task and we are using the DSL dataset and the results for this purpose. Another direction that our work may take is the linguistic analysis of the most informative features in classification as was done recently by Diwersy et al. (2014). Acknowledgements The auth</context>
</contexts>
<marker>Anstein, 2013</marker>
<rawString>Stefanie Anstein. 2013. Computational approaches to the comparison of regional variety corpora : prototyping a semi-automatic system for German. Ph.D. thesis, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Marco Lui</author>
</authors>
<title>Multilingual language identification: ALTW</title>
<date>2010</date>
<booktitle>In Proceedings of Australasian Language Technology Association Workshop,</booktitle>
<pages>4--7</pages>
<contexts>
<context position="4093" citStr="Baldwin and Lui, 2010" startWordPosition="567" endWordPosition="570"> added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1Brown (2013) reports results on a system trained to recognize more than 1,100 languages 2http://corporavm.uni-koeln.de/vardial/sharedtask.html 58 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 58–67, Dublin, Ireland, August 23 2014. decade in which they were published. Other related shared tasks include the ALTW 2010 multilingual language identification shared task, a general-purpose language identification task containing data from 74 languages (Baldwin and Lui, 2010) and finally the Native Language Identification (NLI) shared task (Tetreault et al., 2013) where participants were provided English essays written by foreign students of 11 different mother tongues (Blanchard et al., 2013). Participants had to train their systems to identify the native language of the writer of each text. 2 Related Work Among the first studies to investigate the question of discriminating between similar languages is the study published by Ranaivo-Malanc¸on (2006). The author presents a semi-supervised model to distinguish between Indonesian and Malay, two closely related lang</context>
</contexts>
<marker>Baldwin, Lui, 2010</marker>
<rawString>Timothy Baldwin and Marco Lui. 2010. Multilingual language identification: ALTW 2010 shared task data. In Proceedings of Australasian Language Technology Association Workshop, pages 4–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Blanchard</author>
<author>Joel Tetreault</author>
<author>Derrick Higgins</author>
<author>Aoife Cahill</author>
<author>Martin Chodorow</author>
</authors>
<title>TOEFL11: A Corpus of Non-Native English.</title>
<date>2013</date>
<tech>Technical report, Educational Testing Service.</tech>
<contexts>
<context position="4315" citStr="Blanchard et al., 2013" startWordPosition="599" endWordPosition="602">task.html 58 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 58–67, Dublin, Ireland, August 23 2014. decade in which they were published. Other related shared tasks include the ALTW 2010 multilingual language identification shared task, a general-purpose language identification task containing data from 74 languages (Baldwin and Lui, 2010) and finally the Native Language Identification (NLI) shared task (Tetreault et al., 2013) where participants were provided English essays written by foreign students of 11 different mother tongues (Blanchard et al., 2013). Participants had to train their systems to identify the native language of the writer of each text. 2 Related Work Among the first studies to investigate the question of discriminating between similar languages is the study published by Ranaivo-Malanc¸on (2006). The author presents a semi-supervised model to distinguish between Indonesian and Malay, two closely related languages from the Austronesian family represented in the DSL shared task. The study uses the frequency and rank of character trigrams derived from the most frequent words in each language, lists of exclusive words, and the fo</context>
<context position="8925" citStr="Blanchard et al., 2013" startWordPosition="1303" endWordPosition="1306">er to what previous work describes as ‘national variety’. 59 submission as follows: • Closed Submission: Using only the DSL corpus collection for training. • Open Submission: Using any other dataset including or not the DSL collection for training. In the open submission we did not make any distinction between systems using the DSL corpus collection and those that did not. This is different from the types of submissions for the NLI shared task 2013. The NLI shared task offered proposed two types of open submissions: open submission 1 - any dataset including the aforementioned TOEFL11 dataset (Blanchard et al., 2013) and open submission 2 - any dataset excluding TOEFL11. For each of these submission types, participants were allowed to submit up to three runs, resulting in a maximum of six runs in total (three closed submissions and three open submissions). 3.1 Data As previously mentioned, we decided to compile our own dataset for the shared task. The dataset was entitled DSL corpus collection and its compilation was motivated by the absence of a resource that allowed us to evaluate systems on discriminating similar languages. The methods behind the compilation of this collection and the preliminary basel</context>
</contexts>
<marker>Blanchard, Tetreault, Higgins, Cahill, Chodorow, 2013</marker>
<rawString>Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife Cahill, and Martin Chodorow. 2013. TOEFL11: A Corpus of Non-Native English. Technical report, Educational Testing Service.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Brown</author>
</authors>
<title>Selecting and weighting n-grams to identify 1100 languages.</title>
<date>2013</date>
<booktitle>In Proceedings of the 16th International Conference on Text Speech and Dialogue (TSD2013), Lecture Notes in Artificial Intelligence (LNAI 8082),</booktitle>
<pages>519--526</pages>
<publisher>Springer.</publisher>
<location>Pilsen, Czech Republic.</location>
<contexts>
<context position="3571" citStr="Brown (2013)" startWordPosition="500" endWordPosition="501">t motivated us to organize this shared task is that, to our knowledge, no shared task focusing on the discrimination of similar languages has been organized previously. The most similar shared tasks to DSL are the DEFT 2010 shared task (Grouin et al., 2010), in which systems were required to classify French journalistic texts with respect to their geographical location as well as the This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1Brown (2013) reports results on a system trained to recognize more than 1,100 languages 2http://corporavm.uni-koeln.de/vardial/sharedtask.html 58 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 58–67, Dublin, Ireland, August 23 2014. decade in which they were published. Other related shared tasks include the ALTW 2010 multilingual language identification shared task, a general-purpose language identification task containing data from 74 languages (Baldwin and Lui, 2010) and finally the Native Language Identification (NLI) shared task (Tetreault e</context>
</contexts>
<marker>Brown, 2013</marker>
<rawString>Ralf Brown. 2013. Selecting and weighting n-grams to identify 1100 languages. In Proceedings of the 16th International Conference on Text Speech and Dialogue (TSD2013), Lecture Notes in Artificial Intelligence (LNAI 8082), pages 519–526, Pilsen, Czech Republic. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Cavnar</author>
<author>John Trenkle</author>
</authors>
<title>N-gram-based text catogorization. 3rd</title>
<date>1994</date>
<booktitle>Symposium on Document Analysis and Information Retrieval (SDAIR-94).</booktitle>
<contexts>
<context position="5109" citStr="Cavnar and Trenkle, 1994" startWordPosition="720" endWordPosition="723">of discriminating between similar languages is the study published by Ranaivo-Malanc¸on (2006). The author presents a semi-supervised model to distinguish between Indonesian and Malay, two closely related languages from the Austronesian family represented in the DSL shared task. The study uses the frequency and rank of character trigrams derived from the most frequent words in each language, lists of exclusive words, and the format of numbers (Malay uses decimal points whereas Indonesian uses commas). The author compares the performance of this method with the performance obtained by TextCat (Cavnar and Trenkle, 1994). Ljubeˇsi´c et al. (2007) proposed a computational model for the identification of Croatian texts in comparison to Slovene and Serbian, reporting 99% recall and precision in three processing stages. The approach includes a ‘black list’, which increases the performance of the algorithm. Tiedemann and Ljubeˇsi´c (2012) improved this method and applied it to Bosnian, Croatian and Serbian texts. The study reports significantly higher performance compared to general purpose language identification methods. The methods applied to discriminate between texts from different language varieties and dial</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>William Cavnar and John Trenkle. 1994. N-gram-based text catogorization. 3rd Symposium on Document Analysis and Information Retrieval (SDAIR-94).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack Chambers</author>
<author>Peter Trudgill</author>
</authors>
<title>Dialectology (2nd Edition).</title>
<date>1998</date>
<publisher>Cambridge University Press. Michael</publisher>
<marker>Chambers, Trudgill, 1998</marker>
<rawString>Jack Chambers and Peter Trudgill. 1998. Dialectology (2nd Edition). Cambridge University Press. Michael Clyne. 1992. Pluricentric Languages: Different Norms in Different Nations. CRC Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sascha Diwersy</author>
<author>Stefan Evert</author>
<author>Stella Neumann</author>
</authors>
<title>A semi-supervised multivariate approach to the study of language variation. Linguistic Variation in Text and Speech, within and across Languages.</title>
<date>2014</date>
<marker>Diwersy, Evert, Neumann, 2014</marker>
<rawString>Sascha Diwersy, Stefan Evert, and Stella Neumann. 2014. A semi-supervised multivariate approach to the study of language variation. Linguistic Variation in Text and Speech, within and across Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Goutte</author>
<author>Serge L´eger</author>
<author>Marine Carpuat</author>
</authors>
<title>The NRC system for discriminating similar languages.</title>
<date>2014</date>
<booktitle>In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial),</booktitle>
<location>Dublin, Ireland.</location>
<marker>Goutte, L´eger, Carpuat, 2014</marker>
<rawString>Cyril Goutte, Serge L´eger, and Marine Carpuat. 2014. The NRC system for discriminating similar languages. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Grouin</author>
<author>Dominic Forest</author>
<author>Lyne Da Sylva</author>
<author>Patrick Paroubek</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Pr´esentation et r´esultats du d´efi fouille de texte DEFT2010 o`u et quand un article de presse a-t-il ´et´e ´ecrit? Actes du sixi`eme D ´Efi Fouille de Textes.</title>
<date>2010</date>
<contexts>
<context position="3216" citStr="Grouin et al., 2010" startWordPosition="451" endWordPosition="454">ent years starting with Ranaivo-Malanc¸on (2006) for Malay and Indonesian and Ljubeˇsi´c et al. (2007) for South Slavic languages. In the DSL shared task we tried to include (depending on the availability of data) languages that have been studied in previous experiments, such as Croatian, English, Indonesian, Malay, Portuguese and Spanish. The second aspect that motivated us to organize this shared task is that, to our knowledge, no shared task focusing on the discrimination of similar languages has been organized previously. The most similar shared tasks to DSL are the DEFT 2010 shared task (Grouin et al., 2010), in which systems were required to classify French journalistic texts with respect to their geographical location as well as the This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1Brown (2013) reports results on a system trained to recognize more than 1,100 languages 2http://corporavm.uni-koeln.de/vardial/sharedtask.html 58 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 58–</context>
<context position="10899" citStr="Grouin et al., 2010" startWordPosition="1613" endWordPosition="1616">ared Task For this collection, randomly sampled sentences from journalistic corpora (and corpora collections) were selected for each of the 13 classes. Journalistic corpora were preferred because they represent standard language, which is an important factor to be considered when working with language varieties. Other data sources (e.g. Wikipedia) do not make any distinction between language varieties and they are therefore not suitable for the purpose of the shared task. A number of studies mentioned in the related work section use journalistic texts for similar reasons (Huang and Lee, 2008; Grouin et al., 2010; Zampieri and Gebre, 2012) Given what has been said in this section, we consider the collection to be a suitable comparable corpora from this task, which was compiled to avoid bias in classification towards source, register and topics. The 5We considered a token as orthographic units delimited by white spaces. 6http://www.loc.gov/standards/iso639-2/php/English_list.php 60 DSL corpus collection was distributed in tab delimited format; the first column contains a sentence in the language/variety, the second column states its group and the last column refers to its language code.7 3.1.1 Problems</context>
</contexts>
<marker>Grouin, Forest, Sylva, Paroubek, Zweigenbaum, 2010</marker>
<rawString>Cyril Grouin, Dominic Forest, Lyne Da Sylva, Patrick Paroubek, and Pierre Zweigenbaum. 2010. Pr´esentation et r´esultats du d´efi fouille de texte DEFT2010 o`u et quand un article de presse a-t-il ´et´e ´ecrit? Actes du sixi`eme D ´Efi Fouille de Textes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chu-ren Huang</author>
<author>Lung-hao Lee</author>
</authors>
<title>Contrastive approach towards text source classification based on topbag-of-word similarity.</title>
<date>2008</date>
<booktitle>In Proceedings of PACLIC</booktitle>
<pages>404--410</pages>
<contexts>
<context position="5851" citStr="Huang and Lee (2008)" startWordPosition="827" endWordPosition="830">e and Serbian, reporting 99% recall and precision in three processing stages. The approach includes a ‘black list’, which increases the performance of the algorithm. Tiedemann and Ljubeˇsi´c (2012) improved this method and applied it to Bosnian, Croatian and Serbian texts. The study reports significantly higher performance compared to general purpose language identification methods. The methods applied to discriminate between texts from different language varieties and dialects are similar to those applied to similar languages3. One of the methods proposed to identify language varieties is by Huang and Lee (2008). This study presented a bag-of-words approach to classify Chinese texts from the mainland and Taiwan with results of up to 92% accuracy. Another study that focused on language varieties is the one published by Zampieri and Gebre (2012). In this study, the authors proposed a log-likelihood estimation method along with Laplace smoothing to identify two varieties of Portuguese (Brazilian and European). Their approach was trained and tested in a binary setting using journalistic texts with accuracy results above 99.5% for character n-grams. The algorithm was later adapted to classify Spanish text</context>
<context position="10878" citStr="Huang and Lee, 2008" startWordPosition="1609" endWordPosition="1612"> Groups - DSL 2014 Shared Task For this collection, randomly sampled sentences from journalistic corpora (and corpora collections) were selected for each of the 13 classes. Journalistic corpora were preferred because they represent standard language, which is an important factor to be considered when working with language varieties. Other data sources (e.g. Wikipedia) do not make any distinction between language varieties and they are therefore not suitable for the purpose of the shared task. A number of studies mentioned in the related work section use journalistic texts for similar reasons (Huang and Lee, 2008; Grouin et al., 2010; Zampieri and Gebre, 2012) Given what has been said in this section, we consider the collection to be a suitable comparable corpora from this task, which was compiled to avoid bias in classification towards source, register and topics. The 5We considered a token as orthographic units delimited by white spaces. 6http://www.loc.gov/standards/iso639-2/php/English_list.php 60 DSL corpus collection was distributed in tab delimited format; the first column contains a sentence in the language/variety, the second column states its group and the last column refers to its language </context>
<context position="25587" citStr="Huang and Lee, 2008" startWordPosition="3930" endWordPosition="3933">luence of the cost parameter c (from 1.0 to 100.0), in the classifiers’ performance. The cost parameter c is responsible for the trade-off between maximum margin and classification errors. According to the system description the optimal parameter for this task lies between 30.0 and 50.0. Purver (2014) also notes that the linear SVM classifier performs well with word uni-gram language models in comparison to methods using character n-grams. This observation corroborates the findings of previous experiments that rely on words as important features to distinguish similar languages and varieties (Huang and Lee, 2008; Zampieri, 2013) The features and algorithms presented so far, as well as the system paper descriptions, are summarised in table7.11 Team Algorithm Features System Paper NRC-CNRC Prob. Class. and Linear SVM Words 1-2, Char. 2-6 (Goutte et al., 2014) RAE MaxEnt Words 1-2, Char. 1-5, ‘Whitelist’ (Porta and Sancho, 2014) UMich Naive Bayes Words 1-2, Char. 2-6 (IG Feat. Selection) (King et al., 2014) UniMelb-NLP langid.py Words, Char., POS (IG Feat. Selection) (Lui et al., 2014) QMUL Linear SVM Words 1, Char. 1-3 (Purver, 2014) Table 7: Top 5 Systems - Features and Algorithms at a Glance 6 Conclu</context>
</contexts>
<marker>Huang, Lee, 2008</marker>
<rawString>Chu-ren Huang and Lung-hao Lee. 2008. Contrastive approach towards text source classification based on topbag-of-word similarity. In Proceedings of PACLIC 2008, pages 404–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben King</author>
<author>Dragomir Radev</author>
<author>Steven Abney</author>
</authors>
<title>Experiments in sentence language identification with groups of similar languages.</title>
<date>2014</date>
<booktitle>In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="17431" citStr="King et al., 2014" startWordPosition="2653" endWordPosition="2656">tem description papers. From our point of view, this would create an interesting opportunity to look more carefully at the weaknesses of approaches that did not obtain good results in this task. 4.2 Open Submission Only two systems submitted results for the open submission track and their F-measure and Accuracy results are presented in table 3. Team Macro-avg F-score Overall Accuracy UniMelb-NLP 0.878 0.880 UMich 0.858 0.859 Table 4: Closed Submission - Results The UniMelb-NLP (Lui et al., 2014) group used data from different corpora such as the BNC, EUROPARL and Open Subtitles whereas UMich (King et al., 2014) compiled journalistic corpora from different sizes for each language ranging from 695,597 tokens for Malay to 20,288,294 tokens for British English. 10Visit https://bitbucket.org/alvations/dslsharedtask2014/downloads/dsl-results.html for more detail on the shared task results or at the aforementioned DSL shared task website. 62 Comparing the results of the closed to the open submissions, we observed that the UniMelb-NLP submission was outperformed by UMich system by about 1.5% accuracy in the closed submission, but in the open submission they scored 2.1% better than UMich. This difference can</context>
<context position="24018" citStr="King et al., 2014" startWordPosition="3701" endWordPosition="3704">within each group: binary for groups B-F and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian). An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called ‘white lists’ inspired by the ‘blacklist’ classifier (Tiedemann and Ljubeˇsi´c, 2012). These lists are word lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc¸on (2006) proposed to discriminate between Malay and Indonesian. 64 Two groups used Information Gain (IG) to select the best features for classification, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit open submissions. The UniMelb-NLP team tried different classification methods and features (including delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has been widely used for general-purpose language identification and its performance is regarded superior to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hierarchical</context>
<context position="25987" citStr="King et al., 2014" startWordPosition="3994" endWordPosition="3997">arison to methods using character n-grams. This observation corroborates the findings of previous experiments that rely on words as important features to distinguish similar languages and varieties (Huang and Lee, 2008; Zampieri, 2013) The features and algorithms presented so far, as well as the system paper descriptions, are summarised in table7.11 Team Algorithm Features System Paper NRC-CNRC Prob. Class. and Linear SVM Words 1-2, Char. 2-6 (Goutte et al., 2014) RAE MaxEnt Words 1-2, Char. 1-5, ‘Whitelist’ (Porta and Sancho, 2014) UMich Naive Bayes Words 1-2, Char. 2-6 (IG Feat. Selection) (King et al., 2014) UniMelb-NLP langid.py Words, Char., POS (IG Feat. Selection) (Lui et al., 2014) QMUL Linear SVM Words 1, Char. 1-3 (Purver, 2014) Table 7: Top 5 Systems - Features and Algorithms at a Glance 6 Conclusion Shared tasks are an interesting way of comparing algorithms, computational methods and features using the same dataset. Given what has been presented in this paper, we believe that the DSL shared task filled an important gap in language identification and will allow other researchers to look in more detail at the problem of discriminating similar languages. Accurate methods for discriminating</context>
</contexts>
<marker>King, Radev, Abney, 2014</marker>
<rawString>Ben King, Dragomir Radev, and Steven Abney. 2014. Experiments in sentence language identification with groups of similar languages. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikola Ljubesi´c</author>
<author>Nives Mikelic</author>
<author>Damir Boras</author>
</authors>
<title>Language identification: How to distinguish similar languages?</title>
<date>2007</date>
<booktitle>In Proceedings of the 29th International Conference on Information Technology Interfaces.</booktitle>
<marker>Ljubesi´c, Mikelic, Boras, 2007</marker>
<rawString>Nikola Ljube&amp;quot;si´c, Nives Mikelic, and Damir Boras. 2007. Language identification: How to distinguish similar languages? In Proceedings of the 29th International Conference on Information Technology Interfaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>langid.py: An off-the-shelf language identification tool.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Meeting of the ACL.</booktitle>
<contexts>
<context position="24390" citStr="Lui and Baldwin, 2012" startWordPosition="3753" endWordPosition="3756"> or variety, similar to one of the features that Ranaivo-Malanc¸on (2006) proposed to discriminate between Malay and Indonesian. 64 Two groups used Information Gain (IG) to select the best features for classification, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit open submissions. The UniMelb-NLP team tried different classification methods and features (including delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has been widely used for general-purpose language identification and its performance is regarded superior to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hierarchically firstly identifying the language group that a sentence belongs to and subsequently the specific language, achieving performance comparable to the state-of-the-art, but still slightly below the other three systems. The QMUL team (Purver, 2014) proposed a linear SVM classifier using words and characters as features. The author investigated the influence of the cost par</context>
</contexts>
<marker>Lui, Baldwin, 2012</marker>
<rawString>Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings of the 50th Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Paul Cook</author>
</authors>
<title>Classifying English documents by national dialect.</title>
<date>2013</date>
<booktitle>In Proceedings of Australasian Language Tchnology Workshop,</booktitle>
<pages>5--15</pages>
<contexts>
<context position="1812" citStr="Lui and Cook, 2013" startWordPosition="239" endWordPosition="242">e between more languages1, they still struggle to discriminate between similar languages such as Croatian and Serbian or Malay and Indonesian. From an NLP point of view, the difficulty systems face when discriminating between closely related languages is similar to the problem of discriminating between standard national language varieties (e.g. American English and British English or Brazilian Portuguese and European Portuguese), henceforth varieties. Recent studies show that language varieties can be discriminated automatically using words or characters as features (Zampieri and Gebre, 2012; Lui and Cook, 2013) . However, due to performance limitations, state-of-the-art general-purpose language identification systems do not distinguish texts from different national varieties, modelling pluricentric languages as unique classes. To evaluate how state-of-the-art systems perform in identifying similar languages and varieties, we decided to organize the Discriminating between Similar Languages (DSL)2 shared task. This shared task was organized within the scope of the workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial) in the 2014 edition of COLING. The motivation behind </context>
<context position="6607" citStr="Lui and Cook (2013)" startWordPosition="943" endWordPosition="946"> Another study that focused on language varieties is the one published by Zampieri and Gebre (2012). In this study, the authors proposed a log-likelihood estimation method along with Laplace smoothing to identify two varieties of Portuguese (Brazilian and European). Their approach was trained and tested in a binary setting using journalistic texts with accuracy results above 99.5% for character n-grams. The algorithm was later adapted to classify Spanish texts using not only the classical word and character n-grams but also POS distribution (Zampieri et al., 2013). The aforementioned study by Lui and Cook (2013) investigates computational methods to discriminate between texts from three different English varieties (Canadian, Australian and British) across different domains. The authors state that the results obtained suggest that each variety contains characteristics that are consistent across multiple domains, which enables algorithms to distinguish them regardless of the data source. Zaidan and Callison-Burch (2013) propose computational methods for the identification of Arabic language varieties4 using character and word n-grams. The authors built their own dataset using crowdsourcing and investig</context>
<context position="8263" citStr="Lui and Cook (2013)" startWordPosition="1195" endWordPosition="1198">eation of a corpus collection based on existing datasets as discussed in 3.1 (Tan et al., 2014). Groups interested in participating in the DSL shared task had to register themselves in the shared task website to receive the training and test data. Each group could participate in one or two types of 3In the DSL shared task and in this paper we did not distinguish between language varieties and similar languages. More on this discussion can be found in Clyne (1992) and Chamber and Trudgill (1998). 4Zaidan and Callison-Burch (2013) use the terms ‘varieties’ and ‘dialects’ interchangeably whereas Lui and Cook (2013) use the term ‘national dialect’ to refer to what previous work describes as ‘national variety’. 59 submission as follows: • Closed Submission: Using only the DSL corpus collection for training. • Open Submission: Using any other dataset including or not the DSL collection for training. In the open submission we did not make any distinction between systems using the DSL corpus collection and those that did not. This is different from the types of submissions for the NLI shared task 2013. The NLI shared task offered proposed two types of open submissions: open submission 1 - any dataset includi</context>
<context position="16488" citStr="Lui and Cook, 2013" startWordPosition="2504" endWordPosition="2507">teams that submitted their results for the closed submission track ordered by accuracy. Team Macro-avg F-score Overall Accuracy NRC-CNRC 0.957 0.957 RAE 0.947 0.947 UMich 0.932 0.932 UniMelb-NLP 0.918 0.918 QMUL 0.925 0.906 LIRA 0.721 0.766 UDE 0.657 0.681 CLCG 0.405 0.453 Table 3: Open Submission - Results In the closed submissions, we observed a group of five teams whose systems (best runs) obtained results over 90% accuracy. This is comparable to what is described in the state-of-the-art literature for discriminating similar languages and language varieties (Tiedemann and Ljubeˇsi´c, 2012; Lui and Cook, 2013). These five teams submitted system descriptions that allowed us to look in more detail at successful approaches for this task. System descriptions will be discussed in section 5. Three of the eight teams obtained substantially lower scores, from 45.33% to 76.64% accuracy. These three groups unfortunately did not submit system description papers. From our point of view, this would create an interesting opportunity to look more carefully at the weaknesses of approaches that did not obtain good results in this task. 4.2 Open Submission Only two systems submitted results for the open submission t</context>
<context position="18285" citStr="Lui and Cook (2013)" startWordPosition="2779" endWordPosition="2782">for more detail on the shared task results or at the aforementioned DSL shared task website. 62 Comparing the results of the closed to the open submissions, we observed that the UniMelb-NLP submission was outperformed by UMich system by about 1.5% accuracy in the closed submission, but in the open submission they scored 2.1% better than UMich. This difference can be explained by investigating these two factors: 1) the quality and amount of the collected training data; 2) the robustness of the method to obtain correct predictions across different datasets and domains as previously discussed by Lui and Cook (2013) for English varieties. 4.3 Accuracy per Language Group In this subsection we look more carefully at the performance of systems in discriminating each class within groups A to E. Table 5 presents the accuracy scores obtained per language group for each team sorted alphabetically. The best score per group is displayed in bold. CLCG LIRA NRC-CNRC QMUL RAE UDE UMich UniMelb-NLP A 0.338 0.333 0.936 0.879 0.919 0.785 0.919 0.915 B 0.503 0.982 0.996 0.935 0.994 0.892 0.992 0.972 C 0.500 1.000 1.000 0.962 1.000 0.493 0.999 1.000 D 0.496 0.892 0.956 0.905 0.948 0.493 0.926 0.896 E 0.503 0.843 0.910 0.</context>
<context position="22768" citStr="Lui and Cook, 2013" startWordPosition="3503" endWordPosition="3506">e research whether the poor results for group F reflect only the problems in the dataset or also the actual difficulty in discriminating between these two varieties of English. Moderate differences in orthography (e.g. neighbour (UK) and neighbor (US)) as well as lexical choices (e.g. rubbish (UK) and garbage (US) or trousers (UK) and pants (US)) are present in texts from these two varieties and these can be informative features for algorithms to discriminate between them. Discriminating between other English varieties already proved to be a challenging yet feasible task in previous research (Lui and Cook, 2013). 5 System Descriptions All eight systems that submitted their final results to the shared task were invited to submit papers describing their systems and the top 5 systems in the closed track submitted their papers, namely: NRCCNRC, RAE, UMich, UniMelb-NLP and QMUL. The best scores were obtained by the NRC-CNRC (Goutte et al., 2014) team which proposed a twostep approach to predict first the language group than the language of each instance. The language group was predicted in a 6-way classification using a probabilistic model similar to a Naive Bayes classifier, and later the method applied </context>
</contexts>
<marker>Lui, Cook, 2013</marker>
<rawString>Marco Lui and Paul Cook. 2013. Classifying English documents by national dialect. In Proceedings of Australasian Language Tchnology Workshop, pages 5–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Ned Letcher</author>
<author>Oliver Adams</author>
<author>Long Duong</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Exploring methods and resources for discriminating similar languages.</title>
<date>2014</date>
<booktitle>In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="17313" citStr="Lui et al., 2014" startWordPosition="2632" endWordPosition="2635">ained substantially lower scores, from 45.33% to 76.64% accuracy. These three groups unfortunately did not submit system description papers. From our point of view, this would create an interesting opportunity to look more carefully at the weaknesses of approaches that did not obtain good results in this task. 4.2 Open Submission Only two systems submitted results for the open submission track and their F-measure and Accuracy results are presented in table 3. Team Macro-avg F-score Overall Accuracy UniMelb-NLP 0.878 0.880 UMich 0.858 0.859 Table 4: Closed Submission - Results The UniMelb-NLP (Lui et al., 2014) group used data from different corpora such as the BNC, EUROPARL and Open Subtitles whereas UMich (King et al., 2014) compiled journalistic corpora from different sizes for each language ranging from 695,597 tokens for Malay to 20,288,294 tokens for British English. 10Visit https://bitbucket.org/alvations/dslsharedtask2014/downloads/dsl-results.html for more detail on the shared task results or at the aforementioned DSL shared task website. 62 Comparing the results of the closed to the open submissions, we observed that the UniMelb-NLP submission was outperformed by UMich system by about 1.5%</context>
<context position="24053" citStr="Lui et al., 2014" startWordPosition="3707" endWordPosition="3710"> B-F and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian). An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called ‘white lists’ inspired by the ‘blacklist’ classifier (Tiedemann and Ljubeˇsi´c, 2012). These lists are word lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc¸on (2006) proposed to discriminate between Malay and Indonesian. 64 Two groups used Information Gain (IG) to select the best features for classification, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit open submissions. The UniMelb-NLP team tried different classification methods and features (including delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has been widely used for general-purpose language identification and its performance is regarded superior to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hierarchically firstly identifying the language</context>
<context position="26067" citStr="Lui et al., 2014" startWordPosition="4006" endWordPosition="4009">ings of previous experiments that rely on words as important features to distinguish similar languages and varieties (Huang and Lee, 2008; Zampieri, 2013) The features and algorithms presented so far, as well as the system paper descriptions, are summarised in table7.11 Team Algorithm Features System Paper NRC-CNRC Prob. Class. and Linear SVM Words 1-2, Char. 2-6 (Goutte et al., 2014) RAE MaxEnt Words 1-2, Char. 1-5, ‘Whitelist’ (Porta and Sancho, 2014) UMich Naive Bayes Words 1-2, Char. 2-6 (IG Feat. Selection) (King et al., 2014) UniMelb-NLP langid.py Words, Char., POS (IG Feat. Selection) (Lui et al., 2014) QMUL Linear SVM Words 1, Char. 1-3 (Purver, 2014) Table 7: Top 5 Systems - Features and Algorithms at a Glance 6 Conclusion Shared tasks are an interesting way of comparing algorithms, computational methods and features using the same dataset. Given what has been presented in this paper, we believe that the DSL shared task filled an important gap in language identification and will allow other researchers to look in more detail at the problem of discriminating similar languages. Accurate methods for discriminating similar languages can help to improve performance not only in language identifi</context>
</contexts>
<marker>Lui, Letcher, Adams, Duong, Cook, Baldwin, 2014</marker>
<rawString>Marco Lui, Ned Letcher, Oliver Adams, Long Duong, Paul Cook, and Timothy Baldwin. 2014. Exploring methods and resources for discriminating similar languages. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Piersman</author>
<author>Dirk Geeraerts</author>
<author>Dirk Speelman</author>
</authors>
<title>The automatic identification of lexical variation between language varieties.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<pages>16--469</pages>
<contexts>
<context position="28945" citStr="Piersman et al. (2010)" startWordPosition="4458" endWordPosition="4461"> example, new language groups for which we could not find suitable corpora before the 2014 edition. This includes, most notably, the cases of Dutch and Flemish or the varieties of French and German which could not be included in the DSL shared task due to the lack of available data. The DSL corpus collection is freely available and can be used as a gold standard for language identification or to train algorithms for other NLP tasks involving similar languages. We would like to use the dataset to investigate, for example, lexical variation between similar languages and varieties as proposed by Piersman et al. (2010) and Soares da Silva (2010) or syntactic variation using annotated data as discussed in Anstein (2013). At present, we are investigating the influence of the length of texts in the discrimination of similar languages. It is a well known fact that the longer texts are, the more likely they are to contain features that allow algorithms to identify their language. However, this variable was not explored within the scope of the DSL shared task and we are using the DSL dataset and the results for this purpose. Another direction that our work may take is the linguistic analysis of the most informati</context>
</contexts>
<marker>Piersman, Geeraerts, Speelman, 2010</marker>
<rawString>Yves Piersman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical variation between language varieties. Natural Language Engineering, 16:469–491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordi Porta</author>
<author>Jos´e-Luis Sancho</author>
</authors>
<title>Using maximum entropy models to discriminate between similar languages and varieties.</title>
<date>2014</date>
<booktitle>In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="23612" citStr="Porta and Sancho, 2014" startWordPosition="3640" endWordPosition="3643">ly: NRCCNRC, RAE, UMich, UniMelb-NLP and QMUL. The best scores were obtained by the NRC-CNRC (Goutte et al., 2014) team which proposed a twostep approach to predict first the language group than the language of each instance. The language group was predicted in a 6-way classification using a probabilistic model similar to a Naive Bayes classifier, and later the method applied SVM classifiers to discriminate within each group: binary for groups B-F and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian). An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called ‘white lists’ inspired by the ‘blacklist’ classifier (Tiedemann and Ljubeˇsi´c, 2012). These lists are word lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc¸on (2006) proposed to discriminate between Malay and Indonesian. 64 Two groups used Information Gain (IG) to select the best features for classification, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit open submissions. The UniMelb-NLP team tried different classification methods and features (including delexical</context>
<context position="25907" citStr="Porta and Sancho, 2014" startWordPosition="3980" endWordPosition="3983">at the linear SVM classifier performs well with word uni-gram language models in comparison to methods using character n-grams. This observation corroborates the findings of previous experiments that rely on words as important features to distinguish similar languages and varieties (Huang and Lee, 2008; Zampieri, 2013) The features and algorithms presented so far, as well as the system paper descriptions, are summarised in table7.11 Team Algorithm Features System Paper NRC-CNRC Prob. Class. and Linear SVM Words 1-2, Char. 2-6 (Goutte et al., 2014) RAE MaxEnt Words 1-2, Char. 1-5, ‘Whitelist’ (Porta and Sancho, 2014) UMich Naive Bayes Words 1-2, Char. 2-6 (IG Feat. Selection) (King et al., 2014) UniMelb-NLP langid.py Words, Char., POS (IG Feat. Selection) (Lui et al., 2014) QMUL Linear SVM Words 1, Char. 1-3 (Purver, 2014) Table 7: Top 5 Systems - Features and Algorithms at a Glance 6 Conclusion Shared tasks are an interesting way of comparing algorithms, computational methods and features using the same dataset. Given what has been presented in this paper, we believe that the DSL shared task filled an important gap in language identification and will allow other researchers to look in more detail at the </context>
</contexts>
<marker>Porta, Sancho, 2014</marker>
<rawString>Jordi Porta and Jos´e-Luis Sancho. 2014. Using maximum entropy models to discriminate between similar languages and varieties. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
</authors>
<title>A simple baseline for discriminating similar language.</title>
<date>2014</date>
<booktitle>In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="20870" citStr="Purver, 2014" startWordPosition="3202" endWordPosition="3203">Spanish) were the most difficult to discriminate, particularly the Spanish varieties. These results also corroborate the findings of previous studies (Zampieri et al., 2013). 63 The QMUL system that was the 5th best system in the closed submission track did not outperform any of the other top 5 systems in groups A, B or C. However, the system did better when distinguishing texts from the two most difficult language groups (D and E), outperforming the UniMelb-NLP submission on two occasions. The simplicity of the approach proposed by the QMUL, which the author describes as ‘a simple baseline’ (Purver, 2014) may be an explanation for the regular performance across different language groups. 4.4 Results Group F To document the problems in the group F (British and American English) dataset we included the results of both the open and closed submissions for this language group in table 6. As previously mentioned, submitting group F results was optional and we did not include these results in the final shared task results. Six out of eight systems decided to submit their predictions as closed submissions and the two groups participating in the open submission track also submitted their group F result</context>
<context position="24863" citStr="Purver, 2014" startWordPosition="3822" endWordPosition="3823">ults were obtained by their own method, the off-the-shelf general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has been widely used for general-purpose language identification and its performance is regarded superior to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hierarchically firstly identifying the language group that a sentence belongs to and subsequently the specific language, achieving performance comparable to the state-of-the-art, but still slightly below the other three systems. The QMUL team (Purver, 2014) proposed a linear SVM classifier using words and characters as features. The author investigated the influence of the cost parameter c (from 1.0 to 100.0), in the classifiers’ performance. The cost parameter c is responsible for the trade-off between maximum margin and classification errors. According to the system description the optimal parameter for this task lies between 30.0 and 50.0. Purver (2014) also notes that the linear SVM classifier performs well with word uni-gram language models in comparison to methods using character n-grams. This observation corroborates the findings of previ</context>
<context position="26117" citStr="Purver, 2014" startWordPosition="4017" endWordPosition="4018">ortant features to distinguish similar languages and varieties (Huang and Lee, 2008; Zampieri, 2013) The features and algorithms presented so far, as well as the system paper descriptions, are summarised in table7.11 Team Algorithm Features System Paper NRC-CNRC Prob. Class. and Linear SVM Words 1-2, Char. 2-6 (Goutte et al., 2014) RAE MaxEnt Words 1-2, Char. 1-5, ‘Whitelist’ (Porta and Sancho, 2014) UMich Naive Bayes Words 1-2, Char. 2-6 (IG Feat. Selection) (King et al., 2014) UniMelb-NLP langid.py Words, Char., POS (IG Feat. Selection) (Lui et al., 2014) QMUL Linear SVM Words 1, Char. 1-3 (Purver, 2014) Table 7: Top 5 Systems - Features and Algorithms at a Glance 6 Conclusion Shared tasks are an interesting way of comparing algorithms, computational methods and features using the same dataset. Given what has been presented in this paper, we believe that the DSL shared task filled an important gap in language identification and will allow other researchers to look in more detail at the problem of discriminating similar languages. Accurate methods for discriminating similar languages can help to improve performance not only in language identification but also in a number of NLP tasks and appli</context>
</contexts>
<marker>Purver, 2014</marker>
<rawString>Matthew Purver. 2014. A simple baseline for discriminating similar language. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bali Ranaivo-Malanc¸on</author>
</authors>
<title>Automatic identification of close languages - case study:</title>
<date>2006</date>
<booktitle>Malay and Indonesian. ECTI Transactions on Computer and Information Technology,</booktitle>
<pages>2--126</pages>
<marker>Ranaivo-Malanc¸on, 2006</marker>
<rawString>Bali Ranaivo-Malanc¸on. 2006. Automatic identification of close languages - case study: Malay and Indonesian. ECTI Transactions on Computer and Information Technology, 2:126–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Augusto Soares da Silva</author>
</authors>
<title>Measuring and parameterizing lexical convergence and divergence between European and Brazilian Portuguese: endo/exogeneousness and foreign and normative influence. Advances in Cognitive Sociolinguistics.</title>
<date>2010</date>
<contexts>
<context position="28972" citStr="Silva (2010)" startWordPosition="4465" endWordPosition="4466">ch we could not find suitable corpora before the 2014 edition. This includes, most notably, the cases of Dutch and Flemish or the varieties of French and German which could not be included in the DSL shared task due to the lack of available data. The DSL corpus collection is freely available and can be used as a gold standard for language identification or to train algorithms for other NLP tasks involving similar languages. We would like to use the dataset to investigate, for example, lexical variation between similar languages and varieties as proposed by Piersman et al. (2010) and Soares da Silva (2010) or syntactic variation using annotated data as discussed in Anstein (2013). At present, we are investigating the influence of the length of texts in the discrimination of similar languages. It is a well known fact that the longer texts are, the more likely they are to contain features that allow algorithms to identify their language. However, this variable was not explored within the scope of the DSL shared task and we are using the DSL dataset and the results for this purpose. Another direction that our work may take is the linguistic analysis of the most informative features in classificati</context>
</contexts>
<marker>Silva, 2010</marker>
<rawString>Augusto Soares da Silva. 2010. Measuring and parameterizing lexical convergence and divergence between European and Brazilian Portuguese: endo/exogeneousness and foreign and normative influence. Advances in Cognitive Sociolinguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liling Tan</author>
<author>Marcos Zampieri</author>
<author>Nikola Ljubesi´c</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Merging comparable data sources for the discrimination of similar languages: The DSL corpus collection.</title>
<date>2014</date>
<booktitle>In Proceedings of The Workshop on Building and Using Comparable Corpora (BUCC), Reykjavik,</booktitle>
<marker>Tan, Zampieri, Ljubesi´c, Tiedemann, 2014</marker>
<rawString>Liling Tan, Marcos Zampieri, Nikola Ljube&amp;quot;si´c, and J¨org Tiedemann. 2014. Merging comparable data sources for the discrimination of similar languages: The DSL corpus collection. In Proceedings of The Workshop on Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Daniel Blanchard</author>
<author>Aoife Cahill</author>
</authors>
<title>A report on the first native language identification shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, GA, USA,</location>
<contexts>
<context position="4183" citStr="Tetreault et al., 2013" startWordPosition="580" endWordPosition="583">rown (2013) reports results on a system trained to recognize more than 1,100 languages 2http://corporavm.uni-koeln.de/vardial/sharedtask.html 58 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 58–67, Dublin, Ireland, August 23 2014. decade in which they were published. Other related shared tasks include the ALTW 2010 multilingual language identification shared task, a general-purpose language identification task containing data from 74 languages (Baldwin and Lui, 2010) and finally the Native Language Identification (NLI) shared task (Tetreault et al., 2013) where participants were provided English essays written by foreign students of 11 different mother tongues (Blanchard et al., 2013). Participants had to train their systems to identify the native language of the writer of each text. 2 Related Work Among the first studies to investigate the question of discriminating between similar languages is the study published by Ranaivo-Malanc¸on (2006). The author presents a semi-supervised model to distinguish between Indonesian and Malay, two closely related languages from the Austronesian family represented in the DSL shared task. The study uses the </context>
</contexts>
<marker>Tetreault, Blanchard, Cahill, 2013</marker>
<rawString>Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013. A report on the first native language identification shared task. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, Atlanta, GA, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
<author>Nikola Ljubesi´c</author>
</authors>
<title>Efficient discrimination between closely related languages.</title>
<date>2012</date>
<journal>Omar F Zaidan</journal>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>2619--2634</pages>
<location>Mumbai, India.</location>
<marker>Tiedemann, Ljubesi´c, 2012</marker>
<rawString>J¨org Tiedemann and Nikola Ljube&amp;quot;si´c. 2012. Efficient discrimination between closely related languages. In Proceedings of COLING 2012, pages 2619–2634, Mumbai, India. Omar F Zaidan and Chris Callison-Burch. 2013. Arabic dialect identification. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
<author>Binyam Gebrekidan Gebre</author>
</authors>
<title>Automatic identification of language varieties: The case of Portuguese.</title>
<date>2012</date>
<booktitle>In Proceedings of KONVENS2012,</booktitle>
<pages>233--237</pages>
<location>Vienna, Austria.</location>
<contexts>
<context position="1791" citStr="Zampieri and Gebre, 2012" startWordPosition="235" endWordPosition="238">een trained to discriminate between more languages1, they still struggle to discriminate between similar languages such as Croatian and Serbian or Malay and Indonesian. From an NLP point of view, the difficulty systems face when discriminating between closely related languages is similar to the problem of discriminating between standard national language varieties (e.g. American English and British English or Brazilian Portuguese and European Portuguese), henceforth varieties. Recent studies show that language varieties can be discriminated automatically using words or characters as features (Zampieri and Gebre, 2012; Lui and Cook, 2013) . However, due to performance limitations, state-of-the-art general-purpose language identification systems do not distinguish texts from different national varieties, modelling pluricentric languages as unique classes. To evaluate how state-of-the-art systems perform in identifying similar languages and varieties, we decided to organize the Discriminating between Similar Languages (DSL)2 shared task. This shared task was organized within the scope of the workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial) in the 2014 edition of COLING. T</context>
<context position="6087" citStr="Zampieri and Gebre (2012)" startWordPosition="865" endWordPosition="868">it to Bosnian, Croatian and Serbian texts. The study reports significantly higher performance compared to general purpose language identification methods. The methods applied to discriminate between texts from different language varieties and dialects are similar to those applied to similar languages3. One of the methods proposed to identify language varieties is by Huang and Lee (2008). This study presented a bag-of-words approach to classify Chinese texts from the mainland and Taiwan with results of up to 92% accuracy. Another study that focused on language varieties is the one published by Zampieri and Gebre (2012). In this study, the authors proposed a log-likelihood estimation method along with Laplace smoothing to identify two varieties of Portuguese (Brazilian and European). Their approach was trained and tested in a binary setting using journalistic texts with accuracy results above 99.5% for character n-grams. The algorithm was later adapted to classify Spanish texts using not only the classical word and character n-grams but also POS distribution (Zampieri et al., 2013). The aforementioned study by Lui and Cook (2013) investigates computational methods to discriminate between texts from three dif</context>
<context position="10926" citStr="Zampieri and Gebre, 2012" startWordPosition="1617" endWordPosition="1620">llection, randomly sampled sentences from journalistic corpora (and corpora collections) were selected for each of the 13 classes. Journalistic corpora were preferred because they represent standard language, which is an important factor to be considered when working with language varieties. Other data sources (e.g. Wikipedia) do not make any distinction between language varieties and they are therefore not suitable for the purpose of the shared task. A number of studies mentioned in the related work section use journalistic texts for similar reasons (Huang and Lee, 2008; Grouin et al., 2010; Zampieri and Gebre, 2012) Given what has been said in this section, we consider the collection to be a suitable comparable corpora from this task, which was compiled to avoid bias in classification towards source, register and topics. The 5We considered a token as orthographic units delimited by white spaces. 6http://www.loc.gov/standards/iso639-2/php/English_list.php 60 DSL corpus collection was distributed in tab delimited format; the first column contains a sentence in the language/variety, the second column states its group and the last column refers to its language code.7 3.1.1 Problems with Group F There are no </context>
</contexts>
<marker>Zampieri, Gebre, 2012</marker>
<rawString>Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic identification of language varieties: The case of Portuguese. In Proceedings of KONVENS2012, pages 233–237, Vienna, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
<author>Binyam Gebrekidan Gebre</author>
<author>Sascha Diwersy</author>
</authors>
<title>N-gram language models and POS distribution for the identification of Spanish varieties.</title>
<date>2013</date>
<booktitle>In Proceedings of TALN2013,</booktitle>
<pages>580--587</pages>
<location>Sable d’Olonne, France.</location>
<contexts>
<context position="6558" citStr="Zampieri et al., 2013" startWordPosition="935" endWordPosition="938">nland and Taiwan with results of up to 92% accuracy. Another study that focused on language varieties is the one published by Zampieri and Gebre (2012). In this study, the authors proposed a log-likelihood estimation method along with Laplace smoothing to identify two varieties of Portuguese (Brazilian and European). Their approach was trained and tested in a binary setting using journalistic texts with accuracy results above 99.5% for character n-grams. The algorithm was later adapted to classify Spanish texts using not only the classical word and character n-grams but also POS distribution (Zampieri et al., 2013). The aforementioned study by Lui and Cook (2013) investigates computational methods to discriminate between texts from three different English varieties (Canadian, Australian and British) across different domains. The authors state that the results obtained suggest that each variety contains characteristics that are consistent across multiple domains, which enables algorithms to distinguish them regardless of the data source. Zaidan and Callison-Burch (2013) propose computational methods for the identification of Arabic language varieties4 using character and word n-grams. The authors built t</context>
<context position="20430" citStr="Zampieri et al., 2013" startWordPosition="3125" endWordPosition="3128">to E Accuracy - Top 5 Systems Distinguishing between languages from group A (Bosnian, Croatian and Serbian), the only group containing 3 languages, proved to be a challenging task as discussed in previous research (Ljubeˇsi´c et al., 2007; Tiedemann and Ljubeˇsi´c, 2012). The best result was again obtained by the NRC-CNRC group with 93.5% accuracy. The groups containing texts written in different language varieties, namely D (Portuguese) and E (Spanish) were the most difficult to discriminate, particularly the Spanish varieties. These results also corroborate the findings of previous studies (Zampieri et al., 2013). 63 The QMUL system that was the 5th best system in the closed submission track did not outperform any of the other top 5 systems in groups A, B or C. However, the system did better when distinguishing texts from the two most difficult language groups (D and E), outperforming the UniMelb-NLP submission on two occasions. The simplicity of the approach proposed by the QMUL, which the author describes as ‘a simple baseline’ (Purver, 2014) may be an explanation for the regular performance across different language groups. 4.4 Results Group F To document the problems in the group F (British and Am</context>
</contexts>
<marker>Zampieri, Gebre, Diwersy, 2013</marker>
<rawString>Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and POS distribution for the identification of Spanish varieties. In Proceedings of TALN2013, pages 580–587, Sable d’Olonne, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
</authors>
<title>Using bag-of-words to distinguish similar languages: How efficient are they?</title>
<date>2013</date>
<booktitle>In Proceedings of the 14th IEEE International Symposium on Computational Intelligence and Informatics (CINTI2013),</booktitle>
<pages>37--41</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="25604" citStr="Zampieri, 2013" startWordPosition="3934" endWordPosition="3935">rameter c (from 1.0 to 100.0), in the classifiers’ performance. The cost parameter c is responsible for the trade-off between maximum margin and classification errors. According to the system description the optimal parameter for this task lies between 30.0 and 50.0. Purver (2014) also notes that the linear SVM classifier performs well with word uni-gram language models in comparison to methods using character n-grams. This observation corroborates the findings of previous experiments that rely on words as important features to distinguish similar languages and varieties (Huang and Lee, 2008; Zampieri, 2013) The features and algorithms presented so far, as well as the system paper descriptions, are summarised in table7.11 Team Algorithm Features System Paper NRC-CNRC Prob. Class. and Linear SVM Words 1-2, Char. 2-6 (Goutte et al., 2014) RAE MaxEnt Words 1-2, Char. 1-5, ‘Whitelist’ (Porta and Sancho, 2014) UMich Naive Bayes Words 1-2, Char. 2-6 (IG Feat. Selection) (King et al., 2014) UniMelb-NLP langid.py Words, Char., POS (IG Feat. Selection) (Lui et al., 2014) QMUL Linear SVM Words 1, Char. 1-3 (Purver, 2014) Table 7: Top 5 Systems - Features and Algorithms at a Glance 6 Conclusion Shared tasks</context>
</contexts>
<marker>Zampieri, 2013</marker>
<rawString>Marcos Zampieri. 2013. Using bag-of-words to distinguish similar languages: How efficient are they? In Proceedings of the 14th IEEE International Symposium on Computational Intelligence and Informatics (CINTI2013), pages 37–41, Budapest, Hungary.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
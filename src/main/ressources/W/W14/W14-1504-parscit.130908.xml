<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002648">
<title confidence="0.994426">
Extractive Summarization using Continuous Vector Space Models
</title>
<author confidence="0.997463">
Mikael K˚ageb¨ack, Olof Mogren, Nina Tahmasebi, Devdatt Dubhashi
</author>
<affiliation confidence="0.996361">
Computer Science &amp; Engineering
Chalmers University of Technology
</affiliation>
<address confidence="0.875493">
SE-412 96, G¨oteborg
</address>
<email confidence="0.969433">
{kageback, mogren, ninat, dubhashi}@chalmers.se
</email>
<sectionHeader confidence="0.996925" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999624">
Automatic summarization can help users
extract the most important pieces of infor-
mation from the vast amount of text digi-
tized into electronic form everyday. Cen-
tral to automatic summarization is the no-
tion of similarity between sentences in
text. In this paper we propose the use of
continuous vector representations for se-
mantically aware representations of sen-
tences as a basis for measuring similar-
ity. We evaluate different compositions
for sentence representation on a standard
dataset using the ROUGE evaluation mea-
sures. Our experiments show that the eval-
uated methods improve the performance
of a state-of-the-art summarization frame-
work and strongly indicate the benefits
of continuous word vector representations
for automatic summarization.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962133333333">
The goal of summarization is to capture the im-
portant information contained in large volumes of
text, and present it in a brief, representative, and
consistent summary. A well written summary can
significantly reduce the amount of work needed to
digest large amounts of text on a given topic. The
creation of summaries is currently a task best han-
dled by humans. However, with the explosion of
available textual data, it is no longer financially
possible, or feasible, to produce all types of sum-
maries by hand. This is especially true if the sub-
ject matter has a narrow base of interest, either due
to the number of potential readers or the duration
during which it is of general interest. A summary
describing the events of World War II might for
instance be justified to create manually, while a
summary of all reviews and comments regarding
a certain version of Windows might not. In such
cases, automatic summarization is a way forward.
In this paper we introduce a novel application
of continuous vector representations to the prob-
lem of multi-document summarization. We evalu-
ate different compositions for producing sentence
representations based on two different word em-
beddings on a standard dataset using the ROUGE
evaluation measures. Our experiments show that
the evaluated methods improve the performance of
a state-of-the-art summarization framework which
strongly indicate the benefits of continuous word
vector representations for this tasks.
</bodyText>
<sectionHeader confidence="0.995573" genericHeader="introduction">
2 Summarization
</sectionHeader>
<bodyText confidence="0.997633428571428">
There are two major types of automatic summa-
rization techniques, extractive and abstractive. Ex-
tractive summarization systems create summaries
using representative sentences chosen from the in-
put while abstractive summarization creates new
sentences and is generally considered a more dif-
ficult problem.
</bodyText>
<figureCaption confidence="0.983044">
Figure 1: Illustration of Extractive Multi-
Document Summarization.
</figureCaption>
<bodyText confidence="0.998886142857143">
For this paper we consider extractive multi-
document summarization, that is, sentences are
chosen for inclusion in a summary from a set of
documents D. Typically, extractive summariza-
tion techniques can be divided into two compo-
nents, the summarization framework and the sim-
ilarity measures used to compare sentences. Next
</bodyText>
<page confidence="0.998637">
31
</page>
<note confidence="0.997178">
Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 31–39,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99883">
we present the algorithm used for the framework
and in Sec. 2.2 we discuss a typical sentence sim-
ilarity measure, later to be used as a baseline.
</bodyText>
<subsectionHeader confidence="0.929738">
2.1 Submodular Optimization
</subsectionHeader>
<bodyText confidence="0.999737">
Lin and Bilmes (2011) formulated the problem of
extractive summarization as an optimization prob-
lem using monotone nondecreasing submodular
set functions. A submodular function F on the
set of sentences V satisfies the following property:
for any A C_ B C_ V \{v}, F(A+ {v}) − F(A) ≥
F(B + {v}) − F(B) where v E V . This is called
the diminishing returns property and captures the
intuition that adding a sentence to a small set of
sentences (i.e., summary) makes a greater contri-
bution than adding a sentence to a larger set. The
aim is then to find a summary that maximizes di-
versity of the sentences and the coverage of the in-
put text. This objective function can be formulated
as follows:
</bodyText>
<equation confidence="0.984739">
F(S) = L(S) + λR(S)
</equation>
<bodyText confidence="0.9999444">
where S is the summary, L(S) is the coverage of
the input text, R(S) is a diversity reward function.
The λ is a trade-off coefficient that allows us to
define the importance of coverage versus diversity
of the summary. In general, this kind of optimiza-
tion problem is NP-hard, however, if the objective
function is submodular there is a fast scalable al-
gorithm that returns an approximation with a guar-
antee. In the work of Lin and Bilmes (2011) a sim-
ple submodular function is chosen:
</bodyText>
<equation confidence="0.994414333333333">
L(S) = X Xmin{ Sim(i, j), α X Sim(i, j)}
i∈V j∈S j∈V
(1)
</equation>
<bodyText confidence="0.99963075">
The first argument measures similarity between
sentence i and the summary S, while the sec-
ond argument measures similarity between sen-
tence i and the rest of the input V . Sim(i, j) is
the similarity between sentence i and sentence j
and 0 &lt; α &lt; 1 is a threshold coefficient. The di-
versity reward function R(S) can be found in (Lin
and Bilmes, 2011).
</bodyText>
<subsectionHeader confidence="0.99851">
2.2 Traditional Similarity Measure
</subsectionHeader>
<bodyText confidence="0.99992125">
Central to most extractive summarization sys-
tems is the use of sentence similarity measures
(Sim(i, j) in Eq. 1). Lin and Bilmes measure
similarity between sentences by representing each
sentence using tf-idf (Salton and McGill, 1986)
vectors and measuring the cosine angle between
vectors. Each sentence is represented by a word
vector w = (w1, ... , wN) where N is the size of
the vocabulary. Weights wki correspond to the tf-
idf value of word k in the sentence i. The weights
Sim(i, j) used in the L function in Eq. 1 are found
using the following similarity measure.
</bodyText>
<equation confidence="0.9996975">
P tfw,i X tfw,j X idf2w
w∈i
r Ptf2w i X idfw A
tf2w,j X idfww∈i
</equation>
<bodyText confidence="0.992359777777778">
(2)
where tfw,i and tfw,j are the number of occur-
rences of w in sentence i and j, and idfw is the
inverse document frequency (idf) of w.
In order to have a high similarity between sen-
tences using the above measure, two sentences
must have an overlap of highly scored tf-idf words.
The overlap must be exact to count towards the
similarity, e.g, the terms The US President and
Barack Obama in different sentences will not add
towards the similarity of the sentences. To cap-
ture deeper similarity, in this paper we will inves-
tigate the use of continuous vector representations
for measuring similarity between sentences. In the
next sections we will describe the basics needed
for creating continuous vector representations and
methods used to create sentence representations
that can be used to measure sentence similarity.
</bodyText>
<sectionHeader confidence="0.971038" genericHeader="method">
3 Background on Deep Learning
</sectionHeader>
<bodyText confidence="0.999935411764706">
Deep learning (Hinton et al., 2006; Bengio, 2009)
is a modern interpretation of artificial neural net-
works (ANN), with an emphasis on deep network
architectures. Deep learning can be used for chal-
lenging problems like image and speech recogni-
tion (Krizhevsky et al., 2012; Graves et al., 2013),
as well as language modeling (Mikolov et al.,
2010), and in all cases, able to achieve state-of-
the-art results.
Inspired by the brain, ANNs use a neuron-like
construction as their primary computational unit.
The behavior of a neuron is entirely controlled by
its input weights. Hence, the weights are where
the information learned by the neuron is stored.
More precisely the output of a neuron is computed
as the weighted sum of its inputs, and squeezed
into the interval [0, 1] using a sigmoid function:
</bodyText>
<equation confidence="0.9970008">
yi = 9(θTi x) (3)
1
9(z) = (4)
1 + e−z
Sim(i, j) =
</equation>
<page confidence="0.987288">
32
</page>
<figure confidence="0.929889125">
Input Hidden Output Input Coding Reconstruction
layer layer layer layer layer layer
x1 y3 x1 x′1
x2 x2 x′
x3 x3 2
x4 x4 x′3
x′
4
</figure>
<figureCaption confidence="0.981445">
Figure 2: FFNN with four input neurons, one hid-
</figureCaption>
<bodyText confidence="0.973662777777778">
den layer, and 1 output neuron. This type of ar-
chitecture is appropriate for binary classification
of some data x ∈ R4, however depending on the
complexity of the input, the number and size of the
hidden layers should be scaled accordingly.
where BZ are the weights associated with neuron i
and x is the input. Here the sigmoid function (g) is
chosen to be the logistic function, but it may also
be modeled using other sigmoid shaped functions,
e.g. the hyperbolic tangent function.
The neurons can be organized in many differ-
ent ways. In some architectures, loops are permit-
ted. These are referred to as recurrent neural net-
works. However, all networks considered here are
non-cyclic topologies. In the rest of this section
we discuss a few general architectures in more de-
tail, which will later be employed in the evaluated
models.
</bodyText>
<subsectionHeader confidence="0.994993">
3.1 Feed Forward Neural Network
</subsectionHeader>
<bodyText confidence="0.999881882352941">
A feed forward neural network (FFNN) (Haykin,
2009) is a type of ANN where the neurons are
structured in layers, and only connections to sub-
sequent layers are allowed, see Fig 2. The algo-
rithm is similar to logistic regression using non-
linear terms. However, it does not rely on the
user to choose the non-linear terms needed to fit
the data, making it more adaptable to changing
datasets. The first layer in a FFNN is called the
input layer, the last layer is called the output layer,
and the interim layers are called hidden layers.
The hidden layers are optional but necessary to fit
complex patterns.
Training is achieved by minimizing the network
error (E). How E is defined differs between dif-
ferent network architectures, but is in general a
differentiable function of the produced output and
</bodyText>
<figureCaption confidence="0.84986975">
Figure 3: The figure shows an auto-encoder that
compresses four dimensional data into a two di-
mensional code. This is achieved by using a bot-
tleneck layer, referred to as a coding layer.
</figureCaption>
<bodyText confidence="0.9999413">
the expected output. In order to minimize this
function the gradient ∂E ∂Θfirst needs to be calcu-
lated, where Θ is a matrix of all parameters, or
weights, in the network. This is achieved using
backpropagation (Rumelhart et al., 1986). Sec-
ondly, these gradients are used to minimize E us-
ing e.g. gradient descent. The result of this pro-
cesses is a set of weights that enables the network
to do the desired input-output mapping, as defined
by the training data.
</bodyText>
<subsectionHeader confidence="0.999664">
3.2 Auto-Encoder
</subsectionHeader>
<bodyText confidence="0.99998705882353">
An auto-encoder (AE) (Hinton and Salakhutdinov,
2006), see Fig. 3, is a type of FFNN with a topol-
ogy designed for dimensionality reduction. The
input and the output layers in an AE are identical,
and there is at least one hidden bottleneck layer
that is referred to as the coding layer. The net-
work is trained to reconstruct the input data, and
if it succeeds this implies that all information in
the data is necessarily contained in the compressed
representation of the coding layer.
A shallow AE, i.e. an AE with no extra hid-
den layers, will produce a similar code as princi-
pal component analysis. However, if more layers
are added, before and after the coding layer, non-
linear manifolds can be found. This enables the
network to compress complex data, with minimal
loss of information.
</bodyText>
<subsectionHeader confidence="0.998876">
3.3 Recursive Neural Network
</subsectionHeader>
<bodyText confidence="0.99984975">
A recursive neural network (RvNN), see Fig. 4,
first presented by Socher et al. (2010), is a type of
feed forward neural network that can process data
through an arbitrary binary tree structure, e.g. a
</bodyText>
<page confidence="0.998205">
33
</page>
<figureCaption confidence="0.959237">
Figure 4: The recursive neural network architec-
</figureCaption>
<bodyText confidence="0.957873125">
ture makes it possible to handle variable length in-
put data. By using the same dimensionality for all
layers, arbitrary binary tree structures can be re-
cursively processed.
binary parse tree produced by linguistic parsing of
a sentence. This is achieved by enforcing weight
constraints across all nodes and restricting the out-
put of each node to have the same dimensionality
as its children.
The input data is placed in the leaf nodes of
the tree, and the structure of this tree is used to
guide the recursion up to the root node. A com-
pressed representation is calculated recursively at
each non-terminal node in the tree, using the same
weight matrix at each node. More precisely, the
following formulas can be used:
</bodyText>
<equation confidence="0.952421">
zp = θTp [xl; xr] (5a)
yp = g(zp) (5b)
</equation>
<bodyText confidence="0.999922875">
where yp is the computed parent state of neuron
p, and zp the induced field for the same neuron.
[xl; xr] is the concatenation of the state belonging
to the right and left sibling nodes. This process re-
sults in a fixed length representation for hierarchi-
cal data of arbitrary length. Training of the model
is done using backpropagation through structure,
introduced by Goller and Kuchler (1996).
</bodyText>
<sectionHeader confidence="0.986563" genericHeader="method">
4 Word Embeddings
</sectionHeader>
<bodyText confidence="0.999934625">
Continuous distributed vector representation of
words, also referred to as word embeddings, was
first introduced by Bengio et al. (2003). A word
embedding is a continuous vector representation
that captures semantic and syntactic information
about a word. These representations can be used
to unveil dimensions of similarity between words,
e.g. singular or plural.
</bodyText>
<subsectionHeader confidence="0.998158">
4.1 Collobert &amp; Weston
</subsectionHeader>
<bodyText confidence="0.999955">
Collobert and Weston (2008) introduce an efficient
method for computing word embeddings, in this
work referred to as CW vectors. This is achieved
firstly, by scoring a valid n-gram (x) and a cor-
rupted n-gram (R) (where the center word has been
randomly chosen), and secondly, by training the
network to distinguish between these two n-grams.
This is done by minimizing the hinge loss
</bodyText>
<equation confidence="0.913168">
max(0, 1 − s(x) + s(R)) (6)
</equation>
<bodyText confidence="0.999895333333333">
where s is the scoring function, i.e. the output of
a FFNN that maps between the word embeddings
of an n-gram to a real valued score. Both the pa-
rameters of the scoring function and the word em-
beddings are learned in parallel using backpropa-
gation.
</bodyText>
<subsectionHeader confidence="0.974949">
4.2 Continuous Skip-gram
</subsectionHeader>
<bodyText confidence="0.999623875">
A second method for computing word embeddings
is the Continuous Skip-gram model, see Fig. 5, in-
troduced by Mikolov et al. (2013a). This model is
used in the implementation of their word embed-
dings tool Word2Uec. The model is trained to pre-
dict the context surrounding a given word. This is
accomplished by maximizing the objective func-
tion
</bodyText>
<equation confidence="0.963723">
log p(wt+j|wt) (7)
</equation>
<bodyText confidence="0.999842">
where T is the number of words in the training
set, and c is the length of the training context.
The probability p(wt+j|wt) is approximated using
the hierarchical softmax introduced by Bengio et
al. (2002) and evaluated in a paper by Morin and
Bengio (2005).
</bodyText>
<sectionHeader confidence="0.992958" genericHeader="method">
5 Phrase Embeddings
</sectionHeader>
<bodyText confidence="0.999854142857143">
Word embeddings have proven useful in many nat-
ural language processing (NLP) tasks. For sum-
marization, however, sentences need to be com-
pared. In this section we present two different
methods for deriving phrase embeddings, which
in Section 5.3 will be used to compute sentence to
sentence similarities.
</bodyText>
<subsectionHeader confidence="0.92348">
5.1 Vector addition
</subsectionHeader>
<bodyText confidence="0.999511333333333">
The simplest way to represent a sentence is to
consider it as the sum of all words without re-
garding word orders. This was considered by
</bodyText>
<figure confidence="0.992056342105263">
Input Root
layer layer
x1
x2
x3
y
�
−c≤j≤c,j�0
1 T
T t=1
34
Input
layer
Root
layer
Output
layer
wt
wt−2
wt−1
wt+1
wt+2
projection
layer
Input
layer
Output
layer
x1
x2
x3
θe θd
x 1
x 2
x 3
′
′
′
</figure>
<figureCaption confidence="0.999556">
Figure 5: The continuous Skip-gram model. Us-
</figureCaption>
<bodyText confidence="0.7592172">
ing the input word (wt) the model tries to predict
which words will be in its context (wt±c).
Mikolov et al. (2013b) for representing short
phrases. The model is expressed by the following
equation:
</bodyText>
<equation confidence="0.9809255">
�xp = xw (8)
x,,,∈{sentence}
</equation>
<bodyText confidence="0.99976">
where xp is a phrase embedding, and xw is a word
embedding. We use this method for computing
phrase embeddings as a baseline in our experi-
ments.
</bodyText>
<subsectionHeader confidence="0.999844">
5.2 Unfolding Recursive Auto-encoder
</subsectionHeader>
<bodyText confidence="0.99458465">
The second model is more sophisticated, tak-
ing into account also the order of the words
and the grammar used. An unfolding recursive
auto-encoder (RAE) is used to derive the phrase
embedding on the basis of a binary parse tree.
The unfolding RAE was introduced by Socher et
al. (2011) and uses two RvNNs, one for encoding
the compressed representations, and one for de-
coding them to recover the original sentence, see
Figure 6. The network is subsequently trained by
minimizing the reconstruction error.
Forward propagation in the network is done by
recursively applying Eq. 5a and 5b for each triplet
in the tree in two phases. First, starting at the cen-
ter node (root of the tree) and recursively pulling
the data from the input. Second, again starting
at the center node, recursively pushing the data
towards the output. Backpropagation is done in
a similar manner using backpropagation through
structure (Goller and Kuchler, 1996).
</bodyText>
<figureCaption confidence="0.736342">
Figure 6: The structure of an unfolding RAE, on
</figureCaption>
<bodyText confidence="0.73170675">
a three word phrase ([x1, x2, x3]). The weight ma-
trix θe is used to encode the compressed represen-
tations, while θd is used to decode the representa-
tions and reconstruct the sentence.
</bodyText>
<subsectionHeader confidence="0.998052">
5.3 Measuring Similarity
</subsectionHeader>
<bodyText confidence="0.999938714285714">
Phrase embeddings provide semantically aware
representations for sentences. For summarization,
we need to measure the similarity between two
representations and will make use of the following
two vector similarity measures. The first similar-
ity measure is the cosine similarity, transformed to
the interval of [0, 1]
</bodyText>
<equation confidence="0.979805">
Sim(i, j) = xTX� + 1 /2 (9)
C11xj1111xj11
</equation>
<bodyText confidence="0.999920666666667">
where x denotes a phrase embedding The second
similarity is based on the complement of the Eu-
clidean distance and computed as:
</bodyText>
<equation confidence="0.824576">
(10)
</equation>
<sectionHeader confidence="0.998221" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99994525">
In order to evaluate phrase embeddings for sum-
marization we conduct several experiments and
compare different phrase embeddings with tf-idf
based vectors.
</bodyText>
<subsectionHeader confidence="0.989631">
6.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999898714285714">
Seven different configuration were evaluated. The
first configuration provides us with a baseline and
is denoted Original for the Lin-Bilmes method
described in Sec. 2.1. The remaining configura-
tions comprise selected combinations of word em-
beddings, phrase embeddings, and similarity mea-
sures.
</bodyText>
<equation confidence="0.934053">
1
Sim(i, j) = 1
�11 xj − xi 112
V/
max 11 xk − xn 112
k,n
</equation>
<page confidence="0.985093">
35
</page>
<bodyText confidence="0.999874294117647">
The first group of configurations are based on
vector addition using both Word2Vec and CW vec-
tors. These vectors are subsequently compared us-
ing both cosine similarity and Euclidean distance.
The second group of configurations are built upon
recursive auto-encoders using CW vectors and are
also compared using cosine similarity as well as
Euclidean distance.
The methods are named according to:
VectorType EmbeddingMethodSimilarityMethod,
e.g. W2V_AddCos for Word2Vec vectors com-
bined using vector addition and compared using
cosine similarity.
To get an upper bound for each ROUGE score
an exhaustive search were performed, where each
possible pair of sentences were evaluated, and
maximized w.r.t the ROUGE score.
</bodyText>
<subsectionHeader confidence="0.997335">
6.2 Dataset and Evaluation
</subsectionHeader>
<bodyText confidence="0.997515535714286">
The Opinosis dataset (Ganesan et al., 2010) con-
sists of short user reviews in 51 different top-
ics. Each of these topics contains between 50 and
575 sentences and are a collection of user reviews
made by different authors about a certain charac-
teristic of a hotel, car or a product (e.g. ”Loca-
tion of Holiday Inn, London” and ”Fonts, Ama-
zon Kindle”). The dataset is well suited for multi-
document summarization (each sentence is con-
sidered its own document), and includes between
4 and 5 gold-standard summaries (not sentences
chosen from the documents) created by human au-
thors for each topic.
Each summary is evaluated with ROUGE, that
works by counting word overlaps between gener-
ated summaries and gold standard summaries. Our
results include R-1, R-2, and R-SU4, which counts
matches in unigrams, bigrams, and skip-bigrams
respectively. The skip-bigrams allow four words
in between (Lin, 2004).
The measures reported are recall (R), precision
(P), and F-score (F), computed for each topic indi-
vidually and averaged. Recall measures what frac-
tion of a human created gold standard summary
that is captured, and precision measures what frac-
tion of the generated summary that is in the gold
standard. F-score is a standard way to combine
recall and precision, computed as F = 2 P ∗R
</bodyText>
<equation confidence="0.50993">
P +R.
</equation>
<subsectionHeader confidence="0.993073">
6.3 Implementation
</subsectionHeader>
<bodyText confidence="0.999729846153846">
All results were obtained by running an imple-
mentation of Lin-Bilmes submodular optimization
summarizer, as described in Sec. 2.1. Also, we
have chosen to fix the length of the summaries
to two sentences because the length of the gold-
standard summaries are typically around two sen-
tences. The CW vectors used were trained by
Turian et al. (2010)1, and the Word2Vec vectors
by Mikolov et al. (2013b)2. The unfolding RAE
used is based on the implementation by Socher
et al. (2011)3, and the parse trees for guiding
the recursion was generated using the Stanford
Parser (Klein and Manning, 2003)4.
</bodyText>
<subsectionHeader confidence="0.651832">
6.4 Results
</subsectionHeader>
<bodyText confidence="0.999983310344827">
The results from the ROUGE evaluation are com-
piled in Table 1. We find for all measures (recall,
precision, and F-score), that the phrase embed-
dings outperform the original Lin-Bilmes. For re-
call, we find that CW_AddCos achieves the high-
est result, while for precision and F-score the
CW_AddEuc perform best. These results are con-
sistent for all versions of ROUGE scores reported
(1, 2 and SU4), providing a strong indication for
phrase embeddings in the context of automatic
summarization.
Unfolding RAE on CW vectors and vector ad-
dition on W2V vectors gave comparable results
w.r.t. each other, generally performing better than
original Linn-Bilmes but not performing as well as
vector addition of CW vectors.
The results denoted OPT in Table 1 describe
the upper bound score, where each row repre-
sents optimal recall and F-score respectively. The
best results are achieved for R-1 with a maxi-
mum recall of 57.86%. This is a consequence of
hand created gold standard summaries used in the
evaluation, that is, we cannot achieve full recall
or F-score when the sentences in the gold stan-
dard summaries are not taken from the underly-
ing documents and thus, they can never be fully
matched using extractive summarization. R-2 and
SU4 have lower maximum recall and F-score, with
22.9% and 29.5% respectively.
</bodyText>
<subsectionHeader confidence="0.963919">
6.5 Discussion
</subsectionHeader>
<bodyText confidence="0.9999506">
The results of this paper show great potential for
employing word and phrase embeddings in sum-
marization. We believe that by using embeddings
we move towards more semantically aware sum-
marization systems. In the future, we anticipate
</bodyText>
<footnote confidence="0.99998675">
1http://metaoptimize.com/projects/wordreprs/
2https://code.google.com/p/word2vec/
3http://nlp.stanford.edu/ socherr/codeRAEVectorsNIPS2011.zip
4http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<page confidence="0.998474">
36
</page>
<tableCaption confidence="0.940253333333333">
Table 1: ROUGE scores for summaries using dif-
ferent similarity measures. OPT constitutes the
optimal ROUGE scores on this dataset.
</tableCaption>
<table confidence="0.999780424242424">
ROUGE-1
R P F
OPTR 57.86 21.96 30.28
OPTF 45.93 48.84 46.57
CW_RAECos 27.37 19.89 22.00
CW_RAEEu,- 29.25 19.77 22.62
CW_AddCos 34.72 11.75 17.16
CW_AddEu,- 29.12 22.75 24.88
W2V_AddCos 30.86 16.81 20.93
W2V_AddEu,- 28.71 16.67 20.75
Original 25.82 19.58 20.57
ROUGE-2
R P F
OPTR 22.96 12.31 15.33
OPTF 20.42 19.94 19.49
CW_RAECos 4.68 3.18 3.58
CW_RAEEu,- 4.82 3.24 3.67
CW_AddCos 5.89 1.81 2.71
CW_AddEu,- 5.12 3.60 4.10
W2V_AddCos 5.71 3.08 3.82
W2V_AddEu,- 3.86 1.95 2.54
Original 3.92 2.50 2.87
ROUGE-SU4
R P F
OPTR 29.50 13.53 17.70
OPTF 23.17 26.50 23.70
CW_RAECos 9.61 6.23 6.95
CW_RAEEu,- 9.95 6.17 7.04
CW_AddCos 12.38 3.27 5.03
CW_AddEu,- 10.54 7.59 8.35
W2V_AddCos 11.94 5.52 7.12
W2V_AddEu,- 9.78 4.69 6.15
Original 9.15 6.74 6.73
</table>
<bodyText confidence="0.999965765957447">
improvements for the field of automatic summa-
rization as the quality of the word vectors im-
prove and we find enhanced ways of composing
and comparing the vectors.
It is interesting to compare the results of dif-
ferent composition techniques on the CW vec-
tors, where vector addition surprisingly outper-
forms the considerably more sophisticated unfold-
ing RAE. However, since the unfolding RAE uses
syntactic information, this may be a result of using
a dataset consisting of low quality text.
In the interest of comparing word embeddings,
results using vector addition and cosine similarity
were computed based on both CW and Word2Vec
vectors. Supported by the achieved results CW
vectors seems better suited for sentence similari-
ties in this setting.
An issue we encountered with using precom-
puted word embeddings was their limited vocab-
ulary, in particular missing uncommon (or com-
mon incorrect) spellings. This problem is par-
ticularly pronounced on the evaluated Opinosis
dataset, since the text is of low quality. Future
work is to train word embeddings on a dataset used
for summarization to better capture the specific se-
mantics and vocabulary.
The optimal R-1 scores are higher than R-2 and
SU4 (see Table 1) most likely because the score ig-
nores word order and considers each sentence as a
set of words. We come closest to the optimal score
for R-1, where we achieve 60% of maximal recall
and 49% of F-score. Future work is to investigate
why we achieve a much lower recall and F-score
for the other ROUGE scores.
Our results suggest that the phrase embeddings
capture the kind of information that is needed for
the summarization task. The embeddings are the
underpinnings of the decisions on which sentences
that are representative of the whole input text, and
which sentences that would be redundant when
combined in a summary. However, the fact that
we at most achieve 60% of maximal recall sug-
gests that the phrase embeddings are not complete
w.r.t summarization and might benefit from being
combined with other similarity measures that can
capture complementary information, for example
using multiple kernel learning.
</bodyText>
<sectionHeader confidence="0.999878" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.9998824">
To the best of our knowledge, continuous vector
space models have not previously been used in
summarization tasks. Therefore, we split this sec-
tion in two, handling summarization and continu-
ous vector space models separately.
</bodyText>
<subsectionHeader confidence="0.998812">
7.1 Continuous Vector Space Models
</subsectionHeader>
<bodyText confidence="0.997216">
Continuous distributed vector representation of
words was first introduced by Bengio et al. (2003).
</bodyText>
<page confidence="0.997918">
37
</page>
<bodyText confidence="0.99998975">
They employ a FFNN, using a window of words
as input, and train the model to predict the next
word. This is computed using a big softmax layer
that calculate the probabilities for each word in the
vocabulary. This type of exhaustive estimation is
necessary in some NLP applications, but makes
the model heavy to train.
If the sole purpose of the model is to derive
word embeddings this can be exploited by using
a much lighter output layer. This was suggested
by Collobert and Weston (2008), which swapped
the heavy softmax against a hinge loss function.
The model works by scoring a set of consecutive
words, distorting one of the words, scoring the dis-
torted set, and finally training the network to give
the correct set a higher score.
Taking the lighter concept even further,
Mikolov et al. (2013a) introduced a model called
Continuous Skip-gram. This model is trained
to predict the context surrounding a given word
using a shallow neural network. The model is less
aware of the order of words, than the previously
mentioned models, but can be trained efficiently
on considerably larger datasets.
An early attempt at merging word represen-
tations into representations for phrases and sen-
tences is introduced by Socher et al. (2010). The
authors present a recursive neural network archi-
tecture (RvNN) that is able to jointly learn parsing
and phrase/sentence representation. Though not
able to achieve state-of-the-art results, the method
provides an interesting path forward. The model
uses one neural network to derive all merged rep-
resentations, applied recursively in a binary parse
tree. This makes the model fast and easy to train
but requires labeled data for training.
</bodyText>
<subsectionHeader confidence="0.99792">
7.2 Summarization Techniques
</subsectionHeader>
<bodyText confidence="0.999240096774193">
Radev et al. (2004) pioneered the use of cluster
centroids in their work with the idea to group, in
the same cluster, those sentences which are highly
similar to each other, thus generating a number
of clusters. To measure the similarity between a
pair of sentences, the authors use the cosine simi-
larity measure where sentences are represented as
weighted vectors of tf-idf terms. Once sentences
are clustered, sentence selection is performed by
selecting a subset of sentences from each cluster.
In TextRank (2004), a document is represented
as a graph where each sentence is denoted by a
vertex and pairwise similarities between sentences
are represented by edges with a weight corre-
sponding to the similarity between the sentences.
The Google PageRank ranking algorithm is used
to estimate the importance of different sentences
and the most important sentences are chosen for
inclusion in the summary.
Bonzanini, Martinez, Roelleke (2013) pre-
sented an algorithm that starts with the set of
all sentences in the summary and then iteratively
chooses sentences that are unimportant and re-
moves them. The sentence removal algorithm ob-
tained good results on the Opinosis dataset, in par-
ticular w.r.t F-scores.
We have chosen to compare our work with that
of Lin and Bilmes (2011), described in Sec. 2.1.
Future work is to make an exhaustive comparison
using a larger set similarity measures and summa-
rization frameworks.
</bodyText>
<sectionHeader confidence="0.99956" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999974181818182">
We investigated the effects of using phrase embed-
dings for summarization, and showed that these
can significantly improve the performance of the
state-of-the-art summarization method introduced
by Lin and Bilmes in (2011). Two implementa-
tions of word vectors and two different approaches
for composition where evaluated. All investi-
gated combinations improved the original Lin-
Bilmes approach (using tf-idf representations of
sentences) for at least two ROUGE scores, and top
results where found using vector addition on CW
vectors.
In order to further investigate the applicability
of continuous vector representations for summa-
rization, in future work we plan to try other sum-
marization methods. In particular we will use a
method based on multiple kernel learning were
phrase embeddings can be combined with other
similarity measures. Furthermore, we aim to use
a novel method for sentence representation similar
to the RAE using multiplicative connections con-
trolled by the local context in the sentence.
</bodyText>
<sectionHeader confidence="0.998813" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998039857142857">
The authors would like to acknowledge the project
Towards a knowledge-based culturomics sup-
ported by a framework grant from the Swedish
Research Council (2012–2016; dnr 2012-5738),
and the project Data-driven secure business intel-
ligence grant IIS11-0089 from the Swedish Foun-
dation for Strategic Research (SSF).
</bodyText>
<page confidence="0.998586">
38
</page>
<sectionHeader confidence="0.996324" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999755342592593">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
Yoshua Bengio. 2002. New distributed prob-
abilistic language models. Technical Report
1215, D´epartement d’informatique et recherche
op´erationnelle, Universit´e de Montr´eal.
Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and trends ® in Machine Learning,
2(1):1–127.
Marco Bonzanini, Miguel Martinez-Alvarez, and
Thomas Roelleke. 2013. Extractive summarisa-
tion via sentence removal: Condensing relevant sen-
tences into a short summary. In Proceedings of the
36th International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
SIGIR ’13, pages 893–896. ACM.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: a graph-based approach to abstrac-
tive summarization of highly redundant opinions. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 340–348. ACL.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In IEEE Inter-
national Conference on Neural Networks, volume 1,
pages 347–352. IEEE.
Alex Graves, Abdel-rahman Mohamed, and Geof-
frey Hinton. 2013. Speech recognition with
deep recurrent neural networks. arXiv preprint
arXiv:1303.5778.
S.S. Haykin. 2009. Neural Networks and Learning
Machines. Number v. 10 in Neural networks and
learning machines. Prentice Hall.
Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006.
Reducing the dimensionality of data with neural net-
works. Science, 313(5786):504–507.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A fast learning algorithm for deep be-
lief nets. Neural computation, 18(7):1527–1554.
Dan Klein and Christopher D Manning. 2003. Fast ex-
act inference with a factored model for natural lan-
guage parsing. Advances in neural information pro-
cessing systems, pages 3–10.
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In Advances in Neural Infor-
mation Processing Systems 25, pages 1106–1114.
Hui Lin and Jeff Bilmes. 2011. A class of submodu-
lar functions for document summarization. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 510–520. ACL.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of
EMNLP, volume 4. Barcelona, Spain.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045–1048.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. ArXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
AISTATS’05, pages 246–252.
Dragomir R Radev, Hongyan Jing, Małgorzata Sty´s,
and Daniel Tam. 2004. Centroid-based summariza-
tion of multiple documents. Information Processing
&amp; Management, 40(6):919–938.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1986. Learning representations by back-
propagating errors. Nature, 323(6088):533–536.
Gerard Salton and Michael J. McGill. 1986. Intro-
duction to Modern Information Retrieval. McGraw-
Hill, Inc., New York, NY, USA.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. In Advances in
Neural Information Processing Systems 24.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394. ACL.
</reference>
<page confidence="0.99953">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.235157">
<title confidence="0.998557">Extractive Summarization using Continuous Vector Space Models</title>
<author confidence="0.943714">Olof Mogren</author>
<author confidence="0.943714">Nina Tahmasebi</author>
<author confidence="0.943714">Devdatt</author>
<affiliation confidence="0.987469">Computer Science &amp; Chalmers University of</affiliation>
<phone confidence="0.283172">SE-412 96,</phone>
<email confidence="0.779237">mogren,ninat,</email>
<abstract confidence="0.99832255">Automatic summarization can help users extract the most important pieces of information from the vast amount of text digitized into electronic form everyday. Central to automatic summarization is the notion of similarity between sentences in text. In this paper we propose the use of continuous vector representations for semantically aware representations of sentences as a basis for measuring similarity. We evaluate different compositions for sentence representation on a standard dataset using the ROUGE evaluation measures. Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of continuous word vector representations for automatic summarization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="12463" citStr="Bengio et al. (2003)" startWordPosition="2090" endWordPosition="2093">ollowing formulas can be used: zp = θTp [xl; xr] (5a) yp = g(zp) (5b) where yp is the computed parent state of neuron p, and zp the induced field for the same neuron. [xl; xr] is the concatenation of the state belonging to the right and left sibling nodes. This process results in a fixed length representation for hierarchical data of arbitrary length. Training of the model is done using backpropagation through structure, introduced by Goller and Kuchler (1996). 4 Word Embeddings Continuous distributed vector representation of words, also referred to as word embeddings, was first introduced by Bengio et al. (2003). A word embedding is a continuous vector representation that captures semantic and syntactic information about a word. These representations can be used to unveil dimensions of similarity between words, e.g. singular or plural. 4.1 Collobert &amp; Weston Collobert and Weston (2008) introduce an efficient method for computing word embeddings, in this work referred to as CW vectors. This is achieved firstly, by scoring a valid n-gram (x) and a corrupted n-gram (R) (where the center word has been randomly chosen), and secondly, by training the network to distinguish between these two n-grams. This i</context>
<context position="25171" citStr="Bengio et al. (2003)" startWordPosition="4157" endWordPosition="4160"> maximal recall suggests that the phrase embeddings are not complete w.r.t summarization and might benefit from being combined with other similarity measures that can capture complementary information, for example using multiple kernel learning. 7 Related Work To the best of our knowledge, continuous vector space models have not previously been used in summarization tasks. Therefore, we split this section in two, handling summarization and continuous vector space models separately. 7.1 Continuous Vector Space Models Continuous distributed vector representation of words was first introduced by Bengio et al. (2003). 37 They employ a FFNN, using a window of words as input, and train the model to predict the next word. This is computed using a big softmax layer that calculate the probabilities for each word in the vocabulary. This type of exhaustive estimation is necessary in some NLP applications, but makes the model heavy to train. If the sole purpose of the model is to derive word embeddings this can be exploited by using a much lighter output layer. This was suggested by Collobert and Weston (2008), which swapped the heavy softmax against a hinge loss function. The model works by scoring a set of cons</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>New distributed probabilistic language models.</title>
<date>2002</date>
<tech>Technical Report 1215, D´epartement</tech>
<institution>Universit´e de Montr´eal.</institution>
<marker>Bengio, 2002</marker>
<rawString>Yoshua Bengio. 2002. New distributed probabilistic language models. Technical Report 1215, D´epartement d’informatique et recherche op´erationnelle, Universit´e de Montr´eal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Learning deep architectures for ai. Foundations and trends ®</title>
<date>2009</date>
<booktitle>in Machine Learning,</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="6773" citStr="Bengio, 2009" startWordPosition="1106" endWordPosition="1107">lap must be exact to count towards the similarity, e.g, the terms The US President and Barack Obama in different sentences will not add towards the similarity of the sentences. To capture deeper similarity, in this paper we will investigate the use of continuous vector representations for measuring similarity between sentences. In the next sections we will describe the basics needed for creating continuous vector representations and methods used to create sentence representations that can be used to measure sentence similarity. 3 Background on Deep Learning Deep learning (Hinton et al., 2006; Bengio, 2009) is a modern interpretation of artificial neural networks (ANN), with an emphasis on deep network architectures. Deep learning can be used for challenging problems like image and speech recognition (Krizhevsky et al., 2012; Graves et al., 2013), as well as language modeling (Mikolov et al., 2010), and in all cases, able to achieve state-ofthe-art results. Inspired by the brain, ANNs use a neuron-like construction as their primary computational unit. The behavior of a neuron is entirely controlled by its input weights. Hence, the weights are where the information learned by the neuron is stored</context>
</contexts>
<marker>Bengio, 2009</marker>
<rawString>Yoshua Bengio. 2009. Learning deep architectures for ai. Foundations and trends ® in Machine Learning, 2(1):1–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Bonzanini</author>
<author>Miguel Martinez-Alvarez</author>
<author>Thomas Roelleke</author>
</authors>
<title>Extractive summarisation via sentence removal: Condensing relevant sentences into a short summary.</title>
<date>2013</date>
<booktitle>In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’13,</booktitle>
<pages>893--896</pages>
<publisher>ACM.</publisher>
<marker>Bonzanini, Martinez-Alvarez, Roelleke, 2013</marker>
<rawString>Marco Bonzanini, Miguel Martinez-Alvarez, and Thomas Roelleke. 2013. Extractive summarisation via sentence removal: Condensing relevant sentences into a short summary. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’13, pages 893–896. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="12742" citStr="Collobert and Weston (2008)" startWordPosition="2131" endWordPosition="2134">sults in a fixed length representation for hierarchical data of arbitrary length. Training of the model is done using backpropagation through structure, introduced by Goller and Kuchler (1996). 4 Word Embeddings Continuous distributed vector representation of words, also referred to as word embeddings, was first introduced by Bengio et al. (2003). A word embedding is a continuous vector representation that captures semantic and syntactic information about a word. These representations can be used to unveil dimensions of similarity between words, e.g. singular or plural. 4.1 Collobert &amp; Weston Collobert and Weston (2008) introduce an efficient method for computing word embeddings, in this work referred to as CW vectors. This is achieved firstly, by scoring a valid n-gram (x) and a corrupted n-gram (R) (where the center word has been randomly chosen), and secondly, by training the network to distinguish between these two n-grams. This is done by minimizing the hinge loss max(0, 1 − s(x) + s(R)) (6) where s is the scoring function, i.e. the output of a FFNN that maps between the word embeddings of an n-gram to a real valued score. Both the parameters of the scoring function and the word embeddings are learned i</context>
<context position="25666" citStr="Collobert and Weston (2008)" startWordPosition="4245" endWordPosition="4248">.1 Continuous Vector Space Models Continuous distributed vector representation of words was first introduced by Bengio et al. (2003). 37 They employ a FFNN, using a window of words as input, and train the model to predict the next word. This is computed using a big softmax layer that calculate the probabilities for each word in the vocabulary. This type of exhaustive estimation is necessary in some NLP applications, but makes the model heavy to train. If the sole purpose of the model is to derive word embeddings this can be exploited by using a much lighter output layer. This was suggested by Collobert and Weston (2008), which swapped the heavy softmax against a hinge loss function. The model works by scoring a set of consecutive words, distorting one of the words, scoring the distorted set, and finally training the network to give the correct set a higher score. Taking the lighter concept even further, Mikolov et al. (2013a) introduced a model called Continuous Skip-gram. This model is trained to predict the context surrounding a given word using a shallow neural network. The model is less aware of the order of words, than the previously mentioned models, but can be trained efficiently on considerably large</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kavita Ganesan</author>
<author>ChengXiang Zhai</author>
<author>Jiawei Han</author>
</authors>
<title>Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>340--348</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="18170" citStr="Ganesan et al., 2010" startWordPosition="3039" endWordPosition="3042">y and Euclidean distance. The second group of configurations are built upon recursive auto-encoders using CW vectors and are also compared using cosine similarity as well as Euclidean distance. The methods are named according to: VectorType EmbeddingMethodSimilarityMethod, e.g. W2V_AddCos for Word2Vec vectors combined using vector addition and compared using cosine similarity. To get an upper bound for each ROUGE score an exhaustive search were performed, where each possible pair of sentences were evaluated, and maximized w.r.t the ROUGE score. 6.2 Dataset and Evaluation The Opinosis dataset (Ganesan et al., 2010) consists of short user reviews in 51 different topics. Each of these topics contains between 50 and 575 sentences and are a collection of user reviews made by different authors about a certain characteristic of a hotel, car or a product (e.g. ”Location of Holiday Inn, London” and ”Fonts, Amazon Kindle”). The dataset is well suited for multidocument summarization (each sentence is considered its own document), and includes between 4 and 5 gold-standard summaries (not sentences chosen from the documents) created by human authors for each topic. Each summary is evaluated with ROUGE, that works b</context>
</contexts>
<marker>Ganesan, Zhai, Han, 2010</marker>
<rawString>Kavita Ganesan, ChengXiang Zhai, and Jiawei Han. 2010. Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 340–348. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Goller</author>
<author>Andreas Kuchler</author>
</authors>
<title>Learning task-dependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In IEEE International Conference on Neural Networks,</booktitle>
<volume>1</volume>
<pages>347--352</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="12307" citStr="Goller and Kuchler (1996)" startWordPosition="2067" endWordPosition="2070">de. A compressed representation is calculated recursively at each non-terminal node in the tree, using the same weight matrix at each node. More precisely, the following formulas can be used: zp = θTp [xl; xr] (5a) yp = g(zp) (5b) where yp is the computed parent state of neuron p, and zp the induced field for the same neuron. [xl; xr] is the concatenation of the state belonging to the right and left sibling nodes. This process results in a fixed length representation for hierarchical data of arbitrary length. Training of the model is done using backpropagation through structure, introduced by Goller and Kuchler (1996). 4 Word Embeddings Continuous distributed vector representation of words, also referred to as word embeddings, was first introduced by Bengio et al. (2003). A word embedding is a continuous vector representation that captures semantic and syntactic information about a word. These representations can be used to unveil dimensions of similarity between words, e.g. singular or plural. 4.1 Collobert &amp; Weston Collobert and Weston (2008) introduce an efficient method for computing word embeddings, in this work referred to as CW vectors. This is achieved firstly, by scoring a valid n-gram (x) and a c</context>
<context position="16082" citStr="Goller and Kuchler, 1996" startWordPosition="2711" endWordPosition="2714">for encoding the compressed representations, and one for decoding them to recover the original sentence, see Figure 6. The network is subsequently trained by minimizing the reconstruction error. Forward propagation in the network is done by recursively applying Eq. 5a and 5b for each triplet in the tree in two phases. First, starting at the center node (root of the tree) and recursively pulling the data from the input. Second, again starting at the center node, recursively pushing the data towards the output. Backpropagation is done in a similar manner using backpropagation through structure (Goller and Kuchler, 1996). Figure 6: The structure of an unfolding RAE, on a three word phrase ([x1, x2, x3]). The weight matrix θe is used to encode the compressed representations, while θd is used to decode the representations and reconstruct the sentence. 5.3 Measuring Similarity Phrase embeddings provide semantically aware representations for sentences. For summarization, we need to measure the similarity between two representations and will make use of the following two vector similarity measures. The first similarity measure is the cosine similarity, transformed to the interval of [0, 1] Sim(i, j) = xTX� + 1 /2 </context>
</contexts>
<marker>Goller, Kuchler, 1996</marker>
<rawString>Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In IEEE International Conference on Neural Networks, volume 1, pages 347–352. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>Abdel-rahman Mohamed</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Speech recognition with deep recurrent neural networks. arXiv preprint arXiv:1303.5778.</title>
<date>2013</date>
<contexts>
<context position="7017" citStr="Graves et al., 2013" startWordPosition="1144" endWordPosition="1147">ate the use of continuous vector representations for measuring similarity between sentences. In the next sections we will describe the basics needed for creating continuous vector representations and methods used to create sentence representations that can be used to measure sentence similarity. 3 Background on Deep Learning Deep learning (Hinton et al., 2006; Bengio, 2009) is a modern interpretation of artificial neural networks (ANN), with an emphasis on deep network architectures. Deep learning can be used for challenging problems like image and speech recognition (Krizhevsky et al., 2012; Graves et al., 2013), as well as language modeling (Mikolov et al., 2010), and in all cases, able to achieve state-ofthe-art results. Inspired by the brain, ANNs use a neuron-like construction as their primary computational unit. The behavior of a neuron is entirely controlled by its input weights. Hence, the weights are where the information learned by the neuron is stored. More precisely the output of a neuron is computed as the weighted sum of its inputs, and squeezed into the interval [0, 1] using a sigmoid function: yi = 9(θTi x) (3) 1 9(z) = (4) 1 + e−z Sim(i, j) = 32 Input Hidden Output Input Coding Recons</context>
</contexts>
<marker>Graves, Mohamed, Hinton, 2013</marker>
<rawString>Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013. Speech recognition with deep recurrent neural networks. arXiv preprint arXiv:1303.5778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Haykin</author>
</authors>
<title>Neural Networks and Learning Machines. Number v. 10 in Neural networks and learning machines.</title>
<date>2009</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="8671" citStr="Haykin, 2009" startWordPosition="1439" endWordPosition="1440">input. Here the sigmoid function (g) is chosen to be the logistic function, but it may also be modeled using other sigmoid shaped functions, e.g. the hyperbolic tangent function. The neurons can be organized in many different ways. In some architectures, loops are permitted. These are referred to as recurrent neural networks. However, all networks considered here are non-cyclic topologies. In the rest of this section we discuss a few general architectures in more detail, which will later be employed in the evaluated models. 3.1 Feed Forward Neural Network A feed forward neural network (FFNN) (Haykin, 2009) is a type of ANN where the neurons are structured in layers, and only connections to subsequent layers are allowed, see Fig 2. The algorithm is similar to logistic regression using nonlinear terms. However, it does not rely on the user to choose the non-linear terms needed to fit the data, making it more adaptable to changing datasets. The first layer in a FFNN is called the input layer, the last layer is called the output layer, and the interim layers are called hidden layers. The hidden layers are optional but necessary to fit complex patterns. Training is achieved by minimizing the network</context>
</contexts>
<marker>Haykin, 2009</marker>
<rawString>S.S. Haykin. 2009. Neural Networks and Learning Machines. Number v. 10 in Neural networks and learning machines. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Ruslan R Salakhutdinov</author>
</authors>
<title>Reducing the dimensionality of data with neural networks.</title>
<date>2006</date>
<journal>Science,</journal>
<volume>313</volume>
<issue>5786</issue>
<contexts>
<context position="10138" citStr="Hinton and Salakhutdinov, 2006" startWordPosition="1690" endWordPosition="1693"> two dimensional code. This is achieved by using a bottleneck layer, referred to as a coding layer. the expected output. In order to minimize this function the gradient ∂E ∂Θfirst needs to be calculated, where Θ is a matrix of all parameters, or weights, in the network. This is achieved using backpropagation (Rumelhart et al., 1986). Secondly, these gradients are used to minimize E using e.g. gradient descent. The result of this processes is a set of weights that enables the network to do the desired input-output mapping, as defined by the training data. 3.2 Auto-Encoder An auto-encoder (AE) (Hinton and Salakhutdinov, 2006), see Fig. 3, is a type of FFNN with a topology designed for dimensionality reduction. The input and the output layers in an AE are identical, and there is at least one hidden bottleneck layer that is referred to as the coding layer. The network is trained to reconstruct the input data, and if it succeeds this implies that all information in the data is necessarily contained in the compressed representation of the coding layer. A shallow AE, i.e. an AE with no extra hidden layers, will produce a similar code as principal component analysis. However, if more layers are added, before and after t</context>
</contexts>
<marker>Hinton, Salakhutdinov, 2006</marker>
<rawString>Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Simon Osindero</author>
<author>Yee-Whye Teh</author>
</authors>
<title>A fast learning algorithm for deep belief nets.</title>
<date>2006</date>
<booktitle>Neural computation,</booktitle>
<pages>18--7</pages>
<contexts>
<context position="6758" citStr="Hinton et al., 2006" startWordPosition="1102" endWordPosition="1105">f-idf words. The overlap must be exact to count towards the similarity, e.g, the terms The US President and Barack Obama in different sentences will not add towards the similarity of the sentences. To capture deeper similarity, in this paper we will investigate the use of continuous vector representations for measuring similarity between sentences. In the next sections we will describe the basics needed for creating continuous vector representations and methods used to create sentence representations that can be used to measure sentence similarity. 3 Background on Deep Learning Deep learning (Hinton et al., 2006; Bengio, 2009) is a modern interpretation of artificial neural networks (ANN), with an emphasis on deep network architectures. Deep learning can be used for challenging problems like image and speech recognition (Krizhevsky et al., 2012; Graves et al., 2013), as well as language modeling (Mikolov et al., 2010), and in all cases, able to achieve state-ofthe-art results. Inspired by the brain, ANNs use a neuron-like construction as their primary computational unit. The behavior of a neuron is entirely controlled by its input weights. Hence, the weights are where the information learned by the n</context>
</contexts>
<marker>Hinton, Osindero, Teh, 2006</marker>
<rawString>Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. 2006. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527–1554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing. Advances in neural information processing systems,</title>
<date>2003</date>
<pages>3--10</pages>
<contexts>
<context position="20026" citStr="Klein and Manning, 2003" startWordPosition="3349" endWordPosition="3352">2 P ∗R P +R. 6.3 Implementation All results were obtained by running an implementation of Lin-Bilmes submodular optimization summarizer, as described in Sec. 2.1. Also, we have chosen to fix the length of the summaries to two sentences because the length of the goldstandard summaries are typically around two sentences. The CW vectors used were trained by Turian et al. (2010)1, and the Word2Vec vectors by Mikolov et al. (2013b)2. The unfolding RAE used is based on the implementation by Socher et al. (2011)3, and the parse trees for guiding the recursion was generated using the Stanford Parser (Klein and Manning, 2003)4. 6.4 Results The results from the ROUGE evaluation are compiled in Table 1. We find for all measures (recall, precision, and F-score), that the phrase embeddings outperform the original Lin-Bilmes. For recall, we find that CW_AddCos achieves the highest result, while for precision and F-score the CW_AddEuc perform best. These results are consistent for all versions of ROUGE scores reported (1, 2 and SU4), providing a strong indication for phrase embeddings in the context of automatic summarization. Unfolding RAE on CW vectors and vector addition on W2V vectors gave comparable results w.r.t. </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Fast exact inference with a factored model for natural language parsing. Advances in neural information processing systems, pages 3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoff Hinton</author>
</authors>
<title>Imagenet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems 25,</booktitle>
<pages>1106--1114</pages>
<contexts>
<context position="6995" citStr="Krizhevsky et al., 2012" startWordPosition="1140" endWordPosition="1143">is paper we will investigate the use of continuous vector representations for measuring similarity between sentences. In the next sections we will describe the basics needed for creating continuous vector representations and methods used to create sentence representations that can be used to measure sentence similarity. 3 Background on Deep Learning Deep learning (Hinton et al., 2006; Bengio, 2009) is a modern interpretation of artificial neural networks (ANN), with an emphasis on deep network architectures. Deep learning can be used for challenging problems like image and speech recognition (Krizhevsky et al., 2012; Graves et al., 2013), as well as language modeling (Mikolov et al., 2010), and in all cases, able to achieve state-ofthe-art results. Inspired by the brain, ANNs use a neuron-like construction as their primary computational unit. The behavior of a neuron is entirely controlled by its input weights. Hence, the weights are where the information learned by the neuron is stored. More precisely the output of a neuron is computed as the weighted sum of its inputs, and squeezed into the interval [0, 1] using a sigmoid function: yi = 9(θTi x) (3) 1 9(z) = (4) 1 + e−z Sim(i, j) = 32 Input Hidden Outp</context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>A class of submodular functions for document summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>510--520</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3616" citStr="Lin and Bilmes (2011)" startWordPosition="542" endWordPosition="545">n a summary from a set of documents D. Typically, extractive summarization techniques can be divided into two components, the summarization framework and the similarity measures used to compare sentences. Next 31 Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 31–39, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics we present the algorithm used for the framework and in Sec. 2.2 we discuss a typical sentence similarity measure, later to be used as a baseline. 2.1 Submodular Optimization Lin and Bilmes (2011) formulated the problem of extractive summarization as an optimization problem using monotone nondecreasing submodular set functions. A submodular function F on the set of sentences V satisfies the following property: for any A C_ B C_ V \{v}, F(A+ {v}) − F(A) ≥ F(B + {v}) − F(B) where v E V . This is called the diminishing returns property and captures the intuition that adding a sentence to a small set of sentences (i.e., summary) makes a greater contribution than adding a sentence to a larger set. The aim is then to find a summary that maximizes diversity of the sentences and the coverage o</context>
<context position="5200" citStr="Lin and Bilmes, 2011" startWordPosition="834" endWordPosition="837">hard, however, if the objective function is submodular there is a fast scalable algorithm that returns an approximation with a guarantee. In the work of Lin and Bilmes (2011) a simple submodular function is chosen: L(S) = X Xmin{ Sim(i, j), α X Sim(i, j)} i∈V j∈S j∈V (1) The first argument measures similarity between sentence i and the summary S, while the second argument measures similarity between sentence i and the rest of the input V . Sim(i, j) is the similarity between sentence i and sentence j and 0 &lt; α &lt; 1 is a threshold coefficient. The diversity reward function R(S) can be found in (Lin and Bilmes, 2011). 2.2 Traditional Similarity Measure Central to most extractive summarization systems is the use of sentence similarity measures (Sim(i, j) in Eq. 1). Lin and Bilmes measure similarity between sentences by representing each sentence using tf-idf (Salton and McGill, 1986) vectors and measuring the cosine angle between vectors. Each sentence is represented by a word vector w = (w1, ... , wN) where N is the size of the vocabulary. Weights wki correspond to the tfidf value of word k in the sentence i. The weights Sim(i, j) used in the L function in Eq. 1 are found using the following similarity me</context>
<context position="28163" citStr="Lin and Bilmes (2011)" startWordPosition="4645" endWordPosition="4648"> by edges with a weight corresponding to the similarity between the sentences. The Google PageRank ranking algorithm is used to estimate the importance of different sentences and the most important sentences are chosen for inclusion in the summary. Bonzanini, Martinez, Roelleke (2013) presented an algorithm that starts with the set of all sentences in the summary and then iteratively chooses sentences that are unimportant and removes them. The sentence removal algorithm obtained good results on the Opinosis dataset, in particular w.r.t F-scores. We have chosen to compare our work with that of Lin and Bilmes (2011), described in Sec. 2.1. Future work is to make an exhaustive comparison using a larger set similarity measures and summarization frameworks. 8 Conclusions We investigated the effects of using phrase embeddings for summarization, and showed that these can significantly improve the performance of the state-of-the-art summarization method introduced by Lin and Bilmes in (2011). Two implementations of word vectors and two different approaches for composition where evaluated. All investigated combinations improved the original LinBilmes approach (using tf-idf representations of sentences) for at l</context>
</contexts>
<marker>Lin, Bilmes, 2011</marker>
<rawString>Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 510–520. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="19023" citStr="Lin, 2004" startWordPosition="3181" endWordPosition="3182"> ”Location of Holiday Inn, London” and ”Fonts, Amazon Kindle”). The dataset is well suited for multidocument summarization (each sentence is considered its own document), and includes between 4 and 5 gold-standard summaries (not sentences chosen from the documents) created by human authors for each topic. Each summary is evaluated with ROUGE, that works by counting word overlaps between generated summaries and gold standard summaries. Our results include R-1, R-2, and R-SU4, which counts matches in unigrams, bigrams, and skip-bigrams respectively. The skip-bigrams allow four words in between (Lin, 2004). The measures reported are recall (R), precision (P), and F-score (F), computed for each topic individually and averaged. Recall measures what fraction of a human created gold standard summary that is captured, and precision measures what fraction of the generated summary that is in the gold standard. F-score is a standard way to combine recall and precision, computed as F = 2 P ∗R P +R. 6.3 Implementation All results were obtained by running an implementation of Lin-Bilmes submodular optimization summarizer, as described in Sec. 2.1. Also, we have chosen to fix the length of the summaries to</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>TextRank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<volume>4</volume>
<location>Barcelona,</location>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing order into texts. In Proceedings of EMNLP, volume 4. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. ArXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="13528" citStr="Mikolov et al. (2013" startWordPosition="2269" endWordPosition="2272">rrupted n-gram (R) (where the center word has been randomly chosen), and secondly, by training the network to distinguish between these two n-grams. This is done by minimizing the hinge loss max(0, 1 − s(x) + s(R)) (6) where s is the scoring function, i.e. the output of a FFNN that maps between the word embeddings of an n-gram to a real valued score. Both the parameters of the scoring function and the word embeddings are learned in parallel using backpropagation. 4.2 Continuous Skip-gram A second method for computing word embeddings is the Continuous Skip-gram model, see Fig. 5, introduced by Mikolov et al. (2013a). This model is used in the implementation of their word embeddings tool Word2Uec. The model is trained to predict the context surrounding a given word. This is accomplished by maximizing the objective function log p(wt+j|wt) (7) where T is the number of words in the training set, and c is the length of the training context. The probability p(wt+j|wt) is approximated using the hierarchical softmax introduced by Bengio et al. (2002) and evaluated in a paper by Morin and Bengio (2005). 5 Phrase Embeddings Word embeddings have proven useful in many natural language processing (NLP) tasks. For s</context>
<context position="14852" citStr="Mikolov et al. (2013" startWordPosition="2508" endWordPosition="2511">or deriving phrase embeddings, which in Section 5.3 will be used to compute sentence to sentence similarities. 5.1 Vector addition The simplest way to represent a sentence is to consider it as the sum of all words without regarding word orders. This was considered by Input Root layer layer x1 x2 x3 y � −c≤j≤c,j�0 1 T T t=1 34 Input layer Root layer Output layer wt wt−2 wt−1 wt+1 wt+2 projection layer Input layer Output layer x1 x2 x3 θe θd x 1 x 2 x 3 ′ ′ ′ Figure 5: The continuous Skip-gram model. Using the input word (wt) the model tries to predict which words will be in its context (wt±c). Mikolov et al. (2013b) for representing short phrases. The model is expressed by the following equation: �xp = xw (8) x,,,∈{sentence} where xp is a phrase embedding, and xw is a word embedding. We use this method for computing phrase embeddings as a baseline in our experiments. 5.2 Unfolding Recursive Auto-encoder The second model is more sophisticated, taking into account also the order of the words and the grammar used. An unfolding recursive auto-encoder (RAE) is used to derive the phrase embedding on the basis of a binary parse tree. The unfolding RAE was introduced by Socher et al. (2011) and uses two RvNNs,</context>
<context position="19830" citStr="Mikolov et al. (2013" startWordPosition="3317" endWordPosition="3320">ummary that is captured, and precision measures what fraction of the generated summary that is in the gold standard. F-score is a standard way to combine recall and precision, computed as F = 2 P ∗R P +R. 6.3 Implementation All results were obtained by running an implementation of Lin-Bilmes submodular optimization summarizer, as described in Sec. 2.1. Also, we have chosen to fix the length of the summaries to two sentences because the length of the goldstandard summaries are typically around two sentences. The CW vectors used were trained by Turian et al. (2010)1, and the Word2Vec vectors by Mikolov et al. (2013b)2. The unfolding RAE used is based on the implementation by Socher et al. (2011)3, and the parse trees for guiding the recursion was generated using the Stanford Parser (Klein and Manning, 2003)4. 6.4 Results The results from the ROUGE evaluation are compiled in Table 1. We find for all measures (recall, precision, and F-score), that the phrase embeddings outperform the original Lin-Bilmes. For recall, we find that CW_AddCos achieves the highest result, while for precision and F-score the CW_AddEuc perform best. These results are consistent for all versions of ROUGE scores reported (1, 2 and</context>
<context position="25976" citStr="Mikolov et al. (2013" startWordPosition="4298" endWordPosition="4301">each word in the vocabulary. This type of exhaustive estimation is necessary in some NLP applications, but makes the model heavy to train. If the sole purpose of the model is to derive word embeddings this can be exploited by using a much lighter output layer. This was suggested by Collobert and Weston (2008), which swapped the heavy softmax against a hinge loss function. The model works by scoring a set of consecutive words, distorting one of the words, scoring the distorted set, and finally training the network to give the correct set a higher score. Taking the lighter concept even further, Mikolov et al. (2013a) introduced a model called Continuous Skip-gram. This model is trained to predict the context surrounding a given word using a shallow neural network. The model is less aware of the order of words, than the previously mentioned models, but can be trained efficiently on considerably larger datasets. An early attempt at merging word representations into representations for phrases and sentences is introduced by Socher et al. (2010). The authors present a recursive neural network architecture (RvNN) that is able to jointly learn parsing and phrase/sentence representation. Though not able to ach</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. ArXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="13528" citStr="Mikolov et al. (2013" startWordPosition="2269" endWordPosition="2272">rrupted n-gram (R) (where the center word has been randomly chosen), and secondly, by training the network to distinguish between these two n-grams. This is done by minimizing the hinge loss max(0, 1 − s(x) + s(R)) (6) where s is the scoring function, i.e. the output of a FFNN that maps between the word embeddings of an n-gram to a real valued score. Both the parameters of the scoring function and the word embeddings are learned in parallel using backpropagation. 4.2 Continuous Skip-gram A second method for computing word embeddings is the Continuous Skip-gram model, see Fig. 5, introduced by Mikolov et al. (2013a). This model is used in the implementation of their word embeddings tool Word2Uec. The model is trained to predict the context surrounding a given word. This is accomplished by maximizing the objective function log p(wt+j|wt) (7) where T is the number of words in the training set, and c is the length of the training context. The probability p(wt+j|wt) is approximated using the hierarchical softmax introduced by Bengio et al. (2002) and evaluated in a paper by Morin and Bengio (2005). 5 Phrase Embeddings Word embeddings have proven useful in many natural language processing (NLP) tasks. For s</context>
<context position="14852" citStr="Mikolov et al. (2013" startWordPosition="2508" endWordPosition="2511">or deriving phrase embeddings, which in Section 5.3 will be used to compute sentence to sentence similarities. 5.1 Vector addition The simplest way to represent a sentence is to consider it as the sum of all words without regarding word orders. This was considered by Input Root layer layer x1 x2 x3 y � −c≤j≤c,j�0 1 T T t=1 34 Input layer Root layer Output layer wt wt−2 wt−1 wt+1 wt+2 projection layer Input layer Output layer x1 x2 x3 θe θd x 1 x 2 x 3 ′ ′ ′ Figure 5: The continuous Skip-gram model. Using the input word (wt) the model tries to predict which words will be in its context (wt±c). Mikolov et al. (2013b) for representing short phrases. The model is expressed by the following equation: �xp = xw (8) x,,,∈{sentence} where xp is a phrase embedding, and xw is a word embedding. We use this method for computing phrase embeddings as a baseline in our experiments. 5.2 Unfolding Recursive Auto-encoder The second model is more sophisticated, taking into account also the order of the words and the grammar used. An unfolding recursive auto-encoder (RAE) is used to derive the phrase embedding on the basis of a binary parse tree. The unfolding RAE was introduced by Socher et al. (2011) and uses two RvNNs,</context>
<context position="19830" citStr="Mikolov et al. (2013" startWordPosition="3317" endWordPosition="3320">ummary that is captured, and precision measures what fraction of the generated summary that is in the gold standard. F-score is a standard way to combine recall and precision, computed as F = 2 P ∗R P +R. 6.3 Implementation All results were obtained by running an implementation of Lin-Bilmes submodular optimization summarizer, as described in Sec. 2.1. Also, we have chosen to fix the length of the summaries to two sentences because the length of the goldstandard summaries are typically around two sentences. The CW vectors used were trained by Turian et al. (2010)1, and the Word2Vec vectors by Mikolov et al. (2013b)2. The unfolding RAE used is based on the implementation by Socher et al. (2011)3, and the parse trees for guiding the recursion was generated using the Stanford Parser (Klein and Manning, 2003)4. 6.4 Results The results from the ROUGE evaluation are compiled in Table 1. We find for all measures (recall, precision, and F-score), that the phrase embeddings outperform the original Lin-Bilmes. For recall, we find that CW_AddCos achieves the highest result, while for precision and F-score the CW_AddEuc perform best. These results are consistent for all versions of ROUGE scores reported (1, 2 and</context>
<context position="25976" citStr="Mikolov et al. (2013" startWordPosition="4298" endWordPosition="4301">each word in the vocabulary. This type of exhaustive estimation is necessary in some NLP applications, but makes the model heavy to train. If the sole purpose of the model is to derive word embeddings this can be exploited by using a much lighter output layer. This was suggested by Collobert and Weston (2008), which swapped the heavy softmax against a hinge loss function. The model works by scoring a set of consecutive words, distorting one of the words, scoring the distorted set, and finally training the network to give the correct set a higher score. Taking the lighter concept even further, Mikolov et al. (2013a) introduced a model called Continuous Skip-gram. This model is trained to predict the context surrounding a given word using a shallow neural network. The model is less aware of the order of words, than the previously mentioned models, but can be trained efficiently on considerably larger datasets. An early attempt at merging word representations into representations for phrases and sentences is introduced by Socher et al. (2010). The authors present a recursive neural network architecture (RvNN) that is able to jointly learn parsing and phrase/sentence representation. Though not able to ach</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Morin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In AISTATS’05,</booktitle>
<pages>246--252</pages>
<contexts>
<context position="14017" citStr="Morin and Bengio (2005)" startWordPosition="2353" endWordPosition="2356">p-gram A second method for computing word embeddings is the Continuous Skip-gram model, see Fig. 5, introduced by Mikolov et al. (2013a). This model is used in the implementation of their word embeddings tool Word2Uec. The model is trained to predict the context surrounding a given word. This is accomplished by maximizing the objective function log p(wt+j|wt) (7) where T is the number of words in the training set, and c is the length of the training context. The probability p(wt+j|wt) is approximated using the hierarchical softmax introduced by Bengio et al. (2002) and evaluated in a paper by Morin and Bengio (2005). 5 Phrase Embeddings Word embeddings have proven useful in many natural language processing (NLP) tasks. For summarization, however, sentences need to be compared. In this section we present two different methods for deriving phrase embeddings, which in Section 5.3 will be used to compute sentence to sentence similarities. 5.1 Vector addition The simplest way to represent a sentence is to consider it as the sum of all words without regarding word orders. This was considered by Input Root layer layer x1 x2 x3 y � −c≤j≤c,j�0 1 T T t=1 34 Input layer Root layer Output layer wt wt−2 wt−1 wt+1 wt+</context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In AISTATS’05, pages 246–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Hongyan Jing</author>
<author>Małgorzata Sty´s</author>
<author>Daniel Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<journal>Information Processing &amp; Management,</journal>
<volume>40</volume>
<issue>6</issue>
<marker>Radev, Jing, Sty´s, Tam, 2004</marker>
<rawString>Dragomir R Radev, Hongyan Jing, Małgorzata Sty´s, and Daniel Tam. 2004. Centroid-based summarization of multiple documents. Information Processing &amp; Management, 40(6):919–938.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning representations by backpropagating errors.</title>
<date>1986</date>
<journal>Nature,</journal>
<volume>323</volume>
<issue>6088</issue>
<contexts>
<context position="9841" citStr="Rumelhart et al., 1986" startWordPosition="1640" endWordPosition="1643">terns. Training is achieved by minimizing the network error (E). How E is defined differs between different network architectures, but is in general a differentiable function of the produced output and Figure 3: The figure shows an auto-encoder that compresses four dimensional data into a two dimensional code. This is achieved by using a bottleneck layer, referred to as a coding layer. the expected output. In order to minimize this function the gradient ∂E ∂Θfirst needs to be calculated, where Θ is a matrix of all parameters, or weights, in the network. This is achieved using backpropagation (Rumelhart et al., 1986). Secondly, these gradients are used to minimize E using e.g. gradient descent. The result of this processes is a set of weights that enables the network to do the desired input-output mapping, as defined by the training data. 3.2 Auto-Encoder An auto-encoder (AE) (Hinton and Salakhutdinov, 2006), see Fig. 3, is a type of FFNN with a topology designed for dimensionality reduction. The input and the output layers in an AE are identical, and there is at least one hidden bottleneck layer that is referred to as the coding layer. The network is trained to reconstruct the input data, and if it succe</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning representations by backpropagating errors. Nature, 323(6088):533–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1986</date>
<publisher>McGrawHill, Inc.,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5471" citStr="Salton and McGill, 1986" startWordPosition="874" endWordPosition="877">(1) The first argument measures similarity between sentence i and the summary S, while the second argument measures similarity between sentence i and the rest of the input V . Sim(i, j) is the similarity between sentence i and sentence j and 0 &lt; α &lt; 1 is a threshold coefficient. The diversity reward function R(S) can be found in (Lin and Bilmes, 2011). 2.2 Traditional Similarity Measure Central to most extractive summarization systems is the use of sentence similarity measures (Sim(i, j) in Eq. 1). Lin and Bilmes measure similarity between sentences by representing each sentence using tf-idf (Salton and McGill, 1986) vectors and measuring the cosine angle between vectors. Each sentence is represented by a word vector w = (w1, ... , wN) where N is the size of the vocabulary. Weights wki correspond to the tfidf value of word k in the sentence i. The weights Sim(i, j) used in the L function in Eq. 1 are found using the following similarity measure. P tfw,i X tfw,j X idf2w w∈i r Ptf2w i X idfw A tf2w,j X idfww∈i (2) where tfw,i and tfw,j are the number of occurrences of w in sentence i and j, and idfw is the inverse document frequency (idf) of w. In order to have a high similarity between sentences using the </context>
</contexts>
<marker>Salton, McGill, 1986</marker>
<rawString>Gerard Salton and Michael J. McGill. 1986. Introduction to Modern Information Retrieval. McGrawHill, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</booktitle>
<contexts>
<context position="10989" citStr="Socher et al. (2010)" startWordPosition="1839" endWordPosition="1842">r. The network is trained to reconstruct the input data, and if it succeeds this implies that all information in the data is necessarily contained in the compressed representation of the coding layer. A shallow AE, i.e. an AE with no extra hidden layers, will produce a similar code as principal component analysis. However, if more layers are added, before and after the coding layer, nonlinear manifolds can be found. This enables the network to compress complex data, with minimal loss of information. 3.3 Recursive Neural Network A recursive neural network (RvNN), see Fig. 4, first presented by Socher et al. (2010), is a type of feed forward neural network that can process data through an arbitrary binary tree structure, e.g. a 33 Figure 4: The recursive neural network architecture makes it possible to handle variable length input data. By using the same dimensionality for all layers, arbitrary binary tree structures can be recursively processed. binary parse tree produced by linguistic parsing of a sentence. This is achieved by enforcing weight constraints across all nodes and restricting the output of each node to have the same dimensionality as its children. The input data is placed in the leaf nodes</context>
<context position="26411" citStr="Socher et al. (2010)" startWordPosition="4367" endWordPosition="4370">rting one of the words, scoring the distorted set, and finally training the network to give the correct set a higher score. Taking the lighter concept even further, Mikolov et al. (2013a) introduced a model called Continuous Skip-gram. This model is trained to predict the context surrounding a given word using a shallow neural network. The model is less aware of the order of words, than the previously mentioned models, but can be trained efficiently on considerably larger datasets. An early attempt at merging word representations into representations for phrases and sentences is introduced by Socher et al. (2010). The authors present a recursive neural network architecture (RvNN) that is able to jointly learn parsing and phrase/sentence representation. Though not able to achieve state-of-the-art results, the method provides an interesting path forward. The model uses one neural network to derive all merged representations, applied recursively in a binary parse tree. This makes the model fast and easy to train but requires labeled data for training. 7.2 Summarization Techniques Radev et al. (2004) pioneered the use of cluster centroids in their work with the idea to group, in the same cluster, those se</context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>Richard Socher, Christopher D Manning, and Andrew Y Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems 24.</booktitle>
<contexts>
<context position="15432" citStr="Socher et al. (2011)" startWordPosition="2606" endWordPosition="2609">its context (wt±c). Mikolov et al. (2013b) for representing short phrases. The model is expressed by the following equation: �xp = xw (8) x,,,∈{sentence} where xp is a phrase embedding, and xw is a word embedding. We use this method for computing phrase embeddings as a baseline in our experiments. 5.2 Unfolding Recursive Auto-encoder The second model is more sophisticated, taking into account also the order of the words and the grammar used. An unfolding recursive auto-encoder (RAE) is used to derive the phrase embedding on the basis of a binary parse tree. The unfolding RAE was introduced by Socher et al. (2011) and uses two RvNNs, one for encoding the compressed representations, and one for decoding them to recover the original sentence, see Figure 6. The network is subsequently trained by minimizing the reconstruction error. Forward propagation in the network is done by recursively applying Eq. 5a and 5b for each triplet in the tree in two phases. First, starting at the center node (root of the tree) and recursively pulling the data from the input. Second, again starting at the center node, recursively pushing the data towards the output. Backpropagation is done in a similar manner using backpropag</context>
<context position="19912" citStr="Socher et al. (2011)" startWordPosition="3331" endWordPosition="3334">mary that is in the gold standard. F-score is a standard way to combine recall and precision, computed as F = 2 P ∗R P +R. 6.3 Implementation All results were obtained by running an implementation of Lin-Bilmes submodular optimization summarizer, as described in Sec. 2.1. Also, we have chosen to fix the length of the summaries to two sentences because the length of the goldstandard summaries are typically around two sentences. The CW vectors used were trained by Turian et al. (2010)1, and the Word2Vec vectors by Mikolov et al. (2013b)2. The unfolding RAE used is based on the implementation by Socher et al. (2011)3, and the parse trees for guiding the recursion was generated using the Stanford Parser (Klein and Manning, 2003)4. 6.4 Results The results from the ROUGE evaluation are compiled in Table 1. We find for all measures (recall, precision, and F-score), that the phrase embeddings outperform the original Lin-Bilmes. For recall, we find that CW_AddCos achieves the highest result, while for precision and F-score the CW_AddEuc perform best. These results are consistent for all versions of ROUGE scores reported (1, 2 and SU4), providing a strong indication for phrase embeddings in the context of autom</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In Advances in Neural Information Processing Systems 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="19779" citStr="Turian et al. (2010)" startWordPosition="3308" endWordPosition="3311">es what fraction of a human created gold standard summary that is captured, and precision measures what fraction of the generated summary that is in the gold standard. F-score is a standard way to combine recall and precision, computed as F = 2 P ∗R P +R. 6.3 Implementation All results were obtained by running an implementation of Lin-Bilmes submodular optimization summarizer, as described in Sec. 2.1. Also, we have chosen to fix the length of the summaries to two sentences because the length of the goldstandard summaries are typically around two sentences. The CW vectors used were trained by Turian et al. (2010)1, and the Word2Vec vectors by Mikolov et al. (2013b)2. The unfolding RAE used is based on the implementation by Socher et al. (2011)3, and the parse trees for guiding the recursion was generated using the Stanford Parser (Klein and Manning, 2003)4. 6.4 Results The results from the ROUGE evaluation are compiled in Table 1. We find for all measures (recall, precision, and F-score), that the phrase embeddings outperform the original Lin-Bilmes. For recall, we find that CW_AddCos achieves the highest result, while for precision and F-score the CW_AddEuc perform best. These results are consistent </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
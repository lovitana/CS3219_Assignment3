<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005154">
<title confidence="0.996974">
Power of Confidence:
How Poll Scores Impact Topic Dynamics in Political Debates
</title>
<author confidence="0.996401">
Vinodkumar Prabhakaran
</author>
<affiliation confidence="0.996418">
Dept. of Computer Science
Columbia University
</affiliation>
<address confidence="0.973698">
New York, NY
</address>
<email confidence="0.999111">
vinod@cs.columbia.edu
</email>
<author confidence="0.990312">
Ashima Arora
</author>
<affiliation confidence="0.9963775">
Dept. of Computer Science
Columbia University
</affiliation>
<address confidence="0.973509">
New York, NY
</address>
<email confidence="0.998348">
aa3470@columbia.edu
</email>
<author confidence="0.979552">
Owen Rambow
</author>
<affiliation confidence="0.968447">
CCLS
Columbia University
</affiliation>
<address confidence="0.970071">
New York, NY
</address>
<email confidence="0.99935">
rambow@ccls.columbia.edu
</email>
<sectionHeader confidence="0.993914" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957769230769">
In this paper, we investigate how topic dy-
namics during the course of an interaction
correlate with the power differences be-
tween its participants. We perform this
study on the US presidential debates and
show that a candidate’s power, modeled
after their poll scores, affects how often
he/she attempts to shift topics and whether
he/she succeeds. We ensure the validity
of topic shifts by confirming, through a
simple but effective method, that the turns
that shift topics provide substantive topical
content to the interaction.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970333333333">
Analyzing political speech has gathered great in-
terest within the NLP community. Researchers
have analyzed political text to identify markers of
persuasion (Guerini et al., 2008), predict voting
patterns (Thomas et al., 2006; Gerrish and Blei,
2011), and detect ideological positions (Sim et al.,
2013). Studies have also looked into how per-
sonal attributes of political personalities such as
charisma, confidence and power affect how they
interact (Rosenberg and Hirschberg, 2009; Prab-
hakaran et al., 2013b). Our work belongs to this
genre of studies. We analyze how a presidential
candidate’s power, modeled after his/her relative
poll standings, affect the dynamics of topic shifts
during the course of a presidential debate.
</bodyText>
<sectionHeader confidence="0.993395" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999971957446809">
In early work on correlating personal attributes
to political speech, Rosenberg and Hirschberg
(2009) analyzed speech transcripts in the con-
text of 2004 Democratic presidential primary elec-
tions, to identify prosodic and lexico-syntactic
cues that signal charisma of political personalities.
More recently, Prabhakaran et al. (2013a) intro-
duced the notion of power an election candidate
has at a certain point in the election campaign,
modeled after the confidence that stems from their
recent poll standings. They analyzed the 2012 Re-
publican presidential primary debates and found
that the candidate’s power at the time of a de-
bate impacts the structure of interactions (e.g., fre-
quency of turns and interruption patterns). They
followed up their study with an automatic ranker
to identify leading candidates based on the inter-
action within a debate (Prabhakaran et al., 2013b).
One of the interesting findings by Prabhakaran
et al. (2013a) was that candidates’ power corre-
lates with the distribution of topics they speak
about in the debates. They found that when can-
didates have more power, they speak significantly
more about certain topics (e.g., economy) and less
about certain other topics (e.g., energy). However,
these findings relate to the specific election cycle
they analyzed and will not carry over to all polit-
ical debates in general. A topical dimension with
broader relevance is how topics change during the
course of an interaction (e.g., who introduces more
topics, who attempts to shift topics etc.). For in-
stance, Nguyen et al. (2013) found that topic shifts
within an interaction are correlated with the role
a participant plays in it (e.g., being a moderator).
They also analyzed US presidential debates, but
with the objective of validating a topic segmenta-
tion method they proposed earlier (Nguyen et al.,
2012). They do not study the topic shifting ten-
dencies among the candidates in relation to their
power differences.
In this paper, we bring these two ideas together.
We analyze the 2012 Republican presidential de-
bates, modeling the power of a candidate based
on poll scores as proposed by Prabhakaran et al.
(2013a) and investigate various features that cap-
ture the topical dynamics in the debates. We show
that the power affects how often candidates at-
</bodyText>
<page confidence="0.982614">
77
</page>
<bodyText confidence="0.3738125">
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 77–82,
Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics
</bodyText>
<figure confidence="0.86533225">
Turn # Speaker Turn Text Substantive?
223 PAWLENTY (C) I support a constitutional amendment to define marriage between a man and [S]
woman. I was the co-author of the state – a law in Minnesota to define it
and now we have courts jumping over this.
224 KING (M) OK. Let’s just go through this. [NS]
225 PAUL (C) The federal government shouldn’t be involved. I wouldn’t support an [S]
amendment. [...] I don’t think government should give us a license to
get married. It should be in the church.
226 KING (M) Governor Romney, constitutional amendment or state decision? [NS]
227 ROMNEY (C) Constitutional. [NS]
228 KING (M) Mr. Speaker? [NS]
229 GINGRICH (C) Well, I helped author the Defense of Marriage Act which the Obama ad- [S]
ministration should be frankly protecting in court. [...]
[...]
235 CAIN (C) If I had my druthers, I never would have overturned ”don’t ask/don’t tell” [S]
in the first place. [...] Our men and women have too many other things to
be concerned about rather than have to deal with that as a distraction.
[...]
240 KING (M) Leave it in place, [...] or overturn it? [S]
241 ROMNEY (C) Well, one, we ought to be talking about the economy and jobs. But given [S]
</figure>
<tableCaption confidence="0.787712">
the fact you’re insistent, the – the answer is, I believe that ”don’t ask/don’t
tell” should have been kept in place until conflict was over.
Table 1: Excerpt from Goffstown, NH debate (06/13/11), discussing marriage equality and the “Don’t Ask/Don’t Tell” policy
[S]/ [NS] denote substantiveness of turns
</tableCaption>
<bodyText confidence="0.999943083333333">
tempt to shift topics and whether they succeed in
it or not. In order to correctly model topic shifts,
we ensure that the shifts happen in turns that con-
tribute substantial topical content to the interac-
tion. We introduce the notion of a “non-substantial
turn”, and use a simple, but effective method to au-
tomatically identify non-substantial turns. This al-
lows us to identify different topic segments within
the interaction, while permitting (and capturing)
interruptions within those segments. We will com-
pare the segments that we obtain with those by
Nguyen et al. (2012) in future work.
</bodyText>
<sectionHeader confidence="0.977589" genericHeader="method">
3 Domain and Data
</sectionHeader>
<bodyText confidence="0.999919916666667">
We use the same corpus as Prabhakaran et al.
(2013b). The corpus contains manual transcripts
of 20 debates held between May 2011 and Febru-
ary 2012 as part of the 2012 Republican pres-
idential primaries. The transcripts are obtained
from The American Presidency Project.1 Each
turn is clearly demarcated in the transcripts and
their speakers are identified. The turns in the cor-
pus are preprocessed using the Stanford CoreNLP
package to perform basic NLP steps such as tok-
enization, sentence segmentation, parts-of-speech
tagging and lemmatization. We show an excerpt
</bodyText>
<footnote confidence="0.838917">
1http://www.presidency.ucsb.edu/debates.php
</footnote>
<bodyText confidence="0.9999084">
from one of the debates in Table 1. This segment
of the debate discusses marriage equality followed
by the overturning of the “Don’t Ask/Don’t Tell”
policy prohibiting openly gay, lesbian, or bisexual
persons from US military service.
Prabhakaran et al. (2013b) added each candi-
date’s power at the time of each debate to the cor-
pus, computed based on their relative standing in
recent public polls. We refer the reader to (Prab-
hakaran et al., 2013b) for the detailed description
of how the relative standings in national and state-
level polls from various sources are aggregated to
obtain candidates’ power. The poll numbers cap-
ture how successful candidates are in convincing
the electorate of their candidature, which in turn
affects their confidence within the debates. These
debates serve as a rich domain to explore manifes-
tations of power since they are a medium through
which candidates pursue and maintain power over
other candidates.
</bodyText>
<sectionHeader confidence="0.946026" genericHeader="method">
4 Modeling Topics
</sectionHeader>
<bodyText confidence="0.9999294">
Prabhakaran et al. (2013a) model topics in the de-
bates using Latent Dirichlet Allocation (LDA), as-
signing topic probabilities to each turn. The num-
ber of topics was set to be 15 and the topic that was
assigned the highest probability for a turn was cho-
</bodyText>
<page confidence="0.989399">
78
</page>
<bodyText confidence="0.999986039215687">
sen as its topic. Assigning topics to each turn in
this manner, however, is problematic. Not all turns
by themselves contribute to the conversational top-
ics in an interaction. A large number of turns,
especially by the moderator, manage the conver-
sation rather than contribute content to it. These
include turns redirecting questions to specific can-
didates (e.g., turns 224, 226 and 228 in Table 1) as
well as moderator interruptions (e.g., “Quickly.”,
“We have to save time”). Furthermore, some other
turns address a topic only when considered to-
gether with preceding turns, but not when read in
isolation. These include turns that are short one-
word answers (e.g., turn 227) and turns that are
uninterpretable without resolving anaphora (e.g.,
“That’s right”). While these turns are substantive
to human readers, topic modeling approaches such
as LDA cannot assign them topics correctly be-
cause of their terseness.
We define the turns that do not, in isolation, con-
tribute substantially to the conversational topics as
non-substantive turns. In order to obtain a gold
standard for non-substantivity, two of the authors
manually annotated each turn in one entire debate
(dated 06/13/11) as either substantive (S) or non-
substantive (NS). The annotators were instructed
not to consider the identity of the speaker or the
context of the turn (preceding/following turns) in
making their assessment. We obtained a high
inter-annotator agreement (observed agreement =
89.3%; Kappa = .76). We took the assessments
by one of the annotators as the gold standard, in
which 108 (31.5%) of the 343 turns were identi-
fied as non-substantive. We show the S vs. NS
assessments for each turn in column 4 of Table 1.
Figure 1a shows the line graph of topic proba-
bilities assigned by LDA to the sequence of turns
in Table 1. As the graph shows, non-substantive
turns are assigned spurious topic probabilities by
LDA. For example, turn 224 by KING (“OK. Lets
just go through this.”) was assigned small prob-
abilities for all topics; the highest of which was
economy (probability of 0.12). This error is prob-
lematic when modeling topic shifts, since this turn
and the next one by PAUL would have been incor-
rectly identified as shifts in topic from their cor-
responding previous turns. Instead, if we assume
that the non-substantive turns follow the same
topic probabilities as the most recent substantive
turn, we obtain the line graph shown in Figure 1b.
This topic assignment captures the topic dynam-
</bodyText>
<figure confidence="0.9990195">
(a) Topic Probabilities assigned by LDA
(b) Topic Probabilities after ignoring non-substantive turns
</figure>
<figureCaption confidence="0.7698165">
Figure 1: Line graphs of topic probabilities for turns in
Table 1 (legend shows only the top 5 topics in this segment)
</figureCaption>
<bodyText confidence="0.999917833333333">
ics in the segment more accurately. It identifies
Gay Rights as the predominant topic until turn 234
followed by a mix of Gay Rights and Military as
topics while discussing the “Don’t Ask/Don’t Tell’
policy. It also captures the attempt by ROMNEY
in turn 242 to shift the topic to Economy.
</bodyText>
<subsectionHeader confidence="0.995261">
4.1 Identifying Non-substantive Turns
</subsectionHeader>
<bodyText confidence="0.999968260869565">
In order to automatically detect non-substantive
turns, we investigate a few alternatives. A simple
observation is that many of the NS turns such as
redirections of questions or short responses have
only a few words. We tried a word count thresh-
old based method (WC Thresh) where we assign
a turn to be NS if the number of tokens (words) in
the turn is less than a threshold. Another intuition
is that for a non-substantive turn, it would be hard
for the LDA to assign topics and hence all topics
will get almost equal probabilities assigned. In or-
der to capture this, we used a method based on a
standard deviation threshold (SD Thresh), where
we assign a turn to be NS if the standard deviation
of that turn’s topic probabilities is below a thresh-
old. We also used a combination system where
we tag a turn to be NS if either system tags it to
be. We tuned for the value of the thresholds and
the best performances obtained for each case are
shown in Table 2. We obtained the best results
for the WC Thresh method with a threshold of 28
words, while for SD Thresh the optimal threshold
is .13 (almost twice the mean).
</bodyText>
<page confidence="0.99386">
79
</page>
<table confidence="0.98525925">
Method Accuracy (%) F-measure
WC Thresh 82.6 73.7
SD Thresh 76.2 64.7
WC Thresh + SD Thresh 76.8 70.4
</table>
<tableCaption confidence="0.9516085">
Table 2: Accuracy and F-measure of different methods to
identify non-substantive turns
</tableCaption>
<subsectionHeader confidence="0.973537">
4.2 Topic Assignments
</subsectionHeader>
<bodyText confidence="0.999942">
We first ran the LDA at a turn-level for all debates,
keeping the number of topics to be 15, and se-
lected the best model after 2000 iterations. Then,
we ran the WC Thresh method described above to
detect NS turns. For all NS turns, we replace the
topic probabilities assigned by LDA with the last
substantive turn’s topic probabilities. Note that an
S turn coming after one or more NS turns could
still be of the same topic as the last S turn, i.e.,
non-substantivity of a turn is agnostic to whether
the topic changes after that or not. A topic shift (or
attempt) happens only when LDA assigns a differ-
ent topic to a substantive turn.
</bodyText>
<sectionHeader confidence="0.996566" genericHeader="method">
5 Topical Dimensions
</sectionHeader>
<bodyText confidence="0.9999648">
We now describe various features we use to cap-
ture the topical dynamics within each debate, with
respect to each candidate. When we compute a
feature value, we use the topic probabilities as-
signed to each turn as described in the previous
section. For some features we only use the topic
with the highest probability, while for some oth-
ers, we use the probabilities assigned to all topics.
We consider features along four dimensions which
we describe in detail below.
</bodyText>
<subsectionHeader confidence="0.988524">
5.1 Topic Shift Patterns
</subsectionHeader>
<bodyText confidence="0.9999755">
We build various features to capture how of-
ten a candidate stays on the topic being dis-
cussed. We say a candidate attempted to shift
the topic in a turn if the topic assigned to that
turn differs from the topic of the previous (sub-
stantive) turn. We use a feature to count the
number of times a candidate attempts to shift
topics within a debate (TS Attempt#) and a
version of that feature normalized over the to-
tal number of turns (TS Attempt#N). We also
use a variation of these features which consid-
ers only the instances of topic shift attempts by
the candidates when responding to a question
from the moderator (TS AttemptAfterMod# and
TS AttemptAfterMod#N). We also compute a
softer notion of topic shift where we measure the
average Euclidean distance between topic proba-
bilities of each of the candidate turns and turns
prior to them (EuclideanDist). This feature in
essence captures whether the candidate stayed on
topic, even if he/she did not completely switch
topics in a turn.
</bodyText>
<subsectionHeader confidence="0.998307">
5.2 Topic Shift Sustenance Patterns
</subsectionHeader>
<bodyText confidence="0.9995903">
We use a feature to capture the average number
of turns for which topic shifts by a candidate was
sustained (TS SustTurns). However, as discussed
in Section 4, the turns vary greatly in terms of
length. A more sensible measure is the time pe-
riod for which a topic shift was sustained. We
approximate the time by the number of word to-
kens and compute the average number of tokens
in the turns that topic shifts by a candidate were
sustained (TS SustTime).
</bodyText>
<subsectionHeader confidence="0.994528">
5.3 Topic Shift Success Patterns
</subsectionHeader>
<bodyText confidence="0.999883571428572">
We define a topic shift to be successful if it was
sustained for at least three turns. We compute
three features — total number of successful topic
shifts by a candidate (TS Success#), that number
normalized over the total number of turns by the
candidate (TS Success#N), and the success rate of
candidate’s topic shifts (TS SuccessRate)
</bodyText>
<subsectionHeader confidence="0.934855">
5.4 Topic Introduction Patterns
</subsectionHeader>
<bodyText confidence="0.9995442">
We also looked at cases where a candidate intro-
duces a new topic, i.e., shifts to a topic which
is entirely new for the debate. We use the num-
ber of topics introduced by a candidate as a fea-
ture (TS Intro#). We also use features to cap-
ture how important those topics were, measured
in terms of the number of turns about those top-
ics in the en tire debate (TS IntroImpTurns) and
the time spent on those topics in the entire debate
(TS IntroImpTime).
</bodyText>
<sectionHeader confidence="0.972338" genericHeader="method">
6 Analysis and Results
</sectionHeader>
<bodyText confidence="0.99991725">
We performed a correlation analysis on the fea-
tures described in the previous section with re-
spect to each candidate against the power he/she
had at the time of the debate (based on recent poll
scores). Figure 2 shows the Pearson’s product cor-
relation between each topical feature and candi-
date’s power. Dark bars denote statistically signif-
icant (p &lt; 0.05) features.
</bodyText>
<page confidence="0.996026">
80
</page>
<figureCaption confidence="0.998641">
Figure 2: Pearson Correlations for Topical Features
</figureCaption>
<bodyText confidence="0.999865148148148">
We obtained significant strong positive correla-
tion for TS Attempt# and TS AttemptAfterMod#.
However, the normalized measure TS Attempt#N
did not have any significant correlation, suggest-
ing that the correlation obtained for TS Attempt#
is mostly due to the fact that candidates with
more power have more turns, a finding that is al-
ready established by Prabhakaran et al. (2013b).
However, interestingly, we obtained a weak,
but statistically significant, negative correlation
for TS AttemptAfterMod#N which suggests that
more powerful candidates tend to stay on topic
when responding to moderators. We did not ob-
tain any correlation for EuclideanDist.
We did not obtain any significant correlations
between candidate’s power and their topic shift
sustenance features. We obtained significant cor-
relation for topic shift success (TS Success#),
modeled based on the sustenance of topic shifts,
suggesting that powerful candidates have a higher
number of successful topic shifts. However,
TS SuccessRate or TS Success#N did not obtain
any significant correlation. We also found that
powerful candidates are more likely to introduce
new topics (TS Intro#) and that the topics they in-
troduce tend to be important (TS IntroImpTurns
and TS IntroImpTime).
</bodyText>
<sectionHeader confidence="0.999846" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999962935483871">
Studies in sociolinguistics (e.g., (Ng et al., 1993;
Ng et al., 1995)) have explored how dialog struc-
ture in interactions relates to power and influence.
Reid and Ng (2000) identified that factors such as
frequency of contribution, proportion of turns, and
number of successful interruptions are important
indicators of influence. Within the dialog commu-
nity, researchers have studied notions of control
and initiative in dialogs (Walker and Whittaker,
1990; Jordan and Di Eugenio, 1997). Walker and
Whittaker (1990) define “control of communica-
tion” in terms of whether the discourse partici-
pants are providing new, unsolicited information
in their utterances. Their notion of control dif-
fers from our notion of power; however, the way
we model topic shifts is closely related to their
utterance level control assignment. Within the
NLP community, researchers have studied power
and influence in various genres of interactions,
such as organizational email threads (Bramsen et
al., 2011; Gilbert, 2012; Prabhakaran and Ram-
bow, 2013), online discussion forums (Danescu-
Niculescu-Mizil et al., 2012; Biran et al., 2012)
and online chat dialogs (Strzalkowski et al., 2012).
The correlates analyzed in these studies range
from word/phrase patterns, to derivatives of such
patterns such as linguistic coordination, to deeper
dialogic features such as argumentation and dialog
acts. Our work differs from these studies in that
we study the correlates of power in topic dynam-
ics. Furthermore, we analyze spoken interactions.
</bodyText>
<sectionHeader confidence="0.997292" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99998375">
We studied the topical dynamics in the 2012 US
presidential debates and investigated their corre-
lation with the power differences between candi-
dates. We showed that a candidate’s power, mod-
eled after their poll scores, has significant correla-
tion with how often he/she introduces new topics,
attempts to shift topics, and whether they succeed
in doing so. In order to ensure the validity of our
topic shifts we devised a simple yet effective way
to eliminate turns which do not provide substan-
tial topical content to the interaction. Furthermore,
this allowed us to identify different topic segments
within the interaction. In future work, we will ex-
plore how our way of identifying segments com-
pares to other approaches on topic segmentation
in interactions (e.g., (Nguyen et al., 2012)).
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999779">
This paper is based upon work supported by the
DARPA DEFT Program. The views expressed are
those of the authors and do not reflect the official
policy or position of the Department of Defense
or the U.S. Government. We also thank Debanjan
Ghosh and several anonymous reviewers for their
constructive feedback.
</bodyText>
<page confidence="0.99815">
81
</page>
<sectionHeader confidence="0.978455" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999010136363636">
Or Biran, Sara Rosenthal, Jacob Andreas, Kathleen
McKeown, and Owen Rambow. 2012. Detecting
influencers in written online conversations. In Pro-
ceedings of the Second Workshop on Language in
Social Media, pages 37–45, Montr´eal, Canada, June.
Association for Computational Linguistics.
Philip Bramsen, Martha Escobar-Molano, Ami Patel,
and Rafael Alonso. 2011. Extracting social power
relationships from natural language. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 773–782, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Cristian Danescu-Niculescu-Mizil, Lillian Lee,
Bo Pang, and Jon Kleinberg. 2012. Echoes of
power: language effects and power differences in
social interaction. In Proceedings of the 21st in-
ternational conference on World Wide Web, WWW
’12, New York, NY, USA. ACM.
Sean Gerrish and David Blei. 2011. Predicting legisla-
tive roll calls from text. In Lise Getoor and Tobias
Scheffer, editors, Proceedings of the 28th Interna-
tional Conference on Machine Learning, ICML ’11,
pages 489–496, New York, NY, USA, June. ACM.
Eric Gilbert. 2012. Phrases that signal workplace hier-
archy. In Proceedings of the ACM 2012 conference
on Computer Supported Cooperative Work, CSCW
’12, pages 1037–1046, New York, NY, USA. ACM.
Marco Guerini, Carlo Strapparava, and Oliviero Stock.
2008. Corps: A corpus of tagged political speeches
for persuasive communication processing. Journal
of Information Technology &amp; Politics, 5(1):19–32.
Pamela W. Jordan and Barbara Di Eugenio. 1997.
Control and initiative in collaborative problem solv-
ing dialogues. In Working Notes of the AAAI Spring
Symposium on Computational Models forMixed Ini-
tiative, pages 81–84.
Sik Hung Ng, Dean Bell, and Mark Brooke. 1993.
Gaining turns and achieving high in influence rank-
ing in small conversational groups. British Journal
of Social Psychology, pages 32, 265–275.
Sik Hung Ng, Mark Brooke, and Michael Dunne.
1995. Interruption and in influence in discussion
groups. Journal of Language and Social Psychol-
ogy, pages 14(4),369–381.
Viet-An Nguyen, Jordan Boyd-Graber, and Philip
Resnik. 2012. Sits: A hierarchical nonparametric
model using speaker identity for topic segmentation
in multiparty conversations. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 78–87, Jeju Island, Korea, July. Association
for Computational Linguistics.
Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,
Deborah A. Cai, Jennifer E. Midberry, and Yuanxin
Wang. 2013. Modeling topic control to detect in-
fluence in conversations using nonparametric topic
models. Machine Learning, pages 1–41.
Vinodkumar Prabhakaran and Owen Rambow. 2013.
Written dialog and social power: Manifestations of
different types of power in dialog behavior. In Pro-
ceedings of the IJCNLP, pages 216–224, Nagoya,
Japan, October. Asian Federation of Natural Lan-
guage Processing.
Vinodkumar Prabhakaran, Ajita John, and Dor´ee D.
Seligmann. 2013a. Power dynamics in spoken in-
teractions: a case study on 2012 republican primary
debates. In Proceedings of the 22nd international
conference on World Wide Web companion, pages
99–100. International World Wide Web Conferences
Steering Committee.
Vinodkumar Prabhakaran, Ajita John, and Dor´ee D.
Seligmann. 2013b. Who had the upper hand? rank-
ing participants of interactions based on their rela-
tive power. In Proceedings of the IJCNLP, pages
365–373, Nagoya, Japan, October. Asian Federation
of Natural Language Processing.
Scott A. Reid and Sik Hung Ng. 2000. Conversation as
a resource for in influence: evidence for prototypical
arguments and social identification processes. Euro-
pean Journal of Social Psych., pages 30, 83–100.
Andrew Rosenberg and Julia Hirschberg. 2009.
Charisma perception from text and speech. Speech
Communication, 51(7):640–655.
Yanchuan Sim, Brice D. L. Acree, Justin H. Gross, and
Noah A. Smith. 2013. Measuring ideological pro-
portions in political speeches. In Proceedings of the
2013 Conference on EMNLP, pages 91–101, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Tomek Strzalkowski, Samira Shaikh, Ting Liu,
George Aaron Broadwell, Jenny Stromer-Galley,
Sarah Taylor, Umit Boz, Veena Ravishankar, and
Xiaoai Ren. 2012. Modeling leadership and influ-
ence in multi-party online discourse. In Proceedings
of COLING, pages 2535–2552, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 327–335,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Marilyn Walker and Steve Whittaker. 1990. Mixed ini-
tiative in dialogue: An investigation into discourse
segmentation. In Proceedings of the 28th annual
meeting on Association for Computational Linguis-
tics, pages 70–78. Association for Computational
Linguistics.
</reference>
<page confidence="0.999126">
82
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.101718">
<title confidence="0.9923175">Power of Confidence: How Poll Scores Impact Topic Dynamics in Political Debates</title>
<author confidence="0.913023">Vinodkumar</author>
<affiliation confidence="0.999455">Dept. of Computer</affiliation>
<address confidence="0.8243935">Columbia New York, NY</address>
<email confidence="0.997521">vinod@cs.columbia.edu</email>
<author confidence="0.481173">Ashima</author>
<affiliation confidence="0.999304">Dept. of Computer</affiliation>
<address confidence="0.774134">Columbia New York, NY</address>
<email confidence="0.995865">aa3470@columbia.edu</email>
<author confidence="0.815681">Owen</author>
<affiliation confidence="0.745031">Columbia</affiliation>
<address confidence="0.901687">New York, NY</address>
<email confidence="0.999648">rambow@ccls.columbia.edu</email>
<abstract confidence="0.999610785714286">In this paper, we investigate how topic dynamics during the course of an interaction correlate with the power differences between its participants. We perform this study on the US presidential debates and show that a candidate’s power, modeled after their poll scores, affects how often he/she attempts to shift topics and whether he/she succeeds. We ensure the validity of topic shifts by confirming, through a simple but effective method, that the turns that shift topics provide substantive topical content to the interaction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Sara Rosenthal</author>
<author>Jacob Andreas</author>
<author>Kathleen McKeown</author>
<author>Owen Rambow</author>
</authors>
<title>Detecting influencers in written online conversations.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Language in Social Media,</booktitle>
<pages>37--45</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="18729" citStr="Biran et al., 2012" startWordPosition="3079" endWordPosition="3082">d Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely related to their utterance level control assignment. Within the NLP community, researchers have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online discussion forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word/phrase patterns, to derivatives of such patterns such as linguistic coordination, to deeper dialogic features such as argumentation and dialog acts. Our work differs from these studies in that we study the correlates of power in topic dynamics. Furthermore, we analyze spoken interactions. 8 Conclusion We studied the topical dynamics in the 2012 US presidential debates and investigated their correlation with the power differences between candidates. We showed that a candidate’s power, </context>
</contexts>
<marker>Biran, Rosenthal, Andreas, McKeown, Rambow, 2012</marker>
<rawString>Or Biran, Sara Rosenthal, Jacob Andreas, Kathleen McKeown, and Owen Rambow. 2012. Detecting influencers in written online conversations. In Proceedings of the Second Workshop on Language in Social Media, pages 37–45, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Bramsen</author>
<author>Martha Escobar-Molano</author>
<author>Ami Patel</author>
<author>Rafael Alonso</author>
</authors>
<title>Extracting social power relationships from natural language.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>773--782</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="18599" citStr="Bramsen et al., 2011" startWordPosition="3060" endWordPosition="3063">hers have studied notions of control and initiative in dialogs (Walker and Whittaker, 1990; Jordan and Di Eugenio, 1997). Walker and Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely related to their utterance level control assignment. Within the NLP community, researchers have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online discussion forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word/phrase patterns, to derivatives of such patterns such as linguistic coordination, to deeper dialogic features such as argumentation and dialog acts. Our work differs from these studies in that we study the correlates of power in topic dynamics. Furthermore, we analyze spoken interactions. 8 Conclusion We studied the topical dynamics in the 2012 US president</context>
</contexts>
<marker>Bramsen, Escobar-Molano, Patel, Alonso, 2011</marker>
<rawString>Philip Bramsen, Martha Escobar-Molano, Ami Patel, and Rafael Alonso. 2011. Extracting social power relationships from natural language. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 773–782, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Lillian Lee</author>
<author>Bo Pang</author>
<author>Jon Kleinberg</author>
</authors>
<title>Echoes of power: language effects and power differences in social interaction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference on World Wide Web, WWW ’12,</booktitle>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Danescu-Niculescu-Mizil, Lee, Pang, Kleinberg, 2012</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Lillian Lee, Bo Pang, and Jon Kleinberg. 2012. Echoes of power: language effects and power differences in social interaction. In Proceedings of the 21st international conference on World Wide Web, WWW ’12, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean Gerrish</author>
<author>David Blei</author>
</authors>
<title>Predicting legislative roll calls from text.</title>
<date>2011</date>
<booktitle>In Lise Getoor and Tobias Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning, ICML ’11,</booktitle>
<pages>489--496</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="1154" citStr="Gerrish and Blei, 2011" startWordPosition="165" endWordPosition="168">n the US presidential debates and show that a candidate’s power, modeled after their poll scores, affects how often he/she attempts to shift topics and whether he/she succeeds. We ensure the validity of topic shifts by confirming, through a simple but effective method, that the turns that shift topics provide substantive topical content to the interaction. 1 Introduction Analyzing political speech has gathered great interest within the NLP community. Researchers have analyzed political text to identify markers of persuasion (Guerini et al., 2008), predict voting patterns (Thomas et al., 2006; Gerrish and Blei, 2011), and detect ideological positions (Sim et al., 2013). Studies have also looked into how personal attributes of political personalities such as charisma, confidence and power affect how they interact (Rosenberg and Hirschberg, 2009; Prabhakaran et al., 2013b). Our work belongs to this genre of studies. We analyze how a presidential candidate’s power, modeled after his/her relative poll standings, affect the dynamics of topic shifts during the course of a presidential debate. 2 Motivation In early work on correlating personal attributes to political speech, Rosenberg and Hirschberg (2009) analy</context>
</contexts>
<marker>Gerrish, Blei, 2011</marker>
<rawString>Sean Gerrish and David Blei. 2011. Predicting legislative roll calls from text. In Lise Getoor and Tobias Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning, ICML ’11, pages 489–496, New York, NY, USA, June. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Gilbert</author>
</authors>
<title>Phrases that signal workplace hierarchy.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work, CSCW ’12,</booktitle>
<pages>1037--1046</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="18614" citStr="Gilbert, 2012" startWordPosition="3064" endWordPosition="3065">ons of control and initiative in dialogs (Walker and Whittaker, 1990; Jordan and Di Eugenio, 1997). Walker and Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely related to their utterance level control assignment. Within the NLP community, researchers have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online discussion forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word/phrase patterns, to derivatives of such patterns such as linguistic coordination, to deeper dialogic features such as argumentation and dialog acts. Our work differs from these studies in that we study the correlates of power in topic dynamics. Furthermore, we analyze spoken interactions. 8 Conclusion We studied the topical dynamics in the 2012 US presidential debates and</context>
</contexts>
<marker>Gilbert, 2012</marker>
<rawString>Eric Gilbert. 2012. Phrases that signal workplace hierarchy. In Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work, CSCW ’12, pages 1037–1046, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Guerini</author>
<author>Carlo Strapparava</author>
<author>Oliviero Stock</author>
</authors>
<title>Corps: A corpus of tagged political speeches for persuasive communication processing.</title>
<date>2008</date>
<journal>Journal of Information Technology &amp; Politics,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1083" citStr="Guerini et al., 2008" startWordPosition="154" endWordPosition="157">e power differences between its participants. We perform this study on the US presidential debates and show that a candidate’s power, modeled after their poll scores, affects how often he/she attempts to shift topics and whether he/she succeeds. We ensure the validity of topic shifts by confirming, through a simple but effective method, that the turns that shift topics provide substantive topical content to the interaction. 1 Introduction Analyzing political speech has gathered great interest within the NLP community. Researchers have analyzed political text to identify markers of persuasion (Guerini et al., 2008), predict voting patterns (Thomas et al., 2006; Gerrish and Blei, 2011), and detect ideological positions (Sim et al., 2013). Studies have also looked into how personal attributes of political personalities such as charisma, confidence and power affect how they interact (Rosenberg and Hirschberg, 2009; Prabhakaran et al., 2013b). Our work belongs to this genre of studies. We analyze how a presidential candidate’s power, modeled after his/her relative poll standings, affect the dynamics of topic shifts during the course of a presidential debate. 2 Motivation In early work on correlating persona</context>
</contexts>
<marker>Guerini, Strapparava, Stock, 2008</marker>
<rawString>Marco Guerini, Carlo Strapparava, and Oliviero Stock. 2008. Corps: A corpus of tagged political speeches for persuasive communication processing. Journal of Information Technology &amp; Politics, 5(1):19–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela W Jordan</author>
<author>Barbara Di Eugenio</author>
</authors>
<title>Control and initiative in collaborative problem solving dialogues.</title>
<date>1997</date>
<booktitle>In Working Notes of the AAAI Spring Symposium on Computational Models forMixed Initiative,</booktitle>
<pages>81--84</pages>
<marker>Jordan, Di Eugenio, 1997</marker>
<rawString>Pamela W. Jordan and Barbara Di Eugenio. 1997. Control and initiative in collaborative problem solving dialogues. In Working Notes of the AAAI Spring Symposium on Computational Models forMixed Initiative, pages 81–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sik Hung Ng</author>
<author>Dean Bell</author>
<author>Mark Brooke</author>
</authors>
<title>Gaining turns and achieving high in influence ranking in small conversational groups.</title>
<date>1993</date>
<journal>British Journal of Social Psychology,</journal>
<pages>32--265</pages>
<contexts>
<context position="17663" citStr="Ng et al., 1993" startWordPosition="2918" endWordPosition="2921">between candidate’s power and their topic shift sustenance features. We obtained significant correlation for topic shift success (TS Success#), modeled based on the sustenance of topic shifts, suggesting that powerful candidates have a higher number of successful topic shifts. However, TS SuccessRate or TS Success#N did not obtain any significant correlation. We also found that powerful candidates are more likely to introduce new topics (TS Intro#) and that the topics they introduce tend to be important (TS IntroImpTurns and TS IntroImpTime). 7 Related Work Studies in sociolinguistics (e.g., (Ng et al., 1993; Ng et al., 1995)) have explored how dialog structure in interactions relates to power and influence. Reid and Ng (2000) identified that factors such as frequency of contribution, proportion of turns, and number of successful interruptions are important indicators of influence. Within the dialog community, researchers have studied notions of control and initiative in dialogs (Walker and Whittaker, 1990; Jordan and Di Eugenio, 1997). Walker and Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information in their u</context>
</contexts>
<marker>Ng, Bell, Brooke, 1993</marker>
<rawString>Sik Hung Ng, Dean Bell, and Mark Brooke. 1993. Gaining turns and achieving high in influence ranking in small conversational groups. British Journal of Social Psychology, pages 32, 265–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sik Hung Ng</author>
<author>Mark Brooke</author>
<author>Michael Dunne</author>
</authors>
<title>Interruption and in influence in discussion groups.</title>
<date>1995</date>
<journal>Journal of Language and Social Psychology,</journal>
<pages>14--4</pages>
<contexts>
<context position="17681" citStr="Ng et al., 1995" startWordPosition="2922" endWordPosition="2925">’s power and their topic shift sustenance features. We obtained significant correlation for topic shift success (TS Success#), modeled based on the sustenance of topic shifts, suggesting that powerful candidates have a higher number of successful topic shifts. However, TS SuccessRate or TS Success#N did not obtain any significant correlation. We also found that powerful candidates are more likely to introduce new topics (TS Intro#) and that the topics they introduce tend to be important (TS IntroImpTurns and TS IntroImpTime). 7 Related Work Studies in sociolinguistics (e.g., (Ng et al., 1993; Ng et al., 1995)) have explored how dialog structure in interactions relates to power and influence. Reid and Ng (2000) identified that factors such as frequency of contribution, proportion of turns, and number of successful interruptions are important indicators of influence. Within the dialog community, researchers have studied notions of control and initiative in dialogs (Walker and Whittaker, 1990; Jordan and Di Eugenio, 1997). Walker and Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their n</context>
</contexts>
<marker>Ng, Brooke, Dunne, 1995</marker>
<rawString>Sik Hung Ng, Mark Brooke, and Michael Dunne. 1995. Interruption and in influence in discussion groups. Journal of Language and Social Psychology, pages 14(4),369–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viet-An Nguyen</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Sits: A hierarchical nonparametric model using speaker identity for topic segmentation in multiparty conversations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>78--87</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="3484" citStr="Nguyen et al., 2012" startWordPosition="530" endWordPosition="533">ever, these findings relate to the specific election cycle they analyzed and will not carry over to all political debates in general. A topical dimension with broader relevance is how topics change during the course of an interaction (e.g., who introduces more topics, who attempts to shift topics etc.). For instance, Nguyen et al. (2013) found that topic shifts within an interaction are correlated with the role a participant plays in it (e.g., being a moderator). They also analyzed US presidential debates, but with the objective of validating a topic segmentation method they proposed earlier (Nguyen et al., 2012). They do not study the topic shifting tendencies among the candidates in relation to their power differences. In this paper, we bring these two ideas together. We analyze the 2012 Republican presidential debates, modeling the power of a candidate based on poll scores as proposed by Prabhakaran et al. (2013a) and investigate various features that capture the topical dynamics in the debates. We show that the power affects how often candidates at77 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 77–82, Baltimore, Maryland USA, 27 June 2014. c�2</context>
<context position="6199" citStr="Nguyen et al. (2012)" startWordPosition="989" endWordPosition="992">icy [S]/ [NS] denote substantiveness of turns tempt to shift topics and whether they succeed in it or not. In order to correctly model topic shifts, we ensure that the shifts happen in turns that contribute substantial topical content to the interaction. We introduce the notion of a “non-substantial turn”, and use a simple, but effective method to automatically identify non-substantial turns. This allows us to identify different topic segments within the interaction, while permitting (and capturing) interruptions within those segments. We will compare the segments that we obtain with those by Nguyen et al. (2012) in future work. 3 Domain and Data We use the same corpus as Prabhakaran et al. (2013b). The corpus contains manual transcripts of 20 debates held between May 2011 and February 2012 as part of the 2012 Republican presidential primaries. The transcripts are obtained from The American Presidency Project.1 Each turn is clearly demarcated in the transcripts and their speakers are identified. The turns in the corpus are preprocessed using the Stanford CoreNLP package to perform basic NLP steps such as tokenization, sentence segmentation, parts-of-speech tagging and lemmatization. We show an excerpt</context>
</contexts>
<marker>Nguyen, Boyd-Graber, Resnik, 2012</marker>
<rawString>Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik. 2012. Sits: A hierarchical nonparametric model using speaker identity for topic segmentation in multiparty conversations. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 78–87, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viet-An Nguyen</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
<author>Deborah A Cai</author>
<author>Jennifer E Midberry</author>
<author>Yuanxin Wang</author>
</authors>
<title>Modeling topic control to detect influence in conversations using nonparametric topic models.</title>
<date>2013</date>
<booktitle>Machine Learning,</booktitle>
<pages>1--41</pages>
<contexts>
<context position="3203" citStr="Nguyen et al. (2013)" startWordPosition="485" endWordPosition="488">2013a) was that candidates’ power correlates with the distribution of topics they speak about in the debates. They found that when candidates have more power, they speak significantly more about certain topics (e.g., economy) and less about certain other topics (e.g., energy). However, these findings relate to the specific election cycle they analyzed and will not carry over to all political debates in general. A topical dimension with broader relevance is how topics change during the course of an interaction (e.g., who introduces more topics, who attempts to shift topics etc.). For instance, Nguyen et al. (2013) found that topic shifts within an interaction are correlated with the role a participant plays in it (e.g., being a moderator). They also analyzed US presidential debates, but with the objective of validating a topic segmentation method they proposed earlier (Nguyen et al., 2012). They do not study the topic shifting tendencies among the candidates in relation to their power differences. In this paper, we bring these two ideas together. We analyze the 2012 Republican presidential debates, modeling the power of a candidate based on poll scores as proposed by Prabhakaran et al. (2013a) and inve</context>
</contexts>
<marker>Nguyen, Boyd-Graber, Resnik, Cai, Midberry, Wang, 2013</marker>
<rawString>Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik, Deborah A. Cai, Jennifer E. Midberry, and Yuanxin Wang. 2013. Modeling topic control to detect influence in conversations using nonparametric topic models. Machine Learning, pages 1–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Owen Rambow</author>
</authors>
<title>Written dialog and social power: Manifestations of different types of power in dialog behavior.</title>
<date>2013</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of the IJCNLP,</booktitle>
<pages>216--224</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="18645" citStr="Prabhakaran and Rambow, 2013" startWordPosition="3066" endWordPosition="3070">and initiative in dialogs (Walker and Whittaker, 1990; Jordan and Di Eugenio, 1997). Walker and Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely related to their utterance level control assignment. Within the NLP community, researchers have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online discussion forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word/phrase patterns, to derivatives of such patterns such as linguistic coordination, to deeper dialogic features such as argumentation and dialog acts. Our work differs from these studies in that we study the correlates of power in topic dynamics. Furthermore, we analyze spoken interactions. 8 Conclusion We studied the topical dynamics in the 2012 US presidential debates and investigated their correlation</context>
</contexts>
<marker>Prabhakaran, Rambow, 2013</marker>
<rawString>Vinodkumar Prabhakaran and Owen Rambow. 2013. Written dialog and social power: Manifestations of different types of power in dialog behavior. In Proceedings of the IJCNLP, pages 216–224, Nagoya, Japan, October. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Ajita John</author>
<author>Dor´ee D Seligmann</author>
</authors>
<title>Power dynamics in spoken interactions: a case study on 2012 republican primary debates.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd international conference on World Wide Web companion,</booktitle>
<pages>99--100</pages>
<institution>International World Wide Web Conferences Steering Committee.</institution>
<contexts>
<context position="1411" citStr="Prabhakaran et al., 2013" startWordPosition="203" endWordPosition="207">ctive method, that the turns that shift topics provide substantive topical content to the interaction. 1 Introduction Analyzing political speech has gathered great interest within the NLP community. Researchers have analyzed political text to identify markers of persuasion (Guerini et al., 2008), predict voting patterns (Thomas et al., 2006; Gerrish and Blei, 2011), and detect ideological positions (Sim et al., 2013). Studies have also looked into how personal attributes of political personalities such as charisma, confidence and power affect how they interact (Rosenberg and Hirschberg, 2009; Prabhakaran et al., 2013b). Our work belongs to this genre of studies. We analyze how a presidential candidate’s power, modeled after his/her relative poll standings, affect the dynamics of topic shifts during the course of a presidential debate. 2 Motivation In early work on correlating personal attributes to political speech, Rosenberg and Hirschberg (2009) analyzed speech transcripts in the context of 2004 Democratic presidential primary elections, to identify prosodic and lexico-syntactic cues that signal charisma of political personalities. More recently, Prabhakaran et al. (2013a) introduced the notion of power</context>
<context position="3792" citStr="Prabhakaran et al. (2013" startWordPosition="582" endWordPosition="585"> For instance, Nguyen et al. (2013) found that topic shifts within an interaction are correlated with the role a participant plays in it (e.g., being a moderator). They also analyzed US presidential debates, but with the objective of validating a topic segmentation method they proposed earlier (Nguyen et al., 2012). They do not study the topic shifting tendencies among the candidates in relation to their power differences. In this paper, we bring these two ideas together. We analyze the 2012 Republican presidential debates, modeling the power of a candidate based on poll scores as proposed by Prabhakaran et al. (2013a) and investigate various features that capture the topical dynamics in the debates. We show that the power affects how often candidates at77 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 77–82, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics Turn # Speaker Turn Text Substantive? 223 PAWLENTY (C) I support a constitutional amendment to define marriage between a man and [S] woman. I was the co-author of the state – a law in Minnesota to define it and now we have courts jumping over this. 224 KING (M) </context>
<context position="6284" citStr="Prabhakaran et al. (2013" startWordPosition="1006" endWordPosition="1009">hey succeed in it or not. In order to correctly model topic shifts, we ensure that the shifts happen in turns that contribute substantial topical content to the interaction. We introduce the notion of a “non-substantial turn”, and use a simple, but effective method to automatically identify non-substantial turns. This allows us to identify different topic segments within the interaction, while permitting (and capturing) interruptions within those segments. We will compare the segments that we obtain with those by Nguyen et al. (2012) in future work. 3 Domain and Data We use the same corpus as Prabhakaran et al. (2013b). The corpus contains manual transcripts of 20 debates held between May 2011 and February 2012 as part of the 2012 Republican presidential primaries. The transcripts are obtained from The American Presidency Project.1 Each turn is clearly demarcated in the transcripts and their speakers are identified. The turns in the corpus are preprocessed using the Stanford CoreNLP package to perform basic NLP steps such as tokenization, sentence segmentation, parts-of-speech tagging and lemmatization. We show an excerpt 1http://www.presidency.ucsb.edu/debates.php from one of the debates in Table 1. This</context>
<context position="7828" citStr="Prabhakaran et al. (2013" startWordPosition="1246" endWordPosition="1249">ative standing in recent public polls. We refer the reader to (Prabhakaran et al., 2013b) for the detailed description of how the relative standings in national and statelevel polls from various sources are aggregated to obtain candidates’ power. The poll numbers capture how successful candidates are in convincing the electorate of their candidature, which in turn affects their confidence within the debates. These debates serve as a rich domain to explore manifestations of power since they are a medium through which candidates pursue and maintain power over other candidates. 4 Modeling Topics Prabhakaran et al. (2013a) model topics in the debates using Latent Dirichlet Allocation (LDA), assigning topic probabilities to each turn. The number of topics was set to be 15 and the topic that was assigned the highest probability for a turn was cho78 sen as its topic. Assigning topics to each turn in this manner, however, is problematic. Not all turns by themselves contribute to the conversational topics in an interaction. A large number of turns, especially by the moderator, manage the conversation rather than contribute content to it. These include turns redirecting questions to specific candidates (e.g., turns</context>
<context position="16725" citStr="Prabhakaran et al. (2013" startWordPosition="2779" endWordPosition="2782">ed on recent poll scores). Figure 2 shows the Pearson’s product correlation between each topical feature and candidate’s power. Dark bars denote statistically significant (p &lt; 0.05) features. 80 Figure 2: Pearson Correlations for Topical Features We obtained significant strong positive correlation for TS Attempt# and TS AttemptAfterMod#. However, the normalized measure TS Attempt#N did not have any significant correlation, suggesting that the correlation obtained for TS Attempt# is mostly due to the fact that candidates with more power have more turns, a finding that is already established by Prabhakaran et al. (2013b). However, interestingly, we obtained a weak, but statistically significant, negative correlation for TS AttemptAfterMod#N which suggests that more powerful candidates tend to stay on topic when responding to moderators. We did not obtain any correlation for EuclideanDist. We did not obtain any significant correlations between candidate’s power and their topic shift sustenance features. We obtained significant correlation for topic shift success (TS Success#), modeled based on the sustenance of topic shifts, suggesting that powerful candidates have a higher number of successful topic shifts.</context>
</contexts>
<marker>Prabhakaran, John, Seligmann, 2013</marker>
<rawString>Vinodkumar Prabhakaran, Ajita John, and Dor´ee D. Seligmann. 2013a. Power dynamics in spoken interactions: a case study on 2012 republican primary debates. In Proceedings of the 22nd international conference on World Wide Web companion, pages 99–100. International World Wide Web Conferences Steering Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Ajita John</author>
<author>Dor´ee D Seligmann</author>
</authors>
<title>Who had the upper hand? ranking participants of interactions based on their relative power.</title>
<date>2013</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of the IJCNLP,</booktitle>
<pages>365--373</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="1411" citStr="Prabhakaran et al., 2013" startWordPosition="203" endWordPosition="207">ctive method, that the turns that shift topics provide substantive topical content to the interaction. 1 Introduction Analyzing political speech has gathered great interest within the NLP community. Researchers have analyzed political text to identify markers of persuasion (Guerini et al., 2008), predict voting patterns (Thomas et al., 2006; Gerrish and Blei, 2011), and detect ideological positions (Sim et al., 2013). Studies have also looked into how personal attributes of political personalities such as charisma, confidence and power affect how they interact (Rosenberg and Hirschberg, 2009; Prabhakaran et al., 2013b). Our work belongs to this genre of studies. We analyze how a presidential candidate’s power, modeled after his/her relative poll standings, affect the dynamics of topic shifts during the course of a presidential debate. 2 Motivation In early work on correlating personal attributes to political speech, Rosenberg and Hirschberg (2009) analyzed speech transcripts in the context of 2004 Democratic presidential primary elections, to identify prosodic and lexico-syntactic cues that signal charisma of political personalities. More recently, Prabhakaran et al. (2013a) introduced the notion of power</context>
<context position="3792" citStr="Prabhakaran et al. (2013" startWordPosition="582" endWordPosition="585"> For instance, Nguyen et al. (2013) found that topic shifts within an interaction are correlated with the role a participant plays in it (e.g., being a moderator). They also analyzed US presidential debates, but with the objective of validating a topic segmentation method they proposed earlier (Nguyen et al., 2012). They do not study the topic shifting tendencies among the candidates in relation to their power differences. In this paper, we bring these two ideas together. We analyze the 2012 Republican presidential debates, modeling the power of a candidate based on poll scores as proposed by Prabhakaran et al. (2013a) and investigate various features that capture the topical dynamics in the debates. We show that the power affects how often candidates at77 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 77–82, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics Turn # Speaker Turn Text Substantive? 223 PAWLENTY (C) I support a constitutional amendment to define marriage between a man and [S] woman. I was the co-author of the state – a law in Minnesota to define it and now we have courts jumping over this. 224 KING (M) </context>
<context position="6284" citStr="Prabhakaran et al. (2013" startWordPosition="1006" endWordPosition="1009">hey succeed in it or not. In order to correctly model topic shifts, we ensure that the shifts happen in turns that contribute substantial topical content to the interaction. We introduce the notion of a “non-substantial turn”, and use a simple, but effective method to automatically identify non-substantial turns. This allows us to identify different topic segments within the interaction, while permitting (and capturing) interruptions within those segments. We will compare the segments that we obtain with those by Nguyen et al. (2012) in future work. 3 Domain and Data We use the same corpus as Prabhakaran et al. (2013b). The corpus contains manual transcripts of 20 debates held between May 2011 and February 2012 as part of the 2012 Republican presidential primaries. The transcripts are obtained from The American Presidency Project.1 Each turn is clearly demarcated in the transcripts and their speakers are identified. The turns in the corpus are preprocessed using the Stanford CoreNLP package to perform basic NLP steps such as tokenization, sentence segmentation, parts-of-speech tagging and lemmatization. We show an excerpt 1http://www.presidency.ucsb.edu/debates.php from one of the debates in Table 1. This</context>
<context position="7828" citStr="Prabhakaran et al. (2013" startWordPosition="1246" endWordPosition="1249">ative standing in recent public polls. We refer the reader to (Prabhakaran et al., 2013b) for the detailed description of how the relative standings in national and statelevel polls from various sources are aggregated to obtain candidates’ power. The poll numbers capture how successful candidates are in convincing the electorate of their candidature, which in turn affects their confidence within the debates. These debates serve as a rich domain to explore manifestations of power since they are a medium through which candidates pursue and maintain power over other candidates. 4 Modeling Topics Prabhakaran et al. (2013a) model topics in the debates using Latent Dirichlet Allocation (LDA), assigning topic probabilities to each turn. The number of topics was set to be 15 and the topic that was assigned the highest probability for a turn was cho78 sen as its topic. Assigning topics to each turn in this manner, however, is problematic. Not all turns by themselves contribute to the conversational topics in an interaction. A large number of turns, especially by the moderator, manage the conversation rather than contribute content to it. These include turns redirecting questions to specific candidates (e.g., turns</context>
<context position="16725" citStr="Prabhakaran et al. (2013" startWordPosition="2779" endWordPosition="2782">ed on recent poll scores). Figure 2 shows the Pearson’s product correlation between each topical feature and candidate’s power. Dark bars denote statistically significant (p &lt; 0.05) features. 80 Figure 2: Pearson Correlations for Topical Features We obtained significant strong positive correlation for TS Attempt# and TS AttemptAfterMod#. However, the normalized measure TS Attempt#N did not have any significant correlation, suggesting that the correlation obtained for TS Attempt# is mostly due to the fact that candidates with more power have more turns, a finding that is already established by Prabhakaran et al. (2013b). However, interestingly, we obtained a weak, but statistically significant, negative correlation for TS AttemptAfterMod#N which suggests that more powerful candidates tend to stay on topic when responding to moderators. We did not obtain any correlation for EuclideanDist. We did not obtain any significant correlations between candidate’s power and their topic shift sustenance features. We obtained significant correlation for topic shift success (TS Success#), modeled based on the sustenance of topic shifts, suggesting that powerful candidates have a higher number of successful topic shifts.</context>
</contexts>
<marker>Prabhakaran, John, Seligmann, 2013</marker>
<rawString>Vinodkumar Prabhakaran, Ajita John, and Dor´ee D. Seligmann. 2013b. Who had the upper hand? ranking participants of interactions based on their relative power. In Proceedings of the IJCNLP, pages 365–373, Nagoya, Japan, October. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott A Reid</author>
<author>Sik Hung Ng</author>
</authors>
<title>Conversation as a resource for in influence: evidence for prototypical arguments and social identification processes.</title>
<date>2000</date>
<journal>European Journal of Social Psych.,</journal>
<pages>30--83</pages>
<contexts>
<context position="17784" citStr="Reid and Ng (2000)" startWordPosition="2939" endWordPosition="2942">ft success (TS Success#), modeled based on the sustenance of topic shifts, suggesting that powerful candidates have a higher number of successful topic shifts. However, TS SuccessRate or TS Success#N did not obtain any significant correlation. We also found that powerful candidates are more likely to introduce new topics (TS Intro#) and that the topics they introduce tend to be important (TS IntroImpTurns and TS IntroImpTime). 7 Related Work Studies in sociolinguistics (e.g., (Ng et al., 1993; Ng et al., 1995)) have explored how dialog structure in interactions relates to power and influence. Reid and Ng (2000) identified that factors such as frequency of contribution, proportion of turns, and number of successful interruptions are important indicators of influence. Within the dialog community, researchers have studied notions of control and initiative in dialogs (Walker and Whittaker, 1990; Jordan and Di Eugenio, 1997). Walker and Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely re</context>
</contexts>
<marker>Reid, Ng, 2000</marker>
<rawString>Scott A. Reid and Sik Hung Ng. 2000. Conversation as a resource for in influence: evidence for prototypical arguments and social identification processes. European Journal of Social Psych., pages 30, 83–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Charisma perception from text and speech.</title>
<date>2009</date>
<journal>Speech Communication,</journal>
<volume>51</volume>
<issue>7</issue>
<contexts>
<context position="1385" citStr="Rosenberg and Hirschberg, 2009" startWordPosition="199" endWordPosition="202">rming, through a simple but effective method, that the turns that shift topics provide substantive topical content to the interaction. 1 Introduction Analyzing political speech has gathered great interest within the NLP community. Researchers have analyzed political text to identify markers of persuasion (Guerini et al., 2008), predict voting patterns (Thomas et al., 2006; Gerrish and Blei, 2011), and detect ideological positions (Sim et al., 2013). Studies have also looked into how personal attributes of political personalities such as charisma, confidence and power affect how they interact (Rosenberg and Hirschberg, 2009; Prabhakaran et al., 2013b). Our work belongs to this genre of studies. We analyze how a presidential candidate’s power, modeled after his/her relative poll standings, affect the dynamics of topic shifts during the course of a presidential debate. 2 Motivation In early work on correlating personal attributes to political speech, Rosenberg and Hirschberg (2009) analyzed speech transcripts in the context of 2004 Democratic presidential primary elections, to identify prosodic and lexico-syntactic cues that signal charisma of political personalities. More recently, Prabhakaran et al. (2013a) intr</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2009</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2009. Charisma perception from text and speech. Speech Communication, 51(7):640–655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanchuan Sim</author>
<author>Brice D L Acree</author>
<author>Justin H Gross</author>
<author>Noah A Smith</author>
</authors>
<title>Measuring ideological proportions in political speeches.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on EMNLP,</booktitle>
<pages>91--101</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="1207" citStr="Sim et al., 2013" startWordPosition="173" endWordPosition="176">ower, modeled after their poll scores, affects how often he/she attempts to shift topics and whether he/she succeeds. We ensure the validity of topic shifts by confirming, through a simple but effective method, that the turns that shift topics provide substantive topical content to the interaction. 1 Introduction Analyzing political speech has gathered great interest within the NLP community. Researchers have analyzed political text to identify markers of persuasion (Guerini et al., 2008), predict voting patterns (Thomas et al., 2006; Gerrish and Blei, 2011), and detect ideological positions (Sim et al., 2013). Studies have also looked into how personal attributes of political personalities such as charisma, confidence and power affect how they interact (Rosenberg and Hirschberg, 2009; Prabhakaran et al., 2013b). Our work belongs to this genre of studies. We analyze how a presidential candidate’s power, modeled after his/her relative poll standings, affect the dynamics of topic shifts during the course of a presidential debate. 2 Motivation In early work on correlating personal attributes to political speech, Rosenberg and Hirschberg (2009) analyzed speech transcripts in the context of 2004 Democra</context>
</contexts>
<marker>Sim, Acree, Gross, Smith, 2013</marker>
<rawString>Yanchuan Sim, Brice D. L. Acree, Justin H. Gross, and Noah A. Smith. 2013. Measuring ideological proportions in political speeches. In Proceedings of the 2013 Conference on EMNLP, pages 91–101, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
<author>Samira Shaikh</author>
<author>Ting Liu</author>
<author>George Aaron Broadwell</author>
<author>Jenny Stromer-Galley</author>
<author>Sarah Taylor</author>
<author>Umit Boz</author>
<author>Veena Ravishankar</author>
<author>Xiaoai Ren</author>
</authors>
<title>Modeling leadership and influence in multi-party online discourse.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>2535--2552</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="18781" citStr="Strzalkowski et al., 2012" startWordPosition="3087" endWordPosition="3090">cation” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely related to their utterance level control assignment. Within the NLP community, researchers have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online discussion forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word/phrase patterns, to derivatives of such patterns such as linguistic coordination, to deeper dialogic features such as argumentation and dialog acts. Our work differs from these studies in that we study the correlates of power in topic dynamics. Furthermore, we analyze spoken interactions. 8 Conclusion We studied the topical dynamics in the 2012 US presidential debates and investigated their correlation with the power differences between candidates. We showed that a candidate’s power, modeled after their poll scores, has significant cor</context>
</contexts>
<marker>Strzalkowski, Shaikh, Liu, Broadwell, Stromer-Galley, Taylor, Boz, Ravishankar, Ren, 2012</marker>
<rawString>Tomek Strzalkowski, Samira Shaikh, Ting Liu, George Aaron Broadwell, Jenny Stromer-Galley, Sarah Taylor, Umit Boz, Veena Ravishankar, and Xiaoai Ren. 2012. Modeling leadership and influence in multi-party online discourse. In Proceedings of COLING, pages 2535–2552, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>327--335</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="1129" citStr="Thomas et al., 2006" startWordPosition="161" endWordPosition="164"> perform this study on the US presidential debates and show that a candidate’s power, modeled after their poll scores, affects how often he/she attempts to shift topics and whether he/she succeeds. We ensure the validity of topic shifts by confirming, through a simple but effective method, that the turns that shift topics provide substantive topical content to the interaction. 1 Introduction Analyzing political speech has gathered great interest within the NLP community. Researchers have analyzed political text to identify markers of persuasion (Guerini et al., 2008), predict voting patterns (Thomas et al., 2006; Gerrish and Blei, 2011), and detect ideological positions (Sim et al., 2013). Studies have also looked into how personal attributes of political personalities such as charisma, confidence and power affect how they interact (Rosenberg and Hirschberg, 2009; Prabhakaran et al., 2013b). Our work belongs to this genre of studies. We analyze how a presidential candidate’s power, modeled after his/her relative poll standings, affect the dynamics of topic shifts during the course of a presidential debate. 2 Motivation In early work on correlating personal attributes to political speech, Rosenberg an</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: Determining support or opposition from congressional floor-debate transcripts. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 327–335, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Steve Whittaker</author>
</authors>
<title>Mixed initiative in dialogue: An investigation into discourse segmentation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>70--78</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="18069" citStr="Walker and Whittaker, 1990" startWordPosition="2979" endWordPosition="2982">dates are more likely to introduce new topics (TS Intro#) and that the topics they introduce tend to be important (TS IntroImpTurns and TS IntroImpTime). 7 Related Work Studies in sociolinguistics (e.g., (Ng et al., 1993; Ng et al., 1995)) have explored how dialog structure in interactions relates to power and influence. Reid and Ng (2000) identified that factors such as frequency of contribution, proportion of turns, and number of successful interruptions are important indicators of influence. Within the dialog community, researchers have studied notions of control and initiative in dialogs (Walker and Whittaker, 1990; Jordan and Di Eugenio, 1997). Walker and Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely related to their utterance level control assignment. Within the NLP community, researchers have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online discussion foru</context>
</contexts>
<marker>Walker, Whittaker, 1990</marker>
<rawString>Marilyn Walker and Steve Whittaker. 1990. Mixed initiative in dialogue: An investigation into discourse segmentation. In Proceedings of the 28th annual meeting on Association for Computational Linguistics, pages 70–78. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
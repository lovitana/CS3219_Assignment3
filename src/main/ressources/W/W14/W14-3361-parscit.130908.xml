<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.82586">
Bayesian Reordering Model with Feature Selection
</title>
<author confidence="0.925136">
Abdullah Alrajehab and Mahesan Niranjanb
</author>
<affiliation confidence="0.959784">
aComputer Research Institute, King Abdulaziz City for Science and Technology (KACST)
Riyadh, Saudi Arabia, asrajeh@kacst.edu.sa
bSchool of Electronics and Computer Science, University of Southampton
</affiliation>
<email confidence="0.514144">
Southampton, United Kingdom, {asar1a10, mn}@ecs.soton.ac.uk
</email>
<sectionHeader confidence="0.98362" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999829375">
In phrase-based statistical machine trans-
lation systems, variation in grammatical
structures between source and target lan-
guages can cause large movements of
phrases. Modeling such movements is cru-
cial in achieving translations of long sen-
tences that appear natural in the target lan-
guage. We explore generative learning
approach to phrase reordering in Arabic
to English. Formulating the reordering
problem as a classification problem and
using naive Bayes with feature selection,
we achieve an improvement in the BLEU
score over a lexicalized reordering model.
The proposed model is compact, fast and
scalable to a large corpus.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956444444445">
Currently, the dominant approach to machine
translation is statistical, starting from the math-
ematical formulations and algorithms for param-
eter estimation (Brown et al., 1988), further ex-
tended in (Brown et al., 1993). These early mod-
els, widely known as the IBM models, were word-
based. Recent extensions note that a better ap-
proach is to group collections of words, or phrases,
for translation together, resulting in a significant
focus these days on phrase-based statistical ma-
chine translation systems.
To deal with the alignment problem of one-
to-many word alignments in the IBM model
formulation, whereas phrase-based models may
have many-to-many translation relationships, IBM
models are trained in both directions, source to tar-
get and target to source, and their word alignments
are combined (Och and Ney, 2004).
While phrase-based systems are a significant
improvement over word-based approaches, a par-
ticular issue that emerges is long-range reorder-
ings at the phrase level (Galley and Manning,
2008). Analogous to speech recognition systems,
translation systems relied on language models to
produce more fluent translation. While early work
penalized phrase movements without considering
reorderings arising from vastly differing grammat-
ical structures across language pairs like Arabic-
English, many researchers considered lexical re-
ordering models that attempted to learn orienta-
tion based on content (Tillmann, 2004; Kumar
and Byrne, 2005; Koehn et al., 2005). These
approaches may suffer from the data sparseness
problem since many phrase pairs occur only once
(Nguyen et al., 2009).
As an alternative way of exploiting function ap-
proximation capabilities offered by machine learn-
ing methods, there is recent interest in formulating
a learning problem that aims to predict reorder-
ing from linguistic features that capture their con-
text. An example of this is the maximum entropy
method used by (Xiang et al., 2011; Nguyen et al.,
2009; Zens and Ney, 2006; Xiong et al., 2006).
In this work we apply a naive Bayes classifier,
combined with feature selection to address the re-
ordering problem. To the best of our knowledge,
this simple model of classification has not been
used in this context previously. We present em-
pirical results comparing our work and previously
proposed lexicalized reordering model. We show
that our model is scalable to large corpora.
The remainder of this paper is organized as fol-
lows. Section 2 discusses previous work in the
field and how that is related to our paper. Section 3
gives an overview of the baseline translation sys-
tem. Section 4 introduces the Bayesian reorder-
ing model and gives details of different inference
methods, while, Section 5 describes feature selec-
tion method. Section 6 presents the experiments
and reports the results evaluated as classification
and translation problems. Finally, we end the pa-
per with a summary of our conclusions and per-
spectives.
</bodyText>
<page confidence="0.977618">
477
</page>
<note confidence="0.730646">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 477–485,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.840292333333333">
Symbol Notation
f/e a source / target sentence (string)
¯f/¯e a source / target phrase sequence
N the number of examples
K the number of classes
(¯fn, ¯en) the n-th phrase pair in (¯f, ¯e)
on the orientation of ( fn, ¯en)
φ( fn, ¯en) the feature vector of (
fn, ¯en)
</bodyText>
<tableCaption confidence="0.999047">
Table 1: Notation used in this paper.
</tableCaption>
<sectionHeader confidence="0.998835" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99993862">
The phrase reordering model is a crucial compo-
nent of any translation system, particularly be-
tween language pairs with different grammatical
structures (e.g. Arabic-English). Adding a lex-
icalized reordering model consistently improved
the translation quality for several language pairs
(Koehn et al., 2005). The model tries to predict the
orientation of a phrase pair with respect to the pre-
vious adjacent target words. Ideally, the reorder-
ing model would predict the right position in the
target sentence given a source phrase, which is dif-
ficult to achieve. Therefore, positions are grouped
into limited orientations or classes.The orientation
probability for a phrase pair is simply based on the
relative occurrences in the training corpus.
The lexicalized reordering model has been ex-
tended to tackle long-distance reorderings (Gal-
ley and Manning, 2008). This takes into account
the hierarchical structure of the sentence when
considering such an orientation. Certain exam-
ples are often used to motivate syntax-based sys-
tems were handled by this hierarchical model, and
this approach is shown to improve translation per-
formance for several translation tasks with small
computational cost.
Despite the fact that the lexicalized reordering
model is always biased towards the most frequent
orientation for such a phrase pair, it may suffer
from a data sparseness problem since many phrase
pairs occur only once. Moreover, the context of
a phrase might affect its orientation, which is not
considered as well.
Adopting the idea of predicting orientation
based on content, it has been proposed to represent
each phrase pair by linguistic features as reorder-
ing evidence, and then train a classifier for predic-
tion. The maximum entropy classifier is a popu-
lar choice among many researchers (Zens and Ney,
2006; Xiong et al., 2006; Nguyen et al., 2009; Xi-
ang et al., 2011). Max-margin structure classifiers
were also proposed (Ni et al., 2011). Recently,
Cherry (2013) proposed using sparse features op-
timize BLEU with the decoder instead of training
a classifier independently.
We distinguish our work from the previous ones
in the following. We propose a fast reordering
model using a naive Bayes classifier with feature
selection. In this study, we undertake a compari-
son between our work and lexicalized reordering
model.
</bodyText>
<sectionHeader confidence="0.971902" genericHeader="method">
3 Baseline System
</sectionHeader>
<bodyText confidence="0.9998895">
In statistical machine translation, the most likely
translation ebest of an input sentence f can be
found by maximizing the probability p(elf), as
follows:
</bodyText>
<equation confidence="0.99501">
ebest = arg max
e p(e|f). (1)
</equation>
<bodyText confidence="0.998219">
A log-linear combination of different models
(features) is used for direct modeling of the poste-
rior probability p(elf) (Papineni et al., 1998; Och
and Ney, 2002):
</bodyText>
<equation confidence="0.983498">
λihi(f, e) (2)
</equation>
<bodyText confidence="0.99985285">
where the feature hi(f, e) is a score function
over sentence pairs. The translation model and the
language model are the main features in any sys-
tem although additional features h(.) can be inte-
grated easily (such as word penalty). State-of-the-
art systems usually have around ten features (i.e.
n = 10).
In phrase-based systems, the translation model
can capture the local meaning for each source
phrase. However, to capture the whole meaning
of a sentence, its translated phrases need to be in
the correct order. The language model, which en-
sures fluent translation, plays an important role in
reordering; however, it prefers sentences that are
grammatically correct without considering their
actual meaning. Besides that, it has a bias towards
short translations (Koehn, 2010). Therefore, de-
veloping a reordering model will improve the ac-
curacy particularly when translating between two
grammatically different languages.
</bodyText>
<subsectionHeader confidence="0.994722">
3.1 Lexicalized Reordering Model
</subsectionHeader>
<bodyText confidence="0.9973045">
Phrase reordering modeling involves formulat-
ing phrase movements as a classification problem
</bodyText>
<equation confidence="0.998785">
ebest = arg max
e
n
i=1
</equation>
<page confidence="0.990729">
478
</page>
<bodyText confidence="0.995372571428571">
where each phrase position considered as a class
(Tillmann, 2004). Some researchers classified
phrase movements into three categories (mono-
tone, swap, and discontinuous) but the classes can
be extended to any arbitrary number (Koehn and
Monz, 2005). In general, the distribution of phrase
orientation is:
</bodyText>
<equation confidence="0.982084">
¯ 1¯¯
fn, en) = Z h(fn, en, ok) . (3)
</equation>
<bodyText confidence="0.9999542">
This lexicalized reordering model is estimated
by relative frequency where each phrase pair
(¯fn, ¯en) with such an orientation (ok) is counted
and then normalized to yield the probability as fol-
lows:
The orientation class of a current phrase pair is
defined with respect to the previous target word
or phrase (i.e. word-based classes or phrase-based
classes). In the case of three categories (mono-
tone, swap, and discontinuous): monotone is the
previous source phrase (or word) that is previ-
ously adjacent to the current source phrase, swap
is the previous source phrase (or word) that is next-
adjacent to the current source phrase, and discon-
tinuous is not monotone or swap.
Galley and Manning (2008) extended the lex-
icalized reordering mode to tackle long-distance
phrase reorderings. Their hierarchical model en-
ables phrase movements that are more complex
than swaps between adjacent phrases.
</bodyText>
<sectionHeader confidence="0.987499" genericHeader="method">
4 Bayesian Reordering Model
</sectionHeader>
<bodyText confidence="0.999802826086956">
Many feature-based reordering models have been
proposed to replace the lexicalized reordering
model. The reported results showed consistent im-
provement in terms of various translation metrics.
Naive Bayes method has been a popular clas-
sification model of choice in many natural lan-
guage processing problems (e.g. text classifica-
tion). Naive Bayes is a simple classifier that ig-
nores correlation between features, but has the ap-
peal of computational simplicity. It is a generative
probabilistic model based on Bayes’ theorem as
below:
The class prior can be estimated easily as a rel-
ative frequency (i.e. p(ok) = NkN ). The likeli-
hood distribution p(¯fn, ¯en|ok) is defined based on
the type of data. The classifier will be naive if we
assume that feature variables are conditionally in-
dependent. The naive assumption simplifies our
distribution and hence reduces the parameters that
have to be estimated. In text processing, multi-
nomial is used as a class-conditional distribution
(Rogers and Girolami, 2011). The distribution is
defined as:
</bodyText>
<equation confidence="0.973428875">
Y
p( ¯fn, ¯en|q) = C
m
where C is a multinomial coefficient,
=
C
(Pm φm(
Qm φm(
</equation>
<bodyText confidence="0.993858">
and q are a set of parameters, each of which is a
probability. Estimating these parameters for each
class by maximum likelihood,
</bodyText>
<equation confidence="0.9590845">
p( fn, ¯en|qk), (8)
will result in (Rogers and Girolami, 2011):
</equation>
<bodyText confidence="0.9975845">
MAP estimate It is clear that qkm might be
zero which means the probability of a new phrase
pair with nonzero feature φm( fn, ¯en) is always
zero because of the product in (6). Putting a prior
over q is one smoothing technique. A conjugate
prior for the multinomial likelihood is the Dirich-
let distribution and the MAP estimate for qkm is
(Rogers and Girolami, 2011):
</bodyText>
<equation confidence="0.965224">
α − 1 + PNk
n φm( ¯fn, ¯en)
M(α − 1) + PM/ Pn k φm/ (¯fn, en)
(10)
</equation>
<bodyText confidence="0.998494333333333">
where M is the feature vector’s length or the
feature dictionary size and α is a Dirichlet param-
eter with a value greater than one. The derivation
is in Appendix A.
Bayesian inference Instead of using a point es-
timate of q as shown previously in equation (10),
Bayesian inference is based on the whole param-
eter space in order to incorporate uncertainty into
our multinomial model. This requires a posterior
</bodyText>
<equation confidence="0.96899896969697">
count(
P
o count(
fn, ¯en, ok) (4)
¯fn, en, o)
p(ok |fn, ¯en) =
p(ok |¯fn, ¯en) =Pp( o p(
¯fn, ¯en|o)p(o). (5)
fn,¯en|ok)p(ok)
p(ok|
(6)
fn,¯en)
qφm(
m
(7)
¯fn, ¯en)! ,
fn, ¯en))!
Nk
Y
n
arg max
Qk
PNk
qkm =Mn φm (¯fn, en)
P
m/ Pn k φm/ (¯fn, en) (9)
qkm =
479
probability distribution over q as follows:
Zp( ¯fn, ¯en|ok) = p(¯fn, ¯en|qk)p(qk|αk) dqk
Q
m Γ(αkm + φm( fn, ¯en))
Γ (Pm αkm + φm( ¯fn, ¯en)~.
</equation>
<bodyText confidence="0.999939857142857">
Here αk are new hyperparameters of the pos-
terior derived by means of Bayes theorem as fol-
lows:
For completeness we give a summary of deriva-
tions of equations (11) and (13) in Appendix B,
more detailed discussions can be found in (Barber,
2012).
</bodyText>
<sectionHeader confidence="0.982783" genericHeader="method">
5 Feature Selection
</sectionHeader>
<bodyText confidence="0.999926882352941">
In several high dimensional pattern classification
problems, there is increasing evidence that the
discriminant information may be in small sub-
spaces, motivating feature selection (Li and Niran-
jan, 2013). Having irrelevant or redundant fea-
tures could affect the classification performance
(Liu and Motoda, 1998). They might mislead the
learning algorithms or overfit them to the data and
thus have less accuracy.
The aim of feature selection is to find the op-
timal subset features which maximize the ability
of prediction, which is the main concern, or sim-
plify the learned results to be more understand-
able. There are many ways to measure the good-
ness of a feature or a subset of features; however
the criterion will be discussed is mutual informa-
tion.
</bodyText>
<subsectionHeader confidence="0.934198">
5.1 Mutual Information
</subsectionHeader>
<bodyText confidence="0.999980333333333">
Information criteria are based on the concept of
entropy which is the amount of randomness. The
distribution of a fair coin, for example, is com-
pletely random so the entropy of the coin is very
high. The following equation calculates the en-
tropy of a variable X (MacKay, 2002):
</bodyText>
<equation confidence="0.981376666666667">
X
H (X) = − p(x) log p(x). (14)
x
</equation>
<bodyText confidence="0.9984098">
The mutual information of a feature X can be mea-
sured by calculating the difference between the
prior uncertainty of the class variable Y and the
posterior uncertainty after using the feature as fol-
lows (MacKay, 2002):
</bodyText>
<equation confidence="0.927869666666667">
I(X;Y ) = H(Y ) − H(Y |X) (15)
p(x, y) log p(x, y)
p(x)p(y).
</equation>
<bodyText confidence="0.995516833333333">
The advantage of mutual Information over other
criteria is the ability to detect nonlinear patterns.
The disadvantage is its bias towards higher ar-
bitrary features; however this problem can be
solved by normalizing the information as follows
(Est´evez et al., 2009):
</bodyText>
<equation confidence="0.997565">
I(X; Y )
Inorm(X; Y ) = min(H(X), H(Y )). (16)
</equation>
<sectionHeader confidence="0.995799" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.998435833333333">
The corpus used in our experiments is MultiUN
which is a large-scale parallel corpus extracted
from the United Nations website1 (Eisele and
Chen, 2010). We have used Arabic and English
portion of MultiUN. Table 2 shows the general
statistics.
</bodyText>
<table confidence="0.9973636">
Statistics Arabic English
Sentence Pairs 9.7 M
Running Words 255.5 M 285.7 M
Word/Line 22 25
Vocabulary Size 677 K 410 K
</table>
<tableCaption confidence="0.9712915">
Table 2: General statistics of Arabic-English Mul-
tiUN (M: million, K: thousand).
</tableCaption>
<bodyText confidence="0.9998307">
We simplify the problem by classifying phrase
movements into three categories (monotone,
swap, discontinuous). To train the reordering
models, we used GIZA++ to produce word align-
ments (Och and Ney, 2000). Then, we used the
extract tool that comes with the Moses 2 toolkit
(Koehn et al., 2007) in order to extract phrase pairs
along with their orientation classes.
Each extracted phrase pair is represented by lin-
guistic features as follows:
</bodyText>
<listItem confidence="0.930643">
• Aligned source and target words in a phrase
pair. Each word alignment is a feature.
</listItem>
<footnote confidence="0.994911666666667">
1http://www.ods.un.org/ods/
2Moses is an open source toolkit for statistical machine
translation (www.statmt.org/moses/).
</footnote>
<equation confidence="0.979786916666667">
p(qk|αk) = p(qk|α) QNk
n p(¯fn, ¯en|qk)
The solution of (11) will result in:
αk = α + XNk Φ( fn, ¯en). (13)
n
R p(qk|α) QNk .
n p(¯fn, ¯en|qk)dqk
=C Γ (Pm αkm)
Q
m Γ(αkm)
X=
x,y
</equation>
<page confidence="0.939185">
480
</page>
<bodyText confidence="0.984302">
• Words within a window around the source
phrase to capture the context. We choose ad-
jacent words of the phrase boundary.
Most researchers build one reordering model
for the whole training set (Zens and Ney, 2006;
Xiong et al., 2006; Nguyen et al., 2009; Xiang
et al., 2011). Ni et al. (Ni et al., 2011) simpli-
fied the learning problem to have as many sub-
models as source phrases. Training data were di-
vided into small independent sets where samples
having the same source phrase are considered a
training set. In our experiments, we have chosen
the first method.
We compare lexicalized and Bayesian reorder-
ing models in two phases. In the classification
phase, we see the performance of the models as
a classification problem. In the translation phase,
we test the actual impact of these reordering mod-
els in a translation system.
</bodyText>
<subsectionHeader confidence="0.980726">
6.1 Classification
</subsectionHeader>
<bodyText confidence="0.998589181818182">
We built naive Bayes classifier with both MAP es-
timate and Bayesian inference. We also used mu-
tual Information in order to select the most infor-
mative features for our classification task.
Table 3 reports the error rate of the reorder-
ing models compared to the lexicalized reorder-
ing model. All experiments reported here were
repeated three times to evaluate the uncertainties
in our results. The results shows that there is no
advantage to using Bayesian inference instead of
MAP estimate.
</bodyText>
<table confidence="0.997593">
Classifier Error Rate
Lexicalized model 25.2%
Bayes-MAP estimate 19.53%
Bayes-Bayesian inference 20.13%
</table>
<tableCaption confidence="0.9835315">
Table 3: Classification error rate of both lexical-
ized and Bayesian models.
</tableCaption>
<bodyText confidence="0.9995125625">
The feature selection process reveals that many
features have low mutual information. Hence they
are not related to the classification task and can be
excluded from the model. Figure 1 shows the nor-
malized mutual information for all extracted fea-
tures.
A ranking threshold for selecting features based
on their mutual information is specified experi-
mentally. In Figure 2, we tried different thresh-
olds ranging from 0.001 to 0.05 and measure the
error rate after each reduction. Although there
is no much gain in terms of performance but the
Bayesian model maintains low error rate when the
proportion of selected features is low. The model
with almost half of the feature space is as good as
the one with full feature space.
</bodyText>
<figureCaption confidence="0.996927">
Figure 1: Normalized mutual information for all
extracted features (ranked from lowest to highest).
</figureCaption>
<figure confidence="0.9619725">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Percentage of Feature Reductuion
</figure>
<figureCaption confidence="0.9851255">
Figure 2: Classification error rate of the Baysien
model with different levels of feature reduction.
</figureCaption>
<subsectionHeader confidence="0.793385">
6.2 Translation
6.2.1 Experimental Design
</subsectionHeader>
<bodyText confidence="0.99995525">
We used the Moses toolkit (Koehn et al., 2007)
with its default settings. The language model
is a 5-gram with interpolation and Kneser-Ney
smoothing (Kneser and Ney, 1995). We tuned the
system by using MERT technique (Och, 2003).
We built four Arabic-English translation sys-
tems. Three systems differ in how their reordering
models were estimated and the fourth system is a
</bodyText>
<figure confidence="0.973925">
22.5
22
21.5
20
19.5
19
21
Error Rate
20.5
</figure>
<page confidence="0.997802">
481
</page>
<bodyText confidence="0.998800714285714">
baseline system without reordering model. In all
cases, orientation extraction is hierarchical-based
since it is the best approach while orientations are
monotone, swap and discontinuous. The model is
trained in Moses by specifying the configuration
string hier-msd-backward-fe.
As commonly used in statistical machine trans-
lation, we evaluated the translation performance
by BLEU score (Papineni et al., 2002). The test
sets are NIST MT06 and NIST MT08. Table 4
shows statistics of development and test sets. We
also computed statistical significance for the pro-
posed models using the paired bootstrap resam-
pling method (Koehn, 2004).
</bodyText>
<table confidence="0.999803285714286">
Evaluation Set Arabic English
Development sentences 696 696
words 19 K 21 K
NIST MT06 sentences 1797 7188
words 49 K 223 K
NIST MT08 sentences 813 3252
words 25 K 117 K
</table>
<tableCaption confidence="0.734785333333333">
Table 4: Statistics of development and test sets.
The English side in NIST is larger because there
are four translations for each Arabic sentence.
</tableCaption>
<sectionHeader confidence="0.819392" genericHeader="evaluation">
6.2.2 Results
</sectionHeader>
<bodyText confidence="0.995619190476191">
We first demonstrate in Table 5 a general com-
parison of the proposed model and the lexicalized
model in terms of disc size and average speed in a
translation system. The size of Bayesian model is
far smaller. The lexicalized model is slightly faster
than the Bayesian model because we have over-
head computational cost to extract features and
compute the orientation probabilities. However,
the disc size of our model is much smaller which
makes it more efficient practically for large-scale
tasks.
Table 5: Disc size and average speed of the re-
ordering models in a translation system.
Table 6 shows the BLEU scores for the transla-
tion systems according to two test sets. The base-
line system has no reordering model. In the two
test sets, our Bayesian reordering model is better
than the lexicalized one with at least 95% statis-
tical significance. As we have seen in the clas-
sification section, Bayes classifier with Bayesian
inference has no advantage over MAP estimate.
</bodyText>
<table confidence="0.9999514">
Translation System MT06 MT08
Baseline 28.92 32.13
BL+ Lexicalized model 30.86 34.22
BL+ Bayes-MAP estimate 31.21* 34.72*
BL+ Bayes-Baysien inference 31.20 34.69
</table>
<tableCaption confidence="0.951970666666667">
Table 6: BLEU scores for Arabic-English trans-
lation systems (*: better than the baseline with at
least 95% statistical significance).
</tableCaption>
<sectionHeader confidence="0.986641" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999968805555556">
In this paper, we have presented generative mod-
eling approach to phrase reordering in machine
translation. We have experimented with trans-
lation from Arabic to English and shown im-
provements over the lexicalized model of estimat-
ing probabilities as relative frequencies of phrase
movements. Our proposed Bayesian model with
feature selection is shown to be superior. The
training time of the model is as fast as the lexical-
ized model. Its storage requirement is many times
smaller which makes it more efficient practically
for large-scale tasks.
The feature selection process reveals that many
features have low mutual information. Hence they
are not related to the classification task and can be
excluded from the model. The model with almost
half of the feature space is as good as the one with
full feature space.
Previously proposed discriminative models
might achieve higher score than the reported re-
sults. However, our model is scalable to large-
scale systems since parameter estimation require
only one pass over the data with limited memory
(i.e. no iterative learning). This is a critical advan-
tage over discriminative models.
Our current work focuses on three issues. The
first is improving the translation speed of the pro-
posed model. The lexicalized model is slightly
faster. The second is using more informative fea-
tures. We plan to explore part-of-speech informa-
tion, which is more accurate in capturing content.
Finally, we will explore different feature selection
methods. In our experiments, feature reduction is
based on univariate ranking which is riskier than
multivariate ranking. This is because useless fea-
ture can be useful with others.
</bodyText>
<figure confidence="0.937533777777778">
Size (MB)
Speed (s/sent)
Model
Lexicalized model
Bayesian model
604
18
2.2
2.6
</figure>
<page confidence="0.997848">
482
</page>
<bodyText confidence="0.99281975">
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the ACL 2007 Demo and Poster Sessions, pages
177–180.
</bodyText>
<note confidence="0.917783666666667">
References
D. Barber. 2012. Bayesian Reasoning and Machine
Learning. Cambridge University Press.
</note>
<reference confidence="0.99590842">
P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra,
F. Jelinek, R. Mercer, and P. Roossin. 1988. A sta-
tistical approach to language translation. In 12th In-
ternational Conference on Computational Linguis-
tics (COLING), pages 71–76.
P. Brown, V. Pietra, S. Pietra, and R. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263–311.
C. Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 22–
31, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.
A. Eisele and Y. Chen. 2010. Multiun: A multilingual
corpus from united nation documents. In Daniel
Tapias, Mike Rosner, Stelios Piperidis, Jan Odjik,
Joseph Mariani, Bente Maegaard, Khalid Choukri,
and Nicoletta Calzolari (Conference Chair), editors,
Proceedings of the Seventh conference on Interna-
tional Language Resources and Evaluation, pages
2868–2872. European Language Resources Associ-
ation (ELRA), 5.
P. Est´evez, M. Tesmer, C. Perez, and J. Zurada. 2009.
Normalized mutual information feature selection.
Trans. Neur. Netw., 20(2):189–201, February.
M. Galley and C. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
848–856, Hawaii, October. Association for Compu-
tational Linguistics.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing, pages 181–184.
P. Koehn and C. Monz. 2005. Shared task: Sta-
tistical machine translation between european lan-
guages. In Proceedings of ACL Workshop on Build-
ing and Using Parallel Texts, pages 119–124. Asso-
ciation for Computational Linguistics.
P. Koehn, A. Axelrod, A. Mayne, C. Callison-Burch,
M. Osborne, and D. Talbot. 2005. Edinburgh sys-
tem description for the 2005 IWSLT speech trans-
lation evaluation. In Proceedings of International
Workshop on Spoken Language Translation, Pitts-
burgh, PA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.
P. Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
S. Kumar and W. Byrne. 2005. Local phrase reorder-
ing models for statistical machine translation. In
Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in
Natural Language Processing, pages 161–168, Van-
couver, British Columbia, Canada, October. Associ-
ation for Computational Linguistics.
Hongyu Li and M. Niranjan. 2013. Discriminant sub-
spaces of some high dimensional pattern classifica-
tion problems. In IEEE International Workshop on
Machine Learning for Signal Processing (MLSP),
pages 27–32.
H. Liu and H. Motoda. 1998. Feature Selection for
Knowledge Discovery and Data Mining. Kluwer
Academic Publishers, Norwell, MA, USA.
D. MacKay. 2002. Information Theory, Inference &amp;
Learning Algorithms. Cambridge University Press,
New York, NY, USA.
V. Nguyen, A. Shimazu, M. Nguyen, and T. Nguyen.
2009. Improving a lexicalized hierarchical reorder-
ing model using maximum entropy. In Proceed-
ings of the Twelfth Machine Translation Summit (MT
Summit XII). International Association for Machine
Translation.
Y. Ni, C. Saunders, S. Szedmak, and M. Niranjan.
2011. Exploitation of machine learning techniques
in modelling phrase movements for machine transla-
tion. Journal of Machine Learning Research, 12:1–
30, February.
F. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of the 38th Annual
Meeting of the Association of Computational Lin-
guistics (ACL).
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
F. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417–449.
</reference>
<page confidence="0.996202">
483
</page>
<reference confidence="0.999290388888889">
F. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics - Volume 1, ACL ’03, pages 160–167,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
K. Papineni, S. Roukos, and T. Ward. 1998. Max-
imum likelihood and discriminative training of di-
rect translation models. In Proceedings of ICASSP,
pages 189–192.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 311–318, Stroudsburg, PA, USA. Association
for Computational Linguistics.
S. Rogers and M. Girolami. 2011. A First Course in
Machine Learning. Chapman &amp; Hall/CRC, 1st edi-
tion.
C. Tillmann. 2004. A unigram orientation model for
statistical machine translation. In Proceedings of
HLT-NAACL: Short Papers, pages 101–104.
B. Xiang, N. Ge, and A. Ittycheriah. 2011. Improving
reordering for statistical machine translation with
smoothed priors and syntactic features. In Proceed-
ings of SSST-5, Fifth Workshop on Syntax, Semantics
and Structure in Statistical Translation, pages 61–
69, Portland, Oregon, USA. Association for Com-
putational Linguistics.
D. Xiong, Q. Liu, and S. Lin. 2006. Maximum en-
tropy based phrase reordering model for statistical
machine translation. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the ACL, pages
521–528, Sydney, July. Association for Computa-
tional Linguistics.
</reference>
<bodyText confidence="0.999663333333333">
A maximum a posteriori probability (MAP) es-
timate requires a prior over q. Dirichlet distribu-
tion is a conjugate prior and is defined as:
</bodyText>
<equation confidence="0.9969826">
Γ (P m αm)
Y
p(q|α) = Q qαm−1 (19)
m
m Γ(αm) m
</equation>
<bodyText confidence="0.999750333333333">
where αm is is a parameter with a positive value.
Finding the MAP estimate for q given a data is
as follows:
</bodyText>
<equation confidence="0.992825642857143">
q∗ = arg max p(q|α, X)
4
{p(q|α)p(X|q)}
( )
Y
p(q|α) p(xn|q)
n
)qm−1 Y qm
n,m
= arg max
4 (X )
m n,m
log qm−1 + Xlog qxnm .
m
</equation>
<bodyText confidence="0.964058571428571">
(20)
Since our function is subject to constraints
(Pm qm = 1), we introduce Lagrange multiplier
as follows:
(21)
Now we can find q∗ by taking the partial deriva-
tive with respect to one variable qm:
</bodyText>
<figure confidence="0.9511507">
= arg max
4
= arg max
4
= arg max
4
(Y
m
f(q) = X log qm−1+X log qm m −λ (X qm−1).
m n,m m
</figure>
<reference confidence="0.4449564">
R. Zens and H. Ney. 2006. Discriminative reorder- ∂f(q) αm − 1 + Pn xnm λ
ing models for statistical machine translation. In ∂qm qm . (22)
Proceedings on the Workshop on Statistical Machine qm = αm − 1 + Pn xnm
Translation, pages 55–63, New York City, June. As- λ
sociation for Computational Linguistics.
</reference>
<bodyText confidence="0.98301">
Finally, we sum both sides over M to find λ :
</bodyText>
<sectionHeader confidence="0.605525" genericHeader="references">
A MAP Estimate Derivation
</sectionHeader>
<bodyText confidence="0.489446333333333">
Multinomial distribution is defined as: λX X Xαm − 1 + !xnm
qm = n
m m
</bodyText>
<equation confidence="0.579852666666667">
Y qxm (17) X X xnm. (23)
p(x|q) = C m λ = (αm − 1) +
m m n,m
where C is a multinomial coefficient,
C = (P Q m xm)! (18)
m xm! ,
</equation>
<bodyText confidence="0.987442">
and qm is an event probability (Pm qm = 1).
The solution can be simplified by choosing the
same value for each αm which will result in:
</bodyText>
<equation confidence="0.89208475">
α − 1 + Pn xnm
M(α − 1) + Pn,m&apos; xnm&apos; .
qm =
(24)
</equation>
<page confidence="0.998139">
484
</page>
<subsectionHeader confidence="0.455746">
B Bayesian Inference Derivation
</subsectionHeader>
<bodyText confidence="0.999101888888889">
In Appendix A, the inference is based on a single
point estimate of q that has the highest posterior
probability. However, it can be based on the whole
parameter space to incorporate uncertainty. The
probability of a new data point marginalized over
the posterior as follows:
Finally, we expand and re-arrange Dirichlet and
multinomial distributions inside the integral in
(25) as follows:
</bodyText>
<equation confidence="0.998325333333333">
p(x|α, X) =
Z Y Γ (P m α∗ m) Y qα∗ m−1
C qxm Q m dq
m m Γ(α∗ m)
m m
Zp(x|α, X) = p(x|q)p(q|α, X) dq, (25)
p(q|α, X) = p(q|α)p(X|q)
(26)
R p(q|α)p(X|q)dq.
</equation>
<bodyText confidence="0.9998084">
Since Dirichlet and Multinomial distributions
are conjugate pairs, they form the same density as
the prior. Therefore the posterior is also Dirichlet.
Now we can expand the posterior expression and
re-arrange it to look like a Dirichlet as follows:
</bodyText>
<equation confidence="0.995647857142857">
Y
p(q|α, X) ∝ p(q|α) p(xn|q)
n
qmαm−1 Y
n
(αm+Y- n xnm)−1
qm .(27)
</equation>
<bodyText confidence="0.999762">
Note that inside the integral looks a Dirichlet
without a normalizing constant. If we multiply
and divide by its normalizing constant (i.e. Beta
function), the integral is going to be one because
it is a density function, resulting in:
</bodyText>
<figure confidence="0.993087875">
Y∝
m
Y∝
m
Yqxnm
m
m
m)
p(x|α, X) = C Γ (Pm α∗
Q m Γ(α∗m)
Z 1 Y
B(α∗ + x) qα∗ m+xm−1 dqc
m
B(α∗ + x) m
=C
Γ (Pm α∗ m) B(α∗ + x)
m)
Q m Γ(α∗
=C Γ (Pm α∗ m)
Q m Γ(α∗m)
Qm Γ(α∗ m + xm) Γ (Pm (α∗ m + xm)). (30)
=C 7r7(Pm α∗m)Z Y qm +x�-1 dq. (29)
H. lm Γ(α∗m)
m
</figure>
<bodyText confidence="0.815468">
The new hyperparameters of the posterior is:
</bodyText>
<equation confidence="0.8045465">
Xα∗m = αm + xnm. (28)
n
</equation>
<page confidence="0.995259">
485
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.451404">
<title confidence="0.999704">Bayesian Reordering Model with Feature Selection</title>
<author confidence="0.966765">Mahesan</author>
<affiliation confidence="0.836604">Research Institute, King Abdulaziz City for Science and Technology Riyadh, Saudi Arabia, of Electronics and Computer Science, University of United Kingdom,</affiliation>
<abstract confidence="0.994206705882353">In phrase-based statistical machine translation systems, variation in grammatical structures between source and target languages can cause large movements of phrases. Modeling such movements is crucial in achieving translations of long sentences that appear natural in the target language. We explore generative learning approach to phrase reordering in Arabic to English. Formulating the reordering problem as a classification problem and using naive Bayes with feature selection, we achieve an improvement in the BLEU score over a lexicalized reordering model. The proposed model is compact, fast and scalable to a large corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>F Jelinek</author>
<author>R Mercer</author>
<author>P Roossin</author>
</authors>
<title>A statistical approach to language translation.</title>
<date>1988</date>
<booktitle>In 12th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>71--76</pages>
<contexts>
<context position="1180" citStr="Brown et al., 1988" startWordPosition="160" endWordPosition="163">n achieving translations of long sentences that appear natural in the target language. We explore generative learning approach to phrase reordering in Arabic to English. Formulating the reordering problem as a classification problem and using naive Bayes with feature selection, we achieve an improvement in the BLEU score over a lexicalized reordering model. The proposed model is compact, fast and scalable to a large corpus. 1 Introduction Currently, the dominant approach to machine translation is statistical, starting from the mathematical formulations and algorithms for parameter estimation (Brown et al., 1988), further extended in (Brown et al., 1993). These early models, widely known as the IBM models, were wordbased. Recent extensions note that a better approach is to group collections of words, or phrases, for translation together, resulting in a significant focus these days on phrase-based statistical machine translation systems. To deal with the alignment problem of oneto-many word alignments in the IBM model formulation, whereas phrase-based models may have many-to-many translation relationships, IBM models are trained in both directions, source to target and target to source, and their word </context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Mercer, Roossin, 1988</marker>
<rawString>P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, R. Mercer, and P. Roossin. 1988. A statistical approach to language translation. In 12th International Conference on Computational Linguistics (COLING), pages 71–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>V Pietra</author>
<author>S Pietra</author>
<author>R Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1222" citStr="Brown et al., 1993" startWordPosition="168" endWordPosition="171"> that appear natural in the target language. We explore generative learning approach to phrase reordering in Arabic to English. Formulating the reordering problem as a classification problem and using naive Bayes with feature selection, we achieve an improvement in the BLEU score over a lexicalized reordering model. The proposed model is compact, fast and scalable to a large corpus. 1 Introduction Currently, the dominant approach to machine translation is statistical, starting from the mathematical formulations and algorithms for parameter estimation (Brown et al., 1988), further extended in (Brown et al., 1993). These early models, widely known as the IBM models, were wordbased. Recent extensions note that a better approach is to group collections of words, or phrases, for translation together, resulting in a significant focus these days on phrase-based statistical machine translation systems. To deal with the alignment problem of oneto-many word alignments in the IBM model formulation, whereas phrase-based models may have many-to-many translation relationships, IBM models are trained in both directions, source to target and target to source, and their word alignments are combined (Och and Ney, 2004</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, V. Pietra, S. Pietra, and R. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
</authors>
<title>Improved reordering for phrasebased translation using sparse features.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>22--31</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="6390" citStr="Cherry (2013)" startWordPosition="987" endWordPosition="988"> sparseness problem since many phrase pairs occur only once. Moreover, the context of a phrase might affect its orientation, which is not considered as well. Adopting the idea of predicting orientation based on content, it has been proposed to represent each phrase pair by linguistic features as reordering evidence, and then train a classifier for prediction. The maximum entropy classifier is a popular choice among many researchers (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Max-margin structure classifiers were also proposed (Ni et al., 2011). Recently, Cherry (2013) proposed using sparse features optimize BLEU with the decoder instead of training a classifier independently. We distinguish our work from the previous ones in the following. We propose a fast reordering model using a naive Bayes classifier with feature selection. In this study, we undertake a comparison between our work and lexicalized reordering model. 3 Baseline System In statistical machine translation, the most likely translation ebest of an input sentence f can be found by maximizing the probability p(elf), as follows: ebest = arg max e p(e|f). (1) A log-linear combination of different </context>
</contexts>
<marker>Cherry, 2013</marker>
<rawString>C. Cherry. 2013. Improved reordering for phrasebased translation using sparse features. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 22– 31, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Eisele</author>
<author>Y Chen</author>
</authors>
<title>Multiun: A multilingual corpus from united nation documents.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh conference on International Language Resources and Evaluation,</booktitle>
<pages>2868--2872</pages>
<editor>In Daniel Tapias, Mike Rosner, Stelios Piperidis, Jan Odjik, Joseph Mariani, Bente Maegaard, Khalid Choukri, and Nicoletta Calzolari (Conference Chair), editors,</editor>
<contexts>
<context position="14152" citStr="Eisele and Chen, 2010" startWordPosition="2293" endWordPosition="2296"> and the posterior uncertainty after using the feature as follows (MacKay, 2002): I(X;Y ) = H(Y ) − H(Y |X) (15) p(x, y) log p(x, y) p(x)p(y). The advantage of mutual Information over other criteria is the ability to detect nonlinear patterns. The disadvantage is its bias towards higher arbitrary features; however this problem can be solved by normalizing the information as follows (Est´evez et al., 2009): I(X; Y ) Inorm(X; Y ) = min(H(X), H(Y )). (16) 6 Experiments The corpus used in our experiments is MultiUN which is a large-scale parallel corpus extracted from the United Nations website1 (Eisele and Chen, 2010). We have used Arabic and English portion of MultiUN. Table 2 shows the general statistics. Statistics Arabic English Sentence Pairs 9.7 M Running Words 255.5 M 285.7 M Word/Line 22 25 Vocabulary Size 677 K 410 K Table 2: General statistics of Arabic-English MultiUN (M: million, K: thousand). We simplify the problem by classifying phrase movements into three categories (monotone, swap, discontinuous). To train the reordering models, we used GIZA++ to produce word alignments (Och and Ney, 2000). Then, we used the extract tool that comes with the Moses 2 toolkit (Koehn et al., 2007) in order to </context>
</contexts>
<marker>Eisele, Chen, 2010</marker>
<rawString>A. Eisele and Y. Chen. 2010. Multiun: A multilingual corpus from united nation documents. In Daniel Tapias, Mike Rosner, Stelios Piperidis, Jan Odjik, Joseph Mariani, Bente Maegaard, Khalid Choukri, and Nicoletta Calzolari (Conference Chair), editors, Proceedings of the Seventh conference on International Language Resources and Evaluation, pages 2868–2872. European Language Resources Association (ELRA), 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Est´evez</author>
<author>M Tesmer</author>
<author>C Perez</author>
<author>J Zurada</author>
</authors>
<title>Normalized mutual information feature selection.</title>
<date>2009</date>
<journal>Trans. Neur. Netw.,</journal>
<volume>20</volume>
<issue>2</issue>
<marker>Est´evez, Tesmer, Perez, Zurada, 2009</marker>
<rawString>P. Est´evez, M. Tesmer, C. Perez, and J. Zurada. 2009. Normalized mutual information feature selection. Trans. Neur. Netw., 20(2):189–201, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>C Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>848--856</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Hawaii,</location>
<contexts>
<context position="2014" citStr="Galley and Manning, 2008" startWordPosition="291" endWordPosition="294">ranslation together, resulting in a significant focus these days on phrase-based statistical machine translation systems. To deal with the alignment problem of oneto-many word alignments in the IBM model formulation, whereas phrase-based models may have many-to-many translation relationships, IBM models are trained in both directions, source to target and target to source, and their word alignments are combined (Och and Ney, 2004). While phrase-based systems are a significant improvement over word-based approaches, a particular issue that emerges is long-range reorderings at the phrase level (Galley and Manning, 2008). Analogous to speech recognition systems, translation systems relied on language models to produce more fluent translation. While early work penalized phrase movements without considering reorderings arising from vastly differing grammatical structures across language pairs like ArabicEnglish, many researchers considered lexical reordering models that attempted to learn orientation based on content (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). These approaches may suffer from the data sparseness problem since many phrase pairs occur only once (Nguyen et al., 2009). As an altern</context>
<context position="5282" citStr="Galley and Manning, 2008" startWordPosition="807" endWordPosition="811">ed the translation quality for several language pairs (Koehn et al., 2005). The model tries to predict the orientation of a phrase pair with respect to the previous adjacent target words. Ideally, the reordering model would predict the right position in the target sentence given a source phrase, which is difficult to achieve. Therefore, positions are grouped into limited orientations or classes.The orientation probability for a phrase pair is simply based on the relative occurrences in the training corpus. The lexicalized reordering model has been extended to tackle long-distance reorderings (Galley and Manning, 2008). This takes into account the hierarchical structure of the sentence when considering such an orientation. Certain examples are often used to motivate syntax-based systems were handled by this hierarchical model, and this approach is shown to improve translation performance for several translation tasks with small computational cost. Despite the fact that the lexicalized reordering model is always biased towards the most frequent orientation for such a phrase pair, it may suffer from a data sparseness problem since many phrase pairs occur only once. Moreover, the context of a phrase might affe</context>
<context position="9253" citStr="Galley and Manning (2008)" startWordPosition="1442" endWordPosition="1445">ach phrase pair (¯fn, ¯en) with such an orientation (ok) is counted and then normalized to yield the probability as follows: The orientation class of a current phrase pair is defined with respect to the previous target word or phrase (i.e. word-based classes or phrase-based classes). In the case of three categories (monotone, swap, and discontinuous): monotone is the previous source phrase (or word) that is previously adjacent to the current source phrase, swap is the previous source phrase (or word) that is nextadjacent to the current source phrase, and discontinuous is not monotone or swap. Galley and Manning (2008) extended the lexicalized reordering mode to tackle long-distance phrase reorderings. Their hierarchical model enables phrase movements that are more complex than swaps between adjacent phrases. 4 Bayesian Reordering Model Many feature-based reordering models have been proposed to replace the lexicalized reordering model. The reported results showed consistent improvement in terms of various translation metrics. Naive Bayes method has been a popular classification model of choice in many natural language processing problems (e.g. text classification). Naive Bayes is a simple classifier that ig</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>M. Galley and C. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848–856, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="18012" citStr="Kneser and Ney, 1995" startWordPosition="2932" endWordPosition="2935">roportion of selected features is low. The model with almost half of the feature space is as good as the one with full feature space. Figure 1: Normalized mutual information for all extracted features (ranked from lowest to highest). 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Percentage of Feature Reductuion Figure 2: Classification error rate of the Baysien model with different levels of feature reduction. 6.2 Translation 6.2.1 Experimental Design We used the Moses toolkit (Koehn et al., 2007) with its default settings. The language model is a 5-gram with interpolation and Kneser-Ney smoothing (Kneser and Ney, 1995). We tuned the system by using MERT technique (Och, 2003). We built four Arabic-English translation systems. Three systems differ in how their reordering models were estimated and the fourth system is a 22.5 22 21.5 20 19.5 19 21 Error Rate 20.5 481 baseline system without reordering model. In all cases, orientation extraction is hierarchical-based since it is the best approach while orientations are monotone, swap and discontinuous. The model is trained in Moses by specifying the configuration string hier-msd-backward-fe. As commonly used in statistical machine translation, we evaluated the t</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backing-off for m-gram language modeling. IEEE International Conference on Acoustics, Speech and Signal Processing, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>C Monz</author>
</authors>
<title>Shared task: Statistical machine translation between european languages.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>119--124</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8457" citStr="Koehn and Monz, 2005" startWordPosition="1308" endWordPosition="1311">ing. Besides that, it has a bias towards short translations (Koehn, 2010). Therefore, developing a reordering model will improve the accuracy particularly when translating between two grammatically different languages. 3.1 Lexicalized Reordering Model Phrase reordering modeling involves formulating phrase movements as a classification problem ebest = arg max e n i=1 478 where each phrase position considered as a class (Tillmann, 2004). Some researchers classified phrase movements into three categories (monotone, swap, and discontinuous) but the classes can be extended to any arbitrary number (Koehn and Monz, 2005). In general, the distribution of phrase orientation is: ¯ 1¯¯ fn, en) = Z h(fn, en, ok) . (3) This lexicalized reordering model is estimated by relative frequency where each phrase pair (¯fn, ¯en) with such an orientation (ok) is counted and then normalized to yield the probability as follows: The orientation class of a current phrase pair is defined with respect to the previous target word or phrase (i.e. word-based classes or phrase-based classes). In the case of three categories (monotone, swap, and discontinuous): monotone is the previous source phrase (or word) that is previously adjacen</context>
</contexts>
<marker>Koehn, Monz, 2005</marker>
<rawString>P. Koehn and C. Monz. 2005. Shared task: Statistical machine translation between european languages. In Proceedings of ACL Workshop on Building and Using Parallel Texts, pages 119–124. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>A Axelrod</author>
<author>A Mayne</author>
<author>C Callison-Burch</author>
<author>M Osborne</author>
<author>D Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of International Workshop on Spoken Language Translation,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="2476" citStr="Koehn et al., 2005" startWordPosition="355" endWordPosition="358"> significant improvement over word-based approaches, a particular issue that emerges is long-range reorderings at the phrase level (Galley and Manning, 2008). Analogous to speech recognition systems, translation systems relied on language models to produce more fluent translation. While early work penalized phrase movements without considering reorderings arising from vastly differing grammatical structures across language pairs like ArabicEnglish, many researchers considered lexical reordering models that attempted to learn orientation based on content (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). These approaches may suffer from the data sparseness problem since many phrase pairs occur only once (Nguyen et al., 2009). As an alternative way of exploiting function approximation capabilities offered by machine learning methods, there is recent interest in formulating a learning problem that aims to predict reordering from linguistic features that capture their context. An example of this is the maximum entropy method used by (Xiang et al., 2011; Nguyen et al., 2009; Zens and Ney, 2006; Xiong et al., 2006). In this work we apply a naive Bayes classifier, combined with feature selection t</context>
<context position="4731" citStr="Koehn et al., 2005" startWordPosition="721" endWordPosition="724">Notation f/e a source / target sentence (string) ¯f/¯e a source / target phrase sequence N the number of examples K the number of classes (¯fn, ¯en) the n-th phrase pair in (¯f, ¯e) on the orientation of ( fn, ¯en) φ( fn, ¯en) the feature vector of ( fn, ¯en) Table 1: Notation used in this paper. 2 Related Work The phrase reordering model is a crucial component of any translation system, particularly between language pairs with different grammatical structures (e.g. Arabic-English). Adding a lexicalized reordering model consistently improved the translation quality for several language pairs (Koehn et al., 2005). The model tries to predict the orientation of a phrase pair with respect to the previous adjacent target words. Ideally, the reordering model would predict the right position in the target sentence given a source phrase, which is difficult to achieve. Therefore, positions are grouped into limited orientations or classes.The orientation probability for a phrase pair is simply based on the relative occurrences in the training corpus. The lexicalized reordering model has been extended to tackle long-distance reorderings (Galley and Manning, 2008). This takes into account the hierarchical struct</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>P. Koehn, A. Axelrod, A. Mayne, C. Callison-Burch, M. Osborne, and D. Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proceedings of International Workshop on Spoken Language Translation, Pittsburgh, PA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
</authors>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, </marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="18896" citStr="Koehn, 2004" startWordPosition="3072" endWordPosition="3073">t reordering model. In all cases, orientation extraction is hierarchical-based since it is the best approach while orientations are monotone, swap and discontinuous. The model is trained in Moses by specifying the configuration string hier-msd-backward-fe. As commonly used in statistical machine translation, we evaluated the translation performance by BLEU score (Papineni et al., 2002). The test sets are NIST MT06 and NIST MT08. Table 4 shows statistics of development and test sets. We also computed statistical significance for the proposed models using the paired bootstrap resampling method (Koehn, 2004). Evaluation Set Arabic English Development sentences 696 696 words 19 K 21 K NIST MT06 sentences 1797 7188 words 49 K 223 K NIST MT08 sentences 813 3252 words 25 K 117 K Table 4: Statistics of development and test sets. The English side in NIST is larger because there are four translations for each Arabic sentence. 6.2.2 Results We first demonstrate in Table 5 a general comparison of the proposed model and the lexicalized model in terms of disc size and average speed in a translation system. The size of Bayesian model is far smaller. The lexicalized model is slightly faster than the Bayesian </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7909" citStr="Koehn, 2010" startWordPosition="1230" endWordPosition="1231">al features h(.) can be integrated easily (such as word penalty). State-of-theart systems usually have around ten features (i.e. n = 10). In phrase-based systems, the translation model can capture the local meaning for each source phrase. However, to capture the whole meaning of a sentence, its translated phrases need to be in the correct order. The language model, which ensures fluent translation, plays an important role in reordering; however, it prefers sentences that are grammatically correct without considering their actual meaning. Besides that, it has a bias towards short translations (Koehn, 2010). Therefore, developing a reordering model will improve the accuracy particularly when translating between two grammatically different languages. 3.1 Lexicalized Reordering Model Phrase reordering modeling involves formulating phrase movements as a classification problem ebest = arg max e n i=1 478 where each phrase position considered as a class (Tillmann, 2004). Some researchers classified phrase movements into three categories (monotone, swap, and discontinuous) but the classes can be extended to any arbitrary number (Koehn and Monz, 2005). In general, the distribution of phrase orientation</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>P. Koehn. 2010. Statistical Machine Translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Local phrase reordering models for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>161--168</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="2455" citStr="Kumar and Byrne, 2005" startWordPosition="351" endWordPosition="354">ase-based systems are a significant improvement over word-based approaches, a particular issue that emerges is long-range reorderings at the phrase level (Galley and Manning, 2008). Analogous to speech recognition systems, translation systems relied on language models to produce more fluent translation. While early work penalized phrase movements without considering reorderings arising from vastly differing grammatical structures across language pairs like ArabicEnglish, many researchers considered lexical reordering models that attempted to learn orientation based on content (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). These approaches may suffer from the data sparseness problem since many phrase pairs occur only once (Nguyen et al., 2009). As an alternative way of exploiting function approximation capabilities offered by machine learning methods, there is recent interest in formulating a learning problem that aims to predict reordering from linguistic features that capture their context. An example of this is the maximum entropy method used by (Xiang et al., 2011; Nguyen et al., 2009; Zens and Ney, 2006; Xiong et al., 2006). In this work we apply a naive Bayes classifier, combined wit</context>
</contexts>
<marker>Kumar, Byrne, 2005</marker>
<rawString>S. Kumar and W. Byrne. 2005. Local phrase reordering models for statistical machine translation. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 161–168, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyu Li</author>
<author>M Niranjan</author>
</authors>
<title>Discriminant subspaces of some high dimensional pattern classification problems.</title>
<date>2013</date>
<booktitle>In IEEE International Workshop on Machine Learning for Signal Processing (MLSP),</booktitle>
<pages>27--32</pages>
<contexts>
<context position="12507" citStr="Li and Niranjan, 2013" startWordPosition="2006" endWordPosition="2010">qkm = 479 probability distribution over q as follows: Zp( ¯fn, ¯en|ok) = p(¯fn, ¯en|qk)p(qk|αk) dqk Q m Γ(αkm + φm( fn, ¯en)) Γ (Pm αkm + φm( ¯fn, ¯en)~. Here αk are new hyperparameters of the posterior derived by means of Bayes theorem as follows: For completeness we give a summary of derivations of equations (11) and (13) in Appendix B, more detailed discussions can be found in (Barber, 2012). 5 Feature Selection In several high dimensional pattern classification problems, there is increasing evidence that the discriminant information may be in small subspaces, motivating feature selection (Li and Niranjan, 2013). Having irrelevant or redundant features could affect the classification performance (Liu and Motoda, 1998). They might mislead the learning algorithms or overfit them to the data and thus have less accuracy. The aim of feature selection is to find the optimal subset features which maximize the ability of prediction, which is the main concern, or simplify the learned results to be more understandable. There are many ways to measure the goodness of a feature or a subset of features; however the criterion will be discussed is mutual information. 5.1 Mutual Information Information criteria are b</context>
</contexts>
<marker>Li, Niranjan, 2013</marker>
<rawString>Hongyu Li and M. Niranjan. 2013. Discriminant subspaces of some high dimensional pattern classification problems. In IEEE International Workshop on Machine Learning for Signal Processing (MLSP), pages 27–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Liu</author>
<author>H Motoda</author>
</authors>
<title>Feature Selection for Knowledge Discovery and Data Mining.</title>
<date>1998</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Norwell, MA, USA.</location>
<contexts>
<context position="12615" citStr="Liu and Motoda, 1998" startWordPosition="2022" endWordPosition="2025"> + φm( fn, ¯en)) Γ (Pm αkm + φm( ¯fn, ¯en)~. Here αk are new hyperparameters of the posterior derived by means of Bayes theorem as follows: For completeness we give a summary of derivations of equations (11) and (13) in Appendix B, more detailed discussions can be found in (Barber, 2012). 5 Feature Selection In several high dimensional pattern classification problems, there is increasing evidence that the discriminant information may be in small subspaces, motivating feature selection (Li and Niranjan, 2013). Having irrelevant or redundant features could affect the classification performance (Liu and Motoda, 1998). They might mislead the learning algorithms or overfit them to the data and thus have less accuracy. The aim of feature selection is to find the optimal subset features which maximize the ability of prediction, which is the main concern, or simplify the learned results to be more understandable. There are many ways to measure the goodness of a feature or a subset of features; however the criterion will be discussed is mutual information. 5.1 Mutual Information Information criteria are based on the concept of entropy which is the amount of randomness. The distribution of a fair coin, for examp</context>
</contexts>
<marker>Liu, Motoda, 1998</marker>
<rawString>H. Liu and H. Motoda. 1998. Feature Selection for Knowledge Discovery and Data Mining. Kluwer Academic Publishers, Norwell, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D MacKay</author>
</authors>
<title>Information Theory, Inference &amp; Learning Algorithms.</title>
<date>2002</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="13357" citStr="MacKay, 2002" startWordPosition="2155" endWordPosition="2156">n is to find the optimal subset features which maximize the ability of prediction, which is the main concern, or simplify the learned results to be more understandable. There are many ways to measure the goodness of a feature or a subset of features; however the criterion will be discussed is mutual information. 5.1 Mutual Information Information criteria are based on the concept of entropy which is the amount of randomness. The distribution of a fair coin, for example, is completely random so the entropy of the coin is very high. The following equation calculates the entropy of a variable X (MacKay, 2002): X H (X) = − p(x) log p(x). (14) x The mutual information of a feature X can be measured by calculating the difference between the prior uncertainty of the class variable Y and the posterior uncertainty after using the feature as follows (MacKay, 2002): I(X;Y ) = H(Y ) − H(Y |X) (15) p(x, y) log p(x, y) p(x)p(y). The advantage of mutual Information over other criteria is the ability to detect nonlinear patterns. The disadvantage is its bias towards higher arbitrary features; however this problem can be solved by normalizing the information as follows (Est´evez et al., 2009): I(X; Y ) Inorm(X;</context>
</contexts>
<marker>MacKay, 2002</marker>
<rawString>D. MacKay. 2002. Information Theory, Inference &amp; Learning Algorithms. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Nguyen</author>
<author>A Shimazu</author>
<author>M Nguyen</author>
<author>T Nguyen</author>
</authors>
<title>Improving a lexicalized hierarchical reordering model using maximum entropy.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII). International Association for Machine Translation.</booktitle>
<contexts>
<context position="2600" citStr="Nguyen et al., 2009" startWordPosition="375" endWordPosition="378">e level (Galley and Manning, 2008). Analogous to speech recognition systems, translation systems relied on language models to produce more fluent translation. While early work penalized phrase movements without considering reorderings arising from vastly differing grammatical structures across language pairs like ArabicEnglish, many researchers considered lexical reordering models that attempted to learn orientation based on content (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). These approaches may suffer from the data sparseness problem since many phrase pairs occur only once (Nguyen et al., 2009). As an alternative way of exploiting function approximation capabilities offered by machine learning methods, there is recent interest in formulating a learning problem that aims to predict reordering from linguistic features that capture their context. An example of this is the maximum entropy method used by (Xiang et al., 2011; Nguyen et al., 2009; Zens and Ney, 2006; Xiong et al., 2006). In this work we apply a naive Bayes classifier, combined with feature selection to address the reordering problem. To the best of our knowledge, this simple model of classification has not been used in thi</context>
<context position="6273" citStr="Nguyen et al., 2009" startWordPosition="967" endWordPosition="970">d reordering model is always biased towards the most frequent orientation for such a phrase pair, it may suffer from a data sparseness problem since many phrase pairs occur only once. Moreover, the context of a phrase might affect its orientation, which is not considered as well. Adopting the idea of predicting orientation based on content, it has been proposed to represent each phrase pair by linguistic features as reordering evidence, and then train a classifier for prediction. The maximum entropy classifier is a popular choice among many researchers (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Max-margin structure classifiers were also proposed (Ni et al., 2011). Recently, Cherry (2013) proposed using sparse features optimize BLEU with the decoder instead of training a classifier independently. We distinguish our work from the previous ones in the following. We propose a fast reordering model using a naive Bayes classifier with feature selection. In this study, we undertake a comparison between our work and lexicalized reordering model. 3 Baseline System In statistical machine translation, the most likely translation ebest of an input sentence f can be found b</context>
<context position="15531" citStr="Nguyen et al., 2009" startWordPosition="2527" endWordPosition="2530">get words in a phrase pair. Each word alignment is a feature. 1http://www.ods.un.org/ods/ 2Moses is an open source toolkit for statistical machine translation (www.statmt.org/moses/). p(qk|αk) = p(qk|α) QNk n p(¯fn, ¯en|qk) The solution of (11) will result in: αk = α + XNk Φ( fn, ¯en). (13) n R p(qk|α) QNk . n p(¯fn, ¯en|qk)dqk =C Γ (Pm αkm) Q m Γ(αkm) X= x,y 480 • Words within a window around the source phrase to capture the context. We choose adjacent words of the phrase boundary. Most researchers build one reordering model for the whole training set (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Ni et al. (Ni et al., 2011) simplified the learning problem to have as many submodels as source phrases. Training data were divided into small independent sets where samples having the same source phrase are considered a training set. In our experiments, we have chosen the first method. We compare lexicalized and Bayesian reordering models in two phases. In the classification phase, we see the performance of the models as a classification problem. In the translation phase, we test the actual impact of these reordering models in a translation system. 6.1 Classification We</context>
</contexts>
<marker>Nguyen, Shimazu, Nguyen, Nguyen, 2009</marker>
<rawString>V. Nguyen, A. Shimazu, M. Nguyen, and T. Nguyen. 2009. Improving a lexicalized hierarchical reordering model using maximum entropy. In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII). International Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ni</author>
<author>C Saunders</author>
<author>S Szedmak</author>
<author>M Niranjan</author>
</authors>
<title>Exploitation of machine learning techniques in modelling phrase movements for machine translation.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>30</pages>
<contexts>
<context position="6365" citStr="Ni et al., 2011" startWordPosition="982" endWordPosition="985">r, it may suffer from a data sparseness problem since many phrase pairs occur only once. Moreover, the context of a phrase might affect its orientation, which is not considered as well. Adopting the idea of predicting orientation based on content, it has been proposed to represent each phrase pair by linguistic features as reordering evidence, and then train a classifier for prediction. The maximum entropy classifier is a popular choice among many researchers (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Max-margin structure classifiers were also proposed (Ni et al., 2011). Recently, Cherry (2013) proposed using sparse features optimize BLEU with the decoder instead of training a classifier independently. We distinguish our work from the previous ones in the following. We propose a fast reordering model using a naive Bayes classifier with feature selection. In this study, we undertake a comparison between our work and lexicalized reordering model. 3 Baseline System In statistical machine translation, the most likely translation ebest of an input sentence f can be found by maximizing the probability p(elf), as follows: ebest = arg max e p(e|f). (1) A log-linear </context>
<context position="15581" citStr="Ni et al., 2011" startWordPosition="2538" endWordPosition="2541">feature. 1http://www.ods.un.org/ods/ 2Moses is an open source toolkit for statistical machine translation (www.statmt.org/moses/). p(qk|αk) = p(qk|α) QNk n p(¯fn, ¯en|qk) The solution of (11) will result in: αk = α + XNk Φ( fn, ¯en). (13) n R p(qk|α) QNk . n p(¯fn, ¯en|qk)dqk =C Γ (Pm αkm) Q m Γ(αkm) X= x,y 480 • Words within a window around the source phrase to capture the context. We choose adjacent words of the phrase boundary. Most researchers build one reordering model for the whole training set (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Ni et al. (Ni et al., 2011) simplified the learning problem to have as many submodels as source phrases. Training data were divided into small independent sets where samples having the same source phrase are considered a training set. In our experiments, we have chosen the first method. We compare lexicalized and Bayesian reordering models in two phases. In the classification phase, we see the performance of the models as a classification problem. In the translation phase, we test the actual impact of these reordering models in a translation system. 6.1 Classification We built naive Bayes classifier with both MAP estima</context>
</contexts>
<marker>Ni, Saunders, Szedmak, Niranjan, 2011</marker>
<rawString>Y. Ni, C. Saunders, S. Szedmak, and M. Niranjan. 2011. Exploitation of machine learning techniques in modelling phrase movements for machine translation. Journal of Machine Learning Research, 12:1– 30, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="14650" citStr="Och and Ney, 2000" startWordPosition="2373" endWordPosition="2376">nts is MultiUN which is a large-scale parallel corpus extracted from the United Nations website1 (Eisele and Chen, 2010). We have used Arabic and English portion of MultiUN. Table 2 shows the general statistics. Statistics Arabic English Sentence Pairs 9.7 M Running Words 255.5 M 285.7 M Word/Line 22 25 Vocabulary Size 677 K 410 K Table 2: General statistics of Arabic-English MultiUN (M: million, K: thousand). We simplify the problem by classifying phrase movements into three categories (monotone, swap, discontinuous). To train the reordering models, we used GIZA++ to produce word alignments (Och and Ney, 2000). Then, we used the extract tool that comes with the Moses 2 toolkit (Koehn et al., 2007) in order to extract phrase pairs along with their orientation classes. Each extracted phrase pair is represented by linguistic features as follows: • Aligned source and target words in a phrase pair. Each word alignment is a feature. 1http://www.ods.un.org/ods/ 2Moses is an open source toolkit for statistical machine translation (www.statmt.org/moses/). p(qk|αk) = p(qk|α) QNk n p(¯fn, ¯en|qk) The solution of (11) will result in: αk = α + XNk Φ( fn, ¯en). (13) n R p(qk|α) QNk . n p(¯fn, ¯en|qk)dqk =C Γ (Pm</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. Och and H. Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="7114" citStr="Och and Ney, 2002" startWordPosition="1101" endWordPosition="1104">tly. We distinguish our work from the previous ones in the following. We propose a fast reordering model using a naive Bayes classifier with feature selection. In this study, we undertake a comparison between our work and lexicalized reordering model. 3 Baseline System In statistical machine translation, the most likely translation ebest of an input sentence f can be found by maximizing the probability p(elf), as follows: ebest = arg max e p(e|f). (1) A log-linear combination of different models (features) is used for direct modeling of the posterior probability p(elf) (Papineni et al., 1998; Och and Ney, 2002): λihi(f, e) (2) where the feature hi(f, e) is a score function over sentence pairs. The translation model and the language model are the main features in any system although additional features h(.) can be integrated easily (such as word penalty). State-of-theart systems usually have around ten features (i.e. n = 10). In phrase-based systems, the translation model can capture the local meaning for each source phrase. However, to capture the whole meaning of a sentence, its translated phrases need to be in the correct order. The language model, which ensures fluent translation, plays an import</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1823" citStr="Och and Ney, 2004" startWordPosition="263" endWordPosition="266">own et al., 1993). These early models, widely known as the IBM models, were wordbased. Recent extensions note that a better approach is to group collections of words, or phrases, for translation together, resulting in a significant focus these days on phrase-based statistical machine translation systems. To deal with the alignment problem of oneto-many word alignments in the IBM model formulation, whereas phrase-based models may have many-to-many translation relationships, IBM models are trained in both directions, source to target and target to source, and their word alignments are combined (Och and Ney, 2004). While phrase-based systems are a significant improvement over word-based approaches, a particular issue that emerges is long-range reorderings at the phrase level (Galley and Manning, 2008). Analogous to speech recognition systems, translation systems relied on language models to produce more fluent translation. While early work penalized phrase movements without considering reorderings arising from vastly differing grammatical structures across language pairs like ArabicEnglish, many researchers considered lexical reordering models that attempted to learn orientation based on content (Tillm</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18069" citStr="Och, 2003" startWordPosition="2944" endWordPosition="2945"> the feature space is as good as the one with full feature space. Figure 1: Normalized mutual information for all extracted features (ranked from lowest to highest). 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Percentage of Feature Reductuion Figure 2: Classification error rate of the Baysien model with different levels of feature reduction. 6.2 Translation 6.2.1 Experimental Design We used the Moses toolkit (Koehn et al., 2007) with its default settings. The language model is a 5-gram with interpolation and Kneser-Ney smoothing (Kneser and Ney, 1995). We tuned the system by using MERT technique (Och, 2003). We built four Arabic-English translation systems. Three systems differ in how their reordering models were estimated and the fourth system is a 22.5 22 21.5 20 19.5 19 21 Error Rate 20.5 481 baseline system without reordering model. In all cases, orientation extraction is hierarchical-based since it is the best approach while orientations are monotone, swap and discontinuous. The model is trained in Moses by specifying the configuration string hier-msd-backward-fe. As commonly used in statistical machine translation, we evaluated the translation performance by BLEU score (Papineni et al., 20</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 160–167, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
</authors>
<title>Maximum likelihood and discriminative training of direct translation models.</title>
<date>1998</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>189--192</pages>
<contexts>
<context position="7094" citStr="Papineni et al., 1998" startWordPosition="1097" endWordPosition="1100">a classifier independently. We distinguish our work from the previous ones in the following. We propose a fast reordering model using a naive Bayes classifier with feature selection. In this study, we undertake a comparison between our work and lexicalized reordering model. 3 Baseline System In statistical machine translation, the most likely translation ebest of an input sentence f can be found by maximizing the probability p(elf), as follows: ebest = arg max e p(e|f). (1) A log-linear combination of different models (features) is used for direct modeling of the posterior probability p(elf) (Papineni et al., 1998; Och and Ney, 2002): λihi(f, e) (2) where the feature hi(f, e) is a score function over sentence pairs. The translation model and the language model are the main features in any system although additional features h(.) can be integrated easily (such as word penalty). State-of-theart systems usually have around ten features (i.e. n = 10). In phrase-based systems, the translation model can capture the local meaning for each source phrase. However, to capture the whole meaning of a sentence, its translated phrases need to be in the correct order. The language model, which ensures fluent translat</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1998</marker>
<rawString>K. Papineni, S. Roukos, and T. Ward. 1998. Maximum likelihood and discriminative training of direct translation models. In Proceedings of ICASSP, pages 189–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18672" citStr="Papineni et al., 2002" startWordPosition="3033" endWordPosition="3036">chnique (Och, 2003). We built four Arabic-English translation systems. Three systems differ in how their reordering models were estimated and the fourth system is a 22.5 22 21.5 20 19.5 19 21 Error Rate 20.5 481 baseline system without reordering model. In all cases, orientation extraction is hierarchical-based since it is the best approach while orientations are monotone, swap and discontinuous. The model is trained in Moses by specifying the configuration string hier-msd-backward-fe. As commonly used in statistical machine translation, we evaluated the translation performance by BLEU score (Papineni et al., 2002). The test sets are NIST MT06 and NIST MT08. Table 4 shows statistics of development and test sets. We also computed statistical significance for the proposed models using the paired bootstrap resampling method (Koehn, 2004). Evaluation Set Arabic English Development sentences 696 696 words 19 K 21 K NIST MT06 sentences 1797 7188 words 49 K 223 K NIST MT08 sentences 813 3252 words 25 K 117 K Table 4: Statistics of development and test sets. The English side in NIST is larger because there are four translations for each Arabic sentence. 6.2.2 Results We first demonstrate in Table 5 a general co</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Rogers</author>
<author>M Girolami</author>
</authors>
<title>A First Course</title>
<date>2011</date>
<booktitle>in Machine Learning. Chapman &amp; Hall/CRC, 1st edition.</booktitle>
<contexts>
<context position="10484" citStr="Rogers and Girolami, 2011" startWordPosition="1631" endWordPosition="1634"> correlation between features, but has the appeal of computational simplicity. It is a generative probabilistic model based on Bayes’ theorem as below: The class prior can be estimated easily as a relative frequency (i.e. p(ok) = NkN ). The likelihood distribution p(¯fn, ¯en|ok) is defined based on the type of data. The classifier will be naive if we assume that feature variables are conditionally independent. The naive assumption simplifies our distribution and hence reduces the parameters that have to be estimated. In text processing, multinomial is used as a class-conditional distribution (Rogers and Girolami, 2011). The distribution is defined as: Y p( ¯fn, ¯en|q) = C m where C is a multinomial coefficient, = C (Pm φm( Qm φm( and q are a set of parameters, each of which is a probability. Estimating these parameters for each class by maximum likelihood, p( fn, ¯en|qk), (8) will result in (Rogers and Girolami, 2011): MAP estimate It is clear that qkm might be zero which means the probability of a new phrase pair with nonzero feature φm( fn, ¯en) is always zero because of the product in (6). Putting a prior over q is one smoothing technique. A conjugate prior for the multinomial likelihood is the Dirichlet</context>
</contexts>
<marker>Rogers, Girolami, 2011</marker>
<rawString>S. Rogers and M. Girolami. 2011. A First Course in Machine Learning. Chapman &amp; Hall/CRC, 1st edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL: Short Papers,</booktitle>
<pages>101--104</pages>
<contexts>
<context position="2432" citStr="Tillmann, 2004" startWordPosition="349" endWordPosition="350">2004). While phrase-based systems are a significant improvement over word-based approaches, a particular issue that emerges is long-range reorderings at the phrase level (Galley and Manning, 2008). Analogous to speech recognition systems, translation systems relied on language models to produce more fluent translation. While early work penalized phrase movements without considering reorderings arising from vastly differing grammatical structures across language pairs like ArabicEnglish, many researchers considered lexical reordering models that attempted to learn orientation based on content (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). These approaches may suffer from the data sparseness problem since many phrase pairs occur only once (Nguyen et al., 2009). As an alternative way of exploiting function approximation capabilities offered by machine learning methods, there is recent interest in formulating a learning problem that aims to predict reordering from linguistic features that capture their context. An example of this is the maximum entropy method used by (Xiang et al., 2011; Nguyen et al., 2009; Zens and Ney, 2006; Xiong et al., 2006). In this work we apply a naive Bayes c</context>
<context position="8274" citStr="Tillmann, 2004" startWordPosition="1283" endWordPosition="1284">odel, which ensures fluent translation, plays an important role in reordering; however, it prefers sentences that are grammatically correct without considering their actual meaning. Besides that, it has a bias towards short translations (Koehn, 2010). Therefore, developing a reordering model will improve the accuracy particularly when translating between two grammatically different languages. 3.1 Lexicalized Reordering Model Phrase reordering modeling involves formulating phrase movements as a classification problem ebest = arg max e n i=1 478 where each phrase position considered as a class (Tillmann, 2004). Some researchers classified phrase movements into three categories (monotone, swap, and discontinuous) but the classes can be extended to any arbitrary number (Koehn and Monz, 2005). In general, the distribution of phrase orientation is: ¯ 1¯¯ fn, en) = Z h(fn, en, ok) . (3) This lexicalized reordering model is estimated by relative frequency where each phrase pair (¯fn, ¯en) with such an orientation (ok) is counted and then normalized to yield the probability as follows: The orientation class of a current phrase pair is defined with respect to the previous target word or phrase (i.e. word-b</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>C. Tillmann. 2004. A unigram orientation model for statistical machine translation. In Proceedings of HLT-NAACL: Short Papers, pages 101–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Xiang</author>
<author>N Ge</author>
<author>A Ittycheriah</author>
</authors>
<title>Improving reordering for statistical machine translation with smoothed priors and syntactic features.</title>
<date>2011</date>
<booktitle>In Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<pages>61--69</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="2931" citStr="Xiang et al., 2011" startWordPosition="429" endWordPosition="432">y researchers considered lexical reordering models that attempted to learn orientation based on content (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). These approaches may suffer from the data sparseness problem since many phrase pairs occur only once (Nguyen et al., 2009). As an alternative way of exploiting function approximation capabilities offered by machine learning methods, there is recent interest in formulating a learning problem that aims to predict reordering from linguistic features that capture their context. An example of this is the maximum entropy method used by (Xiang et al., 2011; Nguyen et al., 2009; Zens and Ney, 2006; Xiong et al., 2006). In this work we apply a naive Bayes classifier, combined with feature selection to address the reordering problem. To the best of our knowledge, this simple model of classification has not been used in this context previously. We present empirical results comparing our work and previously proposed lexicalized reordering model. We show that our model is scalable to large corpora. The remainder of this paper is organized as follows. Section 2 discusses previous work in the field and how that is related to our paper. Section 3 gives </context>
<context position="6294" citStr="Xiang et al., 2011" startWordPosition="971" endWordPosition="975"> always biased towards the most frequent orientation for such a phrase pair, it may suffer from a data sparseness problem since many phrase pairs occur only once. Moreover, the context of a phrase might affect its orientation, which is not considered as well. Adopting the idea of predicting orientation based on content, it has been proposed to represent each phrase pair by linguistic features as reordering evidence, and then train a classifier for prediction. The maximum entropy classifier is a popular choice among many researchers (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Max-margin structure classifiers were also proposed (Ni et al., 2011). Recently, Cherry (2013) proposed using sparse features optimize BLEU with the decoder instead of training a classifier independently. We distinguish our work from the previous ones in the following. We propose a fast reordering model using a naive Bayes classifier with feature selection. In this study, we undertake a comparison between our work and lexicalized reordering model. 3 Baseline System In statistical machine translation, the most likely translation ebest of an input sentence f can be found by maximizing the prob</context>
<context position="15552" citStr="Xiang et al., 2011" startWordPosition="2531" endWordPosition="2534"> pair. Each word alignment is a feature. 1http://www.ods.un.org/ods/ 2Moses is an open source toolkit for statistical machine translation (www.statmt.org/moses/). p(qk|αk) = p(qk|α) QNk n p(¯fn, ¯en|qk) The solution of (11) will result in: αk = α + XNk Φ( fn, ¯en). (13) n R p(qk|α) QNk . n p(¯fn, ¯en|qk)dqk =C Γ (Pm αkm) Q m Γ(αkm) X= x,y 480 • Words within a window around the source phrase to capture the context. We choose adjacent words of the phrase boundary. Most researchers build one reordering model for the whole training set (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Ni et al. (Ni et al., 2011) simplified the learning problem to have as many submodels as source phrases. Training data were divided into small independent sets where samples having the same source phrase are considered a training set. In our experiments, we have chosen the first method. We compare lexicalized and Bayesian reordering models in two phases. In the classification phase, we see the performance of the models as a classification problem. In the translation phase, we test the actual impact of these reordering models in a translation system. 6.1 Classification We built naive Bayes cl</context>
</contexts>
<marker>Xiang, Ge, Ittycheriah, 2011</marker>
<rawString>B. Xiang, N. Ge, and A. Ittycheriah. 2011. Improving reordering for statistical machine translation with smoothed priors and syntactic features. In Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 61– 69, Portland, Oregon, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Xiong</author>
<author>Q Liu</author>
<author>S Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</booktitle>
<pages>521--528</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney,</location>
<contexts>
<context position="2993" citStr="Xiong et al., 2006" startWordPosition="441" endWordPosition="444">pted to learn orientation based on content (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). These approaches may suffer from the data sparseness problem since many phrase pairs occur only once (Nguyen et al., 2009). As an alternative way of exploiting function approximation capabilities offered by machine learning methods, there is recent interest in formulating a learning problem that aims to predict reordering from linguistic features that capture their context. An example of this is the maximum entropy method used by (Xiang et al., 2011; Nguyen et al., 2009; Zens and Ney, 2006; Xiong et al., 2006). In this work we apply a naive Bayes classifier, combined with feature selection to address the reordering problem. To the best of our knowledge, this simple model of classification has not been used in this context previously. We present empirical results comparing our work and previously proposed lexicalized reordering model. We show that our model is scalable to large corpora. The remainder of this paper is organized as follows. Section 2 discusses previous work in the field and how that is related to our paper. Section 3 gives an overview of the baseline translation system. Section 4 intr</context>
<context position="6252" citStr="Xiong et al., 2006" startWordPosition="963" endWordPosition="966"> that the lexicalized reordering model is always biased towards the most frequent orientation for such a phrase pair, it may suffer from a data sparseness problem since many phrase pairs occur only once. Moreover, the context of a phrase might affect its orientation, which is not considered as well. Adopting the idea of predicting orientation based on content, it has been proposed to represent each phrase pair by linguistic features as reordering evidence, and then train a classifier for prediction. The maximum entropy classifier is a popular choice among many researchers (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Max-margin structure classifiers were also proposed (Ni et al., 2011). Recently, Cherry (2013) proposed using sparse features optimize BLEU with the decoder instead of training a classifier independently. We distinguish our work from the previous ones in the following. We propose a fast reordering model using a naive Bayes classifier with feature selection. In this study, we undertake a comparison between our work and lexicalized reordering model. 3 Baseline System In statistical machine translation, the most likely translation ebest of an input sent</context>
<context position="15510" citStr="Xiong et al., 2006" startWordPosition="2523" endWordPosition="2526">igned source and target words in a phrase pair. Each word alignment is a feature. 1http://www.ods.un.org/ods/ 2Moses is an open source toolkit for statistical machine translation (www.statmt.org/moses/). p(qk|αk) = p(qk|α) QNk n p(¯fn, ¯en|qk) The solution of (11) will result in: αk = α + XNk Φ( fn, ¯en). (13) n R p(qk|α) QNk . n p(¯fn, ¯en|qk)dqk =C Γ (Pm αkm) Q m Γ(αkm) X= x,y 480 • Words within a window around the source phrase to capture the context. We choose adjacent words of the phrase boundary. Most researchers build one reordering model for the whole training set (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Ni et al. (Ni et al., 2011) simplified the learning problem to have as many submodels as source phrases. Training data were divided into small independent sets where samples having the same source phrase are considered a training set. In our experiments, we have chosen the first method. We compare lexicalized and Bayesian reordering models in two phases. In the classification phase, we see the performance of the models as a classification problem. In the translation phase, we test the actual impact of these reordering models in a translation system. </context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>D. Xiong, Q. Liu, and S. Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 521–528, Sydney, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Discriminative reorder- ing models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<volume>22</volume>
<pages>55--63</pages>
<location>New York City,</location>
<contexts>
<context position="2972" citStr="Zens and Ney, 2006" startWordPosition="437" endWordPosition="440">ng models that attempted to learn orientation based on content (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). These approaches may suffer from the data sparseness problem since many phrase pairs occur only once (Nguyen et al., 2009). As an alternative way of exploiting function approximation capabilities offered by machine learning methods, there is recent interest in formulating a learning problem that aims to predict reordering from linguistic features that capture their context. An example of this is the maximum entropy method used by (Xiang et al., 2011; Nguyen et al., 2009; Zens and Ney, 2006; Xiong et al., 2006). In this work we apply a naive Bayes classifier, combined with feature selection to address the reordering problem. To the best of our knowledge, this simple model of classification has not been used in this context previously. We present empirical results comparing our work and previously proposed lexicalized reordering model. We show that our model is scalable to large corpora. The remainder of this paper is organized as follows. Section 2 discusses previous work in the field and how that is related to our paper. Section 3 gives an overview of the baseline translation s</context>
<context position="6232" citStr="Zens and Ney, 2006" startWordPosition="959" endWordPosition="962">st. Despite the fact that the lexicalized reordering model is always biased towards the most frequent orientation for such a phrase pair, it may suffer from a data sparseness problem since many phrase pairs occur only once. Moreover, the context of a phrase might affect its orientation, which is not considered as well. Adopting the idea of predicting orientation based on content, it has been proposed to represent each phrase pair by linguistic features as reordering evidence, and then train a classifier for prediction. The maximum entropy classifier is a popular choice among many researchers (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Max-margin structure classifiers were also proposed (Ni et al., 2011). Recently, Cherry (2013) proposed using sparse features optimize BLEU with the decoder instead of training a classifier independently. We distinguish our work from the previous ones in the following. We propose a fast reordering model using a naive Bayes classifier with feature selection. In this study, we undertake a comparison between our work and lexicalized reordering model. 3 Baseline System In statistical machine translation, the most likely translation eb</context>
<context position="15490" citStr="Zens and Ney, 2006" startWordPosition="2519" endWordPosition="2522">res as follows: • Aligned source and target words in a phrase pair. Each word alignment is a feature. 1http://www.ods.un.org/ods/ 2Moses is an open source toolkit for statistical machine translation (www.statmt.org/moses/). p(qk|αk) = p(qk|α) QNk n p(¯fn, ¯en|qk) The solution of (11) will result in: αk = α + XNk Φ( fn, ¯en). (13) n R p(qk|α) QNk . n p(¯fn, ¯en|qk)dqk =C Γ (Pm αkm) Q m Γ(αkm) X= x,y 480 • Words within a window around the source phrase to capture the context. We choose adjacent words of the phrase boundary. Most researchers build one reordering model for the whole training set (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Ni et al. (Ni et al., 2011) simplified the learning problem to have as many submodels as source phrases. Training data were divided into small independent sets where samples having the same source phrase are considered a training set. In our experiments, we have chosen the first method. We compare lexicalized and Bayesian reordering models in two phases. In the classification phase, we see the performance of the models as a classification problem. In the translation phase, we test the actual impact of these reordering models in a </context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>R. Zens and H. Ney. 2006. Discriminative reorder- ing models for statistical machine translation. In Proceedings on the Workshop on Statistical Machine Translation, pages 55–63, New York City, June. As- sociation for Computational Linguistics. ∂f(q) ∂qm qm = αm − 1 + Pn xnm qm αm − 1 + Pn xnm λ λ . (22)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000219">
<title confidence="0.957223">
DCU-UVT: Word-Level Language Classification with Code-Mixed Data
</title>
<author confidence="0.999136">
Utsab Barman, Joachim Wagner, Grzegorz Chrupała† and Jennifer Foster
</author>
<affiliation confidence="0.993659">
CNGL Centre for Global Intelligent Content, National Centre for Language Technology
School of Computing, Dublin City University, Dublin, Ireland
†Tilburg School of Humanities, Department of Communication and Information Sciences
Tilburg University, Tilburg, The Netherlands
</affiliation>
<email confidence="0.986623">
{ubarman,jwagner,jfoster}@computing.dcu.ie
G.A.Chrupala@uvt.nl
</email>
<sectionHeader confidence="0.993722" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999648133333333">
This paper describes the DCU-UVT
team’s participation in the Language Iden-
tification in Code-Switched Data shared
task in the Workshop on Computational
Approaches to Code Switching. Word-
level classification experiments were car-
ried out using a simple dictionary-based
method, linear kernel support vector ma-
chines (SVMs) with and without con-
textual clues, and a k-nearest neighbour
approach. Based on these experiments,
we select our SVM-based system with
contextual clues as our final system and
present results for the Nepali-English and
Spanish-English datasets.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901793103448">
This paper describes DCU-UVT’s participation
in the shared task Language Identification in
Code-Switched Data (Solorio et al., 2014) at
the Workshop on Computational Approaches to
Code Switching, EMNLP, 2014. The task is to
make word-level predictions (six labels: lang1,
lang2, ne, mixed, ambiguous and other) for mixed-
language user generated content. We submit pre-
dictions for Nepali-English and Spanish-English
data and perform experiments using dictionaries, a
k-nearest neighbour (k-NN) classifier and a linear-
kernel SVM classifier.
In our dictionary-based approach, we investi-
gate the use of different English dictionaries as
well as the training data. In the k-NN based
approach, we use string edit distance, character-
n-gram overlap and context similarity to make
predictions. For the SVM approach, we experi-
ment with context-independent (word, character-
n-grams, length of a word and capitalisation in-
formation) and context-sensitive (adding the pre-
vious and next word as bigrams) features in differ-
ent combinations. We also experiment with adding
features from the k-NN approach and another set
of features from a neural network. Based on per-
formance in cross-validation, we select the SVM
classifier with basic features (word, character-n-
grams, length of a word, capitalisation information
and context) as our final system.
</bodyText>
<sectionHeader confidence="0.981118" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99598375862069">
While the problem of automatically identify-
ing and analysing code-mixing has been iden-
tified over 30 years ago (Joshi, 1982), it has
only recently drawn wider attention. Specific
problems addressed include language identifica-
tion in multilingual documents, identification of
code-switching points and POS tagging (Solorio
and Liu, 2008b) of code-mixing data. Ap-
proaches taken to the problem of identifying code-
mixing include the use of dictionaries (Nguyen
and Do˘gru¨oz, 2013; Barman et al., 2014; El-
fardy et al., 2013; Solorio and Liu, 2008b), lan-
guage models (Alex, 2008; Nguyen and Do˘gru¨oz,
2013; Elfardy et al., 2013), morphological and
phonological analysis (Elfardy et al., 2013; El-
fardy and Diab, 2012) and various machine learn-
ing algorithms such as sequence labelling with
Hidden Markov Models (Farrugia, 2004; Ros-
ner and Farrugia, 2007) and Conditional Random
Fields (Nguyen and Do˘gru¨oz, 2013; King and
Abney, 2013), as well as word-level classifica-
tion using Naive Bayes (Solorio and Liu, 2008a),
logistic regression (Nguyen and Do˘gru¨oz, 2013)
and SVMs (Barman et al., 2014), using features
such as word, POS, lemma and character-n-grams.
Language pairs that have been explored include
English-Maltese (Farrugia, 2004; Rosner and Far-
rugia, 2007), English-Spanish (Solorio and Liu,
2008b), Turkish-Dutch (Nguyen and Do˘gru¨oz,
</bodyText>
<page confidence="0.965973">
127
</page>
<bodyText confidence="0.798727">
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 127–132,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
2013), modern standard Arabic-Egyptian di-
alect (Elfardy et al., 2013), Mandarin-English (Li
et al., 2012; Lyu et al., 2010), and English-Hindi-
Bengali (Barman et al., 2014).
</bodyText>
<sectionHeader confidence="0.960504" genericHeader="method">
3 Data Statistics
</sectionHeader>
<bodyText confidence="0.99787125">
The training data provided for this task consists of
tweets. Unfortunately, because of deleted tweets,
the full training set could not be downloaded. Out
of 9,993 Nepali-English training tweets, we were
able to download 9,668 and out of 11,400 Spanish-
English training tweets, we were able to download
11,353. Table 1 shows the token-level statistics of
the two datasets.
</bodyText>
<table confidence="0.997052857142857">
Label Nepali-English Spanish-English
lang1 (en) 43,185 76,204
lang2 (ne/es) 59,579 32,477
ne 3,821 2,814
ambiguous 125 341
mixed 112 51
other 34,566 21,813
</table>
<tableCaption confidence="0.9051485">
Table 1: Number of tokens in the Nepali-English
and Spanish-English training data for each label
</tableCaption>
<bodyText confidence="0.994665214285714">
Nepali (lang2) is the dominant language in
the Nepali-English training data but for Spanish-
English, English (lang1) is dominant. The third
largest group contains tokens with the label other.
These are mentions (@username), punctuation
symbols, emoticons, numbers (except numbers
that represent words such as 2 for to), words in a
language other than lang1 and lang2 and unintel-
ligible words. Named entities (ne) are much less
frequent and mixed language words (e.g. ramri-
ness) and words for which there is not enough con-
text to disambiguate them are rare. Hash tags are
annotated as if the hash symbol was not there, e.g.
#truestory is labelled lang1.
</bodyText>
<sectionHeader confidence="0.999417" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999840888888889">
All experiments are carried out for Nepali-English
data. Later we apply the best approach to Spanish-
English. We train our systems in a five-fold cross-
validation and obtain best parameters based on
average cross-validation results. Cross-validation
splits are made based on users, i.e. we avoid the
occurrence of a user’s tweets both in training and
test splits for each cross-validation run. We ad-
dress the task with the following approaches:
</bodyText>
<listItem confidence="0.55249">
1. a simple dictionary-based classifier,
</listItem>
<table confidence="0.9219488">
Resource Accuracy
BNC 43.61
LexNorm 54.60
TrainingData 89.53
TrainingData+BNC+LexNorm 90.71
</table>
<tableCaption confidence="0.952167">
Table 2: Average cross-validation accuracy of
dictionary-based prediction for Nepali-English
</tableCaption>
<listItem confidence="0.4990865">
2. classification using supervised machine
learning with k-nearest neighbour, and
3. classification using supervised machine
learning with SVMs.
</listItem>
<subsectionHeader confidence="0.968889">
4.1 Dictionary-Based Detection
</subsectionHeader>
<bodyText confidence="0.99999653125">
We start with a simple dictionary-based approach
using as dictionaries (a) the British National Cor-
pus (BNC) (Aston and Burnard, 1998), (b) Han
et al.’s lexical normalisation dictionary (LexNorm)
(Han et al., 2012) and (c) the training data.
The BNC and LexNorm dictionaries are built by
recording all words occurring in the respective
corpus or word list as English. For the BNC, we
also collect word frequency information. For the
training data, we obtain dictionaries for each of the
six labels and each of the five cross-validation runs
(using the relevant 4/5 of training data).
To make a prediction, we consult all dictionar-
ies. If there are more than one candidate label,
we choose the label for which the frequency for
the query token is highest. To account for the fact
that the BNC is much larger than the training data,
we normalise all frequencies before comparison.
LexNorm has no frequency information, hence it
is added to our system as a simple word list (we
consider the language of a word to be English if it
appears in LexNorm). If a word appears in multi-
ple dictionaries with the same frequency or if the
word does not appear in any dictionary or list, the
predicted language is chosen based on the domi-
nant language(s)/label(s) of the corpus.
We experiment with the individual dictionar-
ies and the combination of all three dictionaries,
among which the combination achieves the high-
est cross-validation accuracy (90.71%). Table 2
shows the results of dictionary-based detection ob-
tained in five-fold cross-validation.
</bodyText>
<subsectionHeader confidence="0.999251">
4.2 Classification with k-NN
</subsectionHeader>
<bodyText confidence="0.999954">
For Nepali-English, we also experiment with a
simple k-nearest neighbour (k-NN) approach. For
each test item, we select a subset of the training
data using string edit distance and n-gram overlap
</bodyText>
<page confidence="0.990867">
128
</page>
<bodyText confidence="0.99997975">
and choose the majority label of the subset as our
prediction. For efficiency, we first select k1 items
that share an n-gram with the token to be classi-
fied.1 The set of k1 items is then re-ranked ac-
cording to string edit distance to the test item and
the best k2 matches are used to make a prediction.
Apart from varying k1 and k2, we experiment
with (a) lowercasing strings, (b) including context
by concatenating the previous, current and next
token, and (c) weighting context by first calcu-
lating edit distances for the previous, current and
next token separately and using a weighted aver-
age. The best configuration we found in cross-
validation uses lowercasing with k1 = 800 and
k2 = 16 but no context information. It achieves
an accuracy of 94.97%.
</bodyText>
<subsectionHeader confidence="0.984367">
4.3 SVM Classification
</subsectionHeader>
<bodyText confidence="0.999956">
We experiment with linear kernel SVM classifiers
using Liblinear (Fan et al., 2008). Parameter opti-
misation2 is performed for each feature set combi-
nation to obtain best cross-validation accuracy.
</bodyText>
<subsectionHeader confidence="0.608761">
4.3.1 Basic Features
</subsectionHeader>
<bodyText confidence="0.982003047619048">
Following Barman et al. (2014), our basic features
are:
Char-N-Grams (G): We start with a charac-
ter n-gram-based approach (Cavnar and Trenkle,
1994). Following King and Abney (2013), we se-
lect lowercased character n-grams (n=1 to 5) and
the word as the features in our experiments.
Dictionary-Based Labels (D): We use presence
in the dictionary of the 5,000 most frequent words
in the BNC and presence in the LexNorm dictio-
nary as binary features.3
Length of words (L): We create multiple fea-
tures for token length using a decision tree (J48).
We use length as the only feature to train a deci-
sion tree for each fold and use the nodes obtained
from the tree to create boolean features (Rubino et
al., 2013; Wagner et al., 2014).
1Starting with n = 5, we decrease n until there are at
least k1 items and then we randomly remove items added in
the last augmentation step to arrive at exactly k1 items. (For
n = 0, we randomly sample from the full training data.)
</bodyText>
<equation confidence="0.497">
2C = 2&apos; with i = −15, −14,..., 10
</equation>
<footnote confidence="0.993248571428571">
3We chose these parameters based on experiments with
each dictionary, combinations of dictionaries and various fre-
quency thresholds. We apply a frequency threshold to the
BNC to increase precision. We rank the words according to
frequency and used the rank as a threshold (e.g. top-5K, top-
10K etc.). With the top 5,000 ranked words and C = 0.25,
we obtained best accuracy (96.40%).
</footnote>
<table confidence="0.9999628">
Features Accuracy Features Accuracy
G 96.02 GD 96.27
GL 96.11 GDL 96.32
GC 96.15 GDC 96.20
GLC 96.21 GDLC 96.40
</table>
<tableCaption confidence="0.631854">
Table 3: Average cross-validation accuracy of 6-
way SVMs on the Nepali-English data set; G =
char-n-gram, L = binary length features, D = dict.-
based labels and C = capitalisation features
</tableCaption>
<table confidence="0.999703142857143">
Context Accuracy(%)
GDLC + P1 96.41
GDLC + P2 96.38
GDLC + N1 96.41
GDLC + N2 96.41
GDLC + P1 + N1 96.42
GDLC + P2 + N2 96.41
</table>
<tableCaption confidence="0.801799333333333">
Table 4: Average cross-validation accuracy of 6-
way SVMs using contextual features for Nepali-
English
</tableCaption>
<bodyText confidence="0.99488965">
Capitalisation (C): We choose 3 boolean
features to encode capitalisation information:
whether any letter in the word is capitalised,
whether all letters in the word are capitalised and
whether the first letter is capitalised.
Context (Pi and Nj): We consider the previous
i and next j token to be combined with the current
token, forming an (i+1)-gram and a (j+1)-gram,
which we add as features. Six settings are tested.
Table 4 shows that using the bigrams formed with
the previous and next word are the best combina-
tion for the task (among those tested).
Among the eight combinations of the first four
feature sets that contain the first set (G), Table 3
shows that the 6-way SVM classifier4performs
best with all features sets (GDLC), achieving
96.40% accuracy. Adding contextual information
PiNj to GDLC, Table 4 shows best results for
i=j=1, achieving 96.42% accuracy, only slightly
ahead of the context-independent system.
</bodyText>
<sectionHeader confidence="0.841416" genericHeader="method">
4.3.2 Neural Network (Elman) and k-NN
Features
</sectionHeader>
<bodyText confidence="0.99368">
We experiment with two additional features sets
not covered by Barman et al. (2014):
Neural Network (Elman): We extract features
from the hidden layer of a recurrent neural net-
</bodyText>
<footnote confidence="0.879583">
4We also test 3-way SVM classification (lang1, lang2 and
other) and heuristic post-processing, but it does not outper-
form our 6-way classification runs.
</footnote>
<page confidence="0.993813">
129
</page>
<table confidence="0.813506923076923">
Systems Accuracy
GDLC 96.40
k-NN 95.10
Elman 89.96
GDLC+k-NN 96.31
GDLC+Elman 96.46
GDLC+k-NN+Elman 96.40
GDLC+P1N1 96.42
k-NN+P1N1 95.11
Elman+P1N1 91.53
GDLC+P1N1+k-NN 96.33
GDLC+P1N1+Elman 96.45
GDLC+P1N1+k-NN+Elman 96.40
</table>
<tableCaption confidence="0.974593">
Table 5: Average cross-validation accuracy of 6-
</tableCaption>
<bodyText confidence="0.978763133333333">
way SVMs of combinations of GDLC, k-NN, El-
man and P1N1 features for Nepali-English
work that has been trained to predict the next char-
acter in a string (Chrupała, 2014). The 10 most ac-
tive units of the hidden layer for each of the initial
4 bytes and final 4 bytes of each token are bina-
rised by using a threshold of 0.5.
k-Nearest Neighbour (kNN): We obtain fea-
tures from our basic k-NN approach (Section 4.2),
encoding the prediction of the k-NN model with
six binary features (one for each label) and a nu-
meric feature for each label stating the relative
number of votes for the label, e.g. if k2 = 16
and 12 votes are for lang1 the value of the fea-
ture votes4lang1 will be 0.75. Furthermore, we
add two features stating the minimum and maxi-
mum edit distance between the test token and the
k2 selected training tokens.
Table 5 shows cross-validation results for these
new feature sets with and without the P1N1 con-
text features. Excluding the GDLC features, we
can see that best accuracy is with k-NN and P1N1
features (95.11%). For Elman features, the accu-
racy is lower (91.53% with context). In combina-
tion with the GDLC features, however, the Elman
features can achieve a small improvement over
the GDLC+P1N1 combination (+0.04 percentage
points): 96.46% accuracy for the GDLC+Elman
setting (without P1N1 features). Furthermore, the
k-NN features do not combine well.5
</bodyText>
<subsectionHeader confidence="0.981357">
4.3.3 Final System and Test Results
</subsectionHeader>
<bodyText confidence="0.9983925">
At the time of submission of predictions, we had
an error in our GDLC+Elman feature combiner re-
</bodyText>
<footnote confidence="0.88368125">
5A possible explanation may be that the k-NN features
are based on only 3 of 5 folds for the training data (3 folds
are used to make predictions for the 4th set) but 4 of 5 folds
are used for test data predictions in each cross-validation run.
</footnote>
<table confidence="0.999393875">
Tweets
Token-Level Tweet-Level
Nepali-English 96.3 95.8
Spanish-English 84.4 80.4
Surprise Genre
Token-Level Post-Level
Nepali-English 85.6 77.5
Spanish-English 94.4 80.0
</table>
<tableCaption confidence="0.996541">
Table 6: Test set results (overall accuracy) for
</tableCaption>
<bodyText confidence="0.958197714285714">
Nepali-English and Spanish-English tweet data
and surprise genre
sulting in slightly lower performance. Therefore,
we selected SVM-GDLC-P1N1 as our final ap-
proach and trained the final two systems using the
full training data for Nepali-English and Spanish-
English respectively. While we knew that C =
0.125 is best for Nepali-English from our experi-
ments, we had to re-tune parameter C for Spanish-
English using cross-validation on the training data.
We found best accuracy of 94.16% for Spanish-
English with C = 128. Final predictions for the
test sets are made using these systems.
Table 6 shows the test set results. The test
set for this task is divided into tweets and a sur-
prise genre. For the tweets, we achieve 96.3%
and 84.4% accuracy (overall token-level accuracy)
in Nepali-English and in Spanish-English respec-
tively. For this surprise genre (a collection of posts
from Facebook and blogs), we achieve 85.6% for
Nepali-English and 94.4% for Spanish-English.
</bodyText>
<sectionHeader confidence="0.994531" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999899714285714">
To summarise, we achieved reasonable accuracy
with a 6-way SVM classifier by employing basic
features only. We found that using dictionaries
is helpful, as are contextual features. The perfor-
mance of the k-NN classifier is also notable: it is
only 1.45 percentage points behind the final SVM-
based system (in terms of cross-validation accu-
racy). Adding neural network features can further
increase the accuracy of systems.
Briefly opening the test files to check for for-
matting issues, we notice that the surprise genre
data contains language-specific scripts that could
easily be addressed in an English vs. non-English
scenario.
</bodyText>
<sectionHeader confidence="0.988706" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<footnote confidence="0.994996">
This research is supported by the Science Founda-
tion Ireland (Grant 12/CE/I2267) as part of CNGL
(www.cngl.ie) at Dublin City University.
</footnote>
<page confidence="0.995541">
130
</page>
<sectionHeader confidence="0.959062" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999375963636363">
Beatrice Alex. 2008. Automatic detection of English
inclusions in mixed-lingual data with an application
to parsing. Ph.D. thesis, School of Informatics, The
University of Edinburgh, Edinburgh, UK.
Guy Aston and Lou Burnard. 1998. The BNC hand-
book: exploring the British National Corpus with
SARA. Capstone.
Utsab Barman, Amitava Das, Joachim Wagner, and
Jennifer Foster. 2014. Code-mixing: A challenge
for language identification in the language of so-
cial media. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Theo Pavlidis,
editor, Proceedings of SDAIR-94, Third Annual
Symposium on Document Analysis and Information
Retrieval, pages 161–175.
Grzegorz Chrupała. 2014. Normalizing tweets with
edit scripts and recurrent neural embeddings. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 680–686, Baltimore, Mary-
land, June. Association for Computational Linguis-
tics.
Heba Elfardy and Mona Diab. 2012. Token level
identification of linguistic code switching. In Pro-
ceedings of Proceedings of COLING 2012: Posters
(the 24th International Conference on Computa-
tional Linguistics), pages 287–296, Mumbai, India.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2013. Code switch point detection in Ara-
bic. In Natural Language Processing and Informa-
tion Systems, pages 412–416. Springer.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Paulseph-John Farrugia. 2004. TTS pre-processing is-
sues for mixed language support. In Proceedings
of CSAW’04, the second Computer Science Annual
Workshop, pages 36–41. Department of Computer
Science &amp; A.I., University of Malta.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically constructing a normalisation dictio-
nary for microblogs. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 421–432. Association
for Computational Linguistics.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In J. Horeck´y, ed-
itor, Proceedings of the 9th conference on Compu-
tational linguistics - Volume 1 (COLING’82), pages
145–150. Academia Praha, North-Holland Publish-
ing Company.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1110–
1119, Atlanta, Georgia, June. Association for Com-
putational Linguistics.
Ying Li, Yue Yu, and Pascale Fung. 2012. A
mandarin-english code-switching corpus. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Mehmet Uur Doan, Bente Mae-
gaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings
of the Eight International Conference on Language
Resources and Evaluation (LREC’12), Istanbul,
Turkey, may. European Language Resources Asso-
ciation (ELRA).
Dau-Cheng Lyu, Tien Ping Tan, Engsiong Chng, and
Haizhou Li. 2010. SEAME: A Mandarin-English
code-switching speech corpus in South-East Asia.
In INTERSPEECH 2010, 11th Annual Conference
of the International Speech Communication Asso-
ciation, volume 10, pages 1986–1989, Makuhari,
Chiba, Japan. ISCA Archive.
Dong Nguyen and A. Seza Do˘gru¨oz. 2013. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2013), pages 857–862, Seattle,
Washington, USA, October. Association for Com-
putational Linguistics.
Mike Rosner and Paulseph-John Farrugia. 2007. A
tagging algorithm for mixed language identifica-
tion in a noisy domain. In INTERSPEECH-2007,
8th Annual Conference of the International Speech
Communication Association, pages 190–193. ISCA
Archive.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392–397, Sofia, Bulgaria.
Association for Computational Linguistics.
Thamar Solorio and Yang Liu. 2008a. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973–981. Association for
Computational Linguistics.
Thamar Solorio and Yang Liu. 2008b. Part-of-speech
tagging for English-Spanish code-switched text. In
</reference>
<page confidence="0.978462">
131
</page>
<reference confidence="0.9985619">
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1051–
1060. Association for Computational Linguistics.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster, and
Lamia Tounsi. 2014. DCU: Aspect-based polarity
classification for SemEval task 4. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval-2014), pages 392–397, Dublin, Ireland,
August. Association for Computational Linguistics.
</reference>
<page confidence="0.997734">
132
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.613039">
<title confidence="0.998196">DCU-UVT: Word-Level Language Classification with Code-Mixed Data</title>
<author confidence="0.967568">Joachim Wagner Barman</author>
<author confidence="0.967568">Grzegorz Jennifer</author>
<affiliation confidence="0.96486775">CNGL Centre for Global Intelligent Content, National Centre for Language School of Computing, Dublin City University, Dublin, School of Humanities, Department of Communication and Information Tilburg University, Tilburg, The</affiliation>
<email confidence="0.977872">G.A.Chrupala@uvt.nl</email>
<abstract confidence="0.9705294375">This paper describes the DCU-UVT participation in the Idenin Code-Switched Data in the on Computational to Code Wordlevel classification experiments were carried out using a simple dictionary-based method, linear kernel support vector machines (SVMs) with and without conclues, and a neighbour approach. Based on these experiments, we select our SVM-based system with contextual clues as our final system and present results for the Nepali-English and Spanish-English datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Beatrice Alex</author>
</authors>
<title>Automatic detection of English inclusions in mixed-lingual data with an application to parsing.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Informatics, The University of Edinburgh,</institution>
<location>Edinburgh, UK.</location>
<contexts>
<context position="2980" citStr="Alex, 2008" startWordPosition="423" endWordPosition="424">t) as our final system. 2 Background While the problem of automatically identifying and analysing code-mixing has been identified over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as well as word-level classification using Naive Bayes (Solorio and Liu, 2008a), logistic regression (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), using features such as word, POS, lemma and character-n-grams. Language pairs t</context>
</contexts>
<marker>Alex, 2008</marker>
<rawString>Beatrice Alex. 2008. Automatic detection of English inclusions in mixed-lingual data with an application to parsing. Ph.D. thesis, School of Informatics, The University of Edinburgh, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy Aston</author>
<author>Lou Burnard</author>
</authors>
<date>1998</date>
<booktitle>The BNC handbook: exploring the British National Corpus with SARA.</booktitle>
<publisher>Capstone.</publisher>
<contexts>
<context position="6391" citStr="Aston and Burnard, 1998" startWordPosition="928" endWordPosition="931">plits for each cross-validation run. We address the task with the following approaches: 1. a simple dictionary-based classifier, Resource Accuracy BNC 43.61 LexNorm 54.60 TrainingData 89.53 TrainingData+BNC+LexNorm 90.71 Table 2: Average cross-validation accuracy of dictionary-based prediction for Nepali-English 2. classification using supervised machine learning with k-nearest neighbour, and 3. classification using supervised machine learning with SVMs. 4.1 Dictionary-Based Detection We start with a simple dictionary-based approach using as dictionaries (a) the British National Corpus (BNC) (Aston and Burnard, 1998), (b) Han et al.’s lexical normalisation dictionary (LexNorm) (Han et al., 2012) and (c) the training data. The BNC and LexNorm dictionaries are built by recording all words occurring in the respective corpus or word list as English. For the BNC, we also collect word frequency information. For the training data, we obtain dictionaries for each of the six labels and each of the five cross-validation runs (using the relevant 4/5 of training data). To make a prediction, we consult all dictionaries. If there are more than one candidate label, we choose the label for which the frequency for the que</context>
</contexts>
<marker>Aston, Burnard, 1998</marker>
<rawString>Guy Aston and Lou Burnard. 1998. The BNC handbook: exploring the British National Corpus with SARA. Capstone.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Utsab Barman</author>
<author>Amitava Das</author>
<author>Joachim Wagner</author>
<author>Jennifer Foster</author>
</authors>
<title>Code-mixing: A challenge for language identification in the language of social media.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Workshop on Computational Approaches to Code-Switching. EMNLP 2014, Conference on Empirical Methods in Natural Language Processing,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="2904" citStr="Barman et al., 2014" startWordPosition="407" endWordPosition="410">ures (word, character-ngrams, length of a word, capitalisation information and context) as our final system. 2 Background While the problem of automatically identifying and analysing code-mixing has been identified over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as well as word-level classification using Naive Bayes (Solorio and Liu, 2008a), logistic regression (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), usi</context>
<context position="9051" citStr="Barman et al. (2014)" startWordPosition="1371" endWordPosition="1374">atenating the previous, current and next token, and (c) weighting context by first calculating edit distances for the previous, current and next token separately and using a weighted average. The best configuration we found in crossvalidation uses lowercasing with k1 = 800 and k2 = 16 but no context information. It achieves an accuracy of 94.97%. 4.3 SVM Classification We experiment with linear kernel SVM classifiers using Liblinear (Fan et al., 2008). Parameter optimisation2 is performed for each feature set combination to obtain best cross-validation accuracy. 4.3.1 Basic Features Following Barman et al. (2014), our basic features are: Char-N-Grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994). Following King and Abney (2013), we select lowercased character n-grams (n=1 to 5) and the word as the features in our experiments. Dictionary-Based Labels (D): We use presence in the dictionary of the 5,000 most frequent words in the BNC and presence in the LexNorm dictionary as binary features.3 Length of words (L): We create multiple features for token length using a decision tree (J48). We use length as the only feature to train a decision tree for each fold and use the n</context>
<context position="11989" citStr="Barman et al. (2014)" startWordPosition="1878" endWordPosition="1881"> using the bigrams formed with the previous and next word are the best combination for the task (among those tested). Among the eight combinations of the first four feature sets that contain the first set (G), Table 3 shows that the 6-way SVM classifier4performs best with all features sets (GDLC), achieving 96.40% accuracy. Adding contextual information PiNj to GDLC, Table 4 shows best results for i=j=1, achieving 96.42% accuracy, only slightly ahead of the context-independent system. 4.3.2 Neural Network (Elman) and k-NN Features We experiment with two additional features sets not covered by Barman et al. (2014): Neural Network (Elman): We extract features from the hidden layer of a recurrent neural net4We also test 3-way SVM classification (lang1, lang2 and other) and heuristic post-processing, but it does not outperform our 6-way classification runs. 129 Systems Accuracy GDLC 96.40 k-NN 95.10 Elman 89.96 GDLC+k-NN 96.31 GDLC+Elman 96.46 GDLC+k-NN+Elman 96.40 GDLC+P1N1 96.42 k-NN+P1N1 95.11 Elman+P1N1 91.53 GDLC+P1N1+k-NN 96.33 GDLC+P1N1+Elman 96.45 GDLC+P1N1+k-NN+Elman 96.40 Table 5: Average cross-validation accuracy of 6- way SVMs of combinations of GDLC, k-NN, Elman and P1N1 features for Nepali-E</context>
</contexts>
<marker>Barman, Das, Wagner, Foster, 2014</marker>
<rawString>Utsab Barman, Amitava Das, Joachim Wagner, and Jennifer Foster. 2014. Code-mixing: A challenge for language identification in the language of social media. In Proceedings of the First Workshop on Computational Approaches to Code-Switching. EMNLP 2014, Conference on Empirical Methods in Natural Language Processing, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Cavnar</author>
<author>John M Trenkle</author>
</authors>
<title>Ngram-based text categorization.</title>
<date>1994</date>
<booktitle>Proceedings of SDAIR-94, Third Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>161--175</pages>
<editor>In Theo Pavlidis, editor,</editor>
<contexts>
<context position="9169" citStr="Cavnar and Trenkle, 1994" startWordPosition="1389" endWordPosition="1392"> the previous, current and next token separately and using a weighted average. The best configuration we found in crossvalidation uses lowercasing with k1 = 800 and k2 = 16 but no context information. It achieves an accuracy of 94.97%. 4.3 SVM Classification We experiment with linear kernel SVM classifiers using Liblinear (Fan et al., 2008). Parameter optimisation2 is performed for each feature set combination to obtain best cross-validation accuracy. 4.3.1 Basic Features Following Barman et al. (2014), our basic features are: Char-N-Grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994). Following King and Abney (2013), we select lowercased character n-grams (n=1 to 5) and the word as the features in our experiments. Dictionary-Based Labels (D): We use presence in the dictionary of the 5,000 most frequent words in the BNC and presence in the LexNorm dictionary as binary features.3 Length of words (L): We create multiple features for token length using a decision tree (J48). We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features (Rubino et al., 2013; Wagner et al., 2014). 1Starting with n = </context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>William B. Cavnar and John M. Trenkle. 1994. Ngram-based text categorization. In Theo Pavlidis, editor, Proceedings of SDAIR-94, Third Annual Symposium on Document Analysis and Information Retrieval, pages 161–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupała</author>
</authors>
<title>Normalizing tweets with edit scripts and recurrent neural embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>680--686</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="12681" citStr="Chrupała, 2014" startWordPosition="1982" endWordPosition="1983">rrent neural net4We also test 3-way SVM classification (lang1, lang2 and other) and heuristic post-processing, but it does not outperform our 6-way classification runs. 129 Systems Accuracy GDLC 96.40 k-NN 95.10 Elman 89.96 GDLC+k-NN 96.31 GDLC+Elman 96.46 GDLC+k-NN+Elman 96.40 GDLC+P1N1 96.42 k-NN+P1N1 95.11 Elman+P1N1 91.53 GDLC+P1N1+k-NN 96.33 GDLC+P1N1+Elman 96.45 GDLC+P1N1+k-NN+Elman 96.40 Table 5: Average cross-validation accuracy of 6- way SVMs of combinations of GDLC, k-NN, Elman and P1N1 features for Nepali-English work that has been trained to predict the next character in a string (Chrupała, 2014). The 10 most active units of the hidden layer for each of the initial 4 bytes and final 4 bytes of each token are binarised by using a threshold of 0.5. k-Nearest Neighbour (kNN): We obtain features from our basic k-NN approach (Section 4.2), encoding the prediction of the k-NN model with six binary features (one for each label) and a numeric feature for each label stating the relative number of votes for the label, e.g. if k2 = 16 and 12 votes are for lang1 the value of the feature votes4lang1 will be 0.75. Furthermore, we add two features stating the minimum and maximum edit distance betwee</context>
</contexts>
<marker>Chrupała, 2014</marker>
<rawString>Grzegorz Chrupała. 2014. Normalizing tweets with edit scripts and recurrent neural embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 680–686, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heba Elfardy</author>
<author>Mona Diab</author>
</authors>
<title>Token level identification of linguistic code switching.</title>
<date>2012</date>
<booktitle>In Proceedings of Proceedings of COLING 2012: Posters (the 24th International Conference on Computational Linguistics),</booktitle>
<pages>287--296</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="3119" citStr="Elfardy and Diab, 2012" startWordPosition="441" endWordPosition="445">ied over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as well as word-level classification using Naive Bayes (Solorio and Liu, 2008a), logistic regression (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), using features such as word, POS, lemma and character-n-grams. Language pairs that have been explored include English-Maltese (Farrugia, 2004; Rosner and Farrugia, 2007), English-Spanish (Solorio and Liu, 2008b), Turki</context>
</contexts>
<marker>Elfardy, Diab, 2012</marker>
<rawString>Heba Elfardy and Mona Diab. 2012. Token level identification of linguistic code switching. In Proceedings of Proceedings of COLING 2012: Posters (the 24th International Conference on Computational Linguistics), pages 287–296, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heba Elfardy</author>
<author>Mohamed Al-Badrashiny</author>
<author>Mona Diab</author>
</authors>
<title>Code switch point detection in Arabic.</title>
<date>2013</date>
<booktitle>In Natural Language Processing and Information Systems,</booktitle>
<pages>412--416</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2926" citStr="Elfardy et al., 2013" startWordPosition="411" endWordPosition="415">-ngrams, length of a word, capitalisation information and context) as our final system. 2 Background While the problem of automatically identifying and analysing code-mixing has been identified over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as well as word-level classification using Naive Bayes (Solorio and Liu, 2008a), logistic regression (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), using features such as wo</context>
</contexts>
<marker>Elfardy, Al-Badrashiny, Diab, 2013</marker>
<rawString>Heba Elfardy, Mohamed Al-Badrashiny, and Mona Diab. 2013. Code switch point detection in Arabic. In Natural Language Processing and Information Systems, pages 412–416. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="8886" citStr="Fan et al., 2008" startWordPosition="1347" endWordPosition="1350">tem and the best k2 matches are used to make a prediction. Apart from varying k1 and k2, we experiment with (a) lowercasing strings, (b) including context by concatenating the previous, current and next token, and (c) weighting context by first calculating edit distances for the previous, current and next token separately and using a weighted average. The best configuration we found in crossvalidation uses lowercasing with k1 = 800 and k2 = 16 but no context information. It achieves an accuracy of 94.97%. 4.3 SVM Classification We experiment with linear kernel SVM classifiers using Liblinear (Fan et al., 2008). Parameter optimisation2 is performed for each feature set combination to obtain best cross-validation accuracy. 4.3.1 Basic Features Following Barman et al. (2014), our basic features are: Char-N-Grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994). Following King and Abney (2013), we select lowercased character n-grams (n=1 to 5) and the word as the features in our experiments. Dictionary-Based Labels (D): We use presence in the dictionary of the 5,000 most frequent words in the BNC and presence in the LexNorm dictionary as binary features.3 Length of words </context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paulseph-John Farrugia</author>
</authors>
<title>TTS pre-processing issues for mixed language support.</title>
<date>2004</date>
<booktitle>In Proceedings of CSAW’04, the second Computer Science Annual Workshop,</booktitle>
<pages>36--41</pages>
<institution>Department of Computer Science &amp; A.I., University of Malta.</institution>
<contexts>
<context position="3228" citStr="Farrugia, 2004" startWordPosition="460" endWordPosition="461">anguage identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as well as word-level classification using Naive Bayes (Solorio and Liu, 2008a), logistic regression (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), using features such as word, POS, lemma and character-n-grams. Language pairs that have been explored include English-Maltese (Farrugia, 2004; Rosner and Farrugia, 2007), English-Spanish (Solorio and Liu, 2008b), Turkish-Dutch (Nguyen and Do˘gru¨oz, 127 Proceedings of The First Workshop on Computational Approaches to Code Swi</context>
</contexts>
<marker>Farrugia, 2004</marker>
<rawString>Paulseph-John Farrugia. 2004. TTS pre-processing issues for mixed language support. In Proceedings of CSAW’04, the second Computer Science Annual Workshop, pages 36–41. Department of Computer Science &amp; A.I., University of Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatically constructing a normalisation dictionary for microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>421--432</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6471" citStr="Han et al., 2012" startWordPosition="940" endWordPosition="943"> 1. a simple dictionary-based classifier, Resource Accuracy BNC 43.61 LexNorm 54.60 TrainingData 89.53 TrainingData+BNC+LexNorm 90.71 Table 2: Average cross-validation accuracy of dictionary-based prediction for Nepali-English 2. classification using supervised machine learning with k-nearest neighbour, and 3. classification using supervised machine learning with SVMs. 4.1 Dictionary-Based Detection We start with a simple dictionary-based approach using as dictionaries (a) the British National Corpus (BNC) (Aston and Burnard, 1998), (b) Han et al.’s lexical normalisation dictionary (LexNorm) (Han et al., 2012) and (c) the training data. The BNC and LexNorm dictionaries are built by recording all words occurring in the respective corpus or word list as English. For the BNC, we also collect word frequency information. For the training data, we obtain dictionaries for each of the six labels and each of the five cross-validation runs (using the relevant 4/5 of training data). To make a prediction, we consult all dictionaries. If there are more than one candidate label, we choose the label for which the frequency for the query token is highest. To account for the fact that the BNC is much larger than th</context>
</contexts>
<marker>Han, Cook, Baldwin, 2012</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically constructing a normalisation dictionary for microblogs. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 421–432. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Processing of sentences with intra-sentential code-switching.</title>
<date>1982</date>
<booktitle>Proceedings of the 9th conference on Computational linguistics - Volume</booktitle>
<volume>1</volume>
<pages>145--150</pages>
<editor>In J. Horeck´y, editor,</editor>
<publisher>Academia Praha, North-Holland Publishing Company.</publisher>
<contexts>
<context position="2531" citStr="Joshi, 1982" startWordPosition="355" endWordPosition="356">tern-grams, length of a word and capitalisation information) and context-sensitive (adding the previous and next word as bigrams) features in different combinations. We also experiment with adding features from the k-NN approach and another set of features from a neural network. Based on performance in cross-validation, we select the SVM classifier with basic features (word, character-ngrams, length of a word, capitalisation information and context) as our final system. 2 Background While the problem of automatically identifying and analysing code-mixing has been identified over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various</context>
</contexts>
<marker>Joshi, 1982</marker>
<rawString>Aravind K. Joshi. 1982. Processing of sentences with intra-sentential code-switching. In J. Horeck´y, editor, Proceedings of the 9th conference on Computational linguistics - Volume 1 (COLING’82), pages 145–150. Academia Praha, North-Holland Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben King</author>
<author>Steven Abney</author>
</authors>
<title>Labeling the languages of words in mixed-language documents using weakly supervised methods.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1110--1119</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="3337" citStr="King and Abney, 2013" startWordPosition="475" endWordPosition="478">ing (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as well as word-level classification using Naive Bayes (Solorio and Liu, 2008a), logistic regression (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), using features such as word, POS, lemma and character-n-grams. Language pairs that have been explored include English-Maltese (Farrugia, 2004; Rosner and Farrugia, 2007), English-Spanish (Solorio and Liu, 2008b), Turkish-Dutch (Nguyen and Do˘gru¨oz, 127 Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 127–132, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2013),</context>
<context position="9202" citStr="King and Abney (2013)" startWordPosition="1394" endWordPosition="1397"> separately and using a weighted average. The best configuration we found in crossvalidation uses lowercasing with k1 = 800 and k2 = 16 but no context information. It achieves an accuracy of 94.97%. 4.3 SVM Classification We experiment with linear kernel SVM classifiers using Liblinear (Fan et al., 2008). Parameter optimisation2 is performed for each feature set combination to obtain best cross-validation accuracy. 4.3.1 Basic Features Following Barman et al. (2014), our basic features are: Char-N-Grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994). Following King and Abney (2013), we select lowercased character n-grams (n=1 to 5) and the word as the features in our experiments. Dictionary-Based Labels (D): We use presence in the dictionary of the 5,000 most frequent words in the BNC and presence in the LexNorm dictionary as binary features.3 Length of words (L): We create multiple features for token length using a decision tree (J48). We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features (Rubino et al., 2013; Wagner et al., 2014). 1Starting with n = 5, we decrease n until there are </context>
</contexts>
<marker>King, Abney, 2013</marker>
<rawString>Ben King and Steven Abney. 2013. Labeling the languages of words in mixed-language documents using weakly supervised methods. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1110– 1119, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ying Li</author>
<author>Yue Yu</author>
<author>Pascale Fung</author>
</authors>
<title>A mandarin-english code-switching corpus.</title>
<date>2012</date>
<booktitle>Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors,</editor>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="4035" citStr="Li et al., 2012" startWordPosition="572" endWordPosition="575">logistic regression (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), using features such as word, POS, lemma and character-n-grams. Language pairs that have been explored include English-Maltese (Farrugia, 2004; Rosner and Farrugia, 2007), English-Spanish (Solorio and Liu, 2008b), Turkish-Dutch (Nguyen and Do˘gru¨oz, 127 Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 127–132, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2013), modern standard Arabic-Egyptian dialect (Elfardy et al., 2013), Mandarin-English (Li et al., 2012; Lyu et al., 2010), and English-HindiBengali (Barman et al., 2014). 3 Data Statistics The training data provided for this task consists of tweets. Unfortunately, because of deleted tweets, the full training set could not be downloaded. Out of 9,993 Nepali-English training tweets, we were able to download 9,668 and out of 11,400 SpanishEnglish training tweets, we were able to download 11,353. Table 1 shows the token-level statistics of the two datasets. Label Nepali-English Spanish-English lang1 (en) 43,185 76,204 lang2 (ne/es) 59,579 32,477 ne 3,821 2,814 ambiguous 125 341 mixed 112 51 other </context>
</contexts>
<marker>Li, Yu, Fung, 2012</marker>
<rawString>Ying Li, Yue Yu, and Pascale Fung. 2012. A mandarin-english code-switching corpus. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dau-Cheng Lyu</author>
<author>Tien Ping Tan</author>
<author>Engsiong Chng</author>
<author>Haizhou Li</author>
</authors>
<title>SEAME: A Mandarin-English code-switching speech corpus in South-East Asia.</title>
<date>2010</date>
<journal>ISCA Archive.</journal>
<booktitle>In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,</booktitle>
<volume>10</volume>
<pages>1986--1989</pages>
<location>Makuhari, Chiba,</location>
<contexts>
<context position="4054" citStr="Lyu et al., 2010" startWordPosition="576" endWordPosition="579">on (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), using features such as word, POS, lemma and character-n-grams. Language pairs that have been explored include English-Maltese (Farrugia, 2004; Rosner and Farrugia, 2007), English-Spanish (Solorio and Liu, 2008b), Turkish-Dutch (Nguyen and Do˘gru¨oz, 127 Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 127–132, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2013), modern standard Arabic-Egyptian dialect (Elfardy et al., 2013), Mandarin-English (Li et al., 2012; Lyu et al., 2010), and English-HindiBengali (Barman et al., 2014). 3 Data Statistics The training data provided for this task consists of tweets. Unfortunately, because of deleted tweets, the full training set could not be downloaded. Out of 9,993 Nepali-English training tweets, we were able to download 9,668 and out of 11,400 SpanishEnglish training tweets, we were able to download 11,353. Table 1 shows the token-level statistics of the two datasets. Label Nepali-English Spanish-English lang1 (en) 43,185 76,204 lang2 (ne/es) 59,579 32,477 ne 3,821 2,814 ambiguous 125 341 mixed 112 51 other 34,566 21,813 Table</context>
</contexts>
<marker>Lyu, Tan, Chng, Li, 2010</marker>
<rawString>Dau-Cheng Lyu, Tien Ping Tan, Engsiong Chng, and Haizhou Li. 2010. SEAME: A Mandarin-English code-switching speech corpus in South-East Asia. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, volume 10, pages 1986–1989, Makuhari, Chiba, Japan. ISCA Archive.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Nguyen</author>
<author>A Seza Do˘gru¨oz</author>
</authors>
<title>Word level language identification in online multilingual communication.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013),</booktitle>
<pages>857--862</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<marker>Nguyen, Do˘gru¨oz, 2013</marker>
<rawString>Dong Nguyen and A. Seza Do˘gru¨oz. 2013. Word level language identification in online multilingual communication. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), pages 857–862, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Rosner</author>
<author>Paulseph-John Farrugia</author>
</authors>
<title>A tagging algorithm for mixed language identification in a noisy domain.</title>
<date>2007</date>
<booktitle>In INTERSPEECH-2007, 8th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>190--193</pages>
<publisher>ISCA Archive.</publisher>
<contexts>
<context position="3256" citStr="Rosner and Farrugia, 2007" startWordPosition="462" endWordPosition="466">cation in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as well as word-level classification using Naive Bayes (Solorio and Liu, 2008a), logistic regression (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), using features such as word, POS, lemma and character-n-grams. Language pairs that have been explored include English-Maltese (Farrugia, 2004; Rosner and Farrugia, 2007), English-Spanish (Solorio and Liu, 2008b), Turkish-Dutch (Nguyen and Do˘gru¨oz, 127 Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 127–132, Octob</context>
</contexts>
<marker>Rosner, Farrugia, 2007</marker>
<rawString>Mike Rosner and Paulseph-John Farrugia. 2007. A tagging algorithm for mixed language identification in a noisy domain. In INTERSPEECH-2007, 8th Annual Conference of the International Speech Communication Association, pages 190–193. ISCA Archive.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Rubino</author>
<author>Joachim Wagner</author>
<author>Jennifer Foster</author>
<author>Johann Roturier</author>
<author>Rasoul Samad Zadeh Kaljahi</author>
<author>Fred Hollowood</author>
</authors>
<title>DCU-Symantec at the WMT 2013 quality estimation shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>392--397</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<contexts>
<context position="9726" citStr="Rubino et al., 2013" startWordPosition="1489" endWordPosition="1492">h a character n-gram-based approach (Cavnar and Trenkle, 1994). Following King and Abney (2013), we select lowercased character n-grams (n=1 to 5) and the word as the features in our experiments. Dictionary-Based Labels (D): We use presence in the dictionary of the 5,000 most frequent words in the BNC and presence in the LexNorm dictionary as binary features.3 Length of words (L): We create multiple features for token length using a decision tree (J48). We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features (Rubino et al., 2013; Wagner et al., 2014). 1Starting with n = 5, we decrease n until there are at least k1 items and then we randomly remove items added in the last augmentation step to arrive at exactly k1 items. (For n = 0, we randomly sample from the full training data.) 2C = 2&apos; with i = −15, −14,..., 10 3We chose these parameters based on experiments with each dictionary, combinations of dictionaries and various frequency thresholds. We apply a frequency threshold to the BNC to increase precision. We rank the words according to frequency and used the rank as a threshold (e.g. top-5K, top10K etc.). With the t</context>
</contexts>
<marker>Rubino, Wagner, Foster, Roturier, Kaljahi, Hollowood, 2013</marker>
<rawString>Raphael Rubino, Joachim Wagner, Jennifer Foster, Johann Roturier, Rasoul Samad Zadeh Kaljahi, and Fred Hollowood. 2013. DCU-Symantec at the WMT 2013 quality estimation shared task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 392–397, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thamar Solorio</author>
<author>Yang Liu</author>
</authors>
<title>Learning to predict code-switching points.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>973--981</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2742" citStr="Solorio and Liu, 2008" startWordPosition="381" endWordPosition="384">res from the k-NN approach and another set of features from a neural network. Based on performance in cross-validation, we select the SVM classifier with basic features (word, character-ngrams, length of a word, capitalisation information and context) as our final system. 2 Background While the problem of automatically identifying and analysing code-mixing has been identified over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as </context>
</contexts>
<marker>Solorio, Liu, 2008</marker>
<rawString>Thamar Solorio and Yang Liu. 2008a. Learning to predict code-switching points. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 973–981. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thamar Solorio</author>
<author>Yang Liu</author>
</authors>
<title>Part-of-speech tagging for English-Spanish code-switched text.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1051--1060</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="2742" citStr="Solorio and Liu, 2008" startWordPosition="381" endWordPosition="384">res from the k-NN approach and another set of features from a neural network. Based on performance in cross-validation, we select the SVM classifier with basic features (word, character-ngrams, length of a word, capitalisation information and context) as our final system. 2 Background While the problem of automatically identifying and analysing code-mixing has been identified over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as </context>
</contexts>
<marker>Solorio, Liu, 2008</marker>
<rawString>Thamar Solorio and Yang Liu. 2008b. Part-of-speech tagging for English-Spanish code-switched text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1051– 1060. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Thamar Solorio</author>
<author>Elizabeth Blair</author>
<author>Suraj Maharjan</author>
<author>Steve Bethard</author>
<author>Mona Diab</author>
<author>Mahmoud Gonheim</author>
<author>Abdelati Hawwari</author>
<author>Fahad AlGhamdi</author>
<author>Julia Hirshberg</author>
<author>Alison Chang</author>
<author>Pascale Fung</author>
</authors>
<title>Overview for the first shared task on language identification in codeswitched data.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Workshop on Computational Approaches to Code-Switching. EMNLP 2014, Conference on Empirical Methods in Natural Language Processing,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="1193" citStr="Solorio et al., 2014" startWordPosition="151" endWordPosition="154">-Switched Data shared task in the Workshop on Computational Approaches to Code Switching. Wordlevel classification experiments were carried out using a simple dictionary-based method, linear kernel support vector machines (SVMs) with and without contextual clues, and a k-nearest neighbour approach. Based on these experiments, we select our SVM-based system with contextual clues as our final system and present results for the Nepali-English and Spanish-English datasets. 1 Introduction This paper describes DCU-UVT’s participation in the shared task Language Identification in Code-Switched Data (Solorio et al., 2014) at the Workshop on Computational Approaches to Code Switching, EMNLP, 2014. The task is to make word-level predictions (six labels: lang1, lang2, ne, mixed, ambiguous and other) for mixedlanguage user generated content. We submit predictions for Nepali-English and Spanish-English data and perform experiments using dictionaries, a k-nearest neighbour (k-NN) classifier and a linearkernel SVM classifier. In our dictionary-based approach, we investigate the use of different English dictionaries as well as the training data. In the k-NN based approach, we use string edit distance, charactern-gram </context>
</contexts>
<marker>Solorio, Blair, Maharjan, Bethard, Diab, Gonheim, Hawwari, AlGhamdi, Hirshberg, Chang, Fung, 2014</marker>
<rawString>Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve Bethard, Mona Diab, Mahmoud Gonheim, Abdelati Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison Chang, and Pascale Fung. 2014. Overview for the first shared task on language identification in codeswitched data. In Proceedings of the First Workshop on Computational Approaches to Code-Switching. EMNLP 2014, Conference on Empirical Methods in Natural Language Processing, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Wagner</author>
<author>Piyush Arora</author>
<author>Santiago Cortes</author>
<author>Utsab Barman</author>
<author>Dasha Bogdanova</author>
<author>Jennifer Foster</author>
<author>Lamia Tounsi</author>
</authors>
<title>DCU: Aspect-based polarity classification for SemEval task 4.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<pages>392--397</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="9748" citStr="Wagner et al., 2014" startWordPosition="1493" endWordPosition="1496">based approach (Cavnar and Trenkle, 1994). Following King and Abney (2013), we select lowercased character n-grams (n=1 to 5) and the word as the features in our experiments. Dictionary-Based Labels (D): We use presence in the dictionary of the 5,000 most frequent words in the BNC and presence in the LexNorm dictionary as binary features.3 Length of words (L): We create multiple features for token length using a decision tree (J48). We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features (Rubino et al., 2013; Wagner et al., 2014). 1Starting with n = 5, we decrease n until there are at least k1 items and then we randomly remove items added in the last augmentation step to arrive at exactly k1 items. (For n = 0, we randomly sample from the full training data.) 2C = 2&apos; with i = −15, −14,..., 10 3We chose these parameters based on experiments with each dictionary, combinations of dictionaries and various frequency thresholds. We apply a frequency threshold to the BNC to increase precision. We rank the words according to frequency and used the rank as a threshold (e.g. top-5K, top10K etc.). With the top 5,000 ranked words </context>
</contexts>
<marker>Wagner, Arora, Cortes, Barman, Bogdanova, Foster, Tounsi, 2014</marker>
<rawString>Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab Barman, Dasha Bogdanova, Jennifer Foster, and Lamia Tounsi. 2014. DCU: Aspect-based polarity classification for SemEval task 4. In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2014), pages 392–397, Dublin, Ireland, August. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
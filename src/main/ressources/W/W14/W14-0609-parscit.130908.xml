<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.993607">
Mining the Twentieth Century’s History from the Time Magazine Corpus
</title>
<author confidence="0.996653">
Mike Kestemont
</author>
<affiliation confidence="0.996597">
University of Antwerp
</affiliation>
<address confidence="0.70112775">
Prinsstraat 13, D.188
B-2000, Antwerp
Belgium
mike.kestemont
</address>
<email confidence="0.912828">
@uantwerpen.be
</email>
<note confidence="0.5088315">
Folgert Karsdorp
Meertens Institute
Postbus 94264
1090 GG Amsterdam
The Netherlands
Folgert.Karsdorp
</note>
<email confidence="0.984396">
@meertens.knaw.nl
</email>
<author confidence="0.993162">
Marten D¨uring
</author>
<affiliation confidence="0.992531">
University of North-Carolina
</affiliation>
<address confidence="0.66259">
551 Hamilton Hall
CB 3195, Chapel Hill
North Carolina 27599
United States
</address>
<email confidence="0.999166">
marten@live.unc.edu
</email>
<sectionHeader confidence="0.993955" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999440434782609">
In this paper we report on an explorative
study of the history of the twentieth cen-
tury from a lexical point of view. As
data, we use a diachronic collection of
270,000+ English-language articles har-
vested from the electronic archive of the
well-known Time Magazine (1923–2006).
We attempt to automatically identify sig-
nificant shifts in the vocabulary used in
this corpus using efficient, yet unsuper-
vised computational methods, such as Par-
simonious Language Models. We offer a
qualitative interpretation of the outcome
of our experiments in the light of momen-
tous events in the twentieth century, such
as the Second World War or the rise of
the Internet. This paper follows up on a
recent string of frequentist approaches to
studying cultural history (‘Culturomics’),
in which the evolution of human culture is
studied from a quantitative perspective, on
the basis of lexical statistics extracted from
large, textual data sets.
</bodyText>
<sectionHeader confidence="0.995944" genericHeader="method">
1 Introduction: Culturomics
</sectionHeader>
<bodyText confidence="0.99980644">
Although traditionally, the Humanities have been
more strongly associated with qualitative rather
than quantitative methodologies, it is hard to miss
that ‘hipster’ terms like ‘Computational Analy-
sis’, ‘Big Data’ and ‘Digitisation’, are currently
trending in Humanities scholarship. In the interna-
tional initiative of Digital Humanities, researchers
from various disciplines are increasingly explor-
ing novel, computational means to interact with
their object of research. Often, this is done in col-
laboration with researchers from Computational
Linguistics, who seem to have adopted quantita-
tive approaches relatively sooner than other Hu-
manities disciplines. The subfield of Digital His-
tory (Zaagsma, 2013), in which the present paper
is to be situated, is but one of the multiple Human-
ities disciplines in which rapid progress is being
made as to the application of computational meth-
ods. Although the vibrant domain of Digital His-
tory cannot be exhaustively surveyed here due to
space limits, it is nevertheless interesting to refer
to a recent string of frequentist lexical approaches
to the study of human history, and the evolution of
human culture in particular: ‘Culturomics’.
This line of computational, typically data-
intensive research seeks to study various aspects of
human history, by researching the ways in which
(predominantly cultural) phenomena are reflected
in, for instance, word frequency statistics extracted
from large textual data sets. The field has been ini-
tiated in a lively, yet controversial publication by
Michel et al. (2011), which – while it has invited
a lot of attention in popular media – has not gone
uncriticized in the international community of Hu-
manities.1 In this paper, the authors show how a
number of major historical events show interest-
ing correlations with word counts in a vast corpus
of n-grams extracted from the Google Books, al-
legedly containing 4% percent of all books ever
printed.
In recent years, the term ‘Culturomics’ seems
to have become an umbrella term for studies en-
gaging, often at an increasing level of complex-
ity, with the seminal, publicly available Google
Books NGram Corpus (Juola, 2013; Twenge et al.,
2012; Acerbi et al., 2013b). Other studies, like
the inspiring contribution by Leetaru (2011) have
independently explored other data sets for simi-
lar purposes, such as the retroactive prediction of
the Arab Spring Revolution using news data. In
</bodyText>
<footnote confidence="0.576554571428571">
1Consult, for instance, the critical report by A. Grafton
on the occasion of a presentation by Michel and Lieberman
Aiden at one of the annual meetings of the American His-
torical Association (https://www.historians.
org/publications-and-directories/
perspectives-on-history/march-2011/
loneliness-and-freedom).
</footnote>
<page confidence="0.982862">
62
</page>
<note confidence="0.9909075">
Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 62–70,
Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999872041666667">
the present paper, we seek to join this recent line
of Culturomics research: we will discuss a series
of quantitative explorations of the Time Magazine
Corpus (1923–2006), a balanced textual data set
covering a good deal of the twentieth (and early
twenty-first) century.
The structure of the paper is as follows: in the
following section 2, we will discuss the data set
used. In section 3, we will introduce some of
the fundamental assumptions underlying the Cul-
turomics approach for Big Data, and report on
an experiment that replicates an earlier sentiment-
related analysis of the Google Books Corpus
(Acerbi et al., 2013b) using our Time data. Subse-
quently, we will apply a Parsimonious Language
Model to our data (section 4) and assess from a
qualitative perspective how and whether this tech-
nique can be used to extract the characteristic vo-
cabulary from specific time periods. To conclude,
we will use these Parsimonious Language Mod-
els in a variability-based neighbor clustering (sec-
tion 5), in an explorative attempt to computation-
ally identify major turning points in the twentieth
century’s history.
</bodyText>
<sectionHeader confidence="0.8751" genericHeader="method">
2 Data: Time Magazine Corpus
</sectionHeader>
<bodyText confidence="0.999991909090909">
For the present research, we have used a collection
of electronic articles harvested from the archive of
the well-known weekly publication Time Maga-
zine. The magazine’s online archive is protected
by international copyright law and it can only be
consulted via a paying subscription.2 Therefore,
the corpus cannot be freely redistributed in any
format. To construct the corpus, we have used
metadata provided by corpus linguist Mark Davies
who has published a searchable interface to the
Time Corpus (Davies, 2013). For the present
paper, we were only dependent on the unique
identification number and publication year which
Davies provides for each article. Users who are in-
terested in downloading (a portion of) the corpus
which we used, can use this metadata to replicate
our findings.
We have used the Stanford CoreNLP Suite to
annotate this collection (with its default settings
for the English language).3 We have tokenized
and lemmatized the corpus with this tool suite.
Additionally, we have applied part-of-speech tag-
</bodyText>
<footnote confidence="0.986265333333333">
2http://content.time.com/time/archive.
3http://nlp.stanford.edu/software/
corenlp.shtml
</footnote>
<table confidence="0.999456818181818">
Period # Documents # Word forms # Unique forms
1920s 24,332 11,155,681 158,443
1930s 32,788 20,622,526 222,777
1940s 41,832 22,547,958 234,918
1950s 42,249 25,638,032 251,658
1960s 35,440 27,355,389 258,276
1970s 27,804 25,449,488 218,322
1980s 25,651 24,185,889 208,678
1990s 23,300 20,637,179 204,393
2000s 17,299 14,151,399 176,515
Overall 270,695 191,743,541 867,399
</table>
<tableCaption confidence="0.722482666666667">
Table 1: General word frequency statistics on the
reconstructed version of the Time Corpus (1923-
2006).
</tableCaption>
<bodyText confidence="0.999395935483871">
ging (Toutanova et al., 2003) and named entity
recognition (Finkel et al., 2005). In the end, our
reconstructed version of the Time Corpus in to-
tal amounted to 270,695 individual articles. In
its entirety, the corpus counted 191,743,541 dis-
tinct word forms (including punctuation marks),
867,399 forms of which proved unique in their
lowercased format. Some general statistics about
our reconstructed version of the Time Corpus are
given in Table 1. In addition to the cumulative
word count statistics about the corpus, we have
included the frequency information per decade
(1920s, 1930s, etc.), as this periodisation will
prove important for the experiments described in
section 4.
In its entirety, the corpus covers the period
March 1923 throughout December 2006. It only
includes articles from the so-called ‘U.S. edition’
of Time (i.e., it does not contain articles which
only featured in the e.g. European edition of the
Magazine). Because of Time’s remarkably contin-
uous publication history, as well as the consider-
able attention the magazine traditionally pays to
international affairs and politics, the Time Cor-
pus can be expected to offer an interesting, al-
beit exclusively American perspective on the re-
cent world history. As far as we know, the cor-
pus has only been used so far in corpus linguistic
publications and we do not know of any advanced
studies in the field of cultural history that make
extensive use of the corpus.
</bodyText>
<sectionHeader confidence="0.972324" genericHeader="method">
3 Assumption: Lexical frequency
</sectionHeader>
<bodyText confidence="0.9993485">
Previous contributions to the field of Culturomics
all have in common that they attempt to establish a
correlation between word frequency statistics and
cultural phenomena. While this is rarely explicitly
voiced, the broader assumption underlying these
studies is that frequency statistics extracted from
</bodyText>
<page confidence="0.999016">
63
</page>
<bodyText confidence="0.999293411764706">
the texts produced by a society at specific mo-
ment in history, will necessarily reflect that soci-
ety’s cultural specific (e.g. cultural) concerns at
that time. As such, it can for instance be expected
that the frequency of conflict-related terminology
will tend to be more elevated in texts produced by
a society at war than one at peace. (Needless to
say, this need not imply that a society e.g. sup-
ports that war, since the same conflict-related ter-
minology will be frequent in texts that oppose a
particular conflict.) Obviously, the resulting as-
sumption is that the study of developments in the
vocabulary of a large body of texts should enable
the study of the evolution of the broader histori-
cal concerns that exist(ed) in the culture in which
these texts were produced.
Frequency has been considered a key measure
in recent studies into cultural influence (Skiena
and Ward, 2013). The more frequent a word
in a corpus, the more weighty the cultural con-
cerns which that word might be related to. A
naive illustration of this frequency effect can be
gleaned from Figure 1. In the subplots of the fig-
ure, we have plotted the absolute frequency with
which the last names of U.S. presidents have been
yearly mentioned throughout the Time Corpus (in
their lowercased form, and only when tagged as
a named entity). The horizontal axis represents
time, with grey zones indicating start and end
dates of the administration periods. The absolute
frequencies have been normalised in each year, by
taking their ratio over the frequency of the defi-
nite article the. Before plotting, these relative fre-
quencies have been mean-normalised. (Readers
are kindly requested to zoom in on the digital PDF
to view more detail for all figures.) Although this
is by no means a life-changing observation, each
presidential reign is indeed clearly characterised
by a significant boost in the frequency of the cor-
responding president’s last name. Nevertheless,
the graph also displays some notable deficiencies,
such the confusion of father and son Bush, or the
increase in frequency right before an administra-
tion period, which seems related to the presidential
election campaigns.
Importantly, it has been stressed that reliable
frequency information can only be extracted from
large enough corpora, in order to escape the bias
caused by limiting oneself to e.g. too restricted a
number of topics or text varieties. This has caused
studies to stress the importance of so-called ‘Big
</bodyText>
<figureCaption confidence="0.88186525">
Figure 1: Diachronic visualisation of mean-
normalised frequencies-of-mention of the last
names of U.S. presidents in the Time corpus, to-
gether with their administration periods.
</figureCaption>
<figure confidence="0.9943406">
Coolidge
Hoover
Roosevelt
Truman
Eisenhower
Kennedy
Johnson
Nixon
Ford
Carter
Reagan
Bush
Clinton
Bush
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
</figure>
<bodyText confidence="0.9997214375">
Data’ when it comes to Culturomics, reviving the
old adagium from the field of Machine Learning
‘There’s no data like more data’, attributed to Mer-
cer. In terms of data size, it is therefore an impor-
tant question whether the Time Corpus is a reli-
able enough resource for practicing Culturomics.
While the Time Corpus (just under 200 million to-
kens) is not a small data set, it is of course orders
of magnitude smaller than the Google Books cor-
pus with its intimidating 361 billion words. As
such, the Time Corpus might hardly qualify as
‘Big Data’ in the eyes of many contemporary data
scientists. One distinct advantage which the Time
Corpus might offer to counter-balance the disad-
vantage of its limited size, is the high quality, both
of the actual text, as well as the metadata (OCR-
errors are for instance extremely rare).
In order to assess whether a smaller, yet higher-
quality corpus like the Time Corpus might yield
valid results when it comes to Culturomics we
have attempted to replicate an interesting experi-
ment reported by Acerbi et al. (2013b) in the con-
text of a paper on the expression of emotions in
twentieth century books. For their research, they
used the publicly available Google Books unigram
corpus. In our Figure 2 we have reproduced their
‘Figure 1: Historical periods of positive and neg-
ative moods’. For this analysis, they used the so-
called LIWC-procedure: a methodology which at-
tempts to measure the presence of particular emo-
tions in texts by calculating the relative occur-
rences of a set of key words (Tausczik and Pen-
</bodyText>
<page confidence="0.996606">
64
</page>
<bodyText confidence="0.99997296">
nebaker, 2010).4 In the authors’ own words, the
graph “shows that moods tracked broad histori-
cal trends, including a ‘sad’ peak corresponding
to Second World War, and two ‘happy’ peaks, one
in the 1920’s and the other in the 1960’s.”
We have exactly re-engineered their methodol-
ogy and applied it to the Time Corpus. The result
of this entirely parallel LIWC-analysis (Tausczik
and Pennebaker, 2010) of the Time Corpus is visu-
alized in Figure 3. While our data of course only
starts in 1923 instead of 1900 (cf. grey area), it
is clear that our experiment has produced a sur-
prisingly similar curve, especially when it comes
to the ‘sad’ and ‘happy’ periods in the 1940s and
1960s respectively. These pronounced similarities
are especially remarkable because, to our knowl-
edge, the Time Corpus is not only much smaller
but also completely unrelated to the Google Books
corpus. This experiment thus serves to emphasise
the remarkable stability of certain cultural trends
as reflected across various text types and unrelated
text corpora.5 Moreover, these results suggest that
the Time Corpus, in spite of limited size, might
still yield interesting and valid results in the con-
text of Culturomics research.
</bodyText>
<sectionHeader confidence="0.981315" genericHeader="method">
4 Parsimonious Language Models
</sectionHeader>
<bodyText confidence="0.974926260869565">
As discussed above, Michel et al. (2011) have
proposed a methodology in their seminal paper,
whereby, broadly speaking, they try to establish
a correlation between historical events and word
counts in corpora. They show, for instance, that
the term ‘Great War’ is only frequent in their data
until the 1940s: at that point the more distinc-
tive terms ‘World War I’ and ‘World War II’ sud-
denly become more frequent. One interesting is-
sue here is that this methodology is characterised
by a modest form a ‘cherry picking’: with this
way of working, a researcher will only try out
word frequency plots of which (s)he expects be-
forehand that they will display interesting trends.
Inevitably, this fairly supervised approach might
lower one’s chance to discover new phenomena,
and thus reduces the chance for scientific serendip-
ity to occur. An equally interesting, yet much less
4We would like to thank Ben Verhoeven for sharing
his LIWC-implementation. The methodology adopted by
Acerbi et al. (2013b) has been detailed in the follow-
ing blog post: http://acerbialberto.wordpress.
com/tag/emotion/.
</bodyText>
<footnote confidence="0.993371">
5Acerbi et al. (2013a) have studied the robustness of their
own experiments recently, using different metrics.
</footnote>
<figureCaption confidence="0.744335">
Figure 2: Figure reproduced from Acerbi et
al. (2013b): ‘Figure 1: Historical periods of posi-
tive and negative moods’. Also see Figure 3.
</figureCaption>
<bodyText confidence="0.999907034482759">
supervised approach might therefore be to auto-
matically identify which terms are characteristic
for a given time span in a corpus.
In this respect, it is interesting to refer to Par-
simonious Language Models (PLMs), a fairly re-
cent addition to the field of Information Retrieval
(Hiemstra et al., 2004). PLMs can be used to cre-
ate a probabilistic model of a text collection, de-
scribing the relevance of words in individual docu-
ments in contrast to all other texts in the collection.
From the point of view of indexing in Information
Retrieval, the question which a PLM in reality tries
to answer is: ‘Suppose that in the future, a user
will be looking for this document, which search
terms is (s)he most likely to use?’ As such, PLMs
offers a powerful alternative to the established TF-
IDF metric, in that they are also able to estimate
which words are most characteristic of a given
document. While PLMs are completely unsuper-
vised (i.e. no manual annotation of documents is
needed), they do require setting the λ parameter
beforehand. The λ parameter will of course have
a major influence on the final results, since it will
control the rate at which the language of each doc-
ument will grow different from that of all other
documents, during the subsequent updates of the
model. (For the mathematical details on λ, con-
sult Hiemstra et al. (2004).) Thus, PLMs can be
expected to single out more characteristic vocab-
</bodyText>
<page confidence="0.998895">
65
</page>
<figureCaption confidence="0.867292">
Figure 3: LIWC-analysis carried on the Time Cor-
</figureCaption>
<bodyText confidence="0.97693353125">
pus (cf. Figure 2), attempting to replicate the
trends found by Acerbi et al. (2013b). Plotted
is the absolute difference between the z-scores
for the LIWC-categories ‘Positive emotions’ and
‘Negative emotions’. The same smoother (‘Fried-
man’s supersmoother’) has been applied (R Core
Team, 2013).
Year
ulary than simpler frequentist approaches and, in-
terestingly, they are more lightweight to run than
e.g. temporal topic models.
In a series of explorative experiments, we have
applied PLMs to the Time Corpus. In particu-
lar, we have build PLMs for this data, by com-
bining individual articles into much larger docu-
ments: both for each year in the data, as well as all
‘decades’ (e.g. 1930 = 1930−1939) we have con-
structed such large, multi-article documents. For
both document types (years and decades), we have
subsequently generated PLMs. In Figure 4 to Fig-
ure 12 we have plotted the results for the PLMs
based on the decade documents (for λ = 0.1).
In the left subpanel, we show the 25 words (tech-
nically, the lowercased lemma’s) which the PLM
estimated to be most discriminative for a given
decade. In the right subpanel, we have plotted the
evolution of the relevance scores for each of these
25 words in the year-based PLM (the grey zone in-
dicates the decade). Higher scores indicate a more
pronounced relevance for a given decade. For the
sake of interpretation, we have restricted our anal-
ysis to words which were tagged as nouns (‘NN’
</bodyText>
<figure confidence="0.964167375">
depression
wife
ed
newshawk
daughter
bank
m.
railroad
gold
friend
employe
author
name
cinema
governor
son
president
picture
a.
fortnight
cent
p.
week
&amp; ‘NNS’).
</figure>
<bodyText confidence="0.999858277777778">
It is not immediately clear how the output of
these PLMs can be evaluated using quantitative
means. A concise qualitative discussion seems
most appropriate to assess the results. For this
reason, we have combined individual articles into
larger decade documents in these experiments,
since this offers a very intuitive manner of ar-
ranging the available sources from a historical,
interpretative point of view. Often, when peo-
ple address the periodisation of the twentieth cen-
tury they will use decades, where terms like e.g.
‘the seventies’ or ‘the twenties’ refer to a fairly
well-delineated concept in people’s minds, associ-
ated with a particular set of political events, peo-
ple and cultural phenomena, etc. By sticking
to this decade-based periodisation, we can verify
fairly easily to what extent the top 25 yielded by
the PLM corresponds to commonplace historical
</bodyText>
<figure confidence="0.9995372">
Joy−Sadness (z−scores)
0.002 0.004 0.006 0.008 0.010
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●●●
●
●
●●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
</figure>
<figureCaption confidence="0.68921">
Figure 4: PLM for the 1920s.
</figureCaption>
<figure confidence="0.997597952380953">
Highest PML scores (19a0s)
a.
automobile
honor
condition
ship
person
governor
statement
speech
interest
foot
lady
railroad
bill
son
name
newspaper
play
man
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
</figure>
<figureCaption confidence="0.773718">
Figure 5: PLM for the 1930s.
</figureCaption>
<figure confidence="0.962871">
Highest PML scores (1930s)
no.
1900 1920 1940 1960 1980 2000 automobile
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
66
</figure>
<figureCaption confidence="0.973926">
Figure 6: PLM for the 1940s. Figure 9: PLM for the 1970s.
</figureCaption>
<figure confidence="0.999595660377358">
Highest PML scores (19x0s) Highest PML scores (1970s)
plant
troops
tank
job
ton
mile
bomb
bomber
officer
front
labor
admiral
enemy
battle
production
jap
radio
soldier
ship
air
day
plane
man
week
war
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
group
people
city
aide
leader
increase
policy
tax
example
woman
rate
court
area
official
black
state
energy
government
inflation
nation
problem
country
oil
price
%
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
</figure>
<figureCaption confidence="0.99428">
Figure 7: PLM for the 1950s. Figure 10: PLM for the 1980s.
</figureCaption>
<figure confidence="0.999429377358491">
Highest PML scores (1950s) Highest PML scores (1980s)
trouble
farm
girl
show
mile
pp
wife
committee
ft.
party
end
army
world
red
boy
record
hand
picture
production
story
tv
day
time
week
man
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
aide
television
group
hostage
problem
force
drug
system
deficit
arm
weapon
program
rate
company
budget
people
firm
policy
missile
leader
computer
government
country
official
%
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
</figure>
<figureCaption confidence="0.992936">
Figure 8: PLM for the 1960s. Figure 11: PLM for the 1990s.
</figureCaption>
<figure confidence="0.981181092592593">
Highest PML scores (1960s) Highest PML scores (1990s)
rights
market
ft.
fact
moon
life
jet
college
area
today
university
girl
world
space
century
art
state
negro
church
city
school
nation
student
p.m.
man
sex
network
phone
group
thing
media
decade
guy
show
lot
director
film
way
tv
issue
parent
computer
woman
drug
movie
family
kid
no
child
people
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
</figure>
<page confidence="0.958008">
67
</page>
<figureCaption confidence="0.700692">
Figure 12: PLM for the 2000s.
</figureCaption>
<table confidence="0.998810555555556">
Highest PML scores (2000s)
group
terrorism
website
source
way
intelligence
official
study
attack
technology
risk
internet
site
phone
cell
family
thing
guy
drug
movie
lot
parent
company
people
kid
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
</table>
<bodyText confidence="0.999883452380952">
stereotypes about the decades in the twentieth cen-
tury.
Let us start by inspecting the top 25 for the
1940s, a decade in which the Second War II natu-
rally played a major role. Already at first glance,
it is clear that the top 25 is dominated by war-
related terminology (war, soldier, enemy, ... ). In-
terestingly, the list also contains words referring
to WWII, but not from the politically correct jar-
gon which we would nowadays use to address the
issue (e.g. jap). Remarkable is the pronounced
position of aviary vocabulary (bomber, air, plane,
... ), which is perhaps less surprising if we con-
sider the fact that WWI was one of the first inter-
national conflicts in which aircrafts played a major
military role.
Interestingly, the 1920s are hardly characterised
by an equally focused set of relevant words. Al-
though mobility does seem to play an important
role (cf. the recently invented automobile, but
also ship and railroad), a number of less mean-
ingful abbreviations (such as p. for ‘page’) pop
up that seem connected to superficial changes in
Time’s editorial policies, rather than cultural de-
velopments. (Future analyses might want to re-
move such words manually.) On the other hand,
the use of the terms lady and honour might be
rooted in a cultural climate that is different from
ours (‘lady’ seems the equivalent of woman to-
day). A number of parallel observations can be
made for the 1930s, although here, the high rank-
ing word depression is of course striking (cf. the
economic crisis of 1929). Fascinatingly, a variety
of denominations for (popular) media play a ma-
jor role throughout the decade PLMs. Note, that
while the 1920s’ top 25 mentioned the radio as the
primary communication medium, the popular cin-
ema and (moving?) picture show up in the 1930s.
Interestingly, the popular media of tv and record
make their appearance in the 1950s. (In the 1980s
and 1990s top 25, television moreover continues
to show up.)
The PLM also seems to offer an excellent cul-
tural characterisation of the 1960s and the associ-
ated baby boom, with an emphasis on the contro-
versies of the time, debate involving human rights
(rights, negro, nation), and in particular educa-
tional (college, university, school). The use of
‘educational’ words might well be related to the
social unrest, much of which took place in and
around universities. Does the striking presence of
the word ‘today’ in the list reveal an elevated hic et
nunc mentality in the contemporary States? Amer-
ica’s well-documented interest in space traveling
at the time is also appropriately reflected (space,
moon). Perhaps unexpectedly, this seemingly op-
timistic ‘Zeitgeist’ is more strongly associated in
the Time Corpus with the sixties, than with the
seventies: in the flower-power era, Time displays a
remarkable focus on political and especially eco-
nomic issues. Rather, the oil crisis seems to domi-
nate Time’s lexis in the seventies.
In the 1990s and 2000s, we can observe a fo-
cus on what one might unrespectfully call ‘first-
world problems’, involving for instance family re-
lations (family, kid, parent, child, etc.). Apart
from the fact that Time’s vocabulary seems to grow
more colloquial in general in this period (at least
in our eyes, e.g. guy, lot, thing), a number of
controversial taboo subjects seem to have become
discussable: sex, drug. ‘Terrorism’ and ‘intelli-
gence’ seem to have become major concerns in
post-09/11 America, and perhaps the presence of
the word ‘attack’ and ‘technology’ might be (par-
tially) interpreted in the same light. Again, we
see how vocabulary related to media absolutely
dominates the final rankings in the corpus: Hol-
lywood seems to have enjoyed an increasing pop-
ularity (film, director, movie, ...) but it is in-
formation technology that seems to have had the
biggest cultural impact: mobile communication
devices (phone, cell) and Internet-related termi-
nology (network, computer, internet, ...) seem to
have caused a major turning point in Time’s lexis.6
</bodyText>
<footnote confidence="0.984043">
6Due to lack of space, we only report results for λ = 0.1
applied to nouns, but highly similar results could be obtained
</footnote>
<page confidence="0.998847">
68
</page>
<sectionHeader confidence="0.995694" genericHeader="method">
5 Twentieth Century Turning Points?
</sectionHeader>
<bodyText confidence="0.990286760563381">
An interesting technique in this respect is a clus-
tering method called VNC or ‘Variability-Based
Neighbor Clustering’ (Gries and Hilpert, 2008).
The technique has been introduced in the field of
historical linguistics as an aid in the automated
identification of temporal stages in diachronic
data. The method will apply a fairly straightfor-
ward clustering algorithm to a data set (with e.g.
Ward linkage applied to a Cosine distance matrix)
but, importantly, it will add the connectivity con-
straint that (clusters of) data points can only merge
with each other at the next level in a dendrogram,
if they are immediately adjacent. That is to say
that e.g. in a series of yearly observations 1943
would be allowed to merge with 1942 and 1944,
but not with 1928 (even if 1943 would be much
more similar to 1928 than to 1943). We have ap-
plied VNC (with Ward linkage applied to a plain
Cosine distance table) to a series of vectors which
for each year in our data (1923-2006) contained
the PML scores of 5,000 words deemed most rel-
evant for that year by the model.
The dendrogram resulting from the VNC pro-
cedure is visualised in Figure 13. The early his-
tory of Time Magazine (1923-1927) does not re-
ally seem to fit in with the rest and takes up a fairly
deviant position. However, the most attention-
grabbing feature of this tree structure is the major
divide which the dendrogram suggests (cf. red vs.
green-blue cluster) between the years before and
after 1945, the end of the Second World War. An-
other significant rupture seem to be present before
and after 1996: the discussion leaves us to won-
der whether this turning point might related to the
recent introduction of new communication tech-
nologies, in particular the rise of the Internet.
Historically speaking, these turning points do
not come as a surprise. There is, for instance,
widespread acceptance among historians WWII
has indeed been the single most influential event
in the twentieth century. What does surprise, how-
ever, is the relative easy with which a completely
unsupervised procedure has managed to suggest
using other part-of-speech categories and settings for λ. An
interesting effect was associated with ‘fiddling the knob’ of
this last parameter: for lower values (0.01, 0.001 etc.), the
model would come up with perhaps increasingly characteris-
tic, but also increasingly obscure and much less frequent vo-
cabulary. For the fourties, for instance, instead of returning
the word ‘bomber’ the analysis would return the exact name
of a particular bomber type which was used at the time. This
parameter setting deserves further exploration.
Figure 13: Dendrogram resulting from apply-
ing Variability-Based Neighbor Analysis to vec-
tors which contain for each year the 5,000 words
deemed most relevant by the PML.
Variability-Based Neighbor Clustering Dendrogram
(5000 highest PML scores per year)
this identification. Arguably, this is where we
leave the realm of the obvious when it comes to
the computational study of cultural history. The
identification of major events and turning points in
human history is normally a task which requires a
good deal of formal education and some advanced
reasoning skills. Here, we might be nearing a
modest form of Artificial Intelligence when we ap-
ply computational methods to achieve a fairly sim-
ilar goal. Hopefully, these analyses, as well as the
ones reported above, illustrate the huge potential
of computational methods in the study of cultural
history, even if only as a discovery tool.
</bodyText>
<sectionHeader confidence="0.985936" genericHeader="conclusions">
6 Conclusion and criticism
</sectionHeader>
<bodyText confidence="0.99941594117647">
In this paper we have discussed a series of analy-
ses that claim to mine a data-driven cultural char-
acterization of the ‘Zeitgeist’ of some of the main
periods in the twentieth century. Nevertheless, we
must remain vigilant not to overstate the achieve-
ment of these techniques: it remains to be deter-
mined to which extent can we truly call these ap-
plications Digital History and whether these anal-
yses have taught us anything which we did not
know before. Because the twentieth century is
so well known to most of us, the evidence often
tends to be self-referential and self-explanatory,
and merely confirms that which we already knew
intuitively. Like with most distant reading ap-
proaches, the results urge us to go back to the orig-
inal material for the close reading of individual
sources in their historical context, in order to ver-
</bodyText>
<figure confidence="0.9965242">
0.020
0.015
0.010
0.005
0.000
</figure>
<page confidence="0.995111">
69
</page>
<bodyText confidence="0.999961894736842">
ify the macro-hypotheses that might be suggested
at a higher level. Therefore, the proposed method
might in fact be more suitable for the study of time
periods and corpora of which we know less.
Nevertheless, our methodology seems promis-
ing for future applications in Digital History: our
naive periodisation in decades, for instance, might
be hugely fine-tuned by processing the results of
a VNC-dendrogram. Breaking up history into
meaningful units is a much more complex, and of-
ten controversial matter (e.g. ‘When does moder-
nity start?’). In this light, it would be helpful to
have at our disposal unbiased, computational tools
that might help us to identify cultural ruptures or
even turning points in history. Our results reported
in the final section do show that this application
yields interesting results, and again, the method
seems promising for the analysis of lesser known
corpora.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999965666666666">
The authors would like to thank the anonymous re-
viewers and Kalliopi Zervanou for their valuable
feedback on earlier drafts of this paper, as well as
Walter Daelemans and Antal van den Bosch for
the inspiring discussions on the topic. For this
study, Mike Kestemont was funded as a postdoc-
toral research fellow for the Research Foundation
of Flanders (FWO). Folgert Karsdorp was sup-
ported as a Ph.D. candidate by the Computational
Humanities Programme of the Royal Netherlands
Academy of Arts and Sciences, as part of the
Tunes &amp; Tales project.
</bodyText>
<sectionHeader confidence="0.999279" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99989361904762">
Alberto Acerbi, Vasileios Lampos, and Alexander R.
Bentley. 2013a. Robustness of emotion extraction
from 20th century English books. In BigData ’13.
IEEE, IEEE.
Alberto Acerbi, Vasileios Lampos, Philip Garnett, and
Alexander R. Bentley. 2013b. The Expression
of Emotions in 20th Century Books. PLoS ONE,
8(3):e59030.
Mark Davies. 2013. TIME Magazine Corpus: 100
million words, 1920s-2000s.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ’05, pages 363–370, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Stefan Th. Gries and Martin Hilpert. 2008. The iden-
tification of stages in diachronic data: variability-
based neighbour clustering. Corpora, 3(1):59–81.
Djoerd Hiemstra, Stephen E. Robertson, and Hugo
Zaragoza. 2004. Parsimonious language models for
information retrieval. In Mark Sanderson, Kalervo
Jrvelin, James Allan, and Peter Bruza, editors, SI-
GIR, pages 178–185. ACM.
Patrick Juola. 2013. Using the Google N-Gram corpus
to measure cultural complexity. Literary and Lin-
guistic Computing, 28(4):668–675.
Kalev H. Leetaru. 2011. Culturomics 2.0: Forecasting
large-scale human behavior using global news media
tone in time and space. First Monday, 16(9).
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, The
Google Books Team, Joseph P. Pickett, Dale
Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant,
Steven Pinker, Martin A. Nowak, and Erez Lieber-
man Aiden. 2011. Quantitative Analysis of Cul-
ture Using Millions of Digitized Books. Science,
331(6014):176–182.
R Core Team, 2013. R: A Language and Environment
for Statistical Computing. R Foundation for Statis-
tical Computing, Vienna, Austria.
Steve Skiena and Charles Ward. 2013. Who Belongs
in Bonnie’s Textbook? Cambridge University Press.
Yla R. Tausczik and James W. Pennebaker. 2010. The
Psychological Meaning of Words: LIWC and Com-
puterized Text Analysis Methods. Journal of Lan-
guage and Social Psychology, 29(1):24–54.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich Part-of-
speech Tagging with a Cyclic Dependency Network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ’03, pages 173–180, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jean M. Twenge, Keith W. Campbell, and Brittany
Gentile. 2012. Increases in Individualistic Words
and Phrases in American Books, 19602008. PLoS
ONE, 7(7):e40181.
Gerben Zaagsma. 2013. On Digital History. BMGN –
Low Countries Historical Review, 128(4):3–29.
</reference>
<page confidence="0.99845">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.016105">
<title confidence="0.99967">Mining the Twentieth Century’s History from the Time Magazine Corpus</title>
<author confidence="0.993667">Mike</author>
<affiliation confidence="0.641279">University of Prinsstraat 13,</affiliation>
<address confidence="0.320361">B-2000,</address>
<email confidence="0.747387">@uantwerpen.be</email>
<title confidence="0.518755666666667">Folgert Meertens Postbus 1090 GG The @meertens.knaw.nl</title>
<author confidence="0.98945">Marten</author>
<affiliation confidence="0.999713">University of</affiliation>
<address confidence="0.993686">551 Hamilton</address>
<note confidence="0.643955666666667">CB 3195, Chapel North Carolina United</note>
<email confidence="0.884432">marten@live.unc.edu</email>
<abstract confidence="0.999199875">In this paper we report on an explorative study of the history of the twentieth century from a lexical point of view. As data, we use a diachronic collection of 270,000+ English-language articles harvested from the electronic archive of the Magazine We attempt to automatically identify significant shifts in the vocabulary used in this corpus using efficient, yet unsupervised computational methods, such as Parsimonious Language Models. We offer a qualitative interpretation of the outcome of our experiments in the light of momentous events in the twentieth century, such as the Second World War or the rise of the Internet. This paper follows up on a recent string of frequentist approaches to studying cultural history (‘Culturomics’), in which the evolution of human culture is studied from a quantitative perspective, on the basis of lexical statistics extracted from large, textual data sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alberto Acerbi</author>
<author>Vasileios Lampos</author>
<author>Alexander R Bentley</author>
</authors>
<title>Robustness of emotion extraction from 20th century English books.</title>
<date>2013</date>
<booktitle>In BigData ’13.</booktitle>
<publisher>IEEE, IEEE.</publisher>
<contexts>
<context position="3604" citStr="Acerbi et al., 2013" startWordPosition="546" endWordPosition="549">invited a lot of attention in popular media – has not gone uncriticized in the international community of Humanities.1 In this paper, the authors show how a number of major historical events show interesting correlations with word counts in a vast corpus of n-grams extracted from the Google Books, allegedly containing 4% percent of all books ever printed. In recent years, the term ‘Culturomics’ seems to have become an umbrella term for studies engaging, often at an increasing level of complexity, with the seminal, publicly available Google Books NGram Corpus (Juola, 2013; Twenge et al., 2012; Acerbi et al., 2013b). Other studies, like the inspiring contribution by Leetaru (2011) have independently explored other data sets for similar purposes, such as the retroactive prediction of the Arab Spring Revolution using news data. In 1Consult, for instance, the critical report by A. Grafton on the occasion of a presentation by Michel and Lieberman Aiden at one of the annual meetings of the American Historical Association (https://www.historians. org/publications-and-directories/ perspectives-on-history/march-2011/ loneliness-and-freedom). 62 Proceedings of the 8th Workshop on Language Technology for Cultura</context>
<context position="4990" citStr="Acerbi et al., 2013" startWordPosition="750" endWordPosition="753">present paper, we seek to join this recent line of Culturomics research: we will discuss a series of quantitative explorations of the Time Magazine Corpus (1923–2006), a balanced textual data set covering a good deal of the twentieth (and early twenty-first) century. The structure of the paper is as follows: in the following section 2, we will discuss the data set used. In section 3, we will introduce some of the fundamental assumptions underlying the Culturomics approach for Big Data, and report on an experiment that replicates an earlier sentimentrelated analysis of the Google Books Corpus (Acerbi et al., 2013b) using our Time data. Subsequently, we will apply a Parsimonious Language Model to our data (section 4) and assess from a qualitative perspective how and whether this technique can be used to extract the characteristic vocabulary from specific time periods. To conclude, we will use these Parsimonious Language Models in a variability-based neighbor clustering (section 5), in an explorative attempt to computationally identify major turning points in the twentieth century’s history. 2 Data: Time Magazine Corpus For the present research, we have used a collection of electronic articles harvested</context>
<context position="12723" citStr="Acerbi et al. (2013" startWordPosition="1981" endWordPosition="1984">ooks corpus with its intimidating 361 billion words. As such, the Time Corpus might hardly qualify as ‘Big Data’ in the eyes of many contemporary data scientists. One distinct advantage which the Time Corpus might offer to counter-balance the disadvantage of its limited size, is the high quality, both of the actual text, as well as the metadata (OCRerrors are for instance extremely rare). In order to assess whether a smaller, yet higherquality corpus like the Time Corpus might yield valid results when it comes to Culturomics we have attempted to replicate an interesting experiment reported by Acerbi et al. (2013b) in the context of a paper on the expression of emotions in twentieth century books. For their research, they used the publicly available Google Books unigram corpus. In our Figure 2 we have reproduced their ‘Figure 1: Historical periods of positive and negative moods’. For this analysis, they used the socalled LIWC-procedure: a methodology which attempts to measure the presence of particular emotions in texts by calculating the relative occurrences of a set of key words (Tausczik and Pen64 nebaker, 2010).4 In the authors’ own words, the graph “shows that moods tracked broad historical trend</context>
<context position="15452" citStr="Acerbi et al. (2013" startWordPosition="2431" endWordPosition="2434">ar II’ suddenly become more frequent. One interesting issue here is that this methodology is characterised by a modest form a ‘cherry picking’: with this way of working, a researcher will only try out word frequency plots of which (s)he expects beforehand that they will display interesting trends. Inevitably, this fairly supervised approach might lower one’s chance to discover new phenomena, and thus reduces the chance for scientific serendipity to occur. An equally interesting, yet much less 4We would like to thank Ben Verhoeven for sharing his LIWC-implementation. The methodology adopted by Acerbi et al. (2013b) has been detailed in the following blog post: http://acerbialberto.wordpress. com/tag/emotion/. 5Acerbi et al. (2013a) have studied the robustness of their own experiments recently, using different metrics. Figure 2: Figure reproduced from Acerbi et al. (2013b): ‘Figure 1: Historical periods of positive and negative moods’. Also see Figure 3. supervised approach might therefore be to automatically identify which terms are characteristic for a given time span in a corpus. In this respect, it is interesting to refer to Parsimonious Language Models (PLMs), a fairly recent addition to the field</context>
<context position="17347" citStr="Acerbi et al. (2013" startWordPosition="2750" endWordPosition="2753">ed (i.e. no manual annotation of documents is needed), they do require setting the λ parameter beforehand. The λ parameter will of course have a major influence on the final results, since it will control the rate at which the language of each document will grow different from that of all other documents, during the subsequent updates of the model. (For the mathematical details on λ, consult Hiemstra et al. (2004).) Thus, PLMs can be expected to single out more characteristic vocab65 Figure 3: LIWC-analysis carried on the Time Corpus (cf. Figure 2), attempting to replicate the trends found by Acerbi et al. (2013b). Plotted is the absolute difference between the z-scores for the LIWC-categories ‘Positive emotions’ and ‘Negative emotions’. The same smoother (‘Friedman’s supersmoother’) has been applied (R Core Team, 2013). Year ulary than simpler frequentist approaches and, interestingly, they are more lightweight to run than e.g. temporal topic models. In a series of explorative experiments, we have applied PLMs to the Time Corpus. In particular, we have build PLMs for this data, by combining individual articles into much larger documents: both for each year in the data, as well as all ‘decades’ (e.g.</context>
</contexts>
<marker>Acerbi, Lampos, Bentley, 2013</marker>
<rawString>Alberto Acerbi, Vasileios Lampos, and Alexander R. Bentley. 2013a. Robustness of emotion extraction from 20th century English books. In BigData ’13. IEEE, IEEE.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alberto Acerbi</author>
<author>Vasileios Lampos</author>
<author>Philip Garnett</author>
<author>Alexander R Bentley</author>
</authors>
<booktitle>2013b. The Expression of Emotions in 20th Century Books. PLoS ONE,</booktitle>
<pages>8--3</pages>
<marker>Acerbi, Lampos, Garnett, Bentley, </marker>
<rawString>Alberto Acerbi, Vasileios Lampos, Philip Garnett, and Alexander R. Bentley. 2013b. The Expression of Emotions in 20th Century Books. PLoS ONE, 8(3):e59030.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Davies</author>
</authors>
<date>2013</date>
<journal>TIME Magazine Corpus:</journal>
<volume>100</volume>
<note>million words, 1920s-2000s.</note>
<contexts>
<context position="6020" citStr="Davies, 2013" startWordPosition="914" endWordPosition="915">lly identify major turning points in the twentieth century’s history. 2 Data: Time Magazine Corpus For the present research, we have used a collection of electronic articles harvested from the archive of the well-known weekly publication Time Magazine. The magazine’s online archive is protected by international copyright law and it can only be consulted via a paying subscription.2 Therefore, the corpus cannot be freely redistributed in any format. To construct the corpus, we have used metadata provided by corpus linguist Mark Davies who has published a searchable interface to the Time Corpus (Davies, 2013). For the present paper, we were only dependent on the unique identification number and publication year which Davies provides for each article. Users who are interested in downloading (a portion of) the corpus which we used, can use this metadata to replicate our findings. We have used the Stanford CoreNLP Suite to annotate this collection (with its default settings for the English language).3 We have tokenized and lemmatized the corpus with this tool suite. Additionally, we have applied part-of-speech tag2http://content.time.com/time/archive. 3http://nlp.stanford.edu/software/ corenlp.shtml </context>
</contexts>
<marker>Davies, 2013</marker>
<rawString>Mark Davies. 2013. TIME Magazine Corpus: 100 million words, 1920s-2000s.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7176" citStr="Finkel et al., 2005" startWordPosition="1074" endWordPosition="1077">/time/archive. 3http://nlp.stanford.edu/software/ corenlp.shtml Period # Documents # Word forms # Unique forms 1920s 24,332 11,155,681 158,443 1930s 32,788 20,622,526 222,777 1940s 41,832 22,547,958 234,918 1950s 42,249 25,638,032 251,658 1960s 35,440 27,355,389 258,276 1970s 27,804 25,449,488 218,322 1980s 25,651 24,185,889 208,678 1990s 23,300 20,637,179 204,393 2000s 17,299 14,151,399 176,515 Overall 270,695 191,743,541 867,399 Table 1: General word frequency statistics on the reconstructed version of the Time Corpus (1923- 2006). ging (Toutanova et al., 2003) and named entity recognition (Finkel et al., 2005). In the end, our reconstructed version of the Time Corpus in total amounted to 270,695 individual articles. In its entirety, the corpus counted 191,743,541 distinct word forms (including punctuation marks), 867,399 forms of which proved unique in their lowercased format. Some general statistics about our reconstructed version of the Time Corpus are given in Table 1. In addition to the cumulative word count statistics about the corpus, we have included the frequency information per decade (1920s, 1930s, etc.), as this periodisation will prove important for the experiments described in section </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 363–370, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gries</author>
<author>Martin Hilpert</author>
</authors>
<title>The identification of stages in diachronic data: variabilitybased neighbour clustering.</title>
<date>2008</date>
<journal>Corpora,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="26523" citStr="Gries and Hilpert, 2008" startWordPosition="4329" endWordPosition="4332">ve enjoyed an increasing popularity (film, director, movie, ...) but it is information technology that seems to have had the biggest cultural impact: mobile communication devices (phone, cell) and Internet-related terminology (network, computer, internet, ...) seem to have caused a major turning point in Time’s lexis.6 6Due to lack of space, we only report results for λ = 0.1 applied to nouns, but highly similar results could be obtained 68 5 Twentieth Century Turning Points? An interesting technique in this respect is a clustering method called VNC or ‘Variability-Based Neighbor Clustering’ (Gries and Hilpert, 2008). The technique has been introduced in the field of historical linguistics as an aid in the automated identification of temporal stages in diachronic data. The method will apply a fairly straightforward clustering algorithm to a data set (with e.g. Ward linkage applied to a Cosine distance matrix) but, importantly, it will add the connectivity constraint that (clusters of) data points can only merge with each other at the next level in a dendrogram, if they are immediately adjacent. That is to say that e.g. in a series of yearly observations 1943 would be allowed to merge with 1942 and 1944, b</context>
</contexts>
<marker>Gries, Hilpert, 2008</marker>
<rawString>Stefan Th. Gries and Martin Hilpert. 2008. The identification of stages in diachronic data: variabilitybased neighbour clustering. Corpora, 3(1):59–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djoerd Hiemstra</author>
<author>Stephen E Robertson</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Parsimonious language models for information retrieval.</title>
<date>2004</date>
<pages>178--185</pages>
<editor>In Mark Sanderson, Kalervo Jrvelin, James Allan, and Peter Bruza, editors, SIGIR,</editor>
<publisher>ACM.</publisher>
<contexts>
<context position="16101" citStr="Hiemstra et al., 2004" startWordPosition="2531" endWordPosition="2534"> following blog post: http://acerbialberto.wordpress. com/tag/emotion/. 5Acerbi et al. (2013a) have studied the robustness of their own experiments recently, using different metrics. Figure 2: Figure reproduced from Acerbi et al. (2013b): ‘Figure 1: Historical periods of positive and negative moods’. Also see Figure 3. supervised approach might therefore be to automatically identify which terms are characteristic for a given time span in a corpus. In this respect, it is interesting to refer to Parsimonious Language Models (PLMs), a fairly recent addition to the field of Information Retrieval (Hiemstra et al., 2004). PLMs can be used to create a probabilistic model of a text collection, describing the relevance of words in individual documents in contrast to all other texts in the collection. From the point of view of indexing in Information Retrieval, the question which a PLM in reality tries to answer is: ‘Suppose that in the future, a user will be looking for this document, which search terms is (s)he most likely to use?’ As such, PLMs offers a powerful alternative to the established TFIDF metric, in that they are also able to estimate which words are most characteristic of a given document. While PLM</context>
</contexts>
<marker>Hiemstra, Robertson, Zaragoza, 2004</marker>
<rawString>Djoerd Hiemstra, Stephen E. Robertson, and Hugo Zaragoza. 2004. Parsimonious language models for information retrieval. In Mark Sanderson, Kalervo Jrvelin, James Allan, and Peter Bruza, editors, SIGIR, pages 178–185. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Juola</author>
</authors>
<title>Using the Google N-Gram corpus to measure cultural complexity.</title>
<date>2013</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>28--4</pages>
<contexts>
<context position="3562" citStr="Juola, 2013" startWordPosition="540" endWordPosition="541"> al. (2011), which – while it has invited a lot of attention in popular media – has not gone uncriticized in the international community of Humanities.1 In this paper, the authors show how a number of major historical events show interesting correlations with word counts in a vast corpus of n-grams extracted from the Google Books, allegedly containing 4% percent of all books ever printed. In recent years, the term ‘Culturomics’ seems to have become an umbrella term for studies engaging, often at an increasing level of complexity, with the seminal, publicly available Google Books NGram Corpus (Juola, 2013; Twenge et al., 2012; Acerbi et al., 2013b). Other studies, like the inspiring contribution by Leetaru (2011) have independently explored other data sets for similar purposes, such as the retroactive prediction of the Arab Spring Revolution using news data. In 1Consult, for instance, the critical report by A. Grafton on the occasion of a presentation by Michel and Lieberman Aiden at one of the annual meetings of the American Historical Association (https://www.historians. org/publications-and-directories/ perspectives-on-history/march-2011/ loneliness-and-freedom). 62 Proceedings of the 8th W</context>
</contexts>
<marker>Juola, 2013</marker>
<rawString>Patrick Juola. 2013. Using the Google N-Gram corpus to measure cultural complexity. Literary and Linguistic Computing, 28(4):668–675.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalev H Leetaru</author>
</authors>
<title>Culturomics 2.0: Forecasting large-scale human behavior using global news media tone in time and space.</title>
<date>2011</date>
<journal>First Monday,</journal>
<volume>16</volume>
<issue>9</issue>
<contexts>
<context position="3672" citStr="Leetaru (2011)" startWordPosition="557" endWordPosition="558">n the international community of Humanities.1 In this paper, the authors show how a number of major historical events show interesting correlations with word counts in a vast corpus of n-grams extracted from the Google Books, allegedly containing 4% percent of all books ever printed. In recent years, the term ‘Culturomics’ seems to have become an umbrella term for studies engaging, often at an increasing level of complexity, with the seminal, publicly available Google Books NGram Corpus (Juola, 2013; Twenge et al., 2012; Acerbi et al., 2013b). Other studies, like the inspiring contribution by Leetaru (2011) have independently explored other data sets for similar purposes, such as the retroactive prediction of the Arab Spring Revolution using news data. In 1Consult, for instance, the critical report by A. Grafton on the occasion of a presentation by Michel and Lieberman Aiden at one of the annual meetings of the American Historical Association (https://www.historians. org/publications-and-directories/ perspectives-on-history/march-2011/ loneliness-and-freedom). 62 Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pa</context>
</contexts>
<marker>Leetaru, 2011</marker>
<rawString>Kalev H. Leetaru. 2011. Culturomics 2.0: Forecasting large-scale human behavior using global news media tone in time and space. First Monday, 16(9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Baptiste Michel</author>
<author>Yuan Kui Shen</author>
<author>Aviva Presser Aiden</author>
<author>Adrian Veres</author>
<author>Matthew K Gray</author>
</authors>
<title>The Google Books Team,</title>
<date>2011</date>
<journal>Joseph</journal>
<volume>331</volume>
<issue>6014</issue>
<contexts>
<context position="2962" citStr="Michel et al. (2011)" startWordPosition="436" endWordPosition="439">y cannot be exhaustively surveyed here due to space limits, it is nevertheless interesting to refer to a recent string of frequentist lexical approaches to the study of human history, and the evolution of human culture in particular: ‘Culturomics’. This line of computational, typically dataintensive research seeks to study various aspects of human history, by researching the ways in which (predominantly cultural) phenomena are reflected in, for instance, word frequency statistics extracted from large textual data sets. The field has been initiated in a lively, yet controversial publication by Michel et al. (2011), which – while it has invited a lot of attention in popular media – has not gone uncriticized in the international community of Humanities.1 In this paper, the authors show how a number of major historical events show interesting correlations with word counts in a vast corpus of n-grams extracted from the Google Books, allegedly containing 4% percent of all books ever printed. In recent years, the term ‘Culturomics’ seems to have become an umbrella term for studies engaging, often at an increasing level of complexity, with the seminal, publicly available Google Books NGram Corpus (Juola, 2013</context>
<context position="14497" citStr="Michel et al. (2011)" startWordPosition="2275" endWordPosition="2278"> in the 1940s and 1960s respectively. These pronounced similarities are especially remarkable because, to our knowledge, the Time Corpus is not only much smaller but also completely unrelated to the Google Books corpus. This experiment thus serves to emphasise the remarkable stability of certain cultural trends as reflected across various text types and unrelated text corpora.5 Moreover, these results suggest that the Time Corpus, in spite of limited size, might still yield interesting and valid results in the context of Culturomics research. 4 Parsimonious Language Models As discussed above, Michel et al. (2011) have proposed a methodology in their seminal paper, whereby, broadly speaking, they try to establish a correlation between historical events and word counts in corpora. They show, for instance, that the term ‘Great War’ is only frequent in their data until the 1940s: at that point the more distinctive terms ‘World War I’ and ‘World War II’ suddenly become more frequent. One interesting issue here is that this methodology is characterised by a modest form a ‘cherry picking’: with this way of working, a researcher will only try out word frequency plots of which (s)he expects beforehand that the</context>
</contexts>
<marker>Michel, Shen, Aiden, Veres, Gray, 2011</marker>
<rawString>Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres, Matthew K. Gray, The Google Books Team, Joseph P. Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker, Martin A. Nowak, and Erez Lieberman Aiden. 2011. Quantitative Analysis of Culture Using Millions of Digitized Books. Science, 331(6014):176–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Core Team</author>
</authors>
<title>R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing,</title>
<date>2013</date>
<location>Vienna, Austria.</location>
<contexts>
<context position="17559" citStr="Team, 2013" startWordPosition="2781" endWordPosition="2782">ch the language of each document will grow different from that of all other documents, during the subsequent updates of the model. (For the mathematical details on λ, consult Hiemstra et al. (2004).) Thus, PLMs can be expected to single out more characteristic vocab65 Figure 3: LIWC-analysis carried on the Time Corpus (cf. Figure 2), attempting to replicate the trends found by Acerbi et al. (2013b). Plotted is the absolute difference between the z-scores for the LIWC-categories ‘Positive emotions’ and ‘Negative emotions’. The same smoother (‘Friedman’s supersmoother’) has been applied (R Core Team, 2013). Year ulary than simpler frequentist approaches and, interestingly, they are more lightweight to run than e.g. temporal topic models. In a series of explorative experiments, we have applied PLMs to the Time Corpus. In particular, we have build PLMs for this data, by combining individual articles into much larger documents: both for each year in the data, as well as all ‘decades’ (e.g. 1930 = 1930−1939) we have constructed such large, multi-article documents. For both document types (years and decades), we have subsequently generated PLMs. In Figure 4 to Figure 12 we have plotted the results f</context>
</contexts>
<marker>Team, 2013</marker>
<rawString>R Core Team, 2013. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Skiena</author>
<author>Charles Ward</author>
</authors>
<title>Who Belongs in Bonnie’s Textbook?</title>
<date>2013</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="9760" citStr="Skiena and Ward, 2013" startWordPosition="1492" endWordPosition="1495"> be more elevated in texts produced by a society at war than one at peace. (Needless to say, this need not imply that a society e.g. supports that war, since the same conflict-related terminology will be frequent in texts that oppose a particular conflict.) Obviously, the resulting assumption is that the study of developments in the vocabulary of a large body of texts should enable the study of the evolution of the broader historical concerns that exist(ed) in the culture in which these texts were produced. Frequency has been considered a key measure in recent studies into cultural influence (Skiena and Ward, 2013). The more frequent a word in a corpus, the more weighty the cultural concerns which that word might be related to. A naive illustration of this frequency effect can be gleaned from Figure 1. In the subplots of the figure, we have plotted the absolute frequency with which the last names of U.S. presidents have been yearly mentioned throughout the Time Corpus (in their lowercased form, and only when tagged as a named entity). The horizontal axis represents time, with grey zones indicating start and end dates of the administration periods. The absolute frequencies have been normalised in each ye</context>
</contexts>
<marker>Skiena, Ward, 2013</marker>
<rawString>Steve Skiena and Charles Ward. 2013. Who Belongs in Bonnie’s Textbook? Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yla R Tausczik</author>
<author>James W Pennebaker</author>
</authors>
<title>The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods.</title>
<date>2010</date>
<journal>Journal of Language and Social Psychology,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="13620" citStr="Tausczik and Pennebaker, 2010" startWordPosition="2132" endWordPosition="2135">ods’. For this analysis, they used the socalled LIWC-procedure: a methodology which attempts to measure the presence of particular emotions in texts by calculating the relative occurrences of a set of key words (Tausczik and Pen64 nebaker, 2010).4 In the authors’ own words, the graph “shows that moods tracked broad historical trends, including a ‘sad’ peak corresponding to Second World War, and two ‘happy’ peaks, one in the 1920’s and the other in the 1960’s.” We have exactly re-engineered their methodology and applied it to the Time Corpus. The result of this entirely parallel LIWC-analysis (Tausczik and Pennebaker, 2010) of the Time Corpus is visualized in Figure 3. While our data of course only starts in 1923 instead of 1900 (cf. grey area), it is clear that our experiment has produced a surprisingly similar curve, especially when it comes to the ‘sad’ and ‘happy’ periods in the 1940s and 1960s respectively. These pronounced similarities are especially remarkable because, to our knowledge, the Time Corpus is not only much smaller but also completely unrelated to the Google Books corpus. This experiment thus serves to emphasise the remarkable stability of certain cultural trends as reflected across various te</context>
</contexts>
<marker>Tausczik, Pennebaker, 2010</marker>
<rawString>Yla R. Tausczik and James W. Pennebaker. 2010. The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods. Journal of Language and Social Psychology, 29(1):24–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich Part-ofspeech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7125" citStr="Toutanova et al., 2003" startWordPosition="1066" endWordPosition="1069">ave applied part-of-speech tag2http://content.time.com/time/archive. 3http://nlp.stanford.edu/software/ corenlp.shtml Period # Documents # Word forms # Unique forms 1920s 24,332 11,155,681 158,443 1930s 32,788 20,622,526 222,777 1940s 41,832 22,547,958 234,918 1950s 42,249 25,638,032 251,658 1960s 35,440 27,355,389 258,276 1970s 27,804 25,449,488 218,322 1980s 25,651 24,185,889 208,678 1990s 23,300 20,637,179 204,393 2000s 17,299 14,151,399 176,515 Overall 270,695 191,743,541 867,399 Table 1: General word frequency statistics on the reconstructed version of the Time Corpus (1923- 2006). ging (Toutanova et al., 2003) and named entity recognition (Finkel et al., 2005). In the end, our reconstructed version of the Time Corpus in total amounted to 270,695 individual articles. In its entirety, the corpus counted 191,743,541 distinct word forms (including punctuation marks), 867,399 forms of which proved unique in their lowercased format. Some general statistics about our reconstructed version of the Time Corpus are given in Table 1. In addition to the cumulative word count statistics about the corpus, we have included the frequency information per decade (1920s, 1930s, etc.), as this periodisation will prove </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich Part-ofspeech Tagging with a Cyclic Dependency Network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 173–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean M Twenge</author>
<author>Keith W Campbell</author>
<author>Brittany Gentile</author>
</authors>
<date>2012</date>
<booktitle>Increases in Individualistic Words and Phrases in American Books, 19602008. PLoS ONE,</booktitle>
<pages>7--7</pages>
<contexts>
<context position="3583" citStr="Twenge et al., 2012" startWordPosition="542" endWordPosition="545">which – while it has invited a lot of attention in popular media – has not gone uncriticized in the international community of Humanities.1 In this paper, the authors show how a number of major historical events show interesting correlations with word counts in a vast corpus of n-grams extracted from the Google Books, allegedly containing 4% percent of all books ever printed. In recent years, the term ‘Culturomics’ seems to have become an umbrella term for studies engaging, often at an increasing level of complexity, with the seminal, publicly available Google Books NGram Corpus (Juola, 2013; Twenge et al., 2012; Acerbi et al., 2013b). Other studies, like the inspiring contribution by Leetaru (2011) have independently explored other data sets for similar purposes, such as the retroactive prediction of the Arab Spring Revolution using news data. In 1Consult, for instance, the critical report by A. Grafton on the occasion of a presentation by Michel and Lieberman Aiden at one of the annual meetings of the American Historical Association (https://www.historians. org/publications-and-directories/ perspectives-on-history/march-2011/ loneliness-and-freedom). 62 Proceedings of the 8th Workshop on Language T</context>
</contexts>
<marker>Twenge, Campbell, Gentile, 2012</marker>
<rawString>Jean M. Twenge, Keith W. Campbell, and Brittany Gentile. 2012. Increases in Individualistic Words and Phrases in American Books, 19602008. PLoS ONE, 7(7):e40181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerben Zaagsma</author>
</authors>
<title>On Digital History.</title>
<date>2013</date>
<journal>BMGN – Low Countries Historical Review,</journal>
<volume>128</volume>
<issue>4</issue>
<contexts>
<context position="2113" citStr="Zaagsma, 2013" startWordPosition="303" endWordPosition="304">ther than quantitative methodologies, it is hard to miss that ‘hipster’ terms like ‘Computational Analysis’, ‘Big Data’ and ‘Digitisation’, are currently trending in Humanities scholarship. In the international initiative of Digital Humanities, researchers from various disciplines are increasingly exploring novel, computational means to interact with their object of research. Often, this is done in collaboration with researchers from Computational Linguistics, who seem to have adopted quantitative approaches relatively sooner than other Humanities disciplines. The subfield of Digital History (Zaagsma, 2013), in which the present paper is to be situated, is but one of the multiple Humanities disciplines in which rapid progress is being made as to the application of computational methods. Although the vibrant domain of Digital History cannot be exhaustively surveyed here due to space limits, it is nevertheless interesting to refer to a recent string of frequentist lexical approaches to the study of human history, and the evolution of human culture in particular: ‘Culturomics’. This line of computational, typically dataintensive research seeks to study various aspects of human history, by researchi</context>
</contexts>
<marker>Zaagsma, 2013</marker>
<rawString>Gerben Zaagsma. 2013. On Digital History. BMGN – Low Countries Historical Review, 128(4):3–29.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
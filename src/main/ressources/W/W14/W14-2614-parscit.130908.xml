<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003618">
<title confidence="0.9986045">
Credibility Adjusted Term Frequency: A Supervised Term Weighting
Scheme for Sentiment Analysis and Text Classification
</title>
<author confidence="0.976078">
Yoon Kim Owen Zhang
</author>
<affiliation confidence="0.740613">
New York University zhonghua.zhang2006@gmail.com
</affiliation>
<email confidence="0.991341">
yhk255@nyu.edu
</email>
<sectionHeader confidence="0.993723" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999700777777778">
We provide a simple but novel supervised
weighting scheme for adjusting term fre-
quency in tf-idf for sentiment analysis
and text classification. We compare our
method to baseline weighting schemes and
find that it outperforms them on multiple
benchmarks. The method is robust and
works well on both snippets and longer
documents.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999183">
Baseline discriminative methods for text classifi-
cation usually involve training a linear classifier
over bag-of-words (BoW) representations of doc-
uments. In BoW representations (also known as
Vector Space Models), a document is represented
as a vector where each entry is a count (or binary
count) of tokens that occurred in the document.
Given that some tokens are more informative than
others, a common technique is to apply a weight-
ing scheme to give more weight to discriminative
tokens and less weight to non-discriminative ones.
Term frequency-inverse document frequency (tf-
idf) (Salton and McGill, 1983) is an unsupervised
weighting technique that is commonly employed.
In tf-idf, each token i in document d is assigned
the following weight,
</bodyText>
<equation confidence="0.792854333333333">
N
wi,d = tfi,d · log (1)
dfi
</equation>
<bodyText confidence="0.999984724137931">
where tfi,d is the number of times token i occurred
in document d, N is the number of documents in
the corpus, and dfi is the number of documents in
which token i occurred.
Many supervised and unsupervised variants of
tf-idf exist (Debole and Sebastiani (2003); Mar-
tineau and Finin (2009); Wang and Zhang (2013)).
The purpose of this paper is not to perform
an exhaustive comparison of existing weighting
schemes, and hence we do not list them here. In-
terested readers are directed to Paltoglou and Thel-
wall (2010) and Deng et al. (2014) for comprehen-
sive reviews of the different schemes.
In the present work, we propose a simple but
novel supervised method to adjust the term fre-
quency portion in tf-idf by assigning a credibil-
ity adjusted score to each token. We find that
it outperforms the traditional unsupervised tf-idf
weighting scheme on multiple benchmarks. The
benchmarks include both snippets and longer doc-
uments. We also compare our method against
Wang and Manning (2012)’s Naive-Bayes Support
Vector Machine (NBSVM), which has achieved
state-of-the-art results (or close to it) on many
datasets, and find that it performs competitively
against NBSVM. We additionally find that the
traditional tf-idf performs competitively against
other, more sophisticated methods when used with
the right scaling and normalization parameters.
</bodyText>
<sectionHeader confidence="0.972431" genericHeader="method">
2 The Method
</sectionHeader>
<bodyText confidence="0.997709333333333">
Consider a binary classification task. Let Ci,k be
the count of token i in class k, with k E {−1,1}.
Denote Ci to be the count of token i over both
classes, and y(d) to be the class of document d.
For each occurrence of token i in the training set,
we calculate the following,
</bodyText>
<equation confidence="0.997053333333333">
Ci,l if y(d) = 1
Ci ,(2)
i = Ci,−1 if y(d) = −1
</equation>
<bodyText confidence="0.955663333333333">
Intuitively, ˆsi is the average likelihood of mak-
ing the correct classification given token i’s occur-
rence in the document, if i was the only token in
</bodyText>
<equation confidence="0.984912625">
1 Ci
L
j=1
s(j) (3)
i
ˆsi =
Ci
Ci
</equation>
<bodyText confidence="0.990565666666667">
Here, j is the j-th occurrence of token i. Since
there are Ci such occurrences, j indexes from 1 to
Ci. We assign a score to token i by,
</bodyText>
<page confidence="0.977468">
79
</page>
<bodyText confidence="0.8429945">
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 79–83,
Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics
the document. In a binary classification case, this
reduces to,
</bodyText>
<equation confidence="0.989570666666667">
(4)
C2
i
</equation>
<bodyText confidence="0.994809">
Note that by construction, the support of ˆsi is
[0.5, 1].
</bodyText>
<subsectionHeader confidence="0.994958">
2.1 Credibility Adjustment
</subsectionHeader>
<bodyText confidence="0.995045833333334">
Suppose ˆsi = ˆsj = 0.75 for two different tokens
i and j, but Ci = 5 and Cj = 100. Intuition sug-
gests that ˆsj is a more credible score than ˆsi, and
that ˆsi should be shrunk towards the population
mean. Let sˆ be the (weighted) population mean.
That is,
</bodyText>
<equation confidence="0.989635333333333">
�sˆ = Ci · ˆsi (5)
C
i
</equation>
<bodyText confidence="0.999079">
where C is the count of all tokens in the corpus.
We define credibility adjusted score for token i to
be,
</bodyText>
<equation confidence="0.9997835">
C2 i,1 + C2 i,−1 + sˆ · γ
C2i + γ (6)
</equation>
<bodyText confidence="0.999886333333333">
where γ is an additive smoothing parameter. If
Ci,k’s are small, then si ≈ sˆ (otherwise, si ≈ ˆsi).
This is a form of Buhlmann credibility adjustment
from the actuarial literature (Buhlmann and Gisler,
2005). We subsequently define tf, the credibility
adjusted term frequency, to be,
</bodyText>
<equation confidence="0.81886075">
tfi,d = (0.5 + ˆsi) · tfi,d (7)
and tf is replaced with tf. That is,
N
wi,d = tfi,d · log (8)
</equation>
<bodyText confidence="0.7772745">
dfi
We refer to above as cred-tf-idf hereafter.
</bodyText>
<subsectionHeader confidence="0.998974">
2.2 Sublinear Scaling
</subsectionHeader>
<bodyText confidence="0.975428571428571">
It is common practice to apply sublinear scaling to
tf. A word occurring (say) ten times more in a
document is unlikely to be ten times as important.
Paltoglou and Thelwall (2010) confirm that sub-
linear scaling of term frequency results in signif-
icant improvements in various text classification
tasks. We employ logarithmic scaling, where tf is
replaced with log(tf) + 1. For our method, tf is
simply replaced with log(tf) + 1. We found vir-
tually no difference in performance between log
scaling and other sublinear scaling methods (such
as augmented scaling, where tf is replaced with
0.5 + 0.5+tf
max tf ).
</bodyText>
<subsectionHeader confidence="0.99112">
2.3 Normalization
</subsectionHeader>
<bodyText confidence="0.9996395">
Using normalized features resulted in substan-
tial improvements in performance versus using
un-normalized features. We thus use ˆx(d) =
x(d)/||x(d)||2 in the SVM, where x(d) is the fea-
ture vector obtained from cred-tf-idf weights for
document d.
</bodyText>
<subsectionHeader confidence="0.992422">
2.4 Naive-Bayes SVM (NBSVM)
</subsectionHeader>
<bodyText confidence="0.9987308">
Wang and Manning (2012) achieve excellent
(sometimes state-of-the-art) results on many
benchmarks using binary Naive Bayes (NB) log-
count ratios as features in an SVM. In their frame-
work,
</bodyText>
<equation confidence="0.941049">
wi,d = 1{tfi,d} log (dfi,1 + α)/ Ei(dfi,1 + α)
(dfi,−1 + α)/ Ei(dfi,−1 + α)
(9)
</equation>
<bodyText confidence="0.999915">
where dfi,k is the number of documents that con-
tain token i in class k, α is a smoothing parameter,
and 1{·} is the indicator function equal to one if
tfi,d &gt; 0 and zero otherwise. As an additional
benchmark, we implement NBSVM with α = 1.0
and compare against our results.1
</bodyText>
<sectionHeader confidence="0.996252" genericHeader="method">
3 Datasets and Experimental Setup
</sectionHeader>
<bodyText confidence="0.998638">
We test our method on both long and short text
classification tasks, all of which were used to es-
tablish baselines in Wang and Manning (2012).
Table 1 has summary statistics of the datasets. The
snippet datasets are:
</bodyText>
<listItem confidence="0.950900888888889">
• PL-sh: Short movie reviews with one sen-
tence per review. Classification involves de-
tecting whether a review is positive or nega-
tive. (Pang and Lee, 2005).2
• PL-sub: Dataset with short subjective movie
reviews and objective plot summaries. Clas-
sification task is to detect whether the sen-
tence is objective or subjective. (Pang and
Lee, 2004).
</listItem>
<bodyText confidence="0.938853875">
And the longer document datasets are:
1Wang and Manning (2012) use the same α but they dif-
fer from our NBSVM in two ways. One, they use l2 hinge
loss (as opposed to l1 loss in this paper). Two, they in-
terpolate NBSVM weights with Multivariable Naive Bayes
(MNB) weights to get the final weight vector. Further, their
tokenization is slightly different. Hence our NBSVM results
are not directly comparable. We list their results in table 2.
</bodyText>
<footnote confidence="0.9665975">
2https://www.cs.cornell.edu/people/pabo/movie-review-
data/. All the PL datasets are available here.
</footnote>
<equation confidence="0.996720333333333">
ˆsi =
C2i,1 + C2i,−1
si =
</equation>
<page confidence="0.973492">
80
</page>
<table confidence="0.999873428571428">
Dataset Length Pos Neg Test
PL-sh 21 5331 5331 CV
PL-sub 24 5000 5000 CV
PL-2k 746 1000 1000 CV
IMDB 231 12.5k 12.5k 25k
AthR 355 480 377 570
XGraph 276 584 593 784
</table>
<tableCaption confidence="0.999676">
Table 1: Summary statistics for the datasets.
</tableCaption>
<bodyText confidence="0.999583428571429">
Length is the average number of unigram tokens
(including punctuation) per document. Pos/Neg is
the number of positive/negative documents in the
training set. Test is the number of documents in
the test set (CV means that there is no separate
test set for this dataset and thus a 10-fold cross-
validation was used to calculate errors).
</bodyText>
<listItem confidence="0.9921435">
• PL-2k: 2000 full-length movie reviews that
has become the de facto benchmark for sen-
timent analysis (Pang and Lee, 2004).
• IMDB: 50k full-length movie reviews (25k
training, 25k test), from IMDB (Maas et al.,
2011).3
• AthR, XGraph: The 20-Newsgroup dataset,
2nd version with headers removed.4 Clas-
sification task is to classify which topic a
document belongs to. AthR: alt.atheism vs
religion.misc, XGraph: comp.windows.x vs
comp.graphics.
</listItem>
<subsectionHeader confidence="0.998735">
3.1 Support Vector Machine (SVM)
</subsectionHeader>
<bodyText confidence="0.999553">
For each document, we construct the feature vec-
tor x(d) using weights obtained from cred-tf-idf
with log scaling and l2 normalization. For cred-
tf-idf, γ is set to 1.0. NBSVM and tf-idf (also with
log scaling and l2 normalization) are used to es-
tablish baselines. Prediction for a test document is
given by
</bodyText>
<equation confidence="0.992752">
y(d) = sign (wTx(d) + b) (10)
</equation>
<bodyText confidence="0.9567315">
In all experiments, we use a Support Vector Ma-
chine (SVM) with a linear kernel and penalty pa-
rameter of C = 1.0. For the SVM, w, b are ob-
tained by minimizing,
</bodyText>
<equation confidence="0.974627">
N
wT w+C max(0,1−y(d)(wTx(d)+b)) (11)
d=1
</equation>
<footnote confidence="0.923750666666667">
using the LIBLINEAR library (Fan et al., 2008).
3http://ai.stanford.edu/ amaas/data/sentiment/index.html
4http://people.csail.mit.edu/jrennie/20Newsgroups
</footnote>
<subsectionHeader confidence="0.995423">
3.2 Tokenization
</subsectionHeader>
<bodyText confidence="0.99991675">
We lower-case all words but do not perform any
stemming or lemmatization. We restrict the vo-
cabulary to all tokens that occurred at least twice
in the training set.
</bodyText>
<sectionHeader confidence="0.999065" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.9999488">
For PL datasets, there are no separate test sets and
hence we use 10-fold cross validation (as do other
published results) to estimate errors. The standard
train-test splits are used on IMDB and Newsgroup
datasets.
</bodyText>
<subsectionHeader confidence="0.934248">
4.1 cred-tf-idf outperforms tf-idf
</subsectionHeader>
<bodyText confidence="0.999885333333333">
Table 2 has the comparison of results for the dif-
ferent datasets. Our method outperforms the tra-
ditional tf-idf on all benchmarks for both uni-
grams and bigrams. While some of the differ-
ences in performance are significant at the 0.05
level (e.g. IMDB), some are not (e.g. PL-2k). The
Wilcoxon signed ranks test is a non-parametric
test that is often used in cases where two classi-
fiers are compared over multiple datasets (Dem-
sar, 2006). The Wilcoxon signed ranks test indi-
cates that the overall outperformance is significant
at the &lt;0.01 level.
</bodyText>
<subsectionHeader confidence="0.993827">
4.2 NBSVM outperforms cred-tf-idf
</subsectionHeader>
<bodyText confidence="0.999873333333333">
cred-tf-idf did not outperform Wang and Manning
(2012)’s NBSVM (Wilcoxon signed ranks test p-
value = 0.1). But it did outperform our own im-
plementation of NBSVM, implying that the ex-
tra modifications by Wang and Manning (2012)
(i.e. using squared hinge loss in the SVM and in-
terpolating between NBSVM and MNB weights)
are important contributions of their methodology.
This was especially true in the case of shorter doc-
uments, where our uninterpolated NBSVM per-
formed significantly worse than their interpolated
NBSVM.
</bodyText>
<subsectionHeader confidence="0.995599">
4.3 tf-idf still performs well
</subsectionHeader>
<bodyText confidence="0.997337875">
We find that tf-idf still performs remarkably well
with the right scaling and normalization parame-
ters. Indeed, the traditional tf-idf outperformed
many of the more sophisticated methods that
employ distributed representations (Maas et al.
(2011); Socher et al. (2011)) or other weighting
schemes (Martineau and Finin (2009); Deng et al.
(2014)).
</bodyText>
<page confidence="0.99784">
81
</page>
<table confidence="0.999932894736842">
Method PL-sh PL-sub PL-2k IMDB AthR XGraph
tf-idf-uni 77.1 91.5 88.1 88.6 85.8 88.4
tf-idf-bi 78.0 92.3 89.2 90.9 86.5 88.0
Our cred-tfidf-uni 77.5 91.8 88.7 88.8 86.5 89.8
results cred-tfidf-bi 78.6 92.8 89.7 91.3 87.4 88.9
NBSVM-uni 75.5 89.9 87.0 85.9 86.7 88.5
NBSVM-bi 76.0 90.5 89.5 90.5 86.7 88.1
MNB-uni 77.9 92.6 83.5 83.6 85.0 90.0
Wang &amp; MNB-bi 79.0 93.6 85.9 86.6 85.1 91.2
Manning NBSVM-uni 78.1 92.4 87.8 88.3 87.9 91.2
NBSVM-bi 79.4 93.2 89.5 91.2 87.7 90.7
Appr. Tax.* - - 90.2 - - -
Str. SVM* - - 92.4 - - -
aug-tf-mi - - 87.8 88.0 - -
Other Disc. Conn. - - - 91.4 - -
results Word Vec.* - 88.6 88.9 88.9 - -
LLR - - 90.4 - - -
RAE 77.7 - - - - -
MV-RNN 79.0 - - - - -
</table>
<tableCaption confidence="0.994289">
Table 2: Results of our method (cred-tf-idf) against baselines (tf-idf, NBSVM), using unigrams and
</tableCaption>
<bodyText confidence="0.984203272727273">
bigrams. cred-tf-idf and tf-idf both use log scaling and l2 normalization. Best results (that do not use
external sources) are underlined, while top three are in bold. Rows 7-11 are MNB and NBSVM results
from Wang and Manning (2012). Our NBSVM results are not directly comparable to theirs (see footnote
1). Methods with * use external data or software. Appr. Tax: Uses appraisal taxonomies from WordNet
(Whitelaw et al., 2005). Str. SVM: Uses OpinionFinder to find objective versus subjective parts of the
review (Yessenalina et al., 2010). aug-tf-mi: Uses augmented term-frequency with mutual information
gain (Deng et al., 2014). Disc. Conn.: Uses discourse connectors to generate additional features (Trivedi
and Eisenstein, 2013). Word Vec.: Learns sentiment-specific word vectors to use as features combined
with BoW features (Maas et al., 2011). LLR: Uses log-likelihood ratio on features to select features
(Aue and Gamon, 2005). RAE: Recursive autoencoders (Socher et al., 2011). MV-RNN: Matrix-Vector
Recursive Neural Networks (Socher et al., 2012).
</bodyText>
<sectionHeader confidence="0.998186" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999148294117647">
In this paper we presented a novel supervised
weighting scheme, which we call credibility ad-
justed term frequency, to perform sentiment anal-
ysis and text classification. Our method outper-
forms the traditional tf-idf weighting scheme on
multiple benchmarks, which include both snippets
and longer documents. We also showed that tf-idf
is competitive against other state-of-the-art meth-
ods with the right scaling and normalization pa-
rameters.
From a performance standpoint, it would be in-
teresting to see if our method is able to achieve
even better results on the above tasks with proper
tuning of the γ parameter. Relatedly, our method
could potentially be combined with other super-
vised variants of tf-idf, either directly or through
ensembling, to improve performance further.
</bodyText>
<sectionHeader confidence="0.998366" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9799855625">
A. Aue, M. Gamon. 2005. Customizing sentiment
classifiers to new domains: A case study. Proceed-
ings of the International Conference on Recent Ad-
vances in NLP, 2011.
H. Buhlmann, A. Gisler. 2005. A Course in Credi-
bility Theory and its Applications Springer-Verlag,
Berlin.
F. Debole, F. Sebastiani. 2003. Supervised Term
Weighting for Automated Text Categorization Pro-
ceedings of the 2003 ACM symposium on Applied
Computing. 784–788.
J. Demsar. 2006. Statistical Comparison of classifiers
over multiple data sets. Journal of Machine Learn-
ing Research, 7:1-30. 2006.
Z. Deng, K. Luo, H. Yu. 2014. A study of supervised
term weighting scheme for sentiment analysis Ex-
</reference>
<page confidence="0.985558">
82
</page>
<reference confidence="0.960444854166667">
pert Systems with Applications. Volume 41, Issue 7,
3506–3513.
coders for Predicting Sentiment Distributions. In
Proceedings of EMNLP 2011.
R. Socher, B. Huval, C. Manning, A. Ng. 2012. Se-
mantic Compositionality through Recursive Matrix-
Vector Spaces. In Proceedings of EMNLP 2012.
R. Fan, K. Chang, J. Hsieh, X. Wang, C. Lin. 2008. LI-
BLINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871–
1874, June.
A. Maas, R. Daly, P. Pham, D. Huang, A. Ng, C. Potts.
2011. Learning Word Vectors for Sentiment Analy-
sis. In Proceedings of ACL 2011.
J. Martineau, T. Finin. 2009. Delta TFIDF: An Im-
proved Feature Space for Sentiment Analysis. Third
AAAI International Conference on Weblogs and So-
cial Media
G. Paltoglou, M. Thelwall. 2010. A study of Infor-
mation Retrieval weighting schemes for sentiment
analysis. In Proceedings of ACL 2010.
B. Pang, L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of ACL
2004.
B. Pang, L. Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with re-
spect to rating scales. In Proceedings of ACL 2005.
R. Socher, J. Pennington, E. Huang, A. Ng, C. Man-
ning. 2011. Semi-Supervised Recursive Autoen-
R. Trivedi, J. Eisenstein. 2013. Discourse Connec-
tors for Latent Subjectivity in Sentiment Analysis.
In Proceedings of NAACL 2011.
G. Salton, M. McGill. 1983. Introduction to Modern
Information Retrieval. McGraw-Hill.
S. Wang, C. Manning. 2012. Baselines and Bigrams:
Simple, Good Sentiment and Topic Classification.
In proceedings of ACL 2012.
D. Wang, H. Zhang. 2013. Inverse-Category-
Frequency Based Supervised Term Weighting
Schemes for Text Categorization. Journal of Infor-
mation Science and Engineering 29, 209–225.
C. Whitelaw, N. Garg, S. Argamon. 2005. Using ap-
praisal taxonomies for sentiment analysis. In Pro-
ceedings of CIKM 2005.
A. Yessenalina, Y. Yue, C. Cardie. 2010. Multi-
level Structured Models for Document-level Senti-
ment Classification. In Proceedings of ACL 2010.
</reference>
<page confidence="0.999302">
83
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.899203">
<title confidence="0.997697">Credibility Adjusted Term Frequency: A Supervised Term Scheme for Sentiment Analysis and Text Classification</title>
<author confidence="0.999475">Yoon Kim Owen Zhang</author>
<affiliation confidence="0.999973">York University</affiliation>
<email confidence="0.994277">yhk255@nyu.edu</email>
<abstract confidence="0.9905534">We provide a simple but novel supervised weighting scheme for adjusting term frein sentiment analysis and text classification. We compare our method to baseline weighting schemes and find that it outperforms them on multiple benchmarks. The method is robust and works well on both snippets and longer documents.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Aue</author>
<author>M Gamon</author>
</authors>
<title>Customizing sentiment classifiers to new domains: A case study.</title>
<date>2005</date>
<booktitle>Proceedings of the International Conference on Recent Advances in NLP,</booktitle>
<contexts>
<context position="12571" citStr="Aue and Gamon, 2005" startWordPosition="2115" endWordPosition="2118">with * use external data or software. Appr. Tax: Uses appraisal taxonomies from WordNet (Whitelaw et al., 2005). Str. SVM: Uses OpinionFinder to find objective versus subjective parts of the review (Yessenalina et al., 2010). aug-tf-mi: Uses augmented term-frequency with mutual information gain (Deng et al., 2014). Disc. Conn.: Uses discourse connectors to generate additional features (Trivedi and Eisenstein, 2013). Word Vec.: Learns sentiment-specific word vectors to use as features combined with BoW features (Maas et al., 2011). LLR: Uses log-likelihood ratio on features to select features (Aue and Gamon, 2005). RAE: Recursive autoencoders (Socher et al., 2011). MV-RNN: Matrix-Vector Recursive Neural Networks (Socher et al., 2012). 5 Conclusions and Future Work In this paper we presented a novel supervised weighting scheme, which we call credibility adjusted term frequency, to perform sentiment analysis and text classification. Our method outperforms the traditional tf-idf weighting scheme on multiple benchmarks, which include both snippets and longer documents. We also showed that tf-idf is competitive against other state-of-the-art methods with the right scaling and normalization parameters. From </context>
</contexts>
<marker>Aue, Gamon, 2005</marker>
<rawString>A. Aue, M. Gamon. 2005. Customizing sentiment classifiers to new domains: A case study. Proceedings of the International Conference on Recent Advances in NLP, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Buhlmann</author>
<author>A Gisler</author>
</authors>
<title>A Course in Credibility Theory and its Applications Springer-Verlag,</title>
<date>2005</date>
<location>Berlin.</location>
<contexts>
<context position="4350" citStr="Buhlmann and Gisler, 2005" startWordPosition="738" endWordPosition="741">e ˆsi = ˆsj = 0.75 for two different tokens i and j, but Ci = 5 and Cj = 100. Intuition suggests that ˆsj is a more credible score than ˆsi, and that ˆsi should be shrunk towards the population mean. Let sˆ be the (weighted) population mean. That is, �sˆ = Ci · ˆsi (5) C i where C is the count of all tokens in the corpus. We define credibility adjusted score for token i to be, C2 i,1 + C2 i,−1 + sˆ · γ C2i + γ (6) where γ is an additive smoothing parameter. If Ci,k’s are small, then si ≈ sˆ (otherwise, si ≈ ˆsi). This is a form of Buhlmann credibility adjustment from the actuarial literature (Buhlmann and Gisler, 2005). We subsequently define tf, the credibility adjusted term frequency, to be, tfi,d = (0.5 + ˆsi) · tfi,d (7) and tf is replaced with tf. That is, N wi,d = tfi,d · log (8) dfi We refer to above as cred-tf-idf hereafter. 2.2 Sublinear Scaling It is common practice to apply sublinear scaling to tf. A word occurring (say) ten times more in a document is unlikely to be ten times as important. Paltoglou and Thelwall (2010) confirm that sublinear scaling of term frequency results in significant improvements in various text classification tasks. We employ logarithmic scaling, where tf is replaced with</context>
</contexts>
<marker>Buhlmann, Gisler, 2005</marker>
<rawString>H. Buhlmann, A. Gisler. 2005. A Course in Credibility Theory and its Applications Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Debole</author>
<author>F Sebastiani</author>
</authors>
<title>Supervised Term Weighting for Automated Text Categorization</title>
<date>2003</date>
<booktitle>Proceedings of the 2003 ACM symposium on Applied Computing.</booktitle>
<pages>784--788</pages>
<contexts>
<context position="1596" citStr="Debole and Sebastiani (2003)" startWordPosition="245" endWordPosition="248">ique is to apply a weighting scheme to give more weight to discriminative tokens and less weight to non-discriminative ones. Term frequency-inverse document frequency (tfidf) (Salton and McGill, 1983) is an unsupervised weighting technique that is commonly employed. In tf-idf, each token i in document d is assigned the following weight, N wi,d = tfi,d · log (1) dfi where tfi,d is the number of times token i occurred in document d, N is the number of documents in the corpus, and dfi is the number of documents in which token i occurred. Many supervised and unsupervised variants of tf-idf exist (Debole and Sebastiani (2003); Martineau and Finin (2009); Wang and Zhang (2013)). The purpose of this paper is not to perform an exhaustive comparison of existing weighting schemes, and hence we do not list them here. Interested readers are directed to Paltoglou and Thelwall (2010) and Deng et al. (2014) for comprehensive reviews of the different schemes. In the present work, we propose a simple but novel supervised method to adjust the term frequency portion in tf-idf by assigning a credibility adjusted score to each token. We find that it outperforms the traditional unsupervised tf-idf weighting scheme on multiple benc</context>
</contexts>
<marker>Debole, Sebastiani, 2003</marker>
<rawString>F. Debole, F. Sebastiani. 2003. Supervised Term Weighting for Automated Text Categorization Proceedings of the 2003 ACM symposium on Applied Computing. 784–788.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Demsar</author>
</authors>
<title>Statistical Comparison of classifiers over multiple data sets.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--1</pages>
<contexts>
<context position="9807" citStr="Demsar, 2006" startWordPosition="1654" endWordPosition="1656"> cross validation (as do other published results) to estimate errors. The standard train-test splits are used on IMDB and Newsgroup datasets. 4.1 cred-tf-idf outperforms tf-idf Table 2 has the comparison of results for the different datasets. Our method outperforms the traditional tf-idf on all benchmarks for both unigrams and bigrams. While some of the differences in performance are significant at the 0.05 level (e.g. IMDB), some are not (e.g. PL-2k). The Wilcoxon signed ranks test is a non-parametric test that is often used in cases where two classifiers are compared over multiple datasets (Demsar, 2006). The Wilcoxon signed ranks test indicates that the overall outperformance is significant at the &lt;0.01 level. 4.2 NBSVM outperforms cred-tf-idf cred-tf-idf did not outperform Wang and Manning (2012)’s NBSVM (Wilcoxon signed ranks test pvalue = 0.1). But it did outperform our own implementation of NBSVM, implying that the extra modifications by Wang and Manning (2012) (i.e. using squared hinge loss in the SVM and interpolating between NBSVM and MNB weights) are important contributions of their methodology. This was especially true in the case of shorter documents, where our uninterpolated NBSVM</context>
</contexts>
<marker>Demsar, 2006</marker>
<rawString>J. Demsar. 2006. Statistical Comparison of classifiers over multiple data sets. Journal of Machine Learning Research, 7:1-30. 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Deng</author>
<author>K Luo</author>
<author>H Yu</author>
</authors>
<title>A study of supervised term weighting scheme for sentiment analysis Expert Systems with Applications.</title>
<date>2014</date>
<volume>41</volume>
<pages>3506--3513</pages>
<contexts>
<context position="1873" citStr="Deng et al. (2014)" startWordPosition="294" endWordPosition="297">n i in document d is assigned the following weight, N wi,d = tfi,d · log (1) dfi where tfi,d is the number of times token i occurred in document d, N is the number of documents in the corpus, and dfi is the number of documents in which token i occurred. Many supervised and unsupervised variants of tf-idf exist (Debole and Sebastiani (2003); Martineau and Finin (2009); Wang and Zhang (2013)). The purpose of this paper is not to perform an exhaustive comparison of existing weighting schemes, and hence we do not list them here. Interested readers are directed to Paltoglou and Thelwall (2010) and Deng et al. (2014) for comprehensive reviews of the different schemes. In the present work, we propose a simple but novel supervised method to adjust the term frequency portion in tf-idf by assigning a credibility adjusted score to each token. We find that it outperforms the traditional unsupervised tf-idf weighting scheme on multiple benchmarks. The benchmarks include both snippets and longer documents. We also compare our method against Wang and Manning (2012)’s Naive-Bayes Support Vector Machine (NBSVM), which has achieved state-of-the-art results (or close to it) on many datasets, and find that it performs </context>
<context position="10844" citStr="Deng et al. (2014)" startWordPosition="1813" endWordPosition="1816">erpolating between NBSVM and MNB weights) are important contributions of their methodology. This was especially true in the case of shorter documents, where our uninterpolated NBSVM performed significantly worse than their interpolated NBSVM. 4.3 tf-idf still performs well We find that tf-idf still performs remarkably well with the right scaling and normalization parameters. Indeed, the traditional tf-idf outperformed many of the more sophisticated methods that employ distributed representations (Maas et al. (2011); Socher et al. (2011)) or other weighting schemes (Martineau and Finin (2009); Deng et al. (2014)). 81 Method PL-sh PL-sub PL-2k IMDB AthR XGraph tf-idf-uni 77.1 91.5 88.1 88.6 85.8 88.4 tf-idf-bi 78.0 92.3 89.2 90.9 86.5 88.0 Our cred-tfidf-uni 77.5 91.8 88.7 88.8 86.5 89.8 results cred-tfidf-bi 78.6 92.8 89.7 91.3 87.4 88.9 NBSVM-uni 75.5 89.9 87.0 85.9 86.7 88.5 NBSVM-bi 76.0 90.5 89.5 90.5 86.7 88.1 MNB-uni 77.9 92.6 83.5 83.6 85.0 90.0 Wang &amp; MNB-bi 79.0 93.6 85.9 86.6 85.1 91.2 Manning NBSVM-uni 78.1 92.4 87.8 88.3 87.9 91.2 NBSVM-bi 79.4 93.2 89.5 91.2 87.7 90.7 Appr. Tax.* - - 90.2 - - - Str. SVM* - - 92.4 - - - aug-tf-mi - - 87.8 88.0 - - Other Disc. Conn. - - - 91.4 - - results </context>
<context position="12266" citStr="Deng et al., 2014" startWordPosition="2071" endWordPosition="2074">-tf-idf and tf-idf both use log scaling and l2 normalization. Best results (that do not use external sources) are underlined, while top three are in bold. Rows 7-11 are MNB and NBSVM results from Wang and Manning (2012). Our NBSVM results are not directly comparable to theirs (see footnote 1). Methods with * use external data or software. Appr. Tax: Uses appraisal taxonomies from WordNet (Whitelaw et al., 2005). Str. SVM: Uses OpinionFinder to find objective versus subjective parts of the review (Yessenalina et al., 2010). aug-tf-mi: Uses augmented term-frequency with mutual information gain (Deng et al., 2014). Disc. Conn.: Uses discourse connectors to generate additional features (Trivedi and Eisenstein, 2013). Word Vec.: Learns sentiment-specific word vectors to use as features combined with BoW features (Maas et al., 2011). LLR: Uses log-likelihood ratio on features to select features (Aue and Gamon, 2005). RAE: Recursive autoencoders (Socher et al., 2011). MV-RNN: Matrix-Vector Recursive Neural Networks (Socher et al., 2012). 5 Conclusions and Future Work In this paper we presented a novel supervised weighting scheme, which we call credibility adjusted term frequency, to perform sentiment analy</context>
</contexts>
<marker>Deng, Luo, Yu, 2014</marker>
<rawString>Z. Deng, K. Luo, H. Yu. 2014. A study of supervised term weighting scheme for sentiment analysis Expert Systems with Applications. Volume 41, Issue 7, 3506–3513.</rawString>
</citation>
<citation valid="true">
<title>coders for Predicting Sentiment Distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="10746" citStr="(2011)" startWordPosition="1800" endWordPosition="1800">fications by Wang and Manning (2012) (i.e. using squared hinge loss in the SVM and interpolating between NBSVM and MNB weights) are important contributions of their methodology. This was especially true in the case of shorter documents, where our uninterpolated NBSVM performed significantly worse than their interpolated NBSVM. 4.3 tf-idf still performs well We find that tf-idf still performs remarkably well with the right scaling and normalization parameters. Indeed, the traditional tf-idf outperformed many of the more sophisticated methods that employ distributed representations (Maas et al. (2011); Socher et al. (2011)) or other weighting schemes (Martineau and Finin (2009); Deng et al. (2014)). 81 Method PL-sh PL-sub PL-2k IMDB AthR XGraph tf-idf-uni 77.1 91.5 88.1 88.6 85.8 88.4 tf-idf-bi 78.0 92.3 89.2 90.9 86.5 88.0 Our cred-tfidf-uni 77.5 91.8 88.7 88.8 86.5 89.8 results cred-tfidf-bi 78.6 92.8 89.7 91.3 87.4 88.9 NBSVM-uni 75.5 89.9 87.0 85.9 86.7 88.5 NBSVM-bi 76.0 90.5 89.5 90.5 86.7 88.1 MNB-uni 77.9 92.6 83.5 83.6 85.0 90.0 Wang &amp; MNB-bi 79.0 93.6 85.9 86.6 85.1 91.2 Manning NBSVM-uni 78.1 92.4 87.8 88.3 87.9 91.2 NBSVM-bi 79.4 93.2 89.5 91.2 87.7 90.7 Appr. Tax.* - - 90.2 - </context>
</contexts>
<marker>2011</marker>
<rawString>coders for Predicting Sentiment Distributions. In Proceedings of EMNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>B Huval</author>
<author>C Manning</author>
<author>A Ng</author>
</authors>
<title>Semantic Compositionality through Recursive MatrixVector Spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="12693" citStr="Socher et al., 2012" startWordPosition="2131" endWordPosition="2134"> Uses OpinionFinder to find objective versus subjective parts of the review (Yessenalina et al., 2010). aug-tf-mi: Uses augmented term-frequency with mutual information gain (Deng et al., 2014). Disc. Conn.: Uses discourse connectors to generate additional features (Trivedi and Eisenstein, 2013). Word Vec.: Learns sentiment-specific word vectors to use as features combined with BoW features (Maas et al., 2011). LLR: Uses log-likelihood ratio on features to select features (Aue and Gamon, 2005). RAE: Recursive autoencoders (Socher et al., 2011). MV-RNN: Matrix-Vector Recursive Neural Networks (Socher et al., 2012). 5 Conclusions and Future Work In this paper we presented a novel supervised weighting scheme, which we call credibility adjusted term frequency, to perform sentiment analysis and text classification. Our method outperforms the traditional tf-idf weighting scheme on multiple benchmarks, which include both snippets and longer documents. We also showed that tf-idf is competitive against other state-of-the-art methods with the right scaling and normalization parameters. From a performance standpoint, it would be interesting to see if our method is able to achieve even better results on the above</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>R. Socher, B. Huval, C. Manning, A. Ng. 2012. Semantic Compositionality through Recursive MatrixVector Spaces. In Proceedings of EMNLP 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fan</author>
<author>K Chang</author>
<author>J Hsieh</author>
<author>X Wang</author>
<author>C Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>9</volume>
<pages>1874</pages>
<contexts>
<context position="8805" citStr="Fan et al., 2008" startWordPosition="1500" endWordPosition="1503"> 3.1 Support Vector Machine (SVM) For each document, we construct the feature vector x(d) using weights obtained from cred-tf-idf with log scaling and l2 normalization. For credtf-idf, γ is set to 1.0. NBSVM and tf-idf (also with log scaling and l2 normalization) are used to establish baselines. Prediction for a test document is given by y(d) = sign (wTx(d) + b) (10) In all experiments, we use a Support Vector Machine (SVM) with a linear kernel and penalty parameter of C = 1.0. For the SVM, w, b are obtained by minimizing, N wT w+C max(0,1−y(d)(wTx(d)+b)) (11) d=1 using the LIBLINEAR library (Fan et al., 2008). 3http://ai.stanford.edu/ amaas/data/sentiment/index.html 4http://people.csail.mit.edu/jrennie/20Newsgroups 3.2 Tokenization We lower-case all words but do not perform any stemming or lemmatization. We restrict the vocabulary to all tokens that occurred at least twice in the training set. 4 Results and Discussion For PL datasets, there are no separate test sets and hence we use 10-fold cross validation (as do other published results) to estimate errors. The standard train-test splits are used on IMDB and Newsgroup datasets. 4.1 cred-tf-idf outperforms tf-idf Table 2 has the comparison of resu</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>R. Fan, K. Chang, J. Hsieh, X. Wang, C. Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871– 1874, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Maas</author>
<author>R Daly</author>
<author>P Pham</author>
<author>D Huang</author>
<author>A Ng</author>
<author>C Potts</author>
</authors>
<title>Learning Word Vectors for Sentiment Analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="7962" citStr="Maas et al., 2011" startWordPosition="1357" endWordPosition="1360">aph 276 584 593 784 Table 1: Summary statistics for the datasets. Length is the average number of unigram tokens (including punctuation) per document. Pos/Neg is the number of positive/negative documents in the training set. Test is the number of documents in the test set (CV means that there is no separate test set for this dataset and thus a 10-fold crossvalidation was used to calculate errors). • PL-2k: 2000 full-length movie reviews that has become the de facto benchmark for sentiment analysis (Pang and Lee, 2004). • IMDB: 50k full-length movie reviews (25k training, 25k test), from IMDB (Maas et al., 2011).3 • AthR, XGraph: The 20-Newsgroup dataset, 2nd version with headers removed.4 Classification task is to classify which topic a document belongs to. AthR: alt.atheism vs religion.misc, XGraph: comp.windows.x vs comp.graphics. 3.1 Support Vector Machine (SVM) For each document, we construct the feature vector x(d) using weights obtained from cred-tf-idf with log scaling and l2 normalization. For credtf-idf, γ is set to 1.0. NBSVM and tf-idf (also with log scaling and l2 normalization) are used to establish baselines. Prediction for a test document is given by y(d) = sign (wTx(d) + b) (10) In a</context>
<context position="10746" citStr="Maas et al. (2011)" startWordPosition="1797" endWordPosition="1800">e extra modifications by Wang and Manning (2012) (i.e. using squared hinge loss in the SVM and interpolating between NBSVM and MNB weights) are important contributions of their methodology. This was especially true in the case of shorter documents, where our uninterpolated NBSVM performed significantly worse than their interpolated NBSVM. 4.3 tf-idf still performs well We find that tf-idf still performs remarkably well with the right scaling and normalization parameters. Indeed, the traditional tf-idf outperformed many of the more sophisticated methods that employ distributed representations (Maas et al. (2011); Socher et al. (2011)) or other weighting schemes (Martineau and Finin (2009); Deng et al. (2014)). 81 Method PL-sh PL-sub PL-2k IMDB AthR XGraph tf-idf-uni 77.1 91.5 88.1 88.6 85.8 88.4 tf-idf-bi 78.0 92.3 89.2 90.9 86.5 88.0 Our cred-tfidf-uni 77.5 91.8 88.7 88.8 86.5 89.8 results cred-tfidf-bi 78.6 92.8 89.7 91.3 87.4 88.9 NBSVM-uni 75.5 89.9 87.0 85.9 86.7 88.5 NBSVM-bi 76.0 90.5 89.5 90.5 86.7 88.1 MNB-uni 77.9 92.6 83.5 83.6 85.0 90.0 Wang &amp; MNB-bi 79.0 93.6 85.9 86.6 85.1 91.2 Manning NBSVM-uni 78.1 92.4 87.8 88.3 87.9 91.2 NBSVM-bi 79.4 93.2 89.5 91.2 87.7 90.7 Appr. Tax.* - - 90.2 - </context>
<context position="12486" citStr="Maas et al., 2011" startWordPosition="2102" endWordPosition="2105"> Our NBSVM results are not directly comparable to theirs (see footnote 1). Methods with * use external data or software. Appr. Tax: Uses appraisal taxonomies from WordNet (Whitelaw et al., 2005). Str. SVM: Uses OpinionFinder to find objective versus subjective parts of the review (Yessenalina et al., 2010). aug-tf-mi: Uses augmented term-frequency with mutual information gain (Deng et al., 2014). Disc. Conn.: Uses discourse connectors to generate additional features (Trivedi and Eisenstein, 2013). Word Vec.: Learns sentiment-specific word vectors to use as features combined with BoW features (Maas et al., 2011). LLR: Uses log-likelihood ratio on features to select features (Aue and Gamon, 2005). RAE: Recursive autoencoders (Socher et al., 2011). MV-RNN: Matrix-Vector Recursive Neural Networks (Socher et al., 2012). 5 Conclusions and Future Work In this paper we presented a novel supervised weighting scheme, which we call credibility adjusted term frequency, to perform sentiment analysis and text classification. Our method outperforms the traditional tf-idf weighting scheme on multiple benchmarks, which include both snippets and longer documents. We also showed that tf-idf is competitive against othe</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>A. Maas, R. Daly, P. Pham, D. Huang, A. Ng, C. Potts. 2011. Learning Word Vectors for Sentiment Analysis. In Proceedings of ACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Martineau</author>
<author>T Finin</author>
</authors>
<title>Delta TFIDF: An Improved Feature Space for Sentiment Analysis.</title>
<date>2009</date>
<booktitle>Third AAAI International Conference on Weblogs and Social</booktitle>
<location>Media</location>
<contexts>
<context position="1624" citStr="Martineau and Finin (2009)" startWordPosition="249" endWordPosition="253">cheme to give more weight to discriminative tokens and less weight to non-discriminative ones. Term frequency-inverse document frequency (tfidf) (Salton and McGill, 1983) is an unsupervised weighting technique that is commonly employed. In tf-idf, each token i in document d is assigned the following weight, N wi,d = tfi,d · log (1) dfi where tfi,d is the number of times token i occurred in document d, N is the number of documents in the corpus, and dfi is the number of documents in which token i occurred. Many supervised and unsupervised variants of tf-idf exist (Debole and Sebastiani (2003); Martineau and Finin (2009); Wang and Zhang (2013)). The purpose of this paper is not to perform an exhaustive comparison of existing weighting schemes, and hence we do not list them here. Interested readers are directed to Paltoglou and Thelwall (2010) and Deng et al. (2014) for comprehensive reviews of the different schemes. In the present work, we propose a simple but novel supervised method to adjust the term frequency portion in tf-idf by assigning a credibility adjusted score to each token. We find that it outperforms the traditional unsupervised tf-idf weighting scheme on multiple benchmarks. The benchmarks inclu</context>
<context position="10824" citStr="Martineau and Finin (2009)" startWordPosition="1809" endWordPosition="1812">inge loss in the SVM and interpolating between NBSVM and MNB weights) are important contributions of their methodology. This was especially true in the case of shorter documents, where our uninterpolated NBSVM performed significantly worse than their interpolated NBSVM. 4.3 tf-idf still performs well We find that tf-idf still performs remarkably well with the right scaling and normalization parameters. Indeed, the traditional tf-idf outperformed many of the more sophisticated methods that employ distributed representations (Maas et al. (2011); Socher et al. (2011)) or other weighting schemes (Martineau and Finin (2009); Deng et al. (2014)). 81 Method PL-sh PL-sub PL-2k IMDB AthR XGraph tf-idf-uni 77.1 91.5 88.1 88.6 85.8 88.4 tf-idf-bi 78.0 92.3 89.2 90.9 86.5 88.0 Our cred-tfidf-uni 77.5 91.8 88.7 88.8 86.5 89.8 results cred-tfidf-bi 78.6 92.8 89.7 91.3 87.4 88.9 NBSVM-uni 75.5 89.9 87.0 85.9 86.7 88.5 NBSVM-bi 76.0 90.5 89.5 90.5 86.7 88.1 MNB-uni 77.9 92.6 83.5 83.6 85.0 90.0 Wang &amp; MNB-bi 79.0 93.6 85.9 86.6 85.1 91.2 Manning NBSVM-uni 78.1 92.4 87.8 88.3 87.9 91.2 NBSVM-bi 79.4 93.2 89.5 91.2 87.7 90.7 Appr. Tax.* - - 90.2 - - - Str. SVM* - - 92.4 - - - aug-tf-mi - - 87.8 88.0 - - Other Disc. Conn. - -</context>
</contexts>
<marker>Martineau, Finin, 2009</marker>
<rawString>J. Martineau, T. Finin. 2009. Delta TFIDF: An Improved Feature Space for Sentiment Analysis. Third AAAI International Conference on Weblogs and Social Media</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Paltoglou</author>
<author>M Thelwall</author>
</authors>
<title>A study of Information Retrieval weighting schemes for sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1850" citStr="Paltoglou and Thelwall (2010)" startWordPosition="288" endWordPosition="292">nly employed. In tf-idf, each token i in document d is assigned the following weight, N wi,d = tfi,d · log (1) dfi where tfi,d is the number of times token i occurred in document d, N is the number of documents in the corpus, and dfi is the number of documents in which token i occurred. Many supervised and unsupervised variants of tf-idf exist (Debole and Sebastiani (2003); Martineau and Finin (2009); Wang and Zhang (2013)). The purpose of this paper is not to perform an exhaustive comparison of existing weighting schemes, and hence we do not list them here. Interested readers are directed to Paltoglou and Thelwall (2010) and Deng et al. (2014) for comprehensive reviews of the different schemes. In the present work, we propose a simple but novel supervised method to adjust the term frequency portion in tf-idf by assigning a credibility adjusted score to each token. We find that it outperforms the traditional unsupervised tf-idf weighting scheme on multiple benchmarks. The benchmarks include both snippets and longer documents. We also compare our method against Wang and Manning (2012)’s Naive-Bayes Support Vector Machine (NBSVM), which has achieved state-of-the-art results (or close to it) on many datasets, and</context>
<context position="4770" citStr="Paltoglou and Thelwall (2010)" startWordPosition="815" endWordPosition="818"> where γ is an additive smoothing parameter. If Ci,k’s are small, then si ≈ sˆ (otherwise, si ≈ ˆsi). This is a form of Buhlmann credibility adjustment from the actuarial literature (Buhlmann and Gisler, 2005). We subsequently define tf, the credibility adjusted term frequency, to be, tfi,d = (0.5 + ˆsi) · tfi,d (7) and tf is replaced with tf. That is, N wi,d = tfi,d · log (8) dfi We refer to above as cred-tf-idf hereafter. 2.2 Sublinear Scaling It is common practice to apply sublinear scaling to tf. A word occurring (say) ten times more in a document is unlikely to be ten times as important. Paltoglou and Thelwall (2010) confirm that sublinear scaling of term frequency results in significant improvements in various text classification tasks. We employ logarithmic scaling, where tf is replaced with log(tf) + 1. For our method, tf is simply replaced with log(tf) + 1. We found virtually no difference in performance between log scaling and other sublinear scaling methods (such as augmented scaling, where tf is replaced with 0.5 + 0.5+tf max tf ). 2.3 Normalization Using normalized features resulted in substantial improvements in performance versus using un-normalized features. We thus use ˆx(d) = x(d)/||x(d)||2 i</context>
</contexts>
<marker>Paltoglou, Thelwall, 2010</marker>
<rawString>G. Paltoglou, M. Thelwall. 2010. A study of Information Retrieval weighting schemes for sentiment analysis. In Proceedings of ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="6629" citStr="Pang and Lee, 2004" startWordPosition="1128" endWordPosition="1131">e against our results.1 3 Datasets and Experimental Setup We test our method on both long and short text classification tasks, all of which were used to establish baselines in Wang and Manning (2012). Table 1 has summary statistics of the datasets. The snippet datasets are: • PL-sh: Short movie reviews with one sentence per review. Classification involves detecting whether a review is positive or negative. (Pang and Lee, 2005).2 • PL-sub: Dataset with short subjective movie reviews and objective plot summaries. Classification task is to detect whether the sentence is objective or subjective. (Pang and Lee, 2004). And the longer document datasets are: 1Wang and Manning (2012) use the same α but they differ from our NBSVM in two ways. One, they use l2 hinge loss (as opposed to l1 loss in this paper). Two, they interpolate NBSVM weights with Multivariable Naive Bayes (MNB) weights to get the final weight vector. Further, their tokenization is slightly different. Hence our NBSVM results are not directly comparable. We list their results in table 2. 2https://www.cs.cornell.edu/people/pabo/movie-reviewdata/. All the PL datasets are available here. ˆsi = C2i,1 + C2i,−1 si = 80 Dataset Length Pos Neg Test PL</context>
<context position="7867" citStr="Pang and Lee, 2004" startWordPosition="1341" endWordPosition="1344"> PL-sub 24 5000 5000 CV PL-2k 746 1000 1000 CV IMDB 231 12.5k 12.5k 25k AthR 355 480 377 570 XGraph 276 584 593 784 Table 1: Summary statistics for the datasets. Length is the average number of unigram tokens (including punctuation) per document. Pos/Neg is the number of positive/negative documents in the training set. Test is the number of documents in the test set (CV means that there is no separate test set for this dataset and thus a 10-fold crossvalidation was used to calculate errors). • PL-2k: 2000 full-length movie reviews that has become the de facto benchmark for sentiment analysis (Pang and Lee, 2004). • IMDB: 50k full-length movie reviews (25k training, 25k test), from IMDB (Maas et al., 2011).3 • AthR, XGraph: The 20-Newsgroup dataset, 2nd version with headers removed.4 Classification task is to classify which topic a document belongs to. AthR: alt.atheism vs religion.misc, XGraph: comp.windows.x vs comp.graphics. 3.1 Support Vector Machine (SVM) For each document, we construct the feature vector x(d) using weights obtained from cred-tf-idf with log scaling and l2 normalization. For credtf-idf, γ is set to 1.0. NBSVM and tf-idf (also with log scaling and l2 normalization) are used to est</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>B. Pang, L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="6440" citStr="Pang and Lee, 2005" startWordPosition="1098" endWordPosition="1101"> class k, α is a smoothing parameter, and 1{·} is the indicator function equal to one if tfi,d &gt; 0 and zero otherwise. As an additional benchmark, we implement NBSVM with α = 1.0 and compare against our results.1 3 Datasets and Experimental Setup We test our method on both long and short text classification tasks, all of which were used to establish baselines in Wang and Manning (2012). Table 1 has summary statistics of the datasets. The snippet datasets are: • PL-sh: Short movie reviews with one sentence per review. Classification involves detecting whether a review is positive or negative. (Pang and Lee, 2005).2 • PL-sub: Dataset with short subjective movie reviews and objective plot summaries. Classification task is to detect whether the sentence is objective or subjective. (Pang and Lee, 2004). And the longer document datasets are: 1Wang and Manning (2012) use the same α but they differ from our NBSVM in two ways. One, they use l2 hinge loss (as opposed to l1 loss in this paper). Two, they interpolate NBSVM weights with Multivariable Naive Bayes (MNB) weights to get the final weight vector. Further, their tokenization is slightly different. Hence our NBSVM results are not directly comparable. We </context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>B. Pang, L. Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>J Pennington</author>
<author>E Huang</author>
<author>A Ng</author>
<author>C Manning</author>
</authors>
<title>Semi-Supervised Recursive Autoen-</title>
<date>2011</date>
<contexts>
<context position="10768" citStr="Socher et al. (2011)" startWordPosition="1801" endWordPosition="1804">s by Wang and Manning (2012) (i.e. using squared hinge loss in the SVM and interpolating between NBSVM and MNB weights) are important contributions of their methodology. This was especially true in the case of shorter documents, where our uninterpolated NBSVM performed significantly worse than their interpolated NBSVM. 4.3 tf-idf still performs well We find that tf-idf still performs remarkably well with the right scaling and normalization parameters. Indeed, the traditional tf-idf outperformed many of the more sophisticated methods that employ distributed representations (Maas et al. (2011); Socher et al. (2011)) or other weighting schemes (Martineau and Finin (2009); Deng et al. (2014)). 81 Method PL-sh PL-sub PL-2k IMDB AthR XGraph tf-idf-uni 77.1 91.5 88.1 88.6 85.8 88.4 tf-idf-bi 78.0 92.3 89.2 90.9 86.5 88.0 Our cred-tfidf-uni 77.5 91.8 88.7 88.8 86.5 89.8 results cred-tfidf-bi 78.6 92.8 89.7 91.3 87.4 88.9 NBSVM-uni 75.5 89.9 87.0 85.9 86.7 88.5 NBSVM-bi 76.0 90.5 89.5 90.5 86.7 88.1 MNB-uni 77.9 92.6 83.5 83.6 85.0 90.0 Wang &amp; MNB-bi 79.0 93.6 85.9 86.6 85.1 91.2 Manning NBSVM-uni 78.1 92.4 87.8 88.3 87.9 91.2 NBSVM-bi 79.4 93.2 89.5 91.2 87.7 90.7 Appr. Tax.* - - 90.2 - - - Str. SVM* - - 92.4</context>
<context position="12622" citStr="Socher et al., 2011" startWordPosition="2122" endWordPosition="2125">es appraisal taxonomies from WordNet (Whitelaw et al., 2005). Str. SVM: Uses OpinionFinder to find objective versus subjective parts of the review (Yessenalina et al., 2010). aug-tf-mi: Uses augmented term-frequency with mutual information gain (Deng et al., 2014). Disc. Conn.: Uses discourse connectors to generate additional features (Trivedi and Eisenstein, 2013). Word Vec.: Learns sentiment-specific word vectors to use as features combined with BoW features (Maas et al., 2011). LLR: Uses log-likelihood ratio on features to select features (Aue and Gamon, 2005). RAE: Recursive autoencoders (Socher et al., 2011). MV-RNN: Matrix-Vector Recursive Neural Networks (Socher et al., 2012). 5 Conclusions and Future Work In this paper we presented a novel supervised weighting scheme, which we call credibility adjusted term frequency, to perform sentiment analysis and text classification. Our method outperforms the traditional tf-idf weighting scheme on multiple benchmarks, which include both snippets and longer documents. We also showed that tf-idf is competitive against other state-of-the-art methods with the right scaling and normalization parameters. From a performance standpoint, it would be interesting t</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>R. Socher, J. Pennington, E. Huang, A. Ng, C. Manning. 2011. Semi-Supervised Recursive Autoen-</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Trivedi</author>
<author>J Eisenstein</author>
</authors>
<title>Discourse Connectors for Latent Subjectivity in Sentiment Analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="12369" citStr="Trivedi and Eisenstein, 2013" startWordPosition="2084" endWordPosition="2087">external sources) are underlined, while top three are in bold. Rows 7-11 are MNB and NBSVM results from Wang and Manning (2012). Our NBSVM results are not directly comparable to theirs (see footnote 1). Methods with * use external data or software. Appr. Tax: Uses appraisal taxonomies from WordNet (Whitelaw et al., 2005). Str. SVM: Uses OpinionFinder to find objective versus subjective parts of the review (Yessenalina et al., 2010). aug-tf-mi: Uses augmented term-frequency with mutual information gain (Deng et al., 2014). Disc. Conn.: Uses discourse connectors to generate additional features (Trivedi and Eisenstein, 2013). Word Vec.: Learns sentiment-specific word vectors to use as features combined with BoW features (Maas et al., 2011). LLR: Uses log-likelihood ratio on features to select features (Aue and Gamon, 2005). RAE: Recursive autoencoders (Socher et al., 2011). MV-RNN: Matrix-Vector Recursive Neural Networks (Socher et al., 2012). 5 Conclusions and Future Work In this paper we presented a novel supervised weighting scheme, which we call credibility adjusted term frequency, to perform sentiment analysis and text classification. Our method outperforms the traditional tf-idf weighting scheme on multiple</context>
</contexts>
<marker>Trivedi, Eisenstein, 2013</marker>
<rawString>R. Trivedi, J. Eisenstein. 2013. Discourse Connectors for Latent Subjectivity in Sentiment Analysis. In Proceedings of NAACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="1168" citStr="Salton and McGill, 1983" startWordPosition="169" endWordPosition="172">Introduction Baseline discriminative methods for text classification usually involve training a linear classifier over bag-of-words (BoW) representations of documents. In BoW representations (also known as Vector Space Models), a document is represented as a vector where each entry is a count (or binary count) of tokens that occurred in the document. Given that some tokens are more informative than others, a common technique is to apply a weighting scheme to give more weight to discriminative tokens and less weight to non-discriminative ones. Term frequency-inverse document frequency (tfidf) (Salton and McGill, 1983) is an unsupervised weighting technique that is commonly employed. In tf-idf, each token i in document d is assigned the following weight, N wi,d = tfi,d · log (1) dfi where tfi,d is the number of times token i occurred in document d, N is the number of documents in the corpus, and dfi is the number of documents in which token i occurred. Many supervised and unsupervised variants of tf-idf exist (Debole and Sebastiani (2003); Martineau and Finin (2009); Wang and Zhang (2013)). The purpose of this paper is not to perform an exhaustive comparison of existing weighting schemes, and hence we do no</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton, M. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wang</author>
<author>C Manning</author>
</authors>
<title>Baselines and Bigrams: Simple, Good Sentiment and Topic Classification.</title>
<date>2012</date>
<booktitle>In proceedings of ACL</booktitle>
<contexts>
<context position="2321" citStr="Wang and Manning (2012)" startWordPosition="366" endWordPosition="369">xhaustive comparison of existing weighting schemes, and hence we do not list them here. Interested readers are directed to Paltoglou and Thelwall (2010) and Deng et al. (2014) for comprehensive reviews of the different schemes. In the present work, we propose a simple but novel supervised method to adjust the term frequency portion in tf-idf by assigning a credibility adjusted score to each token. We find that it outperforms the traditional unsupervised tf-idf weighting scheme on multiple benchmarks. The benchmarks include both snippets and longer documents. We also compare our method against Wang and Manning (2012)’s Naive-Bayes Support Vector Machine (NBSVM), which has achieved state-of-the-art results (or close to it) on many datasets, and find that it performs competitively against NBSVM. We additionally find that the traditional tf-idf performs competitively against other, more sophisticated methods when used with the right scaling and normalization parameters. 2 The Method Consider a binary classification task. Let Ci,k be the count of token i in class k, with k E {−1,1}. Denote Ci to be the count of token i over both classes, and y(d) to be the class of document d. For each occurrence of token i i</context>
<context position="5515" citStr="Wang and Manning (2012)" startWordPosition="934" endWordPosition="937">. We employ logarithmic scaling, where tf is replaced with log(tf) + 1. For our method, tf is simply replaced with log(tf) + 1. We found virtually no difference in performance between log scaling and other sublinear scaling methods (such as augmented scaling, where tf is replaced with 0.5 + 0.5+tf max tf ). 2.3 Normalization Using normalized features resulted in substantial improvements in performance versus using un-normalized features. We thus use ˆx(d) = x(d)/||x(d)||2 in the SVM, where x(d) is the feature vector obtained from cred-tf-idf weights for document d. 2.4 Naive-Bayes SVM (NBSVM) Wang and Manning (2012) achieve excellent (sometimes state-of-the-art) results on many benchmarks using binary Naive Bayes (NB) logcount ratios as features in an SVM. In their framework, wi,d = 1{tfi,d} log (dfi,1 + α)/ Ei(dfi,1 + α) (dfi,−1 + α)/ Ei(dfi,−1 + α) (9) where dfi,k is the number of documents that contain token i in class k, α is a smoothing parameter, and 1{·} is the indicator function equal to one if tfi,d &gt; 0 and zero otherwise. As an additional benchmark, we implement NBSVM with α = 1.0 and compare against our results.1 3 Datasets and Experimental Setup We test our method on both long and short text </context>
<context position="10005" citStr="Wang and Manning (2012)" startWordPosition="1682" endWordPosition="1685"> the comparison of results for the different datasets. Our method outperforms the traditional tf-idf on all benchmarks for both unigrams and bigrams. While some of the differences in performance are significant at the 0.05 level (e.g. IMDB), some are not (e.g. PL-2k). The Wilcoxon signed ranks test is a non-parametric test that is often used in cases where two classifiers are compared over multiple datasets (Demsar, 2006). The Wilcoxon signed ranks test indicates that the overall outperformance is significant at the &lt;0.01 level. 4.2 NBSVM outperforms cred-tf-idf cred-tf-idf did not outperform Wang and Manning (2012)’s NBSVM (Wilcoxon signed ranks test pvalue = 0.1). But it did outperform our own implementation of NBSVM, implying that the extra modifications by Wang and Manning (2012) (i.e. using squared hinge loss in the SVM and interpolating between NBSVM and MNB weights) are important contributions of their methodology. This was especially true in the case of shorter documents, where our uninterpolated NBSVM performed significantly worse than their interpolated NBSVM. 4.3 tf-idf still performs well We find that tf-idf still performs remarkably well with the right scaling and normalization parameters. I</context>
<context position="11867" citStr="Wang and Manning (2012)" startWordPosition="2011" endWordPosition="2014">NBSVM-uni 78.1 92.4 87.8 88.3 87.9 91.2 NBSVM-bi 79.4 93.2 89.5 91.2 87.7 90.7 Appr. Tax.* - - 90.2 - - - Str. SVM* - - 92.4 - - - aug-tf-mi - - 87.8 88.0 - - Other Disc. Conn. - - - 91.4 - - results Word Vec.* - 88.6 88.9 88.9 - - LLR - - 90.4 - - - RAE 77.7 - - - - - MV-RNN 79.0 - - - - - Table 2: Results of our method (cred-tf-idf) against baselines (tf-idf, NBSVM), using unigrams and bigrams. cred-tf-idf and tf-idf both use log scaling and l2 normalization. Best results (that do not use external sources) are underlined, while top three are in bold. Rows 7-11 are MNB and NBSVM results from Wang and Manning (2012). Our NBSVM results are not directly comparable to theirs (see footnote 1). Methods with * use external data or software. Appr. Tax: Uses appraisal taxonomies from WordNet (Whitelaw et al., 2005). Str. SVM: Uses OpinionFinder to find objective versus subjective parts of the review (Yessenalina et al., 2010). aug-tf-mi: Uses augmented term-frequency with mutual information gain (Deng et al., 2014). Disc. Conn.: Uses discourse connectors to generate additional features (Trivedi and Eisenstein, 2013). Word Vec.: Learns sentiment-specific word vectors to use as features combined with BoW features </context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>S. Wang, C. Manning. 2012. Baselines and Bigrams: Simple, Good Sentiment and Topic Classification. In proceedings of ACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wang</author>
<author>H Zhang</author>
</authors>
<title>Inverse-CategoryFrequency Based Supervised Term Weighting Schemes for Text Categorization.</title>
<date>2013</date>
<journal>Journal of Information Science and Engineering</journal>
<volume>29</volume>
<pages>209--225</pages>
<contexts>
<context position="1647" citStr="Wang and Zhang (2013)" startWordPosition="254" endWordPosition="257"> discriminative tokens and less weight to non-discriminative ones. Term frequency-inverse document frequency (tfidf) (Salton and McGill, 1983) is an unsupervised weighting technique that is commonly employed. In tf-idf, each token i in document d is assigned the following weight, N wi,d = tfi,d · log (1) dfi where tfi,d is the number of times token i occurred in document d, N is the number of documents in the corpus, and dfi is the number of documents in which token i occurred. Many supervised and unsupervised variants of tf-idf exist (Debole and Sebastiani (2003); Martineau and Finin (2009); Wang and Zhang (2013)). The purpose of this paper is not to perform an exhaustive comparison of existing weighting schemes, and hence we do not list them here. Interested readers are directed to Paltoglou and Thelwall (2010) and Deng et al. (2014) for comprehensive reviews of the different schemes. In the present work, we propose a simple but novel supervised method to adjust the term frequency portion in tf-idf by assigning a credibility adjusted score to each token. We find that it outperforms the traditional unsupervised tf-idf weighting scheme on multiple benchmarks. The benchmarks include both snippets and lo</context>
</contexts>
<marker>Wang, Zhang, 2013</marker>
<rawString>D. Wang, H. Zhang. 2013. Inverse-CategoryFrequency Based Supervised Term Weighting Schemes for Text Categorization. Journal of Information Science and Engineering 29, 209–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Whitelaw</author>
<author>N Garg</author>
<author>S Argamon</author>
</authors>
<title>Using appraisal taxonomies for sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of CIKM</booktitle>
<contexts>
<context position="12062" citStr="Whitelaw et al., 2005" startWordPosition="2042" endWordPosition="2045">lts Word Vec.* - 88.6 88.9 88.9 - - LLR - - 90.4 - - - RAE 77.7 - - - - - MV-RNN 79.0 - - - - - Table 2: Results of our method (cred-tf-idf) against baselines (tf-idf, NBSVM), using unigrams and bigrams. cred-tf-idf and tf-idf both use log scaling and l2 normalization. Best results (that do not use external sources) are underlined, while top three are in bold. Rows 7-11 are MNB and NBSVM results from Wang and Manning (2012). Our NBSVM results are not directly comparable to theirs (see footnote 1). Methods with * use external data or software. Appr. Tax: Uses appraisal taxonomies from WordNet (Whitelaw et al., 2005). Str. SVM: Uses OpinionFinder to find objective versus subjective parts of the review (Yessenalina et al., 2010). aug-tf-mi: Uses augmented term-frequency with mutual information gain (Deng et al., 2014). Disc. Conn.: Uses discourse connectors to generate additional features (Trivedi and Eisenstein, 2013). Word Vec.: Learns sentiment-specific word vectors to use as features combined with BoW features (Maas et al., 2011). LLR: Uses log-likelihood ratio on features to select features (Aue and Gamon, 2005). RAE: Recursive autoencoders (Socher et al., 2011). MV-RNN: Matrix-Vector Recursive Neural</context>
</contexts>
<marker>Whitelaw, Garg, Argamon, 2005</marker>
<rawString>C. Whitelaw, N. Garg, S. Argamon. 2005. Using appraisal taxonomies for sentiment analysis. In Proceedings of CIKM 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yessenalina</author>
<author>Y Yue</author>
<author>C Cardie</author>
</authors>
<title>Multilevel Structured Models for Document-level Sentiment Classification.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="12175" citStr="Yessenalina et al., 2010" startWordPosition="2059" endWordPosition="2062">ts of our method (cred-tf-idf) against baselines (tf-idf, NBSVM), using unigrams and bigrams. cred-tf-idf and tf-idf both use log scaling and l2 normalization. Best results (that do not use external sources) are underlined, while top three are in bold. Rows 7-11 are MNB and NBSVM results from Wang and Manning (2012). Our NBSVM results are not directly comparable to theirs (see footnote 1). Methods with * use external data or software. Appr. Tax: Uses appraisal taxonomies from WordNet (Whitelaw et al., 2005). Str. SVM: Uses OpinionFinder to find objective versus subjective parts of the review (Yessenalina et al., 2010). aug-tf-mi: Uses augmented term-frequency with mutual information gain (Deng et al., 2014). Disc. Conn.: Uses discourse connectors to generate additional features (Trivedi and Eisenstein, 2013). Word Vec.: Learns sentiment-specific word vectors to use as features combined with BoW features (Maas et al., 2011). LLR: Uses log-likelihood ratio on features to select features (Aue and Gamon, 2005). RAE: Recursive autoencoders (Socher et al., 2011). MV-RNN: Matrix-Vector Recursive Neural Networks (Socher et al., 2012). 5 Conclusions and Future Work In this paper we presented a novel supervised weig</context>
</contexts>
<marker>Yessenalina, Yue, Cardie, 2010</marker>
<rawString>A. Yessenalina, Y. Yue, C. Cardie. 2010. Multilevel Structured Models for Document-level Sentiment Classification. In Proceedings of ACL 2010.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001441">
<title confidence="0.990947">
Word Confidence Estimation for SMT N-best List Re-ranking
</title>
<author confidence="0.971759">
Ngoc-Quang Luong Laurent Besacier Benjamin Lecouteux
</author>
<affiliation confidence="0.838198">
LIG, Campus de Grenoble
</affiliation>
<address confidence="0.9197825">
41, Rue des Math´ematiques,
UJF - BP53, F-38041 Grenoble Cedex 9, France
</address>
<email confidence="0.999271">
{ngoc-quang.luong,laurent.besacier,benjamin.lecouteux}@imag.fr
</email>
<sectionHeader confidence="0.997393" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999990166666667">
This paper proposes to use Word Confi-
dence Estimation (WCE) information to
improve MT outputs via N-best list re-
ranking. From the confidence label as-
signed for each word in the MT hypoth-
esis, we add six scores to the baseline log-
linear model in order to re-rank the N-best
list. Firstly, the correlation between the
WCE-based sentence-level scores and the
conventional evaluation scores (BLEU,
TER, TERp-A) is investigated. Then, the
N-best list re-ranking is evaluated over dif-
ferent WCE system performance levels:
from our real and efficient WCE system
(ranked 1st during last WMT 2013 Quality
Estimation Task) to an oracle WCE (which
simulates an interactive scenario where a
user simply validates words of a MT hy-
pothesis and the new output will be auto-
matically re-generated). The results sug-
gest that our real WCE system slightly (but
significantly) improves the baseline while
the oracle one extremely boosts it; and bet-
ter WCE leads to better MT quality.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999686537037037">
A number of methods to improve MT hypothe-
ses after decoding have been proposed in the past,
such as: post-editing, re-ranking or re-decoding.
Post-editing (Parton et al., 2012) is a human-
inspired task where the machine post edits trans-
lations in a second automatic pass. In re-ranking
(Zhang et al., 2006; Duh and Kirchhoff, 2008;
Bach et al., 2011), more features are used along
with the multiple model scores for re-determining
the 1-best among N-best list. Meanwhile, re-
decoding process (Venugopal et al., 2007) inter-
venes directly into the decoder’s search graph (e.g.
adds more reward or penalty scores), driving it to
another better path.
This work aims at re-ranking the N-best list to im-
prove MT quality. Generally, during the transla-
tion task, the decoder traverses through paths in
its search space, computes the objective function
values for them and outputs the one with high-
est score as the best hypothesis. Besides, those
with lower scores can also be generated in a so-
called N-best list. The decoder’s function consists
of parameters from different models, such as trans-
lation, distortion, word penalties, reordering, lan-
guage models, etc. In the N-best list, although the
current 1-best beats the remains in terms of model
score, it might not be exactly the closest to the hu-
man reference. Therefore, adding more decoder
independent features would be expected to raise
up a better candidate. In this work, we build six
additional features based on the labels predicted
by our Word Confidence Estimation (WCE) sys-
tem, then integrate them with the existing decoder
scores for re-ranking hypotheses in the N-best
list. More precisely, in the second pass, our re-
ranker aggregates over decoder and WCE-based
weighted scores and utilizes the obtained sum to
sort out the best candidate. The novelty of this pa-
per lies on the following contributions: the corre-
lation between WCE-based sentence-level scores
and conventional evaluation scores (BLEU, TER,
TERp-A) is first investigated. Then, we conduct
the N-best list re-ranking over different WCE sys-
tem performance levels: starting by a real WCE,
passing through several gradually improved (sim-
ulated) systems and finally the “oracle” one. From
these in-depth experiments, the role of WCE in
improving MT quality via re-ranking N-best list
is confirmed and reinforced.
The remaining parts of this article are organized
as follows: in section 2 we summarize some out-
standing approaches in N-best list re-ranking as
well as in WCE. Section 3 describes our WCE sys-
tem construction, followed by proposed features.
</bodyText>
<page confidence="0.822244">
1
</page>
<note confidence="0.9611385">
Workshop on Humans and Computer-assisted Translation, pages 1–9,
Gothenburg, Sweden, 26 April 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9982784">
The experiments along with results and in-depth
analysis of WCE scores’ contribution (as WCE
system gets better) are presented in Section 4 and
Section 5. The last section concludes the paper
and points out some ongoing work.
</bodyText>
<sectionHeader confidence="0.999883" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.984724">
2.1 N-best List Re-ranking
</subsectionHeader>
<bodyText confidence="0.999956510638298">
Walking through various related work concern-
ing this issue, we observe some prominent ideas.
The first attempt focuses on proposing additional
Language Models. Kirchhoff and Yang (2005)
train one word-based 4-gram model (with modi-
fied Kneser-Ney smoothing) and one factored tri-
gram one, then combine them with seven decoder
scores for re-ranking N-best lists of several SMT
systems. Their proposed LMs increase the transla-
tion quality of the baselines (measured by BLEU
score) from 21.6 to 22.0 (Finnish - English), or
from 30.5 to 31.0 (Spanish - English). Meanwhile,
Zhang et al. (2006) experiment a distributed LM
where each server, among the total of 150, hosts a
portion of the data and responses its client, allow-
ing them to exploit an extremely large corpus (2.7
billion word English Gigaword) for estimating N-
gram probability. The quality of their Chinese
- English hypotheses after the re-scoring process
by using this LM is improved 4.8% (from BLEU
31.44 to 32.64, oracle score = 37.48).
In one other direction, several authors propose to
replace the current linear scoring function used by
the decoder by more efficient functions. Sokolov
et al. (2012) learn their non-linear scoring function
in a learning-to-rank paradigm, applying Boosting
algorithm. Their gains on the WMT’{10, 11, 12}
are shown modest yet consistent and higher than
those based on linear scoring functions. Duh and
Kirchhoff (2008) use Minimum Error Rate Train-
ing (MERT) (Och, 2003) as a weak learner and
build their own solution, BoostedMERT, a highly-
expressive re-ranker created by voting among mul-
tiple MERT ones. Their proposed model dramat-
ically beats the decoder’s log-linear model (43.7
vs. 42.0 BLEU) in IWSLT 2007 Arabic - English
task. Applying solely goodness (the sentence con-
fidence) scores, Bach et al. (2011) obtain very con-
sistent TER reductions (0.7 and 0.6 on the dev and
test set) after a 5-list re-ranking for their Arabic -
English SMT hypotheses. This latter work is the
one that is the most related to our paper. However,
the major differences are: (1) our proposed sen-
tence scores are computed based on word confi-
dence labels; and (2) we perform an in-depth study
of the use of WCE for N-best reranking and assess
its usefulness in a simulated interactive scenario.
</bodyText>
<subsectionHeader confidence="0.999129">
2.2 Word Confidence Estimation
</subsectionHeader>
<bodyText confidence="0.9999834">
Confidence Estimation (CE) is the task of iden-
tifying the correct parts and detecting the trans-
lation errors in MT output. If the error is pre-
dicted for each word, this becomes WCE. The in-
teresting uses of WCE include: pointing out the
words that need to be corrected by the post-editor,
telling readers about the reliability of a specific
portion, and selecting the best segments among
options from multiple translation systems for com-
bination.
Dealing with this problem, various approaches
have been proposed: Blatz et al. (2003) combine
several features using neural network and naive
Bayes learning algorithms. One of the most ef-
fective feature combinations is the Word Posterior
Probability (WPP) as suggested by Ueffing et al.
(2003) associated with IBM-model based features
(Blatz et al., 2004). Ueffing and Ney (2005)
propose an approach for phrase-based translation
models: a phrase is a sequence of contiguous
words and is extracted from the word-aligned
bilingual training corpus. The confidence value
of each word is then computed by summing over
all phrase pairs in which the target part contains
this word. Xiong et al. (2010) integrate target
word’s Part-Of-Speech (POS) and train them by
Maximum Entropy Model, allowing significative
gains in comparison to WPP features. The novel
features from source side, alignment context, and
dependency structure (Bach et al., 2011) help to
augment marginally in F-score as well as the Pear-
son correlation with human judgment. Other ap-
proaches are based on external features (Soricut
and Echihabi, 2010; Felice and Specia, 2012) al-
lowing to cope with various MT systems (e.g. sta-
tistical, rule based etc.). Among the numerous
WCE applications, we consider its contribution in
a specific step of SMT pipeline: N-best list re-
ranking. Our WCE system and the proposed re-
ranking features are presented in the next section.
</bodyText>
<sectionHeader confidence="0.983504" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.990363333333333">
Our approach can be expressed in three steps: in-
vestigate the potential of using word-level score in
N-best list re-ranking, build the WCE system and
</bodyText>
<page confidence="0.9882">
2
</page>
<bodyText confidence="0.975307">
extract additional features to integrate with the ex-
isting log-linear model.
</bodyText>
<subsectionHeader confidence="0.959813">
3.1 Investigating the correlation between
“word quality” scores and other metrics
</subsectionHeader>
<bodyText confidence="0.998103666666667">
Firstly, we investigate the correlation between
sentence-level scores (obtained from WCE labels)
and conventional evaluation scores (BLEU (Pa-
pineni et al., 2002), TER and TERp-A (Snover
et al., 2008)). For each sentence, a word quality
score (WQS) is calculated by:
</bodyText>
<equation confidence="0.9375555">
WQS = #&amp;quot;G&amp;quot;(good) words (1)
#words
</equation>
<bodyText confidence="0.999745566666667">
In other words, we are trying to answer the fol-
lowing question: can the high percentage of “G”
(good) words (predicted by WCE system) in a
MT output ensure its possibility of having a better
BLEU and low TER (TERp-A) value ? This inves-
tigation is a strong prerequisite for further exper-
iments in order to check that WCE scores do not
bring additional “noise” to the re-ranking process.
In this experiment, we compute WQS over our en-
tire French - English data set (total of 10,881 1-
best translations) for which WCE oracle labels are
available (see Section 3.2 to see how they were ob-
tained). The results are plotted in Figure 1, where
the y axis shows the “G” (good) word percent-
age, and the x axis shows BLEU (1a), TER (1b) or
TERp-A (1c) scores. It can be seen from Figure 1
that the major parts of points (the densest areas) in
all three cases conform the common tendency: In
Figure 1a, the higher “G” percentage, the higher
BLEU is; on the contrary, in Figure 1b (Figure
1c), the higher “G” percentage, the lower TER
(TERp-A) is. We notice some outliers, i.e. sen-
tences with most or almost words labeled “good”,
yet still have low BLEU or high TER (TERp-A)
scores. This phenomenon is to be expected when
many (unknown) source words are not translated
or when the (unique) reference is simply too far
from the hypothesis. Nevertheless, the informa-
tion extracted from oracle WCE labels seems use-
ful to build an efficient re-ranker.
</bodyText>
<subsectionHeader confidence="0.999449">
3.2 WCE System Preparation
</subsectionHeader>
<bodyText confidence="0.994336333333333">
Essentially, a WCE system construction consists
of two pivotal elements: the features (the SMT
system dependent or independent information
extracted for each word to represent its char-
acteristics) and the machine learning method
(to train the prediction model). Motivated
</bodyText>
<figureCaption confidence="0.846378333333333">
Figure 1: The correlation between WQS in a sen-
tence and its overall quality measured by : (a)
BLEU, (b) TER and (c) TERp-A metrics
</figureCaption>
<bodyText confidence="0.999261285714286">
by the idea of addressing WCE problem as
a sequence labeling process, we employ the
Conditional Random Fields (CRFs) for our model
training, with WAPITI toolkit (Lavergne et al.,
2010). Basically, CRF computes the probabil-
ity of the output sequence Y = (y1, y2, ..., yN)
given the input sequence X = (x1, x2, ..., xN) by:
</bodyText>
<page confidence="0.839436">
3
</page>
<equation confidence="0.9984164">
K
pθ(Y |X) = Zθ(X)exp I:θkFk(X,Y )
k=1
(2)
T
</equation>
<bodyText confidence="0.9832819">
where Fk(X,Y ) = Et=1 fk(yt−1, yt, xt);
{fk} (k = 1, K) is a set of feature functions;
{θk} (k = 1, K) are the associated parameter val-
ues; and Zθ(x) is the normalization function.
In terms of features, a number of knowledge
sources are employed for extracting them, result-
ing in the major types listed below. We briefly
summarize them in this work, further details about
total of 25 features can be referred in (Luong et al.,
2013a).
</bodyText>
<listItem confidence="0.986112866666667">
• Target Side: target word; bigram (trigram)
backward sequences; number of occurrences
• Source Side: source word(s) aligned to the
target word
• Alignment Context: the combinations of the
target (source) word and all aligned source
(target) words in the window ±2
• Word posterior probability
• Pseudo-reference (Google Translate):
whether the current word appears in the
pseudo reference or not1?
• Graph topology: number of alternative paths
in the confusion set, maximum and minimum
values of posterior probability distribution
• Language model (LM) based: length of the
longest sequence of the current word and its
previous ones in the target (resp. source) LM.
For example, with the target word wi: if the
sequence wi−2wi−1wi appears in the target
LM but the sequence wi−3wi−2wi−1wi does
not, the n-gram value for wi will be 3.
• Lexical Features: word’s Part-Of-Speech
(POS); sequence of POS of all its aligned
source words; POS bigram (trigram) back-
ward sequences; punctuation; proper name;
numerical
• Syntactic Features: Null link; constituent la-
bel; depth in the constituent tree
• Semantic Features: number of word senses in
WordNet.
</listItem>
<bodyText confidence="0.6824835">
Interestingly, this feature set was also used in our
English - Spanish WCE system which got the first
</bodyText>
<footnote confidence="0.768803">
1This is our first-time experimented feature and does not
appear in (Luong et al., 2013a)
</footnote>
<bodyText confidence="0.99915408">
rank in WMT 2013 Quality Estimation Shared
Task (Luong et al., 2013b).
For building the WCE training and test sets, we
use a dataset of 10,881 French sentences (Potet
et al., 2012) , and apply a baseline SMT system
to generate hypotheses (1000-best list). Our base-
line SMT system (presented for WMT 2010 eval-
uation) keeps the Moses’s default setting (Koehn
et al., 2007): log-linear model with 14 weighted
feature functions. The translation model is trained
on the Europarl and News parallel corpora of
WMT102 evaluation campaign (1,638,440 sen-
tences). The target language model is trained by
the SRI language modeling toolkit (Stolcke, 2002)
on the news monolingual corpus (48,653,884 sen-
tences).
Translators were then invited to correct MT out-
puts, giving us the same amount of post editions
(Potet et al., 2012). The set of triples (source,
hypothesis, post edition) is then divided into the
training set (10000 first triples) and test set (881
remaining). To train the WCE model, we ex-
tract all above features for words of the 1-best hy-
potheses of the training set. For the test set, the
features are built for all 1000 best translations of
each source sentence. Another essential element
is the word’s confidence labels (or so-called WCE
oracle labels) used to train the prediction model
as well as to judge the WCE results. They are
set by using TERp-A toolkit (Snover et al., 2008)
in one of the following classes: “I’ (insertions),
“S” (substitutions), “T” (stem matches), “Y” (syn-
onym matches), “P” (phrasal substitutions), “E”
(exact matches) and then simplified into binary
class: “G” (good word) or “B” (bad word) (Lu-
ong et al., 2013a).
Once having the prediction model built with all
features, we apply it on the test set (881 x 1000
best = 881000 sentences) and get needed WCE la-
bels. Figure 2 shows an example about the classi-
fication results for one sentence. Comparing with
the reference labels, we can point out easily the
correct classifications for “G” words (e.g. in case
of operation, added) and for “B” words (e.g. is,
have), as well as classification errors (e.g. a, com-
bat). According to the Precision (Pr), Recall (Rc)
and F-score (F) shown in Table 1, our WCE sys-
tem reaches very promising performance in pre-
dicting “G” label, and acceptable for “B” label.
These labels will be used to calculate our proposed
</bodyText>
<footnote confidence="0.977424">
2http://www.statmt.org/wmt10/
</footnote>
<page confidence="0.988087">
4
</page>
<figureCaption confidence="0.999225">
Figure 2: Example of our WCE classification results for one MT hypothesis
</figureCaption>
<figure confidence="0.945902772727273">
12
=
18
= 0.667
7
=
17
= 0.4118
features (section 3.3).
Label Pr(%) Rc(%) F(%)
Good (G) 84.36 91.22 87.65
Bad (B) 51.34 35.95 42.29
scores can be written as:
#good words
#words
#good bigrams
#bigrams
(3)
#good trigrams 3 = 0.1875
Table 1: Pr, Rc and F for “G” and “B” labels of
our WCE system
#trigrams = 16
</figure>
<subsectionHeader confidence="0.974671">
3.3 Proposed Features
</subsectionHeader>
<bodyText confidence="0.99989825">
Since the scores resulted from the WCE system
are for words, we have to synthesize them in sen-
tence level scores for integrating with the 14 de-
coder scores. Six proposed scores involve:
</bodyText>
<listItem confidence="0.99897725">
• The ratio of number of good words to total
number of words. (1 score)
• The ratio of number of good nouns (verbs) to
total number of nouns (verbs)3. (2 scores)
• The ratio of number of n consecutive good
word sequences to the total number of con-
secutive word sequences ; n=2, n=3 and n=4.
(3 scores)
</listItem>
<bodyText confidence="0.99467025">
For instance, in case of the hypothesis in Figure 2:
among the total of 18 words, we have 12 labeled
as “G”; and 7 out of 17 word pairs (bigram) are
labeled as “GG”, etc. Hence, some of the above
</bodyText>
<footnote confidence="0.916557">
3We decide not to experiment with adjectives, adverbs and
conjunctions since their number can be 0 in many cases.
</footnote>
<bodyText confidence="0.9997265">
With the features simply derived from WCE labels
and not from CRF model scores (i.e. the probabil-
ity p(G), p(B)) , we expect to spread out the eval-
uation up to the “oracle” setting, where the users
validate a word as “G” or “B” without providing
any confidence score.
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99824">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999952076923077">
As described in Section 3.2, our SMT system gen-
erates 1000-best list for each source sentence, and
among them, the best hypothesis was determined
by using the objective function based on 14 de-
coder scores, including: 7 reordering scores, 1 lan-
guage model score, 5 translation model scores and
1 word penalty score. Initially, all six additional
WCE-based scores are weighted as 1.0. Then,
two optimization methods: MERT and Margin
Infused Relaxed Algorithm (MIRA) (Watanabe
et al., 2007) are applied to optimize the weights of
all 20 scores of the re-ranker. In both methods, we
carry out a 2-fold cross validation on the N-best
</bodyText>
<page confidence="0.993028">
5
</page>
<table confidence="0.991913777777778">
Systems MERT MIRA
BLEU TER TERp-A BLEU TER TERp-A
BL 52.31 0.2905 0.3058 50.69 0.3087 0.3036
BL+OR 58.10 0.2551 0.2544 55.41 0.2778 0.2682
BL+WCE 52.77 0.2891 0.3025 51.01 0.3055 0.3012
WCE + 25% 53.45 0.2866 0.2903 51.33 0.3010 0.2987
WCE + 50% 55.77 0.2730 0.2745 53.63 0.2933 0.2903
WCE + 75% 56.40 0.2687 0.2669 54.35 0.2848 0.2822
Oracle BLEU score BLEU=60.48
</table>
<tableCaption confidence="0.993911">
Table 2: Translation quality of the baseline system (only decoder scores) and that with additional scores
from real “WCE” or “oracle” WCE system
</tableCaption>
<table confidence="0.999645857142857">
System MERT
Better Equivalent Worse
BL+WCE 159 601 121
BL+OR 517 261 153
WCE+25% 253 436 192
WCE+50% 320 449 112
WCE+75% 461 243 177
</table>
<tableCaption confidence="0.9796925">
Table 3: Quality comparison (measured by TER) between the baseline and two integrated systems in
details (How many sentences are improved, kept equivalent or degraded, out of 881 test sentences?
</tableCaption>
<bodyText confidence="0.99658625">
test set. In other words, we split our N-best test
set into two equivalent subsets: S1 and S2. Play-
ing the role of a development set, S1 will be used
to optimize the 20 weights for re-ranking S2 (and
vice versa). Finally two result subsets (new 1-best
after re-ranking process) are merged for evalua-
tion. To better acknowledge the impact of the pro-
posed scores, we calculate them not only using our
real WCE system, but also using an oracle WCE
(further called “WCE scores” and “oracle scores”,
respectively). To summarize, we experiment with
the three following systems:
</bodyText>
<listItem confidence="0.9994542">
• BL: Baseline SMT system with 14 above de-
coder scores
• BL+WCE: Baseline + 6 real WCE scores
• BL+OR: Baseline + 6 oracle WCE scores
(simulating an interactive scenario).
</listItem>
<subsectionHeader confidence="0.770215">
4.2 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999986472222222">
The translation quality of BL, BL+WCE and
BL+OR, optimized by MERT and MIRA method
are reported in Table 2. Meanwhile, Table 3
depicts in details the number of sentences in
the two integrated systems which outperform, re-
main equivalent or degrade the baseline hypoth-
esis (when match against the references, mea-
sured by TER). It can be observed from Table
2 that the integration of oracle scores signifi-
cantly boosts the MT output quality, measured
by all three metrics and optimized by both meth-
ods employed. We gained 5.79 and 4.72 points
in BLEU score, by MERT and MIRA (respec-
tively). With TER, BL+OR helps to gain 0.03
point in both two methods. Meanwhile, in case of
TERp-A, the improvement is 0.05 point for MERT
and 0.03 point for MIRA. It is worthy to mention
that the possibility of obtaining such oracle labels
is definitely doable through a human-interaction
scenario (which could be built from a tool like
PET (Post-Editing Tool) (Aziz et al., 2012) for
instance). In such an environment, once having
the hypothesis produced by the first pass (trans-
lation task), the human editor could simply click
on words considered as bad (B), the other words
being implicitly considered as correct (G).
Breaking down the analysis into sentence level,
as described on Table 3, BL+OR (MERT) yields
nearly 59% (517 over 881) better outputs than the
baseline and only 17% of worse ones. Further-
more, Table 2 shows that in case of our test set, op-
timizing by MERT is pretty more beneficial than
MIRA (we do not have a clear explanation of this
yet).
For more insightful understanding about WCE
scores’ acuteness, we make a comparison with
</bodyText>
<page confidence="0.998301">
6
</page>
<bodyText confidence="0.999982057142857">
the most possible optimal BLEU score that could
be obtained from the N-best list. Applying the
sentence-level BLEU+1 (Nakov et al., 2012) met-
ric over candidates in the list, we are able to se-
lect the one with highest score and aggregate all
of them in an oracle-best translation; the result-
ing performance obtained is 60.48. This score
accounts for a fact that the simulated interactive
scenario (BL+OR) lacks only 2.38 points (in case
of MERT) to be optimal and clearly overpass the
baseline (8.17 points below the best score).
The contribution of a real WCE system seems
more modest: BL+WCE marginally increases
BLEU scores of BL (0.46 gain in case of opti-
mizing by MERT and 0.32 by MIRA). For both
TER and TERp-A metric, the progressions are
also negligible. To verify the significance of this
result, we estimate the p-value between BLEU of
BL+WCE system and BLEU of baseline BL rely-
ing on Approximate Randomization (AR) method
(Clark et al., 2011) which indicates if the improve-
ment yielded by the optimized system is likely
to be generated again by some random processes
(randomized optimizers). After various optimizer
runs, we selected randomly 5 optimizer outputs,
perform the AR test and obtain a p-value of 0.01.
This result reveals that the improvement yielded
by BL+WCE is significative although small, orig-
inated from the contribution of WCE score, not
by any optimizer variance. This modest but pos-
itive change in BLEU score using WCE features,
encourages us to investigate and analyze further
about WCE scores’ impact, supposing WCE per-
formance is getting better. More in-depth analysis
is presented in the next section.
</bodyText>
<sectionHeader confidence="0.958962" genericHeader="method">
5 Further Understanding of WCE scores
</sectionHeader>
<subsectionHeader confidence="0.7935625">
role in N-best Re-ranking via
Improvement Simulation
</subsectionHeader>
<bodyText confidence="0.999850055555555">
We think it would be very interesting and useful
to answer the following question: do WCE scores
really effectively help to increase MT output qual-
ity when the WCE system is getting better and
better? To do this, our proposition is as follows:
firstly, by using the oracle labels, we filter out all
wrongly classified words in the test set and push
them into a temporary set, called T. Then, we cor-
rect randomly a percentage (25%, 50%, or 75%)
of labels in T. Finally, the altered T will be inte-
grated back with the correctly predicted part (by
the WCE system) in order to form a new “simu-
lated” result set. This strategy results in three “vir-
tual” WCE systems called “WCE+N%” (N=25,
50 or 75), which use 14 decoder scores and 6 “sim-
ulated” WCE scores. Table 4 shows the perfor-
mance of these systems in term of F score (%).
From each of the above systems, the whole exper-
</bodyText>
<table confidence="0.9987534">
System F(“G”) F(“B”) Overall F
WCE+25% 89.87 58.84 63.51
WCE+50% 93.21 73.09 76.11
WCE+75% 96.58 86.87 88.33
Oracle labels 100 100 100
</table>
<tableCaption confidence="0.9590005">
Table 4: The performances (Fscore) of simulated
WCE systems
</tableCaption>
<bodyText confidence="0.999886166666667">
imental setting is identical to what we did with the
original WCE and oracle systems: six scores are
built and combined with existing 14 system scores
for each hypothesis in the N-best list. After that,
MERT and MIRA methods are invoked to opti-
mize their weights, and finally the reordering is
performed thanks to these scores and appropriate
optimal weights. The translation quality measured
by BLEU, TER and TERp-A after re-ranking us-
ing “WCE+N%” (N=25,50,75) can be seen also
in Table 2. The number of translations which out-
perform, keep intact and decline in comparison to
the baseline are shown in Table 3 for MERT opti-
mization.
We note that all obtained scores fit our guess and
expectation: the better performance WCE system
reaches, the clearer its role in improving MT out-
put quality. Diminishing 25% of the wrongly pre-
dicted words leads to a gain 0.68 point (by MERT)
and 0.32 (by MIRA) in BLEU score. More sig-
nificant increases of BLEU 3.00 and BLEU 3.63
(MERT) can be achieved when prediction errors
are cut off up to 50% and 75%. Figure 3 presents
an overview of the results obtained and helps us
to predict the MT improvements expected if the
WCE system improves in the future. Table 5
shows several examples where WCE scores drive
SMT system to better reference-correlated hypoth-
esis. In the first example, the baseline generates
the hypothesis in which the source phrase “pour
sa part” remains untranslated. On the contrary,
WCE+50% overcomes this drawback by result-
ing in a correct translation phrase: “for his part”.
The latter translation needs only one edit opera-
tion (shift for “Bettencourt-Meyers”) to become
its reference. In example 2, BL+OR selects the
</bodyText>
<page confidence="0.998104">
7
</page>
<bodyText confidence="0.7846231">
Example 1 (from WCE+50%)
Source Pour sa part , l’ avocat de Franc¸oise Bettencourt-Meyers , Olivier
Metzner, s’ est f´elicit´e de la d´ecision du tribunal .
Hypothesis (Baseline SMT) The lawyer of Bettencourt-Meyers Franc¸oise , Olivier Metzner,
welcomed the court ’s decision.
Hypothesis (SMT+WCE For his part , the lawyer of Bettencourt-Meyers Franc¸oise ,
scores) Olivier Metzner, welcomed the court ’s decision.
Post-edition For his part , the lawyer of Franc¸oise Bettencourt-Meyers ,
Olivier Metzner, welcomed the court ’s decision.
Example 2 (from BL+OR)
Source Pour l’ otre , l’ accord risque “ de creuser la tombe d’ un tr`es
grand nombre de pme du secteur dans les 12 prochains mois ” .
Hypothesis (Baseline MT) For the otre the agreement is likely to deepen the grave of a very
large number of smes in the sector in the next 12 months ” .
Hypothesis (SMT+WCE For the otre agreement , the risk “ digging the grave of a very
scores) large number of medium-sized businesses in the next 12 months ”
.
Post-edition For the otre , the agreement risks “ digging the grave of a very
large number of small- and medium-sized businesses in the next
12 months ” .
</bodyText>
<figureCaption confidence="0.770292333333333">
Table 5: Examples of MT hypothesis before and after reranking using the additional scores from
WCE+50% (Example 1) and BL+OR (Example 2) system
Figure 3: Comparison of the performance of var-
ious systems: the integrations of WCE features,
which the quality increases gradually, lead to the
linear improvement of translation outputs.
</figureCaption>
<bodyText confidence="0.9971835">
better hypothesis, in which the phrases “creuser
la tombe” and ‘‘pme du secteur” are translated
into “digging the grave” and “medium-sized busi-
nesses”, respectively, better than those of the base-
line (“deepen the grave” and “smes in the sec-
tor”).
</bodyText>
<sectionHeader confidence="0.99722" genericHeader="conclusions">
6 Conclusions And Perspectives
</sectionHeader>
<bodyText confidence="0.99921796969697">
So far, the word confidence scores have been
exploited in several applications, e.g. post-
editing, sentence quality assessment or multiple
MT-system combination, yet very few studies (ex-
cept Bach et al. (2011) ) propose to investigate
them for boosting MT quality. Thus, this pa-
per proposed several features extracted from a
WCE system and combined them with existing de-
coder scores for re-ranking N-best lists. Our WCE
model is built using CRFs, on a variety of types of
features for the French - English SMT task. Due
to its limitations in predicting translation errors
(“B” label), WCE scores ensure only a modest im-
provement in translation quality over the baseline
SMT. Nevertheless, further experiments about the
simulation of WCE performance suggest that such
types of score contribute dramatically if they are
built from an accurate WCE system. They also
show that with the help of an “ideal” WCE, the
MT system reaches quite close to its most optimal
possible quality. These scores are totally indepen-
dent from the decoder, they can be seen as a way
to introduce lexical, syntactic and semantic infor-
mation (used for WCE) in a SMT pipeline.
As future work, we plan to focus on augmenting
our WCE performance using more linguistic fea-
tures as well as advanced techniques (feature se-
lection, Boosting method...). In the same time, we
would like to integrate the WCE scores in the de-
coder’s search graph to redirect the decoding pro-
cess (preliminary experiments, not reported here
yet, have shown that this is a very promising av-
enue of research).
</bodyText>
<page confidence="0.997379">
8
</page>
<sectionHeader confidence="0.99563" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999472344">
Wilker Aziz, Sheila C. M. de Sousa, and Lucia Specia. Pet:
a tool for post-editing and assessing machine translation.
In Proceedings of the Eight International Conference on
Language Resources and Evaluation (LREC’12), Istanbul,
Turkey, May 23-25 2012.
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. Goodness:
A method for measuring machine translation confidence.
In Proceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 211–219, Port-
land, Oregon, June 19-24 2011.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. Confidence estimation for machine trans-
lation. Technical report, JHU/CLSP Summer Workshop,
2003.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. Confidence estimation for machine trans-
lation. In Proceedings of COLING 2004, pages 315–321,
Geneva, April 2004.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith.
Better hypothesis testing for statistical machine transla-
tion: Controlling for optimizer instability. In Proceedings
of the Association for Computational Lingustics, 2011.
Kevin Duh and Katrin Kirchhoff. Beyond log-linear models:
Boosted minimum error rate training for n-best re-ranking.
In Proc. of ACL, Short Papers, 2008.
Mariano Felice and Lucia Specia. Linguistic features for
quality estimation. In Proceedings of the 7th Workshop on
Statistical Machine Translation, pages 96–103, Montreal,
Canada, June 7-8 2012.
Katrin Kirchhoff and Mei Yang. Improved language model-
ing for statistical machine translation. In Proceedings of
the ACL Workshop on Building and Using Parallel Texts,
pages 125–128, Ann Arbor, Michigan, June 2005.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. Moses: Open source toolkit for statisti-
cal machine translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguistics,
pages 177–180, Prague, Czech Republic, June 2007.
Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon. Practi-
cal very large scale crfs. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics,
pages 504–513, 2010.
Ngoc Quang Luong, Laurent Besacier, and Benjamin Lecou-
teux. Word confidence estimation and its integration in
sentence quality estimation for machine translation. In
Proceedings of The Fifth International Conference on
Knowledge and Systems Engineering (KSE 2013), Hanoi,
Vietnam, October 17-19 2013a.
Ngoc Quang Luong, Benjamin Lecouteux, and Laurent Be-
sacier. LIG system for WMT13 QE task: Investigating the
usefulness of features in word confidence estimation for
MT. In Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 396–391, Sofia, Bulgaria,
August 2013b. Association for Computational Linguistics.
Preslav Nakov, Francisco Guzman, and Stephan Vogel. Op-
timizing for sentence-level bleu+1 yields short transla-
tions. In Proceedings of COLING 2012, pages 1979–1994,
Mumbai, India, December 8 -15 2012.
Franz Josef Och. Minimum error rate training in statistical
machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics,
pages 160–167, July 2003.
Kishore Papineni, Salim Roukos, Todd Ard, and Wei-Jing
Zhu. Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics, 2002.
Kristen Parton, Nizar Habash, Kathleen McKeown, Gonzalo
Iglesias, and Adri`a de Gispert. Can automatic post-editing
make mt more meaningful? In Proceedings of the 16th
EAMT, pages 111–118, Trento, Italy, 28-30 May 2012.
M Potet, R Emmanuelle E, L Besacier, and H Blanchon.
Collection of a large database of french-english smt out-
put corrections. In Proceedings of the eighth interna-
tional conference on Language Resources and Evaluation
(LREC), Istanbul, Turkey, May 2012.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard
Schwartz. Terp system description. In MetricsMATR
workshop at AMTA, 2008.
Artem Sokolov, Guillaume Wisniewski, and Francois Yvon.
Non-linear n-best list reranking with few features. In Pro-
ceedings of AMTA, 2012.
Radu Soricut and Abdessamad Echihabi. Trustrank: Inducing
trust in automatic translations via ranking. In Proceedings
of the 48th ACL (Association for Computational Linguis-
tics), pages 612–621, Uppsala, Sweden, July 2010.
Andreas Stolcke. Srilm - an extensible language model-
ing toolkit. In Seventh International Conference on Spo-
ken Language Processing, pages 901–904, Denver, USA,
2002.
Nicola Ueffing and Hermann Ney. Word-level confidence
estimation for machine translation using phrased-based
translation models. In Proceedings of Human Lan-
guage Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages 763–
770, Vancouver, 2005.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. Con-
fidence measures for statistical machine translation. In
Proceedings of the MT Summit IX, pages 394–401, New
Orleans, LA, September 2003.
Ashish Venugopal, Andreas Zollmann, and Stephan Vogel.
An efficient two-pass approach to synchronous-cfg driven
statistical mt. In Proceedings of Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguistics,
April 2007.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. Online large-margin training for statistical ma-
chine translation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning,
pages 64–773,, Prague, Czech Republic, June 2007.
Deyi Xiong, Min Zhang, and Haizhou Li. Error detection
for statistical machine translation using linguistic features.
In Proceedings of the 48th Association for Computational
Linguistics, pages 604–611, Uppsala, Sweden, July 2010.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
Distributed language modeling for n-best list re-ranking.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing (EMNLP 2006),
pages 216–223, Sydney, July 2006.
</reference>
<page confidence="0.997091">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.058814">
<title confidence="0.999861">Confidence Estimation for SMT List Re-ranking</title>
<author confidence="0.968502">Ngoc-Quang Luong Laurent Besacier Benjamin Lecouteux</author>
<affiliation confidence="0.429424">LIG, Campus de</affiliation>
<address confidence="0.490544">41, Rue des</address>
<note confidence="0.326815">UJF - BP53, F-38041 Grenoble Cedex 9,</note>
<abstract confidence="0.984947">This paper proposes to use Word Confidence Estimation (WCE) information to MT outputs via list reranking. From the confidence label assigned for each word in the MT hypothesis, we add six scores to the baseline logmodel in order to re-rank the list. Firstly, the correlation between the WCE-based sentence-level scores and the conventional evaluation scores (BLEU, TER, TERp-A) is investigated. Then, the list re-ranking is evaluated over different WCE system performance levels: from our real and efficient WCE system 1st during last WMT 2013 to an oracle WCE (which simulates an interactive scenario where a user simply validates words of a MT hypothesis and the new output will be automatically re-generated). The results suggest that our real WCE system slightly (but significantly) improves the baseline while the oracle one extremely boosts it; and better WCE leads to better MT quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Wilker Aziz</author>
<author>Sheila C M de Sousa</author>
<author>Lucia Specia</author>
</authors>
<title>Pet: a tool for post-editing and assessing machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<location>Istanbul, Turkey,</location>
<marker>Aziz, de Sousa, Specia, 2012</marker>
<rawString>Wilker Aziz, Sheila C. M. de Sousa, and Lucia Specia. Pet: a tool for post-editing and assessing machine translation. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, May 23-25 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Bach</author>
<author>Fei Huang</author>
<author>Yaser Al-Onaizan</author>
</authors>
<title>Goodness: A method for measuring machine translation confidence.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>211--219</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="1607" citStr="Bach et al., 2011" startWordPosition="248" endWordPosition="251"> a MT hypothesis and the new output will be automatically re-generated). The results suggest that our real WCE system slightly (but significantly) improves the baseline while the oracle one extremely boosts it; and better WCE leads to better MT quality. 1 Introduction A number of methods to improve MT hypotheses after decoding have been proposed in the past, such as: post-editing, re-ranking or re-decoding. Post-editing (Parton et al., 2012) is a humaninspired task where the machine post edits translations in a second automatic pass. In re-ranking (Zhang et al., 2006; Duh and Kirchhoff, 2008; Bach et al., 2011), more features are used along with the multiple model scores for re-determining the 1-best among N-best list. Meanwhile, redecoding process (Venugopal et al., 2007) intervenes directly into the decoder’s search graph (e.g. adds more reward or penalty scores), driving it to another better path. This work aims at re-ranking the N-best list to improve MT quality. Generally, during the translation task, the decoder traverses through paths in its search space, computes the objective function values for them and outputs the one with highest score as the best hypothesis. Besides, those with lower sc</context>
<context position="6054" citStr="Bach et al. (2011)" startWordPosition="955" endWordPosition="958">coring function in a learning-to-rank paradigm, applying Boosting algorithm. Their gains on the WMT’{10, 11, 12} are shown modest yet consistent and higher than those based on linear scoring functions. Duh and Kirchhoff (2008) use Minimum Error Rate Training (MERT) (Och, 2003) as a weak learner and build their own solution, BoostedMERT, a highlyexpressive re-ranker created by voting among multiple MERT ones. Their proposed model dramatically beats the decoder’s log-linear model (43.7 vs. 42.0 BLEU) in IWSLT 2007 Arabic - English task. Applying solely goodness (the sentence confidence) scores, Bach et al. (2011) obtain very consistent TER reductions (0.7 and 0.6 on the dev and test set) after a 5-list re-ranking for their Arabic - English SMT hypotheses. This latter work is the one that is the most related to our paper. However, the major differences are: (1) our proposed sentence scores are computed based on word confidence labels; and (2) we perform an in-depth study of the use of WCE for N-best reranking and assess its usefulness in a simulated interactive scenario. 2.2 Word Confidence Estimation Confidence Estimation (CE) is the task of identifying the correct parts and detecting the translation </context>
<context position="7939" citStr="Bach et al., 2011" startWordPosition="1261" endWordPosition="1264">ed features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from the word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment context, and dependency structure (Bach et al., 2011) help to augment marginally in F-score as well as the Pearson correlation with human judgment. Other approaches are based on external features (Soricut and Echihabi, 2010; Felice and Specia, 2012) allowing to cope with various MT systems (e.g. statistical, rule based etc.). Among the numerous WCE applications, we consider its contribution in a specific step of SMT pipeline: N-best list reranking. Our WCE system and the proposed reranking features are presented in the next section. 3 Our Approach Our approach can be expressed in three steps: investigate the potential of using word-level score i</context>
<context position="27265" citStr="Bach et al. (2011)" startWordPosition="4553" endWordPosition="4556">ms: the integrations of WCE features, which the quality increases gradually, lead to the linear improvement of translation outputs. better hypothesis, in which the phrases “creuser la tombe” and ‘‘pme du secteur” are translated into “digging the grave” and “medium-sized businesses”, respectively, better than those of the baseline (“deepen the grave” and “smes in the sector”). 6 Conclusions And Perspectives So far, the word confidence scores have been exploited in several applications, e.g. postediting, sentence quality assessment or multiple MT-system combination, yet very few studies (except Bach et al. (2011) ) propose to investigate them for boosting MT quality. Thus, this paper proposed several features extracted from a WCE system and combined them with existing decoder scores for re-ranking N-best lists. Our WCE model is built using CRFs, on a variety of types of features for the French - English SMT task. Due to its limitations in predicting translation errors (“B” label), WCE scores ensure only a modest improvement in translation quality over the baseline SMT. Nevertheless, further experiments about the simulation of WCE performance suggest that such types of score contribute dramatically if </context>
</contexts>
<marker>Bach, Huang, Al-Onaizan, 2011</marker>
<rawString>Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. Goodness: A method for measuring machine translation confidence. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 211–219, Portland, Oregon, June 19-24 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
</authors>
<title>Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. Confidence estimation for machine translation.</title>
<date>2003</date>
<tech>Technical report, JHU/CLSP Summer Workshop,</tech>
<contexts>
<context position="7083" citStr="Blatz et al. (2003)" startWordPosition="1130" endWordPosition="1133">sess its usefulness in a simulated interactive scenario. 2.2 Word Confidence Estimation Confidence Estimation (CE) is the task of identifying the correct parts and detecting the translation errors in MT output. If the error is predicted for each word, this becomes WCE. The interesting uses of WCE include: pointing out the words that need to be corrected by the post-editor, telling readers about the reliability of a specific portion, and selecting the best segments among options from multiple translation systems for combination. Dealing with this problem, various approaches have been proposed: Blatz et al. (2003) combine several features using neural network and naive Bayes learning algorithms. One of the most effective feature combinations is the Word Posterior Probability (WPP) as suggested by Ueffing et al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from the word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et a</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, 2003</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. Confidence estimation for machine translation. Technical report, JHU/CLSP Summer Workshop, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
</authors>
<title>Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. Confidence estimation for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING 2004,</booktitle>
<pages>315--321</pages>
<location>Geneva,</location>
<contexts>
<context position="7353" citStr="Blatz et al., 2004" startWordPosition="1171" endWordPosition="1174">The interesting uses of WCE include: pointing out the words that need to be corrected by the post-editor, telling readers about the reliability of a specific portion, and selecting the best segments among options from multiple translation systems for combination. Dealing with this problem, various approaches have been proposed: Blatz et al. (2003) combine several features using neural network and naive Bayes learning algorithms. One of the most effective feature combinations is the Word Posterior Probability (WPP) as suggested by Ueffing et al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from the word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment context, and dependency structure (Bach et al., 2011) help to augme</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, 2004</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. Confidence estimation for machine translation. In Proceedings of COLING 2004, pages 315–321, Geneva, April 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Lingustics,</booktitle>
<contexts>
<context position="21762" citStr="Clark et al., 2011" startWordPosition="3626" endWordPosition="3629">re accounts for a fact that the simulated interactive scenario (BL+OR) lacks only 2.38 points (in case of MERT) to be optimal and clearly overpass the baseline (8.17 points below the best score). The contribution of a real WCE system seems more modest: BL+WCE marginally increases BLEU scores of BL (0.46 gain in case of optimizing by MERT and 0.32 by MIRA). For both TER and TERp-A metric, the progressions are also negligible. To verify the significance of this result, we estimate the p-value between BLEU of BL+WCE system and BLEU of baseline BL relying on Approximate Randomization (AR) method (Clark et al., 2011) which indicates if the improvement yielded by the optimized system is likely to be generated again by some random processes (randomized optimizers). After various optimizer runs, we selected randomly 5 optimizer outputs, perform the AR test and obtain a p-value of 0.01. This result reveals that the improvement yielded by BL+WCE is significative although small, originated from the contribution of WCE score, not by any optimizer variance. This modest but positive change in BLEU score using WCE features, encourages us to investigate and analyze further about WCE scores’ impact, supposing WCE per</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the Association for Computational Lingustics, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Beyond log-linear models: Boosted minimum error rate training for n-best re-ranking.</title>
<date>2008</date>
<booktitle>In Proc. of ACL, Short Papers,</booktitle>
<contexts>
<context position="1587" citStr="Duh and Kirchhoff, 2008" startWordPosition="244" endWordPosition="247">simply validates words of a MT hypothesis and the new output will be automatically re-generated). The results suggest that our real WCE system slightly (but significantly) improves the baseline while the oracle one extremely boosts it; and better WCE leads to better MT quality. 1 Introduction A number of methods to improve MT hypotheses after decoding have been proposed in the past, such as: post-editing, re-ranking or re-decoding. Post-editing (Parton et al., 2012) is a humaninspired task where the machine post edits translations in a second automatic pass. In re-ranking (Zhang et al., 2006; Duh and Kirchhoff, 2008; Bach et al., 2011), more features are used along with the multiple model scores for re-determining the 1-best among N-best list. Meanwhile, redecoding process (Venugopal et al., 2007) intervenes directly into the decoder’s search graph (e.g. adds more reward or penalty scores), driving it to another better path. This work aims at re-ranking the N-best list to improve MT quality. Generally, during the translation task, the decoder traverses through paths in its search space, computes the objective function values for them and outputs the one with highest score as the best hypothesis. Besides,</context>
<context position="5662" citStr="Duh and Kirchhoff (2008)" startWordPosition="891" endWordPosition="894"> Gigaword) for estimating Ngram probability. The quality of their Chinese - English hypotheses after the re-scoring process by using this LM is improved 4.8% (from BLEU 31.44 to 32.64, oracle score = 37.48). In one other direction, several authors propose to replace the current linear scoring function used by the decoder by more efficient functions. Sokolov et al. (2012) learn their non-linear scoring function in a learning-to-rank paradigm, applying Boosting algorithm. Their gains on the WMT’{10, 11, 12} are shown modest yet consistent and higher than those based on linear scoring functions. Duh and Kirchhoff (2008) use Minimum Error Rate Training (MERT) (Och, 2003) as a weak learner and build their own solution, BoostedMERT, a highlyexpressive re-ranker created by voting among multiple MERT ones. Their proposed model dramatically beats the decoder’s log-linear model (43.7 vs. 42.0 BLEU) in IWSLT 2007 Arabic - English task. Applying solely goodness (the sentence confidence) scores, Bach et al. (2011) obtain very consistent TER reductions (0.7 and 0.6 on the dev and test set) after a 5-list re-ranking for their Arabic - English SMT hypotheses. This latter work is the one that is the most related to our pa</context>
</contexts>
<marker>Duh, Kirchhoff, 2008</marker>
<rawString>Kevin Duh and Katrin Kirchhoff. Beyond log-linear models: Boosted minimum error rate training for n-best re-ranking. In Proc. of ACL, Short Papers, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariano Felice</author>
<author>Lucia Specia</author>
</authors>
<title>Linguistic features for quality estimation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 7th Workshop on Statistical Machine Translation,</booktitle>
<pages>96--103</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="8135" citStr="Felice and Specia, 2012" startWordPosition="1293" endWordPosition="1296">igned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment context, and dependency structure (Bach et al., 2011) help to augment marginally in F-score as well as the Pearson correlation with human judgment. Other approaches are based on external features (Soricut and Echihabi, 2010; Felice and Specia, 2012) allowing to cope with various MT systems (e.g. statistical, rule based etc.). Among the numerous WCE applications, we consider its contribution in a specific step of SMT pipeline: N-best list reranking. Our WCE system and the proposed reranking features are presented in the next section. 3 Our Approach Our approach can be expressed in three steps: investigate the potential of using word-level score in N-best list re-ranking, build the WCE system and 2 extract additional features to integrate with the existing log-linear model. 3.1 Investigating the correlation between “word quality” scores an</context>
</contexts>
<marker>Felice, Specia, 2012</marker>
<rawString>Mariano Felice and Lucia Specia. Linguistic features for quality estimation. In Proceedings of the 7th Workshop on Statistical Machine Translation, pages 96–103, Montreal, Canada, June 7-8 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Kirchhoff</author>
<author>Mei Yang</author>
</authors>
<title>Improved language modeling for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>125--128</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="4433" citStr="Kirchhoff and Yang (2005)" startWordPosition="693" endWordPosition="696">y proposed features. 1 Workshop on Humans and Computer-assisted Translation, pages 1–9, Gothenburg, Sweden, 26 April 2014. c�2014 Association for Computational Linguistics The experiments along with results and in-depth analysis of WCE scores’ contribution (as WCE system gets better) are presented in Section 4 and Section 5. The last section concludes the paper and points out some ongoing work. 2 Related Work 2.1 N-best List Re-ranking Walking through various related work concerning this issue, we observe some prominent ideas. The first attempt focuses on proposing additional Language Models. Kirchhoff and Yang (2005) train one word-based 4-gram model (with modified Kneser-Ney smoothing) and one factored trigram one, then combine them with seven decoder scores for re-ranking N-best lists of several SMT systems. Their proposed LMs increase the translation quality of the baselines (measured by BLEU score) from 21.6 to 22.0 (Finnish - English), or from 30.5 to 31.0 (Spanish - English). Meanwhile, Zhang et al. (2006) experiment a distributed LM where each server, among the total of 150, hosts a portion of the data and responses its client, allowing them to exploit an extremely large corpus (2.7 billion word En</context>
</contexts>
<marker>Kirchhoff, Yang, 2005</marker>
<rawString>Katrin Kirchhoff and Mei Yang. Improved language modeling for statistical machine translation. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 125–128, Ann Arbor, Michigan, June 2005.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="13428" citStr="Koehn et al., 2007" startWordPosition="2190" endWordPosition="2193">e • Semantic Features: number of word senses in WordNet. Interestingly, this feature set was also used in our English - Spanish WCE system which got the first 1This is our first-time experimented feature and does not appear in (Luong et al., 2013a) rank in WMT 2013 Quality Estimation Shared Task (Luong et al., 2013b). For building the WCE training and test sets, we use a dataset of 10,881 French sentences (Potet et al., 2012) , and apply a baseline SMT system to generate hypotheses (1000-best list). Our baseline SMT system (presented for WMT 2010 evaluation) keeps the Moses’s default setting (Koehn et al., 2007): log-linear model with 14 weighted feature functions. The translation model is trained on the Europarl and News parallel corpora of WMT102 evaluation campaign (1,638,440 sentences). The target language model is trained by the SRI language modeling toolkit (Stolcke, 2002) on the news monolingual corpus (48,653,884 sentences). Translators were then invited to correct MT outputs, giving us the same amount of post editions (Potet et al., 2012). The set of triples (source, hypothesis, post edition) is then divided into the training set (10000 first triples) and test set (881 remaining). To train t</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 177–180, Prague, Czech Republic, June 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lavergne</author>
<author>Olivier Capp´e</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Practical very large scale crfs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>504--513</pages>
<marker>Lavergne, Capp´e, Yvon, 2010</marker>
<rawString>Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon. Practical very large scale crfs. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504–513, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ngoc Quang Luong</author>
<author>Laurent Besacier</author>
<author>Benjamin Lecouteux</author>
</authors>
<title>Word confidence estimation and its integration in sentence quality estimation for machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of The Fifth International Conference on Knowledge and Systems Engineering (KSE 2013),</booktitle>
<location>Hanoi, Vietnam,</location>
<contexts>
<context position="11716" citStr="Luong et al., 2013" startWordPosition="1912" endWordPosition="1915">0). Basically, CRF computes the probability of the output sequence Y = (y1, y2, ..., yN) given the input sequence X = (x1, x2, ..., xN) by: 3 K pθ(Y |X) = Zθ(X)exp I:θkFk(X,Y ) k=1 (2) T where Fk(X,Y ) = Et=1 fk(yt−1, yt, xt); {fk} (k = 1, K) is a set of feature functions; {θk} (k = 1, K) are the associated parameter values; and Zθ(x) is the normalization function. In terms of features, a number of knowledge sources are employed for extracting them, resulting in the major types listed below. We briefly summarize them in this work, further details about total of 25 features can be referred in (Luong et al., 2013a). • Target Side: target word; bigram (trigram) backward sequences; number of occurrences • Source Side: source word(s) aligned to the target word • Alignment Context: the combinations of the target (source) word and all aligned source (target) words in the window ±2 • Word posterior probability • Pseudo-reference (Google Translate): whether the current word appears in the pseudo reference or not1? • Graph topology: number of alternative paths in the confusion set, maximum and minimum values of posterior probability distribution • Language model (LM) based: length of the longest sequence of t</context>
<context position="13055" citStr="Luong et al., 2013" startWordPosition="2126" endWordPosition="2129">nce wi−2wi−1wi appears in the target LM but the sequence wi−3wi−2wi−1wi does not, the n-gram value for wi will be 3. • Lexical Features: word’s Part-Of-Speech (POS); sequence of POS of all its aligned source words; POS bigram (trigram) backward sequences; punctuation; proper name; numerical • Syntactic Features: Null link; constituent label; depth in the constituent tree • Semantic Features: number of word senses in WordNet. Interestingly, this feature set was also used in our English - Spanish WCE system which got the first 1This is our first-time experimented feature and does not appear in (Luong et al., 2013a) rank in WMT 2013 Quality Estimation Shared Task (Luong et al., 2013b). For building the WCE training and test sets, we use a dataset of 10,881 French sentences (Potet et al., 2012) , and apply a baseline SMT system to generate hypotheses (1000-best list). Our baseline SMT system (presented for WMT 2010 evaluation) keeps the Moses’s default setting (Koehn et al., 2007): log-linear model with 14 weighted feature functions. The translation model is trained on the Europarl and News parallel corpora of WMT102 evaluation campaign (1,638,440 sentences). The target language model is trained by the </context>
<context position="14703" citStr="Luong et al., 2013" startWordPosition="2399" endWordPosition="2403">the 1-best hypotheses of the training set. For the test set, the features are built for all 1000 best translations of each source sentence. Another essential element is the word’s confidence labels (or so-called WCE oracle labels) used to train the prediction model as well as to judge the WCE results. They are set by using TERp-A toolkit (Snover et al., 2008) in one of the following classes: “I’ (insertions), “S” (substitutions), “T” (stem matches), “Y” (synonym matches), “P” (phrasal substitutions), “E” (exact matches) and then simplified into binary class: “G” (good word) or “B” (bad word) (Luong et al., 2013a). Once having the prediction model built with all features, we apply it on the test set (881 x 1000 best = 881000 sentences) and get needed WCE labels. Figure 2 shows an example about the classification results for one sentence. Comparing with the reference labels, we can point out easily the correct classifications for “G” words (e.g. in case of operation, added) and for “B” words (e.g. is, have), as well as classification errors (e.g. a, combat). According to the Precision (Pr), Recall (Rc) and F-score (F) shown in Table 1, our WCE system reaches very promising performance in predicting “G</context>
</contexts>
<marker>Luong, Besacier, Lecouteux, 2013</marker>
<rawString>Ngoc Quang Luong, Laurent Besacier, and Benjamin Lecouteux. Word confidence estimation and its integration in sentence quality estimation for machine translation. In Proceedings of The Fifth International Conference on Knowledge and Systems Engineering (KSE 2013), Hanoi, Vietnam, October 17-19 2013a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ngoc Quang Luong</author>
<author>Benjamin Lecouteux</author>
<author>Laurent Besacier</author>
</authors>
<title>LIG system for WMT13 QE task: Investigating the usefulness of features in word confidence estimation for MT.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>396--391</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="11716" citStr="Luong et al., 2013" startWordPosition="1912" endWordPosition="1915">0). Basically, CRF computes the probability of the output sequence Y = (y1, y2, ..., yN) given the input sequence X = (x1, x2, ..., xN) by: 3 K pθ(Y |X) = Zθ(X)exp I:θkFk(X,Y ) k=1 (2) T where Fk(X,Y ) = Et=1 fk(yt−1, yt, xt); {fk} (k = 1, K) is a set of feature functions; {θk} (k = 1, K) are the associated parameter values; and Zθ(x) is the normalization function. In terms of features, a number of knowledge sources are employed for extracting them, resulting in the major types listed below. We briefly summarize them in this work, further details about total of 25 features can be referred in (Luong et al., 2013a). • Target Side: target word; bigram (trigram) backward sequences; number of occurrences • Source Side: source word(s) aligned to the target word • Alignment Context: the combinations of the target (source) word and all aligned source (target) words in the window ±2 • Word posterior probability • Pseudo-reference (Google Translate): whether the current word appears in the pseudo reference or not1? • Graph topology: number of alternative paths in the confusion set, maximum and minimum values of posterior probability distribution • Language model (LM) based: length of the longest sequence of t</context>
<context position="13055" citStr="Luong et al., 2013" startWordPosition="2126" endWordPosition="2129">nce wi−2wi−1wi appears in the target LM but the sequence wi−3wi−2wi−1wi does not, the n-gram value for wi will be 3. • Lexical Features: word’s Part-Of-Speech (POS); sequence of POS of all its aligned source words; POS bigram (trigram) backward sequences; punctuation; proper name; numerical • Syntactic Features: Null link; constituent label; depth in the constituent tree • Semantic Features: number of word senses in WordNet. Interestingly, this feature set was also used in our English - Spanish WCE system which got the first 1This is our first-time experimented feature and does not appear in (Luong et al., 2013a) rank in WMT 2013 Quality Estimation Shared Task (Luong et al., 2013b). For building the WCE training and test sets, we use a dataset of 10,881 French sentences (Potet et al., 2012) , and apply a baseline SMT system to generate hypotheses (1000-best list). Our baseline SMT system (presented for WMT 2010 evaluation) keeps the Moses’s default setting (Koehn et al., 2007): log-linear model with 14 weighted feature functions. The translation model is trained on the Europarl and News parallel corpora of WMT102 evaluation campaign (1,638,440 sentences). The target language model is trained by the </context>
<context position="14703" citStr="Luong et al., 2013" startWordPosition="2399" endWordPosition="2403">the 1-best hypotheses of the training set. For the test set, the features are built for all 1000 best translations of each source sentence. Another essential element is the word’s confidence labels (or so-called WCE oracle labels) used to train the prediction model as well as to judge the WCE results. They are set by using TERp-A toolkit (Snover et al., 2008) in one of the following classes: “I’ (insertions), “S” (substitutions), “T” (stem matches), “Y” (synonym matches), “P” (phrasal substitutions), “E” (exact matches) and then simplified into binary class: “G” (good word) or “B” (bad word) (Luong et al., 2013a). Once having the prediction model built with all features, we apply it on the test set (881 x 1000 best = 881000 sentences) and get needed WCE labels. Figure 2 shows an example about the classification results for one sentence. Comparing with the reference labels, we can point out easily the correct classifications for “G” words (e.g. in case of operation, added) and for “B” words (e.g. is, have), as well as classification errors (e.g. a, combat). According to the Precision (Pr), Recall (Rc) and F-score (F) shown in Table 1, our WCE system reaches very promising performance in predicting “G</context>
</contexts>
<marker>Luong, Lecouteux, Besacier, 2013</marker>
<rawString>Ngoc Quang Luong, Benjamin Lecouteux, and Laurent Besacier. LIG system for WMT13 QE task: Investigating the usefulness of features in word confidence estimation for MT. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 396–391, Sofia, Bulgaria, August 2013b. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Francisco Guzman</author>
<author>Stephan Vogel</author>
</authors>
<title>Optimizing for sentence-level bleu+1 yields short translations.</title>
<date></date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<volume>8</volume>
<pages>1979--1994</pages>
<location>Mumbai, India,</location>
<marker>Nakov, Guzman, Vogel, </marker>
<rawString>Preslav Nakov, Francisco Guzman, and Stephan Vogel. Optimizing for sentence-level bleu+1 yields short translations. In Proceedings of COLING 2012, pages 1979–1994, Mumbai, India, December 8 -15 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="5713" citStr="Och, 2003" startWordPosition="902" endWordPosition="903"> Chinese - English hypotheses after the re-scoring process by using this LM is improved 4.8% (from BLEU 31.44 to 32.64, oracle score = 37.48). In one other direction, several authors propose to replace the current linear scoring function used by the decoder by more efficient functions. Sokolov et al. (2012) learn their non-linear scoring function in a learning-to-rank paradigm, applying Boosting algorithm. Their gains on the WMT’{10, 11, 12} are shown modest yet consistent and higher than those based on linear scoring functions. Duh and Kirchhoff (2008) use Minimum Error Rate Training (MERT) (Och, 2003) as a weak learner and build their own solution, BoostedMERT, a highlyexpressive re-ranker created by voting among multiple MERT ones. Their proposed model dramatically beats the decoder’s log-linear model (43.7 vs. 42.0 BLEU) in IWSLT 2007 Arabic - English task. Applying solely goodness (the sentence confidence) scores, Bach et al. (2011) obtain very consistent TER reductions (0.7 and 0.6 on the dev and test set) after a 5-list re-ranking for their Arabic - English SMT hypotheses. This latter work is the one that is the most related to our paper. However, the major differences are: (1) our pr</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, July 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ard</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="8912" citStr="Papineni et al., 2002" startWordPosition="1414" endWordPosition="1418">fic step of SMT pipeline: N-best list reranking. Our WCE system and the proposed reranking features are presented in the next section. 3 Our Approach Our approach can be expressed in three steps: investigate the potential of using word-level score in N-best list re-ranking, build the WCE system and 2 extract additional features to integrate with the existing log-linear model. 3.1 Investigating the correlation between “word quality” scores and other metrics Firstly, we investigate the correlation between sentence-level scores (obtained from WCE labels) and conventional evaluation scores (BLEU (Papineni et al., 2002), TER and TERp-A (Snover et al., 2008)). For each sentence, a word quality score (WQS) is calculated by: WQS = #&amp;quot;G&amp;quot;(good) words (1) #words In other words, we are trying to answer the following question: can the high percentage of “G” (good) words (predicted by WCE system) in a MT output ensure its possibility of having a better BLEU and low TER (TERp-A) value ? This investigation is a strong prerequisite for further experiments in order to check that WCE scores do not bring additional “noise” to the re-ranking process. In this experiment, we compute WQS over our entire French - English data se</context>
</contexts>
<marker>Papineni, Roukos, Ard, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ard, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristen Parton</author>
<author>Nizar Habash</author>
<author>Kathleen McKeown</author>
<author>Gonzalo Iglesias</author>
<author>Adri`a de Gispert</author>
</authors>
<title>Can automatic post-editing make mt more meaningful?</title>
<date>2012</date>
<booktitle>In Proceedings of the 16th EAMT,</booktitle>
<pages>111--118</pages>
<location>Trento,</location>
<marker>Parton, Habash, McKeown, Iglesias, de Gispert, 2012</marker>
<rawString>Kristen Parton, Nizar Habash, Kathleen McKeown, Gonzalo Iglesias, and Adri`a de Gispert. Can automatic post-editing make mt more meaningful? In Proceedings of the 16th EAMT, pages 111–118, Trento, Italy, 28-30 May 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Potet</author>
<author>R Emmanuelle E</author>
<author>L Besacier</author>
<author>H Blanchon</author>
</authors>
<title>Collection of a large database of french-english smt output corrections.</title>
<date>2012</date>
<booktitle>In Proceedings of the eighth international conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="13238" citStr="Potet et al., 2012" startWordPosition="2158" endWordPosition="2161">of all its aligned source words; POS bigram (trigram) backward sequences; punctuation; proper name; numerical • Syntactic Features: Null link; constituent label; depth in the constituent tree • Semantic Features: number of word senses in WordNet. Interestingly, this feature set was also used in our English - Spanish WCE system which got the first 1This is our first-time experimented feature and does not appear in (Luong et al., 2013a) rank in WMT 2013 Quality Estimation Shared Task (Luong et al., 2013b). For building the WCE training and test sets, we use a dataset of 10,881 French sentences (Potet et al., 2012) , and apply a baseline SMT system to generate hypotheses (1000-best list). Our baseline SMT system (presented for WMT 2010 evaluation) keeps the Moses’s default setting (Koehn et al., 2007): log-linear model with 14 weighted feature functions. The translation model is trained on the Europarl and News parallel corpora of WMT102 evaluation campaign (1,638,440 sentences). The target language model is trained by the SRI language modeling toolkit (Stolcke, 2002) on the news monolingual corpus (48,653,884 sentences). Translators were then invited to correct MT outputs, giving us the same amount of </context>
</contexts>
<marker>Potet, E, Besacier, Blanchon, 2012</marker>
<rawString>M Potet, R Emmanuelle E, L Besacier, and H Blanchon. Collection of a large database of french-english smt output corrections. In Proceedings of the eighth international conference on Language Resources and Evaluation (LREC), Istanbul, Turkey, May 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Terp system description.</title>
<date>2008</date>
<booktitle>In MetricsMATR workshop at AMTA,</booktitle>
<contexts>
<context position="8950" citStr="Snover et al., 2008" startWordPosition="1422" endWordPosition="1425">ranking. Our WCE system and the proposed reranking features are presented in the next section. 3 Our Approach Our approach can be expressed in three steps: investigate the potential of using word-level score in N-best list re-ranking, build the WCE system and 2 extract additional features to integrate with the existing log-linear model. 3.1 Investigating the correlation between “word quality” scores and other metrics Firstly, we investigate the correlation between sentence-level scores (obtained from WCE labels) and conventional evaluation scores (BLEU (Papineni et al., 2002), TER and TERp-A (Snover et al., 2008)). For each sentence, a word quality score (WQS) is calculated by: WQS = #&amp;quot;G&amp;quot;(good) words (1) #words In other words, we are trying to answer the following question: can the high percentage of “G” (good) words (predicted by WCE system) in a MT output ensure its possibility of having a better BLEU and low TER (TERp-A) value ? This investigation is a strong prerequisite for further experiments in order to check that WCE scores do not bring additional “noise” to the re-ranking process. In this experiment, we compute WQS over our entire French - English data set (total of 10,881 1- best translation</context>
<context position="14446" citStr="Snover et al., 2008" startWordPosition="2359" endWordPosition="2362">e amount of post editions (Potet et al., 2012). The set of triples (source, hypothesis, post edition) is then divided into the training set (10000 first triples) and test set (881 remaining). To train the WCE model, we extract all above features for words of the 1-best hypotheses of the training set. For the test set, the features are built for all 1000 best translations of each source sentence. Another essential element is the word’s confidence labels (or so-called WCE oracle labels) used to train the prediction model as well as to judge the WCE results. They are set by using TERp-A toolkit (Snover et al., 2008) in one of the following classes: “I’ (insertions), “S” (substitutions), “T” (stem matches), “Y” (synonym matches), “P” (phrasal substitutions), “E” (exact matches) and then simplified into binary class: “G” (good word) or “B” (bad word) (Luong et al., 2013a). Once having the prediction model built with all features, we apply it on the test set (881 x 1000 best = 881000 sentences) and get needed WCE labels. Figure 2 shows an example about the classification results for one sentence. Comparing with the reference labels, we can point out easily the correct classifications for “G” words (e.g. in </context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2008</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. Terp system description. In MetricsMATR workshop at AMTA, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Artem Sokolov</author>
<author>Guillaume Wisniewski</author>
<author>Francois Yvon</author>
</authors>
<title>Non-linear n-best list reranking with few features.</title>
<date>2012</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<contexts>
<context position="5411" citStr="Sokolov et al. (2012)" startWordPosition="854" endWordPosition="857">ish - English). Meanwhile, Zhang et al. (2006) experiment a distributed LM where each server, among the total of 150, hosts a portion of the data and responses its client, allowing them to exploit an extremely large corpus (2.7 billion word English Gigaword) for estimating Ngram probability. The quality of their Chinese - English hypotheses after the re-scoring process by using this LM is improved 4.8% (from BLEU 31.44 to 32.64, oracle score = 37.48). In one other direction, several authors propose to replace the current linear scoring function used by the decoder by more efficient functions. Sokolov et al. (2012) learn their non-linear scoring function in a learning-to-rank paradigm, applying Boosting algorithm. Their gains on the WMT’{10, 11, 12} are shown modest yet consistent and higher than those based on linear scoring functions. Duh and Kirchhoff (2008) use Minimum Error Rate Training (MERT) (Och, 2003) as a weak learner and build their own solution, BoostedMERT, a highlyexpressive re-ranker created by voting among multiple MERT ones. Their proposed model dramatically beats the decoder’s log-linear model (43.7 vs. 42.0 BLEU) in IWSLT 2007 Arabic - English task. Applying solely goodness (the sent</context>
</contexts>
<marker>Sokolov, Wisniewski, Yvon, 2012</marker>
<rawString>Artem Sokolov, Guillaume Wisniewski, and Francois Yvon. Non-linear n-best list reranking with few features. In Proceedings of AMTA, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>Trustrank: Inducing trust in automatic translations via ranking.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th ACL (Association for Computational Linguistics),</booktitle>
<pages>612--621</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="8109" citStr="Soricut and Echihabi, 2010" startWordPosition="1289" endWordPosition="1292">s extracted from the word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment context, and dependency structure (Bach et al., 2011) help to augment marginally in F-score as well as the Pearson correlation with human judgment. Other approaches are based on external features (Soricut and Echihabi, 2010; Felice and Specia, 2012) allowing to cope with various MT systems (e.g. statistical, rule based etc.). Among the numerous WCE applications, we consider its contribution in a specific step of SMT pipeline: N-best list reranking. Our WCE system and the proposed reranking features are presented in the next section. 3 Our Approach Our approach can be expressed in three steps: investigate the potential of using word-level score in N-best list re-ranking, build the WCE system and 2 extract additional features to integrate with the existing log-linear model. 3.1 Investigating the correlation betwee</context>
</contexts>
<marker>Soricut, Echihabi, 2010</marker>
<rawString>Radu Soricut and Abdessamad Echihabi. Trustrank: Inducing trust in automatic translations via ranking. In Proceedings of the 48th ACL (Association for Computational Linguistics), pages 612–621, Uppsala, Sweden, July 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Seventh International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, USA,</location>
<contexts>
<context position="13700" citStr="Stolcke, 2002" startWordPosition="2232" endWordPosition="2233">imation Shared Task (Luong et al., 2013b). For building the WCE training and test sets, we use a dataset of 10,881 French sentences (Potet et al., 2012) , and apply a baseline SMT system to generate hypotheses (1000-best list). Our baseline SMT system (presented for WMT 2010 evaluation) keeps the Moses’s default setting (Koehn et al., 2007): log-linear model with 14 weighted feature functions. The translation model is trained on the Europarl and News parallel corpora of WMT102 evaluation campaign (1,638,440 sentences). The target language model is trained by the SRI language modeling toolkit (Stolcke, 2002) on the news monolingual corpus (48,653,884 sentences). Translators were then invited to correct MT outputs, giving us the same amount of post editions (Potet et al., 2012). The set of triples (source, hypothesis, post edition) is then divided into the training set (10000 first triples) and test set (881 remaining). To train the WCE model, we extract all above features for words of the 1-best hypotheses of the training set. For the test set, the features are built for all 1000 best translations of each source sentence. Another essential element is the word’s confidence labels (or so-called WCE</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. Srilm - an extensible language modeling toolkit. In Seventh International Conference on Spoken Language Processing, pages 901–904, Denver, USA, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Word-level confidence estimation for machine translation using phrased-based translation models.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>763--770</pages>
<location>Vancouver,</location>
<contexts>
<context position="7377" citStr="Ueffing and Ney (2005)" startWordPosition="1175" endWordPosition="1178">of WCE include: pointing out the words that need to be corrected by the post-editor, telling readers about the reliability of a specific portion, and selecting the best segments among options from multiple translation systems for combination. Dealing with this problem, various approaches have been proposed: Blatz et al. (2003) combine several features using neural network and naive Bayes learning algorithms. One of the most effective feature combinations is the Word Posterior Probability (WPP) as suggested by Ueffing et al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from the word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment context, and dependency structure (Bach et al., 2011) help to augment marginally in F-score</context>
</contexts>
<marker>Ueffing, Ney, 2005</marker>
<rawString>Nicola Ueffing and Hermann Ney. Word-level confidence estimation for machine translation using phrased-based translation models. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 763– 770, Vancouver, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Klaus Macherey</author>
<author>Hermann Ney</author>
</authors>
<title>Confidence measures for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the MT Summit IX,</booktitle>
<pages>394--401</pages>
<location>New Orleans, LA,</location>
<contexts>
<context position="7291" citStr="Ueffing et al. (2003)" startWordPosition="1162" endWordPosition="1165">put. If the error is predicted for each word, this becomes WCE. The interesting uses of WCE include: pointing out the words that need to be corrected by the post-editor, telling readers about the reliability of a specific portion, and selecting the best segments among options from multiple translation systems for combination. Dealing with this problem, various approaches have been proposed: Blatz et al. (2003) combine several features using neural network and naive Bayes learning algorithms. One of the most effective feature combinations is the Word Posterior Probability (WPP) as suggested by Ueffing et al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from the word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment conte</context>
</contexts>
<marker>Ueffing, Macherey, Ney, 2003</marker>
<rawString>Nicola Ueffing, Klaus Macherey, and Hermann Ney. Confidence measures for statistical machine translation. In Proceedings of the MT Summit IX, pages 394–401, New Orleans, LA, September 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Stephan Vogel</author>
</authors>
<title>An efficient two-pass approach to synchronous-cfg driven statistical mt.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="1772" citStr="Venugopal et al., 2007" startWordPosition="273" endWordPosition="276">baseline while the oracle one extremely boosts it; and better WCE leads to better MT quality. 1 Introduction A number of methods to improve MT hypotheses after decoding have been proposed in the past, such as: post-editing, re-ranking or re-decoding. Post-editing (Parton et al., 2012) is a humaninspired task where the machine post edits translations in a second automatic pass. In re-ranking (Zhang et al., 2006; Duh and Kirchhoff, 2008; Bach et al., 2011), more features are used along with the multiple model scores for re-determining the 1-best among N-best list. Meanwhile, redecoding process (Venugopal et al., 2007) intervenes directly into the decoder’s search graph (e.g. adds more reward or penalty scores), driving it to another better path. This work aims at re-ranking the N-best list to improve MT quality. Generally, during the translation task, the decoder traverses through paths in its search space, computes the objective function values for them and outputs the one with highest score as the best hypothesis. Besides, those with lower scores can also be generated in a socalled N-best list. The decoder’s function consists of parameters from different models, such as translation, distortion, word pena</context>
</contexts>
<marker>Venugopal, Zollmann, Vogel, 2007</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, and Stephan Vogel. An efficient two-pass approach to synchronous-cfg driven statistical mt. In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, April 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
</authors>
<title>Hajime Tsukada, and Hideki Isozaki. Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>64--773</pages>
<location>Prague, Czech Republic,</location>
<marker>Watanabe, Suzuki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. Online large-margin training for statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 64–773,, Prague, Czech Republic, June 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Error detection for statistical machine translation using linguistic features.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Association for Computational Linguistics,</booktitle>
<pages>604--611</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="7692" citStr="Xiong et al. (2010)" startWordPosition="1226" endWordPosition="1229">al. (2003) combine several features using neural network and naive Bayes learning algorithms. One of the most effective feature combinations is the Word Posterior Probability (WPP) as suggested by Ueffing et al. (2003) associated with IBM-model based features (Blatz et al., 2004). Ueffing and Ney (2005) propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted from the word-aligned bilingual training corpus. The confidence value of each word is then computed by summing over all phrase pairs in which the target part contains this word. Xiong et al. (2010) integrate target word’s Part-Of-Speech (POS) and train them by Maximum Entropy Model, allowing significative gains in comparison to WPP features. The novel features from source side, alignment context, and dependency structure (Bach et al., 2011) help to augment marginally in F-score as well as the Pearson correlation with human judgment. Other approaches are based on external features (Soricut and Echihabi, 2010; Felice and Specia, 2012) allowing to cope with various MT systems (e.g. statistical, rule based etc.). Among the numerous WCE applications, we consider its contribution in a specifi</context>
</contexts>
<marker>Xiong, Zhang, Li, 2010</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. Error detection for statistical machine translation using linguistic features. In Proceedings of the 48th Association for Computational Linguistics, pages 604–611, Uppsala, Sweden, July 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Almut Silja Hildebrand</author>
<author>Stephan Vogel</author>
</authors>
<title>Distributed language modeling for n-best list re-ranking.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>216--223</pages>
<location>Sydney,</location>
<contexts>
<context position="1562" citStr="Zhang et al., 2006" startWordPosition="240" endWordPosition="243">enario where a user simply validates words of a MT hypothesis and the new output will be automatically re-generated). The results suggest that our real WCE system slightly (but significantly) improves the baseline while the oracle one extremely boosts it; and better WCE leads to better MT quality. 1 Introduction A number of methods to improve MT hypotheses after decoding have been proposed in the past, such as: post-editing, re-ranking or re-decoding. Post-editing (Parton et al., 2012) is a humaninspired task where the machine post edits translations in a second automatic pass. In re-ranking (Zhang et al., 2006; Duh and Kirchhoff, 2008; Bach et al., 2011), more features are used along with the multiple model scores for re-determining the 1-best among N-best list. Meanwhile, redecoding process (Venugopal et al., 2007) intervenes directly into the decoder’s search graph (e.g. adds more reward or penalty scores), driving it to another better path. This work aims at re-ranking the N-best list to improve MT quality. Generally, during the translation task, the decoder traverses through paths in its search space, computes the objective function values for them and outputs the one with highest score as the </context>
<context position="4836" citStr="Zhang et al. (2006)" startWordPosition="759" endWordPosition="762">ork 2.1 N-best List Re-ranking Walking through various related work concerning this issue, we observe some prominent ideas. The first attempt focuses on proposing additional Language Models. Kirchhoff and Yang (2005) train one word-based 4-gram model (with modified Kneser-Ney smoothing) and one factored trigram one, then combine them with seven decoder scores for re-ranking N-best lists of several SMT systems. Their proposed LMs increase the translation quality of the baselines (measured by BLEU score) from 21.6 to 22.0 (Finnish - English), or from 30.5 to 31.0 (Spanish - English). Meanwhile, Zhang et al. (2006) experiment a distributed LM where each server, among the total of 150, hosts a portion of the data and responses its client, allowing them to exploit an extremely large corpus (2.7 billion word English Gigaword) for estimating Ngram probability. The quality of their Chinese - English hypotheses after the re-scoring process by using this LM is improved 4.8% (from BLEU 31.44 to 32.64, oracle score = 37.48). In one other direction, several authors propose to replace the current linear scoring function used by the decoder by more efficient functions. Sokolov et al. (2012) learn their non-linear s</context>
</contexts>
<marker>Zhang, Hildebrand, Vogel, 2006</marker>
<rawString>Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel. Distributed language modeling for n-best list re-ranking. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 216–223, Sydney, July 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.998313">
Using Feature Structures to Improve Verb Translation in
English-to-German Statistical MT
</title>
<author confidence="0.946589">
Philip Williams* Philipp Koehn*†
</author>
<email confidence="0.54363">
p.j.williams-2@sms.ed.ac.uk pkoehn@inf.ed.ac.uk
</email>
<affiliation confidence="0.9833465">
School of Informatics* Center for Speech and Language Processing†
University of Edinburgh The Johns Hopkins University
</affiliation>
<sectionHeader confidence="0.979815" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998918">
SCFG-based statistical MT models have
proven effective for modelling syntactic
aspects of translation, but still suffer prob-
lems of overgeneration. The production
of German verbal complexes is particu-
larly challenging since highly discontigu-
ous constructions must be formed con-
sistently, often from multiple independent
rules. We extend a strong SCFG-based
string-to-tree model to incorporate a rich
feature-structure based representation of
German verbal complex types and com-
pare verbal complex production against
that of the reference translations, finding a
high baseline rate of error. By developing
model features that use source-side infor-
mation to influence the production of ver-
bal complexes we are able to substantially
improve the type accuracy as compared to
the reference.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9991945">
Syntax-based models of statistical machine trans-
lation (SMT) are becoming increasingly compet-
itive against state-of-the-art phrase-based mod-
els, even surpassing them for some language
pairs. The incorporation of syntactic structure
has proven effective for modelling reordering phe-
nomena and improving the fluency of target out-
put, but these models still suffer from problems of
overgeneration.
One example is the production of German ver-
bal constructions. This is particularly challenging
for SMT models since highly discontiguous con-
structions must be formed consistently, often from
multiple independent rules. Whilst the model’s
</bodyText>
<figure confidence="0.564645">
TOP-S
</figure>
<figureCaption confidence="0.89782">
Figure 1: Alignment graph for a sentence pair
from the training data. The boxes indicate the
components of the target-side verbal complex: a
main verb, fehlgeschlagen, and an auxiliary, ist.
</figureCaption>
<bodyText confidence="0.930953826086957">
grammar may contain rules in which a complete
multi-word verb translation is captured in a single
discontiguous rule, in practice many verb transla-
tions are incompletely or inconsistently produced.
There are many routes by which ill-formed con-
structions come to be licensed by the model, none
of which is easy to address. For instance, Figure 1
shows an example from our training data in which
a missing alignment link (between has and ist) al-
lows the extraction of rules that translate has failed
to the incomplete fehlgeschlagen.
Even with perfect word alignments, the ex-
tracted rules may not include sufficient context to
ensure the overall grammaticality of a derivation.
The extent of this problem will depend partly on
the original treebank annotation style, which typi-
cally will not have been designed with translation
in mind. The problem may be further exacerbated
by errors during automatic parsing.
In this paper, we address the problem by fo-
cusing on the derivation process. We extend a
strong SCFG-based string-to-tree model to incor-
porate a rich feature-structure based representation
</bodyText>
<figure confidence="0.997802">
KON
doch
yet
VAFIN
ist
this policy
PDAT
diese
S-TOP
NP-SB
Strategie
NN
has failed
fehlgeschlagen
VP-OC
VVPP
PUNC.
.
.
</figure>
<page confidence="0.995544">
21
</page>
<note confidence="0.9907975">
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 21–29,
Gothenburg, Sweden, April 27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999895">
of German verbal complex types. During decod-
ing, our model composes type values for every
clause. When we compare these values against
those of the reference translations, we find a high
baseline rate of error (either incomplete or mis-
matching values). By developing model features
that use source-side information to influence the
production of verbal complexes we are able to sub-
stantially improve the type accuracy as compared
to the reference.
</bodyText>
<sectionHeader confidence="0.985842" genericHeader="introduction">
2 Verbal Complex Structures
</sectionHeader>
<bodyText confidence="0.99987525">
Adopting the terminology of Gojun and
Fraser (2012), we use the term ‘verbal com-
plex’ to mean a main verb and any associated
auxiliaries within a single clause.
</bodyText>
<subsectionHeader confidence="0.993782">
2.1 Feature Structures
</subsectionHeader>
<bodyText confidence="0.999925571428571">
We use feature structures to represent the under-
lying grammatical properties of German verbal
complexes. The feature structures serve two main
functions: the first is to specify a type for the
verbal complex. The types describe clause-level
properties and are defined along four dimensions:
1. tense (present, past, perfect, pluperfect, future,
future perfect), 2. voice (active, werden-passive,
sein-passive), 3. mood (indicative, subjunctive I,
subjunctive II), and 4. auxiliary modality (modal,
non-modal).
The second function is to restrict the choice of
individual word forms that are allowed to com-
bine within a given type. For example, a fea-
ture structure value for the verbal complex hat
... gespielt belongs to the perfect, active, indica-
tive, non-modal type. Additionally, it specifies
that for this type, the verbal complex comprises
exactly two verbs: one is a finite, indicative form
of the auxiliary haben or sein, the other is a past-
participle.
</bodyText>
<subsectionHeader confidence="0.999661">
2.2 The Lexicon
</subsectionHeader>
<bodyText confidence="0.9977306">
Our model uses a lexicon that maps each German
verb in the target-side terminal vocabulary to a set
of features structures. Each feature structure con-
tains two top-level features: POS, a part-of-speech
feature, and VC, a verbal complex feature of the
form described above.
Since a verbal complex can comprise multiple
individual verbs, the lexicon entries include partial
VC structures. The full feature structure values are
composed through unification during decoding.
</bodyText>
<equation confidence="0.9976435">
VP-OC —* ( rebuilt, wieder aufgebaut )
( VP-OC VC ) = ( aufgebaut VC )
( aufgebaut POS ) = VVPP
S-TOP —* ( X1 have X2 been X3 ,
PP-MO1 wurde NP-SB2 VP-OC3 )
( S-TOP VC ) = ( wurde VC )
( S-TOP VC ) = (VP-OC VC )
( wurde POS ) = VAFIN
</equation>
<figureCaption confidence="0.981032">
Figure 2: SCFG rules with constraints
</figureCaption>
<bodyText confidence="0.9997703">
The lexicon’s POS values are derived from the
parse trees on the target-side of the training data.
The VC values are assigned according to POS value
from a small set of hand-written feature struc-
tures. Every main verb is assigned VC values from
one of three possible groups, selected according to
whether the verb is finite, a past-participle, or an
infinitive. For the closed class of modal and non-
modal auxiliary verbs, VC values were manually
assigned.
</bodyText>
<sectionHeader confidence="0.957556" genericHeader="method">
3 The Grammar
</sectionHeader>
<bodyText confidence="0.999971111111111">
Our baseline translation model is learned from a
parallel corpus with automatically-derived word
alignments. In the literature, string-to-tree trans-
lation models are typically based on either syn-
chronous context-free grammars (SCFGs) (as in
Chiang et al. (2007)) or tree transducers (as in Gal-
ley et al. (2004)). In this work, we use an SCFG-
based model but our extensions are applicable in
both cases.
Following Williams and Koehn (2011), each
rule of our grammar is supplemented with
a (possibly-empty) set of PATR-II-style identi-
ties (Shieber, 1984). Figure 2 shows two example
rules with identities. The identities should be in-
terpreted as constraints that the feature structures
of the corresponding rule elements are compatible
under unification. During decoding, this imposes
a hard constraint on rule application.
</bodyText>
<subsectionHeader confidence="0.999085">
3.1 Identity Extraction
</subsectionHeader>
<bodyText confidence="0.8267466">
The identities are learned using the following pro-
cedure:
1. The syntax of the German parse trees is used
to identify verbal complexes and label the
participating verb and clause nodes.
</bodyText>
<page confidence="0.999061">
22
</page>
<figureCaption confidence="0.9427985">
Figure 3: Alignment graph for a sentence pair
from the training data. The target sentence has
a single verbal complex. Participating nodes are
indicated by the boxes.
</figureCaption>
<listItem confidence="0.9936005">
2. Grammar rule extraction is extended to gen-
erate identities between VC values when an
SCFG rule contains two or more nodes from
a common verbal complex.
3. POS identities are added for terminals that ap-
pear in VC identities.
</listItem>
<bodyText confidence="0.999965785714286">
Figure 3 shows a sentence-pair from the train-
ing data with the verbal complex highlighted.
The rules in Figure 2 were extracted from this
sentence-pair.
Crucially, in step 2 of the extraction procedure
the identities can be added to SCFG rules that
cover only part of a verbal complex. For example,
the first rule of Figure 2 includes the main verb but
not the auxiliary. On application of this rule, the
partial VC value is propagated from the main verb
to the root. The second rule in Figure 2 identifies
the VC value of an auxiliary with the VC value of
a VP-OC subderivation (such as the subderivation
produced by applying the first rule).
</bodyText>
<sectionHeader confidence="0.991636" genericHeader="method">
4 Source-side Features
</sectionHeader>
<bodyText confidence="0.99986">
Since Och and Ney (2002), most SMT models
have been defined as a log-linear sum of weighted
feature functions. In this section, we define two
verbal-complex-specific feature functions. In or-
der to do so, we first describe ‘clause projection,’
a simple source-syntactic restriction on decoding.
We then describe our heuristic method of obtain-
ing probability estimates for a target verbal com-
plex value given the source clause.
</bodyText>
<subsectionHeader confidence="0.98417">
4.1 Clause Projection
</subsectionHeader>
<bodyText confidence="0.999911454545455">
Our feature functions assume that we have an
alignment from source-side clauses to target
clauses. In order to satisfy this requirement, we
adopt a simple restriction that declarative clauses
(both main and embedded) on the source-side
must be translated as clauses on the target-side.
This is clearly an over-simplification from a lin-
guistic perspective but it appears not to harm trans-
lation quality in practice. Table 1 shows small
gains in BLEU score over our baseline system
with this restriction.
</bodyText>
<table confidence="0.997775">
Test Set Baseline Clause Proj.
newstest2008 15.7 15.8 (+0.1)
newstest2009 14.9 15.0 (+0.1)
newstest2010 16.5 16.8 (+0.3)
newstest2011 15.4 15.5 (+0.1)
</table>
<tableCaption confidence="0.779425666666667">
Table 1: Results with and without clause projec-
tion (baseline tuning weights are used for clause
projection)
</tableCaption>
<bodyText confidence="0.92194">
Clause projection is implemented as follows:
</bodyText>
<listItem confidence="0.985344857142857">
1. The input sentence is parsed and a set
of clause spans is extracted according to
the 1-best parse. We use the Berkeley
parser (Petrov and Klein, 2007), which is
trained on the Penn Treebank and so we base
our definition of a declarative clause on the
treebank annotation guidelines.
2. We modify the decoder to produce deriva-
tions in chart cells only if the cell span is
consistent with the set of clause spans (i.e.
if source span [i,j] is a clause span then no
derivation is built over span [m,n] where i &lt;
m ≤ j and n &gt; j, etc.)
3. We modify the decoder so that grammar rules
</listItem>
<bodyText confidence="0.995601">
can only be applied over clause spans if they
have a clause label (‘S’ or ‘CS’, since the
parser we use is trained on the Tiger tree-
bank).
</bodyText>
<subsectionHeader confidence="0.996905">
4.2 Verbal Complex Probabilities
</subsectionHeader>
<bodyText confidence="0.9999731">
When translating a clause, the source-side verbal
complex will often provide sufficient information
to select a reasonable type for the target verbal
complex, or to give preferences to a few candi-
dates. By matching up source-side and target-side
verbal complexes we estimate co-occurrence fre-
quencies in the training data. To do this for all
pairs in the training data, we would need to align
clauses between the source and target training sen-
tences. However, it is not crucial that we identify
</bodyText>
<figure confidence="0.988662315789474">
S-TOP
only recently have these been rebuilt
ADV
erst
APPR ADJA
in j¨ungster
PP-MO
Zeit
NN
VAFIN
wurde
NP-SB
diese
PDS
wieder
ADV
VP-OC
aufgebaut
VVPP
</figure>
<page confidence="0.994063">
23
</page>
<bodyText confidence="0.999825782608696">
every last verbal complex and so we simplify the
task by restricting training data to sentence pairs in
which both source and target sentences are declar-
ative sentences, making the assumption that the
main clause of the source sentence aligns with the
main clause of the target.
We represent source-side verbal complexes
with a label that is the string of verbs and
particles and their POS tags in the order that
they occur in the clause, e.g. plays VBZ,
is addressing VBZ VBG. The target-side
feature structures are generated by identifying
verbal complex nodes in the training data parse
trees (as in Section 3.1) and then unifying the
corresponding feature structures from the lexicon.
Many source verbal complex labels exhibit a
strong co-occurrence preference for a particular
target type. For example, Table 2 shows the
three most frequent feature structure values for
the target-side clause when the source label is
is closed VBZ VBN. The most frequent value
corresponds to a non-modal, sein-passive con-
struction in the present tense and indicative mood.
</bodyText>
<table confidence="0.999122">
RF F-Structure
0.841  &amp;quot;  
0.045 AUX  
0.034 &amp;quot;LEMMA sein ## 
. . .  FIN MOOD indicative 
  
 TENSE present  
 NON-FIN 1PP/SP h 
PP [LEMMA *�iJ 
FIN 
&amp;quot; h FULL�LEMMA sein�i #
NON-FIN none
&amp;quot;LEMMA werden
 FIN AUX MOOD indicative
 &amp;quot;##
 TENSE present

 PP ~LEMMA *
  �  
    
WORDEN none
 NON-FIN WPP  WERDEN none  
SEIN none
. . .
</table>
<tableCaption confidence="0.74025">
Table 2: Observed values and relative frequencies
(RF) for is closed, which was observed 44 times in
the training data.
</tableCaption>
<subsectionHeader confidence="0.998472">
4.3 Feature Functions
</subsectionHeader>
<bodyText confidence="0.939872529411765">
As with the baseline features, our verbal complex-
specific feature functions are evaluated for every
rule application ri of the synchronous derivation.
Like the language model feature, they are non-
local features and so cannot be pre-computed. Un-
like the baseline features, their value depends on
whether the source span that the rule is applied to
is a declarative clause or not.
Both features are defined in terms of X, the
verbal complex feature structure value of the sub-
derivation at rule application ri.
The first feature function, f(ri), uses the source
verb label, l, and the probability estimate, P(X|l),
learned from the training data:
P(X|l) if ri covers a clause span
with verb label l
and cl ≥ cmin
</bodyText>
<sectionHeader confidence="0.634674" genericHeader="method">
1 otherwise
</sectionHeader>
<bodyText confidence="0.999812285714286">
The probability estimates are not used for scoring
if the number of training observations falls below
a threshold, cmin. We use a threshold of 10 in ex-
periments.
The second feature function, g(ri), is simpler:
it penalizes the absence of a target-side finite verb
when translating a source declarative clause:
</bodyText>
<listItem confidence="0.259289666666667">
exp(1) if ri covers a clause span
and X has no finite verb
1 otherwise
</listItem>
<bodyText confidence="0.996759875">
Unlike f, which requires the verb label to have
been observed a number of times during training,
g is applied to all source spans that cover a declar-
ative clause.
Dropped finite verbs are a frequent problem in
our baseline model and this feature was motivated
by an early version of the analysis presented in
Section 5.3.
</bodyText>
<sectionHeader confidence="0.995182" genericHeader="method">
5 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.99979">
In preliminary experiments, we found that changes
in translation quality resulting from our verb trans-
lation features were difficult to measure using
BLEU. In the following experiments, we mea-
sure accuracy by comparing verbal complex val-
ues against feature structures derived from the ref-
erence sentences.
</bodyText>
<subsectionHeader confidence="0.971288">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.9989486">
Our experiments use the GHKM-based string-to-
tree pipeline implemented in Moses (Koehn et al.,
2007; Williams and Koehn, 2012). We extend a
conventional baseline model using the constraints
and feature functions described earlier.
</bodyText>
<figure confidence="0.851311625">



f(ri) =



g(ri) =
</figure>
<page confidence="0.988127">
24
</page>
<table confidence="0.999961">
Data Set F Reference F Baseline Total Hard Constraint
(MC count) E Total E F E Total
Dev 95.6% 4.4% 100.0% 86.1% 13.9% 100.0% 87.6% 12.4% 100.0%
(633) 637 29 666 545 88 633 559 79 638
Test 92.2% 7.8% 100.0% 83.5% 16.5% 100.0% 85.4% 14.6% 100.0%
(2445) 2439 206 2645 2034 403 2437 2096 359 2455
</table>
<tableCaption confidence="0.817898666666667">
Table 3: Counts of main clause VC structures that are present and contain at least a finite verb (F) versus
those that are empty or absent (E). Declarative main clause counts (MC count) are given for each input
set. Counts for the three test sets are aggregated.
</tableCaption>
<bodyText confidence="0.999636461538462">
We extracted a translation grammar using all
English-German parallel data from the WMT
2012 translation task (Callison-Burch et al., 2012),
a total of 2.0M sentence pairs. We used all of the
WMT 2012 monolingual German data to train a
5-gram language model.
The baseline system uses the feature functions
described in Williams and Koehn (2012). The
feature weights were tuned on the WMT new-
stest2008 development set using MERT (Och,
2003). We use the newstest2009, newstest2010,
and newstest2011 test sets for evaluation. The de-
velopment and test sets all use a single reference.
</bodyText>
<subsectionHeader confidence="0.997572">
5.2 Main Clause Verb Errors
</subsectionHeader>
<bodyText confidence="0.999978446153846">
When translating a declarative main clause, the
translation should usually also be a declarative
main clause – that is, it should usually contain at
least a finite verb. From manually inspecting the
output it is clear that verb dropping is a common
source of translation error in our baseline system.
By making the assumption that a declarative main
clause should always be translated to a declara-
tive main clause, we can use the absence of a finite
verb as a test for translation error.
By evaluating identities, our decoder now gen-
erates a trace of verbal complex feature structures.
We obtain a reference trace by applying the same
process of verbal complex identification and fea-
ture structure unification to a parse of our refer-
ence data. Given these two traces, we compare the
presence or absence of main clause finite-verbs in
the baseline and reference.
Since we do not have alignments between the
clause nodes of the test and reference trees, we re-
strict our analysis to a simpler version of this task:
the translation of declarative input sentences that
contain only a single clause. To select test sen-
tences, we first parse the source-side of the tuning
and test sets. Filtering out sentences that are not
declarative or that contain multiple clauses leaves
633, 699, 793, and 953 input sentences for new-
stest2008, 2009, 2010, and 2011, respectively.
Our baseline system evaluates constraints in or-
der to generate a trace of feature structures but
constraint failures are allowed and hypotheses are
retained. Our hard constraint system discards all
hypotheses for which the constraints fail. The f
and g feature functions are not used in these ex-
periments.
For all main clause nodes in the output tree,
we count the number of feature structure values
that contain finite verbs and are complete versus
the number that are either incomplete or absent.
Since constraint failure results in the production
of empty feature structures, incompatible verbal
combinations do not contribute to the finite verb
total even if a finite verb is produced. We com-
pare the counts of clause nodes with empty fea-
ture structures for these two systems against those
of the reference set.
Table 3 shows total clause counts for the ref-
erence, baseline, and hard constraint system (the
‘total’ columns). For each system, we record how
frequently a complete feature structure containing
at least a finite verb is present (the F columns) or
not (E).
As expected, the finite verb counts for the refer-
ence translations closely match the counts for the
source sentences. The reference sets also contain
verb-less clauses (accounting for 4.4% and 7.8%
of the total clause counts for the dev and test sets).
Verb-less clauses are common in the training data
and so it is not surprising to find them in the refer-
ence sets.
Our baseline and hard constraint systems both
fail to produce complete feature structures for a
high proportion of test sentences. Table 4 shows
the proportion of single-clause declarative source
sentences for which the translation trace does not
</bodyText>
<page confidence="0.995586">
25
</page>
<bodyText confidence="0.9980205">
include a complete feature structure. As well as
suggesting a high level of baseline failure, these
results suggest that using constraints alone is in-
sufficient.
</bodyText>
<table confidence="0.9994732">
Test set Ref. Baseline HC
newstest2008 0.0% 13.9% 11.7%
newstest2009 0.6% 18.6% 16.0%
newstest2010 0.0% 14.5% 12.5%
newstest2011 1.4% 17.4% 14.4%
</table>
<tableCaption confidence="0.998037">
Table 4: Proportion of declarative single-clause
</tableCaption>
<bodyText confidence="0.814198333333333">
sentences for which there is not a complete feature
structure for the translation. Ref. is the reference
and HC is our hard constraint system.
</bodyText>
<subsectionHeader confidence="0.71984">
5.3 Error Classification
</subsectionHeader>
<bodyText confidence="0.972013149253732">
In order to verify that the incomplete feature struc-
tures indicate genuine translation errors and to un-
derstand the types of errors that occur, we manu-
ally check 100 sentences from our baseline system
and classify the errors. We check the verb con-
structions of the sentences containing the first 50
failures in newstest2009 and the first 50 failures in
newstest2011.
Invalid Combination (27) An ungrammatical
combination of auxiliary and main verbs.
Example: im Jahr 2007 hatte es bereits um
zwei Drittel reduziert worden .
Perfect missing aux (25) There is a past-
participle in sentence-final position, but no
auxiliary verb.
Example: der Dow Jones etwas sp¨ater
wieder bereitgestellt .
False positive (14) Output is OK. In the sample
this happens either because the output string
is well-formed in terms of verb structure, but
the tree is wrong, or because the parse of the
source is wrong and the input does not actu-
ally contain a verb.
No verb (13) The input contains at least one verb
that should be translated but the output con-
tains none.
Example: der universelle Charakter der
Handy auch Nachteile .
Invalid sentence structure (13) Verbs are
present and make sense, but sentence struc-
ture is wrong
Example: die rund hunderttausend Men-
schen in Besitz von ihren eigenen Chipcard
Opencard in dieser Zeit , diese Kupon
bekommen kann .
Inf missing aux (5) There is an infinitive in
sentence-final position, but no auxiliary
verb or the main verb is erroneously in final
position (the output is likely to be ambiguous
for this error type).
Example: die Preislisten dieser Un-
ternehmen in der Regel nur ausgew¨ahlte
Personen erreichen .
Unknown verb (2) The input verb is untrans-
lated.
Example: dann scurried ich auf meinem
Platz.
Werden-passive missing aux (1) There is a
werden-passive non-finite part, but no finite
auxiliary verb.
Example: die meisten ger¨aumigen und
luxuri¨osesten Wohnung im ersten Stock f¨ur
die ¨Offentlichkeit ge¨offnet worden .
In our classification, the most common individ-
ual error type in the baseline is the ungrammatical
combination of verbs, at 27 out of 100. However,
there are multiple categories that can be character-
ized as the absence of a required verb and com-
bined these total 44 out of 100 errors. There are
also some false positives and potentially mislead-
ing results in which wider syntactic errors result
in the failure to produce a feature structure, but the
majority are genuine errors. However, this method
fails to identify instances where the verbal com-
plex is grammatical but has the wrong features.
For that, we compare accuracy against reference
values.
</bodyText>
<subsectionHeader confidence="0.998846">
5.4 Feature Structure Accuracy
</subsectionHeader>
<bodyText confidence="0.999599363636364">
If we had gold-standard feature structures for our
reference sets and alignments between test and ref-
erence clauses then we could evaluate accuracy by
counting the number of matches and reporting pre-
cision, recall, and F-measure values for this task.
In the absence of gold reference values, we rely
on values generated automatically from our refer-
ence sets. This requires accepting some level of
error from parsing and verb labelling (we perform
a manual analysis to estimate the degree of this
problem). We also require alignments between
</bodyText>
<page confidence="0.994657">
26
</page>
<table confidence="0.999756222222222">
Data Set Experiment F E g m Prec. Recall F1
Dev Baseline 545 88 637 253 46.4 39.7 42.8
f 610 48 637 312 51.1 49.0 50.0
g 600 58 637 289 48.2 45.4 46.7
f + g 627 29 637 317 50.6 49.8 50.2
Test Baseline 2034 403 2439 993 48.8 40.7 44.4
f 2370 224 2439 1214 51.2 49.8 50.5
g 2307 278 2439 1072 46.5 44.0 45.2
f + g 2437 145 2439 1225 50.3 50.2 50.2
</table>
<tableCaption confidence="0.5395434">
Table 5: Feature structure accuracy for the development and test sets. As in Table 3, counts are given for
main clause VC structures that are present and contain at least a finite verb (F) versus those that are absent
or empty (E). The VC values of the output are compared against the reference values giving the number
of matches (m). The counts F, m, and g, (the number of gold reference values) are used to compute
precision, recall, and F1 values.
</tableCaption>
<figure confidence="0.98159575">
Input Bangladesh ex-PM is denied bail
Reference Ehemaliger Premierministerin von Bangladesch wird Kaution verwehrt
Baseline Bangladesch ex-PM ist keine Kaution
f + g Bangladesch ex-PM wird die Kaution verweigert
Input the stock exchange in Taiwan dropped by 3.6 percent according to the local index.
Reference Die B¨orse in Taiwan sank nach dem dortigen Index um 3,6 Prozent .
Baseline die B¨orse in Taiwan die lokalen Index entsprechend um 3,6 Prozent gesunken .
f + g die B¨orse in Taiwan fiel nach Angaben der ¨ortlichen Index um 3,6 Prozent .
Input the commission had been assembled at the request of Minister of Sport Miroslav Drzeviecki .
Reference Die Kommission war auf Anfrage von Sportminister Miroslaw Drzewiecki zusammengekommen.
Baseline die Kommission hatte auf Antrag der Minister f¨ur Sport Miroslav Drzeviecki montiert worden .
f + g die Kommission war auf Antrag der Minister f¨ur Sport Miroslav Drzeviecki versammelt .
</figure>
<figureCaption confidence="0.958291">
Figure 4: Example translations where the baseline verbal complex type does not match the reference but
the f + g system does.
</figureCaption>
<bodyText confidence="0.943245605263158">
test and reference clauses. Here we make the same
simplification as in Section 5.2 and restrict evalu-
ation to single-clause declarative sentences.
We test the effect of the f and g features on
feature structure accuracy. Their log-linear model
weights were tuned by running a line search to
optimize the F1 score on a subset of the new-
stest2008 dev set containing sentences up to 30
tokens in length (all baseline weights were fixed).
For the experiments in which both features are
used, we first tune the weight for f and then tune
g with the f weight fixed.
Table 5 reports feature structure accuracy for the
development and test sets. On the test set, the indi-
vidual f and g features both improve the F1 score.
f is effective in terms of both precision and recall,
but the g feature degrades precision compared to
the baseline. Using both features appears to offer
little benefit beyond using f alone.
Compared with the baseline or using hard con-
straints alone (Table 3), the proportion of sen-
tences with incomplete or inconsistent verbal
complex values (column E) is substantially re-
duced by the f and g feature functions.
To estimate the false match rate, we manually
checked the first 50 sentences from the 2009 test
set in which one system was reported to agree with
reference and the other not:
37/50 Verb constructions are grammatical. We
agree with comparisons against the reference
value.
9/50 Verb constructions are grammatical. We
agree with the comparison for the test system but
not the baseline.
4/50 Verb constructions are ungrammatical or
difficult to interpret in both baseline and test.
Figure 4 shows some example translations from
our system.
</bodyText>
<page confidence="0.994438">
27
</page>
<sectionHeader confidence="0.554774" genericHeader="method">
5.5 BLEU
</sectionHeader>
<bodyText confidence="0.999745913043478">
Finally, we report BLEU scores for two versions
of our dev and test sets: in addition to the full
data sets (Table 6), we use sub-sets that contain
all source sentences up to 30 tokens in length (Ta-
ble 7). There are two reasons for this: first, we
expect shorter sentences to use simpler sentence
structure with less coordination and fewer relative
and subordinate clauses. All else being equal, we
expect to see a greater degree of high-level struc-
tural divergence between complex source and tar-
get sentence structures than between simple ones.
We therefore anticipate that our naive clause pro-
jection strategy is more likely to break down on
long sentences. Second, we expect the effects on
BLEU score to become diluted as sentence length
increases, for the simple reason that verbs are
likely to account for a smaller proportion of the
total number of words (though this effect seems to
be small: in a parse of the newstest2009-30 subset,
verbs account for 14.2% of tokens; in the full set
they account for 13.1%). We find that the change
in BLEU is larger for the constrained test sets, but
only slightly.
</bodyText>
<table confidence="0.9970454">
Experiment 2008 2009 2010 2011
baseline 15.7 14.9 16.5 15.4
f 15.8 15.0 16.9 15.5
g 15.9 15.1 16.9 15.6
f + g 15.8 15.0 16.9 15.6
</table>
<tableCaption confidence="0.914466">
Table 6: BLEU scores for full dev/test sets
</tableCaption>
<table confidence="0.9996608">
Experiment 2008 2009 2010 2011
baseline 16.1 15.7 16.3 15.1
f 16.2 15.8 16.9 15.3
g 16.4 15.9 16.9 15.4
f + g 16.3 15.9 16.9 15.4
</table>
<tableCaption confidence="0.9439325">
Table 7: BLEU scores for constrained dev/test sets
(max. 30 tokens)
</tableCaption>
<sectionHeader confidence="0.999922" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999988">
The problem of verbal complex translation in
English-to-German is tackled by Gojun and
Fraser (2012) in the context of phrase-based
SMT. They overcome the reordering limitation of
phrase-based SMT by preprocessing the source-
side of the training and test data to move En-
glish verbs within clauses into more ‘German-like’
positions. In contrast, our SCFG-based baseline
model does not place any restriction on reordering
distance.
Arora and Mahesh (2012) address a similar
problem in English-Hindi translation. They im-
prove a phrase-based model by merging verbs and
associated particles into single tokens, thus simpli-
fying the task of word alignment and phrase-pair
extraction. Their approach relies upon the mostly-
contiguous nature of English and Hindi verbal
complexes. The discontiguity of verbal complexes
rules out this approach for translation into Ger-
man.
Our model adopts a similar constraint-based ex-
tension of SCFG to that described in Williams and
Koehn (2011). In that work, constraints are used to
enforce target-side agreement between nouns and
modifiers and between subjects and verbs. Whilst
that constraint model operates purely on the target-
side, our verbal complex feature functions also
take source-side information into account.
</bodyText>
<sectionHeader confidence="0.998205" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999956">
We have presented a model in which a conven-
tional SCFG-based string-to-tree system is ex-
tended with a rich feature-structure based repre-
sentation of German verbal complexes, a gram-
matical construction that is difficult for an SMT
model to produce correctly. Our feature struc-
ture representation enabled us to easily identify
where our baseline model made errors and pro-
vided a means to measure accuracy against the ref-
erence translations. By developing feature func-
tions that use source-side information to influence
verbal complex formation we were able to im-
prove translation quality, measured both in terms
of BLEU score where there were small, consis-
tent gains across the test sets, and in terms of task-
specific accuracy.
In future work we intend to explore the use
of richer models for predicting target-side verbal
complex types. For example, discriminative mod-
els that include non-verbal source features.
</bodyText>
<sectionHeader confidence="0.997641" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9910275">
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. The re-
search leading to these results has received fund-
ing from the European Union Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment 287658 (EU-BRIDGE).
</bodyText>
<page confidence="0.997858">
28
</page>
<sectionHeader confidence="0.993792" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999885554054054">
Karunesh Kumar Arora and R. Mahesh K. Sinha. 2012.
Improving statistical machine translation through
co-joining parts of verbal constructs in english-hindi
translation. In Proceedings of the Sixth Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation, pages 95–101, Jeju, Republic of Korea, July.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10–51, Montr´eal, Canada, June. Association for
Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201–228.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In HLT-NAACL ’04.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of german verbs in english–to–
german smt. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association
for Computational Linguistics, pages 726–735, Avi-
gnon, France, April. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180, Morristown, NJ, USA.
Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ’02, pages 295–302, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 160–
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404–411, Rochester, New York, April.
Association for Computational Linguistics.
Stuart M. Shieber. 1984. The design of a computer lan-
guage for linguistic information. In Proceedings of
the 10th international conference on Computational
linguistics, COLING ’84, pages 362–366, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Philip Williams and Philipp Koehn. 2011. Agree-
ment constraints for statistical machine translation
into german. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 217–226,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Philip Williams and Philipp Koehn. 2012. Ghkm
rule extraction and scope-3 parsing in moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 388–394, Montr´eal,
Canada, June. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.999113">
29
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.491315">
<title confidence="0.7865205">Using Feature Structures to Improve Verb Translation English-to-German Statistical MT</title>
<author confidence="0.76104">p j williams-sms ed ac uk pkoehninf ed ac uk</author>
<affiliation confidence="0.97216">of for Speech and Language University of Edinburgh The Johns Hopkins University</affiliation>
<abstract confidence="0.998266142857143">SCFG-based statistical MT models have proven effective for modelling syntactic aspects of translation, but still suffer problems of overgeneration. The production of German verbal complexes is particularly challenging since highly discontiguous constructions must be formed consistently, often from multiple independent rules. We extend a strong SCFG-based string-to-tree model to incorporate a rich feature-structure based representation of German verbal complex types and compare verbal complex production against that of the reference translations, finding a high baseline rate of error. By developing model features that use source-side information to influence the production of verbal complexes we are able to substantially improve the type accuracy as compared to the reference.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Karunesh Kumar Arora</author>
<author>R Mahesh K Sinha</author>
</authors>
<title>Improving statistical machine translation through co-joining parts of verbal constructs in english-hindi translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<pages>95--101</pages>
<institution>of Korea, July. Association for Computational Linguistics.</institution>
<location>Jeju, Republic</location>
<marker>Arora, Sinha, 2012</marker>
<rawString>Karunesh Kumar Arora and R. Mahesh K. Sinha. 2012. Improving statistical machine translation through co-joining parts of verbal constructs in english-hindi translation. In Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 95–101, Jeju, Republic of Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="15240" citStr="Callison-Burch et al., 2012" startWordPosition="2503" endWordPosition="2506">Total E F E Total Dev 95.6% 4.4% 100.0% 86.1% 13.9% 100.0% 87.6% 12.4% 100.0% (633) 637 29 666 545 88 633 559 79 638 Test 92.2% 7.8% 100.0% 83.5% 16.5% 100.0% 85.4% 14.6% 100.0% (2445) 2439 206 2645 2034 403 2437 2096 359 2455 Table 3: Counts of main clause VC structures that are present and contain at least a finite verb (F) versus those that are empty or absent (E). Declarative main clause counts (MC count) are given for each input set. Counts for the three test sets are aggregated. We extracted a translation grammar using all English-German parallel data from the WMT 2012 translation task (Callison-Burch et al., 2012), a total of 2.0M sentence pairs. We used all of the WMT 2012 monolingual German data to train a 5-gram language model. The baseline system uses the feature functions described in Williams and Koehn (2012). The feature weights were tuned on the WMT newstest2008 development set using MERT (Och, 2003). We use the newstest2009, newstest2010, and newstest2011 test sets for evaluation. The development and test sets all use a single reference. 5.2 Main Clause Verb Errors When translating a declarative main clause, the translation should usually also be a declarative main clause – that is, it should </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Comput. Linguist., 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule? In</title>
<date>2004</date>
<booktitle>HLT-NAACL ’04.</booktitle>
<contexts>
<context position="6495" citStr="Galley et al. (2004)" startWordPosition="1008" endWordPosition="1012">l set of hand-written feature structures. Every main verb is assigned VC values from one of three possible groups, selected according to whether the verb is finite, a past-participle, or an infinitive. For the closed class of modal and nonmodal auxiliary verbs, VC values were manually assigned. 3 The Grammar Our baseline translation model is learned from a parallel corpus with automatically-derived word alignments. In the literature, string-to-tree translation models are typically based on either synchronous context-free grammars (SCFGs) (as in Chiang et al. (2007)) or tree transducers (as in Galley et al. (2004)). In this work, we use an SCFGbased model but our extensions are applicable in both cases. Following Williams and Koehn (2011), each rule of our grammar is supplemented with a (possibly-empty) set of PATR-II-style identities (Shieber, 1984). Figure 2 shows two example rules with identities. The identities should be interpreted as constraints that the feature structures of the corresponding rule elements are compatible under unification. During decoding, this imposes a hard constraint on rule application. 3.1 Identity Extraction The identities are learned using the following procedure: 1. The </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In HLT-NAACL ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anita Gojun</author>
<author>Alexander Fraser</author>
</authors>
<title>Determining the placement of german verbs in english–to– german smt.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>726--735</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<contexts>
<context position="3863" citStr="Gojun and Fraser (2012)" startWordPosition="572" endWordPosition="575">ges 21–29, Gothenburg, Sweden, April 27, 2014. c�2014 Association for Computational Linguistics of German verbal complex types. During decoding, our model composes type values for every clause. When we compare these values against those of the reference translations, we find a high baseline rate of error (either incomplete or mismatching values). By developing model features that use source-side information to influence the production of verbal complexes we are able to substantially improve the type accuracy as compared to the reference. 2 Verbal Complex Structures Adopting the terminology of Gojun and Fraser (2012), we use the term ‘verbal complex’ to mean a main verb and any associated auxiliaries within a single clause. 2.1 Feature Structures We use feature structures to represent the underlying grammatical properties of German verbal complexes. The feature structures serve two main functions: the first is to specify a type for the verbal complex. The types describe clause-level properties and are defined along four dimensions: 1. tense (present, past, perfect, pluperfect, future, future perfect), 2. voice (active, werden-passive, sein-passive), 3. mood (indicative, subjunctive I, subjunctive II), and</context>
<context position="27603" citStr="Gojun and Fraser (2012)" startWordPosition="4587" endWordPosition="4590">s; in the full set they account for 13.1%). We find that the change in BLEU is larger for the constrained test sets, but only slightly. Experiment 2008 2009 2010 2011 baseline 15.7 14.9 16.5 15.4 f 15.8 15.0 16.9 15.5 g 15.9 15.1 16.9 15.6 f + g 15.8 15.0 16.9 15.6 Table 6: BLEU scores for full dev/test sets Experiment 2008 2009 2010 2011 baseline 16.1 15.7 16.3 15.1 f 16.2 15.8 16.9 15.3 g 16.4 15.9 16.9 15.4 f + g 16.3 15.9 16.9 15.4 Table 7: BLEU scores for constrained dev/test sets (max. 30 tokens) 6 Related Work The problem of verbal complex translation in English-to-German is tackled by Gojun and Fraser (2012) in the context of phrase-based SMT. They overcome the reordering limitation of phrase-based SMT by preprocessing the sourceside of the training and test data to move English verbs within clauses into more ‘German-like’ positions. In contrast, our SCFG-based baseline model does not place any restriction on reordering distance. Arora and Mahesh (2012) address a similar problem in English-Hindi translation. They improve a phrase-based model by merging verbs and associated particles into single tokens, thus simplifying the task of word alignment and phrase-pair extraction. Their approach relies u</context>
</contexts>
<marker>Gojun, Fraser, 2012</marker>
<rawString>Anita Gojun and Alexander Fraser. 2012. Determining the placement of german verbs in english–to– german smt. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 726–735, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="14374" citStr="Koehn et al., 2007" startWordPosition="2350" endWordPosition="2353">clarative clause. Dropped finite verbs are a frequent problem in our baseline model and this feature was motivated by an early version of the analysis presented in Section 5.3. 5 Experiments and Analysis In preliminary experiments, we found that changes in translation quality resulting from our verb translation features were difficult to measure using BLEU. In the following experiments, we measure accuracy by comparing verbal complex values against feature structures derived from the reference sentences. 5.1 Setup Our experiments use the GHKM-based string-totree pipeline implemented in Moses (Koehn et al., 2007; Williams and Koehn, 2012). We extend a conventional baseline model using the constraints and feature functions described earlier.    f(ri) =    g(ri) = 24 Data Set F Reference F Baseline Total Hard Constraint (MC count) E Total E F E Total Dev 95.6% 4.4% 100.0% 86.1% 13.9% 100.0% 87.6% 12.4% 100.0% (633) 637 29 666 545 88 633 559 79 638 Test 92.2% 7.8% 100.0% 83.5% 16.5% 100.0% 85.4% 14.6% 100.0% (2445) 2439 206 2645 2034 403 2437 2096 359 2455 Table 3: Counts of main clause VC structures that are present and contain at least a finite verb (F) versus those that are empty or abs</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>295--302</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8304" citStr="Och and Ney (2002)" startWordPosition="1311" endWordPosition="1314">plex highlighted. The rules in Figure 2 were extracted from this sentence-pair. Crucially, in step 2 of the extraction procedure the identities can be added to SCFG rules that cover only part of a verbal complex. For example, the first rule of Figure 2 includes the main verb but not the auxiliary. On application of this rule, the partial VC value is propagated from the main verb to the root. The second rule in Figure 2 identifies the VC value of an auxiliary with the VC value of a VP-OC subderivation (such as the subderivation produced by applying the first rule). 4 Source-side Features Since Och and Ney (2002), most SMT models have been defined as a log-linear sum of weighted feature functions. In this section, we define two verbal-complex-specific feature functions. In order to do so, we first describe ‘clause projection,’ a simple source-syntactic restriction on decoding. We then describe our heuristic method of obtaining probability estimates for a target verbal complex value given the source clause. 4.1 Clause Projection Our feature functions assume that we have an alignment from source-side clauses to target clauses. In order to satisfy this requirement, we adopt a simple restriction that decl</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 295–302, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="15540" citStr="Och, 2003" startWordPosition="2556" endWordPosition="2557"> verb (F) versus those that are empty or absent (E). Declarative main clause counts (MC count) are given for each input set. Counts for the three test sets are aggregated. We extracted a translation grammar using all English-German parallel data from the WMT 2012 translation task (Callison-Burch et al., 2012), a total of 2.0M sentence pairs. We used all of the WMT 2012 monolingual German data to train a 5-gram language model. The baseline system uses the feature functions described in Williams and Koehn (2012). The feature weights were tuned on the WMT newstest2008 development set using MERT (Och, 2003). We use the newstest2009, newstest2010, and newstest2011 test sets for evaluation. The development and test sets all use a single reference. 5.2 Main Clause Verb Errors When translating a declarative main clause, the translation should usually also be a declarative main clause – that is, it should usually contain at least a finite verb. From manually inspecting the output it is clear that verb dropping is a common source of translation error in our baseline system. By making the assumption that a declarative main clause should always be translated to a declarative main clause, we can use the </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 160– 167, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing. In Human Language Technologies</title>
<date>2007</date>
<booktitle>The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="9689" citStr="Petrov and Klein, 2007" startWordPosition="1528" endWordPosition="1531">stic perspective but it appears not to harm translation quality in practice. Table 1 shows small gains in BLEU score over our baseline system with this restriction. Test Set Baseline Clause Proj. newstest2008 15.7 15.8 (+0.1) newstest2009 14.9 15.0 (+0.1) newstest2010 16.5 16.8 (+0.3) newstest2011 15.4 15.5 (+0.1) Table 1: Results with and without clause projection (baseline tuning weights are used for clause projection) Clause projection is implemented as follows: 1. The input sentence is parsed and a set of clause spans is extracted according to the 1-best parse. We use the Berkeley parser (Petrov and Klein, 2007), which is trained on the Penn Treebank and so we base our definition of a declarative clause on the treebank annotation guidelines. 2. We modify the decoder to produce derivations in chart cells only if the cell span is consistent with the set of clause spans (i.e. if source span [i,j] is a clause span then no derivation is built over span [m,n] where i &lt; m ≤ j and n &gt; j, etc.) 3. We modify the decoder so that grammar rules can only be applied over clause spans if they have a clause label (‘S’ or ‘CS’, since the parser we use is trained on the Tiger treebank). 4.2 Verbal Complex Probabilities</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–411, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>The design of a computer language for linguistic information.</title>
<date>1984</date>
<booktitle>In Proceedings of the 10th international conference on Computational linguistics, COLING ’84,</booktitle>
<pages>362--366</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6736" citStr="Shieber, 1984" startWordPosition="1050" endWordPosition="1051">xiliary verbs, VC values were manually assigned. 3 The Grammar Our baseline translation model is learned from a parallel corpus with automatically-derived word alignments. In the literature, string-to-tree translation models are typically based on either synchronous context-free grammars (SCFGs) (as in Chiang et al. (2007)) or tree transducers (as in Galley et al. (2004)). In this work, we use an SCFGbased model but our extensions are applicable in both cases. Following Williams and Koehn (2011), each rule of our grammar is supplemented with a (possibly-empty) set of PATR-II-style identities (Shieber, 1984). Figure 2 shows two example rules with identities. The identities should be interpreted as constraints that the feature structures of the corresponding rule elements are compatible under unification. During decoding, this imposes a hard constraint on rule application. 3.1 Identity Extraction The identities are learned using the following procedure: 1. The syntax of the German parse trees is used to identify verbal complexes and label the participating verb and clause nodes. 22 Figure 3: Alignment graph for a sentence pair from the training data. The target sentence has a single verbal complex</context>
</contexts>
<marker>Shieber, 1984</marker>
<rawString>Stuart M. Shieber. 1984. The design of a computer language for linguistic information. In Proceedings of the 10th international conference on Computational linguistics, COLING ’84, pages 362–366, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Williams</author>
<author>Philipp Koehn</author>
</authors>
<title>Agreement constraints for statistical machine translation into german.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>217--226</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="6622" citStr="Williams and Koehn (2011)" startWordPosition="1031" endWordPosition="1034">d according to whether the verb is finite, a past-participle, or an infinitive. For the closed class of modal and nonmodal auxiliary verbs, VC values were manually assigned. 3 The Grammar Our baseline translation model is learned from a parallel corpus with automatically-derived word alignments. In the literature, string-to-tree translation models are typically based on either synchronous context-free grammars (SCFGs) (as in Chiang et al. (2007)) or tree transducers (as in Galley et al. (2004)). In this work, we use an SCFGbased model but our extensions are applicable in both cases. Following Williams and Koehn (2011), each rule of our grammar is supplemented with a (possibly-empty) set of PATR-II-style identities (Shieber, 1984). Figure 2 shows two example rules with identities. The identities should be interpreted as constraints that the feature structures of the corresponding rule elements are compatible under unification. During decoding, this imposes a hard constraint on rule application. 3.1 Identity Extraction The identities are learned using the following procedure: 1. The syntax of the German parse trees is used to identify verbal complexes and label the participating verb and clause nodes. 22 Fig</context>
<context position="28473" citStr="Williams and Koehn (2011)" startWordPosition="4720" endWordPosition="4723">ur SCFG-based baseline model does not place any restriction on reordering distance. Arora and Mahesh (2012) address a similar problem in English-Hindi translation. They improve a phrase-based model by merging verbs and associated particles into single tokens, thus simplifying the task of word alignment and phrase-pair extraction. Their approach relies upon the mostlycontiguous nature of English and Hindi verbal complexes. The discontiguity of verbal complexes rules out this approach for translation into German. Our model adopts a similar constraint-based extension of SCFG to that described in Williams and Koehn (2011). In that work, constraints are used to enforce target-side agreement between nouns and modifiers and between subjects and verbs. Whilst that constraint model operates purely on the targetside, our verbal complex feature functions also take source-side information into account. 7 Conclusion We have presented a model in which a conventional SCFG-based string-to-tree system is extended with a rich feature-structure based representation of German verbal complexes, a grammatical construction that is difficult for an SMT model to produce correctly. Our feature structure representation enabled us to</context>
</contexts>
<marker>Williams, Koehn, 2011</marker>
<rawString>Philip Williams and Philipp Koehn. 2011. Agreement constraints for statistical machine translation into german. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 217–226, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Williams</author>
<author>Philipp Koehn</author>
</authors>
<title>Ghkm rule extraction and scope-3 parsing in moses.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>388--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="14401" citStr="Williams and Koehn, 2012" startWordPosition="2354" endWordPosition="2357">opped finite verbs are a frequent problem in our baseline model and this feature was motivated by an early version of the analysis presented in Section 5.3. 5 Experiments and Analysis In preliminary experiments, we found that changes in translation quality resulting from our verb translation features were difficult to measure using BLEU. In the following experiments, we measure accuracy by comparing verbal complex values against feature structures derived from the reference sentences. 5.1 Setup Our experiments use the GHKM-based string-totree pipeline implemented in Moses (Koehn et al., 2007; Williams and Koehn, 2012). We extend a conventional baseline model using the constraints and feature functions described earlier.    f(ri) =    g(ri) = 24 Data Set F Reference F Baseline Total Hard Constraint (MC count) E Total E F E Total Dev 95.6% 4.4% 100.0% 86.1% 13.9% 100.0% 87.6% 12.4% 100.0% (633) 637 29 666 545 88 633 559 79 638 Test 92.2% 7.8% 100.0% 83.5% 16.5% 100.0% 85.4% 14.6% 100.0% (2445) 2439 206 2645 2034 403 2437 2096 359 2455 Table 3: Counts of main clause VC structures that are present and contain at least a finite verb (F) versus those that are empty or absent (E). Declarative main c</context>
</contexts>
<marker>Williams, Koehn, 2012</marker>
<rawString>Philip Williams and Philipp Koehn. 2012. Ghkm rule extraction and scope-3 parsing in moses. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 388–394, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014753">
<title confidence="0.9986105">
Detecting Health Related Discussions in Everyday Telephone
Conversations for Studying Medical Events in the Lives of Older Adults
</title>
<author confidence="0.939867">
Golnar Sheikhshab, Izhak Shafran, Jeffrey Kaye
</author>
<affiliation confidence="0.79346">
Oregon Health &amp; Science University
</affiliation>
<email confidence="0.993768">
sheikhsh,shafrani,kaye@ohsu.edu
</email>
<sectionHeader confidence="0.99376" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929739130435">
We apply semi-supervised topic modeling
techniques to detect health-related discus-
sions in everyday telephone conversations,
which has applications in large-scale epi-
demiological studies and for clinical in-
terventions for older adults. The privacy
requirements associated with utilizing ev-
eryday telephone conversations preclude
manual annotations; hence, we explore
semi-supervised methods in this task. We
adopt a semi-supervised version of Latent
Dirichlet Allocation (LDA) to guide the
learning process. Within this framework,
we investigate a strategy to discard irrel-
evant words in the topic distribution and
demonstrate that this strategy improves
the average F-score on the in-domain task
and an out-of-domain task (Fisher corpus).
Our results show that the increase in dis-
cussion of health related conversations is
statistically associated with actual medi-
cal events obtained through weekly self-
reports.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964137254902">
There has been considerable interest in under-
standing, promoting, and monitoring healthy
lifestyles among older adults while minimizing the
frequency of clinical visits. Longitudinal studies
on large cohorts are necessary, for example, to un-
derstand the association between social networks,
depression, dementia, and general health. In this
context, detecting discussions of health are impor-
tant as indicators of under-reported health events
in daily lives as well as for studying healthy so-
cial support networks. The detection of medical
events such as higher levels of pain or discom-
fort may also be useful in providing timely clin-
ical intervention for managing chronic illness and
thus promoting healthy independent living among
older adults.
Motivated by this larger goal, we develop and
investigate techniques for identifying conversa-
tions containing any health related discussion. We
are interested in detecting discussions about med-
ication with doctors, as well as conversations with
others, where among all different topics being dis-
cussed, subjects may also be complaining about
pain or changes in health status.
The privacy concerns of recording and analyz-
ing everyday telephone conversation prevents us
from manually transcribing and annotating con-
versations. So, we automatically transcribe the
conversations using an automatic speech recog-
nition system and look-up the telephone number
corresponding to each conversation as a heuristic
means of deriving labels. This technique is suit-
able for labeling a small subset of the conversa-
tions that are only sufficient for developing semi-
supervised algorithms and for evaluating the meth-
ods for analysis.
Before delving into our approach, we discuss
a few relevant and related studies in Section 2
and describe our unique naturalistic corpus in Sec-
tion 3. Given the restrictive nature of our labeled
in-domain data set, we are interested in a clas-
sifier that generalizes to the unlabeled data. We
evaluate the generalizability of the classifiers us-
ing an out-of-domain corpus. We adopt a semi-
supervised topic modeling approach to address
our task, and develop an iterative feature selec-
tion method to improve our classifier, as described
in Section 4. We evaluate the efficacy of our ap-
proach empirically, on the in-domain as well as an
out-of-domain corpus, and report results in Sec-
tion 5.
</bodyText>
<sectionHeader confidence="0.999798" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9983225">
The task of identifying conversations where health
is mentioned differs from many other tasks in topic
</bodyText>
<page confidence="0.989522">
38
</page>
<bodyText confidence="0.983202803030303">
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 38–44,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
modeling because in this task we are interested in
one particular topic. A similar study is the work of
Prier and colleagues (Prier et al., 2011). They use
a set of predefined seed words as queries to gather
tweets related to tobacco or marijuana usage, and
then use LDA to discover related subtopics. Thus,
their method is sensitive to the seed words chosen.
One way to reduce the sensitivity to the manu-
ally specified seed words is to expand the set us-
ing WordNet. Researchers have investigated this
approach in sentiment analysis (Kim and Hovy,
2004; Yu and Hatzivassiloglou, 2003). However,
when expanding the seed word set using WordNet,
we need to be careful to avoid antonyms and words
that have high degree of linkage with many words
in the vocabulary. Furthermore, we can not ap-
ply such an approach for languages with poor re-
sources, where manually curated knowledge is un-
available. The other drawback of this approach is
that we can not use characteristics of the end task,
in our case health-related conversation retrieval,
to select the words. As an alternative method,
Han and colleagues developed an interactive sys-
tem where users selected the most relevant words
from a set, proposed by an automated system (Han
et al., 2009).
Another idea for expanding the seed words is
using the statistical information. Among statis-
tical methods, the simplest approach is to com-
pute pairwise co-occurrence with the seed words.
Li and Yamanishi ranked the words co-occurring
with the seed words according to information the-
oretic costs, and used the highest ranked words as
the expanded set (Li and Yamanishi, 2003). This
idea can be more effective when the co-occurrence
is performed over subsets instead, as in Hisamitsu
and Niwa’s work (Hisamitsu and Niwa, 2001).
However, it is computationally expensive to search
over subsets of words. Depending on the language
and task, heuristics might be applicable. An ex-
ample of this kind of approach is Zagibalov and
Carroll’s work on sentiment analysis in Chinese
(Zagibalov and Carroll, 2008).
Alternatively, we can treat the task of identify-
ing words associated with seed words as a cluster-
ing problem with the intuition that the seed words
are in the same cluster. An effective strategy to
cluster words into topics, is Latent Dirichlet Allo-
cation (LDA) (Blei et al., 2003) . However, LDA
is an unsupervised algorithm and the clustered top-
ics are not guaranteed to include the topic of inter-
est. The Seeded LDA, a variant of LDA, attempts
to address this problem by incorporating the seed
words as priors over the topics (Jagarlamudi et
al., 2012). However, the estimation procedure is
more complicated. Alternatively, in Topic LDA
(TLDA), a clever extension to LDA, Andrzejewski
and Zhu address this problem by fixing the mem-
bership of the words to valid topics (Andrzejewski
and Zhu, 2009). When the focus is on detecting
just one topic, as in our task, we can expand the
seed words more selectively using the small set of
labeled data and that is the approach adopted in
this paper.
</bodyText>
<sectionHeader confidence="0.995727" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999988714285714">
One interesting aspect of our study is the unique-
ness of our corpus, which is both naturalistic and
exhaustive. We recorded about 41,000 land-line
everyday telephone conversations from 56 volun-
teers, 65 years or older, over a period of approxi-
mately 6 to 12 months. Since these everyday tele-
phone conversations are private conversations, and
might include private information such as names,
telephone numbers, or banking information, we
assured the subjects that no one would listen to the
recorded conversations. Thus, we couldn’t manu-
ally transcribe the conversations; instead, we used
an Automatic Speech Recognition (ASR) system
that we describe here.
</bodyText>
<subsectionHeader confidence="0.652215">
Automatic Speech Recognition System Con-
</subsectionHeader>
<bodyText confidence="0.999992">
versations in our corpus were automatically tran-
scribed using an ASR system, which is structured
after IBM’s conversation telephony system (Soltau
et al., 2005). The acoustic models were trained
on about 2000 hours of telephone speech from
Switchboard and Fisher corpora (Godfrey et al.,
1992). The system has a vocabulary of 47K
and uses a trigram language model with about
10M n-grams, estimated from a mix of transcripts
and web-harvested data. Decoding is performed
in three stages using speaker-independent mod-
els, vocal-tract normalized models and speaker-
adapted models. The three sets of models are sim-
ilar in complexity with 4000 clustered pentaphone
states and 150K Gaussians with diagonal covari-
ances. Our system does not include discriminative
training and performs at a word error rate of about
24% on NIST RT Dev04 which is comparable to
state of the art performance for such systems. We
are unable to measure the performance of this rec-
ognizer on our corpus due to the stringent privacy
</bodyText>
<page confidence="0.996578">
39
</page>
<bodyText confidence="0.9996581">
requirements mentioned earlier. Since both cor-
pora are conversational telephone speech and the
training data contains large number of conversa-
tions (2000 hours), we expect the performance of
our recognizer to be relatively close to results on
NIST benchmark.
Heuristically labeling a small subset of conver-
sations For training and evaluation purposes, we
need a labeled set of conversations; that is, a set
of conversations where we know whether or not
they contain health-related discussions. Since the
privacy concerns do not allow for manually label-
ing the conversations, we used reverse look-up ser-
vice in www.whitepages.com. We sent the
phone number corresponding to each conversation
(when available) to this website to obtain informa-
tion about the other end of the conversation. Based
of the information we got back from this web-
site, we labeled a small subset of the conversations
which fell into unambiguous business categories.
For example, we labeled the calls to “hospital” and
“pharmacy” as health-related, and those to “car re-
pair” and “real estate” as non-health-related.
The limitations of the labeled set The labeled
set we obtained is small and restricted in type of
conversations. Since phone numbers are not avail-
able for many of the conversations we recorded,
and also because www.whitepages.com does
not return unambiguous information for many of
available phone numbers, we managed to label
only 681 conversations – 275 health-related and
406 non-health-related. This labeled set has an-
other limitation: it contains conversations to busi-
ness numbers only. In reality however, we are in-
terested in the much larger set of conversations
between friends, relatives, and other members of
subjects’ social support network. Thus, the gener-
alizability of the classifier we train is very impor-
tant.
Fisher Corpus To explicitly test the generaliz-
ability of our classifier, we use a second evaluation
set from Fisher corpus (Cieri et al., 2004). Fisher
corpus contains telephone conversations with pre-
assigned topics. There are 40 topics and only
one of them, illness, is health-related. We identi-
fied 338 conversations on illness, and sampled 702
conversations from the other 39 non-health topics.
Since we do not train on Fisher corpus, we call
it the out-of-domain task to apply our method on
Fisher corpus; as opposed to the in-domain task
which is to apply our method on the everyday tele-
phone conversations.
Extra information on subjects’ health In the
everyday telephone conversations corpus, we also
have access to the subjects’ weekly self-reports
on their medical status during the week indicating
medical events such as injury or going to emer-
gency room. We will use these pieces of infor-
mation to relate the health-related conversations to
actual medical events in the subjects’ lives.
</bodyText>
<sectionHeader confidence="0.990691" genericHeader="method">
4 Method
</sectionHeader>
<subsectionHeader confidence="0.892432">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.999991486486487">
As we explained in Section 3, we can label a small
set of conversations in the everyday telephone con-
versations corpus as health-related vs. non-health
related. Using this labeled set we can train a sup-
port vector machine (SVM) to classify the con-
versations. In absence of feature selection, the
conversations are represented by a vector of tf-idf
scores for every word in the vocabulary where tf-
idf is a score for measuring the importance of a
word in one document of a corpus. As we see in
Section 5, such a classifier doesn’t generalize to
the out-of-domain Fisher task (i.e. when we test
the classifier on Fisher data set, we do not get good
precision and recalls). Generalizability is impor-
tant in our case, especially because the data we use
for training is limited in number and the nature of
conversations.
One way to improve generalization is to per-
form feature selection. That is, instead of using
tf-idf scores for the whole vocabulary, we would
like to rely only on features relevant to detecting
the health topic. We propose a new way for feature
selection for retrieving documents containing in-
formation about a specific topic when there is only
a limited set of labeled documents available. The
idea is to pick a few words highly related to the
topic of interest as seed words and to use TLDA
(Andrzejewski and Zhu, 2009) to force those seed
words into one (for example, the first) topic. In our
task, the topic of interest is health. So, we choose
doctor, medicine, and pain – often used while dis-
cussing health – as our seed words. Topics in LDA
based methods such as TLDA are usually repre-
sented using the n most probable words; where n
is an arbitrary number. So, the first candidate sets
for expanding our seed words are the sets of 50
most probable words in the topic of health in dif-
</bodyText>
<page confidence="0.992201">
40
</page>
<bodyText confidence="0.99990768">
ferent runs of TLDA. As our experiments reveal,
these candidate sets contain many words that are
unrelated to health. To solve this problem, we use
the small labeled set of conversations to filter out
the unrelated words.
Figure 1 shows the proposed iterative algo-
rithm. The algorithm starts with initializing the
seed words to doctor, medicine, and pain. Then,
in each iteration, TLDA performs semi-supervised
topic modeling and returns the 50 most probable
candidate words in the health topic. We select a
subset of these candidate words which, if added
to the seed words, would maximize the average of
precision and recall on the train set for a simple
classifier. This simple classifier marks a conver-
sion as health related if, and only if, it contains at
least one of the seed words. The algorithm termi-
nates when the subset selection is unable to add
a new word contributing to the average of preci-
sion and recall. The tf-idf vector for the expanded
set represents the conversations in the classifica-
tion process.
It is worth mentioning that we train TLDA using
all 41000 unlabeled conversations, and chose the
number of topics, K, to be 20.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999130375">
In all of our experiments, we trained SVM
classifiers, with different features, to detect the
conversations on health using the popular lib-
SVM (Chang and Lin, 2011) implementation. We
chose the parameters of the SVM using a 30-fold
cross-validated (CV) grid search over the training
data. We also used a 4-fold cross validation over
the labeled set of conversations to maximize the
use of the relatively small labeled set. That is, we
trained the feature selection algorithm on 3-folds
and tested the resulting SVM tested on the fourth.
In in-domain task we always report the average
performance across the folds.
Table 1 shows the results of our experiments
using different input features. We report on re-
call, precision and F-measure in in-domain and
out-of-domain (Fisher) task as well as on average
F-measure of the two. The justification for consid-
ering the average F-measure is that we want our
algorithm to work well on both in-domain corpus
and Fisher corpus since we need to make sure that
our classifier is generalizable (i.e. it works well on
Fisher) and it works well on the private and natu-
ral telephone conversations (i.e. the ones similar
</bodyText>
<figureCaption confidence="0.708222">
Figure 1: Expanding the set of seed words: in each
</figureCaption>
<bodyText confidence="0.978114702702703">
iteration, the current seed words are forced into
the topic of health to guide TLDA towards finding
more health related words. The candidate set con-
sists of the 50 most probable words of the topic of
health in TLDA. We investigate the gain of adding
each word of the candidate set to the seed words by
temporarily adding it to the seed words and look-
ing at the average of precision and recall on the
training set for a classifier that classifies a conver-
sation as health-related if and only if it contains at
least one of the seed words. We select the words
that maximize this objective and add them to the
seed words until no other words contributes to the
average precision and recall.
to the in-domain corpus)
When using the full vocabulary, the in-domain
performance (the performance on the everyday
telephone conversations data) is relatively good
with 75.1% recall and 83.5% precision. But the
out-of-domain recall (recall on the Fisher data set)
is considerably low at 2.8%. Ideally, we want
a classifier that performs well in both domains.
Rows 2 to 5 can be seen as steps to get to such
a classifier.
The second row shows the performance of the
other extreme end of feature selection: the fea-
tures include the manually chosen words doctor,
medicine, and pain only. While this leads to very
good out-of-domain performance, the in-domain
recall has dropped considerably. We trained
TLDA 30 times, and selected the 50 most probable
words in the health topic. The third row in Table 1
shows the average performance of SVM when us-
ing the tf-idf of these sets of words as the feature
vector on in-domain and out-of-domain tasks. Us-
ing the 50 most probable words in health topic sig-
nificantly improves average F-score (71%) across
</bodyText>
<page confidence="0.998906">
41
</page>
<table confidence="0.999907833333333">
Recall Precision F-measure
Feature Words In-Domain Fisher In-Domain Fisher In-Domain Fisher Average
Full vocabulary 75.2 2.8 83.5 91.1 79.1 5.4 42.3
(no feature selection)
Initial words 45.1 69.2 94.8 94.5 61.1 79.9 70.5
(doctor, medicine, pain)
50 most probable words in health 58.4 57.4 86.3 97.5 69.7 72.3 71.0
(average over 30 runs)
Words selected by our method 56.1 66.5 91.0 95.5 69.4 78.4 73.9
(average over 30 runs)
Union of all selected words 67.7 69.4 87.8 95.1 76.5 80.2 78.3
(across 30 runs)
</table>
<tableCaption confidence="0.860955333333333">
Table 1: Performance of SVM classifiers using different feature selection methods. The In-Domain task
involves the everyday telephone conversations corpus. We call Fisher corpus out of domain, because no
example of this corpus was used in training.
</tableCaption>
<bodyText confidence="0.995127295774648">
both tasks over using the full vocabulary (42.3%)
but it is clear that this is only due to improvement
in out-of-domain task. Table 2 shows one set of
the 50 most probable words in health topic,the re-
sult of one run of TLDA. Evidently, these words
contain many irrelevant words. This is the motiva-
tion for our iterative algorithm.
Next, we evaluate the performance of our iter-
ative algorithm. The fourth row in Table 1 shows
the average performance of SVM using expanded
seed words that our algorithm suggested in 30
runs. Our algorithm improves the average F-score
by 3% comparing to the standard TLDA. This is
due to a 5% improvement in out-of-domain task
as opposed to a 0.3% performance decrease in in-
domain task.
Since our algorithm has a probabilistic topic
modeling component (i.e. TLDA), different runs
lead to different sets of expanded seed words. We
extract a union of all the words chosen over 30
runs and evaluate the performance of SVM using
this union set. This improves the performance of
our method further to achieve the best average F-
score of 78.3%, which is an 85% improvement
over using the SVM with full vocabulary. It is im-
portant to notice that the in-domain performance is
still lower than the full-vocabulary baseline by less
than 3% while the out-of-domain performance is
the best obtained. Once again, we are more inter-
ested in the average F-measure because we need
our algorithm to generalize well (work well on
out-of-domain corpus) and to work well on natural
private conversations (on the conversations similar
to the on-domain corpus).
Our last experiment tests statistical associa-
tion between health-related discussions in every-
day telephone conversations, and actual medical
pain, medicine, appointment, medical, doc-
tors, emergency, prescription, contact, med-
ication, dial, insurance, pharmacy, schedule,
moment, reached, questions, services, surgery,
telephone, record, appointments, options, ad-
dress, patient, advice, quality, tuesday, posi-
tion, answered, records, wednesday, therapy,
healthy, correct, department, ensure, numbers,
act, doctor, personal, test, senior, nurse, plan,
kaiser
Table 2: 50 most probable words in the topic of
health returned by one run of TLDA. The bold
words are the ones are hand-picked.
events in older adults. As mentioned in Section 3,
we have access to weekly self-reports on medical
events for subjects’ in everyday telephone conver-
sations corpus. We used our best classifier, the
SVM with union of expanded seed words, to clas-
sify all the conversations in our corpus into health-
containing and health-free conversations. We then
mark each conversation as temporally near a med-
ical event if a reported medical event occurred
within a 3-week time window. We chose a 3-week
window to allow for one report before and after
the event.
Table 3 shows the number of conversations in
different categories. At first glance it might seem
like the number of false positives or false nega-
tives is quite large but we should notice that be-
ing near a medical event is not the ground truth
here. We just want to see if there is any associa-
tion between occurrence of health-related conver-
sations and occurrence of an actual medical event
in lives of our subjects. We can see that 90.9%
</bodyText>
<page confidence="0.996994">
42
</page>
<bodyText confidence="0.9977805">
of the conversations are classified as health-related
but this percentage is slightly different for con-
versations near medical events(91.5%) vs. for the
other conversations (89.1). This slight difference
is significant according to x2 test of independence
(x2(df = 1, N = 47288) = 61.17, p &lt; 0.001).
</bodyText>
<table confidence="0.9783835">
near a Classified as
medical event health-related non-health-related
yes 1348 11067
no 2964 31909
</table>
<tableCaption confidence="0.989922">
Table 3: Number of telephone conversations in
</tableCaption>
<bodyText confidence="0.867451142857143">
different categories. Each conversation is consid-
ered near a medical even if and only if there is
at least one self-report in a window of 3 weeks
around its date. Being near a medical event does
not reveal the true nature of the conversation and
thus is not the ground truth. So, there are no false
positive, true positive, etc. in this table.
</bodyText>
<sectionHeader confidence="0.999534" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999974333333333">
In this paper, we investigated the problem of iden-
tifying conversations with any mention of health.
The private nature of our everyday telephone con-
versations corpus poses constraints on manual
transcription and annotation. Looking up phone
numbers associated with business calls, we labeled
a small set of conversations when the other end
was a business clearly related or unrelated to the
health industry. However, the labeled set is not
large enough for training a robust classifier. We
developed a semi-supervised iterative method for
selecting features, where we learn a distribution
of words on health topic using TLDA, and sub-
sequently filter irrelevant words iteratively. We
demonstrate that our method generalizes well and
improves the average F-score on in-domain and
out-of-domain tasks over two baselines, using full
vocabulary without feature selection or feature se-
lection using TLDA alone. In our task, the gener-
alization of the classifier is important since we are
interested in detecting not only conversations on
health with business (the annotated examples) but
also with others in subjects’ social network. Using
our classifier, we find a significant statistical as-
sociation between the occurrence of conversations
about health and the occurrence of self-reported
medical events.
</bodyText>
<sectionHeader confidence="0.99549" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999943916666666">
This research was supported in part by NIH Grants
1K25AG033723, and P30 AG008017, as well as
by NSF Grants 1027834, and 0964102. Any opin-
ions, findings, conclusions or recommendations
expressed in this publication are those of the au-
thors and do not necessarily reflect the views of the
NIH. We thank Nicole Larimer for help in collect-
ing the data, Maider Lehr for testing the data col-
lection devices and Katherine Wild for early dis-
cussions on this project. We are grateful to Brian
Kingsbury and his colleagues for providing us ac-
cess to IBM’s attila software tools.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999902611111111">
David Andrzejewski and Xiaojin Zhu. 2009. La-
tent dirichlet allocation with topic-in-set knowledge.
In Proceedings of the NAACL HLT 2009 Workshop
on Semi-Supervised Learning for Natural Language
Processing, SemiSupLearn ’09, pages 43–48.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
C.-C. Chang and C.-J. Lin. 2011. Libsvm : a library
for support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The fisher corpus: a resource for the next
generations of speech-to-text. In LREC, volume 4,
pages 69–71.
John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Acoustics,
Speech, and Signal Processing, 1992. ICASSP-92.,
1992 IEEE International Conference on, volume 1,
pages 517–520. IEEE.
Hong-qi Han, Dong-Hua Zhu, and Xue-feng Wang.
2009. Semi-supervised text classification from un-
labeled documents using class associated words. In
Computers &amp; Industrial Engineering, 2009. CIE
2009. International Conference on, pages 1255–
1260. IEEE.
Toru Hisamitsu and Yoshiki Niwa. 2001. Topic-word
selection based on combinatorial probability. In NL-
PRS, volume 1, page 289.
Jagadeesh Jagarlamudi, Hal Daum´e III, and Raghaven-
dra Udupa. 2012. Incorporating lexical priors into
topic models. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 204–213. Associ-
ation for Computational Linguistics.
</reference>
<page confidence="0.995561">
43
</page>
<reference confidence="0.999249242424243">
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th international conference on Computational
Linguistics, page 1367. Association for Computa-
tional Linguistics.
Hang Li and Kenji Yamanishi. 2003. Topic analysis
using a finite mixture model. Information process-
ing &amp; management, 39(4):521–541.
Kyle W. Prier, Matthew S. Smith, Christophe Giraud-
Carrier, and Carl L. Hanson. 2011. Identifying
health-related topics on twitter: an exploration of
tobacco-related tweets as a test topic. In Proceed-
ings of the 4th international conference on Social
computing, behavioral-cultural modeling and pre-
diction, pages 18–25.
Hagen Soltau, Brian Kingsbury, Lidia Mangu, Daniel
Povey, George Saon, and Geoffrey Zweig. 2005.
The ibm 2004 conversational telephony system for
rich transcription. In Proc. ICASSP, volume 1,
pages 205–208.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opinion
sentences. In Proceedings of the 2003 conference on
Empirical methods in natural language processing,
pages 129–136. Association for Computational Lin-
guistics.
Taras Zagibalov and John Carroll. 2008. Automatic
seed word selection for unsupervised sentiment clas-
sification of chinese text. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 1073–1080. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.999294">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.767351">
<title confidence="0.999358">Detecting Health Related Discussions in Everyday Conversations for Studying Medical Events in the Lives of Older Adults</title>
<author confidence="0.995861">Golnar Sheikhshab</author>
<author confidence="0.995861">Izhak Shafran</author>
<author confidence="0.995861">Jeffrey</author>
<affiliation confidence="0.983804">Oregon Health &amp; Science</affiliation>
<email confidence="0.999261">sheikhsh,shafrani,kaye@ohsu.edu</email>
<abstract confidence="0.990839708333333">We apply semi-supervised topic modeling techniques to detect health-related discussions in everyday telephone conversations, which has applications in large-scale epidemiological studies and for clinical interventions for older adults. The privacy requirements associated with utilizing everyday telephone conversations preclude manual annotations; hence, we explore semi-supervised methods in this task. We adopt a semi-supervised version of Latent Dirichlet Allocation (LDA) to guide the learning process. Within this framework, we investigate a strategy to discard irrelevant words in the topic distribution and demonstrate that this strategy improves the average F-score on the in-domain task and an out-of-domain task (Fisher corpus). Our results show that the increase in discussion of health related conversations is statistically associated with actual medical events obtained through weekly selfreports.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Andrzejewski</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Latent dirichlet allocation with topic-in-set knowledge.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing, SemiSupLearn ’09,</booktitle>
<pages>43--48</pages>
<contexts>
<context position="6686" citStr="Andrzejewski and Zhu, 2009" startWordPosition="1037" endWordPosition="1040"> An effective strategy to cluster words into topics, is Latent Dirichlet Allocation (LDA) (Blei et al., 2003) . However, LDA is an unsupervised algorithm and the clustered topics are not guaranteed to include the topic of interest. The Seeded LDA, a variant of LDA, attempts to address this problem by incorporating the seed words as priors over the topics (Jagarlamudi et al., 2012). However, the estimation procedure is more complicated. Alternatively, in Topic LDA (TLDA), a clever extension to LDA, Andrzejewski and Zhu address this problem by fixing the membership of the words to valid topics (Andrzejewski and Zhu, 2009). When the focus is on detecting just one topic, as in our task, we can expand the seed words more selectively using the small set of labeled data and that is the approach adopted in this paper. 3 Data One interesting aspect of our study is the uniqueness of our corpus, which is both naturalistic and exhaustive. We recorded about 41,000 land-line everyday telephone conversations from 56 volunteers, 65 years or older, over a period of approximately 6 to 12 months. Since these everyday telephone conversations are private conversations, and might include private information such as names, telepho</context>
<context position="12755" citStr="Andrzejewski and Zhu, 2009" startWordPosition="2026" endWordPosition="2029">se, especially because the data we use for training is limited in number and the nature of conversations. One way to improve generalization is to perform feature selection. That is, instead of using tf-idf scores for the whole vocabulary, we would like to rely only on features relevant to detecting the health topic. We propose a new way for feature selection for retrieving documents containing information about a specific topic when there is only a limited set of labeled documents available. The idea is to pick a few words highly related to the topic of interest as seed words and to use TLDA (Andrzejewski and Zhu, 2009) to force those seed words into one (for example, the first) topic. In our task, the topic of interest is health. So, we choose doctor, medicine, and pain – often used while discussing health – as our seed words. Topics in LDA based methods such as TLDA are usually represented using the n most probable words; where n is an arbitrary number. So, the first candidate sets for expanding our seed words are the sets of 50 most probable words in the topic of health in dif40 ferent runs of TLDA. As our experiments reveal, these candidate sets contain many words that are unrelated to health. To solve t</context>
</contexts>
<marker>Andrzejewski, Zhu, 2009</marker>
<rawString>David Andrzejewski and Xiaojin Zhu. 2009. Latent dirichlet allocation with topic-in-set knowledge. In Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing, SemiSupLearn ’09, pages 43–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="6168" citStr="Blei et al., 2003" startWordPosition="951" endWordPosition="954">d, as in Hisamitsu and Niwa’s work (Hisamitsu and Niwa, 2001). However, it is computationally expensive to search over subsets of words. Depending on the language and task, heuristics might be applicable. An example of this kind of approach is Zagibalov and Carroll’s work on sentiment analysis in Chinese (Zagibalov and Carroll, 2008). Alternatively, we can treat the task of identifying words associated with seed words as a clustering problem with the intuition that the seed words are in the same cluster. An effective strategy to cluster words into topics, is Latent Dirichlet Allocation (LDA) (Blei et al., 2003) . However, LDA is an unsupervised algorithm and the clustered topics are not guaranteed to include the topic of interest. The Seeded LDA, a variant of LDA, attempts to address this problem by incorporating the seed words as priors over the topics (Jagarlamudi et al., 2012). However, the estimation procedure is more complicated. Alternatively, in Topic LDA (TLDA), a clever extension to LDA, Andrzejewski and Zhu address this problem by fixing the membership of the words to valid topics (Andrzejewski and Zhu, 2009). When the focus is on detecting just one topic, as in our task, we can expand the</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-C Chang</author>
<author>C-J Lin</author>
</authors>
<title>Libsvm : a library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<contexts>
<context position="14553" citStr="Chang and Lin, 2011" startWordPosition="2340" endWordPosition="2343">ion as health related if, and only if, it contains at least one of the seed words. The algorithm terminates when the subset selection is unable to add a new word contributing to the average of precision and recall. The tf-idf vector for the expanded set represents the conversations in the classification process. It is worth mentioning that we train TLDA using all 41000 unlabeled conversations, and chose the number of topics, K, to be 20. 5 Experiments In all of our experiments, we trained SVM classifiers, with different features, to detect the conversations on health using the popular libSVM (Chang and Lin, 2011) implementation. We chose the parameters of the SVM using a 30-fold cross-validated (CV) grid search over the training data. We also used a 4-fold cross validation over the labeled set of conversations to maximize the use of the relatively small labeled set. That is, we trained the feature selection algorithm on 3-folds and tested the resulting SVM tested on the fourth. In in-domain task we always report the average performance across the folds. Table 1 shows the results of our experiments using different input features. We report on recall, precision and F-measure in in-domain and out-of-doma</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>C.-C. Chang and C.-J. Lin. 2011. Libsvm : a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Cieri</author>
<author>David Miller</author>
<author>Kevin Walker</author>
</authors>
<title>The fisher corpus: a resource for the next generations of speech-to-text.</title>
<date>2004</date>
<booktitle>In LREC,</booktitle>
<volume>4</volume>
<pages>69--71</pages>
<contexts>
<context position="10543" citStr="Cieri et al., 2004" startWordPosition="1652" endWordPosition="1655">biguous information for many of available phone numbers, we managed to label only 681 conversations – 275 health-related and 406 non-health-related. This labeled set has another limitation: it contains conversations to business numbers only. In reality however, we are interested in the much larger set of conversations between friends, relatives, and other members of subjects’ social support network. Thus, the generalizability of the classifier we train is very important. Fisher Corpus To explicitly test the generalizability of our classifier, we use a second evaluation set from Fisher corpus (Cieri et al., 2004). Fisher corpus contains telephone conversations with preassigned topics. There are 40 topics and only one of them, illness, is health-related. We identified 338 conversations on illness, and sampled 702 conversations from the other 39 non-health topics. Since we do not train on Fisher corpus, we call it the out-of-domain task to apply our method on Fisher corpus; as opposed to the in-domain task which is to apply our method on the everyday telephone conversations. Extra information on subjects’ health In the everyday telephone conversations corpus, we also have access to the subjects’ weekly </context>
</contexts>
<marker>Cieri, Miller, Walker, 2004</marker>
<rawString>Christopher Cieri, David Miller, and Kevin Walker. 2004. The fisher corpus: a resource for the next generations of speech-to-text. In LREC, volume 4, pages 69–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward C Holliman</author>
<author>Jane McDaniel</author>
</authors>
<title>Switchboard: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>517--520</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="7876" citStr="Godfrey et al., 1992" startWordPosition="1227" endWordPosition="1230">ormation such as names, telephone numbers, or banking information, we assured the subjects that no one would listen to the recorded conversations. Thus, we couldn’t manually transcribe the conversations; instead, we used an Automatic Speech Recognition (ASR) system that we describe here. Automatic Speech Recognition System Conversations in our corpus were automatically transcribed using an ASR system, which is structured after IBM’s conversation telephony system (Soltau et al., 2005). The acoustic models were trained on about 2000 hours of telephone speech from Switchboard and Fisher corpora (Godfrey et al., 1992). The system has a vocabulary of 47K and uses a trigram language model with about 10M n-grams, estimated from a mix of transcripts and web-harvested data. Decoding is performed in three stages using speaker-independent models, vocal-tract normalized models and speakeradapted models. The three sets of models are similar in complexity with 4000 clustered pentaphone states and 150K Gaussians with diagonal covariances. Our system does not include discriminative training and performs at a word error rate of about 24% on NIST RT Dev04 which is comparable to state of the art performance for such syst</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>John J Godfrey, Edward C Holliman, and Jane McDaniel. 1992. Switchboard: Telephone speech corpus for research and development. In Acoustics, Speech, and Signal Processing, 1992. ICASSP-92., 1992 IEEE International Conference on, volume 1, pages 517–520. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong-qi Han</author>
<author>Dong-Hua Zhu</author>
<author>Xue-feng Wang</author>
</authors>
<title>Semi-supervised text classification from unlabeled documents using class associated words.</title>
<date>2009</date>
<booktitle>In Computers &amp; Industrial Engineering,</booktitle>
<pages>1255--1260</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5085" citStr="Han et al., 2009" startWordPosition="775" endWordPosition="778"> set using WordNet, we need to be careful to avoid antonyms and words that have high degree of linkage with many words in the vocabulary. Furthermore, we can not apply such an approach for languages with poor resources, where manually curated knowledge is unavailable. The other drawback of this approach is that we can not use characteristics of the end task, in our case health-related conversation retrieval, to select the words. As an alternative method, Han and colleagues developed an interactive system where users selected the most relevant words from a set, proposed by an automated system (Han et al., 2009). Another idea for expanding the seed words is using the statistical information. Among statistical methods, the simplest approach is to compute pairwise co-occurrence with the seed words. Li and Yamanishi ranked the words co-occurring with the seed words according to information theoretic costs, and used the highest ranked words as the expanded set (Li and Yamanishi, 2003). This idea can be more effective when the co-occurrence is performed over subsets instead, as in Hisamitsu and Niwa’s work (Hisamitsu and Niwa, 2001). However, it is computationally expensive to search over subsets of words</context>
</contexts>
<marker>Han, Zhu, Wang, 2009</marker>
<rawString>Hong-qi Han, Dong-Hua Zhu, and Xue-feng Wang. 2009. Semi-supervised text classification from unlabeled documents using class associated words. In Computers &amp; Industrial Engineering, 2009. CIE 2009. International Conference on, pages 1255– 1260. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toru Hisamitsu</author>
<author>Yoshiki Niwa</author>
</authors>
<title>Topic-word selection based on combinatorial probability.</title>
<date>2001</date>
<booktitle>In NLPRS,</booktitle>
<volume>1</volume>
<pages>289</pages>
<contexts>
<context position="5611" citStr="Hisamitsu and Niwa, 2001" startWordPosition="859" endWordPosition="862">ers selected the most relevant words from a set, proposed by an automated system (Han et al., 2009). Another idea for expanding the seed words is using the statistical information. Among statistical methods, the simplest approach is to compute pairwise co-occurrence with the seed words. Li and Yamanishi ranked the words co-occurring with the seed words according to information theoretic costs, and used the highest ranked words as the expanded set (Li and Yamanishi, 2003). This idea can be more effective when the co-occurrence is performed over subsets instead, as in Hisamitsu and Niwa’s work (Hisamitsu and Niwa, 2001). However, it is computationally expensive to search over subsets of words. Depending on the language and task, heuristics might be applicable. An example of this kind of approach is Zagibalov and Carroll’s work on sentiment analysis in Chinese (Zagibalov and Carroll, 2008). Alternatively, we can treat the task of identifying words associated with seed words as a clustering problem with the intuition that the seed words are in the same cluster. An effective strategy to cluster words into topics, is Latent Dirichlet Allocation (LDA) (Blei et al., 2003) . However, LDA is an unsupervised algorith</context>
</contexts>
<marker>Hisamitsu, Niwa, 2001</marker>
<rawString>Toru Hisamitsu and Yoshiki Niwa. 2001. Topic-word selection based on combinatorial probability. In NLPRS, volume 1, page 289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jagadeesh Jagarlamudi</author>
<author>Hal Daum´e</author>
<author>Raghavendra Udupa</author>
</authors>
<title>Incorporating lexical priors into topic models.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>204--213</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Jagarlamudi, Daum´e, Udupa, 2012</marker>
<rawString>Jagadeesh Jagarlamudi, Hal Daum´e III, and Raghavendra Udupa. 2012. Incorporating lexical priors into topic models. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 204–213. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>1367</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="4397" citStr="Kim and Hovy, 2004" startWordPosition="660" endWordPosition="663">June 26-27 2014. c�2014 Association for Computational Linguistics modeling because in this task we are interested in one particular topic. A similar study is the work of Prier and colleagues (Prier et al., 2011). They use a set of predefined seed words as queries to gather tweets related to tobacco or marijuana usage, and then use LDA to discover related subtopics. Thus, their method is sensitive to the seed words chosen. One way to reduce the sensitivity to the manually specified seed words is to expand the set using WordNet. Researchers have investigated this approach in sentiment analysis (Kim and Hovy, 2004; Yu and Hatzivassiloglou, 2003). However, when expanding the seed word set using WordNet, we need to be careful to avoid antonyms and words that have high degree of linkage with many words in the vocabulary. Furthermore, we can not apply such an approach for languages with poor resources, where manually curated knowledge is unavailable. The other drawback of this approach is that we can not use characteristics of the end task, in our case health-related conversation retrieval, to select the words. As an alternative method, Han and colleagues developed an interactive system where users selecte</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th international conference on Computational Linguistics, page 1367. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Kenji Yamanishi</author>
</authors>
<title>Topic analysis using a finite mixture model.</title>
<date>2003</date>
<journal>Information processing &amp; management,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="5461" citStr="Li and Yamanishi, 2003" startWordPosition="835" endWordPosition="838">se health-related conversation retrieval, to select the words. As an alternative method, Han and colleagues developed an interactive system where users selected the most relevant words from a set, proposed by an automated system (Han et al., 2009). Another idea for expanding the seed words is using the statistical information. Among statistical methods, the simplest approach is to compute pairwise co-occurrence with the seed words. Li and Yamanishi ranked the words co-occurring with the seed words according to information theoretic costs, and used the highest ranked words as the expanded set (Li and Yamanishi, 2003). This idea can be more effective when the co-occurrence is performed over subsets instead, as in Hisamitsu and Niwa’s work (Hisamitsu and Niwa, 2001). However, it is computationally expensive to search over subsets of words. Depending on the language and task, heuristics might be applicable. An example of this kind of approach is Zagibalov and Carroll’s work on sentiment analysis in Chinese (Zagibalov and Carroll, 2008). Alternatively, we can treat the task of identifying words associated with seed words as a clustering problem with the intuition that the seed words are in the same cluster. A</context>
</contexts>
<marker>Li, Yamanishi, 2003</marker>
<rawString>Hang Li and Kenji Yamanishi. 2003. Topic analysis using a finite mixture model. Information processing &amp; management, 39(4):521–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyle W Prier</author>
<author>Matthew S Smith</author>
<author>Christophe GiraudCarrier</author>
<author>Carl L Hanson</author>
</authors>
<title>Identifying health-related topics on twitter: an exploration of tobacco-related tweets as a test topic.</title>
<date>2011</date>
<booktitle>In Proceedings of the 4th international conference on Social computing, behavioral-cultural</booktitle>
<pages>18--25</pages>
<contexts>
<context position="3990" citStr="Prier et al., 2011" startWordPosition="590" endWordPosition="593">ibed in Section 4. We evaluate the efficacy of our approach empirically, on the in-domain as well as an out-of-domain corpus, and report results in Section 5. 2 Related Work The task of identifying conversations where health is mentioned differs from many other tasks in topic 38 Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 38–44, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics modeling because in this task we are interested in one particular topic. A similar study is the work of Prier and colleagues (Prier et al., 2011). They use a set of predefined seed words as queries to gather tweets related to tobacco or marijuana usage, and then use LDA to discover related subtopics. Thus, their method is sensitive to the seed words chosen. One way to reduce the sensitivity to the manually specified seed words is to expand the set using WordNet. Researchers have investigated this approach in sentiment analysis (Kim and Hovy, 2004; Yu and Hatzivassiloglou, 2003). However, when expanding the seed word set using WordNet, we need to be careful to avoid antonyms and words that have high degree of linkage with many words in </context>
</contexts>
<marker>Prier, Smith, GiraudCarrier, Hanson, 2011</marker>
<rawString>Kyle W. Prier, Matthew S. Smith, Christophe GiraudCarrier, and Carl L. Hanson. 2011. Identifying health-related topics on twitter: an exploration of tobacco-related tweets as a test topic. In Proceedings of the 4th international conference on Social computing, behavioral-cultural modeling and prediction, pages 18–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hagen Soltau</author>
<author>Brian Kingsbury</author>
<author>Lidia Mangu</author>
<author>Daniel Povey</author>
<author>George Saon</author>
<author>Geoffrey Zweig</author>
</authors>
<title>The ibm</title>
<date>2005</date>
<booktitle>In Proc. ICASSP,</booktitle>
<volume>1</volume>
<pages>205--208</pages>
<contexts>
<context position="7743" citStr="Soltau et al., 2005" startWordPosition="1206" endWordPosition="1209"> approximately 6 to 12 months. Since these everyday telephone conversations are private conversations, and might include private information such as names, telephone numbers, or banking information, we assured the subjects that no one would listen to the recorded conversations. Thus, we couldn’t manually transcribe the conversations; instead, we used an Automatic Speech Recognition (ASR) system that we describe here. Automatic Speech Recognition System Conversations in our corpus were automatically transcribed using an ASR system, which is structured after IBM’s conversation telephony system (Soltau et al., 2005). The acoustic models were trained on about 2000 hours of telephone speech from Switchboard and Fisher corpora (Godfrey et al., 1992). The system has a vocabulary of 47K and uses a trigram language model with about 10M n-grams, estimated from a mix of transcripts and web-harvested data. Decoding is performed in three stages using speaker-independent models, vocal-tract normalized models and speakeradapted models. The three sets of models are similar in complexity with 4000 clustered pentaphone states and 150K Gaussians with diagonal covariances. Our system does not include discriminative train</context>
</contexts>
<marker>Soltau, Kingsbury, Mangu, Povey, Saon, Zweig, 2005</marker>
<rawString>Hagen Soltau, Brian Kingsbury, Lidia Mangu, Daniel Povey, George Saon, and Geoffrey Zweig. 2005. The ibm 2004 conversational telephony system for rich transcription. In Proc. ICASSP, volume 1, pages 205–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 conference on Empirical methods in natural language processing,</booktitle>
<pages>129--136</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4429" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="664" endWordPosition="667">014 Association for Computational Linguistics modeling because in this task we are interested in one particular topic. A similar study is the work of Prier and colleagues (Prier et al., 2011). They use a set of predefined seed words as queries to gather tweets related to tobacco or marijuana usage, and then use LDA to discover related subtopics. Thus, their method is sensitive to the seed words chosen. One way to reduce the sensitivity to the manually specified seed words is to expand the set using WordNet. Researchers have investigated this approach in sentiment analysis (Kim and Hovy, 2004; Yu and Hatzivassiloglou, 2003). However, when expanding the seed word set using WordNet, we need to be careful to avoid antonyms and words that have high degree of linkage with many words in the vocabulary. Furthermore, we can not apply such an approach for languages with poor resources, where manually curated knowledge is unavailable. The other drawback of this approach is that we can not use characteristics of the end task, in our case health-related conversation retrieval, to select the words. As an alternative method, Han and colleagues developed an interactive system where users selected the most relevant words from a</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of the 2003 conference on Empirical methods in natural language processing, pages 129–136. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taras Zagibalov</author>
<author>John Carroll</author>
</authors>
<title>Automatic seed word selection for unsupervised sentiment classification of chinese text.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>1073--1080</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5885" citStr="Zagibalov and Carroll, 2008" startWordPosition="902" endWordPosition="905"> the seed words. Li and Yamanishi ranked the words co-occurring with the seed words according to information theoretic costs, and used the highest ranked words as the expanded set (Li and Yamanishi, 2003). This idea can be more effective when the co-occurrence is performed over subsets instead, as in Hisamitsu and Niwa’s work (Hisamitsu and Niwa, 2001). However, it is computationally expensive to search over subsets of words. Depending on the language and task, heuristics might be applicable. An example of this kind of approach is Zagibalov and Carroll’s work on sentiment analysis in Chinese (Zagibalov and Carroll, 2008). Alternatively, we can treat the task of identifying words associated with seed words as a clustering problem with the intuition that the seed words are in the same cluster. An effective strategy to cluster words into topics, is Latent Dirichlet Allocation (LDA) (Blei et al., 2003) . However, LDA is an unsupervised algorithm and the clustered topics are not guaranteed to include the topic of interest. The Seeded LDA, a variant of LDA, attempts to address this problem by incorporating the seed words as priors over the topics (Jagarlamudi et al., 2012). However, the estimation procedure is more</context>
</contexts>
<marker>Zagibalov, Carroll, 2008</marker>
<rawString>Taras Zagibalov and John Carroll. 2008. Automatic seed word selection for unsupervised sentiment classification of chinese text. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 1073–1080. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000468">
<title confidence="0.992681">
Finding Arguing Expressions of Divergent Viewpoints in Online Debates
</title>
<author confidence="0.991072">
Amine Trabelsi
</author>
<affiliation confidence="0.997948">
Department of Computing Science
University of Alberta
</affiliation>
<email confidence="0.969655">
atrabels@ualberta.ca
</email>
<author confidence="0.984671">
Osmar R. Za¨ıane
</author>
<affiliation confidence="0.9978995">
Department of Computing Science
University of Alberta
</affiliation>
<email confidence="0.983899">
zaiane@ualberta.ca
</email>
<sectionHeader confidence="0.993564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999955444444444">
This work suggests a fine-grained min-
ing of contentious documents, specifically
online debates, towards a summarization
of contention issues. We propose a Joint
Topic Viewpoint model (JTV) for the un-
supervised identification and the cluster-
ing of arguing expressions according to
the latent topics they discuss and the im-
plicit viewpoints they voice. A set of ex-
periments is conducted on online debates
documents. Qualitative and quantitative
evaluations of the model’s output are per-
formed in context of different contention
issues. Analysis of experimental results
shows the effectiveness of the proposed
model to automatically and accurately de-
tect recurrent patterns of arguing expres-
sions in online debate texts.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998715625">
This paper addresses the issue of improving the
quality of opinion mining from online contentious
texts like the posts in debate sites. Mining and
summarizing these new resources is crucial, es-
pecially when the opinion is related to a subject
that stimulates divergent viewpoints within peo-
ple (e.g. Healthcare Reform, Same-Sex Marriage).
We refer to such subjects as issues of contentions.
A contentious issue is “likely to cause disagree-
ment between people” (cf. Oxford Dictionaries).
Documents such as debate sites’ posts may contain
multiple contrastive viewpoints regarding a partic-
ular issue of contention. Table 1 presents an exam-
ple of short-text documents expressing divergent
opinions where each is exclusively supporting or
opposing a healthcare legislation 1.
</bodyText>
<footnote confidence="0.709478666666667">
1extracted from a Gallup Inc. survey
http://www.gallup.com/poll/126521/favor-oppose-obama-
healthcare-plan.aspx
</footnote>
<bodyText confidence="0.99993385">
Opinion in contentious issues is often expressed
implicitly, not necessarily through the usage of
usual negative or positive opinion words, like
“bad” or “great”. This makes its extraction a chal-
lenging task. It is usually conveyed through the
arguing expression justifying the endorsement of
a particular point of view. The act of arguing is
“to give reasons why you think that something is
right/wrong, true/not true, etc, especially to per-
suade people that you are right” (cf. Oxford Dic-
tionaries). For example, the arguing expression
“many people do not have healthcare”, in Table 1,
implicitly explains that the reform is intended to
fix the problem of uninsured people, and thus, the
opinion is probably on the supporting side. On the
other hand, the arguing expression “it will produce
too much debt” denotes the negative consequence
that may result from passing the bill, making it on
the opposing side.
The automatic identification and clustering of
these kind of arguing expressions, according to
their topics and the viewpoints they convey, is en-
ticing for a variety of application domains. For in-
stance, it can save journalists a substantial amount
of work and provide them with drafting elements
(viewpoints and associated arguing expressions)
about controversial issues. In addition, it would
enhance the output quality of the opinion summa-
rization task in general.
The rest of this paper is organized as follows.
Section 2 covers the details of the problem state-
ment. Section 3 explains the key issues in the con-
text of recent related work. Section 4 provides
the technical details of our model, the Joint Topic
Viewpoint model (JTV) . Section 5 describes the
clustering task that might be used to obtain a fea-
sible solution. Section 6 provides a description of
the experimental set up. Section 7 assesses the ad-
equacy and the performance of our solution. Sec-
tion 8 concludes the paper.
</bodyText>
<page confidence="0.991679">
35
</page>
<note confidence="0.994343">
Proceedings of the 5th Workshop on Language Anal sis for Social Media (LASM) @ EACL 2014, pages 35–43,
Gothenburg, Sweden, April 26-30 2014. (c 2014 Association for Computational Linguistics
</note>
<table confidence="0.80177025">
Support Viewpoint Oppose Viewpoint
Many people do not have health care The government should not be involved
Provide health care for 30 million people It will produce too much debt
The government should help old people The bill would not help the people
</table>
<tableCaption confidence="0.999035">
Table 1: Excerpts of support and opposition opinion to a healthcare bill in the USA.
</tableCaption>
<sectionHeader confidence="0.9279" genericHeader="introduction">
2 Problem Statement
</sectionHeader>
<bodyText confidence="0.999777">
This paper examines the task of mining the topics
and the viewpoints of arguing expressions towards
the summarization of contentious text. An exam-
ple of a human-made summary of arguing expres-
sions (Jones, 2010) on, what is commonly known
as, the Obama healthcare reform is presented in
Table 2. Ultimately, the target is to automatically
generate similar summaries given a corpus of con-
tentious documents. However, this paper tack-
les the sub-problem of identifying recurrent words
and phrases expressing arguing and cluster them
according to their topics and viewpoints. This
would help solve the general problem. We use
Table 2’s examples to define some key concepts
which can help us formulate this latter. Here, the
contentious issue yielding the divergent positions
is the Obama healthcare. The documents are peo-
ple’s verbatim responses to the question “Why do
you favor or oppose a healthcare legislation simi-
lar to President Obama’s ?”.
We define a contention question as a question
that can generate expressions of two or more di-
vergent viewpoints as a response.
While the previous question explicitly asks for
the reasons (“why”), we relax this constraint and
consider also usual opinion questions like “Is the
passing of Obamacare bad for Americans ?” or
“Do you favor or oppose Obamacare ?”.
A contentious document is a document that
contains expressions of one or more divergent
viewpoints in response to the contention question.
In the context of online debate, a post usually ex-
presses one viewpoint, although it can mention ar-
guing used to justify a different viewpoint.
Table 2 is split into two parts according to the
viewpoint: supporting or opposing the healthcare
bill. Each row contains one or more phrases, each
expressing a reason (or an explanation), e.g. “costs
are out of control” and “would help control costs”.
Though lexically different, these phrases share a
common hidden theme (or topic), e.g. insurance’s
cost, and implicitly convey the same hidden view-
point’s semantics, e.g. support the healthcare bill.
Thus, we define an arguing expression as the set
of reasons (words or phrases) sharing a common
topic and justifying the same viewpoint regarding
a contentious issue.
We assume that a viewpoint (e.g. a column of
Table 2) in a contentious document is a stance, in
response to a contention question, which is implic-
itly expressed by a set of arguing expressions (e.g.
rows of a column in Table 2).
Thus, the arguing expressions voicing the same
viewpoint differ in their topics, but agree in the
stance. For example, arguing expressions repre-
sented by “system is broken” and “costs are out
of control” discuss different topics, i.e. healthcare
system and insurance’s cost, but both support the
healthcare bill. On the other hand, arguing ex-
pressions of divergent viewpoints may have sim-
ilar topic or may not. For instance, “government
should help elderly” and “government should not
be involved” share the same topic, i.e. govern-
ment’s role, while conveying opposed viewpoints.
Our research problem and objectives in terms
of the newly introduced concepts are stated as
follows. Given a corpus of unlabeled con-
tentious documents {doc1, doc2, .., docD}, where
each document docd expresses one or more view-
points ~vd from a set of L possible viewpoints
{v1, v2, .., vL}, and each viewpoint vl can be con-
veyed using one or more arguing expressions ~φl
from a set of possible arguing expressions dis-
cussing K different topics {φ1l, φ2l, ..,φKl}, the
objective is to perform the following two tasks:
</bodyText>
<listItem confidence="0.818022833333333">
1. automatically extracting coherent words and
phrases describing any distinct arguing ex-
pression φkl;
2. grouping extracted distinct arguing expres-
sions φkl for different topics, k = 1..K, into
their corresponding viewpoint vl.
</listItem>
<bodyText confidence="0.99980525">
This paper focuses on the first task while laying
the ground for solving the second one. In carry-
ing out the first task, we must meet the main chal-
lenge of recognizing arguing expressions having
</bodyText>
<page confidence="0.998495">
36
</page>
<table confidence="0.990588285714286">
Support Viewpoint Oppose Viewpoint
People need health insurance/too many uninsured Will raise cost of insurance/ less affordable
System is broken/needs to be fixed Does not address real problems
Costs are out of control/would help control costs Need more information on how it works
Moral responsibility to provide/Fair Against big government involvement (general)
Would make healthcare more affordable Government should not be involved in healthcare
Don’t trust insurance companies Cost the government too much
</table>
<tableCaption confidence="0.999593">
Table 2: Human-made summary of arguing expressions supporting and opposing Obamacare.
</tableCaption>
<bodyText confidence="0.9991625">
the same topic and viewpoint but which are lexi-
cally different, e.g. “provide health care for 30
million people ” and “ many people do not have
healthcare”. For this purpose we propose a Joint
Topic Viewpoint Model (JTV) to account for the
dependence structure of topics and viewpoints.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="related work">
3 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999959">
3.1 Classifying Stances
</subsectionHeader>
<bodyText confidence="0.999992130434783">
An early body of work addresses the challenge of
classifying viewpoints in contentious or ideolog-
ical discourses using supervised techniques (Kim
and Hovy, 2007; Lin et al., 2006). Although the
models give good performances, they remain data-
dependent and costly to label, making the unsuper-
vised approach more appropriate for the existing
huge quantity of online data. A similar trend of
studies scrutinizes the discourse aspect of a docu-
ment in order to identify opposed stances (Thomas
et al., 2006; Park et al., 2011). However, these
methods utilize polarity lexicon to detect opinion-
ated text and do not look for arguing expression,
which is shown to be useful in recognizing op-
posed stances (Somasundaran and Wiebe, 2010).
Somasundaran and Wiebe (2010) classify ideolog-
ical stances in online debates using a generated ar-
guing clues from the Multi Perspective Question
Answering (MPQA) opinion corpus2. Our prob-
lem is not to classify documents, but to recognize
recurrent pattern of arguing phrases instead of ar-
guing clues. Moreover, our approach is indepen-
dent of any annotated corpora.
</bodyText>
<subsectionHeader confidence="0.999679">
3.2 Topic Modeling in Reviews Data
</subsectionHeader>
<bodyText confidence="0.9998145">
Another emerging body of work applies proba-
bilistic topic models on reviews data to extract ap-
praisal aspects and the corresponding specific sen-
timent lexicon. These kinds of models are usually
referred to as joint sentiment/aspect topic models
(Jo and Oh, 2011; Titov and McDonald, 2008;
</bodyText>
<footnote confidence="0.74122">
2http://mpqa.cs.pitt.edu/
</footnote>
<bodyText confidence="0.998641222222223">
Zhao et al., 2010). Lin and He (2009) propose the
Joint Sentiment Topic Model (JST) to model the
dependency between sentiment and topics. They
make the assumption that topics discussed on a re-
view are conditioned on sentiment polarity. Re-
versely, our JTV model assumes that a viewpoint
endorsement (e.g., oppose reform) is conditioned
on the discussed topic (e.g., government’s role)
and its application is different from that of JST.
Most of the joint aspect sentiment topic models are
either semi-supervised or weakly supervised using
sentiment polarity words (Paradigm lists) to boost
their efficiency. In our case, viewpoints are often
expressed implicitly and finding specific arguing
lexicon for different stances is a challenging task
in itself. Indeed, our model is enclosed in another
body of work that based on a probabilistic Topic
Model framework to mine divergent viewpoints.
</bodyText>
<subsectionHeader confidence="0.998959">
3.3 Topic Modeling in Contentious Text
</subsectionHeader>
<bodyText confidence="0.999767666666667">
A recent study by Mukherjee and Liu (2012)
examines mining contention from discussion fo-
rums data where the interaction between differ-
ent authors is pivotal. It attempts to jointly
discover contention/agreement indicators (CA-
Expressions) and topics using three different Joint
Topic Expressions Models (JTE). The JTEs’ out-
put is used to discover points (topics) of con-
tention. The model supposes that people ex-
press agreement or disagreement through CA-
expressions. However, this is not often the case
when people express their viewpoint via other
channels than discussion forums like debate sites
or editorials. Moreover, agreement or disagree-
ment may also be conveyed implicitly through ar-
guing expressions rejecting or supporting another
opinion. JTEs do not model viewpoints and use
the supervised Maximum Entropy model to detect
CA-expressions.
Recently, Gottipati et al. (2013) propose a topic
model to infer human interpretable text in the do-
</bodyText>
<page confidence="0.99654">
37
</page>
<bodyText confidence="0.99995824">
main of issues using Debatepedia3 as a corpus of
evidence. Debatepedia is an online authored en-
cyclopedia to summarize and organize the main
arguments of two possible positions. The model
takes advantage of the hierarchical structure of ar-
guments in Debatepedia. Our work aims to model
unstructured online data, with unrestricted num-
ber of positions, in order to, ultimately, output a
Debatepedia-like summary.
The closest work to ours is the one presented
by Paul et al. (2010). It introduces the problem
of contrastive summarization which is very simi-
lar to our stated problem in Section 2. They pro-
pose the Topic Aspect Model (TAM) and use the
output distributions to compute similarities’ scores
for sentences. Scored sentences are used in a mod-
ified Random Walk algorithm to generate the sum-
mary. The assumption of TAM is that any word
in the document can exclusively belong to a topic
(e.g., government), a viewpoint (e.g., good), both
(e.g., involvement) or neither (e.g., think). How-
ever, according to TAM’s generative model, an au-
thor would choose his viewpoint and the topic to
talk about independently. Our JTV encodes the
dependency between topics and viewpoints.
</bodyText>
<sectionHeader confidence="0.978136" genericHeader="method">
4 Joint Topic Viewpoint Model
</sectionHeader>
<bodyText confidence="0.999769304347826">
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) is one of the most popular topic models used
to mine large text data sets. It models a document
as a mixture of topics where each topic is a dis-
tribution over words. However, it fails to model
more complex structures of texts like contention
where viewpoints are hidden.
We augment LDA to model a contentious doc-
ument as a pair of dependent mixtures: a mixture
of arguing topics and a mixture of viewpoints for
each topic. The assumption is that a document dis-
cusses the topics in proportions, (e.g. 80% gov-
ernment’s role, 20% insurance’s cost). Moreover,
as explained in Section 2, each one of these top-
ics can be shared by divergent arguing expres-
sions conveying different viewpoints. We suppose
that for each discussed topic in the document, the
viewpoints are expressed in proportions. For in-
stance, 70% of the document’s text discussing the
government’s role expresses an opposing view-
point to the reform while 30% of it conveys a sup-
porting viewpoint. Thus, each term in a docu-
ment is assigned a pair topic-viewpoint label (e.g.
</bodyText>
<footnote confidence="0.981889">
3http://dbp.idebate.org
</footnote>
<figureCaption confidence="0.956198">
Figure 1: The JTV’s graphical model (plate nota-
tion)
</figureCaption>
<bodyText confidence="0.999937793103448">
“government’s role-oppose reform”). A term is a
word or a phrase i.e. n-grams (n&gt;1). For each
topic-viewpoint pair, the model generates a topic-
viewpoint probability distribution over terms. This
topic-viewpoint distribution would corresponds to
what we define as an arguing expression in Sec-
tion 2, i.e. a set of terms sharing a common topic
and justifying the same viewpoint regarding a con-
tentious issue. The Joint Topic Viewpoint (JTV),
is similar to the Joint Sentiment Topic model (JST)
(Lin and He, 2009), as it models documents as two
dependent mixtures. However, here we condition
viewpoints on topics instead of conditioning top-
ics on sentiment. Moreover, the application is dif-
ferent from that of JST which intend to model re-
views data.
Formally, assume that a corpus contains D doc-
uments di..D, where each document is a term’s
vector wd of size Nd; each term wdn in a docu-
ment belongs to the corpus vocabulary of distinct
terms of size V . Let K be the total number of top-
ics and L be the total number of viewpoints. Let Bd
denote the probabilities (proportions) of K topics
under a document d; ψdk be the probability distri-
butions (proportions) of L viewpoints for a topic
k in the document d (the number of viewpoints L
is the same for all topics); and φkl be the multino-
mial probability distribution over terms associated
with a topic k and a viewpoint l. The generative
</bodyText>
<page confidence="0.992524">
38
</page>
<listItem confidence="0.800169142857143">
process (see. the JTV graphical model in Figure
1) is the following:
• for each topic k and viewpoint l, draw a
multinomial distribution over the vocabulary
V : φkl — Dir(β);
• for each document d,
draw a topic mixture θd — Dir(α)
</listItem>
<equation confidence="0.959530666666667">
for each topic k, draw a viewpoint mixture
ψdk — Dir(γ)
for each term wdn, sample a topic assignment
zdn — Mult(θd); sample a viewpoint assign-
ment vdn — Mult(ψdzdn); and sample a term
wdn — Mult(φzdnvdn).
</equation>
<bodyText confidence="0.99994305">
We use fixed symmetric Dirichlet’s parameters γ,
β and α. They can be interpreted as the prior
counts of: terms assigned to viewpoint l and topic
k in a document; a particular term w assigned to
topic k and viewpoint l within the corpus; terms
assigned to a topic k in a document, respectively.
In order to learn the hidden JTV’s parameters
φkl, ψdk and θd, we draw on approximate in-
ference as exact inference is intractable (Blei et
al., 2003). We use the collapsed Gibbs Sampling
(Griffiths and Steyvers, 2004), a Markov Chain
Monte Carlo algorithm. The collapsed Gibbs sam-
pler integrate out all parameters φ, ψ and θ in the
joint distribution of the model and converge to a
stationary posterior distribution over viewpoints’
assignments v~ and all topics’ assignments z~ in the
corpus. It iterates on each current observed token
wi and samples each corresponding vi and zi given
all the previous sampled assignments in the model
~v¬i, ~z¬i and observed ~w¬i, where v~ = {vi,~v¬iI,
</bodyText>
<equation confidence="0.881868">
z~ = {zi, ~z¬i1, and w~ = {wi, ~w¬i1. The derived
sampling equation is:
p(zi = k, vi = l1~z¬i,~v¬i, wi = t, ~w¬i) (x
n(l)
dk,¬i + γ
E
V n(t)
kl,¬i + V β .
</equation>
<bodyText confidence="0.9210146">
where n(t)
kl,¬i is the number of times term t was as-
signed to topic k and the viewpoint l in the corpus;
n(l)
dk,¬i is the number of times viewpoint l of topic k
was observed in document d; and n(k)
d,¬i is the num-
ber of times topic k was observed in document d.
All these counts are computed excluding the cur-
rent token i, which is indicated by the symbol moi.
</bodyText>
<table confidence="0.996164333333333">
AW GM ObCare
View pt allow not illegal not bad not
#doc 213 136 44 54 129 54
tot.#toks 44482 10666 22733
avg.#toks . 127.45 108.83 124.22
doc.
</table>
<tableCaption confidence="0.99988">
Table 3: Statistics on the three used data sets
</tableCaption>
<bodyText confidence="0.975775666666667">
After the convergence of the Gibbs algorithm, the
parameters φ, ψ and θ are estimated using the last
obtained sample.
</bodyText>
<sectionHeader confidence="0.985331" genericHeader="method">
5 Clustering Arguing Expressions
</sectionHeader>
<bodyText confidence="0.999826235294118">
Although we are not tackling the task of cluster-
ing arguing expressions according to their view-
points in this paper (Task 2 in Section 2), we ex-
plain how the structure of JTV lays the ground for
performing it. We mentioned in the previous Sec-
tion that an inferred topic-viewpoint distribution
φkl can be assimilated to an arguing expression.
For convenience, we will use “arguing expression”
and “topic-viewpoint” interchangeably to refer to
the topic-viewpoint distribution.
Indeed, two topic-viewpoint φkl and φk&apos;l, hav-
ing different topics k and k0, do not necessarily
express the same viewpoint, despite the fact that
they both have the same index l. The reason stems
from the nested structure of the model, where the
generation of the viewpoint assignments for a par-
ticular topic k is completely independent from that
of topic k0. In other words, the model does not
trace and match the viewpoint labeling along dif-
ferent topics. Nevertheless, the JTV can still help
overcome this problem. According to the JTV’s
structure, a topic-viewpoint φkl, is more similar
in distribution to a divergent topic-viewpoint φkl&apos;,
related to the same topic k, than to any other topic-
viewpoint φk&apos;∗, corresponding to a different topic
k0. Therefore, we can formulate the problem of
clustering arguments as a constrained clustering
problem (Basu et al., 2008). The goal is to group
the similar topics-viewpoints φkls into L clusters
(number of viewpoints), given the constraint that
the φkls of the same topic k should not belong to
the same cluster. The similarity between the topic-
viewpoint distributions can be measured using the
Jensen-Shannon Divergence (Bishop, 2006).
</bodyText>
<equation confidence="0.997919125">
n(t)
kl,¬i + β
l
ndk ¬i + Lγ
L
l=1
.n(k)
d,¬i + α (1)
</equation>
<page confidence="0.997605">
39
</page>
<sectionHeader confidence="0.991346" genericHeader="method">
6 Experimental Set up
</sectionHeader>
<bodyText confidence="0.999904255813954">
In order to evaluate the performances of the JTV
model, we experiment with three different cor-
pora of contentious documents. Recall, we assume
that any input document to the JTV is answer-
ing a contentious question which makes it con-
tentious according to the definitions stated in Sec-
tion 2. Posts in online debate websites, like “creat-
edebate.com” or “debate.org”, match this require-
ment. They correspond to online users’ takes on
a clearly stated contention question making them
more adequate for our matter than debate forums’
posts. These latter contain online interactions be-
tween users where the objective is not necessar-
ily answering a contention question but rather dis-
cussing a contentious topic. Classifying a docu-
ment as contentious or not is not an issue consid-
ered in this paper but can be explored in our future
work. Table 3 describes the used data sets.
Assault Weapons (AW) 4: includes posts ex-
tracted from “debate.com”. The contention ques-
tion is “Should assault weapons be allowed in the
United States as means of allowing individuals to
defend themselves?”. The viewpoints are either
“should be allowed” or “should not be allowed”.
Gay Marriage (GM) 5: contains posts from
“debate.com” related to the contention question
“Should gay marriage be illegal?”. The posts’
stance are either “should be illegal” or “should be
legal”.
Obama Healthcare (ObCare) 6: includes posts
from “debate.org” responding to the contention
question “Is the passing of ObamaCare bad for the
American public?”. Stances are either “bad” or
“not bad”.
Paul et al. (2010) stress out the importance of
negation features in detecting contrastive view-
points. Thus, we performed a simple treatment
of merging any negation indicators, like “noth-
ing”, “no one”, “never”, etc., found in text with
the following occurring word to form a single to-
ken. Moreover, we merge the negation “not” with
any Auxiliary verb (e.g., is, was, could, will) pre-
ceding it. Then, we removed the stop-words.
</bodyText>
<footnote confidence="0.911635428571429">
4http://www.debate.org/opinions/should-assault-
weapons-be-allowed-in-the-united-states-as-means-of-
allowing-individuals-to-defend-themselves
5http://www.debate.org/opinions/should-gay-marriage-
be-illegal
6http://www.debate.org/opinions/is-the-passing-of-
obamacare-bad-for-the-american-public
</footnote>
<bodyText confidence="0.999959083333333">
Throughout the experiments below, the JTV’s
hyperparameters are set to fixed values. The γ is
set, according to Steyvers and Griffiths’s (Steyvers
and Griffiths, 2007) hyperparameters settings, to
50/L, where L is the number of viewpoints. Q
and α are adjusted manually, to give reasonable
results, and are both set to 0.01. Along the exper-
iments, we try different number of topics K. The
number of viewpoints L is equal to 2. The TAM
model (Paul et al., 2010) (Section 3.3) is run as a
means of comparison during the evaluation proce-
dure. Its default parameters are used.
</bodyText>
<sectionHeader confidence="0.983326" genericHeader="method">
7 Model Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999373">
7.1 Qualitative Evaluation
</subsectionHeader>
<bodyText confidence="0.999902611111111">
Tables 4 and 5 present the inferred topic-
viewpoints words, i.e. arguing expressions, by
JTV for the Obama Healthcare and Gay Marriage
data sets, respectively. We set a number of topics
of K = 3 for the former and K = 2 for the lat-
ter. The number of viewpoints is L = 2 for both
data sets. For the Obamacare data set, we run the
model with balanced number of posts from “bad”
and “not bad” stances. Each topic-viewpoint pair
(e.g. Topic 1-view 1) is represented by the set of
top terms. The terms are sorted in descending or-
der according to their probabilities. Inferred prob-
abilities over topics, and over viewpoints for each
topic, are also reported. We try to qualitatively
observe the distinctiveness of each arguing (topic-
viewpoint) and assess the coherence in terms of
the topic discussed and the viewpoint conveyed
and its divergence with the corresponding pair-
element.
In both Tables 4 and 5, most of the topic-
viewpoint pairs, corresponding to a same topic,
are conveying opposite stances. For instance, tak-
ing a closer look to the original data suggests that
Topic3-view5 (Table 4) criticizes the healthcare
system and compares it to the other countries (e.g.
a sample from the original documents:“revise our
healthcare system with the most efficient systems
in the world”). On the other side, Topic 3-view
6 explains the negative consequence of obamacare
on middle class, e.g. “ObamaCare was supposed
to help the poor and the middle class. In the
end, the businesses fire all the people because of
the ObamaCare taxes and then IT IS THE MID-
DLE CLASS PEOPLE WHO SUFFER!”. Simi-
larly, Topic1-view1 advances the question of the
costs that the bill will cause at the level of people
</bodyText>
<page confidence="0.996213">
40
</page>
<table confidence="0.992543625">
Topic 1 0.328 Topic 2 0.334 Topic 3 0.337
view 1 0.64 view 2 0.36 view 3 0.59 view 4 0.41 view 5 0.63 view 6 0.37
pay universal people insurance healthcare obamacare
people care insurance health obamacare healthcare
make life good companies system government
money law free medicare americans class
costs act health doctors affordable taxes
government poor work plan country/world middle
</table>
<tableCaption confidence="0.997771">
Table 4: JTV’s generated topics-viewpoints (arguing expressions) from Obamacare data set
</tableCaption>
<table confidence="0.999681166666667">
Topic 1 0.50 Topic 2 0.50
view 1 0.47 view 2 0.53 view 3 0.60 view 4 0.40
marriage marriage people gay
love man gay children
life woman religion people
couples god shouldnt sex
person bible wrong parents
legal illegal rights natural
married wrong government human
happy love marry population
samesex homosexual freedom opposite
illegal word argument race
</table>
<tableCaption confidence="0.999793">
Table 5: JTV’s generated topics-viewpoints (arguing expressions) from Gay Marriage data set
</tableCaption>
<bodyText confidence="0.999901923076923">
and government, e.g. “The government doesn’t
even have enough money to pay of a fraction of the
towering debt that we’ve accrued”, “forcing peo-
ple to buy insurance or pay an even higher tax will
make more families poverty stricken”. However,
Topic1-view2 stresses out the importance of hav-
ing a universal healthcare, e.g. “ObamaCare cer-
tainly has problems, but just like any law, we can
work on these problems and make the law better
(..). The fundamental goal is Universal Health-
care (...)”, “If you were poor and had a hernia
that needed surgery, you need money to pay for
it. Denying Obama’s Plan for a health care sys-
tem means you cannot pay for it which means you
will DIE.”. Similar pattern is observed in Topic 2.
The results on Gay Marriage 1 dataset (Table
5) encompass the notion of shared topic between
divergent arguing expressions (Section 2) more
clearly than the results obtained from Obamacare.
This may be related to the nature of the contention.
For instance, Topic 1 in Table 5 is “the concept of
marriage” and it is shared by both view 1 and view
2. However, the concept is perceived differently
according to the stance. The terms in view 1 (not
illegal) suggest that marriage is about love, hap-
piness and it wouldn’t disturb anyone’s life (as it
may be read from original data). The view 2 (il-
legal) may emphasize the notion of a marriage as
a union between man and woman and the sacred-
ness aspect of it (god, bible). Similarly, Topic 2
is about “people who are gay”. The terms in view
3 (not illegal) may advocate that religious argu-
ments from opposing stance do not make sense
and that gay people are free and have the same
rights as other people. Moreover, the government
should not interfere in this matter. View 4 (illegal)
suggests that gay people can not have children
which raises the problem of population decrease.
It also casts doubt on their ability to be parents.
</bodyText>
<subsectionHeader confidence="0.99373">
7.2 Quantitative Evaluation
</subsectionHeader>
<bodyText confidence="0.99997425">
We assess the ability of the model to fit the online
debate data and generate distinct topic-viewpoint
pairs by comparing it with TAM which models
also the topic-viewpoint dimension.
</bodyText>
<subsubsectionHeader confidence="0.590651">
7.2.1 Held-Out Perplexity
</subsubsectionHeader>
<bodyText confidence="0.9991615">
We use the perplexity criterion to measure the abil-
ity of the learned topic model to fit a new held-
out data. Perplexity assesses the generalization
performance and, subsequently, provides a com-
</bodyText>
<page confidence="0.996911">
41
</page>
<figure confidence="0.99656">
(a) AW (b) GM (c) ObCare
</figure>
<figureCaption confidence="0.99304">
Figure 2: JVT and TAM’s perplexity plots for three different data sets
Figure 3: Average of overall topic-viewpoint di-
vergences of JTV and TAM
</figureCaption>
<bodyText confidence="0.999622631578947">
paring framework of learned topic models. The
lower the perplexity, the less “perplexed” is the
model by unseen data and the better the general-
ization. It algebraically corresponds to the inverse
geometrical mean of the test corpus’ terms likeli-
hoods given the learned model parameters (Hein-
rich, 2009). We compute the perplexity under es-
timated parameters of JTV and compare it to that
of TAM for our three unigrams data sets (Section
6).
Figure 2 exhibits, for each corpus, the perplex-
ity plot as function of the number of topics K
for JTV and TAM. Note that for each K, we run
the model 50 times. The drawn perplexity corre-
sponds to the average perplexity on the 50 runs
where each run compute one-fold perplexity from
a 10-fold cross-validation. The figures show evi-
dence that the JTV outperforms TAM for all data
sets, used in the experimentation.
</bodyText>
<sectionHeader confidence="0.44025" genericHeader="method">
7.2.2 Kullback-Leibler Divergence
</sectionHeader>
<bodyText confidence="0.999928944444444">
Kullback-Leibler (KL) Divergence is used to mea-
sure the degree of separation between two proba-
bility distributions. We utilize it to assess the dis-
tinctiveness of generated topic-viewpoint by JTV
and TAM. This is an indicator of a good ag-
gregation of arguing expressions. We compute
an overall-divergence quantity, which is an av-
erage KL-Divergence between all pairs of topic-
viewpoint distributions, for JTV and TAM and
compare them. Figure 3 illustrates the results for
all datasets. Quantities are averages on 20 runs of
the models. Both models are run with a number
of topics K = 5. Comparing JTV and TAM, we
notice that the overall-divergence of JTV’s topic-
viewpoint is significantly (p − value &lt; 0.01)
higher for all data sets. This result reveals a better
quality of our JTV extracting process of arguing
expressions (the first task stated in Section 2)
</bodyText>
<sectionHeader confidence="0.997373" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999996529411765">
We suggested a fine grained probabilistic frame-
work for improving the quality of opinion min-
ing from online contention texts. We proposed
a Joint Topic Viewpoint model (JTV) for the un-
supervised detection of arguing expressions. Un-
like common approaches the proposed model fo-
cuses on arguing expressions that are implicitly
described in unstructured text according to the la-
tent topics they discuss and the implicit viewpoints
they voice. The qualitative and quantitative analy-
sis of the experimental results show the effective-
ness of our (JTV) model in generating informative
summaries of recurrent topics and viewpoints pat-
terns in online debates’ texts. Future study needs
to give more insights into the clustering of arguing
expressions according to their viewpoints, as well
as their automatic extractive summary.
</bodyText>
<sectionHeader confidence="0.996024" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.920231">
Sugato Basu, Ian Davidson, and Kiri Wagstaff. 2008.
Constrained Clustering: Advances in Algorithms,
</reference>
<page confidence="0.993477">
42
</page>
<reference confidence="0.99823398">
Theory, and Applications. Chapman &amp; Hall/CRC,
1 edition.
Christopher M. Bishop. 2006. Pattern Recognition
and Machine Learning (Information Science and
Statistics). Springer-Verlag New York, Inc., Secau-
cus, NJ, USA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022, March.
Yi Fang, Luo Si, Naveen Somasundaram, and Zhengtao
Yu. 2012. Mining contrastive opinions on political
texts using cross-perspective topic model. In Pro-
ceedings of the fifth ACM international conference
on Web search and data mining, WSDM ’12, pages
63–72, New York, NY, USA. ACM.
Swapna Gottipati, Minghui Qiu, Yanchuan Sim, Jing
Jiang, and Noah A. Smith. 2013. Learning topics
and positions from debatepedia. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’13.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228–5235.
Gregor Heinrich. 2009. Parameter estimation for
text analysis. Technical report, Fraunhofer IGD,
September.
Yohan Jo and Alice H. Oh. 2011. Aspect and sen-
timent unification model for online review analysis.
In Proceedings of the fourth ACM international con-
ference on Web search and data mining, WSDM ’11,
pages 815–824, New York, NY, USA. ACM.
Jeffrey M. Jones. 2010. In u.s., 45% favor, 48% op-
pose obama healthcare plan. Gallup, March.
Soo-Min Kim and Eduard H Hovy. 2007. Crystal: An-
alyzing predictive opinions on the web. In EMNLP-
CoNLL, pages 1056–1064.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of the 18th ACM conference on Informa-
tion and knowledge management, CIKM ’09, pages
375–384, New York, NY, USA. ACM.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on?: identifying perspectives at the document and
sentence levels. In Proceedings of the Tenth Con-
ference on Computational Natural Language Learn-
ing, CoNLL-X ’06, pages 109–116, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Arjun Mukherjee and Bing Liu. 2012. Mining con-
tentions from discussions and debates. In Proceed-
ings of the 18th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
KDD ’12, pages 841–849, New York, NY, USA.
ACM.
Souneil Park, KyungSoon Lee, and Junehwa Song.
2011. Contrasting opposing views of news articles
on contentious issues. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
- Volume 1, HLT ’11, pages 340–349, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ’10, pages 66–76, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Gener-
ation of Emotion in Text, CAAGET ’10, pages 116–
124, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of latent semantic analysis,
427(7):424–440.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: determining support or opposition from
congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’06,
pages 327–335, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ivan Titov and Ryan McDonald. 2008. Modeling
online reviews with multi-grain topic models. In
Proceedings of the 17th international conference on
World Wide Web, WWW ’08, pages 111–120, New
York, NY, USA. ACM.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a maxent-lda hybrid. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ’10, pages 56–
65, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.999833">
43
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.615997">
<title confidence="0.99976">Finding Arguing Expressions of Divergent Viewpoints in Online Debates</title>
<author confidence="0.853653">Amine</author>
<affiliation confidence="0.998003">Department of Computing University of</affiliation>
<email confidence="0.881717">atrabels@ualberta.ca</email>
<author confidence="0.863821">R Osmar</author>
<affiliation confidence="0.998431">Department of Computing University of</affiliation>
<email confidence="0.958146">zaiane@ualberta.ca</email>
<abstract confidence="0.998200421052632">This work suggests a fine-grained mining of contentious documents, specifically online debates, towards a summarization of contention issues. We propose a Joint Topic Viewpoint model (JTV) for the unsupervised identification and the clustering of arguing expressions according to the latent topics they discuss and the implicit viewpoints they voice. A set of experiments is conducted on online debates documents. Qualitative and quantitative evaluations of the model’s output are performed in context of different contention issues. Analysis of experimental results shows the effectiveness of the proposed model to automatically and accurately detect recurrent patterns of arguing expressions in online debate texts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sugato Basu</author>
<author>Ian Davidson</author>
<author>Kiri Wagstaff</author>
</authors>
<title>Constrained Clustering: Advances in Algorithms, Theory, and Applications.</title>
<date>2008</date>
<journal>Chapman &amp; Hall/CRC,</journal>
<volume>1</volume>
<pages>edition.</pages>
<contexts>
<context position="19918" citStr="Basu et al., 2008" startWordPosition="3239" endWordPosition="3242">tion of the viewpoint assignments for a particular topic k is completely independent from that of topic k0. In other words, the model does not trace and match the viewpoint labeling along different topics. Nevertheless, the JTV can still help overcome this problem. According to the JTV’s structure, a topic-viewpoint φkl, is more similar in distribution to a divergent topic-viewpoint φkl&apos;, related to the same topic k, than to any other topicviewpoint φk&apos;∗, corresponding to a different topic k0. Therefore, we can formulate the problem of clustering arguments as a constrained clustering problem (Basu et al., 2008). The goal is to group the similar topics-viewpoints φkls into L clusters (number of viewpoints), given the constraint that the φkls of the same topic k should not belong to the same cluster. The similarity between the topicviewpoint distributions can be measured using the Jensen-Shannon Divergence (Bishop, 2006). n(t) kl,¬i + β l ndk ¬i + Lγ L l=1 .n(k) d,¬i + α (1) 39 6 Experimental Set up In order to evaluate the performances of the JTV model, we experiment with three different corpora of contentious documents. Recall, we assume that any input document to the JTV is answering a contentious </context>
</contexts>
<marker>Basu, Davidson, Wagstaff, 2008</marker>
<rawString>Sugato Basu, Ian Davidson, and Kiri Wagstaff. 2008. Constrained Clustering: Advances in Algorithms, Theory, and Applications. Chapman &amp; Hall/CRC, 1 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Pattern Recognition and Machine Learning (Information Science and Statistics).</title>
<date>2006</date>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc., Secaucus, NJ, USA.</location>
<contexts>
<context position="20232" citStr="Bishop, 2006" startWordPosition="3290" endWordPosition="3291">kl, is more similar in distribution to a divergent topic-viewpoint φkl&apos;, related to the same topic k, than to any other topicviewpoint φk&apos;∗, corresponding to a different topic k0. Therefore, we can formulate the problem of clustering arguments as a constrained clustering problem (Basu et al., 2008). The goal is to group the similar topics-viewpoints φkls into L clusters (number of viewpoints), given the constraint that the φkls of the same topic k should not belong to the same cluster. The similarity between the topicviewpoint distributions can be measured using the Jensen-Shannon Divergence (Bishop, 2006). n(t) kl,¬i + β l ndk ¬i + Lγ L l=1 .n(k) d,¬i + α (1) 39 6 Experimental Set up In order to evaluate the performances of the JTV model, we experiment with three different corpora of contentious documents. Recall, we assume that any input document to the JTV is answering a contentious question which makes it contentious according to the definitions stated in Section 2. Posts in online debate websites, like “createdebate.com” or “debate.org”, match this requirement. They correspond to online users’ takes on a clearly stated contention question making them more adequate for our matter than debat</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="13792" citStr="Blei et al., 2003" startWordPosition="2168" endWordPosition="2171">e the output distributions to compute similarities’ scores for sentences. Scored sentences are used in a modified Random Walk algorithm to generate the summary. The assumption of TAM is that any word in the document can exclusively belong to a topic (e.g., government), a viewpoint (e.g., good), both (e.g., involvement) or neither (e.g., think). However, according to TAM’s generative model, an author would choose his viewpoint and the topic to talk about independently. Our JTV encodes the dependency between topics and viewpoints. 4 Joint Topic Viewpoint Model Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is one of the most popular topic models used to mine large text data sets. It models a document as a mixture of topics where each topic is a distribution over words. However, it fails to model more complex structures of texts like contention where viewpoints are hidden. We augment LDA to model a contentious document as a pair of dependent mixtures: a mixture of arguing topics and a mixture of viewpoints for each topic. The assumption is that a document discusses the topics in proportions, (e.g. 80% government’s role, 20% insurance’s cost). Moreover, as explained in Section 2, each one of thes</context>
<context position="17173" citStr="Blei et al., 2003" startWordPosition="2761" endWordPosition="2764">int mixture ψdk — Dir(γ) for each term wdn, sample a topic assignment zdn — Mult(θd); sample a viewpoint assignment vdn — Mult(ψdzdn); and sample a term wdn — Mult(φzdnvdn). We use fixed symmetric Dirichlet’s parameters γ, β and α. They can be interpreted as the prior counts of: terms assigned to viewpoint l and topic k in a document; a particular term w assigned to topic k and viewpoint l within the corpus; terms assigned to a topic k in a document, respectively. In order to learn the hidden JTV’s parameters φkl, ψdk and θd, we draw on approximate inference as exact inference is intractable (Blei et al., 2003). We use the collapsed Gibbs Sampling (Griffiths and Steyvers, 2004), a Markov Chain Monte Carlo algorithm. The collapsed Gibbs sampler integrate out all parameters φ, ψ and θ in the joint distribution of the model and converge to a stationary posterior distribution over viewpoints’ assignments v~ and all topics’ assignments z~ in the corpus. It iterates on each current observed token wi and samples each corresponding vi and zi given all the previous sampled assignments in the model ~v¬i, ~z¬i and observed ~w¬i, where v~ = {vi,~v¬iI, z~ = {zi, ~z¬i1, and w~ = {wi, ~w¬i1. The derived sampling e</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Fang</author>
<author>Luo Si</author>
<author>Naveen Somasundaram</author>
<author>Zhengtao Yu</author>
</authors>
<title>Mining contrastive opinions on political texts using cross-perspective topic model.</title>
<date>2012</date>
<booktitle>In Proceedings of the fifth ACM international conference on Web search and data mining, WSDM ’12,</booktitle>
<pages>63--72</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Fang, Si, Somasundaram, Yu, 2012</marker>
<rawString>Yi Fang, Luo Si, Naveen Somasundaram, and Zhengtao Yu. 2012. Mining contrastive opinions on political texts using cross-perspective topic model. In Proceedings of the fifth ACM international conference on Web search and data mining, WSDM ’12, pages 63–72, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Gottipati</author>
<author>Minghui Qiu</author>
<author>Yanchuan Sim</author>
<author>Jing Jiang</author>
<author>Noah A Smith</author>
</authors>
<title>Learning topics and positions from debatepedia.</title>
<date>2013</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing, EMNLP ’13.</booktitle>
<contexts>
<context position="12464" citStr="Gottipati et al. (2013)" startWordPosition="1951" endWordPosition="1954">e different Joint Topic Expressions Models (JTE). The JTEs’ output is used to discover points (topics) of contention. The model supposes that people express agreement or disagreement through CAexpressions. However, this is not often the case when people express their viewpoint via other channels than discussion forums like debate sites or editorials. Moreover, agreement or disagreement may also be conveyed implicitly through arguing expressions rejecting or supporting another opinion. JTEs do not model viewpoints and use the supervised Maximum Entropy model to detect CA-expressions. Recently, Gottipati et al. (2013) propose a topic model to infer human interpretable text in the do37 main of issues using Debatepedia3 as a corpus of evidence. Debatepedia is an online authored encyclopedia to summarize and organize the main arguments of two possible positions. The model takes advantage of the hierarchical structure of arguments in Debatepedia. Our work aims to model unstructured online data, with unrestricted number of positions, in order to, ultimately, output a Debatepedia-like summary. The closest work to ours is the one presented by Paul et al. (2010). It introduces the problem of contrastive summarizat</context>
</contexts>
<marker>Gottipati, Qiu, Sim, Jiang, Smith, 2013</marker>
<rawString>Swapna Gottipati, Minghui Qiu, Yanchuan Sim, Jing Jiang, and Noah A. Smith. 2013. Learning topics and positions from debatepedia. In Proceedings of Conference on Empirical Methods in Natural Language Processing, EMNLP ’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl</booktitle>
<pages>1--5228</pages>
<contexts>
<context position="17241" citStr="Griffiths and Steyvers, 2004" startWordPosition="2771" endWordPosition="2774">c assignment zdn — Mult(θd); sample a viewpoint assignment vdn — Mult(ψdzdn); and sample a term wdn — Mult(φzdnvdn). We use fixed symmetric Dirichlet’s parameters γ, β and α. They can be interpreted as the prior counts of: terms assigned to viewpoint l and topic k in a document; a particular term w assigned to topic k and viewpoint l within the corpus; terms assigned to a topic k in a document, respectively. In order to learn the hidden JTV’s parameters φkl, ψdk and θd, we draw on approximate inference as exact inference is intractable (Blei et al., 2003). We use the collapsed Gibbs Sampling (Griffiths and Steyvers, 2004), a Markov Chain Monte Carlo algorithm. The collapsed Gibbs sampler integrate out all parameters φ, ψ and θ in the joint distribution of the model and converge to a stationary posterior distribution over viewpoints’ assignments v~ and all topics’ assignments z~ in the corpus. It iterates on each current observed token wi and samples each corresponding vi and zi given all the previous sampled assignments in the model ~v¬i, ~z¬i and observed ~w¬i, where v~ = {vi,~v¬iI, z~ = {zi, ~z¬i1, and w~ = {wi, ~w¬i1. The derived sampling equation is: p(zi = k, vi = l1~z¬i,~v¬i, wi = t, ~w¬i) (x n(l) dk,¬i </context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Heinrich</author>
</authors>
<title>Parameter estimation for text analysis.</title>
<date>2009</date>
<tech>Technical report, Fraunhofer IGD,</tech>
<contexts>
<context position="28608" citStr="Heinrich, 2009" startWordPosition="4660" endWordPosition="4662">ity of the learned topic model to fit a new heldout data. Perplexity assesses the generalization performance and, subsequently, provides a com41 (a) AW (b) GM (c) ObCare Figure 2: JVT and TAM’s perplexity plots for three different data sets Figure 3: Average of overall topic-viewpoint divergences of JTV and TAM paring framework of learned topic models. The lower the perplexity, the less “perplexed” is the model by unseen data and the better the generalization. It algebraically corresponds to the inverse geometrical mean of the test corpus’ terms likelihoods given the learned model parameters (Heinrich, 2009). We compute the perplexity under estimated parameters of JTV and compare it to that of TAM for our three unigrams data sets (Section 6). Figure 2 exhibits, for each corpus, the perplexity plot as function of the number of topics K for JTV and TAM. Note that for each K, we run the model 50 times. The drawn perplexity corresponds to the average perplexity on the 50 runs where each run compute one-fold perplexity from a 10-fold cross-validation. The figures show evidence that the JTV outperforms TAM for all data sets, used in the experimentation. 7.2.2 Kullback-Leibler Divergence Kullback-Leible</context>
</contexts>
<marker>Heinrich, 2009</marker>
<rawString>Gregor Heinrich. 2009. Parameter estimation for text analysis. Technical report, Fraunhofer IGD, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohan Jo</author>
<author>Alice H Oh</author>
</authors>
<title>Aspect and sentiment unification model for online review analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the fourth ACM international conference on Web search and data mining, WSDM ’11,</booktitle>
<pages>815--824</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10600" citStr="Jo and Oh, 2011" startWordPosition="1670" endWordPosition="1673">ical stances in online debates using a generated arguing clues from the Multi Perspective Question Answering (MPQA) opinion corpus2. Our problem is not to classify documents, but to recognize recurrent pattern of arguing phrases instead of arguing clues. Moreover, our approach is independent of any annotated corpora. 3.2 Topic Modeling in Reviews Data Another emerging body of work applies probabilistic topic models on reviews data to extract appraisal aspects and the corresponding specific sentiment lexicon. These kinds of models are usually referred to as joint sentiment/aspect topic models (Jo and Oh, 2011; Titov and McDonald, 2008; 2http://mpqa.cs.pitt.edu/ Zhao et al., 2010). Lin and He (2009) propose the Joint Sentiment Topic Model (JST) to model the dependency between sentiment and topics. They make the assumption that topics discussed on a review are conditioned on sentiment polarity. Reversely, our JTV model assumes that a viewpoint endorsement (e.g., oppose reform) is conditioned on the discussed topic (e.g., government’s role) and its application is different from that of JST. Most of the joint aspect sentiment topic models are either semi-supervised or weakly supervised using sentiment</context>
</contexts>
<marker>Jo, Oh, 2011</marker>
<rawString>Yohan Jo and Alice H. Oh. 2011. Aspect and sentiment unification model for online review analysis. In Proceedings of the fourth ACM international conference on Web search and data mining, WSDM ’11, pages 815–824, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey M Jones</author>
</authors>
<title>In u.s., 45% favor, 48% oppose obama healthcare plan.</title>
<date>2010</date>
<location>Gallup,</location>
<contexts>
<context position="4539" citStr="Jones, 2010" startWordPosition="700" endWordPosition="701"> Association for Computational Linguistics Support Viewpoint Oppose Viewpoint Many people do not have health care The government should not be involved Provide health care for 30 million people It will produce too much debt The government should help old people The bill would not help the people Table 1: Excerpts of support and opposition opinion to a healthcare bill in the USA. 2 Problem Statement This paper examines the task of mining the topics and the viewpoints of arguing expressions towards the summarization of contentious text. An example of a human-made summary of arguing expressions (Jones, 2010) on, what is commonly known as, the Obama healthcare reform is presented in Table 2. Ultimately, the target is to automatically generate similar summaries given a corpus of contentious documents. However, this paper tackles the sub-problem of identifying recurrent words and phrases expressing arguing and cluster them according to their topics and viewpoints. This would help solve the general problem. We use Table 2’s examples to define some key concepts which can help us formulate this latter. Here, the contentious issue yielding the divergent positions is the Obama healthcare. The documents a</context>
</contexts>
<marker>Jones, 2010</marker>
<rawString>Jeffrey M. Jones. 2010. In u.s., 45% favor, 48% oppose obama healthcare plan. Gallup, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard H Hovy</author>
</authors>
<title>Crystal: Analyzing predictive opinions on the web. In EMNLPCoNLL,</title>
<date>2007</date>
<pages>1056--1064</pages>
<contexts>
<context position="9371" citStr="Kim and Hovy, 2007" startWordPosition="1472" endWordPosition="1475">urance companies Cost the government too much Table 2: Human-made summary of arguing expressions supporting and opposing Obamacare. the same topic and viewpoint but which are lexically different, e.g. “provide health care for 30 million people ” and “ many people do not have healthcare”. For this purpose we propose a Joint Topic Viewpoint Model (JTV) to account for the dependence structure of topics and viewpoints. 3 Related Work 3.1 Classifying Stances An early body of work addresses the challenge of classifying viewpoints in contentious or ideological discourses using supervised techniques (Kim and Hovy, 2007; Lin et al., 2006). Although the models give good performances, they remain datadependent and costly to label, making the unsupervised approach more appropriate for the existing huge quantity of online data. A similar trend of studies scrutinizes the discourse aspect of a document in order to identify opposed stances (Thomas et al., 2006; Park et al., 2011). However, these methods utilize polarity lexicon to detect opinionated text and do not look for arguing expression, which is shown to be useful in recognizing opposed stances (Somasundaran and Wiebe, 2010). Somasundaran and Wiebe (2010) cl</context>
</contexts>
<marker>Kim, Hovy, 2007</marker>
<rawString>Soo-Min Kim and Eduard H Hovy. 2007. Crystal: Analyzing predictive opinions on the web. In EMNLPCoNLL, pages 1056–1064.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenghua Lin</author>
<author>Yulan He</author>
</authors>
<title>Joint sentiment/topic model for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM ’09,</booktitle>
<pages>375--384</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10691" citStr="Lin and He (2009)" startWordPosition="1683" endWordPosition="1686"> Question Answering (MPQA) opinion corpus2. Our problem is not to classify documents, but to recognize recurrent pattern of arguing phrases instead of arguing clues. Moreover, our approach is independent of any annotated corpora. 3.2 Topic Modeling in Reviews Data Another emerging body of work applies probabilistic topic models on reviews data to extract appraisal aspects and the corresponding specific sentiment lexicon. These kinds of models are usually referred to as joint sentiment/aspect topic models (Jo and Oh, 2011; Titov and McDonald, 2008; 2http://mpqa.cs.pitt.edu/ Zhao et al., 2010). Lin and He (2009) propose the Joint Sentiment Topic Model (JST) to model the dependency between sentiment and topics. They make the assumption that topics discussed on a review are conditioned on sentiment polarity. Reversely, our JTV model assumes that a viewpoint endorsement (e.g., oppose reform) is conditioned on the discussed topic (e.g., government’s role) and its application is different from that of JST. Most of the joint aspect sentiment topic models are either semi-supervised or weakly supervised using sentiment polarity words (Paradigm lists) to boost their efficiency. In our case, viewpoints are oft</context>
<context position="15415" citStr="Lin and He, 2009" startWordPosition="2439" endWordPosition="2442">r topic-viewpoint label (e.g. 3http://dbp.idebate.org Figure 1: The JTV’s graphical model (plate notation) “government’s role-oppose reform”). A term is a word or a phrase i.e. n-grams (n&gt;1). For each topic-viewpoint pair, the model generates a topicviewpoint probability distribution over terms. This topic-viewpoint distribution would corresponds to what we define as an arguing expression in Section 2, i.e. a set of terms sharing a common topic and justifying the same viewpoint regarding a contentious issue. The Joint Topic Viewpoint (JTV), is similar to the Joint Sentiment Topic model (JST) (Lin and He, 2009), as it models documents as two dependent mixtures. However, here we condition viewpoints on topics instead of conditioning topics on sentiment. Moreover, the application is different from that of JST which intend to model reviews data. Formally, assume that a corpus contains D documents di..D, where each document is a term’s vector wd of size Nd; each term wdn in a document belongs to the corpus vocabulary of distinct terms of size V . Let K be the total number of topics and L be the total number of viewpoints. Let Bd denote the probabilities (proportions) of K topics under a document d; ψdk </context>
</contexts>
<marker>Lin, He, 2009</marker>
<rawString>Chenghua Lin and Yulan He. 2009. Joint sentiment/topic model for sentiment analysis. In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM ’09, pages 375–384, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Alexander Hauptmann</author>
</authors>
<title>Which side are you on?: identifying perspectives at the document and sentence levels.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06,</booktitle>
<pages>109--116</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9390" citStr="Lin et al., 2006" startWordPosition="1476" endWordPosition="1479">t the government too much Table 2: Human-made summary of arguing expressions supporting and opposing Obamacare. the same topic and viewpoint but which are lexically different, e.g. “provide health care for 30 million people ” and “ many people do not have healthcare”. For this purpose we propose a Joint Topic Viewpoint Model (JTV) to account for the dependence structure of topics and viewpoints. 3 Related Work 3.1 Classifying Stances An early body of work addresses the challenge of classifying viewpoints in contentious or ideological discourses using supervised techniques (Kim and Hovy, 2007; Lin et al., 2006). Although the models give good performances, they remain datadependent and costly to label, making the unsupervised approach more appropriate for the existing huge quantity of online data. A similar trend of studies scrutinizes the discourse aspect of a document in order to identify opposed stances (Thomas et al., 2006; Park et al., 2011). However, these methods utilize polarity lexicon to detect opinionated text and do not look for arguing expression, which is shown to be useful in recognizing opposed stances (Somasundaran and Wiebe, 2010). Somasundaran and Wiebe (2010) classify ideological </context>
</contexts>
<marker>Lin, Wilson, Wiebe, Hauptmann, 2006</marker>
<rawString>Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexander Hauptmann. 2006. Which side are you on?: identifying perspectives at the document and sentence levels. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06, pages 109–116, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
</authors>
<title>Mining contentions from discussions and debates.</title>
<date>2012</date>
<booktitle>In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’12,</booktitle>
<pages>841--849</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11624" citStr="Mukherjee and Liu (2012)" startWordPosition="1827" endWordPosition="1830">ussed topic (e.g., government’s role) and its application is different from that of JST. Most of the joint aspect sentiment topic models are either semi-supervised or weakly supervised using sentiment polarity words (Paradigm lists) to boost their efficiency. In our case, viewpoints are often expressed implicitly and finding specific arguing lexicon for different stances is a challenging task in itself. Indeed, our model is enclosed in another body of work that based on a probabilistic Topic Model framework to mine divergent viewpoints. 3.3 Topic Modeling in Contentious Text A recent study by Mukherjee and Liu (2012) examines mining contention from discussion forums data where the interaction between different authors is pivotal. It attempts to jointly discover contention/agreement indicators (CAExpressions) and topics using three different Joint Topic Expressions Models (JTE). The JTEs’ output is used to discover points (topics) of contention. The model supposes that people express agreement or disagreement through CAexpressions. However, this is not often the case when people express their viewpoint via other channels than discussion forums like debate sites or editorials. Moreover, agreement or disagre</context>
</contexts>
<marker>Mukherjee, Liu, 2012</marker>
<rawString>Arjun Mukherjee and Bing Liu. 2012. Mining contentions from discussions and debates. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’12, pages 841–849, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Souneil Park</author>
<author>KyungSoon Lee</author>
<author>Junehwa Song</author>
</authors>
<title>Contrasting opposing views of news articles on contentious issues.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>340--349</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9731" citStr="Park et al., 2011" startWordPosition="1532" endWordPosition="1535">unt for the dependence structure of topics and viewpoints. 3 Related Work 3.1 Classifying Stances An early body of work addresses the challenge of classifying viewpoints in contentious or ideological discourses using supervised techniques (Kim and Hovy, 2007; Lin et al., 2006). Although the models give good performances, they remain datadependent and costly to label, making the unsupervised approach more appropriate for the existing huge quantity of online data. A similar trend of studies scrutinizes the discourse aspect of a document in order to identify opposed stances (Thomas et al., 2006; Park et al., 2011). However, these methods utilize polarity lexicon to detect opinionated text and do not look for arguing expression, which is shown to be useful in recognizing opposed stances (Somasundaran and Wiebe, 2010). Somasundaran and Wiebe (2010) classify ideological stances in online debates using a generated arguing clues from the Multi Perspective Question Answering (MPQA) opinion corpus2. Our problem is not to classify documents, but to recognize recurrent pattern of arguing phrases instead of arguing clues. Moreover, our approach is independent of any annotated corpora. 3.2 Topic Modeling in Revie</context>
</contexts>
<marker>Park, Lee, Song, 2011</marker>
<rawString>Souneil Park, KyungSoon Lee, and Junehwa Song. 2011. Contrasting opposing views of news articles on contentious issues. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 340–349, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Paul</author>
<author>ChengXiang Zhai</author>
<author>Roxana Girju</author>
</authors>
<title>Summarizing contrastive viewpoints in opinionated text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>66--76</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13011" citStr="Paul et al. (2010)" startWordPosition="2041" endWordPosition="2044">tropy model to detect CA-expressions. Recently, Gottipati et al. (2013) propose a topic model to infer human interpretable text in the do37 main of issues using Debatepedia3 as a corpus of evidence. Debatepedia is an online authored encyclopedia to summarize and organize the main arguments of two possible positions. The model takes advantage of the hierarchical structure of arguments in Debatepedia. Our work aims to model unstructured online data, with unrestricted number of positions, in order to, ultimately, output a Debatepedia-like summary. The closest work to ours is the one presented by Paul et al. (2010). It introduces the problem of contrastive summarization which is very similar to our stated problem in Section 2. They propose the Topic Aspect Model (TAM) and use the output distributions to compute similarities’ scores for sentences. Scored sentences are used in a modified Random Walk algorithm to generate the summary. The assumption of TAM is that any word in the document can exclusively belong to a topic (e.g., government), a viewpoint (e.g., good), both (e.g., involvement) or neither (e.g., think). However, according to TAM’s generative model, an author would choose his viewpoint and the</context>
<context position="21876" citStr="Paul et al. (2010)" startWordPosition="3562" endWordPosition="3565">ention question is “Should assault weapons be allowed in the United States as means of allowing individuals to defend themselves?”. The viewpoints are either “should be allowed” or “should not be allowed”. Gay Marriage (GM) 5: contains posts from “debate.com” related to the contention question “Should gay marriage be illegal?”. The posts’ stance are either “should be illegal” or “should be legal”. Obama Healthcare (ObCare) 6: includes posts from “debate.org” responding to the contention question “Is the passing of ObamaCare bad for the American public?”. Stances are either “bad” or “not bad”. Paul et al. (2010) stress out the importance of negation features in detecting contrastive viewpoints. Thus, we performed a simple treatment of merging any negation indicators, like “nothing”, “no one”, “never”, etc., found in text with the following occurring word to form a single token. Moreover, we merge the negation “not” with any Auxiliary verb (e.g., is, was, could, will) preceding it. Then, we removed the stop-words. 4http://www.debate.org/opinions/should-assaultweapons-be-allowed-in-the-united-states-as-means-ofallowing-individuals-to-defend-themselves 5http://www.debate.org/opinions/should-gay-marriage</context>
</contexts>
<marker>Paul, Zhai, Girju, 2010</marker>
<rawString>Michael J. Paul, ChengXiang Zhai, and Roxana Girju. 2010. Summarizing contrastive viewpoints in opinionated text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 66–76, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in ideological on-line debates.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, CAAGET ’10,</booktitle>
<pages>116--124</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9937" citStr="Somasundaran and Wiebe, 2010" startWordPosition="1565" endWordPosition="1568">ical discourses using supervised techniques (Kim and Hovy, 2007; Lin et al., 2006). Although the models give good performances, they remain datadependent and costly to label, making the unsupervised approach more appropriate for the existing huge quantity of online data. A similar trend of studies scrutinizes the discourse aspect of a document in order to identify opposed stances (Thomas et al., 2006; Park et al., 2011). However, these methods utilize polarity lexicon to detect opinionated text and do not look for arguing expression, which is shown to be useful in recognizing opposed stances (Somasundaran and Wiebe, 2010). Somasundaran and Wiebe (2010) classify ideological stances in online debates using a generated arguing clues from the Multi Perspective Question Answering (MPQA) opinion corpus2. Our problem is not to classify documents, but to recognize recurrent pattern of arguing phrases instead of arguing clues. Moreover, our approach is independent of any annotated corpora. 3.2 Topic Modeling in Reviews Data Another emerging body of work applies probabilistic topic models on reviews data to extract appraisal aspects and the corresponding specific sentiment lexicon. These kinds of models are usually refe</context>
</contexts>
<marker>Somasundaran, Wiebe, 2010</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2010. Recognizing stances in ideological on-line debates. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, CAAGET ’10, pages 116– 124, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Tom Griffiths</author>
</authors>
<title>Probabilistic topic models. Handbook of latent semantic analysis,</title>
<date>2007</date>
<pages>427--7</pages>
<contexts>
<context position="22741" citStr="Steyvers and Griffiths, 2007" startWordPosition="3661" endWordPosition="3664">urring word to form a single token. Moreover, we merge the negation “not” with any Auxiliary verb (e.g., is, was, could, will) preceding it. Then, we removed the stop-words. 4http://www.debate.org/opinions/should-assaultweapons-be-allowed-in-the-united-states-as-means-ofallowing-individuals-to-defend-themselves 5http://www.debate.org/opinions/should-gay-marriagebe-illegal 6http://www.debate.org/opinions/is-the-passing-ofobamacare-bad-for-the-american-public Throughout the experiments below, the JTV’s hyperparameters are set to fixed values. The γ is set, according to Steyvers and Griffiths’s (Steyvers and Griffiths, 2007) hyperparameters settings, to 50/L, where L is the number of viewpoints. Q and α are adjusted manually, to give reasonable results, and are both set to 0.01. Along the experiments, we try different number of topics K. The number of viewpoints L is equal to 2. The TAM model (Paul et al., 2010) (Section 3.3) is run as a means of comparison during the evaluation procedure. Its default parameters are used. 7 Model Evaluation 7.1 Qualitative Evaluation Tables 4 and 5 present the inferred topicviewpoints words, i.e. arguing expressions, by JTV for the Obama Healthcare and Gay Marriage data sets, res</context>
</contexts>
<marker>Steyvers, Griffiths, 2007</marker>
<rawString>Mark Steyvers and Tom Griffiths. 2007. Probabilistic topic models. Handbook of latent semantic analysis, 427(7):424–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: determining support or opposition from congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>327--335</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9711" citStr="Thomas et al., 2006" startWordPosition="1528" endWordPosition="1531">t Model (JTV) to account for the dependence structure of topics and viewpoints. 3 Related Work 3.1 Classifying Stances An early body of work addresses the challenge of classifying viewpoints in contentious or ideological discourses using supervised techniques (Kim and Hovy, 2007; Lin et al., 2006). Although the models give good performances, they remain datadependent and costly to label, making the unsupervised approach more appropriate for the existing huge quantity of online data. A similar trend of studies scrutinizes the discourse aspect of a document in order to identify opposed stances (Thomas et al., 2006; Park et al., 2011). However, these methods utilize polarity lexicon to detect opinionated text and do not look for arguing expression, which is shown to be useful in recognizing opposed stances (Somasundaran and Wiebe, 2010). Somasundaran and Wiebe (2010) classify ideological stances in online debates using a generated arguing clues from the Multi Perspective Question Answering (MPQA) opinion corpus2. Our problem is not to classify documents, but to recognize recurrent pattern of arguing phrases instead of arguing clues. Moreover, our approach is independent of any annotated corpora. 3.2 Top</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: determining support or opposition from congressional floor-debate transcripts. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 327–335, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th international conference on World Wide Web, WWW ’08,</booktitle>
<pages>111--120</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10626" citStr="Titov and McDonald, 2008" startWordPosition="1674" endWordPosition="1677">nline debates using a generated arguing clues from the Multi Perspective Question Answering (MPQA) opinion corpus2. Our problem is not to classify documents, but to recognize recurrent pattern of arguing phrases instead of arguing clues. Moreover, our approach is independent of any annotated corpora. 3.2 Topic Modeling in Reviews Data Another emerging body of work applies probabilistic topic models on reviews data to extract appraisal aspects and the corresponding specific sentiment lexicon. These kinds of models are usually referred to as joint sentiment/aspect topic models (Jo and Oh, 2011; Titov and McDonald, 2008; 2http://mpqa.cs.pitt.edu/ Zhao et al., 2010). Lin and He (2009) propose the Joint Sentiment Topic Model (JST) to model the dependency between sentiment and topics. They make the assumption that topics discussed on a review are conditioned on sentiment polarity. Reversely, our JTV model assumes that a viewpoint endorsement (e.g., oppose reform) is conditioned on the discussed topic (e.g., government’s role) and its application is different from that of JST. Most of the joint aspect sentiment topic models are either semi-supervised or weakly supervised using sentiment polarity words (Paradigm </context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008. Modeling online reviews with multi-grain topic models. In Proceedings of the 17th international conference on World Wide Web, WWW ’08, pages 111–120, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Xin Zhao</author>
<author>Jing Jiang</author>
<author>Hongfei Yan</author>
<author>Xiaoming Li</author>
</authors>
<title>Jointly modeling aspects and opinions with a maxent-lda hybrid.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>56--65</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10672" citStr="Zhao et al., 2010" startWordPosition="1679" endWordPosition="1682">he Multi Perspective Question Answering (MPQA) opinion corpus2. Our problem is not to classify documents, but to recognize recurrent pattern of arguing phrases instead of arguing clues. Moreover, our approach is independent of any annotated corpora. 3.2 Topic Modeling in Reviews Data Another emerging body of work applies probabilistic topic models on reviews data to extract appraisal aspects and the corresponding specific sentiment lexicon. These kinds of models are usually referred to as joint sentiment/aspect topic models (Jo and Oh, 2011; Titov and McDonald, 2008; 2http://mpqa.cs.pitt.edu/ Zhao et al., 2010). Lin and He (2009) propose the Joint Sentiment Topic Model (JST) to model the dependency between sentiment and topics. They make the assumption that topics discussed on a review are conditioned on sentiment polarity. Reversely, our JTV model assumes that a viewpoint endorsement (e.g., oppose reform) is conditioned on the discussed topic (e.g., government’s role) and its application is different from that of JST. Most of the joint aspect sentiment topic models are either semi-supervised or weakly supervised using sentiment polarity words (Paradigm lists) to boost their efficiency. In our case,</context>
</contexts>
<marker>Zhao, Jiang, Yan, Li, 2010</marker>
<rawString>Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming Li. 2010. Jointly modeling aspects and opinions with a maxent-lda hybrid. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 56– 65, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
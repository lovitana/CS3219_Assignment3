<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010733">
<title confidence="0.926218">
MVA: The Multimodal Virtual Assistant
</title>
<author confidence="0.9281815">
Michael Johnston1, John Chen1, Patrick Ehlen2, Hyuckchul Jung1, Jay Lieske2, Aarthi Reddy1,
Ethan Selfridge1, Svetlana Stoyanchev1, Brant Vasilieff2, Jay Wilpon1
</author>
<affiliation confidence="0.628915">
AT&amp;T Labs Research1, AT&amp;T2
</affiliation>
<email confidence="0.6331605">
{johnston,jchen,ehlen,hjung,jlieske,aarthi,
ethan,sveta,vasilieff,jgw}@research.att.com
</email>
<sectionHeader confidence="0.99732" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999220538461538">
The Multimodal Virtual Assistant (MVA)
is an application that enables users to plan
an outing through an interactive multi-
modal dialog with a mobile device. MVA
demonstrates how a cloud-based multi-
modal language processing infrastructure
can support mobile multimodal interac-
tion. This demonstration will highlight in-
cremental recognition, multimodal speech
and gesture input, contextually-aware lan-
guage understanding, and the targeted
clarification of potentially incorrect seg-
ments within user input.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999">
With the recent launch of virtual assistant appli-
cations such as Siri, Google Now, S-Voice, and
Vlingo, spoken access to information and services
on mobile devices has become commonplace. The
Multimodal Virtual Assistant (MVA) project ex-
plores the application of multimodal dialog tech-
nology in the virtual assistant landscape. MVA de-
parts from the existing paradigm for dialog-based
mobile virtual assistants that display the unfold-
ing dialog as a chat display. Instead, the MVA
prototype situates the interaction directly within a
touch-based interface that combines a map with
visual information displays. Users can interact
using combinations of speech and gesture inputs,
and the interpretation of user commands depends
on both map and GUI display manipulation and
the physical location of the device.
MVA is a mobile application that allows users
to plan a day or evening out with friends using
natural language and gesture input. Users can
search and browse over multiple interconnected
domains, including music events, movie show-
ings, and places to eat. They can specify multi-
ple parameters in natural language, such as “Jazz
concerts around San Francisco next Saturday”. As
users find interesting events and places, they can
be collected together into plans and shared with
others. The central components of the graph-
ical user interface are a dynamic map showing
business and event locations, and an information
display showing the current recognition, system
prompts, search result listing, or plans (Figure 1).
</bodyText>
<figureCaption confidence="0.999403">
Figure 1: MVA User Interface
</figureCaption>
<bodyText confidence="0.999959666666667">
Spoken input begins when the user taps a micro-
phone button on the display. As the user speaks,
incremental speech recognition results appear. In
addition to enabling voice input, the microphone
button also activates the map as a drawing can-
vas, and enables the user to combine speech with
drawing in coordinated multimodal commands.
For example, a user might say, “Movies playing
tonight in this area” while simultaneously outlin-
ing a relevant area on the map. Or a user may say,
“Restaurants” while drawing a line down a spe-
cific street. MVA determines the intent and dis-
ambiguates concepts in the input in order to re-
turn relevant results. MVA then responds to user
input multimodally, by updating the display and
using speech synthesis to summarize results, pro-
vide feedback, or make requests for clarification
and additional information.
</bodyText>
<page confidence="0.840515">
257
</page>
<bodyText confidence="0.6963565">
Proceedings of the SIGDIAL 2014 Conference, pages 257–259,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.932614" genericHeader="method">
2 Sample Interaction
</sectionHeader>
<bodyText confidence="0.999856857142857">
In Figure 2 we present a sample of interaction
from MVA that illustrates some of its capabilities.
The user starts with a spoken natural language
query where they specify some constraints: the
type of music (jazz), location (San Francisco), and
time (tomorrow). The system gets low confidence
on the location, so it constructs a targeted clarifi-
cation for clarifying only that constraint. The user
repeats the location, and then the system searches
for events meeting the user’s constraints. The user
then reviews the results, and follows on with a
refinement: “What about blues?”. Even though
many parameters in this query are underspecified,
the system applies contextually-aware natural lan-
guage understanding and interprets this as “Blues
concerts near San Francisco tomorrow”. After
selecting a concert, the user then searches for a
restaurant nearby. The location of the concert re-
mains salient. The user follows up with a mul-
timodal query combining speech and gesture to
search for similar restaurants in an adjoining area.
</bodyText>
<table confidence="0.607382388888889">
U: “Jazz concerts near San Francisco tomorrow.”
S: “Where did you want to see jazz tomorrow?”
U: “San Francisco.”
S: “I found 20 jazz concerts in San
Francisco tomorrow.”
[Zooms map to San Francisco and displays
pins on map and list of results]
U: “What about blues?”
S: “I found 20 blues concerts in
San Francisco tomorrow.”
U: [Clicks on a concert listing and adds it
to the plan]
U: “Sushi restaurants near there.”
S: “I found 10 sushi restaurants.”
U: “What about here?”
[Circles adjoining area on map]
S: “I found 5 sushi restaurants in
the area you indicated”
</table>
<figureCaption confidence="0.845492">
Figure 2: Sample Interaction
</figureCaption>
<sectionHeader confidence="0.985354" genericHeader="method">
3 System Architecture
</sectionHeader>
<bodyText confidence="0.9997098">
Figure 3 shows the underlying multimodal assis-
tant architecture supporting the MVA app. The
user interacts with a native iOS client. When the
user taps the microphone icon, this initiates the
flow of audio interleaved with gesture and context
information streamed over a WebSocket connec-
tion to the platform.
This stream of interleaved data is handled at
the server by a multimodal natural language pro-
cessing pipeline. This fields incoming packets of
</bodyText>
<figureCaption confidence="0.949861">
Figure 3: MVA Multimodal assistant Architecture
</figureCaption>
<bodyText confidence="0.999931567567567">
data from the client, demuxes the incoming data
stream, and sends audio, ink traces, and context
information to three modules that operate in par-
allel. The audio is processed using the AT&amp;T
WatsonSM speech recognition engine (Goffin et
al., 2005). Recognition is performed using a dy-
namic hierarchical language model (Gilbert et al.,
2011) that combines a statistical N-gram language
model with weighted sub-grammars. Ink traces
are classified into gestures using a linear classi-
fier. Speech recognition results serve as input to
two NLU modules. A discriminative stochastic se-
quence tagger assigns tags to phrases within the
input, and then the overall string with tags is as-
signed by a statistical intent classifier to one of
a number of intents handled by the system e.g.
search(music event), refine(location).
The NLU results are passed along with gesture
recognition results and the GUI and device context
to a multimodal dialog manager. The contextual
resolution component determines if the input is a
query refinement or correction. In either case, it
retrieves the previous command from a user con-
text store and combines the new content with the
context through destructive unification (Ehlen and
Johnston, 2012). A location salience component
then applies to handle cases where a location is
not specified verbally. This component uses a su-
pervised classifier to select from among a series
of candidate locations, including the gesture (if
present), the current device location, or the current
map location (Ehlen and Johnston, 2010).
The resolved semantic interpretation of the ut-
terance is then passed to a Localized Error Detec-
tion (LED) module (Stoyanchev et al., 2012). The
LED module contains two maximum entropy clas-
sifiers that independently predict whether a con-
</bodyText>
<page confidence="0.993588">
258
</page>
<bodyText confidence="0.999990458333333">
cept is present in the input, and whether a con-
cept’s current interpretation is correct. These clas-
sifiers use word scores, segment length, confu-
sion networks and other recognition and context
features. The LED module uses these classifiers
to produce two probability distributions; one for
presence and one for correctness. These distri-
butions are then used by a Targeted Clarification
component (TC) to either accept the input as is,
reject all of the input, or ask a targeted clarifica-
tion question (Stoyanchev et al., 2013). These de-
cisions are currently made using thresholds tuned
manually based on an initial corpus of user inter-
action with MVA. In the targeted clarification case,
the input is passed to the natural language gen-
eration component for surface realization, and a
prompt is passed back to the client for playback
to the user. Critically, the TC component decides
what to attempt to add to the common ground
by explicit or implicit confirmation, and what to
explicitly query from the user; e.g. “Where did
you want to see jazz concerts?”. The TC com-
ponent also updates the context so that incoming
responses from the user can be interpreted with re-
spect to the context set up by the clarification.
Once a command is accepted by the multimodal
dialog manager, it is passed to the Semantic Ab-
straction Layer (SAL) for execution. The SAL in-
sulates natural language dialog capabilities from
the specifics of any underlying external APIs that
the system may use in order to respond to queries.
A general purpose time normalization component
projects relative time expressions like “tomorrow
night” or “next week” onto a reference timeframe
provided by the client context and estimates the
intended time interval. A general purpose location
resolution component maps from natural language
expressions of locations such as city names and
neighborhoods to specific geographic coordinates.
These functions are handled by SAL—rather than
relying on any time and location handling in the
underlying information APIs—to provide consis-
tency across application domains.
SAL also includes mechanisms for category
mapping; the NLU component tags a portion
of the utterance as a concept (e.g., a mu-
sic genre or a cuisine) and SAL leverages
this information to map a word sequence to
generic domain-independent ontological represen-
tations/categories that are reusable across different
backend APIs. Wrappers in SAL map from these
categories, time, and location values to the spe-
cific query language syntax and values for each
specific underlying API. In some cases, a single
natural language query to MVA may require mul-
tiple API calls to complete, and this is captured
in the wrapper. SAL also handles API format dif-
ferences by mapping all API responses into a uni-
fied format. This unified format is then passed to
our natural language generation component to be
augmented with prompts, display text, and instruc-
tions to the client for updating the GUI. This com-
bined specification of a multimodal presentation is
passed to the interaction manager and routed back
to the client to be presented to the user.
In addition to testing the capabilities of our mul-
timodal assistant platform, MVA is designed as a
testbed for running experiments with real users.
Among other topics, we are planning experiments
with MVA to evaluate methods of multimodal in-
formation presentation and natural language gen-
eration, error detection and error recovery.
</bodyText>
<sectionHeader confidence="0.998205" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.993853">
Thanks to Mike Kai and to Deepak Talesra for
their work on the MVA project.
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99964776">
Patrick Ehlen and Michael Johnston. 2010. Location
grounding in multimodal local search. In Proceed-
ings of ICMI-MLMI, pages 32–39.
Patrick Ehlen and Michael Johnston. 2012. Multi-
modal dialogue in mobile local search. In Proceed-
ings of ICMI, pages 303–304.
Mazin Gilbert, Iker Arizmendi, Enrico Bocchieri, Dia-
mantino Caseiro, Vincent Goffin, Andrej Ljolje,
Mike Phillips, Chao Wang, and Jay G. Wilpon.
2011. Your mobile virtual assistant just got smarter!
In Proceedings of INTERSPEECH, pages 1101–
1104. ISCA.
Vincent Goffin, Cyril Allauzen, Enrico Bocchieri,
Dilek Hakkani-Tur, Andrej Ljolje, S. Parthasarathy,
Mazim Rahim, Giuseppe Riccardi, and Murat Sar-
aclar. 2005. The AT&amp;T WATSON speech recog-
nizer. In Proceedings of ICASSP, pages 1033–1036,
Philadelphia, PA, USA.
Svetlana Stoyanchev, Philipp Salletmayer, Jingbo
Yang, and Julia Hirschberg. 2012. Localized de-
tection of speech recognition errors. In Proceedings
of SLT, pages 25–30.
Svetlana Stoyanchev, Alex Liu, and Julia Hirschberg.
2013. Modelling human clarification strategies. In
Proceedings of SIGDIAL 2013, pages 137–141.
</reference>
<page confidence="0.998531">
259
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.291960">
<title confidence="0.999222">MVA: The Multimodal Virtual Assistant</title>
<author confidence="0.763345">John Patrick Hyuckchul Jay Aarthi Svetlana Brant Jay Labs</author>
<abstract confidence="0.996329714285714">The Multimodal Virtual Assistant (MVA) is an application that enables users to plan an outing through an interactive multimodal dialog with a mobile device. MVA demonstrates how a cloud-based multimodal language processing infrastructure can support mobile multimodal interaction. This demonstration will highlight incremental recognition, multimodal speech and gesture input, contextually-aware language understanding, and the targeted clarification of potentially incorrect segments within user input.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Patrick Ehlen</author>
<author>Michael Johnston</author>
</authors>
<title>Location grounding in multimodal local search.</title>
<date>2010</date>
<booktitle>In Proceedings of ICMI-MLMI,</booktitle>
<pages>32--39</pages>
<contexts>
<context position="7098" citStr="Ehlen and Johnston, 2010" startWordPosition="1088" endWordPosition="1091">ultimodal dialog manager. The contextual resolution component determines if the input is a query refinement or correction. In either case, it retrieves the previous command from a user context store and combines the new content with the context through destructive unification (Ehlen and Johnston, 2012). A location salience component then applies to handle cases where a location is not specified verbally. This component uses a supervised classifier to select from among a series of candidate locations, including the gesture (if present), the current device location, or the current map location (Ehlen and Johnston, 2010). The resolved semantic interpretation of the utterance is then passed to a Localized Error Detection (LED) module (Stoyanchev et al., 2012). The LED module contains two maximum entropy classifiers that independently predict whether a con258 cept is present in the input, and whether a concept’s current interpretation is correct. These classifiers use word scores, segment length, confusion networks and other recognition and context features. The LED module uses these classifiers to produce two probability distributions; one for presence and one for correctness. These distributions are then used</context>
</contexts>
<marker>Ehlen, Johnston, 2010</marker>
<rawString>Patrick Ehlen and Michael Johnston. 2010. Location grounding in multimodal local search. In Proceedings of ICMI-MLMI, pages 32–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Ehlen</author>
<author>Michael Johnston</author>
</authors>
<title>Multimodal dialogue in mobile local search.</title>
<date>2012</date>
<booktitle>In Proceedings of ICMI,</booktitle>
<pages>303--304</pages>
<contexts>
<context position="6776" citStr="Ehlen and Johnston, 2012" startWordPosition="1038" endWordPosition="1041">gns tags to phrases within the input, and then the overall string with tags is assigned by a statistical intent classifier to one of a number of intents handled by the system e.g. search(music event), refine(location). The NLU results are passed along with gesture recognition results and the GUI and device context to a multimodal dialog manager. The contextual resolution component determines if the input is a query refinement or correction. In either case, it retrieves the previous command from a user context store and combines the new content with the context through destructive unification (Ehlen and Johnston, 2012). A location salience component then applies to handle cases where a location is not specified verbally. This component uses a supervised classifier to select from among a series of candidate locations, including the gesture (if present), the current device location, or the current map location (Ehlen and Johnston, 2010). The resolved semantic interpretation of the utterance is then passed to a Localized Error Detection (LED) module (Stoyanchev et al., 2012). The LED module contains two maximum entropy classifiers that independently predict whether a con258 cept is present in the input, and wh</context>
</contexts>
<marker>Ehlen, Johnston, 2012</marker>
<rawString>Patrick Ehlen and Michael Johnston. 2012. Multimodal dialogue in mobile local search. In Proceedings of ICMI, pages 303–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mazin Gilbert</author>
<author>Iker Arizmendi</author>
<author>Enrico Bocchieri</author>
<author>Diamantino Caseiro</author>
<author>Vincent Goffin</author>
<author>Andrej Ljolje</author>
<author>Mike Phillips</author>
<author>Chao Wang</author>
<author>Jay G Wilpon</author>
</authors>
<title>Your mobile virtual assistant just got smarter!</title>
<date>2011</date>
<booktitle>In Proceedings of INTERSPEECH,</booktitle>
<pages>1101--1104</pages>
<contexts>
<context position="5895" citStr="Gilbert et al., 2011" startWordPosition="900" endWordPosition="903">with gesture and context information streamed over a WebSocket connection to the platform. This stream of interleaved data is handled at the server by a multimodal natural language processing pipeline. This fields incoming packets of Figure 3: MVA Multimodal assistant Architecture data from the client, demuxes the incoming data stream, and sends audio, ink traces, and context information to three modules that operate in parallel. The audio is processed using the AT&amp;T WatsonSM speech recognition engine (Goffin et al., 2005). Recognition is performed using a dynamic hierarchical language model (Gilbert et al., 2011) that combines a statistical N-gram language model with weighted sub-grammars. Ink traces are classified into gestures using a linear classifier. Speech recognition results serve as input to two NLU modules. A discriminative stochastic sequence tagger assigns tags to phrases within the input, and then the overall string with tags is assigned by a statistical intent classifier to one of a number of intents handled by the system e.g. search(music event), refine(location). The NLU results are passed along with gesture recognition results and the GUI and device context to a multimodal dialog manag</context>
</contexts>
<marker>Gilbert, Arizmendi, Bocchieri, Caseiro, Goffin, Ljolje, Phillips, Wang, Wilpon, 2011</marker>
<rawString>Mazin Gilbert, Iker Arizmendi, Enrico Bocchieri, Diamantino Caseiro, Vincent Goffin, Andrej Ljolje, Mike Phillips, Chao Wang, and Jay G. Wilpon. 2011. Your mobile virtual assistant just got smarter! In Proceedings of INTERSPEECH, pages 1101– 1104. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Goffin</author>
<author>Cyril Allauzen</author>
<author>Enrico Bocchieri</author>
<author>Dilek Hakkani-Tur</author>
<author>Andrej Ljolje</author>
<author>S Parthasarathy</author>
<author>Mazim Rahim</author>
<author>Giuseppe Riccardi</author>
<author>Murat Saraclar</author>
</authors>
<title>The AT&amp;T WATSON speech recognizer.</title>
<date>2005</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>1033--1036</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="5802" citStr="Goffin et al., 2005" startWordPosition="886" endWordPosition="889">lient. When the user taps the microphone icon, this initiates the flow of audio interleaved with gesture and context information streamed over a WebSocket connection to the platform. This stream of interleaved data is handled at the server by a multimodal natural language processing pipeline. This fields incoming packets of Figure 3: MVA Multimodal assistant Architecture data from the client, demuxes the incoming data stream, and sends audio, ink traces, and context information to three modules that operate in parallel. The audio is processed using the AT&amp;T WatsonSM speech recognition engine (Goffin et al., 2005). Recognition is performed using a dynamic hierarchical language model (Gilbert et al., 2011) that combines a statistical N-gram language model with weighted sub-grammars. Ink traces are classified into gestures using a linear classifier. Speech recognition results serve as input to two NLU modules. A discriminative stochastic sequence tagger assigns tags to phrases within the input, and then the overall string with tags is assigned by a statistical intent classifier to one of a number of intents handled by the system e.g. search(music event), refine(location). The NLU results are passed along</context>
</contexts>
<marker>Goffin, Allauzen, Bocchieri, Hakkani-Tur, Ljolje, Parthasarathy, Rahim, Riccardi, Saraclar, 2005</marker>
<rawString>Vincent Goffin, Cyril Allauzen, Enrico Bocchieri, Dilek Hakkani-Tur, Andrej Ljolje, S. Parthasarathy, Mazim Rahim, Giuseppe Riccardi, and Murat Saraclar. 2005. The AT&amp;T WATSON speech recognizer. In Proceedings of ICASSP, pages 1033–1036, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Stoyanchev</author>
<author>Philipp Salletmayer</author>
<author>Jingbo Yang</author>
<author>Julia Hirschberg</author>
</authors>
<title>Localized detection of speech recognition errors.</title>
<date>2012</date>
<booktitle>In Proceedings of SLT,</booktitle>
<pages>25--30</pages>
<contexts>
<context position="7238" citStr="Stoyanchev et al., 2012" startWordPosition="1111" endWordPosition="1114"> retrieves the previous command from a user context store and combines the new content with the context through destructive unification (Ehlen and Johnston, 2012). A location salience component then applies to handle cases where a location is not specified verbally. This component uses a supervised classifier to select from among a series of candidate locations, including the gesture (if present), the current device location, or the current map location (Ehlen and Johnston, 2010). The resolved semantic interpretation of the utterance is then passed to a Localized Error Detection (LED) module (Stoyanchev et al., 2012). The LED module contains two maximum entropy classifiers that independently predict whether a con258 cept is present in the input, and whether a concept’s current interpretation is correct. These classifiers use word scores, segment length, confusion networks and other recognition and context features. The LED module uses these classifiers to produce two probability distributions; one for presence and one for correctness. These distributions are then used by a Targeted Clarification component (TC) to either accept the input as is, reject all of the input, or ask a targeted clarification quest</context>
</contexts>
<marker>Stoyanchev, Salletmayer, Yang, Hirschberg, 2012</marker>
<rawString>Svetlana Stoyanchev, Philipp Salletmayer, Jingbo Yang, and Julia Hirschberg. 2012. Localized detection of speech recognition errors. In Proceedings of SLT, pages 25–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Stoyanchev</author>
<author>Alex Liu</author>
<author>Julia Hirschberg</author>
</authors>
<title>Modelling human clarification strategies.</title>
<date>2013</date>
<booktitle>In Proceedings of SIGDIAL 2013,</booktitle>
<pages>137--141</pages>
<contexts>
<context position="7867" citStr="Stoyanchev et al., 2013" startWordPosition="1212" endWordPosition="1215"> LED module contains two maximum entropy classifiers that independently predict whether a con258 cept is present in the input, and whether a concept’s current interpretation is correct. These classifiers use word scores, segment length, confusion networks and other recognition and context features. The LED module uses these classifiers to produce two probability distributions; one for presence and one for correctness. These distributions are then used by a Targeted Clarification component (TC) to either accept the input as is, reject all of the input, or ask a targeted clarification question (Stoyanchev et al., 2013). These decisions are currently made using thresholds tuned manually based on an initial corpus of user interaction with MVA. In the targeted clarification case, the input is passed to the natural language generation component for surface realization, and a prompt is passed back to the client for playback to the user. Critically, the TC component decides what to attempt to add to the common ground by explicit or implicit confirmation, and what to explicitly query from the user; e.g. “Where did you want to see jazz concerts?”. The TC component also updates the context so that incoming responses</context>
</contexts>
<marker>Stoyanchev, Liu, Hirschberg, 2013</marker>
<rawString>Svetlana Stoyanchev, Alex Liu, and Julia Hirschberg. 2013. Modelling human clarification strategies. In Proceedings of SIGDIAL 2013, pages 137–141.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
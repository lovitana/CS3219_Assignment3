<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005697">
<title confidence="0.999465">
Collaborative Exploration in Human-Robot Teams:
What’s in Their Corpora of Dialog, Video, &amp; LIDAR Messages?
</title>
<author confidence="0.991126">
Clare R. Voss∗ Taylor Cassidy†∗ Douglas Summers-Stay∗
</author>
<affiliation confidence="0.955754">
∗Army Research Laboratory, Adelphi, MD 20783
</affiliation>
<address confidence="0.431159">
†IBM T. J. Watson Research Center, Hawthorne, NY 10532
</address>
<email confidence="0.986468">
fclare.r.voss.civ,taylor.cassidy.ctr,douglas.a.summers-stay.civl@mail.mil
</email>
<sectionHeader confidence="0.993597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933631578948">
This paper briefly sketches new work-in-
progress (i) developing task-based scenar-
ios where human-robot teams collabora-
tively explore real-world environments in
which the robot is immersed but the hu-
mans are not, (ii) extracting and construct-
ing “multi-modal interval corpora” from
dialog, video, and LIDAR messages that
were recorded in ROS bagfiles during task
sessions, and (iii) testing automated meth-
ods to identify, track, and align co-referent
content both within and across modalities
in these interval corpora. The pre-pilot
study and its corpora provide a unique,
empirical starting point for our longer-
term research objective: characterizing the
balance of explicitly shared and tacitly as-
sumed information exchanged during ef-
fective teamwork.
</bodyText>
<sectionHeader confidence="0.995511" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.999872714285714">
Robots that are able to move into areas where peo-
ple cannot during emergencies and collaboratively
explore these environments by teaming with hu-
mans, have tremendous potential to impact search
and rescue operations. For human-robot teams
to conduct such shared missions, humans need to
trust that they will be kept apprised, at a miniu-
</bodyText>
<figureCaption confidence="0.998704">
Figure 1: Outside View: Video Image &amp; LIDAR.
</figureCaption>
<bodyText confidence="0.999174">
mum, of where the robot is and what it is sensing,
as it moves about without them present.
To begin documenting the communication chal-
lenges humans face in taking a robot’s perspective,
we conducted a pre-pilot study1 to record, iden-
tify and track the dialog, video, and LIDAR in-
formation that is explicitly shared by, or indirectly
available to, members of human-robot teams when
conducting collaborative tasks.
</bodyText>
<subsectionHeader confidence="0.9802">
1.1 Approach
</subsectionHeader>
<bodyText confidence="0.999879833333333">
We enlisted colleagues to be the commander (C) or
the human (R) controlling a mobile physical robot
in such tasks. Neither could see the robot. Only
R could “see for” the robot, via its onboard video
camera and LIDAR. C and R communicated by
text chat on their computers, as in this example,
</bodyText>
<equation confidence="0.5225585">
R 41: I can see in the entrance.
C 42: Enter and scan the first room.
R 44: I see a door to the right and a door to the left.
C 45: Scan next open room on left.
</equation>
<figureCaption confidence="0.742546888888889">
Utterances R 41 &amp; C 42 occur when the robot is
outdoors (Fig. 1) and R 44 &amp; C 45 occur after it
moves indoors (Fig. 2). Although our approach re-
sembles a Wizard and Oz paradigm (Riek, 2012),
1Statisticians say pre-pilots are for “kicking the tires,”
early-stage tests of scenarios, equipment, and data collection.
Figure 2: Inside View: Video Image &amp; LIDAR.
Brightness and contrast of video image increased
for print publication.
</figureCaption>
<page confidence="0.997355">
43
</page>
<note confidence="0.7171405">
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 43–47,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999732387096774">
with C as User and R as Wizard controlling the
robot, there is no intent for R to deceive C.
In these dialog snippets, notice that the doors
mentioned in R 44 are not visible in the image
of that utterance’s time interval and, even if they
had been visible, their referents were context-
dependent and ambiguous. How are the robot and
human to refer to the same door? This challenge
entails resolving several types of co-reference (lin-
guistic, are they talking about the same door? vi-
sual, are they looking at the door? navigational, is
one backing into a door no longer in view but pre-
viosuly stored in its map?) Successful communi-
cation on human-robot teams, where humans send
messages to direct robot movements and receive
robot-processed messages as the robot navigates,
entails effective identification of named referents
(such as doors), both within and across available
modalities during exploratory tasks. The research
question is, how might the identification and align-
ment of entities using combinations of (i) NLP
on dialog, (ii) image processing on the video and
LIDAR stream, with (iii) robot position, motion,
and orientation coordinates, support more effec-
tive human-robot missions?
We conducted the pre-pilot study with ten trial
sessions to collect multi-modal data from C-R and
R-only scenarios (Table 1). Each session involved
a single participant playing the role of R with con-
trol over the physical robot, or two participants,
one person playing R and one playing C.
</bodyText>
<table confidence="0.995372714285714">
Team R’s Task
R only Rotate in place and describe surroundings.
R only Move along road, describe surroundings.
C, R Follow C’s guidance in navigating build-
ing’s perimeter, describe surroundings.
C, R Follow C’s guidance in searching buildings
for specified objects.
</table>
<tableCaption confidence="0.999765">
Table 1: Pre-pilot Scenarios.
</tableCaption>
<bodyText confidence="0.999547285714286">
Participants sat indoors and could not see the robot
outside, roughly 30 meters away. In each session,
R was instructed to act as though he or she were
situated in the robot’s position and to obey C. R
was to consider the robot’s actions as R’s own,
and to consider available video and LIDAR point
cloud feeds as R’s own perceptions.
</bodyText>
<subsectionHeader confidence="0.963461">
1.2 Equipment
</subsectionHeader>
<bodyText confidence="0.999916238095238">
All participants worked from their own comput-
ers. Each was instructed, for a given scenario, to
be either C or R and to communicate by text only.
On their screen they saw a dedicated dialog (chat)
window in a Linux terminal. For sessions with
both C and R, the same dialog content (the ongo-
ing sequence of typed-in utterances) appeared in
the dialog window on each of their screens.
The physical robot ran under the Robot Operat-
ing System (ROS) (Quigley et al., 2009), equipped
with a video camera, laser sensors, magnetome-
ter, GPS unit, and rotary encoders. R could “see
for the robot” via two ROS rviz windows with live
feeds for video from the robot’s camera and con-
structed 3D point cloud frames.2 R had access to
rotate and zoom functions to alter the screen dis-
play of the point cloud. C saw only a static bird’s-
eye-view map of the area. R remotely controlled
over a network connection the robot’s four wheels
and its motion, using the left joystick of an X-Box
controller.
</bodyText>
<subsectionHeader confidence="0.990408">
1.3 Collection
</subsectionHeader>
<bodyText confidence="0.999984428571429">
During each session, all data from the robot’s sen-
sors and dialog window was recorded via the ros-
bag tool and stored in a single bagfile.3 A bagfile
contains typed messages. Each message contains
a timestamp (specified at nanosecond granularity)
and values for that message type’s attributes. Mes-
sage types geometry msgs/PoseStamped, for ex-
ample, contain a time stamp, a three-dimensional
location vector and a four-dimensional orientation
vector that indicates an estimate of the robot’s lo-
cation and the direction in which it is facing. The
robot’s rotary encoders generate these messages
as the robot moves. The primary bagfile message
types most relevant to our initial analyses4 were:
</bodyText>
<listItem confidence="0.999525142857143">
1) instant messenger/StringStamped
that included speaker id, text utterances
2) sensor msgs/PointCloud2
that included LIDAR data
3) sensor msgs/CompressedImage
with compressed, rectified video images
4) sensor msgs/GPS, with robot coordinates
</listItem>
<bodyText confidence="0.999861625">
Message types are packaged and published at dif-
ferent rates: some are published automatically at
regular intervals (e.g., image frames), while oth-
ers depend on R, C, or robot activity (e.g., dialog
utterances). And the specific rate of publication
for some message types can be limited at times by
network bandwidth constraints (e.g. LIDAR data).
Summary statistics for our initial pre-pilot collec-
</bodyText>
<footnote confidence="0.985737">
2LIDAR measures distance from robot by illuminating
targets with robot lasers and generates point cloud messages.
3http://wiki.ros.org/rosbag
4We omit here details of ROS topics, transformation mes-
sages, and other sensor data collected in the pre-pilot.
</footnote>
<page confidence="0.99878">
44
</page>
<bodyText confidence="0.925876083333333">
tion consisting of ten task sessions conducted over
two days, and that together spanned roughly five
hours in real-time, are presented in Table 2.
#bagfile msgs 15, 131K #dialog utts 434
min per sn 140, 848 min per sn 15
max per sn 3, 030K max per sn 116
#tokens 3, 750 #image msgs 10, 650
min per sn 200 min per sn 417
max per sn 793 max per sn 1, 894
#unique words 568 #LIDAR msgs 8, 422
min per sn 84 min per sn 215
max per sn 176 max per sn 2, 250
</bodyText>
<tableCaption confidence="0.7578865">
Table 2: Collection Statistics (sn = session).
2 From Collection to Interval Corpora
</tableCaption>
<bodyText confidence="0.999982470588235">
After collecting millions of messages in the pre-
pilot with content in different modalities, the im-
mediate research challenge has been identifying
the time interval that covers the messages directly
related to the content in each utterance.
We extracted each utterance message u and its
corresponding time stamp t. For a given u, we ex-
tracted the five image, five point cloud, and five
GPS messages immediately preceding and the five
of each immediately following u, based on mes-
sage time-stamps, for a total of thirty sensor mes-
sages per utterance. These message types were
published independent of the robot’s movement,
approximately once per second. In the second
phase, we assigned the earliest and latest time
stamp from the first-phase messages to delimit an
interval [ts, te] and conducted another extraction
round from the bagfile, this time pulling out all
messages with time stamps in that interval as pub-
lished by the rotary encoders, compass, and iner-
tial measurement unit, only when the robot moved.
The messages from both phases constitute a ten-
second interval corpus for u.
These interval corpora serve as a first approx-
imation at segmenting the massive stream pub-
lished at nanosecond-level into units pertaining to
commander-robot dialog during the task at hand.
With manual inspection, we found that many
automatically-constructed intervals do track rele-
vant changes in the robot’s location. For exam-
ple, the latest interval in a task’s time sequence
that was constructed with the robot being outside a
building is distinct from the first interval that cov-
ers when the robot moves inside the building.5
</bodyText>
<footnote confidence="0.9098465">
5This appears likely due to the paced descriptions in R’s
utterances. Another pre-pilot is needed to test this hypothesis.
</footnote>
<sectionHeader confidence="0.90841" genericHeader="introduction">
3 Corpora Language Processing
</sectionHeader>
<bodyText confidence="0.999655">
Each utterance collected from the sessions was
tokenized, parsed, and semantically interpreted
using SLURP (Brooks et al., 2012), a well-
tested NLP front-end component of a human-robot
system.6 The progression in SLURP’s analysis
pipeline for utterance C 45 is shown in Figure 3.
SLURP extracts a parse tree (top-left), identifies
a sub-tree that constitutes a verb-argument struc-
ture, and enumerates possibly matching sense-
specific verb frames from VerbNet (Schuler, 2005)
(bottom-left). VerbNet provides a syntactic to se-
mantic role mapping for each frame (top-right).
SLURP selects the best mapping and generates a
compact semantic representation (bottom-right).7
In this example, the correct sense of “scan” is se-
lected (investigate-35.4) along with a frame that
matches the syntactic parse. Overall, half the com-
mands run through SLURP generated a semantic
interpretation. Of the other half, roughly one quar-
ter failed or had errors at parsing and the other
quarter at the argument matching stage.
</bodyText>
<figureCaption confidence="0.998734">
Figure 3: Analyses of Scan next open room on left.
</figureCaption>
<bodyText confidence="0.999973571428571">
Our next step is to augment SLURP’s lexicon
and retrain a parser for new vocabulary so that we
can directly map semantic structures of the pre-
pilot corpora into ResearchCyc8, an extensive on-
tology, for cross-reference to other events and ob-
jects, already stored and possibly originated as vi-
sual input. Following McFate (2010), we will test
</bodyText>
<footnote confidence="0.9947346">
6https://github.com/PennNLP/SLURP.
7Verbnet associates each frame with a conjunction of
boolean semantic predicates that specify how and when event
participants interact, for an event variable (not shown).
8ResearchCyc and CycL are trademarks of Cycorp, Inc.
</footnote>
<page confidence="0.999128">
45
</page>
<figureCaption confidence="0.999213">
Figure 4: Outside View: Image, Zones, Overlay
</figureCaption>
<bodyText confidence="0.998843666666667">
the mapping of matched VerbNet frames to Re-
searchCyc’s semantic predicates to assess its lexi-
cal coverage for our corpora.
</bodyText>
<sectionHeader confidence="0.996932" genericHeader="method">
4 Image Processing
</sectionHeader>
<bodyText confidence="0.99998646875">
Interval corpus images were labelled by a neu-
ral network trained for visual scene classifica-
tion (Munoz, 2013) of nine material classes: dirt,
foliage, grass, road, sidewalk, sky, wall, wood, and
ground cover (organic debris). Figures 4 and 5
show the images from Figures 1 and 2 with two
additional versions: one with colored zones for
system-recognized class boundaries and another
with colored zones as trasparent overlays on the
original. The classes differentiate terrain types
that work well with route-finding techniques that
leverage them in selecting traversible paths. As the
robot systems are enhanced with more sophisti-
cated path planning software, that knowledge may
be combined with recognized zones to send team
members messages about navigation problems as
the robot explores where they cannot go.
Accuracy is limited at the single image level:
the actual grass in Figure 4 is mostly mis-classified
as dirt (blue) along with some correctly identified
grass (green), while the floor in Figure 5 is mis-
classified as road, although much of what shows
through the window is correctly classified as fo-
liage. We are experimenting with automatically
assigning natural language (NL) labels to a range
of objects and textures recognized in images from
other larger datasets. We can retrieve labeled im-
ages stored in ResearchCyc via NL query con-
verted into CycL, allowing a commander to, for
example, ask questions about objects and regions
using terms related to but not necessarily equal to
the original recognition system-provided labels.
</bodyText>
<sectionHeader confidence="0.999903" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.962353740740741">
We are aware of no other multi-modal corpora
obtained from human-robot teams conducting ex-
ploratory missions with collected dialog, video
and other sensor data. Corpora with a robot
Figure 5: Inside View: Image, Zones, Overlay.
Brightness and contrast of video image and over-
lay increased for print publication.
recording similar data modalities do exist (Green
et al., 2006; Wienke et al., 2012; Maas et al., 2006)
but for fundamentally different tasks. Tellex et al.
(2011) and Matuszek et al. (2012) pair commands
with formal plans without dialog and Zender et al.
(2008) and Randelli et al. (2013) build multi-level
maps but with a situated commander.
Eberhard et al. (2010)’s CReST corpus contains
a set-up similar to ours minus the robot; a hu-
man task-solver wears a forward-facing camera
instead. The SCARE corpus (Stoia et al., 2008)
records similar modalities but in a virtual environ-
ment, where C has full access to R’s video feed.
Other projects yielded corpora from virtual envi-
ronments that include route descriptions without
dialog (Marge and Rudnicky, 2011; MacMahon et
al., 2006; Vogel and Jurafsky, 2010) or referring
expressions without routes (Sch¨utte et al., 2010;
Fang et al., 2013), assuming pre-existing abstrac-
tions from sensor data.
</bodyText>
<sectionHeader confidence="0.972309" genericHeader="conclusions">
6 Conclusion and Ongoing Work
</sectionHeader>
<bodyText confidence="0.999898421052631">
We have presented our pre-pilot study with data
collection and corpus construction phases. This
work-in-progress requires further analysis. We are
now processing dialog utterances for more system-
atic semantic interpretation using disambiguated
VerbNet frames that map into ResearchCyc pred-
icates. We will run object recognition software
retrained on a broader range of objects so that
it can be applied to images that will be labelled
and stored in ResearchCyc micro-worlds for sub-
sequent co-reference with terms in the dialog ut-
terances. Ultimately we want to establish in real
time links across parts of messages in different
modalities that refer to the same abstract enti-
ties, so that humans and robots can share their
separately-obtained knowledge about the entities
and their spatial relations — whether seen, sensed,
described, or inferred — when communicating on
shared tasks in environments.
</bodyText>
<page confidence="0.999157">
46
</page>
<sectionHeader confidence="0.998332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999609571428571">
Over a dozen engineers and researchers assisted
us in many ways before, during, and after the pre-
pilot, providing technical help with equipment and
data collection, as well as participating in the pre-
pilot. We cannot list everyone here, but special
thanks to Stuart Young for providing clear guid-
ance to everyone working with us.
</bodyText>
<sectionHeader confidence="0.998518" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99981725">
Daniel J. Brooks, Constantine Lignos, Cameron Finu-
cane, Mikhail S. Medvedev, Ian Perera, Vasumathi
Raman, Hadas Kress-Gazit, Mitch Marcus, and
Holly A. Yanco. 2012. Make it so: Continu-
ous, flexible natural language interaction with an au-
tonomous robot. In Proc. AAAI, pages 2–8.
Kathleen M. Eberhard, Hannele Nicholson, Sandra
K¨ubler, Susan Gundersen, and Matthias Scheutz.
2010. The indiana ”cooperative remote search task”
(crest) corpus. In Proc. LREC.
Rui Fang, Changsong Liu, Lanbo She, and Joyce Y.
Chai. 2013. Towards situated dialogue: Revisiting
referring expression generation. In Proc. EMNLP,
pages 392–402.
Anders Green, Helge Httenrauch, and Kerstin Severin-
son Eklundh. 2006. Developing a contextualized
multimodal corpus for human-robot interaction. In
Proc. LREC.
Jan F. Maas, Britta Wrede, and Gerhard Sagerer. 2006.
Towards a multimodal topic tracking system for a
mobile robot. In Proc. INTERSPEECH.
Matt MacMahon, Brian Stankiewicz, and Benjamin
Kuipers. 2006. Walk the talk: Connecting language,
knowledge, and action in route instructions. In Proc.
AAAI, pages 1475–1482.
Matthew Marge and Alexander I Rudnicky. 2011.
The teamtalk corpus: Route instructions in open
spaces. In Proc. RSS, Workshop on Grounding
Human-Robot Dialog for Spatial Tasks.
Cynthia Matuszek, Evan Herbst, Luke S. Zettlemoyer,
and Dieter Fox. 2012. Learning to parse natural
language commands to a robot control system. In
Proc. ISER, pages 403–415.
Clifton McFate. 2010. Expanding verb coverage in
cyc with verbnet. In Proc. ACL, Student Research
Workshop, pages 61–66.
Daniel Munoz. 2013. Inference Machines: Pars-
ing Scenes via Iterated Predictions. Ph.D. thesis,
Carnegie Mellon University.
Morgan Quigley, Ken Conley, Brian Gerkey, Josh
Faust, Tully B. Foote, Jeremy Leibs, Rob Wheeler,
and Andrew Y. Ng. 2009. ROS: an open-source
robot operating system. In Proc. ICRA, Workshop
on Open Source Software.
Gabriele Randelli, Taigo Maria Bonanni, Luca Iocchi,
and Daniele Nardi. 2013. Knowledge acquisition
through human–robot multimodal interaction. Intel-
ligent Service Robotics, 6(1):19–31.
Laurel D Riek. 2012. Wizard of oz studies in hri:
A systematic review and new reporting guidelines.
Journal of Human-Robot Interaction, 1(1).
Karin Kipper Schuler. 2005. Verbnet: A Broad-
coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
Niels Sch¨utte, John D. Kelleher, and Brian Mac
Namee. 2010. Visual salience and reference reso-
lution in situated dialogues: A corpus-based evalu-
ation. In Proc. AAAI, Fall Symposium: Dialog with
Robots.
Laura Stoia, Darla Magdalena Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2008. Scare: a situ-
ated corpus with annotated referring expressions. In
Proc. LREC.
Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew R. Walter, Ashis Gopal Banerjee, Seth J.
Teller, and Nicholas Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. In Proc. AAAI.
Adam Vogel and Daniel Jurafsky. 2010. Learning to
follow navigational directions. In Proc. ACL, pages
806–814.
Johannes Wienke, David Klotz, and Sebastian Wrede.
2012. A framework for the acquisition of mul-
timodal human-robot interaction data sets with a
whole-system perspective. In Proc. LREC, Work-
shop on Multimodal Corpora for Machine Learning.
Hendrik Zender, O Martinez Mozos, Patric Jensfelt, G-
JM Kruijff, and Wolfram Burgard. 2008. Concep-
tual spatial representations for indoor mobile robots.
Robotics and Autonomous Systems, 56(6):493–502.
</reference>
<page confidence="0.999487">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.967238">Collaborative Exploration in Human-Robot What’s in Their Corpora of Dialog, Video, &amp; LIDAR Messages?</title>
<email confidence="0.20932">R.</email>
<address confidence="0.907083">Research Laboratory, Adelphi, MD 20783 T. J. Watson Research Center, Hawthorne, NY 10532</address>
<email confidence="0.996607">fclare.r.voss.civ,taylor.cassidy.ctr,douglas.a.summers-stay.civl@mail.mil</email>
<abstract confidence="0.998182161157024">This paper briefly sketches new work-inprogress (i) developing task-based scenarios where human-robot teams collaboratively explore real-world environments in which the robot is immersed but the humans are not, (ii) extracting and constructing “multi-modal interval corpora” from dialog, video, and LIDAR messages that recorded in bagfiles task sessions, and (iii) testing automated methods to identify, track, and align co-referent content both within and across modalities in these interval corpora. The pre-pilot study and its corpora provide a unique, empirical starting point for our longerterm research objective: characterizing the balance of explicitly shared and tacitly assumed information exchanged during effective teamwork. 1 Overview Robots that are able to move into areas where people cannot during emergencies and collaboratively explore these environments by teaming with humans, have tremendous potential to impact search and rescue operations. For human-robot teams to conduct such shared missions, humans need to that they will be kept apprised, at a miniu- Figure 1: Outside View: Video Image &amp; LIDAR. mum, of where the robot is and what it is sensing, as it moves about without them present. To begin documenting the communication challenges humans face in taking a robot’s perspective, conducted a record, identify and track the dialog, video, and LIDAR information that is explicitly shared by, or indirectly available to, members of human-robot teams when conducting collaborative tasks. 1.1 Approach We enlisted colleagues to be the commander (C) or the human (R) controlling a mobile physical robot in such tasks. Neither could see the robot. Only R could “see for” the robot, via its onboard video camera and LIDAR. C and R communicated by text chat on their computers, as in this example, R 41: I can see in the entrance. C 42: Enter and scan the first room. R 44: I see a door to the right and a door to the left. C 45: Scan next open room on left. Utterances R 41 &amp; C 42 occur when the robot is outdoors (Fig. 1) and R 44 &amp; C 45 occur after it moves indoors (Fig. 2). Although our approach rea (Riek, 2012), say for “kicking the tires,” early-stage tests of scenarios, equipment, and data collection. Figure 2: Inside View: Video Image &amp; LIDAR. Brightness and contrast of video image increased for print publication. 43 of the of the EACL 2014 Workshop on Dialogue in Motion pages 43–47, Sweden, April 26-30 2014. Association for Computational Linguistics with C as User and R as Wizard controlling the robot, there is no intent for R to deceive C. In these dialog snippets, notice that the doors mentioned in R 44 are not visible in the image of that utterance’s time interval and, even if they had been visible, their referents were contextdependent and ambiguous. How are the robot and human to refer to the same door? This challenge entails resolving several types of co-reference (linguistic, are they talking about the same door? visual, are they looking at the door? navigational, is one backing into a door no longer in view but previosuly stored in its map?) Successful communion human-robot teams, humans send messages to direct robot movements and receive robot-processed messages as the robot navigates, entails effective identification of named referents (such as doors), both within and across available modalities during exploratory tasks. The research question is, how might the identification and alignment of entities using combinations of (i) NLP on dialog, (ii) image processing on the video and LIDAR stream, with (iii) robot position, motion, and orientation coordinates, support more effective human-robot missions? We conducted the pre-pilot study with ten trial sessions to collect multi-modal data from C-R and R-only scenarios (Table 1). Each session involved a single participant playing the role of R with control over the physical robot, or two participants, one person playing R and one playing C. Team R’s Task R only Rotate in place and describe surroundings. R only Move along road, describe surroundings. C, R Follow C’s guidance in navigating build-ing’s perimeter, describe surroundings. C, R Follow C’s guidance in searching buildings for specified objects. Table 1: Pre-pilot Scenarios. Participants sat indoors and could not see the robot outside, roughly 30 meters away. In each session, R was instructed to act as though he or she were situated in the robot’s position and to obey C. R was to consider the robot’s actions as R’s own, and to consider available video and LIDAR point cloud feeds as R’s own perceptions. 1.2 Equipment All participants worked from their own computers. Each was instructed, for a given scenario, to be either C or R and to communicate by text only. On their screen they saw a dedicated dialog (chat) window in a Linux terminal. For sessions with both C and R, the same dialog content (the ongoing sequence of typed-in utterances) appeared in the dialog window on each of their screens. The physical robot ran under the Robot Operating System (ROS) (Quigley et al., 2009), equipped with a video camera, laser sensors, magnetometer, GPS unit, and rotary encoders. R could “see the robot” via two ROS with live feeds for video from the robot’s camera and con- 3D point cloud R had access to rotate and zoom functions to alter the screen display of the point cloud. C saw only a static bird’seye-view map of the area. R remotely controlled over a network connection the robot’s four wheels and its motion, using the left joystick of an X-Box controller. 1.3 Collection During each session, all data from the robot’s senand dialog window was recorded via the rosand stored in a single A bagfile typed Each message contains a timestamp (specified at nanosecond granularity) and values for that message type’s attributes. Mestypes for example, contain a time stamp, a three-dimensional location vector and a four-dimensional orientation vector that indicates an estimate of the robot’s location and the direction in which it is facing. The robot’s rotary encoders generate these messages as the robot moves. The primary bagfile message most relevant to our initial were: 1) instant messenger/StringStamped that included speaker id, text utterances 2) sensor msgs/PointCloud2 that included LIDAR data 3) sensor msgs/CompressedImage with compressed, rectified video images 4) sensor msgs/GPS, with robot coordinates types are packaged and different rates: some are published automatically at regular intervals (e.g., image frames), while others depend on R, C, or robot activity (e.g., dialog utterances). And the specific rate of publication for some message types can be limited at times by network bandwidth constraints (e.g. LIDAR data). statistics for our initial pre-pilot collecmeasures distance from robot by illuminating targets with robot lasers and generates point cloud messages. omit here details of ROS mesand other sensor data collected in the pre-pilot. 44 tion consisting of ten task sessions conducted over two days, and that together spanned roughly five hours in real-time, are presented in Table 2. 140, 848 min per 15 3, 030K max per sn 116 min per sn 200 min per sn 417 max per sn 793 max per sn 1, 894 min per sn 84 min per sn 215 max per sn 176 max per sn 2, 250 Table 2: Collection Statistics (sn = session). 2 From Collection to Interval Corpora After collecting millions of messages in the prepilot with content in different modalities, the immediate research challenge has been identifying the time interval that covers the messages directly related to the content in each utterance. extracted each utterance message its time stamp For a given we extracted the five image, five point cloud, and five GPS messages immediately preceding and the five each immediately following based on message time-stamps, for a total of thirty sensor messages per utterance. These message types were published independent of the robot’s movement, approximately once per second. In the second phase, we assigned the earliest and latest time stamp from the first-phase messages to delimit an conducted another extraction round from the bagfile, this time pulling out all messages with time stamps in that interval as published by the rotary encoders, compass, and inertial measurement unit, only when the robot moved. The messages from both phases constitute a tencorpus These interval corpora serve as a first approximation at segmenting the massive stream published at nanosecond-level into units pertaining to commander-robot dialog during the task at hand. With manual inspection, we found that many automatically-constructed intervals do track relevant changes in the robot’s location. For example, the latest interval in a task’s time sequence that was constructed with the robot being outside a building is distinct from the first interval that covwhen the robot moves inside the appears likely due to the paced descriptions in R’s utterances. Another pre-pilot is needed to test this hypothesis. 3 Corpora Language Processing Each utterance collected from the sessions was tokenized, parsed, and semantically interpreted using SLURP (Brooks et al., 2012), a welltested NLP front-end component of a human-robot The progression in SLURP’s analysis pipeline for utterance C 45 is shown in Figure 3. SLURP extracts a parse tree (top-left), identifies a sub-tree that constitutes a verb-argument structure, and enumerates possibly matching senseframes VerbNet (Schuler, 2005) (bottom-left). VerbNet provides a syntactic to semantic role mapping for each frame (top-right). SLURP selects the best mapping and generates a semantic representation In this example, the correct sense of “scan” is sealong with a frame that matches the syntactic parse. Overall, half the commands run through SLURP generated a semantic interpretation. Of the other half, roughly one quarter failed or had errors at parsing and the other quarter at the argument matching stage. 3: Analyses of next open room on left. Our next step is to augment SLURP’s lexicon and retrain a parser for new vocabulary so that we can directly map semantic structures of the precorpora into an extensive ontology, for cross-reference to other events and objects, already stored and possibly originated as visual input. Following McFate (2010), we will test associates each frame with a conjunction of boolean semantic predicates that specify how and when event participants interact, for an event variable (not shown).</abstract>
<affiliation confidence="0.74681">and CycL are trademarks of Cycorp, Inc.</affiliation>
<address confidence="0.634136">45</address>
<abstract confidence="0.993205734042554">Figure 4: Outside View: Image, Zones, Overlay the mapping of matched VerbNet frames to ResearchCyc’s semantic predicates to assess its lexical coverage for our corpora. 4 Image Processing Interval corpus images were labelled by a neural network trained for visual scene classification (Munoz, 2013) of nine material classes: dirt, foliage, grass, road, sidewalk, sky, wall, wood, and ground cover (organic debris). Figures 4 and 5 show the images from Figures 1 and 2 with two additional versions: one with colored zones for system-recognized class boundaries and another with colored zones as trasparent overlays on the original. The classes differentiate terrain types that work well with route-finding techniques that leverage them in selecting traversible paths. As the robot systems are enhanced with more sophisticated path planning software, that knowledge may be combined with recognized zones to send team members messages about navigation problems as the robot explores where they cannot go. Accuracy is limited at the single image level: the actual grass in Figure 4 is mostly mis-classified along with some correctly identified while the floor in Figure 5 is misas much of what shows the window is correctly classified as foare experimenting with automatically assigning natural language (NL) labels to a range of objects and textures recognized in images from other larger datasets. We can retrieve labeled images stored in ResearchCyc via NL query converted into CycL, allowing a commander to, for example, ask questions about objects and regions using terms related to but not necessarily equal to the original recognition system-provided labels. 5 Related Work We are aware of no other multi-modal corpora obtained from human-robot teams conducting exploratory missions with collected dialog, video and other sensor data. Corpora with a robot Figure 5: Inside View: Image, Zones, Overlay. Brightness and contrast of video image and overlay increased for print publication. recording similar data modalities do exist (Green et al., 2006; Wienke et al., 2012; Maas et al., 2006) but for fundamentally different tasks. Tellex et al. (2011) and Matuszek et al. (2012) pair commands with formal plans without dialog and Zender et al. (2008) and Randelli et al. (2013) build multi-level maps but with a situated commander. Eberhard et al. (2010)’s CReST corpus contains a set-up similar to ours minus the robot; a human task-solver wears a forward-facing camera instead. The SCARE corpus (Stoia et al., 2008) records similar modalities but in a virtual environment, where C has full access to R’s video feed. Other projects yielded corpora from virtual environments that include route descriptions without dialog (Marge and Rudnicky, 2011; MacMahon et al., 2006; Vogel and Jurafsky, 2010) or referring expressions without routes (Sch¨utte et al., 2010; Fang et al., 2013), assuming pre-existing abstractions from sensor data. 6 Conclusion and Ongoing Work We have presented our pre-pilot study with data collection and corpus construction phases. This work-in-progress requires further analysis. We are now processing dialog utterances for more systematic semantic interpretation using disambiguated VerbNet frames that map into ResearchCyc predicates. We will run object recognition software retrained on a broader range of objects so that it can be applied to images that will be labelled and stored in ResearchCyc micro-worlds for subsequent co-reference with terms in the dialog utterances. Ultimately we want to establish in real time links across parts of messages in different modalities that refer to the same abstract entities, so that humans and robots can share their separately-obtained knowledge about the entities and their spatial relations — whether seen, sensed, described, or inferred — when communicating on shared tasks in environments. 46 Acknowledgments Over a dozen engineers and researchers assisted us in many ways before, during, and after the prepilot, providing technical help with equipment and data collection, as well as participating in the prepilot. We cannot list everyone here, but special thanks to Stuart Young for providing clear guidance to everyone working with us.</abstract>
<title confidence="0.94798">References</title>
<author confidence="0.52217">Make it so Continu-</author>
<abstract confidence="0.557106">ous, flexible natural language interaction with an aurobot. In pages 2–8.</abstract>
<author confidence="0.793048">Kathleen M Eberhard</author>
<author confidence="0.793048">Hannele Nicholson</author>
<author confidence="0.793048">Sandra</author>
<note confidence="0.611806636363636">K¨ubler, Susan Gundersen, and Matthias Scheutz. 2010. The indiana ”cooperative remote search task” corpus. In Rui Fang, Changsong Liu, Lanbo She, and Joyce Y. Chai. 2013. Towards situated dialogue: Revisiting expression generation. In pages 392–402. Anders Green, Helge Httenrauch, and Kerstin Severinson Eklundh. 2006. Developing a contextualized multimodal corpus for human-robot interaction. In Jan F. Maas, Britta Wrede, and Gerhard Sagerer. 2006.</note>
<title confidence="0.4607945">Towards a multimodal topic tracking system for a robot. In</title>
<author confidence="0.553238">Matt MacMahon</author>
<author confidence="0.553238">Brian Stankiewicz</author>
<author confidence="0.553238">Benjamin</author>
<note confidence="0.450373">Kuipers. 2006. Walk the talk: Connecting language, and action in route instructions. In pages 1475–1482. Matthew Marge and Alexander I Rudnicky. 2011.</note>
<title confidence="0.933924">The teamtalk corpus: Route instructions in open In RSS, Workshop on Grounding Dialog for Spatial</title>
<author confidence="0.995235">Cynthia Matuszek</author>
<author confidence="0.995235">Evan Herbst</author>
<author confidence="0.995235">Luke S Zettlemoyer</author>
<abstract confidence="0.7704795">and Dieter Fox. 2012. Learning to parse natural language commands to a robot control system. In</abstract>
<note confidence="0.5329005">pages 403–415. Clifton McFate. 2010. Expanding verb coverage in with verbnet. In ACL, Student Research pages 61–66. Munoz. 2013. Machines: Pars- Scenes via Iterated Ph.D. thesis,</note>
<affiliation confidence="0.895803">Carnegie Mellon University.</affiliation>
<author confidence="0.876677666666667">ROS an open-source</author>
<title confidence="0.6941985">operating system. In ICRA, Workshop Open Source</title>
<author confidence="0.986047">Gabriele Randelli</author>
<author confidence="0.986047">Taigo Maria Bonanni</author>
<author confidence="0.986047">Luca Iocchi</author>
<abstract confidence="0.362574">and Daniele Nardi. 2013. Knowledge acquisition human–robot multimodal interaction. Intel-</abstract>
<note confidence="0.897723">Service 6(1):19–31. Laurel D Riek. 2012. Wizard of oz studies in hri: A systematic review and new reporting guidelines. of Human-Robot 1(1).</note>
<author confidence="0.844525">A Broad-</author>
<affiliation confidence="0.9233195">Comprehensive Verb Ph.D. thesis, University of Pennsylvania.</affiliation>
<author confidence="0.697143">Niels Sch¨utte</author>
<author confidence="0.697143">John D Kelleher</author>
<author confidence="0.697143">Brian Mac</author>
<abstract confidence="0.856587">Namee. 2010. Visual salience and reference resolution in situated dialogues: A corpus-based evalu- In AAAI, Fall Symposium: Dialog with Laura Stoia, Darla Magdalena Shockley, Donna K. Byron, and Eric Fosler-Lussier. 2008. Scare: a situated corpus with annotated referring expressions. In Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R. Walter, Ashis Gopal Banerjee, Seth J. Teller, and Nicholas Roy. 2011. Understanding natural language commands for robotic navigation and manipulation. In Adam Vogel and Daniel Jurafsky. 2010. Learning to navigational directions. In pages 806–814. Johannes Wienke, David Klotz, and Sebastian Wrede. 2012. A framework for the acquisition of multimodal human-robot interaction data sets with a perspective. In LREC, Work-</abstract>
<title confidence="0.915255">on Multimodal Corpora for Machine</title>
<author confidence="0.90441">Hendrik Zender</author>
<author confidence="0.90441">O Martinez Mozos</author>
<author confidence="0.90441">Patric Jensfelt</author>
<author confidence="0.90441">G-</author>
<note confidence="0.73932475">JM Kruijff, and Wolfram Burgard. 2008. Conceptual spatial representations for indoor mobile robots. and Autonomous 56(6):493–502. 47</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel J Brooks</author>
<author>Constantine Lignos</author>
<author>Cameron Finucane</author>
<author>Mikhail S Medvedev</author>
<author>Ian Perera</author>
<author>Vasumathi Raman</author>
<author>Hadas Kress-Gazit</author>
<author>Mitch Marcus</author>
<author>Holly A Yanco</author>
</authors>
<title>Make it so: Continuous, flexible natural language interaction with an autonomous robot.</title>
<date>2012</date>
<booktitle>In Proc. AAAI,</booktitle>
<pages>2--8</pages>
<contexts>
<context position="10124" citStr="Brooks et al., 2012" startWordPosition="1649" endWordPosition="1652">th manual inspection, we found that many automatically-constructed intervals do track relevant changes in the robot’s location. For example, the latest interval in a task’s time sequence that was constructed with the robot being outside a building is distinct from the first interval that covers when the robot moves inside the building.5 5This appears likely due to the paced descriptions in R’s utterances. Another pre-pilot is needed to test this hypothesis. 3 Corpora Language Processing Each utterance collected from the sessions was tokenized, parsed, and semantically interpreted using SLURP (Brooks et al., 2012), a welltested NLP front-end component of a human-robot system.6 The progression in SLURP’s analysis pipeline for utterance C 45 is shown in Figure 3. SLURP extracts a parse tree (top-left), identifies a sub-tree that constitutes a verb-argument structure, and enumerates possibly matching sensespecific verb frames from VerbNet (Schuler, 2005) (bottom-left). VerbNet provides a syntactic to semantic role mapping for each frame (top-right). SLURP selects the best mapping and generates a compact semantic representation (bottom-right).7 In this example, the correct sense of “scan” is selected (inve</context>
</contexts>
<marker>Brooks, Lignos, Finucane, Medvedev, Perera, Raman, Kress-Gazit, Marcus, Yanco, 2012</marker>
<rawString>Daniel J. Brooks, Constantine Lignos, Cameron Finucane, Mikhail S. Medvedev, Ian Perera, Vasumathi Raman, Hadas Kress-Gazit, Mitch Marcus, and Holly A. Yanco. 2012. Make it so: Continuous, flexible natural language interaction with an autonomous robot. In Proc. AAAI, pages 2–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen M Eberhard</author>
<author>Hannele Nicholson</author>
<author>Sandra K¨ubler</author>
<author>Susan Gundersen</author>
<author>Matthias Scheutz</author>
</authors>
<title>The indiana ”cooperative remote search task” (crest) corpus.</title>
<date>2010</date>
<booktitle>In Proc. LREC.</booktitle>
<marker>Eberhard, Nicholson, K¨ubler, Gundersen, Scheutz, 2010</marker>
<rawString>Kathleen M. Eberhard, Hannele Nicholson, Sandra K¨ubler, Susan Gundersen, and Matthias Scheutz. 2010. The indiana ”cooperative remote search task” (crest) corpus. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Fang</author>
<author>Changsong Liu</author>
<author>Lanbo She</author>
<author>Joyce Y Chai</author>
</authors>
<title>Towards situated dialogue: Revisiting referring expression generation.</title>
<date>2013</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>392--402</pages>
<contexts>
<context position="14607" citStr="Fang et al., 2013" startWordPosition="2354" endWordPosition="2357">i et al. (2013) build multi-level maps but with a situated commander. Eberhard et al. (2010)’s CReST corpus contains a set-up similar to ours minus the robot; a human task-solver wears a forward-facing camera instead. The SCARE corpus (Stoia et al., 2008) records similar modalities but in a virtual environment, where C has full access to R’s video feed. Other projects yielded corpora from virtual environments that include route descriptions without dialog (Marge and Rudnicky, 2011; MacMahon et al., 2006; Vogel and Jurafsky, 2010) or referring expressions without routes (Sch¨utte et al., 2010; Fang et al., 2013), assuming pre-existing abstractions from sensor data. 6 Conclusion and Ongoing Work We have presented our pre-pilot study with data collection and corpus construction phases. This work-in-progress requires further analysis. We are now processing dialog utterances for more systematic semantic interpretation using disambiguated VerbNet frames that map into ResearchCyc predicates. We will run object recognition software retrained on a broader range of objects so that it can be applied to images that will be labelled and stored in ResearchCyc micro-worlds for subsequent co-reference with terms in</context>
</contexts>
<marker>Fang, Liu, She, Chai, 2013</marker>
<rawString>Rui Fang, Changsong Liu, Lanbo She, and Joyce Y. Chai. 2013. Towards situated dialogue: Revisiting referring expression generation. In Proc. EMNLP, pages 392–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Green</author>
</authors>
<title>Helge Httenrauch, and Kerstin Severinson Eklundh.</title>
<date>2006</date>
<booktitle>Proc. LREC.</booktitle>
<marker>Green, 2006</marker>
<rawString>Anders Green, Helge Httenrauch, and Kerstin Severinson Eklundh. 2006. Developing a contextualized multimodal corpus for human-robot interaction. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan F Maas</author>
<author>Britta Wrede</author>
<author>Gerhard Sagerer</author>
</authors>
<title>Towards a multimodal topic tracking system for a mobile robot. In</title>
<date>2006</date>
<booktitle>Proc. INTERSPEECH.</booktitle>
<contexts>
<context position="13818" citStr="Maas et al., 2006" startWordPosition="2227" endWordPosition="2230">to CycL, allowing a commander to, for example, ask questions about objects and regions using terms related to but not necessarily equal to the original recognition system-provided labels. 5 Related Work We are aware of no other multi-modal corpora obtained from human-robot teams conducting exploratory missions with collected dialog, video and other sensor data. Corpora with a robot Figure 5: Inside View: Image, Zones, Overlay. Brightness and contrast of video image and overlay increased for print publication. recording similar data modalities do exist (Green et al., 2006; Wienke et al., 2012; Maas et al., 2006) but for fundamentally different tasks. Tellex et al. (2011) and Matuszek et al. (2012) pair commands with formal plans without dialog and Zender et al. (2008) and Randelli et al. (2013) build multi-level maps but with a situated commander. Eberhard et al. (2010)’s CReST corpus contains a set-up similar to ours minus the robot; a human task-solver wears a forward-facing camera instead. The SCARE corpus (Stoia et al., 2008) records similar modalities but in a virtual environment, where C has full access to R’s video feed. Other projects yielded corpora from virtual environments that include rou</context>
</contexts>
<marker>Maas, Wrede, Sagerer, 2006</marker>
<rawString>Jan F. Maas, Britta Wrede, and Gerhard Sagerer. 2006. Towards a multimodal topic tracking system for a mobile robot. In Proc. INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt MacMahon</author>
<author>Brian Stankiewicz</author>
<author>Benjamin Kuipers</author>
</authors>
<title>Walk the talk: Connecting language, knowledge, and action in route instructions.</title>
<date>2006</date>
<booktitle>In Proc. AAAI,</booktitle>
<pages>1475--1482</pages>
<contexts>
<context position="14497" citStr="MacMahon et al., 2006" startWordPosition="2337" endWordPosition="2340">1) and Matuszek et al. (2012) pair commands with formal plans without dialog and Zender et al. (2008) and Randelli et al. (2013) build multi-level maps but with a situated commander. Eberhard et al. (2010)’s CReST corpus contains a set-up similar to ours minus the robot; a human task-solver wears a forward-facing camera instead. The SCARE corpus (Stoia et al., 2008) records similar modalities but in a virtual environment, where C has full access to R’s video feed. Other projects yielded corpora from virtual environments that include route descriptions without dialog (Marge and Rudnicky, 2011; MacMahon et al., 2006; Vogel and Jurafsky, 2010) or referring expressions without routes (Sch¨utte et al., 2010; Fang et al., 2013), assuming pre-existing abstractions from sensor data. 6 Conclusion and Ongoing Work We have presented our pre-pilot study with data collection and corpus construction phases. This work-in-progress requires further analysis. We are now processing dialog utterances for more systematic semantic interpretation using disambiguated VerbNet frames that map into ResearchCyc predicates. We will run object recognition software retrained on a broader range of objects so that it can be applied to</context>
</contexts>
<marker>MacMahon, Stankiewicz, Kuipers, 2006</marker>
<rawString>Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers. 2006. Walk the talk: Connecting language, knowledge, and action in route instructions. In Proc. AAAI, pages 1475–1482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Marge</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>The teamtalk corpus: Route instructions in open spaces.</title>
<date>2011</date>
<booktitle>In Proc. RSS, Workshop on Grounding Human-Robot Dialog for Spatial Tasks.</booktitle>
<contexts>
<context position="14474" citStr="Marge and Rudnicky, 2011" startWordPosition="2333" endWordPosition="2336"> tasks. Tellex et al. (2011) and Matuszek et al. (2012) pair commands with formal plans without dialog and Zender et al. (2008) and Randelli et al. (2013) build multi-level maps but with a situated commander. Eberhard et al. (2010)’s CReST corpus contains a set-up similar to ours minus the robot; a human task-solver wears a forward-facing camera instead. The SCARE corpus (Stoia et al., 2008) records similar modalities but in a virtual environment, where C has full access to R’s video feed. Other projects yielded corpora from virtual environments that include route descriptions without dialog (Marge and Rudnicky, 2011; MacMahon et al., 2006; Vogel and Jurafsky, 2010) or referring expressions without routes (Sch¨utte et al., 2010; Fang et al., 2013), assuming pre-existing abstractions from sensor data. 6 Conclusion and Ongoing Work We have presented our pre-pilot study with data collection and corpus construction phases. This work-in-progress requires further analysis. We are now processing dialog utterances for more systematic semantic interpretation using disambiguated VerbNet frames that map into ResearchCyc predicates. We will run object recognition software retrained on a broader range of objects so th</context>
</contexts>
<marker>Marge, Rudnicky, 2011</marker>
<rawString>Matthew Marge and Alexander I Rudnicky. 2011. The teamtalk corpus: Route instructions in open spaces. In Proc. RSS, Workshop on Grounding Human-Robot Dialog for Spatial Tasks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Evan Herbst</author>
<author>Luke S Zettlemoyer</author>
<author>Dieter Fox</author>
</authors>
<title>Learning to parse natural language commands to a robot control system.</title>
<date>2012</date>
<booktitle>In Proc. ISER,</booktitle>
<pages>403--415</pages>
<contexts>
<context position="13905" citStr="Matuszek et al. (2012)" startWordPosition="2241" endWordPosition="2244">ons using terms related to but not necessarily equal to the original recognition system-provided labels. 5 Related Work We are aware of no other multi-modal corpora obtained from human-robot teams conducting exploratory missions with collected dialog, video and other sensor data. Corpora with a robot Figure 5: Inside View: Image, Zones, Overlay. Brightness and contrast of video image and overlay increased for print publication. recording similar data modalities do exist (Green et al., 2006; Wienke et al., 2012; Maas et al., 2006) but for fundamentally different tasks. Tellex et al. (2011) and Matuszek et al. (2012) pair commands with formal plans without dialog and Zender et al. (2008) and Randelli et al. (2013) build multi-level maps but with a situated commander. Eberhard et al. (2010)’s CReST corpus contains a set-up similar to ours minus the robot; a human task-solver wears a forward-facing camera instead. The SCARE corpus (Stoia et al., 2008) records similar modalities but in a virtual environment, where C has full access to R’s video feed. Other projects yielded corpora from virtual environments that include route descriptions without dialog (Marge and Rudnicky, 2011; MacMahon et al., 2006; Vogel </context>
</contexts>
<marker>Matuszek, Herbst, Zettlemoyer, Fox, 2012</marker>
<rawString>Cynthia Matuszek, Evan Herbst, Luke S. Zettlemoyer, and Dieter Fox. 2012. Learning to parse natural language commands to a robot control system. In Proc. ISER, pages 403–415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clifton McFate</author>
</authors>
<title>Expanding verb coverage in cyc with verbnet.</title>
<date>2010</date>
<booktitle>In Proc. ACL, Student Research Workshop,</booktitle>
<pages>61--66</pages>
<contexts>
<context position="11375" citStr="McFate (2010)" startWordPosition="1849" endWordPosition="1850">matches the syntactic parse. Overall, half the commands run through SLURP generated a semantic interpretation. Of the other half, roughly one quarter failed or had errors at parsing and the other quarter at the argument matching stage. Figure 3: Analyses of Scan next open room on left. Our next step is to augment SLURP’s lexicon and retrain a parser for new vocabulary so that we can directly map semantic structures of the prepilot corpora into ResearchCyc8, an extensive ontology, for cross-reference to other events and objects, already stored and possibly originated as visual input. Following McFate (2010), we will test 6https://github.com/PennNLP/SLURP. 7Verbnet associates each frame with a conjunction of boolean semantic predicates that specify how and when event participants interact, for an event variable (not shown). 8ResearchCyc and CycL are trademarks of Cycorp, Inc. 45 Figure 4: Outside View: Image, Zones, Overlay the mapping of matched VerbNet frames to ResearchCyc’s semantic predicates to assess its lexical coverage for our corpora. 4 Image Processing Interval corpus images were labelled by a neural network trained for visual scene classification (Munoz, 2013) of nine material classes</context>
</contexts>
<marker>McFate, 2010</marker>
<rawString>Clifton McFate. 2010. Expanding verb coverage in cyc with verbnet. In Proc. ACL, Student Research Workshop, pages 61–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Munoz</author>
</authors>
<title>Inference Machines: Parsing Scenes via Iterated Predictions.</title>
<date>2013</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="11950" citStr="Munoz, 2013" startWordPosition="1935" endWordPosition="1936">visual input. Following McFate (2010), we will test 6https://github.com/PennNLP/SLURP. 7Verbnet associates each frame with a conjunction of boolean semantic predicates that specify how and when event participants interact, for an event variable (not shown). 8ResearchCyc and CycL are trademarks of Cycorp, Inc. 45 Figure 4: Outside View: Image, Zones, Overlay the mapping of matched VerbNet frames to ResearchCyc’s semantic predicates to assess its lexical coverage for our corpora. 4 Image Processing Interval corpus images were labelled by a neural network trained for visual scene classification (Munoz, 2013) of nine material classes: dirt, foliage, grass, road, sidewalk, sky, wall, wood, and ground cover (organic debris). Figures 4 and 5 show the images from Figures 1 and 2 with two additional versions: one with colored zones for system-recognized class boundaries and another with colored zones as trasparent overlays on the original. The classes differentiate terrain types that work well with route-finding techniques that leverage them in selecting traversible paths. As the robot systems are enhanced with more sophisticated path planning software, that knowledge may be combined with recognized zo</context>
</contexts>
<marker>Munoz, 2013</marker>
<rawString>Daniel Munoz. 2013. Inference Machines: Parsing Scenes via Iterated Predictions. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morgan Quigley</author>
<author>Ken Conley</author>
<author>Brian Gerkey</author>
<author>Josh Faust</author>
<author>Tully B Foote</author>
<author>Jeremy Leibs</author>
<author>Rob Wheeler</author>
<author>Andrew Y Ng</author>
</authors>
<title>ROS: an open-source robot operating system.</title>
<date>2009</date>
<booktitle>In Proc. ICRA, Workshop on Open Source Software.</booktitle>
<contexts>
<context position="5571" citStr="Quigley et al., 2009" startWordPosition="906" endWordPosition="909">ey C. R was to consider the robot’s actions as R’s own, and to consider available video and LIDAR point cloud feeds as R’s own perceptions. 1.2 Equipment All participants worked from their own computers. Each was instructed, for a given scenario, to be either C or R and to communicate by text only. On their screen they saw a dedicated dialog (chat) window in a Linux terminal. For sessions with both C and R, the same dialog content (the ongoing sequence of typed-in utterances) appeared in the dialog window on each of their screens. The physical robot ran under the Robot Operating System (ROS) (Quigley et al., 2009), equipped with a video camera, laser sensors, magnetometer, GPS unit, and rotary encoders. R could “see for the robot” via two ROS rviz windows with live feeds for video from the robot’s camera and constructed 3D point cloud frames.2 R had access to rotate and zoom functions to alter the screen display of the point cloud. C saw only a static bird’seye-view map of the area. R remotely controlled over a network connection the robot’s four wheels and its motion, using the left joystick of an X-Box controller. 1.3 Collection During each session, all data from the robot’s sensors and dialog window</context>
</contexts>
<marker>Quigley, Conley, Gerkey, Faust, Foote, Leibs, Wheeler, Ng, 2009</marker>
<rawString>Morgan Quigley, Ken Conley, Brian Gerkey, Josh Faust, Tully B. Foote, Jeremy Leibs, Rob Wheeler, and Andrew Y. Ng. 2009. ROS: an open-source robot operating system. In Proc. ICRA, Workshop on Open Source Software.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriele Randelli</author>
<author>Taigo Maria Bonanni</author>
<author>Luca Iocchi</author>
<author>Daniele Nardi</author>
</authors>
<title>Knowledge acquisition through human–robot multimodal interaction.</title>
<date>2013</date>
<journal>Intelligent Service Robotics,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="14004" citStr="Randelli et al. (2013)" startWordPosition="2258" endWordPosition="2261">bels. 5 Related Work We are aware of no other multi-modal corpora obtained from human-robot teams conducting exploratory missions with collected dialog, video and other sensor data. Corpora with a robot Figure 5: Inside View: Image, Zones, Overlay. Brightness and contrast of video image and overlay increased for print publication. recording similar data modalities do exist (Green et al., 2006; Wienke et al., 2012; Maas et al., 2006) but for fundamentally different tasks. Tellex et al. (2011) and Matuszek et al. (2012) pair commands with formal plans without dialog and Zender et al. (2008) and Randelli et al. (2013) build multi-level maps but with a situated commander. Eberhard et al. (2010)’s CReST corpus contains a set-up similar to ours minus the robot; a human task-solver wears a forward-facing camera instead. The SCARE corpus (Stoia et al., 2008) records similar modalities but in a virtual environment, where C has full access to R’s video feed. Other projects yielded corpora from virtual environments that include route descriptions without dialog (Marge and Rudnicky, 2011; MacMahon et al., 2006; Vogel and Jurafsky, 2010) or referring expressions without routes (Sch¨utte et al., 2010; Fang et al., 20</context>
</contexts>
<marker>Randelli, Bonanni, Iocchi, Nardi, 2013</marker>
<rawString>Gabriele Randelli, Taigo Maria Bonanni, Luca Iocchi, and Daniele Nardi. 2013. Knowledge acquisition through human–robot multimodal interaction. Intelligent Service Robotics, 6(1):19–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurel D Riek</author>
</authors>
<title>Wizard of oz studies in hri: A systematic review and new reporting guidelines.</title>
<date>2012</date>
<journal>Journal of Human-Robot Interaction,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="2557" citStr="Riek, 2012" startWordPosition="412" endWordPosition="413">e commander (C) or the human (R) controlling a mobile physical robot in such tasks. Neither could see the robot. Only R could “see for” the robot, via its onboard video camera and LIDAR. C and R communicated by text chat on their computers, as in this example, R 41: I can see in the entrance. C 42: Enter and scan the first room. R 44: I see a door to the right and a door to the left. C 45: Scan next open room on left. Utterances R 41 &amp; C 42 occur when the robot is outdoors (Fig. 1) and R 44 &amp; C 45 occur after it moves indoors (Fig. 2). Although our approach resembles a Wizard and Oz paradigm (Riek, 2012), 1Statisticians say pre-pilots are for “kicking the tires,” early-stage tests of scenarios, equipment, and data collection. Figure 2: Inside View: Video Image &amp; LIDAR. Brightness and contrast of video image increased for print publication. 43 Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 43–47, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics with C as User and R as Wizard controlling the robot, there is no intent for R to deceive C. In these dialog snippets, notice that the doors mentioned in R 44 are not visible in the i</context>
</contexts>
<marker>Riek, 2012</marker>
<rawString>Laurel D Riek. 2012. Wizard of oz studies in hri: A systematic review and new reporting guidelines. Journal of Human-Robot Interaction, 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper Schuler</author>
</authors>
<title>Verbnet: A Broadcoverage, Comprehensive Verb Lexicon.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="10468" citStr="Schuler, 2005" startWordPosition="1703" endWordPosition="1704">rs likely due to the paced descriptions in R’s utterances. Another pre-pilot is needed to test this hypothesis. 3 Corpora Language Processing Each utterance collected from the sessions was tokenized, parsed, and semantically interpreted using SLURP (Brooks et al., 2012), a welltested NLP front-end component of a human-robot system.6 The progression in SLURP’s analysis pipeline for utterance C 45 is shown in Figure 3. SLURP extracts a parse tree (top-left), identifies a sub-tree that constitutes a verb-argument structure, and enumerates possibly matching sensespecific verb frames from VerbNet (Schuler, 2005) (bottom-left). VerbNet provides a syntactic to semantic role mapping for each frame (top-right). SLURP selects the best mapping and generates a compact semantic representation (bottom-right).7 In this example, the correct sense of “scan” is selected (investigate-35.4) along with a frame that matches the syntactic parse. Overall, half the commands run through SLURP generated a semantic interpretation. Of the other half, roughly one quarter failed or had errors at parsing and the other quarter at the argument matching stage. Figure 3: Analyses of Scan next open room on left. Our next step is to</context>
</contexts>
<marker>Schuler, 2005</marker>
<rawString>Karin Kipper Schuler. 2005. Verbnet: A Broadcoverage, Comprehensive Verb Lexicon. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niels Sch¨utte</author>
<author>John D Kelleher</author>
<author>Brian Mac Namee</author>
</authors>
<title>Visual salience and reference resolution in situated dialogues: A corpus-based evaluation.</title>
<date>2010</date>
<booktitle>In Proc. AAAI, Fall Symposium: Dialog with Robots.</booktitle>
<marker>Sch¨utte, Kelleher, Namee, 2010</marker>
<rawString>Niels Sch¨utte, John D. Kelleher, and Brian Mac Namee. 2010. Visual salience and reference resolution in situated dialogues: A corpus-based evaluation. In Proc. AAAI, Fall Symposium: Dialog with Robots.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Stoia</author>
<author>Darla Magdalena Shockley</author>
<author>Donna K Byron</author>
<author>Eric Fosler-Lussier</author>
</authors>
<title>Scare: a situated corpus with annotated referring expressions.</title>
<date>2008</date>
<booktitle>In Proc. LREC.</booktitle>
<contexts>
<context position="14244" citStr="Stoia et al., 2008" startWordPosition="2297" endWordPosition="2300">rlay. Brightness and contrast of video image and overlay increased for print publication. recording similar data modalities do exist (Green et al., 2006; Wienke et al., 2012; Maas et al., 2006) but for fundamentally different tasks. Tellex et al. (2011) and Matuszek et al. (2012) pair commands with formal plans without dialog and Zender et al. (2008) and Randelli et al. (2013) build multi-level maps but with a situated commander. Eberhard et al. (2010)’s CReST corpus contains a set-up similar to ours minus the robot; a human task-solver wears a forward-facing camera instead. The SCARE corpus (Stoia et al., 2008) records similar modalities but in a virtual environment, where C has full access to R’s video feed. Other projects yielded corpora from virtual environments that include route descriptions without dialog (Marge and Rudnicky, 2011; MacMahon et al., 2006; Vogel and Jurafsky, 2010) or referring expressions without routes (Sch¨utte et al., 2010; Fang et al., 2013), assuming pre-existing abstractions from sensor data. 6 Conclusion and Ongoing Work We have presented our pre-pilot study with data collection and corpus construction phases. This work-in-progress requires further analysis. We are now p</context>
</contexts>
<marker>Stoia, Shockley, Byron, Fosler-Lussier, 2008</marker>
<rawString>Laura Stoia, Darla Magdalena Shockley, Donna K. Byron, and Eric Fosler-Lussier. 2008. Scare: a situated corpus with annotated referring expressions. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Tellex</author>
<author>Thomas Kollar</author>
<author>Steven Dickerson</author>
<author>Matthew R Walter</author>
<author>Ashis Gopal Banerjee</author>
<author>Seth J Teller</author>
<author>Nicholas Roy</author>
</authors>
<title>Understanding natural language commands for robotic navigation and mobile manipulation. In</title>
<date>2011</date>
<booktitle>Proc. AAAI.</booktitle>
<contexts>
<context position="13878" citStr="Tellex et al. (2011)" startWordPosition="2236" endWordPosition="2239">ns about objects and regions using terms related to but not necessarily equal to the original recognition system-provided labels. 5 Related Work We are aware of no other multi-modal corpora obtained from human-robot teams conducting exploratory missions with collected dialog, video and other sensor data. Corpora with a robot Figure 5: Inside View: Image, Zones, Overlay. Brightness and contrast of video image and overlay increased for print publication. recording similar data modalities do exist (Green et al., 2006; Wienke et al., 2012; Maas et al., 2006) but for fundamentally different tasks. Tellex et al. (2011) and Matuszek et al. (2012) pair commands with formal plans without dialog and Zender et al. (2008) and Randelli et al. (2013) build multi-level maps but with a situated commander. Eberhard et al. (2010)’s CReST corpus contains a set-up similar to ours minus the robot; a human task-solver wears a forward-facing camera instead. The SCARE corpus (Stoia et al., 2008) records similar modalities but in a virtual environment, where C has full access to R’s video feed. Other projects yielded corpora from virtual environments that include route descriptions without dialog (Marge and Rudnicky, 2011; Ma</context>
</contexts>
<marker>Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, Roy, 2011</marker>
<rawString>Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R. Walter, Ashis Gopal Banerjee, Seth J. Teller, and Nicholas Roy. 2011. Understanding natural language commands for robotic navigation and mobile manipulation. In Proc. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Vogel</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Learning to follow navigational directions.</title>
<date>2010</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>806--814</pages>
<contexts>
<context position="14524" citStr="Vogel and Jurafsky, 2010" startWordPosition="2341" endWordPosition="2344">(2012) pair commands with formal plans without dialog and Zender et al. (2008) and Randelli et al. (2013) build multi-level maps but with a situated commander. Eberhard et al. (2010)’s CReST corpus contains a set-up similar to ours minus the robot; a human task-solver wears a forward-facing camera instead. The SCARE corpus (Stoia et al., 2008) records similar modalities but in a virtual environment, where C has full access to R’s video feed. Other projects yielded corpora from virtual environments that include route descriptions without dialog (Marge and Rudnicky, 2011; MacMahon et al., 2006; Vogel and Jurafsky, 2010) or referring expressions without routes (Sch¨utte et al., 2010; Fang et al., 2013), assuming pre-existing abstractions from sensor data. 6 Conclusion and Ongoing Work We have presented our pre-pilot study with data collection and corpus construction phases. This work-in-progress requires further analysis. We are now processing dialog utterances for more systematic semantic interpretation using disambiguated VerbNet frames that map into ResearchCyc predicates. We will run object recognition software retrained on a broader range of objects so that it can be applied to images that will be labell</context>
</contexts>
<marker>Vogel, Jurafsky, 2010</marker>
<rawString>Adam Vogel and Daniel Jurafsky. 2010. Learning to follow navigational directions. In Proc. ACL, pages 806–814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Wienke</author>
<author>David Klotz</author>
<author>Sebastian Wrede</author>
</authors>
<title>A framework for the acquisition of multimodal human-robot interaction data sets with a whole-system perspective.</title>
<date>2012</date>
<booktitle>In Proc. LREC, Workshop on Multimodal Corpora for Machine Learning.</booktitle>
<contexts>
<context position="13798" citStr="Wienke et al., 2012" startWordPosition="2223" endWordPosition="2226">NL query converted into CycL, allowing a commander to, for example, ask questions about objects and regions using terms related to but not necessarily equal to the original recognition system-provided labels. 5 Related Work We are aware of no other multi-modal corpora obtained from human-robot teams conducting exploratory missions with collected dialog, video and other sensor data. Corpora with a robot Figure 5: Inside View: Image, Zones, Overlay. Brightness and contrast of video image and overlay increased for print publication. recording similar data modalities do exist (Green et al., 2006; Wienke et al., 2012; Maas et al., 2006) but for fundamentally different tasks. Tellex et al. (2011) and Matuszek et al. (2012) pair commands with formal plans without dialog and Zender et al. (2008) and Randelli et al. (2013) build multi-level maps but with a situated commander. Eberhard et al. (2010)’s CReST corpus contains a set-up similar to ours minus the robot; a human task-solver wears a forward-facing camera instead. The SCARE corpus (Stoia et al., 2008) records similar modalities but in a virtual environment, where C has full access to R’s video feed. Other projects yielded corpora from virtual environme</context>
</contexts>
<marker>Wienke, Klotz, Wrede, 2012</marker>
<rawString>Johannes Wienke, David Klotz, and Sebastian Wrede. 2012. A framework for the acquisition of multimodal human-robot interaction data sets with a whole-system perspective. In Proc. LREC, Workshop on Multimodal Corpora for Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hendrik Zender</author>
<author>O Martinez Mozos</author>
<author>Patric Jensfelt</author>
<author>GJM Kruijff</author>
<author>Wolfram Burgard</author>
</authors>
<title>Conceptual spatial representations for indoor mobile robots. Robotics and Autonomous Systems,</title>
<date>2008</date>
<pages>56--6</pages>
<contexts>
<context position="13977" citStr="Zender et al. (2008)" startWordPosition="2253" endWordPosition="2256">nition system-provided labels. 5 Related Work We are aware of no other multi-modal corpora obtained from human-robot teams conducting exploratory missions with collected dialog, video and other sensor data. Corpora with a robot Figure 5: Inside View: Image, Zones, Overlay. Brightness and contrast of video image and overlay increased for print publication. recording similar data modalities do exist (Green et al., 2006; Wienke et al., 2012; Maas et al., 2006) but for fundamentally different tasks. Tellex et al. (2011) and Matuszek et al. (2012) pair commands with formal plans without dialog and Zender et al. (2008) and Randelli et al. (2013) build multi-level maps but with a situated commander. Eberhard et al. (2010)’s CReST corpus contains a set-up similar to ours minus the robot; a human task-solver wears a forward-facing camera instead. The SCARE corpus (Stoia et al., 2008) records similar modalities but in a virtual environment, where C has full access to R’s video feed. Other projects yielded corpora from virtual environments that include route descriptions without dialog (Marge and Rudnicky, 2011; MacMahon et al., 2006; Vogel and Jurafsky, 2010) or referring expressions without routes (Sch¨utte et</context>
</contexts>
<marker>Zender, Mozos, Jensfelt, Kruijff, Burgard, 2008</marker>
<rawString>Hendrik Zender, O Martinez Mozos, Patric Jensfelt, GJM Kruijff, and Wolfram Burgard. 2008. Conceptual spatial representations for indoor mobile robots. Robotics and Autonomous Systems, 56(6):493–502.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
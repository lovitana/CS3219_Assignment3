<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.995178">
Linguistic Regularities in Sparse and Explicit Word Representations
</title>
<author confidence="0.972811">
Omer Levy* and Yoav Goldberg
</author>
<affiliation confidence="0.9731625">
Computer Science Department
Bar-Ilan University
</affiliation>
<address confidence="0.774318">
Ramat-Gan, Israel
</address>
<email confidence="0.997929">
{omerlevy,yoav.goldberg}@gmail.com
</email>
<sectionHeader confidence="0.993876" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999882">
Recent work has shown that neural-
embedded word representations capture
many relational similarities, which can be
recovered by means of vector arithmetic
in the embedded space. We show that
Mikolov et al.’s method of first adding
and subtracting word vectors, and then
searching for a word similar to the re-
sult, is equivalent to searching for a word
that maximizes a linear combination of
three pairwise word similarities. Based on
this observation, we suggest an improved
method of recovering relational similar-
ities, improving the state-of-the-art re-
sults on two recent word-analogy datasets.
Moreover, we demonstrate that analogy
recovery is not restricted to neural word
embeddings, and that a similar amount
of relational similarities can be recovered
from traditional distributional word repre-
sentations.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997198142857143">
Deep learning methods for language processing
owe much of their success to neural network lan-
guage models, in which words are represented as
dense real-valued vectors in Rd. Such representa-
tions are referred to as distributed word represen-
tations or word embeddings, as they embed an en-
tire vocabulary into a relatively low-dimensional
linear space, whose dimensions are latent contin-
uous features. The embedded word vectors are
trained over large collections of text using vari-
ants of neural networks (Bengio et al., 2003; Col-
lobert and Weston, 2008; Mnih and Hinton, 2008;
Mikolov et al., 2011; Mikolov et al., 2013b). The
∗ Supported by the European Community’s Seventh
Framework Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
word embeddings are designed to capture what
Turney (2006) calls attributional similarities be-
tween vocabulary items: words that appear in sim-
ilar contexts will be close to each other in the
projected space. The effect is grouping of words
that share semantic (“dog cat cow”, “eat devour”)
or syntactic (“cars hats days”, “emptied carried
danced”) properties, and are shown to be effective
as features for various NLP tasks (Turian et al.,
2010; Collobert et al., 2011; Socher et al., 2011;
Al-Rfou et al., 2013). We refer to such word rep-
resentations as neural embeddings or just embed-
dings.
Recently, Mikolov et al. (2013c) demonstrated
that the embeddings created by a recursive neu-
ral network (RNN) encode not only attributional
similarities between words, but also similarities
between pairs of words. Such similarities are
referred to as linguistic regularities by Mikolov
et al. and as relational similarities by Turney
(2006). They capture, for example, the gen-
der relation exhibited by the pairs “man:woman”,
“king:queen”, the language-spoken-in relation in
“france:french”, “mexico:spanish” and the past-
tense relation in “capture:captured”, “go:went”.
Remarkably, Mikolov et al. showed that such rela-
tions are reflected in vector offsets between word
pairs (apples − apple ≈ cars − car), and
that by using simple vector arithmetic one could
apply the relation and solve analogy questions of
the form “a is to a* as b is to —” in which the
nature of the relation is hidden. Perhaps the most
famous example is that the embedded representa-
tion of the word queen can be roughly recovered
from the representations of king, man and woman:
queen ≈ king − man + woman
The recovery of relational similarities using vector
arithmetic on RNN-embedded vectors was evalu-
ated on many relations, achieving state-of-the-art
results in relational similarity identification tasks
</bodyText>
<page confidence="0.976977">
171
</page>
<note confidence="0.694756">
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 171–180,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.998794960784314">
(Mikolov et al., 2013c; Zhila et al., 2013). It was
later demonstrated that relational similarities can
be recovered in a similar fashion also from embed-
dings trained with different architectures (Mikolov
et al., 2013a; Mikolov et al., 2013b).
This fascinating result raises a question: to what
extent are the relational semantic properties a re-
sult of the embedding process? Experiments in
(Mikolov et al., 2013c) show that the RNN-based
embeddings are superior to other dense represen-
tations, but how crucial is it for a representation to
be dense and low-dimensional at all?
An alternative approach to representing words
as vectors is the distributional similarity repre-
sentation, or bag of contexts. In this representa-
tion, each word is associated with a very high-
dimensional but sparse vector capturing the con-
texts in which the word occurs. We call such vec-
tor representations explicit, as each dimension di-
rectly corresponds to a particular context. These
explicit vector-space representations have been
extensively studied in the NLP literature (see (Tur-
ney and Pantel, 2010; Baroni and Lenci, 2010) and
the references therein), and are known to exhibit
a large extent of attributional similarity (Pereira
et al., 1993; Lin, 1998; Lin and Pantel, 2001;
Sahlgren, 2006; Kotlerman et al., 2010).
In this study, we show that similarly to the
neural embedding space, the explicit vector space
also encodes a vast amount of relational similar-
ity which can be recovered in a similar fashion,
suggesting the explicit vector space representation
as a competitive baseline for further work on neu-
ral embeddings. Moreover, this result implies that
the neural embedding process is not discovering
novel patterns, but rather is doing a remarkable
job at preserving the patterns inherent in the word-
context co-occurrence matrix.
A key insight of this work is that the vector
arithmetic method can be decomposed into a linear
combination of three pairwise similarities (Section
3). While mathematically equivalent, we find that
thinking about the method in terms of the decom-
posed formulation is much less puzzling, and pro-
vides a better intuition on why we would expect
the method to perform well on the analogy re-
covery task. Furthermore, the decomposed form
leads us to suggest a modified optimization objec-
tive (Section 6), which outperforms the state-of-
the-art at recovering relational similarities under
both representations.
</bodyText>
<sectionHeader confidence="0.92797" genericHeader="introduction">
2 Explicit Vector Space Representation
</sectionHeader>
<bodyText confidence="0.9933455">
We adopt the traditional word representation used
in the distributional similarity literature (Turney
and Pantel, 2010). Each word is associated with
a sparse vector capturing the contexts in which it
occurs. We call this representation explicit, as each
dimension corresponds to a particular context.
For a vocabulary V and a set of contexts C,
the result is a |V |x|C |sparse matrix S in which
Sij corresponds to the strength of the association
between word i and context j. The association
strength between a word w E V and a context
c E C can take many forms. We chose to use
the popular positive pointwise mutual information
(PPMI) metric:
</bodyText>
<equation confidence="0.99933925">
Sij = PPMI(wi, cj)
�
0 PMI(w, c) &lt; 0
PPMI(w, c) =
PMI(w, c) otherwise
PMI(w, c) = log P (w,c)
P (w)P (c) = log freq(w,c)|corpus|
freq(w)freq(c)
</equation>
<bodyText confidence="0.999869033333333">
where |corpus |is the number of items in the cor-
pus, freq(w, c) is the number of times word w
appeared in context c in the corpus, and freq(w),
freq(c) are the corpus frequencies of the word
and the context respectively.
The use of PMI in distributional similarity mod-
els was introduced by Church and Hanks (1990)
and widely adopted (Dagan et al., 1994; Turney,
2001). The PPMI variant dates back to at least
(Niwa and Nitta, 1994), and was demonstrated to
perform very well in Bullinaria and Levy (2007).
In this work, we take the linear contexts in
which words appear. We consider each word sur-
rounding the target word w in a window of 2 to
each side as a context, distinguishing between dif-
ferent sequential positions. For example, in the
sentence a b c d e the contexts of the word c
are a−2 , b−1, d+1 and e+2. Each vector’s dimen-
stion is thus |C |Pt� 4 |V |. Empirically, the num-
ber of non-zero dimensions for vocabulary items
in our corpus ranges between 3 (for some rare to-
kens) and 474,234 (for the word “and”), with a
mean of 1595 and a median of 415.
Another popular choice of context is the syntac-
tic relations the word participates in (Lin, 1998;
Pad´o and Lapata, 2007; Levy and Goldberg,
2014). In this paper, we chose the sequential
context as it is compatible with the information
available to the state-of-the-art neural embedding
method we are comparing against.
</bodyText>
<page confidence="0.998544">
172
</page>
<sectionHeader confidence="0.976281" genericHeader="method">
3 Analogies and Vector Arithmetic
</sectionHeader>
<bodyText confidence="0.99948">
Mikolov et al. demonstrated that vector space rep-
resentations encode various relational similarities,
which can be recovered using vector arithmetic
and used to solve word-analogy tasks.
</bodyText>
<subsectionHeader confidence="0.999638">
3.1 Analogy Questions
</subsectionHeader>
<bodyText confidence="0.9999935">
In a word-analogy task we are given two pairs of
words that share a relation (e.g. “man:woman”,
“king:queen”). The identity of the fourth word
(“queen”) is hidden, and we need to infer it based
on the other three (e.g. answering the question:
“man is to woman as king is to — ?”). In the rest
of this paper, we will refer to the four words as
a:a∗, b:b∗. Note that the type of the relation is
not explicitly provided in the question, and solv-
ing the question correctly (by a human) involves
first inferring the relation, and then applying it to
the third word (b).
</bodyText>
<subsectionHeader confidence="0.996422">
3.2 Vector Arithmetic
</subsectionHeader>
<bodyText confidence="0.950841307692308">
Mikolov et al. showed that relations between
words are reflected to a large extent in the
offsets between their vector embeddings
(queen − king Pz� woman − man),
and thus the vector of the hidden word b∗ will be
similar to the vector b − a + a∗, suggesting that
the analogy question can be solved by optimizing:
arg max (sim (b∗, b − a + a∗))
b*∈V
where V is the vocabulary excluding the question
words b, a and a∗, and sim is a similarity mea-
sure. Specifically, they used the cosine similarity
measure, defined as:
</bodyText>
<equation confidence="0.754698333333333">
resulting in:
arg max (cos (b∗, b − a + a∗)) (1)
b*∈V
</equation>
<bodyText confidence="0.999809444444444">
Since cosine is inverse to the angle, high cosine
similarity (close to 1) means that the vectors share
a very similar direction. Note that this metric nor-
malizes (and thus ignores) the vectors’ lengths,
unlike the Euclidean distance between them. For
reasons that will be clear later, we refer to (1) as
the 3COSADD method.
An alternative to 3COSADD is to require that
the direction of transformation be conserved:
</bodyText>
<equation confidence="0.384091">
arg max (cos (b∗ − b, a∗ − a)) (2)
b*∈V
</equation>
<bodyText confidence="0.998289">
This basically means that b∗ − b shares the same
direction with a∗ − a, ignoring the distances. We
refer to this method as PAIRDIRECTION. Though
it was not mentioned in the paper, Mikolov
et al. (2013c) used PAIRDIRECTION for solving
the semantic analogies of the SemEval task, and
3COSADD for solving the syntactic analogies.1
</bodyText>
<subsectionHeader confidence="0.999813">
3.3 Reinterpreting Vector Arithmetic
</subsectionHeader>
<bodyText confidence="0.9984465">
In Mikolov et al.’s experiments, all word-vectors
were normalized to unit length. Under such nor-
malization, the argmax in (1) is mathematically
equivalent to (derived using basic algebra):
</bodyText>
<equation confidence="0.561833666666667">
arg max (cos (b∗, b) − cos (b∗, a) + cos (b∗, a∗))
b*∈V
(3)
</equation>
<bodyText confidence="0.998884">
This means that solving analogy questions with
vector arithmetic is mathematically equivalent to
seeking a word (b∗) which is similar to b and a∗
but is different from a. Relational similarity is
thus expressed as a sum of attributional similari-
ties. While (1) and (3) are equal, we find the intu-
ition as to why (3) ought to find analogies clearer.
</bodyText>
<sectionHeader confidence="0.995213" genericHeader="method">
4 Empirical Setup
</sectionHeader>
<bodyText confidence="0.963987583333333">
We derive explicit and neural-embedded vec-
tor representations, and compare their capacities
to recover relational similarities using objectives
3COSADD (eq. 3) and PAIRDIRECTION (eq. 2).
Underlying Corpus and Preprocessing Previ-
ous reported results on the word analogy tasks us-
ing vector arithmetics were obtained using propri-
etary corpora. To make our experiments repro-
ducible, we selected an open and widely accessi-
ble corpus – the English Wikipedia. We extracted
all sentences from article bodies (excluding ti-
tles, infoboxes, captions, etc) and filtered non-
alphanumeric tokens, allowing mid-token symbols
as apostrophes, hyphens, commas, and periods.
All the text was lowercased. Duplicates and sen-
tences with less than 5 tokens were then removed.
Overall, we retained a corpus of about 1.5 billion
tokens, in 77.5 million sentences.
Word Representations To create contexts for
both embedding and sparse representation, we
used a window of two tokens to each side (5-
grams, in total), ignoring words that appeared less
1This was confirmed both by our independent trials and
by corresponding with the authors.
</bodyText>
<equation confidence="0.994590666666667">
u · v
cos (u, v) =
IuIIvI
</equation>
<page confidence="0.988048">
173
</page>
<bodyText confidence="0.999694583333333">
than 100 times in the corpus. The filtered vocabu-
lary contained 189,533 terms.2
The explicit vector representations were created
as described in Section 2. The neural embeddings
were created using the word2vec software3 ac-
companying (Mikolov et al., 2013b). We embed-
ded the vocabulary into a 600 dimensional space,
using the state-of-the-art skip-gram architecture,
the negative-training approach with 15 negative
samples (NEG-15), and sub-sampling of frequent
words with a parameter of 10−5. The parameter
settings follow (Mikolov et al., 2013b).
</bodyText>
<subsectionHeader confidence="0.992409">
4.1 Evaluation Conditions
</subsectionHeader>
<bodyText confidence="0.999909612903226">
We evaluate the different word representations us-
ing the three datasets used in previous work. Two
of them (MSR and GOOGLE) contain analogy
questions, while the third (SEMEVAL) requires
ranking of candidate word pairs according to their
relational similarity to a set of supplied word pairs.
Open Vocabulary The open vocabulary
datasets (MSR and GOOGLE) present questions
of the form “a is to a* as b is to b*”, where b*
is hidden, and must be guessed from the entire
vocabulary. Performance on these datasets is
measured by micro-averaged accuracy.
The MSR dataset4 (Mikolov et al., 2013c) con-
tains 8000 analogy questions. The relations por-
trayed by these questions are morpho-syntactic,
and can be categorized according to parts of
speech – adjectives, nouns and verbs. Adjec-
tive relations include comparative and superlative
(good is to best as smart is to smartest). Noun
relations include single and plural, possessive and
non-possessive (dog is to dog’s as cat is to cat’s).
Verb relations are tense modifications (work is to
worked as accept is to accepted).
The GOOGLE dataset5 (Mikolov et al., 2013a)
contains 19544 questions. It covers 14 relation
types, 7 of which are semantic in nature and 7
are morpho-syntactic (enumerated in Section 8).
The dataset was created by manually constructing
example word-pairs of each relation, and provid-
ing all the pairs of word-pairs (within each relation
type) as analogy questions.
</bodyText>
<footnote confidence="0.754583">
2Initial experiments with different window-sizes and cut-
offs showed similar trends.
3http://code.google.com/p/word2vec
4research.microsoft.com/en-us/
projects/rnn/
5code.google.com/p/word2vec/source/
browse/trunk/questions-words.txt
</footnote>
<bodyText confidence="0.999920740740741">
Out-of-vocabulary words6 were removed from
both test sets.
Closed Vocabulary The SEMEVAL dataset con-
tains the collection of 79 semantic relations that
appeared in SemEval 2012 Task 2: Measuring Re-
lation Similarity (Jurgens et al., 2012). Each rela-
tion is exemplified by a few (usually 3) character-
istic word-pairs. Given a set of several dozen tar-
get word pairs, which supposedly have the same
relation, the task is to rank the target pairs ac-
cording to the degree in which this relation holds.
This can be cast as an analogy question in the
following manner: For example, take the Recipi-
ent:Instrument relation with the prototypical word
pairs king:crown and police:badge. To measure
the degree that a target word pair wife:ring has the
same relation, we form the two analogy questions
“king is to crown as wife is to ring” and “police is
to badge as wife is to ring”. We calculate the score
of each analogy, and average the results. Note that
as opposed to the first two test sets, this one does
not require searching the entire vocabulary for the
most suitable word in the corpus, but rather to rank
a list of existing word pairs.
Following previous work, performance on SE-
MEVAL was measured using accuracy, macro-
averaged across all the relations.
</bodyText>
<sectionHeader confidence="0.977486" genericHeader="method">
5 Preliminary Results
</sectionHeader>
<bodyText confidence="0.999871666666667">
Our first experiment uses 3COSADD (method (3)
in Section 3) to measure the prevalence of linguis-
tic regularities within each representation.
</bodyText>
<table confidence="0.996396">
Representation MSR GOOGLE SEMEVAL
Embedding 53.98% 62.70% 38.49%
Explicit 29.04% 45.05% 38.54%
</table>
<tableCaption confidence="0.959263">
Table 1: Performance of 3COSADD on different tasks with
the explicit and neural embedding representations.
</tableCaption>
<bodyText confidence="0.996291111111111">
The results in Table 1 show that a large amount
of relational similarities can be recovered with
both representations. In fact, both representations
achieve the same accuracy on the SEMEVAL task.
However, there is a large performance gap in favor
of the neural embedding in the open-vocabulary
MSR and GOOGLE tasks.
Next, we run the same experiment with
PAIRDIRECTION (method (2) in Section 3).
</bodyText>
<footnote confidence="0.976545">
6i.e. words that appeared in English Wikipedia less
than 100 times. This removed 882 instances from the
MSR dataset and 286 instances from GOOGLE.
</footnote>
<page confidence="0.967768">
174
</page>
<table confidence="0.999941666666667">
Representation MSR GOOGLE SEMEVAL
Embedding 9.26% 14.51% 44.77%
Explicit 0.66% 0.75% 45.19%
</table>
<tableCaption confidence="0.9585305">
Table 2: Performance of PAIRDIRECTION on different tasks
with the explicit and neural embedding representations.
</tableCaption>
<bodyText confidence="0.999044884615385">
The results in Table 2 show that the PAIRDI-
RECTION method is better than 3COSADD on
the restricted-vocabulary SEMEVAL task (accu-
racy jumps from 38% to 45%), but fails at the
open-vocabulary questions in GOOGLE and MSR.
When the method does work, the numbers for the
explicit and embedded representations are again
comparable to one another.
Why is PAIRDIRECTION performing so well
on the SEMEVAL task, yet so poorly on the oth-
ers? Recall that the PAIRDIRECTION objective
focuses on the similarity of b∗ − b and a∗ − a,
but does not take into account the spatial distances
between the individual vectors. Relying on di-
rection alone, while ignoring spatial distance, is
problematic when considering the entire vocabu-
lary as candidates (as is required in the MSR and
GOOGLE tasks). We are likely to find candidates
b∗ that have the same relation to b as reflected by
a − a∗ but are not necessarily similar to b. As a
concrete example, in man:woman, king:?, we are
likely to recover feminine entities, but not neces-
sarily royal ones. The SEMEVAL test set, on the
other hand, already provides related (and therefore
geometrically close) candidates, leaving mainly
the direction to reason about.
</bodyText>
<sectionHeader confidence="0.992778" genericHeader="method">
6 Refining the Objective Function
</sectionHeader>
<bodyText confidence="0.959803515151515">
The 3COSADD objective, as expressed in (3), re-
veals a “balancing act” between two attractors and
one repeller, i.e. two terms that we wish to maxi-
mize and one that needs to be minimized:
arg max (cos (b∗, b) − cos (b∗, a) + cos (b∗, a∗))
b∗∈V
A known property of such linear objectives is that
they exhibit a “soft-or” behavior and allow one
sufficiently large term to dominate the expression.
This behavior is problematic in our setup, because
each term reflects a different aspect of similarity,
and the different aspects have different scales. For
example, king is more royal than it is masculine,
and will therefore overshadow the gender aspect
of the analogy. It is especially true in the case of
explicit vector representations, as each aspect of
the similarity is manifested by a different set of
features with varying sizes and weights.
A case in point is the analogy question “London
is to England as Baghdad is to — ?”, which we
answer using:
arg max (cos (x, en) − cos (x, lo) + cos (x, ba))
x∈V
We seek a word (Iraq) which is similar to Eng-
land (both are countries), is similar to Baghdad
(similar geography/culture) and is dissimilar to
London (different geography/culture). Maximiz-
ing the sum yields an incorrect answer (under both
representations): Mosul, a large Iraqi city. Look-
ing at the computed similarities in the explicit vec-
tor representation, we see that both Mosul and Iraq
are very close to Baghdad, and are quite far from
England and London:
</bodyText>
<table confidence="0.869600625">
(EXP) T England 1 London T Baghdad Sum
Mosul 0.031 0.031 0.244 0.244
Iraq 0.049 0.038 0.206 0.217
The same trends appear in the neural embedding
vectors, though with different similarity scores:
(EMB) T England 1 London T Baghdad Sum
Mosul 0.130 0.141 0.755 0.748
Iraq 0.153 0.130 0.631 0.655
</table>
<bodyText confidence="0.9989619">
While Iraq is much more similar to England than
Mosul is (both being countries), both similarities
(0.049 and 0.031 in explicit, 0.130 and 0.153 in
embedded) are small and the sums are dominated
by the geographic and cultural aspect of the anal-
ogy: Mosul and Iraq’s similarity to Baghdad (0.24
and 0.20 in explicit, 0.75 and 0.63 in embedded).
To achieve better balance among the different
aspects of similarity, we propose switching from
an additive to a multiplicative combination:
</bodyText>
<equation confidence="0.999015666666667">
cos (b∗, b) cos (b∗, a∗) (4)
cos (b∗, a) + E
(E = 0.001 is used to prevent division by zero)
</equation>
<bodyText confidence="0.885627636363636">
This is equivalent to taking the logarithm of each
term before summation, thus amplifying the dif-
ferences between small quantities and reducing
the differences between larger ones. Using this ob-
jective, Iraq is scored higher than Mosul (0.259 vs
0.236, 0.736 vs 0.691). We refer to objective (4)
as 3COSMUL.7
73COSMUL requires that all similarities be non-negative,
which trivially holds for explicit representations. With em-
beddings, we transform cosine similarities to [0, 1] using
(x + 1)/2 before calculating (4).
</bodyText>
<figure confidence="0.7954295">
arg max
b∗∈V
</figure>
<page confidence="0.989878">
175
</page>
<sectionHeader confidence="0.985138" genericHeader="method">
7 Main Results
</sectionHeader>
<bodyText confidence="0.99978975">
We repeated the experiments, this time using the
3COSMUL method. Table 3 presents the results,
showing that the multiplicative objective recov-
ers more relational similarities in both representa-
tions. The improvements achieved in the explicit
representation are especially dramatic, with an ab-
solute increase of over 20% correctly identified re-
lations in the MSR and GOOGLE datasets.
</bodyText>
<table confidence="0.8046218">
Representation
Embedding
Explicit
Embedding
Explicit
</table>
<tableCaption confidence="0.999061">
Table 3: Comparison of 3COSADD and 3COSMUL.
</tableCaption>
<bodyText confidence="0.980319166666667">
3COSMUL outperforms the state-of-the-art
(3COSADD) on these two datasets. Moreover, the
results illustrate that a comparable amount of rela-
tional similarities can be recovered with both rep-
resentations. This suggests that the linguistic reg-
ularities apparent in neural embeddings are not a
consequence of the embedding process, but rather
are well preserved by it.
On SEMEVAL, 3COSMUL preformed on par
with 3COSADD , recovering a similar amount of
analogies with both explicit and neural representa-
tions (38.37% and 38.67%, respectively).
</bodyText>
<sectionHeader confidence="0.998471" genericHeader="method">
8 Error Analysis
</sectionHeader>
<bodyText confidence="0.99998675">
With 3COSMUL, both the explicit vectors and
the neural embeddings recover similar amounts of
analogies, but are these the same patterns, or per-
haps different types of relational similarities?
</bodyText>
<subsectionHeader confidence="0.998165">
8.1 Agreement between Representations
</subsectionHeader>
<bodyText confidence="0.999782615384615">
Considering the open-vocabulary tasks (MSR and
GOOGLE), we count the number of times both rep-
resentations guessed correctly, both guessed in-
correctly, and when one representations leads to
the right answer while the other does not (Ta-
ble 4). While there is a large amount of agreement
between the representations, there is also a non-
negligible amount of cases in which they comple-
ment each other. If we were to run in an ora-
cle setup, in which an answer is considered cor-
rect if it is correct in either representation, we
would have achieved an accuracy of 71.9% on the
MSR dataset and 77.8% on GOOGLE.
</bodyText>
<table confidence="0.999446">
Both Both Embedding Explicit
Correct Wrong Correct Correct
MSR 43.97% 28.06% 15.12% 12.85%
GOOGLE 57.12% 22.17% 9.59% 11.12%
ALL 53.58% 23.76% 11.08% 11.59%
</table>
<tableCaption confidence="0.98395">
Table 4: Agreement between the representations on open-
vocabulary tasks.
</tableCaption>
<table confidence="0.999695111111111">
Embedding Explicit
90.51% 99.41%
77.61% 92.73%
56.95% 64.69%
14.55% 10.53%
76.48% 60.08%
24.29% 14.01%
37.07% 28.94%
86.11% 77.85%
56.72% 63.45%
63.35% 65.06%
89.37% 90.56%
65.83% 48.85%
72.15% 76.05%
71.15% 55.75%
45.88% 56.46%
56.96% 63.07%
69.90% 52.97%
</table>
<tableCaption confidence="0.997841">
Table 5: Breakdown of relational similarities in each repre-
sentation by relation type, using 3COSMUL.
</tableCaption>
<subsectionHeader confidence="0.995343">
8.2 Breakdown by Relation Type
</subsectionHeader>
<bodyText confidence="0.9999866">
Table 5 presents the amount of analogies dis-
covered in each representation, broken down by
relation type. Some trends emerge: the ex-
plicit representation is superior in some of the
more semantic tasks, especially geography re-
lated ones, as well as the ones superlatives and
nouns. The neural embedding, however, has the
upper hand on most verb inflections, compara-
tives, and family (gender) relations. Some rela-
tions (currency, adjectives-to-adverbs, opposites)
pose a challenge to both representations, though
are somewhat better handled by the embedded
representations. Finally, the nationality-adjectives
and present-participles are equally handled by
both representations.
</bodyText>
<subsectionHeader confidence="0.910018">
8.3 Default-Behavior Errors
</subsectionHeader>
<bodyText confidence="0.999985888888889">
The most common error pattern under both repre-
sentations is that of a “default behavior”, in which
one central representative word is provided as an
answer to many questions of the same type. For
example, the word “Fresno” is returned 82 times
as an incorrect answer in the city-in-state rela-
tion in the embedded representation, and the word
“daughter” is returned 47 times as an incorrect an-
swer in the family relation in the explicit represen-
</bodyText>
<figure confidence="0.999765928571428">
Objective
3COSADD
3COSMUL
MSR GOOGLE
53.98% 62.70%
GOOGLE
29.04% 45.05%
59.09% 66.72%
56.83% 68.24%
MSR
Relation
capital-common-countries
capital-world
city-in-state
currency
family (gender inflections)
gram1-adjective-to-adverb
gram2-opposite
gram3-comparative
gram4-superlative
gram5-present-participle
gram6-nationality-adjective
gram7-past-tense
gram8-plural (nouns)
gram9-plural-verbs
adjectives
nouns
verbs
</figure>
<page confidence="0.992696">
176
</page>
<table confidence="0.9993653125">
WORD EMB EXP
who 0 138
fresno 82 24
slovak 39 39
argentine 37 39
belarusian 37 39
colour 36 35
higher 34 35
smith 1 61
and 0 49
be 0 47
daughter 8 47
illinois 3 40
currency 5 40
and 0 39
enhance 39 20
</table>
<tableCaption confidence="0.82385175">
Table 6: Common default-behavior errors under both repre-
sentations. EMB / EXP: the number of time the word was
returned as an incorrect answer for the given relation under
the embedded or explicit representation.
</tableCaption>
<bodyText confidence="0.999954933333333">
tation. Loosely, “Fresno” is identified by the em-
bedded representation as a prototypical location,
while “daughter” is identified by the explicit rep-
resentation as a prototypical female. Under a def-
inition in which a default behavior error is one in
which the same incorrect answer is returned for a
particular relation 10 or more times, such errors
account for 49% of the errors in the explicit repre-
sentation, and for 39% of the errors in the embed-
ded representation.
Table 6 lists the 15 most common default er-
rors under both representations. In most default er-
rors the category of the default word is closely re-
lated to the analogy question, sharing the category
of either the correct answer, or (as in the case of
“Fresno”) the question word. Notable exceptions
are the words “who”, “and”, “be” and “smith” that
are returned as default answers in the explicit rep-
resentation, and which are very far from the in-
tended relation. It seems that in the explicit repre-
sentation, some very frequent function words act
as “hubs” and confuse the model. In fact, the
performance gap between the representations in
the past-tense and plural-verb relations can be at-
tributed specifically to such function-word errors:
23.4% of the mistakes in past-tense relation are
due to the explicit representation’s default answer
of “who” or “and”, while 19% of the mistakes in
the plural-verb relations are due to default answers
of “is/and/that/who”.
</bodyText>
<subsectionHeader confidence="0.979808">
8.4 Verb-inflection Errors
</subsectionHeader>
<bodyText confidence="0.9999903">
A correct solution to the morphological anal-
ogy task requires recovering both the correct in-
flection (requiring syntactic similarity) and the
correct base word (requiring semantic similar-
ity). We observe that linguistically, the mor-
phological distinctions and similarities tend to
rely on a few common word forms (for exam-
ple, the “walk:walking” relation is characterized
by modals such as “will” appearing before “walk”
and never before “walking”, and be verbs ap-
pearing before walking and never before “walk”),
while the support for the semantic relations is
spread out over many more items. We hypothe-
size that the morphological distinctions in verbs
are much harder to capture than the semantics. In-
deed, under both representations, errors in which
the selected word has a correct form with an incor-
rect inflection are over ten times more likely than
errors in which the selected word has the correct
inflection but an incorrect base form.
</bodyText>
<sectionHeader confidence="0.860385" genericHeader="method">
9 Interpreting Relational Similarities
</sectionHeader>
<bodyText confidence="0.999766451612903">
The ability to capture relational similarities by
performing vector (or similarity) arithmetic is re-
markable. In this section, we try and provide intu-
ition as to why it works.
Consider the word “king”; it has several aspects,
high-level properties that it implies, such as roy-
alty or (male) gender, and its attributional simi-
larity with another word is based on a mixture of
those aspects; e.g. king is related to queen on the
royalty and the human axes, and shares the gender
and the human aspect with man. Relational simi-
larities can be viewed as a composition of attribu-
tional similarities, each one reflecting a different
aspect. In “man is to woman as king is to queen”,
the two main aspects are gender and royalty. Solv-
ing the analogy question involves identifying the
relevant aspects, and trying to change one of them
while preserving the other.
How are concepts such as gender, royalty, or
“cityness” represented in the vector space? While
the neural embeddings are mostly opaque, one of
the appealing properties of explicit vector repre-
sentations is our ability to read and understand the
vectors’ features. For example, king is represented
in our explicit vector space by 51,409 contexts, of
which the top 3 are tut+1, jeongjo+1, adulyadej+2
– all names of monarchs. The explicit representa-
tion allows us to glimpse at the way different as-
pects are represented. To do so, we choose a repre-
sentative pair of words that share an aspect, inter-
sect their vectors, and inspect the highest scoring
</bodyText>
<figure confidence="0.997202">
RELATION
gram7-past-tense
city-in-state
gram6-nationality-adjective
gram6-nationality-adjective
gram6-nationality-adjective
gram8-plural (nouns)
gram3-comparative
city-in-state
gram7-past-tense
gram1-adjective-to-adverb
family (gender inflections)
city-in-state
currency
gram1-adjective-to-adverb
gram7-past-tense
</figure>
<page confidence="0.979306">
177
</page>
<table confidence="0.999669066666667">
Aspect Examples Top Features
woman O queen estrid+1 ketevan+1 adeliza+1 nzinga+1 gunnhild+1 impregnate−2 hippolyta+1
queen O king savang+1 uncrowned−1 pmare+1 sisowath+1 nzinga+1 tupou+1 uvea+2 majesty−1
yen O ruble devalue−2 banknote+1 denominated+1 billion−1 banknotes+1 pegged+2 coin+1
germany O australia emigrates−2 1943-45+2 pentathletes−2 emigrated−2 emigrate−2 hong-kong−1
berlin O canberra hotshots−1 embassy−2 1925-26+2 consulate-general+2 meetups−2 nunciature−2
sweetest O tallest freshest+2 asia’s−1 cleveland’s−2 smartest+1 world’s−1 city’s−1 america’s−1
taller O tallest regnans−2 skyscraper+1 skyscrapers+1 6’4+2 windsor’s−1 smokestacks+1 burj+2
Female
Royalty
Currency
Country
Capital
Superlative
Height
</table>
<tableCaption confidence="0.77867975">
Table 7: The top features of each aspect, recovered by pointwise multiplication of words that share that aspect. The result of
pointwise multiplication is an “aspect vector” in which the features common to both words, characterizing the relation, receive
the highest scores. The feature scores (not shown) correspond to the weight the feature contributes to the cosine similarity
between the vectors. The superscript marks the position of the feature relative to the target word.
</tableCaption>
<bodyText confidence="0.999884913043478">
features in the intersection. Table 7 presents the
top (most influential) features of each aspect.
Many of these features are names of people or
places, which appear rarely in our corpus (e.g.
Adeliza, a historical queen, and Nzinga, a royal
family) but are nonetheless highly indicative of
the shared concept. The prevalence of rare words
stems from PMI, which gives them more weight,
and from the fact that words like woman and queen
are closely related (a queen is a woman), and thus
have many features in common. Ordering the fea-
tures of woman O queen by prevalence reveals
female pronouns (“she”, “her”) and a long list of
common feminine names, reflecting the expected
aspect shared by woman and queen. Word pairs
that share more specific aspects, such as capital
cities or countries, show features that are charac-
teristic of their shared aspect (e.g. capital cities
have embassies and meetups, while immigration
is associated with countries). It is also interesting
to observe how the relatively syntactic “superlativ-
ity” aspect is captured with many regional posses-
sives (“america’s”, “asia’s”, “world’s”).
</bodyText>
<sectionHeader confidence="0.999881" genericHeader="conclusions">
10 Related Work
</sectionHeader>
<bodyText confidence="0.9999616875">
Relational similarity (and answering analogy
questions) was previously tackled using explicit
representations. Previous approaches use task-
specific information, by either relying on a
(word-pair, connectives) matrix rather than the
standard (word, context) matrix (Turney and
Littman, 2005; Turney, 2006), or by treating anal-
ogy detection as a supervised learning task (Ba-
roni and Lenci, 2009; Jurgens et al., 2012; Turney,
2013). In contrast, the vector arithmetic approach
followed here is unsupervised, and works on a
generic single-word representation. Even though
the training process is oblivious to the task of anal-
ogy detection, the resulting representation is able
to detect them quite accurately. Turney (2012) as-
sumes a similar setting but with two types of word
similarities, and combines them with products and
ratios (similar to 3COSMUL) to recover a variety
of semantic relations, including analogies.
Arithmetic combination of explicit word vec-
tors is extensively studied in the context of com-
positional semantics (Mitchell and Lapata, 2010),
where a phrase composed of two or more words
is represented by a single vector, computed by a
function of its component word vectors. Blacoe
and Lapata (2012) compare different arithmetic
functions across multiple representations (includ-
ing embeddings) on a range of compositionality
benchmarks. To the best of our knowledge such
methods of word vector arithmetic have not been
explored for recovering relational similarities in
explicit representations.
</bodyText>
<sectionHeader confidence="0.995258" genericHeader="acknowledgments">
11 Discussion
</sectionHeader>
<bodyText confidence="0.99996045">
Mikolov et al. showed how an unsupervised neural
network can represent words in a space that “nat-
urally” encodes relational similarities in the form
of vector offsets. This study shows that finding
analogies through vector arithmetic is actually a
form of balancing word similarities, and that, con-
trary to the recent findings of Baroni et al. (2014),
under certain conditions traditional word similar-
ities induced by explicit representations can per-
form just as well as neural embeddings on this
task.
Learning to represent words is a fascinating and
important challenge with implications to most cur-
rent NLP efforts, and neural embeddings in par-
ticular are a promising research direction. We
believe that to improve these representations we
should understand how they work, and hope that
the methods and insights provided in this work
will help to deepen our grasp of current and future
investigations of word representations.
</bodyText>
<page confidence="0.997577">
178
</page>
<sectionHeader confidence="0.989835" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999193136363636">
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual nlp. In Proc. of CoNLL 2013.
Marco Baroni and Alessandro Lenci. 2009. One dis-
tributional memory, many semantic spaces. In Pro-
ceedings of the Workshop on Geometrical Models
of Natural Language Semantics, pages 1–8, Athens,
Greece, March. Association for Computational Lin-
guistics.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673–721.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Dont count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), Baltimore, Maryland, USA, June. Association
for Computational Linguistics.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546–556, Jeju Island, Korea,
July. Association for Computational Linguistics.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39(3):510–526.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, pages 160–167.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Ido Dagan, Fernando Pereira, and Lillian Lee. 1994.
Similarity-based estimation of word cooccurrence
probabilities. In Proceedings of the 32nd annual
meeting on Association for Computational Linguis-
tics, pages 272–278. Association for Computational
Linguistics.
David A Jurgens, Peter D Turney, Saif M Mohammad,
and Keith J Holyoak. 2012. Semeval-2012 task 2:
Measuring degrees of relational similarity. In Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics, pages 356–364. As-
sociation for Computational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359–389.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), Balti-
more, Maryland, USA, June. Association for Com-
putational Linguistics.
Dekang Lin and Patrick Pantel. 2001. Dirt: discovery
of inference rules from text. In KDD, pages 323–
328.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 2, ACL ’98, pages
768–774, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Acoustics, Speech and Signal Processing
(ICASSP), 2011 IEEE International Conference on,
pages 5528–5531. IEEE.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States, pages 3111–
3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746–751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1439.
</reference>
<page confidence="0.98712">
179
</page>
<reference confidence="0.998653796875">
Andriy Mnih and Geoffrey E Hinton. 2008. A scal-
able hierarchical distributed language model. In Ad-
vances in Neural Information Processing Systems,
pages 1081–1088.
Yoshiki Niwa and Yoshihiko Nitta. 1994. Co-
occurrence vectors from corpora vs. distance vec-
tors from dictionaries. In Proceedings of the 15th
conference on Computational linguistics-Volume 1,
pages 304–309. Association for Computational Lin-
guistics.
Sebastian Pad´o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161–199.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of the 31st annual meeting on Associa-
tion for Computational Linguistics, pages 183–190.
Association for Computational Linguistics.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stock-
holm.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161. Association for
Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394. Association for
Computational Linguistics.
Peter D. Turney and Michael L. Littman. 2005.
Corpus-based learning of analogies and semantic re-
lations. Machine Learning, 60(1-3):251–278.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141–188.
Peter D. Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Proceedings of the 12th
European Conference on Machine Learning, pages
491–502. Springer-Verlag.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379–416.
Peter D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533–
585.
Peter D. Turney. 2013. Distributional semantics be-
yond words: Supervised learning of analogy and
paraphrase. CoRR, abs/1310.5042.
Alisa Zhila, Wen-tau Yih, Christopher Meek, Geof-
frey Zweig, and Tomas Mikolov. 2013. Combining
heterogeneous models for measuring relational sim-
ilarity. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1000–1009, Atlanta, Georgia, June.
Association for Computational Linguistics.
</reference>
<page confidence="0.997758">
180
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.177469">
<title confidence="0.997063">Linguistic Regularities in Sparse and Explicit Word Representations</title>
<affiliation confidence="0.7781155">Computer Science Bar-Ilan</affiliation>
<address confidence="0.227008">Ramat-Gan,</address>
<abstract confidence="0.996299045454546">Recent work has shown that neuralembedded word representations capture many relational similarities, which can be recovered by means of vector arithmetic in the embedded space. We show that Mikolov et al.’s method of first adding and subtracting word vectors, and then searching for a word similar to the result, is equivalent to searching for a word that maximizes a linear combination of three pairwise word similarities. Based on this observation, we suggest an improved method of recovering relational similarities, improving the state-of-the-art results on two recent word-analogy datasets. Moreover, we demonstrate that analogy recovery is not restricted to neural word embeddings, and that a similar amount of relational similarities can be recovered from traditional distributional word representations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rami Al-Rfou</author>
<author>Bryan Perozzi</author>
<author>Steven Skiena</author>
</authors>
<title>Polyglot: Distributed word representations for multilingual nlp.</title>
<date>2013</date>
<booktitle>In Proc. of CoNLL</booktitle>
<contexts>
<context position="2302" citStr="Al-Rfou et al., 2013" startWordPosition="344" endWordPosition="347">opean Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). We refer to such word representations as neural embeddings or just embeddings. Recently, Mikolov et al. (2013c) demonstrated that the embeddings created by a recursive neural network (RNN) encode not only attributional similarities between words, but also similarities between pairs of words. Such similarities are referred to as linguistic regularities by Mikolov et al. and as relational similarities by Turney (2006). They capture, for example, the gender relation exhibited by the pairs “man:woman”, “king:queen”, the language-spoken-in relation in “france:french”, “mexico:spanish” and the pas</context>
</contexts>
<marker>Al-Rfou, Perozzi, Skiena, 2013</marker>
<rawString>Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. 2013. Polyglot: Distributed word representations for multilingual nlp. In Proc. of CoNLL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>One distributional memory, many semantic spaces.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="32644" citStr="Baroni and Lenci, 2009" startWordPosition="5177" endWordPosition="5181">while immigration is associated with countries). It is also interesting to observe how the relatively syntactic “superlativity” aspect is captured with many regional possessives (“america’s”, “asia’s”, “world’s”). 10 Related Work Relational similarity (and answering analogy questions) was previously tackled using explicit representations. Previous approaches use taskspecific information, by either relying on a (word-pair, connectives) matrix rather than the standard (word, context) matrix (Turney and Littman, 2005; Turney, 2006), or by treating analogy detection as a supervised learning task (Baroni and Lenci, 2009; Jurgens et al., 2012; Turney, 2013). In contrast, the vector arithmetic approach followed here is unsupervised, and works on a generic single-word representation. Even though the training process is oblivious to the task of analogy detection, the resulting representation is able to detect them quite accurately. Turney (2012) assumes a similar setting but with two types of word similarities, and combines them with products and ratios (similar to 3COSMUL) to recover a variety of semantic relations, including analogies. Arithmetic combination of explicit word vectors is extensively studied in t</context>
</contexts>
<marker>Baroni, Lenci, 2009</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2009. One distributional memory, many semantic spaces. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 1–8, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="4956" citStr="Baroni and Lenci, 2010" startWordPosition="753" endWordPosition="756">entations, but how crucial is it for a representation to be dense and low-dimensional at all? An alternative approach to representing words as vectors is the distributional similarity representation, or bag of contexts. In this representation, each word is associated with a very highdimensional but sparse vector capturing the contexts in which the word occurs. We call such vector representations explicit, as each dimension directly corresponds to a particular context. These explicit vector-space representations have been extensively studied in the NLP literature (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein), and are known to exhibit a large extent of attributional similarity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). In this study, we show that similarly to the neural embedding space, the explicit vector space also encodes a vast amount of relational similarity which can be recovered in a similar fashion, suggesting the explicit vector space representation as a competitive baseline for further work on neural embeddings. Moreover, this result implies that the neural embedding process is not discovering novel pattern</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Dont count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Dont count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="1556" citStr="Bengio et al., 2003" startWordPosition="225" endWordPosition="228">milarities can be recovered from traditional distributional word representations. 1 Introduction Deep learning methods for language processing owe much of their success to neural network language models, in which words are represented as dense real-valued vectors in Rd. Such representations are referred to as distributed word representations or word embeddings, as they embed an entire vocabulary into a relatively low-dimensional linear space, whose dimensions are latent continuous features. The embedded word vectors are trained over large collections of text using variants of neural networks (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). The ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and ar</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>546--556</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="33469" citStr="Blacoe and Lapata (2012)" startWordPosition="5307" endWordPosition="5310">oblivious to the task of analogy detection, the resulting representation is able to detect them quite accurately. Turney (2012) assumes a similar setting but with two types of word similarities, and combines them with products and ratios (similar to 3COSMUL) to recover a variety of semantic relations, including analogies. Arithmetic combination of explicit word vectors is extensively studied in the context of compositional semantics (Mitchell and Lapata, 2010), where a phrase composed of two or more words is represented by a single vector, computed by a function of its component word vectors. Blacoe and Lapata (2012) compare different arithmetic functions across multiple representations (including embeddings) on a range of compositionality benchmarks. To the best of our knowledge such methods of word vector arithmetic have not been explored for recovering relational similarities in explicit representations. 11 Discussion Mikolov et al. showed how an unsupervised neural network can represent words in a space that “naturally” encodes relational similarities in the form of vector offsets. This study shows that finding analogies through vector arithmetic is actually a form of balancing word similarities, and </context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting semantic representations from word cooccurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="7607" citStr="Bullinaria and Levy (2007)" startWordPosition="1195" endWordPosition="1198"> PMI(w, c) &lt; 0 PPMI(w, c) = PMI(w, c) otherwise PMI(w, c) = log P (w,c) P (w)P (c) = log freq(w,c)|corpus| freq(w)freq(c) where |corpus |is the number of items in the corpus, freq(w, c) is the number of times word w appeared in context c in the corpus, and freq(w), freq(c) are the corpus frequencies of the word and the context respectively. The use of PMI in distributional similarity models was introduced by Church and Hanks (1990) and widely adopted (Dagan et al., 1994; Turney, 2001). The PPMI variant dates back to at least (Niwa and Nitta, 1994), and was demonstrated to perform very well in Bullinaria and Levy (2007). In this work, we take the linear contexts in which words appear. We consider each word surrounding the target word w in a window of 2 to each side as a context, distinguishing between different sequential positions. For example, in the sentence a b c d e the contexts of the word c are a−2 , b−1, d+1 and e+2. Each vector’s dimenstion is thus |C |Pt� 4 |V |. Empirically, the number of non-zero dimensions for vocabulary items in our corpus ranges between 3 (for some rare tokens) and 474,234 (for the word “and”), with a mean of 1595 and a median of 415. Another popular choice of context is the s</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John A. Bullinaria and Joseph P. Levy. 2007. Extracting semantic representations from word cooccurrence statistics: A computational study. Behavior Research Methods, 39(3):510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<booktitle>Computational linguistics,</booktitle>
<pages>16--1</pages>
<contexts>
<context position="7416" citStr="Church and Hanks (1990)" startWordPosition="1162" endWordPosition="1165">e association strength between a word w E V and a context c E C can take many forms. We chose to use the popular positive pointwise mutual information (PPMI) metric: Sij = PPMI(wi, cj) � 0 PMI(w, c) &lt; 0 PPMI(w, c) = PMI(w, c) otherwise PMI(w, c) = log P (w,c) P (w)P (c) = log freq(w,c)|corpus| freq(w)freq(c) where |corpus |is the number of items in the corpus, freq(w, c) is the number of times word w appeared in context c in the corpus, and freq(w), freq(c) are the corpus frequencies of the word and the context respectively. The use of PMI in distributional similarity models was introduced by Church and Hanks (1990) and widely adopted (Dagan et al., 1994; Turney, 2001). The PPMI variant dates back to at least (Niwa and Nitta, 1994), and was demonstrated to perform very well in Bullinaria and Levy (2007). In this work, we take the linear contexts in which words appear. We consider each word surrounding the target word w in a window of 2 to each side as a context, distinguishing between different sequential positions. For example, in the sentence a b c d e the contexts of the word c are a−2 , b−1, d+1 and e+2. Each vector’s dimenstion is thus |C |Pt� 4 |V |. Empirically, the number of non-zero dimensions f</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="1584" citStr="Collobert and Weston, 2008" startWordPosition="229" endWordPosition="233">overed from traditional distributional word representations. 1 Introduction Deep learning methods for language processing owe much of their success to neural network language models, in which words are represented as dense real-valued vectors in Rd. Such representations are referred to as distributed word representations or word embeddings, as they embed an entire vocabulary into a relatively low-dimensional linear space, whose dimensions are latent continuous features. The embedded word vectors are trained over large collections of text using variants of neural networks (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). The ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as f</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="2258" citStr="Collobert et al., 2011" startWordPosition="336" endWordPosition="339">ov et al., 2013b). The ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). We refer to such word representations as neural embeddings or just embeddings. Recently, Mikolov et al. (2013c) demonstrated that the embeddings created by a recursive neural network (RNN) encode not only attributional similarities between words, but also similarities between pairs of words. Such similarities are referred to as linguistic regularities by Mikolov et al. and as relational similarities by Turney (2006). They capture, for example, the gender relation exhibited by the pairs “man:woman”, “king:queen”, the language-spoken-in relation in “</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Fernando Pereira</author>
<author>Lillian Lee</author>
</authors>
<title>Similarity-based estimation of word cooccurrence probabilities.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>272--278</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7455" citStr="Dagan et al., 1994" startWordPosition="1169" endWordPosition="1172">and a context c E C can take many forms. We chose to use the popular positive pointwise mutual information (PPMI) metric: Sij = PPMI(wi, cj) � 0 PMI(w, c) &lt; 0 PPMI(w, c) = PMI(w, c) otherwise PMI(w, c) = log P (w,c) P (w)P (c) = log freq(w,c)|corpus| freq(w)freq(c) where |corpus |is the number of items in the corpus, freq(w, c) is the number of times word w appeared in context c in the corpus, and freq(w), freq(c) are the corpus frequencies of the word and the context respectively. The use of PMI in distributional similarity models was introduced by Church and Hanks (1990) and widely adopted (Dagan et al., 1994; Turney, 2001). The PPMI variant dates back to at least (Niwa and Nitta, 1994), and was demonstrated to perform very well in Bullinaria and Levy (2007). In this work, we take the linear contexts in which words appear. We consider each word surrounding the target word w in a window of 2 to each side as a context, distinguishing between different sequential positions. For example, in the sentence a b c d e the contexts of the word c are a−2 , b−1, d+1 and e+2. Each vector’s dimenstion is thus |C |Pt� 4 |V |. Empirically, the number of non-zero dimensions for vocabulary items in our corpus range</context>
</contexts>
<marker>Dagan, Pereira, Lee, 1994</marker>
<rawString>Ido Dagan, Fernando Pereira, and Lillian Lee. 1994. Similarity-based estimation of word cooccurrence probabilities. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pages 272–278. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Jurgens</author>
<author>Peter D Turney</author>
<author>Saif M Mohammad</author>
<author>Keith J Holyoak</author>
</authors>
<title>Semeval-2012 task 2: Measuring degrees of relational similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>356--364</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14943" citStr="Jurgens et al., 2012" startWordPosition="2397" endWordPosition="2400"> manually constructing example word-pairs of each relation, and providing all the pairs of word-pairs (within each relation type) as analogy questions. 2Initial experiments with different window-sizes and cutoffs showed similar trends. 3http://code.google.com/p/word2vec 4research.microsoft.com/en-us/ projects/rnn/ 5code.google.com/p/word2vec/source/ browse/trunk/questions-words.txt Out-of-vocabulary words6 were removed from both test sets. Closed Vocabulary The SEMEVAL dataset contains the collection of 79 semantic relations that appeared in SemEval 2012 Task 2: Measuring Relation Similarity (Jurgens et al., 2012). Each relation is exemplified by a few (usually 3) characteristic word-pairs. Given a set of several dozen target word pairs, which supposedly have the same relation, the task is to rank the target pairs according to the degree in which this relation holds. This can be cast as an analogy question in the following manner: For example, take the Recipient:Instrument relation with the prototypical word pairs king:crown and police:badge. To measure the degree that a target word pair wife:ring has the same relation, we form the two analogy questions “king is to crown as wife is to ring” and “police</context>
<context position="32666" citStr="Jurgens et al., 2012" startWordPosition="5182" endWordPosition="5185">ociated with countries). It is also interesting to observe how the relatively syntactic “superlativity” aspect is captured with many regional possessives (“america’s”, “asia’s”, “world’s”). 10 Related Work Relational similarity (and answering analogy questions) was previously tackled using explicit representations. Previous approaches use taskspecific information, by either relying on a (word-pair, connectives) matrix rather than the standard (word, context) matrix (Turney and Littman, 2005; Turney, 2006), or by treating analogy detection as a supervised learning task (Baroni and Lenci, 2009; Jurgens et al., 2012; Turney, 2013). In contrast, the vector arithmetic approach followed here is unsupervised, and works on a generic single-word representation. Even though the training process is oblivious to the task of analogy detection, the resulting representation is able to detect them quite accurately. Turney (2012) assumes a similar setting but with two types of word similarities, and combines them with products and ratios (similar to 3COSMUL) to recover a variety of semantic relations, including analogies. Arithmetic combination of explicit word vectors is extensively studied in the context of composit</context>
</contexts>
<marker>Jurgens, Turney, Mohammad, Holyoak, 2012</marker>
<rawString>David A Jurgens, Peter D Turney, Saif M Mohammad, and Keith J Holyoak. 2012. Semeval-2012 task 2: Measuring degrees of relational similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 356–364. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="5149" citStr="Kotlerman et al., 2010" startWordPosition="784" endWordPosition="787">ation, or bag of contexts. In this representation, each word is associated with a very highdimensional but sparse vector capturing the contexts in which the word occurs. We call such vector representations explicit, as each dimension directly corresponds to a particular context. These explicit vector-space representations have been extensively studied in the NLP literature (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein), and are known to exhibit a large extent of attributional similarity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). In this study, we show that similarly to the neural embedding space, the explicit vector space also encodes a vast amount of relational similarity which can be recovered in a similar fashion, suggesting the explicit vector space representation as a competitive baseline for further work on neural embeddings. Moreover, this result implies that the neural embedding process is not discovering novel patterns, but rather is doing a remarkable job at preserving the patterns inherent in the wordcontext co-occurrence matrix. A key insight of this work is that the vector arithmetic method can be decom</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(4):359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="8311" citStr="Levy and Goldberg, 2014" startWordPosition="1329" endWordPosition="1332">ach word surrounding the target word w in a window of 2 to each side as a context, distinguishing between different sequential positions. For example, in the sentence a b c d e the contexts of the word c are a−2 , b−1, d+1 and e+2. Each vector’s dimenstion is thus |C |Pt� 4 |V |. Empirically, the number of non-zero dimensions for vocabulary items in our corpus ranges between 3 (for some rare tokens) and 474,234 (for the word “and”), with a mean of 1595 and a median of 415. Another popular choice of context is the syntactic relations the word participates in (Lin, 1998; Pad´o and Lapata, 2007; Levy and Goldberg, 2014). In this paper, we chose the sequential context as it is compatible with the information available to the state-of-the-art neural embedding method we are comparing against. 172 3 Analogies and Vector Arithmetic Mikolov et al. demonstrated that vector space representations encode various relational similarities, which can be recovered using vector arithmetic and used to solve word-analogy tasks. 3.1 Analogy Questions In a word-analogy task we are given two pairs of words that share a relation (e.g. “man:woman”, “king:queen”). The identity of the fourth word (“queen”) is hidden, and we need to </context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Dirt: discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In KDD,</booktitle>
<pages>323--328</pages>
<contexts>
<context position="5108" citStr="Lin and Pantel, 2001" startWordPosition="778" endWordPosition="781">he distributional similarity representation, or bag of contexts. In this representation, each word is associated with a very highdimensional but sparse vector capturing the contexts in which the word occurs. We call such vector representations explicit, as each dimension directly corresponds to a particular context. These explicit vector-space representations have been extensively studied in the NLP literature (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein), and are known to exhibit a large extent of attributional similarity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). In this study, we show that similarly to the neural embedding space, the explicit vector space also encodes a vast amount of relational similarity which can be recovered in a similar fashion, suggesting the explicit vector space representation as a competitive baseline for further work on neural embeddings. Moreover, this result implies that the neural embedding process is not discovering novel patterns, but rather is doing a remarkable job at preserving the patterns inherent in the wordcontext co-occurrence matrix. A key insight of this work is that </context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Dirt: discovery of inference rules from text. In KDD, pages 323– 328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 2, ACL ’98,</booktitle>
<pages>768--774</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5086" citStr="Lin, 1998" startWordPosition="776" endWordPosition="777">ectors is the distributional similarity representation, or bag of contexts. In this representation, each word is associated with a very highdimensional but sparse vector capturing the contexts in which the word occurs. We call such vector representations explicit, as each dimension directly corresponds to a particular context. These explicit vector-space representations have been extensively studied in the NLP literature (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein), and are known to exhibit a large extent of attributional similarity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). In this study, we show that similarly to the neural embedding space, the explicit vector space also encodes a vast amount of relational similarity which can be recovered in a similar fashion, suggesting the explicit vector space representation as a competitive baseline for further work on neural embeddings. Moreover, this result implies that the neural embedding process is not discovering novel patterns, but rather is doing a remarkable job at preserving the patterns inherent in the wordcontext co-occurrence matrix. A key insight</context>
<context position="8261" citStr="Lin, 1998" startWordPosition="1323" endWordPosition="1324">n which words appear. We consider each word surrounding the target word w in a window of 2 to each side as a context, distinguishing between different sequential positions. For example, in the sentence a b c d e the contexts of the word c are a−2 , b−1, d+1 and e+2. Each vector’s dimenstion is thus |C |Pt� 4 |V |. Empirically, the number of non-zero dimensions for vocabulary items in our corpus ranges between 3 (for some rare tokens) and 474,234 (for the word “and”), with a mean of 1595 and a median of 415. Another popular choice of context is the syntactic relations the word participates in (Lin, 1998; Pad´o and Lapata, 2007; Levy and Goldberg, 2014). In this paper, we chose the sequential context as it is compatible with the information available to the state-of-the-art neural embedding method we are comparing against. 172 3 Analogies and Vector Arithmetic Mikolov et al. demonstrated that vector space representations encode various relational similarities, which can be recovered using vector arithmetic and used to solve word-analogy tasks. 3.1 Analogy Questions In a word-analogy task we are given two pairs of words that share a relation (e.g. “man:woman”, “king:queen”). The identity of th</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 2, ACL ’98, pages 768–774, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Lukas Burget</author>
<author>JH Cernocky</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Extensions of recurrent neural network language model.</title>
<date>2011</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on,</booktitle>
<pages>5528--5531</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1629" citStr="Mikolov et al., 2011" startWordPosition="238" endWordPosition="241">ntations. 1 Introduction Deep learning methods for language processing owe much of their success to neural network language models, in which words are represented as dense real-valued vectors in Rd. Such representations are referred to as distributed word representations or word embeddings, as they embed an entire vocabulary into a relatively low-dimensional linear space, whose dimensions are latent continuous features. The embedded word vectors are trained over large collections of text using variants of neural networks (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). The ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al.,</context>
</contexts>
<marker>Mikolov, Kombrink, Burget, Cernocky, Khudanpur, 2011</marker>
<rawString>Tomas Mikolov, Stefan Kombrink, Lukas Burget, JH Cernocky, and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5528–5531. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="1651" citStr="Mikolov et al., 2013" startWordPosition="242" endWordPosition="245">on Deep learning methods for language processing owe much of their success to neural network language models, in which words are represented as dense real-valued vectors in Rd. Such representations are referred to as distributed word representations or word embeddings, as they embed an entire vocabulary into a relatively low-dimensional linear space, whose dimensions are latent continuous features. The embedded word vectors are trained over large collections of text using variants of neural networks (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). The ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al</context>
<context position="3870" citStr="Mikolov et al., 2013" startWordPosition="583" endWordPosition="586">he relation is hidden. Perhaps the most famous example is that the embedded representation of the word queen can be roughly recovered from the representations of king, man and woman: queen ≈ king − man + woman The recovery of relational similarities using vector arithmetic on RNN-embedded vectors was evaluated on many relations, achieving state-of-the-art results in relational similarity identification tasks 171 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 171–180, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics (Mikolov et al., 2013c; Zhila et al., 2013). It was later demonstrated that relational similarities can be recovered in a similar fashion also from embeddings trained with different architectures (Mikolov et al., 2013a; Mikolov et al., 2013b). This fascinating result raises a question: to what extent are the relational semantic properties a result of the embedding process? Experiments in (Mikolov et al., 2013c) show that the RNN-based embeddings are superior to other dense representations, but how crucial is it for a representation to be dense and low-dimensional at all? An alternative approach to representing wor</context>
<context position="10544" citStr="Mikolov et al. (2013" startWordPosition="1725" endWordPosition="1728">e, high cosine similarity (close to 1) means that the vectors share a very similar direction. Note that this metric normalizes (and thus ignores) the vectors’ lengths, unlike the Euclidean distance between them. For reasons that will be clear later, we refer to (1) as the 3COSADD method. An alternative to 3COSADD is to require that the direction of transformation be conserved: arg max (cos (b∗ − b, a∗ − a)) (2) b*∈V This basically means that b∗ − b shares the same direction with a∗ − a, ignoring the distances. We refer to this method as PAIRDIRECTION. Though it was not mentioned in the paper, Mikolov et al. (2013c) used PAIRDIRECTION for solving the semantic analogies of the SemEval task, and 3COSADD for solving the syntactic analogies.1 3.3 Reinterpreting Vector Arithmetic In Mikolov et al.’s experiments, all word-vectors were normalized to unit length. Under such normalization, the argmax in (1) is mathematically equivalent to (derived using basic algebra): arg max (cos (b∗, b) − cos (b∗, a) + cos (b∗, a∗)) b*∈V (3) This means that solving analogy questions with vector arithmetic is mathematically equivalent to seeking a word (b∗) which is similar to b and a∗ but is different from a. Relational simi</context>
<context position="12723" citStr="Mikolov et al., 2013" startWordPosition="2071" endWordPosition="2074">pus of about 1.5 billion tokens, in 77.5 million sentences. Word Representations To create contexts for both embedding and sparse representation, we used a window of two tokens to each side (5- grams, in total), ignoring words that appeared less 1This was confirmed both by our independent trials and by corresponding with the authors. u · v cos (u, v) = IuIIvI 173 than 100 times in the corpus. The filtered vocabulary contained 189,533 terms.2 The explicit vector representations were created as described in Section 2. The neural embeddings were created using the word2vec software3 accompanying (Mikolov et al., 2013b). We embedded the vocabulary into a 600 dimensional space, using the state-of-the-art skip-gram architecture, the negative-training approach with 15 negative samples (NEG-15), and sub-sampling of frequent words with a parameter of 10−5. The parameter settings follow (Mikolov et al., 2013b). 4.1 Evaluation Conditions We evaluate the different word representations using the three datasets used in previous work. Two of them (MSR and GOOGLE) contain analogy questions, while the third (SEMEVAL) requires ranking of candidate word pairs according to their relational similarity to a set of supplied </context>
<context position="14150" citStr="Mikolov et al., 2013" startWordPosition="2295" endWordPosition="2298">ance on these datasets is measured by micro-averaged accuracy. The MSR dataset4 (Mikolov et al., 2013c) contains 8000 analogy questions. The relations portrayed by these questions are morpho-syntactic, and can be categorized according to parts of speech – adjectives, nouns and verbs. Adjective relations include comparative and superlative (good is to best as smart is to smartest). Noun relations include single and plural, possessive and non-possessive (dog is to dog’s as cat is to cat’s). Verb relations are tense modifications (work is to worked as accept is to accepted). The GOOGLE dataset5 (Mikolov et al., 2013a) contains 19544 questions. It covers 14 relation types, 7 of which are semantic in nature and 7 are morpho-syntactic (enumerated in Section 8). The dataset was created by manually constructing example word-pairs of each relation, and providing all the pairs of word-pairs (within each relation type) as analogy questions. 2Initial experiments with different window-sizes and cutoffs showed similar trends. 3http://code.google.com/p/word2vec 4research.microsoft.com/en-us/ projects/rnn/ 5code.google.com/p/word2vec/source/ browse/trunk/questions-words.txt Out-of-vocabulary words6 were removed from </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Gregory S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</booktitle>
<pages>3111--3119</pages>
<location>Lake Tahoe, Nevada, United States,</location>
<contexts>
<context position="1651" citStr="Mikolov et al., 2013" startWordPosition="242" endWordPosition="245">on Deep learning methods for language processing owe much of their success to neural network language models, in which words are represented as dense real-valued vectors in Rd. Such representations are referred to as distributed word representations or word embeddings, as they embed an entire vocabulary into a relatively low-dimensional linear space, whose dimensions are latent continuous features. The embedded word vectors are trained over large collections of text using variants of neural networks (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). The ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al</context>
<context position="3870" citStr="Mikolov et al., 2013" startWordPosition="583" endWordPosition="586">he relation is hidden. Perhaps the most famous example is that the embedded representation of the word queen can be roughly recovered from the representations of king, man and woman: queen ≈ king − man + woman The recovery of relational similarities using vector arithmetic on RNN-embedded vectors was evaluated on many relations, achieving state-of-the-art results in relational similarity identification tasks 171 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 171–180, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics (Mikolov et al., 2013c; Zhila et al., 2013). It was later demonstrated that relational similarities can be recovered in a similar fashion also from embeddings trained with different architectures (Mikolov et al., 2013a; Mikolov et al., 2013b). This fascinating result raises a question: to what extent are the relational semantic properties a result of the embedding process? Experiments in (Mikolov et al., 2013c) show that the RNN-based embeddings are superior to other dense representations, but how crucial is it for a representation to be dense and low-dimensional at all? An alternative approach to representing wor</context>
<context position="10544" citStr="Mikolov et al. (2013" startWordPosition="1725" endWordPosition="1728">e, high cosine similarity (close to 1) means that the vectors share a very similar direction. Note that this metric normalizes (and thus ignores) the vectors’ lengths, unlike the Euclidean distance between them. For reasons that will be clear later, we refer to (1) as the 3COSADD method. An alternative to 3COSADD is to require that the direction of transformation be conserved: arg max (cos (b∗ − b, a∗ − a)) (2) b*∈V This basically means that b∗ − b shares the same direction with a∗ − a, ignoring the distances. We refer to this method as PAIRDIRECTION. Though it was not mentioned in the paper, Mikolov et al. (2013c) used PAIRDIRECTION for solving the semantic analogies of the SemEval task, and 3COSADD for solving the syntactic analogies.1 3.3 Reinterpreting Vector Arithmetic In Mikolov et al.’s experiments, all word-vectors were normalized to unit length. Under such normalization, the argmax in (1) is mathematically equivalent to (derived using basic algebra): arg max (cos (b∗, b) − cos (b∗, a) + cos (b∗, a∗)) b*∈V (3) This means that solving analogy questions with vector arithmetic is mathematically equivalent to seeking a word (b∗) which is similar to b and a∗ but is different from a. Relational simi</context>
<context position="12723" citStr="Mikolov et al., 2013" startWordPosition="2071" endWordPosition="2074">pus of about 1.5 billion tokens, in 77.5 million sentences. Word Representations To create contexts for both embedding and sparse representation, we used a window of two tokens to each side (5- grams, in total), ignoring words that appeared less 1This was confirmed both by our independent trials and by corresponding with the authors. u · v cos (u, v) = IuIIvI 173 than 100 times in the corpus. The filtered vocabulary contained 189,533 terms.2 The explicit vector representations were created as described in Section 2. The neural embeddings were created using the word2vec software3 accompanying (Mikolov et al., 2013b). We embedded the vocabulary into a 600 dimensional space, using the state-of-the-art skip-gram architecture, the negative-training approach with 15 negative samples (NEG-15), and sub-sampling of frequent words with a parameter of 10−5. The parameter settings follow (Mikolov et al., 2013b). 4.1 Evaluation Conditions We evaluate the different word representations using the three datasets used in previous work. Two of them (MSR and GOOGLE) contain analogy questions, while the third (SEMEVAL) requires ranking of candidate word pairs according to their relational similarity to a set of supplied </context>
<context position="14150" citStr="Mikolov et al., 2013" startWordPosition="2295" endWordPosition="2298">ance on these datasets is measured by micro-averaged accuracy. The MSR dataset4 (Mikolov et al., 2013c) contains 8000 analogy questions. The relations portrayed by these questions are morpho-syntactic, and can be categorized according to parts of speech – adjectives, nouns and verbs. Adjective relations include comparative and superlative (good is to best as smart is to smartest). Noun relations include single and plural, possessive and non-possessive (dog is to dog’s as cat is to cat’s). Verb relations are tense modifications (work is to worked as accept is to accepted). The GOOGLE dataset5 (Mikolov et al., 2013a) contains 19544 questions. It covers 14 relation types, 7 of which are semantic in nature and 7 are morpho-syntactic (enumerated in Section 8). The dataset was created by manually constructing example word-pairs of each relation, and providing all the pairs of word-pairs (within each relation type) as analogy questions. 2Initial experiments with different window-sizes and cutoffs showed similar trends. 3http://code.google.com/p/word2vec 4research.microsoft.com/en-us/ projects/rnn/ 5code.google.com/p/word2vec/source/ browse/trunk/questions-words.txt Out-of-vocabulary words6 were removed from </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 3111– 3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>746--751</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="1651" citStr="Mikolov et al., 2013" startWordPosition="242" endWordPosition="245">on Deep learning methods for language processing owe much of their success to neural network language models, in which words are represented as dense real-valued vectors in Rd. Such representations are referred to as distributed word representations or word embeddings, as they embed an entire vocabulary into a relatively low-dimensional linear space, whose dimensions are latent continuous features. The embedded word vectors are trained over large collections of text using variants of neural networks (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). The ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al</context>
<context position="3870" citStr="Mikolov et al., 2013" startWordPosition="583" endWordPosition="586">he relation is hidden. Perhaps the most famous example is that the embedded representation of the word queen can be roughly recovered from the representations of king, man and woman: queen ≈ king − man + woman The recovery of relational similarities using vector arithmetic on RNN-embedded vectors was evaluated on many relations, achieving state-of-the-art results in relational similarity identification tasks 171 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 171–180, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics (Mikolov et al., 2013c; Zhila et al., 2013). It was later demonstrated that relational similarities can be recovered in a similar fashion also from embeddings trained with different architectures (Mikolov et al., 2013a; Mikolov et al., 2013b). This fascinating result raises a question: to what extent are the relational semantic properties a result of the embedding process? Experiments in (Mikolov et al., 2013c) show that the RNN-based embeddings are superior to other dense representations, but how crucial is it for a representation to be dense and low-dimensional at all? An alternative approach to representing wor</context>
<context position="10544" citStr="Mikolov et al. (2013" startWordPosition="1725" endWordPosition="1728">e, high cosine similarity (close to 1) means that the vectors share a very similar direction. Note that this metric normalizes (and thus ignores) the vectors’ lengths, unlike the Euclidean distance between them. For reasons that will be clear later, we refer to (1) as the 3COSADD method. An alternative to 3COSADD is to require that the direction of transformation be conserved: arg max (cos (b∗ − b, a∗ − a)) (2) b*∈V This basically means that b∗ − b shares the same direction with a∗ − a, ignoring the distances. We refer to this method as PAIRDIRECTION. Though it was not mentioned in the paper, Mikolov et al. (2013c) used PAIRDIRECTION for solving the semantic analogies of the SemEval task, and 3COSADD for solving the syntactic analogies.1 3.3 Reinterpreting Vector Arithmetic In Mikolov et al.’s experiments, all word-vectors were normalized to unit length. Under such normalization, the argmax in (1) is mathematically equivalent to (derived using basic algebra): arg max (cos (b∗, b) − cos (b∗, a) + cos (b∗, a∗)) b*∈V (3) This means that solving analogy questions with vector arithmetic is mathematically equivalent to seeking a word (b∗) which is similar to b and a∗ but is different from a. Relational simi</context>
<context position="12723" citStr="Mikolov et al., 2013" startWordPosition="2071" endWordPosition="2074">pus of about 1.5 billion tokens, in 77.5 million sentences. Word Representations To create contexts for both embedding and sparse representation, we used a window of two tokens to each side (5- grams, in total), ignoring words that appeared less 1This was confirmed both by our independent trials and by corresponding with the authors. u · v cos (u, v) = IuIIvI 173 than 100 times in the corpus. The filtered vocabulary contained 189,533 terms.2 The explicit vector representations were created as described in Section 2. The neural embeddings were created using the word2vec software3 accompanying (Mikolov et al., 2013b). We embedded the vocabulary into a 600 dimensional space, using the state-of-the-art skip-gram architecture, the negative-training approach with 15 negative samples (NEG-15), and sub-sampling of frequent words with a parameter of 10−5. The parameter settings follow (Mikolov et al., 2013b). 4.1 Evaluation Conditions We evaluate the different word representations using the three datasets used in previous work. Two of them (MSR and GOOGLE) contain analogy questions, while the third (SEMEVAL) requires ranking of candidate word pairs according to their relational similarity to a set of supplied </context>
<context position="14150" citStr="Mikolov et al., 2013" startWordPosition="2295" endWordPosition="2298">ance on these datasets is measured by micro-averaged accuracy. The MSR dataset4 (Mikolov et al., 2013c) contains 8000 analogy questions. The relations portrayed by these questions are morpho-syntactic, and can be categorized according to parts of speech – adjectives, nouns and verbs. Adjective relations include comparative and superlative (good is to best as smart is to smartest). Noun relations include single and plural, possessive and non-possessive (dog is to dog’s as cat is to cat’s). Verb relations are tense modifications (work is to worked as accept is to accepted). The GOOGLE dataset5 (Mikolov et al., 2013a) contains 19544 questions. It covers 14 relation types, 7 of which are semantic in nature and 7 are morpho-syntactic (enumerated in Section 8). The dataset was created by manually constructing example word-pairs of each relation, and providing all the pairs of word-pairs (within each relation type) as analogy questions. 2Initial experiments with different window-sizes and cutoffs showed similar trends. 3http://code.google.com/p/word2vec 4research.microsoft.com/en-us/ projects/rnn/ 5code.google.com/p/word2vec/source/ browse/trunk/questions-words.txt Out-of-vocabulary words6 were removed from </context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="33309" citStr="Mitchell and Lapata, 2010" startWordPosition="5279" endWordPosition="5282">In contrast, the vector arithmetic approach followed here is unsupervised, and works on a generic single-word representation. Even though the training process is oblivious to the task of analogy detection, the resulting representation is able to detect them quite accurately. Turney (2012) assumes a similar setting but with two types of word similarities, and combines them with products and ratios (similar to 3COSMUL) to recover a variety of semantic relations, including analogies. Arithmetic combination of explicit word vectors is extensively studied in the context of compositional semantics (Mitchell and Lapata, 2010), where a phrase composed of two or more words is represented by a single vector, computed by a function of its component word vectors. Blacoe and Lapata (2012) compare different arithmetic functions across multiple representations (including embeddings) on a range of compositionality benchmarks. To the best of our knowledge such methods of word vector arithmetic have not been explored for recovering relational similarities in explicit representations. 11 Discussion Mikolov et al. showed how an unsupervised neural network can represent words in a space that “naturally” encodes relational simil</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1081--1088</pages>
<contexts>
<context position="1607" citStr="Mnih and Hinton, 2008" startWordPosition="234" endWordPosition="237">ributional word representations. 1 Introduction Deep learning methods for language processing owe much of their success to neural network language models, in which words are represented as dense real-valued vectors in Rd. Such representations are referred to as distributed word representations or word embeddings, as they embed an entire vocabulary into a relatively low-dimensional linear space, whose dimensions are latent continuous features. The embedded word vectors are trained over large collections of text using variants of neural networks (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). The ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP</context>
</contexts>
<marker>Mnih, Hinton, 2008</marker>
<rawString>Andriy Mnih and Geoffrey E Hinton. 2008. A scalable hierarchical distributed language model. In Advances in Neural Information Processing Systems, pages 1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshiki Niwa</author>
<author>Yoshihiko Nitta</author>
</authors>
<title>Cooccurrence vectors from corpora vs. distance vectors from dictionaries.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th conference on Computational linguistics-Volume 1,</booktitle>
<pages>304--309</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7534" citStr="Niwa and Nitta, 1994" startWordPosition="1183" endWordPosition="1186">e pointwise mutual information (PPMI) metric: Sij = PPMI(wi, cj) � 0 PMI(w, c) &lt; 0 PPMI(w, c) = PMI(w, c) otherwise PMI(w, c) = log P (w,c) P (w)P (c) = log freq(w,c)|corpus| freq(w)freq(c) where |corpus |is the number of items in the corpus, freq(w, c) is the number of times word w appeared in context c in the corpus, and freq(w), freq(c) are the corpus frequencies of the word and the context respectively. The use of PMI in distributional similarity models was introduced by Church and Hanks (1990) and widely adopted (Dagan et al., 1994; Turney, 2001). The PPMI variant dates back to at least (Niwa and Nitta, 1994), and was demonstrated to perform very well in Bullinaria and Levy (2007). In this work, we take the linear contexts in which words appear. We consider each word surrounding the target word w in a window of 2 to each side as a context, distinguishing between different sequential positions. For example, in the sentence a b c d e the contexts of the word c are a−2 , b−1, d+1 and e+2. Each vector’s dimenstion is thus |C |Pt� 4 |V |. Empirically, the number of non-zero dimensions for vocabulary items in our corpus ranges between 3 (for some rare tokens) and 474,234 (for the word “and”), with a mea</context>
</contexts>
<marker>Niwa, Nitta, 1994</marker>
<rawString>Yoshiki Niwa and Yoshihiko Nitta. 1994. Cooccurrence vectors from corpora vs. distance vectors from dictionaries. In Proceedings of the 15th conference on Computational linguistics-Volume 1, pages 304–309. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of english words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st annual meeting on Association for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5075" citStr="Pereira et al., 1993" startWordPosition="772" endWordPosition="775">epresenting words as vectors is the distributional similarity representation, or bag of contexts. In this representation, each word is associated with a very highdimensional but sparse vector capturing the contexts in which the word occurs. We call such vector representations explicit, as each dimension directly corresponds to a particular context. These explicit vector-space representations have been extensively studied in the NLP literature (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein), and are known to exhibit a large extent of attributional similarity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). In this study, we show that similarly to the neural embedding space, the explicit vector space also encodes a vast amount of relational similarity which can be recovered in a similar fashion, suggesting the explicit vector space representation as a competitive baseline for further work on neural embeddings. Moreover, this result implies that the neural embedding process is not discovering novel patterns, but rather is doing a remarkable job at preserving the patterns inherent in the wordcontext co-occurrence matrix. A </context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of english words. In Proceedings of the 31st annual meeting on Association for Computational Linguistics, pages 183–190. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<location>Stockholm.</location>
<contexts>
<context position="5124" citStr="Sahlgren, 2006" startWordPosition="782" endWordPosition="783">larity representation, or bag of contexts. In this representation, each word is associated with a very highdimensional but sparse vector capturing the contexts in which the word occurs. We call such vector representations explicit, as each dimension directly corresponds to a particular context. These explicit vector-space representations have been extensively studied in the NLP literature (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein), and are known to exhibit a large extent of attributional similarity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). In this study, we show that similarly to the neural embedding space, the explicit vector space also encodes a vast amount of relational similarity which can be recovered in a similar fashion, suggesting the explicit vector space representation as a competitive baseline for further work on neural embeddings. Moreover, this result implies that the neural embedding process is not discovering novel patterns, but rather is doing a remarkable job at preserving the patterns inherent in the wordcontext co-occurrence matrix. A key insight of this work is that the vector arith</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces. Ph.D. thesis, Stockholm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2279" citStr="Socher et al., 2011" startWordPosition="340" endWordPosition="343"> Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). We refer to such word representations as neural embeddings or just embeddings. Recently, Mikolov et al. (2013c) demonstrated that the embeddings created by a recursive neural network (RNN) encode not only attributional similarities between words, but also similarities between pairs of words. Such similarities are referred to as linguistic regularities by Mikolov et al. and as relational similarities by Turney (2006). They capture, for example, the gender relation exhibited by the pairs “man:woman”, “king:queen”, the language-spoken-in relation in “france:french”, “mexi</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2234" citStr="Turian et al., 2010" startWordPosition="332" endWordPosition="335">v et al., 2011; Mikolov et al., 2013b). The ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). We refer to such word representations as neural embeddings or just embeddings. Recently, Mikolov et al. (2013c) demonstrated that the embeddings created by a recursive neural network (RNN) encode not only attributional similarities between words, but also similarities between pairs of words. Such similarities are referred to as linguistic regularities by Mikolov et al. and as relational similarities by Turney (2006). They capture, for example, the gender relation exhibited by the pairs “man:woman”, “king:queen”, the language</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Corpus-based learning of analogies and semantic relations.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="32541" citStr="Turney and Littman, 2005" startWordPosition="5160" endWordPosition="5163">features that are characteristic of their shared aspect (e.g. capital cities have embassies and meetups, while immigration is associated with countries). It is also interesting to observe how the relatively syntactic “superlativity” aspect is captured with many regional possessives (“america’s”, “asia’s”, “world’s”). 10 Related Work Relational similarity (and answering analogy questions) was previously tackled using explicit representations. Previous approaches use taskspecific information, by either relying on a (word-pair, connectives) matrix rather than the standard (word, context) matrix (Turney and Littman, 2005; Turney, 2006), or by treating analogy detection as a supervised learning task (Baroni and Lenci, 2009; Jurgens et al., 2012; Turney, 2013). In contrast, the vector arithmetic approach followed here is unsupervised, and works on a generic single-word representation. Even though the training process is oblivious to the task of analogy detection, the resulting representation is able to detect them quite accurately. Turney (2012) assumes a similar setting but with two types of word similarities, and combines them with products and ratios (similar to 3COSMUL) to recover a variety of semantic rela</context>
</contexts>
<marker>Turney, Littman, 2005</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2005. Corpus-based learning of analogies and semantic relations. Machine Learning, 60(1-3):251–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="4931" citStr="Turney and Pantel, 2010" startWordPosition="748" endWordPosition="752">ior to other dense representations, but how crucial is it for a representation to be dense and low-dimensional at all? An alternative approach to representing words as vectors is the distributional similarity representation, or bag of contexts. In this representation, each word is associated with a very highdimensional but sparse vector capturing the contexts in which the word occurs. We call such vector representations explicit, as each dimension directly corresponds to a particular context. These explicit vector-space representations have been extensively studied in the NLP literature (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein), and are known to exhibit a large extent of attributional similarity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). In this study, we show that similarly to the neural embedding space, the explicit vector space also encodes a vast amount of relational similarity which can be recovered in a similar fashion, suggesting the explicit vector space representation as a competitive baseline for further work on neural embeddings. Moreover, this result implies that the neural embedding process is not </context>
<context position="6433" citStr="Turney and Pantel, 2010" startWordPosition="982" endWordPosition="985">es (Section 3). While mathematically equivalent, we find that thinking about the method in terms of the decomposed formulation is much less puzzling, and provides a better intuition on why we would expect the method to perform well on the analogy recovery task. Furthermore, the decomposed form leads us to suggest a modified optimization objective (Section 6), which outperforms the state-ofthe-art at recovering relational similarities under both representations. 2 Explicit Vector Space Representation We adopt the traditional word representation used in the distributional similarity literature (Turney and Pantel, 2010). Each word is associated with a sparse vector capturing the contexts in which it occurs. We call this representation explicit, as each dimension corresponds to a particular context. For a vocabulary V and a set of contexts C, the result is a |V |x|C |sparse matrix S in which Sij corresponds to the strength of the association between word i and context j. The association strength between a word w E V and a context c E C can take many forms. We chose to use the popular positive pointwise mutual information (PPMI) metric: Sij = PPMI(wi, cj) � 0 PMI(w, c) &lt; 0 PPMI(w, c) = PMI(w, c) otherwise PMI(</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the web for synonyms: Pmi-ir versus lsa on toefl.</title>
<date>2001</date>
<booktitle>In Proceedings of the 12th European Conference on Machine Learning,</booktitle>
<pages>491--502</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="7470" citStr="Turney, 2001" startWordPosition="1173" endWordPosition="1174">can take many forms. We chose to use the popular positive pointwise mutual information (PPMI) metric: Sij = PPMI(wi, cj) � 0 PMI(w, c) &lt; 0 PPMI(w, c) = PMI(w, c) otherwise PMI(w, c) = log P (w,c) P (w)P (c) = log freq(w,c)|corpus| freq(w)freq(c) where |corpus |is the number of items in the corpus, freq(w, c) is the number of times word w appeared in context c in the corpus, and freq(w), freq(c) are the corpus frequencies of the word and the context respectively. The use of PMI in distributional similarity models was introduced by Church and Hanks (1990) and widely adopted (Dagan et al., 1994; Turney, 2001). The PPMI variant dates back to at least (Niwa and Nitta, 1994), and was demonstrated to perform very well in Bullinaria and Levy (2007). In this work, we take the linear contexts in which words appear. We consider each word surrounding the target word w in a window of 2 to each side as a context, distinguishing between different sequential positions. For example, in the sentence a b c d e the contexts of the word c are a−2 , b−1, d+1 and e+2. Each vector’s dimenstion is thus |C |Pt� 4 |V |. Empirically, the number of non-zero dimensions for vocabulary items in our corpus ranges between 3 (fo</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the web for synonyms: Pmi-ir versus lsa on toefl. In Proceedings of the 12th European Conference on Machine Learning, pages 491–502. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="1848" citStr="Turney (2006)" startWordPosition="271" endWordPosition="272">ferred to as distributed word representations or word embeddings, as they embed an entire vocabulary into a relatively low-dimensional linear space, whose dimensions are latent continuous features. The embedded word vectors are trained over large collections of text using variants of neural networks (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). The ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). We refer to such word representations as neural embeddings or just embeddings. Recently, Mikolov et al. (2013c) demonstrated that the embeddings</context>
<context position="32556" citStr="Turney, 2006" startWordPosition="5164" endWordPosition="5165">ristic of their shared aspect (e.g. capital cities have embassies and meetups, while immigration is associated with countries). It is also interesting to observe how the relatively syntactic “superlativity” aspect is captured with many regional possessives (“america’s”, “asia’s”, “world’s”). 10 Related Work Relational similarity (and answering analogy questions) was previously tackled using explicit representations. Previous approaches use taskspecific information, by either relying on a (word-pair, connectives) matrix rather than the standard (word, context) matrix (Turney and Littman, 2005; Turney, 2006), or by treating analogy detection as a supervised learning task (Baroni and Lenci, 2009; Jurgens et al., 2012; Turney, 2013). In contrast, the vector arithmetic approach followed here is unsupervised, and works on a generic single-word representation. Even though the training process is oblivious to the task of analogy detection, the resulting representation is able to detect them quite accurately. Turney (2012) assumes a similar setting but with two types of word similarities, and combines them with products and ratios (similar to 3COSMUL) to recover a variety of semantic relations, includin</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Domain and function: A dualspace model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>44</volume>
<pages>585</pages>
<contexts>
<context position="32972" citStr="Turney (2012)" startWordPosition="5229" endWordPosition="5230">ns. Previous approaches use taskspecific information, by either relying on a (word-pair, connectives) matrix rather than the standard (word, context) matrix (Turney and Littman, 2005; Turney, 2006), or by treating analogy detection as a supervised learning task (Baroni and Lenci, 2009; Jurgens et al., 2012; Turney, 2013). In contrast, the vector arithmetic approach followed here is unsupervised, and works on a generic single-word representation. Even though the training process is oblivious to the task of analogy detection, the resulting representation is able to detect them quite accurately. Turney (2012) assumes a similar setting but with two types of word similarities, and combines them with products and ratios (similar to 3COSMUL) to recover a variety of semantic relations, including analogies. Arithmetic combination of explicit word vectors is extensively studied in the context of compositional semantics (Mitchell and Lapata, 2010), where a phrase composed of two or more words is represented by a single vector, computed by a function of its component word vectors. Blacoe and Lapata (2012) compare different arithmetic functions across multiple representations (including embeddings) on a ran</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>Peter D. Turney. 2012. Domain and function: A dualspace model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44:533– 585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Distributional semantics beyond words: Supervised learning of analogy and paraphrase.</title>
<date>2013</date>
<location>CoRR, abs/1310.5042.</location>
<contexts>
<context position="32681" citStr="Turney, 2013" startWordPosition="5186" endWordPosition="5187">). It is also interesting to observe how the relatively syntactic “superlativity” aspect is captured with many regional possessives (“america’s”, “asia’s”, “world’s”). 10 Related Work Relational similarity (and answering analogy questions) was previously tackled using explicit representations. Previous approaches use taskspecific information, by either relying on a (word-pair, connectives) matrix rather than the standard (word, context) matrix (Turney and Littman, 2005; Turney, 2006), or by treating analogy detection as a supervised learning task (Baroni and Lenci, 2009; Jurgens et al., 2012; Turney, 2013). In contrast, the vector arithmetic approach followed here is unsupervised, and works on a generic single-word representation. Even though the training process is oblivious to the task of analogy detection, the resulting representation is able to detect them quite accurately. Turney (2012) assumes a similar setting but with two types of word similarities, and combines them with products and ratios (similar to 3COSMUL) to recover a variety of semantic relations, including analogies. Arithmetic combination of explicit word vectors is extensively studied in the context of compositional semantics</context>
</contexts>
<marker>Turney, 2013</marker>
<rawString>Peter D. Turney. 2013. Distributional semantics beyond words: Supervised learning of analogy and paraphrase. CoRR, abs/1310.5042.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alisa Zhila</author>
<author>Wen-tau Yih</author>
<author>Christopher Meek</author>
<author>Geoffrey Zweig</author>
<author>Tomas Mikolov</author>
</authors>
<title>Combining heterogeneous models for measuring relational similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1000--1009</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="3892" citStr="Zhila et al., 2013" startWordPosition="587" endWordPosition="590">Perhaps the most famous example is that the embedded representation of the word queen can be roughly recovered from the representations of king, man and woman: queen ≈ king − man + woman The recovery of relational similarities using vector arithmetic on RNN-embedded vectors was evaluated on many relations, achieving state-of-the-art results in relational similarity identification tasks 171 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 171–180, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics (Mikolov et al., 2013c; Zhila et al., 2013). It was later demonstrated that relational similarities can be recovered in a similar fashion also from embeddings trained with different architectures (Mikolov et al., 2013a; Mikolov et al., 2013b). This fascinating result raises a question: to what extent are the relational semantic properties a result of the embedding process? Experiments in (Mikolov et al., 2013c) show that the RNN-based embeddings are superior to other dense representations, but how crucial is it for a representation to be dense and low-dimensional at all? An alternative approach to representing words as vectors is the d</context>
</contexts>
<marker>Zhila, Yih, Meek, Zweig, Mikolov, 2013</marker>
<rawString>Alisa Zhila, Wen-tau Yih, Christopher Meek, Geoffrey Zweig, and Tomas Mikolov. 2013. Combining heterogeneous models for measuring relational similarity. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1000–1009, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
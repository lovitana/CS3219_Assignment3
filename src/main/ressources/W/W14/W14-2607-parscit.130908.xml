<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.990279">
Semantic Role Labeling of Emotions in Tweets
</title>
<author confidence="0.997313">
Saif M. Mohammad, Xiaodan Zhu, and Joel Martin
</author>
<affiliation confidence="0.843298">
National Research Council Canada
Ottawa, Ontario, Canada K1A 0R6
</affiliation>
<email confidence="0.761645">
{saif.mohammad,xiaodan.zhu,joel.martin}@nrc-cnrc.gc.ca
</email>
<sectionHeader confidence="0.994142" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999429375">
Past work on emotion processing has fo-
cused solely on detecting emotions, and
ignored questions such as ‘who is feeling
the emotion (the experiencer)?’ and ‘to-
wards whom is the emotion directed (the
stimulus)?’. We automatically compile a
large dataset of tweets pertaining to the
2012 US presidential elections, and anno-
tate it not only for emotion but also for
the experiencer and the stimulus. We then
develop a classifier for detecting emotion
that obtains an accuracy of 56.84 on an
eight-way classification task. Finally, we
show how the stimulus identification task
can also be framed as a classification task,
obtaining an F-score of 58.30.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999840083333334">
Detecting emotions in text has a number of ap-
plications including tracking sentiment towards
politicians, movies, and products (Pang and Lee,
2008), identifying what emotion a newspaper
headline is trying to evoke (Bellegarda, 2010),
developing more natural text-to-speech systems
(Francisco and Gerv´as, 2006), detecting how peo-
ple use emotion-bearing-words and metaphors to
persuade and coerce others (for example, in pro-
paganda) (Kˇovecses, 2003), tracking response to
natural disasters (Mandel et al., 2012), and so
on. With the rapid proliferation of microblogging,
there is growing amount of emotion analysis re-
search on newly available datasets of Twitter posts
(Mandel et al., 2012; Purver and Battersby, 2012;
Mohammad, 2012b). However, past work has fo-
cused solely on detecting emotional state. It has
ignored questions such as ‘who is feeling the emo-
tion (the experiencer)?’ and ‘towards whom is the
emotion directed (the stimulus)?’.
In this paper, we present a system that analyzes
tweets to determine who is feeling what emotion,
and towards whom. We use tweets from the 2012
US presidential elections as our dataset, since we
expect political tweets to be particularly rich in
emotions. Further, the dataset will be useful for
applications such as determining political align-
ment of tweeters (Golbeck and Hansen, 2011;
Conover et al., 2011b), identifying contentious
issues (Maynard and Funk, 2011), detecting the
amount of polarization in the electorate (Conover
et al., 2011a), and so on.
Detecting the who, what, and towards whom
of emotions is essentially a semantic role-labeling
problem (Gildea and Jurafsky, 2002). The seman-
tic frame for ‘emotions’ in FrameNet (Baker et al.,
1998) is shown in Table 1. In this work, we fo-
cus on the roles of Experiencer, State, and Stim-
ulus. Note, however, that the state or emotion is
often not explicitly present in text. Other roles
such as Reason, Degree, and Event are also of sig-
nificance, and remain suitable avenues for future
work.
We automatically compile a large dataset of
2012 US presidential elections using a small num-
ber of hand-chosen hashtags. Next we annotate
the tweets for Experiencer, State, and Stimulus
by crowdsourcing to Amazon’s Mechanical Turk.1
We analyze the annotations to determine the dis-
tributions of different types of roles, and show that
the dataset is rich in emotions. We develop a clas-
sifier for emotion detection that obtains an accu-
racy of 56.84. We find that most of the tweets
express emotions of the tweeter, and only a few
are indicative of the emotions of someone else.
Finally, we show how the stimulus identification
task can be framed as a classification task that cir-
cumvents more complicated problems of detecting
entity mentions and coreferences. Our supervised
classifier obtains an F-score of 58.30 on this task.
</bodyText>
<footnote confidence="0.987949">
1https://www.mturk.com/mturk/welcome
</footnote>
<page confidence="0.976046">
32
</page>
<note confidence="0.97985">
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 32–41,
Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics
</note>
<tableCaption confidence="0.996995">
Table 1: The FrameNet frame for emotions. The three roles investigated in this paper are shown in bold.
</tableCaption>
<figure confidence="0.960226733333333">
Role Description
Core:
Event The Event is the occasion or happening that Experiencers in a certain emotional state participate in.
Experiencer The Experiencer is the person or sentient entity that experiences or feels the emotions.
Expressor The body part, gesture, or other expression of the Experiencer that reflects his or her emotional state.
State The State is the abstract noun that describes a more lasting experience by the Experiencer.
Stimulus The Stimulus is the person, event, or state of affairs that evokes the emotional response in the Experiencer.
Topic The Topic is the general area in which the emotion occurs. It indicates a range of possible Stimulus.
Non-Core:
Circumstances The Circumstances is the condition(s) under which the Stimulus evokes its response.
Degree The extent to which the Experiencer’s emotion deviates from the norm for the emotion.
Empathy target The Empathy target is the individual or individuals with which the Experiencer identifies emotionally.
Manner Any way the Experiencer experiences the Stimulus which is not covered by more specific frame elements.
Parameter The Parameter is a domain in which the Experiencer experiences the Stimulus.
Reason The Reason is the explanation for why the Stimulus evokes a certain emotional response.
</figure>
<sectionHeader confidence="0.999642" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99885125">
Our work here is related to emotion analysis, se-
mantic role labeling (SRL), and information ex-
traction (IE).
Much of the past work on emotion detection
focuses on emotions argued to be the most ba-
sic. For example, Ekman (1992) proposed six ba-
sic emotions—joy, sadness, anger, fear, disgust,
and surprise. Plutchik (1980) argued in favor
of eight—Ekman’s six, surprise, and anticipation.
Many of the automatic systems use affect lexi-
cons pertaining to these basic emotions such as
the NRC Emotion Lexicon (Mohammad and Tur-
ney, 2010), WordNet Affect (Strapparava and Val-
itutti, 2004), and the Affective Norms for English
Words.2 Affect lexicons are lists of words and as-
sociated emotions.
Emotion analysis techniques have been applied
to many different kinds of text (Mihalcea and Liu,
2006; Genereux and Evans, 2006; Neviarouskaya
et al., 2009; Mohammad, 2012a). More recently
there has been work on tweets as well (Bollen
et al., 2011; Tumasjan et al., 2010; Mohammad,
2012b). Bollen et al. (2011) measured tension,
depression, anger, vigor, fatigue, and confusion
in tweets. Tumasjan et al. (2010) study Twitter
as a forum for political deliberation. Mohammad
(2012b) developed a classifier to identify emotions
using tweets with emotion word hashtags as la-
beled data. However, none of this work explores
the many semantic roles of emotion.
Semantic role labeling (SRL) identifies seman-
tic arguments and roles with regard to a predicate
</bodyText>
<footnote confidence="0.9683635">
2http://www.purl.org/net/NRCEmotionLexicon
http://csea.phhp.ufl.edu/media/anewmessage.html
</footnote>
<bodyText confidence="0.999886652173913">
in a sentence (Gildea and Jurafsky, 2002; M`arquez
et al., 2008; Palmer et al., 2010). More recently,
there has also been some work on semantic role
labeling of tweets for verb and nominal predi-
cates (Liu et al., 2012; Liu et al., 2011). There
exists work on extracting opinions and the top-
ics of opinions, however most of it if focused on
opinions about product features (Popescu and Et-
zioni, 2005; Zhang et al., 2010; Kessler and Ni-
colov, 2009). For example, (Kessler and Nicolov,
2009) identifies semantic relations between sen-
timent expressions and their targets for car and
digital-camera reviews. However, there is no work
on semantic role labeling of emotions in tweets.
We use many of the ideas developed in the senti-
ment analysis work and apply them to detect the
stimulus of emotions in the electoral tweets data.
Our work here is also related to template filling
in information extraction (IE), for example as de-
fined in MUC (Grishman, 1997), which extracts
information (entities) from a document to fill out
a pre-defined template, such as the date, location,
target, and other information about an event.
</bodyText>
<subsectionHeader confidence="0.675919">
3 Challenges of Semantic Role Labeling
of Emotions in Tweets
</subsectionHeader>
<bodyText confidence="0.9997849">
Semantic role labeling of emotions in tweets poses
certain unique challenges. Firstly, there are many
differences between tweets and linguistically well-
formed texts, such as written news (Liu et al.,
2012; Ritter et al., 2011). Tweets are often less
well-formed—they tend to be colloquial, have
misspellings, and have non-standard tokens. Thus,
methods depending heavily on deep language un-
derstanding such as syntactic parsing (Kim and
Hovy, 2006) are less reliable.
</bodyText>
<page confidence="0.998457">
33
</page>
<bodyText confidence="0.99999112195122">
Secondly, in a traditional SRL system, an ar-
gument frame is a cohesive structure with strong
dependencies between the arguments. Thus it is
often beneficial to develop joint models to identify
the various elements of a frame (Toutanova et al.,
2005). However, these assumptions are less viable
when dealing with emotions in tweets. For exam-
ple, there is no reason to believe that people with a
certain name will have the same emotions towards
the same entities. On the other hand, if we make
use of information beyond the target tweet to inde-
pendently identify the political leanings of a per-
son, then that information can help determine the
person’s emotions towards certain entities. How-
ever, that is beyond the scope of this paper. Thus
we develop independent classifiers for identifying
experiencer, state, and stimulus.
Often, the goal in SRL and IE template filling
is the labeling of text spans in the original text.
However, emotions are often not explicitly stated
in text. Thus we develop a system that assigns an
emotion to a tweet even though that emotion is not
explicitly mentioned. The stimulus of the emo-
tion may also not be mentioned. Consider Happy
to see #4moreyears come into reality. The stimu-
lus of the emotion joy is to see #4moreyears come
into reality. However, the tweet essentially con-
veys the tweeter’s joy towards Barack Obama be-
ing re-elected as president. One may argue that
the true stimulus here is Barack Obama. Thus it is
useful to normalize mentions and resolve the co-
reference, for example, all mentions of Barack H.
Obama, Barack, Obama, and #4moreyears should
be directed to the same entity. Thus, we ground
(in the same sense as in language grounding) the
emotional arguments to the predefined entities.
Through our experiments we show the target of an
emotion in political tweets is often one among a
handful of entities. Thus we develop a classifier to
identify which of these pre-chosen entities is the
stimulus in a given tweet.
</bodyText>
<sectionHeader confidence="0.915651" genericHeader="method">
4 Data Collection and Annotation
</sectionHeader>
<subsectionHeader confidence="0.995742">
4.1 Identifying Electoral Tweets
</subsectionHeader>
<bodyText confidence="0.978106111111111">
We created a corpus of tweets by polling the Twit-
ter Search API, during August and September
2012, for tweets that contained commonly known
hashtags pertaining to the 2012 US presidential
elections. Table 2 shows the query terms we
used. Apart from 21 hashtags, we also collected
tweets with the words Obama, Barack, or Rom-
Table 2: Query terms used to collect tweets per-
taining to the 2012 US presidential elections.
</bodyText>
<equation confidence="0.926458857142857">
#4moreyears #Barack #campaign2012
#dems2012 #democrats #election
#election2012 #gop2012 #gop
#joebiden2012 #mitt2012 #Obama
#ObamaBiden2012 #PaulRyan2012 #president
#president2012 #Romney #republicans
#RomneyRyan2012 #veep2012 #VP2012
</equation>
<subsectionHeader confidence="0.664724">
Barack Obama Romney
</subsectionHeader>
<bodyText confidence="0.983860571428571">
ney. We used these additional terms because they
are names of the two presidential candidates, and
the probability that these words were used to refer
to somebody else in tweets posted in August and
September of 2012 was low.
The Twitter Search API was polled every four
hours to obtain new tweets that matched the query.
Close to one million tweets were collected, which
we will make freely available to the research com-
munity. The query terms which produced the high-
est number of tweets were those involving the
names of the presidential candidates, as well as
#election2012, #campaign, #gop, and #president.
We used the metadata tag “iso language code”
to identify English tweets. Since this tag is not al-
ways accurate, we also discarded tweets that did
not have at least two valid English words. We
used the Roget Thesaurus as the English word in-
ventory.3 This step also helps discard very short
tweets and tweets with a large proportion of mis-
spelled words. Since we were interested in deter-
mining the source and target of emotions in tweets,
we decided to focus on original tweets as opposed
to retweets. We discarded retweets, which can eas-
ily be identified through the presence of RT, rt, or
Rt in the tweet (usually in the beginning of the
post). Finally, there remained close to 170,000
original English tweets.
</bodyText>
<subsectionHeader confidence="0.992194">
4.2 Annotating Emotions by Crowdsourcing
</subsectionHeader>
<bodyText confidence="0.981632875">
We used Amazon’s Mechanical Turk service to
crowdsource the annotation of the electoral tweets.
We randomly selected about 2,000 tweets, each by
a different Twitter user. We set up two question-
naires on Mechanical Turk for the tweets. The first
questionnaire was used to determine the number
of emotions in a tweet and also whether the tweet
was truly relevant to the US politics.
</bodyText>
<footnote confidence="0.77343">
3www.gutenberg.org/ebooks/10681
</footnote>
<page confidence="0.997552">
34
</page>
<tableCaption confidence="0.4421955">
Questionnaire 1: Emotions in the US election tweets
Tweet: Mitt Romney is arrogant as hell.
</tableCaption>
<bodyText confidence="0.455394">
Q1. Which of the following best describes the emotions in
this tweet?
</bodyText>
<listItem confidence="0.999163615384615">
• This tweet expresses or suggests an emotional attitude
or response to something.
• This tweet expresses or suggests two or more contrast-
ing emotional attitudes or responses.
• This tweet has no emotional content.
• There is some emotion here, but the tweet does not give
enough context to determine which emotion it is.
• It is not possible to decide which of the above options
is appropriate.
Q2. Is this tweet about US politics and elections?
• Yes, this tweet is about US politics and elections.
• No, this tweet has nothing to do with US politics or
anybody involved in it.
</listItem>
<bodyText confidence="0.999850375">
These questionnaires are called HITs (Human In-
telligence Tasks) in Mechanical Turk parlance. We
posted 2042 HITs corresponding to 2042 tweets.
We requested responses from at least three anno-
tators for each HIT. The response to a HIT by an
annotator is called an assignment. In Mechanical
Turk, an annotator may provide assignments for as
many HITs as they wish. Thus, even though only
three annotations are requested per HIT, dozens
of annotators contribute assignments for the 2,042
tweets.
The tweets that were marked as having one
emotion were chosen for annotation by the Ques-
tionnaire 2. We requested responses from at least
five annotators for each of these HITs. Below is
an example:
</bodyText>
<subsectionHeader confidence="0.806946">
Questionnaire 2:
Who is feeling what, and towards whom?
</subsectionHeader>
<bodyText confidence="0.648186">
Tweet: Mitt Romney is arrogant as hell.
Q1. Who is feeling or who felt an emotion?
</bodyText>
<listItem confidence="0.9997312">
Q2. What emotion? Choose one of the options from below
that best represents the emotion.
• anger or annoyance or hostility or fury
• anticipation or expectancy or interest
• disgust or dislike
• fear or apprehension or panic or terror
• joy or happiness or elation
• sadness or gloominess or grief or sorrow
• surprise
• trust or like
</listItem>
<tableCaption confidence="0.773013">
Table 3: Questionnaire 1: Percentage of tweets
</tableCaption>
<bodyText confidence="0.9957912">
in each category of Q1. Only those tweets that
were annotated by at least two annotators were in-
cluded. A tweet belongs to category X if it is an-
notated with X more often than all other categories
combined. There were 1889 such tweets in total.
</bodyText>
<figure confidence="0.589102">
Percentage
of tweets
suggests an emotional attitude 87.98
suggests two contrasting attitudes 2.22
no emotional content 8.21
some emotion; not enough context 1.32
unknown; not enough context 0.26
all 100.0
Q3. Towards whom or what?
</figure>
<bodyText confidence="0.9866332">
After performing a small pilot annotation
effort, we realized that the stimulus in most of
the electoral tweets was one among a handful
of entities. Thus we reformulated question 3 as
shown below:
</bodyText>
<listItem confidence="0.99224">
Q3. What best describes the target of the emotion?
• Barack Obama and/or Joe Biden
• Mitt Romney and/or Paul Ryan
• Some other individual
• Democratic party, democrats, or DNC
• Republican party, republicans, or RNC
• Some other institution
• Election campaign, election process, or elections
• The target is not specified in the tweet
• None of the above
</listItem>
<subsectionHeader confidence="0.999119">
4.3 Annotation Analyses
</subsectionHeader>
<bodyText confidence="0.999970411764706">
For each annotator and for each question, we cal-
culated the probability with which the annotator
agreed with the response chosen by the majority
of the annotators. We identified poor annotators as
those that had an agreement probability more than
two standard deviations away from the mean. All
annotations by these annotators were discarded.
We determine whether a tweet is to be assigned
a particular category based on strong majority
vote. That is, a tweet belongs to category X if
it was annotated by at least three annotators and
only if at least half of the annotators agreed with
each other. Percentage of tweets in each of the five
categories of Q1 are shown in Table 3. Observe
that the majority category for Q1 is ‘suggests an
emotion’—87.98% of the tweets were identified
as having an emotional attitude.
</bodyText>
<page confidence="0.998108">
35
</page>
<tableCaption confidence="0.610681">
Table 4: Questionnaire 2: Percentage of tweets
</tableCaption>
<bodyText confidence="0.9994592">
in the categories of Q2. Only those tweets that
were annotated by at least three annotators were
included. A tweet belongs to category X if it is
annotated with X more often than all other cate-
gories combined. There were 965 such tweets.
</bodyText>
<table confidence="0.987508181818182">
Emotion Percentage
of tweets
anger 7.41
anticipation 5.01
disgust 47.75
fear 1.98
joy 6.58
sadness 0.83
surprise 6.37
trust 24.03
all 100.00
</table>
<bodyText confidence="0.994961576923077">
Responses to Q2 showed that a large majority
(95.56%) of the tweets were relevant to US pol-
itics and elections. This shows that the hashtags
shown earlier in Table 2 were effective in identify-
ing political tweets.
As mentioned earlier, only those tweets that
were marked as having an emotion (with high
agreement) were annotated further through Ques-
tionnaire 2.
Responses to Q1 of Questionnaire 2 revealed
that in the vast majority of the cases (99.825%),
the tweets contains emotions of the tweeter. The
data did include some tweets that referred to emo-
tions of others such as Romney, GOP, and pres-
ident, but these instances are rare. Tables 4 and
5 give the distributions of the various options for
Questions 2, and 3 of Questionnaire 2. Table 4
shows that disgust (49.32%) is by far the most
dominant emotion in the tweets of 2012 US pres-
idential elections. The next most prominent emo-
tion is that of trust (23.73%). About 61% of the
tweets convey negative emotions towards some-
one or something. Table 5 shows that the stimulus
of emotions was often one of the two presidential
candidates (close to 55% of the time)—Obama:
29.90%, Romney: 24.87%.
</bodyText>
<subsubsectionHeader confidence="0.705134">
4.3.1 Inter-Annotator Agreement
</subsubsectionHeader>
<bodyText confidence="0.99924525">
We calculated agreement statistics on the full set
of annotations, and not just on the annotations with
a strong majority as described in the previous sec-
tion. Table 6 shows inter-annotator agreement
(IAA) for the questions—the average percentage of
times two annotators agree with each other. An-
other way to gauge agreement is by calculating
the average probability with which an annotator
</bodyText>
<tableCaption confidence="0.805607">
Table 5: Questionnaire 2: Percentage of tweets in
the categories of Q3. A tweet belongs to category
X if it is annotated with X more often than all other
categories combined. There were 973 such tweets.
</tableCaption>
<table confidence="0.999650166666667">
Whom Percentage
of tweets
Barack Obama and/or Joe Biden 29.90
Mitt Romney and/or Paul Ryan 24.87
Some other individual 5.03
Democratic party, democrats, or DNC 2.46
Republican party, republicans, or RNC 8.42
Some other institution 1.23
Election campaign or process 4.93
The target is not specified in the tweet 1.95
None of the above 21.17
all 100.00
</table>
<tableCaption confidence="0.917579666666667">
Table 6: Agreement statistics: inter-annotator
agreement (IAA) and average probability of
choosing the majority class (APMS).
</tableCaption>
<table confidence="0.998928125">
IAA APMS
Questionnaire 1:
Q1 78.02 0.845
Q2 96.76 0.974
Questionnaire 2:
Q1 52.95 0.731
Q2 59.59 0.736
Q3 44.47 0.641
</table>
<bodyText confidence="0.999579384615385">
picks the majority class. The last column in Ta-
ble 6 shows the average probability of picking the
majority class (APMS) by the annotators (higher
numbers indicate higher agreement). Observe that
there is high agreement on determining whether a
tweet has an emotion or not, and on determining
whether the tweet is related to the 2012 US pres-
idential elections or not. The questions in Ques-
tionnaire 2 pertaining to the experiencer, state, and
stimulus were less straightforward and tend to re-
quire more context than just the target tweet for
a clear determination, but yet the annotations had
moderate agreement.
</bodyText>
<subsectionHeader confidence="0.997726">
4.4 Access to the data
</subsectionHeader>
<bodyText confidence="0.998597125">
All of the data is made freely available through the
first author’s website:
http://www.purl.org/net/PoliticalTweets2012
It includes: (1) the complete set of tweets collected
from the Twitter API with hashtags shown in Ta-
ble 2, (2) the subset of English tweets, (3) Ques-
tionnaires 1 and 2, (4) and tweets annotated as per
Questionnaires 1 and 2.
</bodyText>
<page confidence="0.996594">
36
</page>
<sectionHeader confidence="0.966246" genericHeader="method">
5 Automatically Detecting Semantic
</sectionHeader>
<subsectionHeader confidence="0.92641">
Roles of Emotions in Tweets
</subsectionHeader>
<bodyText confidence="0.999682148148148">
Since in most instances (99.83%) the experiencer
of emotions in a tweet is the tweeter, we focus
on automatically detecting the other two semantic
roles: the emotional state and the stimulus.
Due to the unique challenges of semantic role
labeling of emotions in tweets described earlier
in the paper, we treat the detection of emotional
state and stimulus as two subtasks for which
we train state-of-the-art support vector machine
(SVM) classifiers. SVM is a learning algorithm
proved to be effective on many classification tasks
and robust on large feature spaces. In our ex-
periments, we exploited several different classi-
fiers and found SVM outperforms others such as
maximum-entropy models (i.e., logistic regres-
sion). We also tested the most popular kernels
such as the polynomial and RBF kernels with dif-
ferent parameters in stratified ten-fold cross val-
idation. We found that a simple linear kernel
yielded the best performance. We used the Lib-
SVM package (Chang and Lin, 2011).
As mentioned earlier, there is fair amount of
work on emotion detection in non-tweet texts
(Boucouvalas, 2002; Holzman and Pottenger,
2003; Ma et al., 2005; John et al., 2006; Mihalcea
and Liu, 2006; Genereux and Evans, 2006; Aman
and Szpakowicz, 2007; Tokuhisa et al., 2008;
Neviarouskaya et al., 2009) as well as on tweets
(Kim et al., 2009; Tumasjan et al., 2010; Bollen et
al., 2011; Mohammad, 2012b; Choudhury et al.,
2012; Wang et al., 2012). In the experiments be-
low we draw from various successfully used fea-
tures described in these papers. More specifically,
the system we use builds on the classifier and fea-
tures used in two previous systems: (1) the sys-
tem described in (Mohammad, 2012b) which was
shown to perform significantly better than some
other previous systems on the news paper head-
lines corpus and the system described in (Moham-
mad et al., 2013) which ranked first (among 44
participating teams) in a 2013 SemEval competi-
tion on detecting sentiment in tweets).
The goal of the experiments in this section is
to apply a state-of-the art emotion detection sys-
tem on the electoral tweets data. We want to
set up baseline performance for emotion detec-
tion on this new dataset and also validate the data
by showing that automatic classifiers can obtain
results that are greater than random and major-
ity baselines. In Section 5.2, we apply the SVM
classifier and various features for the first time on
the task of detecting the stimulus of an emotion in
tweets. In each experiment, we report results of
ten-fold stratified cross-validation.
</bodyText>
<subsectionHeader confidence="0.8494875">
5.1 Detecting emotional state
5.1.1 Features
</subsectionHeader>
<bodyText confidence="0.999587268292683">
We included the following features for detecting
emotional state in tweets.
Word n-grams: We included unigrams (single
words) and bigrams (two-word sequences) into
our feature set. All words were stemmed with
Porter’s stemmer (Porter, 1980).
Punctuations: number of contiguous sequences of
exclamation marks, question marks, or a combina-
tion of them.
Elongated words: the number of words with the
final character repeated 3 or more times (soooo,
mannnnnn, etc). (Elongated words have been used
similarly in (Brody and Diakopoulos, 2011).)
Emoticons: presence/absence of positive and neg-
ative emoticons. The emoticon and its polar-
ity were determined through a regular expres-
sion adopted from Christopher Potts’ tokenizing
script.4
Emotion Lexicons: We used the NRC word–
emotion association lexicon (Mohammad and Tur-
ney, 2010) to check if a tweet contains emo-
tional words. The lexicon contains human anno-
tations of emotion associations for about 14,200
word types. The annotation includes whether
a word is positive or negative (sentiments), and
whether it is associated with the eight basic emo-
tions (joy, sadness, anger, fear, surprise, antici-
pation, trust, and disgust). If a tweet has three
words that have associations with emotion joy,
then the LexEmo emo joy feature takes a value
of 3. We also counted the number of words
with regard to the Osgood’s (Osgood et al., 1957)
semantic differential categories (LexOsg) built
for Wordnet (LexOsg wn) and General Inquirer
(LexOsg gi). To reduce noise, we only consid-
ered the words that have an adjective or adverb
sense in Wordnet.
Negation features: We examined tweets to deter-
mine whether they contained negators such as no,
not, and shouldn’t. An additional feature deter-
mined whether the negator was located close to an
</bodyText>
<footnote confidence="0.958108">
4http://sentiment.christopherpotts.net/tokenizing.html
</footnote>
<page confidence="0.999667">
37
</page>
<tableCaption confidence="0.992202">
Table 7: Results for emotion detection.
</tableCaption>
<table confidence="0.6378126">
Accuracy
random baseline 30.26
majority baseline 47.75
automatic SVM system 56.84
upper bound 69.80
</table>
<tableCaption confidence="0.923946">
Table 8: The accuracies obtained with one of the
</tableCaption>
<bodyText confidence="0.994519875">
feature groups removed. The number in brackets
is the difference with the all features score. The
biggest drop is shown in bold.
emotion word (as determined by the emotion lex-
icon) in the tweet and in the dependency parse of
the tweet. The list of negation words was adopted
from Christopher Potts’ sentiment tutorial.5
Position features: We included a set of position
features to capture whether the feature terms de-
scribed above appeared at the beginning or the end
of the tweet. For example, if one of the first five
terms in a tweet is a joy word, then the feature
LexEmo joy begin was triggered.
Combined features Though non-linear models
like SVM (with non-linear kernels) can cap-
ture interactions between features, we explic-
itly combined some of our features. For ex-
ample, we concatenated all emotion categories
found in a given tweet. If the tweet contained
both surprise and disgust words, a binary feature
“LexEmo surprise disgust” was triggered. Also,
if a tweet contained more than one joy word
and no other emotion words, then the feature
LexEmo joy only was triggered.
</bodyText>
<sectionHeader confidence="0.701038" genericHeader="evaluation">
5.1.2 Results
</sectionHeader>
<bodyText confidence="0.99800275">
Table 7 shows the results. We included two base-
lines here: the random baseline corresponds to a
system that randomly guesses the emotion of a
tweet, whereas the majority baseline assigns all
</bodyText>
<footnote confidence="0.707942">
5http://sentiment.christopherpotts.net/lingstruc.html
</footnote>
<bodyText confidence="0.999930975">
tweets to the majority category (disgust). Since
the data is significantly skewed towards disgust,
the majority baseline is relative high.
The automatic system obtained by the classi-
fier in identifying the emotions (56.84), which is
significantly higher than the majority baseline. It
should be noted that the highest scores in the Se-
mEval 2013 task of detecting sentiment analysis of
tweets was around 69% (Mohammad et al., 2013).
That task even though related involved only three
classes (positive, negative, and neutral). Thus it is
not surprising that for an 8-way classification task,
the performance is somewhat lower.
The upper bound of the task here is not 100%—
human annotators do not always agree with each
other. To estimate the upper bound we can expect
an automatic system to achieve, for each tweet we
randomly sampled an human annotation from its
multiple annotations and treated it as a system out-
put. We compare it with the majority category
chosen from the remaining human annotations for
that tweet. Such sampling is conducted over all
tweets and then evaluated. The results table shows
this upper bound.
Table 8 shows results of ablation experiments—
the accuracies obtained with one of the feature
groups removed. The higher the drop in per-
formance, the more useful is that feature. Ob-
serve that the ngrams are the most useful fea-
tures, followed by the emotion lexicons. Most of
the gain from ngrams come through word ngrams,
but character ngrams provide small gains as well.
Both the manual and automatic sentiment lexi-
cons were found to be useful to a similar degree.
Paying attention to negation was also beneficial,
whereas emotional encodings such as elongated
words, emoticons, and punctuations did not help
much. It is possible that much of the discrimi-
nating information they might have is already pro-
vided by unigram and character ngram features.
</bodyText>
<subsectionHeader confidence="0.999834">
5.2 Detecting emotion stimulus
</subsectionHeader>
<bodyText confidence="0.999781777777778">
As discussed earlier, instead of detecting and la-
beling the original text spans, we ground the emo-
tion stimulus directly to the predefined entities.
This allows us to circumvent mention detection
and co-reference resolution on linguistically less
well-formed text. We treat the problem as a classi-
fication task, in which we classify a tweet into one
of the categories defined in Table 5. We believe
that a similar approach is also possible in other
</bodyText>
<figure confidence="0.457907">
Experiment
all features
all - ngrams
all - word ngrams
all - char. ngrams
all - lexicons
all - manual lex.
all - auto lex.
all - negation
all - encodings (elongated words, emoticons, punctns.,
uppercase) 56.82 -0.02
Accuracy Difference from
all features
56.84 0
53.35 -3.49
54.44 -2.40
56.32 -0.52
54.34 -2.50
55.17 -1.67
55.38 -1.46
55.80 -1.04
</figure>
<page confidence="0.990981">
38
</page>
<tableCaption confidence="0.989998">
Table 9: Results for detecting stimulus.
</tableCaption>
<table confidence="0.997242833333333">
P R F
16.45 20.87 18.39
34.45 38.00 36.14
43.47 55.15 48.62
57.30 59.32 58.30
82.87 81.36 82.11
</table>
<bodyText confidence="0.625250333333333">
domains such as natural disaster tweets and epi-
demic surveillance tweets. We perform a ten-fold
stratified cross-validation.
</bodyText>
<subsectionHeader confidence="0.53715">
5.2.1 Features
</subsectionHeader>
<bodyText confidence="0.998912733333333">
We used the features below for detecting emotion
stimulus:
Word ngrams: Same as described earlier for
emotional state.
Lexical features: We collected lexicons that
contain a variety of words and phrases describing
the categories in Table 5. For example, the Re-
publican party may be called as “gop” or “Grand
Old Party”; all such words or phrases are all put
into the lexicon called “republican”. We counted
how many words in a given tweet are from each of
these lexicons.
Hashtag features: Hashtags related to the U.S.
election were collected. We organized them into
different categories and use them to further smooth
the sparseness. For example, “#4moreyear” and
“#obama” are put into the same hashtag lexicon
and any occurrence of such hashtags in a tweet
triggers the feature “hashtag obama generalized”,
indicating that this is a general version of hashtag
related to president Barack Obama.
Position features: Same as described earlier for
emotional state.
Combined features As discussed earlier, we ex-
plicitly combined some of the above features. For
example, we first concatenate all lexicon and hash-
tag categories found in a given tweet—if the tweet
contains both the general hashtag of “obama”
and “romney”, a binary feature “Hashtag general
obama romney” takes the value of 1.
</bodyText>
<sectionHeader confidence="0.621288" genericHeader="evaluation">
5.2.2 Results
</sectionHeader>
<bodyText confidence="0.999934260869565">
Table 9 shows the results obtained by the system.
Overall, the system obtains an F-measure of 58.30.
The table also shows upper-bound and baselines
calculated just as described earlier for the emo-
tional state category. We added results for an
additional baseline, rule-based system, here that
chose the stimulus to be: Obama if the tweet had
the terms obama or #obama; Romney if the tweet
had the terms romney or #romney; Republicans if
the tweet had the terms republican, republicans,
or #republicans; Democrats if the tweet had the
terms democrats, democrat, or #democrats; and
Campaign if the tweet had the terms #election or
#campaign. If two or more of the above rules are
triggered in the same tweet, then a label is chosen
at random. This rule-based system based on hand-
chosen features obtains an F-score of 48.62, show-
ing that there are sufficiently many tweets where
key words alone are not sufficient to disambiguate
the true stimulus. Observe that the SVM-based au-
tomatic system performs markedly better than the
majority baseline and also the rule-based system
baseline.
</bodyText>
<sectionHeader confidence="0.996256" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999955393939394">
In this paper, we framed emotion detection as a se-
mantic role labeling problem, focusing not just on
emotional state but also on experiencer and stimu-
lus. We chose tweets about the 2012 US presiden-
tial elections as our target domain. We automati-
cally compiled a large dataset of these tweets using
hashtags, and annotated them first for presence of
emotions, and then for the different semantic roles
of emotions. All of the data is made freely avail-
able.
We found that a large majority of these tweets
(88.1%) carry some emotional attitude towards
someone or something. Further, tweets that con-
vey disgust are twice as prevalent than those that
convey trust. We found that most tweets express
emotions of the tweeter themselves, and the stim-
ulus is often one among a few handful of entities.
We developed a classifier for emotion detection
that obtained an accuracy of 56.84 on an eight-
way classification task. Finally, we showed how
the stimulus identification task can be framed as
a classification task in which our system outper-
forms competitive baselines.
Our future work involves exploring the use of
more tweets from the same user to determine their
political leanings, and use that as an additional fea-
ture in emotion detection. We are also interested in
automatically identifying other semantic roles of
emotions such as degree, reason, and empathy tar-
get (described in Table 1). We believe that a more
sophisticated sentiment analysis applications and
a better understanding of affect require the deter-
mination of semantic roles of emotion.
</bodyText>
<figure confidence="0.7347594">
random baseline
majority baseline
automatic rule-based system
automatic SVM system
upper bound
</figure>
<page confidence="0.997614">
39
</page>
<sectionHeader confidence="0.996065" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999017044247787">
Saima Aman and Stan Szpakowicz. 2007. Identifying
expressions of emotion in text. In Vclav Matouˇsek
and Pavel Mautner, editors, Text, Speech and Dia-
logue, volume 4629 of Lecture Notes in Computer
Science, pages 196–205. Springer Berlin / Heidel-
berg.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL ’98, pages 86–90, Stroudsburg, PA.
Association for Computational Linguistics.
Jerome Bellegarda. 2010. Emotion analysis using la-
tent affective folding and embedding. In Proceed-
ings of the NAACL-HLT 2010 Workshop on Compu-
tational Approaches to Analysis and Generation of
Emotion in Text, Los Angeles, California.
Johan Bollen, Alberto Pepe, and Huina Mao. 2011.
Modeling public mood and emotion: Twitter senti-
ment and socio-economic phenomena. In The Inter-
national AAAI Conference on Weblogs and Social
Media (ICWSM), Barcelona, Spain.
Anthony C. Boucouvalas. 2002. Real time text-
to-emotion engine for expressive internet commu-
nication. Emerging Communication: Studies on
New Technologies and Practices in Communication,
5:305–318.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
’11, pages 562–570, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1–27:27.
Munmun De Choudhury, Scott Counts, and Michael
Gamon. 2012. Not all moods are created equal! ex-
ploring human emotional states in social media. In
The International AAAI Conference on Weblogs and
Social Media (ICWSM).
M D Conover, J Ratkiewicz, M Francisco, B Gonc,
A Flammini, and F Menczer. 2011a. Political po-
larization on Twitter. Networks, 133(26):89–96.
Michael D Conover, Bruno Goncalves, Jacob
Ratkiewicz, Alessandro Flammini, and Filippo
Menczer. 2011b. Predicting the political alignment
of Twitter users. In IEEE Third International
Conference on Privacy Security Risk and Trust and
IEEE Third International Conference on Social
Computing, pages 192–199. IEEE.
Paul Ekman. 1992. An argument for basic emotions.
Cognition and Emotion, 6(3):169–200.
Virginia Francisco and Pablo Gerv´as. 2006. Auto-
mated mark up of affective information in english
texts. In Petr Sojka, Ivan Kopecek, and Karel Pala,
editors, Text, Speech and Dialogue, volume 4188 of
Lecture Notes in Computer Science, pages 375–382.
Springer Berlin / Heidelberg.
Michel Genereux and Roger Evans. 2006. Distin-
guishing affective states in weblogs. In AAAI-2006
Spring Symposium on Computational Approaches to
Analysing Weblogs, pages 27–29, Stanford, USA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
Jennifer Golbeck and Derek Hansen. 2011. Com-
puting political preference among twitter followers.
In Proceedings of the SIGCHI Conference on Hu-
man Factors in Computing Systems, CHI ’11, pages
1105–1108, New York, NY. ACM.
Ralph Grishman. 1997. Information extraction: Tech-
niques and challenges. In SCIE, pages 10–27.
Lars E. Holzman and William M. Pottenger. 2003.
Classification of emotions in internet chat: An appli-
cation of machine learning using speech phonemes.
Technical report, Leigh University.
David John, Anthony C. Boucouvalas, and Zhe Xu.
2006. Representing emotional momentum within
expressive internet communication. In Proceed-
ings of the 24th IASTED international conference on
Internet and multimedia systems and applications,
pages 183–188, Anaheim, CA. ACTA Press.
Jason S. Kessler and Nicolas Nicolov. 2009. Targeting
sentiment expressions through supervised ranking of
linguistic configurations. In 3rd Int’l AAAI Confer-
ence on Weblogs and Social Media (ICWSM 2009).
S. Kim and E. Hovy. 2006. Extracting opinions, opin-
ion holders, and topics expressed in online news me-
dia text. In Proceedings of the Workshop on Senti-
ment and Subjectivity in Text, pages 1–8.
Elsa Kim, Sam Gilbert, Michael J. Edwards, and Er-
hardt Graeff. 2009. Detecting sadness in 140
characters: Sentiment analysis of mourning Michael
Jackson on twitter.
Zolt´an Kˇovecses. 2003. Metaphor and Emotion: Lan-
guage, Culture, and Body in Human Feeling (Stud-
ies in Emotion and Social Interaction). Cambridge
University Press.
X. Liu, K. Li, M. Zhou, and Z. Xiong. 2011. En-
hancing semantic role labeling for tweets using self-
training. In AAAI.
X. Liu, Z. Fu, F. Wei, and M. Zhou. 2012. Collective
nominal semantic role labeling for tweets. In AAAI.
Chunling Ma, Helmut Prendinger, and Mitsuru
Ishizuka. 2005. Emotion estimation and reasoning
based on affective textual interaction. In J. Tao and
R. W. Picard, editors, First International Conference
on Affective Computing and Intelligent Interaction
(ACII-2005), pages 622–628, Beijing, China.
</reference>
<page confidence="0.96702">
40
</page>
<reference confidence="0.999909474576271">
Benjamin Mandel, Aron Culotta, John Boulahanis,
Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.
2012. A demographic analysis of online sentiment
during Hurricane Irene. In Proceedings of the Sec-
ond Workshop on Language in Social Media, LSM
’12, pages 27–36, Stroudsburg, PA. Association for
Computational Linguistics.
Llu´ıs M`arquez, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic role labeling: an introduction to the special
issue. Computational Linguistics, 34(2):145–159.
Diana Maynard and Adam Funk. 2011. Automatic
detection of political opinions in tweets. gateacuk,
7117:81–92.
Rada Mihalcea and Hugo Liu. 2006. A corpus-
based approach to finding happiness. In AAAI-2006
Spring Symposium on Computational Approaches to
Analysing Weblogs, pages 139–144. AAAI Press.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Us-
ing mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL-HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, LA, California.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the seventh international workshop on Seman-
tic Evaluation Exercises (SemEval-2013), Atlanta,
Georgia, USA.
Saif Mohammad. 2012a. Portable features for clas-
sifying emotional text. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 587–591, Montr´eal,
Canada.
Saif M. Mohammad. 2012b. #Emotional tweets. In
Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics - Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation, SemEval
’12, pages 246–255, Stroudsburg, PA.
Alena Neviarouskaya, Helmut Prendinger, and Mit-
suru Ishizuka. 2009. Compositionality principle in
recognition of fine-grained emotions from text. In
Proceedings of the Third International Conference
on Weblogs and Social Media, pages 278–281, San
Jose, California.
C.E. Osgood, Suci G., and P. Tannenbaum. 1957.
The measurement of meaning. University of Illinois
Press.
Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010. Semantic role labeling. Synthesis Lectures
on Human Language Technologies, 3(1):1–103.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1–2):1–135.
Robert Plutchik. 1980. A general psychoevolutionary
theory of emotion. Emotion: Theory, research, and
experience, 1(3):3–33.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 339–346, Stroudsburg, PA,
USA.
M. Porter. 1980. An algorithm for suffix stripping.
Program, 14:130–137.
Matthew Purver and Stuart Battersby. 2012. Ex-
perimenting with distant supervision for emotion
classification. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association
for Computational Linguistics, EACL ’12, pages
482–491, Stroudsburg, PA. Association for Compu-
tational Linguistics.
A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011.
Named entity recognition in tweets: An experimen-
tal study. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1524–1534.
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-Affect: An affective extension of WordNet.
In Proceedings of the 4th International Conference
on Language Resources and Evaluation (LREC-
2004), pages 1083–1086, Lisbon, Portugal.
Ryoko Tokuhisa, Kentaro Inui, and Yuji Matsumoto.
2008. Emotion classification using massive exam-
ples extracted from the web. In Proceedings of the
22nd International Conference on Computational
Linguistics - Volume 1, COLING ’08, pages 881–
888, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 589–596, Stroudsburg, PA. Association
for Computational Linguistics.
Andranik Tumasjan, Timm O Sprenger, Philipp G
Sandner, and Isabell M Welpe. 2010. Predicting
elections with Twitter: What 140 characters reveal
about political sentiment. Word Journal Of The In-
ternational Linguistic Association, pages 178–185.
Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,
and Amit P. Sheth. 2012. Harnessing twitter
”big data” for automatic emotion identification. In
Proceedings of the 2012 ASE/IEEE International
Conference on Social Computing, SOCIALCOM-
PASSAT ’12, pages 587–592, Washington, DC,
USA. IEEE Computer Society.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O’Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics: Posters, COLING ’10, pages
1462–1470, Stroudsburg, PA, USA. Association for
Computational Linguistics.
</reference>
<page confidence="0.999447">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.855909">
<title confidence="0.999843">Semantic Role Labeling of Emotions in Tweets</title>
<author confidence="0.997027">Saif M Mohammad</author>
<author confidence="0.997027">Xiaodan Zhu</author>
<author confidence="0.997027">Joel</author>
<affiliation confidence="0.992706">National Research Council</affiliation>
<address confidence="0.94695">Ottawa, Ontario, Canada K1A</address>
<abstract confidence="0.994394470588235">Past work on emotion processing has focused solely on detecting emotions, and ignored questions such as ‘who is feeling the emotion (the experiencer)?’ and ‘towards whom is the emotion directed (the stimulus)?’. We automatically compile a large dataset of tweets pertaining to the 2012 US presidential elections, and annotate it not only for emotion but also for the experiencer and the stimulus. We then develop a classifier for detecting emotion that obtains an accuracy of 56.84 on an eight-way classification task. Finally, we show how the stimulus identification task can also be framed as a classification task, obtaining an F-score of 58.30.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Saima Aman</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Identifying expressions of emotion in text.</title>
<date>2007</date>
<booktitle>In Vclav Matouˇsek and Pavel Mautner, editors, Text, Speech and Dialogue,</booktitle>
<volume>4629</volume>
<pages>196--205</pages>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="21943" citStr="Aman and Szpakowicz, 2007" startWordPosition="3553" endWordPosition="3556">erent classifiers and found SVM outperforms others such as maximum-entropy models (i.e., logistic regression). We also tested the most popular kernels such as the polynomial and RBF kernels with different parameters in stratified ten-fold cross validation. We found that a simple linear kernel yielded the best performance. We used the LibSVM package (Chang and Lin, 2011). As mentioned earlier, there is fair amount of work on emotion detection in non-tweet texts (Boucouvalas, 2002; Holzman and Pottenger, 2003; Ma et al., 2005; John et al., 2006; Mihalcea and Liu, 2006; Genereux and Evans, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Neviarouskaya et al., 2009) as well as on tweets (Kim et al., 2009; Tumasjan et al., 2010; Bollen et al., 2011; Mohammad, 2012b; Choudhury et al., 2012; Wang et al., 2012). In the experiments below we draw from various successfully used features described in these papers. More specifically, the system we use builds on the classifier and features used in two previous systems: (1) the system described in (Mohammad, 2012b) which was shown to perform significantly better than some other previous systems on the news paper headlines corpus and the system described in (Mohamm</context>
</contexts>
<marker>Aman, Szpakowicz, 2007</marker>
<rawString>Saima Aman and Stan Szpakowicz. 2007. Identifying expressions of emotion in text. In Vclav Matouˇsek and Pavel Mautner, editors, Text, Speech and Dialogue, volume 4629 of Lecture Notes in Computer Science, pages 196–205. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics -Volume 1, ACL ’98,</booktitle>
<pages>86--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="2588" citStr="Baker et al., 1998" startWordPosition="390" endWordPosition="393"> 2012 US presidential elections as our dataset, since we expect political tweets to be particularly rich in emotions. Further, the dataset will be useful for applications such as determining political alignment of tweeters (Golbeck and Hansen, 2011; Conover et al., 2011b), identifying contentious issues (Maynard and Funk, 2011), detecting the amount of polarization in the electorate (Conover et al., 2011a), and so on. Detecting the who, what, and towards whom of emotions is essentially a semantic role-labeling problem (Gildea and Jurafsky, 2002). The semantic frame for ‘emotions’ in FrameNet (Baker et al., 1998) is shown in Table 1. In this work, we focus on the roles of Experiencer, State, and Stimulus. Note, however, that the state or emotion is often not explicitly present in text. Other roles such as Reason, Degree, and Event are also of significance, and remain suitable avenues for future work. We automatically compile a large dataset of 2012 US presidential elections using a small number of hand-chosen hashtags. Next we annotate the tweets for Experiencer, State, and Stimulus by crowdsourcing to Amazon’s Mechanical Turk.1 We analyze the annotations to determine the distributions of different ty</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics -Volume 1, ACL ’98, pages 86–90, Stroudsburg, PA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome Bellegarda</author>
</authors>
<title>Emotion analysis using latent affective folding and embedding.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<location>Los Angeles, California.</location>
<contexts>
<context position="1117" citStr="Bellegarda, 2010" startWordPosition="165" endWordPosition="166">e 2012 US presidential elections, and annotate it not only for emotion but also for the experiencer and the stimulus. We then develop a classifier for detecting emotion that obtains an accuracy of 56.84 on an eight-way classification task. Finally, we show how the stimulus identification task can also be framed as a classification task, obtaining an F-score of 58.30. 1 Introduction Detecting emotions in text has a number of applications including tracking sentiment towards politicians, movies, and products (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda, 2010), developing more natural text-to-speech systems (Francisco and Gerv´as, 2006), detecting how people use emotion-bearing-words and metaphors to persuade and coerce others (for example, in propaganda) (Kˇovecses, 2003), tracking response to natural disasters (Mandel et al., 2012), and so on. With the rapid proliferation of microblogging, there is growing amount of emotion analysis research on newly available datasets of Twitter posts (Mandel et al., 2012; Purver and Battersby, 2012; Mohammad, 2012b). However, past work has focused solely on detecting emotional state. It has ignored questions su</context>
</contexts>
<marker>Bellegarda, 2010</marker>
<rawString>Jerome Bellegarda. 2010. Emotion analysis using latent affective folding and embedding. In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bollen</author>
<author>Alberto Pepe</author>
<author>Huina Mao</author>
</authors>
<title>Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena.</title>
<date>2011</date>
<booktitle>In The International AAAI Conference on Weblogs and Social</booktitle>
<location>Media (ICWSM), Barcelona,</location>
<contexts>
<context position="6302" citStr="Bollen et al., 2011" startWordPosition="978" endWordPosition="981">1980) argued in favor of eight—Ekman’s six, surprise, and anticipation. Many of the automatic systems use affect lexicons pertaining to these basic emotions such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), WordNet Affect (Strapparava and Valitutti, 2004), and the Affective Norms for English Words.2 Affect lexicons are lists of words and associated emotions. Emotion analysis techniques have been applied to many different kinds of text (Mihalcea and Liu, 2006; Genereux and Evans, 2006; Neviarouskaya et al., 2009; Mohammad, 2012a). More recently there has been work on tweets as well (Bollen et al., 2011; Tumasjan et al., 2010; Mohammad, 2012b). Bollen et al. (2011) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. Tumasjan et al. (2010) study Twitter as a forum for political deliberation. Mohammad (2012b) developed a classifier to identify emotions using tweets with emotion word hashtags as labeled data. However, none of this work explores the many semantic roles of emotion. Semantic role labeling (SRL) identifies semantic arguments and roles with regard to a predicate 2http://www.purl.org/net/NRCEmotionLexicon http://csea.phhp.ufl.edu/media/anewmessage.html in a </context>
<context position="22078" citStr="Bollen et al., 2011" startWordPosition="3578" endWordPosition="3581">r kernels such as the polynomial and RBF kernels with different parameters in stratified ten-fold cross validation. We found that a simple linear kernel yielded the best performance. We used the LibSVM package (Chang and Lin, 2011). As mentioned earlier, there is fair amount of work on emotion detection in non-tweet texts (Boucouvalas, 2002; Holzman and Pottenger, 2003; Ma et al., 2005; John et al., 2006; Mihalcea and Liu, 2006; Genereux and Evans, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Neviarouskaya et al., 2009) as well as on tweets (Kim et al., 2009; Tumasjan et al., 2010; Bollen et al., 2011; Mohammad, 2012b; Choudhury et al., 2012; Wang et al., 2012). In the experiments below we draw from various successfully used features described in these papers. More specifically, the system we use builds on the classifier and features used in two previous systems: (1) the system described in (Mohammad, 2012b) which was shown to perform significantly better than some other previous systems on the news paper headlines corpus and the system described in (Mohammad et al., 2013) which ranked first (among 44 participating teams) in a 2013 SemEval competition on detecting sentiment in tweets). The</context>
</contexts>
<marker>Bollen, Pepe, Mao, 2011</marker>
<rawString>Johan Bollen, Alberto Pepe, and Huina Mao. 2011. Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena. In The International AAAI Conference on Weblogs and Social Media (ICWSM), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony C Boucouvalas</author>
</authors>
<title>Real time textto-emotion engine for expressive internet communication. Emerging Communication:</title>
<date>2002</date>
<booktitle>Studies on New Technologies and Practices in Communication,</booktitle>
<pages>5--305</pages>
<contexts>
<context position="21801" citStr="Boucouvalas, 2002" startWordPosition="3531" endWordPosition="3532"> proved to be effective on many classification tasks and robust on large feature spaces. In our experiments, we exploited several different classifiers and found SVM outperforms others such as maximum-entropy models (i.e., logistic regression). We also tested the most popular kernels such as the polynomial and RBF kernels with different parameters in stratified ten-fold cross validation. We found that a simple linear kernel yielded the best performance. We used the LibSVM package (Chang and Lin, 2011). As mentioned earlier, there is fair amount of work on emotion detection in non-tweet texts (Boucouvalas, 2002; Holzman and Pottenger, 2003; Ma et al., 2005; John et al., 2006; Mihalcea and Liu, 2006; Genereux and Evans, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Neviarouskaya et al., 2009) as well as on tweets (Kim et al., 2009; Tumasjan et al., 2010; Bollen et al., 2011; Mohammad, 2012b; Choudhury et al., 2012; Wang et al., 2012). In the experiments below we draw from various successfully used features described in these papers. More specifically, the system we use builds on the classifier and features used in two previous systems: (1) the system described in (Mohammad, 2012b) which was</context>
</contexts>
<marker>Boucouvalas, 2002</marker>
<rawString>Anthony C. Boucouvalas. 2002. Real time textto-emotion engine for expressive internet communication. Emerging Communication: Studies on New Technologies and Practices in Communication, 5:305–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Nicholas Diakopoulos</author>
</authors>
<title>Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using word lengthening to detect sentiment in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>562--570</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="23830" citStr="Brody and Diakopoulos, 2011" startWordPosition="3861" endWordPosition="3864">sults of ten-fold stratified cross-validation. 5.1 Detecting emotional state 5.1.1 Features We included the following features for detecting emotional state in tweets. Word n-grams: We included unigrams (single words) and bigrams (two-word sequences) into our feature set. All words were stemmed with Porter’s stemmer (Porter, 1980). Punctuations: number of contiguous sequences of exclamation marks, question marks, or a combination of them. Elongated words: the number of words with the final character repeated 3 or more times (soooo, mannnnnn, etc). (Elongated words have been used similarly in (Brody and Diakopoulos, 2011).) Emoticons: presence/absence of positive and negative emoticons. The emoticon and its polarity were determined through a regular expression adopted from Christopher Potts’ tokenizing script.4 Emotion Lexicons: We used the NRC word– emotion association lexicon (Mohammad and Turney, 2010) to check if a tweet contains emotional words. The lexicon contains human annotations of emotion associations for about 14,200 word types. The annotation includes whether a word is positive or negative (sentiments), and whether it is associated with the eight basic emotions (joy, sadness, anger, fear, surprise</context>
</contexts>
<marker>Brody, Diakopoulos, 2011</marker>
<rawString>Samuel Brody and Nicholas Diakopoulos. 2011. Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using word lengthening to detect sentiment in microblogs. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 562–570, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<contexts>
<context position="21690" citStr="Chang and Lin, 2011" startWordPosition="3512" endWordPosition="3515">subtasks for which we train state-of-the-art support vector machine (SVM) classifiers. SVM is a learning algorithm proved to be effective on many classification tasks and robust on large feature spaces. In our experiments, we exploited several different classifiers and found SVM outperforms others such as maximum-entropy models (i.e., logistic regression). We also tested the most popular kernels such as the polynomial and RBF kernels with different parameters in stratified ten-fold cross validation. We found that a simple linear kernel yielded the best performance. We used the LibSVM package (Chang and Lin, 2011). As mentioned earlier, there is fair amount of work on emotion detection in non-tweet texts (Boucouvalas, 2002; Holzman and Pottenger, 2003; Ma et al., 2005; John et al., 2006; Mihalcea and Liu, 2006; Genereux and Evans, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Neviarouskaya et al., 2009) as well as on tweets (Kim et al., 2009; Tumasjan et al., 2010; Bollen et al., 2011; Mohammad, 2012b; Choudhury et al., 2012; Wang et al., 2012). In the experiments below we draw from various successfully used features described in these papers. More specifically, the system we use builds on th</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Munmun De Choudhury</author>
<author>Scott Counts</author>
<author>Michael Gamon</author>
</authors>
<title>Not all moods are created equal! exploring human emotional states in social media.</title>
<date>2012</date>
<booktitle>In The International AAAI Conference on Weblogs and Social Media (ICWSM).</booktitle>
<marker>De Choudhury, Counts, Gamon, 2012</marker>
<rawString>Munmun De Choudhury, Scott Counts, and Michael Gamon. 2012. Not all moods are created equal! exploring human emotional states in social media. In The International AAAI Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Conover</author>
<author>J Ratkiewicz</author>
<author>M Francisco</author>
<author>B Gonc</author>
<author>A Flammini</author>
<author>F Menczer</author>
</authors>
<title>Political polarization on Twitter.</title>
<date>2011</date>
<journal>Networks,</journal>
<volume>133</volume>
<issue>26</issue>
<contexts>
<context position="2239" citStr="Conover et al., 2011" startWordPosition="337" endWordPosition="340">). However, past work has focused solely on detecting emotional state. It has ignored questions such as ‘who is feeling the emotion (the experiencer)?’ and ‘towards whom is the emotion directed (the stimulus)?’. In this paper, we present a system that analyzes tweets to determine who is feeling what emotion, and towards whom. We use tweets from the 2012 US presidential elections as our dataset, since we expect political tweets to be particularly rich in emotions. Further, the dataset will be useful for applications such as determining political alignment of tweeters (Golbeck and Hansen, 2011; Conover et al., 2011b), identifying contentious issues (Maynard and Funk, 2011), detecting the amount of polarization in the electorate (Conover et al., 2011a), and so on. Detecting the who, what, and towards whom of emotions is essentially a semantic role-labeling problem (Gildea and Jurafsky, 2002). The semantic frame for ‘emotions’ in FrameNet (Baker et al., 1998) is shown in Table 1. In this work, we focus on the roles of Experiencer, State, and Stimulus. Note, however, that the state or emotion is often not explicitly present in text. Other roles such as Reason, Degree, and Event are also of significance, an</context>
</contexts>
<marker>Conover, Ratkiewicz, Francisco, Gonc, Flammini, Menczer, 2011</marker>
<rawString>M D Conover, J Ratkiewicz, M Francisco, B Gonc, A Flammini, and F Menczer. 2011a. Political polarization on Twitter. Networks, 133(26):89–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael D Conover</author>
<author>Bruno Goncalves</author>
<author>Jacob Ratkiewicz</author>
<author>Alessandro Flammini</author>
<author>Filippo Menczer</author>
</authors>
<title>Predicting the political alignment of Twitter users.</title>
<date>2011</date>
<booktitle>In IEEE Third International Conference on Privacy Security Risk and Trust and IEEE Third International Conference on Social Computing,</booktitle>
<pages>192--199</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="2239" citStr="Conover et al., 2011" startWordPosition="337" endWordPosition="340">). However, past work has focused solely on detecting emotional state. It has ignored questions such as ‘who is feeling the emotion (the experiencer)?’ and ‘towards whom is the emotion directed (the stimulus)?’. In this paper, we present a system that analyzes tweets to determine who is feeling what emotion, and towards whom. We use tweets from the 2012 US presidential elections as our dataset, since we expect political tweets to be particularly rich in emotions. Further, the dataset will be useful for applications such as determining political alignment of tweeters (Golbeck and Hansen, 2011; Conover et al., 2011b), identifying contentious issues (Maynard and Funk, 2011), detecting the amount of polarization in the electorate (Conover et al., 2011a), and so on. Detecting the who, what, and towards whom of emotions is essentially a semantic role-labeling problem (Gildea and Jurafsky, 2002). The semantic frame for ‘emotions’ in FrameNet (Baker et al., 1998) is shown in Table 1. In this work, we focus on the roles of Experiencer, State, and Stimulus. Note, however, that the state or emotion is often not explicitly present in text. Other roles such as Reason, Degree, and Event are also of significance, an</context>
</contexts>
<marker>Conover, Goncalves, Ratkiewicz, Flammini, Menczer, 2011</marker>
<rawString>Michael D Conover, Bruno Goncalves, Jacob Ratkiewicz, Alessandro Flammini, and Filippo Menczer. 2011b. Predicting the political alignment of Twitter users. In IEEE Third International Conference on Privacy Security Risk and Trust and IEEE Third International Conference on Social Computing, pages 192–199. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Ekman</author>
</authors>
<title>An argument for basic emotions.</title>
<date>1992</date>
<journal>Cognition and Emotion,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="5594" citStr="Ekman (1992)" startWordPosition="869" endWordPosition="870">e individual or individuals with which the Experiencer identifies emotionally. Manner Any way the Experiencer experiences the Stimulus which is not covered by more specific frame elements. Parameter The Parameter is a domain in which the Experiencer experiences the Stimulus. Reason The Reason is the explanation for why the Stimulus evokes a certain emotional response. 2 Related Work Our work here is related to emotion analysis, semantic role labeling (SRL), and information extraction (IE). Much of the past work on emotion detection focuses on emotions argued to be the most basic. For example, Ekman (1992) proposed six basic emotions—joy, sadness, anger, fear, disgust, and surprise. Plutchik (1980) argued in favor of eight—Ekman’s six, surprise, and anticipation. Many of the automatic systems use affect lexicons pertaining to these basic emotions such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), WordNet Affect (Strapparava and Valitutti, 2004), and the Affective Norms for English Words.2 Affect lexicons are lists of words and associated emotions. Emotion analysis techniques have been applied to many different kinds of text (Mihalcea and Liu, 2006; Genereux and Evans, 2006; Neviarousk</context>
</contexts>
<marker>Ekman, 1992</marker>
<rawString>Paul Ekman. 1992. An argument for basic emotions. Cognition and Emotion, 6(3):169–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Virginia Francisco</author>
<author>Pablo Gerv´as</author>
</authors>
<title>Automated mark up of affective information in english texts.</title>
<date>2006</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>4188</volume>
<pages>375--382</pages>
<editor>In Petr Sojka, Ivan Kopecek, and Karel Pala, editors, Text, Speech and Dialogue,</editor>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<marker>Francisco, Gerv´as, 2006</marker>
<rawString>Virginia Francisco and Pablo Gerv´as. 2006. Automated mark up of affective information in english texts. In Petr Sojka, Ivan Kopecek, and Karel Pala, editors, Text, Speech and Dialogue, volume 4188 of Lecture Notes in Computer Science, pages 375–382. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Genereux</author>
<author>Roger Evans</author>
</authors>
<title>Distinguishing affective states in weblogs.</title>
<date>2006</date>
<booktitle>In AAAI-2006 Spring Symposium on Computational Approaches to Analysing Weblogs,</booktitle>
<pages>27--29</pages>
<location>Stanford, USA.</location>
<contexts>
<context position="6182" citStr="Genereux and Evans, 2006" startWordPosition="958" endWordPosition="961">ost basic. For example, Ekman (1992) proposed six basic emotions—joy, sadness, anger, fear, disgust, and surprise. Plutchik (1980) argued in favor of eight—Ekman’s six, surprise, and anticipation. Many of the automatic systems use affect lexicons pertaining to these basic emotions such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), WordNet Affect (Strapparava and Valitutti, 2004), and the Affective Norms for English Words.2 Affect lexicons are lists of words and associated emotions. Emotion analysis techniques have been applied to many different kinds of text (Mihalcea and Liu, 2006; Genereux and Evans, 2006; Neviarouskaya et al., 2009; Mohammad, 2012a). More recently there has been work on tweets as well (Bollen et al., 2011; Tumasjan et al., 2010; Mohammad, 2012b). Bollen et al. (2011) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. Tumasjan et al. (2010) study Twitter as a forum for political deliberation. Mohammad (2012b) developed a classifier to identify emotions using tweets with emotion word hashtags as labeled data. However, none of this work explores the many semantic roles of emotion. Semantic role labeling (SRL) identifies semantic arguments and roles wit</context>
<context position="21916" citStr="Genereux and Evans, 2006" startWordPosition="3549" endWordPosition="3552"> we exploited several different classifiers and found SVM outperforms others such as maximum-entropy models (i.e., logistic regression). We also tested the most popular kernels such as the polynomial and RBF kernels with different parameters in stratified ten-fold cross validation. We found that a simple linear kernel yielded the best performance. We used the LibSVM package (Chang and Lin, 2011). As mentioned earlier, there is fair amount of work on emotion detection in non-tweet texts (Boucouvalas, 2002; Holzman and Pottenger, 2003; Ma et al., 2005; John et al., 2006; Mihalcea and Liu, 2006; Genereux and Evans, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Neviarouskaya et al., 2009) as well as on tweets (Kim et al., 2009; Tumasjan et al., 2010; Bollen et al., 2011; Mohammad, 2012b; Choudhury et al., 2012; Wang et al., 2012). In the experiments below we draw from various successfully used features described in these papers. More specifically, the system we use builds on the classifier and features used in two previous systems: (1) the system described in (Mohammad, 2012b) which was shown to perform significantly better than some other previous systems on the news paper headlines corpus and the </context>
</contexts>
<marker>Genereux, Evans, 2006</marker>
<rawString>Michel Genereux and Roger Evans. 2006. Distinguishing affective states in weblogs. In AAAI-2006 Spring Symposium on Computational Approaches to Analysing Weblogs, pages 27–29, Stanford, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="2520" citStr="Gildea and Jurafsky, 2002" startWordPosition="378" endWordPosition="381">rmine who is feeling what emotion, and towards whom. We use tweets from the 2012 US presidential elections as our dataset, since we expect political tweets to be particularly rich in emotions. Further, the dataset will be useful for applications such as determining political alignment of tweeters (Golbeck and Hansen, 2011; Conover et al., 2011b), identifying contentious issues (Maynard and Funk, 2011), detecting the amount of polarization in the electorate (Conover et al., 2011a), and so on. Detecting the who, what, and towards whom of emotions is essentially a semantic role-labeling problem (Gildea and Jurafsky, 2002). The semantic frame for ‘emotions’ in FrameNet (Baker et al., 1998) is shown in Table 1. In this work, we focus on the roles of Experiencer, State, and Stimulus. Note, however, that the state or emotion is often not explicitly present in text. Other roles such as Reason, Degree, and Event are also of significance, and remain suitable avenues for future work. We automatically compile a large dataset of 2012 US presidential elections using a small number of hand-chosen hashtags. Next we annotate the tweets for Experiencer, State, and Stimulus by crowdsourcing to Amazon’s Mechanical Turk.1 We an</context>
<context position="6937" citStr="Gildea and Jurafsky, 2002" startWordPosition="1064" endWordPosition="1067"> et al., 2010; Mohammad, 2012b). Bollen et al. (2011) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. Tumasjan et al. (2010) study Twitter as a forum for political deliberation. Mohammad (2012b) developed a classifier to identify emotions using tweets with emotion word hashtags as labeled data. However, none of this work explores the many semantic roles of emotion. Semantic role labeling (SRL) identifies semantic arguments and roles with regard to a predicate 2http://www.purl.org/net/NRCEmotionLexicon http://csea.phhp.ufl.edu/media/anewmessage.html in a sentence (Gildea and Jurafsky, 2002; M`arquez et al., 2008; Palmer et al., 2010). More recently, there has also been some work on semantic role labeling of tweets for verb and nominal predicates (Liu et al., 2012; Liu et al., 2011). There exists work on extracting opinions and the topics of opinions, however most of it if focused on opinions about product features (Popescu and Etzioni, 2005; Zhang et al., 2010; Kessler and Nicolov, 2009). For example, (Kessler and Nicolov, 2009) identifies semantic relations between sentiment expressions and their targets for car and digital-camera reviews. However, there is no work on semantic</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Golbeck</author>
<author>Derek Hansen</author>
</authors>
<title>Computing political preference among twitter followers.</title>
<date>2011</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’11,</booktitle>
<pages>1105--1108</pages>
<publisher>ACM.</publisher>
<location>New York, NY.</location>
<contexts>
<context position="2217" citStr="Golbeck and Hansen, 2011" startWordPosition="333" endWordPosition="336">sby, 2012; Mohammad, 2012b). However, past work has focused solely on detecting emotional state. It has ignored questions such as ‘who is feeling the emotion (the experiencer)?’ and ‘towards whom is the emotion directed (the stimulus)?’. In this paper, we present a system that analyzes tweets to determine who is feeling what emotion, and towards whom. We use tweets from the 2012 US presidential elections as our dataset, since we expect political tweets to be particularly rich in emotions. Further, the dataset will be useful for applications such as determining political alignment of tweeters (Golbeck and Hansen, 2011; Conover et al., 2011b), identifying contentious issues (Maynard and Funk, 2011), detecting the amount of polarization in the electorate (Conover et al., 2011a), and so on. Detecting the who, what, and towards whom of emotions is essentially a semantic role-labeling problem (Gildea and Jurafsky, 2002). The semantic frame for ‘emotions’ in FrameNet (Baker et al., 1998) is shown in Table 1. In this work, we focus on the roles of Experiencer, State, and Stimulus. Note, however, that the state or emotion is often not explicitly present in text. Other roles such as Reason, Degree, and Event are al</context>
</contexts>
<marker>Golbeck, Hansen, 2011</marker>
<rawString>Jennifer Golbeck and Derek Hansen. 2011. Computing political preference among twitter followers. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’11, pages 1105–1108, New York, NY. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
</authors>
<title>Information extraction: Techniques and challenges.</title>
<date>1997</date>
<booktitle>In SCIE,</booktitle>
<pages>10--27</pages>
<contexts>
<context position="7849" citStr="Grishman, 1997" startWordPosition="1222" endWordPosition="1223">on opinions about product features (Popescu and Etzioni, 2005; Zhang et al., 2010; Kessler and Nicolov, 2009). For example, (Kessler and Nicolov, 2009) identifies semantic relations between sentiment expressions and their targets for car and digital-camera reviews. However, there is no work on semantic role labeling of emotions in tweets. We use many of the ideas developed in the sentiment analysis work and apply them to detect the stimulus of emotions in the electoral tweets data. Our work here is also related to template filling in information extraction (IE), for example as defined in MUC (Grishman, 1997), which extracts information (entities) from a document to fill out a pre-defined template, such as the date, location, target, and other information about an event. 3 Challenges of Semantic Role Labeling of Emotions in Tweets Semantic role labeling of emotions in tweets poses certain unique challenges. Firstly, there are many differences between tweets and linguistically wellformed texts, such as written news (Liu et al., 2012; Ritter et al., 2011). Tweets are often less well-formed—they tend to be colloquial, have misspellings, and have non-standard tokens. Thus, methods depending heavily on</context>
</contexts>
<marker>Grishman, 1997</marker>
<rawString>Ralph Grishman. 1997. Information extraction: Techniques and challenges. In SCIE, pages 10–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars E Holzman</author>
<author>William M Pottenger</author>
</authors>
<title>Classification of emotions in internet chat: An application of machine learning using speech phonemes.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Leigh University.</institution>
<contexts>
<context position="21830" citStr="Holzman and Pottenger, 2003" startWordPosition="3533" endWordPosition="3536">tive on many classification tasks and robust on large feature spaces. In our experiments, we exploited several different classifiers and found SVM outperforms others such as maximum-entropy models (i.e., logistic regression). We also tested the most popular kernels such as the polynomial and RBF kernels with different parameters in stratified ten-fold cross validation. We found that a simple linear kernel yielded the best performance. We used the LibSVM package (Chang and Lin, 2011). As mentioned earlier, there is fair amount of work on emotion detection in non-tweet texts (Boucouvalas, 2002; Holzman and Pottenger, 2003; Ma et al., 2005; John et al., 2006; Mihalcea and Liu, 2006; Genereux and Evans, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Neviarouskaya et al., 2009) as well as on tweets (Kim et al., 2009; Tumasjan et al., 2010; Bollen et al., 2011; Mohammad, 2012b; Choudhury et al., 2012; Wang et al., 2012). In the experiments below we draw from various successfully used features described in these papers. More specifically, the system we use builds on the classifier and features used in two previous systems: (1) the system described in (Mohammad, 2012b) which was shown to perform significant</context>
</contexts>
<marker>Holzman, Pottenger, 2003</marker>
<rawString>Lars E. Holzman and William M. Pottenger. 2003. Classification of emotions in internet chat: An application of machine learning using speech phonemes. Technical report, Leigh University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David John</author>
<author>Anthony C Boucouvalas</author>
<author>Zhe Xu</author>
</authors>
<title>Representing emotional momentum within expressive internet communication.</title>
<date>2006</date>
<booktitle>In Proceedings of the 24th IASTED international conference on Internet and multimedia systems and applications,</booktitle>
<pages>183--188</pages>
<publisher>ACTA Press.</publisher>
<location>Anaheim, CA.</location>
<contexts>
<context position="21866" citStr="John et al., 2006" startWordPosition="3541" endWordPosition="3544">n large feature spaces. In our experiments, we exploited several different classifiers and found SVM outperforms others such as maximum-entropy models (i.e., logistic regression). We also tested the most popular kernels such as the polynomial and RBF kernels with different parameters in stratified ten-fold cross validation. We found that a simple linear kernel yielded the best performance. We used the LibSVM package (Chang and Lin, 2011). As mentioned earlier, there is fair amount of work on emotion detection in non-tweet texts (Boucouvalas, 2002; Holzman and Pottenger, 2003; Ma et al., 2005; John et al., 2006; Mihalcea and Liu, 2006; Genereux and Evans, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Neviarouskaya et al., 2009) as well as on tweets (Kim et al., 2009; Tumasjan et al., 2010; Bollen et al., 2011; Mohammad, 2012b; Choudhury et al., 2012; Wang et al., 2012). In the experiments below we draw from various successfully used features described in these papers. More specifically, the system we use builds on the classifier and features used in two previous systems: (1) the system described in (Mohammad, 2012b) which was shown to perform significantly better than some other previous s</context>
</contexts>
<marker>John, Boucouvalas, Xu, 2006</marker>
<rawString>David John, Anthony C. Boucouvalas, and Zhe Xu. 2006. Representing emotional momentum within expressive internet communication. In Proceedings of the 24th IASTED international conference on Internet and multimedia systems and applications, pages 183–188, Anaheim, CA. ACTA Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason S Kessler</author>
<author>Nicolas Nicolov</author>
</authors>
<title>Targeting sentiment expressions through supervised ranking of linguistic configurations.</title>
<date>2009</date>
<booktitle>In 3rd Int’l AAAI Conference on Weblogs and Social Media (ICWSM</booktitle>
<contexts>
<context position="7343" citStr="Kessler and Nicolov, 2009" startWordPosition="1136" endWordPosition="1140">c role labeling (SRL) identifies semantic arguments and roles with regard to a predicate 2http://www.purl.org/net/NRCEmotionLexicon http://csea.phhp.ufl.edu/media/anewmessage.html in a sentence (Gildea and Jurafsky, 2002; M`arquez et al., 2008; Palmer et al., 2010). More recently, there has also been some work on semantic role labeling of tweets for verb and nominal predicates (Liu et al., 2012; Liu et al., 2011). There exists work on extracting opinions and the topics of opinions, however most of it if focused on opinions about product features (Popescu and Etzioni, 2005; Zhang et al., 2010; Kessler and Nicolov, 2009). For example, (Kessler and Nicolov, 2009) identifies semantic relations between sentiment expressions and their targets for car and digital-camera reviews. However, there is no work on semantic role labeling of emotions in tweets. We use many of the ideas developed in the sentiment analysis work and apply them to detect the stimulus of emotions in the electoral tweets data. Our work here is also related to template filling in information extraction (IE), for example as defined in MUC (Grishman, 1997), which extracts information (entities) from a document to fill out a pre-defined template, su</context>
</contexts>
<marker>Kessler, Nicolov, 2009</marker>
<rawString>Jason S. Kessler and Nicolas Nicolov. 2009. Targeting sentiment expressions through supervised ranking of linguistic configurations. In 3rd Int’l AAAI Conference on Weblogs and Social Media (ICWSM 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>E Hovy</author>
</authors>
<title>Extracting opinions, opinion holders, and topics expressed in online news media text.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Sentiment and Subjectivity in Text,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="8524" citStr="Kim and Hovy, 2006" startWordPosition="1322" endWordPosition="1325">to fill out a pre-defined template, such as the date, location, target, and other information about an event. 3 Challenges of Semantic Role Labeling of Emotions in Tweets Semantic role labeling of emotions in tweets poses certain unique challenges. Firstly, there are many differences between tweets and linguistically wellformed texts, such as written news (Liu et al., 2012; Ritter et al., 2011). Tweets are often less well-formed—they tend to be colloquial, have misspellings, and have non-standard tokens. Thus, methods depending heavily on deep language understanding such as syntactic parsing (Kim and Hovy, 2006) are less reliable. 33 Secondly, in a traditional SRL system, an argument frame is a cohesive structure with strong dependencies between the arguments. Thus it is often beneficial to develop joint models to identify the various elements of a frame (Toutanova et al., 2005). However, these assumptions are less viable when dealing with emotions in tweets. For example, there is no reason to believe that people with a certain name will have the same emotions towards the same entities. On the other hand, if we make use of information beyond the target tweet to independently identify the political le</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>S. Kim and E. Hovy. 2006. Extracting opinions, opinion holders, and topics expressed in online news media text. In Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elsa Kim</author>
<author>Sam Gilbert</author>
<author>Michael J Edwards</author>
<author>Erhardt Graeff</author>
</authors>
<title>Detecting sadness in 140 characters: Sentiment analysis of mourning Michael Jackson on twitter.</title>
<date>2009</date>
<contexts>
<context position="22034" citStr="Kim et al., 2009" startWordPosition="3570" endWordPosition="3573">gression). We also tested the most popular kernels such as the polynomial and RBF kernels with different parameters in stratified ten-fold cross validation. We found that a simple linear kernel yielded the best performance. We used the LibSVM package (Chang and Lin, 2011). As mentioned earlier, there is fair amount of work on emotion detection in non-tweet texts (Boucouvalas, 2002; Holzman and Pottenger, 2003; Ma et al., 2005; John et al., 2006; Mihalcea and Liu, 2006; Genereux and Evans, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Neviarouskaya et al., 2009) as well as on tweets (Kim et al., 2009; Tumasjan et al., 2010; Bollen et al., 2011; Mohammad, 2012b; Choudhury et al., 2012; Wang et al., 2012). In the experiments below we draw from various successfully used features described in these papers. More specifically, the system we use builds on the classifier and features used in two previous systems: (1) the system described in (Mohammad, 2012b) which was shown to perform significantly better than some other previous systems on the news paper headlines corpus and the system described in (Mohammad et al., 2013) which ranked first (among 44 participating teams) in a 2013 SemEval compet</context>
</contexts>
<marker>Kim, Gilbert, Edwards, Graeff, 2009</marker>
<rawString>Elsa Kim, Sam Gilbert, Michael J. Edwards, and Erhardt Graeff. 2009. Detecting sadness in 140 characters: Sentiment analysis of mourning Michael Jackson on twitter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zolt´an Kˇovecses</author>
</authors>
<title>Metaphor and Emotion: Language, Culture, and Body in Human Feeling (Studies in Emotion and Social Interaction).</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<marker>Kˇovecses, 2003</marker>
<rawString>Zolt´an Kˇovecses. 2003. Metaphor and Emotion: Language, Culture, and Body in Human Feeling (Studies in Emotion and Social Interaction). Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Liu</author>
<author>K Li</author>
<author>M Zhou</author>
<author>Z Xiong</author>
</authors>
<title>Enhancing semantic role labeling for tweets using selftraining.</title>
<date>2011</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="7133" citStr="Liu et al., 2011" startWordPosition="1100" endWordPosition="1103">on. Mohammad (2012b) developed a classifier to identify emotions using tweets with emotion word hashtags as labeled data. However, none of this work explores the many semantic roles of emotion. Semantic role labeling (SRL) identifies semantic arguments and roles with regard to a predicate 2http://www.purl.org/net/NRCEmotionLexicon http://csea.phhp.ufl.edu/media/anewmessage.html in a sentence (Gildea and Jurafsky, 2002; M`arquez et al., 2008; Palmer et al., 2010). More recently, there has also been some work on semantic role labeling of tweets for verb and nominal predicates (Liu et al., 2012; Liu et al., 2011). There exists work on extracting opinions and the topics of opinions, however most of it if focused on opinions about product features (Popescu and Etzioni, 2005; Zhang et al., 2010; Kessler and Nicolov, 2009). For example, (Kessler and Nicolov, 2009) identifies semantic relations between sentiment expressions and their targets for car and digital-camera reviews. However, there is no work on semantic role labeling of emotions in tweets. We use many of the ideas developed in the sentiment analysis work and apply them to detect the stimulus of emotions in the electoral tweets data. Our work her</context>
</contexts>
<marker>Liu, Li, Zhou, Xiong, 2011</marker>
<rawString>X. Liu, K. Li, M. Zhou, and Z. Xiong. 2011. Enhancing semantic role labeling for tweets using selftraining. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Liu</author>
<author>Z Fu</author>
<author>F Wei</author>
<author>M Zhou</author>
</authors>
<title>Collective nominal semantic role labeling for tweets.</title>
<date>2012</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="7114" citStr="Liu et al., 2012" startWordPosition="1096" endWordPosition="1099">litical deliberation. Mohammad (2012b) developed a classifier to identify emotions using tweets with emotion word hashtags as labeled data. However, none of this work explores the many semantic roles of emotion. Semantic role labeling (SRL) identifies semantic arguments and roles with regard to a predicate 2http://www.purl.org/net/NRCEmotionLexicon http://csea.phhp.ufl.edu/media/anewmessage.html in a sentence (Gildea and Jurafsky, 2002; M`arquez et al., 2008; Palmer et al., 2010). More recently, there has also been some work on semantic role labeling of tweets for verb and nominal predicates (Liu et al., 2012; Liu et al., 2011). There exists work on extracting opinions and the topics of opinions, however most of it if focused on opinions about product features (Popescu and Etzioni, 2005; Zhang et al., 2010; Kessler and Nicolov, 2009). For example, (Kessler and Nicolov, 2009) identifies semantic relations between sentiment expressions and their targets for car and digital-camera reviews. However, there is no work on semantic role labeling of emotions in tweets. We use many of the ideas developed in the sentiment analysis work and apply them to detect the stimulus of emotions in the electoral tweets</context>
</contexts>
<marker>Liu, Fu, Wei, Zhou, 2012</marker>
<rawString>X. Liu, Z. Fu, F. Wei, and M. Zhou. 2012. Collective nominal semantic role labeling for tweets. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunling Ma</author>
<author>Helmut Prendinger</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Emotion estimation and reasoning based on affective textual interaction.</title>
<date>2005</date>
<booktitle>First International Conference on Affective Computing and Intelligent Interaction (ACII-2005),</booktitle>
<pages>622--628</pages>
<editor>In J. Tao and R. W. Picard, editors,</editor>
<location>Beijing, China.</location>
<contexts>
<context position="21847" citStr="Ma et al., 2005" startWordPosition="3537" endWordPosition="3540">asks and robust on large feature spaces. In our experiments, we exploited several different classifiers and found SVM outperforms others such as maximum-entropy models (i.e., logistic regression). We also tested the most popular kernels such as the polynomial and RBF kernels with different parameters in stratified ten-fold cross validation. We found that a simple linear kernel yielded the best performance. We used the LibSVM package (Chang and Lin, 2011). As mentioned earlier, there is fair amount of work on emotion detection in non-tweet texts (Boucouvalas, 2002; Holzman and Pottenger, 2003; Ma et al., 2005; John et al., 2006; Mihalcea and Liu, 2006; Genereux and Evans, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Neviarouskaya et al., 2009) as well as on tweets (Kim et al., 2009; Tumasjan et al., 2010; Bollen et al., 2011; Mohammad, 2012b; Choudhury et al., 2012; Wang et al., 2012). In the experiments below we draw from various successfully used features described in these papers. More specifically, the system we use builds on the classifier and features used in two previous systems: (1) the system described in (Mohammad, 2012b) which was shown to perform significantly better than so</context>
</contexts>
<marker>Ma, Prendinger, Ishizuka, 2005</marker>
<rawString>Chunling Ma, Helmut Prendinger, and Mitsuru Ishizuka. 2005. Emotion estimation and reasoning based on affective textual interaction. In J. Tao and R. W. Picard, editors, First International Conference on Affective Computing and Intelligent Interaction (ACII-2005), pages 622–628, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Mandel</author>
<author>Aron Culotta</author>
<author>John Boulahanis</author>
<author>Danielle Stark</author>
<author>Bonnie Lewis</author>
<author>Jeremy Rodrigue</author>
</authors>
<title>A demographic analysis of online sentiment during Hurricane Irene.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Language in Social Media, LSM ’12,</booktitle>
<pages>27--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="1396" citStr="Mandel et al., 2012" startWordPosition="201" endWordPosition="204">entification task can also be framed as a classification task, obtaining an F-score of 58.30. 1 Introduction Detecting emotions in text has a number of applications including tracking sentiment towards politicians, movies, and products (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda, 2010), developing more natural text-to-speech systems (Francisco and Gerv´as, 2006), detecting how people use emotion-bearing-words and metaphors to persuade and coerce others (for example, in propaganda) (Kˇovecses, 2003), tracking response to natural disasters (Mandel et al., 2012), and so on. With the rapid proliferation of microblogging, there is growing amount of emotion analysis research on newly available datasets of Twitter posts (Mandel et al., 2012; Purver and Battersby, 2012; Mohammad, 2012b). However, past work has focused solely on detecting emotional state. It has ignored questions such as ‘who is feeling the emotion (the experiencer)?’ and ‘towards whom is the emotion directed (the stimulus)?’. In this paper, we present a system that analyzes tweets to determine who is feeling what emotion, and towards whom. We use tweets from the 2012 US presidential elect</context>
</contexts>
<marker>Mandel, Culotta, Boulahanis, Stark, Lewis, Rodrigue, 2012</marker>
<rawString>Benjamin Mandel, Aron Culotta, John Boulahanis, Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue. 2012. A demographic analysis of online sentiment during Hurricane Irene. In Proceedings of the Second Workshop on Language in Social Media, LSM ’12, pages 27–36, Stroudsburg, PA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs M`arquez</author>
<author>Xavier Carreras</author>
<author>Kenneth C Litkowski</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Semantic role labeling: an introduction to the special issue.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<marker>M`arquez, Carreras, Litkowski, Stevenson, 2008</marker>
<rawString>Llu´ıs M`arquez, Xavier Carreras, Kenneth C. Litkowski, and Suzanne Stevenson. 2008. Semantic role labeling: an introduction to the special issue. Computational Linguistics, 34(2):145–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana Maynard</author>
<author>Adam Funk</author>
</authors>
<title>Automatic detection of political opinions in tweets. gateacuk,</title>
<date>2011</date>
<pages>7117--81</pages>
<contexts>
<context position="2298" citStr="Maynard and Funk, 2011" startWordPosition="344" endWordPosition="347">otional state. It has ignored questions such as ‘who is feeling the emotion (the experiencer)?’ and ‘towards whom is the emotion directed (the stimulus)?’. In this paper, we present a system that analyzes tweets to determine who is feeling what emotion, and towards whom. We use tweets from the 2012 US presidential elections as our dataset, since we expect political tweets to be particularly rich in emotions. Further, the dataset will be useful for applications such as determining political alignment of tweeters (Golbeck and Hansen, 2011; Conover et al., 2011b), identifying contentious issues (Maynard and Funk, 2011), detecting the amount of polarization in the electorate (Conover et al., 2011a), and so on. Detecting the who, what, and towards whom of emotions is essentially a semantic role-labeling problem (Gildea and Jurafsky, 2002). The semantic frame for ‘emotions’ in FrameNet (Baker et al., 1998) is shown in Table 1. In this work, we focus on the roles of Experiencer, State, and Stimulus. Note, however, that the state or emotion is often not explicitly present in text. Other roles such as Reason, Degree, and Event are also of significance, and remain suitable avenues for future work. We automatically</context>
</contexts>
<marker>Maynard, Funk, 2011</marker>
<rawString>Diana Maynard and Adam Funk. 2011. Automatic detection of political opinions in tweets. gateacuk, 7117:81–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Hugo Liu</author>
</authors>
<title>A corpusbased approach to finding happiness.</title>
<date>2006</date>
<booktitle>In AAAI-2006 Spring Symposium on Computational Approaches to Analysing Weblogs,</booktitle>
<pages>139--144</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="6156" citStr="Mihalcea and Liu, 2006" startWordPosition="954" endWordPosition="957">tions argued to be the most basic. For example, Ekman (1992) proposed six basic emotions—joy, sadness, anger, fear, disgust, and surprise. Plutchik (1980) argued in favor of eight—Ekman’s six, surprise, and anticipation. Many of the automatic systems use affect lexicons pertaining to these basic emotions such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), WordNet Affect (Strapparava and Valitutti, 2004), and the Affective Norms for English Words.2 Affect lexicons are lists of words and associated emotions. Emotion analysis techniques have been applied to many different kinds of text (Mihalcea and Liu, 2006; Genereux and Evans, 2006; Neviarouskaya et al., 2009; Mohammad, 2012a). More recently there has been work on tweets as well (Bollen et al., 2011; Tumasjan et al., 2010; Mohammad, 2012b). Bollen et al. (2011) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. Tumasjan et al. (2010) study Twitter as a forum for political deliberation. Mohammad (2012b) developed a classifier to identify emotions using tweets with emotion word hashtags as labeled data. However, none of this work explores the many semantic roles of emotion. Semantic role labeling (SRL) identifies semant</context>
<context position="21890" citStr="Mihalcea and Liu, 2006" startWordPosition="3545" endWordPosition="3548">ces. In our experiments, we exploited several different classifiers and found SVM outperforms others such as maximum-entropy models (i.e., logistic regression). We also tested the most popular kernels such as the polynomial and RBF kernels with different parameters in stratified ten-fold cross validation. We found that a simple linear kernel yielded the best performance. We used the LibSVM package (Chang and Lin, 2011). As mentioned earlier, there is fair amount of work on emotion detection in non-tweet texts (Boucouvalas, 2002; Holzman and Pottenger, 2003; Ma et al., 2005; John et al., 2006; Mihalcea and Liu, 2006; Genereux and Evans, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Neviarouskaya et al., 2009) as well as on tweets (Kim et al., 2009; Tumasjan et al., 2010; Bollen et al., 2011; Mohammad, 2012b; Choudhury et al., 2012; Wang et al., 2012). In the experiments below we draw from various successfully used features described in these papers. More specifically, the system we use builds on the classifier and features used in two previous systems: (1) the system described in (Mohammad, 2012b) which was shown to perform significantly better than some other previous systems on the news paper</context>
</contexts>
<marker>Mihalcea, Liu, 2006</marker>
<rawString>Rada Mihalcea and Hugo Liu. 2006. A corpusbased approach to finding happiness. In AAAI-2006 Spring Symposium on Computational Approaches to Analysing Weblogs, pages 139–144. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<location>LA, California.</location>
<contexts>
<context position="5899" citStr="Mohammad and Turney, 2010" startWordPosition="913" endWordPosition="917">on is the explanation for why the Stimulus evokes a certain emotional response. 2 Related Work Our work here is related to emotion analysis, semantic role labeling (SRL), and information extraction (IE). Much of the past work on emotion detection focuses on emotions argued to be the most basic. For example, Ekman (1992) proposed six basic emotions—joy, sadness, anger, fear, disgust, and surprise. Plutchik (1980) argued in favor of eight—Ekman’s six, surprise, and anticipation. Many of the automatic systems use affect lexicons pertaining to these basic emotions such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), WordNet Affect (Strapparava and Valitutti, 2004), and the Affective Norms for English Words.2 Affect lexicons are lists of words and associated emotions. Emotion analysis techniques have been applied to many different kinds of text (Mihalcea and Liu, 2006; Genereux and Evans, 2006; Neviarouskaya et al., 2009; Mohammad, 2012a). More recently there has been work on tweets as well (Bollen et al., 2011; Tumasjan et al., 2010; Mohammad, 2012b). Bollen et al. (2011) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. Tumasjan et al. (2010) study Twitter as a forum for pol</context>
<context position="24119" citStr="Mohammad and Turney, 2010" startWordPosition="3902" endWordPosition="3906">mmed with Porter’s stemmer (Porter, 1980). Punctuations: number of contiguous sequences of exclamation marks, question marks, or a combination of them. Elongated words: the number of words with the final character repeated 3 or more times (soooo, mannnnnn, etc). (Elongated words have been used similarly in (Brody and Diakopoulos, 2011).) Emoticons: presence/absence of positive and negative emoticons. The emoticon and its polarity were determined through a regular expression adopted from Christopher Potts’ tokenizing script.4 Emotion Lexicons: We used the NRC word– emotion association lexicon (Mohammad and Turney, 2010) to check if a tweet contains emotional words. The lexicon contains human annotations of emotion associations for about 14,200 word types. The annotation includes whether a word is positive or negative (sentiments), and whether it is associated with the eight basic emotions (joy, sadness, anger, fear, surprise, anticipation, trust, and disgust). If a tweet has three words that have associations with emotion joy, then the LexEmo emo joy feature takes a value of 3. We also counted the number of words with regard to the Osgood’s (Osgood et al., 1957) semantic differential categories (LexOsg) buil</context>
</contexts>
<marker>Mohammad, Turney, 2010</marker>
<rawString>Saif M. Mohammad and Peter D. Turney. 2010. Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon. In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, LA, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>Nrc-canada: Building the state-of-theart in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013),</booktitle>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="22559" citStr="Mohammad et al., 2013" startWordPosition="3659" endWordPosition="3663">, 2007; Tokuhisa et al., 2008; Neviarouskaya et al., 2009) as well as on tweets (Kim et al., 2009; Tumasjan et al., 2010; Bollen et al., 2011; Mohammad, 2012b; Choudhury et al., 2012; Wang et al., 2012). In the experiments below we draw from various successfully used features described in these papers. More specifically, the system we use builds on the classifier and features used in two previous systems: (1) the system described in (Mohammad, 2012b) which was shown to perform significantly better than some other previous systems on the news paper headlines corpus and the system described in (Mohammad et al., 2013) which ranked first (among 44 participating teams) in a 2013 SemEval competition on detecting sentiment in tweets). The goal of the experiments in this section is to apply a state-of-the art emotion detection system on the electoral tweets data. We want to set up baseline performance for emotion detection on this new dataset and also validate the data by showing that automatic classifiers can obtain results that are greater than random and majority baselines. In Section 5.2, we apply the SVM classifier and various features for the first time on the task of detecting the stimulus of an emotion </context>
<context position="27087" citStr="Mohammad et al., 2013" startWordPosition="4377" endWordPosition="4380">here: the random baseline corresponds to a system that randomly guesses the emotion of a tweet, whereas the majority baseline assigns all 5http://sentiment.christopherpotts.net/lingstruc.html tweets to the majority category (disgust). Since the data is significantly skewed towards disgust, the majority baseline is relative high. The automatic system obtained by the classifier in identifying the emotions (56.84), which is significantly higher than the majority baseline. It should be noted that the highest scores in the SemEval 2013 task of detecting sentiment analysis of tweets was around 69% (Mohammad et al., 2013). That task even though related involved only three classes (positive, negative, and neutral). Thus it is not surprising that for an 8-way classification task, the performance is somewhat lower. The upper bound of the task here is not 100%— human annotators do not always agree with each other. To estimate the upper bound we can expect an automatic system to achieve, for each tweet we randomly sampled an human annotation from its multiple annotations and treated it as a system output. We compare it with the majority category chosen from the remaining human annotations for that tweet. Such sampl</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. Nrc-canada: Building the state-of-theart in sentiment analysis of tweets. In Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013), Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
</authors>
<title>Portable features for classifying emotional text.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>587--591</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="1618" citStr="Mohammad, 2012" startWordPosition="238" endWordPosition="239">roducts (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda, 2010), developing more natural text-to-speech systems (Francisco and Gerv´as, 2006), detecting how people use emotion-bearing-words and metaphors to persuade and coerce others (for example, in propaganda) (Kˇovecses, 2003), tracking response to natural disasters (Mandel et al., 2012), and so on. With the rapid proliferation of microblogging, there is growing amount of emotion analysis research on newly available datasets of Twitter posts (Mandel et al., 2012; Purver and Battersby, 2012; Mohammad, 2012b). However, past work has focused solely on detecting emotional state. It has ignored questions such as ‘who is feeling the emotion (the experiencer)?’ and ‘towards whom is the emotion directed (the stimulus)?’. In this paper, we present a system that analyzes tweets to determine who is feeling what emotion, and towards whom. We use tweets from the 2012 US presidential elections as our dataset, since we expect political tweets to be particularly rich in emotions. Further, the dataset will be useful for applications such as determining political alignment of tweeters (Golbeck and Hansen, 2011;</context>
<context position="6226" citStr="Mohammad, 2012" startWordPosition="966" endWordPosition="967">c emotions—joy, sadness, anger, fear, disgust, and surprise. Plutchik (1980) argued in favor of eight—Ekman’s six, surprise, and anticipation. Many of the automatic systems use affect lexicons pertaining to these basic emotions such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), WordNet Affect (Strapparava and Valitutti, 2004), and the Affective Norms for English Words.2 Affect lexicons are lists of words and associated emotions. Emotion analysis techniques have been applied to many different kinds of text (Mihalcea and Liu, 2006; Genereux and Evans, 2006; Neviarouskaya et al., 2009; Mohammad, 2012a). More recently there has been work on tweets as well (Bollen et al., 2011; Tumasjan et al., 2010; Mohammad, 2012b). Bollen et al. (2011) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. Tumasjan et al. (2010) study Twitter as a forum for political deliberation. Mohammad (2012b) developed a classifier to identify emotions using tweets with emotion word hashtags as labeled data. However, none of this work explores the many semantic roles of emotion. Semantic role labeling (SRL) identifies semantic arguments and roles with regard to a predicate 2http://www.purl.org</context>
<context position="22094" citStr="Mohammad, 2012" startWordPosition="3582" endWordPosition="3583"> polynomial and RBF kernels with different parameters in stratified ten-fold cross validation. We found that a simple linear kernel yielded the best performance. We used the LibSVM package (Chang and Lin, 2011). As mentioned earlier, there is fair amount of work on emotion detection in non-tweet texts (Boucouvalas, 2002; Holzman and Pottenger, 2003; Ma et al., 2005; John et al., 2006; Mihalcea and Liu, 2006; Genereux and Evans, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Neviarouskaya et al., 2009) as well as on tweets (Kim et al., 2009; Tumasjan et al., 2010; Bollen et al., 2011; Mohammad, 2012b; Choudhury et al., 2012; Wang et al., 2012). In the experiments below we draw from various successfully used features described in these papers. More specifically, the system we use builds on the classifier and features used in two previous systems: (1) the system described in (Mohammad, 2012b) which was shown to perform significantly better than some other previous systems on the news paper headlines corpus and the system described in (Mohammad et al., 2013) which ranked first (among 44 participating teams) in a 2013 SemEval competition on detecting sentiment in tweets). The goal of the exp</context>
</contexts>
<marker>Mohammad, 2012</marker>
<rawString>Saif Mohammad. 2012a. Portable features for classifying emotional text. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 587–591, Montr´eal, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>2012b</author>
</authors>
<title>Emotional tweets.</title>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12,</booktitle>
<pages>246--255</pages>
<location>Stroudsburg, PA.</location>
<marker>2012b, </marker>
<rawString>Saif M. Mohammad. 2012b. #Emotional tweets. In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12, pages 246–255, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alena Neviarouskaya</author>
<author>Helmut Prendinger</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Compositionality principle in recognition of fine-grained emotions from text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third International Conference on Weblogs and Social Media,</booktitle>
<pages>278--281</pages>
<location>San Jose, California.</location>
<contexts>
<context position="6210" citStr="Neviarouskaya et al., 2009" startWordPosition="962" endWordPosition="965">man (1992) proposed six basic emotions—joy, sadness, anger, fear, disgust, and surprise. Plutchik (1980) argued in favor of eight—Ekman’s six, surprise, and anticipation. Many of the automatic systems use affect lexicons pertaining to these basic emotions such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), WordNet Affect (Strapparava and Valitutti, 2004), and the Affective Norms for English Words.2 Affect lexicons are lists of words and associated emotions. Emotion analysis techniques have been applied to many different kinds of text (Mihalcea and Liu, 2006; Genereux and Evans, 2006; Neviarouskaya et al., 2009; Mohammad, 2012a). More recently there has been work on tweets as well (Bollen et al., 2011; Tumasjan et al., 2010; Mohammad, 2012b). Bollen et al. (2011) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. Tumasjan et al. (2010) study Twitter as a forum for political deliberation. Mohammad (2012b) developed a classifier to identify emotions using tweets with emotion word hashtags as labeled data. However, none of this work explores the many semantic roles of emotion. Semantic role labeling (SRL) identifies semantic arguments and roles with regard to a predicate 2htt</context>
<context position="21995" citStr="Neviarouskaya et al., 2009" startWordPosition="3561" endWordPosition="3564"> such as maximum-entropy models (i.e., logistic regression). We also tested the most popular kernels such as the polynomial and RBF kernels with different parameters in stratified ten-fold cross validation. We found that a simple linear kernel yielded the best performance. We used the LibSVM package (Chang and Lin, 2011). As mentioned earlier, there is fair amount of work on emotion detection in non-tweet texts (Boucouvalas, 2002; Holzman and Pottenger, 2003; Ma et al., 2005; John et al., 2006; Mihalcea and Liu, 2006; Genereux and Evans, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Neviarouskaya et al., 2009) as well as on tweets (Kim et al., 2009; Tumasjan et al., 2010; Bollen et al., 2011; Mohammad, 2012b; Choudhury et al., 2012; Wang et al., 2012). In the experiments below we draw from various successfully used features described in these papers. More specifically, the system we use builds on the classifier and features used in two previous systems: (1) the system described in (Mohammad, 2012b) which was shown to perform significantly better than some other previous systems on the news paper headlines corpus and the system described in (Mohammad et al., 2013) which ranked first (among 44 partic</context>
</contexts>
<marker>Neviarouskaya, Prendinger, Ishizuka, 2009</marker>
<rawString>Alena Neviarouskaya, Helmut Prendinger, and Mitsuru Ishizuka. 2009. Compositionality principle in recognition of fine-grained emotions from text. In Proceedings of the Third International Conference on Weblogs and Social Media, pages 278–281, San Jose, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Osgood</author>
<author>G Suci</author>
<author>P Tannenbaum</author>
</authors>
<title>The measurement of meaning.</title>
<date>1957</date>
<publisher>University of Illinois Press.</publisher>
<contexts>
<context position="24672" citStr="Osgood et al., 1957" startWordPosition="3997" endWordPosition="4000">e NRC word– emotion association lexicon (Mohammad and Turney, 2010) to check if a tweet contains emotional words. The lexicon contains human annotations of emotion associations for about 14,200 word types. The annotation includes whether a word is positive or negative (sentiments), and whether it is associated with the eight basic emotions (joy, sadness, anger, fear, surprise, anticipation, trust, and disgust). If a tweet has three words that have associations with emotion joy, then the LexEmo emo joy feature takes a value of 3. We also counted the number of words with regard to the Osgood’s (Osgood et al., 1957) semantic differential categories (LexOsg) built for Wordnet (LexOsg wn) and General Inquirer (LexOsg gi). To reduce noise, we only considered the words that have an adjective or adverb sense in Wordnet. Negation features: We examined tweets to determine whether they contained negators such as no, not, and shouldn’t. An additional feature determined whether the negator was located close to an 4http://sentiment.christopherpotts.net/tokenizing.html 37 Table 7: Results for emotion detection. Accuracy random baseline 30.26 majority baseline 47.75 automatic SVM system 56.84 upper bound 69.80 Table </context>
</contexts>
<marker>Osgood, Suci, Tannenbaum, 1957</marker>
<rawString>C.E. Osgood, Suci G., and P. Tannenbaum. 1957. The measurement of meaning. University of Illinois Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Nianwen Xue</author>
</authors>
<date>2010</date>
<booktitle>Semantic role labeling. Synthesis Lectures on Human Language Technologies,</booktitle>
<pages>3--1</pages>
<contexts>
<context position="6982" citStr="Palmer et al., 2010" startWordPosition="1072" endWordPosition="1075">11) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. Tumasjan et al. (2010) study Twitter as a forum for political deliberation. Mohammad (2012b) developed a classifier to identify emotions using tweets with emotion word hashtags as labeled data. However, none of this work explores the many semantic roles of emotion. Semantic role labeling (SRL) identifies semantic arguments and roles with regard to a predicate 2http://www.purl.org/net/NRCEmotionLexicon http://csea.phhp.ufl.edu/media/anewmessage.html in a sentence (Gildea and Jurafsky, 2002; M`arquez et al., 2008; Palmer et al., 2010). More recently, there has also been some work on semantic role labeling of tweets for verb and nominal predicates (Liu et al., 2012; Liu et al., 2011). There exists work on extracting opinions and the topics of opinions, however most of it if focused on opinions about product features (Popescu and Etzioni, 2005; Zhang et al., 2010; Kessler and Nicolov, 2009). For example, (Kessler and Nicolov, 2009) identifies semantic relations between sentiment expressions and their targets for car and digital-camera reviews. However, there is no work on semantic role labeling of emotions in tweets. We use </context>
</contexts>
<marker>Palmer, Gildea, Xue, 2010</marker>
<rawString>Martha Palmer, Daniel Gildea, and Nianwen Xue. 2010. Semantic role labeling. Synthesis Lectures on Human Language Technologies, 3(1):1–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="1032" citStr="Pang and Lee, 2008" startWordPosition="151" endWordPosition="154">d (the stimulus)?’. We automatically compile a large dataset of tweets pertaining to the 2012 US presidential elections, and annotate it not only for emotion but also for the experiencer and the stimulus. We then develop a classifier for detecting emotion that obtains an accuracy of 56.84 on an eight-way classification task. Finally, we show how the stimulus identification task can also be framed as a classification task, obtaining an F-score of 58.30. 1 Introduction Detecting emotions in text has a number of applications including tracking sentiment towards politicians, movies, and products (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda, 2010), developing more natural text-to-speech systems (Francisco and Gerv´as, 2006), detecting how people use emotion-bearing-words and metaphors to persuade and coerce others (for example, in propaganda) (Kˇovecses, 2003), tracking response to natural disasters (Mandel et al., 2012), and so on. With the rapid proliferation of microblogging, there is growing amount of emotion analysis research on newly available datasets of Twitter posts (Mandel et al., 2012; Purver and Battersby, 2012; Mohammad, 2012b). However, p</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1–2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Plutchik</author>
</authors>
<title>A general psychoevolutionary theory of emotion. Emotion: Theory, research, and experience,</title>
<date>1980</date>
<pages>1--3</pages>
<contexts>
<context position="5688" citStr="Plutchik (1980)" startWordPosition="882" endWordPosition="883">ay the Experiencer experiences the Stimulus which is not covered by more specific frame elements. Parameter The Parameter is a domain in which the Experiencer experiences the Stimulus. Reason The Reason is the explanation for why the Stimulus evokes a certain emotional response. 2 Related Work Our work here is related to emotion analysis, semantic role labeling (SRL), and information extraction (IE). Much of the past work on emotion detection focuses on emotions argued to be the most basic. For example, Ekman (1992) proposed six basic emotions—joy, sadness, anger, fear, disgust, and surprise. Plutchik (1980) argued in favor of eight—Ekman’s six, surprise, and anticipation. Many of the automatic systems use affect lexicons pertaining to these basic emotions such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), WordNet Affect (Strapparava and Valitutti, 2004), and the Affective Norms for English Words.2 Affect lexicons are lists of words and associated emotions. Emotion analysis techniques have been applied to many different kinds of text (Mihalcea and Liu, 2006; Genereux and Evans, 2006; Neviarouskaya et al., 2009; Mohammad, 2012a). More recently there has been work on tweets as well (Bolle</context>
</contexts>
<marker>Plutchik, 1980</marker>
<rawString>Robert Plutchik. 1980. A general psychoevolutionary theory of emotion. Emotion: Theory, research, and experience, 1(3):3–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>339--346</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7295" citStr="Popescu and Etzioni, 2005" startWordPosition="1127" endWordPosition="1131">res the many semantic roles of emotion. Semantic role labeling (SRL) identifies semantic arguments and roles with regard to a predicate 2http://www.purl.org/net/NRCEmotionLexicon http://csea.phhp.ufl.edu/media/anewmessage.html in a sentence (Gildea and Jurafsky, 2002; M`arquez et al., 2008; Palmer et al., 2010). More recently, there has also been some work on semantic role labeling of tweets for verb and nominal predicates (Liu et al., 2012; Liu et al., 2011). There exists work on extracting opinions and the topics of opinions, however most of it if focused on opinions about product features (Popescu and Etzioni, 2005; Zhang et al., 2010; Kessler and Nicolov, 2009). For example, (Kessler and Nicolov, 2009) identifies semantic relations between sentiment expressions and their targets for car and digital-camera reviews. However, there is no work on semantic role labeling of emotions in tweets. We use many of the ideas developed in the sentiment analysis work and apply them to detect the stimulus of emotions in the electoral tweets data. Our work here is also related to template filling in information extraction (IE), for example as defined in MUC (Grishman, 1997), which extracts information (entities) from a</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 339–346, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<tech>Program,</tech>
<pages>14--130</pages>
<contexts>
<context position="23534" citStr="Porter, 1980" startWordPosition="3818" endWordPosition="3819">ng that automatic classifiers can obtain results that are greater than random and majority baselines. In Section 5.2, we apply the SVM classifier and various features for the first time on the task of detecting the stimulus of an emotion in tweets. In each experiment, we report results of ten-fold stratified cross-validation. 5.1 Detecting emotional state 5.1.1 Features We included the following features for detecting emotional state in tweets. Word n-grams: We included unigrams (single words) and bigrams (two-word sequences) into our feature set. All words were stemmed with Porter’s stemmer (Porter, 1980). Punctuations: number of contiguous sequences of exclamation marks, question marks, or a combination of them. Elongated words: the number of words with the final character repeated 3 or more times (soooo, mannnnnn, etc). (Elongated words have been used similarly in (Brody and Diakopoulos, 2011).) Emoticons: presence/absence of positive and negative emoticons. The emoticon and its polarity were determined through a regular expression adopted from Christopher Potts’ tokenizing script.4 Emotion Lexicons: We used the NRC word– emotion association lexicon (Mohammad and Turney, 2010) to check if a </context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. Porter. 1980. An algorithm for suffix stripping. Program, 14:130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Stuart Battersby</author>
</authors>
<title>Experimenting with distant supervision for emotion classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12,</booktitle>
<pages>482--491</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="1602" citStr="Purver and Battersby, 2012" startWordPosition="234" endWordPosition="237">s politicians, movies, and products (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda, 2010), developing more natural text-to-speech systems (Francisco and Gerv´as, 2006), detecting how people use emotion-bearing-words and metaphors to persuade and coerce others (for example, in propaganda) (Kˇovecses, 2003), tracking response to natural disasters (Mandel et al., 2012), and so on. With the rapid proliferation of microblogging, there is growing amount of emotion analysis research on newly available datasets of Twitter posts (Mandel et al., 2012; Purver and Battersby, 2012; Mohammad, 2012b). However, past work has focused solely on detecting emotional state. It has ignored questions such as ‘who is feeling the emotion (the experiencer)?’ and ‘towards whom is the emotion directed (the stimulus)?’. In this paper, we present a system that analyzes tweets to determine who is feeling what emotion, and towards whom. We use tweets from the 2012 US presidential elections as our dataset, since we expect political tweets to be particularly rich in emotions. Further, the dataset will be useful for applications such as determining political alignment of tweeters (Golbeck a</context>
</contexts>
<marker>Purver, Battersby, 2012</marker>
<rawString>Matthew Purver and Stuart Battersby. 2012. Experimenting with distant supervision for emotion classification. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12, pages 482–491, Stroudsburg, PA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritter</author>
<author>S Clark</author>
<author>Mausam</author>
<author>O Etzioni</author>
</authors>
<title>Named entity recognition in tweets: An experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1524--1534</pages>
<contexts>
<context position="8302" citStr="Ritter et al., 2011" startWordPosition="1290" endWordPosition="1293">emotions in the electoral tweets data. Our work here is also related to template filling in information extraction (IE), for example as defined in MUC (Grishman, 1997), which extracts information (entities) from a document to fill out a pre-defined template, such as the date, location, target, and other information about an event. 3 Challenges of Semantic Role Labeling of Emotions in Tweets Semantic role labeling of emotions in tweets poses certain unique challenges. Firstly, there are many differences between tweets and linguistically wellformed texts, such as written news (Liu et al., 2012; Ritter et al., 2011). Tweets are often less well-formed—they tend to be colloquial, have misspellings, and have non-standard tokens. Thus, methods depending heavily on deep language understanding such as syntactic parsing (Kim and Hovy, 2006) are less reliable. 33 Secondly, in a traditional SRL system, an argument frame is a cohesive structure with strong dependencies between the arguments. Thus it is often beneficial to develop joint models to identify the various elements of a frame (Toutanova et al., 2005). However, these assumptions are less viable when dealing with emotions in tweets. For example, there is n</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011. Named entity recognition in tweets: An experimental study. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1524–1534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Alessandro Valitutti</author>
</authors>
<title>Wordnet-Affect: An affective extension of WordNet.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC2004),</booktitle>
<pages>1083--1086</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="5949" citStr="Strapparava and Valitutti, 2004" startWordPosition="920" endWordPosition="924">vokes a certain emotional response. 2 Related Work Our work here is related to emotion analysis, semantic role labeling (SRL), and information extraction (IE). Much of the past work on emotion detection focuses on emotions argued to be the most basic. For example, Ekman (1992) proposed six basic emotions—joy, sadness, anger, fear, disgust, and surprise. Plutchik (1980) argued in favor of eight—Ekman’s six, surprise, and anticipation. Many of the automatic systems use affect lexicons pertaining to these basic emotions such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), WordNet Affect (Strapparava and Valitutti, 2004), and the Affective Norms for English Words.2 Affect lexicons are lists of words and associated emotions. Emotion analysis techniques have been applied to many different kinds of text (Mihalcea and Liu, 2006; Genereux and Evans, 2006; Neviarouskaya et al., 2009; Mohammad, 2012a). More recently there has been work on tweets as well (Bollen et al., 2011; Tumasjan et al., 2010; Mohammad, 2012b). Bollen et al. (2011) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. Tumasjan et al. (2010) study Twitter as a forum for political deliberation. Mohammad (2012b) developed a </context>
</contexts>
<marker>Strapparava, Valitutti, 2004</marker>
<rawString>Carlo Strapparava and Alessandro Valitutti. 2004. Wordnet-Affect: An affective extension of WordNet. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC2004), pages 1083–1086, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryoko Tokuhisa</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Emotion classification using massive examples extracted from the web.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, COLING ’08,</booktitle>
<pages>881--888</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="21966" citStr="Tokuhisa et al., 2008" startWordPosition="3557" endWordPosition="3560"> SVM outperforms others such as maximum-entropy models (i.e., logistic regression). We also tested the most popular kernels such as the polynomial and RBF kernels with different parameters in stratified ten-fold cross validation. We found that a simple linear kernel yielded the best performance. We used the LibSVM package (Chang and Lin, 2011). As mentioned earlier, there is fair amount of work on emotion detection in non-tweet texts (Boucouvalas, 2002; Holzman and Pottenger, 2003; Ma et al., 2005; John et al., 2006; Mihalcea and Liu, 2006; Genereux and Evans, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Neviarouskaya et al., 2009) as well as on tweets (Kim et al., 2009; Tumasjan et al., 2010; Bollen et al., 2011; Mohammad, 2012b; Choudhury et al., 2012; Wang et al., 2012). In the experiments below we draw from various successfully used features described in these papers. More specifically, the system we use builds on the classifier and features used in two previous systems: (1) the system described in (Mohammad, 2012b) which was shown to perform significantly better than some other previous systems on the news paper headlines corpus and the system described in (Mohammad et al., 2013) which </context>
</contexts>
<marker>Tokuhisa, Inui, Matsumoto, 2008</marker>
<rawString>Ryoko Tokuhisa, Kentaro Inui, and Yuji Matsumoto. 2008. Emotion classification using massive examples extracted from the web. In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, COLING ’08, pages 881– 888, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>589--596</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="8796" citStr="Toutanova et al., 2005" startWordPosition="1367" endWordPosition="1370">many differences between tweets and linguistically wellformed texts, such as written news (Liu et al., 2012; Ritter et al., 2011). Tweets are often less well-formed—they tend to be colloquial, have misspellings, and have non-standard tokens. Thus, methods depending heavily on deep language understanding such as syntactic parsing (Kim and Hovy, 2006) are less reliable. 33 Secondly, in a traditional SRL system, an argument frame is a cohesive structure with strong dependencies between the arguments. Thus it is often beneficial to develop joint models to identify the various elements of a frame (Toutanova et al., 2005). However, these assumptions are less viable when dealing with emotions in tweets. For example, there is no reason to believe that people with a certain name will have the same emotions towards the same entities. On the other hand, if we make use of information beyond the target tweet to independently identify the political leanings of a person, then that information can help determine the person’s emotions towards certain entities. However, that is beyond the scope of this paper. Thus we develop independent classifiers for identifying experiencer, state, and stimulus. Often, the goal in SRL a</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2005. Joint learning improves semantic role labeling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 589–596, Stroudsburg, PA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andranik Tumasjan</author>
<author>Timm O Sprenger</author>
<author>Philipp G Sandner</author>
<author>Isabell M Welpe</author>
</authors>
<title>Predicting elections with Twitter: What 140 characters reveal about political sentiment. Word Journal Of The International Linguistic Association,</title>
<date>2010</date>
<pages>178--185</pages>
<contexts>
<context position="6325" citStr="Tumasjan et al., 2010" startWordPosition="982" endWordPosition="985"> of eight—Ekman’s six, surprise, and anticipation. Many of the automatic systems use affect lexicons pertaining to these basic emotions such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), WordNet Affect (Strapparava and Valitutti, 2004), and the Affective Norms for English Words.2 Affect lexicons are lists of words and associated emotions. Emotion analysis techniques have been applied to many different kinds of text (Mihalcea and Liu, 2006; Genereux and Evans, 2006; Neviarouskaya et al., 2009; Mohammad, 2012a). More recently there has been work on tweets as well (Bollen et al., 2011; Tumasjan et al., 2010; Mohammad, 2012b). Bollen et al. (2011) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. Tumasjan et al. (2010) study Twitter as a forum for political deliberation. Mohammad (2012b) developed a classifier to identify emotions using tweets with emotion word hashtags as labeled data. However, none of this work explores the many semantic roles of emotion. Semantic role labeling (SRL) identifies semantic arguments and roles with regard to a predicate 2http://www.purl.org/net/NRCEmotionLexicon http://csea.phhp.ufl.edu/media/anewmessage.html in a sentence (Gildea and Ju</context>
<context position="22057" citStr="Tumasjan et al., 2010" startWordPosition="3574" endWordPosition="3577"> tested the most popular kernels such as the polynomial and RBF kernels with different parameters in stratified ten-fold cross validation. We found that a simple linear kernel yielded the best performance. We used the LibSVM package (Chang and Lin, 2011). As mentioned earlier, there is fair amount of work on emotion detection in non-tweet texts (Boucouvalas, 2002; Holzman and Pottenger, 2003; Ma et al., 2005; John et al., 2006; Mihalcea and Liu, 2006; Genereux and Evans, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Neviarouskaya et al., 2009) as well as on tweets (Kim et al., 2009; Tumasjan et al., 2010; Bollen et al., 2011; Mohammad, 2012b; Choudhury et al., 2012; Wang et al., 2012). In the experiments below we draw from various successfully used features described in these papers. More specifically, the system we use builds on the classifier and features used in two previous systems: (1) the system described in (Mohammad, 2012b) which was shown to perform significantly better than some other previous systems on the news paper headlines corpus and the system described in (Mohammad et al., 2013) which ranked first (among 44 participating teams) in a 2013 SemEval competition on detecting sent</context>
</contexts>
<marker>Tumasjan, Sprenger, Sandner, Welpe, 2010</marker>
<rawString>Andranik Tumasjan, Timm O Sprenger, Philipp G Sandner, and Isabell M Welpe. 2010. Predicting elections with Twitter: What 140 characters reveal about political sentiment. Word Journal Of The International Linguistic Association, pages 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbo Wang</author>
<author>Lu Chen</author>
<author>Krishnaprasad Thirunarayan</author>
<author>Amit P Sheth</author>
</authors>
<title>Harnessing twitter ”big data” for automatic emotion identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 ASE/IEEE International Conference on Social Computing, SOCIALCOMPASSAT ’12,</booktitle>
<pages>587--592</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="22139" citStr="Wang et al., 2012" startWordPosition="3588" endWordPosition="3591">t parameters in stratified ten-fold cross validation. We found that a simple linear kernel yielded the best performance. We used the LibSVM package (Chang and Lin, 2011). As mentioned earlier, there is fair amount of work on emotion detection in non-tweet texts (Boucouvalas, 2002; Holzman and Pottenger, 2003; Ma et al., 2005; John et al., 2006; Mihalcea and Liu, 2006; Genereux and Evans, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Neviarouskaya et al., 2009) as well as on tweets (Kim et al., 2009; Tumasjan et al., 2010; Bollen et al., 2011; Mohammad, 2012b; Choudhury et al., 2012; Wang et al., 2012). In the experiments below we draw from various successfully used features described in these papers. More specifically, the system we use builds on the classifier and features used in two previous systems: (1) the system described in (Mohammad, 2012b) which was shown to perform significantly better than some other previous systems on the news paper headlines corpus and the system described in (Mohammad et al., 2013) which ranked first (among 44 participating teams) in a 2013 SemEval competition on detecting sentiment in tweets). The goal of the experiments in this section is to apply a state-</context>
</contexts>
<marker>Wang, Chen, Thirunarayan, Sheth, 2012</marker>
<rawString>Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan, and Amit P. Sheth. 2012. Harnessing twitter ”big data” for automatic emotion identification. In Proceedings of the 2012 ASE/IEEE International Conference on Social Computing, SOCIALCOMPASSAT ’12, pages 587–592, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Zhang</author>
<author>Bing Liu</author>
<author>Suk Hwan Lim</author>
<author>Eamonn O’Brien-Strain</author>
</authors>
<title>Extracting and ranking product features in opinion documents.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>1462--1470</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Zhang, Liu, Lim, O’Brien-Strain, 2010</marker>
<rawString>Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn O’Brien-Strain. 2010. Extracting and ranking product features in opinion documents. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 1462–1470, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
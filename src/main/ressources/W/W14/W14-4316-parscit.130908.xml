<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000121">
<title confidence="0.9978705">
Combining Task and Dialogue Streams in
Unsupervised Dialogue Act Models
</title>
<author confidence="0.990982">
Aysu Ezen-Can and Kristy Elizabeth Boyer
</author>
<affiliation confidence="0.8345925">
Department of Computer Science
North Carolina State University
</affiliation>
<email confidence="0.998818">
aezen,keboyer@ncsu.edu
</email>
<sectionHeader confidence="0.99739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99980744">
Unsupervised machine learning ap-
proaches hold great promise for recog-
nizing dialogue acts, but the performance
of these models tends to be much lower
than the accuracies reached by supervised
models. However, some dialogues, such
as task-oriented dialogues with parallel
task streams, hold rich information that
has not yet been leveraged within unsu-
pervised dialogue act models. This paper
investigates incorporating task features
into an unsupervised dialogue act model
trained on a corpus of human tutoring in
introductory computer science. Exper-
imental results show that incorporating
task features and dialogue history fea-
tures significantly improve unsupervised
dialogue act classification, particularly
within a hierarchical framework that gives
prominence to dialogue history. This
work constitutes a step toward building
high-performing unsupervised dialogue
act models that will be used in the next
generation of task-oriented dialogue
systems.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999895924528302">
Dialogue acts represent the underlying intent of ut-
terances (Austin, 1975; Searle, 1969), and consti-
tute a crucial level of representation for dialogue
systems (Sridhar et al., 2009). The task of auto-
matic dialogue act classification has been exten-
sively studied for decades within several domains
including train fares and timetables (Allen et al.,
1995; Core and Allen, 1997; Crook et al., 2009;
Traum, 1999), virtual personal assistants (Chen
and Di Eugenio, 2013), conversational telephone
speech (Stolcke et al., 2000), Wikipedia talk pages
(Ferschke et al., 2012) and as in the case of this
paper, tutorial dialogue (Serafin and Di Eugenio,
2004; Forbes-Riley and Litman, 2005; Boyer et
al., 2011; Dzikovska et al., 2013).
Most of the prior work on dialogue act classi-
fication has depended on manually applying dia-
logue act tags and then leveraging supervised ma-
chine learning (Di Eugenio et al., 2010; Keizer
et al., 2002; Reithinger and Klesen, 1997; Ser-
afin and Di Eugenio, 2004). This process involves
engineering a dialogue act taxonomy (or using an
existing one, though domain-specific phenomena
can be difficult to capture within multi-purpose di-
alogue act taxonomies) and manually annotating
each utterance in the corpus. Then, the tagged
utterances are provided to a supervised machine
learner. This supervised approach can achieve
strong performance, in excess of 75% accuracy
on manual tags, approaching the agreement level
that is sometimes observed between human anno-
tators (Sridhar et al., 2009; Serafin and Di Euge-
nio, 2004; Chen and Di Eugenio, 2013).
However, the supervised approach has several
major drawbacks, including the fact that hand-
crafting dialogue act tagsets and applying them
manually tend to be bottlenecks within the re-
search and design process. To overcome these
drawbacks, the field has recently seen growing
momentum surrounding unsupervised approaches,
which do not require any manual labels during
model training (Crook et al., 2009; Joty et al.,
2011; Lee et al., 2013). A variety of unsupervised
machine learning techniques have been investi-
gated for dialogue act classification, and each line
of investigation has explored which features best
support this goal. However, to date the best per-
forming unsupervised models achieve in the range
of 40% (Rus et al., 2012) to 60% (Joty et al., 2011)
training set accuracy on manual tags, substantially
lower than the mid-70% accuracy (Sridhar et al.,
2009) often achieved on testing sets with super-
vised models.
</bodyText>
<page confidence="0.989855">
113
</page>
<note confidence="0.7308435">
Proceedings of the SIGDIAL 2014 Conference, pages 113–122,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999977466666667">
In order to close this performance gap between
unsupervised and supervised techniques, we sug-
gest that it is crucial to enrich the features available
to unsupervised models. In particular, when a di-
alogue is task-oriented and includes a rich source
of information within a parallel task stream, these
features may substantially boost the ability of an
unsupervised model to distinguish dialogue acts.
For example, in situated dialogue, features rep-
resenting the state of the physical world may
be highly influential for dialogue act modeling
(Grosz and Sidner, 1986).
Human tutorial dialogue, which is the domain
being considered in the current work, often ex-
hibits this structure: the task artifact is external to
the dialogue utterances themselves (in the case of
our work, this artifact is a computer program that
the student is constructing). Task features have
already been shown beneficial for supervised di-
alogue act classification in our domain (Ha et al.,
2012). We hypothesize that including these task
features within an unsupervised model will signif-
icantly improve its performance. In addition, we
hypothesize that including dialogue history as a
prominent feature within an unsupervised model
will provide significant improvement.
This paper represents the first investigation into
combining task and dialogue features within an
unsupervised dialogue act classification model.
First, we discuss representation of these task fea-
tures and dialogue structure features, and compare
these representations within both flat and hierar-
chical clustering approaches. Second, we report
on experiments that demonstrate that the inclusion
of task features significantly improves dialogue
act classification, and that a hierarchical cluster
structure which explicitly captures dialogue his-
tory performs best. Finally, we break down the
model’s performance by dialogue act and investi-
gate which features are most beneficial for distin-
guishing particular acts. These contributions con-
stitute a step toward building high-performing un-
supervised dialogue act models that can be used in
the next generation of task-oriented dialogue sys-
tems.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999977931034483">
There is a rich body of work on dialogue act clas-
sification. Supervised approaches for dialogue act
classification aimed at improving performance by
using several features such as dialogue structure
including position of the turn (Ferschke et al.,
2012), speaker of an utterance (Tavafi et al., 2013),
previous dialogue acts (Kim et al., 2010), lexical
features such as words (Stolcke et al., 2000), syn-
tactic features including part-of-speech tags (Ban-
galore et al., 2008; Marineau et al., 2000), task-
subtask structure (Boyer et al., 2010) acoustic and
prosodic cues (Sridhar et al., 2009; Jurafsky et al.,
1998), and body posture (Ha et al., 2012).
For the growing body of work in unsupervised
dialogue act classification a subset of these fea-
tures have been utilized. The words (Crook et
al., 2009), topic words (Ritter et al., 2010), func-
tion words (Ezen-Can and Boyer, 2013b), begin-
ning portions of utterances (Rus et al., 2012), part-
of-speech tags and dependency trees (Joty et al.,
2011), and state transition probabilities in Markov
models (Lee et al., 2013) are among the list of
features investigated for unsupervised modeling of
dialogue acts. However, the accuracies achieved
by the best of these models are well below the ac-
curacies achieved by supervised techniques. To
improve performance of unsupervised models for
task-oriented dialogue, utilizing a combination of
task and dialogue features is a promising direction.
</bodyText>
<sectionHeader confidence="0.99511" genericHeader="method">
3 Corpus
</sectionHeader>
<bodyText confidence="0.999878041666667">
The task-oriented dialogue corpus used in this
work was collected in a computer-mediated hu-
man tutorial dialogue study. Students (n =
42) and tutors interacted through textual dialogue
within an online learning environment for intro-
ductory Java programming (Ha et al., 2012). The
students were novices, never having programmed
in Java previously. The tutorial dialogue inter-
face consisted of four windows, one describing the
learning task, another where students wrote pro-
gramming code, beneath that the output of either
compiling or executing the program, and finally
the textual dialogue window (Figure 1).
As students and tutors interacted through this
interface, all dialogue messages and keystroke-
level task events were logged to a database. Only
students could compose, compile, and execute the
code, so task actions represent student actions
while dialogue messages were composed by both
participants. The corpus contains six lessons for
each student-tutor pair, of which only the first les-
son was annotated with dialogue act tags (κ=0.80).
This annotated set contains 5,705 utterances
(4,065 tutor and 1,640 student). The average num-
</bodyText>
<page confidence="0.998291">
114
</page>
<table confidence="0.998606625">
Student Dialogue Act Distribution
Answer (A) 39.85
Acknowledgement (ACK) 21.31
Statement (S) 21.20
Question (Q) 15.15
Request for Feedback (RF) 0.98
Clarification (C) 0.79
Other (O) 0.61
</table>
<tableCaption confidence="0.880939">
Table 1: Student dialogue act tags and their fre-
quencies.
</tableCaption>
<figureCaption confidence="0.985799">
Figure 1: The tutorial dialogue interface with four
windows.
</figureCaption>
<bodyText confidence="0.999957967741936">
ber of utterances (both tutor and student) per tutor-
ing session was 116 (min = 70, max = 211). The
average number of tutor utterances per session is
96 (min=44, max=156) whereas for students it is
39 (min=18, max=69) for the annotated set. The
average number of words per utterance for stu-
dents is 4.4 and for tutors it is 5.4. This annotated
set is used in the current analysis for both training
and testing where cross-validation is applied. As
described later, a separate set containing 462 un-
annotated utterances is used as a development set
for determining the number of clusters.
The dialogue stream of this corpus was manu-
ally annotated as part of previous work on super-
vised dialogue act modeling which achieved 69%
accuracy with Conditional Random Fields (Ha et
al., 2012). A brief description of the student di-
alogue act tags, which are the focus of the mod-
els reported in this paper, is shown in Table 1.
The most frequent dialogue act (A) constitutes the
baseline chance (39.85%). In the current work, the
manually applied dialogue act labels are not uti-
lized during model training, but are only used for
evaluation purposes as our models’ accuracies are
reported for manual tags on a held-out test set.
An excerpt from the corpus is shown in Table 2.
Note that the current work focuses on classifying
student dialogue act tags, since in an automated di-
alogue system the tutor moves would be generated
by the system and their dialogue acts tags would
therefore be known.
</bodyText>
<sectionHeader confidence="0.999734" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.964209857142857">
A key issue for dialogue act classification in task-
oriented dialogue involves how to represent dia-
Tutor: ready? [Q]
Student: yep [A]
Tutor moves on to next task
Student: cool [S]
Student compiles and runs the code.
</bodyText>
<subsubsectionHeader confidence="0.837214">
Program output: ‘Hello World’
</subsubsectionHeader>
<bodyText confidence="0.896148217391304">
Tutor: excellent [PF]
Tutor: add a space to make the output look
prettier [DIR]
Student: why doesnt it stop on the next line
in this case? [Q]
Program halts
Tutor: it did [A]
Student runs the program successfully.
Tutor: good. [PF]
Table 2: Excerpt of dialogue from the corpus and
the task action that follows utterances.
logue and task events. This section describes how
features were extracted from the corpus of human
tutorial dialogue.
We use three sets of features: lexical features,
dialogue context features, and task features. The
lexical and dialogue context features are extracted
from the textual dialogue utterances within the
corpus. The task features are extracted from the
interaction traces within the computer-mediated
learning environment and represent a keystroke-
level log of events as students worked toward solv-
ing the computer programming problems.
</bodyText>
<subsectionHeader confidence="0.997492">
4.1 Lexical Features
</subsectionHeader>
<bodyText confidence="0.999896833333333">
Because one of the main goals of our work in the
longer term is to perform automatic dialogue act
classification in real time, we took as a primary
consideration the ability to quickly extract lexical
features. The features utilized in the current in-
vestigation consist only of word unigrams. In ad-
</bodyText>
<page confidence="0.994815">
115
</page>
<bodyText confidence="0.999982454545455">
dition to their ease of extraction, our prior work
has shown that addition of part-of-speech tags and
and syntax features did not significantly improve
the accuracy of supervised dialogue act classifiers
in this domain (Boyer et al., 2010), and these fea-
tures can be time-consuming to extract in real time
(Ha et al., 2012).
The choice to use word unigrams rather than
higher order n-grams is further facilitated by the
fact that our clustering technique leverages the
longest common sub-sequence (LCS) metric to
measure distances between utterances. This met-
ric counts shared sub-sequences of not-necessarily
contiguous words (Hirschberg, 1975). In this way,
the LCS metric provides a flexible way for n-
grams and skip-n-grams to be treated as impor-
tant units within the clustering, while the raw fea-
tures themselves consist only of word unigrams.
(We report on a comparison between LCS and bi-
grams later in the discussion section.) Utilizing
LCS, there exists a distance (1-similarity) value
from each utterance to every other utterance.
</bodyText>
<subsectionHeader confidence="0.969599">
4.2 Dialogue Context Features
</subsectionHeader>
<bodyText confidence="0.991933727272727">
Based on previous work on a similar human tuto-
rial dialogue corpus (Ha et al., 2012), we utilize
four features that provide information about the di-
alogue structure. These features are depicted in
Table 3. Note that our goal within this work is to
classify student dialogue moves, not tutor moves,
because in a dialogue system the tutor’s moves are
system-generated with associated known dialogue
acts.
Feature Description
Utterance The relative position of an
position utterance from the beginning of
the dialogue.
Utterance The number of tokens in the
length utterance, including words and
punctuation.
Previous Author of the previous dialogue
author message (tutor or student) at the
time message sent.
Previous Dialogue act of the previous
tutor tutor utterance.
dialogue act
</bodyText>
<tableCaption confidence="0.8641675">
Table 3: Dialogue context features and their de-
scriptions.
</tableCaption>
<subsectionHeader confidence="0.997986">
4.3 Task Features
</subsectionHeader>
<bodyText confidence="0.999996529411765">
As described previously, the corpus contains two
channels of information: the dialogue utterances,
from which the lexical and dialogue context fea-
tures were extracted, and in addition, the task
stream consisting of student problem-solving ac-
tivities such as authoring code, compiling, and ex-
ecuting the program. The programming activities
of students were logged to a database along with
all of the dialogue events during tutoring.
A set of task features was found to be impor-
tant for dialogue act classification in this domain
in prior work, including most recent programming
action, status of the most recent task activity and
task activity flag representing whether the utter-
ance was preceded by a student’s task activity (Ha
et al., 2012). We expand this set of features as
shown in Table 4.
</bodyText>
<sectionHeader confidence="0.999611" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999286">
The goal of this work is to investigate the im-
pact of including task and dialogue context fea-
tures on unsupervised dialogue act models. We
hypothesize that incorporating task features will
significantly improve the performance of an un-
supervised model, and we also hypothesize that
properly incorporating dialogue context features,
which are at a different granularity than the lex-
ical features extracted from utterances, will sub-
stantially improve model accuracy.
</bodyText>
<subsectionHeader confidence="0.8841805">
5.1 Dialogue Act Modeling With k-medoids
Clustering
</subsectionHeader>
<bodyText confidence="0.998128222222222">
The unsupervised models investigated here use k-
medoids clustering, which is a well-known clus-
tering technique that takes actual data points as
the center of each cluster (Ng and Han, 1994),
in contrast to k-means which may have synthetic
points as centroids. In k-medoids, the centroids
are initially selected and then the algorithm iter-
ates, reassigning data points in each iteration, un-
til the clusters converge. In standard k-medoids
clustering the initial seeds are selected randomly
and then a correct distribution of data points is
identified through the iteration and convergence
process. For dialogue act classification, the in-
fluence of the initial seeds is substantial because
the frequencies across dialogue tags are typically
unbalanced. To overcome this challenge, we use
a greedy seed selection approach similar to the
one used in k-means++ (Arthur and Vassilvitskii,
</bodyText>
<page confidence="0.988683">
116
</page>
<table confidence="0.994156810810811">
Feature Description
prev action Most recent action of the
student (composing a dialogue
utterance, constructing code,
compiling or executing code).
task begin Whether the student utterance is
the first utterance since the
beginning of the subtask.
task stu Whether the student utterance
was preceded by a task event.
task prev tut Task activity flag indicating
whether the closest tutor
utterance in this subtask was
preceded by a task activity.
task status The status of the most recent
coding action (begin, stop,
success, error and input sent).
time elapsed Time elapsed between the
previous tutor message and the
current student utterance.
errors Number of errors in the
student’s latest code.
delta errors Difference in the number of
errors in the task between two
utterances in the same dialogue.
stu # task Number of student dialogue
messages sent within the current
task.
stu # dial Number of student dialogue
messages sent within the current
dialogue.
tut # task Number of tutor dialogue
messages sent within the current
subtask.
tut # dial Number of tutor dialogue
messages sent within the current
dialogue.
</table>
<tableCaption confidence="0.8259855">
Table 4: Task features extracted from student com-
puter programming activities.
</tableCaption>
<bodyText confidence="0.999829714285714">
2007) which selects the first seed randomly and
then greedily chooses seeds that are farthest from
the chosen seeds. The goal of using this approach
in our application is to choose seeds from different
dialogue acts so that the final model achieves good
coverage. Our preliminary experiments demon-
strated that this greedy seed selection combined
with k-medoids outperforms other clustering ap-
proaches including those utilized in our prior work
(Ezen-Can and Boyer, 2013a).
In order to select the number of clusters k,
a subset of the corpus, constituting 25% of the
full corpus (that were not tagged) composed of
462 utterances, was separated as a development
set. First, we examined the coherence of clus-
ters at different values of k using intra-cluster dis-
tances. This technique involves identifying an ‘el-
bow’ where the decrease in intra-cluster distance
becomes less rapid (since adding more clusters can
continue to decrease intra-cluster distance to the
point of overfitting) (Figure 2). The graph sug-
gests an elbow at k=5. Because there may be mul-
tiple elbows in the intra-cluster distance, a sec-
ond method utilizing Bayesian Information Crite-
rion (BIC) was used which penalizes models as
the number of parameters increases. The lower the
BIC value, the better the model is, achieved at k=5
as well.
</bodyText>
<figureCaption confidence="0.598808">
Figure 2: Intra-cluster distances with varying
number of clusters.
</figureCaption>
<bodyText confidence="0.9996894">
Unlike many other investigations into unsuper-
vised dialogue act classification, the current ap-
proach reports accuracy on held-out test data, not
on the data on which the model was trained. Even
though the model training process does not utilize
available manual tags, requiring the learned unsu-
pervised model to perform well on held-out test
data more closely mimics the broader goal of our
work which is to utilize these unsupervised mod-
els within deployed dialogue systems, where most
utterances to be classified have never been encoun-
tered by the model before.
The procedure for model training and test-
ing uses leave-one-student-out cross-validation.
Rather than other forms of leave-one-out or strat-
ified cross-validation, leave-one-student-out en-
sures that each student’s set of dialogue utterances
are treated as the testing set while the model is
trained on all other students’ utterances. This
process is repeated until each student’s utterances
</bodyText>
<page confidence="0.990532">
117
</page>
<bodyText confidence="0.999992615384615">
have served as a held-out test set (in our case, this
results in n=42 folds). Within each fold, the clus-
ters are learned during training and then for each
utterance in the test set, its closest cluster is com-
puted by taking the average distance of the test ut-
terance to the elements in the cluster. The majority
label of the closest cluster is assigned as the dia-
logue act tag for the test utterance. If the assigned
dialogue act tag matches the manual label of the
test utterance, the utterance is counted as correct
classification. The average accuracy is computed
as the number of correct classifications divided by
the total number of classifications.
</bodyText>
<subsectionHeader confidence="0.992729">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999612736842105">
We conducted experiments with seven different
feature combinations: L, lexical features only,
T, task features only, D, dialogue context fea-
tures only, and then the combinations of these fea-
tures, T + D, T + L, D + L, and T + D + L.
We hypothesized that the addition of task features
would significantly improve the models’ accuracy.
As shown in Table 5, adding task features to di-
alogue context features significantly outperforms
dialogue context features alone (T + D &gt; D).
Similarly, adding task features to lexical features
provides significant improvement (T + L &gt; L).
However, adding task features to the dialogue con-
text plus lexical features model does not provide
benefit, and in fact slightly (not significantly) de-
grades performance (T + D + L &gt;� D + L). As
reflected by the Kappa scores, the test set perfor-
mance attained by these models is hardly better
than would be expected by chance.
</bodyText>
<subsectionHeader confidence="0.997207">
5.3 Utilizing Dialogue History
</subsectionHeader>
<bodyText confidence="0.999956636363636">
The importance of dialogue history, particularly
the influence of the most recent turn on an upcom-
ing turn, is widely recognized within dialogue re-
search, notably by work on adjacency pairs (Sche-
gloff and Sacks, 1973; Forbes-Riley et al., 2007;
Midgley et al., 2009). Based on these findings, we
hypothesized that dialogue history would be sub-
stantially beneficial for unsupervised dialogue act
models as it has been observed to be in numer-
ous studies on supervised classification. However,
as seen in the previous section, adding these di-
alogue context features with equal weight to the
model using Cosine distance only improved its
performance slightly though statistically signifi-
cantly (for example, T+D &gt; T), while the overall
performance is still barely above random chance.
In an attempt to substantially boost the perfor-
mance of the unsupervised dialogue act classi-
fier, we experimented with a hierarchical cluster-
ing structure in which the model first branches on
the previous tutor move, and then the clustering
models are learned as described previously at the
leaves of the tree (Figure 3).
This branching approach results in some
branches with too few utterances to train a multi-
cluster model. To deal with this situation we set a
threshold of n=10 utterances. For those subgroups
with fewer than 10 utterances, we take a simple
majority vote to classify test cases, and for those
subgroups with 10 or larger utterances we train a
cluster model and use it to classify test cases. For
the entire corpus, the number of utterances in each
branch is presented in Table 6.
</bodyText>
<table confidence="0.9471508">
Features Accuracy Kappa
(%)
L 33 0.02
T 37.7 0.07
D 37.6 0.07
T+D 39.1* 0.07
T+L 38* 0.06
D+L 38.3 0.07
T+D+L 37.3 0.05
Tutor’s Previous Dialogue Act
</table>
<figure confidence="0.992199090909091">
Q S PF A
Flat Clustering
do
clustering
do
clustering
do
clustering
...
do
clustering
</figure>
<figureCaption confidence="0.832697857142857">
Table 5: Test set accuracies and Kappa for the flat
clustering model (L: Lexical features, D: Dialogue
context features, T: Task features) *indicates sta-
tistically significant compared to the similar model
without task features (p &lt; 0.05).
Figure 3: Branching student utterances according
to previous tutor dialogue act.
</figureCaption>
<bodyText confidence="0.996541">
As the results in Table 7 show, the performance
of the model with hierarchical structure is signif-
icantly better than the flat clustering model. Note
that each feature in this table leverages previous
</bodyText>
<page confidence="0.999085">
118
</page>
<tableCaption confidence="0.723771">
Table 6: The number of student utterances after
branching on the previous tutor dialogue act.
</tableCaption>
<bodyText confidence="0.976223083333333">
tutor dialogue act while branching. Branching
on previous tutor move boosted the model’s accu-
racy for student move dialogue act classification
by approximately 30% accuracy across all feature
sets, a difference that is statistically significant in
every case. With the hierarchical model struc-
ture, the best performance is achieved by includ-
ing all three types of features: lexical, dialogue
context and task. However, our hypothesis that
task features would significantly improve the ac-
curacy does not hold within the hierarchical clus-
tering model (T + D &gt;� D and T + L &gt;� L).
</bodyText>
<table confidence="0.9973008">
Hierarchical Features Kappa
Accuracy
(%)
T 64.2† 0.45
D 63.2† 0.46
L 60.7† 0.41
T+D 62.1† 0.44
T+L 63.3*† 0.45
D+L 63.6† 0.46
T+D+L 65*† 0.48
</table>
<tableCaption confidence="0.998089">
Table 7: Test set accuracies and Kappa for branch-
</tableCaption>
<bodyText confidence="0.831964142857143">
ing on previous tutor dialogue act (L: Lexical fea-
tures, D: Dialogue context features, T: Task fea-
tures) *indicates statistically significant compared
to the similar model without task features and † in-
dicates hierarchical clustering performing signifi-
cantly better than flat with same features. (p &lt;
0.05).
</bodyText>
<sectionHeader confidence="0.999114" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.997155739130435">
The experimental results provide compelling ev-
idence that an inclusive approach to features for
unsupervised dialogue act modeling holds great
promise. However, we observed a stark difference
in model performance when the tutor’s previous
move was simply included as one of many features
within a flat clustering model compared to when
the previous tutor move was treated as a branch-
ing feature. In this section we take a closer look
and discuss the features that help distinguish par-
ticular dialogue acts from each other.
Using the hierarchical T + D + L model which
performed best within the experiments, we exam-
ine the confusion matrix (Figure 4). Statements
and acknowledgments prove challenging for the
model, 51.3% and 61.5% accuracy overall. More-
over, these two tags are easily confused with each
other: 29.7% of statements were misclassified
as acknowledgments, while 21.2% of acknowl-
edgments were misclassified as statements. The
worst overall classification accuracy was for ques-
tions (6%) and the best was achieved for answers
(95.3%).
</bodyText>
<figureCaption confidence="0.981534">
Figure 4: Confusion matrix for hierarchical model
utilizing all features: T+D+L.
</figureCaption>
<bodyText confidence="0.9994139375">
When we analyze the performance of different
sets of features with respect to individual dialogue
acts, some interesting results emerge. The anal-
ysis shows that task features are especially good
for classifying statements. Using only task fea-
tures, the model correctly classified 61.8% state-
ments, compared to the lower 51.3% accuracy that
the overall best model (T + D + L) achieved on
statements. When we consider the nature of the
statement dialogue act within this corpus, we note
that it is a large category that encompasses a vari-
ety of utterances, some of which have lexical fea-
tures in common with acknowledgments. In this
case, task features are particularly helpful.
For acknowledgments, a combination of task
and lexical features performed best (63.6% ac-
</bodyText>
<figure confidence="0.996048818181818">
Tutor Dialogue
Act
# of student
utterances
818
464
125
91
61
11
8
8
6
Q
S
H
PF
A
ACK
C
O
RACK
</figure>
<page confidence="0.99641">
119
</page>
<bodyText confidence="0.993339970588235">
curacy) compared to the overall best performing
model which achieved a slightly lower 61.5% ac-
curacy on acknowledgments. Acknowledgments
are another example of an act that may take am-
biguous surface form; for example, in our cor-
pus an utterance ‘yes’ appears as both an answer
and an acknowledgment depending on its context.
Therefore, higher level features such as the ones
provided by task may be more helpful.
For questions, the highest performing feature
set is L. However, as shown in Table 8, the model
performed poorly on questions. Inspection of the
models reveals that questions are varied in terms
of structure throughout the corpus and it is hard to
distinguish them from other dialogue acts. For in-
stance there are two consequent utterances “i need
a write statement” and “don’t i”, both of which are
manually labeled as questions. However, in terms
of the structure, the first utterance looks very sim-
ilar to a statement and therefore the model has dif-
ficulty grouping it with questions. Due to the large
variety of question forms in the corpus, it is pos-
sible that the clustering performed poorly on this
dialogue act. In future work it will be promising to
investigate the dialogue structures which produce
questions and to weight them more in the feature
set in order to increase performance of clustering
for questions.
We performed one additional experiment to
compare the performance of the LCS metric with
bigrams. For bigrams, the average leave-one-
student-out test accuracy was 25% with flat clus-
tering compared to the lexical-only case using
LCS (L) which reached 33%.
</bodyText>
<table confidence="0.999241125">
Features S A Q ACK
L 21.5 41.3 14.2 20.4
T 61.76 95.27 7.30 40.90
D 48.16 95.27 3.00 60.30
T+D 52.69 94.68 3.43 51.64
T+L 42.78 95.13 6.01 63.58
D+L 43.63 94.98 8.58 62.09
T+D+L 51.27 95.27 6.01 61.49
</table>
<tableCaption confidence="0.723128333333333">
Table 8: Accuracies for individual dialogue acts.
Acts with fewer than 10 utterances after branching
are omitted from the table.
</tableCaption>
<sectionHeader confidence="0.995496" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999982743589744">
Dialogue act classification is crucial for dialogue
management, and unsupervised modeling ap-
proaches hold great promise for automatically ex-
tracting classification models from corpora. This
paper has focused on unsupervised dialogue act
classification for task-oriented dialogue, investi-
gating the impact of task features and dialogue
context features on model accuracy within both
flat and hierarchical clusterings. Experimental
results confirm that utilizing a combination of
task and dialogue features improves accuracy and
that incorporating one previous tutor move as a
high-level branching feature a provides particu-
larly marked benefit. Moreover, it was found that
task features are particularly important for iden-
tifying particular dialogue moves such as state-
ments, for which the model with task features only
outperformed the model with all features.
In addition to the task stream, future work
should consider other sources of nonverbal cues
such as posture, gesture and facial expressions to
investigate the extent to which these can be suc-
cessfully incorporated in unsupervised dialogue
act models. Second, models that are built in spe-
cialized ways to different user groups (e.g., by
gender or by incoming skill level) should be inves-
tigated. Finally, the performance of unsupervised
dialogue act classification models must ultimately
move toward evaluation within implemented dia-
logue systems (Ezen-Can and Boyer, 2013a). The
overarching goal of these investigations is to cre-
ate unsupervised dialogue act models that perform
well enough to be used within deployed dialogue
systems and enable the system to respond success-
fully. It is hoped that in the future, dialogue act
classification models for many domains can be ex-
tracted automatically from corpora of human dia-
logue in those domains without the need for any
manual annotation.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9979247">
Thanks to the members of the LearnDialogue
group at North Carolina State University for their
helpful input. This work is supported in part by the
National Science Foundation through Grant DRL-
1007962 and the STARS Alliance, CNS-1042468.
Any opinions, findings, conclusions, or recom-
mendations expressed in this report are those of
the participants, and do not necessarily represent
the official views, opinions, or policy of the Na-
tional Science Foundation.
</bodyText>
<page confidence="0.992515">
120
</page>
<sectionHeader confidence="0.886202" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.977230764150943">
James F. Allen, Lenhart K. Schubert, George Ferguson,
Peter Heeman, Chung Hee Hwang, Tsuneaki Kato,
Marc Light, Nathaniel Martin, Bradford Miller,
Massimo Poesio, et al. 1995. The TRAINS project:
A case study in building a conversational planning
agent. Journal of Experimental &amp; Theoretical Arti-
ficial Intelligence, 7(1):7–48.
David Arthur and Sergei Vassilvitskii. 2007. k-
means++: The advantages of careful seeding. In
Proceedings of the 18th ACM-SIAM Symposium on
Discrete Algorithms, pages 1027–1035. Society for
Industrial and Applied Mathematics.
John Langshaw Austin. 1975. How To Do Things with
Words, volume 1955. Oxford University Press.
Srinivas Bangalore, Giuseppe Di Fabbrizio, and
Amanda Stent. 2008. Learning the structure of
task-driven human–human dialogs. IEEE Transac-
tions on Audio, Speech, and Language Processing,
16(7):1249–1259.
Kristy Elizabeth Boyer, Eun Young Ha, Robert
Phillips, Michael D. Wallis, Mladen A. Vouk, and
James C. Lester. 2010. Dialogue act modeling in
a complex task-oriented domain. In Proceedings of
SIGDIAL, pages 297–305. Association for Compu-
tational Linguistics.
Kristy Elizabeth Boyer, Eun Young Ha, Robert
Phillips, and James Lester. 2011. The impact of
task-oriented feature sets on HMMs for dialogue
modeling. In Proceedings of SIGDIAL, pages 49–
58. Association for Computational Linguistics.
Lin Chen and Barbara Di Eugenio. 2013. Multimodal-
ity and dialogue act classification in the RoboHelper
project. In Proceedings of SIGDIAL, pages 183–
192.
Mark G. Core and James Allen. 1997. Coding dialogs
with the DAMSL annotation scheme. In Proceed-
ings of the AAAI Fall Symposium on Communicative
Action in Humans and Machines, pages 28–35.
Nigel Crook, Ramon Granell, and Stephen Pulman.
2009. Unsupervised classification of dialogue acts
using a Dirichlet process mixture model. In Pro-
ceedings of SIGDIAL, pages 341–348. Association
for Computational Linguistics.
Barbara Di Eugenio, Zhuli Xie, and Riccardo Serafin.
2010. Dialogue act classification, higher order di-
alogue structure, and instance-based learning. Dia-
logue &amp; Discourse, 1(2):1–24.
Myroslava O. Dzikovska, Elaine Farrow, and Jo-
hanna D. Moore. 2013. Combining semantic inter-
pretation and statistical classification for improved
explanation processing in a tutorial dialogue system.
In Artificial Intelligence in Education, pages 279–
288.
Aysu Ezen-Can and Kristy Elizabeth Boyer. 2013a.
In-context evaluation of unsupervised dialogue act
models for tutorial dialogue. In Proceedings of SIG-
DIAL, pages 324–328.
Aysu Ezen-Can and Kristy Elizabeth Boyer. 2013b.
Unsupervised classification of student dialogue acts
with query-likelihood clustering. In International
Conference on Educational Data Mining, pages 20–
27.
Oliver Ferschke, Iryna Gurevych, and Yevgen Chebo-
tar. 2012. Behind the article: Recognizing dialog
acts in Wikipedia talk pages. In Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 777–
786.
Kate Forbes-Riley and Diane J. Litman. 2005. Us-
ing bigrams to identify relationships between stu-
dent certainness states and tutor responses in a spo-
ken dialogue corpus. In Proceedings of the SIG-
DIAL Workshop, pages 87–96.
Kate Forbes-Riley, Mihai Rotaru, Diane J. Litman, and
Joel Tetreault. 2007. Exploring affect-context de-
pendencies for adaptive system development. In
Human Language Technologies 2007: The Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 41–44.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175–204.
Eun Young Ha, Joseph F. Grafsgaard, Christopher M.
Mitchell, Kristy Elizabeth Boyer, and James C.
Lester. 2012. Combining verbal and nonverbal
features to overcome the ‘information gap’ in task-
oriented dialogue. In Proceedings of SIGDIAL,
pages 247–256.
Daniel S. Hirschberg. 1975. A linear space al-
gorithm for computing maximal common subse-
quences. Communications of the ACM, 18(6):341–
343.
Shafiq Joty, Giuseppe Carenini, and Chin-Yew Lin.
2011. Unsupervised modeling of dialog acts in
asynchronous conversations. In Proceedings of the
22nd International Joint Conference on Artificial In-
telligence, pages 1807–1813.
Daniel Jurafsky, Elizabeth Shriberg, Barbara Fox, and
Traci Curl. 1998. Lexical, prosodic, and syn-
tactic cues for dialog acts. In Proceedings of the
ACL/COLING-98 Workshop on Discourse Relations
and Discourse Markers, pages 114–120.
Simon Keizer, Rieks op den Akker, and Anton Nijholt.
2002. Dialogue act recognition with Bayesian net-
works for Dutch dialogues. In Proceedings of the
SIGDIAL Workshop, pages 88–94.
</reference>
<page confidence="0.972876">
121
</page>
<reference confidence="0.999845426470588">
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010. Classifying dialogue acts in one-on-
one live chats. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 862–871.
Donghyeon Lee, Minwoo Jeong, Kyungduk Kim, and
Seonghan Ryu. 2013. Unsupervised spoken lan-
guage understanding for a multi-domain dialog sys-
tem. IEEE Transactions On Audio, Speech, and
Language Processing, 21(11):2451–2464.
Johanna Marineau, Peter Wiemer-Hastings, Derek Har-
ter, Brent Olde, Patrick Chipman, Ashish Karnavat,
Victoria Pomeroy, Sonya Rajan, Art Graesser, Tutor-
ing Research Group, et al. 2000. Classification of
speech acts in tutorial dialog. In Proceedings of the
Workshop on Modeling Human Teaching Tactics and
Strategies at the Intelligent Tutoring Systems Con-
ference, pages 65–71.
T. Daniel Midgley, Shelly Harrison, and Cara Mac-
Nish. 2009. Empirical verification of adjacency
pairs using dialogue segmentation. In Proceedings
of SIGDIAL, pages 104–108.
Raymond Ng and Jiawei Han. 1994. Efficient and ef-
fective clustering methods for spatial data mining.
In Proceedings of the 20th International Conference
on Very Large Data Bases, pages 144–155.
Norbert Reithinger and Martin Klesen. 1997. Dia-
logue act classification using language models. In
Proceedings of EuroSpeech, pages 2235–2238.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Pro-
ceedings of the Association for Computational Lin-
guistics, pages 172–180.
Vasile Rus, Cristian Moldovan, Nobal Niraula, and
Arthur C. Graesser. 2012. Automated discovery of
speech act categories in educational games. In Inter-
national Conference on Educational Data Mining,
pages 25–32.
Emanuel A. Schegloff and Harvey Sacks. 1973. Open-
ing up closings. Semiotica, 8(4):289–327.
John R. Searle. 1969. Speech Acts: An Essay in
the Philosophy of Language. Cambridge University
Press.
Riccardo Serafin and Barbara Di Eugenio. 2004.
FLSA: Extending latent semantic analysis with fea-
tures for dialogue act classification. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 692–699. Association
for Computational Linguistics.
Rangarajan Sridhar, Vivek Kumar, Srinivas Bangalore,
and Shrikanth Narayanan. 2009. Combining lexi-
cal, syntactic and prosodic cues for improved online
dialog act tagging. Computer Speech &amp; Language,
23(4):407–422.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339–373.
Maryam Tavafi, Yashar Mehdad, Shafiq Joty, Giuseppe
Carenini, and Raymond Ng. 2013. Dialogue act
recognition in synchronous and asynchronous con-
versations. In Proceedings of SIGDIAL, pages 117–
121.
David R. Traum. 1999. Speech acts for dialogue
agents. In Foundations of Rational Agency, pages
169–201. Springer.
</reference>
<page confidence="0.997492">
122
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.918405">
<title confidence="0.9949465">Combining Task and Dialogue Streams Unsupervised Dialogue Act Models</title>
<author confidence="0.99586">Aysu Ezen-Can</author>
<author confidence="0.99586">Kristy Elizabeth</author>
<affiliation confidence="0.982859">Department of Computer</affiliation>
<author confidence="0.970617">North Carolina State</author>
<email confidence="0.99944">aezen,keboyer@ncsu.edu</email>
<abstract confidence="0.998988961538462">Unsupervised machine learning approaches hold great promise for recognizing dialogue acts, but the performance of these models tends to be much lower than the accuracies reached by supervised models. However, some dialogues, such as task-oriented dialogues with parallel task streams, hold rich information that has not yet been leveraged within unsupervised dialogue act models. This paper investigates incorporating task features into an unsupervised dialogue act model trained on a corpus of human tutoring in introductory computer science. Experimental results show that incorporating task features and dialogue history features significantly improve unsupervised dialogue act classification, particularly within a hierarchical framework that gives prominence to dialogue history. This work constitutes a step toward building high-performing unsupervised dialogue act models that will be used in the next generation of task-oriented dialogue systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James F Allen</author>
<author>Lenhart K Schubert</author>
<author>George Ferguson</author>
<author>Peter Heeman</author>
<author>Chung Hee Hwang</author>
<author>Tsuneaki Kato</author>
<author>Marc Light</author>
<author>Nathaniel Martin</author>
<author>Bradford Miller</author>
<author>Massimo Poesio</author>
</authors>
<title>The TRAINS project: A case study in building a conversational planning agent.</title>
<date>1995</date>
<journal>Journal of Experimental &amp; Theoretical Artificial Intelligence,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="1532" citStr="Allen et al., 1995" startWordPosition="213" endWordPosition="216">rly within a hierarchical framework that gives prominence to dialogue history. This work constitutes a step toward building high-performing unsupervised dialogue act models that will be used in the next generation of task-oriented dialogue systems. 1 Introduction Dialogue acts represent the underlying intent of utterances (Austin, 1975; Searle, 1969), and constitute a crucial level of representation for dialogue systems (Sridhar et al., 2009). The task of automatic dialogue act classification has been extensively studied for decades within several domains including train fares and timetables (Allen et al., 1995; Core and Allen, 1997; Crook et al., 2009; Traum, 1999), virtual personal assistants (Chen and Di Eugenio, 2013), conversational telephone speech (Stolcke et al., 2000), Wikipedia talk pages (Ferschke et al., 2012) and as in the case of this paper, tutorial dialogue (Serafin and Di Eugenio, 2004; Forbes-Riley and Litman, 2005; Boyer et al., 2011; Dzikovska et al., 2013). Most of the prior work on dialogue act classification has depended on manually applying dialogue act tags and then leveraging supervised machine learning (Di Eugenio et al., 2010; Keizer et al., 2002; Reithinger and Klesen, 1</context>
</contexts>
<marker>Allen, Schubert, Ferguson, Heeman, Hwang, Kato, Light, Martin, Miller, Poesio, 1995</marker>
<rawString>James F. Allen, Lenhart K. Schubert, George Ferguson, Peter Heeman, Chung Hee Hwang, Tsuneaki Kato, Marc Light, Nathaniel Martin, Bradford Miller, Massimo Poesio, et al. 1995. The TRAINS project: A case study in building a conversational planning agent. Journal of Experimental &amp; Theoretical Artificial Intelligence, 7(1):7–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Arthur</author>
<author>Sergei Vassilvitskii</author>
</authors>
<title>kmeans++: The advantages of careful seeding.</title>
<date>2007</date>
<journal>Society for Industrial and Applied Mathematics.</journal>
<booktitle>In Proceedings of the 18th ACM-SIAM Symposium on Discrete Algorithms,</booktitle>
<pages>1027--1035</pages>
<marker>Arthur, Vassilvitskii, 2007</marker>
<rawString>David Arthur and Sergei Vassilvitskii. 2007. kmeans++: The advantages of careful seeding. In Proceedings of the 18th ACM-SIAM Symposium on Discrete Algorithms, pages 1027–1035. Society for Industrial and Applied Mathematics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Langshaw Austin</author>
</authors>
<title>How To Do Things with Words,</title>
<date>1975</date>
<volume>volume</volume>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="1251" citStr="Austin, 1975" startWordPosition="171" endWordPosition="172"> into an unsupervised dialogue act model trained on a corpus of human tutoring in introductory computer science. Experimental results show that incorporating task features and dialogue history features significantly improve unsupervised dialogue act classification, particularly within a hierarchical framework that gives prominence to dialogue history. This work constitutes a step toward building high-performing unsupervised dialogue act models that will be used in the next generation of task-oriented dialogue systems. 1 Introduction Dialogue acts represent the underlying intent of utterances (Austin, 1975; Searle, 1969), and constitute a crucial level of representation for dialogue systems (Sridhar et al., 2009). The task of automatic dialogue act classification has been extensively studied for decades within several domains including train fares and timetables (Allen et al., 1995; Core and Allen, 1997; Crook et al., 2009; Traum, 1999), virtual personal assistants (Chen and Di Eugenio, 2013), conversational telephone speech (Stolcke et al., 2000), Wikipedia talk pages (Ferschke et al., 2012) and as in the case of this paper, tutorial dialogue (Serafin and Di Eugenio, 2004; Forbes-Riley and Lit</context>
</contexts>
<marker>Austin, 1975</marker>
<rawString>John Langshaw Austin. 1975. How To Do Things with Words, volume 1955. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Giuseppe Di Fabbrizio</author>
<author>Amanda Stent</author>
</authors>
<title>Learning the structure of task-driven human–human dialogs.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>16</volume>
<issue>7</issue>
<marker>Bangalore, Di Fabbrizio, Stent, 2008</marker>
<rawString>Srinivas Bangalore, Giuseppe Di Fabbrizio, and Amanda Stent. 2008. Learning the structure of task-driven human–human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7):1249–1259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristy Elizabeth Boyer</author>
<author>Eun Young Ha</author>
<author>Robert Phillips</author>
<author>Michael D Wallis</author>
<author>Mladen A Vouk</author>
<author>James C Lester</author>
</authors>
<title>Dialogue act modeling in a complex task-oriented domain.</title>
<date>2010</date>
<booktitle>In Proceedings of SIGDIAL,</booktitle>
<pages>297--305</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6522" citStr="Boyer et al., 2010" startWordPosition="974" endWordPosition="977"> used in the next generation of task-oriented dialogue systems. 2 Related Work There is a rich body of work on dialogue act classification. Supervised approaches for dialogue act classification aimed at improving performance by using several features such as dialogue structure including position of the turn (Ferschke et al., 2012), speaker of an utterance (Tavafi et al., 2013), previous dialogue acts (Kim et al., 2010), lexical features such as words (Stolcke et al., 2000), syntactic features including part-of-speech tags (Bangalore et al., 2008; Marineau et al., 2000), tasksubtask structure (Boyer et al., 2010) acoustic and prosodic cues (Sridhar et al., 2009; Jurafsky et al., 1998), and body posture (Ha et al., 2012). For the growing body of work in unsupervised dialogue act classification a subset of these features have been utilized. The words (Crook et al., 2009), topic words (Ritter et al., 2010), function words (Ezen-Can and Boyer, 2013b), beginning portions of utterances (Rus et al., 2012), partof-speech tags and dependency trees (Joty et al., 2011), and state transition probabilities in Markov models (Lee et al., 2013) are among the list of features investigated for unsupervised modeling of </context>
<context position="12049" citStr="Boyer et al., 2010" startWordPosition="1868" endWordPosition="1871">worked toward solving the computer programming problems. 4.1 Lexical Features Because one of the main goals of our work in the longer term is to perform automatic dialogue act classification in real time, we took as a primary consideration the ability to quickly extract lexical features. The features utilized in the current investigation consist only of word unigrams. In ad115 dition to their ease of extraction, our prior work has shown that addition of part-of-speech tags and and syntax features did not significantly improve the accuracy of supervised dialogue act classifiers in this domain (Boyer et al., 2010), and these features can be time-consuming to extract in real time (Ha et al., 2012). The choice to use word unigrams rather than higher order n-grams is further facilitated by the fact that our clustering technique leverages the longest common sub-sequence (LCS) metric to measure distances between utterances. This metric counts shared sub-sequences of not-necessarily contiguous words (Hirschberg, 1975). In this way, the LCS metric provides a flexible way for ngrams and skip-n-grams to be treated as important units within the clustering, while the raw features themselves consist only of word u</context>
</contexts>
<marker>Boyer, Ha, Phillips, Wallis, Vouk, Lester, 2010</marker>
<rawString>Kristy Elizabeth Boyer, Eun Young Ha, Robert Phillips, Michael D. Wallis, Mladen A. Vouk, and James C. Lester. 2010. Dialogue act modeling in a complex task-oriented domain. In Proceedings of SIGDIAL, pages 297–305. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristy Elizabeth Boyer</author>
<author>Eun Young Ha</author>
<author>Robert Phillips</author>
<author>James Lester</author>
</authors>
<title>The impact of task-oriented feature sets on HMMs for dialogue modeling.</title>
<date>2011</date>
<booktitle>In Proceedings of SIGDIAL,</booktitle>
<pages>49</pages>
<contexts>
<context position="1880" citStr="Boyer et al., 2011" startWordPosition="268" endWordPosition="271">969), and constitute a crucial level of representation for dialogue systems (Sridhar et al., 2009). The task of automatic dialogue act classification has been extensively studied for decades within several domains including train fares and timetables (Allen et al., 1995; Core and Allen, 1997; Crook et al., 2009; Traum, 1999), virtual personal assistants (Chen and Di Eugenio, 2013), conversational telephone speech (Stolcke et al., 2000), Wikipedia talk pages (Ferschke et al., 2012) and as in the case of this paper, tutorial dialogue (Serafin and Di Eugenio, 2004; Forbes-Riley and Litman, 2005; Boyer et al., 2011; Dzikovska et al., 2013). Most of the prior work on dialogue act classification has depended on manually applying dialogue act tags and then leveraging supervised machine learning (Di Eugenio et al., 2010; Keizer et al., 2002; Reithinger and Klesen, 1997; Serafin and Di Eugenio, 2004). This process involves engineering a dialogue act taxonomy (or using an existing one, though domain-specific phenomena can be difficult to capture within multi-purpose dialogue act taxonomies) and manually annotating each utterance in the corpus. Then, the tagged utterances are provided to a supervised machine l</context>
</contexts>
<marker>Boyer, Ha, Phillips, Lester, 2011</marker>
<rawString>Kristy Elizabeth Boyer, Eun Young Ha, Robert Phillips, and James Lester. 2011. The impact of task-oriented feature sets on HMMs for dialogue modeling. In Proceedings of SIGDIAL, pages 49– 58. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Chen</author>
<author>Barbara Di Eugenio</author>
</authors>
<title>Multimodality and dialogue act classification in the RoboHelper project.</title>
<date>2013</date>
<booktitle>In Proceedings of SIGDIAL,</booktitle>
<pages>183--192</pages>
<marker>Chen, Di Eugenio, 2013</marker>
<rawString>Lin Chen and Barbara Di Eugenio. 2013. Multimodality and dialogue act classification in the RoboHelper project. In Proceedings of SIGDIAL, pages 183– 192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark G Core</author>
<author>James Allen</author>
</authors>
<title>Coding dialogs with the DAMSL annotation scheme.</title>
<date>1997</date>
<booktitle>In Proceedings of the AAAI Fall Symposium on Communicative Action in Humans and Machines,</booktitle>
<pages>28--35</pages>
<contexts>
<context position="1554" citStr="Core and Allen, 1997" startWordPosition="217" endWordPosition="220">hical framework that gives prominence to dialogue history. This work constitutes a step toward building high-performing unsupervised dialogue act models that will be used in the next generation of task-oriented dialogue systems. 1 Introduction Dialogue acts represent the underlying intent of utterances (Austin, 1975; Searle, 1969), and constitute a crucial level of representation for dialogue systems (Sridhar et al., 2009). The task of automatic dialogue act classification has been extensively studied for decades within several domains including train fares and timetables (Allen et al., 1995; Core and Allen, 1997; Crook et al., 2009; Traum, 1999), virtual personal assistants (Chen and Di Eugenio, 2013), conversational telephone speech (Stolcke et al., 2000), Wikipedia talk pages (Ferschke et al., 2012) and as in the case of this paper, tutorial dialogue (Serafin and Di Eugenio, 2004; Forbes-Riley and Litman, 2005; Boyer et al., 2011; Dzikovska et al., 2013). Most of the prior work on dialogue act classification has depended on manually applying dialogue act tags and then leveraging supervised machine learning (Di Eugenio et al., 2010; Keizer et al., 2002; Reithinger and Klesen, 1997; Serafin and Di Eu</context>
</contexts>
<marker>Core, Allen, 1997</marker>
<rawString>Mark G. Core and James Allen. 1997. Coding dialogs with the DAMSL annotation scheme. In Proceedings of the AAAI Fall Symposium on Communicative Action in Humans and Machines, pages 28–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel Crook</author>
<author>Ramon Granell</author>
<author>Stephen Pulman</author>
</authors>
<title>Unsupervised classification of dialogue acts using a Dirichlet process mixture model.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGDIAL,</booktitle>
<pages>341--348</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1574" citStr="Crook et al., 2009" startWordPosition="221" endWordPosition="224">ives prominence to dialogue history. This work constitutes a step toward building high-performing unsupervised dialogue act models that will be used in the next generation of task-oriented dialogue systems. 1 Introduction Dialogue acts represent the underlying intent of utterances (Austin, 1975; Searle, 1969), and constitute a crucial level of representation for dialogue systems (Sridhar et al., 2009). The task of automatic dialogue act classification has been extensively studied for decades within several domains including train fares and timetables (Allen et al., 1995; Core and Allen, 1997; Crook et al., 2009; Traum, 1999), virtual personal assistants (Chen and Di Eugenio, 2013), conversational telephone speech (Stolcke et al., 2000), Wikipedia talk pages (Ferschke et al., 2012) and as in the case of this paper, tutorial dialogue (Serafin and Di Eugenio, 2004; Forbes-Riley and Litman, 2005; Boyer et al., 2011; Dzikovska et al., 2013). Most of the prior work on dialogue act classification has depended on manually applying dialogue act tags and then leveraging supervised machine learning (Di Eugenio et al., 2010; Keizer et al., 2002; Reithinger and Klesen, 1997; Serafin and Di Eugenio, 2004). This p</context>
<context position="3153" citStr="Crook et al., 2009" startWordPosition="465" endWordPosition="468">formance, in excess of 75% accuracy on manual tags, approaching the agreement level that is sometimes observed between human annotators (Sridhar et al., 2009; Serafin and Di Eugenio, 2004; Chen and Di Eugenio, 2013). However, the supervised approach has several major drawbacks, including the fact that handcrafting dialogue act tagsets and applying them manually tend to be bottlenecks within the research and design process. To overcome these drawbacks, the field has recently seen growing momentum surrounding unsupervised approaches, which do not require any manual labels during model training (Crook et al., 2009; Joty et al., 2011; Lee et al., 2013). A variety of unsupervised machine learning techniques have been investigated for dialogue act classification, and each line of investigation has explored which features best support this goal. However, to date the best performing unsupervised models achieve in the range of 40% (Rus et al., 2012) to 60% (Joty et al., 2011) training set accuracy on manual tags, substantially lower than the mid-70% accuracy (Sridhar et al., 2009) often achieved on testing sets with supervised models. 113 Proceedings of the SIGDIAL 2014 Conference, pages 113–122, Philadelphi</context>
<context position="6783" citStr="Crook et al., 2009" startWordPosition="1019" endWordPosition="1022">alogue structure including position of the turn (Ferschke et al., 2012), speaker of an utterance (Tavafi et al., 2013), previous dialogue acts (Kim et al., 2010), lexical features such as words (Stolcke et al., 2000), syntactic features including part-of-speech tags (Bangalore et al., 2008; Marineau et al., 2000), tasksubtask structure (Boyer et al., 2010) acoustic and prosodic cues (Sridhar et al., 2009; Jurafsky et al., 1998), and body posture (Ha et al., 2012). For the growing body of work in unsupervised dialogue act classification a subset of these features have been utilized. The words (Crook et al., 2009), topic words (Ritter et al., 2010), function words (Ezen-Can and Boyer, 2013b), beginning portions of utterances (Rus et al., 2012), partof-speech tags and dependency trees (Joty et al., 2011), and state transition probabilities in Markov models (Lee et al., 2013) are among the list of features investigated for unsupervised modeling of dialogue acts. However, the accuracies achieved by the best of these models are well below the accuracies achieved by supervised techniques. To improve performance of unsupervised models for task-oriented dialogue, utilizing a combination of task and dialogue f</context>
</contexts>
<marker>Crook, Granell, Pulman, 2009</marker>
<rawString>Nigel Crook, Ramon Granell, and Stephen Pulman. 2009. Unsupervised classification of dialogue acts using a Dirichlet process mixture model. In Proceedings of SIGDIAL, pages 341–348. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Zhuli Xie</author>
<author>Riccardo Serafin</author>
</authors>
<title>Dialogue act classification, higher order dialogue structure, and instance-based learning.</title>
<date>2010</date>
<journal>Dialogue &amp; Discourse,</journal>
<volume>1</volume>
<issue>2</issue>
<marker>Di Eugenio, Xie, Serafin, 2010</marker>
<rawString>Barbara Di Eugenio, Zhuli Xie, and Riccardo Serafin. 2010. Dialogue act classification, higher order dialogue structure, and instance-based learning. Dialogue &amp; Discourse, 1(2):1–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Elaine Farrow</author>
<author>Johanna D Moore</author>
</authors>
<title>Combining semantic interpretation and statistical classification for improved explanation processing in a tutorial dialogue system.</title>
<date>2013</date>
<booktitle>In Artificial Intelligence in Education,</booktitle>
<pages>279--288</pages>
<contexts>
<context position="1905" citStr="Dzikovska et al., 2013" startWordPosition="272" endWordPosition="275"> a crucial level of representation for dialogue systems (Sridhar et al., 2009). The task of automatic dialogue act classification has been extensively studied for decades within several domains including train fares and timetables (Allen et al., 1995; Core and Allen, 1997; Crook et al., 2009; Traum, 1999), virtual personal assistants (Chen and Di Eugenio, 2013), conversational telephone speech (Stolcke et al., 2000), Wikipedia talk pages (Ferschke et al., 2012) and as in the case of this paper, tutorial dialogue (Serafin and Di Eugenio, 2004; Forbes-Riley and Litman, 2005; Boyer et al., 2011; Dzikovska et al., 2013). Most of the prior work on dialogue act classification has depended on manually applying dialogue act tags and then leveraging supervised machine learning (Di Eugenio et al., 2010; Keizer et al., 2002; Reithinger and Klesen, 1997; Serafin and Di Eugenio, 2004). This process involves engineering a dialogue act taxonomy (or using an existing one, though domain-specific phenomena can be difficult to capture within multi-purpose dialogue act taxonomies) and manually annotating each utterance in the corpus. Then, the tagged utterances are provided to a supervised machine learner. This supervised a</context>
</contexts>
<marker>Dzikovska, Farrow, Moore, 2013</marker>
<rawString>Myroslava O. Dzikovska, Elaine Farrow, and Johanna D. Moore. 2013. Combining semantic interpretation and statistical classification for improved explanation processing in a tutorial dialogue system. In Artificial Intelligence in Education, pages 279– 288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aysu Ezen-Can</author>
<author>Kristy Elizabeth Boyer</author>
</authors>
<title>In-context evaluation of unsupervised dialogue act models for tutorial dialogue.</title>
<date>2013</date>
<booktitle>In Proceedings of SIGDIAL,</booktitle>
<pages>324--328</pages>
<contexts>
<context position="6860" citStr="Ezen-Can and Boyer, 2013" startWordPosition="1032" endWordPosition="1035"> speaker of an utterance (Tavafi et al., 2013), previous dialogue acts (Kim et al., 2010), lexical features such as words (Stolcke et al., 2000), syntactic features including part-of-speech tags (Bangalore et al., 2008; Marineau et al., 2000), tasksubtask structure (Boyer et al., 2010) acoustic and prosodic cues (Sridhar et al., 2009; Jurafsky et al., 1998), and body posture (Ha et al., 2012). For the growing body of work in unsupervised dialogue act classification a subset of these features have been utilized. The words (Crook et al., 2009), topic words (Ritter et al., 2010), function words (Ezen-Can and Boyer, 2013b), beginning portions of utterances (Rus et al., 2012), partof-speech tags and dependency trees (Joty et al., 2011), and state transition probabilities in Markov models (Lee et al., 2013) are among the list of features investigated for unsupervised modeling of dialogue acts. However, the accuracies achieved by the best of these models are well below the accuracies achieved by supervised techniques. To improve performance of unsupervised models for task-oriented dialogue, utilizing a combination of task and dialogue features is a promising direction. 3 Corpus The task-oriented dialogue corpus </context>
<context position="17615" citStr="Ezen-Can and Boyer, 2013" startWordPosition="2740" endWordPosition="2743"> # dial Number of tutor dialogue messages sent within the current dialogue. Table 4: Task features extracted from student computer programming activities. 2007) which selects the first seed randomly and then greedily chooses seeds that are farthest from the chosen seeds. The goal of using this approach in our application is to choose seeds from different dialogue acts so that the final model achieves good coverage. Our preliminary experiments demonstrated that this greedy seed selection combined with k-medoids outperforms other clustering approaches including those utilized in our prior work (Ezen-Can and Boyer, 2013a). In order to select the number of clusters k, a subset of the corpus, constituting 25% of the full corpus (that were not tagged) composed of 462 utterances, was separated as a development set. First, we examined the coherence of clusters at different values of k using intra-cluster distances. This technique involves identifying an ‘elbow’ where the decrease in intra-cluster distance becomes less rapid (since adding more clusters can continue to decrease intra-cluster distance to the point of overfitting) (Figure 2). The graph suggests an elbow at k=5. Because there may be multiple elbows in</context>
<context position="29990" citStr="Ezen-Can and Boyer, 2013" startWordPosition="4756" endWordPosition="4759">tures only outperformed the model with all features. In addition to the task stream, future work should consider other sources of nonverbal cues such as posture, gesture and facial expressions to investigate the extent to which these can be successfully incorporated in unsupervised dialogue act models. Second, models that are built in specialized ways to different user groups (e.g., by gender or by incoming skill level) should be investigated. Finally, the performance of unsupervised dialogue act classification models must ultimately move toward evaluation within implemented dialogue systems (Ezen-Can and Boyer, 2013a). The overarching goal of these investigations is to create unsupervised dialogue act models that perform well enough to be used within deployed dialogue systems and enable the system to respond successfully. It is hoped that in the future, dialogue act classification models for many domains can be extracted automatically from corpora of human dialogue in those domains without the need for any manual annotation. Acknowledgments Thanks to the members of the LearnDialogue group at North Carolina State University for their helpful input. This work is supported in part by the National Science Fo</context>
</contexts>
<marker>Ezen-Can, Boyer, 2013</marker>
<rawString>Aysu Ezen-Can and Kristy Elizabeth Boyer. 2013a. In-context evaluation of unsupervised dialogue act models for tutorial dialogue. In Proceedings of SIGDIAL, pages 324–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aysu Ezen-Can</author>
<author>Kristy Elizabeth Boyer</author>
</authors>
<title>Unsupervised classification of student dialogue acts with query-likelihood clustering.</title>
<date>2013</date>
<booktitle>In International Conference on Educational Data Mining,</booktitle>
<pages>20--27</pages>
<contexts>
<context position="6860" citStr="Ezen-Can and Boyer, 2013" startWordPosition="1032" endWordPosition="1035"> speaker of an utterance (Tavafi et al., 2013), previous dialogue acts (Kim et al., 2010), lexical features such as words (Stolcke et al., 2000), syntactic features including part-of-speech tags (Bangalore et al., 2008; Marineau et al., 2000), tasksubtask structure (Boyer et al., 2010) acoustic and prosodic cues (Sridhar et al., 2009; Jurafsky et al., 1998), and body posture (Ha et al., 2012). For the growing body of work in unsupervised dialogue act classification a subset of these features have been utilized. The words (Crook et al., 2009), topic words (Ritter et al., 2010), function words (Ezen-Can and Boyer, 2013b), beginning portions of utterances (Rus et al., 2012), partof-speech tags and dependency trees (Joty et al., 2011), and state transition probabilities in Markov models (Lee et al., 2013) are among the list of features investigated for unsupervised modeling of dialogue acts. However, the accuracies achieved by the best of these models are well below the accuracies achieved by supervised techniques. To improve performance of unsupervised models for task-oriented dialogue, utilizing a combination of task and dialogue features is a promising direction. 3 Corpus The task-oriented dialogue corpus </context>
<context position="17615" citStr="Ezen-Can and Boyer, 2013" startWordPosition="2740" endWordPosition="2743"> # dial Number of tutor dialogue messages sent within the current dialogue. Table 4: Task features extracted from student computer programming activities. 2007) which selects the first seed randomly and then greedily chooses seeds that are farthest from the chosen seeds. The goal of using this approach in our application is to choose seeds from different dialogue acts so that the final model achieves good coverage. Our preliminary experiments demonstrated that this greedy seed selection combined with k-medoids outperforms other clustering approaches including those utilized in our prior work (Ezen-Can and Boyer, 2013a). In order to select the number of clusters k, a subset of the corpus, constituting 25% of the full corpus (that were not tagged) composed of 462 utterances, was separated as a development set. First, we examined the coherence of clusters at different values of k using intra-cluster distances. This technique involves identifying an ‘elbow’ where the decrease in intra-cluster distance becomes less rapid (since adding more clusters can continue to decrease intra-cluster distance to the point of overfitting) (Figure 2). The graph suggests an elbow at k=5. Because there may be multiple elbows in</context>
<context position="29990" citStr="Ezen-Can and Boyer, 2013" startWordPosition="4756" endWordPosition="4759">tures only outperformed the model with all features. In addition to the task stream, future work should consider other sources of nonverbal cues such as posture, gesture and facial expressions to investigate the extent to which these can be successfully incorporated in unsupervised dialogue act models. Second, models that are built in specialized ways to different user groups (e.g., by gender or by incoming skill level) should be investigated. Finally, the performance of unsupervised dialogue act classification models must ultimately move toward evaluation within implemented dialogue systems (Ezen-Can and Boyer, 2013a). The overarching goal of these investigations is to create unsupervised dialogue act models that perform well enough to be used within deployed dialogue systems and enable the system to respond successfully. It is hoped that in the future, dialogue act classification models for many domains can be extracted automatically from corpora of human dialogue in those domains without the need for any manual annotation. Acknowledgments Thanks to the members of the LearnDialogue group at North Carolina State University for their helpful input. This work is supported in part by the National Science Fo</context>
</contexts>
<marker>Ezen-Can, Boyer, 2013</marker>
<rawString>Aysu Ezen-Can and Kristy Elizabeth Boyer. 2013b. Unsupervised classification of student dialogue acts with query-likelihood clustering. In International Conference on Educational Data Mining, pages 20– 27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Ferschke</author>
<author>Iryna Gurevych</author>
<author>Yevgen Chebotar</author>
</authors>
<title>Behind the article: Recognizing dialog acts in Wikipedia talk pages.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>777--786</pages>
<contexts>
<context position="1747" citStr="Ferschke et al., 2012" startWordPosition="245" endWordPosition="248">on of task-oriented dialogue systems. 1 Introduction Dialogue acts represent the underlying intent of utterances (Austin, 1975; Searle, 1969), and constitute a crucial level of representation for dialogue systems (Sridhar et al., 2009). The task of automatic dialogue act classification has been extensively studied for decades within several domains including train fares and timetables (Allen et al., 1995; Core and Allen, 1997; Crook et al., 2009; Traum, 1999), virtual personal assistants (Chen and Di Eugenio, 2013), conversational telephone speech (Stolcke et al., 2000), Wikipedia talk pages (Ferschke et al., 2012) and as in the case of this paper, tutorial dialogue (Serafin and Di Eugenio, 2004; Forbes-Riley and Litman, 2005; Boyer et al., 2011; Dzikovska et al., 2013). Most of the prior work on dialogue act classification has depended on manually applying dialogue act tags and then leveraging supervised machine learning (Di Eugenio et al., 2010; Keizer et al., 2002; Reithinger and Klesen, 1997; Serafin and Di Eugenio, 2004). This process involves engineering a dialogue act taxonomy (or using an existing one, though domain-specific phenomena can be difficult to capture within multi-purpose dialogue act</context>
<context position="6235" citStr="Ferschke et al., 2012" startWordPosition="928" endWordPosition="931">ue history performs best. Finally, we break down the model’s performance by dialogue act and investigate which features are most beneficial for distinguishing particular acts. These contributions constitute a step toward building high-performing unsupervised dialogue act models that can be used in the next generation of task-oriented dialogue systems. 2 Related Work There is a rich body of work on dialogue act classification. Supervised approaches for dialogue act classification aimed at improving performance by using several features such as dialogue structure including position of the turn (Ferschke et al., 2012), speaker of an utterance (Tavafi et al., 2013), previous dialogue acts (Kim et al., 2010), lexical features such as words (Stolcke et al., 2000), syntactic features including part-of-speech tags (Bangalore et al., 2008; Marineau et al., 2000), tasksubtask structure (Boyer et al., 2010) acoustic and prosodic cues (Sridhar et al., 2009; Jurafsky et al., 1998), and body posture (Ha et al., 2012). For the growing body of work in unsupervised dialogue act classification a subset of these features have been utilized. The words (Crook et al., 2009), topic words (Ritter et al., 2010), function words </context>
</contexts>
<marker>Ferschke, Gurevych, Chebotar, 2012</marker>
<rawString>Oliver Ferschke, Iryna Gurevych, and Yevgen Chebotar. 2012. Behind the article: Recognizing dialog acts in Wikipedia talk pages. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 777– 786.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Forbes-Riley</author>
<author>Diane J Litman</author>
</authors>
<title>Using bigrams to identify relationships between student certainness states and tutor responses in a spoken dialogue corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of the SIGDIAL Workshop,</booktitle>
<pages>87--96</pages>
<contexts>
<context position="1860" citStr="Forbes-Riley and Litman, 2005" startWordPosition="264" endWordPosition="267">rances (Austin, 1975; Searle, 1969), and constitute a crucial level of representation for dialogue systems (Sridhar et al., 2009). The task of automatic dialogue act classification has been extensively studied for decades within several domains including train fares and timetables (Allen et al., 1995; Core and Allen, 1997; Crook et al., 2009; Traum, 1999), virtual personal assistants (Chen and Di Eugenio, 2013), conversational telephone speech (Stolcke et al., 2000), Wikipedia talk pages (Ferschke et al., 2012) and as in the case of this paper, tutorial dialogue (Serafin and Di Eugenio, 2004; Forbes-Riley and Litman, 2005; Boyer et al., 2011; Dzikovska et al., 2013). Most of the prior work on dialogue act classification has depended on manually applying dialogue act tags and then leveraging supervised machine learning (Di Eugenio et al., 2010; Keizer et al., 2002; Reithinger and Klesen, 1997; Serafin and Di Eugenio, 2004). This process involves engineering a dialogue act taxonomy (or using an existing one, though domain-specific phenomena can be difficult to capture within multi-purpose dialogue act taxonomies) and manually annotating each utterance in the corpus. Then, the tagged utterances are provided to a </context>
</contexts>
<marker>Forbes-Riley, Litman, 2005</marker>
<rawString>Kate Forbes-Riley and Diane J. Litman. 2005. Using bigrams to identify relationships between student certainness states and tutor responses in a spoken dialogue corpus. In Proceedings of the SIGDIAL Workshop, pages 87–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Forbes-Riley</author>
<author>Mihai Rotaru</author>
<author>Diane J Litman</author>
<author>Joel Tetreault</author>
</authors>
<title>Exploring affect-context dependencies for adaptive system development.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>41--44</pages>
<contexts>
<context position="21334" citStr="Forbes-Riley et al., 2007" startWordPosition="3353" endWordPosition="3356">mprovement (T + L &gt; L). However, adding task features to the dialogue context plus lexical features model does not provide benefit, and in fact slightly (not significantly) degrades performance (T + D + L &gt;� D + L). As reflected by the Kappa scores, the test set performance attained by these models is hardly better than would be expected by chance. 5.3 Utilizing Dialogue History The importance of dialogue history, particularly the influence of the most recent turn on an upcoming turn, is widely recognized within dialogue research, notably by work on adjacency pairs (Schegloff and Sacks, 1973; Forbes-Riley et al., 2007; Midgley et al., 2009). Based on these findings, we hypothesized that dialogue history would be substantially beneficial for unsupervised dialogue act models as it has been observed to be in numerous studies on supervised classification. However, as seen in the previous section, adding these dialogue context features with equal weight to the model using Cosine distance only improved its performance slightly though statistically significantly (for example, T+D &gt; T), while the overall performance is still barely above random chance. In an attempt to substantially boost the performance of the un</context>
</contexts>
<marker>Forbes-Riley, Rotaru, Litman, Tetreault, 2007</marker>
<rawString>Kate Forbes-Riley, Mihai Rotaru, Diane J. Litman, and Joel Tetreault. 2007. Exploring affect-context dependencies for adaptive system development. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, pages 41–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="4396" citStr="Grosz and Sidner, 1986" startWordPosition="658" endWordPosition="661">e 2014. c�2014 Association for Computational Linguistics In order to close this performance gap between unsupervised and supervised techniques, we suggest that it is crucial to enrich the features available to unsupervised models. In particular, when a dialogue is task-oriented and includes a rich source of information within a parallel task stream, these features may substantially boost the ability of an unsupervised model to distinguish dialogue acts. For example, in situated dialogue, features representing the state of the physical world may be highly influential for dialogue act modeling (Grosz and Sidner, 1986). Human tutorial dialogue, which is the domain being considered in the current work, often exhibits this structure: the task artifact is external to the dialogue utterances themselves (in the case of our work, this artifact is a computer program that the student is constructing). Task features have already been shown beneficial for supervised dialogue act classification in our domain (Ha et al., 2012). We hypothesize that including these task features within an unsupervised model will significantly improve its performance. In addition, we hypothesize that including dialogue history as a promin</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eun Young Ha</author>
<author>Joseph F Grafsgaard</author>
<author>Christopher M Mitchell</author>
<author>Kristy Elizabeth Boyer</author>
<author>James C Lester</author>
</authors>
<title>Combining verbal and nonverbal features to overcome the ‘information gap’ in taskoriented dialogue.</title>
<date>2012</date>
<booktitle>In Proceedings of SIGDIAL,</booktitle>
<pages>247--256</pages>
<contexts>
<context position="4800" citStr="Ha et al., 2012" startWordPosition="723" endWordPosition="726">supervised model to distinguish dialogue acts. For example, in situated dialogue, features representing the state of the physical world may be highly influential for dialogue act modeling (Grosz and Sidner, 1986). Human tutorial dialogue, which is the domain being considered in the current work, often exhibits this structure: the task artifact is external to the dialogue utterances themselves (in the case of our work, this artifact is a computer program that the student is constructing). Task features have already been shown beneficial for supervised dialogue act classification in our domain (Ha et al., 2012). We hypothesize that including these task features within an unsupervised model will significantly improve its performance. In addition, we hypothesize that including dialogue history as a prominent feature within an unsupervised model will provide significant improvement. This paper represents the first investigation into combining task and dialogue features within an unsupervised dialogue act classification model. First, we discuss representation of these task features and dialogue structure features, and compare these representations within both flat and hierarchical clustering approaches.</context>
<context position="6631" citStr="Ha et al., 2012" startWordPosition="993" endWordPosition="996">ialogue act classification. Supervised approaches for dialogue act classification aimed at improving performance by using several features such as dialogue structure including position of the turn (Ferschke et al., 2012), speaker of an utterance (Tavafi et al., 2013), previous dialogue acts (Kim et al., 2010), lexical features such as words (Stolcke et al., 2000), syntactic features including part-of-speech tags (Bangalore et al., 2008; Marineau et al., 2000), tasksubtask structure (Boyer et al., 2010) acoustic and prosodic cues (Sridhar et al., 2009; Jurafsky et al., 1998), and body posture (Ha et al., 2012). For the growing body of work in unsupervised dialogue act classification a subset of these features have been utilized. The words (Crook et al., 2009), topic words (Ritter et al., 2010), function words (Ezen-Can and Boyer, 2013b), beginning portions of utterances (Rus et al., 2012), partof-speech tags and dependency trees (Joty et al., 2011), and state transition probabilities in Markov models (Lee et al., 2013) are among the list of features investigated for unsupervised modeling of dialogue acts. However, the accuracies achieved by the best of these models are well below the accuracies ach</context>
<context position="9658" citStr="Ha et al., 2012" startWordPosition="1473" endWordPosition="1476">156) whereas for students it is 39 (min=18, max=69) for the annotated set. The average number of words per utterance for students is 4.4 and for tutors it is 5.4. This annotated set is used in the current analysis for both training and testing where cross-validation is applied. As described later, a separate set containing 462 unannotated utterances is used as a development set for determining the number of clusters. The dialogue stream of this corpus was manually annotated as part of previous work on supervised dialogue act modeling which achieved 69% accuracy with Conditional Random Fields (Ha et al., 2012). A brief description of the student dialogue act tags, which are the focus of the models reported in this paper, is shown in Table 1. The most frequent dialogue act (A) constitutes the baseline chance (39.85%). In the current work, the manually applied dialogue act labels are not utilized during model training, but are only used for evaluation purposes as our models’ accuracies are reported for manual tags on a held-out test set. An excerpt from the corpus is shown in Table 2. Note that the current work focuses on classifying student dialogue act tags, since in an automated dialogue system th</context>
<context position="12133" citStr="Ha et al., 2012" startWordPosition="1884" endWordPosition="1887">ne of the main goals of our work in the longer term is to perform automatic dialogue act classification in real time, we took as a primary consideration the ability to quickly extract lexical features. The features utilized in the current investigation consist only of word unigrams. In ad115 dition to their ease of extraction, our prior work has shown that addition of part-of-speech tags and and syntax features did not significantly improve the accuracy of supervised dialogue act classifiers in this domain (Boyer et al., 2010), and these features can be time-consuming to extract in real time (Ha et al., 2012). The choice to use word unigrams rather than higher order n-grams is further facilitated by the fact that our clustering technique leverages the longest common sub-sequence (LCS) metric to measure distances between utterances. This metric counts shared sub-sequences of not-necessarily contiguous words (Hirschberg, 1975). In this way, the LCS metric provides a flexible way for ngrams and skip-n-grams to be treated as important units within the clustering, while the raw features themselves consist only of word unigrams. (We report on a comparison between LCS and bigrams later in the discussion </context>
<context position="14478" citStr="Ha et al., 2012" startWordPosition="2251" endWordPosition="2254">dialogue context features were extracted, and in addition, the task stream consisting of student problem-solving activities such as authoring code, compiling, and executing the program. The programming activities of students were logged to a database along with all of the dialogue events during tutoring. A set of task features was found to be important for dialogue act classification in this domain in prior work, including most recent programming action, status of the most recent task activity and task activity flag representing whether the utterance was preceded by a student’s task activity (Ha et al., 2012). We expand this set of features as shown in Table 4. 5 Experiments The goal of this work is to investigate the impact of including task and dialogue context features on unsupervised dialogue act models. We hypothesize that incorporating task features will significantly improve the performance of an unsupervised model, and we also hypothesize that properly incorporating dialogue context features, which are at a different granularity than the lexical features extracted from utterances, will substantially improve model accuracy. 5.1 Dialogue Act Modeling With k-medoids Clustering The unsupervise</context>
</contexts>
<marker>Ha, Grafsgaard, Mitchell, Boyer, Lester, 2012</marker>
<rawString>Eun Young Ha, Joseph F. Grafsgaard, Christopher M. Mitchell, Kristy Elizabeth Boyer, and James C. Lester. 2012. Combining verbal and nonverbal features to overcome the ‘information gap’ in taskoriented dialogue. In Proceedings of SIGDIAL, pages 247–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel S Hirschberg</author>
</authors>
<title>A linear space algorithm for computing maximal common subsequences.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>6</issue>
<pages>343</pages>
<contexts>
<context position="12455" citStr="Hirschberg, 1975" startWordPosition="1931" endWordPosition="1932">action, our prior work has shown that addition of part-of-speech tags and and syntax features did not significantly improve the accuracy of supervised dialogue act classifiers in this domain (Boyer et al., 2010), and these features can be time-consuming to extract in real time (Ha et al., 2012). The choice to use word unigrams rather than higher order n-grams is further facilitated by the fact that our clustering technique leverages the longest common sub-sequence (LCS) metric to measure distances between utterances. This metric counts shared sub-sequences of not-necessarily contiguous words (Hirschberg, 1975). In this way, the LCS metric provides a flexible way for ngrams and skip-n-grams to be treated as important units within the clustering, while the raw features themselves consist only of word unigrams. (We report on a comparison between LCS and bigrams later in the discussion section.) Utilizing LCS, there exists a distance (1-similarity) value from each utterance to every other utterance. 4.2 Dialogue Context Features Based on previous work on a similar human tutorial dialogue corpus (Ha et al., 2012), we utilize four features that provide information about the dialogue structure. These feat</context>
</contexts>
<marker>Hirschberg, 1975</marker>
<rawString>Daniel S. Hirschberg. 1975. A linear space algorithm for computing maximal common subsequences. Communications of the ACM, 18(6):341– 343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Unsupervised modeling of dialog acts in asynchronous conversations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 22nd International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1807--1813</pages>
<contexts>
<context position="3172" citStr="Joty et al., 2011" startWordPosition="469" endWordPosition="472">of 75% accuracy on manual tags, approaching the agreement level that is sometimes observed between human annotators (Sridhar et al., 2009; Serafin and Di Eugenio, 2004; Chen and Di Eugenio, 2013). However, the supervised approach has several major drawbacks, including the fact that handcrafting dialogue act tagsets and applying them manually tend to be bottlenecks within the research and design process. To overcome these drawbacks, the field has recently seen growing momentum surrounding unsupervised approaches, which do not require any manual labels during model training (Crook et al., 2009; Joty et al., 2011; Lee et al., 2013). A variety of unsupervised machine learning techniques have been investigated for dialogue act classification, and each line of investigation has explored which features best support this goal. However, to date the best performing unsupervised models achieve in the range of 40% (Rus et al., 2012) to 60% (Joty et al., 2011) training set accuracy on manual tags, substantially lower than the mid-70% accuracy (Sridhar et al., 2009) often achieved on testing sets with supervised models. 113 Proceedings of the SIGDIAL 2014 Conference, pages 113–122, Philadelphia, U.S.A., 18-20 Ju</context>
<context position="6976" citStr="Joty et al., 2011" startWordPosition="1051" endWordPosition="1054">(Stolcke et al., 2000), syntactic features including part-of-speech tags (Bangalore et al., 2008; Marineau et al., 2000), tasksubtask structure (Boyer et al., 2010) acoustic and prosodic cues (Sridhar et al., 2009; Jurafsky et al., 1998), and body posture (Ha et al., 2012). For the growing body of work in unsupervised dialogue act classification a subset of these features have been utilized. The words (Crook et al., 2009), topic words (Ritter et al., 2010), function words (Ezen-Can and Boyer, 2013b), beginning portions of utterances (Rus et al., 2012), partof-speech tags and dependency trees (Joty et al., 2011), and state transition probabilities in Markov models (Lee et al., 2013) are among the list of features investigated for unsupervised modeling of dialogue acts. However, the accuracies achieved by the best of these models are well below the accuracies achieved by supervised techniques. To improve performance of unsupervised models for task-oriented dialogue, utilizing a combination of task and dialogue features is a promising direction. 3 Corpus The task-oriented dialogue corpus used in this work was collected in a computer-mediated human tutorial dialogue study. Students (n = 42) and tutors i</context>
</contexts>
<marker>Joty, Carenini, Lin, 2011</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, and Chin-Yew Lin. 2011. Unsupervised modeling of dialog acts in asynchronous conversations. In Proceedings of the 22nd International Joint Conference on Artificial Intelligence, pages 1807–1813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>Elizabeth Shriberg</author>
<author>Barbara Fox</author>
<author>Traci Curl</author>
</authors>
<title>Lexical, prosodic, and syntactic cues for dialog acts.</title>
<date>1998</date>
<booktitle>In Proceedings of the ACL/COLING-98 Workshop on Discourse Relations and Discourse Markers,</booktitle>
<pages>114--120</pages>
<contexts>
<context position="6595" citStr="Jurafsky et al., 1998" startWordPosition="986" endWordPosition="989">ted Work There is a rich body of work on dialogue act classification. Supervised approaches for dialogue act classification aimed at improving performance by using several features such as dialogue structure including position of the turn (Ferschke et al., 2012), speaker of an utterance (Tavafi et al., 2013), previous dialogue acts (Kim et al., 2010), lexical features such as words (Stolcke et al., 2000), syntactic features including part-of-speech tags (Bangalore et al., 2008; Marineau et al., 2000), tasksubtask structure (Boyer et al., 2010) acoustic and prosodic cues (Sridhar et al., 2009; Jurafsky et al., 1998), and body posture (Ha et al., 2012). For the growing body of work in unsupervised dialogue act classification a subset of these features have been utilized. The words (Crook et al., 2009), topic words (Ritter et al., 2010), function words (Ezen-Can and Boyer, 2013b), beginning portions of utterances (Rus et al., 2012), partof-speech tags and dependency trees (Joty et al., 2011), and state transition probabilities in Markov models (Lee et al., 2013) are among the list of features investigated for unsupervised modeling of dialogue acts. However, the accuracies achieved by the best of these mode</context>
</contexts>
<marker>Jurafsky, Shriberg, Fox, Curl, 1998</marker>
<rawString>Daniel Jurafsky, Elizabeth Shriberg, Barbara Fox, and Traci Curl. 1998. Lexical, prosodic, and syntactic cues for dialog acts. In Proceedings of the ACL/COLING-98 Workshop on Discourse Relations and Discourse Markers, pages 114–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Keizer</author>
<author>Rieks op den Akker</author>
<author>Anton Nijholt</author>
</authors>
<title>Dialogue act recognition with Bayesian networks for Dutch dialogues.</title>
<date>2002</date>
<booktitle>In Proceedings of the SIGDIAL Workshop,</booktitle>
<pages>88--94</pages>
<marker>Keizer, den Akker, Nijholt, 2002</marker>
<rawString>Simon Keizer, Rieks op den Akker, and Anton Nijholt. 2002. Dialogue act recognition with Bayesian networks for Dutch dialogues. In Proceedings of the SIGDIAL Workshop, pages 88–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Lawrence Cavedon</author>
<author>Timothy Baldwin</author>
</authors>
<title>Classifying dialogue acts in one-onone live chats.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>862--871</pages>
<contexts>
<context position="6325" citStr="Kim et al., 2010" startWordPosition="943" endWordPosition="946">vestigate which features are most beneficial for distinguishing particular acts. These contributions constitute a step toward building high-performing unsupervised dialogue act models that can be used in the next generation of task-oriented dialogue systems. 2 Related Work There is a rich body of work on dialogue act classification. Supervised approaches for dialogue act classification aimed at improving performance by using several features such as dialogue structure including position of the turn (Ferschke et al., 2012), speaker of an utterance (Tavafi et al., 2013), previous dialogue acts (Kim et al., 2010), lexical features such as words (Stolcke et al., 2000), syntactic features including part-of-speech tags (Bangalore et al., 2008; Marineau et al., 2000), tasksubtask structure (Boyer et al., 2010) acoustic and prosodic cues (Sridhar et al., 2009; Jurafsky et al., 1998), and body posture (Ha et al., 2012). For the growing body of work in unsupervised dialogue act classification a subset of these features have been utilized. The words (Crook et al., 2009), topic words (Ritter et al., 2010), function words (Ezen-Can and Boyer, 2013b), beginning portions of utterances (Rus et al., 2012), partof-s</context>
</contexts>
<marker>Kim, Cavedon, Baldwin, 2010</marker>
<rawString>Su Nam Kim, Lawrence Cavedon, and Timothy Baldwin. 2010. Classifying dialogue acts in one-onone live chats. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 862–871.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donghyeon Lee</author>
<author>Minwoo Jeong</author>
<author>Kyungduk Kim</author>
<author>Seonghan Ryu</author>
</authors>
<title>Unsupervised spoken language understanding for a multi-domain dialog system.</title>
<date>2013</date>
<journal>IEEE Transactions On Audio, Speech, and Language Processing,</journal>
<volume>21</volume>
<issue>11</issue>
<contexts>
<context position="3191" citStr="Lee et al., 2013" startWordPosition="473" endWordPosition="476">manual tags, approaching the agreement level that is sometimes observed between human annotators (Sridhar et al., 2009; Serafin and Di Eugenio, 2004; Chen and Di Eugenio, 2013). However, the supervised approach has several major drawbacks, including the fact that handcrafting dialogue act tagsets and applying them manually tend to be bottlenecks within the research and design process. To overcome these drawbacks, the field has recently seen growing momentum surrounding unsupervised approaches, which do not require any manual labels during model training (Crook et al., 2009; Joty et al., 2011; Lee et al., 2013). A variety of unsupervised machine learning techniques have been investigated for dialogue act classification, and each line of investigation has explored which features best support this goal. However, to date the best performing unsupervised models achieve in the range of 40% (Rus et al., 2012) to 60% (Joty et al., 2011) training set accuracy on manual tags, substantially lower than the mid-70% accuracy (Sridhar et al., 2009) often achieved on testing sets with supervised models. 113 Proceedings of the SIGDIAL 2014 Conference, pages 113–122, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Ass</context>
<context position="7048" citStr="Lee et al., 2013" startWordPosition="1062" endWordPosition="1065">(Bangalore et al., 2008; Marineau et al., 2000), tasksubtask structure (Boyer et al., 2010) acoustic and prosodic cues (Sridhar et al., 2009; Jurafsky et al., 1998), and body posture (Ha et al., 2012). For the growing body of work in unsupervised dialogue act classification a subset of these features have been utilized. The words (Crook et al., 2009), topic words (Ritter et al., 2010), function words (Ezen-Can and Boyer, 2013b), beginning portions of utterances (Rus et al., 2012), partof-speech tags and dependency trees (Joty et al., 2011), and state transition probabilities in Markov models (Lee et al., 2013) are among the list of features investigated for unsupervised modeling of dialogue acts. However, the accuracies achieved by the best of these models are well below the accuracies achieved by supervised techniques. To improve performance of unsupervised models for task-oriented dialogue, utilizing a combination of task and dialogue features is a promising direction. 3 Corpus The task-oriented dialogue corpus used in this work was collected in a computer-mediated human tutorial dialogue study. Students (n = 42) and tutors interacted through textual dialogue within an online learning environment</context>
</contexts>
<marker>Lee, Jeong, Kim, Ryu, 2013</marker>
<rawString>Donghyeon Lee, Minwoo Jeong, Kyungduk Kim, and Seonghan Ryu. 2013. Unsupervised spoken language understanding for a multi-domain dialog system. IEEE Transactions On Audio, Speech, and Language Processing, 21(11):2451–2464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna Marineau</author>
<author>Peter Wiemer-Hastings</author>
<author>Derek Harter</author>
<author>Brent Olde</author>
<author>Patrick Chipman</author>
<author>Ashish Karnavat</author>
<author>Victoria Pomeroy</author>
<author>Sonya Rajan</author>
</authors>
<title>Art Graesser, Tutoring Research Group, et al.</title>
<date>2000</date>
<booktitle>In Proceedings of the Workshop on Modeling Human Teaching Tactics and Strategies at the Intelligent Tutoring Systems Conference,</booktitle>
<pages>65--71</pages>
<contexts>
<context position="6478" citStr="Marineau et al., 2000" startWordPosition="967" endWordPosition="970">ng unsupervised dialogue act models that can be used in the next generation of task-oriented dialogue systems. 2 Related Work There is a rich body of work on dialogue act classification. Supervised approaches for dialogue act classification aimed at improving performance by using several features such as dialogue structure including position of the turn (Ferschke et al., 2012), speaker of an utterance (Tavafi et al., 2013), previous dialogue acts (Kim et al., 2010), lexical features such as words (Stolcke et al., 2000), syntactic features including part-of-speech tags (Bangalore et al., 2008; Marineau et al., 2000), tasksubtask structure (Boyer et al., 2010) acoustic and prosodic cues (Sridhar et al., 2009; Jurafsky et al., 1998), and body posture (Ha et al., 2012). For the growing body of work in unsupervised dialogue act classification a subset of these features have been utilized. The words (Crook et al., 2009), topic words (Ritter et al., 2010), function words (Ezen-Can and Boyer, 2013b), beginning portions of utterances (Rus et al., 2012), partof-speech tags and dependency trees (Joty et al., 2011), and state transition probabilities in Markov models (Lee et al., 2013) are among the list of feature</context>
</contexts>
<marker>Marineau, Wiemer-Hastings, Harter, Olde, Chipman, Karnavat, Pomeroy, Rajan, 2000</marker>
<rawString>Johanna Marineau, Peter Wiemer-Hastings, Derek Harter, Brent Olde, Patrick Chipman, Ashish Karnavat, Victoria Pomeroy, Sonya Rajan, Art Graesser, Tutoring Research Group, et al. 2000. Classification of speech acts in tutorial dialog. In Proceedings of the Workshop on Modeling Human Teaching Tactics and Strategies at the Intelligent Tutoring Systems Conference, pages 65–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Daniel Midgley</author>
<author>Shelly Harrison</author>
<author>Cara MacNish</author>
</authors>
<title>Empirical verification of adjacency pairs using dialogue segmentation.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGDIAL,</booktitle>
<pages>104--108</pages>
<contexts>
<context position="21357" citStr="Midgley et al., 2009" startWordPosition="3357" endWordPosition="3360">ever, adding task features to the dialogue context plus lexical features model does not provide benefit, and in fact slightly (not significantly) degrades performance (T + D + L &gt;� D + L). As reflected by the Kappa scores, the test set performance attained by these models is hardly better than would be expected by chance. 5.3 Utilizing Dialogue History The importance of dialogue history, particularly the influence of the most recent turn on an upcoming turn, is widely recognized within dialogue research, notably by work on adjacency pairs (Schegloff and Sacks, 1973; Forbes-Riley et al., 2007; Midgley et al., 2009). Based on these findings, we hypothesized that dialogue history would be substantially beneficial for unsupervised dialogue act models as it has been observed to be in numerous studies on supervised classification. However, as seen in the previous section, adding these dialogue context features with equal weight to the model using Cosine distance only improved its performance slightly though statistically significantly (for example, T+D &gt; T), while the overall performance is still barely above random chance. In an attempt to substantially boost the performance of the unsupervised dialogue act</context>
</contexts>
<marker>Midgley, Harrison, MacNish, 2009</marker>
<rawString>T. Daniel Midgley, Shelly Harrison, and Cara MacNish. 2009. Empirical verification of adjacency pairs using dialogue segmentation. In Proceedings of SIGDIAL, pages 104–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond Ng</author>
<author>Jiawei Han</author>
</authors>
<title>Efficient and effective clustering methods for spatial data mining.</title>
<date>1994</date>
<booktitle>In Proceedings of the 20th International Conference on Very Large Data Bases,</booktitle>
<pages>144--155</pages>
<contexts>
<context position="15251" citStr="Ng and Han, 1994" startWordPosition="2372" endWordPosition="2375">t features on unsupervised dialogue act models. We hypothesize that incorporating task features will significantly improve the performance of an unsupervised model, and we also hypothesize that properly incorporating dialogue context features, which are at a different granularity than the lexical features extracted from utterances, will substantially improve model accuracy. 5.1 Dialogue Act Modeling With k-medoids Clustering The unsupervised models investigated here use kmedoids clustering, which is a well-known clustering technique that takes actual data points as the center of each cluster (Ng and Han, 1994), in contrast to k-means which may have synthetic points as centroids. In k-medoids, the centroids are initially selected and then the algorithm iterates, reassigning data points in each iteration, until the clusters converge. In standard k-medoids clustering the initial seeds are selected randomly and then a correct distribution of data points is identified through the iteration and convergence process. For dialogue act classification, the influence of the initial seeds is substantial because the frequencies across dialogue tags are typically unbalanced. To overcome this challenge, we use a g</context>
</contexts>
<marker>Ng, Han, 1994</marker>
<rawString>Raymond Ng and Jiawei Han. 1994. Efficient and effective clustering methods for spatial data mining. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 144–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Reithinger</author>
<author>Martin Klesen</author>
</authors>
<title>Dialogue act classification using language models.</title>
<date>1997</date>
<booktitle>In Proceedings of EuroSpeech,</booktitle>
<pages>2235--2238</pages>
<contexts>
<context position="2135" citStr="Reithinger and Klesen, 1997" startWordPosition="311" endWordPosition="314">bles (Allen et al., 1995; Core and Allen, 1997; Crook et al., 2009; Traum, 1999), virtual personal assistants (Chen and Di Eugenio, 2013), conversational telephone speech (Stolcke et al., 2000), Wikipedia talk pages (Ferschke et al., 2012) and as in the case of this paper, tutorial dialogue (Serafin and Di Eugenio, 2004; Forbes-Riley and Litman, 2005; Boyer et al., 2011; Dzikovska et al., 2013). Most of the prior work on dialogue act classification has depended on manually applying dialogue act tags and then leveraging supervised machine learning (Di Eugenio et al., 2010; Keizer et al., 2002; Reithinger and Klesen, 1997; Serafin and Di Eugenio, 2004). This process involves engineering a dialogue act taxonomy (or using an existing one, though domain-specific phenomena can be difficult to capture within multi-purpose dialogue act taxonomies) and manually annotating each utterance in the corpus. Then, the tagged utterances are provided to a supervised machine learner. This supervised approach can achieve strong performance, in excess of 75% accuracy on manual tags, approaching the agreement level that is sometimes observed between human annotators (Sridhar et al., 2009; Serafin and Di Eugenio, 2004; Chen and Di</context>
</contexts>
<marker>Reithinger, Klesen, 1997</marker>
<rawString>Norbert Reithinger and Martin Klesen. 1997. Dialogue act classification using language models. In Proceedings of EuroSpeech, pages 2235–2238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of twitter conversations.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>172--180</pages>
<contexts>
<context position="6818" citStr="Ritter et al., 2010" startWordPosition="1025" endWordPosition="1028">n of the turn (Ferschke et al., 2012), speaker of an utterance (Tavafi et al., 2013), previous dialogue acts (Kim et al., 2010), lexical features such as words (Stolcke et al., 2000), syntactic features including part-of-speech tags (Bangalore et al., 2008; Marineau et al., 2000), tasksubtask structure (Boyer et al., 2010) acoustic and prosodic cues (Sridhar et al., 2009; Jurafsky et al., 1998), and body posture (Ha et al., 2012). For the growing body of work in unsupervised dialogue act classification a subset of these features have been utilized. The words (Crook et al., 2009), topic words (Ritter et al., 2010), function words (Ezen-Can and Boyer, 2013b), beginning portions of utterances (Rus et al., 2012), partof-speech tags and dependency trees (Joty et al., 2011), and state transition probabilities in Markov models (Lee et al., 2013) are among the list of features investigated for unsupervised modeling of dialogue acts. However, the accuracies achieved by the best of these models are well below the accuracies achieved by supervised techniques. To improve performance of unsupervised models for task-oriented dialogue, utilizing a combination of task and dialogue features is a promising direction. 3</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations. In Proceedings of the Association for Computational Linguistics, pages 172–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
<author>Cristian Moldovan</author>
<author>Nobal Niraula</author>
<author>Arthur C Graesser</author>
</authors>
<title>Automated discovery of speech act categories in educational games.</title>
<date>2012</date>
<booktitle>In International Conference on Educational Data Mining,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="3489" citStr="Rus et al., 2012" startWordPosition="520" endWordPosition="523">and applying them manually tend to be bottlenecks within the research and design process. To overcome these drawbacks, the field has recently seen growing momentum surrounding unsupervised approaches, which do not require any manual labels during model training (Crook et al., 2009; Joty et al., 2011; Lee et al., 2013). A variety of unsupervised machine learning techniques have been investigated for dialogue act classification, and each line of investigation has explored which features best support this goal. However, to date the best performing unsupervised models achieve in the range of 40% (Rus et al., 2012) to 60% (Joty et al., 2011) training set accuracy on manual tags, substantially lower than the mid-70% accuracy (Sridhar et al., 2009) often achieved on testing sets with supervised models. 113 Proceedings of the SIGDIAL 2014 Conference, pages 113–122, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics In order to close this performance gap between unsupervised and supervised techniques, we suggest that it is crucial to enrich the features available to unsupervised models. In particular, when a dialogue is task-oriented and includes a rich source of informa</context>
<context position="6915" citStr="Rus et al., 2012" startWordPosition="1041" endWordPosition="1044">gue acts (Kim et al., 2010), lexical features such as words (Stolcke et al., 2000), syntactic features including part-of-speech tags (Bangalore et al., 2008; Marineau et al., 2000), tasksubtask structure (Boyer et al., 2010) acoustic and prosodic cues (Sridhar et al., 2009; Jurafsky et al., 1998), and body posture (Ha et al., 2012). For the growing body of work in unsupervised dialogue act classification a subset of these features have been utilized. The words (Crook et al., 2009), topic words (Ritter et al., 2010), function words (Ezen-Can and Boyer, 2013b), beginning portions of utterances (Rus et al., 2012), partof-speech tags and dependency trees (Joty et al., 2011), and state transition probabilities in Markov models (Lee et al., 2013) are among the list of features investigated for unsupervised modeling of dialogue acts. However, the accuracies achieved by the best of these models are well below the accuracies achieved by supervised techniques. To improve performance of unsupervised models for task-oriented dialogue, utilizing a combination of task and dialogue features is a promising direction. 3 Corpus The task-oriented dialogue corpus used in this work was collected in a computer-mediated </context>
</contexts>
<marker>Rus, Moldovan, Niraula, Graesser, 2012</marker>
<rawString>Vasile Rus, Cristian Moldovan, Nobal Niraula, and Arthur C. Graesser. 2012. Automated discovery of speech act categories in educational games. In International Conference on Educational Data Mining, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel A Schegloff</author>
<author>Harvey Sacks</author>
</authors>
<title>Opening up closings.</title>
<date>1973</date>
<journal>Semiotica,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="21307" citStr="Schegloff and Sacks, 1973" startWordPosition="3348" endWordPosition="3352">ures provides significant improvement (T + L &gt; L). However, adding task features to the dialogue context plus lexical features model does not provide benefit, and in fact slightly (not significantly) degrades performance (T + D + L &gt;� D + L). As reflected by the Kappa scores, the test set performance attained by these models is hardly better than would be expected by chance. 5.3 Utilizing Dialogue History The importance of dialogue history, particularly the influence of the most recent turn on an upcoming turn, is widely recognized within dialogue research, notably by work on adjacency pairs (Schegloff and Sacks, 1973; Forbes-Riley et al., 2007; Midgley et al., 2009). Based on these findings, we hypothesized that dialogue history would be substantially beneficial for unsupervised dialogue act models as it has been observed to be in numerous studies on supervised classification. However, as seen in the previous section, adding these dialogue context features with equal weight to the model using Cosine distance only improved its performance slightly though statistically significantly (for example, T+D &gt; T), while the overall performance is still barely above random chance. In an attempt to substantially boos</context>
</contexts>
<marker>Schegloff, Sacks, 1973</marker>
<rawString>Emanuel A. Schegloff and Harvey Sacks. 1973. Opening up closings. Semiotica, 8(4):289–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Searle</author>
</authors>
<title>Speech Acts: An Essay in the Philosophy of Language.</title>
<date>1969</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1266" citStr="Searle, 1969" startWordPosition="173" endWordPosition="174">ervised dialogue act model trained on a corpus of human tutoring in introductory computer science. Experimental results show that incorporating task features and dialogue history features significantly improve unsupervised dialogue act classification, particularly within a hierarchical framework that gives prominence to dialogue history. This work constitutes a step toward building high-performing unsupervised dialogue act models that will be used in the next generation of task-oriented dialogue systems. 1 Introduction Dialogue acts represent the underlying intent of utterances (Austin, 1975; Searle, 1969), and constitute a crucial level of representation for dialogue systems (Sridhar et al., 2009). The task of automatic dialogue act classification has been extensively studied for decades within several domains including train fares and timetables (Allen et al., 1995; Core and Allen, 1997; Crook et al., 2009; Traum, 1999), virtual personal assistants (Chen and Di Eugenio, 2013), conversational telephone speech (Stolcke et al., 2000), Wikipedia talk pages (Ferschke et al., 2012) and as in the case of this paper, tutorial dialogue (Serafin and Di Eugenio, 2004; Forbes-Riley and Litman, 2005; Boye</context>
</contexts>
<marker>Searle, 1969</marker>
<rawString>John R. Searle. 1969. Speech Acts: An Essay in the Philosophy of Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Riccardo Serafin</author>
<author>Barbara Di Eugenio</author>
</authors>
<title>FLSA: Extending latent semantic analysis with features for dialogue act classification.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>692--699</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Serafin, Di Eugenio, 2004</marker>
<rawString>Riccardo Serafin and Barbara Di Eugenio. 2004. FLSA: Extending latent semantic analysis with features for dialogue act classification. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 692–699. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rangarajan Sridhar</author>
<author>Vivek Kumar</author>
<author>Srinivas Bangalore</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>Combining lexical, syntactic and prosodic cues for improved online dialog act tagging.</title>
<date>2009</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="1360" citStr="Sridhar et al., 2009" startWordPosition="186" endWordPosition="189">ter science. Experimental results show that incorporating task features and dialogue history features significantly improve unsupervised dialogue act classification, particularly within a hierarchical framework that gives prominence to dialogue history. This work constitutes a step toward building high-performing unsupervised dialogue act models that will be used in the next generation of task-oriented dialogue systems. 1 Introduction Dialogue acts represent the underlying intent of utterances (Austin, 1975; Searle, 1969), and constitute a crucial level of representation for dialogue systems (Sridhar et al., 2009). The task of automatic dialogue act classification has been extensively studied for decades within several domains including train fares and timetables (Allen et al., 1995; Core and Allen, 1997; Crook et al., 2009; Traum, 1999), virtual personal assistants (Chen and Di Eugenio, 2013), conversational telephone speech (Stolcke et al., 2000), Wikipedia talk pages (Ferschke et al., 2012) and as in the case of this paper, tutorial dialogue (Serafin and Di Eugenio, 2004; Forbes-Riley and Litman, 2005; Boyer et al., 2011; Dzikovska et al., 2013). Most of the prior work on dialogue act classification</context>
<context position="2692" citStr="Sridhar et al., 2009" startWordPosition="394" endWordPosition="397"> et al., 2010; Keizer et al., 2002; Reithinger and Klesen, 1997; Serafin and Di Eugenio, 2004). This process involves engineering a dialogue act taxonomy (or using an existing one, though domain-specific phenomena can be difficult to capture within multi-purpose dialogue act taxonomies) and manually annotating each utterance in the corpus. Then, the tagged utterances are provided to a supervised machine learner. This supervised approach can achieve strong performance, in excess of 75% accuracy on manual tags, approaching the agreement level that is sometimes observed between human annotators (Sridhar et al., 2009; Serafin and Di Eugenio, 2004; Chen and Di Eugenio, 2013). However, the supervised approach has several major drawbacks, including the fact that handcrafting dialogue act tagsets and applying them manually tend to be bottlenecks within the research and design process. To overcome these drawbacks, the field has recently seen growing momentum surrounding unsupervised approaches, which do not require any manual labels during model training (Crook et al., 2009; Joty et al., 2011; Lee et al., 2013). A variety of unsupervised machine learning techniques have been investigated for dialogue act class</context>
<context position="6571" citStr="Sridhar et al., 2009" startWordPosition="982" endWordPosition="985">alogue systems. 2 Related Work There is a rich body of work on dialogue act classification. Supervised approaches for dialogue act classification aimed at improving performance by using several features such as dialogue structure including position of the turn (Ferschke et al., 2012), speaker of an utterance (Tavafi et al., 2013), previous dialogue acts (Kim et al., 2010), lexical features such as words (Stolcke et al., 2000), syntactic features including part-of-speech tags (Bangalore et al., 2008; Marineau et al., 2000), tasksubtask structure (Boyer et al., 2010) acoustic and prosodic cues (Sridhar et al., 2009; Jurafsky et al., 1998), and body posture (Ha et al., 2012). For the growing body of work in unsupervised dialogue act classification a subset of these features have been utilized. The words (Crook et al., 2009), topic words (Ritter et al., 2010), function words (Ezen-Can and Boyer, 2013b), beginning portions of utterances (Rus et al., 2012), partof-speech tags and dependency trees (Joty et al., 2011), and state transition probabilities in Markov models (Lee et al., 2013) are among the list of features investigated for unsupervised modeling of dialogue acts. However, the accuracies achieved b</context>
</contexts>
<marker>Sridhar, Kumar, Bangalore, Narayanan, 2009</marker>
<rawString>Rangarajan Sridhar, Vivek Kumar, Srinivas Bangalore, and Shrikanth Narayanan. 2009. Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech &amp; Language, 23(4):407–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Daniel Jurafsky</author>
<author>Paul Taylor</author>
<author>Rachel Martin</author>
<author>Carol Van Ess-Dykema</author>
<author>Marie Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van Ess-Dykema, Meteer, 2000</marker>
<rawString>Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maryam Tavafi</author>
<author>Yashar Mehdad</author>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
</authors>
<title>Dialogue act recognition in synchronous and asynchronous conversations.</title>
<date>2013</date>
<booktitle>In Proceedings of SIGDIAL,</booktitle>
<pages>117--121</pages>
<contexts>
<context position="6282" citStr="Tavafi et al., 2013" startWordPosition="936" endWordPosition="939">the model’s performance by dialogue act and investigate which features are most beneficial for distinguishing particular acts. These contributions constitute a step toward building high-performing unsupervised dialogue act models that can be used in the next generation of task-oriented dialogue systems. 2 Related Work There is a rich body of work on dialogue act classification. Supervised approaches for dialogue act classification aimed at improving performance by using several features such as dialogue structure including position of the turn (Ferschke et al., 2012), speaker of an utterance (Tavafi et al., 2013), previous dialogue acts (Kim et al., 2010), lexical features such as words (Stolcke et al., 2000), syntactic features including part-of-speech tags (Bangalore et al., 2008; Marineau et al., 2000), tasksubtask structure (Boyer et al., 2010) acoustic and prosodic cues (Sridhar et al., 2009; Jurafsky et al., 1998), and body posture (Ha et al., 2012). For the growing body of work in unsupervised dialogue act classification a subset of these features have been utilized. The words (Crook et al., 2009), topic words (Ritter et al., 2010), function words (Ezen-Can and Boyer, 2013b), beginning portions</context>
</contexts>
<marker>Tavafi, Mehdad, Joty, Carenini, Ng, 2013</marker>
<rawString>Maryam Tavafi, Yashar Mehdad, Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2013. Dialogue act recognition in synchronous and asynchronous conversations. In Proceedings of SIGDIAL, pages 117– 121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
</authors>
<title>Speech acts for dialogue agents.</title>
<date>1999</date>
<booktitle>In Foundations of Rational Agency,</booktitle>
<pages>169--201</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1588" citStr="Traum, 1999" startWordPosition="225" endWordPosition="226">ialogue history. This work constitutes a step toward building high-performing unsupervised dialogue act models that will be used in the next generation of task-oriented dialogue systems. 1 Introduction Dialogue acts represent the underlying intent of utterances (Austin, 1975; Searle, 1969), and constitute a crucial level of representation for dialogue systems (Sridhar et al., 2009). The task of automatic dialogue act classification has been extensively studied for decades within several domains including train fares and timetables (Allen et al., 1995; Core and Allen, 1997; Crook et al., 2009; Traum, 1999), virtual personal assistants (Chen and Di Eugenio, 2013), conversational telephone speech (Stolcke et al., 2000), Wikipedia talk pages (Ferschke et al., 2012) and as in the case of this paper, tutorial dialogue (Serafin and Di Eugenio, 2004; Forbes-Riley and Litman, 2005; Boyer et al., 2011; Dzikovska et al., 2013). Most of the prior work on dialogue act classification has depended on manually applying dialogue act tags and then leveraging supervised machine learning (Di Eugenio et al., 2010; Keizer et al., 2002; Reithinger and Klesen, 1997; Serafin and Di Eugenio, 2004). This process involve</context>
</contexts>
<marker>Traum, 1999</marker>
<rawString>David R. Traum. 1999. Speech acts for dialogue agents. In Foundations of Rational Agency, pages 169–201. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004288">
<title confidence="0.939411">
LIMSI @ WMT’14 Medical Translation Task
</title>
<author confidence="0.8280342">
Nicolas P´echeux1,2, Li Gong1,2, Quoc Khanh Do1,2, Benjamin Marie2,3,
Yulia Ivanishcheva2,4, Alexandre Allauzen1,2, Thomas Lavergne1,2,
Jan Niehues2, Aur´elien Max1,2, Franc¸ois Yvon2
Univ. Paris-Sud1, LIMSI-CNRS2
B.P. 133, 91403 Orsay, France
</author>
<affiliation confidence="0.452281">
Lingua et Machina3, Centre Cochrane franc¸ais4
</affiliation>
<email confidence="0.992812">
{firstname.lastname}@limsi.fr
</email>
<sectionHeader confidence="0.993688" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941722222222">
This paper describes LIMSI’s submission
to the first medical translation task at
WMT’14. We report results for English-
French on the subtask of sentence trans-
lation from summaries of medical ar-
ticles. Our main submission uses a
combination of NCODE (n-gram-based)
and MOSES (phrase-based) output and
continuous-space language models used in
a post-processing step for each system.
Other characteristics of our submission in-
clude: the use of sampling for building
MOSES’ phrase table; the implementation
of the vector space model proposed by
Chen et al. (2013); adaptation of the POS-
tagger used by NCODE to the medical do-
main; and a report of error analysis based
on the typology of Vilar et al. (2006).
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969117647059">
This paper describes LIMSI’s submission to the
first medical translation task at WMT’14. This
task is characterized by high-quality input text
and the availability of large amounts of training
data from the same domain, yielding unusually
high translation performance. This prompted us
to experiment with two systems exploring differ-
ent translation spaces, the n-gram-based NCODE
(§2.1) and an on-the-fly variant of the phrase-
based MOSES (§2.2), and to later combine their
output. Further attempts at improving translation
quality were made by resorting to continuous lan-
guage model rescoring (§2.4), vector space sub-
corpus adaptation (§2.3), and POS-tagging adap-
tation to the medical domain (§3.3). We also per-
formed a small-scale error analysis of the outputs
of some of our systems (§5).
</bodyText>
<sectionHeader confidence="0.7759645" genericHeader="method">
2 System Overview
2.1 NCODE
</sectionHeader>
<bodyText confidence="0.999916368421053">
NCODE implements the bilingual n-gram ap-
proach to SMT (Casacuberta and Vidal, 2004;
Mari˜no et al., 2006; Crego and Mari˜no, 2006) that
is closely related to the standard phrase-based ap-
proach (Zens et al., 2002). In this framework, the
translation is divided into two steps. To translate
a source sentence f into a target sentence e, the
source sentence is first reordered according to a
set of rewriting rules so as to reproduce the tar-
get word order. This generates a word lattice con-
taining the most promising source permutations,
which is then translated. Since the translation step
is monotonic, the peculiarity of this approach is to
rely on the n-gram assumption to decompose the
joint probability of a sentence pair in a sequence
of bilingual units called tuples.
The best translation is selected by maximizing
a linear combination of feature functions using the
following inference rule:
</bodyText>
<equation confidence="0.983411333333333">
K
e∗ = argmax Akfk(f, e, a) (1)
e,a k=1
</equation>
<bodyText confidence="0.9999936875">
where K feature functions (fk) are weighted by
a set of coefficients (Ak) and a denotes the set of
hidden variables corresponding to the reordering
and segmentation of the source sentence. Along
with the n-gram translation models and target n-
gram language models, 13 conventional features
are combined: 4 lexicon models similar to the ones
used in standard phrase-based systems; 6 lexical-
ized reordering models (Tillmann, 2004; Crego et
al., 2011) aimed at predicting the orientation of
the next translation unit; a “weak” distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. Features
are estimated during the training phase. Training
source sentences are first reordered so as to match
</bodyText>
<page confidence="0.979498">
246
</page>
<note confidence="0.714489">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 246–253,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999825916666667">
the target word order by unfolding the word align-
ments (Crego and Mari˜no, 2006). Tuples are then
extracted in such a way that a unique segmenta-
tion of the bilingual corpus is achieved (Mari˜no et
al., 2006) and n-gram translation models are then
estimated over the training corpus composed of tu-
ple sequences made of surface forms or POS tags.
Reordering rules are automatically learned during
the unfolding procedure and are built using part-
of-speech (POS), rather than surface word forms,
to increase their generalization power (Crego and
Mari˜no, 2006).
</bodyText>
<subsectionHeader confidence="0.999636">
2.2 On-the-fly System (OTF)
</subsectionHeader>
<bodyText confidence="0.9998252">
We develop an alternative approach implement-
ing an on-the-fly estimation of the parameter of
a standard phrase-based model as in (Le et al.,
2012b), also adding an inverse translation model.
Given an input source file, it is possible to compute
only those statistics which are required to trans-
late the phrases it contains. As in previous works
on on-the-fly model estimation for SMT (Callison-
Burch et al., 2005; Lopez, 2008), we first build
a suffix array for the source corpus. Only a lim-
ited number of translation examples, selected by
deterministic random sampling, are then used by
traversing the suffix array appropriately. A coher-
ent translation probability (Lopez, 2008) (which
also takes into account examples where translation
extraction failed) is then estimated. As we cannot
compute exactly an inverse translation probability
(because sampling is performed independently for
each source phrase), we resort to the following ap-
proximation:
</bodyText>
<equation confidence="0.856794">
p( (¯f|¯e) = min 1.0, p(¯e|
</equation>
<bodyText confidence="0.999887333333333">
where the freq(·) is the number of occurrences of
the given phrase in the whole corpus, and the nu-
merator p(¯e |¯f)xfreq(¯f) represents the predicted
joint count of f¯ and ¯e. The other models in this
system are the same as in the default configuration
of MOSES.
</bodyText>
<subsectionHeader confidence="0.986323">
2.3 Vector Space Model (VSM)
</subsectionHeader>
<bodyText confidence="0.987088766666666">
We used the vector space model (VSM) of Chen
et al. (2013) to perform domain adaptation. In
this approach, each phrase pair ( ¯f, ¯e) present in
the phrase table is represented by a C-dimensional
vector of TF-IDF scores, one for each sub-corpus,
where C represents the number of sub-corpora
(see Table 1). Each component wc( f, ¯e) is a stan-
dard TF-IDF weight of each phrase pair for the
cth sub-corpus. TF(¯f, ¯e) is the raw joint count of
( ¯f, ¯e) in the sub-corpus; the IDF(¯f, ¯e) is the in-
verse document frequency across all sub-corpora.
A similar C-dimensional representation of the
development set is computed as follows: we first
perform word alignment and phrase pairs extrac-
tion. For each extracted phrase pair, we compute
its TF-IDF vector and finally combine all vectors
to obtain the vector for the develompent set:
fj, ¯ek)wc( fj, ¯ek) (3)
where J and K are the total numbers of source
and target phrases extracted from the development
data, respectively, and countdev( fj, ¯ek) is the joint
count of phrase pairs ( fj, ¯ek) found in the devel-
opment set. The similarity score between each
phrase pair’s vector and the development set vec-
tor is added into the phrase table as a VSM fea-
ture. We also replace the joint count with the
marginal count of the source/target phrase to com-
pute an alternative average representation for the
development set, thus adding two VSM additional
features.
</bodyText>
<subsectionHeader confidence="0.957278">
2.4 SOUL
</subsectionHeader>
<bodyText confidence="0.999850045454545">
Neural networks, working on top of conventional
n-gram back-off language models, have been in-
troduced in (Bengio et al., 2003; Schwenk et al.,
2006) as a potential means to improve discrete
language models. As for our submitted transla-
tion systems to WMT’12 and WMT’13 (Le et al.,
2012b; Allauzen et al., 2013), we take advantage
of the recent proposal of (Le et al., 2011). Using
a specific neural network architecture, the Struc-
tured OUtput Layer (SOUL), it becomes possible
to estimate n-gram models that use large vocab-
ulary, thereby making the training of large neural
network language models feasible both for target
language models and translation models (Le et al.,
2012a). Moreover, the peculiar parameterization
of continuous models allows us to consider longer
dependencies than the one used by conventional
n-gram models (e.g. n = 10 instead of n = 4).
Additionally, continuous models can also be
easily and efficiently adapted as in (Lavergne et
al., 2011). Starting from a previously trained
SOUL model, only a few more training epochs are
</bodyText>
<equation confidence="0.614362666666667">
f) x freq( f) �(2)
freq(¯e)
J
j=0
countdev(
wdev
c =
K
k=0
</equation>
<page confidence="0.986729">
247
</page>
<table confidence="0.999650833333333">
Corpus Sentences Tokens (en-fr) Description wrd-lm pos-lm
COPPA 454246 10-12M -3 -15
EMEA 324189 6-7M 26 -1
PATTR-ABSTRACTS 634616 20-24M 22 21
in-domain PATTR-CLAIMS 888725 32-36M 6 2
PATTR-TITLES 385 829 3-4M 4 -17
UMLS 2166 612 8-8M term dictionary -7 -22
WIKIPEDIA 8421 17-18k short titles -5 -13
NEWSCOMMENTARY 171277 4-5M 6 16
out-of-domain EUROPARL 1982 937 54-60M -7 -33
GIGA 9 625 480 260-319M 27 52
all parallel all 17M 397-475M concatenation 33 69
medical-data -146M 69 -
target-lm wmt13-data -2 536M 49 -
DEVEL 500 10-12k khresmoi-summary
devel/test LMTEST 3 000 61-69k see Section 3.4
NEWSTEST12 3003 73-82k from WMT’12
TEST 1000 21-26k khresmoi-summary
</table>
<tableCaption confidence="0.997894">
Table 1: Parallel corpora used in this work, along with the number of sentences and the number of English
</tableCaption>
<bodyText confidence="0.9904195">
and French tokens, respectively. Weights (λk) from our best NCODE configuration are indicated for each
sub-corpora’s bilingual word language model (wrd-lm) and POS factor language model (pos-lm).
needed on a new corpus in order to adapt the pa-
rameters to the new domain.
</bodyText>
<sectionHeader confidence="0.948997" genericHeader="method">
3 Data and Systems Preparation
</sectionHeader>
<subsectionHeader confidence="0.994165">
3.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999989789473684">
We use all the available (constrained) medical data
extracted using the scripts provided by the orga-
nizers. This resulted in 7 sub-corpora from the
medical domain with distinctive features. As out-
of-domain data, we reuse the data processed for
WMT’13 (Allauzen et al., 2013).
For pre-processing of medical data, we closely
followed (Allauzen et al., 2013) so as to be able to
directly integrate existing translation and language
models, using in-house text processing tools for
tokenization and detokenization steps (D´echelotte
et al., 2008). All systems are built using a
“true case” scheme, but sentences fully capital-
ized (plentiful especially in PATTR-TITLES) are
previously lowercased. Duplicate sentence pairs
are removed, yielding a sentence reduction up to
70% for EMEA. Table 1 summarizes the data used
along with some statistics after the cleaning and
pre-processing steps.
</bodyText>
<subsectionHeader confidence="0.999175">
3.2 Language Models
</subsectionHeader>
<bodyText confidence="0.9999545">
A medical-domain 4-gram language model is built
by concatenating the target side of the paral-
lel data and all the available monolingual data1,
with modified Kneser-Ney smoothing (Kneser and
Ney, 1995; Chen and Goodman, 1996), using the
SRILM (Stolcke, 2002) and KENLM (Heafield,
2011) toolkits. Although more similar to term-to-
term dictionaries, UMLS and WIKIPEDIA proved
better to be included in the language model.
The large out-of-domain language model used for
WMT’13 (Allauzen et al., 2013) is additionaly
used (see Table 1).
</bodyText>
<subsectionHeader confidence="0.999217">
3.3 Part-of-Speech Tagging
</subsectionHeader>
<bodyText confidence="0.999957176470588">
Medical data exhibit many peculiarities, includ-
ing different syntactic constructions and a specific
vocabulary. As standard POS-taggers are known
not to perform very well for this type of texts, we
use a specific model trained on the Penn Treebank
and on medical data from the MedPost project
(Smith et al., 2004). We use Wapiti (Lavergne
et al., 2010), a state-of-the-art CRF implementa-
tion, with a standard feature set. Adaptation is per-
formed as in (Chelba and Acero, 2004) using the
out-of-domain model as a prior when training the
in-domain model on medical data. On a medical
test set, this adaptation leads to a 8 point reduc-
tion of the error rate. A standard model is used for
WMT’13 data. For the French side, due to the lack
of annotaded data for the medical domain, corpora
are tagged using the TreeTagger (Schmid, 1994).
</bodyText>
<footnote confidence="0.937636">
1Attempting include one language model per sub-corpora
yielded a significant drop in performance.
</footnote>
<page confidence="0.981965">
248
</page>
<subsectionHeader confidence="0.903083">
3.4 Proxy Test Set
</subsectionHeader>
<bodyText confidence="0.999876555555556">
For this first edition of a Medical Translation Task,
only a very small development set was made avail-
able (DEVEL in Table 1). This made both system
design and tuning challenging. In fact, with such a
small development set, conventional tuning meth-
ods are known to be very unstable and prone to
overfitting, and it would be suboptimal to select
a configuration based on results on the develop-
ment set only.2 To circumvent this, we artificially
created our own internal test set by randomly se-
lecting 3 000 sentences out from the 30 000 sen-
tences from PATTR-ABSTRACTS having the low-
est perplexity according to 3-gram language mod-
els trained on both sides of the DEVEL set. This
test set, denoted by LMTEST, is however highly
biaised, especially because of the high redundancy
in PATTR-ABSTRACTS, and should be used with
great care when tuning or comparing systems.
</bodyText>
<subsectionHeader confidence="0.599299">
3.5 Systems
</subsectionHeader>
<bodyText confidence="0.999736291666667">
NCODE We use NCODE with default settings, 3-
gram bilingual translation models on words and 4-
gram bilingual translation factor models on POS,
for each included corpora (see Table 1) and for the
concatenation of them all.
OTF When using our OTF system, all in-
domain and out-of-domain data are concatenated,
respectively. For both corpora, we use a maxi-
mum random sampling size of 1000 examples and
a maximum phrase length of 15. However, all
sub-corpora but GIGA3 are used to compute the
vectors for VSM features. Decoding is done with
MOSES4 (Koehn et al., 2007).
SOUL Given the computational cost of com-
puting n-gram probabilities with neural network
models, we resort to a reranking approach. In
the following experiments, we use 10-gram SOUL
models to rescore 1000-best lists. SOUL models
provide five new features: a target language model
score and four translation scores (Le et al., 2012a).
We reused the SOUL models trained for our par-
ticipation to WMT’12 (Le et al., 2012b). More-
over, target language models are adapted by run-
ning 6 more epochs on the new medical data.
</bodyText>
<footnote confidence="0.9991742">
2This issue is traditionally solved in Machine Learning by
folded cross-validation, an approach that would be too pro-
hibitive to use here.
3The GIGA corpus is actually very varied in content.
4http://www.statmt.org/moses/
</footnote>
<bodyText confidence="0.996633173913044">
System Combination As NCODE and OTF dif-
fer in many aspects and make different errors, we
use system combination techniques to take advan-
tage of their complementarity. This is done by
reranking the concatenation of the 1000-best lists
of both systems. For each hypothesis within this
list, we use two global features, corresponding
either to the score computed by the correspond-
ing system or 0 otherwise. We then learn rerank-
ing weights using Minimum Error Rate Training
(MERT) (Och, 2003) on the development set for
this combined list, using only these two features
(SysComb-2). In an alternative configuration, we
use the two systems without the SOUL rescoring,
and add instead the five SOUL scores as features in
the system combination reranking (SysComb-7).
Evaluation Metrics All BLEU scores (Pap-
ineni et al., 2002) are computed using cased
multi-bleu with our internal tokenization. Re-
ported results correspond to the average and stan-
dard deviation across 3 optimization runs to bet-
ter account for the optimizer variance (Clark et al.,
2011).
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99932">
4.1 Tuning Optimization Method
</subsectionHeader>
<bodyText confidence="0.9997875">
MERT is usually used to optimize Equation 1.
However, with up to 42 features when using
SOUL, this method is known to become very sen-
sitive to local minima. Table 2 compares MERT,
a batch variant of the Margin Infused Relaxation
Algorithm (MIRA) (Cherry and Foster, 2012) and
PRO (Hopkins and May, 2011) when tuning an
NCODE system. MIRA slightly outperforms PRO
on DEVEL, but seems prone to overfitting. How-
ever this was not possible to detect before the re-
lease of the test set (TEST), and so we use MIRA
in all our experiments.
</bodyText>
<table confidence="0.988802">
DEVEL TEST
MERT 47.0± 0.4 44.1± 0.8
MIRA 47.9± 0.0 44.8± 0.1
PRO 47.1± 0.1 45.1± 0.1
</table>
<tableCaption confidence="0.954122666666667">
Table 2: Impact of the optimization method during
the tuning process on BLEU score, for a baseline
NCODE system.
</tableCaption>
<page confidence="0.997905">
249
</page>
<subsectionHeader confidence="0.971656">
4.2 Importance of the Data Sources
</subsectionHeader>
<bodyText confidence="0.998568909090909">
Table 3 shows that using the out-of-domain data
from WMT’13 yields better scores than only using
the provided medical data only. Moreover, com-
bining both data sources drastically boosts perfor-
mance. Table 1 displays the weights (λk) given by
NCODE to the different sub-corpora bilingual lan-
guage models. Three corpora seems particulary
useful: EMEA, PATTR-ABSTRACTS and GIGA.
Note that several models are given a negative
weight, but removing them from the model sur-
prisingly results in a drop of performance.
</bodyText>
<table confidence="0.98730475">
DEVEL TEST
medical 42.2± 0.1 39.6± 0.1
WMT’13 43.0± 0.1 41.0± 0.0
both 48.3± 0.1 45.4± 0.0
</table>
<tableCaption confidence="0.992818">
Table 3: BLEU scores obtained by NCODE trained
on medical data only, WMT’13 data only, or both.
</tableCaption>
<subsectionHeader confidence="0.996928">
4.3 Part-of-Speech Tagging
</subsectionHeader>
<bodyText confidence="0.991508529411764">
Using the specialized POS-tagging models for
medical data described in Section 3.3 instead of a
standart POS-tagger, a 0.5 BLEU points increase
is observed. Table 4 suggests that a better POS
tagging quality is mainly beneficial to the reorder-
ing mechanism in NCODE, in contrast with the
POS-POS factor models included as features.
Reordering Factor model DEVEL TEST
std std 47.9± 0.0 44.8± 0.1
std spec 47.9± 0.1 45.0± 0.1
spec std 48.4± 0.1 45.3± 0.1
spec spec 48.3± 0.1 45.4± 0.0
Table 4: BLEU results when using a standard POS
tagging (std) or our medical adapted specialized
method (spec), either for the reordering rule mech-
anism (Reordering) or for the POS-POS bilingual
language models features (Factor model).
</bodyText>
<subsectionHeader confidence="0.994876">
4.4 Development and Proxy Test Sets
</subsectionHeader>
<bodyText confidence="0.9995364">
In Table 5, we assess the importance of domain
adaptation via tuning on the development set used
and investigate the benefits of our internal test set.
Best scores are obtained when using the pro-
vided development set in the tuning process. Us-
</bodyText>
<table confidence="0.99757525">
DEVEL LMTEST NEWSTEST12 TEST
48.3± 0.1 46.8± 0.1 26.2± 0.1 45.4± 0.0
41.8± 0.2 48.9± 0.1 18.5± 0.1 40.1± 0.1
39.8± 0.1 37.4± 0.2 29.0± 0.1 39.0± 0.3
</table>
<tableCaption confidence="0.9102122">
Table 5: Influence of the choice of the develop-
ment set when using our baseline NCODE system.
Each row corresponds to the choice of a develop-
ment set used in the tuning process, indicated by a
surrounded BLEU score.
Table 6: Contrast of our two main systems and
their combination, when adding SOUL language
(LM) and translation (TM) models. Stars indicate
an adapted LM. BLEU results for the best run on
the development set are reported.
</tableCaption>
<table confidence="0.999127785714286">
DEVEL TEST
NCODE 48.5 45.2
+ SOUL LM 49.4 45.7
+ SOUL LM* 49.8 45.9
+ SOUL LM + TM 50.1 47.0
+ SOUL LM*+ TM 50.1 47.0
OTF 46.6 42.5
+ VSM 46.9 42.8
+ SOUL LM 48.6 44.0
+ SOUL LM* 48.4 44.2
+ SOUL LM + TM 49.6 44.8
+ SOUL LM*+ TM 49.7 44.9
SysComb-2 50.5 46.6
SysComb-7 50.7 46.5
</table>
<bodyText confidence="0.999213">
ing NEWSTEST12 as development set unsurpris-
ingly leads to poor results, as no domain adapta-
tion is carried out. However, using LMTEST does
not result in much better TEST score. We also note
a positive correlation between DEVEL and TEST.
From the first three columns, we decided to use the
DEVEL data set as development set for our sub-
mission, which is a posteriori the right choice.
</bodyText>
<subsectionHeader confidence="0.961102">
4.5 NCODE vs. OTF
</subsectionHeader>
<bodyText confidence="0.99731875">
Table 6 contrasts our different approaches. Prelim-
inary experiments suggest that OTF is a compara-
ble but cheaper alternative to a full MOSES sys-
tem.5 We find a large difference in performance,
</bodyText>
<footnote confidence="0.996618666666667">
5A control experiment for a full MOSES system (using a
single phrase table) yielded a BLEU score of 45.9 on DEVEL
and 43.2 on TEST, and took 3 more days to complete.
</footnote>
<page confidence="0.984184">
250
</page>
<table confidence="0.9725645">
extra missing incorrect unknown
word content filler disamb. form style term order word term all
syscomb 4 13 20 47 62 8 18 21 1 11 205
OTF+VSM+SOUL 4 4 31 44 82 6 20 42 3 12 248
</table>
<tableCaption confidence="0.99675">
Table 7: Results for manual error analysis following (Vilar et al., 2006) for the first 100 test sentences.
</tableCaption>
<bodyText confidence="0.999185">
NCODE outperforming OTF by 2.8 BLEU points
on the TEST set. VSM does not yield any signifi-
cant improvement, contrarily to the work of Chen
et al. (2013); it may be the case all individual sub-
corpus are equally good (or bad) at approximating
the stylistic preferences of the TEST set.
</bodyText>
<subsectionHeader confidence="0.975501">
4.6 Integrating SOUL
</subsectionHeader>
<bodyText confidence="0.999939571428571">
Table 6 shows the substantial impact of adding
SOUL models for both baseline systems. With
only the SOUL LM, improvements on the test set
range from 0.5 BLEU points for NCODE system
to 1.2 points for the OTF system. The adaptation
of SOUL LM with the medical data brings an ad-
ditional improvement of about 0.2 BLEU points.
Adding all SOUL translation models yield an
improvement of 1.8 BLEU points for NCODE and
of 2.4 BLEU points with the OTF system using
VSM models. However, the SOUL adaptation step
has then only a modest impact. In future work, we
plan to also adapt the translation models in order
to increase the benefit of using in-domain data.
</bodyText>
<subsectionHeader confidence="0.993806">
4.7 System Combination
</subsectionHeader>
<bodyText confidence="0.9998930625">
Table 6 shows that performing the system combi-
nation allows a gain up to 0.6 BLEU points on the
DEVEL set. However this gain does not transfer to
the TEST set, where instead a drop of 0.5 BLEU
is observed. The system combination using SOUL
scores showed the best result over all of our other
systems on the DEVEL set, so we chose this (a
posteriori sub-obtimal) configuration as our main
system submission.
Our system combination strategy chose for DE-
VEL about 50% hypotheses among those produced
by NCODE and 25% hypotheses from OTF, the
remainder been common to both systems. As ex-
pected, the system combination prefers hypothe-
ses coming from the best system. We can observe
nearly the same distribution for TEST.
</bodyText>
<sectionHeader confidence="0.998448" genericHeader="method">
5 Error Analysis
</sectionHeader>
<bodyText confidence="0.999980190476191">
The high level of scores for automatic metrics
encouraged us to perform a detailed, small-scale
analysis of our system output, using the error types
proposed by Vilar et al. (2006). A single annota-
tor analyzed the output of our main submission, as
well as our OTF variant. Results are in Table 7.
Looking at the most important types of errors,
assuming the translation hypotheses were to be
used for rapid assimilation of the text content, we
find a moderate number of unknown terms and in-
correctly translated terms. The most frequent er-
ror types include missing fillers, incorrect disam-
biguation, form and order, which all have some
significant impact on automatic metrics. Compar-
ing more specifically the two systems used in this
small-scale study, we find that our combination
(which reused more than 70% of hypotheses from
NCODE) mostly improves over the OTF variant on
the choice of correct word form and word order.
We may attribute this in part to a more efficient
reordering strategy that better exploits POS tags.
</bodyText>
<sectionHeader confidence="0.995338" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999975705882353">
In this paper, we have demonstrated a successful
approach that makes use of two flexible transla-
tion systems, an n-gram system and an on-the-fly
phrase-based model, in a new medical translation
task, through various approaches to perform do-
main adaptation. When combined with continu-
ous language models, which yield additional gains
of up to 2 BLEU points, moderate to high-quality
translations are obtained, as confirmed by a fine-
grained error analysis. The most challenging part
of the task was undoubtedly the lack on an internal
test to guide system development. Another inter-
esting negative result lies in the absence of success
for our configuration of the vector space model
of Chen et al. (2013) for adaptation. Lastly, a more
careful integration of medical terminology, as pro-
vided by the UMLS, proved necessary.
</bodyText>
<sectionHeader confidence="0.99885" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999731666666667">
We would like to thank Guillaume Wisniewski and
the anonymous reviewers for their helpful com-
ments and suggestions.
</bodyText>
<page confidence="0.996174">
251
</page>
<sectionHeader confidence="0.990042" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999690731481481">
Alexandre Allauzen, Nicolas P´echeux, Quoc Khanh
Do, Marco Dinarelli, Thomas Lavergne, Aur´elien
Max, Hai-son Le, and Franc¸ois Yvon. 2013. LIMSI
@ WMT13. In Proceedings of the Workshkop on
Statistical Machine Translation, pages 62–69, Sofia,
Bulgaria.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3(6):1137–1155.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL, Ann Arbor, USA.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205–
225.
Ciprian Chelba and Alex Acero. 2004. Adaptation
of maximum entropy classifier: Little data can help
a lot. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Barcelona, Spain.
Stanley F. Chen and Joshua T. Goodman. 1996. An
empirical study of smoothing techniques for lan-
guage modeling. In Proceedings of the 34th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 310–318, Santa Cruz, NM.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector space model for adaptation in statistical ma-
chine translation. In Proceedings of ACL, Sofia,
Bulgaria.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427–436. Association for Computational Lin-
guistics.
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better Hypothesis Testing for Statisti-
cal Machine Translation: Controlling for Optimizer
Instability. In Better Hypothesis Testing for Statisti-
cal Machine Translation: Controlling for Optimizer
Instability, pages 176–181, Portland, Oregon.
Josep M. Crego and Jos´e B. Mari˜no. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199–215.
Josep M. Crego, Franc¸ois Yvon, and Jos´e B. Mari˜no.
2011. N-code: an open-source bilingual N-gram
SMT toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49–58.
Daniel D´echelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, H´el`ene May-
nard, and Franc¸ois Yvon. 2008. LIMSI’s statisti-
cal translation systems for WMT’08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187–197, Edinburgh, Scotland, July. Associa-
tion for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 1352–1362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, ICASSP’95,
pages 181–184, Detroit, MI.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504–513.
Association for Computational Linguistics, July.
Thomas Lavergne, Hai-Son Le, Alexandre Allauzen,
and Franc¸ois Yvon. 2011. LIMSI’s experiments
in domain adaptation for IWSLT11. In Mei-Yuh
Hwang and Sebastian St¨uker, editors, Proceedings
of the heigth International Workshop on Spoken
Language Translation (IWSLT), San Francisco, CA.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc¸ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP, pages 5524–5527.
Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon.
2012a. Continuous space translation models with
neural networks. In Proceedings of the 2012 confer-
ence of the north american chapter of the associa-
tion for computational linguistics: Human language
technologies, pages 39–48, Montr´eal, Canada, June.
Association for Computational Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aur´elien
</reference>
<page confidence="0.961153">
252
</page>
<reference confidence="0.999664740740741">
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc¸ois Yvon. 2012b. LIMSI @ WMT12. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 330–337, Montr´eal,
Canada.
Adam Lopez. 2008. Tera-Scale Translation Models
via Pattern Matching. In Proceedings of COLING,
Manchester, UK.
Jos´e B. Mari˜no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrick Lambert, Jos´e A.R. Fonol-
losa, and Marta R. Costa-Juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527–549.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160–167, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
USA, July. Association for Computational Linguis-
tics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, September.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models for
statistical machine translation. In Proceedings of the
COLING/ACL on Main conference poster sessions,
pages 723–730, Morristown, NJ, USA. Association
for Computational Linguistics.
L. Smith, T. Rindflesch, and W. J. Wilbur. 2004. Med-
post: a part of speech tagger for biomedical text.
Bioinformatics, 20(14):2320–2321.
A. Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), pages 901–904, Denver, Colorado,
September.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL, pages 101–104.
David Vilar, Jia Xu, Luis Fernando D’Haro, and Her-
mann Ney. 2006. Error Analysis of Statistical Ma-
chine Translation Output. In LREC, Genoa, Italy.
Richard Zens, Franz Joseph Och, and Herman Ney.
2002. Phrase-based statistical machine translation.
In M. Jarke, J. Koehler, and G. Lakemeyer, editors,
KI-2002: Advances in artificial intelligence, volume
2479 of LNAI, pages 18–32. Springer Verlag.
</reference>
<page confidence="0.998942">
253
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.271932">
<title confidence="0.987252">LIMSI @ WMT’14 Medical Translation Task</title>
<author confidence="0.946727">Li Quoc Khanh Benjamin</author>
<address confidence="0.4719015">B.P. 133, 91403 Orsay, et Centre Cochrane</address>
<abstract confidence="0.99222452631579">This paper describes LIMSI’s submission to the first medical translation task at WMT’14. We report results for English- French on the subtask of sentence transfrom summaries of medical articles. Our main submission uses of output and continuous-space language models used in a post-processing step for each system. Other characteristics of our submission include: the use of sampling for building phrase table; the implementation of the vector space model proposed by Chen et al. (2013); adaptation of the POSused by the medical domain; and a report of error analysis based on the typology of Vilar et al. (2006).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandre Allauzen</author>
<author>Nicolas P´echeux</author>
<author>Quoc Khanh Do</author>
<author>Marco Dinarelli</author>
<author>Thomas Lavergne</author>
<author>Aur´elien Max</author>
<author>Hai-son Le</author>
<author>Franc¸ois Yvon</author>
</authors>
<date>2013</date>
<booktitle>LIMSI @ WMT13. In Proceedings of the Workshkop on Statistical Machine Translation,</booktitle>
<pages>62--69</pages>
<location>Sofia, Bulgaria.</location>
<marker>Allauzen, P´echeux, Do, Dinarelli, Lavergne, Max, Le, Yvon, 2013</marker>
<rawString>Alexandre Allauzen, Nicolas P´echeux, Quoc Khanh Do, Marco Dinarelli, Thomas Lavergne, Aur´elien Max, Hai-son Le, and Franc¸ois Yvon. 2013. LIMSI @ WMT13. In Proceedings of the Workshkop on Statistical Machine Translation, pages 62–69, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<issue>6</issue>
<contexts>
<context position="7209" citStr="Bengio et al., 2003" startWordPosition="1146" endWordPosition="1149"> extracted from the development data, respectively, and countdev( fj, ¯ek) is the joint count of phrase pairs ( fj, ¯ek) found in the development set. The similarity score between each phrase pair’s vector and the development set vector is added into the phrase table as a VSM feature. We also replace the joint count with the marginal count of the source/target phrase to compute an alternative average representation for the development set, thus adding two VSM additional features. 2.4 SOUL Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve discrete language models. As for our submitted translation systems to WMT’12 and WMT’13 (Le et al., 2012b; Allauzen et al., 2013), we take advantage of the recent proposal of (Le et al., 2011). Using a specific neural network architecture, the Structured OUtput Layer (SOUL), it becomes possible to estimate n-gram models that use large vocabulary, thereby making the training of large neural network language models feasible both for target language models and translation models (Le et al., 2012a). Moreover, the peculiar parameterization of </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3(6):1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Colin Bannard</author>
<author>Josh Schroeder</author>
</authors>
<title>Scaling phrase-based statistical machine translation to larger corpora and longer phrases.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Ann Arbor, USA.</location>
<marker>Callison-Burch, Bannard, Schroeder, 2005</marker>
<rawString>Chris Callison-Burch, Colin Bannard, and Josh Schroeder. 2005. Scaling phrase-based statistical machine translation to larger corpora and longer phrases. In Proceedings of ACL, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesco Casacuberta</author>
<author>Enrique Vidal</author>
</authors>
<title>Machine translation with inferred stochastic finite-state transducers.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<pages>225</pages>
<contexts>
<context position="1988" citStr="Casacuberta and Vidal, 2004" startWordPosition="294" endWordPosition="297">prompted us to experiment with two systems exploring different translation spaces, the n-gram-based NCODE (§2.1) and an on-the-fly variant of the phrasebased MOSES (§2.2), and to later combine their output. Further attempts at improving translation quality were made by resorting to continuous language model rescoring (§2.4), vector space subcorpus adaptation (§2.3), and POS-tagging adaptation to the medical domain (§3.3). We also performed a small-scale error analysis of the outputs of some of our systems (§5). 2 System Overview 2.1 NCODE NCODE implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, the peculiarity of this approach is to rely on the n-gram assumption to decompos</context>
</contexts>
<marker>Casacuberta, Vidal, 2004</marker>
<rawString>Francesco Casacuberta and Enrique Vidal. 2004. Machine translation with inferred stochastic finite-state transducers. Computational Linguistics, 30(3):205– 225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Alex Acero</author>
</authors>
<title>Adaptation of maximum entropy classifier: Little data can help a lot.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="11227" citStr="Chelba and Acero, 2004" startWordPosition="1788" endWordPosition="1791">odel. The large out-of-domain language model used for WMT’13 (Allauzen et al., 2013) is additionaly used (see Table 1). 3.3 Part-of-Speech Tagging Medical data exhibit many peculiarities, including different syntactic constructions and a specific vocabulary. As standard POS-taggers are known not to perform very well for this type of texts, we use a specific model trained on the Penn Treebank and on medical data from the MedPost project (Smith et al., 2004). We use Wapiti (Lavergne et al., 2010), a state-of-the-art CRF implementation, with a standard feature set. Adaptation is performed as in (Chelba and Acero, 2004) using the out-of-domain model as a prior when training the in-domain model on medical data. On a medical test set, this adaptation leads to a 8 point reduction of the error rate. A standard model is used for WMT’13 data. For the French side, due to the lack of annotaded data for the medical domain, corpora are tagged using the TreeTagger (Schmid, 1994). 1Attempting include one language model per sub-corpora yielded a significant drop in performance. 248 3.4 Proxy Test Set For this first edition of a Medical Translation Task, only a very small development set was made available (DEVEL in Table</context>
</contexts>
<marker>Chelba, Acero, 2004</marker>
<rawString>Ciprian Chelba and Alex Acero. 2004. Adaptation of maximum entropy classifier: Little data can help a lot. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua T Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>310--318</pages>
<location>Santa Cruz, NM.</location>
<contexts>
<context position="10417" citStr="Chen and Goodman, 1996" startWordPosition="1660" endWordPosition="1663">ion steps (D´echelotte et al., 2008). All systems are built using a “true case” scheme, but sentences fully capitalized (plentiful especially in PATTR-TITLES) are previously lowercased. Duplicate sentence pairs are removed, yielding a sentence reduction up to 70% for EMEA. Table 1 summarizes the data used along with some statistics after the cleaning and pre-processing steps. 3.2 Language Models A medical-domain 4-gram language model is built by concatenating the target side of the parallel data and all the available monolingual data1, with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1996), using the SRILM (Stolcke, 2002) and KENLM (Heafield, 2011) toolkits. Although more similar to term-toterm dictionaries, UMLS and WIKIPEDIA proved better to be included in the language model. The large out-of-domain language model used for WMT’13 (Allauzen et al., 2013) is additionaly used (see Table 1). 3.3 Part-of-Speech Tagging Medical data exhibit many peculiarities, including different syntactic constructions and a specific vocabulary. As standard POS-taggers are known not to perform very well for this type of texts, we use a specific model trained on the Penn Treebank and on medical dat</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua T. Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL), pages 310–318, Santa Cruz, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
<author>George Foster</author>
</authors>
<title>Vector space model for adaptation in statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="928" citStr="Chen et al. (2013)" startWordPosition="125" endWordPosition="128"> Cochrane franc¸ais4 {firstname.lastname}@limsi.fr Abstract This paper describes LIMSI’s submission to the first medical translation task at WMT’14. We report results for EnglishFrench on the subtask of sentence translation from summaries of medical articles. Our main submission uses a combination of NCODE (n-gram-based) and MOSES (phrase-based) output and continuous-space language models used in a post-processing step for each system. Other characteristics of our submission include: the use of sampling for building MOSES’ phrase table; the implementation of the vector space model proposed by Chen et al. (2013); adaptation of the POStagger used by NCODE to the medical domain; and a report of error analysis based on the typology of Vilar et al. (2006). 1 Introduction This paper describes LIMSI’s submission to the first medical translation task at WMT’14. This task is characterized by high-quality input text and the availability of large amounts of training data from the same domain, yielding unusually high translation performance. This prompted us to experiment with two systems exploring different translation spaces, the n-gram-based NCODE (§2.1) and an on-the-fly variant of the phrasebased MOSES (§2</context>
<context position="5729" citStr="Chen et al. (2013)" startWordPosition="893" endWordPosition="896">xamples where translation extraction failed) is then estimated. As we cannot compute exactly an inverse translation probability (because sampling is performed independently for each source phrase), we resort to the following approximation: p( (¯f|¯e) = min 1.0, p(¯e| where the freq(·) is the number of occurrences of the given phrase in the whole corpus, and the numerator p(¯e |¯f)xfreq(¯f) represents the predicted joint count of f¯ and ¯e. The other models in this system are the same as in the default configuration of MOSES. 2.3 Vector Space Model (VSM) We used the vector space model (VSM) of Chen et al. (2013) to perform domain adaptation. In this approach, each phrase pair ( ¯f, ¯e) present in the phrase table is represented by a C-dimensional vector of TF-IDF scores, one for each sub-corpus, where C represents the number of sub-corpora (see Table 1). Each component wc( f, ¯e) is a standard TF-IDF weight of each phrase pair for the cth sub-corpus. TF(¯f, ¯e) is the raw joint count of ( ¯f, ¯e) in the sub-corpus; the IDF(¯f, ¯e) is the inverse document frequency across all sub-corpora. A similar C-dimensional representation of the development set is computed as follows: we first perform word alignm</context>
<context position="19533" citStr="Chen et al. (2013)" startWordPosition="3217" endWordPosition="3220">formance, 5A control experiment for a full MOSES system (using a single phrase table) yielded a BLEU score of 45.9 on DEVEL and 43.2 on TEST, and took 3 more days to complete. 250 extra missing incorrect unknown word content filler disamb. form style term order word term all syscomb 4 13 20 47 62 8 18 21 1 11 205 OTF+VSM+SOUL 4 4 31 44 82 6 20 42 3 12 248 Table 7: Results for manual error analysis following (Vilar et al., 2006) for the first 100 test sentences. NCODE outperforming OTF by 2.8 BLEU points on the TEST set. VSM does not yield any significant improvement, contrarily to the work of Chen et al. (2013); it may be the case all individual subcorpus are equally good (or bad) at approximating the stylistic preferences of the TEST set. 4.6 Integrating SOUL Table 6 shows the substantial impact of adding SOUL models for both baseline systems. With only the SOUL LM, improvements on the test set range from 0.5 BLEU points for NCODE system to 1.2 points for the OTF system. The adaptation of SOUL LM with the medical data brings an additional improvement of about 0.2 BLEU points. Adding all SOUL translation models yield an improvement of 1.8 BLEU points for NCODE and of 2.4 BLEU points with the OTF sys</context>
</contexts>
<marker>Chen, Kuhn, Foster, 2013</marker>
<rawString>Boxing Chen, Roland Kuhn, and George Foster. 2013. Vector space model for adaptation in statistical machine translation. In Proceedings of ACL, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>427--436</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15247" citStr="Cherry and Foster, 2012" startWordPosition="2454" endWordPosition="2457">ranking (SysComb-7). Evaluation Metrics All BLEU scores (Papineni et al., 2002) are computed using cased multi-bleu with our internal tokenization. Reported results correspond to the average and standard deviation across 3 optimization runs to better account for the optimizer variance (Clark et al., 2011). 4 Experiments 4.1 Tuning Optimization Method MERT is usually used to optimize Equation 1. However, with up to 42 features when using SOUL, this method is known to become very sensitive to local minima. Table 2 compares MERT, a batch variant of the Margin Infused Relaxation Algorithm (MIRA) (Cherry and Foster, 2012) and PRO (Hopkins and May, 2011) when tuning an NCODE system. MIRA slightly outperforms PRO on DEVEL, but seems prone to overfitting. However this was not possible to detect before the release of the test set (TEST), and so we use MIRA in all our experiments. DEVEL TEST MERT 47.0± 0.4 44.1± 0.8 MIRA 47.9± 0.0 44.8± 0.1 PRO 47.1± 0.1 45.1± 0.1 Table 2: Impact of the optimization method during the tuning process on BLEU score, for a baseline NCODE system. 249 4.2 Importance of the Data Sources Table 3 shows that using the out-of-domain data from WMT’13 yields better scores than only using the pr</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427–436. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability. In Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability,</title>
<date>2011</date>
<pages>176--181</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="14929" citStr="Clark et al., 2011" startWordPosition="2401" endWordPosition="2404"> weights using Minimum Error Rate Training (MERT) (Och, 2003) on the development set for this combined list, using only these two features (SysComb-2). In an alternative configuration, we use the two systems without the SOUL rescoring, and add instead the five SOUL scores as features in the system combination reranking (SysComb-7). Evaluation Metrics All BLEU scores (Papineni et al., 2002) are computed using cased multi-bleu with our internal tokenization. Reported results correspond to the average and standard deviation across 3 optimization runs to better account for the optimizer variance (Clark et al., 2011). 4 Experiments 4.1 Tuning Optimization Method MERT is usually used to optimize Equation 1. However, with up to 42 features when using SOUL, this method is known to become very sensitive to local minima. Table 2 compares MERT, a batch variant of the Margin Infused Relaxation Algorithm (MIRA) (Cherry and Foster, 2012) and PRO (Hopkins and May, 2011) when tuning an NCODE system. MIRA slightly outperforms PRO on DEVEL, but seems prone to overfitting. However this was not possible to detect before the release of the test set (TEST), and so we use MIRA in all our experiments. DEVEL TEST MERT 47.0± </context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A Smith. 2011. Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability. In Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability, pages 176–181, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Jos´e B Mari˜no</author>
</authors>
<title>Improving statistical MT by coupling reordering and decoding.</title>
<date>2006</date>
<journal>Machine Translation,</journal>
<volume>20</volume>
<issue>3</issue>
<marker>Crego, Mari˜no, 2006</marker>
<rawString>Josep M. Crego and Jos´e B. Mari˜no. 2006. Improving statistical MT by coupling reordering and decoding. Machine Translation, 20(3):199–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Franc¸ois Yvon</author>
<author>Jos´e B Mari˜no</author>
</authors>
<date>2011</date>
<booktitle>N-code: an open-source bilingual N-gram SMT toolkit. Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>96--49</pages>
<marker>Crego, Yvon, Mari˜no, 2011</marker>
<rawString>Josep M. Crego, Franc¸ois Yvon, and Jos´e B. Mari˜no. 2011. N-code: an open-source bilingual N-gram SMT toolkit. Prague Bulletin of Mathematical Linguistics, 96:49–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D´echelotte</author>
<author>Gilles Adda</author>
<author>Alexandre Allauzen</author>
<author>Olivier Galibert</author>
<author>Jean-Luc Gauvain</author>
<author>H´el`ene Maynard</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>LIMSI’s statistical translation systems for WMT’08.</title>
<date>2008</date>
<booktitle>In Proc. of the NAACL-HTL Statistical Machine Translation Workshop,</booktitle>
<location>Columbus, Ohio.</location>
<marker>D´echelotte, Adda, Allauzen, Galibert, Gauvain, Maynard, Yvon, 2008</marker>
<rawString>Daniel D´echelotte, Gilles Adda, Alexandre Allauzen, Olivier Galibert, Jean-Luc Gauvain, H´el`ene Maynard, and Franc¸ois Yvon. 2008. LIMSI’s statistical translation systems for WMT’08. In Proc. of the NAACL-HTL Statistical Machine Translation Workshop, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and Smaller Language Model Queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="10477" citStr="Heafield, 2011" startWordPosition="1671" endWordPosition="1672">“true case” scheme, but sentences fully capitalized (plentiful especially in PATTR-TITLES) are previously lowercased. Duplicate sentence pairs are removed, yielding a sentence reduction up to 70% for EMEA. Table 1 summarizes the data used along with some statistics after the cleaning and pre-processing steps. 3.2 Language Models A medical-domain 4-gram language model is built by concatenating the target side of the parallel data and all the available monolingual data1, with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1996), using the SRILM (Stolcke, 2002) and KENLM (Heafield, 2011) toolkits. Although more similar to term-toterm dictionaries, UMLS and WIKIPEDIA proved better to be included in the language model. The large out-of-domain language model used for WMT’13 (Allauzen et al., 2013) is additionaly used (see Table 1). 3.3 Part-of-Speech Tagging Medical data exhibit many peculiarities, including different syntactic constructions and a specific vocabulary. As standard POS-taggers are known not to perform very well for this type of texts, we use a specific model trained on the Penn Treebank and on medical data from the MedPost project (Smith et al., 2004). We use Wapi</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and Smaller Language Model Queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1352--1362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="15279" citStr="Hopkins and May, 2011" startWordPosition="2460" endWordPosition="2463">trics All BLEU scores (Papineni et al., 2002) are computed using cased multi-bleu with our internal tokenization. Reported results correspond to the average and standard deviation across 3 optimization runs to better account for the optimizer variance (Clark et al., 2011). 4 Experiments 4.1 Tuning Optimization Method MERT is usually used to optimize Equation 1. However, with up to 42 features when using SOUL, this method is known to become very sensitive to local minima. Table 2 compares MERT, a batch variant of the Margin Infused Relaxation Algorithm (MIRA) (Cherry and Foster, 2012) and PRO (Hopkins and May, 2011) when tuning an NCODE system. MIRA slightly outperforms PRO on DEVEL, but seems prone to overfitting. However this was not possible to detect before the release of the test set (TEST), and so we use MIRA in all our experiments. DEVEL TEST MERT 47.0± 0.4 44.1± 0.8 MIRA 47.9± 0.0 44.8± 0.1 PRO 47.1± 0.1 45.1± 0.1 Table 2: Impact of the optimization method during the tuning process on BLEU score, for a baseline NCODE system. 249 4.2 Importance of the Data Sources Table 3 shows that using the out-of-domain data from WMT’13 yields better scores than only using the provided medical data only. Moreov</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1352–1362, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Herman Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, ICASSP’95,</booktitle>
<pages>181--184</pages>
<location>Detroit, MI.</location>
<contexts>
<context position="10392" citStr="Kneser and Ney, 1995" startWordPosition="1656" endWordPosition="1659">zation and detokenization steps (D´echelotte et al., 2008). All systems are built using a “true case” scheme, but sentences fully capitalized (plentiful especially in PATTR-TITLES) are previously lowercased. Duplicate sentence pairs are removed, yielding a sentence reduction up to 70% for EMEA. Table 1 summarizes the data used along with some statistics after the cleaning and pre-processing steps. 3.2 Language Models A medical-domain 4-gram language model is built by concatenating the target side of the parallel data and all the available monolingual data1, with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1996), using the SRILM (Stolcke, 2002) and KENLM (Heafield, 2011) toolkits. Although more similar to term-toterm dictionaries, UMLS and WIKIPEDIA proved better to be included in the language model. The large out-of-domain language model used for WMT’13 (Allauzen et al., 2013) is additionaly used (see Table 1). 3.3 Part-of-Speech Tagging Medical data exhibit many peculiarities, including different syntactic constructions and a specific vocabulary. As standard POS-taggers are known not to perform very well for this type of texts, we use a specific model trained on the Penn Tr</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Herman Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, ICASSP’95, pages 181–184, Detroit, MI.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="13145" citStr="Koehn et al., 2007" startWordPosition="2115" endWordPosition="2118">eat care when tuning or comparing systems. 3.5 Systems NCODE We use NCODE with default settings, 3- gram bilingual translation models on words and 4- gram bilingual translation factor models on POS, for each included corpora (see Table 1) and for the concatenation of them all. OTF When using our OTF system, all indomain and out-of-domain data are concatenated, respectively. For both corpora, we use a maximum random sampling size of 1000 examples and a maximum phrase length of 15. However, all sub-corpora but GIGA3 are used to compute the vectors for VSM features. Decoding is done with MOSES4 (Koehn et al., 2007). SOUL Given the computational cost of computing n-gram probabilities with neural network models, we resort to a reranking approach. In the following experiments, we use 10-gram SOUL models to rescore 1000-best lists. SOUL models provide five new features: a target language model score and four translation scores (Le et al., 2012a). We reused the SOUL models trained for our participation to WMT’12 (Le et al., 2012b). Moreover, target language models are adapted by running 6 more epochs on the new medical data. 2This issue is traditionally solved in Machine Learning by folded cross-validation, </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lavergne</author>
<author>Olivier Capp´e</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Practical very large scale CRFs.</title>
<date>2010</date>
<booktitle>In Proceedings the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>504--513</pages>
<marker>Lavergne, Capp´e, Yvon, 2010</marker>
<rawString>Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon. 2010. Practical very large scale CRFs. In Proceedings the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504–513. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lavergne</author>
<author>Hai-Son Le</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>LIMSI’s experiments in domain adaptation for IWSLT11.</title>
<date>2011</date>
<booktitle>In Mei-Yuh Hwang and Sebastian St¨uker, editors, Proceedings of the heigth International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="8053" citStr="Lavergne et al., 2011" startWordPosition="1283" endWordPosition="1286">posal of (Le et al., 2011). Using a specific neural network architecture, the Structured OUtput Layer (SOUL), it becomes possible to estimate n-gram models that use large vocabulary, thereby making the training of large neural network language models feasible both for target language models and translation models (Le et al., 2012a). Moreover, the peculiar parameterization of continuous models allows us to consider longer dependencies than the one used by conventional n-gram models (e.g. n = 10 instead of n = 4). Additionally, continuous models can also be easily and efficiently adapted as in (Lavergne et al., 2011). Starting from a previously trained SOUL model, only a few more training epochs are f) x freq( f) �(2) freq(¯e) J j=0 countdev( wdev c = K k=0 247 Corpus Sentences Tokens (en-fr) Description wrd-lm pos-lm COPPA 454246 10-12M -3 -15 EMEA 324189 6-7M 26 -1 PATTR-ABSTRACTS 634616 20-24M 22 21 in-domain PATTR-CLAIMS 888725 32-36M 6 2 PATTR-TITLES 385 829 3-4M 4 -17 UMLS 2166 612 8-8M term dictionary -7 -22 WIKIPEDIA 8421 17-18k short titles -5 -13 NEWSCOMMENTARY 171277 4-5M 6 16 out-of-domain EUROPARL 1982 937 54-60M -7 -33 GIGA 9 625 480 260-319M 27 52 all parallel all 17M 397-475M concatenation</context>
</contexts>
<marker>Lavergne, Le, Allauzen, Yvon, 2011</marker>
<rawString>Thomas Lavergne, Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon. 2011. LIMSI’s experiments in domain adaptation for IWSLT11. In Mei-Yuh Hwang and Sebastian St¨uker, editors, Proceedings of the heigth International Workshop on Spoken Language Translation (IWSLT), San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Ilya Oparin</author>
<author>Alexandre Allauzen</author>
<author>JeanLuc Gauvain</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Structured output layer neural network language model.</title>
<date>2011</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>5524--5527</pages>
<contexts>
<context position="7457" citStr="Le et al., 2011" startWordPosition="1190" endWordPosition="1193">the phrase table as a VSM feature. We also replace the joint count with the marginal count of the source/target phrase to compute an alternative average representation for the development set, thus adding two VSM additional features. 2.4 SOUL Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve discrete language models. As for our submitted translation systems to WMT’12 and WMT’13 (Le et al., 2012b; Allauzen et al., 2013), we take advantage of the recent proposal of (Le et al., 2011). Using a specific neural network architecture, the Structured OUtput Layer (SOUL), it becomes possible to estimate n-gram models that use large vocabulary, thereby making the training of large neural network language models feasible both for target language models and translation models (Le et al., 2012a). Moreover, the peculiar parameterization of continuous models allows us to consider longer dependencies than the one used by conventional n-gram models (e.g. n = 10 instead of n = 4). Additionally, continuous models can also be easily and efficiently adapted as in (Lavergne et al., 2011). St</context>
</contexts>
<marker>Le, Oparin, Allauzen, Gauvain, Yvon, 2011</marker>
<rawString>Hai-Son Le, Ilya Oparin, Alexandre Allauzen, JeanLuc Gauvain, and Franc¸ois Yvon. 2011. Structured output layer neural network language model. In Proceedings of ICASSP, pages 5524–5527.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous space translation models with neural networks.</title>
<date>2012</date>
<booktitle>In Proceedings of the</booktitle>
<pages>39--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="4545" citStr="Le et al., 2012" startWordPosition="700" endWordPosition="703">a way that a unique segmentation of the bilingual corpus is achieved (Mari˜no et al., 2006) and n-gram translation models are then estimated over the training corpus composed of tuple sequences made of surface forms or POS tags. Reordering rules are automatically learned during the unfolding procedure and are built using partof-speech (POS), rather than surface word forms, to increase their generalization power (Crego and Mari˜no, 2006). 2.2 On-the-fly System (OTF) We develop an alternative approach implementing an on-the-fly estimation of the parameter of a standard phrase-based model as in (Le et al., 2012b), also adding an inverse translation model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on on-the-fly model estimation for SMT (CallisonBurch et al., 2005; Lopez, 2008), we first build a suffix array for the source corpus. Only a limited number of translation examples, selected by deterministic random sampling, are then used by traversing the suffix array appropriately. A coherent translation probability (Lopez, 2008) (which also takes into account examples where translation extracti</context>
<context position="7369" citStr="Le et al., 2012" startWordPosition="1174" endWordPosition="1177">y score between each phrase pair’s vector and the development set vector is added into the phrase table as a VSM feature. We also replace the joint count with the marginal count of the source/target phrase to compute an alternative average representation for the development set, thus adding two VSM additional features. 2.4 SOUL Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve discrete language models. As for our submitted translation systems to WMT’12 and WMT’13 (Le et al., 2012b; Allauzen et al., 2013), we take advantage of the recent proposal of (Le et al., 2011). Using a specific neural network architecture, the Structured OUtput Layer (SOUL), it becomes possible to estimate n-gram models that use large vocabulary, thereby making the training of large neural network language models feasible both for target language models and translation models (Le et al., 2012a). Moreover, the peculiar parameterization of continuous models allows us to consider longer dependencies than the one used by conventional n-gram models (e.g. n = 10 instead of n = 4). Additionally, contin</context>
<context position="13476" citStr="Le et al., 2012" startWordPosition="2167" endWordPosition="2170">main data are concatenated, respectively. For both corpora, we use a maximum random sampling size of 1000 examples and a maximum phrase length of 15. However, all sub-corpora but GIGA3 are used to compute the vectors for VSM features. Decoding is done with MOSES4 (Koehn et al., 2007). SOUL Given the computational cost of computing n-gram probabilities with neural network models, we resort to a reranking approach. In the following experiments, we use 10-gram SOUL models to rescore 1000-best lists. SOUL models provide five new features: a target language model score and four translation scores (Le et al., 2012a). We reused the SOUL models trained for our participation to WMT’12 (Le et al., 2012b). Moreover, target language models are adapted by running 6 more epochs on the new medical data. 2This issue is traditionally solved in Machine Learning by folded cross-validation, an approach that would be too prohibitive to use here. 3The GIGA corpus is actually very varied in content. 4http://www.statmt.org/moses/ System Combination As NCODE and OTF differ in many aspects and make different errors, we use system combination techniques to take advantage of their complementarity. This is done by reranking </context>
</contexts>
<marker>Le, Allauzen, Yvon, 2012</marker>
<rawString>Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon. 2012a. Continuous space translation models with neural networks. In Proceedings of the 2012 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 39–48, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hai-Son Le</author>
<author>Thomas Lavergne</author>
<author>Alexandre Allauzen</author>
<author>Marianna Apidianaki</author>
<author>Li Gong</author>
<author>Aur´elien Max</author>
<author>Artem Sokolov</author>
<author>Guillaume Wisniewski</author>
<author>Franc¸ois Yvon</author>
</authors>
<booktitle>2012b. LIMSI @ WMT12. In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>330--337</pages>
<location>Montr´eal, Canada.</location>
<marker>Le, Lavergne, Allauzen, Apidianaki, Gong, Max, Sokolov, Wisniewski, Yvon, </marker>
<rawString>Hai-Son Le, Thomas Lavergne, Alexandre Allauzen, Marianna Apidianaki, Li Gong, Aur´elien Max, Artem Sokolov, Guillaume Wisniewski, and Franc¸ois Yvon. 2012b. LIMSI @ WMT12. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 330–337, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Tera-Scale Translation Models via Pattern Matching.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="4825" citStr="Lopez, 2008" startWordPosition="748" endWordPosition="749">unfolding procedure and are built using partof-speech (POS), rather than surface word forms, to increase their generalization power (Crego and Mari˜no, 2006). 2.2 On-the-fly System (OTF) We develop an alternative approach implementing an on-the-fly estimation of the parameter of a standard phrase-based model as in (Le et al., 2012b), also adding an inverse translation model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on on-the-fly model estimation for SMT (CallisonBurch et al., 2005; Lopez, 2008), we first build a suffix array for the source corpus. Only a limited number of translation examples, selected by deterministic random sampling, are then used by traversing the suffix array appropriately. A coherent translation probability (Lopez, 2008) (which also takes into account examples where translation extraction failed) is then estimated. As we cannot compute exactly an inverse translation probability (because sampling is performed independently for each source phrase), we resort to the following approximation: p( (¯f|¯e) = min 1.0, p(¯e| where the freq(·) is the number of occurrences</context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>Adam Lopez. 2008. Tera-Scale Translation Models via Pattern Matching. In Proceedings of COLING, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e B Mari˜no</author>
<author>Rafael E Banchs</author>
<author>Josep M Crego</author>
<author>Adri`a de Gispert</author>
<author>Patrick Lambert</author>
<author>Jos´e A R Fonollosa</author>
<author>Marta R Costa-Juss`a</author>
</authors>
<title>N-grambased machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Mari˜no, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-Juss`a, 2006</marker>
<rawString>Jos´e B. Mari˜no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrick Lambert, Jos´e A.R. Fonollosa, and Marta R. Costa-Juss`a. 2006. N-grambased machine translation. Computational Linguistics, 32(4):527–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14371" citStr="Och, 2003" startWordPosition="2315" endWordPosition="2316"> would be too prohibitive to use here. 3The GIGA corpus is actually very varied in content. 4http://www.statmt.org/moses/ System Combination As NCODE and OTF differ in many aspects and make different errors, we use system combination techniques to take advantage of their complementarity. This is done by reranking the concatenation of the 1000-best lists of both systems. For each hypothesis within this list, we use two global features, corresponding either to the score computed by the corresponding system or 0 otherwise. We then learn reranking weights using Minimum Error Rate Training (MERT) (Och, 2003) on the development set for this combined list, using only these two features (SysComb-2). In an alternative configuration, we use the two systems without the SOUL rescoring, and add instead the five SOUL scores as features in the system combination reranking (SysComb-7). Evaluation Metrics All BLEU scores (Papineni et al., 2002) are computed using cased multi-bleu with our internal tokenization. Reported results correspond to the average and standard deviation across 3 optimization runs to better account for the optimizer variance (Clark et al., 2011). 4 Experiments 4.1 Tuning Optimization Me</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160–167, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, USA,</location>
<contexts>
<context position="14702" citStr="Papineni et al., 2002" startWordPosition="2364" endWordPosition="2368">concatenation of the 1000-best lists of both systems. For each hypothesis within this list, we use two global features, corresponding either to the score computed by the corresponding system or 0 otherwise. We then learn reranking weights using Minimum Error Rate Training (MERT) (Och, 2003) on the development set for this combined list, using only these two features (SysComb-2). In an alternative configuration, we use the two systems without the SOUL rescoring, and add instead the five SOUL scores as features in the system combination reranking (SysComb-7). Evaluation Metrics All BLEU scores (Papineni et al., 2002) are computed using cased multi-bleu with our internal tokenization. Reported results correspond to the average and standard deviation across 3 optimization runs to better account for the optimizer variance (Clark et al., 2011). 4 Experiments 4.1 Tuning Optimization Method MERT is usually used to optimize Equation 1. However, with up to 42 features when using SOUL, this method is known to become very sensitive to local minima. Table 2 compares MERT, a batch variant of the Margin Infused Relaxation Algorithm (MIRA) (Cherry and Foster, 2012) and PRO (Hopkins and May, 2011) when tuning an NCODE s</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing,</booktitle>
<contexts>
<context position="11582" citStr="Schmid, 1994" startWordPosition="1854" endWordPosition="1855"> trained on the Penn Treebank and on medical data from the MedPost project (Smith et al., 2004). We use Wapiti (Lavergne et al., 2010), a state-of-the-art CRF implementation, with a standard feature set. Adaptation is performed as in (Chelba and Acero, 2004) using the out-of-domain model as a prior when training the in-domain model on medical data. On a medical test set, this adaptation leads to a 8 point reduction of the error rate. A standard model is used for WMT’13 data. For the French side, due to the lack of annotaded data for the medical domain, corpora are tagged using the TreeTagger (Schmid, 1994). 1Attempting include one language model per sub-corpora yielded a significant drop in performance. 248 3.4 Proxy Test Set For this first edition of a Medical Translation Task, only a very small development set was made available (DEVEL in Table 1). This made both system design and tuning challenging. In fact, with such a small development set, conventional tuning methods are known to be very unstable and prone to overfitting, and it would be suboptimal to select a configuration based on results on the development set only.2 To circumvent this, we artificially created our own internal test set</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Daniel Dchelotte</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Continuous space language models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>723--730</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7232" citStr="Schwenk et al., 2006" startWordPosition="1150" endWordPosition="1153">evelopment data, respectively, and countdev( fj, ¯ek) is the joint count of phrase pairs ( fj, ¯ek) found in the development set. The similarity score between each phrase pair’s vector and the development set vector is added into the phrase table as a VSM feature. We also replace the joint count with the marginal count of the source/target phrase to compute an alternative average representation for the development set, thus adding two VSM additional features. 2.4 SOUL Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve discrete language models. As for our submitted translation systems to WMT’12 and WMT’13 (Le et al., 2012b; Allauzen et al., 2013), we take advantage of the recent proposal of (Le et al., 2011). Using a specific neural network architecture, the Structured OUtput Layer (SOUL), it becomes possible to estimate n-gram models that use large vocabulary, thereby making the training of large neural network language models feasible both for target language models and translation models (Le et al., 2012a). Moreover, the peculiar parameterization of continuous models allow</context>
</contexts>
<marker>Schwenk, Dchelotte, Gauvain, 2006</marker>
<rawString>Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gauvain. 2006. Continuous space language models for statistical machine translation. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 723–730, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Smith</author>
<author>T Rindflesch</author>
<author>W J Wilbur</author>
</authors>
<title>Medpost: a part of speech tagger for biomedical text.</title>
<date>2004</date>
<journal>Bioinformatics,</journal>
<volume>20</volume>
<issue>14</issue>
<contexts>
<context position="11064" citStr="Smith et al., 2004" startWordPosition="1761" endWordPosition="1764">2002) and KENLM (Heafield, 2011) toolkits. Although more similar to term-toterm dictionaries, UMLS and WIKIPEDIA proved better to be included in the language model. The large out-of-domain language model used for WMT’13 (Allauzen et al., 2013) is additionaly used (see Table 1). 3.3 Part-of-Speech Tagging Medical data exhibit many peculiarities, including different syntactic constructions and a specific vocabulary. As standard POS-taggers are known not to perform very well for this type of texts, we use a specific model trained on the Penn Treebank and on medical data from the MedPost project (Smith et al., 2004). We use Wapiti (Lavergne et al., 2010), a state-of-the-art CRF implementation, with a standard feature set. Adaptation is performed as in (Chelba and Acero, 2004) using the out-of-domain model as a prior when training the in-domain model on medical data. On a medical test set, this adaptation leads to a 8 point reduction of the error rate. A standard model is used for WMT’13 data. For the French side, due to the lack of annotaded data for the medical domain, corpora are tagged using the TreeTagger (Schmid, 1994). 1Attempting include one language model per sub-corpora yielded a significant dro</context>
</contexts>
<marker>Smith, Rindflesch, Wilbur, 2004</marker>
<rawString>L. Smith, T. Rindflesch, and W. J. Wilbur. 2004. Medpost: a part of speech tagger for biomedical text. Bioinformatics, 20(14):2320–2321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP),</booktitle>
<pages>901--904</pages>
<location>Denver, Colorado,</location>
<contexts>
<context position="10450" citStr="Stolcke, 2002" startWordPosition="1667" endWordPosition="1668">systems are built using a “true case” scheme, but sentences fully capitalized (plentiful especially in PATTR-TITLES) are previously lowercased. Duplicate sentence pairs are removed, yielding a sentence reduction up to 70% for EMEA. Table 1 summarizes the data used along with some statistics after the cleaning and pre-processing steps. 3.2 Language Models A medical-domain 4-gram language model is built by concatenating the target side of the parallel data and all the available monolingual data1, with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1996), using the SRILM (Stolcke, 2002) and KENLM (Heafield, 2011) toolkits. Although more similar to term-toterm dictionaries, UMLS and WIKIPEDIA proved better to be included in the language model. The large out-of-domain language model used for WMT’13 (Allauzen et al., 2013) is additionaly used (see Table 1). 3.3 Part-of-Speech Tagging Medical data exhibit many peculiarities, including different syntactic constructions and a specific vocabulary. As standard POS-taggers are known not to perform very well for this type of texts, we use a specific model trained on the Penn Treebank and on medical data from the MedPost project (Smith</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing (ICSLP), pages 901–904, Denver, Colorado, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>101--104</pages>
<contexts>
<context position="3270" citStr="Tillmann, 2004" startWordPosition="507" endWordPosition="508">l units called tuples. The best translation is selected by maximizing a linear combination of feature functions using the following inference rule: K e∗ = argmax Akfk(f, e, a) (1) e,a k=1 where K feature functions (fk) are weighted by a set of coefficients (Ak) and a denotes the set of hidden variables corresponding to the reordering and segmentation of the source sentence. Along with the n-gram translation models and target ngram language models, 13 conventional features are combined: 4 lexicon models similar to the ones used in standard phrase-based systems; 6 lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aimed at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. Features are estimated during the training phase. Training source sentences are first reordered so as to match 246 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 246–253, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics the target word order by unfolding the word alignments (</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christoph Tillmann. 2004. A unigram orientation model for statistical machine translation. In Proceedings of HLT-NAACL, pages 101–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Jia Xu</author>
<author>Luis Fernando D’Haro</author>
<author>Hermann Ney</author>
</authors>
<title>Error Analysis of Statistical Machine Translation Output. In LREC,</title>
<date>2006</date>
<location>Genoa, Italy.</location>
<marker>Vilar, Xu, D’Haro, Ney, 2006</marker>
<rawString>David Vilar, Jia Xu, Luis Fernando D’Haro, and Hermann Ney. 2006. Error Analysis of Statistical Machine Translation Output. In LREC, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Franz Joseph Och</author>
<author>Herman Ney</author>
</authors>
<title>Phrase-based statistical machine translation.</title>
<date>2002</date>
<booktitle>KI-2002: Advances in artificial intelligence,</booktitle>
<volume>2479</volume>
<pages>18--32</pages>
<editor>In M. Jarke, J. Koehler, and G. Lakemeyer, editors,</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="2118" citStr="Zens et al., 2002" startWordPosition="316" endWordPosition="319">of the phrasebased MOSES (§2.2), and to later combine their output. Further attempts at improving translation quality were made by resorting to continuous language model rescoring (§2.4), vector space subcorpus adaptation (§2.3), and POS-tagging adaptation to the medical domain (§3.3). We also performed a small-scale error analysis of the outputs of some of our systems (§5). 2 System Overview 2.1 NCODE NCODE implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, the peculiarity of this approach is to rely on the n-gram assumption to decompose the joint probability of a sentence pair in a sequence of bilingual units called tuples. The best translation is selected by max</context>
</contexts>
<marker>Zens, Och, Ney, 2002</marker>
<rawString>Richard Zens, Franz Joseph Och, and Herman Ney. 2002. Phrase-based statistical machine translation. In M. Jarke, J. Koehler, and G. Lakemeyer, editors, KI-2002: Advances in artificial intelligence, volume 2479 of LNAI, pages 18–32. Springer Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005841">
<title confidence="0.987743">
Cooking with Semantics
</title>
<author confidence="0.938385">
Jon Malmaud Earl J. Wagner, Nancy Chang, Kevin Murphy
</author>
<note confidence="0.553453">
Brain and Cognitive Sciences
MIT Google
43 Vassar St. 1600 Amphitheatre Pkwy
Cambridge, MA Mountain View, CA
</note>
<email confidence="0.9732345">
malmaud@mit.edu wag@google.com, ncchang@google.com,
kpmurphy@google.com
</email>
<sectionHeader confidence="0.993487" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999926615384615">
We are interested in the automatic inter-
pretation of how-to instructions, such as
cooking recipes, into semantic representa-
tions that can facilitate sophisticated ques-
tion answering. Recent work has shown
impressive results on semantic parsing of
instructions with minimal supervision, but
such techniques cannot handle much of the
situated and ambiguous language used in
instructions found on the web. In this pa-
per, we suggest how to extend such meth-
ods using a model of pragmatics, based on
a rich representation of world state.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999898619047619">
Understanding instructional text found on the web
presents unique challenges and opportunities that
represent a frontier for semantic parsing. Cru-
cially, instructional language is situated: it as-
sumes a situational context within which the agent
(i.e., the reader) is to carry out a sequence of ac-
tions, as applied to objects that are (or become)
available in the immediate environment. These ac-
tions and objects may not be explicitly specified;
indeed, much instructional language is ambigu-
ous, underspecified and often even ungrammatical
relative to conventional usage.
In this “vision paper”, we focus on interpreting
cooking recipes. While there are several services
that already support searching for recipes (such
as Google Recipe Search1, Yummly, Foodily, and
MyTaste), the faceted search capabilities they pro-
vide are limited to recipe meta-data such as ingre-
dients, genres, cooking time, portions, and nutri-
tion values. Some of this information is explicitly
marked up in machine-readable form2. However,
</bodyText>
<footnote confidence="0.9872375">
1http://www.google.com/insidesearch/
features/recipes/
2See e.g. http://microformats.org/wiki/
recipe-formats
</footnote>
<figureCaption confidence="0.9970465">
Figure 1: Example recipes. Left: for a mixed
drink. Right: for guacamole dip.
</figureCaption>
<bodyText confidence="0.957172714285714">
the actual steps of the recipe are treated as an un-
structured blob of text. (The same problem ap-
plies to other instructional sites, such as ehow.
com,wikihow.com,answers.yahoo.com,
www.instructables.com, etc.) Interpreting
the steps of recipes (and instructions more gener-
ally) is the goal of this paper.
</bodyText>
<sectionHeader confidence="0.99267" genericHeader="introduction">
2 Challenges
</sectionHeader>
<bodyText confidence="0.999866173913043">
This section surveys some of the linguistic chal-
lenges typical of the cooking domain, as illustrated
by the two recipes in Figure 1. These difficulties
can be classified broadly as problems arising from
the interpretation of arguments, actions and con-
trol structure.
Arguments: One particularly salient character-
istic of recipes is that they often feature arguments
that are omitted, underspecified or otherwise de-
pendent on the context. Arguments may be elided
in syntactic contexts where they are usually re-
quired (the so-called “zero anaphora” problem),
especially when they are easily filled by an object
in the immediate context. For example, the item
to set aside in (1a) is the just-treated cocktail glass,
and the item to fill in (1b) and shake and then strain
in (1c) is the recently mentioned shaker. Note that
the context may include the ingredient list itself, as
illustrated by the elided argument(s) to be added
in the one-line recipe “Add to a cocktail glass in
the order listed.” Arguments may be implicitly
available, based on either domain-specific expec-
tations of the initial context or the results of pre-
</bodyText>
<page confidence="0.988583">
33
</page>
<bodyText confidence="0.98685194">
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 33–38,
Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics
ceding steps. The ice in (1b) isn’t listed in the
corresponding recipes ingredient list, since many
common ingredients (water, ice, salt, pepper) are
assumed to be available in most kitchens. Some-
times, the argument may never have been directly
verbalized, but rather is the result of a previous ac-
tion. Thus in the recipe “Pour ingredients over ice
and shake vigorously,” the object to shake is the
container (only implicitly available) along with its
contents — which, once the “pour” instruction is
executed, include both ice and the (listed) ingre-
dients. Note also that interpreting “the remain-
ing ingredients” in (1b) requires an understand-
ing of which ingredients have yet to be used at
that point in the recipe. Arguments may be in-
directly available, by association with an explic-
itly available argument. Recipe 2 mentions avo-
cados in several explicit and implicit referring ex-
pressions; of these only the “them” in (2a) may
be considered straightforward anaphoric reference
(to the just-cut avocados). Step (2b) involves a
metonymic reference to the “skin and pits” where
the part-whole relation between these items and
the avocado is what makes the instruction inter-
pretable. Step (2c) once again mentions “avoca-
dos”, but note that this now refers to the flesh of
the avocados, i.e., the implicit scooped-out object
from (2a). Arguments may be incompletely speci-
fied, especially with respect to amount. The exact
amount of sugar needed in (1a) is not mentioned,
for example. Similarly, the amount of ice needed
in (1b) depends on the size of the shaker and is not
precisely specified.
Actions: Like arguments, action interpretation
also depends on the situational context. For exam-
ple, actions may have ambiguous senses, mainly
due to the elided arguments noted above. The verb
“shake” in (1c), for example, yields a spurious in-
transitive reading. Actions may have argument-
dependent senses: certain verbs may resolve to
different motor actions depending on the affor-
dances of their arguments. For example, the ac-
tion intended by the verb “garnish” in (1d) might
involve careful perching of the peel on the rim
of the glass; in other recipes, the same verb ap-
plied to nutmeg or cut fruit may be better inter-
preted as an add action. Actions may be omitted
or implied, in particular by the way certain argu-
ments are expressed. Most recipes involving eggs,
for example, do not explicitly mention the need to
crack them and extract their contents; this is a de-
fault preparatory step. Other ingredients vary in
how strongly they are associated with (implicit)
preparatory steps. For example, recipes calling
for “1/4 avocado” may require that something like
steps (2a-b) be undertaken (and their results quar-
tered); the “orange peel” of (1d) may likewise de-
pend on a separate procedure for extracting peel
from an orange.
Control structure: Instructions sometimes
provide more complex information about se-
quence, coordination and control conditions. Con-
ditions: An action may be specified as being per-
formed until some finish condition holds. In (2c),
the “until smooth” condition—itself featuring an
elided avocado argument—controls how long the
blending action should continue. Other conditions
mentioned in recipes include “Add crushed ice un-
til the glass is almost full”, “Stir until the glass be-
gins to frost”, and “Add salt to taste”. Sequence:
Though implicitly sequential, recipes occasion-
ally include explicit sequencing language. In the
recipe “Add to a cocktail glass in the order listed”,
the order reflects that of the ingredient list. Other
recipes specify that certain steps can or should be
done “ahead of time”, or else while other steps are
in progress. Alternatives: Recipes sometimes al-
low for some variability, by specifying alternative
options for specific ingredients (“Garnish with a
twist of lemon or lime”), appliances or utensils
(“Using a large fork (or a blender)...”), and even
actions (“Chop or mash the avocados”).
As should be clear from these examples, the in-
terpretation of a given step in a set of instructions
may hinge on many aspects of situated and proce-
dural knowledge, including at least: the physical
context (including the particular props and tools
assumed available); the incremental state result-
ing from successful execution of previous steps;
and general commonsense knowledge about the
affordances of specific objects or expected argu-
ments of specific actions (or more conveniently,
corpus-based verb-argument expectations that ap-
proximate such knowledge, see e.g., (Nyga and
Beetz, 2012)). All of these sources of knowl-
edge go significantly beyond those employed in
semantic parsing models for single utterances and
in non-procedural contexts.
</bodyText>
<sectionHeader confidence="0.96053" genericHeader="method">
3 Proposed approach
</sectionHeader>
<bodyText confidence="0.99984">
We propose to maintain a rich latent context that
persists while parsing an entire recipe, in contrast
</bodyText>
<page confidence="0.983345">
34
</page>
<figure confidence="0.997566838709678">
Bowl
Flour
Milk
S0 S1
S2 S3
VA1 Video VS3 Image
Empty
In(packet)
In(jug)
Has(milk)
In(packet)
In(bowl)
Has(milk, flour)
In(bowl)
In(bowl)
Has(milk, flour)
In(bowl)
In(bowl)
A1 Add: A2 Add: A3 Mix:
from: jug from: packet what: bowl
to: bowl to: bowl manner: well
what: milk what: flour
manner: pouring
&amp;quot;Pour milk
into a bowl&amp;quot;
&amp;quot;Add flour&amp;quot;
&amp;quot;Mix well&amp;quot;
&amp;quot;The paste
should be
smooth.&amp;quot;
WA1 WA2 WA3 WS3
</figure>
<figureCaption confidence="0.8268315">
Figure 2: Our proposed probabilistic model, showing a possible trace of observed and latent variables
after parsing each step of a pancake recipe. See text for description of notation.
</figureCaption>
<bodyText confidence="0.999902959183674">
to approaches that interpret each sentence inde-
pendently. This context represents the state of the
kitchen, and statements in the recipes are inter-
preted pragmatically with respect to the evolving
context. More precisely, our model has the over-
all structure of a discrete-time, partially observed,
object-oriented Markov Decision Process, as il-
lustrated in Figure 2. The states and actions are
both hidden. What we observe is text and/or im-
ages/video; our goal is to infer the posterior over
the sequence of actions (i.e., to recover the “true”
recipe), given the noisy evidence.
States and actions. The world state St is repre-
sented as a set of objects, such as ingredients and
containers, along with various predicates, encod-
ing the quantity, location, and condition (e.g., raw
or cooked, empty or full) of each object. Note that
previous work on situated semantic parsing often
uses grid world environments where the only flu-
ent is the agent’s location; in contrast, we allow
any object to undergo state transformations. In
particular, objects can be created and destroyed.
Each action At is represented by a semantic
frame, corresponding to a verb with various ar-
guments or roles. This specifies how to trans-
form the state. We also allow for sequencing
and loop frames c.f., the “robot control language”
in (Matuszek et al., 2013). We assume access
to a simple cooking simulator that can take in a
stream of low-level instructions to produce a new
state; this implements the world dynamics model
p(St|St−1,At).
Text data. We assume that the text of the
t’th sentence, represented by WAt, describes the
t’th primitive action, At. We represent the con-
ditional distribution p(At|WAt, St−1) as a log-
linear model, as in prior work on frame-semantic
parsing/ semantic role labeling (SRL) (Das et al.,
2014).3 However, we extend this prior work by al-
lowing roles to be filled not just from spans from
the text, but also by objects in the latent state vec-
tor. We will use various pragmatically-inspired
features to represent the compatibility between
candidate objects in the state vector and roles in
the action frame, including: whether the object
has been recently mentioned or touched, whether
the object has the right affordances for the cor-
responding role (e.g., if the frame is “mix”, and
the role is “what”, the object should be mixable),
</bodyText>
<footnote confidence="0.9966725">
3Although CCGs have been used in previous work on
(situated) semantic parsing, such as (Artzi and Zettlemoyer,
2013), we chose to use the simpler approach based on frames
because the nature of the language that occurs in recipes
is sufficiently simple (there are very few complex nested
clauses).
</footnote>
<page confidence="0.998726">
35
</page>
<bodyText confidence="0.999954040816327">
etc. More sophisticated models, based on model-
ing the belief state of the listener (e.g., (Goodman
and Stuhlm¨uller, 2013; Vogel et al., 2013)) are also
possible and within the scope of future work.
In addition to imperative sentences, we some-
times encounter descriptive sentences that de-
scribe what the state should look like at a given
step (c.f., (Lau et al., 2009)). We let WSt denote a
sentence (possibly empty) describing the t’th state,
St. The distribution p(St|WSt) is a discriminative
probabilistic classifier of some form.
Visual data. Much instructional information is
available in the form of how-to videos. In addi-
tion, some textual instructions are accompanied by
static images. We would like to extend the model
to exploit such data, when available.
Let a video clip associated with an action at
time t be denoted by VAt. We propose to learn
p(At|VAt) using supervised machine learning.
For features, we could use the output of standard
object detectors and their temporal trajectories, as
in (Yu and Siskind, 2013), bags of visual words
derived from temporal HOG descriptors as in (Das
et al., 2013), or features derived from RGB-D sen-
sors such as Kinect, as in (Song et al., 2013; Lei et
al., 2012).
There are many possible ways to fuse the in-
formation from vision and text, i.e., to com-
pute p(At|VAt, WAt, St−1). The simplest ap-
proach is to separately train the two conditionals,
p(At|WAt,St−1) and p(At|VAt), and then train
another model to combine them, using a separate
validation set; this will learn the relative reliability
of the two sources of signal.
Learning and inference. We assume that we
have manually labeled the actions At, and that the
initial state S0 is fully observed (e.g., a list of in-
gredients, with all containers empty). If we ad-
ditional assume that the world dynamics model is
known4 and deterministic, then we can uniquely
infer the sequence of states S1:T. This lets us use
standard supervised learning to fit the log-linear
model p(At|WAt, St−1).
In the future, we plan to relax the assumption
of fully labeled training data, and to allow for
learning from a distant supervision signal, simi-
lar to (Artzi and Zettlemoyer, 2013; Branavan et
al., 2009). For example, we can prefer a parse that
results in a final state in which all the ingredients
</bodyText>
<footnote confidence="0.689467666666667">
4There has been prior work on learning world models
from text, see e.g., (Sil and Yates, 2011; Branavan et al.,
2012).
</footnote>
<note confidence="0.373981">
have been consumed, and the meal is prepared.
</note>
<sectionHeader confidence="0.949384" genericHeader="method">
4 Preliminary results
</sectionHeader>
<bodyText confidence="0.999993791666667">
We conducted a preliminary analysis to gauge the
feasibility and expected performance benefits of
our approach. We used the raw recipes provided
in the CMU Recipe Database (Tasse and Smith,
2008), which consists of 260 English recipes
downloaded from allrecipes.com. We then
applied a state-of-the art SRL system (Das et al.,
2014) to the corpus, using Propbank (Palmer et al.,
2005) as our frame repository. Figure 3 summa-
rizes our findings.
To judge the variance of predicates used in the
cooking domain, we computed the frequency of
each word tagged as a present-tense verb by a sta-
tistical part-of-speech tagger, filtering out a small
number of common auxiliary verbs. Our find-
ings suggest a relatively small number of verbs
account for a large percentage of observed instruc-
tions (e.g, “add”, “bake”, and “stir”). The majority
of these verbs have corresponding framesets that
are usually correctly recognized, with some no-
table exceptions. Further, the most common ob-
served framesets have a straightforward mapping
to our set of kitchen state transformations, such
as object creation via combination (“add”, “mix”,
“combine”, “stir in”), location transfers (“place”,
“set”), and discrete state changes over a small
space of features (“cook”, “cut”, “cool”, “bake”).
To gain a preliminary understand of the limi-
tations of the current SRL system and the possi-
ble performance benefits of our proposed system,
we hand-annotated five of our recipes as follows:
Each verb in the recipe corresponding to an action
was annotated with its best corresponding roleset
(if any). Each role in that roleset was marked as
either being explicitly present in the text, implic-
itly present in our latent kitchen model but not in
the text (and so in principle, fillable by our model),
or neither present in the text nor in our model. For
example, in“cover for forty minutes”, the frameset
“cover” has an explicit temporal role-filling (“for
forty minutes”) and an implicit role-filling (“the
pot” as the patient of “cover”).
For each verb in the annotation, we checked if
the SRL system mapped that verb to the correct
roleset and if so, whether it filled the same seman-
tic roles as the annotator indicated were explicitly
present in the text. Overall, we found 54% recall
of the annotations by the SRL system. We quali-
</bodyText>
<page confidence="0.990249">
36
</page>
<figure confidence="0.959718153846154">
120
100
Frequency
80
60
40
20
14
12
10
Frequency
8
6
4
2
140
0
add stir mix bake combine remove place pour cook cool
Verb
add.02 combine.01cook.01 mix.01 place.01 stir.02 remain.01 bring.01 serve.01 melt.01
Roleset
Heat oil in a large pot [until hot]; brown chicken [in the pot].
Remove chicken [from the pot] and set [the chicken] aside.
Saute onions until [the onions are] soft, about 5 minutes.
Add broth, beans, half the pepper, and all the chicken [to the pot]; cover and simmer [the pot contents] for 40 minutes.
Add parsley, cilantro, salt, and remaining pepper [to the pot], and simmer [the mixture] 5 more minutes.
</figure>
<figureCaption confidence="0.9866015">
Figure 3: Results. Top: Distribution of the ten most common verbs and framesets in 260 recipes from
allrecipes.com. Bottom: An example recipe annotation. Blue indicates propbank predicates. Bracketed
</figureCaption>
<bodyText confidence="0.9767713125">
red indicates implicit propbank arguments not in the text, but in principle recognizable by our model.
Green indicates quantifier adjectives which our model could resolve to an exact quantity, given initial
ingredient amounts.
tatively notes several failure modes. Many errors
arise from not recognizing predicates represented
in the text as an imperative verb, likely because
PropBank contains few examples of such language
for the labeler to learn from. Other errors result
from ungrammatical constructs (e.g. in “cook five
minutes”, the eliding of “for” causes “five min-
utes” to incorrectly parse as a direct argument).
Certain cooking-related verbs lack framesets en-
tirely, such as “prebake”. Occasionally, the wrong
roleset is chosen. For example, in“Stir the mix-
ture” , “Stir” is labeled as “stir.02: cause (emo-
tional) reaction” rather than “stir.01: mix with a
circular motion”.
We also analyzed the quantity and qualitative
trends in the human annotations that refer to roles
fillable from the latent kitchen model but not lit-
erally present in the text. Overall, 52% of verb
annotations referenced at least one such role. The
most common situation (occurring for 36% of all
annotated verbs) is the “patient/direct object” role
is elided in the text but inferable from the world
state, as in “simmer [the mixture] for 40 min-
utes”. The second most common is the “location”
modifier role is elided in the text, as in “Remove
chicken [from the pot]”. Overall, we believe our
proposed approach will improve the quality of the
SRL system, and thus the overall interpretability
of the recipes.
</bodyText>
<sectionHeader confidence="0.992305" genericHeader="conclusions">
5 Possible applications
</sectionHeader>
<bodyText confidence="0.999497351351352">
We believe that semantic parsing of recipes and
other instructional text could support a rich array
of applications, such as the following:
Deriving a “canonical” recipe. It would be
useful to align different versions of the same
recipe to derive a “canonical form” cf., (Druck and
Pang, 2012; Tenorth et al., 2013b).
Explaining individual steps. It would be help-
ful if a user could click on a confusing step in a
recipe and get a more detailed explanation and/or
an illustrative video clip.
Automatically interpreting software instruc-
tions. Going beyond the recipe domain, it would
be useful to develop a system which can interpret
instructions such as how to install software, and
then automatically execute them (i.e., install the
software for you). In practice, this may be too
hard, so we could allow the system to ask for hu-
man help if it gets stuck, cf. (Deits et al., 2013).
Robotics. (Tenorth et al., 2013a) suggest min-
ing natural language “action recipes” as a way to
specify tasks for service robots. In the domain
of food recipes, there have already been several
demonstrations (e.g., (Beetz et al., 2011; Bollini et
al., 2013)) of robots automatically cooking meals
based on recipes.
Task assistance using augmented reality.
Imagine tracking the user as they follow some in-
structions using a device such as Google glass, and
offering help when needed. Such systems have
been developed before for specialized domains
like maintenance and repair of military hardware5,
but automatic parsing of natural language text po-
tentially opens this up to the consumer market.
(Note that there is already a recipe app for Google
Glass6, although it just displays a static list of in-
structions.)
</bodyText>
<footnote confidence="0.99388775">
5For example, see http://graphics.cs.
columbia.edu/projects/armar/index.htm.
6See http://www.glassappsource.com/
listing/all-the-cooks-recipes.
</footnote>
<page confidence="0.999319">
37
</page>
<sectionHeader confidence="0.990206" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99975635">
Y. Artzi and L. Zettlemoyer. 2013. Weakly supervised
learning of semantic parsers for mapping instruc-
tions to actions. Trans. Assoc. for Computational
Linguistics, 1:49–62.
M. Beetz, U. Klank, I. Kreese, A. Maldonado,
L. Mosenlechner, D. Pangercic, T. Ruhr, and
M. Tenorth. 2011. Robotic roommates making pan-
cakes. In Intl. Conf. on Humanoid Robots.
Mario Bollini, Stefanie Tellex, Tyler Thompson,
Nicholas Roy, and Daniela Rus. 2013. Interpreting
and executing recipes with a cooking robot. Experi-
mental Robotics.
SRK Branavan, H. Chen, L. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping in-
structions to actions. In Association for Computa-
tional Linguistics.
S.R.K. Branavan, N. Kushman, T. Lei, and R. Barzilay.
2012. Learning High-Level Planning from Text. In
ACL.
P. Das, C. Xu, R. F. Doell, and J. J. Corso. 2013. A
thousand frames in just a few words: Lingual de-
scription of videos through latent topics and sparse
object stitching. In CVPR.
D. Das, D. Chen, A. Martins, N. Schneider, and
N. Smith. 2014. Frame-semantic parsing. Com-
putational Linguistics.
R. Deits, S. Tellex, P. Thaker, D. Simeonov, T. Kol-
lar, and N. Roy. 2013. Clarifying Commands
with Information-Theoretic Human-Robot Dialog.
J. Human-Robot Interaction.
G. Druck and B. Pang. 2012. Spice it Up? Mining
Renements to Online Instructions from User Gener-
ated Content. In ACL.
Noah D Goodman and Andreas Stuhlm¨uller. 2013.
Knowledge and implicature: Modeling language un-
derstanding as social cognition. Topics in cognitive
science, 5(1):173–184.
TA Lau, Clemens Drews, and Jeffrey Nichols. 2009.
Interpreting Written How-To Instructions. IJCAI.
J. Lei, X. Ren, and D. Fox. 2012. Fine-grained kitchen
activity recognition using RGB-D. In Ubicomp.
C. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox.
2013. Learning to parse natural language commands
to a robot control system. Experimental Robotics,
pages 1–14.
D. Nyga and M. Beetz. 2012. Everything robots al-
ways wanted to know about housework (but were
afraid to ask). In Intl. Conf. on Intelligent Robots
and Systems.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71–106.
A. Sil and A. Yates. 2011. Extracting STRIPS Rep-
resentations of Actions and Events. In Recent Ad-
vances in NLP.
Young Chol Song, Henry Kautz, James Allen, Mary
Swift, Yuncheng Li, Jiebo Luo, and Ce Zhang.
2013. A markov logic framework for recognizing
complex events from multimodal data. In Proc. 15th
ACMIntl. Conf. Multimodal Interaction, pages 141–
148. ACM.
D. Tasse and N. Smith. 2008. SOUR CREAM: To-
ward Semantic Processing of Recipes. Technical
Report CMU-LTI-08-005, Carnegie Mellon Univer-
sity, Pittsburgh, PA.
M. Tenorth, A. Perzylo, R. Lafrenz, and M. Beetz.
2013a. Representation and exchange of knowl-
edge about actions, objects, and environments in the
roboearth framework. IEEE Trans. on Automation
Science and Engineering, 10(3):643–651.
M. Tenorth, J. Ziegltrum, and M. Beetz. 2013b. Auto-
mated alignment of specifications of everyday ma-
nipulation tasks. In IEEE Intl. Conf. on Intelligent
Robots and Systems.
A. Vogel, M. Bodoia, C. Potts, and D. Jurafsky. 2013.
Emergence of Gricean Maxims from Multi-Agent
Decision Theory. In NAACL.
Haonan Yu and JM Siskind. 2013. Grounded language
learning from video described with sentences. In As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.999351">
38
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.272457">
<title confidence="0.996304">Cooking with Semantics</title>
<author confidence="0.994265">Jon Malmaud Earl J Wagner</author>
<author confidence="0.994265">Nancy Chang</author>
<author confidence="0.994265">Kevin Murphy</author>
<affiliation confidence="0.7618895">Brain and Cognitive Sciences MIT Google</affiliation>
<address confidence="0.729725">43 Vassar St. 1600 Amphitheatre Pkwy Cambridge, MA Mountain View,</address>
<email confidence="0.981676">malmaud@mit.eduwag@google.com,ncchang@google.com,kpmurphy@google.com</email>
<abstract confidence="0.9985945">We are interested in the automatic interpretation of how-to instructions, such as cooking recipes, into semantic representations that can facilitate sophisticated question answering. Recent work has shown impressive results on semantic parsing of instructions with minimal supervision, but such techniques cannot handle much of the situated and ambiguous language used in instructions found on the web. In this paper, we suggest how to extend such methods using a model of pragmatics, based on a rich representation of world state.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<journal>Trans. Assoc. for Computational Linguistics,</journal>
<pages>1--49</pages>
<contexts>
<context position="11481" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="1811" endWordPosition="1814">prior work by allowing roles to be filled not just from spans from the text, but also by objects in the latent state vector. We will use various pragmatically-inspired features to represent the compatibility between candidate objects in the state vector and roles in the action frame, including: whether the object has been recently mentioned or touched, whether the object has the right affordances for the corresponding role (e.g., if the frame is “mix”, and the role is “what”, the object should be mixable), 3Although CCGs have been used in previous work on (situated) semantic parsing, such as (Artzi and Zettlemoyer, 2013), we chose to use the simpler approach based on frames because the nature of the language that occurs in recipes is sufficiently simple (there are very few complex nested clauses). 35 etc. More sophisticated models, based on modeling the belief state of the listener (e.g., (Goodman and Stuhlm¨uller, 2013; Vogel et al., 2013)) are also possible and within the scope of future work. In addition to imperative sentences, we sometimes encounter descriptive sentences that describe what the state should look like at a given step (c.f., (Lau et al., 2009)). We let WSt denote a sentence (possibly empty)</context>
<context position="13839" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="2206" endWordPosition="2209">y of the two sources of signal. Learning and inference. We assume that we have manually labeled the actions At, and that the initial state S0 is fully observed (e.g., a list of ingredients, with all containers empty). If we additional assume that the world dynamics model is known4 and deterministic, then we can uniquely infer the sequence of states S1:T. This lets us use standard supervised learning to fit the log-linear model p(At|WAt, St−1). In the future, we plan to relax the assumption of fully labeled training data, and to allow for learning from a distant supervision signal, similar to (Artzi and Zettlemoyer, 2013; Branavan et al., 2009). For example, we can prefer a parse that results in a final state in which all the ingredients 4There has been prior work on learning world models from text, see e.g., (Sil and Yates, 2011; Branavan et al., 2012). have been consumed, and the meal is prepared. 4 Preliminary results We conducted a preliminary analysis to gauge the feasibility and expected performance benefits of our approach. We used the raw recipes provided in the CMU Recipe Database (Tasse and Smith, 2008), which consists of 260 English recipes downloaded from allrecipes.com. We then applied a state-of</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Y. Artzi and L. Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Trans. Assoc. for Computational Linguistics, 1:49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Beetz</author>
<author>U Klank</author>
<author>I Kreese</author>
<author>A Maldonado</author>
<author>L Mosenlechner</author>
<author>D Pangercic</author>
<author>T Ruhr</author>
<author>M Tenorth</author>
</authors>
<title>Robotic roommates making pancakes.</title>
<date>2011</date>
<booktitle>In Intl. Conf. on Humanoid Robots.</booktitle>
<contexts>
<context position="20012" citStr="Beetz et al., 2011" startWordPosition="3220" endWordPosition="3223">tically interpreting software instructions. Going beyond the recipe domain, it would be useful to develop a system which can interpret instructions such as how to install software, and then automatically execute them (i.e., install the software for you). In practice, this may be too hard, so we could allow the system to ask for human help if it gets stuck, cf. (Deits et al., 2013). Robotics. (Tenorth et al., 2013a) suggest mining natural language “action recipes” as a way to specify tasks for service robots. In the domain of food recipes, there have already been several demonstrations (e.g., (Beetz et al., 2011; Bollini et al., 2013)) of robots automatically cooking meals based on recipes. Task assistance using augmented reality. Imagine tracking the user as they follow some instructions using a device such as Google glass, and offering help when needed. Such systems have been developed before for specialized domains like maintenance and repair of military hardware5, but automatic parsing of natural language text potentially opens this up to the consumer market. (Note that there is already a recipe app for Google Glass6, although it just displays a static list of instructions.) 5For example, see htt</context>
</contexts>
<marker>Beetz, Klank, Kreese, Maldonado, Mosenlechner, Pangercic, Ruhr, Tenorth, 2011</marker>
<rawString>M. Beetz, U. Klank, I. Kreese, A. Maldonado, L. Mosenlechner, D. Pangercic, T. Ruhr, and M. Tenorth. 2011. Robotic roommates making pancakes. In Intl. Conf. on Humanoid Robots.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Bollini</author>
<author>Stefanie Tellex</author>
<author>Tyler Thompson</author>
<author>Nicholas Roy</author>
<author>Daniela Rus</author>
</authors>
<title>Interpreting and executing recipes with a cooking robot. Experimental Robotics.</title>
<date>2013</date>
<contexts>
<context position="20035" citStr="Bollini et al., 2013" startWordPosition="3224" endWordPosition="3227"> software instructions. Going beyond the recipe domain, it would be useful to develop a system which can interpret instructions such as how to install software, and then automatically execute them (i.e., install the software for you). In practice, this may be too hard, so we could allow the system to ask for human help if it gets stuck, cf. (Deits et al., 2013). Robotics. (Tenorth et al., 2013a) suggest mining natural language “action recipes” as a way to specify tasks for service robots. In the domain of food recipes, there have already been several demonstrations (e.g., (Beetz et al., 2011; Bollini et al., 2013)) of robots automatically cooking meals based on recipes. Task assistance using augmented reality. Imagine tracking the user as they follow some instructions using a device such as Google glass, and offering help when needed. Such systems have been developed before for specialized domains like maintenance and repair of military hardware5, but automatic parsing of natural language text potentially opens this up to the consumer market. (Note that there is already a recipe app for Google Glass6, although it just displays a static list of instructions.) 5For example, see http://graphics.cs. columb</context>
</contexts>
<marker>Bollini, Tellex, Thompson, Roy, Rus, 2013</marker>
<rawString>Mario Bollini, Stefanie Tellex, Tyler Thompson, Nicholas Roy, and Daniela Rus. 2013. Interpreting and executing recipes with a cooking robot. Experimental Robotics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SRK Branavan</author>
<author>H Chen</author>
<author>L Zettlemoyer</author>
<author>R Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions. In Association for Computational Linguistics.</title>
<date>2009</date>
<contexts>
<context position="13863" citStr="Branavan et al., 2009" startWordPosition="2210" endWordPosition="2213">l. Learning and inference. We assume that we have manually labeled the actions At, and that the initial state S0 is fully observed (e.g., a list of ingredients, with all containers empty). If we additional assume that the world dynamics model is known4 and deterministic, then we can uniquely infer the sequence of states S1:T. This lets us use standard supervised learning to fit the log-linear model p(At|WAt, St−1). In the future, we plan to relax the assumption of fully labeled training data, and to allow for learning from a distant supervision signal, similar to (Artzi and Zettlemoyer, 2013; Branavan et al., 2009). For example, we can prefer a parse that results in a final state in which all the ingredients 4There has been prior work on learning world models from text, see e.g., (Sil and Yates, 2011; Branavan et al., 2012). have been consumed, and the meal is prepared. 4 Preliminary results We conducted a preliminary analysis to gauge the feasibility and expected performance benefits of our approach. We used the raw recipes provided in the CMU Recipe Database (Tasse and Smith, 2008), which consists of 260 English recipes downloaded from allrecipes.com. We then applied a state-of-the art SRL system (Das</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>SRK Branavan, H. Chen, L. Zettlemoyer, and R. Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>N Kushman</author>
<author>T Lei</author>
<author>R Barzilay</author>
</authors>
<title>Learning High-Level Planning from Text.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="14076" citStr="Branavan et al., 2012" startWordPosition="2249" endWordPosition="2252">that the world dynamics model is known4 and deterministic, then we can uniquely infer the sequence of states S1:T. This lets us use standard supervised learning to fit the log-linear model p(At|WAt, St−1). In the future, we plan to relax the assumption of fully labeled training data, and to allow for learning from a distant supervision signal, similar to (Artzi and Zettlemoyer, 2013; Branavan et al., 2009). For example, we can prefer a parse that results in a final state in which all the ingredients 4There has been prior work on learning world models from text, see e.g., (Sil and Yates, 2011; Branavan et al., 2012). have been consumed, and the meal is prepared. 4 Preliminary results We conducted a preliminary analysis to gauge the feasibility and expected performance benefits of our approach. We used the raw recipes provided in the CMU Recipe Database (Tasse and Smith, 2008), which consists of 260 English recipes downloaded from allrecipes.com. We then applied a state-of-the art SRL system (Das et al., 2014) to the corpus, using Propbank (Palmer et al., 2005) as our frame repository. Figure 3 summarizes our findings. To judge the variance of predicates used in the cooking domain, we computed the frequen</context>
</contexts>
<marker>Branavan, Kushman, Lei, Barzilay, 2012</marker>
<rawString>S.R.K. Branavan, N. Kushman, T. Lei, and R. Barzilay. 2012. Learning High-Level Planning from Text. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Das</author>
<author>C Xu</author>
<author>R F Doell</author>
<author>J J Corso</author>
</authors>
<title>A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching.</title>
<date>2013</date>
<booktitle>In CVPR.</booktitle>
<contexts>
<context position="12781" citStr="Das et al., 2013" startWordPosition="2025" endWordPosition="2028">abilistic classifier of some form. Visual data. Much instructional information is available in the form of how-to videos. In addition, some textual instructions are accompanied by static images. We would like to extend the model to exploit such data, when available. Let a video clip associated with an action at time t be denoted by VAt. We propose to learn p(At|VAt) using supervised machine learning. For features, we could use the output of standard object detectors and their temporal trajectories, as in (Yu and Siskind, 2013), bags of visual words derived from temporal HOG descriptors as in (Das et al., 2013), or features derived from RGB-D sensors such as Kinect, as in (Song et al., 2013; Lei et al., 2012). There are many possible ways to fuse the information from vision and text, i.e., to compute p(At|VAt, WAt, St−1). The simplest approach is to separately train the two conditionals, p(At|WAt,St−1) and p(At|VAt), and then train another model to combine them, using a separate validation set; this will learn the relative reliability of the two sources of signal. Learning and inference. We assume that we have manually labeled the actions At, and that the initial state S0 is fully observed (e.g., a </context>
</contexts>
<marker>Das, Xu, Doell, Corso, 2013</marker>
<rawString>P. Das, C. Xu, R. F. Doell, and J. J. Corso. 2013. A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching. In CVPR.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Das</author>
<author>D Chen</author>
<author>A Martins</author>
<author>N Schneider</author>
</authors>
<location>and</location>
<marker>Das, Chen, Martins, Schneider, </marker>
<rawString>D. Das, D. Chen, A. Martins, N. Schneider, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Smith</author>
</authors>
<title>Frame-semantic parsing. Computational Linguistics.</title>
<date>2014</date>
<marker>Smith, 2014</marker>
<rawString>N. Smith. 2014. Frame-semantic parsing. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Deits</author>
<author>S Tellex</author>
<author>P Thaker</author>
<author>D Simeonov</author>
<author>T Kollar</author>
<author>N Roy</author>
</authors>
<title>Clarifying Commands with Information-Theoretic Human-Robot Dialog.</title>
<date>2013</date>
<journal>J. Human-Robot Interaction.</journal>
<contexts>
<context position="19777" citStr="Deits et al., 2013" startWordPosition="3182" endWordPosition="3185">rm” cf., (Druck and Pang, 2012; Tenorth et al., 2013b). Explaining individual steps. It would be helpful if a user could click on a confusing step in a recipe and get a more detailed explanation and/or an illustrative video clip. Automatically interpreting software instructions. Going beyond the recipe domain, it would be useful to develop a system which can interpret instructions such as how to install software, and then automatically execute them (i.e., install the software for you). In practice, this may be too hard, so we could allow the system to ask for human help if it gets stuck, cf. (Deits et al., 2013). Robotics. (Tenorth et al., 2013a) suggest mining natural language “action recipes” as a way to specify tasks for service robots. In the domain of food recipes, there have already been several demonstrations (e.g., (Beetz et al., 2011; Bollini et al., 2013)) of robots automatically cooking meals based on recipes. Task assistance using augmented reality. Imagine tracking the user as they follow some instructions using a device such as Google glass, and offering help when needed. Such systems have been developed before for specialized domains like maintenance and repair of military hardware5, b</context>
</contexts>
<marker>Deits, Tellex, Thaker, Simeonov, Kollar, Roy, 2013</marker>
<rawString>R. Deits, S. Tellex, P. Thaker, D. Simeonov, T. Kollar, and N. Roy. 2013. Clarifying Commands with Information-Theoretic Human-Robot Dialog. J. Human-Robot Interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Druck</author>
<author>B Pang</author>
</authors>
<title>Spice it Up? Mining Renements to Online Instructions from User Generated Content.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="19188" citStr="Druck and Pang, 2012" startWordPosition="3080" endWordPosition="3083">, as in “simmer [the mixture] for 40 minutes”. The second most common is the “location” modifier role is elided in the text, as in “Remove chicken [from the pot]”. Overall, we believe our proposed approach will improve the quality of the SRL system, and thus the overall interpretability of the recipes. 5 Possible applications We believe that semantic parsing of recipes and other instructional text could support a rich array of applications, such as the following: Deriving a “canonical” recipe. It would be useful to align different versions of the same recipe to derive a “canonical form” cf., (Druck and Pang, 2012; Tenorth et al., 2013b). Explaining individual steps. It would be helpful if a user could click on a confusing step in a recipe and get a more detailed explanation and/or an illustrative video clip. Automatically interpreting software instructions. Going beyond the recipe domain, it would be useful to develop a system which can interpret instructions such as how to install software, and then automatically execute them (i.e., install the software for you). In practice, this may be too hard, so we could allow the system to ask for human help if it gets stuck, cf. (Deits et al., 2013). Robotics.</context>
</contexts>
<marker>Druck, Pang, 2012</marker>
<rawString>G. Druck and B. Pang. 2012. Spice it Up? Mining Renements to Online Instructions from User Generated Content. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah D Goodman</author>
<author>Andreas Stuhlm¨uller</author>
</authors>
<title>Knowledge and implicature: Modeling language understanding as social cognition. Topics in cognitive science,</title>
<date>2013</date>
<pages>5--1</pages>
<marker>Goodman, Stuhlm¨uller, 2013</marker>
<rawString>Noah D Goodman and Andreas Stuhlm¨uller. 2013. Knowledge and implicature: Modeling language understanding as social cognition. Topics in cognitive science, 5(1):173–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TA Lau</author>
<author>Clemens Drews</author>
<author>Jeffrey Nichols</author>
</authors>
<title>Interpreting Written How-To Instructions.</title>
<date>2009</date>
<publisher>IJCAI.</publisher>
<contexts>
<context position="12033" citStr="Lau et al., 2009" startWordPosition="1904" endWordPosition="1907">situated) semantic parsing, such as (Artzi and Zettlemoyer, 2013), we chose to use the simpler approach based on frames because the nature of the language that occurs in recipes is sufficiently simple (there are very few complex nested clauses). 35 etc. More sophisticated models, based on modeling the belief state of the listener (e.g., (Goodman and Stuhlm¨uller, 2013; Vogel et al., 2013)) are also possible and within the scope of future work. In addition to imperative sentences, we sometimes encounter descriptive sentences that describe what the state should look like at a given step (c.f., (Lau et al., 2009)). We let WSt denote a sentence (possibly empty) describing the t’th state, St. The distribution p(St|WSt) is a discriminative probabilistic classifier of some form. Visual data. Much instructional information is available in the form of how-to videos. In addition, some textual instructions are accompanied by static images. We would like to extend the model to exploit such data, when available. Let a video clip associated with an action at time t be denoted by VAt. We propose to learn p(At|VAt) using supervised machine learning. For features, we could use the output of standard object detector</context>
</contexts>
<marker>Lau, Drews, Nichols, 2009</marker>
<rawString>TA Lau, Clemens Drews, and Jeffrey Nichols. 2009. Interpreting Written How-To Instructions. IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lei</author>
<author>X Ren</author>
<author>D Fox</author>
</authors>
<title>Fine-grained kitchen activity recognition using RGB-D. In Ubicomp.</title>
<date>2012</date>
<contexts>
<context position="12881" citStr="Lei et al., 2012" startWordPosition="2045" endWordPosition="2048">orm of how-to videos. In addition, some textual instructions are accompanied by static images. We would like to extend the model to exploit such data, when available. Let a video clip associated with an action at time t be denoted by VAt. We propose to learn p(At|VAt) using supervised machine learning. For features, we could use the output of standard object detectors and their temporal trajectories, as in (Yu and Siskind, 2013), bags of visual words derived from temporal HOG descriptors as in (Das et al., 2013), or features derived from RGB-D sensors such as Kinect, as in (Song et al., 2013; Lei et al., 2012). There are many possible ways to fuse the information from vision and text, i.e., to compute p(At|VAt, WAt, St−1). The simplest approach is to separately train the two conditionals, p(At|WAt,St−1) and p(At|VAt), and then train another model to combine them, using a separate validation set; this will learn the relative reliability of the two sources of signal. Learning and inference. We assume that we have manually labeled the actions At, and that the initial state S0 is fully observed (e.g., a list of ingredients, with all containers empty). If we additional assume that the world dynamics mod</context>
</contexts>
<marker>Lei, Ren, Fox, 2012</marker>
<rawString>J. Lei, X. Ren, and D. Fox. 2012. Fine-grained kitchen activity recognition using RGB-D. In Ubicomp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Matuszek</author>
<author>E Herbst</author>
<author>L Zettlemoyer</author>
<author>D Fox</author>
</authors>
<title>Learning to parse natural language commands to a robot control system. Experimental Robotics,</title>
<date>2013</date>
<pages>1--14</pages>
<contexts>
<context position="10356" citStr="Matuszek et al., 2013" startWordPosition="1625" endWordPosition="1628">edicates, encoding the quantity, location, and condition (e.g., raw or cooked, empty or full) of each object. Note that previous work on situated semantic parsing often uses grid world environments where the only fluent is the agent’s location; in contrast, we allow any object to undergo state transformations. In particular, objects can be created and destroyed. Each action At is represented by a semantic frame, corresponding to a verb with various arguments or roles. This specifies how to transform the state. We also allow for sequencing and loop frames c.f., the “robot control language” in (Matuszek et al., 2013). We assume access to a simple cooking simulator that can take in a stream of low-level instructions to produce a new state; this implements the world dynamics model p(St|St−1,At). Text data. We assume that the text of the t’th sentence, represented by WAt, describes the t’th primitive action, At. We represent the conditional distribution p(At|WAt, St−1) as a loglinear model, as in prior work on frame-semantic parsing/ semantic role labeling (SRL) (Das et al., 2014).3 However, we extend this prior work by allowing roles to be filled not just from spans from the text, but also by objects in the</context>
</contexts>
<marker>Matuszek, Herbst, Zettlemoyer, Fox, 2013</marker>
<rawString>C. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox. 2013. Learning to parse natural language commands to a robot control system. Experimental Robotics, pages 1–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Nyga</author>
<author>M Beetz</author>
</authors>
<title>Everything robots always wanted to know about housework (but were afraid to ask).</title>
<date>2012</date>
<booktitle>In Intl. Conf. on Intelligent Robots and Systems.</booktitle>
<contexts>
<context position="8160" citStr="Nyga and Beetz, 2012" startWordPosition="1267" endWordPosition="1270">(“Chop or mash the avocados”). As should be clear from these examples, the interpretation of a given step in a set of instructions may hinge on many aspects of situated and procedural knowledge, including at least: the physical context (including the particular props and tools assumed available); the incremental state resulting from successful execution of previous steps; and general commonsense knowledge about the affordances of specific objects or expected arguments of specific actions (or more conveniently, corpus-based verb-argument expectations that approximate such knowledge, see e.g., (Nyga and Beetz, 2012)). All of these sources of knowledge go significantly beyond those employed in semantic parsing models for single utterances and in non-procedural contexts. 3 Proposed approach We propose to maintain a rich latent context that persists while parsing an entire recipe, in contrast 34 Bowl Flour Milk S0 S1 S2 S3 VA1 Video VS3 Image Empty In(packet) In(jug) Has(milk) In(packet) In(bowl) Has(milk, flour) In(bowl) In(bowl) Has(milk, flour) In(bowl) In(bowl) A1 Add: A2 Add: A3 Mix: from: jug from: packet what: bowl to: bowl to: bowl manner: well what: milk what: flour manner: pouring &amp;quot;Pour milk into </context>
</contexts>
<marker>Nyga, Beetz, 2012</marker>
<rawString>D. Nyga and M. Beetz. 2012. Everything robots always wanted to know about housework (but were afraid to ask). In Intl. Conf. on Intelligent Robots and Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="14529" citStr="Palmer et al., 2005" startWordPosition="2321" endWordPosition="2324">lts in a final state in which all the ingredients 4There has been prior work on learning world models from text, see e.g., (Sil and Yates, 2011; Branavan et al., 2012). have been consumed, and the meal is prepared. 4 Preliminary results We conducted a preliminary analysis to gauge the feasibility and expected performance benefits of our approach. We used the raw recipes provided in the CMU Recipe Database (Tasse and Smith, 2008), which consists of 260 English recipes downloaded from allrecipes.com. We then applied a state-of-the art SRL system (Das et al., 2014) to the corpus, using Propbank (Palmer et al., 2005) as our frame repository. Figure 3 summarizes our findings. To judge the variance of predicates used in the cooking domain, we computed the frequency of each word tagged as a present-tense verb by a statistical part-of-speech tagger, filtering out a small number of common auxiliary verbs. Our findings suggest a relatively small number of verbs account for a large percentage of observed instructions (e.g, “add”, “bake”, and “stir”). The majority of these verbs have corresponding framesets that are usually correctly recognized, with some notable exceptions. Further, the most common observed fram</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sil</author>
<author>A Yates</author>
</authors>
<date>2011</date>
<booktitle>Extracting STRIPS Representations of Actions and Events. In Recent Advances in NLP.</booktitle>
<contexts>
<context position="14052" citStr="Sil and Yates, 2011" startWordPosition="2245" endWordPosition="2248">we additional assume that the world dynamics model is known4 and deterministic, then we can uniquely infer the sequence of states S1:T. This lets us use standard supervised learning to fit the log-linear model p(At|WAt, St−1). In the future, we plan to relax the assumption of fully labeled training data, and to allow for learning from a distant supervision signal, similar to (Artzi and Zettlemoyer, 2013; Branavan et al., 2009). For example, we can prefer a parse that results in a final state in which all the ingredients 4There has been prior work on learning world models from text, see e.g., (Sil and Yates, 2011; Branavan et al., 2012). have been consumed, and the meal is prepared. 4 Preliminary results We conducted a preliminary analysis to gauge the feasibility and expected performance benefits of our approach. We used the raw recipes provided in the CMU Recipe Database (Tasse and Smith, 2008), which consists of 260 English recipes downloaded from allrecipes.com. We then applied a state-of-the art SRL system (Das et al., 2014) to the corpus, using Propbank (Palmer et al., 2005) as our frame repository. Figure 3 summarizes our findings. To judge the variance of predicates used in the cooking domain,</context>
</contexts>
<marker>Sil, Yates, 2011</marker>
<rawString>A. Sil and A. Yates. 2011. Extracting STRIPS Representations of Actions and Events. In Recent Advances in NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young Chol Song</author>
<author>Henry Kautz</author>
<author>James Allen</author>
<author>Mary Swift</author>
<author>Yuncheng Li</author>
<author>Jiebo Luo</author>
<author>Ce Zhang</author>
</authors>
<title>A markov logic framework for recognizing complex events from multimodal data.</title>
<date>2013</date>
<booktitle>In Proc. 15th ACMIntl. Conf. Multimodal Interaction,</booktitle>
<pages>141--148</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="12862" citStr="Song et al., 2013" startWordPosition="2041" endWordPosition="2044"> available in the form of how-to videos. In addition, some textual instructions are accompanied by static images. We would like to extend the model to exploit such data, when available. Let a video clip associated with an action at time t be denoted by VAt. We propose to learn p(At|VAt) using supervised machine learning. For features, we could use the output of standard object detectors and their temporal trajectories, as in (Yu and Siskind, 2013), bags of visual words derived from temporal HOG descriptors as in (Das et al., 2013), or features derived from RGB-D sensors such as Kinect, as in (Song et al., 2013; Lei et al., 2012). There are many possible ways to fuse the information from vision and text, i.e., to compute p(At|VAt, WAt, St−1). The simplest approach is to separately train the two conditionals, p(At|WAt,St−1) and p(At|VAt), and then train another model to combine them, using a separate validation set; this will learn the relative reliability of the two sources of signal. Learning and inference. We assume that we have manually labeled the actions At, and that the initial state S0 is fully observed (e.g., a list of ingredients, with all containers empty). If we additional assume that the</context>
</contexts>
<marker>Song, Kautz, Allen, Swift, Li, Luo, Zhang, 2013</marker>
<rawString>Young Chol Song, Henry Kautz, James Allen, Mary Swift, Yuncheng Li, Jiebo Luo, and Ce Zhang. 2013. A markov logic framework for recognizing complex events from multimodal data. In Proc. 15th ACMIntl. Conf. Multimodal Interaction, pages 141– 148. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tasse</author>
<author>N Smith</author>
</authors>
<title>SOUR CREAM: Toward Semantic Processing of Recipes.</title>
<date>2008</date>
<tech>Technical Report CMU-LTI-08-005,</tech>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="14341" citStr="Tasse and Smith, 2008" startWordPosition="2291" endWordPosition="2294">eled training data, and to allow for learning from a distant supervision signal, similar to (Artzi and Zettlemoyer, 2013; Branavan et al., 2009). For example, we can prefer a parse that results in a final state in which all the ingredients 4There has been prior work on learning world models from text, see e.g., (Sil and Yates, 2011; Branavan et al., 2012). have been consumed, and the meal is prepared. 4 Preliminary results We conducted a preliminary analysis to gauge the feasibility and expected performance benefits of our approach. We used the raw recipes provided in the CMU Recipe Database (Tasse and Smith, 2008), which consists of 260 English recipes downloaded from allrecipes.com. We then applied a state-of-the art SRL system (Das et al., 2014) to the corpus, using Propbank (Palmer et al., 2005) as our frame repository. Figure 3 summarizes our findings. To judge the variance of predicates used in the cooking domain, we computed the frequency of each word tagged as a present-tense verb by a statistical part-of-speech tagger, filtering out a small number of common auxiliary verbs. Our findings suggest a relatively small number of verbs account for a large percentage of observed instructions (e.g, “add</context>
</contexts>
<marker>Tasse, Smith, 2008</marker>
<rawString>D. Tasse and N. Smith. 2008. SOUR CREAM: Toward Semantic Processing of Recipes. Technical Report CMU-LTI-08-005, Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tenorth</author>
<author>A Perzylo</author>
<author>R Lafrenz</author>
<author>M Beetz</author>
</authors>
<title>Representation and exchange of knowledge about actions, objects, and environments in the roboearth framework.</title>
<date>2013</date>
<journal>IEEE Trans. on Automation Science and Engineering,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="19210" citStr="Tenorth et al., 2013" startWordPosition="3084" endWordPosition="3087">ixture] for 40 minutes”. The second most common is the “location” modifier role is elided in the text, as in “Remove chicken [from the pot]”. Overall, we believe our proposed approach will improve the quality of the SRL system, and thus the overall interpretability of the recipes. 5 Possible applications We believe that semantic parsing of recipes and other instructional text could support a rich array of applications, such as the following: Deriving a “canonical” recipe. It would be useful to align different versions of the same recipe to derive a “canonical form” cf., (Druck and Pang, 2012; Tenorth et al., 2013b). Explaining individual steps. It would be helpful if a user could click on a confusing step in a recipe and get a more detailed explanation and/or an illustrative video clip. Automatically interpreting software instructions. Going beyond the recipe domain, it would be useful to develop a system which can interpret instructions such as how to install software, and then automatically execute them (i.e., install the software for you). In practice, this may be too hard, so we could allow the system to ask for human help if it gets stuck, cf. (Deits et al., 2013). Robotics. (Tenorth et al., 2013</context>
</contexts>
<marker>Tenorth, Perzylo, Lafrenz, Beetz, 2013</marker>
<rawString>M. Tenorth, A. Perzylo, R. Lafrenz, and M. Beetz. 2013a. Representation and exchange of knowledge about actions, objects, and environments in the roboearth framework. IEEE Trans. on Automation Science and Engineering, 10(3):643–651.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tenorth</author>
<author>J Ziegltrum</author>
<author>M Beetz</author>
</authors>
<title>Automated alignment of specifications of everyday manipulation tasks.</title>
<date>2013</date>
<booktitle>In IEEE Intl. Conf. on Intelligent Robots and Systems.</booktitle>
<contexts>
<context position="19210" citStr="Tenorth et al., 2013" startWordPosition="3084" endWordPosition="3087">ixture] for 40 minutes”. The second most common is the “location” modifier role is elided in the text, as in “Remove chicken [from the pot]”. Overall, we believe our proposed approach will improve the quality of the SRL system, and thus the overall interpretability of the recipes. 5 Possible applications We believe that semantic parsing of recipes and other instructional text could support a rich array of applications, such as the following: Deriving a “canonical” recipe. It would be useful to align different versions of the same recipe to derive a “canonical form” cf., (Druck and Pang, 2012; Tenorth et al., 2013b). Explaining individual steps. It would be helpful if a user could click on a confusing step in a recipe and get a more detailed explanation and/or an illustrative video clip. Automatically interpreting software instructions. Going beyond the recipe domain, it would be useful to develop a system which can interpret instructions such as how to install software, and then automatically execute them (i.e., install the software for you). In practice, this may be too hard, so we could allow the system to ask for human help if it gets stuck, cf. (Deits et al., 2013). Robotics. (Tenorth et al., 2013</context>
</contexts>
<marker>Tenorth, Ziegltrum, Beetz, 2013</marker>
<rawString>M. Tenorth, J. Ziegltrum, and M. Beetz. 2013b. Automated alignment of specifications of everyday manipulation tasks. In IEEE Intl. Conf. on Intelligent Robots and Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vogel</author>
<author>M Bodoia</author>
<author>C Potts</author>
<author>D Jurafsky</author>
</authors>
<title>Emergence of Gricean Maxims from Multi-Agent Decision Theory.</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="11807" citStr="Vogel et al., 2013" startWordPosition="1865" endWordPosition="1868">ntioned or touched, whether the object has the right affordances for the corresponding role (e.g., if the frame is “mix”, and the role is “what”, the object should be mixable), 3Although CCGs have been used in previous work on (situated) semantic parsing, such as (Artzi and Zettlemoyer, 2013), we chose to use the simpler approach based on frames because the nature of the language that occurs in recipes is sufficiently simple (there are very few complex nested clauses). 35 etc. More sophisticated models, based on modeling the belief state of the listener (e.g., (Goodman and Stuhlm¨uller, 2013; Vogel et al., 2013)) are also possible and within the scope of future work. In addition to imperative sentences, we sometimes encounter descriptive sentences that describe what the state should look like at a given step (c.f., (Lau et al., 2009)). We let WSt denote a sentence (possibly empty) describing the t’th state, St. The distribution p(St|WSt) is a discriminative probabilistic classifier of some form. Visual data. Much instructional information is available in the form of how-to videos. In addition, some textual instructions are accompanied by static images. We would like to extend the model to exploit suc</context>
</contexts>
<marker>Vogel, Bodoia, Potts, Jurafsky, 2013</marker>
<rawString>A. Vogel, M. Bodoia, C. Potts, and D. Jurafsky. 2013. Emergence of Gricean Maxims from Multi-Agent Decision Theory. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haonan Yu</author>
<author>JM Siskind</author>
</authors>
<title>Grounded language learning from video described with sentences.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics.</booktitle>
<contexts>
<context position="12696" citStr="Yu and Siskind, 2013" startWordPosition="2010" endWordPosition="2013">empty) describing the t’th state, St. The distribution p(St|WSt) is a discriminative probabilistic classifier of some form. Visual data. Much instructional information is available in the form of how-to videos. In addition, some textual instructions are accompanied by static images. We would like to extend the model to exploit such data, when available. Let a video clip associated with an action at time t be denoted by VAt. We propose to learn p(At|VAt) using supervised machine learning. For features, we could use the output of standard object detectors and their temporal trajectories, as in (Yu and Siskind, 2013), bags of visual words derived from temporal HOG descriptors as in (Das et al., 2013), or features derived from RGB-D sensors such as Kinect, as in (Song et al., 2013; Lei et al., 2012). There are many possible ways to fuse the information from vision and text, i.e., to compute p(At|VAt, WAt, St−1). The simplest approach is to separately train the two conditionals, p(At|WAt,St−1) and p(At|VAt), and then train another model to combine them, using a separate validation set; this will learn the relative reliability of the two sources of signal. Learning and inference. We assume that we have manua</context>
</contexts>
<marker>Yu, Siskind, 2013</marker>
<rawString>Haonan Yu and JM Siskind. 2013. Grounded language learning from video described with sentences. In Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
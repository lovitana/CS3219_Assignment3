<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.994007">
Vector Space Models for Phrase-based Machine Translation
</title>
<author confidence="0.99959">
Tamer Alkhouli1, Andreas Guta1, and Hermann Ney1,2
</author>
<affiliation confidence="0.78221775">
1Human Language Technology and Pattern Recognition Group
RWTH Aachen University, Aachen, Germany
2Spoken Language Processing Group
Univ. Paris-Sud, France and LIMSI/CNRS, Orsay, France
</affiliation>
<email confidence="0.996131">
{surname}@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.99735" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999827842105263">
This paper investigates the application
of vector space models (VSMs) to the
standard phrase-based machine translation
pipeline. VSMs are models based on
continuous word representations embed-
ded in a vector space. We exploit word
vectors to augment the phrase table with
new inferred phrase pairs. This helps
reduce out-of-vocabulary (OOV) words.
In addition, we present a simple way to
learn bilingually-constrained phrase vec-
tors. The phrase vectors are then used to
provide additional scoring of phrase pairs,
which fits into the standard log-linear
framework of phrase-based statistical ma-
chine translation. Both methods result
in significant improvements over a com-
petitive in-domain baseline applied to the
Arabic-to-English task of IWSLT 2013.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999871228070175">
Categorical word representation has been widely
used in many natural language processing (NLP)
applications including statistical machine transla-
tion (SMT), where words are treated as discrete
random variables. Continuous word representa-
tions, on the other hand, have been applied suc-
cessfully in many NLP areas (Manning et al.,
2008; Collobert and Weston, 2008). However,
their application to machine translation is still an
open research question. Several works tried to ad-
dress the question recently (Mikolov et al., 2013b;
Zhang et al., 2014; Zou et al., 2013), and this work
is but another step in that direction.
While categorical representations do not encode
any information about word identities, continuous
representations embed words in a vector space, re-
sulting in geometric arrangements that reflect in-
formation about the represented words. Such em-
beddings open the potential for applying informa-
tion retrieval approaches where it becomes possi-
ble to define and compute similarity between dif-
ferent words. We focus on continuous represen-
tations whose training is influenced by the sur-
rounding context of the token being represented.
One motivation for such representations is to cap-
ture word semantics (Turney et al., 2010). This
is based on the distributional hypothesis (Harris,
1954) which says that words that occur in similar
contexts tend to have similar meanings.
We make use of continuous vectors learned
using simple neural networks. Neural networks
have been gaining increasing attention recently,
where they have been able to enhance strong SMT
baselines (Devlin et al., 2014; Sundermeyer et
al., 2014). While neural language and transla-
tion modeling make intermediate use of continu-
ous representations, there have been also attempts
at explicit learning of continuous representations
to improve translation (Zhang et al., 2014; Gao et
al., 2013).
This work explores the potential of word se-
mantics based on continuous vector representa-
tions to enhance the performance of phrase-based
machine translation. We present a greedy algo-
rithm that employs the phrase table to identify
phrases in a training corpus. The phrase table
serves to bilingually restrict the phrases spotted
in the monolingual corpus. The algorithm is ap-
plied separately to the source and target sides of
the training data, resulting in source and target cor-
pora of phrases (instead of words). The phrase
corpus is used to learn phrase vectors using the
same methods that produce word vectors. The
vectors are then used to provide semantic scor-
ing of phrase pairs. We also learn word vectors
and employ them to augment the phrase table with
paraphrased entries. This leads to a reduction in
</bodyText>
<page confidence="0.82576">
1
</page>
<note confidence="0.7842225">
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 1–10,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999794714285714">
the OOV rate which translates to improved BLEU
and and TER scores. We apply the two methods on
the IWSLT 2013 Arabic-to-English task and show
significant improvements over a strong in-domain
baseline.
The rest of the paper is structured as follows.
Section 2 presents a background on word and
phrase vectors. The construction of the phrase
corpus is discussed in Section 3, while Section 4
demonstrates how to use word and phrase vectors
in the standard phrase-based SMT pipeline. Ex-
periments are presented in Section 5, followed by
an overview of the related word in Section 6, and
finally Section 7 concludes the work.
</bodyText>
<sectionHeader confidence="0.996657" genericHeader="introduction">
2 Vector Space Models
</sectionHeader>
<bodyText confidence="0.999886436363636">
One way to obtain context-based word vectors is
through a neural network (Bengio et al., 2003;
Schwenk, 2007). With a vocabulary size V, one-
hot encoding of V-dimensional vectors is used to
represent input words, effectively associating each
word with a D-dimensional vector in the V × D
input weight matrix, where D is the size of the
hidden layer. Similarly, one-hot encoding on the
output layer associates words with vectors in the
output weight matrix.
Alternatively, a count-based V-dimensional
word co-occurrence vector can serve as a word
representation (Lund and Burgess, 1996; Lan-
dauer and Dumais, 1997). Such representations
are sparse and high-dimensional, which might re-
quire an additional dimensionality reduction step
(e.g. using SVD). In contrast, learning word rep-
resentations via neural models results directly in
relatively low-dimensional, dense vectors. In this
work, we follow the neural network approach to
extract the feature vectors. Whether word vectors
are extracted by means of a neural network or co-
occurrence counts, the context surrounding a word
influences its final representation by design. Such
context-based representations can be used to de-
termine semantic similarities.
The construction of phrase representations, on
the other hand, can be done in different ways.
The compositional approach constructs the vector
representation of a phrase by resorting to its con-
stituent words (or sub-phrases) (Gao et al., 2013;
Chen et al., 2010). Kalchbrenner and Blunsom
(2013) obtain continuous sentence representations
by applying a sequence of convolutions, starting
with word representations.
Another approach for phrase representation
considers phrases as atomic units that can not be
divided further. The representations are learned
directly in this case (Mikolov et al., 2013b; Hu et
al., 2014).
In this work, we follow the second approach to
obtain phrase vectors. To this end, we apply the
same methods that yield word vectors, with the
difference that phrases are used instead of words.
In the case of neural word representations, a neural
network that is presented with words at the input
layer is presented with phrases instead. The result-
ing vocabulary size in this case would be the num-
ber of distinct phrases observed during training.
Although learning phrase embeddings directly is
amenable to data sparsity issues, it provides us
with a simple means to build phrase vectors mak-
ing use of tools already developed for word vec-
tors, focussing the effort on preprocessing the data
as will be discussed in the next section.
</bodyText>
<sectionHeader confidence="0.994383" genericHeader="method">
3 Phrase Corpus
</sectionHeader>
<bodyText confidence="0.99968072">
When training word vectors using neural net-
works, the network is presented with a corpus.
To build phrase vectors, we first identify phrases
in the corpus and generate a phrase corpus. The
phrase corpus is similar to the original corpus ex-
cept that its words are joined to make up phrases.
The new corpus is then used to train the neural net-
work. The columns of the resulting input weight
matrix of the network are the phrase vectors corre-
sponding to the phrases encountered during train-
ing.
Mikolov et al. (2013b) identify phrases using a
monolingual point-wise mutual information crite-
rion with discounting. Since our end goal is to
generate phrase vectors that are helpful for trans-
lation, we follow a different approach: we con-
strain the phrases by the conventional phrase table
of phrase-based machine translation. This is done
by limiting the phrases identified in the corpus to
high quality phrases occurring in the phrase table.
The quality is determined using bilingual scores
of phrase pairs. While the phrase vectors of a lan-
guage are eventually obtained by training the neu-
ral network on the monolingual phrase corpus of
that language, the reliance on bilingual scores to
</bodyText>
<page confidence="0.989604">
2
</page>
<construct confidence="0.381714">
Algorithm 1 Phrase Corpus Construction
</construct>
<listItem confidence="0.986302791666667">
1: p ← 1
2: for p ≤ numPasses do
3: i ← 2
4: for i ≤ corpus.size −1 do
5: w&amp;quot; ← join(ti,ti+1) &gt; create a phrase using the current and next tokens
6: v&amp;quot; ← join(ti−1,ti) &gt; create a phrase using the previous and current tokens
7: joinForward ← score( &amp;quot;w)
8: joinBackward ← score(v&amp;quot;)
9: if joinForward ≥ joinBackward and joinForward ≥ 9 then
10: ti ← w&amp;quot;
11: remove ti+1
12: i ← i+2 &gt; newly created phrase not available for further merge during current pass
13: else
14: if joinBackward &gt; joinForward and joinBackward ≥ 9 then
15: ti−1 ← v&amp;quot;
16: remove ti
17: i ← i + 2 &gt; newly created phrase not available for further merge during current pass
18: else
19: i ← i+1
20: end if
21: end if
22: end for
23: p ← p+1
24: end for
</listItem>
<bodyText confidence="0.999183727272727">
construct the monolingual phrase corpus encodes
bilingual information in the corpus, namely, the
corpus will include phrases that having a match-
ing phrase in the other language, which is in line
with the purpose for which the phrases are con-
structed, that is, their use in the phrase-based ma-
chine translation pipeline which is explained in the
next section. In addition, the aforementioned scor-
ing serves to exclude noisy phrase-pair entries dur-
ing the construction of the phrase corpus. Next, we
explain the details of the construction algorithm.
</bodyText>
<subsectionHeader confidence="0.999845">
3.1 Phrase Spotting
</subsectionHeader>
<bodyText confidence="0.9999748125">
We propose Algorithm 1 as a greedy approach for
phrase corpus construction. It is a multi-pass algo-
rithm where each pass can extend tokens obtained
during the previous pass by a single token at most.
Before the first pass, all tokens are words. During
the passes the tokens might remain as words or can
be extended to become phrases. Given a token ti
at position i, a scoring function is used to score
the phrase (ti,ti+1) and the phrase (ti−1,ti). The
phrase having a higher score is adopted as long as
its score exceeds a predefined threshold 9. The
scoring function used in lines 7 and 8 is based on
the phrase table. If the phrase does not belong to
the phrase table it is given a score 90 &lt; 9. If the
phrase exists, a bilingual score is computed using
the phrase table fields as follows:
</bodyText>
<equation confidence="0.97654">
� score( f&amp;quot;) = max∑wi gi (f&amp;quot;, &amp;quot;e) (1)
e i=1
L
</equation>
<bodyText confidence="0.999312111111111">
where
e&amp;quot;) is the ith feature of the bilingual
phrase pair
e&amp;quot;). The maximization is carried out
over all phrases e&amp;quot; of the other language. The score
is the weighted sum of the phrase pair features.
Throughout our experiments, we use 2 phrasal and
2 lexical features for scoring, with manual tuning
of the weights wi
</bodyText>
<equation confidence="0.812159333333333">
gi( f&amp;quot;,
(f&amp;quot;,
.
</equation>
<bodyText confidence="0.999556333333333">
The resulting corpus is then used to train phrase
vectors following the same procedure of training
word vectors.
</bodyText>
<sectionHeader confidence="0.998753" genericHeader="method">
4 End-to-end Translation
</sectionHeader>
<bodyText confidence="0.993465">
In this section we will show how to employ phrase
vectors in the phrase-based statistical machine
slation pipeline.
tran
</bodyText>
<page confidence="0.986959">
3
</page>
<subsectionHeader confidence="0.995083">
4.1 Phrase-based Machine Translation
</subsectionHeader>
<bodyText confidence="0.999679333333333">
The phrase-based decoder consists of a search us-
ing a log-linear framework (Och and Ney, 2002)
as follows:
</bodyText>
<equation confidence="0.99917875">
M
max max∑λmhm (eI1, sK1, f1J) (2)
I,� K,� m=1
i = arg
</equation>
<bodyText confidence="0.8805197">
where eI1 =
is the target sentence,
=
f1...
is the source sentence,
=
is
the hidden alignment or derivation. The mod-
els
are weighted by the weights
</bodyText>
<equation confidence="0.943267571428571">
e1...eI
f1J
fJ
sK1
s1...sK
hm(eI1,sK1,f1J)
λm
</equation>
<bodyText confidence="0.988065823529412">
which are tuned using minimum error rate train-
ing (MERT) (Och, 2003). The rest of the section
presents two ways to integrate vector representa-
tions into the system described above.
learned separately, we do not have an immedi-
ate mapping between them. As such mapping is
needed for the training of the projection matrix,
we resort to the phrase table to obtain it. A source
and a target phrase vectors are paired if there is a
corresponding phrase pair entry in the phrase table
whose score exceeds a certain threshold. Scoring
is computed using Eq. 1. Similarly, word vectors
are paired using IBM 1
and p(f
lexica.
Noisy entries are assumed to have a probability
less than a certain threshold an
</bodyText>
<equation confidence="0.932943">
p(e|f)
|e)
</equation>
<bodyText confidence="0.952266">
d are not used to
pair word vectors.
</bodyText>
<subsectionHeader confidence="0.999344">
4.3 Paraphrasing
</subsectionHeader>
<bodyText confidence="0.856854666666667">
Given a phrase pair
e&amp;quot;), we can use the phrase
vectors of the source and target phrases to compute
a seman
(f&amp;quot;,
tic phrase feature as follows:
</bodyText>
<equation confidence="0.47817">
(3)
</equation>
<bodyText confidence="0.9898053">
where sim is a similarity function,
and
are the
S-dimensional source and T-dimensional target
vectors respectively corresponding to the source
phrase
and target phrase e&amp;quot;. W is an S x T linear
projection matrix that maps the source space to the
target space (Mikolov et al., 2013a). The matrix
is estimated by optimizing the foll
</bodyText>
<equation confidence="0.7171765">
hM+1(f&amp;quot; , &amp;quot;e) = sim(Wxf&amp;quot; ,z&amp;quot;e)
xf&amp;quot;
z&amp;quot;e
f&amp;quot;
</equation>
<bodyText confidence="0.7695625">
owing criterion
with stochastic gradient descent:
</bodyText>
<equation confidence="0.926064">
||Wxi−zi||2 (4)
</equation>
<bodyText confidence="0.753082">
{(x1,z1),...,(xN,zN)}
d target vectors.
corresponding to the source
an
</bodyText>
<subsectionHeader confidence="0.859934">
4.2 Semantic Phrase Feature
</subsectionHeader>
<bodyText confidence="0.999674909090909">
Words that occur in similar contexts tend to have
similar meanings. This idea is known as the dis-
tributional hypothesis (Harris, 1954), and it moti-
vates the use of word context to learn word repre-
sentations that capture word semantics (Turney et
al., 2010). Extending this notion to phrases, phrase
vectors that are learned based on the surrounding
context encode phrase semantics. Since we will
use phrase vectors to compute a feature of a phrase
pair in the following, we refer to the feature as a
semantic phrase feature.
</bodyText>
<equation confidence="0.9247712">
min
W
N
∑
i=1
</equation>
<bodyText confidence="0.999711675675676">
where the training data consists of the pairs
While the standard phrase table is extracted using
parallel training data, we propose to extend it and
infer new entries relying on continuous representa-
tions. With a similarity measure (e.g. cosine sim-
ilarity) that computes the similarity between two
phrases, a new phrase pair can be generated by re-
placing either or both of its constituent phrases by
similar phrases. The new phrase is referred to as a
paraphrase of the phrase it replaces. This enables
a richer use of the bilingual data, as a source para-
phrase can be borrowed from a sentence that is not
aligned to a sentence containing the target side of
the phrase pair. It also enables the use of monolin-
gual data, as the source and target paraphrases do
not have to occur in the parallel data. The cross-
interaction between sentences in the parallel data
and the inclusion of the monolingual data to ex-
tend the phrase table are potentially capable of re-
ducing the out-of-vocabulary (OOV) rate.
In order to generate a new phrase rule, we en-
sure that noisy rules do not contribute to the gener-
ationprocess, depending on the score of the phrase
pair (cf. Eq. 1). High scoring entries are para-
phrased as follows. To paraphrase the source side,
we perform a k-nearest neighbor search over the
source phrase vectors. The top-k similar entries
are considered paraphrases of the given phrase.
The same can be done for the target side. We as-
sign the newly generated phrase pair the same fea-
ture values of the pair used to induce it. However,
two extra phrase features are added: one measur-
ing the similarity between the source phrase and
its paraphrase, and another for the target phrase
an
d its paraphrase. The new feature values for
the original non-paraphrased entries are set to the
</bodyText>
<sectionHeader confidence="0.488879" genericHeader="method">
4 Since the source and target phrase vectors are
</sectionHeader>
<bodyText confidence="0.996399333333333">
highest similarity value.
We focus on a certain setting that avoids in-
terference with original phrase rules, by extend-
ing the phrase table to cover OOVs only. That
is, source-side paraphrasing is performed only if
the source paraphrase does not already occur in
the phrase table. This ensures that original entries
are not interfered with and only OOVs are affected
during translation. Reducing OOVs by extending
the phrase table has the advantage of exploiting
the full decoding capabilities (e.g. LM scoring),
as opposed to post-decoding translation of OOVs,
which would not exhibit any decoding benefits.
The k-nearest neighbor (k-NN) approach is
computationally prohibitive for large phrase tables
and large number of vectors. This can be allevi-
ated by resorting to approximate k-NN search (e.g.
locality sensitive hashing). Note that this search
is performed during training time to generate ad-
ditional phrase table entries, and does not affect
decoding time, except through the increase of the
phrase table size. In our experiments, the train-
ing time using exact k-NN search was acceptable,
therefore no search approximations were made.
</bodyText>
<sectionHeader confidence="0.999742" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99949365">
In the following we first provide an analysis of the
word vectors that are later used for translation ex-
periments. We use word vectors (as opposed to
phrase vectors) for phrase table paraphrasing to
reduce the OOV rate. Next, we present end-to-
end translation results using the proposed seman-
tic feature and our OOV reduction method.
The experiments are based on vectors trained
using the word2vec1 toolkit, setting vector dimen-
sionality to 800 for Arabic and 200 for English
vectors. We used the skip-gram model with a max-
imum skip length of 10. The phrase corpus was
constructed using 5 passes, with scores computed
according to Eq. 1 using 2 phrasal and 2 lexical
features. The phrasal and lexical weights were set
to 1 and 0.5 respectively, with all features being
negative log-probabilities, and the scoring thresh-
old θ was set to 10. All translation experiments
are performed with the Jane toolkit (Vilar et al.,
2010; Wuebker et al., 2012).
</bodyText>
<footnote confidence="0.981489">
1https://code.google.com/p/word2vec/
</footnote>
<subsectionHeader confidence="0.983912">
5.1 Baseline System
</subsectionHeader>
<bodyText confidence="0.99992245">
Our phrase-based baseline system consists of two
phrasal and two lexical translation models, trained
using a word-aligned bilingual training corpus.
Word alignment is automatically generated by
GIZA++ (Och and Ney, 2003) given a sentence-
aligned bilingual corpus. We also include bi-
nary count features and bidirectional hierarchical
reordering models (Galley and Manning, 2008),
with three orientation classes per direction result-
ing in six reordering models. The baseline also in-
cludes word penalty, phrase penalty and a simple
distance-based distortion model.
The language model (LM) is a 4-gram mix-
ture LM trained on several data sets using mod-
ified Kneser-Ney discounting with interpolation,
and combined with weights tuned to achieve the
lowest perplexity on a development set using the
SRILM toolkit (Stolcke, 2002). Data selection
is performed using cross-entropy filtering (Moore
and Lewis, 2010).
</bodyText>
<subsectionHeader confidence="0.998292">
5.2 Word Vectors
</subsectionHeader>
<bodyText confidence="0.999979521739131">
Here we analyze the quality of word vectors used
in the OOV reduction experiments. The vectors
are trained using an unaltered word corpus. We
build a lexicon using source and target word vec-
tors together with the projection matrix using the
similarity score sim(Wxf,ze)), where the projec-
tion matrix W is used to project the source word
vector xf, corresponding to the source word f, to
the target vector space. The similarity between the
projection result Wxf and the target word vector
ze is computed. In the following we will refer to
these scores computed using vector representation
as VSM-based scores.
The resulting lexicon is compared to the IBM
1 lexicon2. Given a source word, we select the
the best target word according to the VSM-based
score. This is compared to the best translation
based on the IBM 1 probability. If both transla-
tions coincide, we refer to this as a 1-best match.
We also check whether the best translation accord-
ing to IBM 1 matches any of the top-5 translations
based on the VSM model. A match in this case is
referred to as a 5-best match.
</bodyText>
<footnote confidence="0.998696666666667">
2We assume for the purpose of this experiment that the
IBM 1 lexicon provides perfect translations, which is not nec-
essarily the case in practice.
</footnote>
<page confidence="0.987364">
5
</page>
<table confidence="0.999854142857143">
corpus Lang. # tokens # segments
WIT Ar 3,185,357 147,256
UN Ar 228,302,244 7,884,752
arGiga3 Ar 782,638,101 27,190,387
WIT En 2,951,851 147,256
UN En 226,280,918 7,884,752
news En 1,129,871,814 45,240,651
</table>
<tableCaption confidence="0.999893">
Table 1: Arabic and English corpora statistics.
</tableCaption>
<bodyText confidence="0.9995957">
The vectors are trained on a mixture of in-
domain data (WIT) which correspond to TED
talks, and out-of-domain data (UN). These sets are
provided as part of the IWSLT 2013 evaluation
campaign. We include the LDC2007T40 Arabic
Gigaword v3 (arGiga3) and English news crawl ar-
ticles (2007 through 2012) to experiment with the
effect of increasing the size of the training corpus
on the quality of the word vectors. Table 1 shows
the corpora statistics obtained after preprocessing.
The fractions of the 1- and 5-best matches are
shown in table 2. The table is split into two halves.
The upper part investigates the effect of increasing
the amount of Arabic data while keeping the En-
glish data fixed (2nd row), the effect of increasing
the amount of the English data while keeping the
Arabic data fixed (3rd row), and the effect of using
more data on both sides (4th row). The projection
is done on the representation of the Arabic word f,
and the similarity is computed between the projec-
tion and the representation of the English word e.
In the lower half of the table, the same effects are
explored, except that the projection is performed
on the English side instead. The results indicate
that the accuracy increases when increasing the
amount of data only on the side being projected.
More data on the corresponding side (i.e. the side
being projected to) decreases the accuracy. The
same behavior is observed whether the projected
side is Arabic (upper half) or English (lower half).
All in all, the accuracy values are low. The accu-
racy increases about three times when looking at
the 5-best instead of the 1-best accuracy. While the
accuracies 32.2% and 33.1% are low, they reflect
that the word representations are encoding some
information about the words, although this infor-
mation might not be good enough to build a word-
to-word lexicon. However, using this information
for OOV reduction might still yield improvements
as we will see in the translation results.
</bodyText>
<table confidence="0.9988875">
Arabic English
word corpus size 231M 229M
phrase corpus size 126M 115M
word corpus vocab. size 467K 421K
phrase corpus vocab. size 5.8M 5.3M
# phrase vectors 934K 913K
</table>
<tableCaption confidence="0.999333">
Table 3: Phrase vectors statistics.
</tableCaption>
<subsectionHeader confidence="0.998613">
5.3 Phrase Vectors
</subsectionHeader>
<bodyText confidence="0.999918897435897">
Translation experiments pertaining to the pro-
posed semantic feature are presented here. The
feature is based on phrase vectors which are built
with the word2vec toolkit in a similar way word
vectors are trained, except that the training cor-
pus is the phrase corpus containing phrases con-
structed as described in section 3. Once trained, a
new feature is added to the phrase table. The fea-
ture is computed for each phrase pair using phrase
vectors as described in Eq. 3.
Table 3 shows statistics about the phrase corpus
and the original word corpus it is based on. Al-
gorithm 1 is used to build the phrase corpus using
5 passes. The number of phrase vectors trained
using the phrase corpus are also shown. Note that
the tool used does not produce vectors for all 5.8M
Arabic and 5.3M English phrases in the vocab-
ulary. Rather, noisy phrases are excluded from
training, eventually leading to 934K Arabic and
913K English phrase embeddings.
We perform two experiments on the IWSLT
2013 Arabic-to-English evaluation data set. In the
first experiment, we examine how the semantic
feature affects a small phrase table (2.3M phrase
pairs) trained on the in-domain data (WIT). The
second experiment deals with a larger phrase table
(34M phrase pairs), constructed by a linear inter-
polation between in- and out-of-domain phrase ta-
bles including UN data, resulting in a competitive
baseline. The two baselines have hierarchical re-
ordering models (HRMs) and a tuned mixture LM,
in addition to the standard models, as described in
section 5.1. The results are shown in table 4.
In the small experiment, the semantic phrase
feature improves TER by 0.7%, and BLEU by
0.4% on the test set eval13. The translation seems
to benefit from the contextual information en-
coded in the phrase vectors during training. This
is in contrast to the training of the standard phrase
</bodyText>
<page confidence="0.99921">
6
</page>
<table confidence="0.9996928">
Arabic Data English Data 1-best 5-best
Match % Matches %
WIT+UN WIT+UN 8.0 26.1
WIT+UN+arGiga3 WIT+UN 10.9 32.2
WIT+UN WIT+UN+news 4.9 17.9
WIT+UN+arGiga3 WIT+UN+news 7.5 25.7
WIT+UN WIT+UN 8.4 27.2
WIT+UN WIT+UN+news 10.9 33.1
WIT+UN+arGiga3 WIT+UN 5.7 18.9
WIT+UN+arGiga3 WIT+UN+news 8.3 25.2
</table>
<tableCaption confidence="0.998942">
Table 2: The effect of increasing the amount of data on the quality of word vectors. VSM-based scores are
</tableCaption>
<bodyText confidence="0.9066944">
compared to IBM model 1 p(e|f) (upper half) and p(f |e) (lower half), effectively regarding the IBM 1
models as the true probability distributions. In the upper part, the projection is done on the representation
of the Arabic word f, and the similarity is computed between the projection and the representation of the
English word e. In the lower half of the table, the role of f and e is interchanged, where the English side
in this case will be projected.
</bodyText>
<table confidence="0.998780375">
system dev2010 TER eval2013 TER
BLEU BLEU
WIT 29.1 50.5 28.9 52.5
+ feature 29.1 $50.1 $29.3 $51.8
+ paraph. 29.2 $50.2 $29.5 $51.8
+ both 29.2 50.2 $29.4 $51.8
WIT+UN 29.7 49.3 30.5 50.5
+ feature 29.8 49.2 30.2 50.7
</table>
<tableCaption confidence="0.798903666666667">
Table 4: Semantic feature and paraphrasing re-
sults. The symbol $ indicates statistical signifi-
cance with p &lt; 0.01.
</tableCaption>
<bodyText confidence="0.99960947368421">
features, which disregards context. As for the hi-
erarchical reordering models which are part of the
baseline, they do not capture lexical information
about the context. They are only limited to the or-
dering information. The skip-gram-based phrase
vectors used for the semantic feature, on the other
hand, discard ordering information, but uses con-
textual lexical information for phrase representa-
tion. In this sense, HRMs and the semantic feature
can be said to complement each other. Using the
semantic feature for the large phrase table did not
yield improvements. The difference compared to
the baseline in this case is not statistically signifi-
cant.
All reported results are averages of 3 MERT op-
timizer runs. Statistical significance is computed
using the Approximate Randomization (AR) test.
We used the multeval toolkit (Clark et al., 2011)
for evaluation.
</bodyText>
<subsectionHeader confidence="0.997646">
5.4 Paraphrasing and OOV Reduction
</subsectionHeader>
<bodyText confidence="0.9999272">
The next set of experiments investigates the re-
duction of the OOV rate through paraphrasing,
and its impact on translation. Paraphrasing is per-
formed employing the cosine similarity, and the k-
NN search is done on the source side, with k = 3.
The nearest neighbors are required to satisfy a ra-
dius threshold r &gt; 0.3, i.e., neighbors with a simi-
larity value less or equal to r are rejected. Training
the projection matrices is performed using a small
amount of training data amounting to less than 30k
translation pairs.
To examine the effect of OOV reduction, we
perform paraphrasing on a resource-limited sys-
tem, where a small amount of parallel data ex-
ists, but a larger amount of monolingual data is
available. Such a system is simulated by train-
ing word vectors on the WIT+UN data monolin-
gually , while extracting the phrase table using the
much smaller in-domain WIT data set only. Table
5 shows the change in the number of OOV words
after introducing the paraphrased rules to the WIT-
based phrase table. 19% and 30% of the original
OOVs are eliminated in the dev and eval13 sets,
respectively. This reduction translates to an im-
provement of 0.6% BLEU and 0.7% TER as indi-
cated in table 4.
Since BLEU or TER are based on word iden-
tities and do not detect semantic similarities, we
make a comparison between the reference transla-
tions and translations of the system that employed
</bodyText>
<page confidence="0.999494">
7
</page>
<table confidence="0.99909">
phrase table dev # OOV
eval13
WIT 185 254
WIT+paraph. 150 183
Vocab. size 3,714 4,734
</table>
<tableCaption confidence="0.997735333333333">
Table 5: OOV change due to paraphrasing. Vocab-
ulary refers to the number of unique tokens in the
Arabic dev and test sets.
</tableCaption>
<table confidence="0.999913307692308">
OOV VSM-based Reference
Translation
��� ����� found unfolded
��� interested keen
����
���� jail imprisoned
úæm
�
�&gt;I claim report
��� confusing confounding
�����
���� encourage rallied for
������� villagers redneck
</table>
<tableCaption confidence="0.8019925">
Table 6: Examples of OOV words that were trans-
lated due to paraphrasing. The examples are
</tableCaption>
<bodyText confidence="0.956069625">
extracted from the translation hypotheses of the
small experiment.
OOV reduction. Examples are shown in Table 6.
Although the reference words are not matched ex-
actly, the VSM translations are semantically close
to them, suggesting that OOV reduction in these
cases was somewhat successful, although not re-
warded by either of the scoring measures used.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999982936507937">
Bilingually-constrained phrase embeddings were
developed in (Zhang et al., 2014). Initial embed-
dings were trained in an unsupervised manner, fol-
lowed by fine-tuning using bilingual knowledge to
minimize the semantic distance between transla-
tion equivalents, and maximizing the distance be-
tween non-translation pairs. The embeddings are
learned using recursive neural networks by de-
composing phrases to their constituents. While
our work includes bilingual constraints to learn
phrase vectors, the constraints are implicit in the
phrase corpus. Our approach is simple, focusing
on the preprocessing step of preparing the phrase
corpus, and therefore it can be used with different
existing frameworks that were developed for word
vectors.
Zou et al. (2013) learn bilingual word embed-
dings by designing an objective function that com-
bines unsupervised training with bilingual con-
straints based on word alignments. Similar to
our work, they compute an additional feature for
phrase pairs using cosine similarity. Word vec-
tors are averaged to obtain phrase representations.
In contrast, our approach learns phrase representa-
tions directly.
Recurrent neural networks were used with min-
imum translation units (Hu et al., 2014), which are
phrase pairs undergoing certain constraints. At the
input layer, each of the source and target phrases
are modeled as a bag of words, while the output
phrase is predicted word-by-word assuming con-
ditional independence. The approach seeks to al-
leviate data sparsity problems that would arise if
phrases were to be uniquely distinguished. Our
approach does not break phrases down to words,
but learns phrase embeddings directly.
Chen et al. (2010) represent a rule in the hierar-
chical phrase table using a bag-of-words approach.
Instead, we learn phrase vectors directly without
resorting to their constituent words. Moreover,
they apply a count-based approach and employ
IBM model 1 probabilities to project the target
space to the source space. In contrast, our map-
ping is similar to that of Mikolov et al. (2013a)
and is learned directly from a small set of bilin-
gual data.
Mikolov et al. (2013a) proposed an efficient
method to learn word vectors through feed-
forward neural networks by eliminating the hid-
den layer. They do not report end-to-end sentence
translation results as we do in this work.
Mikolov et al. (2013b) learn direct representa-
tions of phrases after joining a training corpus us-
ing a simple monolingual point-wise mutual in-
formation criterion with discounting. Our work
exploits the rich bilingual knowledge provided by
the phrase table to join the corpus instead.
Gao et al. (2013) learn shared space mappings
using a feed-forward neural network and represent
a phrase vector as a bag-of-words vector. The vec-
tors are learned aiming to optimize an expected
BLEU criterion. Our work is different in that we
learn two separate source and target mappings.
</bodyText>
<page confidence="0.991744">
8
</page>
<bodyText confidence="0.999963476190476">
We also do not follow their bag-of-words phrase
model approach.
Marton et al. (2009) proposed to eliminate
OOVs by looking for similar words using distri-
butional vectors, but they prune the search space
limiting it to candidates observed in the same con-
text as that of the OOV. We do not employ such a
heuristic. Instead, we perform a k-nearest neigh-
bor search spanning the full phrase table to para-
phrase its rules and generate new entries.
Estimating phrase table scores using monolin-
gual data was investigated in (Klementiev et al.,
2012), by building co-occurrence context vectors
and using a small dictionary to induce new scores
for existing phrase rules. Our work explores the
use of distributional vectors extracted from neu-
ral networks, moreover, we induce new phrase
rules to extend the phrase table. New phrase rules
were also generated in (Irvine and Callison-Burch,
2014), where new phrases were produced as a
composition of unigram translations.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999964571428571">
In this work we adapted vector space models to
provide the state-of-the-art phrase-based statisti-
cal machine translation system with semantic in-
formation. We leveraged the bilingual knowledge
of the phrase table to construct source and target
phrase corpora to learn phrase vectors, which were
used to provide semantic scoring of phrase pairs.
Word vectors allowed to extend the phrase table
and eliminate OOVs. Both methods proved bene-
ficial for low-resource tasks.
Future work would investigate decoder inte-
gration of semantic scoring that extends beyond
phrase boundaries to provide semantically coher-
ent translations.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999037">
This material is partially based upon work sup-
ported by the DARPA BOLT project under Con-
tract No. HR0011- 12-C-0015. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA.
The research leading to these results has also re-
ceived funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement no 287658.
</bodyText>
<sectionHeader confidence="0.995982" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999292666666666">
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137–1155.
Boxing Chen, George Foster, and Roland Kuhn. 2010.
Bilingual sense similarity for statistical machine
translation. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 834–843.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis test-
ing for statistical machine translation: Controlling
for optimizer instability. In 49th Annual Meet-
ing of the Association for Computational Linguis-
tics:shortpapers, pages 176–181, Portland, Oregon,
June.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and Robust Neural Network Joint Models for
Statistical Machine Translation. In 52nd Annual
Meeting of the Association for Computational Lin-
guistics, Baltimore, MD, USA, June.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reorder-
ing model. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’08, pages 848–856, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2013. Learning semantic representations
for the phrase translation model. arXiv preprint
arXiv:1312.0482.
Zellig S Harris. 1954. Distributional structure. Word.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum translation modeling with recur-
rent neural networks. In Proceedings of the 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 20–29,
Gothenburg, Sweden, April. Association for Com-
putational Linguistics.
Ann Irvine and Chris Callison-Burch. 2014. Hal-
lucinating phrase translations for low resource mt.
In Proceedings of the Conference on Computational
Natural Language Learning (CoNLL).
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700–1709, Seattle,
Washington, USA, October. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.944524">
9
</page>
<reference confidence="0.999888086021506">
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward statisti-
cal machine translation without parallel corpora. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 130–140. Association for Computa-
tional Linguistics.
Thomas K Landauer and Susan T Dumais. 1997. A
solution to plato’s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
104(2):211.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, &amp; Computers, 28(2):203–208.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to informa-
tion retrieval, volume 1. Cambridge university press
Cambridge.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1-Volume 1, pages 381–390. Association for Com-
putational Linguistics.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013a. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short
Papers), pages 220–224, Uppsala, Sweden, July.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native Training and Maximum Entropy Models for
Statistical Machine Translation. In Proc. of the 40th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 295–302, Philadel-
phia, PA, July.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51,
March.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160–167, Sap-
poro, Japan, July.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech &amp; Language, 21(3):492–
518.
Andreas Stolcke. 2002. SRILM – An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901–904, Denver, CO, September.
Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,
and Hermann Ney. 2014. Translation Modeling
with Bidirectional Recurrent Neural Networks. In
Proceedings of the Conference on Empirical Meth-
ods on Natural Language Processing, October.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141–188.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open source hierarchi-
cal translation, extended with reordering and lexi-
con models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262–270, Uppsala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483–491, Mum-
bai, India, December.
Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and
Chengqing Zong. 2014. Bilingually-constrained
phrase embeddings for machine translation. In Pro-
ceedings of the 52th Annual Meeting on Associa-
tion for Computational Linguistics. Association for
Computational Linguistics.
Will Y Zou, Richard Socher, Daniel M Cer, and
Christopher D Manning. 2013. Bilingual word em-
beddings for phrase-based machine translation. In
EMNLP, pages 1393–1398.
</reference>
<page confidence="0.997794">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.168091">
<title confidence="0.999857">Vector Space Models for Phrase-based Machine Translation</title>
<author confidence="0.939304">Andreas</author>
<author confidence="0.939304">Hermann</author>
<affiliation confidence="0.9465465">Language Technology and Pattern Recognition RWTH Aachen University, Aachen,</affiliation>
<address confidence="0.6674945">Language Processing Univ. Paris-Sud, France and LIMSI/CNRS, Orsay,</address>
<abstract confidence="0.986994789473684">This paper investigates the application of vector space models (VSMs) to the standard phrase-based machine translation pipeline. VSMs are models based on continuous word representations embedded in a vector space. We exploit word vectors to augment the phrase table with new inferred phrase pairs. This helps reduce out-of-vocabulary (OOV) words. In addition, we present a simple way to learn bilingually-constrained phrase vectors. The phrase vectors are then used to provide additional scoring of phrase pairs, which fits into the standard log-linear framework of phrase-based statistical machine translation. Both methods result in significant improvements over a competitive in-domain baseline applied to the</abstract>
<note confidence="0.607608">Arabic-to-English task of IWSLT 2013.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rejean Ducharme</author>
<author>Pascal Vincent</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="4711" citStr="Bengio et al., 2003" startWordPosition="717" endWordPosition="720">to-English task and show significant improvements over a strong in-domain baseline. The rest of the paper is structured as follows. Section 2 presents a background on word and phrase vectors. The construction of the phrase corpus is discussed in Section 3, while Section 4 demonstrates how to use word and phrase vectors in the standard phrase-based SMT pipeline. Experiments are presented in Section 5, followed by an overview of the related word in Section 6, and finally Section 7 concludes the work. 2 Vector Space Models One way to obtain context-based word vectors is through a neural network (Bengio et al., 2003; Schwenk, 2007). With a vocabulary size V, onehot encoding of V-dimensional vectors is used to represent input words, effectively associating each word with a D-dimensional vector in the V × D input weight matrix, where D is the size of the hidden layer. Similarly, one-hot encoding on the output layer associates words with vectors in the output weight matrix. Alternatively, a count-based V-dimensional word co-occurrence vector can serve as a word representation (Lund and Burgess, 1996; Landauer and Dumais, 1997). Such representations are sparse and high-dimensional, which might require an add</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, 2003</marker>
<rawString>Yoshua Bengio, Rejean Ducharme, and Pascal Vincent. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Bilingual sense similarity for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>834--843</pages>
<contexts>
<context position="6086" citStr="Chen et al., 2010" startWordPosition="926" endWordPosition="929">sional, dense vectors. In this work, we follow the neural network approach to extract the feature vectors. Whether word vectors are extracted by means of a neural network or cooccurrence counts, the context surrounding a word influences its final representation by design. Such context-based representations can be used to determine semantic similarities. The construction of phrase representations, on the other hand, can be done in different ways. The compositional approach constructs the vector representation of a phrase by resorting to its constituent words (or sub-phrases) (Gao et al., 2013; Chen et al., 2010). Kalchbrenner and Blunsom (2013) obtain continuous sentence representations by applying a sequence of convolutions, starting with word representations. Another approach for phrase representation considers phrases as atomic units that can not be divided further. The representations are learned directly in this case (Mikolov et al., 2013b; Hu et al., 2014). In this work, we follow the second approach to obtain phrase vectors. To this end, we apply the same methods that yield word vectors, with the difference that phrases are used instead of words. In the case of neural word representations, a n</context>
<context position="29983" citStr="Chen et al. (2010)" startWordPosition="4937" endWordPosition="4940">entations. In contrast, our approach learns phrase representations directly. Recurrent neural networks were used with minimum translation units (Hu et al., 2014), which are phrase pairs undergoing certain constraints. At the input layer, each of the source and target phrases are modeled as a bag of words, while the output phrase is predicted word-by-word assuming conditional independence. The approach seeks to alleviate data sparsity problems that would arise if phrases were to be uniquely distinguished. Our approach does not break phrases down to words, but learns phrase embeddings directly. Chen et al. (2010) represent a rule in the hierarchical phrase table using a bag-of-words approach. Instead, we learn phrase vectors directly without resorting to their constituent words. Moreover, they apply a count-based approach and employ IBM model 1 probabilities to project the target space to the source space. In contrast, our mapping is similar to that of Mikolov et al. (2013a) and is learned directly from a small set of bilingual data. Mikolov et al. (2013a) proposed an efficient method to learn word vectors through feedforward neural networks by eliminating the hidden layer. They do not report end-to-e</context>
</contexts>
<marker>Chen, Foster, Kuhn, 2010</marker>
<rawString>Boxing Chen, George Foster, and Roland Kuhn. 2010. Bilingual sense similarity for statistical machine translation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 834–843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In 49th Annual Meeting of the Association for Computational Linguistics:shortpapers,</booktitle>
<pages>176--181</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="25990" citStr="Clark et al., 2011" startWordPosition="4288" endWordPosition="4291">-gram-based phrase vectors used for the semantic feature, on the other hand, discard ordering information, but uses contextual lexical information for phrase representation. In this sense, HRMs and the semantic feature can be said to complement each other. Using the semantic feature for the large phrase table did not yield improvements. The difference compared to the baseline in this case is not statistically significant. All reported results are averages of 3 MERT optimizer runs. Statistical significance is computed using the Approximate Randomization (AR) test. We used the multeval toolkit (Clark et al., 2011) for evaluation. 5.4 Paraphrasing and OOV Reduction The next set of experiments investigates the reduction of the OOV rate through paraphrasing, and its impact on translation. Paraphrasing is performed employing the cosine similarity, and the kNN search is done on the source side, with k = 3. The nearest neighbors are required to satisfy a radius threshold r &gt; 0.3, i.e., neighbors with a similarity value less or equal to r are rejected. Training the projection matrices is performed using a small amount of training data amounting to less than 30k translation pairs. To examine the effect of OOV </context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176–181, Portland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1458" citStr="Collobert and Weston, 2008" startWordPosition="200" endWordPosition="203">rase pairs, which fits into the standard log-linear framework of phrase-based statistical machine translation. Both methods result in significant improvements over a competitive in-domain baseline applied to the Arabic-to-English task of IWSLT 2013. 1 Introduction Categorical word representation has been widely used in many natural language processing (NLP) applications including statistical machine translation (SMT), where words are treated as discrete random variables. Continuous word representations, on the other hand, have been applied successfully in many NLP areas (Manning et al., 2008; Collobert and Weston, 2008). However, their application to machine translation is still an open research question. Several works tried to address the question recently (Mikolov et al., 2013b; Zhang et al., 2014; Zou et al., 2013), and this work is but another step in that direction. While categorical representations do not encode any information about word identities, continuous representations embed words in a vector space, resulting in geometric arrangements that reflect information about the represented words. Such embeddings open the potential for applying information retrieval approaches where it becomes possible t</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and Robust Neural Network Joint Models for Statistical Machine Translation.</title>
<date>2014</date>
<booktitle>In 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="2695" citStr="Devlin et al., 2014" startWordPosition="392" endWordPosition="395">te similarity between different words. We focus on continuous representations whose training is influenced by the surrounding context of the token being represented. One motivation for such representations is to capture word semantics (Turney et al., 2010). This is based on the distributional hypothesis (Harris, 1954) which says that words that occur in similar contexts tend to have similar meanings. We make use of continuous vectors learned using simple neural networks. Neural networks have been gaining increasing attention recently, where they have been able to enhance strong SMT baselines (Devlin et al., 2014; Sundermeyer et al., 2014). While neural language and translation modeling make intermediate use of continuous representations, there have been also attempts at explicit learning of continuous representations to improve translation (Zhang et al., 2014; Gao et al., 2013). This work explores the potential of word semantics based on continuous vector representations to enhance the performance of phrase-based machine translation. We present a greedy algorithm that employs the phrase table to identify phrases in a training corpus. The phrase table serves to bilingually restrict the phrases spotted</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and Robust Neural Network Joint Models for Statistical Machine Translation. In 52nd Annual Meeting of the Association for Computational Linguistics, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>848--856</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17882" citStr="Galley and Manning, 2008" startWordPosition="2935" endWordPosition="2938">atures being negative log-probabilities, and the scoring threshold θ was set to 10. All translation experiments are performed with the Jane toolkit (Vilar et al., 2010; Wuebker et al., 2012). 1https://code.google.com/p/word2vec/ 5.1 Baseline System Our phrase-based baseline system consists of two phrasal and two lexical translation models, trained using a word-aligned bilingual training corpus. Word alignment is automatically generated by GIZA++ (Och and Ney, 2003) given a sentencealigned bilingual corpus. We also include binary count features and bidirectional hierarchical reordering models (Galley and Manning, 2008), with three orientation classes per direction resulting in six reordering models. The baseline also includes word penalty, phrase penalty and a simple distance-based distortion model. The language model (LM) is a 4-gram mixture LM trained on several data sets using modified Kneser-Ney discounting with interpolation, and combined with weights tuned to achieve the lowest perplexity on a development set using the SRILM toolkit (Stolcke, 2002). Data selection is performed using cross-entropy filtering (Moore and Lewis, 2010). 5.2 Word Vectors Here we analyze the quality of word vectors used in th</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 848–856, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Wen-tau Yih</author>
<author>Li Deng</author>
</authors>
<title>Learning semantic representations for the phrase translation model. arXiv preprint arXiv:1312.0482.</title>
<date>2013</date>
<contexts>
<context position="2966" citStr="Gao et al., 2013" startWordPosition="432" endWordPosition="435">n the distributional hypothesis (Harris, 1954) which says that words that occur in similar contexts tend to have similar meanings. We make use of continuous vectors learned using simple neural networks. Neural networks have been gaining increasing attention recently, where they have been able to enhance strong SMT baselines (Devlin et al., 2014; Sundermeyer et al., 2014). While neural language and translation modeling make intermediate use of continuous representations, there have been also attempts at explicit learning of continuous representations to improve translation (Zhang et al., 2014; Gao et al., 2013). This work explores the potential of word semantics based on continuous vector representations to enhance the performance of phrase-based machine translation. We present a greedy algorithm that employs the phrase table to identify phrases in a training corpus. The phrase table serves to bilingually restrict the phrases spotted in the monolingual corpus. The algorithm is applied separately to the source and target sides of the training data, resulting in source and target corpora of phrases (instead of words). The phrase corpus is used to learn phrase vectors using the same methods that produc</context>
<context position="6066" citStr="Gao et al., 2013" startWordPosition="922" endWordPosition="925">latively low-dimensional, dense vectors. In this work, we follow the neural network approach to extract the feature vectors. Whether word vectors are extracted by means of a neural network or cooccurrence counts, the context surrounding a word influences its final representation by design. Such context-based representations can be used to determine semantic similarities. The construction of phrase representations, on the other hand, can be done in different ways. The compositional approach constructs the vector representation of a phrase by resorting to its constituent words (or sub-phrases) (Gao et al., 2013; Chen et al., 2010). Kalchbrenner and Blunsom (2013) obtain continuous sentence representations by applying a sequence of convolutions, starting with word representations. Another approach for phrase representation considers phrases as atomic units that can not be divided further. The representations are learned directly in this case (Mikolov et al., 2013b; Hu et al., 2014). In this work, we follow the second approach to obtain phrase vectors. To this end, we apply the same methods that yield word vectors, with the difference that phrases are used instead of words. In the case of neural word </context>
<context position="30939" citStr="Gao et al. (2013)" startWordPosition="5094" endWordPosition="5097">that of Mikolov et al. (2013a) and is learned directly from a small set of bilingual data. Mikolov et al. (2013a) proposed an efficient method to learn word vectors through feedforward neural networks by eliminating the hidden layer. They do not report end-to-end sentence translation results as we do in this work. Mikolov et al. (2013b) learn direct representations of phrases after joining a training corpus using a simple monolingual point-wise mutual information criterion with discounting. Our work exploits the rich bilingual knowledge provided by the phrase table to join the corpus instead. Gao et al. (2013) learn shared space mappings using a feed-forward neural network and represent a phrase vector as a bag-of-words vector. The vectors are learned aiming to optimize an expected BLEU criterion. Our work is different in that we learn two separate source and target mappings. 8 We also do not follow their bag-of-words phrase model approach. Marton et al. (2009) proposed to eliminate OOVs by looking for similar words using distributional vectors, but they prune the search space limiting it to candidates observed in the same context as that of the OOV. We do not employ such a heuristic. Instead, we p</context>
</contexts>
<marker>Gao, He, Yih, Deng, 2013</marker>
<rawString>Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. 2013. Learning semantic representations for the phrase translation model. arXiv preprint arXiv:1312.0482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<date>1954</date>
<note>Distributional structure. Word.</note>
<contexts>
<context position="2395" citStr="Harris, 1954" startWordPosition="347" endWordPosition="348">ut word identities, continuous representations embed words in a vector space, resulting in geometric arrangements that reflect information about the represented words. Such embeddings open the potential for applying information retrieval approaches where it becomes possible to define and compute similarity between different words. We focus on continuous representations whose training is influenced by the surrounding context of the token being represented. One motivation for such representations is to capture word semantics (Turney et al., 2010). This is based on the distributional hypothesis (Harris, 1954) which says that words that occur in similar contexts tend to have similar meanings. We make use of continuous vectors learned using simple neural networks. Neural networks have been gaining increasing attention recently, where they have been able to enhance strong SMT baselines (Devlin et al., 2014; Sundermeyer et al., 2014). While neural language and translation modeling make intermediate use of continuous representations, there have been also attempts at explicit learning of continuous representations to improve translation (Zhang et al., 2014; Gao et al., 2013). This work explores the pote</context>
<context position="13107" citStr="Harris, 1954" startWordPosition="2149" endWordPosition="2150">nal source and T-dimensional target vectors respectively corresponding to the source phrase and target phrase e&amp;quot;. W is an S x T linear projection matrix that maps the source space to the target space (Mikolov et al., 2013a). The matrix is estimated by optimizing the foll hM+1(f&amp;quot; , &amp;quot;e) = sim(Wxf&amp;quot; ,z&amp;quot;e) xf&amp;quot; z&amp;quot;e f&amp;quot; owing criterion with stochastic gradient descent: ||Wxi−zi||2 (4) {(x1,z1),...,(xN,zN)} d target vectors. corresponding to the source an 4.2 Semantic Phrase Feature Words that occur in similar contexts tend to have similar meanings. This idea is known as the distributional hypothesis (Harris, 1954), and it motivates the use of word context to learn word representations that capture word semantics (Turney et al., 2010). Extending this notion to phrases, phrase vectors that are learned based on the surrounding context encode phrase semantics. Since we will use phrase vectors to compute a feature of a phrase pair in the following, we refer to the feature as a semantic phrase feature. min W N ∑ i=1 where the training data consists of the pairs While the standard phrase table is extracted using parallel training data, we propose to extend it and infer new entries relying on continuous repres</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S Harris. 1954. Distributional structure. Word.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuening Hu</author>
<author>Michael Auli</author>
<author>Qin Gao</author>
<author>Jianfeng Gao</author>
</authors>
<title>Minimum translation modeling with recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="6443" citStr="Hu et al., 2014" startWordPosition="976" endWordPosition="979"> construction of phrase representations, on the other hand, can be done in different ways. The compositional approach constructs the vector representation of a phrase by resorting to its constituent words (or sub-phrases) (Gao et al., 2013; Chen et al., 2010). Kalchbrenner and Blunsom (2013) obtain continuous sentence representations by applying a sequence of convolutions, starting with word representations. Another approach for phrase representation considers phrases as atomic units that can not be divided further. The representations are learned directly in this case (Mikolov et al., 2013b; Hu et al., 2014). In this work, we follow the second approach to obtain phrase vectors. To this end, we apply the same methods that yield word vectors, with the difference that phrases are used instead of words. In the case of neural word representations, a neural network that is presented with words at the input layer is presented with phrases instead. The resulting vocabulary size in this case would be the number of distinct phrases observed during training. Although learning phrase embeddings directly is amenable to data sparsity issues, it provides us with a simple means to build phrase vectors making use</context>
<context position="29526" citStr="Hu et al., 2014" startWordPosition="4864" endWordPosition="4867">of preparing the phrase corpus, and therefore it can be used with different existing frameworks that were developed for word vectors. Zou et al. (2013) learn bilingual word embeddings by designing an objective function that combines unsupervised training with bilingual constraints based on word alignments. Similar to our work, they compute an additional feature for phrase pairs using cosine similarity. Word vectors are averaged to obtain phrase representations. In contrast, our approach learns phrase representations directly. Recurrent neural networks were used with minimum translation units (Hu et al., 2014), which are phrase pairs undergoing certain constraints. At the input layer, each of the source and target phrases are modeled as a bag of words, while the output phrase is predicted word-by-word assuming conditional independence. The approach seeks to alleviate data sparsity problems that would arise if phrases were to be uniquely distinguished. Our approach does not break phrases down to words, but learns phrase embeddings directly. Chen et al. (2010) represent a rule in the hierarchical phrase table using a bag-of-words approach. Instead, we learn phrase vectors directly without resorting t</context>
</contexts>
<marker>Hu, Auli, Gao, Gao, 2014</marker>
<rawString>Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao. 2014. Minimum translation modeling with recurrent neural networks. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 20–29, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Hallucinating phrase translations for low resource mt.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="32097" citStr="Irvine and Callison-Burch, 2014" startWordPosition="5284" endWordPosition="5287">ntext as that of the OOV. We do not employ such a heuristic. Instead, we perform a k-nearest neighbor search spanning the full phrase table to paraphrase its rules and generate new entries. Estimating phrase table scores using monolingual data was investigated in (Klementiev et al., 2012), by building co-occurrence context vectors and using a small dictionary to induce new scores for existing phrase rules. Our work explores the use of distributional vectors extracted from neural networks, moreover, we induce new phrase rules to extend the phrase table. New phrase rules were also generated in (Irvine and Callison-Burch, 2014), where new phrases were produced as a composition of unigram translations. 7 Conclusion In this work we adapted vector space models to provide the state-of-the-art phrase-based statistical machine translation system with semantic information. We leveraged the bilingual knowledge of the phrase table to construct source and target phrase corpora to learn phrase vectors, which were used to provide semantic scoring of phrase pairs. Word vectors allowed to extend the phrase table and eliminate OOVs. Both methods proved beneficial for low-resource tasks. Future work would investigate decoder integr</context>
</contexts>
<marker>Irvine, Callison-Burch, 2014</marker>
<rawString>Ann Irvine and Chris Callison-Burch. 2014. Hallucinating phrase translations for low resource mt. In Proceedings of the Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1700--1709</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="6119" citStr="Kalchbrenner and Blunsom (2013)" startWordPosition="930" endWordPosition="933">s. In this work, we follow the neural network approach to extract the feature vectors. Whether word vectors are extracted by means of a neural network or cooccurrence counts, the context surrounding a word influences its final representation by design. Such context-based representations can be used to determine semantic similarities. The construction of phrase representations, on the other hand, can be done in different ways. The compositional approach constructs the vector representation of a phrase by resorting to its constituent words (or sub-phrases) (Gao et al., 2013; Chen et al., 2010). Kalchbrenner and Blunsom (2013) obtain continuous sentence representations by applying a sequence of convolutions, starting with word representations. Another approach for phrase representation considers phrases as atomic units that can not be divided further. The representations are learned directly in this case (Mikolov et al., 2013b; Hu et al., 2014). In this work, we follow the second approach to obtain phrase vectors. To this end, we apply the same methods that yield word vectors, with the difference that phrases are used instead of words. In the case of neural word representations, a neural network that is presented w</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Ann Irvine</author>
<author>Chris CallisonBurch</author>
<author>David Yarowsky</author>
</authors>
<title>Toward statistical machine translation without parallel corpora.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>130--140</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31754" citStr="Klementiev et al., 2012" startWordPosition="5231" endWordPosition="5234"> Our work is different in that we learn two separate source and target mappings. 8 We also do not follow their bag-of-words phrase model approach. Marton et al. (2009) proposed to eliminate OOVs by looking for similar words using distributional vectors, but they prune the search space limiting it to candidates observed in the same context as that of the OOV. We do not employ such a heuristic. Instead, we perform a k-nearest neighbor search spanning the full phrase table to paraphrase its rules and generate new entries. Estimating phrase table scores using monolingual data was investigated in (Klementiev et al., 2012), by building co-occurrence context vectors and using a small dictionary to induce new scores for existing phrase rules. Our work explores the use of distributional vectors extracted from neural networks, moreover, we induce new phrase rules to extend the phrase table. New phrase rules were also generated in (Irvine and Callison-Burch, 2014), where new phrases were produced as a composition of unigram translations. 7 Conclusion In this work we adapted vector space models to provide the state-of-the-art phrase-based statistical machine translation system with semantic information. We leveraged </context>
</contexts>
<marker>Klementiev, Irvine, CallisonBurch, Yarowsky, 2012</marker>
<rawString>Alexandre Klementiev, Ann Irvine, Chris CallisonBurch, and David Yarowsky. 2012. Toward statistical machine translation without parallel corpora. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 130–140. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="5229" citStr="Landauer and Dumais, 1997" startWordPosition="798" endWordPosition="802">r Space Models One way to obtain context-based word vectors is through a neural network (Bengio et al., 2003; Schwenk, 2007). With a vocabulary size V, onehot encoding of V-dimensional vectors is used to represent input words, effectively associating each word with a D-dimensional vector in the V × D input weight matrix, where D is the size of the hidden layer. Similarly, one-hot encoding on the output layer associates words with vectors in the output weight matrix. Alternatively, a count-based V-dimensional word co-occurrence vector can serve as a word representation (Lund and Burgess, 1996; Landauer and Dumais, 1997). Such representations are sparse and high-dimensional, which might require an additional dimensionality reduction step (e.g. using SVD). In contrast, learning word representations via neural models results directly in relatively low-dimensional, dense vectors. In this work, we follow the neural network approach to extract the feature vectors. Whether word vectors are extracted by means of a neural network or cooccurrence counts, the context surrounding a word influences its final representation by design. Such context-based representations can be used to determine semantic similarities. The c</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K Landauer and Susan T Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, 104(2):211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="5201" citStr="Lund and Burgess, 1996" startWordPosition="794" endWordPosition="797">cludes the work. 2 Vector Space Models One way to obtain context-based word vectors is through a neural network (Bengio et al., 2003; Schwenk, 2007). With a vocabulary size V, onehot encoding of V-dimensional vectors is used to represent input words, effectively associating each word with a D-dimensional vector in the V × D input weight matrix, where D is the size of the hidden layer. Similarly, one-hot encoding on the output layer associates words with vectors in the output weight matrix. Alternatively, a count-based V-dimensional word co-occurrence vector can serve as a word representation (Lund and Burgess, 1996; Landauer and Dumais, 1997). Such representations are sparse and high-dimensional, which might require an additional dimensionality reduction step (e.g. using SVD). In contrast, learning word representations via neural models results directly in relatively low-dimensional, dense vectors. In this work, we follow the neural network approach to extract the feature vectors. Whether word vectors are extracted by means of a neural network or cooccurrence counts, the context surrounding a word influences its final representation by design. Such context-based representations can be used to determine </context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, &amp; Computers, 28(2):203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to information retrieval, volume 1. Cambridge university press Cambridge.</title>
<date>2008</date>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to information retrieval, volume 1. Cambridge university press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Chris Callison-Burch</author>
<author>Philip Resnik</author>
</authors>
<title>Improved statistical machine translation using monolingually-derived paraphrases.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>381--390</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31297" citStr="Marton et al. (2009)" startWordPosition="5153" endWordPosition="5156">resentations of phrases after joining a training corpus using a simple monolingual point-wise mutual information criterion with discounting. Our work exploits the rich bilingual knowledge provided by the phrase table to join the corpus instead. Gao et al. (2013) learn shared space mappings using a feed-forward neural network and represent a phrase vector as a bag-of-words vector. The vectors are learned aiming to optimize an expected BLEU criterion. Our work is different in that we learn two separate source and target mappings. 8 We also do not follow their bag-of-words phrase model approach. Marton et al. (2009) proposed to eliminate OOVs by looking for similar words using distributional vectors, but they prune the search space limiting it to candidates observed in the same context as that of the OOV. We do not employ such a heuristic. Instead, we perform a k-nearest neighbor search spanning the full phrase table to paraphrase its rules and generate new entries. Estimating phrase table scores using monolingual data was investigated in (Klementiev et al., 2012), by building co-occurrence context vectors and using a small dictionary to induce new scores for existing phrase rules. Our work explores the </context>
</contexts>
<marker>Marton, Callison-Burch, Resnik, 2009</marker>
<rawString>Yuval Marton, Chris Callison-Burch, and Philip Resnik. 2009. Improved statistical machine translation using monolingually-derived paraphrases. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 381–390. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</title>
<date>2013</date>
<contexts>
<context position="1620" citStr="Mikolov et al., 2013" startWordPosition="225" endWordPosition="228">petitive in-domain baseline applied to the Arabic-to-English task of IWSLT 2013. 1 Introduction Categorical word representation has been widely used in many natural language processing (NLP) applications including statistical machine translation (SMT), where words are treated as discrete random variables. Continuous word representations, on the other hand, have been applied successfully in many NLP areas (Manning et al., 2008; Collobert and Weston, 2008). However, their application to machine translation is still an open research question. Several works tried to address the question recently (Mikolov et al., 2013b; Zhang et al., 2014; Zou et al., 2013), and this work is but another step in that direction. While categorical representations do not encode any information about word identities, continuous representations embed words in a vector space, resulting in geometric arrangements that reflect information about the represented words. Such embeddings open the potential for applying information retrieval approaches where it becomes possible to define and compute similarity between different words. We focus on continuous representations whose training is influenced by the surrounding context of the tok</context>
<context position="6424" citStr="Mikolov et al., 2013" startWordPosition="972" endWordPosition="975">antic similarities. The construction of phrase representations, on the other hand, can be done in different ways. The compositional approach constructs the vector representation of a phrase by resorting to its constituent words (or sub-phrases) (Gao et al., 2013; Chen et al., 2010). Kalchbrenner and Blunsom (2013) obtain continuous sentence representations by applying a sequence of convolutions, starting with word representations. Another approach for phrase representation considers phrases as atomic units that can not be divided further. The representations are learned directly in this case (Mikolov et al., 2013b; Hu et al., 2014). In this work, we follow the second approach to obtain phrase vectors. To this end, we apply the same methods that yield word vectors, with the difference that phrases are used instead of words. In the case of neural word representations, a neural network that is presented with words at the input layer is presented with phrases instead. The resulting vocabulary size in this case would be the number of distinct phrases observed during training. Although learning phrase embeddings directly is amenable to data sparsity issues, it provides us with a simple means to build phrase</context>
<context position="7706" citStr="Mikolov et al. (2013" startWordPosition="1195" endWordPosition="1198">s, focussing the effort on preprocessing the data as will be discussed in the next section. 3 Phrase Corpus When training word vectors using neural networks, the network is presented with a corpus. To build phrase vectors, we first identify phrases in the corpus and generate a phrase corpus. The phrase corpus is similar to the original corpus except that its words are joined to make up phrases. The new corpus is then used to train the neural network. The columns of the resulting input weight matrix of the network are the phrase vectors corresponding to the phrases encountered during training. Mikolov et al. (2013b) identify phrases using a monolingual point-wise mutual information criterion with discounting. Since our end goal is to generate phrase vectors that are helpful for translation, we follow a different approach: we constrain the phrases by the conventional phrase table of phrase-based machine translation. This is done by limiting the phrases identified in the corpus to high quality phrases occurring in the phrase table. The quality is determined using bilingual scores of phrase pairs. While the phrase vectors of a language are eventually obtained by training the neural network on the monoling</context>
<context position="12715" citStr="Mikolov et al., 2013" startWordPosition="2087" endWordPosition="2090"> vectors are paired using IBM 1 and p(f lexica. Noisy entries are assumed to have a probability less than a certain threshold an p(e|f) |e) d are not used to pair word vectors. 4.3 Paraphrasing Given a phrase pair e&amp;quot;), we can use the phrase vectors of the source and target phrases to compute a seman (f&amp;quot;, tic phrase feature as follows: (3) where sim is a similarity function, and are the S-dimensional source and T-dimensional target vectors respectively corresponding to the source phrase and target phrase e&amp;quot;. W is an S x T linear projection matrix that maps the source space to the target space (Mikolov et al., 2013a). The matrix is estimated by optimizing the foll hM+1(f&amp;quot; , &amp;quot;e) = sim(Wxf&amp;quot; ,z&amp;quot;e) xf&amp;quot; z&amp;quot;e f&amp;quot; owing criterion with stochastic gradient descent: ||Wxi−zi||2 (4) {(x1,z1),...,(xN,zN)} d target vectors. corresponding to the source an 4.2 Semantic Phrase Feature Words that occur in similar contexts tend to have similar meanings. This idea is known as the distributional hypothesis (Harris, 1954), and it motivates the use of word context to learn word representations that capture word semantics (Turney et al., 2010). Extending this notion to phrases, phrase vectors that are learned based on the surro</context>
<context position="30350" citStr="Mikolov et al. (2013" startWordPosition="4997" endWordPosition="5000"> conditional independence. The approach seeks to alleviate data sparsity problems that would arise if phrases were to be uniquely distinguished. Our approach does not break phrases down to words, but learns phrase embeddings directly. Chen et al. (2010) represent a rule in the hierarchical phrase table using a bag-of-words approach. Instead, we learn phrase vectors directly without resorting to their constituent words. Moreover, they apply a count-based approach and employ IBM model 1 probabilities to project the target space to the source space. In contrast, our mapping is similar to that of Mikolov et al. (2013a) and is learned directly from a small set of bilingual data. Mikolov et al. (2013a) proposed an efficient method to learn word vectors through feedforward neural networks by eliminating the hidden layer. They do not report end-to-end sentence translation results as we do in this work. Mikolov et al. (2013b) learn direct representations of phrases after joining a training corpus using a simple monolingual point-wise mutual information criterion with discounting. Our work exploits the rich bilingual knowledge provided by the phrase table to join the corpus instead. Gao et al. (2013) learn shar</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="1620" citStr="Mikolov et al., 2013" startWordPosition="225" endWordPosition="228">petitive in-domain baseline applied to the Arabic-to-English task of IWSLT 2013. 1 Introduction Categorical word representation has been widely used in many natural language processing (NLP) applications including statistical machine translation (SMT), where words are treated as discrete random variables. Continuous word representations, on the other hand, have been applied successfully in many NLP areas (Manning et al., 2008; Collobert and Weston, 2008). However, their application to machine translation is still an open research question. Several works tried to address the question recently (Mikolov et al., 2013b; Zhang et al., 2014; Zou et al., 2013), and this work is but another step in that direction. While categorical representations do not encode any information about word identities, continuous representations embed words in a vector space, resulting in geometric arrangements that reflect information about the represented words. Such embeddings open the potential for applying information retrieval approaches where it becomes possible to define and compute similarity between different words. We focus on continuous representations whose training is influenced by the surrounding context of the tok</context>
<context position="6424" citStr="Mikolov et al., 2013" startWordPosition="972" endWordPosition="975">antic similarities. The construction of phrase representations, on the other hand, can be done in different ways. The compositional approach constructs the vector representation of a phrase by resorting to its constituent words (or sub-phrases) (Gao et al., 2013; Chen et al., 2010). Kalchbrenner and Blunsom (2013) obtain continuous sentence representations by applying a sequence of convolutions, starting with word representations. Another approach for phrase representation considers phrases as atomic units that can not be divided further. The representations are learned directly in this case (Mikolov et al., 2013b; Hu et al., 2014). In this work, we follow the second approach to obtain phrase vectors. To this end, we apply the same methods that yield word vectors, with the difference that phrases are used instead of words. In the case of neural word representations, a neural network that is presented with words at the input layer is presented with phrases instead. The resulting vocabulary size in this case would be the number of distinct phrases observed during training. Although learning phrase embeddings directly is amenable to data sparsity issues, it provides us with a simple means to build phrase</context>
<context position="7706" citStr="Mikolov et al. (2013" startWordPosition="1195" endWordPosition="1198">s, focussing the effort on preprocessing the data as will be discussed in the next section. 3 Phrase Corpus When training word vectors using neural networks, the network is presented with a corpus. To build phrase vectors, we first identify phrases in the corpus and generate a phrase corpus. The phrase corpus is similar to the original corpus except that its words are joined to make up phrases. The new corpus is then used to train the neural network. The columns of the resulting input weight matrix of the network are the phrase vectors corresponding to the phrases encountered during training. Mikolov et al. (2013b) identify phrases using a monolingual point-wise mutual information criterion with discounting. Since our end goal is to generate phrase vectors that are helpful for translation, we follow a different approach: we constrain the phrases by the conventional phrase table of phrase-based machine translation. This is done by limiting the phrases identified in the corpus to high quality phrases occurring in the phrase table. The quality is determined using bilingual scores of phrase pairs. While the phrase vectors of a language are eventually obtained by training the neural network on the monoling</context>
<context position="12715" citStr="Mikolov et al., 2013" startWordPosition="2087" endWordPosition="2090"> vectors are paired using IBM 1 and p(f lexica. Noisy entries are assumed to have a probability less than a certain threshold an p(e|f) |e) d are not used to pair word vectors. 4.3 Paraphrasing Given a phrase pair e&amp;quot;), we can use the phrase vectors of the source and target phrases to compute a seman (f&amp;quot;, tic phrase feature as follows: (3) where sim is a similarity function, and are the S-dimensional source and T-dimensional target vectors respectively corresponding to the source phrase and target phrase e&amp;quot;. W is an S x T linear projection matrix that maps the source space to the target space (Mikolov et al., 2013a). The matrix is estimated by optimizing the foll hM+1(f&amp;quot; , &amp;quot;e) = sim(Wxf&amp;quot; ,z&amp;quot;e) xf&amp;quot; z&amp;quot;e f&amp;quot; owing criterion with stochastic gradient descent: ||Wxi−zi||2 (4) {(x1,z1),...,(xN,zN)} d target vectors. corresponding to the source an 4.2 Semantic Phrase Feature Words that occur in similar contexts tend to have similar meanings. This idea is known as the distributional hypothesis (Harris, 1954), and it motivates the use of word context to learn word representations that capture word semantics (Turney et al., 2010). Extending this notion to phrases, phrase vectors that are learned based on the surro</context>
<context position="30350" citStr="Mikolov et al. (2013" startWordPosition="4997" endWordPosition="5000"> conditional independence. The approach seeks to alleviate data sparsity problems that would arise if phrases were to be uniquely distinguished. Our approach does not break phrases down to words, but learns phrase embeddings directly. Chen et al. (2010) represent a rule in the hierarchical phrase table using a bag-of-words approach. Instead, we learn phrase vectors directly without resorting to their constituent words. Moreover, they apply a count-based approach and employ IBM model 1 probabilities to project the target space to the source space. In contrast, our mapping is similar to that of Mikolov et al. (2013a) and is learned directly from a small set of bilingual data. Mikolov et al. (2013a) proposed an efficient method to learn word vectors through feedforward neural networks by eliminating the hidden layer. They do not report end-to-end sentence translation results as we do in this work. Mikolov et al. (2013b) learn direct representations of phrases after joining a training corpus using a simple monolingual point-wise mutual information criterion with discounting. Our work exploits the rich bilingual knowledge provided by the phrase table to join the corpus instead. Gao et al. (2013) learn shar</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>W Lewis</author>
</authors>
<title>Intelligent Selection of Language Model Training Data.</title>
<date>2010</date>
<booktitle>In ACL (Short Papers),</booktitle>
<pages>220--224</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="18409" citStr="Moore and Lewis, 2010" startWordPosition="3015" endWordPosition="3018">nary count features and bidirectional hierarchical reordering models (Galley and Manning, 2008), with three orientation classes per direction resulting in six reordering models. The baseline also includes word penalty, phrase penalty and a simple distance-based distortion model. The language model (LM) is a 4-gram mixture LM trained on several data sets using modified Kneser-Ney discounting with interpolation, and combined with weights tuned to achieve the lowest perplexity on a development set using the SRILM toolkit (Stolcke, 2002). Data selection is performed using cross-entropy filtering (Moore and Lewis, 2010). 5.2 Word Vectors Here we analyze the quality of word vectors used in the OOV reduction experiments. The vectors are trained using an unaltered word corpus. We build a lexicon using source and target word vectors together with the projection matrix using the similarity score sim(Wxf,ze)), where the projection matrix W is used to project the source word vector xf, corresponding to the source word f, to the target vector space. The similarity between the projection result Wxf and the target word vector ze is computed. In the following we will refer to these scores computed using vector represen</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>R.C. Moore and W. Lewis. 2010. Intelligent Selection of Language Model Training Data. In ACL (Short Papers), pages 220–224, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="11266" citStr="Och and Ney, 2002" startWordPosition="1829" endWordPosition="1832">phrases e&amp;quot; of the other language. The score is the weighted sum of the phrase pair features. Throughout our experiments, we use 2 phrasal and 2 lexical features for scoring, with manual tuning of the weights wi gi( f&amp;quot;, (f&amp;quot;, . The resulting corpus is then used to train phrase vectors following the same procedure of training word vectors. 4 End-to-end Translation In this section we will show how to employ phrase vectors in the phrase-based statistical machine slation pipeline. tran 3 4.1 Phrase-based Machine Translation The phrase-based decoder consists of a search using a log-linear framework (Och and Ney, 2002) as follows: M max max∑λmhm (eI1, sK1, f1J) (2) I,� K,� m=1 i = arg where eI1 = is the target sentence, = f1... is the source sentence, = is the hidden alignment or derivation. The models are weighted by the weights e1...eI f1J fJ sK1 s1...sK hm(eI1,sK1,f1J) λm which are tuned using minimum error rate training (MERT) (Och, 2003). The rest of the section presents two ways to integrate vector representations into the system described above. learned separately, we do not have an immediate mapping between them. As such mapping is needed for the training of the projection matrix, we resort to the p</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 295–302, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="17726" citStr="Och and Ney, 2003" startWordPosition="2913" endWordPosition="2916">s computed according to Eq. 1 using 2 phrasal and 2 lexical features. The phrasal and lexical weights were set to 1 and 0.5 respectively, with all features being negative log-probabilities, and the scoring threshold θ was set to 10. All translation experiments are performed with the Jane toolkit (Vilar et al., 2010; Wuebker et al., 2012). 1https://code.google.com/p/word2vec/ 5.1 Baseline System Our phrase-based baseline system consists of two phrasal and two lexical translation models, trained using a word-aligned bilingual training corpus. Word alignment is automatically generated by GIZA++ (Och and Ney, 2003) given a sentencealigned bilingual corpus. We also include binary count features and bidirectional hierarchical reordering models (Galley and Manning, 2008), with three orientation classes per direction resulting in six reordering models. The baseline also includes word penalty, phrase penalty and a simple distance-based distortion model. The language model (LM) is a 4-gram mixture LM trained on several data sets using modified Kneser-Ney discounting with interpolation, and combined with weights tuned to achieve the lowest perplexity on a development set using the SRILM toolkit (Stolcke, 2002)</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="11596" citStr="Och, 2003" startWordPosition="1893" endWordPosition="1894"> 4 End-to-end Translation In this section we will show how to employ phrase vectors in the phrase-based statistical machine slation pipeline. tran 3 4.1 Phrase-based Machine Translation The phrase-based decoder consists of a search using a log-linear framework (Och and Ney, 2002) as follows: M max max∑λmhm (eI1, sK1, f1J) (2) I,� K,� m=1 i = arg where eI1 = is the target sentence, = f1... is the source sentence, = is the hidden alignment or derivation. The models are weighted by the weights e1...eI f1J fJ sK1 s1...sK hm(eI1,sK1,f1J) λm which are tuned using minimum error rate training (MERT) (Och, 2003). The rest of the section presents two ways to integrate vector representations into the system described above. learned separately, we do not have an immediate mapping between them. As such mapping is needed for the training of the projection matrix, we resort to the phrase table to obtain it. A source and a target phrase vectors are paired if there is a corresponding phrase pair entry in the phrase table whose score exceeds a certain threshold. Scoring is computed using Eq. 1. Similarly, word vectors are paired using IBM 1 and p(f lexica. Noisy entries are assumed to have a probability less </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>21</volume>
<issue>3</issue>
<pages>518</pages>
<contexts>
<context position="4727" citStr="Schwenk, 2007" startWordPosition="721" endWordPosition="722">how significant improvements over a strong in-domain baseline. The rest of the paper is structured as follows. Section 2 presents a background on word and phrase vectors. The construction of the phrase corpus is discussed in Section 3, while Section 4 demonstrates how to use word and phrase vectors in the standard phrase-based SMT pipeline. Experiments are presented in Section 5, followed by an overview of the related word in Section 6, and finally Section 7 concludes the work. 2 Vector Space Models One way to obtain context-based word vectors is through a neural network (Bengio et al., 2003; Schwenk, 2007). With a vocabulary size V, onehot encoding of V-dimensional vectors is used to represent input words, effectively associating each word with a D-dimensional vector in the V × D input weight matrix, where D is the size of the hidden layer. Similarly, one-hot encoding on the output layer associates words with vectors in the output weight matrix. Alternatively, a count-based V-dimensional word co-occurrence vector can serve as a word representation (Lund and Burgess, 1996; Landauer and Dumais, 1997). Such representations are sparse and high-dimensional, which might require an additional dimensio</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Holger Schwenk. 2007. Continuous space language models. Computer Speech &amp; Language, 21(3):492– 518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of the Int. Conf. on Speech and Language Processing (ICSLP),</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, CO,</location>
<contexts>
<context position="18326" citStr="Stolcke, 2002" startWordPosition="3006" endWordPosition="3007">and Ney, 2003) given a sentencealigned bilingual corpus. We also include binary count features and bidirectional hierarchical reordering models (Galley and Manning, 2008), with three orientation classes per direction resulting in six reordering models. The baseline also includes word penalty, phrase penalty and a simple distance-based distortion model. The language model (LM) is a 4-gram mixture LM trained on several data sets using modified Kneser-Ney discounting with interpolation, and combined with weights tuned to achieve the lowest perplexity on a development set using the SRILM toolkit (Stolcke, 2002). Data selection is performed using cross-entropy filtering (Moore and Lewis, 2010). 5.2 Word Vectors Here we analyze the quality of word vectors used in the OOV reduction experiments. The vectors are trained using an unaltered word corpus. We build a lexicon using source and target word vectors together with the projection matrix using the similarity score sim(Wxf,ze)), where the projection matrix W is used to project the source word vector xf, corresponding to the source word f, to the target vector space. The similarity between the projection result Wxf and the target word vector ze is comp</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Proc. of the Int. Conf. on Speech and Language Processing (ICSLP), volume 2, pages 901–904, Denver, CO, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Tamer Alkhouli</author>
<author>Joern Wuebker</author>
<author>Hermann Ney</author>
</authors>
<title>Translation Modeling with Bidirectional Recurrent Neural Networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods on Natural Language Processing,</booktitle>
<contexts>
<context position="2722" citStr="Sundermeyer et al., 2014" startWordPosition="396" endWordPosition="399"> different words. We focus on continuous representations whose training is influenced by the surrounding context of the token being represented. One motivation for such representations is to capture word semantics (Turney et al., 2010). This is based on the distributional hypothesis (Harris, 1954) which says that words that occur in similar contexts tend to have similar meanings. We make use of continuous vectors learned using simple neural networks. Neural networks have been gaining increasing attention recently, where they have been able to enhance strong SMT baselines (Devlin et al., 2014; Sundermeyer et al., 2014). While neural language and translation modeling make intermediate use of continuous representations, there have been also attempts at explicit learning of continuous representations to improve translation (Zhang et al., 2014; Gao et al., 2013). This work explores the potential of word semantics based on continuous vector representations to enhance the performance of phrase-based machine translation. We present a greedy algorithm that employs the phrase table to identify phrases in a training corpus. The phrase table serves to bilingually restrict the phrases spotted in the monolingual corpus.</context>
</contexts>
<marker>Sundermeyer, Alkhouli, Wuebker, Ney, 2014</marker>
<rawString>Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker, and Hermann Ney. 2014. Translation Modeling with Bidirectional Recurrent Neural Networks. In Proceedings of the Conference on Empirical Methods on Natural Language Processing, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of artificial intelligence research,</journal>
<volume>37</volume>
<issue>1</issue>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D Turney, Patrick Pantel, et al. 2010. From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Daniel Stein</author>
<author>Matthias Huck</author>
<author>Hermann Ney</author>
</authors>
<title>Jane: Open source hierarchical translation, extended with reordering and lexicon models.</title>
<date>2010</date>
<booktitle>In ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR,</booktitle>
<pages>262--270</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="17424" citStr="Vilar et al., 2010" startWordPosition="2874" endWordPosition="2877">nd our OOV reduction method. The experiments are based on vectors trained using the word2vec1 toolkit, setting vector dimensionality to 800 for Arabic and 200 for English vectors. We used the skip-gram model with a maximum skip length of 10. The phrase corpus was constructed using 5 passes, with scores computed according to Eq. 1 using 2 phrasal and 2 lexical features. The phrasal and lexical weights were set to 1 and 0.5 respectively, with all features being negative log-probabilities, and the scoring threshold θ was set to 10. All translation experiments are performed with the Jane toolkit (Vilar et al., 2010; Wuebker et al., 2012). 1https://code.google.com/p/word2vec/ 5.1 Baseline System Our phrase-based baseline system consists of two phrasal and two lexical translation models, trained using a word-aligned bilingual training corpus. Word alignment is automatically generated by GIZA++ (Och and Ney, 2003) given a sentencealigned bilingual corpus. We also include binary count features and bidirectional hierarchical reordering models (Galley and Manning, 2008), with three orientation classes per direction resulting in six reordering models. The baseline also includes word penalty, phrase penalty and</context>
</contexts>
<marker>Vilar, Stein, Huck, Ney, 2010</marker>
<rawString>David Vilar, Daniel Stein, Matthias Huck, and Hermann Ney. 2010. Jane: Open source hierarchical translation, extended with reordering and lexicon models. In ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR, pages 262–270, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Matthias Huck</author>
<author>Stephan Peitz</author>
<author>Malte Nuhn</author>
<author>Markus Freitag</author>
<author>Jan-Thorsten Peter</author>
<author>Saab Mansour</author>
<author>Hermann Ney</author>
</authors>
<title>Jane 2: Open source phrase-based and hierarchical statistical machine translation.</title>
<date>2012</date>
<booktitle>In International Conference on Computational Linguistics,</booktitle>
<pages>483--491</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="17447" citStr="Wuebker et al., 2012" startWordPosition="2878" endWordPosition="2881"> method. The experiments are based on vectors trained using the word2vec1 toolkit, setting vector dimensionality to 800 for Arabic and 200 for English vectors. We used the skip-gram model with a maximum skip length of 10. The phrase corpus was constructed using 5 passes, with scores computed according to Eq. 1 using 2 phrasal and 2 lexical features. The phrasal and lexical weights were set to 1 and 0.5 respectively, with all features being negative log-probabilities, and the scoring threshold θ was set to 10. All translation experiments are performed with the Jane toolkit (Vilar et al., 2010; Wuebker et al., 2012). 1https://code.google.com/p/word2vec/ 5.1 Baseline System Our phrase-based baseline system consists of two phrasal and two lexical translation models, trained using a word-aligned bilingual training corpus. Word alignment is automatically generated by GIZA++ (Och and Ney, 2003) given a sentencealigned bilingual corpus. We also include binary count features and bidirectional hierarchical reordering models (Galley and Manning, 2008), with three orientation classes per direction resulting in six reordering models. The baseline also includes word penalty, phrase penalty and a simple distance-base</context>
</contexts>
<marker>Wuebker, Huck, Peitz, Nuhn, Freitag, Peter, Mansour, Ney, 2012</marker>
<rawString>Joern Wuebker, Matthias Huck, Stephan Peitz, Malte Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab Mansour, and Hermann Ney. 2012. Jane 2: Open source phrase-based and hierarchical statistical machine translation. In International Conference on Computational Linguistics, pages 483–491, Mumbai, India, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Chengqing Zong</author>
</authors>
<title>Bilingually-constrained phrase embeddings for machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1641" citStr="Zhang et al., 2014" startWordPosition="229" endWordPosition="232">line applied to the Arabic-to-English task of IWSLT 2013. 1 Introduction Categorical word representation has been widely used in many natural language processing (NLP) applications including statistical machine translation (SMT), where words are treated as discrete random variables. Continuous word representations, on the other hand, have been applied successfully in many NLP areas (Manning et al., 2008; Collobert and Weston, 2008). However, their application to machine translation is still an open research question. Several works tried to address the question recently (Mikolov et al., 2013b; Zhang et al., 2014; Zou et al., 2013), and this work is but another step in that direction. While categorical representations do not encode any information about word identities, continuous representations embed words in a vector space, resulting in geometric arrangements that reflect information about the represented words. Such embeddings open the potential for applying information retrieval approaches where it becomes possible to define and compute similarity between different words. We focus on continuous representations whose training is influenced by the surrounding context of the token being represented.</context>
<context position="2947" citStr="Zhang et al., 2014" startWordPosition="428" endWordPosition="431">10). This is based on the distributional hypothesis (Harris, 1954) which says that words that occur in similar contexts tend to have similar meanings. We make use of continuous vectors learned using simple neural networks. Neural networks have been gaining increasing attention recently, where they have been able to enhance strong SMT baselines (Devlin et al., 2014; Sundermeyer et al., 2014). While neural language and translation modeling make intermediate use of continuous representations, there have been also attempts at explicit learning of continuous representations to improve translation (Zhang et al., 2014; Gao et al., 2013). This work explores the potential of word semantics based on continuous vector representations to enhance the performance of phrase-based machine translation. We present a greedy algorithm that employs the phrase table to identify phrases in a training corpus. The phrase table serves to bilingually restrict the phrases spotted in the monolingual corpus. The algorithm is applied separately to the source and target sides of the training data, resulting in source and target corpora of phrases (instead of words). The phrase corpus is used to learn phrase vectors using the same </context>
<context position="28387" citStr="Zhang et al., 2014" startWordPosition="4694" endWordPosition="4697"> confusing confounding ����� ���� encourage rallied for ������� villagers redneck Table 6: Examples of OOV words that were translated due to paraphrasing. The examples are extracted from the translation hypotheses of the small experiment. OOV reduction. Examples are shown in Table 6. Although the reference words are not matched exactly, the VSM translations are semantically close to them, suggesting that OOV reduction in these cases was somewhat successful, although not rewarded by either of the scoring measures used. 6 Related Work Bilingually-constrained phrase embeddings were developed in (Zhang et al., 2014). Initial embeddings were trained in an unsupervised manner, followed by fine-tuning using bilingual knowledge to minimize the semantic distance between translation equivalents, and maximizing the distance between non-translation pairs. The embeddings are learned using recursive neural networks by decomposing phrases to their constituents. While our work includes bilingual constraints to learn phrase vectors, the constraints are implicit in the phrase corpus. Our approach is simple, focusing on the preprocessing step of preparing the phrase corpus, and therefore it can be used with different e</context>
</contexts>
<marker>Zhang, Liu, Li, Zhou, Zong, 2014</marker>
<rawString>Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and Chengqing Zong. 2014. Bilingually-constrained phrase embeddings for machine translation. In Proceedings of the 52th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Y Zou</author>
<author>Richard Socher</author>
<author>Daniel M Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Bilingual word embeddings for phrase-based machine translation.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1393--1398</pages>
<contexts>
<context position="1660" citStr="Zou et al., 2013" startWordPosition="233" endWordPosition="236">Arabic-to-English task of IWSLT 2013. 1 Introduction Categorical word representation has been widely used in many natural language processing (NLP) applications including statistical machine translation (SMT), where words are treated as discrete random variables. Continuous word representations, on the other hand, have been applied successfully in many NLP areas (Manning et al., 2008; Collobert and Weston, 2008). However, their application to machine translation is still an open research question. Several works tried to address the question recently (Mikolov et al., 2013b; Zhang et al., 2014; Zou et al., 2013), and this work is but another step in that direction. While categorical representations do not encode any information about word identities, continuous representations embed words in a vector space, resulting in geometric arrangements that reflect information about the represented words. Such embeddings open the potential for applying information retrieval approaches where it becomes possible to define and compute similarity between different words. We focus on continuous representations whose training is influenced by the surrounding context of the token being represented. One motivation for</context>
<context position="29061" citStr="Zou et al. (2013)" startWordPosition="4794" endWordPosition="4797">er, followed by fine-tuning using bilingual knowledge to minimize the semantic distance between translation equivalents, and maximizing the distance between non-translation pairs. The embeddings are learned using recursive neural networks by decomposing phrases to their constituents. While our work includes bilingual constraints to learn phrase vectors, the constraints are implicit in the phrase corpus. Our approach is simple, focusing on the preprocessing step of preparing the phrase corpus, and therefore it can be used with different existing frameworks that were developed for word vectors. Zou et al. (2013) learn bilingual word embeddings by designing an objective function that combines unsupervised training with bilingual constraints based on word alignments. Similar to our work, they compute an additional feature for phrase pairs using cosine similarity. Word vectors are averaged to obtain phrase representations. In contrast, our approach learns phrase representations directly. Recurrent neural networks were used with minimum translation units (Hu et al., 2014), which are phrase pairs undergoing certain constraints. At the input layer, each of the source and target phrases are modeled as a bag</context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Y Zou, Richard Socher, Daniel M Cer, and Christopher D Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In EMNLP, pages 1393–1398.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000129">
<title confidence="0.998646">
A Deep Architecture for Semantic Parsing
</title>
<author confidence="0.999518">
Edward Grefenstette, Phil Blunsom, Nando de Freitas and Karl Moritz Hermann
</author>
<affiliation confidence="0.993085">
Department of Computer Science
University of Oxford, UK
</affiliation>
<email confidence="0.95865">
{edwgre, pblunsom, nando, karher}@cs.ox.ac.uk
</email>
<sectionHeader confidence="0.997042" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929055555556">
Many successful approaches to semantic
parsing build on top of the syntactic anal-
ysis of text, and make use of distribu-
tional representations or statistical mod-
els to match parses to ontology-specific
queries. This paper presents a novel deep
learning architecture which provides a se-
mantic parsing system through the union
of two neural models of language se-
mantics. It allows for the generation of
ontology-specific queries from natural lan-
guage statements and questions without
the need for parsing, which makes it es-
pecially suitable to grammatically mal-
formed or syntactically atypical text, such
as tweets, as well as permitting the devel-
opment of semantic parsers for resource-
poor languages.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985619047619">
The ubiquity of always-online computers in the
form of smartphones, tablets, and notebooks has
boosted the demand for effective question answer-
ing systems. This is exemplified by the grow-
ing popularity of products like Apple’s Siri or
Google’s Google Now services. In turn, this cre-
ates the need for increasingly sophisticated meth-
ods for semantic parsing. Recent work (Artzi and
Zettlemoyer, 2013; Kwiatkowski et al., 2013; Ma-
tuszek et al., 2012; Liang et al., 2011, inter alia)
has answered this call by progressively moving
away from strictly rule-based semantic parsing, to-
wards the use of distributed representations in con-
junction with traditional grammatically-motivated
re-write rules. This paper seeks to extend this line
of thinking to its logical conclusion, by provid-
ing the first (to our knowledge) entirely distributed
neural semantic generative parsing model. It does
so by adapting deep learning methods from related
work in sentiment analysis (Socher et al., 2012;
Hermann and Blunsom, 2013), document classifi-
cation (Yih et al., 2011; Lauly et al., 2014; Her-
mann and Blunsom, 2014a), frame-semantic pars-
ing (Hermann et al., 2014), and machine trans-
lation (Mikolov et al., 2010; Kalchbrenner and
Blunsom, 2013a), inter alia, combining two em-
pirically successful deep learning models to form
a new architecture for semantic parsing.
The structure of this short paper is as follows.
We first provide a brief overview of the back-
ground literature this model builds on in §2. In §3,
we begin by introducing two deep learning models
with different aims, namely the joint learning of
embeddings in parallel corpora, and the generation
of strings of a language conditioned on a latent
variable, respectively. We then discuss how both
models can be combined and jointly trained to
form a deep learning model supporting the gener-
ation of knowledgebase queries from natural lan-
guage questions. Finally, in §4 we conclude by
discussing planned experiments and the data re-
quirements to effectively train this model.
</bodyText>
<sectionHeader confidence="0.989309" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9999575625">
Semantic parsing describes a task within the larger
field of natural language understanding. Within
computational linguistics, semantic parsing is typ-
ically understood to be the task of mapping nat-
ural language sentences to formal representations
of their underlying meaning. This semantic rep-
resentation varies significantly depending on the
task context. For instance, semantic parsing has
been applied to interpreting movement instruc-
tions (Artzi and Zettlemoyer, 2013) or robot con-
trol (Matuszek et al., 2012), where the underlying
representation would consist of actions.
Within the context of question answering—the
focus of this paper—semantic parsing typically
aims to map natural language to database queries
that would answer a given question. Kwiatkowski
</bodyText>
<page confidence="0.957519">
22
</page>
<bodyText confidence="0.855788222222222">
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 22–27,
Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics
et al. (2013) approach this problem using a multi-
step model. First, they use a CCG-like parser
to convert natural language into an underspecified
logical form (ULF). Second, the ULF is converted
into a specified form (here a FreeBase query),
which can be used to lookup the answer to the
given natural language question.
</bodyText>
<sectionHeader confidence="0.979296" genericHeader="method">
3 Model Description
</sectionHeader>
<bodyText confidence="0.999783631578947">
We describe a semantic-parsing model that learns
to derive quasi-logical database queries from nat-
ural language. The model follows the structure of
Kwiatkowski et al. (2013), but relies on a series of
neural networks and distributed representations in
lieu of the CCG and λ-Calculus based representa-
tions used in that paper.
The model described here borrows heavily from
two approaches in the deep learning literature.
First, a noise-contrastive neural network similar to
that of Hermann and Blunsom (2014a, 2014b) is
used to learn a joint latent representation for nat-
ural language and database queries (§3.1). Sec-
ond, we employ a structured conditional neural
language model in §3.2 to generate queries given
such latent representations. Below we provide the
necessary background on these two components,
before introducing the combined model and de-
scribing its learning setup.
</bodyText>
<subsectionHeader confidence="0.999722">
3.1 Bilingual Compositional Sentence Models
</subsectionHeader>
<bodyText confidence="0.997239565217391">
The bilingual compositional sentence model
(BiCVM) of Hermann and Blunsom (2014a) pro-
vides a state-of-the-art method for learning se-
mantically informative distributed representations
for sentences of language pairs from parallel cor-
pora. Through the joint production of a shared la-
tent representation for semantically aligned sen-
tence pairs, it optimises sentence embeddings
so that the respective representations of dissim-
ilar cross-lingual sentence pairs will be weakly
aligned, while those of similar sentence pairs will
be strongly aligned. Both the ability to jointly
learn sentence embeddings, and to produce latent
shared representations, will be relevant to our se-
mantic parsing pipeline.
The BiCVM model shown in Fig. 1 assumes
vector composition functions g and h, which map
an ordered set of vectors (here, word embed-
dings from DA, DB) onto a single vector in Rn.
As stated above, for semantically equivalent sen-
tences a, b across languages LA, LB, the model
aims to minimise the distance between these com-
posed representations:
</bodyText>
<equation confidence="0.718095">
Ebi(a, b) = g(a) − h(b) 2
</equation>
<bodyText confidence="0.999076833333333">
In order to avoid strong alignment between dis-
similar cross-lingual sentence pairs, this error
is combined with a noise-contrastive hinge loss,
where n E LB is a randomly sampled sentence,
dissimilar to the parallel pair {a, b}, and m de-
notes some margin:
</bodyText>
<equation confidence="0.666356">
Ehl(a, b, n) = [m + Ebi(a, b) − Ebi(a, n)]+ ,
</equation>
<bodyText confidence="0.998231">
where [x]+ = max(0, x). The resulting objective
function is as follows
</bodyText>
<equation confidence="0.945761">
�
λ
Ehl(a, b, ni) + 2 θ 2 ,
</equation>
<bodyText confidence="0.825986">
with λ2 θ 2 as the L2 regularization term and
θ={g, h, DA, DB} as the set of model variables.
</bodyText>
<figureCaption confidence="0.817657">
Figure 1: Diagrammatic representation of a
BiCVM.
</figureCaption>
<bodyText confidence="0.99939">
While Hermann and Blunsom (2014a) applied
this model only to parallel corpora of sentences,
it is important to note that the model is agnostic
concerning the inputs of functions g and h. In this
paper we will discuss how this model can be ap-
plied to non-sentential inputs.
</bodyText>
<figure confidence="0.834704857142857">
L2 word embeddings
h
L2 sentence embedding
contrastive estimation
L1 sentence embedding
L1 word embeddings
g
</figure>
<equation confidence="0.856458">
NNN
k
J(θ) = �
(a,b)∈C i=1
</equation>
<page confidence="0.985124">
23
</page>
<subsectionHeader confidence="0.998799">
3.2 Conditional Neural Language Models
</subsectionHeader>
<bodyText confidence="0.999771552631579">
Neural language models (Bengio et al., 2006) pro-
vide a distributed alternative to n-gram language
models, permitting the joint learning of a pre-
diction function for the next word in a sequence
given the distributed representations of a subset
of the last n−1 words alongside the representa-
tions themselves. Recent work in dialogue act la-
belling (Kalchbrenner and Blunsom, 2013b) and
in machine translation (Kalchbrenner and Blun-
som, 2013a) has demonstrated that a particular
kind of neural language model based on recurrent
neural networks (Mikolov et al., 2010; Sutskever
et al., 2011) could be extended so that the next
word in a sequence is jointly generated by the
word history and the distributed representation for
a conditioning element, such as the dialogue class
of a previous sentence, or the vector representation
of a source sentence. In this section, we briefly de-
scribe a general formulation of conditional neural
language models, based on the log-bilinear mod-
els of Mnih and Hinton (2007) due to their relative
simplicity.
A log-bilinear language model is a neural net-
work modelling a probability distribution over the
next word in a sequence given the previous n−1,
i.e. p(wn|w1:n−1). Let |V  |be the size of our vo-
cabulary, and R be a |V  |× d vocabulary matrix
where the Rwi demnotes the row containing the
word embedding in Rd of a word wi, with d be-
ing a hyper-parameter indicating embedding size.
Let Ci be the context transform matrix in Rdxd
which modifies the representation of the ith word
in the word history. Let bwi be a scalar bias as-
sociated with a word wi, and bR be a bias vector
in Rd associated with the model. A log-bilinear
model expressed the probability of wn given a his-
tory of n−1 words as a function of the energy of
the network:
</bodyText>
<equation confidence="0.7938525">
E(wn; w1:n−1) =
!RT wiCi Rwn − bTRRwn − bwn
</equation>
<bodyText confidence="0.9981855">
From this, the probability distribution over the
next word is obtained:
</bodyText>
<equation confidence="0.976176333333333">
−E(wniw1:n−1)
Pwn e
−E(wniw1:n−1)
</equation>
<bodyText confidence="0.824713">
To reframe a log-bilinear language model as a
conditional language model (CNLM), illustrated
</bodyText>
<figureCaption confidence="0.949735">
Figure 2: Diagrammatic representation of a Con-
ditional Neural Language Model.
</figureCaption>
<bodyText confidence="0.999049222222222">
in Fig. 2, let us suppose that we wish to jointly
condition the next word on its history and some
variable β, for which an embedding rβ has been
obtained through a previous step, in order to com-
pute p(wn|w1:n−1, β). The simplest way to do this
additively, which allows us to treat the contribu-
tion of the embedding for β as similar to that of an
extra word in the history. We define a new energy
function:
</bodyText>
<equation confidence="0.999208">
E(wn; w1:n−1, β) =
! !
RT wiCi + rTβ Cβ Rwn− bTRRwn− bwn
−E(wniw1:n−1,β)
Pwn e
−E(wniw1:n−1,β)
</equation>
<bodyText confidence="0.999717">
Log-bilinear language models and their condi-
tional variants alike are typically trained by max-
imising the log-probability of observed sequences.
</bodyText>
<subsectionHeader confidence="0.998931">
3.3 A Combined Semantic Parsing Model
</subsectionHeader>
<bodyText confidence="0.999969">
The models in §§3.1–3.2 can be combined to form
a model capable of jointly learning a shared la-
tent representation for question/query pairs using
a BiCVM, and using this latent representation to
learn a conditional log-bilinear CNLM. The full
model is shown in Fig. 3. Here, we explain the
final model architecture both for training and for
subsequent use as a generative model. The details
of the training procedure will be discussed in §3.4.
The combination is fairly straightforward, and
happens in two steps at training time. For the
</bodyText>
<equation confidence="0.950314666666667">
wn-3 wn-2 wn-1
wn
β
− n−1X
i=1
e
p(wn|w1:n−1) =
− n−1X
i=1
to obtain the probability
e
p(wn|w1:n−1, β) =
</equation>
<page confidence="0.980813">
24
</page>
<figure confidence="0.567325">
Question
</figure>
<figureCaption confidence="0.668560833333333">
Figure 3: Diagrammatic representation of the full
model. First the mappings for obtaining latent
forms of questions and queries are jointly learned
through a BiCVM. The latent form for questions
then serves as conditioning element in a log-
bilinear CNLM.
</figureCaption>
<bodyText confidence="0.999753846153846">
first step, shown in the left hand side of Fig. 3,
a BiCVM is trained against a parallel corpora
of natural language question and knowledgebase
query pairs. Optionally, the embeddings for the
query symbol representations and question words
are initialised and/or fine-tuned during training,
as discussed in §3.4. For the natural language
side of the model, the composition function g can
be a simple additive model as in Hermann and
Blunsom (2014a), although the semantic informa-
tion required for the task proposed here would
probably benefit from a more complex composi-
tion function such as a convolution neural net-
work. Function h, which maps the knowledgebase
queries into the shared space could also rely on
convolution, although the structure of the database
queries might favour a setup relying primarily on
bi-gram composition.
Using function g and the original training data,
the training data for the second stage is created
by obtaining the latent representation for the ques-
tions of the original dataset. We thereby obtain
pairs of aligned latent question representations and
knowledgebase queries. This data allows us to
train a log-bilinear CNLM as shown on the right
side of Fig. 3.
</bodyText>
<figureCaption confidence="0.758430666666667">
Once trained, the models can be fully joined to
produce a generative neural network as shown in
Fig. 4. The network modelling g from the BiCVM
</figureCaption>
<figure confidence="0.9183335">
Generated Query
Question
</figure>
<figureCaption confidence="0.68190025">
Figure 4: Diagrammatic representation of the final
network. The question-compositional segment of
the BiCVM produces a latent representation, con-
ditioning a CNLM generating a query.
</figureCaption>
<bodyText confidence="0.9637435">
takes the distributed representations of question
words from unseen questions, and produces a la-
tent representation. The latent representation is
then passed to the log-bilinear CNLM, which con-
ditionally generates a knowledgebase query corre-
sponding to the question.
</bodyText>
<subsectionHeader confidence="0.963501">
3.4 Learning Model Parameters
</subsectionHeader>
<bodyText confidence="0.999976272727273">
We propose training the model of §3.3 in a two
stage process, in line with the symbolic model of
Kwiatkowski et al. (2013).
First, a BiCVM is trained on a parallel corpus
C of question-query pairs (Q, R) E C, using com-
position functions g for natural language questions
and h for database queries. While functions g and
h may differ from those discussed in Hermann and
Blunsom (2014a), the basic noise-contrastive op-
timisation function remains the same. It is possi-
ble to initialise the model fully randomly, in which
</bodyText>
<figure confidence="0.996443">
Knowledgebase query
Conditional
Log-bilinear
Language Model
Relation/object
embeddings
h
Query embedding
Question embedding
Word embeddings
g
...
Latent
representation
...
</figure>
<page confidence="0.995253">
25
</page>
<bodyText confidence="0.999928782608696">
case the model parameters θ learned at this stage
include the two distributed representation lexica
for questions and queries, DQ and DR respec-
tively, as well as all parameters for g and h.
Alternatively, word embeddings in DQ could be
initialised with representations learned separately,
for instance with a neural language model or a
similar system (Mikolov et al., 2010; Turian et al.,
2010; Collobert et al., 2011, inter alia). Likewise,
the relation and object embeddings in DR could be
initialised with representations learned from dis-
tributed relation extraction schemas such as that
of Riedel et al. (2013).
Having learned representations for queries in
DR as well as function g, the second training phase
of the model uses a new parallel corpus consisting
of pairs (g(Q), R) E C&apos; to train the CNLM as pre-
sented in §3.3.
The two training steps can be applied iteratively,
and further, it is trivial to modify the learning
procedure to use composition function h as an-
other input for the CNLM training phrase in an
autoencoder-like setup.
</bodyText>
<sectionHeader confidence="0.862946" genericHeader="method">
4 Experimental Requirements and
</sectionHeader>
<subsectionHeader confidence="0.66917">
Further Work
</subsectionHeader>
<bodyText confidence="0.999984666666667">
The particular training procedure for the model
described in this paper requires aligned ques-
tion/knowledgebase query pairs. There exist some
small corpora that could be used for this task
(Zelle and Mooney, 1996; Cai and Yates, 2013). In
order to scale training beyond these small corpora,
we hypothesise that larger amounts of (potentially
noisy) training data could be obtained using a
boot-strapping technique similar to Kwiatkowski
et al. (2013).
To evaluate this model, we will follow the ex-
perimental setup of Kwiatkowski et al. (2013).
With the provisio that the model can generate
freebase queries correctly, further work will seek
to determine whether this architecture can gener-
ate other structured formal language expressions,
such as lambda expressions for use in textual en-
tailement tasks.
</bodyText>
<sectionHeader confidence="0.997751" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9972796">
This work was supported by a Xerox Foundation
Award, EPSRC grants number EP/I03808X/1 and
EP/K036580/1, and the Canadian Institute for Ad-
vanced Research (CIFAR) Program on Adaptive
Perception and Neural Computation.
</bodyText>
<sectionHeader confidence="0.995172" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998254888888889">
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49–62.
Yoshua Bengio, Holger Schwenk, Jean-S´ebastien
Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137–186.
Springer.
Qingqing Cai and Alexander Yates. 2013. Large-scale
Semantic Parsing via Schema Matching and Lexi-
con Extension. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Composi-
tional Semantics. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), Sofia, Bulgaria,
August. Association for Computational Linguistics.
Karl Moritz Hermann and Phil Blunsom. 2014a. Mul-
tilingual Distributed Representations without Word
Alignment. In Proceedings of the 2nd International
Conference on Learning Representations, Banff,
Canada, April.
Karl Moritz Hermann and Phil Blunsom. 2014b. Mul-
tilingual Models for Compositional Distributional
Semantics. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), Baltimore, USA,
June. Association for Computational Linguistics.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic Frame Iden-
tification with Distributed Word Representations. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), Baltimore, USA, June. Association
for Computational Linguistics.
Nal Kalchbrenner and Phil Blunsom. 2013a. Re-
current continuous translation models. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing (EMNLP), Seattle,
USA. Association for Computational Linguistics.
Nal Kalchbrenner and Phil Blunsom. 2013b. Re-
current convolutional neural networks for discourse
compositionality. arXiv preprint arXiv:1306.3584.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
</reference>
<page confidence="0.952319">
26
</page>
<reference confidence="0.998555131147541">
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1545–1556, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Stanislas Lauly, Alex Boulanger, and Hugo Larochelle.
2014. Learning multilingual word representa-
tions using a bag-of-words autoencoder. CoRR,
abs/1401.1803.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ’11,
pages 590–599, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Cynthia Matuszek, Nicholas FitzGerald, Luke S.
Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A
joint model of language and perception for grounded
attribute learning. In Proceedings of the 29th Inter-
national Conference on Machine Learning, ICML
2012, Edinburgh, Scotland, UK, June 26 - July 1,
2012.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045–1048.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641–648. ACM.
Sebastian Riedel, Limin Yao, Benjamin M. Marlin, and
Andrew McCallum. 2013. Relation extraction with
matrix factorization and universal schemas. In Joint
Human Language Technology Conference/Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-NAACL
’13), June.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of EMNLP-CoNLL, pages 1201–1211.
Ilya Sutskever, James Martens, and Geoffrey E Hin-
ton. 2011. Generating text with recurrent neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
1017–1024.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of
ACL, Stroudsburg, PA, USA.
Wen-Tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning Discrimina-
tive Projections for Text Similarity Measures. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning, CoNLL ’11,
pages 247–256, Stroudsburg, PA, USA. Association
for Computational Linguistics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the National Con-
ference on Artificial Intelligence, pages 1050–1055.
</reference>
<page confidence="0.998805">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.891969">
<title confidence="0.999675">A Deep Architecture for Semantic Parsing</title>
<author confidence="0.973721">Phil Blunsom Grefenstette</author>
<author confidence="0.973721">Nando de_Freitas Moritz</author>
<affiliation confidence="0.999965">Department of Computer University of Oxford,</affiliation>
<email confidence="0.95914">pblunsom,nando,</email>
<abstract confidence="0.997600789473684">Many successful approaches to semantic parsing build on top of the syntactic analysis of text, and make use of distributional representations or statistical models to match parses to ontology-specific queries. This paper presents a novel deep learning architecture which provides a semantic parsing system through the union of two neural models of language semantics. It allows for the generation of ontology-specific queries from natural language statements and questions without the need for parsing, which makes it especially suitable to grammatically malformed or syntactically atypical text, such as tweets, as well as permitting the development of semantic parsers for resourcepoor languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="1339" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="200" endWordPosition="203"> need for parsing, which makes it especially suitable to grammatically malformed or syntactically atypical text, such as tweets, as well as permitting the development of semantic parsers for resourcepoor languages. 1 Introduction The ubiquity of always-online computers in the form of smartphones, tablets, and notebooks has boosted the demand for effective question answering systems. This is exemplified by the growing popularity of products like Apple’s Siri or Google’s Google Now services. In turn, this creates the need for increasingly sophisticated methods for semantic parsing. Recent work (Artzi and Zettlemoyer, 2013; Kwiatkowski et al., 2013; Matuszek et al., 2012; Liang et al., 2011, inter alia) has answered this call by progressively moving away from strictly rule-based semantic parsing, towards the use of distributed representations in conjunction with traditional grammatically-motivated re-write rules. This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model. It does so by adapting deep learning methods from related work in sentiment analysis (Socher et al., 2012; Hermann and Blu</context>
<context position="3448" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="525" endWordPosition="528">al language questions. Finally, in §4 we conclude by discussing planned experiments and the data requirements to effectively train this model. 2 Background Semantic parsing describes a task within the larger field of natural language understanding. Within computational linguistics, semantic parsing is typically understood to be the task of mapping natural language sentences to formal representations of their underlying meaning. This semantic representation varies significantly depending on the task context. For instance, semantic parsing has been applied to interpreting movement instructions (Artzi and Zettlemoyer, 2013) or robot control (Matuszek et al., 2012), where the underlying representation would consist of actions. Within the context of question answering—the focus of this paper—semantic parsing typically aims to map natural language to database queries that would answer a given question. Kwiatkowski 22 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 22–27, Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics et al. (2013) approach this problem using a multistep model. First, they use a CCG-like parser to convert natural language into an underspecifie</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1(1):49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Holger Schwenk</author>
<author>Jean-S´ebastien Sen´ecal</author>
<author>Fr´ederic Morin</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Neural probabilistic language models.</title>
<date>2006</date>
<booktitle>In Innovations in Machine Learning,</booktitle>
<pages>137--186</pages>
<publisher>Springer.</publisher>
<marker>Bengio, Schwenk, Sen´ecal, Morin, Gauvain, 2006</marker>
<rawString>Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. 2006. Neural probabilistic language models. In Innovations in Machine Learning, pages 137–186. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Large-scale Semantic Parsing via Schema Matching and Lexicon Extension.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="14776" citStr="Cai and Yates, 2013" startWordPosition="2380" endWordPosition="2383">ng phase of the model uses a new parallel corpus consisting of pairs (g(Q), R) E C&apos; to train the CNLM as presented in §3.3. The two training steps can be applied iteratively, and further, it is trivial to modify the learning procedure to use composition function h as another input for the CNLM training phrase in an autoencoder-like setup. 4 Experimental Requirements and Further Work The particular training procedure for the model described in this paper requires aligned question/knowledgebase query pairs. There exist some small corpora that could be used for this task (Zelle and Mooney, 1996; Cai and Yates, 2013). In order to scale training beyond these small corpora, we hypothesise that larger amounts of (potentially noisy) training data could be obtained using a boot-strapping technique similar to Kwiatkowski et al. (2013). To evaluate this model, we will follow the experimental setup of Kwiatkowski et al. (2013). With the provisio that the model can generate freebase queries correctly, further work will seek to determine whether this architecture can generate other structured formal language expressions, such as lambda expressions for use in textual entailement tasks. Acknowledgements This work was</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013. Large-scale Semantic Parsing via Schema Matching and Lexicon Extension. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>Leon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="13868" citStr="Collobert et al., 2011" startWordPosition="2231" endWordPosition="2234">odel fully randomly, in which Knowledgebase query Conditional Log-bilinear Language Model Relation/object embeddings h Query embedding Question embedding Word embeddings g ... Latent representation ... 25 case the model parameters θ learned at this stage include the two distributed representation lexica for questions and queries, DQ and DR respectively, as well as all parameters for g and h. Alternatively, word embeddings in DQ could be initialised with representations learned separately, for instance with a neural language model or a similar system (Mikolov et al., 2010; Turian et al., 2010; Collobert et al., 2011, inter alia). Likewise, the relation and object embeddings in DR could be initialised with representations learned from distributed relation extraction schemas such as that of Riedel et al. (2013). Having learned representations for queries in DR as well as function g, the second training phase of the model uses a new parallel corpus consisting of pairs (g(Q), R) E C&apos; to train the CNLM as presented in §3.3. The two training steps can be applied iteratively, and further, it is trivial to modify the learning procedure to use composition function h as another input for the CNLM training phrase i</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>The Role of Syntax in Vector Space Models of Compositional Semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1950" citStr="Hermann and Blunsom, 2013" startWordPosition="293" endWordPosition="296">ttlemoyer, 2013; Kwiatkowski et al., 2013; Matuszek et al., 2012; Liang et al., 2011, inter alia) has answered this call by progressively moving away from strictly rule-based semantic parsing, towards the use of distributed representations in conjunction with traditional grammatically-motivated re-write rules. This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model. It does so by adapting deep learning methods from related work in sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), document classification (Yih et al., 2011; Lauly et al., 2014; Hermann and Blunsom, 2014a), frame-semantic parsing (Hermann et al., 2014), and machine translation (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013a), inter alia, combining two empirically successful deep learning models to form a new architecture for semantic parsing. The structure of this short paper is as follows. We first provide a brief overview of the background literature this model builds on in §2. In §3, we begin by introducing two deep learning models with different aims, namely the joint learning of embeddings in</context>
</contexts>
<marker>Hermann, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2013. The Role of Syntax in Vector Space Models of Compositional Semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Multilingual Distributed Representations without Word Alignment.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2nd International Conference on Learning Representations,</booktitle>
<location>Banff, Canada,</location>
<contexts>
<context position="2040" citStr="Hermann and Blunsom, 2014" startWordPosition="308" endWordPosition="312"> alia) has answered this call by progressively moving away from strictly rule-based semantic parsing, towards the use of distributed representations in conjunction with traditional grammatically-motivated re-write rules. This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model. It does so by adapting deep learning methods from related work in sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), document classification (Yih et al., 2011; Lauly et al., 2014; Hermann and Blunsom, 2014a), frame-semantic parsing (Hermann et al., 2014), and machine translation (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013a), inter alia, combining two empirically successful deep learning models to form a new architecture for semantic parsing. The structure of this short paper is as follows. We first provide a brief overview of the background literature this model builds on in §2. In §3, we begin by introducing two deep learning models with different aims, namely the joint learning of embeddings in parallel corpora, and the generation of strings of a language conditioned on a latent var</context>
<context position="4750" citStr="Hermann and Blunsom (2014" startWordPosition="726" endWordPosition="729">e a FreeBase query), which can be used to lookup the answer to the given natural language question. 3 Model Description We describe a semantic-parsing model that learns to derive quasi-logical database queries from natural language. The model follows the structure of Kwiatkowski et al. (2013), but relies on a series of neural networks and distributed representations in lieu of the CCG and λ-Calculus based representations used in that paper. The model described here borrows heavily from two approaches in the deep learning literature. First, a noise-contrastive neural network similar to that of Hermann and Blunsom (2014a, 2014b) is used to learn a joint latent representation for natural language and database queries (§3.1). Second, we employ a structured conditional neural language model in §3.2 to generate queries given such latent representations. Below we provide the necessary background on these two components, before introducing the combined model and describing its learning setup. 3.1 Bilingual Compositional Sentence Models The bilingual compositional sentence model (BiCVM) of Hermann and Blunsom (2014a) provides a state-of-the-art method for learning semantically informative distributed representation</context>
<context position="6811" citStr="Hermann and Blunsom (2014" startWordPosition="1064" endWordPosition="1067">posed representations: Ebi(a, b) = g(a) − h(b) 2 In order to avoid strong alignment between dissimilar cross-lingual sentence pairs, this error is combined with a noise-contrastive hinge loss, where n E LB is a randomly sampled sentence, dissimilar to the parallel pair {a, b}, and m denotes some margin: Ehl(a, b, n) = [m + Ebi(a, b) − Ebi(a, n)]+ , where [x]+ = max(0, x). The resulting objective function is as follows � λ Ehl(a, b, ni) + 2 θ 2 , with λ2 θ 2 as the L2 regularization term and θ={g, h, DA, DB} as the set of model variables. Figure 1: Diagrammatic representation of a BiCVM. While Hermann and Blunsom (2014a) applied this model only to parallel corpora of sentences, it is important to note that the model is agnostic concerning the inputs of functions g and h. In this paper we will discuss how this model can be applied to non-sentential inputs. L2 word embeddings h L2 sentence embedding contrastive estimation L1 sentence embedding L1 word embeddings g NNN k J(θ) = � (a,b)∈C i=1 23 3.2 Conditional Neural Language Models Neural language models (Bengio et al., 2006) provide a distributed alternative to n-gram language models, permitting the joint learning of a prediction function for the next word i</context>
<context position="11360" citStr="Hermann and Blunsom (2014" startWordPosition="1836" endWordPosition="1839"> for obtaining latent forms of questions and queries are jointly learned through a BiCVM. The latent form for questions then serves as conditioning element in a logbilinear CNLM. first step, shown in the left hand side of Fig. 3, a BiCVM is trained against a parallel corpora of natural language question and knowledgebase query pairs. Optionally, the embeddings for the query symbol representations and question words are initialised and/or fine-tuned during training, as discussed in §3.4. For the natural language side of the model, the composition function g can be a simple additive model as in Hermann and Blunsom (2014a), although the semantic information required for the task proposed here would probably benefit from a more complex composition function such as a convolution neural network. Function h, which maps the knowledgebase queries into the shared space could also rely on convolution, although the structure of the database queries might favour a setup relying primarily on bi-gram composition. Using function g and the original training data, the training data for the second stage is created by obtaining the latent representation for the questions of the original dataset. We thereby obtain pairs of ali</context>
<context position="13140" citStr="Hermann and Blunsom (2014" startWordPosition="2121" endWordPosition="2124">s from unseen questions, and produces a latent representation. The latent representation is then passed to the log-bilinear CNLM, which conditionally generates a knowledgebase query corresponding to the question. 3.4 Learning Model Parameters We propose training the model of §3.3 in a two stage process, in line with the symbolic model of Kwiatkowski et al. (2013). First, a BiCVM is trained on a parallel corpus C of question-query pairs (Q, R) E C, using composition functions g for natural language questions and h for database queries. While functions g and h may differ from those discussed in Hermann and Blunsom (2014a), the basic noise-contrastive optimisation function remains the same. It is possible to initialise the model fully randomly, in which Knowledgebase query Conditional Log-bilinear Language Model Relation/object embeddings h Query embedding Question embedding Word embeddings g ... Latent representation ... 25 case the model parameters θ learned at this stage include the two distributed representation lexica for questions and queries, DQ and DR respectively, as well as all parameters for g and h. Alternatively, word embeddings in DQ could be initialised with representations learned separately, </context>
</contexts>
<marker>Hermann, Blunsom, 2014</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2014a. Multilingual Distributed Representations without Word Alignment. In Proceedings of the 2nd International Conference on Learning Representations, Banff, Canada, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Multilingual Models for Compositional Distributional Semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA,</location>
<contexts>
<context position="2040" citStr="Hermann and Blunsom, 2014" startWordPosition="308" endWordPosition="312"> alia) has answered this call by progressively moving away from strictly rule-based semantic parsing, towards the use of distributed representations in conjunction with traditional grammatically-motivated re-write rules. This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model. It does so by adapting deep learning methods from related work in sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), document classification (Yih et al., 2011; Lauly et al., 2014; Hermann and Blunsom, 2014a), frame-semantic parsing (Hermann et al., 2014), and machine translation (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013a), inter alia, combining two empirically successful deep learning models to form a new architecture for semantic parsing. The structure of this short paper is as follows. We first provide a brief overview of the background literature this model builds on in §2. In §3, we begin by introducing two deep learning models with different aims, namely the joint learning of embeddings in parallel corpora, and the generation of strings of a language conditioned on a latent var</context>
<context position="4750" citStr="Hermann and Blunsom (2014" startWordPosition="726" endWordPosition="729">e a FreeBase query), which can be used to lookup the answer to the given natural language question. 3 Model Description We describe a semantic-parsing model that learns to derive quasi-logical database queries from natural language. The model follows the structure of Kwiatkowski et al. (2013), but relies on a series of neural networks and distributed representations in lieu of the CCG and λ-Calculus based representations used in that paper. The model described here borrows heavily from two approaches in the deep learning literature. First, a noise-contrastive neural network similar to that of Hermann and Blunsom (2014a, 2014b) is used to learn a joint latent representation for natural language and database queries (§3.1). Second, we employ a structured conditional neural language model in §3.2 to generate queries given such latent representations. Below we provide the necessary background on these two components, before introducing the combined model and describing its learning setup. 3.1 Bilingual Compositional Sentence Models The bilingual compositional sentence model (BiCVM) of Hermann and Blunsom (2014a) provides a state-of-the-art method for learning semantically informative distributed representation</context>
<context position="6811" citStr="Hermann and Blunsom (2014" startWordPosition="1064" endWordPosition="1067">posed representations: Ebi(a, b) = g(a) − h(b) 2 In order to avoid strong alignment between dissimilar cross-lingual sentence pairs, this error is combined with a noise-contrastive hinge loss, where n E LB is a randomly sampled sentence, dissimilar to the parallel pair {a, b}, and m denotes some margin: Ehl(a, b, n) = [m + Ebi(a, b) − Ebi(a, n)]+ , where [x]+ = max(0, x). The resulting objective function is as follows � λ Ehl(a, b, ni) + 2 θ 2 , with λ2 θ 2 as the L2 regularization term and θ={g, h, DA, DB} as the set of model variables. Figure 1: Diagrammatic representation of a BiCVM. While Hermann and Blunsom (2014a) applied this model only to parallel corpora of sentences, it is important to note that the model is agnostic concerning the inputs of functions g and h. In this paper we will discuss how this model can be applied to non-sentential inputs. L2 word embeddings h L2 sentence embedding contrastive estimation L1 sentence embedding L1 word embeddings g NNN k J(θ) = � (a,b)∈C i=1 23 3.2 Conditional Neural Language Models Neural language models (Bengio et al., 2006) provide a distributed alternative to n-gram language models, permitting the joint learning of a prediction function for the next word i</context>
<context position="11360" citStr="Hermann and Blunsom (2014" startWordPosition="1836" endWordPosition="1839"> for obtaining latent forms of questions and queries are jointly learned through a BiCVM. The latent form for questions then serves as conditioning element in a logbilinear CNLM. first step, shown in the left hand side of Fig. 3, a BiCVM is trained against a parallel corpora of natural language question and knowledgebase query pairs. Optionally, the embeddings for the query symbol representations and question words are initialised and/or fine-tuned during training, as discussed in §3.4. For the natural language side of the model, the composition function g can be a simple additive model as in Hermann and Blunsom (2014a), although the semantic information required for the task proposed here would probably benefit from a more complex composition function such as a convolution neural network. Function h, which maps the knowledgebase queries into the shared space could also rely on convolution, although the structure of the database queries might favour a setup relying primarily on bi-gram composition. Using function g and the original training data, the training data for the second stage is created by obtaining the latent representation for the questions of the original dataset. We thereby obtain pairs of ali</context>
<context position="13140" citStr="Hermann and Blunsom (2014" startWordPosition="2121" endWordPosition="2124">s from unseen questions, and produces a latent representation. The latent representation is then passed to the log-bilinear CNLM, which conditionally generates a knowledgebase query corresponding to the question. 3.4 Learning Model Parameters We propose training the model of §3.3 in a two stage process, in line with the symbolic model of Kwiatkowski et al. (2013). First, a BiCVM is trained on a parallel corpus C of question-query pairs (Q, R) E C, using composition functions g for natural language questions and h for database queries. While functions g and h may differ from those discussed in Hermann and Blunsom (2014a), the basic noise-contrastive optimisation function remains the same. It is possible to initialise the model fully randomly, in which Knowledgebase query Conditional Log-bilinear Language Model Relation/object embeddings h Query embedding Question embedding Word embeddings g ... Latent representation ... 25 case the model parameters θ learned at this stage include the two distributed representation lexica for questions and queries, DQ and DR respectively, as well as all parameters for g and h. Alternatively, word embeddings in DQ could be initialised with representations learned separately, </context>
</contexts>
<marker>Hermann, Blunsom, 2014</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2014b. Multilingual Models for Compositional Distributional Semantics. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Baltimore, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Dipanjan Das</author>
<author>Jason Weston</author>
<author>Kuzman Ganchev</author>
</authors>
<title>Semantic Frame Identification with Distributed Word Representations.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA,</location>
<contexts>
<context position="2089" citStr="Hermann et al., 2014" startWordPosition="316" endWordPosition="319"> away from strictly rule-based semantic parsing, towards the use of distributed representations in conjunction with traditional grammatically-motivated re-write rules. This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model. It does so by adapting deep learning methods from related work in sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), document classification (Yih et al., 2011; Lauly et al., 2014; Hermann and Blunsom, 2014a), frame-semantic parsing (Hermann et al., 2014), and machine translation (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013a), inter alia, combining two empirically successful deep learning models to form a new architecture for semantic parsing. The structure of this short paper is as follows. We first provide a brief overview of the background literature this model builds on in §2. In §3, we begin by introducing two deep learning models with different aims, namely the joint learning of embeddings in parallel corpora, and the generation of strings of a language conditioned on a latent variable, respectively. We then discuss how both mod</context>
</contexts>
<marker>Hermann, Das, Weston, Ganchev, 2014</marker>
<rawString>Karl Moritz Hermann, Dipanjan Das, Jason Weston, and Kuzman Ganchev. 2014. Semantic Frame Identification with Distributed Word Representations. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Baltimore, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, USA.</location>
<contexts>
<context position="2168" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="328" endWordPosition="331">stributed representations in conjunction with traditional grammatically-motivated re-write rules. This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model. It does so by adapting deep learning methods from related work in sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), document classification (Yih et al., 2011; Lauly et al., 2014; Hermann and Blunsom, 2014a), frame-semantic parsing (Hermann et al., 2014), and machine translation (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013a), inter alia, combining two empirically successful deep learning models to form a new architecture for semantic parsing. The structure of this short paper is as follows. We first provide a brief overview of the background literature this model builds on in §2. In §3, we begin by introducing two deep learning models with different aims, namely the joint learning of embeddings in parallel corpora, and the generation of strings of a language conditioned on a latent variable, respectively. We then discuss how both models can be combined and jointly trained to form a deep learning model supportin</context>
<context position="7607" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="1196" endWordPosition="1199">per we will discuss how this model can be applied to non-sentential inputs. L2 word embeddings h L2 sentence embedding contrastive estimation L1 sentence embedding L1 word embeddings g NNN k J(θ) = � (a,b)∈C i=1 23 3.2 Conditional Neural Language Models Neural language models (Bengio et al., 2006) provide a distributed alternative to n-gram language models, permitting the joint learning of a prediction function for the next word in a sequence given the distributed representations of a subset of the last n−1 words alongside the representations themselves. Recent work in dialogue act labelling (Kalchbrenner and Blunsom, 2013b) and in machine translation (Kalchbrenner and Blunsom, 2013a) has demonstrated that a particular kind of neural language model based on recurrent neural networks (Mikolov et al., 2010; Sutskever et al., 2011) could be extended so that the next word in a sequence is jointly generated by the word history and the distributed representation for a conditioning element, such as the dialogue class of a previous sentence, or the vector representation of a source sentence. In this section, we briefly describe a general formulation of conditional neural language models, based on the log-bilinear model</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013a. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), Seattle, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent convolutional neural networks for discourse compositionality. arXiv preprint arXiv:1306.3584.</title>
<date>2013</date>
<contexts>
<context position="2168" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="328" endWordPosition="331">stributed representations in conjunction with traditional grammatically-motivated re-write rules. This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model. It does so by adapting deep learning methods from related work in sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), document classification (Yih et al., 2011; Lauly et al., 2014; Hermann and Blunsom, 2014a), frame-semantic parsing (Hermann et al., 2014), and machine translation (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013a), inter alia, combining two empirically successful deep learning models to form a new architecture for semantic parsing. The structure of this short paper is as follows. We first provide a brief overview of the background literature this model builds on in §2. In §3, we begin by introducing two deep learning models with different aims, namely the joint learning of embeddings in parallel corpora, and the generation of strings of a language conditioned on a latent variable, respectively. We then discuss how both models can be combined and jointly trained to form a deep learning model supportin</context>
<context position="7607" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="1196" endWordPosition="1199">per we will discuss how this model can be applied to non-sentential inputs. L2 word embeddings h L2 sentence embedding contrastive estimation L1 sentence embedding L1 word embeddings g NNN k J(θ) = � (a,b)∈C i=1 23 3.2 Conditional Neural Language Models Neural language models (Bengio et al., 2006) provide a distributed alternative to n-gram language models, permitting the joint learning of a prediction function for the next word in a sequence given the distributed representations of a subset of the last n−1 words alongside the representations themselves. Recent work in dialogue act labelling (Kalchbrenner and Blunsom, 2013b) and in machine translation (Kalchbrenner and Blunsom, 2013a) has demonstrated that a particular kind of neural language model based on recurrent neural networks (Mikolov et al., 2010; Sutskever et al., 2011) could be extended so that the next word in a sequence is jointly generated by the word history and the distributed representation for a conditioning element, such as the dialogue class of a previous sentence, or the vector representation of a source sentence. In this section, we briefly describe a general formulation of conditional neural language models, based on the log-bilinear model</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013b. Recurrent convolutional neural networks for discourse compositionality. arXiv preprint arXiv:1306.3584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1545--1556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="1365" citStr="Kwiatkowski et al., 2013" startWordPosition="204" endWordPosition="207">s it especially suitable to grammatically malformed or syntactically atypical text, such as tweets, as well as permitting the development of semantic parsers for resourcepoor languages. 1 Introduction The ubiquity of always-online computers in the form of smartphones, tablets, and notebooks has boosted the demand for effective question answering systems. This is exemplified by the growing popularity of products like Apple’s Siri or Google’s Google Now services. In turn, this creates the need for increasingly sophisticated methods for semantic parsing. Recent work (Artzi and Zettlemoyer, 2013; Kwiatkowski et al., 2013; Matuszek et al., 2012; Liang et al., 2011, inter alia) has answered this call by progressively moving away from strictly rule-based semantic parsing, towards the use of distributed representations in conjunction with traditional grammatically-motivated re-write rules. This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model. It does so by adapting deep learning methods from related work in sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), document clas</context>
<context position="4418" citStr="Kwiatkowski et al. (2013)" startWordPosition="674" endWordPosition="677"> Parsing, pages 22–27, Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics et al. (2013) approach this problem using a multistep model. First, they use a CCG-like parser to convert natural language into an underspecified logical form (ULF). Second, the ULF is converted into a specified form (here a FreeBase query), which can be used to lookup the answer to the given natural language question. 3 Model Description We describe a semantic-parsing model that learns to derive quasi-logical database queries from natural language. The model follows the structure of Kwiatkowski et al. (2013), but relies on a series of neural networks and distributed representations in lieu of the CCG and λ-Calculus based representations used in that paper. The model described here borrows heavily from two approaches in the deep learning literature. First, a noise-contrastive neural network similar to that of Hermann and Blunsom (2014a, 2014b) is used to learn a joint latent representation for natural language and database queries (§3.1). Second, we employ a structured conditional neural language model in §3.2 to generate queries given such latent representations. Below we provide the necessary ba</context>
<context position="12880" citStr="Kwiatkowski et al. (2013)" startWordPosition="2075" endWordPosition="2078">enerated Query Question Figure 4: Diagrammatic representation of the final network. The question-compositional segment of the BiCVM produces a latent representation, conditioning a CNLM generating a query. takes the distributed representations of question words from unseen questions, and produces a latent representation. The latent representation is then passed to the log-bilinear CNLM, which conditionally generates a knowledgebase query corresponding to the question. 3.4 Learning Model Parameters We propose training the model of §3.3 in a two stage process, in line with the symbolic model of Kwiatkowski et al. (2013). First, a BiCVM is trained on a parallel corpus C of question-query pairs (Q, R) E C, using composition functions g for natural language questions and h for database queries. While functions g and h may differ from those discussed in Hermann and Blunsom (2014a), the basic noise-contrastive optimisation function remains the same. It is possible to initialise the model fully randomly, in which Knowledgebase query Conditional Log-bilinear Language Model Relation/object embeddings h Query embedding Question embedding Word embeddings g ... Latent representation ... 25 case the model parameters θ l</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1545–1556, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanislas Lauly</author>
<author>Alex Boulanger</author>
<author>Hugo Larochelle</author>
</authors>
<title>Learning multilingual word representations using a bag-of-words autoencoder.</title>
<date>2014</date>
<location>CoRR, abs/1401.1803.</location>
<contexts>
<context position="2013" citStr="Lauly et al., 2014" startWordPosition="304" endWordPosition="307"> et al., 2011, inter alia) has answered this call by progressively moving away from strictly rule-based semantic parsing, towards the use of distributed representations in conjunction with traditional grammatically-motivated re-write rules. This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model. It does so by adapting deep learning methods from related work in sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), document classification (Yih et al., 2011; Lauly et al., 2014; Hermann and Blunsom, 2014a), frame-semantic parsing (Hermann et al., 2014), and machine translation (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013a), inter alia, combining two empirically successful deep learning models to form a new architecture for semantic parsing. The structure of this short paper is as follows. We first provide a brief overview of the background literature this model builds on in §2. In §3, we begin by introducing two deep learning models with different aims, namely the joint learning of embeddings in parallel corpora, and the generation of strings of a language </context>
</contexts>
<marker>Lauly, Boulanger, Larochelle, 2014</marker>
<rawString>Stanislas Lauly, Alex Boulanger, and Hugo Larochelle. 2014. Learning multilingual word representations using a bag-of-words autoencoder. CoRR, abs/1401.1803.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>590--599</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1408" citStr="Liang et al., 2011" startWordPosition="213" endWordPosition="216">ed or syntactically atypical text, such as tweets, as well as permitting the development of semantic parsers for resourcepoor languages. 1 Introduction The ubiquity of always-online computers in the form of smartphones, tablets, and notebooks has boosted the demand for effective question answering systems. This is exemplified by the growing popularity of products like Apple’s Siri or Google’s Google Now services. In turn, this creates the need for increasingly sophisticated methods for semantic parsing. Recent work (Artzi and Zettlemoyer, 2013; Kwiatkowski et al., 2013; Matuszek et al., 2012; Liang et al., 2011, inter alia) has answered this call by progressively moving away from strictly rule-based semantic parsing, towards the use of distributed representations in conjunction with traditional grammatically-motivated re-write rules. This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model. It does so by adapting deep learning methods from related work in sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), document classification (Yih et al., 2011; Lauly et al.,</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 590–599, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Nicholas FitzGerald</author>
<author>Luke S Zettlemoyer</author>
<author>Liefeng Bo</author>
<author>Dieter Fox</author>
</authors>
<title>A joint model of language and perception for grounded attribute learning.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine Learning, ICML 2012,</booktitle>
<location>Edinburgh, Scotland, UK,</location>
<contexts>
<context position="1388" citStr="Matuszek et al., 2012" startWordPosition="208" endWordPosition="212">o grammatically malformed or syntactically atypical text, such as tweets, as well as permitting the development of semantic parsers for resourcepoor languages. 1 Introduction The ubiquity of always-online computers in the form of smartphones, tablets, and notebooks has boosted the demand for effective question answering systems. This is exemplified by the growing popularity of products like Apple’s Siri or Google’s Google Now services. In turn, this creates the need for increasingly sophisticated methods for semantic parsing. Recent work (Artzi and Zettlemoyer, 2013; Kwiatkowski et al., 2013; Matuszek et al., 2012; Liang et al., 2011, inter alia) has answered this call by progressively moving away from strictly rule-based semantic parsing, towards the use of distributed representations in conjunction with traditional grammatically-motivated re-write rules. This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model. It does so by adapting deep learning methods from related work in sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), document classification (Yih et al.,</context>
<context position="3489" citStr="Matuszek et al., 2012" startWordPosition="533" endWordPosition="536">de by discussing planned experiments and the data requirements to effectively train this model. 2 Background Semantic parsing describes a task within the larger field of natural language understanding. Within computational linguistics, semantic parsing is typically understood to be the task of mapping natural language sentences to formal representations of their underlying meaning. This semantic representation varies significantly depending on the task context. For instance, semantic parsing has been applied to interpreting movement instructions (Artzi and Zettlemoyer, 2013) or robot control (Matuszek et al., 2012), where the underlying representation would consist of actions. Within the context of question answering—the focus of this paper—semantic parsing typically aims to map natural language to database queries that would answer a given question. Kwiatkowski 22 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 22–27, Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics et al. (2013) approach this problem using a multistep model. First, they use a CCG-like parser to convert natural language into an underspecified logical form (ULF). Second, the ULF is </context>
</contexts>
<marker>Matuszek, FitzGerald, Zettlemoyer, Bo, Fox, 2012</marker>
<rawString>Cynthia Matuszek, Nicholas FitzGerald, Luke S. Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A joint model of language and perception for grounded attribute learning. In Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th international conference on Machine learning,</booktitle>
<pages>641--648</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8234" citStr="Mnih and Hinton (2007)" startWordPosition="1297" endWordPosition="1300">d in machine translation (Kalchbrenner and Blunsom, 2013a) has demonstrated that a particular kind of neural language model based on recurrent neural networks (Mikolov et al., 2010; Sutskever et al., 2011) could be extended so that the next word in a sequence is jointly generated by the word history and the distributed representation for a conditioning element, such as the dialogue class of a previous sentence, or the vector representation of a source sentence. In this section, we briefly describe a general formulation of conditional neural language models, based on the log-bilinear models of Mnih and Hinton (2007) due to their relative simplicity. A log-bilinear language model is a neural network modelling a probability distribution over the next word in a sequence given the previous n−1, i.e. p(wn|w1:n−1). Let |V |be the size of our vocabulary, and R be a |V |× d vocabulary matrix where the Rwi demnotes the row containing the word embedding in Rd of a word wi, with d being a hyper-parameter indicating embedding size. Let Ci be the context transform matrix in Rdxd which modifies the representation of the ith word in the word history. Let bwi be a scalar bias associated with a word wi, and bR be a bias </context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of the 24th international conference on Machine learning, pages 641–648. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Benjamin M Marlin</author>
<author>Andrew McCallum</author>
</authors>
<title>Relation extraction with matrix factorization and universal schemas.</title>
<date>2013</date>
<booktitle>In Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’13),</booktitle>
<contexts>
<context position="14065" citStr="Riedel et al. (2013)" startWordPosition="2261" endWordPosition="2264">.. 25 case the model parameters θ learned at this stage include the two distributed representation lexica for questions and queries, DQ and DR respectively, as well as all parameters for g and h. Alternatively, word embeddings in DQ could be initialised with representations learned separately, for instance with a neural language model or a similar system (Mikolov et al., 2010; Turian et al., 2010; Collobert et al., 2011, inter alia). Likewise, the relation and object embeddings in DR could be initialised with representations learned from distributed relation extraction schemas such as that of Riedel et al. (2013). Having learned representations for queries in DR as well as function g, the second training phase of the model uses a new parallel corpus consisting of pairs (g(Q), R) E C&apos; to train the CNLM as presented in §3.3. The two training steps can be applied iteratively, and further, it is trivial to modify the learning procedure to use composition function h as another input for the CNLM training phrase in an autoencoder-like setup. 4 Experimental Requirements and Further Work The particular training procedure for the model described in this paper requires aligned question/knowledgebase query pairs</context>
</contexts>
<marker>Riedel, Yao, Marlin, McCallum, 2013</marker>
<rawString>Sebastian Riedel, Limin Yao, Benjamin M. Marlin, and Andrew McCallum. 2013. Relation extraction with matrix factorization and universal schemas. In Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’13), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>1201--1211</pages>
<contexts>
<context position="1922" citStr="Socher et al., 2012" startWordPosition="289" endWordPosition="292">nt work (Artzi and Zettlemoyer, 2013; Kwiatkowski et al., 2013; Matuszek et al., 2012; Liang et al., 2011, inter alia) has answered this call by progressively moving away from strictly rule-based semantic parsing, towards the use of distributed representations in conjunction with traditional grammatically-motivated re-write rules. This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model. It does so by adapting deep learning methods from related work in sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), document classification (Yih et al., 2011; Lauly et al., 2014; Hermann and Blunsom, 2014a), frame-semantic parsing (Hermann et al., 2014), and machine translation (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013a), inter alia, combining two empirically successful deep learning models to form a new architecture for semantic parsing. The structure of this short paper is as follows. We first provide a brief overview of the background literature this model builds on in §2. In §3, we begin by introducing two deep learning models with different aims, namely the joi</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of EMNLP-CoNLL, pages 1201–1211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>James Martens</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Generating text with recurrent neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>1017--1024</pages>
<contexts>
<context position="7817" citStr="Sutskever et al., 2011" startWordPosition="1228" endWordPosition="1231">2 Conditional Neural Language Models Neural language models (Bengio et al., 2006) provide a distributed alternative to n-gram language models, permitting the joint learning of a prediction function for the next word in a sequence given the distributed representations of a subset of the last n−1 words alongside the representations themselves. Recent work in dialogue act labelling (Kalchbrenner and Blunsom, 2013b) and in machine translation (Kalchbrenner and Blunsom, 2013a) has demonstrated that a particular kind of neural language model based on recurrent neural networks (Mikolov et al., 2010; Sutskever et al., 2011) could be extended so that the next word in a sequence is jointly generated by the word history and the distributed representation for a conditioning element, such as the dialogue class of a previous sentence, or the vector representation of a source sentence. In this section, we briefly describe a general formulation of conditional neural language models, based on the log-bilinear models of Mnih and Hinton (2007) due to their relative simplicity. A log-bilinear language model is a neural network modelling a probability distribution over the next word in a sequence given the previous n−1, i.e.</context>
</contexts>
<marker>Sutskever, Martens, Hinton, 2011</marker>
<rawString>Ilya Sutskever, James Martens, and Geoffrey E Hinton. 2011. Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017–1024.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13844" citStr="Turian et al., 2010" startWordPosition="2227" endWordPosition="2230">e to initialise the model fully randomly, in which Knowledgebase query Conditional Log-bilinear Language Model Relation/object embeddings h Query embedding Question embedding Word embeddings g ... Latent representation ... 25 case the model parameters θ learned at this stage include the two distributed representation lexica for questions and queries, DQ and DR respectively, as well as all parameters for g and h. Alternatively, word embeddings in DQ could be initialised with representations learned separately, for instance with a neural language model or a similar system (Mikolov et al., 2010; Turian et al., 2010; Collobert et al., 2011, inter alia). Likewise, the relation and object embeddings in DR could be initialised with representations learned from distributed relation extraction schemas such as that of Riedel et al. (2013). Having learned representations for queries in DR as well as function g, the second training phase of the model uses a new parallel corpus consisting of pairs (g(Q), R) E C&apos; to train the CNLM as presented in §3.3. The two training steps can be applied iteratively, and further, it is trivial to modify the learning procedure to use composition function h as another input for th</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of ACL, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-Tau Yih</author>
<author>Kristina Toutanova</author>
<author>John C Platt</author>
<author>Christopher Meek</author>
</authors>
<title>Learning Discriminative Projections for Text Similarity Measures.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL ’11,</booktitle>
<pages>247--256</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1993" citStr="Yih et al., 2011" startWordPosition="300" endWordPosition="303">t al., 2012; Liang et al., 2011, inter alia) has answered this call by progressively moving away from strictly rule-based semantic parsing, towards the use of distributed representations in conjunction with traditional grammatically-motivated re-write rules. This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model. It does so by adapting deep learning methods from related work in sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), document classification (Yih et al., 2011; Lauly et al., 2014; Hermann and Blunsom, 2014a), frame-semantic parsing (Hermann et al., 2014), and machine translation (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013a), inter alia, combining two empirically successful deep learning models to form a new architecture for semantic parsing. The structure of this short paper is as follows. We first provide a brief overview of the background literature this model builds on in §2. In §3, we begin by introducing two deep learning models with different aims, namely the joint learning of embeddings in parallel corpora, and the generation of st</context>
</contexts>
<marker>Yih, Toutanova, Platt, Meek, 2011</marker>
<rawString>Wen-Tau Yih, Kristina Toutanova, John C. Platt, and Christopher Meek. 2011. Learning Discriminative Projections for Text Similarity Measures. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL ’11, pages 247–256, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="14754" citStr="Zelle and Mooney, 1996" startWordPosition="2376" endWordPosition="2379">ion g, the second training phase of the model uses a new parallel corpus consisting of pairs (g(Q), R) E C&apos; to train the CNLM as presented in §3.3. The two training steps can be applied iteratively, and further, it is trivial to modify the learning procedure to use composition function h as another input for the CNLM training phrase in an autoencoder-like setup. 4 Experimental Requirements and Further Work The particular training procedure for the model described in this paper requires aligned question/knowledgebase query pairs. There exist some small corpora that could be used for this task (Zelle and Mooney, 1996; Cai and Yates, 2013). In order to scale training beyond these small corpora, we hypothesise that larger amounts of (potentially noisy) training data could be obtained using a boot-strapping technique similar to Kwiatkowski et al. (2013). To evaluate this model, we will follow the experimental setup of Kwiatkowski et al. (2013). With the provisio that the model can generate freebase queries correctly, further work will seek to determine whether this architecture can generate other structured formal language expressions, such as lambda expressions for use in textual entailement tasks. Acknowle</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the National Conference on Artificial Intelligence, pages 1050–1055.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
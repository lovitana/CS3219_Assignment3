<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.87754">
Learning from a Neighbor: Adapting a Japanese Parser for Korean
through Feature Transfer Learning
</title>
<author confidence="0.976968">
Hiroshi Kanayama
</author>
<affiliation confidence="0.803989">
IBM Research - Tokyo
Koto-ku, Tokyo, Japan
</affiliation>
<email confidence="0.99447">
hkana@jp.ibm.com
</email>
<author confidence="0.98788">
Yuta Tsuboi
</author>
<affiliation confidence="0.817413">
IBM Research - Tokyo
Koto-ku, Tokyo, Japan
</affiliation>
<email confidence="0.995809">
yutat@jp.ibm.com
</email>
<author confidence="0.918145">
Youngja Park
</author>
<affiliation confidence="0.870565">
IBM Research - T.J.Watson Research Center
Yorktown Heights, NY, USA
</affiliation>
<email confidence="0.970488">
young park@us.ibm.com
</email>
<author confidence="0.995923">
Dongmook Yi
</author>
<affiliation confidence="0.995946">
Korea Software Solutions Laboratory, IBM Korea
</affiliation>
<address confidence="0.780235">
Gangnam-gu, Seoul, Korea
</address>
<email confidence="0.998212">
dmyi@kr.ibm.com
</email>
<sectionHeader confidence="0.998597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999615">
We present a new dependency parsing
method for Korean applying cross-lingual
transfer learning and domain adaptation
techniques. Unlike existing transfer learn-
ing methods relying on aligned corpora or
bilingual lexicons, we propose a feature
transfer learning method with minimal su-
pervision, which adapts an existing parser
to the target language by transferring the
features for the source language to the tar-
get language. Specifically, we utilize the
Triplet/Quadruplet Model, a hybrid pars-
ing algorithm for Japanese, and apply a
delexicalized feature transfer for Korean.
Experiments with Penn Korean Treebank
show that even using only the transferred
features from Japanese achieves a high
accuracy (81.6%) for Korean dependency
parsing. Further improvements were ob-
tained when a small annotated Korean cor-
pus was combined with the Japanese train-
ing corpus, confirming that efficient cross-
lingual transfer learning can be achieved
without expensive linguistic resources.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999665085106383">
Motivated by increasing demands for advanced
natural language processing (NLP) applications
such as sentiment analysis (Pang et al., 2002;
Nasukawa and Yi, 2003) and question answer-
ing (Kwok et al., 2001; Ferrucci et al., 2010), there
is a growing need for accurate syntactic parsing
and semantic analysis of languages, especially for
non-English languages with limited linguistic re-
sources. In this paper, we propose a new depen-
dency parsing method for Korean which requires
minimal human supervision. Dependency parsing
can handle long-distance relationships and coor-
dination phenomena very well, and has proven to
be very effective for parsing free-order languages
such as Korean and Japanese (K¨ubler et al., 2009).
Most statistical parsing methods rely on anno-
tated corpora labeled with phrase structures or
dependency relationships, but it is very expen-
sive to create a large number of consistent anno-
tations. Recently, treebanks have become avail-
able for many languages such as English, Ger-
man, Arabic, and Chinese. However, the pars-
ing results on these treebanks vary a lot depend-
ing on the size of annotated sentences and the type
of annotations (Levy and Manning, 2003; Mc-
Donald et al., 2013). Further, many languages
lack annotated corpus, or the size of the anno-
tated corpus is too small to develop a reliable sta-
tistical method. To address these problems, there
have been several attempts at unsupervised pars-
ing (Seginer, 2007; Spitkovsky et al., 2011), gram-
mar induction (Klein and Manning, 2004; Naseem
et al., 2010), and cross-lingual transfer learning
using annotated corpora of other languages (Mc-
Donald et al., 2011). However, the accuracies of
unsupervised methods are unacceptably low, and
results from cross-lingual transfer learning show
different outcomes for different pairs of languages,
but, in most cases, the parsing accuracy is still low
for practical purposes. A recent study by McDon-
ald et al. (2013) concludes that cross-lingual trans-
fer learning is beneficial when the source and tar-
get languages were similar. In particular, it reports
that Korean is an outlier with the lowest scores
(42% or less in UAS) when a model was trained
from European languages.
In this paper, we present a new cross-lingual
</bodyText>
<page confidence="0.920344">
2
</page>
<note confidence="0.6501855">
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 2–12,
October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999941981481482">
transfer learning method that learns a new model
for the target language by transferring the fea-
tures for the source language. Unlike other ap-
proaches which rely on aligned corpora or a
bilingual lexicon, we learn a parsing model for
Korean by reusing the features and annotated
data used in the Japanese dependency parsing,
the Triplet/Quadruplet Model (Kanayama et al.,
2000), which is a hybrid approach utilizing both
grammatical knowledge and statistics.
We exploit many similarities between the two
languages, such as the head-final structure, the
noun to verb modification via case and topic mark-
ers, and the similar word-order constraints. It was
reported that the mapping of the grammar formal-
ism in the language pair was relatively easy (Kim
et al., 2003b; Kim et al., 2003a). However, as the
two languages are classified into independent lan-
guage families (Gordon and Grimes, 2005), there
are many significant differences in their morphol-
ogy and grammar (especially in the writing sys-
tems), so it is not trivial to handle the two lan-
guages in a uniform way.
We show the Triplet/Quadruplet Model is suit-
able for bilingual transfer learning, because the
grammar rules and heuristics reduce the number
of modification candidates and can mitigate the
differences between two languages efficiently. In
addition, this model can handle the relationships
among the candidates as a richer feature space,
making the model less dependent upon the lexi-
cal features of the content words that are difficult
to align between the two languages. Similarly to
the delexicalized parsing model in (McDonald et
al., 2011), we transfer only part-of-speech infor-
mation of the features for the content words. We
create new mapping rules to extract syntactic fea-
tures for Korean parsing from the Japanese anno-
tated corpus and refine the grammar rules to get
closer modification distributions in two languages.
Our experiments with Penn Korean Tree-
bank (Han et al., 2002) confirm that the
Triplet/Quadruplet Model adapted for Korean out-
performs a distance-based dependency parsing
method, achieving 81.6% accuracy when no an-
notated Korean corpus was used. Further perfor-
mance improvements were obtained when a small
size of annotated Korean corpus was added, con-
firming that our algorithm can be applied with-
out more expensive linguistic resources such as an
aligned corpora or bilingual lexicons. Moreover,
the delexicalized feature transfer method enables
the algorithm applicable to any two languages that
have similar syntactic structures.
</bodyText>
<sectionHeader confidence="0.999932" genericHeader="introduction">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.995711">
2.1 Parsing for Korean
</subsectionHeader>
<bodyText confidence="0.999957842105263">
Since Korean is a morphologically-rich language,
many efforts for Korean parsing have focused
on automatically extracting rich lexical informa-
tion such as the use of case frame patterns for
the verbs (Lee et al., 2007), the acquisition of
case frames and nominal phrases from raw cor-
pora (Park et al., 2013), and effective features
from phrases and their neighboring contexts (Choi
and Palmer, 2011). Recently, Choi et al. (2012)
discussed the transformation of eojeol-based Ko-
rean treebank to entity-based treebank to effec-
tively train probabilistic CFG parsers. We apply
similar techniques as in (Choi et al., 2012) to miti-
gate the differences between Korean and Japanese
syntactic structures.
Chung and Rim (2003) applied the
Triplet/Quadruplet Model for Korean parsing
as done in our work. They reported that the model
performed well for long-distance dependencies,
but, in their experiments, the number of modi-
fication candidates was not effectively reduced
(only 91.5% of phrases were in one of the three
positions, while it was 98.6% in Kanayama’s
work for Japanese). In this paper, we introduce
more sophisticated grammatical knowledge and
heuristics to have similar dependency distribu-
tions in the two languages. Smith and Smith
(2004) attempted a bilingual parsing for English
and Korean by combining statistical dependency
parsers, probabilistic context-free grammars, and
word translation models into a unified framework
that jointly searches for the best English parse,
Korean parse and word alignment. However, we
utilize an existing parser and align the features
from the source language to the features for
the target language, and, thus, our method is
applicable to situations where there is no aligned
corpora or word translation models.
</bodyText>
<subsectionHeader confidence="0.999406">
2.2 Transfer learning and domain adaptation
</subsectionHeader>
<bodyText confidence="0.9997472">
Recently, transfer learning has attracted much at-
tention, as it can overcome the lack of training
data for new languages or new domains for both
classification and regression tasks (Pan and Yang,
2010). Transfer learning has also been applied to
</bodyText>
<page confidence="0.998416">
3
</page>
<table confidence="0.991822333333333">
�} R*::�:°l °1_1 711--&amp;-
h}/VV _/EAN aP&amp;&apos;—&apos;/NPR 91/PAN W-I/NNC 7}1t/NNC -&amp;/PCA
‘buy-PAST’ ‘France-GEN’ ‘travel’ ‘bag-ACC’
J01- AlV4 �
_V I/VV aa/ECS I/VX c}/EFN ./SFN
‘show’ ‘want’ ‘.’
</table>
<equation confidence="0.855647428571429">
01471
o}1A/NNC I/PCA
‘wife-NOM’
il-7O11X11
l-7/NNC q7fl/PAD
‘friend-DAT’
e1 e2 e3 e4 e5 es e7 es eg
</equation>
<figureCaption confidence="0.965109666666667">
Figure 1: An example of dependency structures of a Korean sentence “447} + —ifo 91 01a� 7}%
EE L�-71711 .V-&amp;quot;l-�? -+.” (‘(I) want to show the French travel bag which (my) wife bought to (my)
friend’). Each box corresponds to a Korean phrasal unit eojeol.
</figureCaption>
<figure confidence="0.999172857142857">
��
/n ffi/pc
‘wife-NOM’
R-Df 7&apos;7✓A0) )ATr3)aUh7�_-
$/v f/aux 77✓A/np OJ/pn RfT/n ;6�rXJv/n Vpc
‘buy-PAST’ ‘France-GEN’ ‘travel bag-ACC’
WM,
&amp;1I/n lVpc
‘friend-DAT’
JQ-tl-fv�
A-/v fV N/aux �/
‘want to show’
I V V
b1 b2 b3 b4 b5 bs
</figure>
<figureCaption confidence="0.990883">
Figure 2: A dependency structure of a Japanese sentence which corresponds to the Korean sentence in
Figure 1, “妻が買ったフランスの旅行かばんを友達に見せたい。”. Each box corresponds to a Japanese
phrasal unit bunsetsu.
</figureCaption>
<bodyText confidence="0.999778294117647">
syntactic parsing, where a parsing model for a tar-
get language is learned from linguistic resources
in one or more different languages (Hwa et al.,
2005; Zeman and Resnik, 2008; McDonald et al.,
2011; Durrett et al., 2012; Georgi et al., 2012;
Naseem et al., 2012). McDonald et al. (2011)
proposed a delexicalized parsing model for cross-
lingual dependency parsing and demonstrated that
a high accuracy parsing was achieved for Indo-
European languages where significant amount of
parallel texts exist. However, in more recent work,
McDonald et al. (2013) showed that, unlike trans-
fer learning within close language families, build-
ing a Korean parser from European languages was
not successful with a very low accuracy. Durrett
et al. (2012) and Georgi et al. (2012) show that
transfer parsing can be improved when additional
bilingual resources are available, such as bilingual
dictionaries and parallel corpora of glossed texts
respectively.
Our method does not require such resources and
does not have restrictions on the sentence type that
can be parsed. Instead, we use a mixture of a
small corpus in a target language (i.e., Korean) and
a larger corpus of a source language (Japanese).
This task is similar to domain adaptation, and our
objective is to outperform the training model built
on each language separately. To avoid the loss of
accuracy due to the differences between two do-
mains, we apply the domain adaptation technique
proposed by Daum´e III (2007) which duplicates
the feature space into three categories with each
of the features trained by source, by target, and by
combined domains.
</bodyText>
<sectionHeader confidence="0.9544435" genericHeader="method">
3 Dependency Structures of Korean and
Japanese
</sectionHeader>
<bodyText confidence="0.999934777777778">
A dependency structure in Korean is typically an-
alyzed in terms of eojeol units, a basic phrase
that consists of a content word agglutinated with
optional functional morphemes such as postposi-
tional particles or endings for verbs. Figure 1
shows an example Korean sentence with the de-
pendencies between the eojeols indicated by ar-
rows. Figure 2 illustrates the dependency struc-
ture between the bunsetsus in Japanese that cor-
responds to the Korean structure in Figure 1. As
these figures show, the syntactic structures are
quite similar in these languages: All of the de-
pendencies are directed from left to right, and the
postpositional particles determine if the content
word modifies a verb (“7}” in e1 and “が” in b1)
or a noun (“91” in e3 and “の” in b3). The eojeols
in Korean roughly correspond to the bunsetsus in
Japanese. In the remainder of this paper, we de-
note both an eojeol or a bunsetsu as a ‘PU’ (phrasal
unit) when distinction is not needed.
While Korean and Japanese have similar syn-
tactic structures, the two languages have many dif-
ferences. The eojeols in Korean are separated by
white space, while the bunsetsus in Japanese are
not. Further, the statistics show several differences
in the two languages. Table 1 compares a Korean
corpus, Penn Korean Treebank (henceforth KTB)
</bodyText>
<page confidence="0.998779">
4
</page>
<tableCaption confidence="0.999805">
Table 1: Statistics of Korean and Japanese corpora.
</tableCaption>
<table confidence="0.9988286">
KTB (Korean) EDR (Japanese)
Average number of characters (except for whitespace) per sentence 73.7 28.0
Average number of PUs per sentence 25.5 8.53
Average number of morphemes per PU 1.83 2.86
Ratio of modification to the next PU 70.0% 61.8%
</table>
<tableCaption confidence="0.999522">
Table 2: Simplified examples of Japanese grammar rules.
</tableCaption>
<bodyText confidence="0.94556062962963">
Rightmost morpheme of the modifier PU Conditions for the modified PUs
postpositional “�” wo (accusative) verb, adjective
postpositional “O)” no (genitive, nominative) noun, verb, adjective
postpositional “�” to (conjunctive) noun, verb, adjective, adverb “—r`f6�” isshoni (‘together’)
adverb verb, adjective, adverb, copula
(Han et al., 2002), and a Japanese corpus, EDR
Corpus (EDR, 1996). Both corpora consist of
word-level bracketed constituents, so they are con-
verted into PU-level dependency structures using
the method described in Choi and Palmer (2011).
Though both corpora consist mainly of newspaper
or magazine articles, the sentences are not aligned
with each other, so the statistics show the compar-
isons of the two corpora, rather than the theoret-
ical comparisons of the two languages. However,
we can see that Korean sentences tend to be longer
than Japanese sentences both in terms of the num-
ber of characters and PUs. More eojeols modify an
adjacent eojeol in Korean than in Japanese. For in-
stance, e1, e4, e6, e7, and e8 modify the next eojeol
in Figure 1, but only b1, b3, and b5 modify the next
bunsetsu in Figure 2. Those differences suggest
some of the difficulties in applying the Japanese
dependency model to Korean. The Japanese pars-
ing method that will be described in the next sec-
tion exploits these characteristics, which we apply
to Korean parsing.
</bodyText>
<sectionHeader confidence="0.999845" genericHeader="method">
4 Triplet/Quadruplet Model
</sectionHeader>
<bodyText confidence="0.999986375">
This section describes the Triplet/Quadruplet
Model (Kanayama et al., 2000) which was origi-
nally designed for Japanese parsing. First, we re-
view the two main ideas of the model – restriction
of modification candidates and feature selection
for probability calculation. Then, we describe how
we apply the Triplet/Quadruplet Model to Korean
parsing in Section 4.3.
</bodyText>
<subsectionHeader confidence="0.998392">
4.1 Restriction of modification candidates
</subsectionHeader>
<bodyText confidence="0.999288913043478">
The Triplet/Quadruplet Model utilizes a small
number (about 50) of hand-crafted grammar rules
that determine whether a PU can modify each PU
to its right in a sentence. The main goal of the
grammar rules is to maximize the coverage, and
the rules are simple describing high-level syntac-
tic dependencies, and, thus, the rules can be eas-
ily created without worrying about the precision
or contradictory rules. The statistical information
is later used to select the right rules for a given
sentence to produce an accurate parsing result. Ta-
ble 2 shows several grammar rules for Japanese, in
which the modified PUs are determined depend-
ing on the conditions of the rightmost morpheme
in the modifier PU.
An analysis of the EDR corpus shows that
98.6% of the correct dependencies are either the
nearest PU, the second nearest PU, or the farthest
PU from the modifier (more details in Table 4(a)).
Therefore, the model can be simplified by restrict-
ing the candidates to these three candidates and
by ignoring the other PUs with a small sacrifice
(1.4%) of parsing accuracy.
</bodyText>
<subsectionHeader confidence="0.999676">
4.2 Calculation of modification probabilities
</subsectionHeader>
<bodyText confidence="0.996470285714286">
Let u be a modifier PU in question, cun the u’s n-
th modification candidate PU, Φu and Xpcun the at-
tributes of u and cun, respectively. Then the prob-
ability that u modifies its n-th candidate is calcu-
lated by the triplet equation (1) or the quadruplet
equation (2) when u has two or three candidates,
respectively 1.
</bodyText>
<equation confidence="0.9997415">
P(u → cun) = P(n  |Φu,Xpcul, Xpcu2) (1)
P(u → cun) = P(n  |Φu, Xpcul, Xpcu2, Xpcul) (2)
</equation>
<footnote confidence="0.6339635">
&apos;It is trivial to show that P(u → cul) = 1, when u has
only one candidate.
</footnote>
<page confidence="0.99699">
5
</page>
<tableCaption confidence="0.999663">
Table 3: Simplified examples of Korean grammar rules.
</tableCaption>
<table confidence="0.6536475">
Rightmost morpheme of the modifier PU Conditions for the modified PUs
PCA,PCJ,PAD,PAU (postpositional particles) V&amp;quot; (verb, adjective or auxiliary), CO (copula)
EAN (nominal verb ending e.g. “은” eun) N&amp;quot; (noun)
ADV (adverb), ADC (conjunction) N&amp;quot; (noun), V&amp;quot; (verb, adjective or auxiliary), ADV (adverb), ADC (conjunction)
postpositional “과” gwa (conjunctive) N&amp;quot; (noun), V&amp;quot; (verb, adjective or aux), adverb “함께” hamkke (‘together’)
N&amp;quot; (noun) N&amp;quot; (noun), V&amp;quot; (verb, adjective or auxiliary)
</table>
<tableCaption confidence="0.992731">
Table 4: Distribution (percentage) of the position of the correct modified PU among the candidate PUs
selected by the initial grammar rules. The column ‘Sum’ shows the coverage of the 1st, 2nd and last
PUs. The EDR corpus was used for Japanese, and the KTB was used for Korean in this analysis.
</tableCaption>
<table confidence="0.997797875">
(a) Japanese (b) Korean (with the initial grammar)
# of Candidates Ratio 1st 2nd Last Sum # of Candidates Ratio 1st 2nd Last Sum
1 32.7 100.0 − − 100.0 1 10.5 100.0 − − 100.0
2 28.1 74.3 26.7 − 100.0 2 11.4 85.9 14.1 − 100.0
3 17.5 70.6 12.6 16.8 100.0 3 10.4 76.2 13.4 10.4 100.0
4 9.9 70.4 11.1 13.8 95.3 4 9.3 74.7 11.3 8.0 93.9
&gt;5 11.8 70.2 11.1 10.5 91.9 &gt;5 58.4 75.5 10.0 4.9 90.5
Total 100 − − − 98.6 Total 100 − − − 93.9
</table>
<bodyText confidence="0.999703888888889">
These probabilities are estimated by the maxi-
mum entropy method with a feature set to express
Φ and Ψ. Assuming the independence of those
modifications, the probability of the dependency
tree for an entire sentence P(T) is calculated as
the product of the probabilities of all of the depen-
dencies in the sentence using beam search to max-
imize P(T) under the constraints of the projected
structure.
</bodyText>
<equation confidence="0.932505333333333">
∏
P(T) ≃ P(u → cu,,) (3)
u
</equation>
<bodyText confidence="0.997986">
In comparison, a traditional statistical parser
(Collins, 1997) uses Equation (4) to calculate the
probability of u modifying t.
</bodyText>
<equation confidence="0.998178">
P(u → t) = P(True  |Φu, Ψt, ∆u,t) (4)
</equation>
<bodyText confidence="0.999940888888889">
We call the model based on Equation (4) the Dis-
tance Model, since ∆u,t (the distance between u
and t) is typically used as the key feature. Though
other contextual information, in addition to the at-
tributes of u and t, can be added, the model calcu-
lates the probabilities of the dependencies between
u and t independently and thus often fails to incor-
porate appropriate contextual information.
Equations (1) and (2) have two major advan-
tages over the Distance Model: First, all the at-
tributes of the modifier and its candidates can be
handled simultaneously. The combination of those
attributes helps the model to express the context of
the modifications. Second, the probability of each
modification is calculated based on the relative po-
sitions of the candidates, instead of the distance
from the modifier PU in the surface sentence, and,
thus, the model is more robust.
</bodyText>
<subsectionHeader confidence="0.884868">
4.3 Korean dependency parsing with the
Triplet/Quadruplet Model
</subsectionHeader>
<bodyText confidence="0.999905818181818">
We design the Korean parser by adapting the
Triplet/Quadruplet Model based on the analogous
characteristics of Japanese and Korean. First, we
created the Korean grammar rules for generating
candidate modified PUs by modifying the rules
for Japanese shown in Table 2 for Korean. The
rule set, containing fewer than 50 rules, is sim-
ple enough to be created manually, because the
rules simply describe possible dependencies, and
Japanese phenomena are good hints for Korean
phenomena. Table 3 shows some examples of the
rules for Korean based on the POS schema used in
the KTB corpus. We did not automatically extract
the rules from the annotated corpora so that the
rules are general and independent of the training
corpus. Nonetheless, 96.6% of the dependencies
in KTB are covered by the grammar rules. The re-
maining dependencies (3.4%) not covered by the
rule set are mainly due to rare modifications and
may indicate inconsistencies in the annotations,
so we do not seek any grammar rules to achieve
nearly 100%.
</bodyText>
<page confidence="0.942867">
6
</page>
<equation confidence="0.623759">
(1) NNC (common noun) (5) VV (verb) (5) VV (verb) (5) VX (auxiliary)
(2) PCA (postpositional) (7) EAN (adnominal ending) (7) ECS (ending) (7) EFN (final ending)
(3)“이” (‘-NOM’) (8)“은” (past adnominal) (8)“고” (conjunctive) (8)“다” (predicative)
아내가 (‘wife-NOM’)
e1 e2 e7 es
</equation>
<figureCaption confidence="0.7280605">
Figure 3: The features used to select the modified PU of e1 among its three candidates. The full sentence
of this example is shown in Figure 1. The numbers in brackets correspond to the feature IDs in Table 5.
</figureCaption>
<tableCaption confidence="0.959388">
Table 5: The features to express attributes of a modifier and modification candidates.
</tableCaption>
<table confidence="0.539921">
Feature set ID Description
</table>
<listItem confidence="0.999439090909091">
(1) PoS of the head morpheme of the modifier
(2) PoS of the last morpheme of the modifier
(3) Lex of the postpositional or endings of the modifier
(4) Lex of the adverb of the modifier
(5) PoS of the head morpheme of the modification candidate
(6) Lex of the head morpheme of the modification candidate
(7) PoS of the last morpheme of the modification candidate
(8) Lex of the postpositional or endings of the modification candidate
(9) Existence of a quotation expression “다고” dago or “라고” rago
(10) Number of “은” eun (TOPIC marker) between the modifier and modification candidate
(11) Number of commas between the modifier and modification candidate
</listItem>
<equation confidence="0.99857">
combination (1) x (5) / (2) x (5) / (2) x (7) / (3) x (5) / (3) x (8)
V v v
산 (‘buy’)
보이고 (‘show’)
싶다 (‘want’)
</equation>
<bodyText confidence="0.999744851851852">
Table 4(a) and (b) show the distribution of the
numbers of candidate PUs and the position of the
correct modified PUs obtained from the analysis
of the EDR corpus and the KTB corpus respec-
tively. As we can see, the first candidate is pre-
ferred in both languages, but the preference of the
nearer candidate is stronger in Korean. For in-
stance, when there are more than one candidates,
the probability that the first candidate is the cor-
rect one is 78% for Korean but 71% for Japanese.
Further, when there are more than 2 candidates,
Japanese prefers the last candidate, while Korean
prefers the second candidate. Based on the analy-
sis results, the number of modification candidates
is restricted to at most three (the first, second and
last candidates) for Korean as well.
The next step is to design Φ� and Ψ���, which
are required in Equations (1) and (2) to choose the
correct modified PU. We converted the feature set
from the Japanese study to get the Korean features
as listed in Table 5. For example, to find the mod-
ified PU of e1 “아내가” anae-ga (’wife-NOM’) in
the sentence shown in Figure 1, the attributes of
e1 and the attributes of the three candidates, e2,
e7, and e8, are extracted as shown in Figure 3, and
their attributes are used to estimate the probability
of each candidate in Equation (2).
</bodyText>
<sectionHeader confidence="0.823929" genericHeader="method">
5 Adaptation for Bilingual Transfer
Learning
</sectionHeader>
<bodyText confidence="0.999968363636364">
In Section 4.3, we explained how the
Triplet/Quadruplet Model can be used for
Korean. In this section, we describe the feature
adaption techniques in more detail and investigate
if the new model with transferred features works
well when a small amount of annotated corpus for
the target language is provided. Further, we study
if we can leverage the annotated corpus for the
source language in addition to the parsing model
and train a model for the target language using the
training data for the source language.
</bodyText>
<subsectionHeader confidence="0.998699">
5.1 Feature Transfer
</subsectionHeader>
<bodyText confidence="0.998676">
With the assumption that Korean and Japanese
have similar syntactic dependencies, we adopt
the delexicalized parsing model presented in Mc-
Donald et al. (2011). We transfer the part-of-
speech (POS) in the Japanese features to the POS
scheme in the KTB corpus, and translate Japanese
functional words to the corresponding functional
words in Korean. This transfer process is manda-
tory because we use the language specific POS
systems to capture language-specific dependency
phenomena, unlike other works using language
universal but coarser POS systems.
We do not transfer lexical knowledge on con-
</bodyText>
<page confidence="0.999848">
7
</page>
<tableCaption confidence="0.99584">
Table 6: Example of mapping rules for parts-of-speech and functional words.
</tableCaption>
<figure confidence="0.9815278">
Japanese PoS Korean PoS
common noun NNC
verb VV
adjective VJ
nominal suf~x XSF
“で”,“に”,“へ”,“から” PAD
case particle
others PCA
Japanese particle Korean particle
“を” wo (‘-ACC’) “z” eul
“よh” yori (‘from’) “�Ej” buteo
“は” ha (‘-TOPIC’) “_q.” eun
“も” mo (‘too’) “SI” do
“が” ga case particle (‘-NOM’) “01” i
conjunctive particle (‘but’) “-Kl°}” jiman
</figure>
<bodyText confidence="0.987764833333333">
tent words and exceptional cases, so feature sets
(4) and (6) are not transferred. Table 6 shows
some examples of the feature transfer which han-
dle POS tags and functional words. We note that
the Korean features shown in Figure 3 are directly
extracted from Japanese corpus using those rules.
</bodyText>
<subsectionHeader confidence="0.99981">
5.2 Adaptation of parsing rules
</subsectionHeader>
<bodyText confidence="0.999803931818182">
While Japanese and Korean are similar in terms of
syntactic dependencies, there are significant dif-
ferences between the two languages in the distri-
bution of modification as shown in the Table 4(a)
and (b): In Korean, more than half of modifiers
have 5 or more candidates, while only 12% of
Japanese modifiers do. In Japanese, 98.6% of cor-
rect modified PUs are located in one of the three
positions (1st, 2nd or last), but, in Korean, the ratio
falls to 93.9% as shown in Table 4. Another ma-
jor difference of the two languages is the different
average numbers of PUs per sentence as shown in
Table 1. Korean has 25.5 PUs per sentence, while
the number is only 8.5 in Japanese. This is mainly
caused by the difference between eojeol in Korean
and bunsetsu in Japanese. In Japanese, compound
nouns and verb phrases with an auxiliary verb are
likely to form a single bunsetsu, while, in Korean,
they are split into multiple eojeols with a whites-
pace in-between.
These differences significantly reduce the ef-
fect of transfer learning. To address these prob-
lems, we further refine the grammar rules as in
the following. We added heuristic rules for the
Korean model to effectively reduce the number of
candidates in compound nouns which consist of a
noun sequence in multiple eojeols, and verbs or
adjectives followed by auxiliary verbs. Figure 4
shows an algorithm to reduce the number of mod-
ified PUs considering the structure of compound
nouns. In this example, both PUs e4 (“travel”) and
e5 (“bag-ACC”) can be candidate PUs for eojeol
e3. However, based on the rule in Figure 4, e4 and
e5 are considered as a compound noun (line 1),
and e4 is determined to modify e5 (line 3). Sub-
sequently, e3’s modifiability to e4 is rejected (line
5), and, thus, the correct modified PU of e3 is de-
termined as e5. After refining the rules for com-
pound nouns and auxiliary verbs, the probability
of the correct modified PU being the 1st, 2nd or
last candidate PU increases from 93.9% to 96.3%
as shown in Table 7, and the distribution of the
candidate’s positions for Korean became closer to
the Japanese distribution shown in Table 4(a).
</bodyText>
<subsectionHeader confidence="0.8740635">
5.3 Learning from heterogeneous bilingual
corpora
</subsectionHeader>
<bodyText confidence="0.9999864375">
The feature transfer and rule adaptation methods
described in previous sections generate a very ac-
curate Korean parser using only a Japanese cor-
pus as shown in the first row in Table 8. The next
question is if we can leverage bilingual corpora to
further improve the accuracy, when annotated cor-
pus for the target language (Korean) is available.
We note that the two corpora do not need to be
aligned and can come from different domains. To
mitigate the side effects of merging heterogeneous
training data in different languages, we apply the
domain adaptation method proposed by Daum´e III
(2007) and augment the feature set to a source
language-specific version, a target-specific version
and a general version. Specifically, a feature set x
in Table 5 is expanded as follows:
</bodyText>
<equation confidence="0.999523">
xK=&lt; x, 0, x &gt; (5)
xJ=&lt; 0, x, x &gt; (6)
</equation>
<bodyText confidence="0.999794857142857">
where xK and xJ denote the feature sets extracted
from the Korean corpus and the Japanese corpus
respectively. Then, the features specific to Korean
and Japanese get higher weights for the first part
or the second part respectively, and the character-
istics existing in both languages influence the last
part.
</bodyText>
<page confidence="0.989522">
8
</page>
<figure confidence="0.981172611111111">
if PoS of ui’s last morpheme is N&amp;quot; and PoS of ui+1’s first morpheme is N&amp;quot;
then
ui must modify ui+1
if ui−1’s last morpheme is not “=A” then ui−1 cannot modify ui+1
else ui−1 cannot modify ui
u1 to ui−2 cannot modify ui
3f*::�:I
!P&amp;?1/NPR 91/PAN
‘France-GEN’
CII
W-I/NNC
‘travel’
711--&amp;-
7}I/NNC z/PCA
‘bag-ACC’
e3 e4 e5
IF
11 IF
</figure>
<figureCaption confidence="0.977568">
Figure 4: Heuristic rules to reduce the number of modification candidates surrounding compound nouns
in Korean. The example in the right figure shows that candidates in the dotted lines are removed by the
heuristics.
</figureCaption>
<tableCaption confidence="0.5317715">
Table 7: Distribution of the position of correct modified PU for Korean after the refinement of the Korean
grammar rules.
</tableCaption>
<table confidence="0.999783857142857">
# of candidates Ratio 1st 2nd Last Sum
1 46.4% 100.0% − − 100.0%
2 9.8% 79.0% 21.0% − 100.0%
3 9.2% 75.5% 12.7% 11.8% 100.0%
4 8.0% 71.0% 11.8% 9.6% 92.4%
&gt; 5 26.6% 70.4% 10.1% 7.8% 88.3%
Total 100% − − − 96.3%
</table>
<sectionHeader confidence="0.998986" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999961285714286">
In this section, we validate the effectiveness of
learning a Korean parser using the feature transfer
learning from the Japanese parser and compare the
Korean model with other baseline cases. We also
compare the parsing results when various sizes of
bilingual corpora were used to train the Korean
model.
</bodyText>
<subsectionHeader confidence="0.973138">
6.1 Korean parsing using the
Triplet/Quadruplet Model
</subsectionHeader>
<bodyText confidence="0.999990857142857">
First, to validate the effectiveness of the
Triplet/Quadruplet Model for parsing Korean, we
built eight Korean dependency parsing models us-
ing different numbers of training sentences for Ko-
rean. The KTB corpus Version 2.0 (Han et al.,
2002) containing 5,010 annotated sentences was
used in this study. We first divide the corpus into 5
subsets by putting each sentence into its (sentence
ID mod 5)-th group. We use sentences from the
first subgroup for estimating the parameters, sen-
tences from the second subgroup for testing, and
use the remaining three subgroups for training. We
built 8 models in total, using from 0 sentence up
to 3,006 sentences selected from the training set.
The number of training sentences in each model
is shown in the first column in Table 8. The pa-
rameters were estimated by the maximum entropy
method, and the most preferable tree is selected
using each dependency probability and the beam
search. The test data set contains 1,043 sentences.
We compare the Triplet/Quadruplet Model-
based models with the Distance Model. For the
Distance Model, we used the same feature set as
in Table 5, and added the distance feature (∆,,�t)
by grouping the distance between two PUs into 3
categories (1, 2 to 5, and 6 or more). The perfor-
mances are measured by UAS (unlabeled attach-
ment score), and the results of the two methods
are shown in the second column, where Japanese
Corpus Size=0, in Table 8 (a) and (b) respectively.
The top leftmost cells (80.61% and 71.63%) show
the parsing accuracies without any training cor-
pora. In these cases the nearest candidate PU is
selected as the modified PU. The difference be-
tween two models suggests the effect of restriction
of modification candidates by the grammar rules.
We note that the Triplet/Quadruplet Model pro-
duces more accurate results and outperforms the
Distance Model by more than 2 percentage points
in all cases. The results confirm that the method
for Japanese parsing is suitable for Korean pars-
ing.
</bodyText>
<subsectionHeader confidence="0.998643">
6.2 Results of bilingual transfer learning
</subsectionHeader>
<bodyText confidence="0.999937">
Next, we evaluate the transfer learning when anno-
tated sentences for Japanese were also added. Ta-
ble 8(a) shows the accuracies of our model when
various numbers of training sentences from Ko-
rean and Japanese are used. The first row shows
the accuracies of Korean parsing when the models
were trained only with the Japanese corpus, and
</bodyText>
<page confidence="0.998432">
9
</page>
<tableCaption confidence="0.977732333333333">
Table 8: The accuracy of Korean dependency parsing with various numbers of annotated sentences in the
two languages. † denotes that the mixture of bilingual corpora significantly outperformed (p &lt; .05)
the parser trained with only the Korean corpus without Japanese corpus.
</tableCaption>
<table confidence="0.946003954545455">
(a) Triplet/Quadruplet Model (b) Distance Model
Japanese Corpus Size Japanese Corpus Size
0 2,500 5,000 10,000
0 80.61% 80.78% 81.23% † 81.58% †
50 82.21% 82.32% 82.40% † 82.43% †
98 82.36% 82.66% † 82.69% † 82.70% †
197 83.13% 83.18% 83.30% † 83.28%
383 83.62% 83.92% † 83.94% † 83.91% †
750 84.03% 84.00% 84.06% 84.06%
1,502 84.41% 84.34% 84.32% 84.28%
3,006 84.77% 84.64% 84.64% 84.65%
0 2,500 5,000
0 71.63% 62.42% 54.92%
50 79.31% 79.55% † 79.54% †
98 80.53% 80.63% † 80.72% †
197 80.91% 80.84% 80.85%
383 81.86% 81.75% 81.76%
750 82.10% 81.92% 81.94%
1,502 82.50% 82.48% 82.50%
3,006 82.66% 82.57% 82.54%
Korean Corpus Size
Korean Corpus Size
</table>
<bodyText confidence="0.999984133333333">
other rows show the results when the Korean and
Japanese corpora were mixed using the method
described in Section 5.3.
As we can see from the results, the bene-
fit of transfer learning is larger when the size
of the annotated corpus for Korean (i.e., target
language) is smaller. In our experiments with
Triplet/Quadruplet Model, positive results were
obtained by the mixture of the two languages when
the Korean corpus is less than 500 sentences, that
is, the annotations in the source language success-
fully compensated the small corpus of the target
language. When the size of the Korean corpus is
relatively large (≥ 1, 500 sentences), adding the
Japanese corpus decreased the accuracy slightly,
due to syntactic differences between the two lan-
guages. Also the effect of the corpus from the
source language tends to saturate as the size of
the source corpus, when the target corpus is larger.
This is mainly because our mapping rules ignore
lexical features, so few new features found in the
larger corpus were incorrectly processed.
When merging the corpus in two languages,
if we simply concatenate the transferred features
from the source language and the features from
the target language (instead of using the dupli-
cated features shown in Equations (5) and (6)), the
accuracy dropped from 82.70% to 82.26% when
the Korean corpus size was 98 and Japanese cor-
pus size was 10,000, and from 83.91% to 83.40%
when Korean=383. These results support that
there are significant differences in the dependen-
cies between two languages even if we have im-
proved the feature mapping, and our approach
with the domain adaptation technique (Daum´e III,
2007) successfully solved the difficulty.
Table 8(b) shows the results of the Distance
Model. As we can see from the first row, using
only the Japanese corpus did not help the Dis-
tance Model at all in this case. The Distance
Model was not able to mitigate the differences be-
tween the two languages, because it does not use
any grammatical rules to control the modifiability.
This demonstrates that the hybrid parsing method
with the grammar rules makes the transfer learn-
ing more effective. On the other hand, the domain
adaptation method described in (5) and (6) suc-
cessfully counteracted the contradictory phenom-
ena in the two languages and increased the accu-
racy when the size of the Korean corpus was small
(size=50 and 98). This is because the interactions
among multiple candidates which cannot be cap-
tured from the small Korean corpus were provided
by the Japanese corpus.
Some of previous work reported the parsing
accuracy with the same KTB corpus; 81% with
trained grammar (Chung et al., 2010) and 83%
with Stanford parser after corpus transformation
(Choi et al., 2012), but as Choi et al. (2012) noted
it is difficult to directly compare the accuracies.
</bodyText>
<subsectionHeader confidence="0.952942">
6.3 Discussion
</subsectionHeader>
<bodyText confidence="0.9997326">
The analysis of e2’s dependency in Figure 1
is a good example to illustrate how the
Triplet/Quadruplet Model and the Japanese
corpus help Korean parsing. Eojeol e2 has three
modification candidates, e3, e5, and e6. In the
</bodyText>
<page confidence="0.994368">
10
</page>
<bodyText confidence="0.99980036">
Distance Model, e3 is chosen because the distance
between the two eojeols (∆e2�e3) was 1, which
is a very strong clue for dependency. Also, in
the Triplet/Quadruplet Model trained only with
a small Korean corpus, e3 received a higher
probability than e5 and e6. However, when a
larger Japanese corpus was combined with the
Korean corpus, e5 was correctly selected as the
Japanese corpus provided more samples of the
dependency relation of “verb-PAST” (e2) and
“common noun-ACC (을)” (e5) than that of
“verb-PAST” and “proper noun-GEN (의)” (e3).
As we can notice, larger contextual information
is required to make the right decision for this case,
which may not exist sufficiently in a small cor-
pus due to data sparseness. The grammar rules in
the Triplet/Quadruplet Model can effectively cap-
ture such contextual knowledge even from a rel-
atively small corpus. Further, since the grammar
rules are based only on part-of-speech tags and a
small number of functional words, they are sim-
ilar to the delexicalized parser (McDonald et al.,
2011). These delexicalized rules are more robust
to linguistic idiosyncrasies, and, thus, are more ef-
fective for transfer learning.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999983380952381">
We presented a new dependency parsing algo-
rithm for Korean by applying transfer learning
from an existing parser for Japanese. Unlike other
transfer learning methods relying on aligned cor-
pora or bilingual lexical resources, we proposed a
feature transfer method utilizing a small number
of hand-crafted grammar rules that exploit syn-
tactic similarities of the source and target lan-
guages. Experimental results confirm that the fea-
tures learned from the Japanese training corpus
were successfully applied for parsing Korean sen-
tences and mitigated the data sparseness problem.
The grammar rules are mostly delexicalized com-
prising only POS tags and a few functional words
(e.g., case markers), and some techniques to re-
duce the syntactic difference between two lan-
guages makes the transfer learning more effective.
This methodology is expected to be applied to any
two languages that have similar syntactic struc-
tures, and it is especially useful when the target
language is a low-resource language.
</bodyText>
<sectionHeader confidence="0.993411" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998798660377359">
Jinho D. Choi and Martha Palmer. 2011. Statistical
dependency parsing in Korean: From corpus gener-
ation to automatic parsing. In Proceedings of the
Second Workshop on Statistical Parsing of Morpho-
logically Rich Languages, pages 1–11.
DongHyun Choi, Jungyeul Park, and Key-Sun Choi.
2012. Korean treebank transformation for parser
training. In Proceedings of the ACL 2012 Joint
Workshop on Statistical Parsing and Semantic Pro-
cessing of Morphologically Rich Languages, pages
78–88.
Hoojung Chung and Heechang Rim. 2003. A
new probabilistic dependency parsing model for
head-final, free word order languages. IEICE
TRANSACTIONS on Information and Systems, E86-
1(11):2490–2493.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of korean parsing. In
Proceedings of the NAACL HLT 2010 First Work-
shop on Statistical Parsing of Morphologically-Rich
Languages, pages 49–57.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics and Eighth Conference of the
European Chapter of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256–263.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic transfer using a bilingual lexicon. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1–11.
EDR. 1996. EDR (Japan Electronic Dictionary Re-
search Institute, Ltd.) electronic dictionary version
1.5 technical guide.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-
Carroll, James Fan, David Gondek, Aditya Kalyan-
pur, Adam Lally, J. William Murdock, Eric Nyberg,
John M. Prager, Nico Schlaefer, and Christopher A.
Welty. 2010. Building Watson: An overview of the
DeepQA project. AI Magazine, 31(3):59–79.
Ryan Georgi, Fei Xia, and William D Lewis. 2012.
Improving dependency parsing with interlinear
glossed text and syntactic projection. In Proceed-
ings of COLING 2012, pages 371–380.
Raymond G Gordon and Barbara F Grimes. 2005. Eth-
nologue: Languages of the world, volume 15. SIL
international Dallas, TX.
</reference>
<page confidence="0.989813">
11
</page>
<reference confidence="0.999874784313726">
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Heejong
Yi, and Martha Palmer. 2002. Penn Korean tree-
bank: Development and evaluation. In Proc. Pacific
Asian Conf. Language and Comp.
Rebecca Hwa, Philip Resnik, and Amy Weinberg.
2005. Breaking the resource bottleneck for multi-
lingual parsing. Technical report, DTIC Document.
Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mitsu-
ishi, and Jun’ichi Tsujii. 2000. A hybrid Japanese
parser with hand-crafted grammar and statistics. In
Proceedings of the 18th International Conference on
Computational Linguistics, pages 411–417.
Roger Kim, Mary Dalrymple, Ronald M Kaplan, and
Tracy Holloway King. 2003a. Porting grammars
between typologically similar languages: Japanese
to korean. In Proceedings of the 17th Pacific Asia
Conference on Language, Information.
Roger Kim, Mary Dalrymple, Ronald M Kaplan,
Tracy Holloway King, Hiroshi Masuichi, and
Tomoko Ohkuma. 2003b. Multilingual grammar
development via grammar porting. In ESSLLI 2003
Workshop on Ideas and Strategies for Multilingual
Grammar Development, pages 49–56.
Dan Klein and Christopher D Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, pages 478–487.
Sandra K¨ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. Synthesis Lectures on
Human Language Technologies, 1(1):1–127.
Cody Kwok, Oren Etzioni, and Daniel S Weld. 2001.
Scaling question answering to the web. ACM Trans-
actions on Information Systems (TOIS), 19(3):242–
262.
Hyeon-Yeong Lee, Yi-Gyu Hwang, and Yong-Seok
Lee. 2007. Parsing of Korean based on CFG using
sentence pattern information. International Journal
of Computer Science and Network Security, 7(7).
Roger Levy and Christopher Manning. 2003. Is it
harder to parse chinese, or the chinese treebank? In
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, ACL, pages
439–446.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 62–72.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
T¨ackstr¨om, et al. 2013. Universal dependency an-
notation for multilingual parsing. In Proceedings of
ACL 2013.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1234–1244.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 629–637.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Senti-
ment analysis: Capturing favorability using natural
language processing. In Proceedings of the Second
International Conferences on Knowledge Capture,
pages 70–77.
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. Knowledge and Data Engineer-
ing, IEEE Transactions on, 22(10):1345–1359.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in nat-
ural language processing (EMNLP), pages 79–86,
Philadelphia, Pennsylvania.
Jungyeul Park, Daisuke Kawahara, Sadao Kurohashi,
and Key-Sun Choi. 2013. Towards fully lexicalized
dependency parsing for Korean. In Proceedings of
The 13th International Conference on Parsing Tech-
nologies.
Yoav Seginer. 2007. Fast unsupervised incremental
parsing. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 384–391.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 49–56.
Valentin I Spitkovsky, Hiyan Alshawi, Angel X Chang,
and Daniel Jurafsky. 2011. Unsupervised depen-
dency parsing without gold part-of-speech tags. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1281–
1290.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In IJCNLP, pages 35–42.
</reference>
<page confidence="0.998455">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.177547">
<title confidence="0.9989525">Learning from a Neighbor: Adapting a Japanese Parser for through Feature Transfer Learning</title>
<author confidence="0.982906">Hiroshi Kanayama</author>
<affiliation confidence="0.999665">IBM Research - Tokyo</affiliation>
<address confidence="0.991009">Koto-ku, Tokyo, Japan</address>
<email confidence="0.999622">hkana@jp.ibm.com</email>
<author confidence="0.999666">Yuta Tsuboi</author>
<affiliation confidence="0.999884">IBM Research - Tokyo</affiliation>
<address confidence="0.988222">Koto-ku, Tokyo, Japan</address>
<email confidence="0.999776">yutat@jp.ibm.com</email>
<author confidence="0.879099">Youngja</author>
<affiliation confidence="0.941924">IBM Research - T.J.Watson Research Yorktown Heights, NY,</affiliation>
<email confidence="0.790811">youngpark@us.ibm.com</email>
<author confidence="0.629736">Dongmook</author>
<affiliation confidence="0.724487">Korea Software Solutions Laboratory, IBM</affiliation>
<address confidence="0.420736">Gangnam-gu, Seoul,</address>
<email confidence="0.999659">dmyi@kr.ibm.com</email>
<abstract confidence="0.99951648">We present a new dependency parsing method for Korean applying cross-lingual transfer learning and domain adaptation techniques. Unlike existing transfer learning methods relying on aligned corpora or lexicons, we propose a method with minimal supervision, which adapts an existing parser to the target language by transferring the features for the source language to the target language. Specifically, we utilize the a hybrid parsing algorithm for Japanese, and apply a delexicalized feature transfer for Korean. with Korean Treebank show that even using only the transferred features from Japanese achieves a high accuracy (81.6%) for Korean dependency parsing. Further improvements were obtained when a small annotated Korean corpus was combined with the Japanese training corpus, confirming that efficient crosslingual transfer learning can be achieved without expensive linguistic resources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Martha Palmer</author>
</authors>
<title>Statistical dependency parsing in Korean: From corpus generation to automatic parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Parsing of Morphologically Rich Languages,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="6835" citStr="Choi and Palmer, 2011" startWordPosition="1046" endWordPosition="1049">aligned corpora or bilingual lexicons. Moreover, the delexicalized feature transfer method enables the algorithm applicable to any two languages that have similar syntactic structures. 2 Related Work 2.1 Parsing for Korean Since Korean is a morphologically-rich language, many efforts for Korean parsing have focused on automatically extracting rich lexical information such as the use of case frame patterns for the verbs (Lee et al., 2007), the acquisition of case frames and nominal phrases from raw corpora (Park et al., 2013), and effective features from phrases and their neighboring contexts (Choi and Palmer, 2011). Recently, Choi et al. (2012) discussed the transformation of eojeol-based Korean treebank to entity-based treebank to effectively train probabilistic CFG parsers. We apply similar techniques as in (Choi et al., 2012) to mitigate the differences between Korean and Japanese syntactic structures. Chung and Rim (2003) applied the Triplet/Quadruplet Model for Korean parsing as done in our work. They reported that the model performed well for long-distance dependencies, but, in their experiments, the number of modification candidates was not effectively reduced (only 91.5% of phrases were in one o</context>
<context position="13307" citStr="Choi and Palmer (2011)" startWordPosition="2075" endWordPosition="2078">61.8% Table 2: Simplified examples of Japanese grammar rules. Rightmost morpheme of the modifier PU Conditions for the modified PUs postpositional “�” wo (accusative) verb, adjective postpositional “O)” no (genitive, nominative) noun, verb, adjective postpositional “�” to (conjunctive) noun, verb, adjective, adverb “—r`f6�” isshoni (‘together’) adverb verb, adjective, adverb, copula (Han et al., 2002), and a Japanese corpus, EDR Corpus (EDR, 1996). Both corpora consist of word-level bracketed constituents, so they are converted into PU-level dependency structures using the method described in Choi and Palmer (2011). Though both corpora consist mainly of newspaper or magazine articles, the sentences are not aligned with each other, so the statistics show the comparisons of the two corpora, rather than the theoretical comparisons of the two languages. However, we can see that Korean sentences tend to be longer than Japanese sentences both in terms of the number of characters and PUs. More eojeols modify an adjacent eojeol in Korean than in Japanese. For instance, e1, e4, e6, e7, and e8 modify the next eojeol in Figure 1, but only b1, b3, and b5 modify the next bunsetsu in Figure 2. Those differences sugge</context>
</contexts>
<marker>Choi, Palmer, 2011</marker>
<rawString>Jinho D. Choi and Martha Palmer. 2011. Statistical dependency parsing in Korean: From corpus generation to automatic parsing. In Proceedings of the Second Workshop on Statistical Parsing of Morphologically Rich Languages, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DongHyun Choi</author>
<author>Jungyeul Park</author>
<author>Key-Sun Choi</author>
</authors>
<title>Korean treebank transformation for parser training.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 Joint Workshop on Statistical Parsing and Semantic Processing of Morphologically Rich Languages,</booktitle>
<pages>78--88</pages>
<contexts>
<context position="6865" citStr="Choi et al. (2012)" startWordPosition="1051" endWordPosition="1054">ons. Moreover, the delexicalized feature transfer method enables the algorithm applicable to any two languages that have similar syntactic structures. 2 Related Work 2.1 Parsing for Korean Since Korean is a morphologically-rich language, many efforts for Korean parsing have focused on automatically extracting rich lexical information such as the use of case frame patterns for the verbs (Lee et al., 2007), the acquisition of case frames and nominal phrases from raw corpora (Park et al., 2013), and effective features from phrases and their neighboring contexts (Choi and Palmer, 2011). Recently, Choi et al. (2012) discussed the transformation of eojeol-based Korean treebank to entity-based treebank to effectively train probabilistic CFG parsers. We apply similar techniques as in (Choi et al., 2012) to mitigate the differences between Korean and Japanese syntactic structures. Chung and Rim (2003) applied the Triplet/Quadruplet Model for Korean parsing as done in our work. They reported that the model performed well for long-distance dependencies, but, in their experiments, the number of modification candidates was not effectively reduced (only 91.5% of phrases were in one of the three positions, while i</context>
<context position="35106" citStr="Choi et al., 2012" startWordPosition="5783" endWordPosition="5786">fer learning more effective. On the other hand, the domain adaptation method described in (5) and (6) successfully counteracted the contradictory phenomena in the two languages and increased the accuracy when the size of the Korean corpus was small (size=50 and 98). This is because the interactions among multiple candidates which cannot be captured from the small Korean corpus were provided by the Japanese corpus. Some of previous work reported the parsing accuracy with the same KTB corpus; 81% with trained grammar (Chung et al., 2010) and 83% with Stanford parser after corpus transformation (Choi et al., 2012), but as Choi et al. (2012) noted it is difficult to directly compare the accuracies. 6.3 Discussion The analysis of e2’s dependency in Figure 1 is a good example to illustrate how the Triplet/Quadruplet Model and the Japanese corpus help Korean parsing. Eojeol e2 has three modification candidates, e3, e5, and e6. In the 10 Distance Model, e3 is chosen because the distance between the two eojeols (∆e2�e3) was 1, which is a very strong clue for dependency. Also, in the Triplet/Quadruplet Model trained only with a small Korean corpus, e3 received a higher probability than e5 and e6. However, whe</context>
</contexts>
<marker>Choi, Park, Choi, 2012</marker>
<rawString>DongHyun Choi, Jungyeul Park, and Key-Sun Choi. 2012. Korean treebank transformation for parser training. In Proceedings of the ACL 2012 Joint Workshop on Statistical Parsing and Semantic Processing of Morphologically Rich Languages, pages 78–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoojung Chung</author>
<author>Heechang Rim</author>
</authors>
<title>A new probabilistic dependency parsing model for head-final, free word order languages.</title>
<date>2003</date>
<journal>IEICE TRANSACTIONS on Information and Systems,</journal>
<pages>86--1</pages>
<contexts>
<context position="7152" citStr="Chung and Rim (2003)" startWordPosition="1094" endWordPosition="1097"> automatically extracting rich lexical information such as the use of case frame patterns for the verbs (Lee et al., 2007), the acquisition of case frames and nominal phrases from raw corpora (Park et al., 2013), and effective features from phrases and their neighboring contexts (Choi and Palmer, 2011). Recently, Choi et al. (2012) discussed the transformation of eojeol-based Korean treebank to entity-based treebank to effectively train probabilistic CFG parsers. We apply similar techniques as in (Choi et al., 2012) to mitigate the differences between Korean and Japanese syntactic structures. Chung and Rim (2003) applied the Triplet/Quadruplet Model for Korean parsing as done in our work. They reported that the model performed well for long-distance dependencies, but, in their experiments, the number of modification candidates was not effectively reduced (only 91.5% of phrases were in one of the three positions, while it was 98.6% in Kanayama’s work for Japanese). In this paper, we introduce more sophisticated grammatical knowledge and heuristics to have similar dependency distributions in the two languages. Smith and Smith (2004) attempted a bilingual parsing for English and Korean by combining stati</context>
</contexts>
<marker>Chung, Rim, 2003</marker>
<rawString>Hoojung Chung and Heechang Rim. 2003. A new probabilistic dependency parsing model for head-final, free word order languages. IEICE TRANSACTIONS on Information and Systems, E86-1(11):2490–2493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Factors affecting the accuracy of korean parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<pages>49--57</pages>
<contexts>
<context position="35029" citStr="Chung et al., 2010" startWordPosition="5771" endWordPosition="5774">nstrates that the hybrid parsing method with the grammar rules makes the transfer learning more effective. On the other hand, the domain adaptation method described in (5) and (6) successfully counteracted the contradictory phenomena in the two languages and increased the accuracy when the size of the Korean corpus was small (size=50 and 98). This is because the interactions among multiple candidates which cannot be captured from the small Korean corpus were provided by the Japanese corpus. Some of previous work reported the parsing accuracy with the same KTB corpus; 81% with trained grammar (Chung et al., 2010) and 83% with Stanford parser after corpus transformation (Choi et al., 2012), but as Choi et al. (2012) noted it is difficult to directly compare the accuracies. 6.3 Discussion The analysis of e2’s dependency in Figure 1 is a good example to illustrate how the Triplet/Quadruplet Model and the Japanese corpus help Korean parsing. Eojeol e2 has three modification candidates, e3, e5, and e6. In the 10 Distance Model, e3 is chosen because the distance between the two eojeols (∆e2�e3) was 1, which is a very strong clue for dependency. Also, in the Triplet/Quadruplet Model trained only with a small</context>
</contexts>
<marker>Chung, Post, Gildea, 2010</marker>
<rawString>Tagyoung Chung, Matt Post, and Daniel Gildea. 2010. Factors affecting the accuracy of korean parsing. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 49–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="17901" citStr="Collins, 1997" startWordPosition="2870" endWordPosition="2871">5.3 4 9.3 74.7 11.3 8.0 93.9 &gt;5 11.8 70.2 11.1 10.5 91.9 &gt;5 58.4 75.5 10.0 4.9 90.5 Total 100 − − − 98.6 Total 100 − − − 93.9 These probabilities are estimated by the maximum entropy method with a feature set to express Φ and Ψ. Assuming the independence of those modifications, the probability of the dependency tree for an entire sentence P(T) is calculated as the product of the probabilities of all of the dependencies in the sentence using beam search to maximize P(T) under the constraints of the projected structure. ∏ P(T) ≃ P(u → cu,,) (3) u In comparison, a traditional statistical parser (Collins, 1997) uses Equation (4) to calculate the probability of u modifying t. P(u → t) = P(True |Φu, Ψt, ∆u,t) (4) We call the model based on Equation (4) the Distance Model, since ∆u,t (the distance between u and t) is typically used as the key feature. Though other contextual information, in addition to the attributes of u and t, can be added, the model calculates the probabilities of the dependencies between u and t independently and thus often fails to incorporate appropriate contextual information. Equations (1) and (2) have two major advantages over the Distance Model: First, all the attributes of t</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>256--263</pages>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Syntactic transfer using a bilingual lexicon.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="9670" citStr="Durrett et al., 2012" startWordPosition="1492" endWordPosition="1495">f 7&apos;7✓A0) )ATr3)aUh7�_- $/v f/aux 77✓A/np OJ/pn RfT/n ;6�rXJv/n Vpc ‘buy-PAST’ ‘France-GEN’ ‘travel bag-ACC’ WM, &amp;1I/n lVpc ‘friend-DAT’ JQ-tl-fv� A-/v fV N/aux �/ ‘want to show’ I V V b1 b2 b3 b4 b5 bs Figure 2: A dependency structure of a Japanese sentence which corresponds to the Korean sentence in Figure 1, “妻が買ったフランスの旅行かばんを友達に見せたい。”. Each box corresponds to a Japanese phrasal unit bunsetsu. syntactic parsing, where a parsing model for a target language is learned from linguistic resources in one or more different languages (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Durrett et al., 2012; Georgi et al., 2012; Naseem et al., 2012). McDonald et al. (2011) proposed a delexicalized parsing model for crosslingual dependency parsing and demonstrated that a high accuracy parsing was achieved for IndoEuropean languages where significant amount of parallel texts exist. However, in more recent work, McDonald et al. (2013) showed that, unlike transfer learning within close language families, building a Korean parser from European languages was not successful with a very low accuracy. Durrett et al. (2012) and Georgi et al. (2012) show that transfer parsing can be improved when additiona</context>
</contexts>
<marker>Durrett, Pauls, Klein, 2012</marker>
<rawString>Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syntactic transfer using a bilingual lexicon. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>EDR</author>
</authors>
<title>EDR (Japan Electronic Dictionary Research Institute, Ltd.) electronic dictionary version 1.5 technical guide.</title>
<date>1996</date>
<contexts>
<context position="13136" citStr="EDR, 1996" startWordPosition="2052" endWordPosition="2053">e) per sentence 73.7 28.0 Average number of PUs per sentence 25.5 8.53 Average number of morphemes per PU 1.83 2.86 Ratio of modification to the next PU 70.0% 61.8% Table 2: Simplified examples of Japanese grammar rules. Rightmost morpheme of the modifier PU Conditions for the modified PUs postpositional “�” wo (accusative) verb, adjective postpositional “O)” no (genitive, nominative) noun, verb, adjective postpositional “�” to (conjunctive) noun, verb, adjective, adverb “—r`f6�” isshoni (‘together’) adverb verb, adjective, adverb, copula (Han et al., 2002), and a Japanese corpus, EDR Corpus (EDR, 1996). Both corpora consist of word-level bracketed constituents, so they are converted into PU-level dependency structures using the method described in Choi and Palmer (2011). Though both corpora consist mainly of newspaper or magazine articles, the sentences are not aligned with each other, so the statistics show the comparisons of the two corpora, rather than the theoretical comparisons of the two languages. However, we can see that Korean sentences tend to be longer than Japanese sentences both in terms of the number of characters and PUs. More eojeols modify an adjacent eojeol in Korean than </context>
</contexts>
<marker>EDR, 1996</marker>
<rawString>EDR. 1996. EDR (Japan Electronic Dictionary Research Institute, Ltd.) electronic dictionary version 1.5 technical guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Ferrucci</author>
<author>Eric W Brown</author>
<author>Jennifer ChuCarroll</author>
<author>James Fan</author>
<author>David Gondek</author>
<author>Aditya Kalyanpur</author>
<author>Adam Lally</author>
<author>J William Murdock</author>
<author>Eric Nyberg</author>
<author>John M Prager</author>
<author>Nico Schlaefer</author>
<author>Christopher A Welty</author>
</authors>
<title>Building Watson: An overview of the DeepQA project.</title>
<date>2010</date>
<journal>AI Magazine,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="1677" citStr="Ferrucci et al., 2010" startWordPosition="236" endWordPosition="239">n Korean Treebank show that even using only the transferred features from Japanese achieves a high accuracy (81.6%) for Korean dependency parsing. Further improvements were obtained when a small annotated Korean corpus was combined with the Japanese training corpus, confirming that efficient crosslingual transfer learning can be achieved without expensive linguistic resources. 1 Introduction Motivated by increasing demands for advanced natural language processing (NLP) applications such as sentiment analysis (Pang et al., 2002; Nasukawa and Yi, 2003) and question answering (Kwok et al., 2001; Ferrucci et al., 2010), there is a growing need for accurate syntactic parsing and semantic analysis of languages, especially for non-English languages with limited linguistic resources. In this paper, we propose a new dependency parsing method for Korean which requires minimal human supervision. Dependency parsing can handle long-distance relationships and coordination phenomena very well, and has proven to be very effective for parsing free-order languages such as Korean and Japanese (K¨ubler et al., 2009). Most statistical parsing methods rely on annotated corpora labeled with phrase structures or dependency rel</context>
</contexts>
<marker>Ferrucci, Brown, ChuCarroll, Fan, Gondek, Kalyanpur, Lally, Murdock, Nyberg, Prager, Schlaefer, Welty, 2010</marker>
<rawString>David A. Ferrucci, Eric W. Brown, Jennifer ChuCarroll, James Fan, David Gondek, Aditya Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John M. Prager, Nico Schlaefer, and Christopher A. Welty. 2010. Building Watson: An overview of the DeepQA project. AI Magazine, 31(3):59–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Georgi</author>
<author>Fei Xia</author>
<author>William D Lewis</author>
</authors>
<title>Improving dependency parsing with interlinear glossed text and syntactic projection.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>371--380</pages>
<contexts>
<context position="9691" citStr="Georgi et al., 2012" startWordPosition="1496" endWordPosition="1499">- $/v f/aux 77✓A/np OJ/pn RfT/n ;6�rXJv/n Vpc ‘buy-PAST’ ‘France-GEN’ ‘travel bag-ACC’ WM, &amp;1I/n lVpc ‘friend-DAT’ JQ-tl-fv� A-/v fV N/aux �/ ‘want to show’ I V V b1 b2 b3 b4 b5 bs Figure 2: A dependency structure of a Japanese sentence which corresponds to the Korean sentence in Figure 1, “妻が買ったフランスの旅行かばんを友達に見せたい。”. Each box corresponds to a Japanese phrasal unit bunsetsu. syntactic parsing, where a parsing model for a target language is learned from linguistic resources in one or more different languages (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Durrett et al., 2012; Georgi et al., 2012; Naseem et al., 2012). McDonald et al. (2011) proposed a delexicalized parsing model for crosslingual dependency parsing and demonstrated that a high accuracy parsing was achieved for IndoEuropean languages where significant amount of parallel texts exist. However, in more recent work, McDonald et al. (2013) showed that, unlike transfer learning within close language families, building a Korean parser from European languages was not successful with a very low accuracy. Durrett et al. (2012) and Georgi et al. (2012) show that transfer parsing can be improved when additional bilingual resources</context>
</contexts>
<marker>Georgi, Xia, Lewis, 2012</marker>
<rawString>Ryan Georgi, Fei Xia, and William D Lewis. 2012. Improving dependency parsing with interlinear glossed text and syntactic projection. In Proceedings of COLING 2012, pages 371–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond G Gordon</author>
<author>Barbara F Grimes</author>
</authors>
<date>2005</date>
<journal>Ethnologue: Languages of the world,</journal>
<volume>15</volume>
<publisher>SIL international</publisher>
<location>Dallas, TX.</location>
<contexts>
<context position="4761" citStr="Gordon and Grimes, 2005" startWordPosition="721" endWordPosition="724">d annotated data used in the Japanese dependency parsing, the Triplet/Quadruplet Model (Kanayama et al., 2000), which is a hybrid approach utilizing both grammatical knowledge and statistics. We exploit many similarities between the two languages, such as the head-final structure, the noun to verb modification via case and topic markers, and the similar word-order constraints. It was reported that the mapping of the grammar formalism in the language pair was relatively easy (Kim et al., 2003b; Kim et al., 2003a). However, as the two languages are classified into independent language families (Gordon and Grimes, 2005), there are many significant differences in their morphology and grammar (especially in the writing systems), so it is not trivial to handle the two languages in a uniform way. We show the Triplet/Quadruplet Model is suitable for bilingual transfer learning, because the grammar rules and heuristics reduce the number of modification candidates and can mitigate the differences between two languages efficiently. In addition, this model can handle the relationships among the candidates as a richer feature space, making the model less dependent upon the lexical features of the content words that ar</context>
</contexts>
<marker>Gordon, Grimes, 2005</marker>
<rawString>Raymond G Gordon and Barbara F Grimes. 2005. Ethnologue: Languages of the world, volume 15. SIL international Dallas, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung-hye Han</author>
<author>Na-Rae Han</author>
<author>Eon-Suk Ko</author>
<author>Heejong Yi</author>
<author>Martha Palmer</author>
</authors>
<title>Penn Korean treebank: Development and evaluation.</title>
<date>2002</date>
<booktitle>In Proc. Pacific Asian Conf. Language and Comp.</booktitle>
<contexts>
<context position="5825" citStr="Han et al., 2002" startWordPosition="892" endWordPosition="895"> the relationships among the candidates as a richer feature space, making the model less dependent upon the lexical features of the content words that are difficult to align between the two languages. Similarly to the delexicalized parsing model in (McDonald et al., 2011), we transfer only part-of-speech information of the features for the content words. We create new mapping rules to extract syntactic features for Korean parsing from the Japanese annotated corpus and refine the grammar rules to get closer modification distributions in two languages. Our experiments with Penn Korean Treebank (Han et al., 2002) confirm that the Triplet/Quadruplet Model adapted for Korean outperforms a distance-based dependency parsing method, achieving 81.6% accuracy when no annotated Korean corpus was used. Further performance improvements were obtained when a small size of annotated Korean corpus was added, confirming that our algorithm can be applied without more expensive linguistic resources such as an aligned corpora or bilingual lexicons. Moreover, the delexicalized feature transfer method enables the algorithm applicable to any two languages that have similar syntactic structures. 2 Related Work 2.1 Parsing </context>
<context position="13089" citStr="Han et al., 2002" startWordPosition="2042" endWordPosition="2045">se) Average number of characters (except for whitespace) per sentence 73.7 28.0 Average number of PUs per sentence 25.5 8.53 Average number of morphemes per PU 1.83 2.86 Ratio of modification to the next PU 70.0% 61.8% Table 2: Simplified examples of Japanese grammar rules. Rightmost morpheme of the modifier PU Conditions for the modified PUs postpositional “�” wo (accusative) verb, adjective postpositional “O)” no (genitive, nominative) noun, verb, adjective postpositional “�” to (conjunctive) noun, verb, adjective, adverb “—r`f6�” isshoni (‘together’) adverb verb, adjective, adverb, copula (Han et al., 2002), and a Japanese corpus, EDR Corpus (EDR, 1996). Both corpora consist of word-level bracketed constituents, so they are converted into PU-level dependency structures using the method described in Choi and Palmer (2011). Though both corpora consist mainly of newspaper or magazine articles, the sentences are not aligned with each other, so the statistics show the comparisons of the two corpora, rather than the theoretical comparisons of the two languages. However, we can see that Korean sentences tend to be longer than Japanese sentences both in terms of the number of characters and PUs. More eo</context>
<context position="29339" citStr="Han et al., 2002" startWordPosition="4817" endWordPosition="4820">.3% 6 Experiments In this section, we validate the effectiveness of learning a Korean parser using the feature transfer learning from the Japanese parser and compare the Korean model with other baseline cases. We also compare the parsing results when various sizes of bilingual corpora were used to train the Korean model. 6.1 Korean parsing using the Triplet/Quadruplet Model First, to validate the effectiveness of the Triplet/Quadruplet Model for parsing Korean, we built eight Korean dependency parsing models using different numbers of training sentences for Korean. The KTB corpus Version 2.0 (Han et al., 2002) containing 5,010 annotated sentences was used in this study. We first divide the corpus into 5 subsets by putting each sentence into its (sentence ID mod 5)-th group. We use sentences from the first subgroup for estimating the parameters, sentences from the second subgroup for testing, and use the remaining three subgroups for training. We built 8 models in total, using from 0 sentence up to 3,006 sentences selected from the training set. The number of training sentences in each model is shown in the first column in Table 8. The parameters were estimated by the maximum entropy method, and the</context>
</contexts>
<marker>Han, Han, Ko, Yi, Palmer, 2002</marker>
<rawString>Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Heejong Yi, and Martha Palmer. 2002. Penn Korean treebank: Development and evaluation. In Proc. Pacific Asian Conf. Language and Comp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
</authors>
<title>Breaking the resource bottleneck for multilingual parsing.</title>
<date>2005</date>
<tech>Technical report, DTIC Document.</tech>
<contexts>
<context position="9601" citStr="Hwa et al., 2005" startWordPosition="1480" endWordPosition="1483">onds to a Korean phrasal unit eojeol. �� /n ffi/pc ‘wife-NOM’ R-Df 7&apos;7✓A0) )ATr3)aUh7�_- $/v f/aux 77✓A/np OJ/pn RfT/n ;6�rXJv/n Vpc ‘buy-PAST’ ‘France-GEN’ ‘travel bag-ACC’ WM, &amp;1I/n lVpc ‘friend-DAT’ JQ-tl-fv� A-/v fV N/aux �/ ‘want to show’ I V V b1 b2 b3 b4 b5 bs Figure 2: A dependency structure of a Japanese sentence which corresponds to the Korean sentence in Figure 1, “妻が買ったフランスの旅行かばんを友達に見せたい。”. Each box corresponds to a Japanese phrasal unit bunsetsu. syntactic parsing, where a parsing model for a target language is learned from linguistic resources in one or more different languages (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Durrett et al., 2012; Georgi et al., 2012; Naseem et al., 2012). McDonald et al. (2011) proposed a delexicalized parsing model for crosslingual dependency parsing and demonstrated that a high accuracy parsing was achieved for IndoEuropean languages where significant amount of parallel texts exist. However, in more recent work, McDonald et al. (2013) showed that, unlike transfer learning within close language families, building a Korean parser from European languages was not successful with a very low accuracy. Durrett et al. (2012) and Georgi et</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, and Amy Weinberg. 2005. Breaking the resource bottleneck for multilingual parsing. Technical report, DTIC Document.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Kanayama</author>
<author>Kentaro Torisawa</author>
<author>Yutaka Mitsuishi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A hybrid Japanese parser with hand-crafted grammar and statistics.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>411--417</pages>
<contexts>
<context position="4247" citStr="Kanayama et al., 2000" startWordPosition="639" endWordPosition="642">opean languages. In this paper, we present a new cross-lingual 2 Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 2–12, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics transfer learning method that learns a new model for the target language by transferring the features for the source language. Unlike other approaches which rely on aligned corpora or a bilingual lexicon, we learn a parsing model for Korean by reusing the features and annotated data used in the Japanese dependency parsing, the Triplet/Quadruplet Model (Kanayama et al., 2000), which is a hybrid approach utilizing both grammatical knowledge and statistics. We exploit many similarities between the two languages, such as the head-final structure, the noun to verb modification via case and topic markers, and the similar word-order constraints. It was reported that the mapping of the grammar formalism in the language pair was relatively easy (Kim et al., 2003b; Kim et al., 2003a). However, as the two languages are classified into independent language families (Gordon and Grimes, 2005), there are many significant differences in their morphology and grammar (especially i</context>
<context position="14227" citStr="Kanayama et al., 2000" startWordPosition="2230" endWordPosition="2233">r than Japanese sentences both in terms of the number of characters and PUs. More eojeols modify an adjacent eojeol in Korean than in Japanese. For instance, e1, e4, e6, e7, and e8 modify the next eojeol in Figure 1, but only b1, b3, and b5 modify the next bunsetsu in Figure 2. Those differences suggest some of the difficulties in applying the Japanese dependency model to Korean. The Japanese parsing method that will be described in the next section exploits these characteristics, which we apply to Korean parsing. 4 Triplet/Quadruplet Model This section describes the Triplet/Quadruplet Model (Kanayama et al., 2000) which was originally designed for Japanese parsing. First, we review the two main ideas of the model – restriction of modification candidates and feature selection for probability calculation. Then, we describe how we apply the Triplet/Quadruplet Model to Korean parsing in Section 4.3. 4.1 Restriction of modification candidates The Triplet/Quadruplet Model utilizes a small number (about 50) of hand-crafted grammar rules that determine whether a PU can modify each PU to its right in a sentence. The main goal of the grammar rules is to maximize the coverage, and the rules are simple describing </context>
</contexts>
<marker>Kanayama, Torisawa, Mitsuishi, Tsujii, 2000</marker>
<rawString>Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mitsuishi, and Jun’ichi Tsujii. 2000. A hybrid Japanese parser with hand-crafted grammar and statistics. In Proceedings of the 18th International Conference on Computational Linguistics, pages 411–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Kim</author>
<author>Mary Dalrymple</author>
<author>Ronald M Kaplan</author>
<author>Tracy Holloway King</author>
</authors>
<title>Porting grammars between typologically similar languages: Japanese to korean.</title>
<date>2003</date>
<booktitle>In Proceedings of the 17th Pacific Asia Conference on Language, Information.</booktitle>
<contexts>
<context position="4633" citStr="Kim et al., 2003" startWordPosition="701" endWordPosition="704">hes which rely on aligned corpora or a bilingual lexicon, we learn a parsing model for Korean by reusing the features and annotated data used in the Japanese dependency parsing, the Triplet/Quadruplet Model (Kanayama et al., 2000), which is a hybrid approach utilizing both grammatical knowledge and statistics. We exploit many similarities between the two languages, such as the head-final structure, the noun to verb modification via case and topic markers, and the similar word-order constraints. It was reported that the mapping of the grammar formalism in the language pair was relatively easy (Kim et al., 2003b; Kim et al., 2003a). However, as the two languages are classified into independent language families (Gordon and Grimes, 2005), there are many significant differences in their morphology and grammar (especially in the writing systems), so it is not trivial to handle the two languages in a uniform way. We show the Triplet/Quadruplet Model is suitable for bilingual transfer learning, because the grammar rules and heuristics reduce the number of modification candidates and can mitigate the differences between two languages efficiently. In addition, this model can handle the relationships among </context>
</contexts>
<marker>Kim, Dalrymple, Kaplan, King, 2003</marker>
<rawString>Roger Kim, Mary Dalrymple, Ronald M Kaplan, and Tracy Holloway King. 2003a. Porting grammars between typologically similar languages: Japanese to korean. In Proceedings of the 17th Pacific Asia Conference on Language, Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Kim</author>
<author>Mary Dalrymple</author>
<author>Ronald M Kaplan</author>
<author>Tracy Holloway King</author>
<author>Hiroshi Masuichi</author>
<author>Tomoko Ohkuma</author>
</authors>
<title>Multilingual grammar development via grammar porting.</title>
<date>2003</date>
<booktitle>In ESSLLI 2003 Workshop on Ideas and Strategies for Multilingual Grammar Development,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="4633" citStr="Kim et al., 2003" startWordPosition="701" endWordPosition="704">hes which rely on aligned corpora or a bilingual lexicon, we learn a parsing model for Korean by reusing the features and annotated data used in the Japanese dependency parsing, the Triplet/Quadruplet Model (Kanayama et al., 2000), which is a hybrid approach utilizing both grammatical knowledge and statistics. We exploit many similarities between the two languages, such as the head-final structure, the noun to verb modification via case and topic markers, and the similar word-order constraints. It was reported that the mapping of the grammar formalism in the language pair was relatively easy (Kim et al., 2003b; Kim et al., 2003a). However, as the two languages are classified into independent language families (Gordon and Grimes, 2005), there are many significant differences in their morphology and grammar (especially in the writing systems), so it is not trivial to handle the two languages in a uniform way. We show the Triplet/Quadruplet Model is suitable for bilingual transfer learning, because the grammar rules and heuristics reduce the number of modification candidates and can mitigate the differences between two languages efficiently. In addition, this model can handle the relationships among </context>
</contexts>
<marker>Kim, Dalrymple, Kaplan, King, Masuichi, Ohkuma, 2003</marker>
<rawString>Roger Kim, Mary Dalrymple, Ronald M Kaplan, Tracy Holloway King, Hiroshi Masuichi, and Tomoko Ohkuma. 2003b. Multilingual grammar development via grammar porting. In ESSLLI 2003 Workshop on Ideas and Strategies for Multilingual Grammar Development, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>478--487</pages>
<contexts>
<context position="2960" citStr="Klein and Manning, 2004" startWordPosition="440" endWordPosition="443">r of consistent annotations. Recently, treebanks have become available for many languages such as English, German, Arabic, and Chinese. However, the parsing results on these treebanks vary a lot depending on the size of annotated sentences and the type of annotations (Levy and Manning, 2003; McDonald et al., 2013). Further, many languages lack annotated corpus, or the size of the annotated corpus is too small to develop a reliable statistical method. To address these problems, there have been several attempts at unsupervised parsing (Seginer, 2007; Spitkovsky et al., 2011), grammar induction (Klein and Manning, 2004; Naseem et al., 2010), and cross-lingual transfer learning using annotated corpora of other languages (McDonald et al., 2011). However, the accuracies of unsupervised methods are unacceptably low, and results from cross-lingual transfer learning show different outcomes for different pairs of languages, but, in most cases, the parsing accuracy is still low for practical purposes. A recent study by McDonald et al. (2013) concludes that cross-lingual transfer learning is beneficial when the source and target languages were similar. In particular, it reports that Korean is an outlier with the low</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 478–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<date>2009</date>
<booktitle>Dependency parsing. Synthesis Lectures on Human Language Technologies,</booktitle>
<volume>1</volume>
<issue>1</issue>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>Sandra K¨ubler, Ryan McDonald, and Joakim Nivre. 2009. Dependency parsing. Synthesis Lectures on Human Language Technologies, 1(1):1–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cody Kwok</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
</authors>
<title>Scaling question answering to the web.</title>
<date>2001</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>19</volume>
<issue>3</issue>
<pages>262</pages>
<contexts>
<context position="1653" citStr="Kwok et al., 2001" startWordPosition="232" endWordPosition="235">xperiments with Penn Korean Treebank show that even using only the transferred features from Japanese achieves a high accuracy (81.6%) for Korean dependency parsing. Further improvements were obtained when a small annotated Korean corpus was combined with the Japanese training corpus, confirming that efficient crosslingual transfer learning can be achieved without expensive linguistic resources. 1 Introduction Motivated by increasing demands for advanced natural language processing (NLP) applications such as sentiment analysis (Pang et al., 2002; Nasukawa and Yi, 2003) and question answering (Kwok et al., 2001; Ferrucci et al., 2010), there is a growing need for accurate syntactic parsing and semantic analysis of languages, especially for non-English languages with limited linguistic resources. In this paper, we propose a new dependency parsing method for Korean which requires minimal human supervision. Dependency parsing can handle long-distance relationships and coordination phenomena very well, and has proven to be very effective for parsing free-order languages such as Korean and Japanese (K¨ubler et al., 2009). Most statistical parsing methods rely on annotated corpora labeled with phrase stru</context>
</contexts>
<marker>Kwok, Etzioni, Weld, 2001</marker>
<rawString>Cody Kwok, Oren Etzioni, and Daniel S Weld. 2001. Scaling question answering to the web. ACM Transactions on Information Systems (TOIS), 19(3):242– 262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hyeon-Yeong Lee</author>
<author>Yi-Gyu Hwang</author>
<author>Yong-Seok Lee</author>
</authors>
<title>Parsing of Korean based on CFG using sentence pattern information.</title>
<date>2007</date>
<journal>International Journal of Computer Science and Network Security,</journal>
<volume>7</volume>
<issue>7</issue>
<contexts>
<context position="6654" citStr="Lee et al., 2007" startWordPosition="1017" endWordPosition="1020">ments were obtained when a small size of annotated Korean corpus was added, confirming that our algorithm can be applied without more expensive linguistic resources such as an aligned corpora or bilingual lexicons. Moreover, the delexicalized feature transfer method enables the algorithm applicable to any two languages that have similar syntactic structures. 2 Related Work 2.1 Parsing for Korean Since Korean is a morphologically-rich language, many efforts for Korean parsing have focused on automatically extracting rich lexical information such as the use of case frame patterns for the verbs (Lee et al., 2007), the acquisition of case frames and nominal phrases from raw corpora (Park et al., 2013), and effective features from phrases and their neighboring contexts (Choi and Palmer, 2011). Recently, Choi et al. (2012) discussed the transformation of eojeol-based Korean treebank to entity-based treebank to effectively train probabilistic CFG parsers. We apply similar techniques as in (Choi et al., 2012) to mitigate the differences between Korean and Japanese syntactic structures. Chung and Rim (2003) applied the Triplet/Quadruplet Model for Korean parsing as done in our work. They reported that the m</context>
</contexts>
<marker>Lee, Hwang, Lee, 2007</marker>
<rawString>Hyeon-Yeong Lee, Yi-Gyu Hwang, and Yong-Seok Lee. 2007. Parsing of Korean based on CFG using sentence pattern information. International Journal of Computer Science and Network Security, 7(7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Christopher Manning</author>
</authors>
<title>Is it harder to parse chinese, or the chinese treebank?</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, ACL,</booktitle>
<pages>439--446</pages>
<contexts>
<context position="2628" citStr="Levy and Manning, 2003" startWordPosition="385" endWordPosition="388">hips and coordination phenomena very well, and has proven to be very effective for parsing free-order languages such as Korean and Japanese (K¨ubler et al., 2009). Most statistical parsing methods rely on annotated corpora labeled with phrase structures or dependency relationships, but it is very expensive to create a large number of consistent annotations. Recently, treebanks have become available for many languages such as English, German, Arabic, and Chinese. However, the parsing results on these treebanks vary a lot depending on the size of annotated sentences and the type of annotations (Levy and Manning, 2003; McDonald et al., 2013). Further, many languages lack annotated corpus, or the size of the annotated corpus is too small to develop a reliable statistical method. To address these problems, there have been several attempts at unsupervised parsing (Seginer, 2007; Spitkovsky et al., 2011), grammar induction (Klein and Manning, 2004; Naseem et al., 2010), and cross-lingual transfer learning using annotated corpora of other languages (McDonald et al., 2011). However, the accuracies of unsupervised methods are unacceptably low, and results from cross-lingual transfer learning show different outcom</context>
</contexts>
<marker>Levy, Manning, 2003</marker>
<rawString>Roger Levy and Christopher Manning. 2003. Is it harder to parse chinese, or the chinese treebank? In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, ACL, pages 439–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Slav Petrov</author>
<author>Keith Hall</author>
</authors>
<title>Multi-source transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>62--72</pages>
<contexts>
<context position="3086" citStr="McDonald et al., 2011" startWordPosition="458" endWordPosition="462">Chinese. However, the parsing results on these treebanks vary a lot depending on the size of annotated sentences and the type of annotations (Levy and Manning, 2003; McDonald et al., 2013). Further, many languages lack annotated corpus, or the size of the annotated corpus is too small to develop a reliable statistical method. To address these problems, there have been several attempts at unsupervised parsing (Seginer, 2007; Spitkovsky et al., 2011), grammar induction (Klein and Manning, 2004; Naseem et al., 2010), and cross-lingual transfer learning using annotated corpora of other languages (McDonald et al., 2011). However, the accuracies of unsupervised methods are unacceptably low, and results from cross-lingual transfer learning show different outcomes for different pairs of languages, but, in most cases, the parsing accuracy is still low for practical purposes. A recent study by McDonald et al. (2013) concludes that cross-lingual transfer learning is beneficial when the source and target languages were similar. In particular, it reports that Korean is an outlier with the lowest scores (42% or less in UAS) when a model was trained from European languages. In this paper, we present a new cross-lingua</context>
<context position="5480" citStr="McDonald et al., 2011" startWordPosition="836" endWordPosition="839">ng systems), so it is not trivial to handle the two languages in a uniform way. We show the Triplet/Quadruplet Model is suitable for bilingual transfer learning, because the grammar rules and heuristics reduce the number of modification candidates and can mitigate the differences between two languages efficiently. In addition, this model can handle the relationships among the candidates as a richer feature space, making the model less dependent upon the lexical features of the content words that are difficult to align between the two languages. Similarly to the delexicalized parsing model in (McDonald et al., 2011), we transfer only part-of-speech information of the features for the content words. We create new mapping rules to extract syntactic features for Korean parsing from the Japanese annotated corpus and refine the grammar rules to get closer modification distributions in two languages. Our experiments with Penn Korean Treebank (Han et al., 2002) confirm that the Triplet/Quadruplet Model adapted for Korean outperforms a distance-based dependency parsing method, achieving 81.6% accuracy when no annotated Korean corpus was used. Further performance improvements were obtained when a small size of an</context>
<context position="9648" citStr="McDonald et al., 2011" startWordPosition="1488" endWordPosition="1491">n ffi/pc ‘wife-NOM’ R-Df 7&apos;7✓A0) )ATr3)aUh7�_- $/v f/aux 77✓A/np OJ/pn RfT/n ;6�rXJv/n Vpc ‘buy-PAST’ ‘France-GEN’ ‘travel bag-ACC’ WM, &amp;1I/n lVpc ‘friend-DAT’ JQ-tl-fv� A-/v fV N/aux �/ ‘want to show’ I V V b1 b2 b3 b4 b5 bs Figure 2: A dependency structure of a Japanese sentence which corresponds to the Korean sentence in Figure 1, “妻が買ったフランスの旅行かばんを友達に見せたい。”. Each box corresponds to a Japanese phrasal unit bunsetsu. syntactic parsing, where a parsing model for a target language is learned from linguistic resources in one or more different languages (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Durrett et al., 2012; Georgi et al., 2012; Naseem et al., 2012). McDonald et al. (2011) proposed a delexicalized parsing model for crosslingual dependency parsing and demonstrated that a high accuracy parsing was achieved for IndoEuropean languages where significant amount of parallel texts exist. However, in more recent work, McDonald et al. (2013) showed that, unlike transfer learning within close language families, building a Korean parser from European languages was not successful with a very low accuracy. Durrett et al. (2012) and Georgi et al. (2012) show that transfer parsing can be i</context>
<context position="23363" citStr="McDonald et al. (2011)" startWordPosition="3800" endWordPosition="3804">d for Korean. In this section, we describe the feature adaption techniques in more detail and investigate if the new model with transferred features works well when a small amount of annotated corpus for the target language is provided. Further, we study if we can leverage the annotated corpus for the source language in addition to the parsing model and train a model for the target language using the training data for the source language. 5.1 Feature Transfer With the assumption that Korean and Japanese have similar syntactic dependencies, we adopt the delexicalized parsing model presented in McDonald et al. (2011). We transfer the part-ofspeech (POS) in the Japanese features to the POS scheme in the KTB corpus, and translate Japanese functional words to the corresponding functional words in Korean. This transfer process is mandatory because we use the language specific POS systems to capture language-specific dependency phenomena, unlike other works using language universal but coarser POS systems. We do not transfer lexical knowledge on con7 Table 6: Example of mapping rules for parts-of-speech and functional words. Japanese PoS Korean PoS common noun NNC verb VV adjective VJ nominal suf~x XSF “で”,“に”</context>
<context position="36469" citStr="McDonald et al., 2011" startWordPosition="6008" endWordPosition="6011"> dependency relation of “verb-PAST” (e2) and “common noun-ACC (을)” (e5) than that of “verb-PAST” and “proper noun-GEN (의)” (e3). As we can notice, larger contextual information is required to make the right decision for this case, which may not exist sufficiently in a small corpus due to data sparseness. The grammar rules in the Triplet/Quadruplet Model can effectively capture such contextual knowledge even from a relatively small corpus. Further, since the grammar rules are based only on part-of-speech tags and a small number of functional words, they are similar to the delexicalized parser (McDonald et al., 2011). These delexicalized rules are more robust to linguistic idiosyncrasies, and, thus, are more effective for transfer learning. 7 Conclusion We presented a new dependency parsing algorithm for Korean by applying transfer learning from an existing parser for Japanese. Unlike other transfer learning methods relying on aligned corpora or bilingual lexical resources, we proposed a feature transfer method utilizing a small number of hand-crafted grammar rules that exploit syntactic similarities of the source and target languages. Experimental results confirm that the features learned from the Japane</context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 62–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Joakim Nivre, Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev,</title>
<date>2013</date>
<booktitle>In Proceedings of ACL</booktitle>
<location>Keith Hall, Slav Petrov, Hao Zhang, Oscar T¨ackstr¨om, et</location>
<marker>McDonald, 2013</marker>
<rawString>Ryan McDonald, Joakim Nivre, Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar T¨ackstr¨om, et al. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of ACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Harr Chen</author>
<author>Regina Barzilay</author>
<author>Mark Johnson</author>
</authors>
<title>Using universal linguistic knowledge to guide grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1234--1244</pages>
<contexts>
<context position="2982" citStr="Naseem et al., 2010" startWordPosition="444" endWordPosition="447">ns. Recently, treebanks have become available for many languages such as English, German, Arabic, and Chinese. However, the parsing results on these treebanks vary a lot depending on the size of annotated sentences and the type of annotations (Levy and Manning, 2003; McDonald et al., 2013). Further, many languages lack annotated corpus, or the size of the annotated corpus is too small to develop a reliable statistical method. To address these problems, there have been several attempts at unsupervised parsing (Seginer, 2007; Spitkovsky et al., 2011), grammar induction (Klein and Manning, 2004; Naseem et al., 2010), and cross-lingual transfer learning using annotated corpora of other languages (McDonald et al., 2011). However, the accuracies of unsupervised methods are unacceptably low, and results from cross-lingual transfer learning show different outcomes for different pairs of languages, but, in most cases, the parsing accuracy is still low for practical purposes. A recent study by McDonald et al. (2013) concludes that cross-lingual transfer learning is beneficial when the source and target languages were similar. In particular, it reports that Korean is an outlier with the lowest scores (42% or les</context>
</contexts>
<marker>Naseem, Chen, Barzilay, Johnson, 2010</marker>
<rawString>Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. 2010. Using universal linguistic knowledge to guide grammar induction. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1234–1244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
<author>Amir Globerson</author>
</authors>
<title>Selective sharing for multilingual dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume</booktitle>
<volume>1</volume>
<pages>629--637</pages>
<contexts>
<context position="9713" citStr="Naseem et al., 2012" startWordPosition="1500" endWordPosition="1503">J/pn RfT/n ;6�rXJv/n Vpc ‘buy-PAST’ ‘France-GEN’ ‘travel bag-ACC’ WM, &amp;1I/n lVpc ‘friend-DAT’ JQ-tl-fv� A-/v fV N/aux �/ ‘want to show’ I V V b1 b2 b3 b4 b5 bs Figure 2: A dependency structure of a Japanese sentence which corresponds to the Korean sentence in Figure 1, “妻が買ったフランスの旅行かばんを友達に見せたい。”. Each box corresponds to a Japanese phrasal unit bunsetsu. syntactic parsing, where a parsing model for a target language is learned from linguistic resources in one or more different languages (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Durrett et al., 2012; Georgi et al., 2012; Naseem et al., 2012). McDonald et al. (2011) proposed a delexicalized parsing model for crosslingual dependency parsing and demonstrated that a high accuracy parsing was achieved for IndoEuropean languages where significant amount of parallel texts exist. However, in more recent work, McDonald et al. (2013) showed that, unlike transfer learning within close language families, building a Korean parser from European languages was not successful with a very low accuracy. Durrett et al. (2012) and Georgi et al. (2012) show that transfer parsing can be improved when additional bilingual resources are available, such a</context>
</contexts>
<marker>Naseem, Barzilay, Globerson, 2012</marker>
<rawString>Tahira Naseem, Regina Barzilay, and Amir Globerson. 2012. Selective sharing for multilingual dependency parsing. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 629–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Nasukawa</author>
<author>Jeonghee Yi</author>
</authors>
<title>Sentiment analysis: Capturing favorability using natural language processing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second International Conferences on Knowledge Capture,</booktitle>
<pages>70--77</pages>
<contexts>
<context position="1611" citStr="Nasukawa and Yi, 2003" startWordPosition="224" endWordPosition="227"> a delexicalized feature transfer for Korean. Experiments with Penn Korean Treebank show that even using only the transferred features from Japanese achieves a high accuracy (81.6%) for Korean dependency parsing. Further improvements were obtained when a small annotated Korean corpus was combined with the Japanese training corpus, confirming that efficient crosslingual transfer learning can be achieved without expensive linguistic resources. 1 Introduction Motivated by increasing demands for advanced natural language processing (NLP) applications such as sentiment analysis (Pang et al., 2002; Nasukawa and Yi, 2003) and question answering (Kwok et al., 2001; Ferrucci et al., 2010), there is a growing need for accurate syntactic parsing and semantic analysis of languages, especially for non-English languages with limited linguistic resources. In this paper, we propose a new dependency parsing method for Korean which requires minimal human supervision. Dependency parsing can handle long-distance relationships and coordination phenomena very well, and has proven to be very effective for parsing free-order languages such as Korean and Japanese (K¨ubler et al., 2009). Most statistical parsing methods rely on </context>
</contexts>
<marker>Nasukawa, Yi, 2003</marker>
<rawString>Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment analysis: Capturing favorability using natural language processing. In Proceedings of the Second International Conferences on Knowledge Capture, pages 70–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Qiang Yang</author>
</authors>
<title>A survey on transfer learning.</title>
<date>2010</date>
<journal>Knowledge and Data Engineering, IEEE Transactions on,</journal>
<volume>22</volume>
<issue>10</issue>
<contexts>
<context position="8437" citStr="Pan and Yang, 2010" startWordPosition="1290" endWordPosition="1293">d word translation models into a unified framework that jointly searches for the best English parse, Korean parse and word alignment. However, we utilize an existing parser and align the features from the source language to the features for the target language, and, thus, our method is applicable to situations where there is no aligned corpora or word translation models. 2.2 Transfer learning and domain adaptation Recently, transfer learning has attracted much attention, as it can overcome the lack of training data for new languages or new domains for both classification and regression tasks (Pan and Yang, 2010). Transfer learning has also been applied to 3 �} R*::�:°l °1_1 711--&amp;- h}/VV _/EAN aP&amp;&apos;—&apos;/NPR 91/PAN W-I/NNC 7}1t/NNC -&amp;/PCA ‘buy-PAST’ ‘France-GEN’ ‘travel’ ‘bag-ACC’ J01- AlV4 � _V I/VV aa/ECS I/VX c}/EFN ./SFN ‘show’ ‘want’ ‘.’ 01471 o}1A/NNC I/PCA ‘wife-NOM’ il-7O11X11 l-7/NNC q7fl/PAD ‘friend-DAT’ e1 e2 e3 e4 e5 es e7 es eg Figure 1: An example of dependency structures of a Korean sentence “447} + —ifo 91 01a� 7}% EE L�-71711 .V-&amp;quot;l-�? -+.” (‘(I) want to show the French travel bag which (my) wife bought to (my) friend’). Each box corresponds to a Korean phrasal unit eojeol. �� /n ffi/pc ‘</context>
</contexts>
<marker>Pan, Yang, 2010</marker>
<rawString>Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. Knowledge and Data Engineering, IEEE Transactions on, 22(10):1345–1359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing (EMNLP),</booktitle>
<pages>79--86</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="1587" citStr="Pang et al., 2002" startWordPosition="220" endWordPosition="223">Japanese, and apply a delexicalized feature transfer for Korean. Experiments with Penn Korean Treebank show that even using only the transferred features from Japanese achieves a high accuracy (81.6%) for Korean dependency parsing. Further improvements were obtained when a small annotated Korean corpus was combined with the Japanese training corpus, confirming that efficient crosslingual transfer learning can be achieved without expensive linguistic resources. 1 Introduction Motivated by increasing demands for advanced natural language processing (NLP) applications such as sentiment analysis (Pang et al., 2002; Nasukawa and Yi, 2003) and question answering (Kwok et al., 2001; Ferrucci et al., 2010), there is a growing need for accurate syntactic parsing and semantic analysis of languages, especially for non-English languages with limited linguistic resources. In this paper, we propose a new dependency parsing method for Korean which requires minimal human supervision. Dependency parsing can handle long-distance relationships and coordination phenomena very well, and has proven to be very effective for parsing free-order languages such as Korean and Japanese (K¨ubler et al., 2009). Most statistical </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing (EMNLP), pages 79–86, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jungyeul Park</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Key-Sun Choi</author>
</authors>
<title>Towards fully lexicalized dependency parsing for Korean.</title>
<date>2013</date>
<booktitle>In Proceedings of The 13th International Conference on Parsing Technologies.</booktitle>
<contexts>
<context position="6743" citStr="Park et al., 2013" startWordPosition="1033" endWordPosition="1036">hat our algorithm can be applied without more expensive linguistic resources such as an aligned corpora or bilingual lexicons. Moreover, the delexicalized feature transfer method enables the algorithm applicable to any two languages that have similar syntactic structures. 2 Related Work 2.1 Parsing for Korean Since Korean is a morphologically-rich language, many efforts for Korean parsing have focused on automatically extracting rich lexical information such as the use of case frame patterns for the verbs (Lee et al., 2007), the acquisition of case frames and nominal phrases from raw corpora (Park et al., 2013), and effective features from phrases and their neighboring contexts (Choi and Palmer, 2011). Recently, Choi et al. (2012) discussed the transformation of eojeol-based Korean treebank to entity-based treebank to effectively train probabilistic CFG parsers. We apply similar techniques as in (Choi et al., 2012) to mitigate the differences between Korean and Japanese syntactic structures. Chung and Rim (2003) applied the Triplet/Quadruplet Model for Korean parsing as done in our work. They reported that the model performed well for long-distance dependencies, but, in their experiments, the number</context>
</contexts>
<marker>Park, Kawahara, Kurohashi, Choi, 2013</marker>
<rawString>Jungyeul Park, Daisuke Kawahara, Sadao Kurohashi, and Key-Sun Choi. 2013. Towards fully lexicalized dependency parsing for Korean. In Proceedings of The 13th International Conference on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Seginer</author>
</authors>
<title>Fast unsupervised incremental parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>384--391</pages>
<contexts>
<context position="2890" citStr="Seginer, 2007" startWordPosition="431" endWordPosition="432">ationships, but it is very expensive to create a large number of consistent annotations. Recently, treebanks have become available for many languages such as English, German, Arabic, and Chinese. However, the parsing results on these treebanks vary a lot depending on the size of annotated sentences and the type of annotations (Levy and Manning, 2003; McDonald et al., 2013). Further, many languages lack annotated corpus, or the size of the annotated corpus is too small to develop a reliable statistical method. To address these problems, there have been several attempts at unsupervised parsing (Seginer, 2007; Spitkovsky et al., 2011), grammar induction (Klein and Manning, 2004; Naseem et al., 2010), and cross-lingual transfer learning using annotated corpora of other languages (McDonald et al., 2011). However, the accuracies of unsupervised methods are unacceptably low, and results from cross-lingual transfer learning show different outcomes for different pairs of languages, but, in most cases, the parsing accuracy is still low for practical purposes. A recent study by McDonald et al. (2013) concludes that cross-lingual transfer learning is beneficial when the source and target languages were sim</context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Yoav Seginer. 2007. Fast unsupervised incremental parsing. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 384–391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Noah A Smith</author>
</authors>
<title>Bilingual parsing with factored estimation: Using English to parse Korean.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="7680" citStr="Smith and Smith (2004)" startWordPosition="1175" endWordPosition="1178">itigate the differences between Korean and Japanese syntactic structures. Chung and Rim (2003) applied the Triplet/Quadruplet Model for Korean parsing as done in our work. They reported that the model performed well for long-distance dependencies, but, in their experiments, the number of modification candidates was not effectively reduced (only 91.5% of phrases were in one of the three positions, while it was 98.6% in Kanayama’s work for Japanese). In this paper, we introduce more sophisticated grammatical knowledge and heuristics to have similar dependency distributions in the two languages. Smith and Smith (2004) attempted a bilingual parsing for English and Korean by combining statistical dependency parsers, probabilistic context-free grammars, and word translation models into a unified framework that jointly searches for the best English parse, Korean parse and word alignment. However, we utilize an existing parser and align the features from the source language to the features for the target language, and, thus, our method is applicable to situations where there is no aligned corpora or word translation models. 2.2 Transfer learning and domain adaptation Recently, transfer learning has attracted mu</context>
</contexts>
<marker>Smith, Smith, 2004</marker>
<rawString>David A. Smith and Noah A. Smith. 2004. Bilingual parsing with factored estimation: Using English to parse Korean. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Angel X Chang</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Unsupervised dependency parsing without gold part-of-speech tags.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1281--1290</pages>
<contexts>
<context position="2916" citStr="Spitkovsky et al., 2011" startWordPosition="433" endWordPosition="436"> it is very expensive to create a large number of consistent annotations. Recently, treebanks have become available for many languages such as English, German, Arabic, and Chinese. However, the parsing results on these treebanks vary a lot depending on the size of annotated sentences and the type of annotations (Levy and Manning, 2003; McDonald et al., 2013). Further, many languages lack annotated corpus, or the size of the annotated corpus is too small to develop a reliable statistical method. To address these problems, there have been several attempts at unsupervised parsing (Seginer, 2007; Spitkovsky et al., 2011), grammar induction (Klein and Manning, 2004; Naseem et al., 2010), and cross-lingual transfer learning using annotated corpora of other languages (McDonald et al., 2011). However, the accuracies of unsupervised methods are unacceptably low, and results from cross-lingual transfer learning show different outcomes for different pairs of languages, but, in most cases, the parsing accuracy is still low for practical purposes. A recent study by McDonald et al. (2013) concludes that cross-lingual transfer learning is beneficial when the source and target languages were similar. In particular, it re</context>
</contexts>
<marker>Spitkovsky, Alshawi, Chang, Jurafsky, 2011</marker>
<rawString>Valentin I Spitkovsky, Hiyan Alshawi, Angel X Chang, and Daniel Jurafsky. 2011. Unsupervised dependency parsing without gold part-of-speech tags. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1281– 1290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>Philip Resnik</author>
</authors>
<title>Crosslanguage parser adaptation between related languages. In</title>
<date>2008</date>
<booktitle>IJCNLP,</booktitle>
<pages>35--42</pages>
<contexts>
<context position="9625" citStr="Zeman and Resnik, 2008" startWordPosition="1484" endWordPosition="1487">hrasal unit eojeol. �� /n ffi/pc ‘wife-NOM’ R-Df 7&apos;7✓A0) )ATr3)aUh7�_- $/v f/aux 77✓A/np OJ/pn RfT/n ;6�rXJv/n Vpc ‘buy-PAST’ ‘France-GEN’ ‘travel bag-ACC’ WM, &amp;1I/n lVpc ‘friend-DAT’ JQ-tl-fv� A-/v fV N/aux �/ ‘want to show’ I V V b1 b2 b3 b4 b5 bs Figure 2: A dependency structure of a Japanese sentence which corresponds to the Korean sentence in Figure 1, “妻が買ったフランスの旅行かばんを友達に見せたい。”. Each box corresponds to a Japanese phrasal unit bunsetsu. syntactic parsing, where a parsing model for a target language is learned from linguistic resources in one or more different languages (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Durrett et al., 2012; Georgi et al., 2012; Naseem et al., 2012). McDonald et al. (2011) proposed a delexicalized parsing model for crosslingual dependency parsing and demonstrated that a high accuracy parsing was achieved for IndoEuropean languages where significant amount of parallel texts exist. However, in more recent work, McDonald et al. (2013) showed that, unlike transfer learning within close language families, building a Korean parser from European languages was not successful with a very low accuracy. Durrett et al. (2012) and Georgi et al. (2012) show that tr</context>
</contexts>
<marker>Zeman, Resnik, 2008</marker>
<rawString>Daniel Zeman and Philip Resnik. 2008. Crosslanguage parser adaptation between related languages. In IJCNLP, pages 35–42.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
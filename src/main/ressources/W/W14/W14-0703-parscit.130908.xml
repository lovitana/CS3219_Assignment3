<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000624">
<title confidence="0.991415">
Automatic detection of causal relations in German multilogs
</title>
<author confidence="0.997946">
Tina B¨ogel Annette Hautli-Janisz Sebastian Sulger Miriam Butt
</author>
<affiliation confidence="0.9978775">
Department of Linguistics
University of Konstanz
</affiliation>
<email confidence="0.737409">
firstname.lastname@uni-konstanz.de
</email>
<sectionHeader confidence="0.987114" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951631578947">
This paper introduces a linguistically-
motivated, rule-based annotation system
for causal discourse relations in transcripts
of spoken multilogs in German. The over-
all aim is an automatic means of determin-
ing the degree of justification provided by
a speaker in the delivery of an argument
in a multiparty discussion. The system
comprises of two parts: A disambiguation
module which differentiates causal con-
nectors from their other senses, and a dis-
course relation annotation system which
marks the spans of text that constitute the
reason and the result/conclusion expressed
by the causal relation. The system is eval-
uated against a gold standard of German
transcribed spoken dialogue. The results
show that our system performs reliably
well with respect to both tasks.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999320625">
In general, causality refers to the way of know-
ing whether one state of affairs is causally related
to another.1 Within linguistics, causality has long
been established as a central phenomenon for in-
vestigation. In this paper, we look at causality
from the perspective of a research question from
political science, where the notion is particularly
important when it comes to determining (a.o.) the
deliberative quality of a discussion. The notion of
deliberation is originally due to Habermas (1981),
who assumes that within a deliberative democ-
racy, stakeholders participating in a multilog, i.e.
a multi-party conversation, justify their positions
truthfully, rationally and respectfully and eventu-
ally defer to the better argument. Within polit-
ical science, the question arises whether actual
</bodyText>
<footnote confidence="0.998538666666667">
1This work is part of the BMBF funded eHumanities
project VisArgue, an interdisciplinary cooperation between
political science, computer science and linguistics.
</footnote>
<bodyText confidence="0.999818964285714">
multilogs conducted in the process of a demo-
cratic decision making indeed follow this ideal
and whether/how one can use automatic means to
analyze the degree of deliberativity of a multilog
(Dryzek (1990; 2000), Bohman (1996), Gutmann
and Thompson (1996), Holzinger and Landwehr
(2010)). The disambiguation of causal discourse
markers and the determination of the relations they
entail is a crucial aspect of measuring the delibera-
tive quality of a multilog. In this paper, we develop
a system that is designed to perform this task.
We describe a linguistically motivated, rule-
based annotation system for German which disam-
biguates the multiple usages of causal discourse
connectors in the language and reliably annotates
the reason and result/conclusion relations that the
connectors introduce. The paper proceeds as fol-
lows: Section 2 briefly reviews related work on the
automatic extraction and annotation of causal rela-
tions, followed by a set of examples that illustrate
some of the linguistic patterns in German (Sec-
tion 3). We then introduce our rule-based anno-
tation system (Section 4) and evaluate it against a
hand-crafted gold standard in Section 5, where we
also present the results from the same annotation
task performed by a group of human annotators.
In Section 6, we provide an in-depth system error
analysis. Section 7 concludes the paper.
</bodyText>
<sectionHeader confidence="0.99969" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999733818181818">
The automatic detection and annotation of causal-
ity in language has been approached from various
angles, for example by providing gold-standard,
(manually) annotated resources such as the Penn
Discourse Treebank for English (Prasad et al.,
2008), which was used, e.g., in the disambigua-
tion of English connectives by Pitler and Nenkova
(2009), the Potsdam Commentary Corpus for Ger-
man (Stede, 2004) and the discourse annotation
layer of T¨uba-D/Z, a corpus of written German
text (Versley and Gastel, 2012). Training auto-
</bodyText>
<page confidence="0.940203">
20
</page>
<note confidence="0.8365935">
Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 20–27,
Gothenburg, Sweden, April 26, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999867452380953">
matic systems that learn patterns of causality (Do
et al., 2011; Mulkar-Mehta et al., 2011b, inter
alia) is a crucial factor in measuring discourse
coherence (Sanders, 2005), and is beneficial in
approaches to question-answering (Girju, 2003;
Prasad and Joshi, 2008).
With respect to automatically detecting causal
relations in German, Versley (2010) uses English
training data from the Penn Discourse Treebank in
order to train an English annotation model. These
English annotations can be projected to German
in an English-German parallel corpus and on the
basis of this a classifier of German discourse rela-
tions is trained. However, as previous studies have
shown (Mulkar-Mehta et al., 2011a, inter alia), the
reliability of detecting causal relations with auto-
matic means differs highly between different gen-
res. Our data consist of transcriptions of originally
spoken multilogs and this type of data differs sub-
stantially from newspaper or other written texts.
Regarding the disambiguation of German con-
nectives, Schneider and Stede (2012) carried out
a corpus study of 42 German discourse connec-
tives which are listed by Dipper and Stede (2006)
as exhibiting a certain degree of ambiguity. Their
results indicate that for a majority of ambigu-
ous connectives, plain POS tagging is not reliable
enough, and even contextual POS patterns are not
sufficient in all cases. This is the same conclu-
sion drawn by Dipper and Stede (2006), who also
state that off-the-shelf POS taggers are too unre-
liable for the task. They instead suggest a map-
ping approach for 9 out of the 42 connectives
and show that this assists considerably with dis-
ambiguation. As this also tallies with our experi-
ments with POS taggers, we decided to implement
a rule-based disambiguation module. This mod-
ule takes into account contextual patterns and fea-
tures of spoken communication and reliably de-
tects causal connectors as well as the reason and
result/conclusion discourse relations expressed in
the connected clauses.
</bodyText>
<sectionHeader confidence="0.98255" genericHeader="method">
3 Linguistic phenomenon
</sectionHeader>
<bodyText confidence="0.999972523809524">
In general, causality can hold between single
concepts, e.g. between ‘smoke’ and ‘fire’, or be-
tween larger phrases. The phrases can be put into
a causal relation via overt discourse connectors
like ‘because’ or ‘as’, whereas other phrases en-
code causality implicitly by taking into account
world knowledge about the connected events. In
this paper, we restrict ourselves to the analysis of
explicit discourse markers; in particular we inves-
tigate the eight most frequent German causal con-
nectors, listed in Table 1. The markers of reason
on the left head a subordinate clause that describes
the cause of an effect stated in the matrix clause
(or in the previous sentence(s)). The markers of
result/conclusion on the other hand introduce a
clause that describes the overall effect of a cause
contained in the preceding clause/sentence(s). In
the genre of argumentation that we are working
with, the “results” tend to be logical conclusions
that the speaker sees as following irrevocably from
the cause presented in the argument.
</bodyText>
<sectionHeader confidence="0.753243" genericHeader="method">
Reason Result
</sectionHeader>
<bodyText confidence="0.9875646">
‘because of’ ‘thus’
daher
darum
deshalb
deswegen
</bodyText>
<tableCaption confidence="0.983513">
Table 1: German causal discourse connectors
</tableCaption>
<bodyText confidence="0.9672281875">
The sentences in (1) and (2) provide exam-
ples of the phenomenon of explicit causal mark-
ers in German in our multilogs. Note that all
of the causal markers in Table 1 connect a re-
sult/conclusion with a cause/reason. The differ-
ence lies in which of these relations is expressed
in the clause headed by the causal connector.
The constructions in (1) and (2) exemplify this.2
In (1), da ‘since’ introduces the reason for the con-
clusion in the matrix clause, i.e., the reason for
the travel times being irrelevant is that they are not
carried out as specified. In (2), daher ‘thus’ heads
the conclusion of the reason which is provided in
the matrix clause: Because the speaker has never
stated a fact, the accusation of the interlocutor is
not correct.
There are several challenges in the automatic
annotation of these relations. First, some of the
connectors can be ambiguous. In our case, four
out of the eight causal discourse connectors in Ta-
ble 1 are ambiguous (da, denn, daher and darum)
and have, in addition to their causal meaning, tem-
poral, locational or other usages. In example (3),
denn is used as a particle signaling disbelief, while
daher is used as a locational verb particle, having,
together with the verb ‘to come’, the interpretation
2These examples are taken from the Stuttgart 21 arbitra-
tion process, see section 5.1 for more information.
da
weil
denn
zumal
</bodyText>
<page confidence="0.98951">
21
</page>
<listItem confidence="0.845759">
(1) Diese Fahrzeiten sind irrelevant, da sie so nicht gefahren werden.
Art.Dem travel time.Pl be.3.Pl irrelevant because they like not drive.Perf.Part be.Fut.3.Pl
</listItem>
<sectionHeader confidence="0.804038" genericHeader="method">
Result/Conclusion Reason
</sectionHeader>
<bodyText confidence="0.832235">
‘These travel times are irrelevant, because they are not executed as specified.’
</bodyText>
<listItem confidence="0.968998">
(2) Das habe ich nicht gesagt, daher ist Ihr Vorwurf nicht richtig
Pron have.Pres.1.Sg I not say.Past.Part thus be.3.Sg you.Sg.Pol/Pl accusation not correct
</listItem>
<sectionHeader confidence="0.717931" genericHeader="method">
Reason Result/Conclusion
</sectionHeader>
<bodyText confidence="0.707571">
‘I did not say that, therefore your accusation is not correct.’
</bodyText>
<listItem confidence="0.985832125">
(3) Wie kommen Sie denn daher?
how come.Inf you.Sg.Pol then VPart
‘What is your problem anyway?’ (lit. ‘In what manner are you coming here?’)
(4) Da bin ich mir nicht sicher.
there be.Pres.1.Sg I I.Dat not sure
‘I’m not sure about that.’
(5) Das kommt daher, dass keiner etwas sagt.
Pron come.Pres.3.Sg thus that nobody something say.Pres.3.Sg
</listItem>
<sectionHeader confidence="0.919872" genericHeader="method">
Result/Conclusion Reason
</sectionHeader>
<bodyText confidence="0.99458585">
‘This is because nobody says anything.’
of ‘coming from somewhere to where the speaker
is’ (literally and metaphorically). In a second ex-
ample in (4), da is used as the pronominal ‘there’.
Second, some of the causal connectors do not
always work the same way. In (5), the re-
sult/conclusion connector daher does not head
an embedded clause, rather it is part of the
matrix clause. In this case, the embedded
clause expresses the reason rather than the re-
sult/conclusion. A third challenge is the span of
the respective reason and result. While there are
some indications as to how to define the stretch
of these spans, there are some difficult challenges,
further discussed in the error analysis in Section 6.
In the following, we present the rule-based an-
notation system, which deals with the identifica-
tion of phrases expressing the result and reason,
along the lines illustrated in (1) and (2), as well as
with the disambiguation of causal connectors.
</bodyText>
<sectionHeader confidence="0.959725" genericHeader="method">
4 Rule-based annotation system
</sectionHeader>
<bodyText confidence="0.999897827586207">
The automatic annotation system that we intro-
duce is based on a linguistically informed, hand-
crafted set of rules that deals with the disambigua-
tion of causal markers and the identification of
causal relations in text. As a first step, we divide
all of the utterances into smaller units of text in or-
der to be able to work with a more fine-grained
structure of the discourse. Following the liter-
ature, we call these discourse units. Although
there is no consensus in the literature on what ex-
actly a discourse unit consists of, it is generally
assumed that each discourse unit describes a sin-
gle event (Polanyi et al., 2004). Following Marcu
(2000), we term these elementary discourse units
(EDUs) and approximate the assumption made by
Polanyi et al. (2004) by inserting a boundary at
every punctuation mark and every clausal con-
nector (conjunctions, complementizers). Sentence
boundaries are additionally marked.
The annotation of discourse information is per-
formed at the level of EDUs. There are sometimes
instances in which a given relation such as “rea-
son” spans multiple EDUs. In these cases, each of
the EDUs involved is marked/annotated individu-
ally with the appropriate relation.
In the following, we briefly lay out the two ele-
ments of the annotation system, namely the disam-
biguation module and the system for identifying
the causal relations.
</bodyText>
<page confidence="0.982475">
22
</page>
<subsectionHeader confidence="0.889535">
4.1 Disambiguation
</subsectionHeader>
<bodyText confidence="0.999867533333333">
As shown in the examples above, markers like
da, denn, darum and daher ‘because/thus’ have a
number of different senses. The results presented
in Dipper and Stede (2006) indicate that POS tag-
ging alone does not help in disambiguating the
causal usages from the other functions, particu-
larly not for our data type, which includes much
noise and exceptional constructions that are not
present in written corpora. As a consequence, we
propose a set of rules built on heuristics, which
take into account a number of factors in the clause
in order to disambiguate the connector. To il-
lustrate the underlying procedure, (6) schematizes
part of the disambiguation rule for the German
causal connector da ‘since’.
</bodyText>
<listItem confidence="0.657578">
(6) IF da is not followed directly by a verb AND
no other particle or connector precedes da
AND
</listItem>
<bodyText confidence="0.994006">
da is not late in the EDU THEN
da is a causal connector.
In total, the system comprises of 37 rules that
disambiguate the causal connectors shown in Ta-
ble 1. The evaluation in Section 5 shows that the
system performs well overall.3
</bodyText>
<subsectionHeader confidence="0.944663">
4.2 Relation identification
</subsectionHeader>
<bodyText confidence="0.982822965517242">
After disambiguation, a second set of rules anno-
tates discourse units as being part of the reason or
the result portion of a causal relation. One aspect
of deliberation is the assumption that participants
in a negotiation justify their positions. Therefore,
in this paper, we analyze causal relations within a
3Two reviewers expressed interest in being able to access
our full set of rules. Their reasons were two-fold. For one,
sharing our rules would benefit a larger community. For an-
other, the reviewers cited concerns with respect to replicabil-
ity. With respect to the first concern, we will naturally be
happy to share our rule set with interested researchers. With
respect to the second concern, it is not clear to us that we
have understood it. As far as we can tell, what seems to be at
the root of the comments is a very narrow notion of replica-
bility, one which involves a freely available corpus in combi-
nation with a freely available automatic processing tool (e.g.,
a machine learning algorithm) that can then be used together
without the need of specialist language knowledge. We freely
admit that our approach requires specialist linguistic training,
but would like to note that linguistic analysis is routinely sub-
ject to replicability in the sense that given a set of data, the
linguistic analysis arrived at should be consistent across dif-
ferent sets of linguists. In this sense, our work is immediately
replicable. Moreover, given the publically available S21 data
set and the easily accessible and comprehensive descriptions
of German grammar, replication of our work is eminently
possible.
single utterance of a speaker, i.e., causal relations
that are expressed in a sequence of clauses which
a speaker utters without interference from another
speaker. As a consequence, the annotation system
does not take into account causal relations that are
split up between utterances of one speaker or ut-
terances of different speakers.
Nevertheless, the reason and result portion
of a causal relation can extend over multiple
EDUs/sentences and this means that not only EDUs
which contain the connector itself are annotated,
but preceding/following units that are part of the
causal relation also have to be marked. This in-
volves deep linguistic knowledge about the cues
that delimit or license relations, information which
is encoded in a set of heuristics that feed the 20 dif-
ferent annotation rules and mark the relevant units.
An example for a (simplified) relation annotation
is given in (7).
(7) IF result connector not in first EDU of sen-
tence AND
result connector not preceded by other con-
nector within same sentence THEN
mark every EDU from sentence beginning to
current EDU with reason.
ELSIF result connector in first EDU of sen-
tence THEN
mark every EDU in previous sentence with
reason UNLESS
encountering another connector.
</bodyText>
<sectionHeader confidence="0.997806" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9999762">
The evaluation is split into two parts. On the one
hand, we evaluate the inter-annotator agreement
between five, minimally trained annotators (§5.2).
On the other hand, we evaluate the rule-based
annotation system against this hand-crafted gold-
standard (§5.3). Each evaluation is again split into
two parts: One concerns the successful identifica-
tion of the causal connectors. The other concerns
the identification of the spans of multilog that in-
dicate a result/conclusion vs. a reason.
</bodyText>
<subsectionHeader confidence="0.948989">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999887142857143">
The underlying data comprises of two data sets,
the development and the test set. The develop-
ment set, on which the above-mentioned heuristics
for disambiguation and relation identification are
based, consists of the transcribed protocols of the
Stuttgart 21 arbitration process (henceforth: S21).
This public arbitration process took place in 2010
</bodyText>
<page confidence="0.99571">
23
</page>
<bodyText confidence="0.999968166666667">
and was concerned with a railway and urban de-
velopment project in the German city of Stuttgart.
The project remains highly controversial and has
gained international attention. In total, the tran-
scripts contain around 265.000 tokens in 1330 ut-
terances of more than 70 participants.4
The test set is based on different, but also tran-
scribed natural speech data, namely on experi-
ments simulating deliberative processes for estab-
lishing a governmental form for a hypothetical
new African country.5 For testing, we randomly
collected utterances from two versions of the ex-
periment. Each utterance contained at least two
causal discourse connectors. In total, we extracted
60 utterances with an average length of 71 words.
There are a total of 666 EDUs and 105 instances
of the markers in Table 1. The composition of the
test set for each (possible) connector is in Table 2.
</bodyText>
<table confidence="0.996004142857143">
Reason Result
‘because of’ ‘due to’
da 23 daher 10
weil 17 darum 11
denn 17 deshalb 12
zumal 4 deswegen 11
Total: 61 44
</table>
<tableCaption confidence="0.999368">
Table 2: Structure of the evaluation set
</tableCaption>
<bodyText confidence="0.999905875">
For the creation of a gold standard, the test set
was manually annotated by two linguistic experts.
238 out of 666 EDUs were marked as being part
of the reason of a causal relation, with the re-
sult/conclusion contributed by 180 EDUs. Out of
105 connectors found in the test set, 87 have a
causal usage. In 18 cases, the markers have other
functions.
</bodyText>
<subsectionHeader confidence="0.970462">
5.2 Inter-annotator agreement
</subsectionHeader>
<bodyText confidence="0.999981777777778">
The task for the annotators comprised of two parts:
First, five students (undergraduates in linguistics)
had to decide wether an occurence of one of the
elements in Table 1 was a causal marker or not.
In a second step, they had to mark the bound-
aries for the reason and result/conclusion parts of
the causal relation, based on the boundaries of the
automatically generated EDUs. Their annotation
choice was not restricted by, e.g., instructing them
</bodyText>
<footnote confidence="0.9916948">
4The transcripts are publicly available for down-
load under http://stuttgart21.wikiwam.de/
Schlichtungsprotokolle
5These have been produced by our collaborators in polit-
ical science, Katharina Holzinger and Valentin Gold.
</footnote>
<bodyText confidence="0.9997467">
to choose a ‘wider’ or more ‘narrow’ span when
in doubt. These tasks served two purposes: On
the one hand, we were able to evaluate how easily
causal markers can be disambiguated from their
other usages and how clearly they introduce either
the reason or the result/conclusion of a causal re-
lation. On the other hand, we gained insights into
what span of discourse native speakers take to con-
stitute a result/conclusion and cause/reason.
For calculating the inter-annotator agreement
(IAA), we used Fleiss’ kappa (Fleiss, 1971), which
measures the reliability of the agreement between
more than two annotators. In the disambiguation
task, the annotators’ kappa is κ = 0.96 (“almost
perfect agreement”), which shows that the annota-
tors exhibit a high degree of confidence when dif-
ferentiating between causal and other usages of the
markers. When marking whether a connector an-
notates the reason or the result/conclusion portion
of a causal relation, the annotators have a kappa
of κ = 0.86. This shows that not only are anno-
tators capable of reliably disambiguating connec-
tors, they are also reliably labeling each connector
with the correct causal relation.
In evaluating the IAA of the spans, we mea-
sured three types of relations (reason, result and
no causal relation) over the whole utterance, i.e.
each EDU which is neither part of the result nor the
reason relation was tagged as having no causal re-
lation. We calculated four different κ values: one
for each relation type (vs. all other relation types),
and one across all relation types. The IAA fig-
ures are summarized in Table 3: For the causal
relation types, κReason=0.86 and κResult=0.90 in-
dicate near-perfect agreement. κ is significantly
higher for causal EDUs than for non-causal (i.e.,
unmarked) EDUs (κNon-causal=0.82); this is in fact
expected since causal EDUs are the marked case
and are thus easier to identify for annotators in a
coherent manner.
</bodyText>
<table confidence="0.900653">
IAA
κReason 0.86
κResult 0.90
κNon-causal 0.82
κAll 0.73
</table>
<tableCaption confidence="0.998603">
Table 3: IAA of span annotations
</tableCaption>
<figureCaption confidence="0.501853333333333">
Across all relation types, κAll=0.73 indicates
“substantial agreement”. The drop in the agree-
ment is anticipated and mirrors the problem that
</figureCaption>
<page confidence="0.99668">
24
</page>
<bodyText confidence="0.99996275">
is generally found in the literature when evalu-
ating spans of discourse relations (Sporleder and
Lascarides, 2008). First, measuring κAll involves
three categories, whereas the other measures in-
volve two. Second, a preliminary error analysis
shows that there is substantial disagreement re-
garding the extent of both reason and result spans.
The examples in (8)–(9) illustrate this. While an-
notator 1 marks the result span (indicated by the
( S tag) as starting at the beginning of the sentence,
annotator 2 excludes the first EDU from the result
span.6 In such cases, we thus register a mismatch
in the annotation of the first EDU.
Nevertheless, the numbers indicate a substantial
agreement. We thus conclude that the task we set
the annotators could be accomplished reliably.
</bodyText>
<subsectionHeader confidence="0.999376">
5.3 System performance
</subsectionHeader>
<bodyText confidence="0.99996525">
In order to evaluate the automatic annotation sys-
tem described in Section 4, we match the system
output against the manually-annotated gold stan-
dard, calculating precision, recall and (balanced)
f-score of the annotation. For the disambiguation
of the connectors in terms of causal versus other
usages, the system performs as shown in Table 4
(the 0 indicates the average of both values).
</bodyText>
<table confidence="0.99978875">
Precision Recall F-score
Causal 1 0.94 0.97
Non-causal 0.85 1 0.92
0 0.93 0.97 0.95
</table>
<tableCaption confidence="0.999489">
Table 4: Causal marker disambiguation
</tableCaption>
<bodyText confidence="0.999961235294118">
This result is very promising and shows that
even though the development data consists of data
from a different source, the patterns in the de-
velopment set are mirrored in the test set. This
means that the genre of the spoken exchange of
arguments in a multilog does not exhibit the dif-
ferences usually found when looking at data from
different genres, as Mulkar-Mehta et al. (2011a)
report when comparing newspaper articles from fi-
nance and sport.
For evaluating the annotated spans of reason
and result, we base the calculation on whether an
EDU is marked with a particular relation or not, i.e.
if the system marks an EDU as belonging to the
reason or result part of a particular causal marker
and the gold standard encodes the same informa-
tion, then the two discourse units match. As a con-
</bodyText>
<footnote confidence="0.462656">
6We use the I sign to indicate EDU boundaries.
</footnote>
<bodyText confidence="0.999627">
sequence, spans which do not match perfectly, for
example in cases where their boundaries do not
match, are not treated as non-matching instances
as a whole, but are considered to be made up of
smaller units which match individually. Table 5
shows the results.
</bodyText>
<table confidence="0.99975625">
Precision Recall F-score
Reason 0.88 0.75 0.81
Result 0.81 0.94 0.87
0 0.84 0.84 0.84
</table>
<tableCaption confidence="0.998196">
Table 5: Results for relation identification
</tableCaption>
<bodyText confidence="0.9999441">
These results are promising insofar as the de-
tection of spans of causal relations is known to be
a problem. Again, this shows that development
and test set seem to exhibit similar patterns, de-
spite their different origins (actual political argu-
mentation vs. an experimental set-up). In the fol-
lowing, we present a detailed error analysis and
show that we find recurrent patterns of mismatch,
most of which can in principle be dealt with quite
straightforwardly.
</bodyText>
<sectionHeader confidence="0.981799" genericHeader="conclusions">
6 Error analysis
</sectionHeader>
<figureCaption confidence="0.997788">
Figure 1: Error analysis, in percent.
</figureCaption>
<bodyText confidence="0.997724625">
Figure 1 shows a pie chart in which each prob-
lem is identified and shown with its share in
the overall error occurrence. In total, the sys-
tem makes 26 annotation errors. Starting from
the top, empty connector position refers to struc-
tures which an annotator can easily define as rea-
son/result, but which do not contain an overt con-
nector. This causes the automatic annotation sys-
</bodyText>
<page confidence="0.989533">
25
</page>
<listItem confidence="0.5511515">
(8) Annotator 1:
( S Ich m¨ochte an dieser Stelle einwerfen, I dass die Frage, ob ...
</listItem>
<bodyText confidence="0.9050165">
I would like.Pres.1.Sg at this point add.Inf that the question if ...
‘I’d like to add at this point that the question if...
</bodyText>
<listItem confidence="0.692102">
(9) Annotator 2:
</listItem>
<bodyText confidence="0.997099114285714">
Ich m¨ochte an dieser Stelle einwerfen, ( S dass die Frage, ob ...
I would like.Pres.1.Sg at this point add.Inf that the question if ...
‘I’d like to add at this point that the question if...
tem to fail. The group of other connectors refers to
cases where a non-causal connector (e.g., the ad-
versative conjunction aber ‘but’) signals the end
of the result/conclusion or cause span for a human
annotator. The presence of these other connectors
and their effect is not yet taken into account by the
automatic annotation system. The error group iaa
refers to the cases where we find a debatable dif-
ference of opinion with respect to the length of a
span. Speaker opinion refers to those cases where
a statement starts with expressions like “I believe
/ I think / in my opinion etc.”. These are mostly
excluded from a relation span by human anno-
tators, but (again: as of yet) not by the system.
Span over several sentences refers to those cases
where the span includes several sentences. And
last, but not least, since the corpus consists of spo-
ken data, an external transcriptor had to transcribe
the speech signal into written text. Some low-level
errors in this category are missing sentence punc-
tuation. The human annotators were able to com-
pensate for this, but not the automatic system.
Roughly, three groups of errors can be distin-
guished. Some of the errors are relatively easy
to solve, by, e.g., adding another class of con-
nectors, by adding expressions or by correcting
the transcriptors script. A second group (span
over several sentences and empty connector po-
sition) needs a much more sophisticated system,
including deep linguistic knowledge on semantics,
pragmatics and notoriously difficult aspects of dis-
course analysis like anaphora resolution.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="acknowledgments">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999980538461538">
In conclusion, we have presented an automatic an-
notation system which can reliably and precisely
detect German causal relations with respect to
eight causal connectors in multilogs in which ar-
guments are exchanged and each party is trying to
convince the other of the rightness of their stance.
Our system is rule-based and takes into account
linguistic knowledge at a similar level as that used
by human annotators. Our work will directly ben-
efit research in political science as it can flow into
providing one measure for the deliberative qual-
ity of a multilog, namely, do interlocutors support
their arguments with reasons or not?
</bodyText>
<sectionHeader confidence="0.998952" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999702151515152">
James Bohman. 1996. Public Deliberation: Plural-
ism, Complexity and Democracy. The MIT Press,
Cambridge, MA.
Stefanie Dipper and Manfred Stede. 2006. Disam-
biguating potential connectives. In Proceedings of
KONVENS (Conference on Natural Language Pro-
cessing) 2006.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally Supervised Event Causality Identifica-
tion. In Proceedings of EMNLP’11, pages 294–303.
John S. Dryzek. 1990. Discursive Democracy: Poli-
tics, Policy, and Political Science. Cambridge Uni-
versity Press, Cambridge, MA.
John S. Dryzek. 2000. Deliberative Democracy and
Beyond: Liberals, Critics, Contestations. Oxford
University Press, Oxford.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5):378–382.
Roxana Girju. 2003. Automatic Detection of Causal
Relations for Question-Answering. In Proceedings
of the ACL Workshop on Multilingual summariza-
tion and question-answering, pages 76–83.
Amy Gutmann and Dennis Frank Thompson. 1996.
Democracy and Disagreement. Why moral conflict
cannot be avoided in politics, and what should be
done about it. Harvard University Press, Cam-
bridge, MA.
J¨urgen Habermas. 1981. Theorie des kommunikativen
Handelns. Suhrkamp, Frankfurt am Main.
Katharina Holzinger and Claudia Landwehr. 2010. In-
stitutional determinants of deliberative interaction.
European Political Science Review, 2:373–400.
</reference>
<page confidence="0.948774">
26
</page>
<reference confidence="0.999604673076923">
Daniel Marcu. 2000. The Theory and Practice of
Discourse Parsing and Summarization. MIT Press,
Cambridge, Mass.
Rutu Mulkar-Mehta, Andrew S. Gordon, Jerry Hobbs,
and Eduard Hovy. 2011a. Causal markers across
domains and genres of discourse. In The 6th Inter-
national Conference on Knowledge Capture.
Rutu Mulkar-Mehta, Christopher Welty, Jerry R.
Hoobs, and Eduard Hovy. 2011b. Using granularity
concepts for discovering causal relations. In Pro-
ceedings of the FLAIRS conference.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text.
In Proceedings of ACL-IJCNLP, pages 13–16.
Livia Polanyi, Chris Culy, Martin van den Berg,
Gian Lorenzo Thione, and David Ahn. 2004. Sen-
tential structure and discourse parsing. In Proceed-
ings of the 2004 ACL Workshop on Discourse Anno-
tation, pages 80–87.
Rashmi Prasad and Aravind Joshi. 2008. A Discourse-
based Approach to Generating Why-Questions from
Texts. In Proceedings of the Workshop on the Ques-
tion Generation Shared Task and Evaluation Chal-
lenge.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of LREC 2008, pages 2961–2968.
Ted Sanders. 2005. Coherence, Causality and Cog-
nitive Complexity in Discourse. In Proceedings of
SEM-05, First International Symposium on the Ex-
ploratiaon and Modelling of Meaning, pages 105–
114.
Angela Schneider and Manfred Stede. 2012. Ambi-
guity in German Connectives: A Corpus Study. In
Proceedings of KONVENS (Conference on Natural
Language Processing) 2012.
Caroline Sporleder and Alex Lascarides. 2008. Us-
ing Automatically Labelled Examples to Classify
Rhetorical Relations: An Assessment. Natural Lan-
guage Engineering, 14(3):369–416.
Manfred Stede. 2004. The Potsdam Commentary Cor-
pus. In In Proceedings of the ACL’04 Workshop on
Discourse Annotation, pages 96–102.
Yannick Versley and Anna Gastel. 2012. Linguistic
Tests for Discourse Relations in the T¨uba-D/Z Cor-
pus of Written German. Dialogue and Discourse,
1(2):1–24.
Yannick Versley. 2010. Discovery of Ambiguous and
Unambiguous Discourse Connectives via Annota-
tion Projection. In Workshop on the Annotation and
Exploitation of Parallel Corpora (AEPC).
</reference>
<page confidence="0.998812">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.973823">
<title confidence="0.999435">Automatic detection of causal relations in German multilogs</title>
<author confidence="0.99999">Tina B¨ogel Annette Hautli-Janisz Sebastian Sulger Miriam Butt</author>
<affiliation confidence="0.9996815">Department of University of Konstanz</affiliation>
<email confidence="0.998029">firstname.lastname@uni-konstanz.de</email>
<abstract confidence="0.9988362">This paper introduces a linguisticallymotivated, rule-based annotation system for causal discourse relations in transcripts of spoken multilogs in German. The overall aim is an automatic means of determining the degree of justification provided by a speaker in the delivery of an argument in a multiparty discussion. The system comprises of two parts: A disambiguation module which differentiates causal connectors from their other senses, and a discourse relation annotation system which marks the spans of text that constitute the reason and the result/conclusion expressed by the causal relation. The system is evaluated against a gold standard of German transcribed spoken dialogue. The results show that our system performs reliably well with respect to both tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Bohman</author>
</authors>
<title>Public Deliberation: Pluralism, Complexity and Democracy.</title>
<date>1996</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2186" citStr="Bohman (1996)" startWordPosition="321" endWordPosition="322">ipating in a multilog, i.e. a multi-party conversation, justify their positions truthfully, rationally and respectfully and eventually defer to the better argument. Within political science, the question arises whether actual 1This work is part of the BMBF funded eHumanities project VisArgue, an interdisciplinary cooperation between political science, computer science and linguistics. multilogs conducted in the process of a democratic decision making indeed follow this ideal and whether/how one can use automatic means to analyze the degree of deliberativity of a multilog (Dryzek (1990; 2000), Bohman (1996), Gutmann and Thompson (1996), Holzinger and Landwehr (2010)). The disambiguation of causal discourse markers and the determination of the relations they entail is a crucial aspect of measuring the deliberative quality of a multilog. In this paper, we develop a system that is designed to perform this task. We describe a linguistically motivated, rulebased annotation system for German which disambiguates the multiple usages of causal discourse connectors in the language and reliably annotates the reason and result/conclusion relations that the connectors introduce. The paper proceeds as follows</context>
</contexts>
<marker>Bohman, 1996</marker>
<rawString>James Bohman. 1996. Public Deliberation: Pluralism, Complexity and Democracy. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Dipper</author>
<author>Manfred Stede</author>
</authors>
<title>Disambiguating potential connectives.</title>
<date>2006</date>
<booktitle>In Proceedings of KONVENS (Conference on Natural Language Processing)</booktitle>
<contexts>
<context position="5199" citStr="Dipper and Stede (2006)" startWordPosition="784" endWordPosition="787">man parallel corpus and on the basis of this a classifier of German discourse relations is trained. However, as previous studies have shown (Mulkar-Mehta et al., 2011a, inter alia), the reliability of detecting causal relations with automatic means differs highly between different genres. Our data consist of transcriptions of originally spoken multilogs and this type of data differs substantially from newspaper or other written texts. Regarding the disambiguation of German connectives, Schneider and Stede (2012) carried out a corpus study of 42 German discourse connectives which are listed by Dipper and Stede (2006) as exhibiting a certain degree of ambiguity. Their results indicate that for a majority of ambiguous connectives, plain POS tagging is not reliable enough, and even contextual POS patterns are not sufficient in all cases. This is the same conclusion drawn by Dipper and Stede (2006), who also state that off-the-shelf POS taggers are too unreliable for the task. They instead suggest a mapping approach for 9 out of the 42 connectives and show that this assists considerably with disambiguation. As this also tallies with our experiments with POS taggers, we decided to implement a rule-based disamb</context>
<context position="11997" citStr="Dipper and Stede (2006)" startWordPosition="1893" endWordPosition="1896">ation of discourse information is performed at the level of EDUs. There are sometimes instances in which a given relation such as “reason” spans multiple EDUs. In these cases, each of the EDUs involved is marked/annotated individually with the appropriate relation. In the following, we briefly lay out the two elements of the annotation system, namely the disambiguation module and the system for identifying the causal relations. 22 4.1 Disambiguation As shown in the examples above, markers like da, denn, darum and daher ‘because/thus’ have a number of different senses. The results presented in Dipper and Stede (2006) indicate that POS tagging alone does not help in disambiguating the causal usages from the other functions, particularly not for our data type, which includes much noise and exceptional constructions that are not present in written corpora. As a consequence, we propose a set of rules built on heuristics, which take into account a number of factors in the clause in order to disambiguate the connector. To illustrate the underlying procedure, (6) schematizes part of the disambiguation rule for the German causal connector da ‘since’. (6) IF da is not followed directly by a verb AND no other parti</context>
</contexts>
<marker>Dipper, Stede, 2006</marker>
<rawString>Stefanie Dipper and Manfred Stede. 2006. Disambiguating potential connectives. In Proceedings of KONVENS (Conference on Natural Language Processing) 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quang Xuan Do</author>
<author>Yee Seng Chan</author>
<author>Dan Roth</author>
</authors>
<title>Minimally Supervised Event Causality Identification.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP’11,</booktitle>
<pages>294--303</pages>
<contexts>
<context position="4111" citStr="Do et al., 2011" startWordPosition="618" endWordPosition="621">ted resources such as the Penn Discourse Treebank for English (Prasad et al., 2008), which was used, e.g., in the disambiguation of English connectives by Pitler and Nenkova (2009), the Potsdam Commentary Corpus for German (Stede, 2004) and the discourse annotation layer of T¨uba-D/Z, a corpus of written German text (Versley and Gastel, 2012). Training auto20 Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 20–27, Gothenburg, Sweden, April 26, 2014. c�2014 Association for Computational Linguistics matic systems that learn patterns of causality (Do et al., 2011; Mulkar-Mehta et al., 2011b, inter alia) is a crucial factor in measuring discourse coherence (Sanders, 2005), and is beneficial in approaches to question-answering (Girju, 2003; Prasad and Joshi, 2008). With respect to automatically detecting causal relations in German, Versley (2010) uses English training data from the Penn Discourse Treebank in order to train an English annotation model. These English annotations can be projected to German in an English-German parallel corpus and on the basis of this a classifier of German discourse relations is trained. However, as previous studies have s</context>
</contexts>
<marker>Do, Chan, Roth, 2011</marker>
<rawString>Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011. Minimally Supervised Event Causality Identification. In Proceedings of EMNLP’11, pages 294–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Dryzek</author>
</authors>
<title>Discursive Democracy: Politics, Policy, and Political Science.</title>
<date>1990</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2164" citStr="Dryzek (1990" startWordPosition="318" endWordPosition="319">, stakeholders participating in a multilog, i.e. a multi-party conversation, justify their positions truthfully, rationally and respectfully and eventually defer to the better argument. Within political science, the question arises whether actual 1This work is part of the BMBF funded eHumanities project VisArgue, an interdisciplinary cooperation between political science, computer science and linguistics. multilogs conducted in the process of a democratic decision making indeed follow this ideal and whether/how one can use automatic means to analyze the degree of deliberativity of a multilog (Dryzek (1990; 2000), Bohman (1996), Gutmann and Thompson (1996), Holzinger and Landwehr (2010)). The disambiguation of causal discourse markers and the determination of the relations they entail is a crucial aspect of measuring the deliberative quality of a multilog. In this paper, we develop a system that is designed to perform this task. We describe a linguistically motivated, rulebased annotation system for German which disambiguates the multiple usages of causal discourse connectors in the language and reliably annotates the reason and result/conclusion relations that the connectors introduce. The pap</context>
</contexts>
<marker>Dryzek, 1990</marker>
<rawString>John S. Dryzek. 1990. Discursive Democracy: Politics, Policy, and Political Science. Cambridge University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Dryzek</author>
</authors>
<title>Deliberative Democracy and Beyond: Liberals, Critics, Contestations.</title>
<date>2000</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<marker>Dryzek, 2000</marker>
<rawString>John S. Dryzek. 2000. Deliberative Democracy and Beyond: Liberals, Critics, Contestations. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="19210" citStr="Fleiss, 1971" startWordPosition="3074" endWordPosition="3075">ced by our collaborators in political science, Katharina Holzinger and Valentin Gold. to choose a ‘wider’ or more ‘narrow’ span when in doubt. These tasks served two purposes: On the one hand, we were able to evaluate how easily causal markers can be disambiguated from their other usages and how clearly they introduce either the reason or the result/conclusion of a causal relation. On the other hand, we gained insights into what span of discourse native speakers take to constitute a result/conclusion and cause/reason. For calculating the inter-annotator agreement (IAA), we used Fleiss’ kappa (Fleiss, 1971), which measures the reliability of the agreement between more than two annotators. In the disambiguation task, the annotators’ kappa is κ = 0.96 (“almost perfect agreement”), which shows that the annotators exhibit a high degree of confidence when differentiating between causal and other usages of the markers. When marking whether a connector annotates the reason or the result/conclusion portion of a causal relation, the annotators have a kappa of κ = 0.86. This shows that not only are annotators capable of reliably disambiguating connectors, they are also reliably labeling each connector wit</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
</authors>
<title>Automatic Detection of Causal Relations for Question-Answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL Workshop on Multilingual summarization and question-answering,</booktitle>
<pages>76--83</pages>
<contexts>
<context position="4289" citStr="Girju, 2003" startWordPosition="645" endWordPosition="646">the Potsdam Commentary Corpus for German (Stede, 2004) and the discourse annotation layer of T¨uba-D/Z, a corpus of written German text (Versley and Gastel, 2012). Training auto20 Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 20–27, Gothenburg, Sweden, April 26, 2014. c�2014 Association for Computational Linguistics matic systems that learn patterns of causality (Do et al., 2011; Mulkar-Mehta et al., 2011b, inter alia) is a crucial factor in measuring discourse coherence (Sanders, 2005), and is beneficial in approaches to question-answering (Girju, 2003; Prasad and Joshi, 2008). With respect to automatically detecting causal relations in German, Versley (2010) uses English training data from the Penn Discourse Treebank in order to train an English annotation model. These English annotations can be projected to German in an English-German parallel corpus and on the basis of this a classifier of German discourse relations is trained. However, as previous studies have shown (Mulkar-Mehta et al., 2011a, inter alia), the reliability of detecting causal relations with automatic means differs highly between different genres. Our data consist of tra</context>
</contexts>
<marker>Girju, 2003</marker>
<rawString>Roxana Girju. 2003. Automatic Detection of Causal Relations for Question-Answering. In Proceedings of the ACL Workshop on Multilingual summarization and question-answering, pages 76–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amy Gutmann</author>
<author>Dennis Frank Thompson</author>
</authors>
<title>Democracy and Disagreement. Why moral conflict cannot be avoided in politics, and what should be done about it.</title>
<date>1996</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2215" citStr="Gutmann and Thompson (1996)" startWordPosition="323" endWordPosition="326">ltilog, i.e. a multi-party conversation, justify their positions truthfully, rationally and respectfully and eventually defer to the better argument. Within political science, the question arises whether actual 1This work is part of the BMBF funded eHumanities project VisArgue, an interdisciplinary cooperation between political science, computer science and linguistics. multilogs conducted in the process of a democratic decision making indeed follow this ideal and whether/how one can use automatic means to analyze the degree of deliberativity of a multilog (Dryzek (1990; 2000), Bohman (1996), Gutmann and Thompson (1996), Holzinger and Landwehr (2010)). The disambiguation of causal discourse markers and the determination of the relations they entail is a crucial aspect of measuring the deliberative quality of a multilog. In this paper, we develop a system that is designed to perform this task. We describe a linguistically motivated, rulebased annotation system for German which disambiguates the multiple usages of causal discourse connectors in the language and reliably annotates the reason and result/conclusion relations that the connectors introduce. The paper proceeds as follows: Section 2 briefly reviews r</context>
</contexts>
<marker>Gutmann, Thompson, 1996</marker>
<rawString>Amy Gutmann and Dennis Frank Thompson. 1996. Democracy and Disagreement. Why moral conflict cannot be avoided in politics, and what should be done about it. Harvard University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨urgen Habermas</author>
</authors>
<title>Theorie des kommunikativen Handelns. Suhrkamp,</title>
<date>1981</date>
<location>Frankfurt am Main.</location>
<contexts>
<context position="1502" citStr="Habermas (1981)" startWordPosition="223" endWordPosition="224">poken dialogue. The results show that our system performs reliably well with respect to both tasks. 1 Introduction In general, causality refers to the way of knowing whether one state of affairs is causally related to another.1 Within linguistics, causality has long been established as a central phenomenon for investigation. In this paper, we look at causality from the perspective of a research question from political science, where the notion is particularly important when it comes to determining (a.o.) the deliberative quality of a discussion. The notion of deliberation is originally due to Habermas (1981), who assumes that within a deliberative democracy, stakeholders participating in a multilog, i.e. a multi-party conversation, justify their positions truthfully, rationally and respectfully and eventually defer to the better argument. Within political science, the question arises whether actual 1This work is part of the BMBF funded eHumanities project VisArgue, an interdisciplinary cooperation between political science, computer science and linguistics. multilogs conducted in the process of a democratic decision making indeed follow this ideal and whether/how one can use automatic means to an</context>
</contexts>
<marker>Habermas, 1981</marker>
<rawString>J¨urgen Habermas. 1981. Theorie des kommunikativen Handelns. Suhrkamp, Frankfurt am Main.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katharina Holzinger</author>
<author>Claudia Landwehr</author>
</authors>
<title>Institutional determinants of deliberative interaction.</title>
<date>2010</date>
<journal>European Political Science Review,</journal>
<pages>2--373</pages>
<contexts>
<context position="2246" citStr="Holzinger and Landwehr (2010)" startWordPosition="327" endWordPosition="330">nversation, justify their positions truthfully, rationally and respectfully and eventually defer to the better argument. Within political science, the question arises whether actual 1This work is part of the BMBF funded eHumanities project VisArgue, an interdisciplinary cooperation between political science, computer science and linguistics. multilogs conducted in the process of a democratic decision making indeed follow this ideal and whether/how one can use automatic means to analyze the degree of deliberativity of a multilog (Dryzek (1990; 2000), Bohman (1996), Gutmann and Thompson (1996), Holzinger and Landwehr (2010)). The disambiguation of causal discourse markers and the determination of the relations they entail is a crucial aspect of measuring the deliberative quality of a multilog. In this paper, we develop a system that is designed to perform this task. We describe a linguistically motivated, rulebased annotation system for German which disambiguates the multiple usages of causal discourse connectors in the language and reliably annotates the reason and result/conclusion relations that the connectors introduce. The paper proceeds as follows: Section 2 briefly reviews related work on the automatic ex</context>
</contexts>
<marker>Holzinger, Landwehr, 2010</marker>
<rawString>Katharina Holzinger and Claudia Landwehr. 2010. Institutional determinants of deliberative interaction. European Political Science Review, 2:373–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="11098" citStr="Marcu (2000)" startWordPosition="1754" endWordPosition="1755"> we introduce is based on a linguistically informed, handcrafted set of rules that deals with the disambiguation of causal markers and the identification of causal relations in text. As a first step, we divide all of the utterances into smaller units of text in order to be able to work with a more fine-grained structure of the discourse. Following the literature, we call these discourse units. Although there is no consensus in the literature on what exactly a discourse unit consists of, it is generally assumed that each discourse unit describes a single event (Polanyi et al., 2004). Following Marcu (2000), we term these elementary discourse units (EDUs) and approximate the assumption made by Polanyi et al. (2004) by inserting a boundary at every punctuation mark and every clausal connector (conjunctions, complementizers). Sentence boundaries are additionally marked. The annotation of discourse information is performed at the level of EDUs. There are sometimes instances in which a given relation such as “reason” spans multiple EDUs. In these cases, each of the EDUs involved is marked/annotated individually with the appropriate relation. In the following, we briefly lay out the two elements of t</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rutu Mulkar-Mehta</author>
<author>Andrew S Gordon</author>
<author>Jerry Hobbs</author>
<author>Eduard Hovy</author>
</authors>
<title>Causal markers across domains and genres of discourse.</title>
<date>2011</date>
<booktitle>In The 6th International Conference on Knowledge Capture.</booktitle>
<contexts>
<context position="4138" citStr="Mulkar-Mehta et al., 2011" startWordPosition="622" endWordPosition="625">h as the Penn Discourse Treebank for English (Prasad et al., 2008), which was used, e.g., in the disambiguation of English connectives by Pitler and Nenkova (2009), the Potsdam Commentary Corpus for German (Stede, 2004) and the discourse annotation layer of T¨uba-D/Z, a corpus of written German text (Versley and Gastel, 2012). Training auto20 Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 20–27, Gothenburg, Sweden, April 26, 2014. c�2014 Association for Computational Linguistics matic systems that learn patterns of causality (Do et al., 2011; Mulkar-Mehta et al., 2011b, inter alia) is a crucial factor in measuring discourse coherence (Sanders, 2005), and is beneficial in approaches to question-answering (Girju, 2003; Prasad and Joshi, 2008). With respect to automatically detecting causal relations in German, Versley (2010) uses English training data from the Penn Discourse Treebank in order to train an English annotation model. These English annotations can be projected to German in an English-German parallel corpus and on the basis of this a classifier of German discourse relations is trained. However, as previous studies have shown (Mulkar-Mehta et al., </context>
<context position="22528" citStr="Mulkar-Mehta et al. (2011" startWordPosition="3614" endWordPosition="3617">erms of causal versus other usages, the system performs as shown in Table 4 (the 0 indicates the average of both values). Precision Recall F-score Causal 1 0.94 0.97 Non-causal 0.85 1 0.92 0 0.93 0.97 0.95 Table 4: Causal marker disambiguation This result is very promising and shows that even though the development data consists of data from a different source, the patterns in the development set are mirrored in the test set. This means that the genre of the spoken exchange of arguments in a multilog does not exhibit the differences usually found when looking at data from different genres, as Mulkar-Mehta et al. (2011a) report when comparing newspaper articles from finance and sport. For evaluating the annotated spans of reason and result, we base the calculation on whether an EDU is marked with a particular relation or not, i.e. if the system marks an EDU as belonging to the reason or result part of a particular causal marker and the gold standard encodes the same information, then the two discourse units match. As a con6We use the I sign to indicate EDU boundaries. sequence, spans which do not match perfectly, for example in cases where their boundaries do not match, are not treated as non-matching insta</context>
</contexts>
<marker>Mulkar-Mehta, Gordon, Hobbs, Hovy, 2011</marker>
<rawString>Rutu Mulkar-Mehta, Andrew S. Gordon, Jerry Hobbs, and Eduard Hovy. 2011a. Causal markers across domains and genres of discourse. In The 6th International Conference on Knowledge Capture.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rutu Mulkar-Mehta</author>
<author>Christopher Welty</author>
<author>Jerry R Hoobs</author>
<author>Eduard Hovy</author>
</authors>
<title>Using granularity concepts for discovering causal relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the FLAIRS conference.</booktitle>
<contexts>
<context position="4138" citStr="Mulkar-Mehta et al., 2011" startWordPosition="622" endWordPosition="625">h as the Penn Discourse Treebank for English (Prasad et al., 2008), which was used, e.g., in the disambiguation of English connectives by Pitler and Nenkova (2009), the Potsdam Commentary Corpus for German (Stede, 2004) and the discourse annotation layer of T¨uba-D/Z, a corpus of written German text (Versley and Gastel, 2012). Training auto20 Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 20–27, Gothenburg, Sweden, April 26, 2014. c�2014 Association for Computational Linguistics matic systems that learn patterns of causality (Do et al., 2011; Mulkar-Mehta et al., 2011b, inter alia) is a crucial factor in measuring discourse coherence (Sanders, 2005), and is beneficial in approaches to question-answering (Girju, 2003; Prasad and Joshi, 2008). With respect to automatically detecting causal relations in German, Versley (2010) uses English training data from the Penn Discourse Treebank in order to train an English annotation model. These English annotations can be projected to German in an English-German parallel corpus and on the basis of this a classifier of German discourse relations is trained. However, as previous studies have shown (Mulkar-Mehta et al., </context>
<context position="22528" citStr="Mulkar-Mehta et al. (2011" startWordPosition="3614" endWordPosition="3617">erms of causal versus other usages, the system performs as shown in Table 4 (the 0 indicates the average of both values). Precision Recall F-score Causal 1 0.94 0.97 Non-causal 0.85 1 0.92 0 0.93 0.97 0.95 Table 4: Causal marker disambiguation This result is very promising and shows that even though the development data consists of data from a different source, the patterns in the development set are mirrored in the test set. This means that the genre of the spoken exchange of arguments in a multilog does not exhibit the differences usually found when looking at data from different genres, as Mulkar-Mehta et al. (2011a) report when comparing newspaper articles from finance and sport. For evaluating the annotated spans of reason and result, we base the calculation on whether an EDU is marked with a particular relation or not, i.e. if the system marks an EDU as belonging to the reason or result part of a particular causal marker and the gold standard encodes the same information, then the two discourse units match. As a con6We use the I sign to indicate EDU boundaries. sequence, spans which do not match perfectly, for example in cases where their boundaries do not match, are not treated as non-matching insta</context>
</contexts>
<marker>Mulkar-Mehta, Welty, Hoobs, Hovy, 2011</marker>
<rawString>Rutu Mulkar-Mehta, Christopher Welty, Jerry R. Hoobs, and Eduard Hovy. 2011b. Using granularity concepts for discovering causal relations. In Proceedings of the FLAIRS conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Using syntax to disambiguate explicit discourse connectives in text.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP,</booktitle>
<pages>13--16</pages>
<contexts>
<context position="3676" citStr="Pitler and Nenkova (2009)" startWordPosition="553" endWordPosition="556"> 4) and evaluate it against a hand-crafted gold standard in Section 5, where we also present the results from the same annotation task performed by a group of human annotators. In Section 6, we provide an in-depth system error analysis. Section 7 concludes the paper. 2 Related work The automatic detection and annotation of causality in language has been approached from various angles, for example by providing gold-standard, (manually) annotated resources such as the Penn Discourse Treebank for English (Prasad et al., 2008), which was used, e.g., in the disambiguation of English connectives by Pitler and Nenkova (2009), the Potsdam Commentary Corpus for German (Stede, 2004) and the discourse annotation layer of T¨uba-D/Z, a corpus of written German text (Versley and Gastel, 2012). Training auto20 Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 20–27, Gothenburg, Sweden, April 26, 2014. c�2014 Association for Computational Linguistics matic systems that learn patterns of causality (Do et al., 2011; Mulkar-Mehta et al., 2011b, inter alia) is a crucial factor in measuring discourse coherence (Sanders, 2005), and is beneficial in approaches to question-answering</context>
</contexts>
<marker>Pitler, Nenkova, 2009</marker>
<rawString>Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In Proceedings of ACL-IJCNLP, pages 13–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
<author>Chris Culy</author>
<author>Martin van den Berg</author>
<author>Gian Lorenzo Thione</author>
<author>David Ahn</author>
</authors>
<title>Sentential structure and discourse parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 ACL Workshop on Discourse Annotation,</booktitle>
<pages>80--87</pages>
<marker>Polanyi, Culy, van den Berg, Thione, Ahn, 2004</marker>
<rawString>Livia Polanyi, Chris Culy, Martin van den Berg, Gian Lorenzo Thione, and David Ahn. 2004. Sentential structure and discourse parsing. In Proceedings of the 2004 ACL Workshop on Discourse Annotation, pages 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
</authors>
<title>A Discoursebased Approach to Generating Why-Questions from Texts.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on the Question Generation Shared Task and Evaluation Challenge.</booktitle>
<contexts>
<context position="4314" citStr="Prasad and Joshi, 2008" startWordPosition="647" endWordPosition="650">ommentary Corpus for German (Stede, 2004) and the discourse annotation layer of T¨uba-D/Z, a corpus of written German text (Versley and Gastel, 2012). Training auto20 Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 20–27, Gothenburg, Sweden, April 26, 2014. c�2014 Association for Computational Linguistics matic systems that learn patterns of causality (Do et al., 2011; Mulkar-Mehta et al., 2011b, inter alia) is a crucial factor in measuring discourse coherence (Sanders, 2005), and is beneficial in approaches to question-answering (Girju, 2003; Prasad and Joshi, 2008). With respect to automatically detecting causal relations in German, Versley (2010) uses English training data from the Penn Discourse Treebank in order to train an English annotation model. These English annotations can be projected to German in an English-German parallel corpus and on the basis of this a classifier of German discourse relations is trained. However, as previous studies have shown (Mulkar-Mehta et al., 2011a, inter alia), the reliability of detecting causal relations with automatic means differs highly between different genres. Our data consist of transcriptions of originally</context>
</contexts>
<marker>Prasad, Joshi, 2008</marker>
<rawString>Rashmi Prasad and Aravind Joshi. 2008. A Discoursebased Approach to Generating Why-Questions from Texts. In Proceedings of the Workshop on the Question Generation Shared Task and Evaluation Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse Treebank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>2961--2968</pages>
<contexts>
<context position="3579" citStr="Prasad et al., 2008" startWordPosition="537" endWordPosition="540"> patterns in German (Section 3). We then introduce our rule-based annotation system (Section 4) and evaluate it against a hand-crafted gold standard in Section 5, where we also present the results from the same annotation task performed by a group of human annotators. In Section 6, we provide an in-depth system error analysis. Section 7 concludes the paper. 2 Related work The automatic detection and annotation of causality in language has been approached from various angles, for example by providing gold-standard, (manually) annotated resources such as the Penn Discourse Treebank for English (Prasad et al., 2008), which was used, e.g., in the disambiguation of English connectives by Pitler and Nenkova (2009), the Potsdam Commentary Corpus for German (Stede, 2004) and the discourse annotation layer of T¨uba-D/Z, a corpus of written German text (Versley and Gastel, 2012). Training auto20 Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 20–27, Gothenburg, Sweden, April 26, 2014. c�2014 Association for Computational Linguistics matic systems that learn patterns of causality (Do et al., 2011; Mulkar-Mehta et al., 2011b, inter alia) is a crucial factor in mea</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse Treebank 2.0. In Proceedings of LREC 2008, pages 2961–2968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Sanders</author>
</authors>
<title>Coherence, Causality and Cognitive Complexity in Discourse.</title>
<date>2005</date>
<booktitle>In Proceedings of SEM-05, First International Symposium on the Exploratiaon and Modelling of Meaning,</booktitle>
<pages>105--114</pages>
<contexts>
<context position="4221" citStr="Sanders, 2005" startWordPosition="636" endWordPosition="637">he disambiguation of English connectives by Pitler and Nenkova (2009), the Potsdam Commentary Corpus for German (Stede, 2004) and the discourse annotation layer of T¨uba-D/Z, a corpus of written German text (Versley and Gastel, 2012). Training auto20 Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 20–27, Gothenburg, Sweden, April 26, 2014. c�2014 Association for Computational Linguistics matic systems that learn patterns of causality (Do et al., 2011; Mulkar-Mehta et al., 2011b, inter alia) is a crucial factor in measuring discourse coherence (Sanders, 2005), and is beneficial in approaches to question-answering (Girju, 2003; Prasad and Joshi, 2008). With respect to automatically detecting causal relations in German, Versley (2010) uses English training data from the Penn Discourse Treebank in order to train an English annotation model. These English annotations can be projected to German in an English-German parallel corpus and on the basis of this a classifier of German discourse relations is trained. However, as previous studies have shown (Mulkar-Mehta et al., 2011a, inter alia), the reliability of detecting causal relations with automatic me</context>
</contexts>
<marker>Sanders, 2005</marker>
<rawString>Ted Sanders. 2005. Coherence, Causality and Cognitive Complexity in Discourse. In Proceedings of SEM-05, First International Symposium on the Exploratiaon and Modelling of Meaning, pages 105– 114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angela Schneider</author>
<author>Manfred Stede</author>
</authors>
<title>Ambiguity in German Connectives: A Corpus Study.</title>
<date>2012</date>
<booktitle>In Proceedings of KONVENS (Conference on Natural Language Processing)</booktitle>
<contexts>
<context position="5093" citStr="Schneider and Stede (2012)" startWordPosition="765" endWordPosition="768"> to train an English annotation model. These English annotations can be projected to German in an English-German parallel corpus and on the basis of this a classifier of German discourse relations is trained. However, as previous studies have shown (Mulkar-Mehta et al., 2011a, inter alia), the reliability of detecting causal relations with automatic means differs highly between different genres. Our data consist of transcriptions of originally spoken multilogs and this type of data differs substantially from newspaper or other written texts. Regarding the disambiguation of German connectives, Schneider and Stede (2012) carried out a corpus study of 42 German discourse connectives which are listed by Dipper and Stede (2006) as exhibiting a certain degree of ambiguity. Their results indicate that for a majority of ambiguous connectives, plain POS tagging is not reliable enough, and even contextual POS patterns are not sufficient in all cases. This is the same conclusion drawn by Dipper and Stede (2006), who also state that off-the-shelf POS taggers are too unreliable for the task. They instead suggest a mapping approach for 9 out of the 42 connectives and show that this assists considerably with disambiguatio</context>
</contexts>
<marker>Schneider, Stede, 2012</marker>
<rawString>Angela Schneider and Manfred Stede. 2012. Ambiguity in German Connectives: A Corpus Study. In Proceedings of KONVENS (Conference on Natural Language Processing) 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Alex Lascarides</author>
</authors>
<title>Using Automatically Labelled Examples to Classify Rhetorical Relations: An Assessment.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="20950" citStr="Sporleder and Lascarides, 2008" startWordPosition="3354" endWordPosition="3357">κResult=0.90 indicate near-perfect agreement. κ is significantly higher for causal EDUs than for non-causal (i.e., unmarked) EDUs (κNon-causal=0.82); this is in fact expected since causal EDUs are the marked case and are thus easier to identify for annotators in a coherent manner. IAA κReason 0.86 κResult 0.90 κNon-causal 0.82 κAll 0.73 Table 3: IAA of span annotations Across all relation types, κAll=0.73 indicates “substantial agreement”. The drop in the agreement is anticipated and mirrors the problem that 24 is generally found in the literature when evaluating spans of discourse relations (Sporleder and Lascarides, 2008). First, measuring κAll involves three categories, whereas the other measures involve two. Second, a preliminary error analysis shows that there is substantial disagreement regarding the extent of both reason and result spans. The examples in (8)–(9) illustrate this. While annotator 1 marks the result span (indicated by the ( S tag) as starting at the beginning of the sentence, annotator 2 excludes the first EDU from the result span.6 In such cases, we thus register a mismatch in the annotation of the first EDU. Nevertheless, the numbers indicate a substantial agreement. We thus conclude that </context>
</contexts>
<marker>Sporleder, Lascarides, 2008</marker>
<rawString>Caroline Sporleder and Alex Lascarides. 2008. Using Automatically Labelled Examples to Classify Rhetorical Relations: An Assessment. Natural Language Engineering, 14(3):369–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Stede</author>
</authors>
<title>The Potsdam Commentary Corpus. In</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL’04 Workshop on Discourse Annotation,</booktitle>
<pages>96--102</pages>
<contexts>
<context position="3732" citStr="Stede, 2004" startWordPosition="564" endWordPosition="565">, where we also present the results from the same annotation task performed by a group of human annotators. In Section 6, we provide an in-depth system error analysis. Section 7 concludes the paper. 2 Related work The automatic detection and annotation of causality in language has been approached from various angles, for example by providing gold-standard, (manually) annotated resources such as the Penn Discourse Treebank for English (Prasad et al., 2008), which was used, e.g., in the disambiguation of English connectives by Pitler and Nenkova (2009), the Potsdam Commentary Corpus for German (Stede, 2004) and the discourse annotation layer of T¨uba-D/Z, a corpus of written German text (Versley and Gastel, 2012). Training auto20 Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 20–27, Gothenburg, Sweden, April 26, 2014. c�2014 Association for Computational Linguistics matic systems that learn patterns of causality (Do et al., 2011; Mulkar-Mehta et al., 2011b, inter alia) is a crucial factor in measuring discourse coherence (Sanders, 2005), and is beneficial in approaches to question-answering (Girju, 2003; Prasad and Joshi, 2008). With respect to </context>
</contexts>
<marker>Stede, 2004</marker>
<rawString>Manfred Stede. 2004. The Potsdam Commentary Corpus. In In Proceedings of the ACL’04 Workshop on Discourse Annotation, pages 96–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
<author>Anna Gastel</author>
</authors>
<date>2012</date>
<booktitle>Linguistic Tests for Discourse Relations in the T¨uba-D/Z Corpus of Written German. Dialogue and Discourse,</booktitle>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="3840" citStr="Versley and Gastel, 2012" startWordPosition="579" endWordPosition="582"> annotators. In Section 6, we provide an in-depth system error analysis. Section 7 concludes the paper. 2 Related work The automatic detection and annotation of causality in language has been approached from various angles, for example by providing gold-standard, (manually) annotated resources such as the Penn Discourse Treebank for English (Prasad et al., 2008), which was used, e.g., in the disambiguation of English connectives by Pitler and Nenkova (2009), the Potsdam Commentary Corpus for German (Stede, 2004) and the discourse annotation layer of T¨uba-D/Z, a corpus of written German text (Versley and Gastel, 2012). Training auto20 Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 20–27, Gothenburg, Sweden, April 26, 2014. c�2014 Association for Computational Linguistics matic systems that learn patterns of causality (Do et al., 2011; Mulkar-Mehta et al., 2011b, inter alia) is a crucial factor in measuring discourse coherence (Sanders, 2005), and is beneficial in approaches to question-answering (Girju, 2003; Prasad and Joshi, 2008). With respect to automatically detecting causal relations in German, Versley (2010) uses English training data from the Penn </context>
</contexts>
<marker>Versley, Gastel, 2012</marker>
<rawString>Yannick Versley and Anna Gastel. 2012. Linguistic Tests for Discourse Relations in the T¨uba-D/Z Corpus of Written German. Dialogue and Discourse, 1(2):1–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
</authors>
<title>Discovery of Ambiguous and Unambiguous Discourse Connectives via Annotation Projection.</title>
<date>2010</date>
<booktitle>In Workshop on the Annotation and Exploitation of Parallel Corpora (AEPC).</booktitle>
<contexts>
<context position="4398" citStr="Versley (2010)" startWordPosition="660" endWordPosition="661">corpus of written German text (Versley and Gastel, 2012). Training auto20 Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 20–27, Gothenburg, Sweden, April 26, 2014. c�2014 Association for Computational Linguistics matic systems that learn patterns of causality (Do et al., 2011; Mulkar-Mehta et al., 2011b, inter alia) is a crucial factor in measuring discourse coherence (Sanders, 2005), and is beneficial in approaches to question-answering (Girju, 2003; Prasad and Joshi, 2008). With respect to automatically detecting causal relations in German, Versley (2010) uses English training data from the Penn Discourse Treebank in order to train an English annotation model. These English annotations can be projected to German in an English-German parallel corpus and on the basis of this a classifier of German discourse relations is trained. However, as previous studies have shown (Mulkar-Mehta et al., 2011a, inter alia), the reliability of detecting causal relations with automatic means differs highly between different genres. Our data consist of transcriptions of originally spoken multilogs and this type of data differs substantially from newspaper or othe</context>
</contexts>
<marker>Versley, 2010</marker>
<rawString>Yannick Versley. 2010. Discovery of Ambiguous and Unambiguous Discourse Connectives via Annotation Projection. In Workshop on the Annotation and Exploitation of Parallel Corpora (AEPC).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.126999">
<title confidence="0.999287">
Investigating the Contribution of Distributional Semantic Information for
Dialogue Act Classification
</title>
<author confidence="0.987021">
Dmitrijs Milajevs
</author>
<affiliation confidence="0.990713">
Queen Mary University of London
</affiliation>
<email confidence="0.98028">
d.milajevs@qmul.ac.uk
</email>
<author confidence="0.985129">
Matthew Purver
</author>
<affiliation confidence="0.988268">
Queen Mary University of London
</affiliation>
<email confidence="0.991869">
m.purver@qmul.ac.uk
</email>
<sectionHeader confidence="0.993744" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999696333333334">
This paper presents a series of experiments
in applying compositional distributional
semantic models to dialogue act classifica-
tion. In contrast to the widely used bag-of-
words approach, we build the meaning of
an utterance from its parts by composing
the distributional word vectors using vec-
tor addition and multiplication. We inves-
tigate the contribution of word sequence,
dialogue act sequence, and distributional
information to the performance, and com-
pare with the current state of the art ap-
proaches. Our experiment suggests that
that distributional information is useful for
dialogue act tagging but that simple mod-
els of compositionality fail to capture cru-
cial information from word and utterance
sequence; more advanced approaches (e.g.
sequence- or grammar-driven, such as cat-
egorical, word vector composition) are re-
quired.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999910339285715">
One of the fundamental tasks in automatic dia-
logue processing is dialogue act tagging: labelling
each utterance with a tag relating to its function
in the dialogue and effect on the emerging con-
text: greeting, query, statement etc (see e.g. (Core,
1998)). Although factors such as intonation also
play a role (see e.g. (Jurafsky et al., 1998)), one
of the most important sources of information in
this task is the semantic meaning of an utterance,
and this is reflected in the fact that people use
similar words when they perform similar utterance
acts. For example, utterances which state opinion
(tagged sv in the standard DAMSL schema, see
below) often include words such as “I think”, “I
believe”, “I guess” etc. Hence, a similarity-based
model of meaning — for instance, a distributional
semantic model — should provide benefits over
a purely word-based model for dialogue act tag-
ging. However, since utterances generally con-
sist of more than one word, one has to be able
to extend such similarity-based models from sin-
gle words to sentences and/or complete utterances.
Hence, we consider here the application of compo-
sitional distributional semantics for this task.
Here, we extend bag-of-word models com-
mon in previous approaches (Serafin et al., 2003)
with simple compositional distributional opera-
tions (Mitchell and Lapata, 2008) and examine the
improvements gained. These improvements sug-
gest that distributional information does improve
performance, but that more sophisticated compo-
sitional operations such as matrix multiplication
(Baroni and Zamparelli, 2010; Grefenstette and
Sadrzadeh, 2011) should provide further benefits.
The state of the art is a supervised method
based on Recurrent Convolutional Neural Net-
works (Kalchbrenner and Blunsom, 2013). This
method learns both the sentence model and the
discourse model from the same training corpus,
making it hard to understand how much of the
contribution comes from the inclusion of distribu-
tional word meaning, and how much from learn-
ing patterns specific to the corpus at hand. Here,
in contrast, we use an external unlabeled resource
to obtain a model of word meaning, composing
words to obtain representations for utterances, and
rely on training data only for discourse learning
for the tagging task itself.
We proceed as follows. First, we discuss related
work by introducing distributional semantics and
describe common approaches for dialogue act tag-
ging in Section 2. Section 3 proposes several mod-
els for utterance representation based on the bag of
words approach and word vector composition. We
describe the experiment and discuss the result in
Section 4. Finally, Section 5 concludes the work.
</bodyText>
<page confidence="0.984261">
40
</page>
<note confidence="0.998474">
Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 40–47,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.9983" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99997703125">
Distributional semantics The aim of natural
language semantics is to provide logical represen-
tations of meaning for information in textual form.
Distributional semantics is based on the idea that
“You shall know a word by the company it keeps”
(Firth, 1957) – in other words, the meaning of a
word is related to the contexts it appears in. Fol-
lowing this idea, word meaning can be represented
as a vector where its dimensions correspond to the
usage contexts, usually other words observed to
co-occur, and the values are the co-occurrence fre-
quencies. Such a meaning representation is easy
to build from raw data and does not need rich an-
notation.
Methods based on this distributional hypothe-
sis have recently been applied to many tasks, but
mostly at the word level: for instance, word sense
disambiguation (Zhitomirsky-Geffet and Dagan,
2009) and lexical substitution (Thater et al., 2010).
They exploit the notion of similarity which corre-
lates with the angle between word vectors (Turney
et al., 2010). Compositional distributional seman-
tics goes beyond the word level and models the
meaning of phrases or sentences based on their
parts. Mitchell and Lapata (2008) perform com-
position of word vectors using vector addition and
multiplication operations. The limitation of this
approach is the operator associativity, which ig-
nores the argument order, and thus word order. As
a result, “John loves Mary” and “Mary loves John”
get assigned the same meaning.
To capture word order, various approaches
have been proposed. Grefenstette and Sadrzadeh
(2011) extend the compositional approach by us-
ing non-associative linear algebra operators as
proposed in the theoretical work of (Coecke et
al., 2010). Socher et al. (2012) present a recur-
sive technique to build compositional meaning of
phrases from their constituents, where the non-
linear composition operators are learned by Neural
Networks.
Dialogue act tagging There are many ways to
approach the task of dialogue act tagging (Stol-
cke et al., 2000). The most successful approaches
combine intra-utterance features, such as the (se-
quences of) words and intonational contours used,
together with inter-utterance features, such as the
sequence of utterance tags being used previously.
To capture both of these aspects, sequence models
such as Hidden Markov Models are widely used
(Stolcke et al., 2000; Surendran and Levow, 2006).
The sequence of words is an observable variable,
while the sequence of dialogue act tags is a hidden
variable.
However, some approaches have shown com-
petitive results without exploiting features of inter-
utterance context. Webb et al. (2005) concentrate
only on features found inside an utterance, identi-
fying ngrams that correlate strongly with particu-
lar utterance tags, and propose a statistical model
for prediction which produces close to the state of
the art results.
The current state of the art (Kalchbrenner and
Blunsom, 2013) uses Recurrent Convolutional
Neural Networks to achieve high accuracy. This
model includes information about word identity,
intra-utterance word sequence, and inter-utterance
tag sequence, by using a vector space model of
words with a compositional approach. The words
vectors are not based on distributional frequencies
in this case, however, but on randomly initialised
vectors, with the model trained on a specific cor-
pus. This raises several questions: what is the con-
tribution of word sequence and/or utterance (tag)
sequence; and might further gains be made by ex-
ploiting the distributional hypothesis?
As our baseline, we start with an approach
which uses only word information, and excludes
word sequence, tag sequence and word distribu-
tions. Serafin et al. (2003) use Latent Semantic
Analysis for dialogue act tagging: utterances are
represented using a bag-of-words representation
in a word-document matrix. The rows in the ma-
trix correspond to words, the columns correspond
to documents and each cell in the matrix contains
the number of times a word occurs in a document.
Singular Value Decomposition (SVD) is then ap-
plied to reduce the number of rows in the matrix,
with the number of components in the reduced
space set to 50. To predict the tag of an unseen
utterance, the utterance vector is mapped to the re-
duced space and the tag of the closest neighbor is
assigned to it (using cosine similarity as a similar-
ity measure). The reported accuracy on the Span-
ish Call Home corpus for predicting 37 different
utterance tags is 65.36%.
</bodyText>
<sectionHeader confidence="0.987335" genericHeader="method">
3 Utterance models
</sectionHeader>
<bodyText confidence="0.98887">
In this paper, we investigate the extent to which
distributional representations, word order infor-
</bodyText>
<page confidence="0.996668">
41
</page>
<bodyText confidence="0.999621068965517">
mation, and utterance order information can im-
prove this basic model, by choosing different ways
to represent an utterance in a vector space. We de-
sign three basic models. The first model is based
directly on the bag-of-words model which serves
as the baseline in our experiment, following (Ser-
afin et al., 2003); and extends this to investigate the
effect of word order information by moving from
word unigrams to bigrams. The second model
investigates distributional information, by calcu-
lating word vector representations from a general
corpus, and obtaining utterance representations by
composing the word vectors using simple opera-
tors. The third model extends this idea to inves-
tigate the role of utterance order information, by
including the information about the previous ut-
terance.
Bag of words The first model represents an ut-
terance as a vector where each component corre-
sponds to a word. The values of vector compo-
nents are the number of times the corresponding
words occured in the utterance. The model is sim-
ilar to (Serafin et al., 2003), but the matrix is trans-
posed. We refer to it as bag of unigrams in Table 1.
However, this bag of words approach does not
preserve any word order information. As it has
been said previously, for the dialogue act tagging
word order may be crucial. Consider these utter-
ances:
</bodyText>
<listItem confidence="0.999959">
• John, are there cookies
• John, there are cookies
</listItem>
<bodyText confidence="0.999383403508772">
One of the utterances is a question (or request)
while the other is a statement. However, the bag
of words model will extract the same vector repre-
sentation for both.
To overcome this problem we also represent an
utterance as a bag of bigrams. When bigrams are
used in place of single words, the utterance rep-
resentation will differ. The question contains the
bigram “are there”, while the statement contains
the bigram “there are”.
Simple composition Our second model ex-
ploits the distributional hypothesis, by represent-
ing words not as atomic types (i.e. individual di-
mensions in the utterance matrix, as above), but
as vectors encoding their observed co-occurrence
distributions. We estimate these from a large cor-
pus of general written English (the Google Books
Ngrams corpus – see below).
However, this raises the question of how to
compose these word vectors into a single repre-
sentation for an utterance. Various approaches to
compositional vector space modelling have been
successfully applied to capture the meaning of a
phrase in a range of tasks (Mitchell and Lapata,
2008; Grefenstette and Sadrzadeh, 2011; Socher
et al., 2013). In this work, we follow (Mitchell and
Lapata, 2008) and apply vector addition and point-
wise multiplication to obtain the vector of an ut-
terance from the words it consists of. This has the
advantage of simplicity and domain-generality, re-
quiring no sentence grammar (problematic for the
non-canonical language in dialogue) or training on
a specific corpus to obtain the appropriate compo-
sitionality operators or associative model; but has
the disadvantage of losing word order information.
The corresponding models are referred as addition
and multiplication in Table 1 and Table 2.
Previous utterance A conversation is a se-
quence of utterances, and the tag of an utter-
ance often depends on the previous utterance
(e.g. answers tend to follow questions). Hid-
den Markov Models (Surendran and Levow, 2006;
Stolcke et al., 2000) are often used to cap-
ture these dependencies; Recurrent Convolutional
Neural Networks (Kalchbrenner and Blunsom,
2013) have been used to simultaneously capture
the intra-utterance sequence of words and the
inter-utterance sequence of dialog tags in a con-
versation.
In this model, we are interested specifically in
the effect of inter-utterance (tag) sequence. We
provide previous addition and previous multipli-
cation models as simple attempts to capture this
phenomenon: the vector of an utterance is the con-
catenation of its vector obtained in the correspond-
ing compositional model (addition or multiplica-
tion) and the vector of the previous utterance.
</bodyText>
<sectionHeader confidence="0.936073" genericHeader="method">
4 Predicting dialogue acts
</sectionHeader>
<subsectionHeader confidence="0.98009">
4.1 The resources
</subsectionHeader>
<bodyText confidence="0.999666875">
This section describes the resources we use to
evaluate and compare the proposed models.
Switchboard corpus The Switchboard corpus
(Godfrey et al., 1992) is a corpus of telephone con-
versations on selected topics. It consists of about
2500 conversations by 500 speakers from the U.S.
The conversations in the corpus are labeled with
42 unique dialogue act tags and split to 1115 train
</bodyText>
<page confidence="0.991285">
42
</page>
<figure confidence="0.9988998125">
A o : Okay. /
A qw : 1D So, }
B qy&amp;quot;d: [ [I guess, +
A + : What kind of experience
[ do you, + do you ] have,
then with child care? /
B + : I think, ] + 1F uh, }
I wonder if that worked. /
(a) A conversation with interrupted utterances.
A o : Okay.
A qw : So What kind of experience
do you do you have then
with child care?
B qy&amp;quot;d: I guess I think uh I wonder
if that worked.
(b) A preprocessed conversation.
</figure>
<figureCaption confidence="0.762787333333333">
Figure 1: A example of interrupted utterances from Switchboard and their transformation.
and 19 test conversations (Jurafsky et al., 1997;
Stolcke et al., 2000).
</figureCaption>
<bodyText confidence="0.999933535714286">
In addition to the dialog act tags, utterances
interrupted by the other speaker (and thus split
into two or more parts) have their continuations
marked with a special tag “+”. Tag prediction of
one part of an interrupted utterance in isolation is
a difficult task even for a human; for example, it
would not be clear why the utterance “So,” should
be assigned the tag qw (wh-question) in Figure 1a
without the second part “What kind of experience
do you have [... ]”. Following (Webb et al., 2005)
we preprocess Switchboard by concatenating the
parts of an interrupted utterance together, giving
the result the tag of the first part and putting it in
its place in the conversation sequence. We also
remove commas and disfluency markers from the
raw text. Figure 1b illustrates the transformation
we do as preprocessing.
We split the utterances between training and
testing as suggested in (Stolcke et al., 2000).
Google Books Ngram Corpus The Google
Books Ngram Corpus (Lin et al., 2012) is a col-
lection of n-gram frequencies over books written
in 8 languages. The English part of the corpus is
based on more than 4.5 million books and contains
more than four thousand billion tokens. The re-
source provides frequencies of n-grams of length
1 to 5. For our experiments we use 5-grams from
the English part of the resource.
</bodyText>
<subsectionHeader confidence="0.995675">
4.2 Word vector spaces
</subsectionHeader>
<bodyText confidence="0.999935648648649">
In distributional semantics, the meanings of words
are captured by a vector space model based on a
word co-occurrence matrix. Each row in the ma-
trix represents a target word, and each column rep-
resents a context word; each element in the matrix
is the number of times a target word co-occured
with a corresponding context word. The frequency
counts are typically normalized, or weighted us-
ing tf-idf or log-likelihood ratio to obtain better re-
sults, see (Mitchell and Lapata, 2008; Agirre et al.,
2009) for various approaches. It is also common
to apply dimensionality reduction to get higher
performance (Dinu and Lapata, 2010; Baroni and
Zamparelli, 2010).
As target words we select all the words in our
(Switchboard) training split. As context words
we choose the 3000 most frequent words in the
Google Ngram Corpus, excluding the 100 most
frequent. To obtain co-occurrence frequencies
from ngrams we sum up the frequency of a 5-gram
over the years, treat the word in the middle as a
target, and the other words as its contexts.
For normalization, we experiment with a vec-
tor space based on raw co-occurrences; a vector
space where frequencies are weighted using tf-idf;
and another one with the number of dimensions
reduced to 1000 using Non-negative Matrix Fac-
torization (NMF) (Hoyer, 2004).
We use the NMF and tf-idf implementations
provided by scikit-learn version 0.14 (Pe-
dregosa et al., 2011). For tf-idf, the term vectors
are L2 normalized. For NMF, NNDSVD initial-
ization (Boutsidis and Gallopoulos, 2008) is used,
and the tolerance value for stopping conditions is
set to 0.001. The co-occurrence matrix is line-
normalized, so the sum of the values in each row
is 1 before applying NMF.1
</bodyText>
<subsectionHeader confidence="0.992712">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999303111111111">
To evaluate these possible models we follow (Ser-
afin et al., 2003). Once we have applied a model
to extract features from utterances and build a vec-
tor space, the dimensionality of the vector space
is reduced using SVD to 50 dimensions. Then a
k-nearest neighbours (KNN) classifier is trained
and used for utterance tag prediction. In contrast
to (Serafin et al., 2003), we use Euclidean dis-
tance as a distance metric and choose the most
</bodyText>
<footnote confidence="0.986206">
1The co-occurrence matrix and the information about the
software used in the experiment are available at
http://www.eecs.qmul.ac.uk/˜dm303/cvsc14.html
</footnote>
<page confidence="0.999361">
43
</page>
<table confidence="0.999700636363636">
Method Accuracy
(Kalchbrenner and Blunsom, 2013) 0.739
(Webb et al., 2005) 0.719
(Stolcke et al., 2000) 0.710
(Serafin et al., 2003) 0.654
Bag of unigrams 0.602
Bag of bigrams 0.621
Addition 0.639
Multiplication 0.572
Previous addition 0.569
Previous multiplication 0.497
</table>
<tableCaption confidence="0.999887">
Table 1: Comparison with previous work. Note
</tableCaption>
<bodyText confidence="0.976318515151515">
that (Serafin et al., 2003) do not use Switchboard
and therefore their results are not directly compa-
rable to others.
frequent label among the 5 closest neighbors.
The SVD and KNN classifier implementations in
scikit-learn are used.
Baseline In our experiments, the bag of uni-
grams model accuracy of 0.602 is lower than the
accuracy of 0.654 reported in (Serafin et al., 2003),
see Table 1. The lower performance may be due
to the differences between Switchboard and Call-
Home37 corpora, in particular the tag distribu-
tion.2 In CallHome37, 42.7% of utterances are la-
beled with the most frequent dialogue act, while
the figure in Switchboard is 31.5%; the more even
distribution in Switchboard is likely to make over-
all average accuracy levels lower.
Word order As Table 1 shows, the bag of bi-
grams model improves over unigrams. This con-
firms that word order provides important informa-
tion for predicting dialogue act tags.
Distributional models Performance of compo-
sitional distributional models depends both on
compositional operator and weighting. Table 2
demonstrates accuracy of the models. We instan-
tiate 3 vector spaces from Google Ngrams: one
space with raw co-occurrence frequencies, a tf-idf
weighted space and a reduced space using NMF.
Addition outperforms multiplication in our ex-
periments, although for other tasks multiplication
has been shown to perform better (Grefenstette
and Sadrzadeh, 2011; Mitchell and Lapata, 2008).
Lower multiplication performance here might be
</bodyText>
<footnote confidence="0.893461">
2The CallHome37 corpus is not currently available to us.
</footnote>
<table confidence="0.99945">
Model Space
Raw tf-idf NMF
Addition without SVD 0.592
Addition 0.610 0.639 0.620
Multiplication 0.572 0.568 0.525
Previous addition 0.569
Previous multiplication 0.497
</table>
<tableCaption confidence="0.906506">
Table 2: Accuracy results for different composi-
tional models and vector spaces.
</tableCaption>
<bodyText confidence="0.999592567567568">
due to the fact that some utterances are rather long
(for example, more than 70 tokens), and the result-
ing vectors get many zero components.
Selection of the optimal weighting method
could be crucial for overall model performance.
The 3 weighting schemes we use give a broad va-
riety of results; more elaborate weighting and con-
text selection might give higher results.
Figure 2 illustrates dialog tag assignment us-
ing addition and the tf-idf weighted vector space.
As we do not use any inter-utterance features, the
first two statements, which consist only of the
word Okay, got assigned wrong tags. However,
the Wh-question in the conversation got classified
as a Yes-No-question, probably because what did
not influence the classification decision strongly
enough and could have been classified correctly
using only intra-utterance features. Also, the ex-
ample shows how important grammatical features
are: the verb think appears in many different con-
text, and its presence does not indicate a certain
type of an utterance.
In addition, we observed that SVD improves
classification accuracy. The accuracy of KNN
classification without prior dimensionality reduc-
tion drops from 0.610 to 0.592 for vector addition
on the raw vector space.
Utterance sequence To solve the issue of utter-
ances that can be tagged correctly only by consid-
ering inter-utterance features, we included previ-
ous utterance. However, in our experiment, such
inclusion by vector concatenation does not im-
prove tagging accuracy (Table 2). The reason for
this could be that after concatenation the dimen-
sionality of the space doubles, and SVD can not
handle it properly. We evaluated only dimension-
ally reduced spaces because of the memory limit.
</bodyText>
<page confidence="0.996722">
44
</page>
<figure confidence="0.999542538461539">
B ** (b) : Okay.
A bˆm (b) : Okay.
B qw (qy): Well what do you think about the idea of uh kids having to do public
service work for a year?
B qy (sd): Do you think it’s a &lt;breathing&gt;
A sv (sv): Well I I think it’s a pretty good idea.
A sv (sd): I think they should either do that or or afford some time to the military
or or helping elderly people.
B aa (aa): Yes
B aa (b) : yes
B % (%) : def
A sv (sv): I I you know I think that we have a bunch of elderly folks in the country
that could use some help
</figure>
<figureCaption confidence="0.888444">
Figure 2: The beginning of the conversation 2151 from the test split of Switchboard. In brackets the
tags predicted using vector addition as a composition method on the tf-idf space are given. We mark
fo o fw &amp;quot; by bc as **.
</figureCaption>
<bodyText confidence="0.999859538461539">
Summary Our accuracy is lower compared to
other work. Webb et al. (2005)’s method, based
only on intra-utterance lexical features, but incor-
porating longer ngram sequences and feature se-
lection, yields accuracy of 0.719. Advanced treat-
ment of both utterance and discourse level features
yields accuracy of 0.739 (Kalchbrenner and Blun-
som, 2013). However, our experiments allow us to
evaluate the contribution of various kinds of infor-
mation: vector spaces based on word bigrams and
on co-occurrence distributions both outperformed
the bag of words approach; but incorporation of
previous utterance information did not.
</bodyText>
<sectionHeader confidence="0.972451" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999984839285714">
In this work we evaluated the contribution of
word and utterance sequence, and of distributional
information using simple compositional vector
space models, for dialogue act tagging. Our exper-
iments show that information about intra-utterance
word order (ngrams), and information about word
co-occurence distributions, outperforms the bag of
words models, although not competitive with the
state of the art given the simplistic compositional
approach used here. Information about utterance
tag sequence, on the other hand, did not.
The usage of an external, large scale resource
(here, the Google Ngram Corpus) to model word
senses improves the tagging accuracy in compari-
son to the bag of word model, suggesting that the
dialogue act tag of an utterance depends on its se-
mantics.
However, the improvements in performance of
the bag of bigrams model in comparison to bag of
unigrams, and the much higher results of Webb et
al. (2005)’s intra-utterance approach, suggest that
the sequence of words inside an utterance is cru-
cial for the dialogue act tagging task. This sug-
gests that our simplistic approaches to vector com-
position (addition and multiplication) are likely
to be insufficient: more advanced, sequence- or
grammar-driven composition, such as categorical
composition (Coecke et al., 2010), might improve
the tagging accuracy.
In addition, our results show that the perfor-
mance of distributional models depends on many
factors, including compositional operator selec-
tion and weighting of the initial co-occurrence ma-
trix. Our work leaves much scope for improve-
ments in these factors, including co-occurrence
matrix instantiation. For example, the window
size of 2, which we used to obtain co-occurrence
counts, is lower than the usual size of 5 (Dinu and
Lapata, 2010), or the sentence level (Baroni and
Zamparelli, 2010). Word representation in a vec-
tor space using neural networks might improve ac-
curacy as well (Mikolov et al., 2013).
Previous approaches to dialogue act tagging
have shown utterance/tag sequence to be a use-
ful source of information for improved accuracy
(Stolcke et al., 2000). We therefore conclude that
the lower accuracy we obtained using models that
include information about the previous utterance
is due again to our simplistic method of compo-
sition (vector concatenation); models which re-
flect dialogue structure or sequence explicitly are
likely to be more suited. Kalchbrenner and Blun-
som (2013) give one way in which this can be
achieved by learning from a specific corpus, and
the question of possible alternatives and more gen-
eral models remains for future research.
</bodyText>
<page confidence="0.999085">
45
</page>
<sectionHeader confidence="0.998332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.993903875">
We thank Mehrnoosh Sadrzadeh for her helpful
advice and valuable discussion. We would like
to thank anonymous reviewers for their effective
comments. Milajevs is supported by the EP-
SRC project EP/J002607/1. Purver is supported
in part by the European Community’s Seventh
Framework Programme under grant agreement no
611733 (ConCreTe).
</bodyText>
<sectionHeader confidence="0.998179" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999516206185567">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19–27. Association for Computational Lin-
guistics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183–1193. Association for Computational Linguis-
tics.
Christos Boutsidis and Efstratios Gallopoulos. 2008.
Svd based initialization: A head start for nonneg-
ative matrix factorization. Pattern Recognition,
41(4):1350–1362.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. CoRR,
abs/1003.4394.
Mark Core. 1998. Analyzing and predicting patterns
of damsl utterance tags. In Proceedings of the AAAI
spring symposium on Applying machine learning to
discourse processing.
G. Dinu and M. Lapata. 2010. Measuring distribu-
tional similarity in context. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162–1172. Associa-
tion for Computational Linguistics.
John R. Firth. 1957. A Synopsis of Linguistic Theory,
1930-1955. Studies in Linguistic Analysis, pages 1–
32.
John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Acoustics,
Speech, and Signal Processing, 1992. ICASSP-92.,
1992 IEEE International Conference on, volume 1,
pages 517–520. IEEE.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1394–1404. Asso-
ciation for Computational Linguistics.
Patrik O Hoyer. 2004. Non-negative matrix factor-
ization with sparseness constraints. The Journal of
Machine Learning Research, 5:1457–1469.
Daniel Jurafsky, Elizabeth Shriberg, and Debra Biasca.
1997. Switchboard swbd-damsl shallow-discourse-
function annotation coders manual, draft 13. Tech-
nical Report 97-02, University of Colorado, Boul-
der. Institute of Cognitive Science.
Daniel Jurafsky, Elizabeth Shriberg, Barbara Fox, and
Traci Curl. 1998. Lexical, prosodic, and syn-
tactic cues for dialog acts. In Proceedings of the
ACL-COLING Workshop on Discourse Relations
and Discourse Markers.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. In Proceedings of the Workshop on Con-
tinuous Vector Space Models and their Composition-
ality, pages 119–126, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden,
Jon Orwant, Will Brockman, and Slav Petrov. 2012.
Syntactic annotations for the Google Books ngram
corpus. In Proceedings of the ACL 2012 System
Demonstrations, pages 169–174. Association for
Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236–244. Association for
Computational Linguistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
Riccardo Serafin, Barbara Di Eugenio, and Michael
Glass. 2003. Latent semantic analysis for dia-
logue act classification. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology: companion volume of
the Proceedings of HLT-NAACL 2003–short papers-
Volume 2, pages 94–96. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.989673">
46
</page>
<reference confidence="0.999733307692308">
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211. Association for Computational Linguis-
tics.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In In Proceedings of the
ACL conference. Citeseer.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Carol Van Ess-Dykema, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339–373.
Dinoj Surendran and Gina-Anne Levow. 2006. Dialog
act tagging with support vector machines and hidden
markov models. In INTERSPEECH.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ’10, pages
948–957, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141–188.
Nick Webb, Mark Hepple, and Yorick Wilks. 2005.
Dialogue act classification based on intra-utterance
features. In Proceedings of the AAAI Workshop on
Spoken Language Understanding. Citeseer.
M. Zhitomirsky-Geffet and I. Dagan. 2009. Bootstrap-
ping distributional feature vector quality. Computa-
tional Linguistics, 35(3):435–461.
</reference>
<page confidence="0.999492">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.214111">
<title confidence="0.928660333333333">Investigating the Contribution of Distributional Semantic Information Dialogue Act Classification Dmitrijs Milajevs</title>
<affiliation confidence="0.479541">Queen Mary University of London</affiliation>
<email confidence="0.940645">d.milajevs@qmul.ac.uk</email>
<author confidence="0.998079">Matthew Purver</author>
<affiliation confidence="0.986096">Queen Mary University of London</affiliation>
<email confidence="0.997859">m.purver@qmul.ac.uk</email>
<abstract confidence="0.990164045454546">This paper presents a series of experiments in applying compositional distributional semantic models to dialogue act classification. In contrast to the widely used bag-ofwords approach, we build the meaning of an utterance from its parts by composing the distributional word vectors using vector addition and multiplication. We investigate the contribution of word sequence, dialogue act sequence, and distributional information to the performance, and compare with the current state of the art approaches. Our experiment suggests that that distributional information is useful for dialogue act tagging but that simple models of compositionality fail to capture crucial information from word and utterance sequence; more advanced approaches (e.g. sequenceor grammar-driven, such as categorical, word vector composition) are required.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19–27. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2674" citStr="Baroni and Zamparelli, 2010" startWordPosition="397" endWordPosition="400">n one word, one has to be able to extend such similarity-based models from single words to sentences and/or complete utterances. Hence, we consider here the application of compositional distributional semantics for this task. Here, we extend bag-of-word models common in previous approaches (Serafin et al., 2003) with simple compositional distributional operations (Mitchell and Lapata, 2008) and examine the improvements gained. These improvements suggest that distributional information does improve performance, but that more sophisticated compositional operations such as matrix multiplication (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011) should provide further benefits. The state of the art is a supervised method based on Recurrent Convolutional Neural Networks (Kalchbrenner and Blunsom, 2013). This method learns both the sentence model and the discourse model from the same training corpus, making it hard to understand how much of the contribution comes from the inclusion of distributional word meaning, and how much from learning patterns specific to the corpus at hand. Here, in contrast, we use an external unlabeled resource to obtain a model of word meaning, composing words to obtain repre</context>
<context position="15598" citStr="Baroni and Zamparelli, 2010" startWordPosition="2512" endWordPosition="2515">the meanings of words are captured by a vector space model based on a word co-occurrence matrix. Each row in the matrix represents a target word, and each column represents a context word; each element in the matrix is the number of times a target word co-occured with a corresponding context word. The frequency counts are typically normalized, or weighted using tf-idf or log-likelihood ratio to obtain better results, see (Mitchell and Lapata, 2008; Agirre et al., 2009) for various approaches. It is also common to apply dimensionality reduction to get higher performance (Dinu and Lapata, 2010; Baroni and Zamparelli, 2010). As target words we select all the words in our (Switchboard) training split. As context words we choose the 3000 most frequent words in the Google Ngram Corpus, excluding the 100 most frequent. To obtain co-occurrence frequencies from ngrams we sum up the frequency of a 5-gram over the years, treat the word in the middle as a target, and the other words as its contexts. For normalization, we experiment with a vector space based on raw co-occurrences; a vector space where frequencies are weighted using tf-idf; and another one with the number of dimensions reduced to 1000 using Non-negative Ma</context>
<context position="24280" citStr="Baroni and Zamparelli, 2010" startWordPosition="3918" endWordPosition="3921">, sequence- or grammar-driven composition, such as categorical composition (Coecke et al., 2010), might improve the tagging accuracy. In addition, our results show that the performance of distributional models depends on many factors, including compositional operator selection and weighting of the initial co-occurrence matrix. Our work leaves much scope for improvements in these factors, including co-occurrence matrix instantiation. For example, the window size of 2, which we used to obtain co-occurrence counts, is lower than the usual size of 5 (Dinu and Lapata, 2010), or the sentence level (Baroni and Zamparelli, 2010). Word representation in a vector space using neural networks might improve accuracy as well (Mikolov et al., 2013). Previous approaches to dialogue act tagging have shown utterance/tag sequence to be a useful source of information for improved accuracy (Stolcke et al., 2000). We therefore conclude that the lower accuracy we obtained using models that include information about the previous utterance is due again to our simplistic method of composition (vector concatenation); models which reflect dialogue structure or sequence explicitly are likely to be more suited. Kalchbrenner and Blunsom (2</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Boutsidis</author>
<author>Efstratios Gallopoulos</author>
</authors>
<title>Svd based initialization: A head start for nonnegative matrix factorization.</title>
<date>2008</date>
<journal>Pattern Recognition,</journal>
<volume>41</volume>
<issue>4</issue>
<contexts>
<context position="16456" citStr="Boutsidis and Gallopoulos, 2008" startWordPosition="2653" endWordPosition="2656">ncies from ngrams we sum up the frequency of a 5-gram over the years, treat the word in the middle as a target, and the other words as its contexts. For normalization, we experiment with a vector space based on raw co-occurrences; a vector space where frequencies are weighted using tf-idf; and another one with the number of dimensions reduced to 1000 using Non-negative Matrix Factorization (NMF) (Hoyer, 2004). We use the NMF and tf-idf implementations provided by scikit-learn version 0.14 (Pedregosa et al., 2011). For tf-idf, the term vectors are L2 normalized. For NMF, NNDSVD initialization (Boutsidis and Gallopoulos, 2008) is used, and the tolerance value for stopping conditions is set to 0.001. The co-occurrence matrix is linenormalized, so the sum of the values in each row is 1 before applying NMF.1 4.3 Evaluation To evaluate these possible models we follow (Serafin et al., 2003). Once we have applied a model to extract features from utterances and build a vector space, the dimensionality of the vector space is reduced using SVD to 50 dimensions. Then a k-nearest neighbours (KNN) classifier is trained and used for utterance tag prediction. In contrast to (Serafin et al., 2003), we use Euclidean distance as a </context>
</contexts>
<marker>Boutsidis, Gallopoulos, 2008</marker>
<rawString>Christos Boutsidis and Efstratios Gallopoulos. 2008. Svd based initialization: A head start for nonnegative matrix factorization. Pattern Recognition, 41(4):1350–1362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning.</title>
<date>2010</date>
<journal>CoRR,</journal>
<pages>1003--4394</pages>
<contexts>
<context position="5709" citStr="Coecke et al., 2010" startWordPosition="876" endWordPosition="879">els the meaning of phrases or sentences based on their parts. Mitchell and Lapata (2008) perform composition of word vectors using vector addition and multiplication operations. The limitation of this approach is the operator associativity, which ignores the argument order, and thus word order. As a result, “John loves Mary” and “Mary loves John” get assigned the same meaning. To capture word order, various approaches have been proposed. Grefenstette and Sadrzadeh (2011) extend the compositional approach by using non-associative linear algebra operators as proposed in the theoretical work of (Coecke et al., 2010). Socher et al. (2012) present a recursive technique to build compositional meaning of phrases from their constituents, where the nonlinear composition operators are learned by Neural Networks. Dialogue act tagging There are many ways to approach the task of dialogue act tagging (Stolcke et al., 2000). The most successful approaches combine intra-utterance features, such as the (sequences of) words and intonational contours used, together with inter-utterance features, such as the sequence of utterance tags being used previously. To capture both of these aspects, sequence models such as Hidden</context>
<context position="23748" citStr="Coecke et al., 2010" startWordPosition="3835" endWordPosition="3838">he bag of word model, suggesting that the dialogue act tag of an utterance depends on its semantics. However, the improvements in performance of the bag of bigrams model in comparison to bag of unigrams, and the much higher results of Webb et al. (2005)’s intra-utterance approach, suggest that the sequence of words inside an utterance is crucial for the dialogue act tagging task. This suggests that our simplistic approaches to vector composition (addition and multiplication) are likely to be insufficient: more advanced, sequence- or grammar-driven composition, such as categorical composition (Coecke et al., 2010), might improve the tagging accuracy. In addition, our results show that the performance of distributional models depends on many factors, including compositional operator selection and weighting of the initial co-occurrence matrix. Our work leaves much scope for improvements in these factors, including co-occurrence matrix instantiation. For example, the window size of 2, which we used to obtain co-occurrence counts, is lower than the usual size of 5 (Dinu and Lapata, 2010), or the sentence level (Baroni and Zamparelli, 2010). Word representation in a vector space using neural networks might </context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. CoRR, abs/1003.4394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Core</author>
</authors>
<title>Analyzing and predicting patterns of damsl utterance tags.</title>
<date>1998</date>
<booktitle>In Proceedings of the AAAI</booktitle>
<contexts>
<context position="1354" citStr="Core, 1998" startWordPosition="196" endWordPosition="197">t approaches. Our experiment suggests that that distributional information is useful for dialogue act tagging but that simple models of compositionality fail to capture crucial information from word and utterance sequence; more advanced approaches (e.g. sequence- or grammar-driven, such as categorical, word vector composition) are required. 1 Introduction One of the fundamental tasks in automatic dialogue processing is dialogue act tagging: labelling each utterance with a tag relating to its function in the dialogue and effect on the emerging context: greeting, query, statement etc (see e.g. (Core, 1998)). Although factors such as intonation also play a role (see e.g. (Jurafsky et al., 1998)), one of the most important sources of information in this task is the semantic meaning of an utterance, and this is reflected in the fact that people use similar words when they perform similar utterance acts. For example, utterances which state opinion (tagged sv in the standard DAMSL schema, see below) often include words such as “I think”, “I believe”, “I guess” etc. Hence, a similarity-based model of meaning — for instance, a distributional semantic model — should provide benefits over a purely word-</context>
</contexts>
<marker>Core, 1998</marker>
<rawString>Mark Core. 1998. Analyzing and predicting patterns of damsl utterance tags. In Proceedings of the AAAI spring symposium on Applying machine learning to discourse processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Dinu</author>
<author>M Lapata</author>
</authors>
<title>Measuring distributional similarity in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1162--1172</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15568" citStr="Dinu and Lapata, 2010" startWordPosition="2508" endWordPosition="2511">tributional semantics, the meanings of words are captured by a vector space model based on a word co-occurrence matrix. Each row in the matrix represents a target word, and each column represents a context word; each element in the matrix is the number of times a target word co-occured with a corresponding context word. The frequency counts are typically normalized, or weighted using tf-idf or log-likelihood ratio to obtain better results, see (Mitchell and Lapata, 2008; Agirre et al., 2009) for various approaches. It is also common to apply dimensionality reduction to get higher performance (Dinu and Lapata, 2010; Baroni and Zamparelli, 2010). As target words we select all the words in our (Switchboard) training split. As context words we choose the 3000 most frequent words in the Google Ngram Corpus, excluding the 100 most frequent. To obtain co-occurrence frequencies from ngrams we sum up the frequency of a 5-gram over the years, treat the word in the middle as a target, and the other words as its contexts. For normalization, we experiment with a vector space based on raw co-occurrences; a vector space where frequencies are weighted using tf-idf; and another one with the number of dimensions reduced</context>
<context position="24227" citStr="Dinu and Lapata, 2010" startWordPosition="3910" endWordPosition="3913">n) are likely to be insufficient: more advanced, sequence- or grammar-driven composition, such as categorical composition (Coecke et al., 2010), might improve the tagging accuracy. In addition, our results show that the performance of distributional models depends on many factors, including compositional operator selection and weighting of the initial co-occurrence matrix. Our work leaves much scope for improvements in these factors, including co-occurrence matrix instantiation. For example, the window size of 2, which we used to obtain co-occurrence counts, is lower than the usual size of 5 (Dinu and Lapata, 2010), or the sentence level (Baroni and Zamparelli, 2010). Word representation in a vector space using neural networks might improve accuracy as well (Mikolov et al., 2013). Previous approaches to dialogue act tagging have shown utterance/tag sequence to be a useful source of information for improved accuracy (Stolcke et al., 2000). We therefore conclude that the lower accuracy we obtained using models that include information about the previous utterance is due again to our simplistic method of composition (vector concatenation); models which reflect dialogue structure or sequence explicitly are </context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>G. Dinu and M. Lapata. 2010. Measuring distributional similarity in context. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162–1172. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Firth</author>
</authors>
<title>A Synopsis of Linguistic Theory,</title>
<date>1957</date>
<booktitle>Studies in Linguistic Analysis,</booktitle>
<pages>1--32</pages>
<contexts>
<context position="4266" citStr="Firth, 1957" startWordPosition="648" endWordPosition="649"> vector composition. We describe the experiment and discuss the result in Section 4. Finally, Section 5 concludes the work. 40 Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 40–47, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related work Distributional semantics The aim of natural language semantics is to provide logical representations of meaning for information in textual form. Distributional semantics is based on the idea that “You shall know a word by the company it keeps” (Firth, 1957) – in other words, the meaning of a word is related to the contexts it appears in. Following this idea, word meaning can be represented as a vector where its dimensions correspond to the usage contexts, usually other words observed to co-occur, and the values are the co-occurrence frequencies. Such a meaning representation is easy to build from raw data and does not need rich annotation. Methods based on this distributional hypothesis have recently been applied to many tasks, but mostly at the word level: for instance, word sense disambiguation (Zhitomirsky-Geffet and Dagan, 2009) and lexical </context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>John R. Firth. 1957. A Synopsis of Linguistic Theory, 1930-1955. Studies in Linguistic Analysis, pages 1– 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward C Holliman</author>
<author>Jane McDaniel</author>
</authors>
<title>Switchboard: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>517--520</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="12792" citStr="Godfrey et al., 1992" startWordPosition="2012" endWordPosition="2015">of dialog tags in a conversation. In this model, we are interested specifically in the effect of inter-utterance (tag) sequence. We provide previous addition and previous multiplication models as simple attempts to capture this phenomenon: the vector of an utterance is the concatenation of its vector obtained in the corresponding compositional model (addition or multiplication) and the vector of the previous utterance. 4 Predicting dialogue acts 4.1 The resources This section describes the resources we use to evaluate and compare the proposed models. Switchboard corpus The Switchboard corpus (Godfrey et al., 1992) is a corpus of telephone conversations on selected topics. It consists of about 2500 conversations by 500 speakers from the U.S. The conversations in the corpus are labeled with 42 unique dialogue act tags and split to 1115 train 42 A o : Okay. / A qw : 1D So, } B qy&amp;quot;d: [ [I guess, + A + : What kind of experience [ do you, + do you ] have, then with child care? / B + : I think, ] + 1F uh, } I wonder if that worked. / (a) A conversation with interrupted utterances. A o : Okay. A qw : So What kind of experience do you do you have then with child care? B qy&amp;quot;d: I guess I think uh I wonder if that</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>John J Godfrey, Edward C Holliman, and Jane McDaniel. 1992. Switchboard: Telephone speech corpus for research and development. In Acoustics, Speech, and Signal Processing, 1992. ICASSP-92., 1992 IEEE International Conference on, volume 1, pages 517–520. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1394--1404</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2709" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="401" endWordPosition="404">e to extend such similarity-based models from single words to sentences and/or complete utterances. Hence, we consider here the application of compositional distributional semantics for this task. Here, we extend bag-of-word models common in previous approaches (Serafin et al., 2003) with simple compositional distributional operations (Mitchell and Lapata, 2008) and examine the improvements gained. These improvements suggest that distributional information does improve performance, but that more sophisticated compositional operations such as matrix multiplication (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011) should provide further benefits. The state of the art is a supervised method based on Recurrent Convolutional Neural Networks (Kalchbrenner and Blunsom, 2013). This method learns both the sentence model and the discourse model from the same training corpus, making it hard to understand how much of the contribution comes from the inclusion of distributional word meaning, and how much from learning patterns specific to the corpus at hand. Here, in contrast, we use an external unlabeled resource to obtain a model of word meaning, composing words to obtain representations for utterances, and rely</context>
<context position="5564" citStr="Grefenstette and Sadrzadeh (2011)" startWordPosition="854" endWordPosition="857">milarity which correlates with the angle between word vectors (Turney et al., 2010). Compositional distributional semantics goes beyond the word level and models the meaning of phrases or sentences based on their parts. Mitchell and Lapata (2008) perform composition of word vectors using vector addition and multiplication operations. The limitation of this approach is the operator associativity, which ignores the argument order, and thus word order. As a result, “John loves Mary” and “Mary loves John” get assigned the same meaning. To capture word order, various approaches have been proposed. Grefenstette and Sadrzadeh (2011) extend the compositional approach by using non-associative linear algebra operators as proposed in the theoretical work of (Coecke et al., 2010). Socher et al. (2012) present a recursive technique to build compositional meaning of phrases from their constituents, where the nonlinear composition operators are learned by Neural Networks. Dialogue act tagging There are many ways to approach the task of dialogue act tagging (Stolcke et al., 2000). The most successful approaches combine intra-utterance features, such as the (sequences of) words and intonational contours used, together with inter-u</context>
<context position="11091" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="1748" endWordPosition="1751">tributional hypothesis, by representing words not as atomic types (i.e. individual dimensions in the utterance matrix, as above), but as vectors encoding their observed co-occurrence distributions. We estimate these from a large corpus of general written English (the Google Books Ngrams corpus – see below). However, this raises the question of how to compose these word vectors into a single representation for an utterance. Various approaches to compositional vector space modelling have been successfully applied to capture the meaning of a phrase in a range of tasks (Mitchell and Lapata, 2008; Grefenstette and Sadrzadeh, 2011; Socher et al., 2013). In this work, we follow (Mitchell and Lapata, 2008) and apply vector addition and pointwise multiplication to obtain the vector of an utterance from the words it consists of. This has the advantage of simplicity and domain-generality, requiring no sentence grammar (problematic for the non-canonical language in dialogue) or training on a specific corpus to obtain the appropriate compositionality operators or associative model; but has the disadvantage of losing word order information. The corresponding models are referred as addition and multiplication in Table 1 and Tab</context>
<context position="18971" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="3052" endWordPosition="3055">, the bag of bigrams model improves over unigrams. This confirms that word order provides important information for predicting dialogue act tags. Distributional models Performance of compositional distributional models depends both on compositional operator and weighting. Table 2 demonstrates accuracy of the models. We instantiate 3 vector spaces from Google Ngrams: one space with raw co-occurrence frequencies, a tf-idf weighted space and a reduced space using NMF. Addition outperforms multiplication in our experiments, although for other tasks multiplication has been shown to perform better (Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2008). Lower multiplication performance here might be 2The CallHome37 corpus is not currently available to us. Model Space Raw tf-idf NMF Addition without SVD 0.592 Addition 0.610 0.639 0.620 Multiplication 0.572 0.568 0.525 Previous addition 0.569 Previous multiplication 0.497 Table 2: Accuracy results for different compositional models and vector spaces. due to the fact that some utterances are rather long (for example, more than 70 tokens), and the resulting vectors get many zero components. Selection of the optimal weighting method could be crucial for overall model </context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1394–1404. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrik O Hoyer</author>
</authors>
<title>Non-negative matrix factorization with sparseness constraints.</title>
<date>2004</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>5--1457</pages>
<contexts>
<context position="16236" citStr="Hoyer, 2004" startWordPosition="2621" endWordPosition="2622">t all the words in our (Switchboard) training split. As context words we choose the 3000 most frequent words in the Google Ngram Corpus, excluding the 100 most frequent. To obtain co-occurrence frequencies from ngrams we sum up the frequency of a 5-gram over the years, treat the word in the middle as a target, and the other words as its contexts. For normalization, we experiment with a vector space based on raw co-occurrences; a vector space where frequencies are weighted using tf-idf; and another one with the number of dimensions reduced to 1000 using Non-negative Matrix Factorization (NMF) (Hoyer, 2004). We use the NMF and tf-idf implementations provided by scikit-learn version 0.14 (Pedregosa et al., 2011). For tf-idf, the term vectors are L2 normalized. For NMF, NNDSVD initialization (Boutsidis and Gallopoulos, 2008) is used, and the tolerance value for stopping conditions is set to 0.001. The co-occurrence matrix is linenormalized, so the sum of the values in each row is 1 before applying NMF.1 4.3 Evaluation To evaluate these possible models we follow (Serafin et al., 2003). Once we have applied a model to extract features from utterances and build a vector space, the dimensionality of t</context>
</contexts>
<marker>Hoyer, 2004</marker>
<rawString>Patrik O Hoyer. 2004. Non-negative matrix factorization with sparseness constraints. The Journal of Machine Learning Research, 5:1457–1469.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>Elizabeth Shriberg</author>
<author>Debra Biasca</author>
</authors>
<title>Switchboard swbd-damsl shallow-discoursefunction annotation coders manual, draft 13.</title>
<date>1997</date>
<tech>Technical Report 97-02,</tech>
<institution>University of Colorado, Boulder. Institute of Cognitive Science.</institution>
<contexts>
<context position="13571" citStr="Jurafsky et al., 1997" startWordPosition="2169" endWordPosition="2172">pus are labeled with 42 unique dialogue act tags and split to 1115 train 42 A o : Okay. / A qw : 1D So, } B qy&amp;quot;d: [ [I guess, + A + : What kind of experience [ do you, + do you ] have, then with child care? / B + : I think, ] + 1F uh, } I wonder if that worked. / (a) A conversation with interrupted utterances. A o : Okay. A qw : So What kind of experience do you do you have then with child care? B qy&amp;quot;d: I guess I think uh I wonder if that worked. (b) A preprocessed conversation. Figure 1: A example of interrupted utterances from Switchboard and their transformation. and 19 test conversations (Jurafsky et al., 1997; Stolcke et al., 2000). In addition to the dialog act tags, utterances interrupted by the other speaker (and thus split into two or more parts) have their continuations marked with a special tag “+”. Tag prediction of one part of an interrupted utterance in isolation is a difficult task even for a human; for example, it would not be clear why the utterance “So,” should be assigned the tag qw (wh-question) in Figure 1a without the second part “What kind of experience do you have [... ]”. Following (Webb et al., 2005) we preprocess Switchboard by concatenating the parts of an interrupted uttera</context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>Daniel Jurafsky, Elizabeth Shriberg, and Debra Biasca. 1997. Switchboard swbd-damsl shallow-discoursefunction annotation coders manual, draft 13. Technical Report 97-02, University of Colorado, Boulder. Institute of Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>Elizabeth Shriberg</author>
<author>Barbara Fox</author>
<author>Traci Curl</author>
</authors>
<title>Lexical, prosodic, and syntactic cues for dialog acts.</title>
<date>1998</date>
<booktitle>In Proceedings of the ACL-COLING Workshop on Discourse Relations and Discourse Markers.</booktitle>
<contexts>
<context position="1443" citStr="Jurafsky et al., 1998" startWordPosition="209" endWordPosition="212">useful for dialogue act tagging but that simple models of compositionality fail to capture crucial information from word and utterance sequence; more advanced approaches (e.g. sequence- or grammar-driven, such as categorical, word vector composition) are required. 1 Introduction One of the fundamental tasks in automatic dialogue processing is dialogue act tagging: labelling each utterance with a tag relating to its function in the dialogue and effect on the emerging context: greeting, query, statement etc (see e.g. (Core, 1998)). Although factors such as intonation also play a role (see e.g. (Jurafsky et al., 1998)), one of the most important sources of information in this task is the semantic meaning of an utterance, and this is reflected in the fact that people use similar words when they perform similar utterance acts. For example, utterances which state opinion (tagged sv in the standard DAMSL schema, see below) often include words such as “I think”, “I believe”, “I guess” etc. Hence, a similarity-based model of meaning — for instance, a distributional semantic model — should provide benefits over a purely word-based model for dialogue act tagging. However, since utterances generally consist of more</context>
</contexts>
<marker>Jurafsky, Shriberg, Fox, Curl, 1998</marker>
<rawString>Daniel Jurafsky, Elizabeth Shriberg, Barbara Fox, and Traci Curl. 1998. Lexical, prosodic, and syntactic cues for dialog acts. In Proceedings of the ACL-COLING Workshop on Discourse Relations and Discourse Markers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent convolutional neural networks for discourse compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<pages>119--126</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2868" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="425" endWordPosition="428">ributional semantics for this task. Here, we extend bag-of-word models common in previous approaches (Serafin et al., 2003) with simple compositional distributional operations (Mitchell and Lapata, 2008) and examine the improvements gained. These improvements suggest that distributional information does improve performance, but that more sophisticated compositional operations such as matrix multiplication (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011) should provide further benefits. The state of the art is a supervised method based on Recurrent Convolutional Neural Networks (Kalchbrenner and Blunsom, 2013). This method learns both the sentence model and the discourse model from the same training corpus, making it hard to understand how much of the contribution comes from the inclusion of distributional word meaning, and how much from learning patterns specific to the corpus at hand. Here, in contrast, we use an external unlabeled resource to obtain a model of word meaning, composing words to obtain representations for utterances, and rely on training data only for discourse learning for the tagging task itself. We proceed as follows. First, we discuss related work by introducing distributional </context>
<context position="6925" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="1065" endWordPosition="1068"> such as Hidden Markov Models are widely used (Stolcke et al., 2000; Surendran and Levow, 2006). The sequence of words is an observable variable, while the sequence of dialogue act tags is a hidden variable. However, some approaches have shown competitive results without exploiting features of interutterance context. Webb et al. (2005) concentrate only on features found inside an utterance, identifying ngrams that correlate strongly with particular utterance tags, and propose a statistical model for prediction which produces close to the state of the art results. The current state of the art (Kalchbrenner and Blunsom, 2013) uses Recurrent Convolutional Neural Networks to achieve high accuracy. This model includes information about word identity, intra-utterance word sequence, and inter-utterance tag sequence, by using a vector space model of words with a compositional approach. The words vectors are not based on distributional frequencies in this case, however, but on randomly initialised vectors, with the model trained on a specific corpus. This raises several questions: what is the contribution of word sequence and/or utterance (tag) sequence; and might further gains be made by exploiting the distributional hy</context>
<context position="12058" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="1900" endWordPosition="1903">dialogue) or training on a specific corpus to obtain the appropriate compositionality operators or associative model; but has the disadvantage of losing word order information. The corresponding models are referred as addition and multiplication in Table 1 and Table 2. Previous utterance A conversation is a sequence of utterances, and the tag of an utterance often depends on the previous utterance (e.g. answers tend to follow questions). Hidden Markov Models (Surendran and Levow, 2006; Stolcke et al., 2000) are often used to capture these dependencies; Recurrent Convolutional Neural Networks (Kalchbrenner and Blunsom, 2013) have been used to simultaneously capture the intra-utterance sequence of words and the inter-utterance sequence of dialog tags in a conversation. In this model, we are interested specifically in the effect of inter-utterance (tag) sequence. We provide previous addition and previous multiplication models as simple attempts to capture this phenomenon: the vector of an utterance is the concatenation of its vector obtained in the corresponding compositional model (addition or multiplication) and the vector of the previous utterance. 4 Predicting dialogue acts 4.1 The resources This section descri</context>
<context position="17294" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="2788" endWordPosition="2791">te these possible models we follow (Serafin et al., 2003). Once we have applied a model to extract features from utterances and build a vector space, the dimensionality of the vector space is reduced using SVD to 50 dimensions. Then a k-nearest neighbours (KNN) classifier is trained and used for utterance tag prediction. In contrast to (Serafin et al., 2003), we use Euclidean distance as a distance metric and choose the most 1The co-occurrence matrix and the information about the software used in the experiment are available at http://www.eecs.qmul.ac.uk/˜dm303/cvsc14.html 43 Method Accuracy (Kalchbrenner and Blunsom, 2013) 0.739 (Webb et al., 2005) 0.719 (Stolcke et al., 2000) 0.710 (Serafin et al., 2003) 0.654 Bag of unigrams 0.602 Bag of bigrams 0.621 Addition 0.639 Multiplication 0.572 Previous addition 0.569 Previous multiplication 0.497 Table 1: Comparison with previous work. Note that (Serafin et al., 2003) do not use Switchboard and therefore their results are not directly comparable to others. frequent label among the 5 closest neighbors. The SVD and KNN classifier implementations in scikit-learn are used. Baseline In our experiments, the bag of unigrams model accuracy of 0.602 is lower than the accurac</context>
<context position="22143" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="3589" endWordPosition="3593">bunch of elderly folks in the country that could use some help Figure 2: The beginning of the conversation 2151 from the test split of Switchboard. In brackets the tags predicted using vector addition as a composition method on the tf-idf space are given. We mark fo o fw &amp;quot; by bc as **. Summary Our accuracy is lower compared to other work. Webb et al. (2005)’s method, based only on intra-utterance lexical features, but incorporating longer ngram sequences and feature selection, yields accuracy of 0.719. Advanced treatment of both utterance and discourse level features yields accuracy of 0.739 (Kalchbrenner and Blunsom, 2013). However, our experiments allow us to evaluate the contribution of various kinds of information: vector spaces based on word bigrams and on co-occurrence distributions both outperformed the bag of words approach; but incorporation of previous utterance information did not. 5 Conclusions and future work In this work we evaluated the contribution of word and utterance sequence, and of distributional information using simple compositional vector space models, for dialogue act tagging. Our experiments show that information about intra-utterance word order (ngrams), and information about word co-o</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent convolutional neural networks for discourse compositionality. In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 119–126, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuri Lin</author>
<author>Jean-Baptiste Michel</author>
<author>Erez Lieberman Aiden</author>
<author>Jon Orwant</author>
<author>Will Brockman</author>
<author>Slav Petrov</author>
</authors>
<title>Syntactic annotations for the Google Books ngram corpus.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 System Demonstrations,</booktitle>
<pages>169--174</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14583" citStr="Lin et al., 2012" startWordPosition="2340" endWordPosition="2343"> (wh-question) in Figure 1a without the second part “What kind of experience do you have [... ]”. Following (Webb et al., 2005) we preprocess Switchboard by concatenating the parts of an interrupted utterance together, giving the result the tag of the first part and putting it in its place in the conversation sequence. We also remove commas and disfluency markers from the raw text. Figure 1b illustrates the transformation we do as preprocessing. We split the utterances between training and testing as suggested in (Stolcke et al., 2000). Google Books Ngram Corpus The Google Books Ngram Corpus (Lin et al., 2012) is a collection of n-gram frequencies over books written in 8 languages. The English part of the corpus is based on more than 4.5 million books and contains more than four thousand billion tokens. The resource provides frequencies of n-grams of length 1 to 5. For our experiments we use 5-grams from the English part of the resource. 4.2 Word vector spaces In distributional semantics, the meanings of words are captured by a vector space model based on a word co-occurrence matrix. Each row in the matrix represents a target word, and each column represents a context word; each element in the matr</context>
</contexts>
<marker>Lin, Michel, Aiden, Orwant, Brockman, Petrov, 2012</marker>
<rawString>Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden, Jon Orwant, Will Brockman, and Slav Petrov. 2012. Syntactic annotations for the Google Books ngram corpus. In Proceedings of the ACL 2012 System Demonstrations, pages 169–174. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="24395" citStr="Mikolov et al., 2013" startWordPosition="3938" endWordPosition="3941">g accuracy. In addition, our results show that the performance of distributional models depends on many factors, including compositional operator selection and weighting of the initial co-occurrence matrix. Our work leaves much scope for improvements in these factors, including co-occurrence matrix instantiation. For example, the window size of 2, which we used to obtain co-occurrence counts, is lower than the usual size of 5 (Dinu and Lapata, 2010), or the sentence level (Baroni and Zamparelli, 2010). Word representation in a vector space using neural networks might improve accuracy as well (Mikolov et al., 2013). Previous approaches to dialogue act tagging have shown utterance/tag sequence to be a useful source of information for improved accuracy (Stolcke et al., 2000). We therefore conclude that the lower accuracy we obtained using models that include information about the previous utterance is due again to our simplistic method of composition (vector concatenation); models which reflect dialogue structure or sequence explicitly are likely to be more suited. Kalchbrenner and Blunsom (2013) give one way in which this can be achieved by learning from a specific corpus, and the question of possible al</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2440" citStr="Mitchell and Lapata, 2008" startWordPosition="367" endWordPosition="370">tc. Hence, a similarity-based model of meaning — for instance, a distributional semantic model — should provide benefits over a purely word-based model for dialogue act tagging. However, since utterances generally consist of more than one word, one has to be able to extend such similarity-based models from single words to sentences and/or complete utterances. Hence, we consider here the application of compositional distributional semantics for this task. Here, we extend bag-of-word models common in previous approaches (Serafin et al., 2003) with simple compositional distributional operations (Mitchell and Lapata, 2008) and examine the improvements gained. These improvements suggest that distributional information does improve performance, but that more sophisticated compositional operations such as matrix multiplication (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011) should provide further benefits. The state of the art is a supervised method based on Recurrent Convolutional Neural Networks (Kalchbrenner and Blunsom, 2013). This method learns both the sentence model and the discourse model from the same training corpus, making it hard to understand how much of the contribution comes from the</context>
<context position="5177" citStr="Mitchell and Lapata (2008)" startWordPosition="795" endWordPosition="798">quencies. Such a meaning representation is easy to build from raw data and does not need rich annotation. Methods based on this distributional hypothesis have recently been applied to many tasks, but mostly at the word level: for instance, word sense disambiguation (Zhitomirsky-Geffet and Dagan, 2009) and lexical substitution (Thater et al., 2010). They exploit the notion of similarity which correlates with the angle between word vectors (Turney et al., 2010). Compositional distributional semantics goes beyond the word level and models the meaning of phrases or sentences based on their parts. Mitchell and Lapata (2008) perform composition of word vectors using vector addition and multiplication operations. The limitation of this approach is the operator associativity, which ignores the argument order, and thus word order. As a result, “John loves Mary” and “Mary loves John” get assigned the same meaning. To capture word order, various approaches have been proposed. Grefenstette and Sadrzadeh (2011) extend the compositional approach by using non-associative linear algebra operators as proposed in the theoretical work of (Coecke et al., 2010). Socher et al. (2012) present a recursive technique to build compos</context>
<context position="11057" citStr="Mitchell and Lapata, 2008" startWordPosition="1744" endWordPosition="1747">cond model exploits the distributional hypothesis, by representing words not as atomic types (i.e. individual dimensions in the utterance matrix, as above), but as vectors encoding their observed co-occurrence distributions. We estimate these from a large corpus of general written English (the Google Books Ngrams corpus – see below). However, this raises the question of how to compose these word vectors into a single representation for an utterance. Various approaches to compositional vector space modelling have been successfully applied to capture the meaning of a phrase in a range of tasks (Mitchell and Lapata, 2008; Grefenstette and Sadrzadeh, 2011; Socher et al., 2013). In this work, we follow (Mitchell and Lapata, 2008) and apply vector addition and pointwise multiplication to obtain the vector of an utterance from the words it consists of. This has the advantage of simplicity and domain-generality, requiring no sentence grammar (problematic for the non-canonical language in dialogue) or training on a specific corpus to obtain the appropriate compositionality operators or associative model; but has the disadvantage of losing word order information. The corresponding models are referred as addition and</context>
<context position="15421" citStr="Mitchell and Lapata, 2008" startWordPosition="2485" endWordPosition="2488">ovides frequencies of n-grams of length 1 to 5. For our experiments we use 5-grams from the English part of the resource. 4.2 Word vector spaces In distributional semantics, the meanings of words are captured by a vector space model based on a word co-occurrence matrix. Each row in the matrix represents a target word, and each column represents a context word; each element in the matrix is the number of times a target word co-occured with a corresponding context word. The frequency counts are typically normalized, or weighted using tf-idf or log-likelihood ratio to obtain better results, see (Mitchell and Lapata, 2008; Agirre et al., 2009) for various approaches. It is also common to apply dimensionality reduction to get higher performance (Dinu and Lapata, 2010; Baroni and Zamparelli, 2010). As target words we select all the words in our (Switchboard) training split. As context words we choose the 3000 most frequent words in the Google Ngram Corpus, excluding the 100 most frequent. To obtain co-occurrence frequencies from ngrams we sum up the frequency of a 5-gram over the years, treat the word in the middle as a target, and the other words as its contexts. For normalization, we experiment with a vector s</context>
<context position="18999" citStr="Mitchell and Lapata, 2008" startWordPosition="3056" endWordPosition="3059">s over unigrams. This confirms that word order provides important information for predicting dialogue act tags. Distributional models Performance of compositional distributional models depends both on compositional operator and weighting. Table 2 demonstrates accuracy of the models. We instantiate 3 vector spaces from Google Ngrams: one space with raw co-occurrence frequencies, a tf-idf weighted space and a reduced space using NMF. Addition outperforms multiplication in our experiments, although for other tasks multiplication has been shown to perform better (Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2008). Lower multiplication performance here might be 2The CallHome37 corpus is not currently available to us. Model Space Raw tf-idf NMF Addition without SVD 0.592 Addition 0.610 0.639 0.620 Multiplication 0.572 0.568 0.525 Previous addition 0.569 Previous multiplication 0.497 Table 2: Accuracy results for different compositional models and vector spaces. due to the fact that some utterances are rather long (for example, more than 70 tokens), and the resulting vectors get many zero components. Selection of the optimal weighting method could be crucial for overall model performance. The 3 weighting</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="16342" citStr="Pedregosa et al., 2011" startWordPosition="2635" endWordPosition="2639">requent words in the Google Ngram Corpus, excluding the 100 most frequent. To obtain co-occurrence frequencies from ngrams we sum up the frequency of a 5-gram over the years, treat the word in the middle as a target, and the other words as its contexts. For normalization, we experiment with a vector space based on raw co-occurrences; a vector space where frequencies are weighted using tf-idf; and another one with the number of dimensions reduced to 1000 using Non-negative Matrix Factorization (NMF) (Hoyer, 2004). We use the NMF and tf-idf implementations provided by scikit-learn version 0.14 (Pedregosa et al., 2011). For tf-idf, the term vectors are L2 normalized. For NMF, NNDSVD initialization (Boutsidis and Gallopoulos, 2008) is used, and the tolerance value for stopping conditions is set to 0.001. The co-occurrence matrix is linenormalized, so the sum of the values in each row is 1 before applying NMF.1 4.3 Evaluation To evaluate these possible models we follow (Serafin et al., 2003). Once we have applied a model to extract features from utterances and build a vector space, the dimensionality of the vector space is reduced using SVD to 50 dimensions. Then a k-nearest neighbours (KNN) classifier is tra</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Riccardo Serafin</author>
<author>Barbara Di Eugenio</author>
<author>Michael Glass</author>
</authors>
<title>Latent semantic analysis for dialogue act classification.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: companion volume of the Proceedings of HLT-NAACL 2003–short</booktitle>
<volume>2</volume>
<pages>94--96</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Serafin, Di Eugenio, Glass, 2003</marker>
<rawString>Riccardo Serafin, Barbara Di Eugenio, and Michael Glass. 2003. Latent semantic analysis for dialogue act classification. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: companion volume of the Proceedings of HLT-NAACL 2003–short papersVolume 2, pages 94–96. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5731" citStr="Socher et al. (2012)" startWordPosition="880" endWordPosition="883">ases or sentences based on their parts. Mitchell and Lapata (2008) perform composition of word vectors using vector addition and multiplication operations. The limitation of this approach is the operator associativity, which ignores the argument order, and thus word order. As a result, “John loves Mary” and “Mary loves John” get assigned the same meaning. To capture word order, various approaches have been proposed. Grefenstette and Sadrzadeh (2011) extend the compositional approach by using non-associative linear algebra operators as proposed in the theoretical work of (Coecke et al., 2010). Socher et al. (2012) present a recursive technique to build compositional meaning of phrases from their constituents, where the nonlinear composition operators are learned by Neural Networks. Dialogue act tagging There are many ways to approach the task of dialogue act tagging (Stolcke et al., 2000). The most successful approaches combine intra-utterance features, such as the (sequences of) words and intonational contours used, together with inter-utterance features, such as the sequence of utterance tags being used previously. To capture both of these aspects, sequence models such as Hidden Markov Models are wid</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars. In</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL conference. Citeseer.</booktitle>
<contexts>
<context position="11113" citStr="Socher et al., 2013" startWordPosition="1752" endWordPosition="1755">nting words not as atomic types (i.e. individual dimensions in the utterance matrix, as above), but as vectors encoding their observed co-occurrence distributions. We estimate these from a large corpus of general written English (the Google Books Ngrams corpus – see below). However, this raises the question of how to compose these word vectors into a single representation for an utterance. Various approaches to compositional vector space modelling have been successfully applied to capture the meaning of a phrase in a range of tasks (Mitchell and Lapata, 2008; Grefenstette and Sadrzadeh, 2011; Socher et al., 2013). In this work, we follow (Mitchell and Lapata, 2008) and apply vector addition and pointwise multiplication to obtain the vector of an utterance from the words it consists of. This has the advantage of simplicity and domain-generality, requiring no sentence grammar (problematic for the non-canonical language in dialogue) or training on a specific corpus to obtain the appropriate compositionality operators or associative model; but has the disadvantage of losing word order information. The corresponding models are referred as addition and multiplication in Table 1 and Table 2. Previous utteran</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013. Parsing with compositional vector grammars. In In Proceedings of the ACL conference. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Daniel Jurafsky</author>
<author>Paul Taylor</author>
<author>Carol Van Ess-Dykema</author>
<author>Rachel Martin</author>
<author>Marie Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Van Ess-Dykema, Martin, Meteer, 2000</marker>
<rawString>Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Carol Van Ess-Dykema, Rachel Martin, and Marie Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dinoj Surendran</author>
<author>Gina-Anne Levow</author>
</authors>
<title>Dialog act tagging with support vector machines and hidden markov models.</title>
<date>2006</date>
<booktitle>In INTERSPEECH.</booktitle>
<contexts>
<context position="6389" citStr="Surendran and Levow, 2006" startWordPosition="981" endWordPosition="984">o build compositional meaning of phrases from their constituents, where the nonlinear composition operators are learned by Neural Networks. Dialogue act tagging There are many ways to approach the task of dialogue act tagging (Stolcke et al., 2000). The most successful approaches combine intra-utterance features, such as the (sequences of) words and intonational contours used, together with inter-utterance features, such as the sequence of utterance tags being used previously. To capture both of these aspects, sequence models such as Hidden Markov Models are widely used (Stolcke et al., 2000; Surendran and Levow, 2006). The sequence of words is an observable variable, while the sequence of dialogue act tags is a hidden variable. However, some approaches have shown competitive results without exploiting features of interutterance context. Webb et al. (2005) concentrate only on features found inside an utterance, identifying ngrams that correlate strongly with particular utterance tags, and propose a statistical model for prediction which produces close to the state of the art results. The current state of the art (Kalchbrenner and Blunsom, 2013) uses Recurrent Convolutional Neural Networks to achieve high ac</context>
<context position="11916" citStr="Surendran and Levow, 2006" startWordPosition="1880" endWordPosition="1883">his has the advantage of simplicity and domain-generality, requiring no sentence grammar (problematic for the non-canonical language in dialogue) or training on a specific corpus to obtain the appropriate compositionality operators or associative model; but has the disadvantage of losing word order information. The corresponding models are referred as addition and multiplication in Table 1 and Table 2. Previous utterance A conversation is a sequence of utterances, and the tag of an utterance often depends on the previous utterance (e.g. answers tend to follow questions). Hidden Markov Models (Surendran and Levow, 2006; Stolcke et al., 2000) are often used to capture these dependencies; Recurrent Convolutional Neural Networks (Kalchbrenner and Blunsom, 2013) have been used to simultaneously capture the intra-utterance sequence of words and the inter-utterance sequence of dialog tags in a conversation. In this model, we are interested specifically in the effect of inter-utterance (tag) sequence. We provide previous addition and previous multiplication models as simple attempts to capture this phenomenon: the vector of an utterance is the concatenation of its vector obtained in the corresponding compositional</context>
</contexts>
<marker>Surendran, Levow, 2006</marker>
<rawString>Dinoj Surendran and Gina-Anne Levow. 2006. Dialog act tagging with support vector machines and hidden markov models. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>948--957</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Thater, F¨urstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 948–957, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of artificial intelligence research,</journal>
<volume>37</volume>
<issue>1</issue>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D Turney, Patrick Pantel, et al. 2010. From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Webb</author>
<author>Mark Hepple</author>
<author>Yorick Wilks</author>
</authors>
<title>Dialogue act classification based on intra-utterance features.</title>
<date>2005</date>
<booktitle>In Proceedings of the AAAI Workshop on Spoken Language Understanding. Citeseer.</booktitle>
<contexts>
<context position="6631" citStr="Webb et al. (2005)" startWordPosition="1019" endWordPosition="1022">The most successful approaches combine intra-utterance features, such as the (sequences of) words and intonational contours used, together with inter-utterance features, such as the sequence of utterance tags being used previously. To capture both of these aspects, sequence models such as Hidden Markov Models are widely used (Stolcke et al., 2000; Surendran and Levow, 2006). The sequence of words is an observable variable, while the sequence of dialogue act tags is a hidden variable. However, some approaches have shown competitive results without exploiting features of interutterance context. Webb et al. (2005) concentrate only on features found inside an utterance, identifying ngrams that correlate strongly with particular utterance tags, and propose a statistical model for prediction which produces close to the state of the art results. The current state of the art (Kalchbrenner and Blunsom, 2013) uses Recurrent Convolutional Neural Networks to achieve high accuracy. This model includes information about word identity, intra-utterance word sequence, and inter-utterance tag sequence, by using a vector space model of words with a compositional approach. The words vectors are not based on distributio</context>
<context position="14093" citStr="Webb et al., 2005" startWordPosition="2261" endWordPosition="2264">nces from Switchboard and their transformation. and 19 test conversations (Jurafsky et al., 1997; Stolcke et al., 2000). In addition to the dialog act tags, utterances interrupted by the other speaker (and thus split into two or more parts) have their continuations marked with a special tag “+”. Tag prediction of one part of an interrupted utterance in isolation is a difficult task even for a human; for example, it would not be clear why the utterance “So,” should be assigned the tag qw (wh-question) in Figure 1a without the second part “What kind of experience do you have [... ]”. Following (Webb et al., 2005) we preprocess Switchboard by concatenating the parts of an interrupted utterance together, giving the result the tag of the first part and putting it in its place in the conversation sequence. We also remove commas and disfluency markers from the raw text. Figure 1b illustrates the transformation we do as preprocessing. We split the utterances between training and testing as suggested in (Stolcke et al., 2000). Google Books Ngram Corpus The Google Books Ngram Corpus (Lin et al., 2012) is a collection of n-gram frequencies over books written in 8 languages. The English part of the corpus is ba</context>
<context position="17320" citStr="Webb et al., 2005" startWordPosition="2793" endWordPosition="2796">afin et al., 2003). Once we have applied a model to extract features from utterances and build a vector space, the dimensionality of the vector space is reduced using SVD to 50 dimensions. Then a k-nearest neighbours (KNN) classifier is trained and used for utterance tag prediction. In contrast to (Serafin et al., 2003), we use Euclidean distance as a distance metric and choose the most 1The co-occurrence matrix and the information about the software used in the experiment are available at http://www.eecs.qmul.ac.uk/˜dm303/cvsc14.html 43 Method Accuracy (Kalchbrenner and Blunsom, 2013) 0.739 (Webb et al., 2005) 0.719 (Stolcke et al., 2000) 0.710 (Serafin et al., 2003) 0.654 Bag of unigrams 0.602 Bag of bigrams 0.621 Addition 0.639 Multiplication 0.572 Previous addition 0.569 Previous multiplication 0.497 Table 1: Comparison with previous work. Note that (Serafin et al., 2003) do not use Switchboard and therefore their results are not directly comparable to others. frequent label among the 5 closest neighbors. The SVD and KNN classifier implementations in scikit-learn are used. Baseline In our experiments, the bag of unigrams model accuracy of 0.602 is lower than the accuracy of 0.654 reported in (Se</context>
<context position="21871" citStr="Webb et al. (2005)" startWordPosition="3550" endWordPosition="3553">&gt; A sv (sv): Well I I think it’s a pretty good idea. A sv (sd): I think they should either do that or or afford some time to the military or or helping elderly people. B aa (aa): Yes B aa (b) : yes B % (%) : def A sv (sv): I I you know I think that we have a bunch of elderly folks in the country that could use some help Figure 2: The beginning of the conversation 2151 from the test split of Switchboard. In brackets the tags predicted using vector addition as a composition method on the tf-idf space are given. We mark fo o fw &amp;quot; by bc as **. Summary Our accuracy is lower compared to other work. Webb et al. (2005)’s method, based only on intra-utterance lexical features, but incorporating longer ngram sequences and feature selection, yields accuracy of 0.719. Advanced treatment of both utterance and discourse level features yields accuracy of 0.739 (Kalchbrenner and Blunsom, 2013). However, our experiments allow us to evaluate the contribution of various kinds of information: vector spaces based on word bigrams and on co-occurrence distributions both outperformed the bag of words approach; but incorporation of previous utterance information did not. 5 Conclusions and future work In this work we evaluat</context>
<context position="23381" citStr="Webb et al. (2005)" startWordPosition="3782" endWordPosition="3785">ns, outperforms the bag of words models, although not competitive with the state of the art given the simplistic compositional approach used here. Information about utterance tag sequence, on the other hand, did not. The usage of an external, large scale resource (here, the Google Ngram Corpus) to model word senses improves the tagging accuracy in comparison to the bag of word model, suggesting that the dialogue act tag of an utterance depends on its semantics. However, the improvements in performance of the bag of bigrams model in comparison to bag of unigrams, and the much higher results of Webb et al. (2005)’s intra-utterance approach, suggest that the sequence of words inside an utterance is crucial for the dialogue act tagging task. This suggests that our simplistic approaches to vector composition (addition and multiplication) are likely to be insufficient: more advanced, sequence- or grammar-driven composition, such as categorical composition (Coecke et al., 2010), might improve the tagging accuracy. In addition, our results show that the performance of distributional models depends on many factors, including compositional operator selection and weighting of the initial co-occurrence matrix. </context>
</contexts>
<marker>Webb, Hepple, Wilks, 2005</marker>
<rawString>Nick Webb, Mark Hepple, and Yorick Wilks. 2005. Dialogue act classification based on intra-utterance features. In Proceedings of the AAAI Workshop on Spoken Language Understanding. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhitomirsky-Geffet</author>
<author>I Dagan</author>
</authors>
<title>Bootstrapping distributional feature vector quality.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="4853" citStr="Zhitomirsky-Geffet and Dagan, 2009" startWordPosition="744" endWordPosition="747">ow a word by the company it keeps” (Firth, 1957) – in other words, the meaning of a word is related to the contexts it appears in. Following this idea, word meaning can be represented as a vector where its dimensions correspond to the usage contexts, usually other words observed to co-occur, and the values are the co-occurrence frequencies. Such a meaning representation is easy to build from raw data and does not need rich annotation. Methods based on this distributional hypothesis have recently been applied to many tasks, but mostly at the word level: for instance, word sense disambiguation (Zhitomirsky-Geffet and Dagan, 2009) and lexical substitution (Thater et al., 2010). They exploit the notion of similarity which correlates with the angle between word vectors (Turney et al., 2010). Compositional distributional semantics goes beyond the word level and models the meaning of phrases or sentences based on their parts. Mitchell and Lapata (2008) perform composition of word vectors using vector addition and multiplication operations. The limitation of this approach is the operator associativity, which ignores the argument order, and thus word order. As a result, “John loves Mary” and “Mary loves John” get assigned th</context>
</contexts>
<marker>Zhitomirsky-Geffet, Dagan, 2009</marker>
<rawString>M. Zhitomirsky-Geffet and I. Dagan. 2009. Bootstrapping distributional feature vector quality. Computational Linguistics, 35(3):435–461.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
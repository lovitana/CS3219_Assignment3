<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.079494">
<title confidence="0.991244">
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2014
</title>
<author confidence="0.9241395">
Teresa Herrmann, Mohammed Mediani, Eunah Cho, Thanh-Le Ha,
Jan Niehues, Isabel Slawik, Yuqi Zhang and Alex Waibel
</author>
<affiliation confidence="0.5310515">
Institute for Anthropomatics and Robotics
KIT - Karlsruhe Institute of Technology
</affiliation>
<email confidence="0.998019">
firstname.lastname@kit.edu
</email>
<sectionHeader confidence="0.993877" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999901842105263">
In this paper, we present the KIT
systems participating in the Shared
Translation Task translating between
EnglishHGerman and EnglishHFrench.
All translations are generated using
phrase-based translation systems, using
different kinds of word-based, part-of-
speech-based and cluster-based language
models trained on the provided data.
Additional models include bilingual lan-
guage models, reordering models based
on part-of-speech tags and syntactic parse
trees, as well as a lexicalized reordering
model. In order to make use of noisy
web-crawled data, we apply filtering
and data selection methods for language
modeling. A discriminative word lexicon
using source context information proved
beneficial for all translation directions.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999785">
We describe the KIT systems for the Shared Trans-
lation Task of the ACL 2014 Ninth Workshop on
Statistical Machine Translation. We participated
in the EnglishHGerman and EnglishHFrench
translation directions, using a phrase-based de-
coder with lattice input.
The paper is organized as follows: the next sec-
tion describes the data used for each translation
direction. Section 3 gives a detailed description of
our systems including all the models. The trans-
lation results for all directions are presented after-
wards and we close with a conclusion.
</bodyText>
<sectionHeader confidence="0.99648" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.99988375">
We utilize the provided EPPS, NC and Common
Crawl parallel corpora for EnglishHGerman and
GermanHEnglish, plus Giga for EnglishHFrench
and FrenchHEnglish. The monolingual part
of those parallel corpora, the News Shuffle
corpus for all four directions and additionally
the Gigaword corpus for EnglishHFrench and
GermanHEnglish are used as monolingual train-
ing data for the different language models. For
optimizing the system parameters, newstest2012
and newstest2013 are used as development and
test data respectively.
</bodyText>
<sectionHeader confidence="0.99688" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.99998835483871">
Before training we perform a common preprocess-
ing of the raw data, which includes removing long
sentences and sentences with a length mismatch
exceeding a certain threshold. Afterwards, we nor-
malize special symbols, dates, and numbers. Then
we perform smart-casing of the first letter of every
sentence. Compound splitting (Koehn and Knight,
2003) is performed on the source side of the cor-
pus for GermanHEnglish translation. In order to
improve the quality of the web-crawled Common
Crawl corpus, we filter out noisy sentence pairs us-
ing an SVM classifier for all four translation tasks
as described in Mediani et al. (2011).
Unless stated otherwise, we use 4-gram lan-
guage models (LM) with modified Kneser-Ney
smoothing, trained with the SRILM toolkit (Stol-
cke, 2002). All translations are generated by
an in-house phrase-based translation system (Vo-
gel, 2003), and we use Minimum Error Rate
Training (MERT) as described in Venugopal et
al. (2005) for optimization. The word align-
ment of the parallel corpora is generated using
the GIZA++ Toolkit (Och and Ney, 2003) for
both directions. Afterwards, the alignments are
combined using the grow-diag-final-and heuris-
tic. For EnglishHGerman, we use discrimi-
native word alignment trained on hand-aligned
data as described in Niehues and Vogel (2008).
The phrase table (PT) is built using the Moses
toolkit (Koehn et al., 2007). The phrase scoring
for the small data sets (GermanHEnglish) is also
</bodyText>
<page confidence="0.958115">
130
</page>
<bodyText confidence="0.908319642857143">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 130–135,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
done by the Moses toolkit, whereas the bigger sets
(French↔English) are scored by our in-house par-
allel phrase scorer (Mediani et al., 2012a). The
phrase pair probabilities are computed using mod-
ified Kneser-Ney smoothing as described in Foster
et al. (2006).
Since German is a highly inflected language,
we try to alleviate the out-of-vocabulary prob-
lem through quasi-morphological operations that
change the lexical entry of a known word form to
an unknown word form as described in Niehues
and Waibel (2011).
</bodyText>
<subsectionHeader confidence="0.999712">
3.1 Word Reordering Models
</subsectionHeader>
<bodyText confidence="0.999986275">
We apply automatically learned reordering rules
based on part-of-speech (POS) sequences and syn-
tactic parse tree constituents to perform source
sentence reordering according to the target lan-
guage word order. The rules are learned
from a parallel corpus with POS tags (Schmid,
1994) for the source side and a word align-
ment to learn reordering rules that cover short
range (Rottmann and Vogel, 2007) and long
range reorderings (Niehues and Kolss, 2009).
In addition, we apply a tree-based reordering
model (Herrmann et al., 2013) to better address
the differences in word order between German and
English. Here, a word alignment and syntactic
parse trees (Rafferty and Manning, 2008; Klein
and Manning, 2003) for the source side of the
training corpus are required to learn rules on how
to reorder the constituents in the source sentence.
The POS-based and tree-based reordering rules
are applied to each input sentence before transla-
tion. The resulting reordered sentence variants as
well as the original sentence are encoded in a re-
ordering lattice. The lattice, which also includes
the original position of each word, is used as input
to the decoder.
In order to acquire phrase pairs matching the
reordered sentence variants, we perform lattice
phrase extraction (LPE) on the training corpus
where phrase are extracted from the reordered
word lattices instead of the original sentences.
In addition, we use a lexicalized reordering
model (Koehn et al., 2005) which stores reorder-
ing probabilities for each phrase pair. During
decoding the lexicalized reordering model deter-
mines the reordering orientation of each phrase
pair at the phrase boundaries. The probability for
the respective orientation with respect to the orig-
inal position of the words is included as an addi-
tional score in the log-linear model of the transla-
tion system.
</bodyText>
<subsectionHeader confidence="0.999398">
3.2 Adaptation
</subsectionHeader>
<bodyText confidence="0.99996137037037">
In the French→English and English→French sys-
tems, we perform adaptation for translation mod-
els as well as for language models. The EPPS and
NC corpora are used as in-domain data for the di-
rection English→French, while NC corpus is the
in-domain data for French→English.
Two phrase tables are built: one is the out-
of-domain phrase table, which is trained on all
corpora; the other is the in-domain phrase table,
which is trained on in-domain data. We adapt the
translation model by using the scores from the two
phrase tables with the backoff approach described
in Niehues and Waibel (2012). This results in a
phrase table with six scores, the four scores from
the general phrase table as well as the two condi-
tional probabilities from the in-domain phrase ta-
ble. In addition, we take the union of the candidate
phrase pairs collected from both phrase tables A
detailed description of the union method can be
found in Mediani et al. (2012b).
The language model is adapted by log-linearly
combining the general language model and an in-
domain language model. We train a separate lan-
guage model using only the in-domain data. Then
it is used as an additional language model during
decoding. Optimal weights are set during tuning
by MERT.
</bodyText>
<subsectionHeader confidence="0.998677">
3.3 Special Language Models
</subsectionHeader>
<bodyText confidence="0.999941">
In addition to word-based language models, we
use different types of non-word language models
for each of the systems. With the help of a bilin-
gual language model (Niehues et al., 2011) we
are able to increase the bilingual context between
source and target words beyond phrase bound-
aries. This language model is trained on bilin-
gual tokens created from a target word and all its
aligned source words. The tokens are ordered ac-
cording to the target language word order.
Furthermore, we use language models based
on fine-grained part-of-speech tags (Schmid and
Laws, 2008) as well as word classes to allevi-
ate the sparsity problem for surface words. The
word classes are automatically learned by clus-
tering the words of the corpus using the MKCLS
algorithm (Och, 1999). These n-gram language
models are trained on the target language corpus,
</bodyText>
<page confidence="0.992127">
131
</page>
<bodyText confidence="0.9999961">
where the words have been replaced either by their
corresponding POS tag or cluster ID. During de-
coding, these language models are used as addi-
tional models in the log-linear combination.
The data selection language model is trained
on data automatically selected using cross-entropy
differences between development sets from pre-
vious WMT workshops and the noisy crawled
data (Moore and Lewis, 2010). We selected the
top 10M sentences to train this language model.
</bodyText>
<subsectionHeader confidence="0.986638">
3.4 Discriminative Word Lexicon
</subsectionHeader>
<bodyText confidence="0.99992825">
A discriminative word lexicon (DWL) models the
probability of a target word appearing in the trans-
lation given the words of the source sentence.
DWLs were first introduced by Mauser et al.
(2009). For every target word, they train a maxi-
mum entropy model to determine whether this tar-
get word should be in the translated sentence or
not using one feature per source word.
We use two simplifications of this model that
have shown beneficial to translation quality and
training time in the past (Mediani et al., 2011).
Firstly, we calculate the score for every phrase pair
before translating. Secondly, we restrict the nega-
tive training examples to words that occur within
matching phrase pairs.
In this evaluation, we extended the DWL
with n-gram source context features proposed
by Niehues and Waibel (2013). Instead of rep-
resenting the source sentence as a bag-of-words,
we model it as a bag-of-n-grams. This allows us
to include information about source word order in
the model. We used one feature per n-gram up to
the order of three and applied count filtering for
bigrams and trigrams.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999666">
This section presents the participating systems
used for the submissions in the four translation
directions of the evaluation. We describe the in-
dividual components that form part of each of
the systems and report the translation qualities
achieved during system development. The scores
are reported in case-sensitive BLEU (Papineni et
al., 2002).
</bodyText>
<subsectionHeader confidence="0.985193">
4.1 English-French
</subsectionHeader>
<bodyText confidence="0.99965425">
The development of our English→French system
is shown in Table 1.
It is noteworthy that, for this direction, we chose
to tune on a subset of 1,000 pairs from news-
test2012, due to the long time the whole set takes
to be decoded. In a preliminary set of experiments
(not reported here), we found no significant differ-
ences between tuning on the small or the big devel-
opment sets. The translation model of the baseline
system is trained on the whole parallel data after
filtering (EPPS, NC, Common Crawl, Giga). The
same data was also used for language modeling.
We also use POS-based reordering.
The biggest improvement was due to using two
additional language models. One consists of a log-
linear interpolation of individual language models
trained on the target side of the parallel data, the
News shuffle, Gigaword and NC corpora. In ad-
dition, an in-domain language model trained only
on NC data is used. This improves the score by
more than 1.4 points. Adaptation of the translation
model towards a smaller model trained on EPPS
and NC brings an additional 0.3 points.
Another 0.3 BLEU points could be gained by
using other special language models: a bilingual
language model together with a 4-gram cluster
language model (trained on all monolingual data
using the MKCLS tool and 500 clusters). Incor-
porating a lexicalized reordering model into the
system had a very noticeable effect on test namely
more than half a BLEU point.
Finally, using a discriminative word lexicon
with source context has a very small positive ef-
fect on the test score, however more than 0.3 on
dev. This final configuration was the basis of our
submitted official translation.
</bodyText>
<table confidence="0.999266142857143">
System Dev Test
Baseline 15.63 27.61
+ Big LMs 16.56 29.02
+ PT Adaptation 16.77 29.32
+ Bilingual + Cluster LM 16.87 29.64
+ Lexicalized Reordering 16.92 30.17
+ Source DWL 17.28 30.19
</table>
<tableCaption confidence="0.995013">
Table 1: Experiments for English→French
</tableCaption>
<subsectionHeader confidence="0.993303">
4.2 French-English
</subsectionHeader>
<bodyText confidence="0.999958">
Several experiments were conducted for the
French→English translation system. They are
summarized in Table 2.
The baseline system is essentially a phrase-
based translation system with some preprocess-
</bodyText>
<page confidence="0.991099">
132
</page>
<bodyText confidence="0.999701333333333">
ing steps on the source side and utilizing the
short-range POS-based reordering on all parallel
data and fine-grained monolingual corpora such as
EPPS and NC.
Adapting the translation model using a small in-
domain phrase table trained on NC data only helps
us gain more than 0.4 BLEU points.
Using non-word language models including a
bilingual language model and a 4-gram 50-cluster
language model trained on the whole parallel data
attains 0.24 BLEU points on the test set.
Lexicalized reordering improves our system on
the development set by 0.3 BLEU points but has
less effect on the test set with a minor improve-
ment of around 0.1 BLEU points.
We achieve our best system, which is used for
the evaluation, by adding a DWL with source con-
text yielding 31.54 BLEU points on the test set.
</bodyText>
<table confidence="0.998897571428571">
System Dev Test
Baseline 30.16 30.70
+ LM Adaptation 30.58 30.94
+ PT Adaptation 30.69 31.14
+ Bilingual + Cluster LM 30.85 31.38
+ Lexicalized Reordering 31.14 31.46
+ Source DWL 31.19 31.54
</table>
<tableCaption confidence="0.954513">
Table 2: Experiments for French→English
</tableCaption>
<subsectionHeader confidence="0.995843">
4.3 English-German
</subsectionHeader>
<bodyText confidence="0.999941071428572">
Table 3 presents how the English-German transla-
tion system is improved step by step.
In the baseline system, we used parallel data
which consists of the EPPS and NC corpora. The
phrase table is built using discriminative word
alignment. For word reordering, we use word lat-
tices with long range reordering rules. Five lan-
guage models are used in the baseline system; two
word-based language models, a bilingual language
model, and two 9-gram POS-based language mod-
els. The two word-based language models use 4-
gram context and are trained on the parallel data
and the filtered Common Crawl data separately,
while the bilingual language model is built only
on the Common Crawl corpus. The two POS-
based language models are also based on the paral-
lel data and the filtered crawled data, respectively.
When using a 9-gram cluster language model,
we get a slight improvement. The cluster is trained
with 1,000 classes using EPPS, NC, and Common
Crawl data.
We use the filtered crawled data in addition to
the parallel data in order to build the phrase table;
this gave us 1 BLEU point of improvement.
The system is improved by 0.1 BLEU points
when we use lattice phrase extraction along with
lexicalized reordering rules.
Tree-based reordering rules improved the sys-
tem performance further by another 0.1 BLEU
points.
By reducing the context of the two POS-based
language models from 9-grams to 5-grams and
shortening the context of the language model
trained on word classes to 4-grams, the score on
the development set hardly changes but we can see
a slightly improvement for the test case.
Finally, we use the DWL with source context
and build a big bilingual language model using
both the crawled and parallel data. By doing so,
we improved the translation performance by an-
other 0.3 BLEU points. This system was used for
the translation of the official test set.
</bodyText>
<table confidence="0.999026">
System Dev Test
Baseline 16.64 18.60
+ Cluster LM 16.76 18.66
+ Common Crawl Data 17.27 19.66
+ LPE + Lexicalized Reordering 17.45 19.75
+ Tree Rules 17.53 19.85
+ Shorter n-grams 17.55 19.92
+ Source DWL + Big BiLM 17.82 20.21
</table>
<tableCaption confidence="0.993656">
Table 3: Experiments for English→German
</tableCaption>
<subsectionHeader confidence="0.997078">
4.4 German-English
</subsectionHeader>
<bodyText confidence="0.9999214375">
Table 4 shows the development steps of the
German-English translation system.
For the baseline system, the training data of the
translation model consists of EPPS, NC and the
filtered parallel crawled data. The phrase table
is built using GIZA++ word alignment and lattice
phrase extraction. All language models are trained
with SRILM and scored in the decoding process
with KenLM (Heafield, 2011). We use word lat-
tices generated by short and long range reordering
rules as input to the decoder. In addition, a bilin-
gual language model and a target language model
trained on word clusters with 1,000 classes are in-
cluded in the system.
Enhancing the word reordering with tree-based
reordering rules and a lexicalized reordering
</bodyText>
<page confidence="0.998022">
133
</page>
<bodyText confidence="0.999630583333333">
model improved the system performance by 0.6
BLEU points.
Adding a language model trained on selected
data from the monolingual corpora gave another
small improvement.
The DWL with source context increased the
score on the test set by another 0.5 BLEU points
and applying morphological operations to un-
known words reduced the out-of-vocabulary rate,
even though no improvement in BLEU can be ob-
served. This system was used to generate the
translation submitted to the evaluation.
</bodyText>
<table confidence="0.998970428571428">
System Dev Test
Baseline 24.40 26.34
+ Tree Rules 24.71 26.86
+ Lexicalized Reordering 24.89 26.93
+ LM Data Selection 24.96 27.03
+ Source DWL 25.32 27.53
+ Morphological Operations - 27.53
</table>
<tableCaption confidence="0.984277">
Table 4: Experiments for German→English
</tableCaption>
<sectionHeader confidence="0.987443" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999963">
In this paper, we have described the systems
developed for our participation in the Shared
Translation Task of the WMT 2014 evaluation
for English↔German and English↔French. All
translations were generated using a phrase-based
translation system which was extended by addi-
tional models such as bilingual and fine-grained
part-of-speech language models. Discriminative
word lexica with source context proved beneficial
in all four language directions.
For English-French translation using a smaller
development set performed reasonably well and
reduced development time. The most noticeable
gain comes from log-linear interpolation of multi-
ple language models.
Due to the large amounts and diversity of
the data available for French-English, adapta-
tion methods and non-word language models con-
tribute the major improvements to the system.
For English-German translation, the crawled
data and a DWL using source context to guide
word choice brought most of the improvements.
Enhanced word reordering models, namely
tree-based reordering rules and a lexicalized re-
ordering model as well as the source-side fea-
tures for the discriminative word lexicon helped
improve the system performance for German-
English translation.
In average we achieved an improvement of over
1.5 BLEU over the respective baselines for all our
systems.
</bodyText>
<sectionHeader confidence="0.977628" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997695">
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n◦ 287658.
</bodyText>
<sectionHeader confidence="0.991616" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999588707317073">
George F. Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In Proceedings of the 2006 Con-
ference on Empirical Methods on Natural Language
Processing (EMNLP), Sydney, Australia.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
Edinburgh, Scotland, United Kingdom.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate Unlexicalized Parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics (ACL 2003), Sapporo, Japan.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings
of the Eleventh Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL 2003), Budapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the Second International Workshop
on Spoken Language Translation (IWSLT 2005),
Pittsburgh, PA, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL
2007), Demonstration Session, Prague, Czech Re-
public.
</reference>
<page confidence="0.994082">
134
</page>
<reference confidence="0.999254990291262">
Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models.
In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Suntec, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight International
Workshop on Spoken Language Translation (IWSLT
2011), San Francisco, CA, USA.
Mohammed Mediani, Jan Niehues, and Alex Waibel.
2012a. Parallel Phrase Scoring for Extra-large Cor-
pora. In The Prague Bulletin of Mathematical Lin-
guistics, number 98.
Mohammed Mediani, Yuqi Zhang, Thanh-Le Ha, Jan
Niehues, Eunach Cho, Teresa Herrmann, Rainer
K¨argel, and Alexander Waibel. 2012b. The KIT
Translation Systems for IWSLT 2012. In Proceed-
ings of the Ninth International Workshop on Spoken
Language Translation (IWSLT 2012), Hong Kong,
HK.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In Proceedings
of the ACL 2010 Conference Short Papers, Uppsala,
Sweden.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation (WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of the Third Workshop on Statistical
Machine Translation (WMT 2008), Columbus, OH,
USA.
Jan Niehues and Alex Waibel. 2011. Using Wikipedia
to Translate Domain-specific Terms in SMT. In
Proceedings of the Eight International Workshop on
Spoken Language Translation (IWSLT 2008), San
Francisco, CA, USA.
J. Niehues and A. Waibel. 2012. Detailed Analysis of
Different Strategies for Phrase Table Adaptation in
SMT. In Proceedings of the Tenth Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA 2012), San Diego, CA, USA.
J. Niehues and A. Waibel. 2013. An MT Error-Driven
Discriminative Word Lexicon using Sentence Struc-
ture Features. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, Sofia, Bul-
garia.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, Scotland, United King-
dom.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 1999. An Efficient Method for Deter-
mining Bilingual Word Classes. In Proceedings of
the Ninth Conference of the European Chapter of the
Association for Computational Linguistics (EACL
1999), Bergen, Norway.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, T. J. Watson Research Center.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German, Columbus, OH,
USA.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of the
11th International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI
2007), Sk¨ovde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In In-
ternational Conference on Computational Linguis-
tics (COLING 2008), Manchester, Great Britain.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, United Kingdom.
Andreas Stolcke. 2002. SRILM – An Extensible Lan-
guage Modeling Toolkit. In International Confer-
ence on Spoken Language Processing, Denver, Col-
orado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts, Ann Arbor, Michigan, USA.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
</reference>
<page confidence="0.998784">
135
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.371826">
<title confidence="0.9465085">The Karlsruhe Institute of Technology Translation for the WMT 2014</title>
<author confidence="0.9207885">Teresa Herrmann</author>
<author confidence="0.9207885">Mohammed Mediani</author>
<author confidence="0.9207885">Eunah Cho</author>
<author confidence="0.9207885">Thanh-Le_Jan Niehues</author>
<author confidence="0.9207885">Isabel Slawik</author>
<author confidence="0.9207885">Yuqi Zhang</author>
<author confidence="0.9207885">Alex</author>
<affiliation confidence="0.6631515">Institute for Anthropomatics and KIT - Karlsruhe Institute of</affiliation>
<email confidence="0.998363">firstname.lastname@kit.edu</email>
<abstract confidence="0.9993351">In this paper, we present the KIT systems participating in the Shared Translation Task translating between and All translations are generated using phrase-based translation systems, using different kinds of word-based, part-ofspeech-based and cluster-based language models trained on the provided data. Additional models include bilingual language models, reordering models based on part-of-speech tags and syntactic parse trees, as well as a lexicalized reordering model. In order to make use of noisy web-crawled data, we apply filtering and data selection methods for language modeling. A discriminative word lexicon using source context information proved beneficial for all translation directions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>George F Foster</author>
<author>Roland Kuhn</author>
<author>Howard Johnson</author>
</authors>
<title>Phrasetable Smoothing for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods on Natural Language Processing (EMNLP),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="4031" citStr="Foster et al. (2006)" startWordPosition="595" endWordPosition="598">cribed in Niehues and Vogel (2008). The phrase table (PT) is built using the Moses toolkit (Koehn et al., 2007). The phrase scoring for the small data sets (GermanHEnglish) is also 130 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 130–135, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics done by the Moses toolkit, whereas the bigger sets (French↔English) are scored by our in-house parallel phrase scorer (Mediani et al., 2012a). The phrase pair probabilities are computed using modified Kneser-Ney smoothing as described in Foster et al. (2006). Since German is a highly inflected language, we try to alleviate the out-of-vocabulary problem through quasi-morphological operations that change the lexical entry of a known word form to an unknown word form as described in Niehues and Waibel (2011). 3.1 Word Reordering Models We apply automatically learned reordering rules based on part-of-speech (POS) sequences and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word align</context>
</contexts>
<marker>Foster, Kuhn, Johnson, 2006</marker>
<rawString>George F. Foster, Roland Kuhn, and Howard Johnson. 2006. Phrasetable Smoothing for Statistical Machine Translation. In Proceedings of the 2006 Conference on Empirical Methods on Natural Language Processing (EMNLP), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and Smaller Language Model Queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<location>Edinburgh, Scotland, United Kingdom.</location>
<contexts>
<context position="15929" citStr="Heafield, 2011" startWordPosition="2557" endWordPosition="2558">n Crawl Data 17.27 19.66 + LPE + Lexicalized Reordering 17.45 19.75 + Tree Rules 17.53 19.85 + Shorter n-grams 17.55 19.92 + Source DWL + Big BiLM 17.82 20.21 Table 3: Experiments for English→German 4.4 German-English Table 4 shows the development steps of the German-English translation system. For the baseline system, the training data of the translation model consists of EPPS, NC and the filtered parallel crawled data. The phrase table is built using GIZA++ word alignment and lattice phrase extraction. All language models are trained with SRILM and scored in the decoding process with KenLM (Heafield, 2011). We use word lattices generated by short and long range reordering rules as input to the decoder. In addition, a bilingual language model and a target language model trained on word clusters with 1,000 classes are included in the system. Enhancing the word reordering with tree-based reordering rules and a lexicalized reordering 133 model improved the system performance by 0.6 BLEU points. Adding a language model trained on selected data from the monolingual corpora gave another small improvement. The DWL with source context increased the score on the test set by another 0.5 BLEU points and ap</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and Smaller Language Model Queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, Edinburgh, Scotland, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teresa Herrmann</author>
<author>Jan Niehues</author>
<author>Alex Waibel</author>
</authors>
<title>Combining Word Reordering Methods on different Linguistic Abstraction Levels for Statistical Machine Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<location>Altanta, Georgia, USA.</location>
<contexts>
<context position="4841" citStr="Herrmann et al., 2013" startWordPosition="723" endWordPosition="726">o an unknown word form as described in Niehues and Waibel (2011). 3.1 Word Reordering Models We apply automatically learned reordering rules based on part-of-speech (POS) sequences and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word order between German and English. Here, a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) for the source side of the training corpus are required to learn rules on how to reorder the constituents in the source sentence. The POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a reordering lattice. The lattice, which also includes the original position of each word, is used </context>
</contexts>
<marker>Herrmann, Niehues, Waibel, 2013</marker>
<rawString>Teresa Herrmann, Jan Niehues, and Alex Waibel. 2013. Combining Word Reordering Methods on different Linguistic Abstraction Levels for Statistical Machine Translation. In Proceedings of the Seventh Workshop on Syntax, Semantics and Structure in Statistical Translation, Altanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="5020" citStr="Klein and Manning, 2003" startWordPosition="751" endWordPosition="754">ces and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word order between German and English. Here, a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) for the source side of the training corpus are required to learn rules on how to reorder the constituents in the source sentence. The POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a reordering lattice. The lattice, which also includes the original position of each word, is used as input to the decoder. In order to acquire phrase pairs matching the reordered sentence variants, we perform lattice phrase extraction (LPE) on the training corpus where phrase </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL 2003), Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical Methods for Compound Splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics (EACL</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="2495" citStr="Koehn and Knight, 2003" startWordPosition="356" endWordPosition="359">lly the Gigaword corpus for EnglishHFrench and GermanHEnglish are used as monolingual training data for the different language models. For optimizing the system parameters, newstest2012 and newstest2013 are used as development and test data respectively. 3 System Description Before training we perform a common preprocessing of the raw data, which includes removing long sentences and sentences with a length mismatch exceeding a certain threshold. Afterwards, we normalize special symbols, dates, and numbers. Then we perform smart-casing of the first letter of every sentence. Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for GermanHEnglish translation. In order to improve the quality of the web-crawled Common Crawl corpus, we filter out noisy sentence pairs using an SVM classifier for all four translation tasks as described in Mediani et al. (2011). Unless stated otherwise, we use 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (200</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philipp Koehn and Kevin Knight. 2003. Empirical Methods for Compound Splitting. In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics (EACL 2003), Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra B Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second International Workshop on Spoken Language Translation (IWSLT 2005),</booktitle>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="5773" citStr="Koehn et al., 2005" startWordPosition="872" endWordPosition="875"> POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a reordering lattice. The lattice, which also includes the original position of each word, is used as input to the decoder. In order to acquire phrase pairs matching the reordered sentence variants, we perform lattice phrase extraction (LPE) on the training corpus where phrase are extracted from the reordered word lattices instead of the original sentences. In addition, we use a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair. During decoding the lexicalized reordering model determines the reordering orientation of each phrase pair at the phrase boundaries. The probability for the respective orientation with respect to the original position of the words is included as an additional score in the log-linear model of the translation system. 3.2 Adaptation In the French→English and English→French systems, we perform adaptation for translation models as well as for language models. The EPPS and NC corpora are used as in-domain data for the direction English→Fre</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of the Second International Workshop on Spoken Language Translation (IWSLT 2005), Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007), Demonstration Session,</booktitle>
<location>Alexandra</location>
<contexts>
<context position="3522" citStr="Koehn et al., 2007" startWordPosition="520" endWordPosition="523">Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) for optimization. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For EnglishHGerman, we use discriminative word alignment trained on hand-aligned data as described in Niehues and Vogel (2008). The phrase table (PT) is built using the Moses toolkit (Koehn et al., 2007). The phrase scoring for the small data sets (GermanHEnglish) is also 130 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 130–135, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics done by the Moses toolkit, whereas the bigger sets (French↔English) are scored by our in-house parallel phrase scorer (Mediani et al., 2012a). The phrase pair probabilities are computed using modified Kneser-Ney smoothing as described in Foster et al. (2006). Since German is a highly inflected language, we try to alleviate the out-of-vocabulary pr</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007), Demonstration Session, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>Saˇsa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Extending Statistical Machine Translation with Discriminative and Trigger-based Lexicon Models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), Suntec,</booktitle>
<contexts>
<context position="8965" citStr="Mauser et al. (2009)" startWordPosition="1399" endWordPosition="1402"> or cluster ID. During decoding, these language models are used as additional models in the log-linear combination. The data selection language model is trained on data automatically selected using cross-entropy differences between development sets from previous WMT workshops and the noisy crawled data (Moore and Lewis, 2010). We selected the top 10M sentences to train this language model. 3.4 Discriminative Word Lexicon A discriminative word lexicon (DWL) models the probability of a target word appearing in the translation given the words of the source sentence. DWLs were first introduced by Mauser et al. (2009). For every target word, they train a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per source word. We use two simplifications of this model that have shown beneficial to translation quality and training time in the past (Mediani et al., 2011). Firstly, we calculate the score for every phrase pair before translating. Secondly, we restrict the negative training examples to words that occur within matching phrase pairs. In this evaluation, we extended the DWL with n-gram source context features proposed by Niehues and W</context>
</contexts>
<marker>Mauser, Hasan, Ney, 2009</marker>
<rawString>Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2009. Extending Statistical Machine Translation with Discriminative and Trigger-based Lexicon Models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Mediani</author>
<author>Eunah Cho</author>
<author>Jan Niehues</author>
<author>Teresa Herrmann</author>
<author>Alex Waibel</author>
</authors>
<title>The KIT English-French Translation systems for IWSLT</title>
<date>2011</date>
<booktitle>In Proceedings of the Eight International Workshop on Spoken Language Translation (IWSLT 2011),</booktitle>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="2773" citStr="Mediani et al. (2011)" startWordPosition="404" endWordPosition="407">re training we perform a common preprocessing of the raw data, which includes removing long sentences and sentences with a length mismatch exceeding a certain threshold. Afterwards, we normalize special symbols, dates, and numbers. Then we perform smart-casing of the first letter of every sentence. Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for GermanHEnglish translation. In order to improve the quality of the web-crawled Common Crawl corpus, we filter out noisy sentence pairs using an SVM classifier for all four translation tasks as described in Mediani et al. (2011). Unless stated otherwise, we use 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) for optimization. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For EnglishHGerman, we use discriminative word alignme</context>
<context position="9285" citStr="Mediani et al., 2011" startWordPosition="1455" endWordPosition="1458">s, 2010). We selected the top 10M sentences to train this language model. 3.4 Discriminative Word Lexicon A discriminative word lexicon (DWL) models the probability of a target word appearing in the translation given the words of the source sentence. DWLs were first introduced by Mauser et al. (2009). For every target word, they train a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per source word. We use two simplifications of this model that have shown beneficial to translation quality and training time in the past (Mediani et al., 2011). Firstly, we calculate the score for every phrase pair before translating. Secondly, we restrict the negative training examples to words that occur within matching phrase pairs. In this evaluation, we extended the DWL with n-gram source context features proposed by Niehues and Waibel (2013). Instead of representing the source sentence as a bag-of-words, we model it as a bag-of-n-grams. This allows us to include information about source word order in the model. We used one feature per n-gram up to the order of three and applied count filtering for bigrams and trigrams. 4 Results This section p</context>
</contexts>
<marker>Mediani, Cho, Niehues, Herrmann, Waibel, 2011</marker>
<rawString>Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa Herrmann, and Alex Waibel. 2011. The KIT English-French Translation systems for IWSLT 2011. In Proceedings of the Eight International Workshop on Spoken Language Translation (IWSLT 2011), San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Mediani</author>
<author>Jan Niehues</author>
<author>Alex Waibel</author>
</authors>
<title>Parallel Phrase Scoring for Extra-large Corpora.</title>
<date>2012</date>
<booktitle>In The Prague Bulletin of Mathematical Linguistics, number 98.</booktitle>
<contexts>
<context position="3912" citStr="Mediani et al., 2012" startWordPosition="577" endWordPosition="580">-diag-final-and heuristic. For EnglishHGerman, we use discriminative word alignment trained on hand-aligned data as described in Niehues and Vogel (2008). The phrase table (PT) is built using the Moses toolkit (Koehn et al., 2007). The phrase scoring for the small data sets (GermanHEnglish) is also 130 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 130–135, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics done by the Moses toolkit, whereas the bigger sets (French↔English) are scored by our in-house parallel phrase scorer (Mediani et al., 2012a). The phrase pair probabilities are computed using modified Kneser-Ney smoothing as described in Foster et al. (2006). Since German is a highly inflected language, we try to alleviate the out-of-vocabulary problem through quasi-morphological operations that change the lexical entry of a known word form to an unknown word form as described in Niehues and Waibel (2011). 3.1 Word Reordering Models We apply automatically learned reordering rules based on part-of-speech (POS) sequences and syntactic parse tree constituents to perform source sentence reordering according to the target language wor</context>
<context position="7103" citStr="Mediani et al. (2012" startWordPosition="1095" endWordPosition="1098">omain phrase table, which is trained on all corpora; the other is the in-domain phrase table, which is trained on in-domain data. We adapt the translation model by using the scores from the two phrase tables with the backoff approach described in Niehues and Waibel (2012). This results in a phrase table with six scores, the four scores from the general phrase table as well as the two conditional probabilities from the in-domain phrase table. In addition, we take the union of the candidate phrase pairs collected from both phrase tables A detailed description of the union method can be found in Mediani et al. (2012b). The language model is adapted by log-linearly combining the general language model and an indomain language model. We train a separate language model using only the in-domain data. Then it is used as an additional language model during decoding. Optimal weights are set during tuning by MERT. 3.3 Special Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. With the help of a bilingual language model (Niehues et al., 2011) we are able to increase the bilingual context between source and target words beyond phra</context>
</contexts>
<marker>Mediani, Niehues, Waibel, 2012</marker>
<rawString>Mohammed Mediani, Jan Niehues, and Alex Waibel. 2012a. Parallel Phrase Scoring for Extra-large Corpora. In The Prague Bulletin of Mathematical Linguistics, number 98.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mohammed Mediani</author>
<author>Yuqi Zhang</author>
<author>Jan Niehues Thanh-Le Ha</author>
<author>Eunach Cho</author>
<author>Teresa Herrmann</author>
<author>Rainer K¨argel</author>
<author>Alexander Waibel</author>
</authors>
<booktitle>2012b. The KIT Translation Systems for IWSLT 2012. In Proceedings of the Ninth International Workshop on Spoken Language Translation (IWSLT 2012),</booktitle>
<location>Hong Kong, HK.</location>
<marker>Mediani, Zhang, Thanh-Le Ha, Cho, Herrmann, K¨argel, Waibel, </marker>
<rawString>Mohammed Mediani, Yuqi Zhang, Thanh-Le Ha, Jan Niehues, Eunach Cho, Teresa Herrmann, Rainer K¨argel, and Alexander Waibel. 2012b. The KIT Translation Systems for IWSLT 2012. In Proceedings of the Ninth International Workshop on Spoken Language Translation (IWSLT 2012), Hong Kong, HK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>W Lewis</author>
</authors>
<title>Intelligent Selection of Language Model Training Data.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="8672" citStr="Moore and Lewis, 2010" startWordPosition="1351" endWordPosition="1354">problem for surface words. The word classes are automatically learned by clustering the words of the corpus using the MKCLS algorithm (Och, 1999). These n-gram language models are trained on the target language corpus, 131 where the words have been replaced either by their corresponding POS tag or cluster ID. During decoding, these language models are used as additional models in the log-linear combination. The data selection language model is trained on data automatically selected using cross-entropy differences between development sets from previous WMT workshops and the noisy crawled data (Moore and Lewis, 2010). We selected the top 10M sentences to train this language model. 3.4 Discriminative Word Lexicon A discriminative word lexicon (DWL) models the probability of a target word appearing in the translation given the words of the source sentence. DWLs were first introduced by Mauser et al. (2009). For every target word, they train a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per source word. We use two simplifications of this model that have shown beneficial to translation quality and training time in the past (Mediani </context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>R.C. Moore and W. Lewis. 2010. Intelligent Selection of Language Model Training Data. In Proceedings of the ACL 2010 Conference Short Papers, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Muntsin Kolss</author>
</authors>
<title>A POS-Based Model for Long-Range Reorderings in SMT.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation (WMT</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="4764" citStr="Niehues and Kolss, 2009" startWordPosition="711" endWordPosition="714">i-morphological operations that change the lexical entry of a known word form to an unknown word form as described in Niehues and Waibel (2011). 3.1 Word Reordering Models We apply automatically learned reordering rules based on part-of-speech (POS) sequences and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word order between German and English. Here, a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) for the source side of the training corpus are required to learn rules on how to reorder the constituents in the source sentence. The POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a reordering lattice. </context>
</contexts>
<marker>Niehues, Kolss, 2009</marker>
<rawString>Jan Niehues and Muntsin Kolss. 2009. A POS-Based Model for Long-Range Reorderings in SMT. In Proceedings of the Fourth Workshop on Statistical Machine Translation (WMT 2009), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Stephan Vogel</author>
</authors>
<title>Discriminative Word Alignment via Alignment Matrix Modeling.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation (WMT 2008),</booktitle>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="3445" citStr="Niehues and Vogel (2008)" startWordPosition="506" endWordPosition="509">e models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) for optimization. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For EnglishHGerman, we use discriminative word alignment trained on hand-aligned data as described in Niehues and Vogel (2008). The phrase table (PT) is built using the Moses toolkit (Koehn et al., 2007). The phrase scoring for the small data sets (GermanHEnglish) is also 130 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 130–135, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics done by the Moses toolkit, whereas the bigger sets (French↔English) are scored by our in-house parallel phrase scorer (Mediani et al., 2012a). The phrase pair probabilities are computed using modified Kneser-Ney smoothing as described in Foster et al. (2006). Since German</context>
</contexts>
<marker>Niehues, Vogel, 2008</marker>
<rawString>Jan Niehues and Stephan Vogel. 2008. Discriminative Word Alignment via Alignment Matrix Modeling. In Proceedings of the Third Workshop on Statistical Machine Translation (WMT 2008), Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Alex Waibel</author>
</authors>
<title>Using Wikipedia to Translate Domain-specific Terms in SMT.</title>
<date>2011</date>
<booktitle>In Proceedings of the Eight International Workshop on Spoken Language Translation (IWSLT 2008),</booktitle>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="4283" citStr="Niehues and Waibel (2011)" startWordPosition="635" endWordPosition="638">lation, pages 130–135, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics done by the Moses toolkit, whereas the bigger sets (French↔English) are scored by our in-house parallel phrase scorer (Mediani et al., 2012a). The phrase pair probabilities are computed using modified Kneser-Ney smoothing as described in Foster et al. (2006). Since German is a highly inflected language, we try to alleviate the out-of-vocabulary problem through quasi-morphological operations that change the lexical entry of a known word form to an unknown word form as described in Niehues and Waibel (2011). 3.1 Word Reordering Models We apply automatically learned reordering rules based on part-of-speech (POS) sequences and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word</context>
</contexts>
<marker>Niehues, Waibel, 2011</marker>
<rawString>Jan Niehues and Alex Waibel. 2011. Using Wikipedia to Translate Domain-specific Terms in SMT. In Proceedings of the Eight International Workshop on Spoken Language Translation (IWSLT 2008), San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Niehues</author>
<author>A Waibel</author>
</authors>
<title>Detailed Analysis of Different Strategies for Phrase Table Adaptation in SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of the Tenth Conference of the Association for Machine Translation in the Americas (AMTA 2012),</booktitle>
<location>San Diego, CA, USA.</location>
<contexts>
<context position="6755" citStr="Niehues and Waibel (2012)" startWordPosition="1033" endWordPosition="1036"> translation system. 3.2 Adaptation In the French→English and English→French systems, we perform adaptation for translation models as well as for language models. The EPPS and NC corpora are used as in-domain data for the direction English→French, while NC corpus is the in-domain data for French→English. Two phrase tables are built: one is the outof-domain phrase table, which is trained on all corpora; the other is the in-domain phrase table, which is trained on in-domain data. We adapt the translation model by using the scores from the two phrase tables with the backoff approach described in Niehues and Waibel (2012). This results in a phrase table with six scores, the four scores from the general phrase table as well as the two conditional probabilities from the in-domain phrase table. In addition, we take the union of the candidate phrase pairs collected from both phrase tables A detailed description of the union method can be found in Mediani et al. (2012b). The language model is adapted by log-linearly combining the general language model and an indomain language model. We train a separate language model using only the in-domain data. Then it is used as an additional language model during decoding. Op</context>
</contexts>
<marker>Niehues, Waibel, 2012</marker>
<rawString>J. Niehues and A. Waibel. 2012. Detailed Analysis of Different Strategies for Phrase Table Adaptation in SMT. In Proceedings of the Tenth Conference of the Association for Machine Translation in the Americas (AMTA 2012), San Diego, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Niehues</author>
<author>A Waibel</author>
</authors>
<title>An MT Error-Driven Discriminative Word Lexicon using Sentence Structure Features.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="9577" citStr="Niehues and Waibel (2013)" startWordPosition="1500" endWordPosition="1503">et al. (2009). For every target word, they train a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per source word. We use two simplifications of this model that have shown beneficial to translation quality and training time in the past (Mediani et al., 2011). Firstly, we calculate the score for every phrase pair before translating. Secondly, we restrict the negative training examples to words that occur within matching phrase pairs. In this evaluation, we extended the DWL with n-gram source context features proposed by Niehues and Waibel (2013). Instead of representing the source sentence as a bag-of-words, we model it as a bag-of-n-grams. This allows us to include information about source word order in the model. We used one feature per n-gram up to the order of three and applied count filtering for bigrams and trigrams. 4 Results This section presents the participating systems used for the submissions in the four translation directions of the evaluation. We describe the individual components that form part of each of the systems and report the translation qualities achieved during system development. The scores are reported in cas</context>
</contexts>
<marker>Niehues, Waibel, 2013</marker>
<rawString>J. Niehues and A. Waibel. 2013. An MT Error-Driven Discriminative Word Lexicon using Sentence Structure Features. In Proceedings of the Eighth Workshop on Statistical Machine Translation, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Teresa Herrmann</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Wider Context by Using Bilingual Language Models in Machine Translation.</title>
<date>2011</date>
<booktitle>In Sixth Workshop on Statistical Machine Translation (WMT 2011),</booktitle>
<location>Edinburgh, Scotland, United Kingdom.</location>
<contexts>
<context position="7613" citStr="Niehues et al., 2011" startWordPosition="1180" endWordPosition="1183">llected from both phrase tables A detailed description of the union method can be found in Mediani et al. (2012b). The language model is adapted by log-linearly combining the general language model and an indomain language model. We train a separate language model using only the in-domain data. Then it is used as an additional language model during decoding. Optimal weights are set during tuning by MERT. 3.3 Special Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. With the help of a bilingual language model (Niehues et al., 2011) we are able to increase the bilingual context between source and target words beyond phrase boundaries. This language model is trained on bilingual tokens created from a target word and all its aligned source words. The tokens are ordered according to the target language word order. Furthermore, we use language models based on fine-grained part-of-speech tags (Schmid and Laws, 2008) as well as word classes to alleviate the sparsity problem for surface words. The word classes are automatically learned by clustering the words of the corpus using the MKCLS algorithm (Och, 1999). These n-gram lan</context>
</contexts>
<marker>Niehues, Herrmann, Vogel, Waibel, 2011</marker>
<rawString>Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex Waibel. 2011. Wider Context by Using Bilingual Language Models in Machine Translation. In Sixth Workshop on Statistical Machine Translation (WMT 2011), Edinburgh, Scotland, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="3216" citStr="Och and Ney, 2003" startWordPosition="474" endWordPosition="477">quality of the web-crawled Common Crawl corpus, we filter out noisy sentence pairs using an SVM classifier for all four translation tasks as described in Mediani et al. (2011). Unless stated otherwise, we use 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) for optimization. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For EnglishHGerman, we use discriminative word alignment trained on hand-aligned data as described in Niehues and Vogel (2008). The phrase table (PT) is built using the Moses toolkit (Koehn et al., 2007). The phrase scoring for the small data sets (GermanHEnglish) is also 130 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 130–135, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics done by the Moses toolkit, whereas the bigg</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>An Efficient Method for Determining Bilingual Word Classes.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics (EACL</booktitle>
<location>Bergen, Norway.</location>
<contexts>
<context position="8195" citStr="Och, 1999" startWordPosition="1279" endWordPosition="1280">model (Niehues et al., 2011) we are able to increase the bilingual context between source and target words beyond phrase boundaries. This language model is trained on bilingual tokens created from a target word and all its aligned source words. The tokens are ordered according to the target language word order. Furthermore, we use language models based on fine-grained part-of-speech tags (Schmid and Laws, 2008) as well as word classes to alleviate the sparsity problem for surface words. The word classes are automatically learned by clustering the words of the corpus using the MKCLS algorithm (Och, 1999). These n-gram language models are trained on the target language corpus, 131 where the words have been replaced either by their corresponding POS tag or cluster ID. During decoding, these language models are used as additional models in the log-linear combination. The data selection language model is trained on data automatically selected using cross-entropy differences between development sets from previous WMT workshops and the noisy crawled data (Moore and Lewis, 2010). We selected the top 10M sentences to train this language model. 3.4 Discriminative Word Lexicon A discriminative word lex</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>Franz Josef Och. 1999. An Efficient Method for Determining Bilingual Word Classes. In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics (EACL 1999), Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<journal>IBM Research Division, T. J. Watson Research Center.</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<contexts>
<context position="10217" citStr="Papineni et al., 2002" startWordPosition="1603" endWordPosition="1606">senting the source sentence as a bag-of-words, we model it as a bag-of-n-grams. This allows us to include information about source word order in the model. We used one feature per n-gram up to the order of three and applied count filtering for bigrams and trigrams. 4 Results This section presents the participating systems used for the submissions in the four translation directions of the evaluation. We describe the individual components that form part of each of the systems and report the translation qualities achieved during system development. The scores are reported in case-sensitive BLEU (Papineni et al., 2002). 4.1 English-French The development of our English→French system is shown in Table 1. It is noteworthy that, for this direction, we chose to tune on a subset of 1,000 pairs from newstest2012, due to the long time the whole set takes to be decoded. In a preliminary set of experiments (not reported here), we found no significant differences between tuning on the small or the big development sets. The translation model of the baseline system is trained on the whole parallel data after filtering (EPPS, NC, Common Crawl, Giga). The same data was also used for language modeling. We also use POS-bas</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. Technical Report RC22176 (W0109-022), IBM Research Division, T. J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna N Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Parsing German,</booktitle>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="4994" citStr="Rafferty and Manning, 2008" startWordPosition="747" endWordPosition="750"> part-of-speech (POS) sequences and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word order between German and English. Here, a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) for the source side of the training corpus are required to learn rules on how to reorder the constituents in the source sentence. The POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a reordering lattice. The lattice, which also includes the original position of each word, is used as input to the decoder. In order to acquire phrase pairs matching the reordered sentence variants, we perform lattice phrase extraction (LPE) on the tra</context>
</contexts>
<marker>Rafferty, Manning, 2008</marker>
<rawString>Anna N. Rafferty and Christopher D. Manning. 2008. Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines. In Proceedings of the Workshop on Parsing German, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kay Rottmann</author>
<author>Stephan Vogel</author>
</authors>
<title>Word Reordering in Statistical Machine Translation with a POS-Based Distortion Model.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI</booktitle>
<location>Sk¨ovde, Sweden.</location>
<contexts>
<context position="4711" citStr="Rottmann and Vogel, 2007" startWordPosition="703" endWordPosition="706">o alleviate the out-of-vocabulary problem through quasi-morphological operations that change the lexical entry of a known word form to an unknown word form as described in Niehues and Waibel (2011). 3.1 Word Reordering Models We apply automatically learned reordering rules based on part-of-speech (POS) sequences and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word order between German and English. Here, a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) for the source side of the training corpus are required to learn rules on how to reorder the constituents in the source sentence. The POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the or</context>
</contexts>
<marker>Rottmann, Vogel, 2007</marker>
<rawString>Kay Rottmann and Stephan Vogel. 2007. Word Reordering in Statistical Machine Translation with a POS-Based Distortion Model. In Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI 2007), Sk¨ovde, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
<author>Florian Laws</author>
</authors>
<title>Estimation of Conditional Probabilities with Decision Trees and an Application to Fine-Grained POS Tagging.</title>
<date>2008</date>
<booktitle>In International Conference on Computational Linguistics (COLING 2008),</booktitle>
<location>Manchester, Great Britain.</location>
<contexts>
<context position="7999" citStr="Schmid and Laws, 2008" startWordPosition="1243" endWordPosition="1246">during tuning by MERT. 3.3 Special Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. With the help of a bilingual language model (Niehues et al., 2011) we are able to increase the bilingual context between source and target words beyond phrase boundaries. This language model is trained on bilingual tokens created from a target word and all its aligned source words. The tokens are ordered according to the target language word order. Furthermore, we use language models based on fine-grained part-of-speech tags (Schmid and Laws, 2008) as well as word classes to alleviate the sparsity problem for surface words. The word classes are automatically learned by clustering the words of the corpus using the MKCLS algorithm (Och, 1999). These n-gram language models are trained on the target language corpus, 131 where the words have been replaced either by their corresponding POS tag or cluster ID. During decoding, these language models are used as additional models in the log-linear combination. The data selection language model is trained on data automatically selected using cross-entropy differences between development sets from </context>
</contexts>
<marker>Schmid, Laws, 2008</marker>
<rawString>Helmut Schmid and Florian Laws. 2008. Estimation of Conditional Probabilities with Decision Trees and an Application to Fine-Grained POS Tagging. In International Conference on Computational Linguistics (COLING 2008), Manchester, Great Britain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, United Kingdom.</location>
<contexts>
<context position="4594" citStr="Schmid, 1994" startWordPosition="684" endWordPosition="685">Ney smoothing as described in Foster et al. (2006). Since German is a highly inflected language, we try to alleviate the out-of-vocabulary problem through quasi-morphological operations that change the lexical entry of a known word form to an unknown word form as described in Niehues and Waibel (2011). 3.1 Word Reordering Models We apply automatically learned reordering rules based on part-of-speech (POS) sequences and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word order between German and English. Here, a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) for the source side of the training corpus are required to learn rules on how to reorder the constituents in the source sentence. The POS-based and tree-based reordering rul</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference on New Methods in Language Processing, Manchester, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In International Conference on Spoken Language Processing,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="2917" citStr="Stolcke, 2002" startWordPosition="427" endWordPosition="429">certain threshold. Afterwards, we normalize special symbols, dates, and numbers. Then we perform smart-casing of the first letter of every sentence. Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for GermanHEnglish translation. In order to improve the quality of the web-crawled Common Crawl corpus, we filter out noisy sentence pairs using an SVM classifier for all four translation tasks as described in Mediani et al. (2011). Unless stated otherwise, we use 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) for optimization. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For EnglishHGerman, we use discriminative word alignment trained on hand-aligned data as described in Niehues and Vogel (2008). The phrase table (PT) is built using the Moses toolkit (Koehn et al., </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In International Conference on Spoken Language Processing, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollman</author>
<author>Alex Waibel</author>
</authors>
<title>Training and Evaluation Error Minimization Rules for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="3097" citStr="Venugopal et al. (2005)" startWordPosition="454" endWordPosition="457">ehn and Knight, 2003) is performed on the source side of the corpus for GermanHEnglish translation. In order to improve the quality of the web-crawled Common Crawl corpus, we filter out noisy sentence pairs using an SVM classifier for all four translation tasks as described in Mediani et al. (2011). Unless stated otherwise, we use 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) for optimization. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For EnglishHGerman, we use discriminative word alignment trained on hand-aligned data as described in Niehues and Vogel (2008). The phrase table (PT) is built using the Moses toolkit (Koehn et al., 2007). The phrase scoring for the small data sets (GermanHEnglish) is also 130 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 130–135, Baltimore, Maryl</context>
</contexts>
<marker>Venugopal, Zollman, Waibel, 2005</marker>
<rawString>Ashish Venugopal, Andreas Zollman, and Alex Waibel. 2005. Training and Evaluation Error Minimization Rules for Statistical Machine Translation. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
</authors>
<title>SMT Decoder Dissected: Word Reordering.</title>
<date>2003</date>
<booktitle>In International Conference on Natural Language Processing and Knowledge Engineering,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="3010" citStr="Vogel, 2003" startWordPosition="440" endWordPosition="442">m smart-casing of the first letter of every sentence. Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for GermanHEnglish translation. In order to improve the quality of the web-crawled Common Crawl corpus, we filter out noisy sentence pairs using an SVM classifier for all four translation tasks as described in Mediani et al. (2011). Unless stated otherwise, we use 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) for optimization. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For EnglishHGerman, we use discriminative word alignment trained on hand-aligned data as described in Niehues and Vogel (2008). The phrase table (PT) is built using the Moses toolkit (Koehn et al., 2007). The phrase scoring for the small data sets (GermanHEnglish) is also 130 Proceedings of</context>
</contexts>
<marker>Vogel, 2003</marker>
<rawString>Stephan Vogel. 2003. SMT Decoder Dissected: Word Reordering. In International Conference on Natural Language Processing and Knowledge Engineering, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
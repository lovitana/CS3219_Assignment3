<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000414">
<title confidence="0.997285">
Modeling Blame to Avoid Positive Face Threats in Natural Language
Generation
</title>
<author confidence="0.986564">
Gordon Briggs
</author>
<affiliation confidence="0.977529">
Human-Robot Interaction Laboratory
Tufts University
</affiliation>
<address confidence="0.900323">
Medford, MA USA
</address>
<email confidence="0.999435">
gbriggs@cs.tufts.edu
</email>
<sectionHeader confidence="0.997396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953692307692">
Prior approaches to politeness modulation
in natural language generation (NLG) of-
ten focus on manipulating factors such as
the directness of requests that pertain to
preserving the autonomy of the addressee
(negative face threats), but do not have a
systematic way of understanding potential
impoliteness from inadvertently critical or
blame-oriented communications (positive
face threats). In this paper, we discuss on-
going work to integrate a computational
model of blame to prevent inappropriate
threats to positive face.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99946336">
When communicating with one another, people
often modulate their language based on a variety
of social factors. Enabling natural and human-
like interactions with virtual and robotic agents
may require engineering these agents to be able
to demonstrate appropriate social behaviors. For
instance, increasing attention is being paid to the
effects of utilizing politeness strategies in both
human-computer and human-robot dialogue inter-
actions (Cassell and Bickmore, 2003; Torrey et
al., 2013; Strait et al., 2014). This work has
shown that, depending on context, the deployment
of politeness strategies by artificial agents can in-
crease human interactants’ positive assessments of
an agent along multiple dimensions (e.g. likeabil-
ity).
However, while these studies investigated the
human factors aspects of utilizing politeness
strategies, they were not concerned with the nat-
ural language generation (NLG) mechanisms nec-
essary to appropriately realize and deploy these
strategies. Instead, there is a small, but grow-
ing, body of work on natural language genera-
tion architectures that seek to address this chal-
lenge (Gupta et al., 2007; Miller et al., 2008;
</bodyText>
<note confidence="0.27358725">
Matthias Scheutz
Human-Robot Interaction Laboratory
Tufts University
Medford, MA USA
</note>
<email confidence="0.987351">
mscheutz@cs.tufts.edu
</email>
<bodyText confidence="0.99970115625">
Briggs and Scheutz, 2013). The common ap-
proach taken by these architectures is the opera-
tionalization of key factors in Brown and Levin-
son’s seminal work on politeness theory, in partic-
ular, the degree to which an utterance can be con-
sidered a face-threatening act (FTA) (Brown and
Levinson, 1987).
While this prior work demonstrates the abilities
of these NLG architectures to successfully pro-
duce polite language, there remain some key chal-
lenges. Perhaps the most crucial question is: how
does one calculate the degree to which an utter-
ance is a FTA1? This is a complex issue, as not
only is this value modulated by factors such as so-
cial distance, power, and context, but also the mul-
tifaceted nature of “face.” An utterance may be
polite in relation to negative face (i.e. the agent’s
autonomy), but may be quite impolite with regard
to positive face (i.e. the agent’s image and per-
ceived character).
In this paper, we investigate the problem of
modeling threats to positive face. First we discuss
how prior work that has focused primarily on miti-
gating threats to negative face, and examine a spe-
cific example, taken from the human subject data
of (Gupta et al., 2007), to show why accounting
for positive face is necessary. Next, we discuss
our proposed solution to begin to model threats to
positive face– specifically, integrating a computa-
tional model of blame. Finally, we discuss the jus-
tification behind and limitations of this proposed
approach.
</bodyText>
<sectionHeader confidence="0.98813" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.9996266">
Brown and Levinson (1987) articulated a tax-
onomy of politeness strategies, distinguishing
broadly between the notion of positive and neg-
ative politeness (with many distinct strategies for
each). These categories of politeness correspond
</bodyText>
<footnote confidence="0.9991105">
1Less crucially, what is the appropriate notation for this
value? It is denoted differently in each paper: ⇥, W, and ⌘.
</footnote>
<page confidence="0.545306">
1
</page>
<bodyText confidence="0.949366555555555">
Proceedings of the INLG and SIGDIAL 2014 Joint Session, pages 1–5,
Philadelphia, Pennsylvania, 19 June 2014. c�2014 Association for Computational Linguistics
to the concepts of positive and negative face, re-
spectively. An example of a positive politeness
strategy is the use of praise (“Great!”), whereas
a common negative politeness strategy is the use
of an indirect speech act (ISA), in particular, an
indirect request. An example of an indirect re-
quest is the question, “Could you get me a cof-
fee?”, which avoids the autonomy-threatening di-
rect imperative, while still potentially being con-
strued as a request. This is an example of a con-
ventionalized form, in which the implied request
is more directly associated with the implicit form.
Often considered even less of a threat to negative
face are unconventionalized ISAs, which often re-
quire a deeper chain of inference to derive their
implied meaning. It is primarily the modulation of
the level of request indirectness that is the focus of
(Gupta et al., 2007; Briggs and Scheutz, 2013).
To provide an empirical evaluation of their sys-
tem, Gupta et al. (2007) asked human subjects
to rate the politeness of generated requests on a
five-point Likert scale in order of most rude (1)
to to most polite (5). The results from (Gupta et
al., 2007) for each of their politeness strategy cat-
egories are below:
</bodyText>
<listItem confidence="0.998855">
1. Autonomy [3.4] (e.g. “Could you possibly do
X for me?”)
2. Approval [3.0] (e.g. “Could you please do X
mate?”)
3. Direct [2.0] (e.g. “Do X.”)
4. Indirect [1.8] (e.g. “X is not done yet.”)
</listItem>
<bodyText confidence="0.999945461538462">
This finding is, in some sense, counterintuitive,
as unconventionalized request forms should be
the least face-threatening. However, Gupta et al.
(2007) briefly often an explanation, saying that the
utterances generated in the indirect category sound
a bit like a “complaint or sarcasm.” We agree with
this assessment. More precisely, while negative
face is protected by the use of their unconvention-
alized ISAs, positive face was not.
To model whether or not utterances may be in-
terpreted as being complaints or criticisms, we
seek to determine whether or not they can be in-
terpreted as an act of blame2.
</bodyText>
<footnote confidence="0.839185">
2What the precise ontological relationship is between
concepts such as complaining, criticizing, and blaming is be-
yond the scope of this paper.
</footnote>
<sectionHeader confidence="0.990094" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999976">
Like praise, blame (its negative counterpart) is
both a cognitive and social phenomenon (Malle et
al., 2012). The cognitive component pertains to
the internal attitudes of an agent regarding another
agent and their actions, while the social compo-
nent involves the expression of these internal at-
titudes through communicative acts. To achieve
blame-sensitivity in NLG, we need to model both
these aspects. In the following sections, we briefly
discuss how this could be accomplished.
</bodyText>
<subsectionHeader confidence="0.999259">
3.1 Pragmatic and Belief Reasoning
</subsectionHeader>
<bodyText confidence="0.978315342857143">
Before a speaker 5 can determine the high-level
perlocutionary effects of an utterance on an ad-
dressee (H) vis-´a-vis whether or not they feel crit-
icized or blamed, it is first necessary to determine
the precise set of beliefs and intentions of the ad-
dressee upon hearing an utterance u in context c.
We denote this updated set of beliefs and inten-
tions XFH(u, c). Note that this set is a model of
agent H’s beliefs and intentions from the speaker
5’s perspective, and not necessarily equivalent to
the actual belief state of agent H. In order to per-
form this mental modeling, we utilize a reason-
ing system similar to that in (Briggs and Scheutz,
2011). This pragmatic reasoning architecture uti-
lizes a set of rules of the form:
[[U]]C := 01 n ... n 0n
where U denotes an utterance form, C
denotes a set of contextual constraints that
must hold, and 0 denotes a belief update
predicate. An utterance form is specified
by u = UtteranceType(a, 0, X, M), where
UtteranceType denotes the dialogue turn type
(e.g. statement, y/n-question), a denotes the
speaker of the utterance u, 0 denotes the addressee
of the utterance, X denotes the surface semantics
of the utterance, and M denotes a set of sentential
modifiers. An example of such a pragmatic rule is
found below:
[[5tmt(5, H, X, {})]]o := want(5, bel(H, X))
which denotes that a statement by the speaker
5 to an addressee H that X holds should in-
dicate that, “5 wants H to believe X,” in all
contexts (given the empty set of contextual con-
straints). If this rule matches a recognized ut-
terance (and the contextual constraints are satis-
</bodyText>
<page confidence="0.970355">
2
</page>
<bodyText confidence="0.9996305">
fied, which is trivial in this case), then the men-
tal model of the addressee is updated such that:
want(S, bel(H, X)) 2 H(u, c).
Of particular interest with regard to the Gupta
et al. (2007) results, Briggs and Scheutz (2011)
describe how they can use their system to un-
derstand the semantics of the adverbial modifier
“yet,” which they describe as being indicative of
mutually understood intentionality. More accu-
rately, “yet,” is likely indicative of a belief regard-
ing expectation of an action being performed or
state being achieved. Therefore, a plausible prag-
matic rule to interpret, “X is not done yet,” could
be:
</bodyText>
<equation confidence="0.945954">
[[Stmt(S, H, -,done(X), {yet})]]0 :=
want(S, bel(H, -,done(X))) ^
expects(S, done(X))
</equation>
<bodyText confidence="0.960753045454545">
Furthermore, in a cooperative, task-driven con-
text, such as that described in (Gupta et al., 2007),
it would not be surprising for an interactant to infer
that this expectation is further indicative of a belief
in a particular intention or a task-based obligation
to achieve X.3
As such, if we consider an utterance ud as being
a standard direct request form (strategy 3), and an
utterance uy as being an indirect construction with
a yet modifier (strategy 4), the following facts may
hold:
bel(S, promised(H, S, X, tp)) 26 H(ud, c)
bel(S, promised(H, S, X, tp)) 2 H(uy, c)
If S is making a request to H, there is no be-
lieved agreement to achieve X. However, if “yet,”
is utilized, this may indicate to H a belief that S
thinks there is such an agreement.
Having calculated an updated mental model of
the addressee’s beliefs after hearing a candidate ut-
terance u, we now can attempt to infer the degree
to which u is interpreted as an act of criticism or
blame.
</bodyText>
<subsectionHeader confidence="0.99919">
3.2 Blame Modeling
</subsectionHeader>
<bodyText confidence="0.99975625">
Attributions of blame are influenced by several
factors including, but not limited to, beliefs about
an agent’s intentionality, capacity, foreknowledge,
obligations, and possible justifications (Malle et
</bodyText>
<footnote confidence="0.967837333333333">
3How precisely this reasoning is and/or ought to be per-
formed is an important question, but is outside the scope of
this paper.
</footnote>
<bodyText confidence="0.999411111111111">
al., 2012). Given the centrality of intentionality
in blame attribution, it is unsurprising that current
computational models involve reasoning within a
symbolic BDI (belief, desire, intention) frame-
work, utilizing rules to infer an ordinal degree of
blame based on the precise set of facts regarding
these factors (Mao and Gratch, 2012; Tomai and
Forbus, 2007). A rule that is similar to those found
in these systems is:
</bodyText>
<equation confidence="0.468953">
bel(S, promised(H, S, X, tp)) ^ bel(S, -,X) ^
bel(S, (t &gt; tp)) ^ bel(S, capable of(H, X))
) blames(S, H, high)
</equation>
<bodyText confidence="0.995025777777778">
that is to say, if agent S believes agent H
promised to him or her to achieve X by time
tp, and S believes X has not been achieved and
the current time t is past tp, and S believes H
is capable of fulfilling this promise, then S will
blame H to a high degree. Continuing our discus-
sion regarding the perlocutionary effects of ud and
uy, it is likely then that: blames(S, H, high) 26
H(ud, c) and blames(S, H, high) 2 H(uy, c).
</bodyText>
<subsectionHeader confidence="0.98992">
3.3 FTA Modeling
</subsectionHeader>
<bodyText confidence="0.999766">
Having determined whether or not an addressee
would feel criticized or blamed by a particu-
lar candidate utterance, it is then necessary to
translate this assessment back into the terms of
FTA-degree (the currency of the NLG system).
This requires a function O( ) that maps the or-
dinal blame assessment of the speaker toward
the hearer based on a set of beliefs , de-
scribed in the previous section, to a numerical
value than can be utilized to calculate the sever-
ity of the FTA (e.g. blames(S, H, high) = 9.0,
blames(S, H, medium) = 4.5). For the purposes
of this paper we adopt the theta-notation of Gupta
et al. (2007) to denote the degree to which an ut-
terance is a FTA. With the 0 function, we can then
express the blame-related FTA severity of an utter-
ance as:
</bodyText>
<equation confidence="0.988775">
⇥blame(u,c) = OH( H(u,c)) — ↵(c) · 0S( S)
</equation>
<bodyText confidence="0.999959666666667">
where OH denotes the level of blame the speaker
believes the hearer has inferred based on the ad-
dressee’s belief state after hearing utterance u with
context c ( H(u, c))). 0S denotes the level of
blame the speaker believes is appropriate given his
or her current belief state. Finally, ↵(c) denotes a
</bodyText>
<page confidence="0.997782">
3
</page>
<bodyText confidence="0.9997574">
multiplicative factor that models the appropriate-
ness of blame given the current social context. For
instance, independent of the objective blamewor-
thiness of a superior, it may be inappropriate for a
subordinate to criticize his or her superior in cer-
tain contexts.
Finally, then, the degree to which an utterance is
a FTA is the sum of all the contributions of evalu-
ations of possible threats to positive face and pos-
sible threats to negative face:
</bodyText>
<equation confidence="0.991817333333333">
1: 1:
O(u, c) = Op(u, c) +
p2P nEN
</equation>
<bodyText confidence="0.999934384615385">
where P denotes the set of all possible threats
to positive face (e.g. blame) and N denotes the set
of all possible threats to negative face (e.g. direct-
ness).
We can see how this would account for the
human-subject results from (Gupta et al., 2007), as
conventionally indirect requests (strategies 1 and
2) would not produce large threat-value contri-
butions from either the positive or negative FTA
components. Direct requests (strategy 3) would,
however, potentially produce a large ON contribu-
tion, while their set of indirect requests (strategy
4) would trigger a large OP contribution.
</bodyText>
<sectionHeader confidence="0.99922" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.99998503508772">
Having presented an approach to avoid certain
types of positive-FTAs through reasoning about
blame, one may be inclined to ask some questions
regarding the justification behind this approach.
Why should we want to better model one highly
complex social phenomenon (politeness) through
the inclusion of a model of another highly complex
social phenomenon (blame)? Does the integration
of a computational model of blame actually add
anything that would justify the effort?
At a superficial level, it does not. The
criticism/blame-related threat of a specific speech
act can be implicitly factored into the base FTA-
degree evaluation function supplied to the sys-
tem, determined by empirical data or designer-
consensus as is the case of (Miller et al., 2008).
However, this approach is limited in a couple
ways. First, this does not account for the fact that,
in addition to the set of social factors Brown and
Levinson articulated, the appropriateness of an act
of criticism or blame is also dependent on whether
or not it is justified. Reasoning about whether or
not an act of blame is justified requires: a compu-
tational model of blame.
Second, the inclusion of blame-reasoning
within the larger scope of the entire agent ar-
chitecture may enable useful behaviors both in-
side and outside the natural language system.
There is a growing community of researchers in-
terested in developing ethical-reasoning capabili-
ties for autonomous agents (Wallach and Allen,
2008), and the ability to reason about blame has
been proposed as one key competency for such
an ethically-sensitive agent (Bello and Bringsjord,
2013). Not only is there interest in utilizing such
mechanisms to influence general action-selection
in autonomous agents, but there is also interest in
the ability to understand and generate valid expla-
nations and justifications for adopted courses of
action in ethically-charged scenarios, which is of
direct relevance to the design of NLG architec-
tures.
While our proposed solution tackles threats
to positive face that arise due to unduly
critical/blame-oriented utterances, there are many
different ways of threatening positive face aside
from criticism/blame. These include phenomena
such as the discussion of inappropriate/sensitive
topics or non-cooperative behavior (e.g. purpose-
fully ignoring an interlocutor’s dialogue contribu-
tion). Indeed, empirical results show that referring
to an interlocutor in a dyadic interaction using an
impersonal pronoun (e.g. “someone”) may consti-
tute another such positive face threat (De Jong et
al., 2008). Future work will need to be done to de-
velop mechanisms to address these other possible
threats to positive face.
</bodyText>
<sectionHeader confidence="0.999302" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999761555555556">
Enabling politeness in NLG is a challenging prob-
lem that requires the modeling of a host of com-
plex, social psychological factors. In this paper,
we discuss ongoing work to integrate a compu-
tational model of blame to prevent inappropriate
threats to positive face that can account for prior
human-subject data. As an ongoing project, future
work is needed to further test and evaluate this pro-
posed approach.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999197333333333">
We would like to thank the reviewers for their
helpful feedback. This work was supported by
NSF grant #111323.
</bodyText>
<equation confidence="0.359159">
On(u, c)
</equation>
<page confidence="0.992415">
4
</page>
<sectionHeader confidence="0.995978" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999098548387097">
Paul Bello and Selmer Bringsjord. 2013. On how to
build a moral machine. Topoi, 32(2):251–266.
Gordon Briggs and Matthias Scheutz. 2011. Facilitat-
ing mental modeling in collaborative human-robot
interaction through adverbial cues. In Proceedings
of the SIGDIAL 2011 Conference, pages 239–247,
Portland, Oregon, June. Association for Computa-
tional Linguistics.
Gordon Briggs and Matthias Scheutz. 2013. A hybrid
architectural approach to understanding and appro-
priately generating indirect speech acts. In Proceed-
ings of the 27th AAAI Conference on Artificial Intel-
ligence.
Penelope Brown and Stephen C. Levinson. 1987. Po-
liteness: Some universals in language usage. Cam-
bridge University Press.
Justine Cassell and Timothy Bickmore. 2003. Negoti-
ated collusion: Modeling social language and its re-
lationship effects in intelligent agents. User Model-
ing and User-Adapted Interaction, 13(1-2):89–132.
Markus De Jong, Mari¨et Theune, and Dennis Hofs.
2008. Politeness and alignment in dialogues with
a virtual guide. In Proceedings of the 7th interna-
tional joint conference on Autonomous agents and
multiagent systems-Volume 1, pages 207–214. In-
ternational Foundation for Autonomous Agents and
Multiagent Systems.
Swati Gupta, Marilyn A Walker, and Daniela M Ro-
mano. 2007. How rude are you?: Evaluating po-
liteness and affect in interaction. In Affective Com-
puting and Intelligent Interaction, pages 203–217.
Springer.
Bertram F Malle, Steve Guglielmo, and Andrew E
Monroe. 2012. Moral, cognitive, and social: The
nature of blame. Social thinking and interpersonal
behavior, 14:313.
Wenji Mao and Jonathan Gratch. 2012. Modeling so-
cial causality and responsibility judgment in multi-
agent interactions. Journal of Artificial Intelligence
Research, 44(1):223–273.
Christopher A Miller, Peggy Wu, and Harry B Funk.
2008. A computational approach to etiquette: Oper-
ationalizing brown and levinson’s politeness model.
Intelligent Systems, IEEE, 23(4):28–35.
Megan Strait, Cody Canning, and Matthias Scheutz.
2014. Let me tell you! investigating the ef-
fects of robot communication strategies in advice-
giving situations based on robot appearance, inter-
action modality and distance. In Proceedings of
the 2014 ACM/IEEE international conference on
Human-robot interaction, pages 479–486. ACM.
Emmett Tomai and Ken Forbus. 2007. Plenty of blame
to go around: a qualitative approach to attribution of
moral responsibility. Technical report, DTIC Docu-
ment.
Cristen Torrey, Susan R Fussell, and Sara Kiesler.
2013. How a robot should give advice. In Human-
Robot Interaction (HRI), 2013 8th ACM/IEEE Inter-
national Conference on, pages 275–282. IEEE.
Wendell Wallach and Colin Allen. 2008. Moral ma-
chines: Teaching robots right from wrong. Oxford
University Press.
</reference>
<page confidence="0.994119">
5
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.508484">
<title confidence="0.9166158">Modeling Blame to Avoid Positive Face Threats in Natural Language Generation Gordon Human-Robot Interaction Tufts</title>
<author confidence="0.63429">MA Medford</author>
<email confidence="0.999363">gbriggs@cs.tufts.edu</email>
<abstract confidence="0.995598285714286">Prior approaches to politeness modulation in natural language generation (NLG) often focus on manipulating factors such as the directness of requests that pertain to preserving the autonomy of the addressee (negative face threats), but do not have a systematic way of understanding potential impoliteness from inadvertently critical or blame-oriented communications (positive face threats). In this paper, we discuss ongoing work to integrate a computational model of blame to prevent inappropriate threats to positive face.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paul Bello</author>
<author>Selmer Bringsjord</author>
</authors>
<title>On how to build a moral machine.</title>
<date>2013</date>
<journal>Topoi,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="15048" citStr="Bello and Bringsjord, 2013" startWordPosition="2493" endWordPosition="2496">is also dependent on whether or not it is justified. Reasoning about whether or not an act of blame is justified requires: a computational model of blame. Second, the inclusion of blame-reasoning within the larger scope of the entire agent architecture may enable useful behaviors both inside and outside the natural language system. There is a growing community of researchers interested in developing ethical-reasoning capabilities for autonomous agents (Wallach and Allen, 2008), and the ability to reason about blame has been proposed as one key competency for such an ethically-sensitive agent (Bello and Bringsjord, 2013). Not only is there interest in utilizing such mechanisms to influence general action-selection in autonomous agents, but there is also interest in the ability to understand and generate valid explanations and justifications for adopted courses of action in ethically-charged scenarios, which is of direct relevance to the design of NLG architectures. While our proposed solution tackles threats to positive face that arise due to unduly critical/blame-oriented utterances, there are many different ways of threatening positive face aside from criticism/blame. These include phenomena such as the dis</context>
</contexts>
<marker>Bello, Bringsjord, 2013</marker>
<rawString>Paul Bello and Selmer Bringsjord. 2013. On how to build a moral machine. Topoi, 32(2):251–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gordon Briggs</author>
<author>Matthias Scheutz</author>
</authors>
<title>Facilitating mental modeling in collaborative human-robot interaction through adverbial cues.</title>
<date>2011</date>
<booktitle>In Proceedings of the SIGDIAL 2011 Conference,</booktitle>
<pages>239--247</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<contexts>
<context position="7306" citStr="Briggs and Scheutz, 2011" startWordPosition="1164" endWordPosition="1167">rmine the high-level perlocutionary effects of an utterance on an addressee (H) vis-´a-vis whether or not they feel criticized or blamed, it is first necessary to determine the precise set of beliefs and intentions of the addressee upon hearing an utterance u in context c. We denote this updated set of beliefs and intentions XFH(u, c). Note that this set is a model of agent H’s beliefs and intentions from the speaker 5’s perspective, and not necessarily equivalent to the actual belief state of agent H. In order to perform this mental modeling, we utilize a reasoning system similar to that in (Briggs and Scheutz, 2011). This pragmatic reasoning architecture utilizes a set of rules of the form: [[U]]C := 01 n ... n 0n where U denotes an utterance form, C denotes a set of contextual constraints that must hold, and 0 denotes a belief update predicate. An utterance form is specified by u = UtteranceType(a, 0, X, M), where UtteranceType denotes the dialogue turn type (e.g. statement, y/n-question), a denotes the speaker of the utterance u, 0 denotes the addressee of the utterance, X denotes the surface semantics of the utterance, and M denotes a set of sentential modifiers. An example of such a pragmatic rule is</context>
</contexts>
<marker>Briggs, Scheutz, 2011</marker>
<rawString>Gordon Briggs and Matthias Scheutz. 2011. Facilitating mental modeling in collaborative human-robot interaction through adverbial cues. In Proceedings of the SIGDIAL 2011 Conference, pages 239–247, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gordon Briggs</author>
<author>Matthias Scheutz</author>
</authors>
<title>A hybrid architectural approach to understanding and appropriately generating indirect speech acts.</title>
<date>2013</date>
<booktitle>In Proceedings of the 27th AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="2017" citStr="Briggs and Scheutz, 2013" startWordPosition="285" endWordPosition="288"> assessments of an agent along multiple dimensions (e.g. likeability). However, while these studies investigated the human factors aspects of utilizing politeness strategies, they were not concerned with the natural language generation (NLG) mechanisms necessary to appropriately realize and deploy these strategies. Instead, there is a small, but growing, body of work on natural language generation architectures that seek to address this challenge (Gupta et al., 2007; Miller et al., 2008; Matthias Scheutz Human-Robot Interaction Laboratory Tufts University Medford, MA USA mscheutz@cs.tufts.edu Briggs and Scheutz, 2013). The common approach taken by these architectures is the operationalization of key factors in Brown and Levinson’s seminal work on politeness theory, in particular, the degree to which an utterance can be considered a face-threatening act (FTA) (Brown and Levinson, 1987). While this prior work demonstrates the abilities of these NLG architectures to successfully produce polite language, there remain some key challenges. Perhaps the most crucial question is: how does one calculate the degree to which an utterance is a FTA1? This is a complex issue, as not only is this value modulated by factor</context>
<context position="4868" citStr="Briggs and Scheutz, 2013" startWordPosition="754" endWordPosition="757">st. An example of an indirect request is the question, “Could you get me a coffee?”, which avoids the autonomy-threatening direct imperative, while still potentially being construed as a request. This is an example of a conventionalized form, in which the implied request is more directly associated with the implicit form. Often considered even less of a threat to negative face are unconventionalized ISAs, which often require a deeper chain of inference to derive their implied meaning. It is primarily the modulation of the level of request indirectness that is the focus of (Gupta et al., 2007; Briggs and Scheutz, 2013). To provide an empirical evaluation of their system, Gupta et al. (2007) asked human subjects to rate the politeness of generated requests on a five-point Likert scale in order of most rude (1) to to most polite (5). The results from (Gupta et al., 2007) for each of their politeness strategy categories are below: 1. Autonomy [3.4] (e.g. “Could you possibly do X for me?”) 2. Approval [3.0] (e.g. “Could you please do X mate?”) 3. Direct [2.0] (e.g. “Do X.”) 4. Indirect [1.8] (e.g. “X is not done yet.”) This finding is, in some sense, counterintuitive, as unconventionalized request forms should </context>
</contexts>
<marker>Briggs, Scheutz, 2013</marker>
<rawString>Gordon Briggs and Matthias Scheutz. 2013. A hybrid architectural approach to understanding and appropriately generating indirect speech acts. In Proceedings of the 27th AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penelope Brown</author>
<author>Stephen C Levinson</author>
</authors>
<title>Politeness: Some universals in language usage.</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2289" citStr="Brown and Levinson, 1987" startWordPosition="331" endWordPosition="334">riately realize and deploy these strategies. Instead, there is a small, but growing, body of work on natural language generation architectures that seek to address this challenge (Gupta et al., 2007; Miller et al., 2008; Matthias Scheutz Human-Robot Interaction Laboratory Tufts University Medford, MA USA mscheutz@cs.tufts.edu Briggs and Scheutz, 2013). The common approach taken by these architectures is the operationalization of key factors in Brown and Levinson’s seminal work on politeness theory, in particular, the degree to which an utterance can be considered a face-threatening act (FTA) (Brown and Levinson, 1987). While this prior work demonstrates the abilities of these NLG architectures to successfully produce polite language, there remain some key challenges. Perhaps the most crucial question is: how does one calculate the degree to which an utterance is a FTA1? This is a complex issue, as not only is this value modulated by factors such as social distance, power, and context, but also the multifaceted nature of “face.” An utterance may be polite in relation to negative face (i.e. the agent’s autonomy), but may be quite impolite with regard to positive face (i.e. the agent’s image and perceived cha</context>
</contexts>
<marker>Brown, Levinson, 1987</marker>
<rawString>Penelope Brown and Stephen C. Levinson. 1987. Politeness: Some universals in language usage. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justine Cassell</author>
<author>Timothy Bickmore</author>
</authors>
<title>Negotiated collusion: Modeling social language and its relationship effects in intelligent agents. User Modeling and User-Adapted Interaction,</title>
<date>2003</date>
<pages>13--1</pages>
<contexts>
<context position="1197" citStr="Cassell and Bickmore, 2003" startWordPosition="164" endWordPosition="167">hreats). In this paper, we discuss ongoing work to integrate a computational model of blame to prevent inappropriate threats to positive face. 1 Introduction When communicating with one another, people often modulate their language based on a variety of social factors. Enabling natural and humanlike interactions with virtual and robotic agents may require engineering these agents to be able to demonstrate appropriate social behaviors. For instance, increasing attention is being paid to the effects of utilizing politeness strategies in both human-computer and human-robot dialogue interactions (Cassell and Bickmore, 2003; Torrey et al., 2013; Strait et al., 2014). This work has shown that, depending on context, the deployment of politeness strategies by artificial agents can increase human interactants’ positive assessments of an agent along multiple dimensions (e.g. likeability). However, while these studies investigated the human factors aspects of utilizing politeness strategies, they were not concerned with the natural language generation (NLG) mechanisms necessary to appropriately realize and deploy these strategies. Instead, there is a small, but growing, body of work on natural language generation arch</context>
</contexts>
<marker>Cassell, Bickmore, 2003</marker>
<rawString>Justine Cassell and Timothy Bickmore. 2003. Negotiated collusion: Modeling social language and its relationship effects in intelligent agents. User Modeling and User-Adapted Interaction, 13(1-2):89–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus De Jong</author>
<author>Mari¨et Theune</author>
<author>Dennis Hofs</author>
</authors>
<title>Politeness and alignment in dialogues with a virtual guide.</title>
<date>2008</date>
<booktitle>In Proceedings of the 7th international joint conference on Autonomous agents and multiagent</booktitle>
<volume>1</volume>
<pages>207--214</pages>
<marker>De Jong, Theune, Hofs, 2008</marker>
<rawString>Markus De Jong, Mari¨et Theune, and Dennis Hofs. 2008. Politeness and alignment in dialogues with a virtual guide. In Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems-Volume 1, pages 207–214. International Foundation for Autonomous Agents and Multiagent Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swati Gupta</author>
<author>Marilyn A Walker</author>
<author>Daniela M Romano</author>
</authors>
<title>How rude are you?: Evaluating politeness and affect in interaction.</title>
<date>2007</date>
<booktitle>In Affective Computing and Intelligent Interaction,</booktitle>
<pages>203--217</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1862" citStr="Gupta et al., 2007" startWordPosition="266" endWordPosition="269">is work has shown that, depending on context, the deployment of politeness strategies by artificial agents can increase human interactants’ positive assessments of an agent along multiple dimensions (e.g. likeability). However, while these studies investigated the human factors aspects of utilizing politeness strategies, they were not concerned with the natural language generation (NLG) mechanisms necessary to appropriately realize and deploy these strategies. Instead, there is a small, but growing, body of work on natural language generation architectures that seek to address this challenge (Gupta et al., 2007; Miller et al., 2008; Matthias Scheutz Human-Robot Interaction Laboratory Tufts University Medford, MA USA mscheutz@cs.tufts.edu Briggs and Scheutz, 2013). The common approach taken by these architectures is the operationalization of key factors in Brown and Levinson’s seminal work on politeness theory, in particular, the degree to which an utterance can be considered a face-threatening act (FTA) (Brown and Levinson, 1987). While this prior work demonstrates the abilities of these NLG architectures to successfully produce polite language, there remain some key challenges. Perhaps the most cru</context>
<context position="3166" citStr="Gupta et al., 2007" startWordPosition="483" endWordPosition="486">This is a complex issue, as not only is this value modulated by factors such as social distance, power, and context, but also the multifaceted nature of “face.” An utterance may be polite in relation to negative face (i.e. the agent’s autonomy), but may be quite impolite with regard to positive face (i.e. the agent’s image and perceived character). In this paper, we investigate the problem of modeling threats to positive face. First we discuss how prior work that has focused primarily on mitigating threats to negative face, and examine a specific example, taken from the human subject data of (Gupta et al., 2007), to show why accounting for positive face is necessary. Next, we discuss our proposed solution to begin to model threats to positive face– specifically, integrating a computational model of blame. Finally, we discuss the justification behind and limitations of this proposed approach. 2 Motivation Brown and Levinson (1987) articulated a taxonomy of politeness strategies, distinguishing broadly between the notion of positive and negative politeness (with many distinct strategies for each). These categories of politeness correspond 1Less crucially, what is the appropriate notation for this value</context>
<context position="4841" citStr="Gupta et al., 2007" startWordPosition="750" endWordPosition="753">r, an indirect request. An example of an indirect request is the question, “Could you get me a coffee?”, which avoids the autonomy-threatening direct imperative, while still potentially being construed as a request. This is an example of a conventionalized form, in which the implied request is more directly associated with the implicit form. Often considered even less of a threat to negative face are unconventionalized ISAs, which often require a deeper chain of inference to derive their implied meaning. It is primarily the modulation of the level of request indirectness that is the focus of (Gupta et al., 2007; Briggs and Scheutz, 2013). To provide an empirical evaluation of their system, Gupta et al. (2007) asked human subjects to rate the politeness of generated requests on a five-point Likert scale in order of most rude (1) to to most polite (5). The results from (Gupta et al., 2007) for each of their politeness strategy categories are below: 1. Autonomy [3.4] (e.g. “Could you possibly do X for me?”) 2. Approval [3.0] (e.g. “Could you please do X mate?”) 3. Direct [2.0] (e.g. “Do X.”) 4. Indirect [1.8] (e.g. “X is not done yet.”) This finding is, in some sense, counterintuitive, as unconventiona</context>
<context position="8434" citStr="Gupta et al. (2007)" startWordPosition="1366" endWordPosition="1369">erance, and M denotes a set of sentential modifiers. An example of such a pragmatic rule is found below: [[5tmt(5, H, X, {})]]o := want(5, bel(H, X)) which denotes that a statement by the speaker 5 to an addressee H that X holds should indicate that, “5 wants H to believe X,” in all contexts (given the empty set of contextual constraints). If this rule matches a recognized utterance (and the contextual constraints are satis2 fied, which is trivial in this case), then the mental model of the addressee is updated such that: want(S, bel(H, X)) 2 H(u, c). Of particular interest with regard to the Gupta et al. (2007) results, Briggs and Scheutz (2011) describe how they can use their system to understand the semantics of the adverbial modifier “yet,” which they describe as being indicative of mutually understood intentionality. More accurately, “yet,” is likely indicative of a belief regarding expectation of an action being performed or state being achieved. Therefore, a plausible pragmatic rule to interpret, “X is not done yet,” could be: [[Stmt(S, H, -,done(X), {yet})]]0 := want(S, bel(H, -,done(X))) ^ expects(S, done(X)) Furthermore, in a cooperative, task-driven context, such as that described in (Gupt</context>
<context position="11860" citStr="Gupta et al. (2007)" startWordPosition="1961" endWordPosition="1964">Having determined whether or not an addressee would feel criticized or blamed by a particular candidate utterance, it is then necessary to translate this assessment back into the terms of FTA-degree (the currency of the NLG system). This requires a function O( ) that maps the ordinal blame assessment of the speaker toward the hearer based on a set of beliefs , described in the previous section, to a numerical value than can be utilized to calculate the severity of the FTA (e.g. blames(S, H, high) = 9.0, blames(S, H, medium) = 4.5). For the purposes of this paper we adopt the theta-notation of Gupta et al. (2007) to denote the degree to which an utterance is a FTA. With the 0 function, we can then express the blame-related FTA severity of an utterance as: ⇥blame(u,c) = OH( H(u,c)) — ↵(c) · 0S( S) where OH denotes the level of blame the speaker believes the hearer has inferred based on the addressee’s belief state after hearing utterance u with context c ( H(u, c))). 0S denotes the level of blame the speaker believes is appropriate given his or her current belief state. Finally, ↵(c) denotes a 3 multiplicative factor that models the appropriateness of blame given the current social context. For instanc</context>
<context position="13087" citStr="Gupta et al., 2007" startWordPosition="2184" endWordPosition="2187">endent of the objective blameworthiness of a superior, it may be inappropriate for a subordinate to criticize his or her superior in certain contexts. Finally, then, the degree to which an utterance is a FTA is the sum of all the contributions of evaluations of possible threats to positive face and possible threats to negative face: 1: 1: O(u, c) = Op(u, c) + p2P nEN where P denotes the set of all possible threats to positive face (e.g. blame) and N denotes the set of all possible threats to negative face (e.g. directness). We can see how this would account for the human-subject results from (Gupta et al., 2007), as conventionally indirect requests (strategies 1 and 2) would not produce large threat-value contributions from either the positive or negative FTA components. Direct requests (strategy 3) would, however, potentially produce a large ON contribution, while their set of indirect requests (strategy 4) would trigger a large OP contribution. 4 Discussion Having presented an approach to avoid certain types of positive-FTAs through reasoning about blame, one may be inclined to ask some questions regarding the justification behind this approach. Why should we want to better model one highly complex</context>
</contexts>
<marker>Gupta, Walker, Romano, 2007</marker>
<rawString>Swati Gupta, Marilyn A Walker, and Daniela M Romano. 2007. How rude are you?: Evaluating politeness and affect in interaction. In Affective Computing and Intelligent Interaction, pages 203–217. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bertram F Malle</author>
<author>Steve Guglielmo</author>
<author>Andrew E Monroe</author>
</authors>
<title>Moral, cognitive, and social: The nature of blame. Social thinking and interpersonal behavior,</title>
<date>2012</date>
<pages>14--313</pages>
<contexts>
<context position="6244" citStr="Malle et al., 2012" startWordPosition="985" endWordPosition="988"> a bit like a “complaint or sarcasm.” We agree with this assessment. More precisely, while negative face is protected by the use of their unconventionalized ISAs, positive face was not. To model whether or not utterances may be interpreted as being complaints or criticisms, we seek to determine whether or not they can be interpreted as an act of blame2. 2What the precise ontological relationship is between concepts such as complaining, criticizing, and blaming is beyond the scope of this paper. 3 Approach Like praise, blame (its negative counterpart) is both a cognitive and social phenomenon (Malle et al., 2012). The cognitive component pertains to the internal attitudes of an agent regarding another agent and their actions, while the social component involves the expression of these internal attitudes through communicative acts. To achieve blame-sensitivity in NLG, we need to model both these aspects. In the following sections, we briefly discuss how this could be accomplished. 3.1 Pragmatic and Belief Reasoning Before a speaker 5 can determine the high-level perlocutionary effects of an utterance on an addressee (H) vis-´a-vis whether or not they feel criticized or blamed, it is first necessary to </context>
</contexts>
<marker>Malle, Guglielmo, Monroe, 2012</marker>
<rawString>Bertram F Malle, Steve Guglielmo, and Andrew E Monroe. 2012. Moral, cognitive, and social: The nature of blame. Social thinking and interpersonal behavior, 14:313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenji Mao</author>
<author>Jonathan Gratch</author>
</authors>
<title>Modeling social causality and responsibility judgment in multiagent interactions.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>44</volume>
<issue>1</issue>
<contexts>
<context position="10600" citStr="Mao and Gratch, 2012" startWordPosition="1722" endWordPosition="1725">factors including, but not limited to, beliefs about an agent’s intentionality, capacity, foreknowledge, obligations, and possible justifications (Malle et 3How precisely this reasoning is and/or ought to be performed is an important question, but is outside the scope of this paper. al., 2012). Given the centrality of intentionality in blame attribution, it is unsurprising that current computational models involve reasoning within a symbolic BDI (belief, desire, intention) framework, utilizing rules to infer an ordinal degree of blame based on the precise set of facts regarding these factors (Mao and Gratch, 2012; Tomai and Forbus, 2007). A rule that is similar to those found in these systems is: bel(S, promised(H, S, X, tp)) ^ bel(S, -,X) ^ bel(S, (t &gt; tp)) ^ bel(S, capable of(H, X)) ) blames(S, H, high) that is to say, if agent S believes agent H promised to him or her to achieve X by time tp, and S believes X has not been achieved and the current time t is past tp, and S believes H is capable of fulfilling this promise, then S will blame H to a high degree. Continuing our discussion regarding the perlocutionary effects of ud and uy, it is likely then that: blames(S, H, high) 26 H(ud, c) and blames(</context>
</contexts>
<marker>Mao, Gratch, 2012</marker>
<rawString>Wenji Mao and Jonathan Gratch. 2012. Modeling social causality and responsibility judgment in multiagent interactions. Journal of Artificial Intelligence Research, 44(1):223–273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher A Miller</author>
<author>Peggy Wu</author>
<author>Harry B Funk</author>
</authors>
<title>A computational approach to etiquette: Operationalizing brown and levinson’s politeness model. Intelligent Systems,</title>
<date>2008</date>
<pages>23--4</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="1883" citStr="Miller et al., 2008" startWordPosition="270" endWordPosition="273">at, depending on context, the deployment of politeness strategies by artificial agents can increase human interactants’ positive assessments of an agent along multiple dimensions (e.g. likeability). However, while these studies investigated the human factors aspects of utilizing politeness strategies, they were not concerned with the natural language generation (NLG) mechanisms necessary to appropriately realize and deploy these strategies. Instead, there is a small, but growing, body of work on natural language generation architectures that seek to address this challenge (Gupta et al., 2007; Miller et al., 2008; Matthias Scheutz Human-Robot Interaction Laboratory Tufts University Medford, MA USA mscheutz@cs.tufts.edu Briggs and Scheutz, 2013). The common approach taken by these architectures is the operationalization of key factors in Brown and Levinson’s seminal work on politeness theory, in particular, the degree to which an utterance can be considered a face-threatening act (FTA) (Brown and Levinson, 1987). While this prior work demonstrates the abilities of these NLG architectures to successfully produce polite language, there remain some key challenges. Perhaps the most crucial question is: how</context>
<context position="14194" citStr="Miller et al., 2008" startWordPosition="2354" endWordPosition="2357">questions regarding the justification behind this approach. Why should we want to better model one highly complex social phenomenon (politeness) through the inclusion of a model of another highly complex social phenomenon (blame)? Does the integration of a computational model of blame actually add anything that would justify the effort? At a superficial level, it does not. The criticism/blame-related threat of a specific speech act can be implicitly factored into the base FTAdegree evaluation function supplied to the system, determined by empirical data or designerconsensus as is the case of (Miller et al., 2008). However, this approach is limited in a couple ways. First, this does not account for the fact that, in addition to the set of social factors Brown and Levinson articulated, the appropriateness of an act of criticism or blame is also dependent on whether or not it is justified. Reasoning about whether or not an act of blame is justified requires: a computational model of blame. Second, the inclusion of blame-reasoning within the larger scope of the entire agent architecture may enable useful behaviors both inside and outside the natural language system. There is a growing community of researc</context>
</contexts>
<marker>Miller, Wu, Funk, 2008</marker>
<rawString>Christopher A Miller, Peggy Wu, and Harry B Funk. 2008. A computational approach to etiquette: Operationalizing brown and levinson’s politeness model. Intelligent Systems, IEEE, 23(4):28–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Megan Strait</author>
<author>Cody Canning</author>
<author>Matthias Scheutz</author>
</authors>
<title>Let me tell you! investigating the effects of robot communication strategies in advicegiving situations based on robot appearance, interaction modality and distance.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction,</booktitle>
<pages>479--486</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1240" citStr="Strait et al., 2014" startWordPosition="172" endWordPosition="175">o integrate a computational model of blame to prevent inappropriate threats to positive face. 1 Introduction When communicating with one another, people often modulate their language based on a variety of social factors. Enabling natural and humanlike interactions with virtual and robotic agents may require engineering these agents to be able to demonstrate appropriate social behaviors. For instance, increasing attention is being paid to the effects of utilizing politeness strategies in both human-computer and human-robot dialogue interactions (Cassell and Bickmore, 2003; Torrey et al., 2013; Strait et al., 2014). This work has shown that, depending on context, the deployment of politeness strategies by artificial agents can increase human interactants’ positive assessments of an agent along multiple dimensions (e.g. likeability). However, while these studies investigated the human factors aspects of utilizing politeness strategies, they were not concerned with the natural language generation (NLG) mechanisms necessary to appropriately realize and deploy these strategies. Instead, there is a small, but growing, body of work on natural language generation architectures that seek to address this challen</context>
</contexts>
<marker>Strait, Canning, Scheutz, 2014</marker>
<rawString>Megan Strait, Cody Canning, and Matthias Scheutz. 2014. Let me tell you! investigating the effects of robot communication strategies in advicegiving situations based on robot appearance, interaction modality and distance. In Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction, pages 479–486. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmett Tomai</author>
<author>Ken Forbus</author>
</authors>
<title>Plenty of blame to go around: a qualitative approach to attribution of moral responsibility.</title>
<date>2007</date>
<tech>Technical report, DTIC Document.</tech>
<contexts>
<context position="10625" citStr="Tomai and Forbus, 2007" startWordPosition="1726" endWordPosition="1729"> not limited to, beliefs about an agent’s intentionality, capacity, foreknowledge, obligations, and possible justifications (Malle et 3How precisely this reasoning is and/or ought to be performed is an important question, but is outside the scope of this paper. al., 2012). Given the centrality of intentionality in blame attribution, it is unsurprising that current computational models involve reasoning within a symbolic BDI (belief, desire, intention) framework, utilizing rules to infer an ordinal degree of blame based on the precise set of facts regarding these factors (Mao and Gratch, 2012; Tomai and Forbus, 2007). A rule that is similar to those found in these systems is: bel(S, promised(H, S, X, tp)) ^ bel(S, -,X) ^ bel(S, (t &gt; tp)) ^ bel(S, capable of(H, X)) ) blames(S, H, high) that is to say, if agent S believes agent H promised to him or her to achieve X by time tp, and S believes X has not been achieved and the current time t is past tp, and S believes H is capable of fulfilling this promise, then S will blame H to a high degree. Continuing our discussion regarding the perlocutionary effects of ud and uy, it is likely then that: blames(S, H, high) 26 H(ud, c) and blames(S, H, high) 2 H(uy, c). 3</context>
</contexts>
<marker>Tomai, Forbus, 2007</marker>
<rawString>Emmett Tomai and Ken Forbus. 2007. Plenty of blame to go around: a qualitative approach to attribution of moral responsibility. Technical report, DTIC Document.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristen Torrey</author>
<author>Susan R Fussell</author>
<author>Sara Kiesler</author>
</authors>
<title>How a robot should give advice.</title>
<date>2013</date>
<booktitle>In HumanRobot Interaction (HRI), 2013 8th ACM/IEEE International Conference on,</booktitle>
<pages>275--282</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1218" citStr="Torrey et al., 2013" startWordPosition="168" endWordPosition="171">iscuss ongoing work to integrate a computational model of blame to prevent inappropriate threats to positive face. 1 Introduction When communicating with one another, people often modulate their language based on a variety of social factors. Enabling natural and humanlike interactions with virtual and robotic agents may require engineering these agents to be able to demonstrate appropriate social behaviors. For instance, increasing attention is being paid to the effects of utilizing politeness strategies in both human-computer and human-robot dialogue interactions (Cassell and Bickmore, 2003; Torrey et al., 2013; Strait et al., 2014). This work has shown that, depending on context, the deployment of politeness strategies by artificial agents can increase human interactants’ positive assessments of an agent along multiple dimensions (e.g. likeability). However, while these studies investigated the human factors aspects of utilizing politeness strategies, they were not concerned with the natural language generation (NLG) mechanisms necessary to appropriately realize and deploy these strategies. Instead, there is a small, but growing, body of work on natural language generation architectures that seek t</context>
</contexts>
<marker>Torrey, Fussell, Kiesler, 2013</marker>
<rawString>Cristen Torrey, Susan R Fussell, and Sara Kiesler. 2013. How a robot should give advice. In HumanRobot Interaction (HRI), 2013 8th ACM/IEEE International Conference on, pages 275–282. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendell Wallach</author>
<author>Colin Allen</author>
</authors>
<title>Moral machines: Teaching robots right from wrong.</title>
<date>2008</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="14902" citStr="Wallach and Allen, 2008" startWordPosition="2470" endWordPosition="2473">or the fact that, in addition to the set of social factors Brown and Levinson articulated, the appropriateness of an act of criticism or blame is also dependent on whether or not it is justified. Reasoning about whether or not an act of blame is justified requires: a computational model of blame. Second, the inclusion of blame-reasoning within the larger scope of the entire agent architecture may enable useful behaviors both inside and outside the natural language system. There is a growing community of researchers interested in developing ethical-reasoning capabilities for autonomous agents (Wallach and Allen, 2008), and the ability to reason about blame has been proposed as one key competency for such an ethically-sensitive agent (Bello and Bringsjord, 2013). Not only is there interest in utilizing such mechanisms to influence general action-selection in autonomous agents, but there is also interest in the ability to understand and generate valid explanations and justifications for adopted courses of action in ethically-charged scenarios, which is of direct relevance to the design of NLG architectures. While our proposed solution tackles threats to positive face that arise due to unduly critical/blame-o</context>
</contexts>
<marker>Wallach, Allen, 2008</marker>
<rawString>Wendell Wallach and Colin Allen. 2008. Moral machines: Teaching robots right from wrong. Oxford University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
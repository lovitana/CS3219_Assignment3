<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.197310">
<title confidence="0.983094">
Grammatical Error Detection and Correction Using Tagger Disagreement
</title>
<author confidence="0.876232">
Anubhav Gupta
</author>
<affiliation confidence="0.793037">
Universit´e de Franche-Comt´e
</affiliation>
<email confidence="0.986514">
anubhav.gupta@edu.univ-fcompte.fr
</email>
<sectionHeader confidence="0.993567" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999559714285714">
This paper presents a rule-based approach
for correcting grammatical errors made
by non-native speakers of English. The
approach relies on the differences in the
outputs of two POS taggers. This paper
is submitted in response to CoNLL-2014
Shared Task.
</bodyText>
<sectionHeader confidence="0.999265" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965678571429">
A part-of-speech (POS) tagger, like any other soft-
ware, has a set of inputs and outputs. The input
for a POS tagger is a group of words and a tagset,
and the output is a POS tag for a word (Jurafsky
and Martin, 2009). Given that a software is bound
to provide incorrect output for an incorrect input
(garbage in, garbage out), it is quite likely that tag-
gers trained to tag grammatically correct sentences
(the expected input) would not tag grammatically
incorrect sentences properly. Furthermore, it is
possible that the output of two different taggers for
a given incorrect input would be different.
For this shared task, the POS taggers used were
the Stanford Parser, which was used to preprocess
the training and test data (Ng et al., 2014) and the
TreeTagger (Schmid, 1994). The Stanford Parser
employs unlexicalized PCFG1 (Klein and Man-
ning, 2003), whereas the TreeTagger uses decision
trees. The TreeTagger is freely available2, and its
performance is comparable to that of the Stanford
Log-Linear Part-of-Speech Tagger (Toutanova et
al., 2003). Since the preprocessed dataset was al-
ready annotated with POS tags, the Stanford Log-
Linear POS Tagger was not used.
If the annotation of preprocessed data differed
from that of the TreeTagger, it was assumed that
the sentence might have grammatical errors. Once
an error was detected it was corrected using the
</bodyText>
<footnote confidence="0.9978155">
1Probabilistic Context-Free Grammar
2http://www.cis.uni-muenchen.de/⇠schmid/tools/TreeTagger/
</footnote>
<bodyText confidence="0.7810025">
Nodebox English Linguistics library3 (De Bleser
et al., 2002).
</bodyText>
<sectionHeader confidence="0.99696" genericHeader="method">
2 Error Detection
</sectionHeader>
<bodyText confidence="0.9999465">
The POS tag for each token in the data was com-
pared with the tag given by the TreeTagger. Sen-
tences were considered grammatically incorrect
upon meeting the following conditions:
</bodyText>
<listItem confidence="0.9812385">
• The number of tags in the preprocessed
dataset for a given sentence should be equal
to the number of tags returned by the Tree-
Tagger for the same sentence.
• There should be at least one token with dif-
ferent POS tags.
</listItem>
<bodyText confidence="0.999618916666667">
As an exception, if the taggers differed only on the
first token, such that the Stanford Parser tagged it
as NNP or NNPS, then the sentence was not con-
sidered for correction, as this difference can be
attributed to the capitalisation of the first token,
which the Stanford Parser interprets as a proper
noun.
Table 1 shows the precision (P) and the recall
(R) scores of this method for detecting erroneous
sentences in the training and test data. The low
recall score indicates that for most of the incorrect
sentences, the output of the taggers was identical.
</bodyText>
<subsectionHeader confidence="0.992739">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999891">
The output of the TreeTagger was modified so that
it had the same tag set as that used by the Stan-
ford Parser. The differences in the output tagset is
displayed in the Table 2.
</bodyText>
<subsectionHeader confidence="0.98433">
2.2 Errors
</subsectionHeader>
<bodyText confidence="0.99983">
Where the mismatch of tags is indicative of error,
it does not offer insight into the nature of the er-
ror and thus does not aid in error correction per se.
For example, the identification of a token as VBD
</bodyText>
<footnote confidence="0.928735">
3http://nodebox.net/code/index.php/Linguistics
</footnote>
<page confidence="0.986975">
49
</page>
<note confidence="0.734083">
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 49–52,
Baltimore, Maryland, 26-27 July 2014. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.9997128">
Dataset Total Erroneous Sentences with Erroneous Sentences P R
Sentences Tag Mismatch Identified Correctly
Training 21860 26282 11769 44.77 53.83
Test 1176 642 391 60.90 33.24
Test (Alternative)† 1195 642 398 61.99 33.30
</table>
<tableCaption confidence="0.86236">
† consists of additional error annotations provided by the participating teams.
Table 1: Performance of Error Detection.
</tableCaption>
<table confidence="0.9985615">
TreeTagger Stanford Parser
Tagset Tagset
( -LRB-
) -RRB-
NP NNP
NPS NNPS
PP PRP
SENT .
</table>
<tableCaption confidence="0.999718">
Table 2: Comparison of Tagsets.
</tableCaption>
<bodyText confidence="0.9996425">
(past tense) by one tagger and as VBN (past par-
ticiple) another does not imply that the mistake is
necessarily a verb tense (Vt) error. Table 4 lists
some of the errors detected by this approach.
</bodyText>
<sectionHeader confidence="0.999559" genericHeader="method">
3 Error Correction
</sectionHeader>
<bodyText confidence="0.99987775">
Since mismatched tag pairs did not consistently
correspond to a particular error type, not all er-
rors detected were corrected. Certain errors were
detected using hand-crafted rules.
</bodyText>
<subsectionHeader confidence="0.992846">
3.1 Subject-Verb Agreement (SVA) Errors
</subsectionHeader>
<bodyText confidence="0.9999055">
SVA errors were corrected with aid of dependency
relationships provided in the preprocessed data. If
a singular verb (VBZ) referred to a plural noun
(NNS) appearing before it, then the verb was made
plural. Similarly, if the singular verb (VBZ) was
the root of the dependency tree and was referred
to by a plural noun (NNS), then it was changed to
the plural.
</bodyText>
<subsectionHeader confidence="0.993624">
3.2 Verb Form (Vform) Errors
</subsectionHeader>
<bodyText confidence="0.999976">
If a modal verb (MD) preceded a singular verb
(VBZ), then the second verb was changed to the
bare infinitive form. Also, if the preposition
to preceded a singular verb, then the verb was
changed to its bare infinitive form.
</bodyText>
<subsectionHeader confidence="0.943746">
3.3 Errors Detected by POS Tag Mismatch
</subsectionHeader>
<bodyText confidence="0.9290035">
If a token followed by a noun is tagged as an ad-
jective (JJ) in the preprocessed data and as an ad-
</bodyText>
<table confidence="0.999953">
Dataset P R F,3_0.5
Training 23.89 0.31 1.49
Test 70.00 1.72 7.84
Test (Alternative) 72.00 1.90 8.60
</table>
<tableCaption confidence="0.9999">
Table 3: Performance of the Approach.
</tableCaption>
<bodyText confidence="0.99998352173913">
verb (RB) by the TreeTagger, then the adverbial
morpheme -ly was removed, resulting in the ad-
jective. For example, completely is changed to
complete in the second sentence of the fifth para-
graph of the essay 837 (Dahlmeier et al., 2013).
On the other hand, adverbs (RB) in the prepro-
cessed dataset that were labelled as adjectives (JJ)
by the TreeTagger were changed into their corre-
sponding adverbs.
A token preceded by the verb to be, tagged as
JJ by the Stanford Parser and identified by the
TreeTagger as a verb is assumed to be a verb
and accordingly converted into its past partici-
ple. Finally, the tokens labelled NNS and VBZ
by the Stanford Parser and the TreeTagger respec-
tively are likely to be Mec4 or Wform5 errors.
These tokens are replaced by plural nouns hav-
ing same initial substring (this is achieved using
the get close matches API of the difflib Python
library).
The performance of this approach, as measured
by the M2 scorer (Dahlmeier and Ng, 2012), is
presented in Table 3.
</bodyText>
<sectionHeader confidence="0.998568" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999742666666667">
The approach used in this paper is useful in de-
tecting mainly verb form, word form and spelling
errors. These errors result in ambiguous or incor-
rect input to the POS tagger, thus forcing it to pro-
duce incorrect output. However, it is quite likely
that with a different pair of taggers, different rules
</bodyText>
<footnote confidence="0.992575">
4Punctuation, capitalisation, spelling, typographical er-
rors
5Word form
</footnote>
<page confidence="0.968113">
50
</page>
<table confidence="0.9997769">
nid 829 caused problem like the appearance
Sentence This VBD NN IN DT NN
Stanford Parser DT VBN NN IN DT NN
TreeTagger DT
Error Type Vt
nid 829 also to reforms the land
Sentence but RB TO VB DT NN
Stanford Parser CC RB TO NNS DT NN
TreeTagger CC
Error Type Wci
nid 840 , their population amount to
Sentence India ,PRP$ NN VB TO
Stanford Parser NNP ,PRP$ NN NN TO
TreeTagger NNP (This was not an error in the training corpus.)
Error Type Vform
nid 1051 is currently a develop country
Sentence Singapore VBZ RB DT JJ NN
Stanford Parser NNP VBZ RB DT VB NN
TreeTagger NNP
Error Type Vform
nid 858 most of China enterprisers focus
Sentence Therefore JJS IN NNP VBZ NN
Stanford Parser RB RBS IN NNP NNS VBP
TreeTagger RB
Error Type Wform
nid 847 social constrains faced by engineers
Sentence and JJ NNS VBN IN NNS
Stanford Parser CC JJ VBZ VBN IN NNS
TreeTagger CC
Error Type Mec
</table>
<tableCaption confidence="0.999761">
Table 4: Errors Detected.
</tableCaption>
<bodyText confidence="0.999913">
would be required to correct these errors. Errors
concerning noun number, determiners and prepo-
sitions, which constitute a large portion of errors
committed by L2 learners (Chodorow et al., 2010;
De Felice and Pulman, 2009; Gamon et al., 2009),
were not addressed in this paper. This is the main
reason for low recall.
</bodyText>
<sectionHeader confidence="0.997493" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995971">
I would like to thank Calvin Cheng for proofread-
ing the paper and providing valuable feedback.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999457">
Martin Chodorow, Michael Gamon, and Joel Tetreault.
2010. The Utility of Article and Preposition Error
Correction Systems for English Language Learners:
Feedback and Assessment. Language Testing 27 (3):
419–436. doi:10.1177/0265532210364391.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei
Wu. 2013. Building a Large Annotated Corpus of
Learner English: The NUS Corpus of Learner En-
glish. Proceedings of the Eighth Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations (BEA 2013). 22 –31. Atlanta, Georgia, USA.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Bet-
ter Evaluation for Grammatical Error Correction.
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL 2012). 568 – 572. Montreal, Canada.
Frederik De Bleser, Tom De Smedt, and Lucas Nijs
2002. NodeBox version 1.9.6 for Mac OS X.
Rachele De Felice and Stephen G Pulman. 2009. Au-
tomatic Detection of Preposition Errors in Learner
Writing. CALICO Journal 26 (3): 512–528.
Michael Gamon, Claudia Leacock, Chris Brockett,
William B. Dolan, Jianfeng Gao, Dmitriy Belenko,
</reference>
<page confidence="0.979954">
51
</page>
<reference confidence="0.998625888888889">
and Alexandre Klementiev. 2009. Using Statistical
Techniques and Web Search to Correct ESL Errors.
CALICO Journal 26 (3): 491–511.
Daniel Jurafsky and James H. Martin. 2009. Part-of-
Speech Tagging. Speech and Language Processing:
An Introduction to Natural Language Processing,
Speech Recognition, and Computational Linguistics.
Prentice-Hall.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics. 423–430.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 Shared Task
on Grammatical Error Correction. Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task (CoNLL-2014
Shared Task). Baltimore, Maryland, USA.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. Proceedings of Inter-
national Conference on New Methods in Language
Processing. Manchester, UK.
Kristina Toutanova, Dan Klein, Christopher Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency Net-
work. Proceedings of HLT-NAACL 2003. 252–259.
</reference>
<page confidence="0.998858">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.628169">
<title confidence="0.997205">Grammatical Error Detection and Correction Using Tagger Disagreement</title>
<author confidence="0.976349">Anubhav Gupta</author>
<affiliation confidence="0.962878">Universit´e de Franche-Comt´e</affiliation>
<email confidence="0.961442">anubhav.gupta@edu.univ-fcompte.fr</email>
<abstract confidence="0.9805805">This paper presents a rule-based approach for correcting grammatical errors made by non-native speakers of English. The approach relies on the differences in the outputs of two POS taggers. This paper</abstract>
<note confidence="0.835856">is submitted in response to CoNLL-2014 Shared Task.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Michael Gamon</author>
<author>Joel Tetreault</author>
</authors>
<title>The Utility of Article and Preposition Error Correction Systems for English Language Learners: Feedback and Assessment.</title>
<date>2010</date>
<journal>Language Testing</journal>
<volume>27</volume>
<issue>3</issue>
<pages>419--436</pages>
<marker>Chodorow, Gamon, Tetreault, 2010</marker>
<rawString>Martin Chodorow, Michael Gamon, and Joel Tetreault. 2010. The Utility of Article and Preposition Error Correction Systems for English Language Learners: Feedback and Assessment. Language Testing 27 (3): 419–436. doi:10.1177/0265532210364391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
</authors>
<title>Building a Large Annotated Corpus of Learner English: The NUS Corpus of Learner English.</title>
<date>2013</date>
<booktitle>Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2013). 22 –31.</booktitle>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="5603" citStr="Dahlmeier et al., 2013" startWordPosition="913" endWordPosition="916"> Also, if the preposition to preceded a singular verb, then the verb was changed to its bare infinitive form. 3.3 Errors Detected by POS Tag Mismatch If a token followed by a noun is tagged as an adjective (JJ) in the preprocessed data and as an adDataset P R F,3_0.5 Training 23.89 0.31 1.49 Test 70.00 1.72 7.84 Test (Alternative) 72.00 1.90 8.60 Table 3: Performance of the Approach. verb (RB) by the TreeTagger, then the adverbial morpheme -ly was removed, resulting in the adjective. For example, completely is changed to complete in the second sentence of the fifth paragraph of the essay 837 (Dahlmeier et al., 2013). On the other hand, adverbs (RB) in the preprocessed dataset that were labelled as adjectives (JJ) by the TreeTagger were changed into their corresponding adverbs. A token preceded by the verb to be, tagged as JJ by the Stanford Parser and identified by the TreeTagger as a verb is assumed to be a verb and accordingly converted into its past participle. Finally, the tokens labelled NNS and VBZ by the Stanford Parser and the TreeTagger respectively are likely to be Mec4 or Wform5 errors. These tokens are replaced by plural nouns having same initial substring (this is achieved using the get clos</context>
</contexts>
<marker>Dahlmeier, Ng, Wu, 2013</marker>
<rawString>Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu. 2013. Building a Large Annotated Corpus of Learner English: The NUS Corpus of Learner English. Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2013). 22 –31. Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better Evaluation for Grammatical Error Correction.</title>
<date>2012</date>
<booktitle>Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2012). 568 – 572.</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="6336" citStr="Dahlmeier and Ng, 2012" startWordPosition="1042" endWordPosition="1045">eTagger were changed into their corresponding adverbs. A token preceded by the verb to be, tagged as JJ by the Stanford Parser and identified by the TreeTagger as a verb is assumed to be a verb and accordingly converted into its past participle. Finally, the tokens labelled NNS and VBZ by the Stanford Parser and the TreeTagger respectively are likely to be Mec4 or Wform5 errors. These tokens are replaced by plural nouns having same initial substring (this is achieved using the get close matches API of the difflib Python library). The performance of this approach, as measured by the M2 scorer (Dahlmeier and Ng, 2012), is presented in Table 3. 4 Conclusion The approach used in this paper is useful in detecting mainly verb form, word form and spelling errors. These errors result in ambiguous or incorrect input to the POS tagger, thus forcing it to produce incorrect output. However, it is quite likely that with a different pair of taggers, different rules 4Punctuation, capitalisation, spelling, typographical errors 5Word form 50 nid 829 caused problem like the appearance Sentence This VBD NN IN DT NN Stanford Parser DT VBN NN IN DT NN TreeTagger DT Error Type Vt nid 829 also to reforms the land Sentence but </context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2012. Better Evaluation for Grammatical Error Correction. Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2012). 568 – 572. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederik De Bleser</author>
<author>Tom De Smedt</author>
<author>Lucas Nijs</author>
</authors>
<title>NodeBox version 1.9.6 for Mac OS X.</title>
<date>2002</date>
<marker>De Bleser, De Smedt, Nijs, 2002</marker>
<rawString>Frederik De Bleser, Tom De Smedt, and Lucas Nijs 2002. NodeBox version 1.9.6 for Mac OS X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachele De Felice</author>
<author>Stephen G Pulman</author>
</authors>
<title>Automatic Detection of Preposition Errors in Learner Writing.</title>
<date>2009</date>
<journal>CALICO Journal</journal>
<volume>26</volume>
<issue>3</issue>
<pages>512--528</pages>
<marker>De Felice, Pulman, 2009</marker>
<rawString>Rachele De Felice and Stephen G Pulman. 2009. Automatic Detection of Preposition Errors in Learner Writing. CALICO Journal 26 (3): 512–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Claudia Leacock</author>
<author>Chris Brockett</author>
<author>William B Dolan</author>
<author>Jianfeng Gao</author>
<author>Dmitriy Belenko</author>
<author>Alexandre Klementiev</author>
</authors>
<title>Using Statistical Techniques and Web Search to Correct ESL Errors.</title>
<date>2009</date>
<journal>CALICO Journal</journal>
<volume>26</volume>
<issue>3</issue>
<pages>491--511</pages>
<marker>Gamon, Leacock, Brockett, Dolan, Gao, Belenko, Klementiev, 2009</marker>
<rawString>Michael Gamon, Claudia Leacock, Chris Brockett, William B. Dolan, Jianfeng Gao, Dmitriy Belenko, and Alexandre Klementiev. 2009. Using Statistical Techniques and Web Search to Correct ESL Errors. CALICO Journal 26 (3): 491–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Part-ofSpeech Tagging. Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics.</title>
<date>2009</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="640" citStr="Jurafsky and Martin, 2009" startWordPosition="96" endWordPosition="99">ror Detection and Correction Using Tagger Disagreement Anubhav Gupta Universit´e de Franche-Comt´e anubhav.gupta@edu.univ-fcompte.fr Abstract This paper presents a rule-based approach for correcting grammatical errors made by non-native speakers of English. The approach relies on the differences in the outputs of two POS taggers. This paper is submitted in response to CoNLL-2014 Shared Task. 1 Introduction A part-of-speech (POS) tagger, like any other software, has a set of inputs and outputs. The input for a POS tagger is a group of words and a tagset, and the output is a POS tag for a word (Jurafsky and Martin, 2009). Given that a software is bound to provide incorrect output for an incorrect input (garbage in, garbage out), it is quite likely that taggers trained to tag grammatically correct sentences (the expected input) would not tag grammatically incorrect sentences properly. Furthermore, it is possible that the output of two different taggers for a given incorrect input would be different. For this shared task, the POS taggers used were the Stanford Parser, which was used to preprocess the training and test data (Ng et al., 2014) and the TreeTagger (Schmid, 1994). The Stanford Parser employs unlexica</context>
</contexts>
<marker>Jurafsky, Martin, 2009</marker>
<rawString>Daniel Jurafsky and James H. Martin. 2009. Part-ofSpeech Tagging. Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Meeting of the Association for Computational Linguistics.</booktitle>
<pages>423--430</pages>
<contexts>
<context position="1277" citStr="Klein and Manning, 2003" startWordPosition="197" endWordPosition="201">a software is bound to provide incorrect output for an incorrect input (garbage in, garbage out), it is quite likely that taggers trained to tag grammatically correct sentences (the expected input) would not tag grammatically incorrect sentences properly. Furthermore, it is possible that the output of two different taggers for a given incorrect input would be different. For this shared task, the POS taggers used were the Stanford Parser, which was used to preprocess the training and test data (Ng et al., 2014) and the TreeTagger (Schmid, 1994). The Stanford Parser employs unlexicalized PCFG1 (Klein and Manning, 2003), whereas the TreeTagger uses decision trees. The TreeTagger is freely available2, and its performance is comparable to that of the Stanford Log-Linear Part-of-Speech Tagger (Toutanova et al., 2003). Since the preprocessed dataset was already annotated with POS tags, the Stanford LogLinear POS Tagger was not used. If the annotation of preprocessed data differed from that of the TreeTagger, it was assumed that the sentence might have grammatical errors. Once an error was detected it was corrected using the 1Probabilistic Context-Free Grammar 2http://www.cis.uni-muenchen.de/⇠schmid/tools/TreeTag</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. Proceedings of the 41st Meeting of the Association for Computational Linguistics. 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
<author>Ted Briscoe</author>
<author>Christian Hadiwinoto</author>
<author>Raymond Hendy Susanto</author>
<author>Christopher Bryant</author>
</authors>
<date>2014</date>
<booktitle>The CoNLL-2014 Shared Task on Grammatical Error Correction. Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2014 Shared Task).</booktitle>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="1168" citStr="Ng et al., 2014" startWordPosition="182" endWordPosition="185">f words and a tagset, and the output is a POS tag for a word (Jurafsky and Martin, 2009). Given that a software is bound to provide incorrect output for an incorrect input (garbage in, garbage out), it is quite likely that taggers trained to tag grammatically correct sentences (the expected input) would not tag grammatically incorrect sentences properly. Furthermore, it is possible that the output of two different taggers for a given incorrect input would be different. For this shared task, the POS taggers used were the Stanford Parser, which was used to preprocess the training and test data (Ng et al., 2014) and the TreeTagger (Schmid, 1994). The Stanford Parser employs unlexicalized PCFG1 (Klein and Manning, 2003), whereas the TreeTagger uses decision trees. The TreeTagger is freely available2, and its performance is comparable to that of the Stanford Log-Linear Part-of-Speech Tagger (Toutanova et al., 2003). Since the preprocessed dataset was already annotated with POS tags, the Stanford LogLinear POS Tagger was not used. If the annotation of preprocessed data differed from that of the TreeTagger, it was assumed that the sentence might have grammatical errors. Once an error was detected it was </context>
</contexts>
<marker>Ng, Wu, Briscoe, Hadiwinoto, Susanto, Bryant, 2014</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher Bryant. 2014. The CoNLL-2014 Shared Task on Grammatical Error Correction. Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2014 Shared Task). Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>Proceedings of International Conference on New Methods in Language Processing.</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="1202" citStr="Schmid, 1994" startWordPosition="189" endWordPosition="190">is a POS tag for a word (Jurafsky and Martin, 2009). Given that a software is bound to provide incorrect output for an incorrect input (garbage in, garbage out), it is quite likely that taggers trained to tag grammatically correct sentences (the expected input) would not tag grammatically incorrect sentences properly. Furthermore, it is possible that the output of two different taggers for a given incorrect input would be different. For this shared task, the POS taggers used were the Stanford Parser, which was used to preprocess the training and test data (Ng et al., 2014) and the TreeTagger (Schmid, 1994). The Stanford Parser employs unlexicalized PCFG1 (Klein and Manning, 2003), whereas the TreeTagger uses decision trees. The TreeTagger is freely available2, and its performance is comparable to that of the Stanford Log-Linear Part-of-Speech Tagger (Toutanova et al., 2003). Since the preprocessed dataset was already annotated with POS tags, the Stanford LogLinear POS Tagger was not used. If the annotation of preprocessed data differed from that of the TreeTagger, it was assumed that the sentence might have grammatical errors. Once an error was detected it was corrected using the 1Probabilistic</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. Proceedings of International Conference on New Methods in Language Processing. Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-Rich Partof-Speech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<booktitle>Proceedings of HLT-NAACL 2003.</booktitle>
<pages>252--259</pages>
<contexts>
<context position="1475" citStr="Toutanova et al., 2003" startWordPosition="226" endWordPosition="229">ould not tag grammatically incorrect sentences properly. Furthermore, it is possible that the output of two different taggers for a given incorrect input would be different. For this shared task, the POS taggers used were the Stanford Parser, which was used to preprocess the training and test data (Ng et al., 2014) and the TreeTagger (Schmid, 1994). The Stanford Parser employs unlexicalized PCFG1 (Klein and Manning, 2003), whereas the TreeTagger uses decision trees. The TreeTagger is freely available2, and its performance is comparable to that of the Stanford Log-Linear Part-of-Speech Tagger (Toutanova et al., 2003). Since the preprocessed dataset was already annotated with POS tags, the Stanford LogLinear POS Tagger was not used. If the annotation of preprocessed data differed from that of the TreeTagger, it was assumed that the sentence might have grammatical errors. Once an error was detected it was corrected using the 1Probabilistic Context-Free Grammar 2http://www.cis.uni-muenchen.de/⇠schmid/tools/TreeTagger/ Nodebox English Linguistics library3 (De Bleser et al., 2002). 2 Error Detection The POS tag for each token in the data was compared with the tag given by the TreeTagger. Sentences were conside</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-Rich Partof-Speech Tagging with a Cyclic Dependency Network. Proceedings of HLT-NAACL 2003. 252–259.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
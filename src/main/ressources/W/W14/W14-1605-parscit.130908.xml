<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002721">
<title confidence="0.9956495">
Learning to Rank Answer Candidates
for Automatic Resolution of Crossword Puzzles
</title>
<author confidence="0.999551">
Gianni Barlacchi Massimo Nicosia and Alessandro Moschitti
</author>
<affiliation confidence="0.999887">
University of Trento Qatar Computing Research Institute
</affiliation>
<address confidence="0.533457">
38123 Povo (TN), Italy 5825 Doha, Qatar
</address>
<email confidence="0.994606">
gianni.barlacchi@gmail.com m.nicosia@gmail.com, amoschitti@qf.org.qa
</email>
<sectionHeader confidence="0.993842" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999246">
In this paper, we study the impact of rela-
tional and syntactic representations for an
interesting and challenging task: the au-
tomatic resolution of crossword puzzles.
Automatic solvers are typically based on
two answer retrieval modules: (i) a web
search engine, e.g., Google, Bing, etc. and
(ii) a database (DB) system for access-
ing previously resolved crossword puz-
zles. We show that learning to rank models
based on relational syntactic structures de-
fined between the clues and the answer can
improve both modules above. In particu-
lar, our approach accesses the DB using
a search engine and reranks its output by
modeling paraphrasing. This improves on
the MRR of previous system up to 53% in
ranking answer candidates and greatly im-
pacts on the resolution accuracy of cross-
word puzzles up to 15%.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999907949152543">
Crossword puzzles (CPs) are probably the most
popular language games played around the world.
It is very challenging for human intelligence as it
requires high level of general knowledge, logical
thinking, intuition and the ability to deal with am-
biguities and puns. CPs normally have the form
of a square or rectangular grid of white and black
shaded squares. The white squares on the border
of the grid or adjacent to the black ones are associ-
ated with clues. The goal of the game is to fill the
sequences of white squares with words answering
the clues.
There have been many attempts to build auto-
matic CP solving systems, which have also par-
ticipated in competitions such as The American
Crossword Puzzle Tournament (ACPT). This is
the oldest and largest CP tournament for cross-
word experts held in the United States. The goal
of such systems is to outperform human players
in solving crosswords more accurately and in less
time.
Automatic CP solvers have been mainly tar-
geted by the artificial intelligence (AI) community,
who has mostly focused on AI techniques for fill-
ing the puzzle grid, given a set of answer candi-
dates for each clue. The basic idea is to optimize
the overall probability of correctly filling the entire
grid by exploiting the likelihood of each candidate
answer, fulfilling at the same time the grid con-
straints. After several failures in approaching the
human expert performance, it has become clear
that designing more accurate solvers would not
have provided a winning system. In contrast, the
Precision and Recall of the answer candidates are
obviously a key factor: a very high value for both
of them would enable the solver to quickly find the
correct solution.
This basically suggests that, similarly to the
Jeopardy! challenge case (Ferrucci et al., 2010b),
the solution relies on Question Answering (QA)
research. However, although some CP clues are
rather similar to standard questions, as for ex-
ample, in the clue/answer pair: &lt;&lt;What keeps a
camera rolling?: dolly&gt;&gt;, some specific differences
hold: (i) clues can be in interrogative form or not,
e.g., &lt;&lt;Capital of USA: Washington~; (ii) they can
contain riddles or be deliberately ambiguous and
misleading (e.g., &lt;&lt;It’s green at first: orange&gt;&gt;);
(iii) the exact length of the answer keyword is
known in advance; and (vi) the confidence in the
answers is an extremely important input for the CP
solver.
In this paper, we study methods for improving
the quality of automatic extraction of answer can-
didate lists for automatic CP resolution. For this
purpose, we designed learning to rank models for
reordering the answers produced with two differ-
ent techniques typically used in CP systems: (i)
searching the Web with clue representations, e.g.,
</bodyText>
<page confidence="0.993347">
39
</page>
<note confidence="0.6884915">
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 39–48,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999406428571429">
exploiting Bing search engine1; and (ii) querying
the DB of previously resolved CP clues, e.g., using
standard SQL techniques.
We rerank the text snippets returned by Bing by
means of SVM preference ranking (Herbrich et al.,
2000) for improving the first technique. One in-
teresting contribution is that our model exploits a
syntactic representation of clues to improve Web
search. More in detail, we use structural kernels
(e.g., see (Moschitti, 2006; Moschitti, 2008)) in
SVMs applied to our syntactic representation of
pairs, formed by clues with their candidate snip-
pets. Regarding the DB approach, we provide a
completely novel solution by substituting it and
the SQL function with a search engine for retriev-
ing clues similar to the target one. Then, we rerank
the retrieved clues by applying SVMs and struc-
tural kernels to the syntactic representation of clue
pairs. This way, SVMs learn to choose the best
candidate among similar clues that are available in
the DB. The syntactic representation captures clue
paraphrasing properties.
In order to carry out our study, we created two
different corpora, one for each task: (i) a snip-
pets reranking dataset and (ii) a clue similarity
dataset. The first includes 21,000 clues, each asso-
ciated with 150 candidate snippets whereas the lat-
ter comprises 794,190 clues. These datasets con-
stitute interesting resources that we made available
to the research community2.
We compare our methods with one of the best
systems for automatic CP resolution, WebCrow
(Ernandes et al., 2005). Such system does use
the two approaches mentioned before. Regarding
snippet reranking, our structural models improve
on the basic approach of WebCrow based on Bing
by more than 4 absolute percent points in MRR,
for a relative improvement of 23%. Concerning
the similar clues retrieval, our methods improve
on the one used by WebCrow, based on DBs, by
25% absolute, i.e., about 53% of error reduction
whereas the answer accuracy at first position im-
proves up to 70%.
Given such promising results, we used our clue
reranking method in WebCrow, and obtained an
average improvement of 15% in resolving com-
plete CPs. This demonstrates that advanced QA
methods such as those based on syntactic struc-
tures and learning to rank methods can help to win
</bodyText>
<footnote confidence="0.986688666666667">
1https://www.bing.com/
2http://projects.disi.unitn.it/
iKernels/projects/webcrow/
</footnote>
<bodyText confidence="0.989216555555556">
the CP resolution challenge.
In the reminder of this paper, Sec. 2 introduces
the automatic CP resolution task in the context
of the related work, Sec. 3 introduces WebCrow,
Sec. 4 illustrates our models for snippets rerank-
ing and similar clue retrieval using kernel meth-
ods, syntactic structures, and traditional feature
vectors, Sec. 5 describes our experiments, and fi-
nally, Sec. 6 derives the conclusions.
</bodyText>
<sectionHeader confidence="0.999724" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999833425">
Proverb (Littman et al., 2002) was the first sys-
tem for the automatic resolution of CPs. It in-
cludes several modules for generating lists of can-
didate answers. These lists are merged and used to
solve a Probabilistic-Constraint Satisfaction Prob-
lem. Proverb relies on a very large crossword
database as well as several expert modules, each of
them mainly based on domain-specific databases
(e.g., movies, writers and geography). In addition,
it employs generic-word list generators and clue-
specific modules to find solutions for particular
kinds of clues like &lt;cTel (4): aviv ». Proverb’s
modules use many knowledge sources: databases
of clues, encyclopedias and Web documents. Dur-
ing the 1998 ACPT, Proverb placed 109th out of
251 contestants.
WebCrow (Ernandes et al., 2005) is based on
Proverb. It incorporates additional knowledge
sources, provides a solver for the Italian language
and improves the clues retrieval model from DB.
In particular, it enables partial matching to re-
trieve clues that do not perfectly overlap with the
query. WebCrow carries out basic linguistic anal-
ysis such as Part-Of-Speech tagging and lemma-
tization. It takes advantage of semantic relations
contained in WordNet, dictionaries and gazetteers.
Its Web module is constituted by a search en-
gine, which can retrieve text snippets or docu-
ments related to the clue. Answer candidates
and their confidence scores are generated from
this content. WebCrow uses a WA* algorithm
(Pohl, 1970) for Probabilistic-Constraint Satisfac-
tion Problems, adapted for CP resolution. The
solver fills the grid entries for which no solution
was found by the previous modules. It tries com-
binations of letters that satisfy the crossword con-
straints, where the letters are derived from words
found in dictionaries or in the generated candidate
lists. WebCrow participated in international com-
petitions with good results.
</bodyText>
<page confidence="0.998565">
40
</page>
<figureCaption confidence="0.999915">
Figure 1: Overview of WebCrow’s architecture.
</figureCaption>
<bodyText confidence="0.999351">
Dr. Fill (Ginsberg, 2011) targets the crossword
filling task with a Weighted-Constraint Satisfac-
tion Problem. Constraint violations are weighted
and can be tolerated. It heavily relies on huge
databases of clues. It was placed 92nd out of more
than 600 opponents in the 2013 ACPT.
Specifically for QA using syntactic structures,
a referring work for our research is the IBM Wat-
son system (Ferrucci et al., 2010a). This is an ad-
vanced QA pipeline based on deep linguistic pro-
cessing and semantic resources. It demonstrated
that automatic methods can be more accurate than
human experts in answering complex questions.
More traditional studies on passage reranking,
exploiting structural information, were carried out
in (Katz and Lin, 2003), whereas other meth-
ods explored soft matching (i.e., lexical similarity)
based on answer and named entity types (Aktolga
et al., 2011). (Radlinski and Joachims, 2006; Jeon
et al., 2005) applied question and answer classi-
fiers for passage reranking. In this context, sev-
eral approaches focused on reranking the answers
to definition/description questions, e.g., (Shen and
Lapata, 2007; Moschitti et al., 2007; Surdeanu et
al., 2008; Severyn and Moschitti, 2012; Severyn
et al., 2013b).
</bodyText>
<sectionHeader confidence="0.98234" genericHeader="method">
3 WebCrow Architecture
</sectionHeader>
<bodyText confidence="0.999916172413793">
Our research focuses on the generation of accurate
answer candidate lists, which, when used in a CP
resolution systems, can improve the overall solu-
tion accuracy. Therefore, the quality of our mod-
ules can be assessed by testing them within such
systems. For this purpose, we selected WebCrow
as it is rather modular, accurate and it was kindly
made available by the authors. Its architecture is
illustrated in Figure 1.
The solving process is divided in two phases:
in the first phase, the coordinator module forwards
the clues of an input CP to a set of modules for
the generation of several candidate answer lists.
Each module returns a list of possible solutions
for each clue. Such individual clue lists are then
merged by a specific Merger component, which
uses list confidence values and the probabilities of
correctness of each candidate in the lists. Eventu-
ally, a single list of candidate-probability pairs is
generated for each input clue. During the second
phase WebCrow fills the crossword grid by solving
a constraint-satisfaction problem. WebCrow se-
lects a single answer from each candidate merged
list, trying to satisfy the imposed constraints. The
goal of this phase is to find an admissible solution
maximizing the number of correct inserted words.
In this paper, we focus on two essential modules
of WebCrow: the Web and the DB modules, de-
scribed in the next sections.
</bodyText>
<subsectionHeader confidence="0.997863">
3.1 WebSearch Module (WSM)
</subsectionHeader>
<bodyText confidence="0.999986411764706">
WSM carries out four different tasks: (i) the re-
trieval of useful text snippets (TS) and web docu-
ments, (ii) the extraction of the answer candidates
from such text, (iii) the scoring/filtering of the can-
didates, and (iv) the estimation of the list confi-
dence. The retrieval of TS is performed by the
Bing search engine by simply providing it the clue
through its APIs. Then, the latter again are used
to access the retrieved TS. The word list gener-
ator extracts possible candidate answers from TS
or Web documents by picking the terms (also mul-
tiwords) of the correct length. The generated lists
are merged and sorted using the candidate confi-
dence computed by two filters: the statistical filter
and the morphological filter. The score associated
with each candidate word w is given by the fol-
lowing heuristic formula:
</bodyText>
<equation confidence="0.581459">
p(w, C) = k(scoresf(w, C) × scoremf(w, C)),
</equation>
<bodyText confidence="0.996012714285714">
where (i) C is the target clue, (ii) k is a
constant tuned on a validation set such that
Eni=0 p(wn, C) = 1, (iii) scoresf(w, C) is com-
puted using statistical information extracted from
the text, e.g., the classical TF×IDF, and (iv)
scoremf(w, C) is computed using morphological
features of w.
</bodyText>
<page confidence="0.995635">
41
</page>
<figure confidence="0.9913935">
S
REL-NP REL-NP VP NP
REL-NNP REL-POS TO VB CC RB TO VB NN
hamlet &apos;s to be or not to be addressee
S
NP VP REL-NP REL-NP VP VP NP VP PP NP ADVP PP NP PP NP
DT RBS JJ NN VBZ REL-NNP REL-POS TO VB CC RB TO VB DT NN VBN VBZ IN DT NN RB TO PRP TO DT NN
the most obvious example be hamlet &apos;s to be or not to be the monologue be address by a character either to himself to the audience
</figure>
<figureCaption confidence="0.9977">
Figure 2: Shallow syntactic trees of clue (upper) and snippet (lower) and their relational links.
</figureCaption>
<subsectionHeader confidence="0.995976">
3.2 Database module (CWDB)
</subsectionHeader>
<bodyText confidence="0.99998636">
The knowledge about previous CPs is essential for
solving new ones. Indeed, clues often repeat in
different CPs, thus the availability of a large DB
of clue-answer pairs allows for easily finding the
answers to previously used clues. In order to ex-
ploit the database of clue-answer pairs, WebCrow
uses three different modules:
CWDB-EXACT, which simply checks for an
exact matching between the target clue and those
in the DB. The score of the match is computed
using the number of occurrences of the matched
clue.
CWDB-PARTIAL, which employs MySQL’s
partial matching function, query expansion and
positional term distances to compute clue-
similarity scores, along with the Full-Text search
functions.
CWDB-DICTIO, which simply returns the full
list of words of correct length, ranked by their
number of occurrences in the initial list.
We improve WSM and CWDB by applying
learning-to-rank algorithms based on SVMs and
tree kernels applied to structural representations.
We describe our models in detail in the next sec-
tion.
</bodyText>
<sectionHeader confidence="0.771298" genericHeader="method">
4 Learning to rank with kernels
</sectionHeader>
<bodyText confidence="0.999949">
The basic architecture of our reranking framework
is relatively simple: it uses a standard preference
kernel reranking approach (e.g., see (Shen and
Joshi, 2005; Moschitti et al., 2006)). The struc-
tural kernel reranking framework is a specializa-
tion of the one we proposed in (Severyn and Mos-
chitti, 2012; Severyn et al., 2013b; Severyn et al.,
2013a). However, to tackle the novelty of the task,
especially for clue DB retrieval, we modeled inno-
vative kernels. In the following, we first describe
the general framework and then we instantiate it
for the two reranking tasks studied in this paper.
</bodyText>
<subsectionHeader confidence="0.964363">
4.1 Kernel framework
</subsectionHeader>
<bodyText confidence="0.999990392857143">
The framework takes a textual query and retrieves
a list of related text candidates using a search en-
gine (applied to the Web or a DB), according to
some similarity criteria. Then, the query and can-
didates are processed by an NLP pipeline. The
pipeline is based on the UIMA framework (Fer-
rucci and Lally, 2004) and contains many text
analysis components. The latter used for our spe-
cific tasks are: the tokenizer3, sentence detector1,
lemmatizer1, part-of-speech (POS) tagger1, chun-
ker4 and stopword marker5.
The annotations produced by such processors
are used by additional components to produce
structural models representing clues and TS. The
structure component converts the text fragments
into trees. We use both trees and feature vectors
to represent pairs of clues and TS, which are em-
ployed to train kernel-based rerankers for reorder-
ing the candidate lists provided by a search engine.
Since the syntactic parsing accuracy can impact
the quality of our structure and thus the accuracy
of our learning to rank algorithms, we preferred
to use shallow syntactic trees over full syntactic
representations. In the next section, we first de-
scribe the structures we used in our kernels, then
the tree kernels used as building blocks for our
models. Finally, we show the reranking models
for both tasks, TS and clue reranking.
</bodyText>
<footnote confidence="0.9989938">
3http://nlp.stanford.edu/software/
corenlp.shtml
4http://cogcomp.cs.illinois.edu/page/
software_view/13
5Based on a standard stoplist.
</footnote>
<page confidence="0.992166">
42
</page>
<table confidence="0.804543166666667">
Rank Clue Answer
1 Kind of support for a computer user tech
2 Kind of computer connection wifi
3 Computer connection port
4 Comb users bees
5 Traveling bag grip
</table>
<tableCaption confidence="0.9618565">
Table 1: Clue ranking for the query: Kind of con-
nection for traveling computer users (wifi)
</tableCaption>
<subsectionHeader confidence="0.942747">
4.2 Relational shallow tree representation
</subsectionHeader>
<bodyText confidence="0.999892708333333">
The structures we adopt are similar to those de-
fined in (Severyn et al., 2013b). They are essen-
tially shallow syntactic trees built from POS tags
grouped into chunks. Each clue and its answer
candidate (either a TS or clue) are encoded into
a tree having word lemmas at the leaves and POS
tags as pre-terminals. The higher tree level orga-
nizes POS tags into chunks. For example, the up-
per tree of Figure 2, shows a shallow tree for the
clue: Hamlet’s ”To be, or not to be” addressee,
whereas the lower tree represents a retrieved TS
containing the answer, himself: The most obvious
example is Hamlet’s ”To be or not to be ... the
monologue is addressed by a character either to
himself or to the audience.
Additionally, we use a special REL tag to link
the clue/snippet trees above such that structural re-
lations will be captured by tree fragments. The
links are established as follows: words from a
clue and a snippet sharing a lemma get their par-
ents (POS tags) and grandparents, i.e., chunk la-
bels, marked by a prepending REL tag. We build
such structural representations for both snippet
and similar clue reranking tasks.
</bodyText>
<subsectionHeader confidence="0.997987">
4.3 Tree kernels
</subsectionHeader>
<bodyText confidence="0.999467571428572">
We briefly report the different types of kernels
(see, e.g., (Moschitti, 2006) for more details).
Syntactic Tree Kernel (STK), also known as a
subset tree kernel (Collins and Duffy, 2002), maps
objects in the space of all possible tree fragments
constrained by the rule that the sibling nodes from
their parents cannot be separated. In other words,
substructures are composed by atomic building
blocks corresponding to nodes along with all of
their direct children. These, in case of a syntac-
tic parse tree, are complete production rules of the
associated parser grammar.
STKb extends STK by allowing leaf nodes to be
part of the feature space. Leaf in syntactic trees are
words, from this the subscript b (bag-of-words).
Subtree Kernel (SbtK) is one of the simplest tree
kernels as it only generates complete subtrees, i.e.,
tree fragments that, given any arbitrary starting
node, necessarily include all its descendants.
Partial Tree Kernel (PTK) (Moschitti, 2006) can
be effectively applied to both constituency and de-
pendency parse trees. It generates all possible
connected tree fragments, e.g., sibling nodes can
also be separated and be part of different tree frag-
ments. In other words, a fragment is any possible
tree path, from whose nodes other tree paths can
depart. Thus, it can generate a very rich feature
space resulting in higher generalization ability.
</bodyText>
<subsectionHeader confidence="0.995695">
4.4 Snippet reranking
</subsectionHeader>
<bodyText confidence="0.99964775">
The task of snippet reranking consists in reorder-
ing the list of snippets retrieved from the search
engine such that those containing the correct an-
swer can be pushed at the top of the list. For this
purpose, we transform the target clue in a search
query and retrieve candidate text snippets. In our
training set, these candidate text snippets are con-
sidered as positive examples if they contain the an-
swer to the target clue.
We rerank snippets using preference reranking
approach (see, e.g., (Shen and Joshi, 2005)). This
means that two snippets are compared to derive
which one is the best, i.e., which snippet contains
the answer with higher probability. Since we aim
at using kernel methods, we apply the following
preference kernel:
</bodyText>
<equation confidence="0.9917465">
PK((s1, s2), (s01, s02)) = K(s1, s01)+
+K(s2, s02) − K(s1, s02) − K(s2, s01),
</equation>
<bodyText confidence="0.999962">
where sr and s0 r refer to two sets of candidates
associated with two rankings and K is a kernel
applied to pairs of candidates. We represent the
latter as pairs of clue and snippet trees. More for-
mally, given two candidates, si = (si(c), si(s))
and s0i = (s0i(c), s0i(s)), whose members are the
clue and snippet trees, we define
</bodyText>
<equation confidence="0.52796">
K(si, s0i) = TK(si(c), s0i(c))+TK(si(s), s0i(s)),
</equation>
<bodyText confidence="0.999960166666667">
where TK can be any tree kernel function, e.g.,
STK or PTK. Finally, it should be noted that, to
add traditional feature vectors to the reranker, it is
enough to add the product (xs1 −xs2) (�xs, 1 −xs,2)
to the structural kernel PK, where Ys is the feature
vector associated with the snippet s.
</bodyText>
<page confidence="0.998877">
43
</page>
<table confidence="0.944914375">
S
REL-NP PP REL-NP PP REL-NP
REL-NNP IN REL-NN IN VBG REL-NN NNS
kind of connection for travel computer user
S
REL-NP PP REL-NP
REL-NNP IN REL-NN REL-NN
kind of computer connection
</table>
<figureCaption confidence="0.956332">
Figure 3: Two similar clues leading to the same answer.
</figureCaption>
<subsectionHeader confidence="0.960576">
4.5 Similar clue reranking
</subsectionHeader>
<bodyText confidence="0.999492230769231">
WebCrow creates answer lists by retrieving clues
from the DB of previously solved crosswords. It
simply uses the classical SQL operator and full-
text search. We instead verified the hypothesis
that a search engine could achieve a better re-
sult. Thus we opted for indexing the DB clues
and their answers with the open source search en-
gine Lucene (McCandless et al., 2010), using the
state-of-the-art BM25 retrieval model. This alone
significantly improved the quality of the retrieved
clue list, which could be further refined by apply-
ing reranking. The latter consists in (i) retrieving
a list of similar clues using a search engine and
(ii) moving those more similar, which more prob-
ably contain the same answer to the clue query,
at the top. For example, Table 1 shows the first
five clues, retrieved for a query built from the clue:
Kind of connection for traveling computer users.
The search engine retrieves the wrong clue, Kind
of connection for traveling computer users, at the
top since it overlaps more with the query.
To solve these kinds of problems by also en-
hancing the generalization power of our reranking
algorithm, we use a structural representation
similar to the one for TS that we illustrated in
the previous section. The main difference with
the previous models is that the reranking pair is
only constituted by clues. For example, Fig. 3
shows the representation of the pairs constituted
by the query clue and the correct clue ranked
in the second position (see Table 1). The rela-
tional arrows suggest a syntactic transformation
from connection for * computer to
computer connection, which can be used
by the reranker to prefer the correct clue to
the wrong one. Note that such transformation
corresponds to the pair of tree fragments: [S
[REL-NP[REL-NN]][PP][NP[VBG][REL-NN]]]
→ [S [REL-NP[REL-NN][REL-NN]]], where the
node pairs, REL-NN, REL-NN) define the arguments
of the syntactic transformation. Such fragments
can be generated by PTK, which can thus be used
for learning clue paraphrasing.
To build the reranking training set, we used
the training clues for querying the search engine,
which draws candidates from the indexed clues.
We stress the fact that this set of clues is disjoint
from the clues in the training and test sets. Thus,
identical clues are not present across sets. At clas-
sification time, the new clue is used as a search
query. Similar candidate clues are retrieved and
used to form pairs.
</bodyText>
<subsectionHeader confidence="0.972458">
4.6 Feature Vectors
</subsectionHeader>
<bodyText confidence="0.907902161290323">
In addition to structural representations, we also
used features for capturing the degrees of similar-
ity between clues within a pair.
DKPro Similarity. We used similarity features
from a top performing system in the Semantic
Textual Similarity (STS) task, namely DKPro
from the UKP Lab (B¨ar et al., 2013). These
features were effective in predicting the degree
of similarity between two sentences. DKPro in-
cludes the following syntactic similarity metrics,
operating on string sequences, and more advanced
semantic similarities:
– Longest common substring measure (Gusfield,
1997). It determines the length of the longest
substring shared by two text segments.
– Longest common subsequence measure (Allison
and Dix, 1986). It extends the notion of substrings
to word subsequences by applying word insertions
or deletions to the original input text pairs.
– Running-Karp-Rabin Greedy String Tiling
(Wise, 1996). It provides a similarity between two
sentences by counting the number of shuffles in
their subparts.
– Resnik similarity (Resnik, 1995). The WordNet
hypernymy hierarchy is used to compute a mea-
sure of semantic relatedness between concepts
expressed in the text. The aggregation algorithm
by Mihalcea et al. (Mihalcea et al., 2006) is
applied to extend the measure from words to
sentences.
– Explicit Semantic Analysis (ESA) similarity
</bodyText>
<page confidence="0.998327">
44
</page>
<bodyText confidence="0.992981821428571">
(Gabrilovich and Markovitch, 2007). It represents
documents as weighted vectors of concepts
learned from Wikipedia, WordNet and Wik-
tionary.
– Lexical Substitution (Biemann, 2013). A super-
vised word sense disambiguation system is used
to substitute a wide selection of high-frequency
English nouns with generalizations. Resnik
and ESA features are then computed on the
transformed text.
New features. Hereafter, we describe new fea-
tures that we designed for CP reranking tasks.
– Feasible Candidate. It is a binary feature sig-
naling the presence or absence of words with the
same length of the clue answer (only used for snip-
pet reranking).
– Term overlap features. They compute the co-
sine similarity of text pairs encoded into sets of n-
grams extracted from different text features: sur-
face forms of words, lemmas and POS-tags. They
are computed keeping and removing stop-words.
They complement DKPro features.
– Kernel similarities. These are computed using
(i) string kernels applied to sentences, or PTK ap-
plied to structural representations with and with-
out embedded relational information (REL). This
similarity is computed between the members of a
(clue, snippet) or a (clue, clue) pair.
</bodyText>
<sectionHeader confidence="0.999423" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999990727272727">
Our experiments aim at demonstrating the effec-
tiveness of our models on two different tasks: (i)
Snippet Reranking and (ii) Similar Clue Retrieval
(SCR). Additionally, we measured the impact of
our best model for SCR in the WebCrow system
by comparing with it. Our referring database of
clues is composed by 1,158,202 clues, which be-
long to eight different crossword editors (down-
loaded from the Web6). We use the latter to create
one dataset for snippet reranking and one dataset
for clues retrieval.
</bodyText>
<subsectionHeader confidence="0.984724">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9999862">
To train our models, we adopted SVM-light-TK7,
which enables the use of structural kernels (Mos-
chitti, 2006) in SVM-light (Joachims, 2002), with
default parameters. We applied a polynomial ker-
nel of degree 3 to the explicit feature vectors,
</bodyText>
<footnote confidence="0.969735">
6http://www.crosswordgiant.com
7http://disi.unitn.it/moschitti/
Tree-Kernel.htm
</footnote>
<table confidence="0.9999316875">
Model MAP MRR AvgRec REC@1 REC@5
Bing 16.00 18.09 69.00 12.50 24.80
V 18.00 19.88 76.00 14.20 26.10
SbtK 17.00 19.6 75.00 13.80 26.40
STK 18.00 20.44 76.00 15.10 27.00
STKb 18.00 20.68 76.00 15.30 27.40
PTK 19.00 21.65 77.00 16.10 28.70
V+SbtK 20.00 22.39 80.00 17.20 29.10
V+STK 19.00 20.82 78.00 14.90 27.90
STKb 19.00 21.20 79.00 15.60 28.40
V+PTK 19.00 21.68 79.00 16.00 29.40
V+DK 18.00 20.48 77.00 14.60 26.80
V+DK+SbtK 20.00 22.29 80.00 16.90 28.70
V+DK+STK 19.00 21.47 79.00 15.50 28.30
V+DK+STKb 19.00 21.58 79.00 15.4 28.60
V+DK+PTK 20.00 22.24 80.00 16.80 29.30
</table>
<tableCaption confidence="0.779058">
Table 2: Snippet reranking
</tableCaption>
<table confidence="0.99997275">
Model MAP MRR AvgRec REC@1 REC@5
MB25 69.00 73.78 80.00 62.11 81.23
WebCrow - 53.22 58.00 39.60 62.85
SbtK 52.00 54.72 69.00 36.50 64.05
STK 63.00 68.21 77.00 54.57 76.11
STKb 63.00 67.68 77.00 53.85 75.63
PTK 65.00 70.12 78.00 57.39 77.65
V+SbtK 68.00 73.26 80.00 60.95 81.28
V+STK 71.00 76.01 82.00 64.58 83.95
V+STKb 70.00 75.68 82.00 63.95 83.77
V+PTK 71.00 76.67 82.00 65.67 84.07
V+DK 71.00 76.76 81.00 65.55 84.29
V+DK+SbtK 72.00 76.91 82.00 65.87 84.51
V+DK+STK 73.00 78.37 84.00 67.83 85.87
V+DK+STKb 73.00 78.29 84.00 67.71 85.77
V+DK+PTK 73.00 78.13 83.00 67.39 85.75
</table>
<tableCaption confidence="0.999936">
Table 3: Reranking of similar clues.
</tableCaption>
<bodyText confidence="0.999993222222222">
as we believe feature combinations can be valu-
able. To measure the impact of the rerankers as
well as the baselines, we used well known met-
rics for assessing the accuracy of QA and re-
trieval systems, i.e.: Recall at rank 1 (R@1 and
5), Mean Reciprocal Rank (MRR), Mean Average
Precision (MAP), the average Recall (AvgRec).
R@k is the percentage of questions with a cor-
rect answer ranked at the first position. MRR is
computed as follows: MRR = |Q |Eq=1 rank(q)
where rank(q) is the position of the first correct an-
swer in the candidate list. For a set of queries Q,
MAP is the mean over the average precision scores
for each query: Q EQ1 AveP(q). AvgRec and
all the measures are evaluated on the first 10 re-
trieved snippets/clues. For training and testing the
reranker, only the first 10 snippets/clues retrieved
by the search engine are used.
</bodyText>
<subsectionHeader confidence="0.999114">
5.2 Snippet Reranking
</subsectionHeader>
<bodyText confidence="0.9975015">
The retrieval from the Web is affected by a sig-
nificant query processing delay, which prevents us
</bodyText>
<page confidence="0.998364">
45
</page>
<bodyText confidence="0.999957815789474">
to use entire documents. Thus, we only consid-
ered the text from Bing snippets. Moreover, since
our reranking approach does not include the treat-
ment of special clues such as anagrams or linguis-
tic games, e.g., fill-in-the blank clues, we have ex-
cluded them by our dataset. We crawled the lat-
ter from the Web. We converted each clue into a
query and downloaded the first 10 snippets as re-
sult of a Bing query. In order to reduce noise from
the data, we created a black list containing URLs
that must not be considered in the download phase,
e.g., crossword websites. The training set is com-
posed by 20,000 clues while the test set comprises
1,000 clues.
We implemented and compared many models
for reranking the correct snippets higher, i.e., con-
taining the answer to the clue. The compared sys-
tems are listed on the first column of Table 2,
where: V is the approach using the vector only
constituted by the new feature set (see Sec. 4.6);
DK is the model using the features made available
by DKPro; the systems ending in TK are described
in Sec. 4.3; and the plus operator indicates models
obtained by summing the related kernels.
Depending on the target measure they suggest
slightly different findings. Hereafter, we comment
on MRR as it is the most interesting from a rank-
ing viewpoint. We note that: (i) Bing is improved
by the reranker based on the new feature vector by
2 absolute points; (ii) DK+V improves on V by
just half point; (iii) PTK provides the highest re-
sult among individual systems; (iv) combinations
improve on the individual systems; and (v) over-
all, our reranking improves on the ranking of para-
graphs of Bing by 4 points in MRR and 5 points
in accuracy on the first candidate (REC@1), cor-
responding to about 20% and 50% of relative im-
provement and error reduction, respectively.
</bodyText>
<subsectionHeader confidence="0.99774">
5.3 Similar clue retrieval
</subsectionHeader>
<bodyText confidence="0.999800454545455">
We compiled a crossword database of 794,190
unique pairs of clue-answer. Using the clues con-
tained in this set, we created three different sets:
training and test sets and the database of clues.
The database of clues can be indexed for retriev-
ing similar clues. It contains 700,000 unique clue-
answer pairs. The training set contains 39,504
clues whose answer may be found in database. Us-
ing the same approach, we created a test set con-
taining 5,060 clues that (i) are not in the training
set and (ii) have at least an answer in the database.
</bodyText>
<table confidence="0.995629333333333">
Model MRR REC@1 REC@5 REC@10
WebCrow 41.00 33.00 51.00 58.00
Our Model 46.00 39.00 56.00 59.00
</table>
<tableCaption confidence="0.993624">
Table 4: Performance on the word list candidates
averaged over the clues of 10 entire CPs
</tableCaption>
<table confidence="0.989487">
Model %Correct words %Correct letters
WebCrow 34.45 49.72
Our Model 39.69 54.30
</table>
<tableCaption confidence="0.937778">
Table 5: Performance given in terms of correct
words and letters averaged on the 10 CPs
</tableCaption>
<bodyText confidence="0.9999599">
We experimented with all models, as in the
previous section, trained for the similar clue re-
trieval task. However, since WebCrow includes a
database module, in Tab. 3, we have an extra row
indicating its accuracy. We note that: (i) BM25
shows a very accurate MRR, 73.78%. It largely
improves on WebCrow by about 20.5 absolute per-
cent points, demonstrating the superiority of an IR
approach over DB methods. (ii) All TK types do
not improve alone on BM25, this happens since
they do not exploit the initial rank provided by
BM25. (iii) All the feature vector and TK combi-
nations achieve high MRR, up to 4.5 absolute per-
cent points of improvement over BM25 and thus
25 points more than WebCrow, corresponding to
53% of error reduction. Finally, (iv) the relative
improvement on REC@1 is up to 71% (28.23%
absolute). This high result is promising in the light
of improving WebCrow for the end task of solving
complete CPs.
</bodyText>
<subsectionHeader confidence="0.985925">
5.4 Impact on WebCrow
</subsectionHeader>
<bodyText confidence="0.998958466666667">
In these experiments, we used our reranking
model of similar clues (more specifically, the
V+DK+STK model) using 10 complete CPs (for
a total of 760 clues) from the New York Times
and Washington Post. This way, we could mea-
sure the impact of our model on the complete task
carried out by WebCrow. More specifically, we
give our reranked list of answers to WebCrow in
place of the list it would have extracted with the
CWDB module. It should be noted that to evalu-
ate the impact of our list, we disabled WebCrow
access to other lists, e.g., dictionaries. This means
that the absolute resolution accuracy of WebCrow
using our and its own lists can be higher (see (Er-
nandes et al., 2008) for more details).
</bodyText>
<page confidence="0.997709">
46
</page>
<bodyText confidence="0.999988846153846">
The first result that we derive is the accuracy
of the answer list produced from the new data,
i.e., constituted by the 10 entire CPs. The results
are reported in Tab. 4. We note that the improve-
ment of our model is lower than before as a non-
negligible percentage of clues are not solved us-
ing the clue DB. However, when we compute the
accuracy in solving the complete CPs, the impact
is still remarkable as reported by Tab. 5. Indeed,
the results show that when the lists reordered by
our reranker are used by WebCrow, the latter im-
proves by more than 5 absolute percent points in
both word and character accuracy.
</bodyText>
<sectionHeader confidence="0.999747" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9997325">
In this paper, we improve automatic CP resolution
by modeling two innovative reranking tasks for:
</bodyText>
<listItem confidence="0.790242">
(i) CP answer list derived from Web search and
(ii) CP clue retrieval from clue DBs.
</listItem>
<bodyText confidence="0.999970555555555">
Our rankers are based on SVMs and structural
kernels, where the latter are applied to robust shal-
low syntactic structures. Our model applied to
clue reranking is very interesting as it allows us
to learn clue paraphrasing by exploiting relational
syntactic structures representing pairs of clues.
For our study, we created two different corpora
for Snippet Reranking Dataset and Clue Similarity
Dataset on which we tested our methods. The lat-
ter improve on the lists generated by WebCrow by
25 absolute percent points in MRR (about 53% of
relative improvement). When such improved lists
are used in WebCrow, its resolution accuracy in-
creases by 15%, demonstrating that there is a large
room for improvement in automatic CP resolution.
In the future, we would like to add more seman-
tic information to our rerankers and include an an-
swer extraction component in the pipeline.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995898">
We are deeply in debt with Marco Gori and
Marco Ernandes for making available WebCrow,
for helping us with their system and for the useful
technical discussion regarding research directions.
This research has been partially supported by the
EC’s Seventh Framework Programme (FP7/2007-
2013) under the grants #288024: LIMOSINE
– Linguistically Motivated Semantic aggregation
engiNes. Many thanks to the anonymous review-
ers for their valuable work.
</bodyText>
<sectionHeader confidence="0.987267" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995139923076923">
Elif Aktolga, James Allan, and David A. Smith. 2011.
Passage reranking for question answering using syn-
tactic structures and answer types. In ECIR.
L Allison and T I Dix. 1986. A bit-string longest-
common-subsequence algorithm. Inf. Process. Lett.,
23(6):305–310, December.
Daniel B¨ar, Torsten Zesch, and Iryna Gurevych. 2013.
Dkpro similarity: An open source framework for
text similarity. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (System Demonstrations) (ACL 2013),
pages 121–126, Stroudsburg, PA, USA, August. As-
sociation for Computational Linguistics.
Chris Biemann. 2013. Creating a system for lexi-
cal substitutions from scratch using crowdsourcing.
Lang. Resour. Eval., 47(1):97–122, March.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, ACL ’02, pages 263–
270, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Marco Ernandes, Giovanni Angelini, and Marco Gori.
2005. Webcrow: A web-based system for crossword
solving. In In Proc. of AAAI 05, pages 1412–1417.
Menlo Park, Calif., AAAI Press.
Marco Ernandes, Giovanni Angelini, and Marco Gori.
2008. A web-based agent challenges human experts
on crosswords. AI Magazine, 29(1).
David Ferrucci and Adam Lally. 2004. Uima: An
architectural approach to unstructured information
processing in the corporate research environment.
Nat. Lang. Eng., 10(3-4):327–348, September.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John
Prager, Nico Schlaefer, and Chris Welty. 2010a.
Building watson: An overview of the deepqa
project. AI Magazine, 31(3).
David A. Ferrucci, Eric W. Brown, Jennifer Chu-
Carroll, James Fan, David Gondek, Aditya Kalyan-
pur, Adam Lally, J. William Murdock, Eric Nyberg,
John M. Prager, Nico Schlaefer, and Christopher A.
Welty. 2010b. Building watson: An overview of the
deepqa project. AI Magazine, 31(3):59–79.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artifical
Intelligence, IJCAI’07, pages 1606–1611, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
</reference>
<page confidence="0.999648">
47
</page>
<bodyText confidence="0.974345166666667">
Filip Radlinski and Thorsten Joachims. 2006. Query
chains: Learning to rank from implicit feedback.
CoRR.
Matthew L. Ginsberg. 2011. Dr.fill: Crosswords and
an implemented solver for singly weighted csps. J.
Artif. Int. Res., 42(1):851–886, September.
</bodyText>
<reference confidence="0.996785166666667">
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences: Computer Science and Computational
Biology. Cambridge University Press, New York,
NY, USA.
R Herbrich, T Graepel, and K Obermayer. 2000. Large
margin rank boundaries for ordinal regression. In
A.J. Smola, P.L. Bartlett, B. Sch¨olkopf, and D. Schu-
urmans, editors, Advances in Large Margin Classi-
fiers, pages 115–132, Cambridge, MA. MIT Press.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In CIKM.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of the
Eighth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’02,
pages 133–142, New York, NY, USA. ACM.
Boris Katz and Jimmy Lin. 2003. Selectively using re-
lations to improve precision in question answering.
Michael L. Littman, Greg A. Keim, and Noam Shazeer.
2002. A probabilistic approach to solving crossword
puzzles. Artificial Intelligence, 134(12):23 – 55.
Michael McCandless, Erik Hatcher, and Otis Gospod-
netic. 2010. Lucene in Action, Second Edition:
Covers Apache Lucene 3.0. Manning Publications
Co., Greenwich, CT, USA.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceedings
of the 21st National Conference on Artificial Intelli-
gence - Volume 1, AAAI’06, pages 775–780. AAAI
Press.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree kernel
joint inference. In Proceedings of CoNLL-X, New
York City.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for ques-
tion/answer classification. In ACL.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In ECML, pages 318–329.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Ira Pohl. 1970. Heuristic search viewed as path finding
in a graph. Artificial Intelligence, 1(34):193 – 204.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In
Proceedings of the 14th International Joint Confer-
ence on Artificial Intelligence - Volume 1, IJCAI’95,
pages 448–453, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In Proceedings of the 35th in-
ternational ACM SIGIR conference on Research and
development in information retrieval (SIGIR), pages
741–750. ACM.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Building structures from clas-
sifiers for passage reranking. In CIKM, pages 969–
978.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning adaptable patterns for
passage reranking. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, pages 75–83, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Libin Shen and Aravind K. Joshi. 2005. Ranking
and reranking with perceptron. Machine Learning,
60(1-3):73–96.
D. Shen and M. Lapata. 2007. Using semantic roles to
improve question answering. In EMNLP-CoNLL.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online QA collec-
tions. In Proceedings ofACL-HLT.
Michael J. Wise. 1996. Yap3: Improved detection
of similarities in computer program and other texts.
In Proceedings of the Twenty-seventh SIGCSE Tech-
nical Symposium on Computer Science Education,
SIGCSE ’96, pages 130–134, New York, NY, USA.
ACM.
</reference>
<page confidence="0.999339">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.640968">
<title confidence="0.9994545">Learning to Rank Answer for Automatic Resolution of Crossword Puzzles</title>
<author confidence="0.999899">Gianni Barlacchi Massimo Nicosia Moschitti</author>
<affiliation confidence="0.999998">University of Trento Qatar Computing Research Institute</affiliation>
<address confidence="0.999282">38123 Povo (TN), Italy 5825 Doha, Qatar</address>
<email confidence="0.997386">gianni.barlacchi@gmail.comm.nicosia@gmail.com,amoschitti@qf.org.qa</email>
<abstract confidence="0.981824904761905">In this paper, we study the impact of relational and syntactic representations for an interesting and challenging task: the automatic resolution of crossword puzzles. Automatic solvers are typically based on two answer retrieval modules: (i) a web search engine, e.g., Google, Bing, etc. and (ii) a database (DB) system for accessing previously resolved crossword puzzles. We show that learning to rank models based on relational syntactic structures defined between the clues and the answer can improve both modules above. In particular, our approach accesses the DB using a search engine and reranks its output by modeling paraphrasing. This improves on the MRR of previous system up to 53% in ranking answer candidates and greatly impacts on the resolution accuracy of crossword puzzles up to 15%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Elif Aktolga</author>
<author>James Allan</author>
<author>David A Smith</author>
</authors>
<title>Passage reranking for question answering using syntactic structures and answer types.</title>
<date>2011</date>
<booktitle>In ECIR.</booktitle>
<contexts>
<context position="9634" citStr="Aktolga et al., 2011" startWordPosition="1518" endWordPosition="1521">opponents in the 2013 ACPT. Specifically for QA using syntactic structures, a referring work for our research is the IBM Watson system (Ferrucci et al., 2010a). This is an advanced QA pipeline based on deep linguistic processing and semantic resources. It demonstrated that automatic methods can be more accurate than human experts in answering complex questions. More traditional studies on passage reranking, exploiting structural information, were carried out in (Katz and Lin, 2003), whereas other methods explored soft matching (i.e., lexical similarity) based on answer and named entity types (Aktolga et al., 2011). (Radlinski and Joachims, 2006; Jeon et al., 2005) applied question and answer classifiers for passage reranking. In this context, several approaches focused on reranking the answers to definition/description questions, e.g., (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Severyn and Moschitti, 2012; Severyn et al., 2013b). 3 WebCrow Architecture Our research focuses on the generation of accurate answer candidate lists, which, when used in a CP resolution systems, can improve the overall solution accuracy. Therefore, the quality of our modules can be assessed by testin</context>
</contexts>
<marker>Aktolga, Allan, Smith, 2011</marker>
<rawString>Elif Aktolga, James Allan, and David A. Smith. 2011. Passage reranking for question answering using syntactic structures and answer types. In ECIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Allison</author>
<author>T I Dix</author>
</authors>
<title>A bit-string longestcommon-subsequence algorithm.</title>
<date>1986</date>
<journal>Inf. Process. Lett.,</journal>
<volume>23</volume>
<issue>6</issue>
<contexts>
<context position="24000" citStr="Allison and Dix, 1986" startWordPosition="3913" endWordPosition="3916">rity between clues within a pair. DKPro Similarity. We used similarity features from a top performing system in the Semantic Textual Similarity (STS) task, namely DKPro from the UKP Lab (B¨ar et al., 2013). These features were effective in predicting the degree of similarity between two sentences. DKPro includes the following syntactic similarity metrics, operating on string sequences, and more advanced semantic similarities: – Longest common substring measure (Gusfield, 1997). It determines the length of the longest substring shared by two text segments. – Longest common subsequence measure (Allison and Dix, 1986). It extends the notion of substrings to word subsequences by applying word insertions or deletions to the original input text pairs. – Running-Karp-Rabin Greedy String Tiling (Wise, 1996). It provides a similarity between two sentences by counting the number of shuffles in their subparts. – Resnik similarity (Resnik, 1995). The WordNet hypernymy hierarchy is used to compute a measure of semantic relatedness between concepts expressed in the text. The aggregation algorithm by Mihalcea et al. (Mihalcea et al., 2006) is applied to extend the measure from words to sentences. – Explicit Semantic A</context>
</contexts>
<marker>Allison, Dix, 1986</marker>
<rawString>L Allison and T I Dix. 1986. A bit-string longestcommon-subsequence algorithm. Inf. Process. Lett., 23(6):305–310, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Dkpro similarity: An open source framework for text similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (System Demonstrations) (ACL 2013),</booktitle>
<pages>121--126</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA,</location>
<marker>B¨ar, Zesch, Gurevych, 2013</marker>
<rawString>Daniel B¨ar, Torsten Zesch, and Iryna Gurevych. 2013. Dkpro similarity: An open source framework for text similarity. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (System Demonstrations) (ACL 2013), pages 121–126, Stroudsburg, PA, USA, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Creating a system for lexical substitutions from scratch using crowdsourcing.</title>
<date>2013</date>
<journal>Lang. Resour. Eval.,</journal>
<volume>47</volume>
<issue>1</issue>
<contexts>
<context position="24806" citStr="Biemann, 2013" startWordPosition="4036" endWordPosition="4037"> provides a similarity between two sentences by counting the number of shuffles in their subparts. – Resnik similarity (Resnik, 1995). The WordNet hypernymy hierarchy is used to compute a measure of semantic relatedness between concepts expressed in the text. The aggregation algorithm by Mihalcea et al. (Mihalcea et al., 2006) is applied to extend the measure from words to sentences. – Explicit Semantic Analysis (ESA) similarity 44 (Gabrilovich and Markovitch, 2007). It represents documents as weighted vectors of concepts learned from Wikipedia, WordNet and Wiktionary. – Lexical Substitution (Biemann, 2013). A supervised word sense disambiguation system is used to substitute a wide selection of high-frequency English nouns with generalizations. Resnik and ESA features are then computed on the transformed text. New features. Hereafter, we describe new features that we designed for CP reranking tasks. – Feasible Candidate. It is a binary feature signaling the presence or absence of words with the same length of the clue answer (only used for snippet reranking). – Term overlap features. They compute the cosine similarity of text pairs encoded into sets of ngrams extracted from different text featur</context>
</contexts>
<marker>Biemann, 2013</marker>
<rawString>Chris Biemann. 2013. Creating a system for lexical substitutions from scratch using crowdsourcing. Lang. Resour. Eval., 47(1):97–122, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17855" citStr="Collins and Duffy, 2002" startWordPosition="2898" endWordPosition="2901">Additionally, we use a special REL tag to link the clue/snippet trees above such that structural relations will be captured by tree fragments. The links are established as follows: words from a clue and a snippet sharing a lemma get their parents (POS tags) and grandparents, i.e., chunk labels, marked by a prepending REL tag. We build such structural representations for both snippet and similar clue reranking tasks. 4.3 Tree kernels We briefly report the different types of kernels (see, e.g., (Moschitti, 2006) for more details). Syntactic Tree Kernel (STK), also known as a subset tree kernel (Collins and Duffy, 2002), maps objects in the space of all possible tree fragments constrained by the rule that the sibling nodes from their parents cannot be separated. In other words, substructures are composed by atomic building blocks corresponding to nodes along with all of their direct children. These, in case of a syntactic parse tree, are complete production rules of the associated parser grammar. STKb extends STK by allowing leaf nodes to be part of the feature space. Leaf in syntactic trees are words, from this the subscript b (bag-of-words). Subtree Kernel (SbtK) is one of the simplest tree kernels as it o</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 263– 270, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Ernandes</author>
<author>Giovanni Angelini</author>
<author>Marco Gori</author>
</authors>
<title>Webcrow: A web-based system for crossword solving. In</title>
<date>2005</date>
<booktitle>In Proc. of AAAI 05,</booktitle>
<pages>1412--1417</pages>
<publisher>AAAI Press.</publisher>
<location>Menlo Park, Calif.,</location>
<contexts>
<context position="5577" citStr="Ernandes et al., 2005" startWordPosition="886" endWordPosition="889">e best candidate among similar clues that are available in the DB. The syntactic representation captures clue paraphrasing properties. In order to carry out our study, we created two different corpora, one for each task: (i) a snippets reranking dataset and (ii) a clue similarity dataset. The first includes 21,000 clues, each associated with 150 candidate snippets whereas the latter comprises 794,190 clues. These datasets constitute interesting resources that we made available to the research community2. We compare our methods with one of the best systems for automatic CP resolution, WebCrow (Ernandes et al., 2005). Such system does use the two approaches mentioned before. Regarding snippet reranking, our structural models improve on the basic approach of WebCrow based on Bing by more than 4 absolute percent points in MRR, for a relative improvement of 23%. Concerning the similar clues retrieval, our methods improve on the one used by WebCrow, based on DBs, by 25% absolute, i.e., about 53% of error reduction whereas the answer accuracy at first position improves up to 70%. Given such promising results, we used our clue reranking method in WebCrow, and obtained an average improvement of 15% in resolving </context>
<context position="7606" citStr="Ernandes et al., 2005" startWordPosition="1204" endWordPosition="1207">wers. These lists are merged and used to solve a Probabilistic-Constraint Satisfaction Problem. Proverb relies on a very large crossword database as well as several expert modules, each of them mainly based on domain-specific databases (e.g., movies, writers and geography). In addition, it employs generic-word list generators and cluespecific modules to find solutions for particular kinds of clues like &lt;cTel (4): aviv ». Proverb’s modules use many knowledge sources: databases of clues, encyclopedias and Web documents. During the 1998 ACPT, Proverb placed 109th out of 251 contestants. WebCrow (Ernandes et al., 2005) is based on Proverb. It incorporates additional knowledge sources, provides a solver for the Italian language and improves the clues retrieval model from DB. In particular, it enables partial matching to retrieve clues that do not perfectly overlap with the query. WebCrow carries out basic linguistic analysis such as Part-Of-Speech tagging and lemmatization. It takes advantage of semantic relations contained in WordNet, dictionaries and gazetteers. Its Web module is constituted by a search engine, which can retrieve text snippets or documents related to the clue. Answer candidates and their c</context>
</contexts>
<marker>Ernandes, Angelini, Gori, 2005</marker>
<rawString>Marco Ernandes, Giovanni Angelini, and Marco Gori. 2005. Webcrow: A web-based system for crossword solving. In In Proc. of AAAI 05, pages 1412–1417. Menlo Park, Calif., AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Ernandes</author>
<author>Giovanni Angelini</author>
<author>Marco Gori</author>
</authors>
<title>A web-based agent challenges human experts on crosswords.</title>
<date>2008</date>
<journal>AI Magazine,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="33215" citStr="Ernandes et al., 2008" startWordPosition="5466" endWordPosition="5470">lues (more specifically, the V+DK+STK model) using 10 complete CPs (for a total of 760 clues) from the New York Times and Washington Post. This way, we could measure the impact of our model on the complete task carried out by WebCrow. More specifically, we give our reranked list of answers to WebCrow in place of the list it would have extracted with the CWDB module. It should be noted that to evaluate the impact of our list, we disabled WebCrow access to other lists, e.g., dictionaries. This means that the absolute resolution accuracy of WebCrow using our and its own lists can be higher (see (Ernandes et al., 2008) for more details). 46 The first result that we derive is the accuracy of the answer list produced from the new data, i.e., constituted by the 10 entire CPs. The results are reported in Tab. 4. We note that the improvement of our model is lower than before as a nonnegligible percentage of clues are not solved using the clue DB. However, when we compute the accuracy in solving the complete CPs, the impact is still remarkable as reported by Tab. 5. Indeed, the results show that when the lists reordered by our reranker are used by WebCrow, the latter improves by more than 5 absolute percent point</context>
</contexts>
<marker>Ernandes, Angelini, Gori, 2008</marker>
<rawString>Marco Ernandes, Giovanni Angelini, and Marco Gori. 2008. A web-based agent challenges human experts on crosswords. AI Magazine, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Adam Lally</author>
</authors>
<title>Uima: An architectural approach to unstructured information processing in the corporate research environment.</title>
<date>2004</date>
<journal>Nat. Lang. Eng.,</journal>
<pages>10--3</pages>
<contexts>
<context position="15072" citStr="Ferrucci and Lally, 2004" startWordPosition="2442" endWordPosition="2446">; Severyn et al., 2013b; Severyn et al., 2013a). However, to tackle the novelty of the task, especially for clue DB retrieval, we modeled innovative kernels. In the following, we first describe the general framework and then we instantiate it for the two reranking tasks studied in this paper. 4.1 Kernel framework The framework takes a textual query and retrieves a list of related text candidates using a search engine (applied to the Web or a DB), according to some similarity criteria. Then, the query and candidates are processed by an NLP pipeline. The pipeline is based on the UIMA framework (Ferrucci and Lally, 2004) and contains many text analysis components. The latter used for our specific tasks are: the tokenizer3, sentence detector1, lemmatizer1, part-of-speech (POS) tagger1, chunker4 and stopword marker5. The annotations produced by such processors are used by additional components to produce structural models representing clues and TS. The structure component converts the text fragments into trees. We use both trees and feature vectors to represent pairs of clues and TS, which are employed to train kernel-based rerankers for reordering the candidate lists provided by a search engine. Since the synt</context>
</contexts>
<marker>Ferrucci, Lally, 2004</marker>
<rawString>David Ferrucci and Adam Lally. 2004. Uima: An architectural approach to unstructured information processing in the corporate research environment. Nat. Lang. Eng., 10(3-4):327–348, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Eric Brown</author>
<author>Jennifer Chu-Carroll</author>
<author>James Fan</author>
<author>David Gondek</author>
<author>Aditya Kalyanpur</author>
<author>Adam Lally</author>
<author>J William Murdock</author>
</authors>
<title>Eric Nyberg,</title>
<date>2010</date>
<journal>AI Magazine,</journal>
<volume>31</volume>
<issue>3</issue>
<location>John Prager, Nico</location>
<contexts>
<context position="2919" citStr="Ferrucci et al., 2010" startWordPosition="468" endWordPosition="471">imize the overall probability of correctly filling the entire grid by exploiting the likelihood of each candidate answer, fulfilling at the same time the grid constraints. After several failures in approaching the human expert performance, it has become clear that designing more accurate solvers would not have provided a winning system. In contrast, the Precision and Recall of the answer candidates are obviously a key factor: a very high value for both of them would enable the solver to quickly find the correct solution. This basically suggests that, similarly to the Jeopardy! challenge case (Ferrucci et al., 2010b), the solution relies on Question Answering (QA) research. However, although some CP clues are rather similar to standard questions, as for example, in the clue/answer pair: &lt;&lt;What keeps a camera rolling?: dolly&gt;&gt;, some specific differences hold: (i) clues can be in interrogative form or not, e.g., &lt;&lt;Capital of USA: Washington~; (ii) they can contain riddles or be deliberately ambiguous and misleading (e.g., &lt;&lt;It’s green at first: orange&gt;&gt;); (iii) the exact length of the answer keyword is known in advance; and (vi) the confidence in the answers is an extremely important input for the CP solv</context>
<context position="9170" citStr="Ferrucci et al., 2010" startWordPosition="1448" endWordPosition="1451">e letters are derived from words found in dictionaries or in the generated candidate lists. WebCrow participated in international competitions with good results. 40 Figure 1: Overview of WebCrow’s architecture. Dr. Fill (Ginsberg, 2011) targets the crossword filling task with a Weighted-Constraint Satisfaction Problem. Constraint violations are weighted and can be tolerated. It heavily relies on huge databases of clues. It was placed 92nd out of more than 600 opponents in the 2013 ACPT. Specifically for QA using syntactic structures, a referring work for our research is the IBM Watson system (Ferrucci et al., 2010a). This is an advanced QA pipeline based on deep linguistic processing and semantic resources. It demonstrated that automatic methods can be more accurate than human experts in answering complex questions. More traditional studies on passage reranking, exploiting structural information, were carried out in (Katz and Lin, 2003), whereas other methods explored soft matching (i.e., lexical similarity) based on answer and named entity types (Aktolga et al., 2011). (Radlinski and Joachims, 2006; Jeon et al., 2005) applied question and answer classifiers for passage reranking. In this context, seve</context>
</contexts>
<marker>Ferrucci, Brown, Chu-Carroll, Fan, Gondek, Kalyanpur, Lally, Murdock, 2010</marker>
<rawString>David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welty. 2010a. Building watson: An overview of the deepqa project. AI Magazine, 31(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Ferrucci</author>
<author>Eric W Brown</author>
<author>Jennifer ChuCarroll</author>
<author>James Fan</author>
<author>David Gondek</author>
<author>Aditya Kalyanpur</author>
<author>Adam Lally</author>
<author>J William Murdock</author>
<author>Eric Nyberg</author>
<author>John M Prager</author>
<author>Nico Schlaefer</author>
<author>Christopher A Welty</author>
</authors>
<title>Building watson: An overview of the deepqa project.</title>
<date>2010</date>
<journal>AI Magazine,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="2919" citStr="Ferrucci et al., 2010" startWordPosition="468" endWordPosition="471">imize the overall probability of correctly filling the entire grid by exploiting the likelihood of each candidate answer, fulfilling at the same time the grid constraints. After several failures in approaching the human expert performance, it has become clear that designing more accurate solvers would not have provided a winning system. In contrast, the Precision and Recall of the answer candidates are obviously a key factor: a very high value for both of them would enable the solver to quickly find the correct solution. This basically suggests that, similarly to the Jeopardy! challenge case (Ferrucci et al., 2010b), the solution relies on Question Answering (QA) research. However, although some CP clues are rather similar to standard questions, as for example, in the clue/answer pair: &lt;&lt;What keeps a camera rolling?: dolly&gt;&gt;, some specific differences hold: (i) clues can be in interrogative form or not, e.g., &lt;&lt;Capital of USA: Washington~; (ii) they can contain riddles or be deliberately ambiguous and misleading (e.g., &lt;&lt;It’s green at first: orange&gt;&gt;); (iii) the exact length of the answer keyword is known in advance; and (vi) the confidence in the answers is an extremely important input for the CP solv</context>
<context position="9170" citStr="Ferrucci et al., 2010" startWordPosition="1448" endWordPosition="1451">e letters are derived from words found in dictionaries or in the generated candidate lists. WebCrow participated in international competitions with good results. 40 Figure 1: Overview of WebCrow’s architecture. Dr. Fill (Ginsberg, 2011) targets the crossword filling task with a Weighted-Constraint Satisfaction Problem. Constraint violations are weighted and can be tolerated. It heavily relies on huge databases of clues. It was placed 92nd out of more than 600 opponents in the 2013 ACPT. Specifically for QA using syntactic structures, a referring work for our research is the IBM Watson system (Ferrucci et al., 2010a). This is an advanced QA pipeline based on deep linguistic processing and semantic resources. It demonstrated that automatic methods can be more accurate than human experts in answering complex questions. More traditional studies on passage reranking, exploiting structural information, were carried out in (Katz and Lin, 2003), whereas other methods explored soft matching (i.e., lexical similarity) based on answer and named entity types (Aktolga et al., 2011). (Radlinski and Joachims, 2006; Jeon et al., 2005) applied question and answer classifiers for passage reranking. In this context, seve</context>
</contexts>
<marker>Ferrucci, Brown, ChuCarroll, Fan, Gondek, Kalyanpur, Lally, Murdock, Nyberg, Prager, Schlaefer, Welty, 2010</marker>
<rawString>David A. Ferrucci, Eric W. Brown, Jennifer ChuCarroll, James Fan, David Gondek, Aditya Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John M. Prager, Nico Schlaefer, and Christopher A. Welty. 2010b. Building watson: An overview of the deepqa project. AI Magazine, 31(3):59–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipediabased explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI’07,</booktitle>
<pages>1606--1611</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="24662" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="4014" endWordPosition="4017">bstrings to word subsequences by applying word insertions or deletions to the original input text pairs. – Running-Karp-Rabin Greedy String Tiling (Wise, 1996). It provides a similarity between two sentences by counting the number of shuffles in their subparts. – Resnik similarity (Resnik, 1995). The WordNet hypernymy hierarchy is used to compute a measure of semantic relatedness between concepts expressed in the text. The aggregation algorithm by Mihalcea et al. (Mihalcea et al., 2006) is applied to extend the measure from words to sentences. – Explicit Semantic Analysis (ESA) similarity 44 (Gabrilovich and Markovitch, 2007). It represents documents as weighted vectors of concepts learned from Wikipedia, WordNet and Wiktionary. – Lexical Substitution (Biemann, 2013). A supervised word sense disambiguation system is used to substitute a wide selection of high-frequency English nouns with generalizations. Resnik and ESA features are then computed on the transformed text. New features. Hereafter, we describe new features that we designed for CP reranking tasks. – Feasible Candidate. It is a binary feature signaling the presence or absence of words with the same length of the clue answer (only used for snippet rerank</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipediabased explicit semantic analysis. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI’07, pages 1606–1611, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<title>Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology.</title>
<date>1997</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="23859" citStr="Gusfield, 1997" startWordPosition="3893" endWordPosition="3894">o form pairs. 4.6 Feature Vectors In addition to structural representations, we also used features for capturing the degrees of similarity between clues within a pair. DKPro Similarity. We used similarity features from a top performing system in the Semantic Textual Similarity (STS) task, namely DKPro from the UKP Lab (B¨ar et al., 2013). These features were effective in predicting the degree of similarity between two sentences. DKPro includes the following syntactic similarity metrics, operating on string sequences, and more advanced semantic similarities: – Longest common substring measure (Gusfield, 1997). It determines the length of the longest substring shared by two text segments. – Longest common subsequence measure (Allison and Dix, 1986). It extends the notion of substrings to word subsequences by applying word insertions or deletions to the original input text pairs. – Running-Karp-Rabin Greedy String Tiling (Wise, 1996). It provides a similarity between two sentences by counting the number of shuffles in their subparts. – Resnik similarity (Resnik, 1995). The WordNet hypernymy hierarchy is used to compute a measure of semantic relatedness between concepts expressed in the text. The agg</context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>Dan Gusfield. 1997. Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Herbrich</author>
<author>T Graepel</author>
<author>K Obermayer</author>
</authors>
<title>Large margin rank boundaries for ordinal regression.</title>
<date>2000</date>
<booktitle>Advances in Large Margin Classifiers,</booktitle>
<pages>115--132</pages>
<editor>In A.J. Smola, P.L. Bartlett, B. Sch¨olkopf, and D. Schuurmans, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4279" citStr="Herbrich et al., 2000" startWordPosition="678" endWordPosition="681">n. For this purpose, we designed learning to rank models for reordering the answers produced with two different techniques typically used in CP systems: (i) searching the Web with clue representations, e.g., 39 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 39–48, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics exploiting Bing search engine1; and (ii) querying the DB of previously resolved CP clues, e.g., using standard SQL techniques. We rerank the text snippets returned by Bing by means of SVM preference ranking (Herbrich et al., 2000) for improving the first technique. One interesting contribution is that our model exploits a syntactic representation of clues to improve Web search. More in detail, we use structural kernels (e.g., see (Moschitti, 2006; Moschitti, 2008)) in SVMs applied to our syntactic representation of pairs, formed by clues with their candidate snippets. Regarding the DB approach, we provide a completely novel solution by substituting it and the SQL function with a search engine for retrieving clues similar to the target one. Then, we rerank the retrieved clues by applying SVMs and structural kernels to t</context>
</contexts>
<marker>Herbrich, Graepel, Obermayer, 2000</marker>
<rawString>R Herbrich, T Graepel, and K Obermayer. 2000. Large margin rank boundaries for ordinal regression. In A.J. Smola, P.L. Bartlett, B. Sch¨olkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 115–132, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
</authors>
<title>Finding similar questions in large question and answer archives.</title>
<date>2005</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="9685" citStr="Jeon et al., 2005" startWordPosition="1526" endWordPosition="1529">syntactic structures, a referring work for our research is the IBM Watson system (Ferrucci et al., 2010a). This is an advanced QA pipeline based on deep linguistic processing and semantic resources. It demonstrated that automatic methods can be more accurate than human experts in answering complex questions. More traditional studies on passage reranking, exploiting structural information, were carried out in (Katz and Lin, 2003), whereas other methods explored soft matching (i.e., lexical similarity) based on answer and named entity types (Aktolga et al., 2011). (Radlinski and Joachims, 2006; Jeon et al., 2005) applied question and answer classifiers for passage reranking. In this context, several approaches focused on reranking the answers to definition/description questions, e.g., (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Severyn and Moschitti, 2012; Severyn et al., 2013b). 3 WebCrow Architecture Our research focuses on the generation of accurate answer candidate lists, which, when used in a CP resolution systems, can improve the overall solution accuracy. Therefore, the quality of our modules can be assessed by testing them within such systems. For this purpose, we se</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005. Finding similar questions in large question and answer archives. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’02,</booktitle>
<pages>133--142</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="26498" citStr="Joachims, 2002" startWordPosition="4308" endWordPosition="4309">veness of our models on two different tasks: (i) Snippet Reranking and (ii) Similar Clue Retrieval (SCR). Additionally, we measured the impact of our best model for SCR in the WebCrow system by comparing with it. Our referring database of clues is composed by 1,158,202 clues, which belong to eight different crossword editors (downloaded from the Web6). We use the latter to create one dataset for snippet reranking and one dataset for clues retrieval. 5.1 Experimental Setup To train our models, we adopted SVM-light-TK7, which enables the use of structural kernels (Moschitti, 2006) in SVM-light (Joachims, 2002), with default parameters. We applied a polynomial kernel of degree 3 to the explicit feature vectors, 6http://www.crosswordgiant.com 7http://disi.unitn.it/moschitti/ Tree-Kernel.htm Model MAP MRR AvgRec REC@1 REC@5 Bing 16.00 18.09 69.00 12.50 24.80 V 18.00 19.88 76.00 14.20 26.10 SbtK 17.00 19.6 75.00 13.80 26.40 STK 18.00 20.44 76.00 15.10 27.00 STKb 18.00 20.68 76.00 15.30 27.40 PTK 19.00 21.65 77.00 16.10 28.70 V+SbtK 20.00 22.39 80.00 17.20 29.10 V+STK 19.00 20.82 78.00 14.90 27.90 STKb 19.00 21.20 79.00 15.60 28.40 V+PTK 19.00 21.68 79.00 16.00 29.40 V+DK 18.00 20.48 77.00 14.60 26.80 V</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’02, pages 133–142, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Katz</author>
<author>Jimmy Lin</author>
</authors>
<title>Selectively using relations to improve precision in question answering.</title>
<date>2003</date>
<contexts>
<context position="9499" citStr="Katz and Lin, 2003" startWordPosition="1497" endWordPosition="1500"> violations are weighted and can be tolerated. It heavily relies on huge databases of clues. It was placed 92nd out of more than 600 opponents in the 2013 ACPT. Specifically for QA using syntactic structures, a referring work for our research is the IBM Watson system (Ferrucci et al., 2010a). This is an advanced QA pipeline based on deep linguistic processing and semantic resources. It demonstrated that automatic methods can be more accurate than human experts in answering complex questions. More traditional studies on passage reranking, exploiting structural information, were carried out in (Katz and Lin, 2003), whereas other methods explored soft matching (i.e., lexical similarity) based on answer and named entity types (Aktolga et al., 2011). (Radlinski and Joachims, 2006; Jeon et al., 2005) applied question and answer classifiers for passage reranking. In this context, several approaches focused on reranking the answers to definition/description questions, e.g., (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Severyn and Moschitti, 2012; Severyn et al., 2013b). 3 WebCrow Architecture Our research focuses on the generation of accurate answer candidate lists, which, when used</context>
</contexts>
<marker>Katz, Lin, 2003</marker>
<rawString>Boris Katz and Jimmy Lin. 2003. Selectively using relations to improve precision in question answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael L Littman</author>
<author>Greg A Keim</author>
<author>Noam Shazeer</author>
</authors>
<title>A probabilistic approach to solving crossword puzzles.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<volume>134</volume>
<issue>12</issue>
<contexts>
<context position="6860" citStr="Littman et al., 2002" startWordPosition="1087" endWordPosition="1090">as those based on syntactic structures and learning to rank methods can help to win 1https://www.bing.com/ 2http://projects.disi.unitn.it/ iKernels/projects/webcrow/ the CP resolution challenge. In the reminder of this paper, Sec. 2 introduces the automatic CP resolution task in the context of the related work, Sec. 3 introduces WebCrow, Sec. 4 illustrates our models for snippets reranking and similar clue retrieval using kernel methods, syntactic structures, and traditional feature vectors, Sec. 5 describes our experiments, and finally, Sec. 6 derives the conclusions. 2 Related Work Proverb (Littman et al., 2002) was the first system for the automatic resolution of CPs. It includes several modules for generating lists of candidate answers. These lists are merged and used to solve a Probabilistic-Constraint Satisfaction Problem. Proverb relies on a very large crossword database as well as several expert modules, each of them mainly based on domain-specific databases (e.g., movies, writers and geography). In addition, it employs generic-word list generators and cluespecific modules to find solutions for particular kinds of clues like &lt;cTel (4): aviv ». Proverb’s modules use many knowledge sources: datab</context>
</contexts>
<marker>Littman, Keim, Shazeer, 2002</marker>
<rawString>Michael L. Littman, Greg A. Keim, and Noam Shazeer. 2002. A probabilistic approach to solving crossword puzzles. Artificial Intelligence, 134(12):23 – 55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael McCandless</author>
<author>Erik Hatcher</author>
<author>Otis Gospodnetic</author>
</authors>
<date>2010</date>
<booktitle>Lucene in Action, Second Edition: Covers Apache Lucene 3.0. Manning Publications Co.,</booktitle>
<location>Greenwich, CT, USA.</location>
<contexts>
<context position="21189" citStr="McCandless et al., 2010" startWordPosition="3462" endWordPosition="3465">PP REL-NP PP REL-NP REL-NNP IN REL-NN IN VBG REL-NN NNS kind of connection for travel computer user S REL-NP PP REL-NP REL-NNP IN REL-NN REL-NN kind of computer connection Figure 3: Two similar clues leading to the same answer. 4.5 Similar clue reranking WebCrow creates answer lists by retrieving clues from the DB of previously solved crosswords. It simply uses the classical SQL operator and fulltext search. We instead verified the hypothesis that a search engine could achieve a better result. Thus we opted for indexing the DB clues and their answers with the open source search engine Lucene (McCandless et al., 2010), using the state-of-the-art BM25 retrieval model. This alone significantly improved the quality of the retrieved clue list, which could be further refined by applying reranking. The latter consists in (i) retrieving a list of similar clues using a search engine and (ii) moving those more similar, which more probably contain the same answer to the clue query, at the top. For example, Table 1 shows the first five clues, retrieved for a query built from the clue: Kind of connection for traveling computer users. The search engine retrieves the wrong clue, Kind of connection for traveling computer</context>
</contexts>
<marker>McCandless, Hatcher, Gospodnetic, 2010</marker>
<rawString>Michael McCandless, Erik Hatcher, and Otis Gospodnetic. 2010. Lucene in Action, Second Edition: Covers Apache Lucene 3.0. Manning Publications Co., Greenwich, CT, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence -</booktitle>
<volume>1</volume>
<pages>775--780</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="24520" citStr="Mihalcea et al., 2006" startWordPosition="3993" endWordPosition="3996">st substring shared by two text segments. – Longest common subsequence measure (Allison and Dix, 1986). It extends the notion of substrings to word subsequences by applying word insertions or deletions to the original input text pairs. – Running-Karp-Rabin Greedy String Tiling (Wise, 1996). It provides a similarity between two sentences by counting the number of shuffles in their subparts. – Resnik similarity (Resnik, 1995). The WordNet hypernymy hierarchy is used to compute a measure of semantic relatedness between concepts expressed in the text. The aggregation algorithm by Mihalcea et al. (Mihalcea et al., 2006) is applied to extend the measure from words to sentences. – Explicit Semantic Analysis (ESA) similarity 44 (Gabrilovich and Markovitch, 2007). It represents documents as weighted vectors of concepts learned from Wikipedia, WordNet and Wiktionary. – Lexical Substitution (Biemann, 2013). A supervised word sense disambiguation system is used to substitute a wide selection of high-frequency English nouns with generalizations. Resnik and ESA features are then computed on the transformed text. New features. Hereafter, we describe new features that we designed for CP reranking tasks. – Feasible Cand</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the 21st National Conference on Artificial Intelligence - Volume 1, AAAI’06, pages 775–780. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Semantic role labeling via tree kernel joint inference.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X,</booktitle>
<location>New York City.</location>
<contexts>
<context position="14328" citStr="Moschitti et al., 2006" startWordPosition="2314" endWordPosition="2317">ances to compute cluesimilarity scores, along with the Full-Text search functions. CWDB-DICTIO, which simply returns the full list of words of correct length, ranked by their number of occurrences in the initial list. We improve WSM and CWDB by applying learning-to-rank algorithms based on SVMs and tree kernels applied to structural representations. We describe our models in detail in the next section. 4 Learning to rank with kernels The basic architecture of our reranking framework is relatively simple: it uses a standard preference kernel reranking approach (e.g., see (Shen and Joshi, 2005; Moschitti et al., 2006)). The structural kernel reranking framework is a specialization of the one we proposed in (Severyn and Moschitti, 2012; Severyn et al., 2013b; Severyn et al., 2013a). However, to tackle the novelty of the task, especially for clue DB retrieval, we modeled innovative kernels. In the following, we first describe the general framework and then we instantiate it for the two reranking tasks studied in this paper. 4.1 Kernel framework The framework takes a textual query and retrieves a list of related text candidates using a search engine (applied to the Web or a DB), according to some similarity c</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2006</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2006. Semantic role labeling via tree kernel joint inference. In Proceedings of CoNLL-X, New York City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
<author>Roberto Basili</author>
<author>Suresh Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question/answer classification.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9907" citStr="Moschitti et al., 2007" startWordPosition="1558" endWordPosition="1561">t automatic methods can be more accurate than human experts in answering complex questions. More traditional studies on passage reranking, exploiting structural information, were carried out in (Katz and Lin, 2003), whereas other methods explored soft matching (i.e., lexical similarity) based on answer and named entity types (Aktolga et al., 2011). (Radlinski and Joachims, 2006; Jeon et al., 2005) applied question and answer classifiers for passage reranking. In this context, several approaches focused on reranking the answers to definition/description questions, e.g., (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Severyn and Moschitti, 2012; Severyn et al., 2013b). 3 WebCrow Architecture Our research focuses on the generation of accurate answer candidate lists, which, when used in a CP resolution systems, can improve the overall solution accuracy. Therefore, the quality of our modules can be assessed by testing them within such systems. For this purpose, we selected WebCrow as it is rather modular, accurate and it was kindly made available by the authors. Its architecture is illustrated in Figure 1. The solving process is divided in two phases: in the first phase, the coordinat</context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and Suresh Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question/answer classification. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In ECML,</booktitle>
<pages>318--329</pages>
<contexts>
<context position="4499" citStr="Moschitti, 2006" startWordPosition="714" endWordPosition="715">he Eighteenth Conference on Computational Language Learning, pages 39–48, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics exploiting Bing search engine1; and (ii) querying the DB of previously resolved CP clues, e.g., using standard SQL techniques. We rerank the text snippets returned by Bing by means of SVM preference ranking (Herbrich et al., 2000) for improving the first technique. One interesting contribution is that our model exploits a syntactic representation of clues to improve Web search. More in detail, we use structural kernels (e.g., see (Moschitti, 2006; Moschitti, 2008)) in SVMs applied to our syntactic representation of pairs, formed by clues with their candidate snippets. Regarding the DB approach, we provide a completely novel solution by substituting it and the SQL function with a search engine for retrieving clues similar to the target one. Then, we rerank the retrieved clues by applying SVMs and structural kernels to the syntactic representation of clue pairs. This way, SVMs learn to choose the best candidate among similar clues that are available in the DB. The syntactic representation captures clue paraphrasing properties. In order </context>
<context position="17746" citStr="Moschitti, 2006" startWordPosition="2882" endWordPosition="2883"> be or not to be ... the monologue is addressed by a character either to himself or to the audience. Additionally, we use a special REL tag to link the clue/snippet trees above such that structural relations will be captured by tree fragments. The links are established as follows: words from a clue and a snippet sharing a lemma get their parents (POS tags) and grandparents, i.e., chunk labels, marked by a prepending REL tag. We build such structural representations for both snippet and similar clue reranking tasks. 4.3 Tree kernels We briefly report the different types of kernels (see, e.g., (Moschitti, 2006) for more details). Syntactic Tree Kernel (STK), also known as a subset tree kernel (Collins and Duffy, 2002), maps objects in the space of all possible tree fragments constrained by the rule that the sibling nodes from their parents cannot be separated. In other words, substructures are composed by atomic building blocks corresponding to nodes along with all of their direct children. These, in case of a syntactic parse tree, are complete production rules of the associated parser grammar. STKb extends STK by allowing leaf nodes to be part of the feature space. Leaf in syntactic trees are words</context>
<context position="26468" citStr="Moschitti, 2006" startWordPosition="4303" endWordPosition="4305">im at demonstrating the effectiveness of our models on two different tasks: (i) Snippet Reranking and (ii) Similar Clue Retrieval (SCR). Additionally, we measured the impact of our best model for SCR in the WebCrow system by comparing with it. Our referring database of clues is composed by 1,158,202 clues, which belong to eight different crossword editors (downloaded from the Web6). We use the latter to create one dataset for snippet reranking and one dataset for clues retrieval. 5.1 Experimental Setup To train our models, we adopted SVM-light-TK7, which enables the use of structural kernels (Moschitti, 2006) in SVM-light (Joachims, 2002), with default parameters. We applied a polynomial kernel of degree 3 to the explicit feature vectors, 6http://www.crosswordgiant.com 7http://disi.unitn.it/moschitti/ Tree-Kernel.htm Model MAP MRR AvgRec REC@1 REC@5 Bing 16.00 18.09 69.00 12.50 24.80 V 18.00 19.88 76.00 14.20 26.10 SbtK 17.00 19.6 75.00 13.80 26.40 STK 18.00 20.44 76.00 15.10 27.00 STKb 18.00 20.68 76.00 15.30 27.40 PTK 19.00 21.65 77.00 16.10 28.70 V+SbtK 20.00 22.39 80.00 17.20 29.10 V+STK 19.00 20.82 78.00 14.90 27.90 STKb 19.00 21.20 79.00 15.60 28.40 V+PTK 19.00 21.68 79.00 16.00 29.40 V+DK 1</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In ECML, pages 318–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Kernel methods, syntax and semantics for relational text categorization.</title>
<date>2008</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="4517" citStr="Moschitti, 2008" startWordPosition="716" endWordPosition="717">ference on Computational Language Learning, pages 39–48, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics exploiting Bing search engine1; and (ii) querying the DB of previously resolved CP clues, e.g., using standard SQL techniques. We rerank the text snippets returned by Bing by means of SVM preference ranking (Herbrich et al., 2000) for improving the first technique. One interesting contribution is that our model exploits a syntactic representation of clues to improve Web search. More in detail, we use structural kernels (e.g., see (Moschitti, 2006; Moschitti, 2008)) in SVMs applied to our syntactic representation of pairs, formed by clues with their candidate snippets. Regarding the DB approach, we provide a completely novel solution by substituting it and the SQL function with a search engine for retrieving clues similar to the target one. Then, we rerank the retrieved clues by applying SVMs and structural kernels to the syntactic representation of clue pairs. This way, SVMs learn to choose the best candidate among similar clues that are available in the DB. The syntactic representation captures clue paraphrasing properties. In order to carry out our s</context>
</contexts>
<marker>Moschitti, 2008</marker>
<rawString>Alessandro Moschitti. 2008. Kernel methods, syntax and semantics for relational text categorization. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ira Pohl</author>
</authors>
<title>Heuristic search viewed as path finding in a graph.</title>
<date>1970</date>
<journal>Artificial Intelligence,</journal>
<volume>1</volume>
<issue>34</issue>
<pages>204</pages>
<contexts>
<context position="8297" citStr="Pohl, 1970" startWordPosition="1314" endWordPosition="1315">olver for the Italian language and improves the clues retrieval model from DB. In particular, it enables partial matching to retrieve clues that do not perfectly overlap with the query. WebCrow carries out basic linguistic analysis such as Part-Of-Speech tagging and lemmatization. It takes advantage of semantic relations contained in WordNet, dictionaries and gazetteers. Its Web module is constituted by a search engine, which can retrieve text snippets or documents related to the clue. Answer candidates and their confidence scores are generated from this content. WebCrow uses a WA* algorithm (Pohl, 1970) for Probabilistic-Constraint Satisfaction Problems, adapted for CP resolution. The solver fills the grid entries for which no solution was found by the previous modules. It tries combinations of letters that satisfy the crossword constraints, where the letters are derived from words found in dictionaries or in the generated candidate lists. WebCrow participated in international competitions with good results. 40 Figure 1: Overview of WebCrow’s architecture. Dr. Fill (Ginsberg, 2011) targets the crossword filling task with a Weighted-Constraint Satisfaction Problem. Constraint violations are w</context>
</contexts>
<marker>Pohl, 1970</marker>
<rawString>Ira Pohl. 1970. Heuristic search viewed as path finding in a graph. Artificial Intelligence, 1(34):193 – 204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 1, IJCAI’95,</booktitle>
<pages>448--453</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="24325" citStr="Resnik, 1995" startWordPosition="3964" endWordPosition="3965">tic similarity metrics, operating on string sequences, and more advanced semantic similarities: – Longest common substring measure (Gusfield, 1997). It determines the length of the longest substring shared by two text segments. – Longest common subsequence measure (Allison and Dix, 1986). It extends the notion of substrings to word subsequences by applying word insertions or deletions to the original input text pairs. – Running-Karp-Rabin Greedy String Tiling (Wise, 1996). It provides a similarity between two sentences by counting the number of shuffles in their subparts. – Resnik similarity (Resnik, 1995). The WordNet hypernymy hierarchy is used to compute a measure of semantic relatedness between concepts expressed in the text. The aggregation algorithm by Mihalcea et al. (Mihalcea et al., 2006) is applied to extend the measure from words to sentences. – Explicit Semantic Analysis (ESA) similarity 44 (Gabrilovich and Markovitch, 2007). It represents documents as weighted vectors of concepts learned from Wikipedia, WordNet and Wiktionary. – Lexical Substitution (Biemann, 2013). A supervised word sense disambiguation system is used to substitute a wide selection of high-frequency English nouns </context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 1, IJCAI’95, pages 448–453, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structural relationships for large-scale learning of answer re-ranking.</title>
<date>2012</date>
<booktitle>In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval (SIGIR),</booktitle>
<pages>741--750</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9959" citStr="Severyn and Moschitti, 2012" startWordPosition="1566" endWordPosition="1569">uman experts in answering complex questions. More traditional studies on passage reranking, exploiting structural information, were carried out in (Katz and Lin, 2003), whereas other methods explored soft matching (i.e., lexical similarity) based on answer and named entity types (Aktolga et al., 2011). (Radlinski and Joachims, 2006; Jeon et al., 2005) applied question and answer classifiers for passage reranking. In this context, several approaches focused on reranking the answers to definition/description questions, e.g., (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Severyn and Moschitti, 2012; Severyn et al., 2013b). 3 WebCrow Architecture Our research focuses on the generation of accurate answer candidate lists, which, when used in a CP resolution systems, can improve the overall solution accuracy. Therefore, the quality of our modules can be assessed by testing them within such systems. For this purpose, we selected WebCrow as it is rather modular, accurate and it was kindly made available by the authors. Its architecture is illustrated in Figure 1. The solving process is divided in two phases: in the first phase, the coordinator module forwards the clues of an input CP to a set</context>
<context position="14447" citStr="Severyn and Moschitti, 2012" startWordPosition="2334" endWordPosition="2338">s the full list of words of correct length, ranked by their number of occurrences in the initial list. We improve WSM and CWDB by applying learning-to-rank algorithms based on SVMs and tree kernels applied to structural representations. We describe our models in detail in the next section. 4 Learning to rank with kernels The basic architecture of our reranking framework is relatively simple: it uses a standard preference kernel reranking approach (e.g., see (Shen and Joshi, 2005; Moschitti et al., 2006)). The structural kernel reranking framework is a specialization of the one we proposed in (Severyn and Moschitti, 2012; Severyn et al., 2013b; Severyn et al., 2013a). However, to tackle the novelty of the task, especially for clue DB retrieval, we modeled innovative kernels. In the following, we first describe the general framework and then we instantiate it for the two reranking tasks studied in this paper. 4.1 Kernel framework The framework takes a textual query and retrieves a list of related text candidates using a search engine (applied to the Web or a DB), according to some similarity criteria. Then, the query and candidates are processed by an NLP pipeline. The pipeline is based on the UIMA framework (</context>
</contexts>
<marker>Severyn, Moschitti, 2012</marker>
<rawString>Aliaksei Severyn and Alessandro Moschitti. 2012. Structural relationships for large-scale learning of answer re-ranking. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval (SIGIR), pages 741–750. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Massimo Nicosia</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Building structures from classifiers for passage reranking. In</title>
<date>2013</date>
<booktitle>CIKM,</booktitle>
<pages>969--978</pages>
<contexts>
<context position="9981" citStr="Severyn et al., 2013" startWordPosition="1570" endWordPosition="1573">plex questions. More traditional studies on passage reranking, exploiting structural information, were carried out in (Katz and Lin, 2003), whereas other methods explored soft matching (i.e., lexical similarity) based on answer and named entity types (Aktolga et al., 2011). (Radlinski and Joachims, 2006; Jeon et al., 2005) applied question and answer classifiers for passage reranking. In this context, several approaches focused on reranking the answers to definition/description questions, e.g., (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Severyn and Moschitti, 2012; Severyn et al., 2013b). 3 WebCrow Architecture Our research focuses on the generation of accurate answer candidate lists, which, when used in a CP resolution systems, can improve the overall solution accuracy. Therefore, the quality of our modules can be assessed by testing them within such systems. For this purpose, we selected WebCrow as it is rather modular, accurate and it was kindly made available by the authors. Its architecture is illustrated in Figure 1. The solving process is divided in two phases: in the first phase, the coordinator module forwards the clues of an input CP to a set of modules for the ge</context>
<context position="14469" citStr="Severyn et al., 2013" startWordPosition="2339" endWordPosition="2342">orrect length, ranked by their number of occurrences in the initial list. We improve WSM and CWDB by applying learning-to-rank algorithms based on SVMs and tree kernels applied to structural representations. We describe our models in detail in the next section. 4 Learning to rank with kernels The basic architecture of our reranking framework is relatively simple: it uses a standard preference kernel reranking approach (e.g., see (Shen and Joshi, 2005; Moschitti et al., 2006)). The structural kernel reranking framework is a specialization of the one we proposed in (Severyn and Moschitti, 2012; Severyn et al., 2013b; Severyn et al., 2013a). However, to tackle the novelty of the task, especially for clue DB retrieval, we modeled innovative kernels. In the following, we first describe the general framework and then we instantiate it for the two reranking tasks studied in this paper. 4.1 Kernel framework The framework takes a textual query and retrieves a list of related text candidates using a search engine (applied to the Web or a DB), according to some similarity criteria. Then, the query and candidates are processed by an NLP pipeline. The pipeline is based on the UIMA framework (Ferrucci and Lally, 20</context>
<context position="16602" citStr="Severyn et al., 2013" startWordPosition="2680" endWordPosition="2683">building blocks for our models. Finally, we show the reranking models for both tasks, TS and clue reranking. 3http://nlp.stanford.edu/software/ corenlp.shtml 4http://cogcomp.cs.illinois.edu/page/ software_view/13 5Based on a standard stoplist. 42 Rank Clue Answer 1 Kind of support for a computer user tech 2 Kind of computer connection wifi 3 Computer connection port 4 Comb users bees 5 Traveling bag grip Table 1: Clue ranking for the query: Kind of connection for traveling computer users (wifi) 4.2 Relational shallow tree representation The structures we adopt are similar to those defined in (Severyn et al., 2013b). They are essentially shallow syntactic trees built from POS tags grouped into chunks. Each clue and its answer candidate (either a TS or clue) are encoded into a tree having word lemmas at the leaves and POS tags as pre-terminals. The higher tree level organizes POS tags into chunks. For example, the upper tree of Figure 2, shows a shallow tree for the clue: Hamlet’s ”To be, or not to be” addressee, whereas the lower tree represents a retrieved TS containing the answer, himself: The most obvious example is Hamlet’s ”To be or not to be ... the monologue is addressed by a character either to</context>
</contexts>
<marker>Severyn, Nicosia, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013a. Building structures from classifiers for passage reranking. In CIKM, pages 969– 978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Massimo Nicosia</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Learning adaptable patterns for passage reranking.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,</booktitle>
<pages>75--83</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="9981" citStr="Severyn et al., 2013" startWordPosition="1570" endWordPosition="1573">plex questions. More traditional studies on passage reranking, exploiting structural information, were carried out in (Katz and Lin, 2003), whereas other methods explored soft matching (i.e., lexical similarity) based on answer and named entity types (Aktolga et al., 2011). (Radlinski and Joachims, 2006; Jeon et al., 2005) applied question and answer classifiers for passage reranking. In this context, several approaches focused on reranking the answers to definition/description questions, e.g., (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Severyn and Moschitti, 2012; Severyn et al., 2013b). 3 WebCrow Architecture Our research focuses on the generation of accurate answer candidate lists, which, when used in a CP resolution systems, can improve the overall solution accuracy. Therefore, the quality of our modules can be assessed by testing them within such systems. For this purpose, we selected WebCrow as it is rather modular, accurate and it was kindly made available by the authors. Its architecture is illustrated in Figure 1. The solving process is divided in two phases: in the first phase, the coordinator module forwards the clues of an input CP to a set of modules for the ge</context>
<context position="14469" citStr="Severyn et al., 2013" startWordPosition="2339" endWordPosition="2342">orrect length, ranked by their number of occurrences in the initial list. We improve WSM and CWDB by applying learning-to-rank algorithms based on SVMs and tree kernels applied to structural representations. We describe our models in detail in the next section. 4 Learning to rank with kernels The basic architecture of our reranking framework is relatively simple: it uses a standard preference kernel reranking approach (e.g., see (Shen and Joshi, 2005; Moschitti et al., 2006)). The structural kernel reranking framework is a specialization of the one we proposed in (Severyn and Moschitti, 2012; Severyn et al., 2013b; Severyn et al., 2013a). However, to tackle the novelty of the task, especially for clue DB retrieval, we modeled innovative kernels. In the following, we first describe the general framework and then we instantiate it for the two reranking tasks studied in this paper. 4.1 Kernel framework The framework takes a textual query and retrieves a list of related text candidates using a search engine (applied to the Web or a DB), according to some similarity criteria. Then, the query and candidates are processed by an NLP pipeline. The pipeline is based on the UIMA framework (Ferrucci and Lally, 20</context>
<context position="16602" citStr="Severyn et al., 2013" startWordPosition="2680" endWordPosition="2683">building blocks for our models. Finally, we show the reranking models for both tasks, TS and clue reranking. 3http://nlp.stanford.edu/software/ corenlp.shtml 4http://cogcomp.cs.illinois.edu/page/ software_view/13 5Based on a standard stoplist. 42 Rank Clue Answer 1 Kind of support for a computer user tech 2 Kind of computer connection wifi 3 Computer connection port 4 Comb users bees 5 Traveling bag grip Table 1: Clue ranking for the query: Kind of connection for traveling computer users (wifi) 4.2 Relational shallow tree representation The structures we adopt are similar to those defined in (Severyn et al., 2013b). They are essentially shallow syntactic trees built from POS tags grouped into chunks. Each clue and its answer candidate (either a TS or clue) are encoded into a tree having word lemmas at the leaves and POS tags as pre-terminals. The higher tree level organizes POS tags into chunks. For example, the upper tree of Figure 2, shows a shallow tree for the clue: Hamlet’s ”To be, or not to be” addressee, whereas the lower tree represents a retrieved TS containing the answer, himself: The most obvious example is Hamlet’s ”To be or not to be ... the monologue is addressed by a character either to</context>
</contexts>
<marker>Severyn, Nicosia, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013b. Learning adaptable patterns for passage reranking. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 75–83, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>Ranking and reranking with perceptron.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="14303" citStr="Shen and Joshi, 2005" startWordPosition="2310" endWordPosition="2313">d positional term distances to compute cluesimilarity scores, along with the Full-Text search functions. CWDB-DICTIO, which simply returns the full list of words of correct length, ranked by their number of occurrences in the initial list. We improve WSM and CWDB by applying learning-to-rank algorithms based on SVMs and tree kernels applied to structural representations. We describe our models in detail in the next section. 4 Learning to rank with kernels The basic architecture of our reranking framework is relatively simple: it uses a standard preference kernel reranking approach (e.g., see (Shen and Joshi, 2005; Moschitti et al., 2006)). The structural kernel reranking framework is a specialization of the one we proposed in (Severyn and Moschitti, 2012; Severyn et al., 2013b; Severyn et al., 2013a). However, to tackle the novelty of the task, especially for clue DB retrieval, we modeled innovative kernels. In the following, we first describe the general framework and then we instantiate it for the two reranking tasks studied in this paper. 4.1 Kernel framework The framework takes a textual query and retrieves a list of related text candidates using a search engine (applied to the Web or a DB), accor</context>
<context position="19577" citStr="Shen and Joshi, 2005" startWordPosition="3180" endWordPosition="3183">an generate a very rich feature space resulting in higher generalization ability. 4.4 Snippet reranking The task of snippet reranking consists in reordering the list of snippets retrieved from the search engine such that those containing the correct answer can be pushed at the top of the list. For this purpose, we transform the target clue in a search query and retrieve candidate text snippets. In our training set, these candidate text snippets are considered as positive examples if they contain the answer to the target clue. We rerank snippets using preference reranking approach (see, e.g., (Shen and Joshi, 2005)). This means that two snippets are compared to derive which one is the best, i.e., which snippet contains the answer with higher probability. Since we aim at using kernel methods, we apply the following preference kernel: PK((s1, s2), (s01, s02)) = K(s1, s01)+ +K(s2, s02) − K(s1, s02) − K(s2, s01), where sr and s0 r refer to two sets of candidates associated with two rankings and K is a kernel applied to pairs of candidates. We represent the latter as pairs of clue and snippet trees. More formally, given two candidates, si = (si(c), si(s)) and s0i = (s0i(c), s0i(s)), whose members are the clu</context>
</contexts>
<marker>Shen, Joshi, 2005</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2005. Ranking and reranking with perceptron. Machine Learning, 60(1-3):73–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>M Lapata</author>
</authors>
<title>Using semantic roles to improve question answering.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="9883" citStr="Shen and Lapata, 2007" startWordPosition="1554" endWordPosition="1557">es. It demonstrated that automatic methods can be more accurate than human experts in answering complex questions. More traditional studies on passage reranking, exploiting structural information, were carried out in (Katz and Lin, 2003), whereas other methods explored soft matching (i.e., lexical similarity) based on answer and named entity types (Aktolga et al., 2011). (Radlinski and Joachims, 2006; Jeon et al., 2005) applied question and answer classifiers for passage reranking. In this context, several approaches focused on reranking the answers to definition/description questions, e.g., (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Severyn and Moschitti, 2012; Severyn et al., 2013b). 3 WebCrow Architecture Our research focuses on the generation of accurate answer candidate lists, which, when used in a CP resolution systems, can improve the overall solution accuracy. Therefore, the quality of our modules can be assessed by testing them within such systems. For this purpose, we selected WebCrow as it is rather modular, accurate and it was kindly made available by the authors. Its architecture is illustrated in Figure 1. The solving process is divided in two phases: in the fi</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>D. Shen and M. Lapata. 2007. Using semantic roles to improve question answering. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>M Ciaramita</author>
<author>H Zaragoza</author>
</authors>
<title>Learning to rank answers on large online QA collections.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-HLT.</booktitle>
<contexts>
<context position="9930" citStr="Surdeanu et al., 2008" startWordPosition="1562" endWordPosition="1565">be more accurate than human experts in answering complex questions. More traditional studies on passage reranking, exploiting structural information, were carried out in (Katz and Lin, 2003), whereas other methods explored soft matching (i.e., lexical similarity) based on answer and named entity types (Aktolga et al., 2011). (Radlinski and Joachims, 2006; Jeon et al., 2005) applied question and answer classifiers for passage reranking. In this context, several approaches focused on reranking the answers to definition/description questions, e.g., (Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Severyn and Moschitti, 2012; Severyn et al., 2013b). 3 WebCrow Architecture Our research focuses on the generation of accurate answer candidate lists, which, when used in a CP resolution systems, can improve the overall solution accuracy. Therefore, the quality of our modules can be assessed by testing them within such systems. For this purpose, we selected WebCrow as it is rather modular, accurate and it was kindly made available by the authors. Its architecture is illustrated in Figure 1. The solving process is divided in two phases: in the first phase, the coordinator module forwards the </context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2008</marker>
<rawString>M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008. Learning to rank answers on large online QA collections. In Proceedings ofACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Wise</author>
</authors>
<title>Yap3: Improved detection of similarities in computer program and other texts.</title>
<date>1996</date>
<booktitle>In Proceedings of the Twenty-seventh SIGCSE Technical Symposium on Computer Science Education, SIGCSE ’96,</booktitle>
<pages>130--134</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="24188" citStr="Wise, 1996" startWordPosition="3943" endWordPosition="3944"> 2013). These features were effective in predicting the degree of similarity between two sentences. DKPro includes the following syntactic similarity metrics, operating on string sequences, and more advanced semantic similarities: – Longest common substring measure (Gusfield, 1997). It determines the length of the longest substring shared by two text segments. – Longest common subsequence measure (Allison and Dix, 1986). It extends the notion of substrings to word subsequences by applying word insertions or deletions to the original input text pairs. – Running-Karp-Rabin Greedy String Tiling (Wise, 1996). It provides a similarity between two sentences by counting the number of shuffles in their subparts. – Resnik similarity (Resnik, 1995). The WordNet hypernymy hierarchy is used to compute a measure of semantic relatedness between concepts expressed in the text. The aggregation algorithm by Mihalcea et al. (Mihalcea et al., 2006) is applied to extend the measure from words to sentences. – Explicit Semantic Analysis (ESA) similarity 44 (Gabrilovich and Markovitch, 2007). It represents documents as weighted vectors of concepts learned from Wikipedia, WordNet and Wiktionary. – Lexical Substituti</context>
</contexts>
<marker>Wise, 1996</marker>
<rawString>Michael J. Wise. 1996. Yap3: Improved detection of similarities in computer program and other texts. In Proceedings of the Twenty-seventh SIGCSE Technical Symposium on Computer Science Education, SIGCSE ’96, pages 130–134, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.50458">
Domain-Specific Image Captioning
</title>
<note confidence="0.89553">
Rebecca Mason and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University, Providence, RI 02912
</note>
<email confidence="0.975872">
{rebecca,ec}@cs.brown.edu
</email>
<sectionHeader confidence="0.98386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999928045454545">
We present a data-driven framework for
image caption generation which incorpo-
rates visual and textual features with vary-
ing degrees of spatial structure. We pro-
pose the task of domain-specific image
captioning, where many relevant visual
details cannot be captured by off-the-shelf
general-domain entity detectors. We ex-
tract previously-written descriptions from
a database and adapt them to new query
images, using a joint visual and textual
bag-of-words model to determine the cor-
rectness of individual words. We imple-
ment our model using a large, unlabeled
dataset of women’s shoes images and nat-
ural language descriptions (Berg et al.,
2010). Using both automatic and human
evaluations, we show that our caption-
ing method effectively deletes inaccurate
words from extracted captions while main-
taining a high level of detail in the gener-
ated output.
</bodyText>
<sectionHeader confidence="0.992534" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999431866666667">
Broadly, the task of image captioning is: given a
query image, generate a natural language descrip-
tion of the image’s visual content. Both the im-
age understanding and language generation com-
ponents of this task are challenging open problems
in their respective fields. A wide variety of ap-
proaches have been proposed in the literature, for
both the specific task of caption generation as well
as related problems in understanding images and
text.
Typically, image understanding systems use su-
pervised algorithms to detect visual entities and
concepts in images. However, these typically re-
quire accurate hand-labeled training data, which
is not available in most specific domains. Ideally,
</bodyText>
<listItem confidence="0.736677285714286">
1. Extract existing human-authored caption according to
similarity of coarse visual features.
Query Image Nearest-Neighbor
Nearest-neighbor caption: This sporty sneaker clog keeps
foot cool and comfortable and fully supported.
2. Estimate correctness of extracted words using domain-
specific joint model of text and visual bag-of-word features.
</listItem>
<bodyText confidence="0.59081">
This sporty sneaker clog keeps foot cool and comfortable and
fully supported.
</bodyText>
<listItem confidence="0.8645665">
3. Compress extracted caption to adapt its content while
maintaining grammatical correctness.
</listItem>
<subsubsectionHeader confidence="0.406631">
Output: This clog keeps foot comfortable and supported.
</subsubsectionHeader>
<bodyText confidence="0.9998335">
a domain-specific image captioning system would
learn in a less supervised fashion, using captioned
images found on the web.
This paper focuses on image caption genera-
tion for a specific domain – images of women’s
shoes, collected from online shopping websites.
Our framework has three main components. We
extract an existing description from a database
of human-captions, by projecting query images
into a multi-dimensional space where structurally
similar images are near each other. We also
train a joint topic model to discover the latent
topics which generate both captions and images.
We combine these two approaches using sentence
compression to delete modifying details in the ex-
tracted caption which are not relevant to the query
image.
Our captioning framework is inspired by sev-
eral recent approaches at the intersection of Nat-
ural Language Processing and Computer Vision.
Previous work such as Farhadi et al. (2010) and
Ordonez et al. (2011) explore extractive methods
for image captioning, but these rely on general-
domain visual detection systems, and only gener-
</bodyText>
<page confidence="0.663078">
11
</page>
<note confidence="0.9287925">
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 11–20,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999824">
ate extractive captions. Other models learn corre-
spondences between domain-specific images and
natural language captions (Berg et al., 2010; Feng
and Lapata, 2010b) but cannot generate descrip-
tions for new images without the use of auxil-
iary text. Kuznetsova et al. (2013) propose a
sentence compression model for editing image
captions, but their compression objective is not
conditioned on a query image, and their system
also requires general-domain visual detections.
This paper proposes an image captioning frame-
work which extends these ideas and culminates in
the first domain-specific image caption generation
system.
More broadly, our goal for image caption gener-
ation is to work toward less supervised captioning
methods which could be used to generate detailed
and accurate descriptions for a variety of long-tail
domains of captioned image data, such as in nature
and medicine.
</bodyText>
<sectionHeader confidence="0.999326" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999865">
Our framework for domain-specific image cap-
tioning consists of three main components: ex-
tractive caption generation, image understanding
through topic modeling, and sentence compres-
sion. 1 These methods have previously been ap-
plied individually to related tasks such as gen-
eral domain image captioning and annotation. We
briefly describe some of the related work:
</bodyText>
<subsectionHeader confidence="0.973506">
2.1 Extractive Caption Generation
</subsectionHeader>
<bodyText confidence="0.999998818181818">
In previous work on image caption extraction, cap-
tions are generated by retrieving human-authored
descriptions from visually similar images. Farhadi
et al. (2010) and Ordonez et al. (2011) retrieve
whole captions to apply to a query image, while
Kuznetsova et al. (2012) generate captions using
text retrieved from multiple sources. The descrip-
tions are related to visual concepts in the query
image, but these models use visual similarity to
approximate textual relevance; they do not model
image and textual features jointly.
</bodyText>
<subsectionHeader confidence="0.998789">
2.2 Image Understanding
</subsectionHeader>
<bodyText confidence="0.982185375">
Recent improvements in state-of-the-art visual ob-
ject class detections (Felzenszwalb et al., 2010)
1A research proposal for this framework and other image
captioning ideas was previously presented at NAACL Stu-
dent Research Workshop in 2013 (Mason, 2013). This paper
presents a completed project including implementation de-
tails and experimental results.
have enabled much recent work in image caption
generation (Farhadi et al., 2010; Ordonez et al.,
2011; Kulkarni et al., 2011; Yang et al., 2011;
Mitchell et al., 2012; Yu and Siskind, 2013). How-
ever, these systems typically rely on a small num-
ber of detection types, e.g. the twenty object cate-
gories from the PASCAL VOC challenge.2 These
object categories include entities which are com-
monly described in general domain images (peo-
ple, cars, cats, etc) but these require labeled train-
ing data which is not typically available for the vi-
sually relevant entities in specific domains.
Our caption generation system employs a multi-
modal topic model from our previous work (Ma-
son and Charniak, 2013) which generates descrip-
tive words, but lacks the spatial structure needed
to generate a full sentence caption. Other previ-
ous work uses topic models to learn the semantic
correspondence between images and labels (e.g.
Blei and Jordan (2003)), but learning from natural
language descriptions is considerably more diffi-
cult because of polysemy, hypernymy, and mis-
alginment between the visual content of an im-
age and the content humans choose to describe.
The MixLDA model (Feng and Lapata, 2010b;
Feng and Lapata, 2010a) learns from news images
and natural language descriptions, but to generate
words for a new image it requires both a query
image and query text in the form of a news arti-
cle. Berg et al. (2010) use discriminative models
to discover visual attributes from online shopping
images and captions, but their models do not gen-
erate descriptive words for unseen images.
</bodyText>
<subsectionHeader confidence="0.999295">
2.3 Sentence Compression
</subsectionHeader>
<bodyText confidence="0.999944142857143">
Typical models for sentence compression (Knight
and Marcu, 2002; Furui et al., 2004; Turner and
Charniak, 2005; Clarke and Lapata, 2008) have a
summarization objective: reduce the length of a
source sentence without changing its meaning. In
contrast, our objective is to change the meaning of
the source sentence, letting its overall correctness
relative to the query image determine the length
of the output. Our objective differs from that of
Kuznetsova et al. (2013), who compress image
caption sentences with the objective of creating a
corpus of generally transferrable image captions.
Their compression objective is to maximize the
probability of a caption conditioned on the source
</bodyText>
<equation confidence="0.565807">
2http://pascallin.ecs.soton.ac.uk/
challenges/VOC/
12
</equation>
<bodyText confidence="0.986429434782609">
Two adjustable buckle
straps top a classic rubber
rain boot grounded by a
thick lug sole for excellent
wet-weather traction.
Available in Plus Size. Faux
snake skin flats with a large
crossover buckle at the toe.
Padded insole for a comfort-
able all day fit.
Glitter-covered elastic up-
per in a two-piece dress san-
dal style with round open
toe. Single vamp strap with
contrasting trim matching
elasticized heel strap criss-
crosses at instep.
Explosive! These white
leather joggers are sure to
make a big impression. De-
tails count, including a toe
overlay, millennium trim
and lightweight raised sole.
</bodyText>
<tableCaption confidence="0.995951">
Table 1: Example data from the Attribute Discovery Dataset (Berg et al., 2010). See Section 3.
</tableCaption>
<bodyText confidence="0.9999495">
image, while our objective is conditioned on the
query image that we are generating a caption for.
Additionally, their model also relies on general-
domain trained visual detections.
</bodyText>
<sectionHeader confidence="0.963583" genericHeader="method">
3 Dataset and Preprocessing
</sectionHeader>
<bodyText confidence="0.999984368421053">
The dataset we use is the women’s shoes sec-
tion of the publicly available Attribute Discov-
ery Dataset3 from Berg et al. (2010), which con-
sists of product images and captions scraped from
the shopping website Like.com. We use the
women’s shoes section of the dataset which has
14764 captioned images. Product descriptions de-
scribe many different attributes such as styles, col-
ors, fabrics, patterns, decorations, and affordances
(activities that can be performed while wearing the
shoe). Some examples are shown in Table 1.
For preprocessing in our framework, we first de-
termine an 80/20% train test split. We define a tex-
tual vocabulary of “descriptive words”, which are
non-function words – adjectives, adverbs, nouns
(except proper nouns), and verbs. This gives us
a total of 9578 descriptive words in the training
set, with an average of 16.33 descriptive words per
caption.
</bodyText>
<sectionHeader confidence="0.974348" genericHeader="method">
4 Image Captioning Framework
</sectionHeader>
<subsectionHeader confidence="0.981239">
4.1 Extraction
</subsectionHeader>
<bodyText confidence="0.99995575">
To repeat, our overall process is to first find a cap-
tion sentence from our database to use as a tem-
plate, and then correct the template sentences us-
ing sentence compresion. We compress by remov-
</bodyText>
<equation confidence="0.470613">
3http://tamaraberg.com/
attributesDataset/index.html
</equation>
<bodyText confidence="0.999947653846154">
ing details that are probably not correct for the test
image. For example, if the sentence describes “a
red slipper” but the shoe in the query image is yel-
low, we want to remove “red” and keep the rest.
As in this simple example, the basic paradigm
for compression is to keep the head words of
phrases (“slipper”) and remove modifiers. Thus
we want to extraction stage of our scheme to be
more likely to find a candidate sentence with cor-
rect head words, figuring that the compression
stage can edit the mistakes. Our hypothesis is that
headwords tend to describe more spatially struc-
tured visual concepts, while modifier words de-
scribe those that are more easily represented using
local or unstructured features.4 Table 2 contains
additional example captions with parses.
GIST (Oliva and Torralba, 2001) is a com-
monly used feature in Computer Vision which
coarsely localizes perceptual attributes (e.g. rough
vs smooth, natural vs manmade). By computing
the GIST of the images, we project them into a
multi-dimensional Euclidean space where images
with semantically similar structures are located
near each other. Thus the extraction stage of our
caption generation process selects a sentence from
the GIST nearest-neighbor to the query image.5
</bodyText>
<subsectionHeader confidence="0.991353">
4.2 Joint Topic Model
</subsectionHeader>
<bodyText confidence="0.925513285714286">
The second component of our framework incorpo-
rates visual and textual features using a less struc-
tured model. We use a multi-modal topic model
4For example, the color “red” can be described using a
bag of random pixels, while a “slipper” is a spatial configura-
tion of parts in relationship to each other.
5See Section 5.1 for additional implementation details.
</bodyText>
<page confidence="0.8713">
13
</page>
<tableCaption confidence="0.626113">
Table 2: Example parses of women’s shoes descriptions. Our hypothesis is that the headwords in phrases
</tableCaption>
<bodyText confidence="0.998041923076923">
are more likely to describe visual concepts which rely on spatial locations or relationships, while modi-
fiers words can be represented using less-structured visual bag-of-words features.
to learn the latent topics which generate bag-of-
words features for an image and its caption.
The bag-of-words model for Computer Vision
represents images as a mixture of topics. Mea-
sures of shape, color, texture, and intensity are
computed at various points on the image and clus-
tered into discrete “codewords” using the k-means
algorithm.6 Unlike text words, an individual code-
word has little meaning on its own, but distri-
butions of codewords can provide a meaningful,
though unstructured, representation of an image.
An image and its caption do not express exactly
the same information, but they are topically re-
lated. We employ the Polylingual Topic Model
(Mimno et al., 2009), which is originally used to
model corresponding documents in different lan-
guages that are topically comparable, but not par-
allel translations. In particular, we employ our
previous work (Mason and Charniak, 2013) which
extends this model to topically similar images and
natural language captions. The generative process
for a captioned image starts with a single topic
distribution drawn from concentration parameter
α and base measure m:
</bodyText>
<equation confidence="0.947613">
θ ∼ Dir(θ, αm) (1)
</equation>
<bodyText confidence="0.994313666666667">
Modality-specific latent topic assignments zimg
and ztxt are drawn for each of the text words and
codewords:
</bodyText>
<equation confidence="0.963831">
�zimg ∼ P (zimg|θ) =
n
</equation>
<bodyText confidence="0.981743333333333">
6While space limits a more detailed explanation of visual
bag-of-word features, Section 5.2 provides a brief overview
of the specific visual attributes used in this model.
</bodyText>
<equation confidence="0.966328">
�ztxt ∼ P (ztxt|θ) =
n
</equation>
<bodyText confidence="0.998726">
Observed words are generated according to their
probabilities in the modality-specific topics:
</bodyText>
<equation confidence="0.9757176">
wimg ∼ P(wimg|zimg, Φimg) = φimg |zimg
wimg n (4)
n
wtxt ∼ P(wtxt|ztxt, Φtxt) = φtxt ztnxt (5)
wn
</equation>
<bodyText confidence="0.99961275">
Given the uncaptioned query image qimg and
the trained multi-modal topic model, it is now pos-
sible to infer the shared topic proportion for qimg
using Gibbs sampling:
</bodyText>
<equation confidence="0.991439">
P(zn = t|qimg, z\n, Φimg, αm)
(Nt)\n + αmt (6)
Et Nt − 1 + α
</equation>
<subsectionHeader confidence="0.993872">
4.3 Sentence Compression
</subsectionHeader>
<bodyText confidence="0.999582666666667">
Let w = w1, w2, ..., wn be the words in the ex-
tracted caption for qimg. For each word, we de-
fine a binary decision variable δ, such that δi = 1
if wi is included in the output compression, and
δi = 0 otherwise. Our objective is to find values
of δ which generate a caption for qimg which is
both semantically and grammatically correct.
We cast this problem as an Integer Linear Pro-
gram (ILP), which has previously been used for
the standard sentence compression task (Clarke
and Lapata, 2008; Martins and Smith, 2009). ILP
is a mathematical optimization method for deter-
mining the optimal values of integer variables in
order to maximize an objective given a set of con-
straints.
</bodyText>
<figure confidence="0.922604666666667">
(2)
θimg
zn
θztxt (3)
n
∝ φimg
img|tqn
14
4.3.1 Objective
</figure>
<bodyText confidence="0.998906875">
The ILP objective is a weighted linear combina-
tion of two measures which represent the correct-
ness and fluency of the output compression:
Correctness: Recall in Section 3 we defined
words as either descriptive words or function
words. For each descriptive word, we estimate
P(wi|qimg), using topic proportions estimated us-
ing Equation 6:
</bodyText>
<equation confidence="0.865433333333333">
� P(wi|ztxt
P(wi|qimg) = t )P(zt|qimg) (7)
t
</equation>
<bodyText confidence="0.8627545">
This is used to find I(wi), a function of the likeli-
hood of each word in the extracted caption:
</bodyText>
<table confidence="0.999752304347826">
Sequential 1.) Pi αi = 1
k−2 k−1
2.) Sk − αk − Pi=0 Pj=1 7ijk = 0
�` [��` `dk : k ∈ 1...n
3.) Sj −/moi=0 /mak=j+1 7ijk − PLO Nij = 0
∀j: j ∈ 1...n
4.) Pn−1 Pn k=j+1 7ijk − Pn j=i+1 Nij −
j=i+1
Pi−1 /�
h=0 0hi − Si = 0
∀i : i ∈ 1...n
En−1 n
5.) �i=0 Pn
aij = 1
Modifier 1. If head of the extracted sentence= wi, then
Si = 1
2. If wi is head of a noun phrase, then Si = 1
3. Punctuation and coordinating conjunctions
follow special rules (below). Otherwise, if
headof(wi) = wj, then Si ≤ Sj
Other 1. Pi Si ≥ 3
2. Define valid use of puncutation and coordi-
nating conjunctions.
</table>
<tableCaption confidence="0.999405">
Table 3: Summary of ILP constraints.
</tableCaption>
<equation confidence="0.862333">
�
I(wi)
P(wi  |qimg) − P(wi), if descriptive
=
</equation>
<bodyText confidence="0.999568714285714">
0, function word
(8)
This function considers the prior probability of wi
because frequent words often have a high posterior
probability even when they are inaccurate. Thus
the sum Eni=1 Si · I(wi) is the overall measure of
the correctness of a proposed caption conditioned
on qimg.
Fluency: We formulate a trigram language
model as an ILP, which requires additional binary
decision variables: αi = 1 if wi begins the out-
put compression, Qij = 1 if the bigram sequence
wi, wj ends the compression, -Yijk = 1 if the tri-
gram sequence wi, wj, wk is in the compression,
and a special “start token” S0 = 1. This language
model favors shorter sentences, which is not nec-
essarily the objective for image captioning, so we
introduce a weighting factor, A, to lessen the ef-
fect.
Here is the combined objective, using P to rep-
resent log P:
</bodyText>
<subsubsectionHeader confidence="0.586823">
4.3.2 ILP Constraints
</subsubsectionHeader>
<bodyText confidence="0.999943857142857">
The ILP constraints ensure both the mathematical
validity of the model, and the grammatical correct-
ness of its output. Table 3 summarizes the list of
constraints. Sequential constraints are defined as
in Clarke (2008) ensure that the ordering of the tri-
grams is valid, and that the mathematical validity
of the model holds.
</bodyText>
<sectionHeader confidence="0.985495" genericHeader="method">
5 Implementation Details
</sectionHeader>
<subsectionHeader confidence="0.956335">
5.1 Extraction
</subsectionHeader>
<bodyText confidence="0.999902235294117">
GIST features are computed using code by Oliva
and Torralba (2001)7. GIST is computed with im-
ages converted to grayscale; since color features
tend to act as modifiers in this domain. Nearest-
neighbors are selected according to minimum dis-
tance from qimg to both a regularly-oriented and a
horizontally-flipped training image.
Only one sentence from the first nearest-
neighbor caption is extracted. In the case of multi-
sentence captions, we select the first suitable sen-
tence according to the following criteria 1.) has
at least five tokens, 2.) does not contain NNP or
NNPS (brand names), 3.) does not fail to parse
using Stanford Parser (Klein and Manning, 2003).
If the nearest-neighbor caption does not have any
sentences meeting these criteria, caption sentences
from the next nearest-neighbor(s) are considered.
</bodyText>
<figure confidence="0.949002466666667">
n
max z = αi · P(wi|start)
i=1
n−2� n−1� n -Yijk · P(wk|wi, wj)
+ j=i+1 k=j+1
i=1
n−1� n �Qij · P(end|wi, wj) · A
+ j=i+1
i=0
n Si · I(wi) (9)
+
i=1
7http://people.csail.mit.edu/torralba/
code/spatialenvelope/
15
</figure>
<subsectionHeader confidence="0.986758">
5.2 Joint Topic Model
</subsectionHeader>
<bodyText confidence="0.999959652173913">
We use the Joint Topic Model that we imple-
mented in our previous work; please see Mason
and Charniak (2013) for the full model and imple-
mentation details. The topic model is trained with
200 topics using the polylingual topic model im-
plementation from MALLET8. Briefly, the code-
words represent the following attributes:
SHAPE: SIFT (Lowe, 1999) describes the
shapes of detected edges in the image, using de-
scriptors which are invariant to changes in rotation
and scale.
COLOR: RGB (red, green, blue) and HSV (hue,
saturation, value) pixel values are sampled from a
central area of the image to represent colors.
TEXTURE: Textons (Leung and Malik, 2001)
are computed by convolving images with Gabor
filters at multiple orientations and scales, then
sampling the outputs at random locations.
INTENSITY: HOG (histogram of gradients)
(Dalal and Triggs, 2005) describes the direction
and intensity of changes in light. These features
are computed on the image over a densely sam-
pled grid.
</bodyText>
<subsectionHeader confidence="0.985753">
5.3 Compression
</subsectionHeader>
<bodyText confidence="0.999924090909091">
The sentence compression ILP is implemented us-
ing the CPLEX optimization toolkit9. The lan-
guage model weighting factor in the objective is
λ = 10−3, which was hand-tuned according to
observed output. The trigram language model
is trained on training set captions using Berke-
leyLM (Pauls and Klein, 2011) with Kneser-Ney
smoothing. For the constraints, we use parses
from Stanford Parser (Klein and Manning, 2003)
and the “semantic head” variation of the Collins
headfinder Collins (1999).
</bodyText>
<sectionHeader confidence="0.995723" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.981462">
6.1 Setup
</subsectionHeader>
<bodyText confidence="0.981295571428571">
We compare the following systems and baselines:
KL (EXTRACTION): The top performing ex-
tractive model from Feng and Lapata (2010a), and
the second-best captioning model overall. Using
estimated topic distributions from our joint model,
we extract the source with minimum KL Diver-
gence from qimg.
</bodyText>
<footnote confidence="0.669492">
8http://mallet.cs.umass.edu/
9http://www-01.ibm.com/
</footnote>
<table confidence="0.938719736842105">
software/integration/optimization/
cplex-optimization-studio/
ROUGE-2 Average 95% Confidence int.
KL (EXTRACTION)
P .06114 ( .05690 - .06554 )
R .02499 ( .02325 - .02686)
F .03360 ( .03133 - .03600 )
GIST (EXTRACTION)
P .10894 ( .09934 - .11921 )
R .05474 ( .04926 - .06045)
F .06863 ( .06207 - .07534)
LM-ONLY (COMPRESSION)
P .13782 ( .12602 - .14864 )
R .02437 ( .02193 - .02700 )
F .03864 ( .03512 - .04229)
SYSTEM (COMPRESSION)
P .16752 (.15679 -.17882 )
R .05060 ( .04675 - .05524 )
F .07204 ( .06685 - .07802 )
</table>
<tableCaption confidence="0.97678">
Table 4: ROUGE-2 (bigram) scores. The pre-
</tableCaption>
<bodyText confidence="0.99128780952381">
cision of our system compression (bolded) sig-
nificantly improves over the caption that it com-
presses (GIST), without a significant decrease in
recall.
GIST (EXTRACTION): The sentence extracted
using GIST nearest-neighbors, and the uncom-
pressed source for the compression systems.
LM-ONLY (COMPRESSION): We include this
baseline to demonstrate that our model is effec-
tively conditioning output compressions on qimg,
as opposed to simply generalizing captions as in
Kuznetsova et al. (2013)10. We modify the com-
pression ILP to ignore the content objective and
only maximize the trigram language model (still
subject to the constraints).
SYSTEM (COMPRESSION): Our full system.
Unfortunately, we cannot compare our system
against prior work in general-domain image cap-
tioning, because those models use visual detec-
tion systems which train on labeled data that is not
available in our domain.
</bodyText>
<subsectionHeader confidence="0.999652">
6.2 Automatic Evaluation
</subsectionHeader>
<bodyText confidence="0.976379875">
We perform automatic evaluation using similar-
ity measures between automatically generated and
human-authored captions. Note that currently
our system and baselines only generate single-
sentence captions, but we compare against entire
10Technically their model is conditioned on the source im-
age, in order to address alignment issues which are not appli-
cable in our setup.
</bodyText>
<page confidence="0.645706">
16
</page>
<table confidence="0.9980696">
BLEU@1
KL (EXTRACTION) .2098
GIST (EXTRACTION) .4259
LM-ONLY (COMPRESSION) .4780
SYSTEM (COMPRESSION) .4841
</table>
<tableCaption confidence="0.971698">
Table 5: BLEU@1 scores of generated captions
</tableCaption>
<bodyText confidence="0.9912669375">
against human authored captions. Our model
(bolded) has the highest BLEU@1 score with sig-
nificance.
held-out captions in order to increase the amount
of text we have to compare against.
ROUGE (Lin, 2004) is a summarization eval-
uation metric which has also been used to eval-
uate image captions (Yang et al., 2011). It is
usually a recall-oriented measure, but we also re-
port precision and f-measure because our sen-
tence compressions do not improve recall. Table 4
shows ROUGE-2 (bigram) scores computed with-
out stopwords.
We observe that our system very significantly
improves ROUGE-2 precision of the GIST ex-
tracted caption, without significantly reducing re-
call. While LM-Only also improves precision
against GIST extraction, it indiscriminately re-
moves some words which are relevant to the
query image. We also observe that GIST extrac-
tion strongly outperforms the KL model, which
demonstrates the importance of visual structure.
We also report BLEU (Papineni et al., 2002)
scores, which are the most popularly accepted au-
tomatic metric for captioning evaluation (Farhadi
et al., 2010; Kulkarni et al., 2011; Ordonez et
al., 2011; Kuznetsova et al., 2012; Kuznetsova
et al., 2013). Results are very similar to the
ROUGE-2 precision scores, except the difference
between our system and LM-Only is less pro-
nounced because BLEU counts function words,
while ROUGE does not.
</bodyText>
<subsectionHeader confidence="0.99938">
6.3 Human Evaluation
</subsectionHeader>
<bodyText confidence="0.998690555555555">
We perform human evaluation of compressions
generated by our system and LM-Only. Users are
shown the query image, the original uncompressed
caption, and a compressed caption, and are asked
two questions: does the compression improve the
accuracy of the caption, and is the compression
grammatical.
We collect 553 judgments from six women who
are native English-speakers and knowledgeable
</bodyText>
<table confidence="0.970831318181818">
Query Image GIST Nearest-Neighbor
Extraction: Shimmering snake-embossed leather upper in a
slingback evening dress sandal style with a round open toe.
Compression: Shimmering upper in a slingback evening
dress sandal style with a round open toe.
Query Image GIST Nearest-Neighbor
Extraction: This sporty sneaker clog keeps foot cool and
comfortable and fully supported.
Compression: This clog keeps foot comfortable and sup-
ported.
Query Image GIST Nearest-Neighbor
Extraction: Italian patent leather peep-toe ballet flat with a
signature tailored grosgrain bow.
Compression: leather ballet flat with a signature tailored
grosgrain bow.
Query Image GIST Nearest-Neighbor
Extraction: Platform high heel open toe pump with horsebit
available in silver guccissima leather with nickel hardware
with leather sole.
Compression: Platform high heel open toe pump with
horsebit available in leather with nickel hardware with
leather sole.
</table>
<tableCaption confidence="0.770095666666667">
Table 6: Example output from our full system.
Red underlined words indicate the words which
are deleted by our compression model.
</tableCaption>
<table confidence="0.9714913">
17
SYSTEM LM-ONLY
Yes No Yes No
Compression 63.2% 36.8% 42.6% 57.4%
improves
accuracy
Compression is 73.1% 26.9% 82.2% 17.8%
grammatical
Query Image GIST Nearest-Neighbor
Extraction: Classic ballet flats with decorative canvas
</table>
<tableCaption confidence="0.999105">
Table 7: Human evaluation results.
</tableCaption>
<bodyText confidence="0.999560066666667">
about fashion.11 Users were recruited via email
and did the study over the internet.
Table 7 reports the results of the human evalu-
ation. Users report 63.2% of SYSTEM compres-
sions improve accuracy over the original, while
the other 36.8% did not improve accuracy. (Keep
in mind that a bad compression does not make the
caption less accurate, just less descriptive.) LM-
ONLY improves accuracy for less than half of the
captions, which is significantly worse than SYS-
TEM captions (Fisher exact test, two-tailed p less
than 0.01).
Users find LM-Only compressions to be slightly
more grammatical than System compressions, but
the difference is not significant. (p &gt; 0.05)
</bodyText>
<sectionHeader confidence="0.996434" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.980230725">
We introduce the task of domain-specific image
captioning and propose a captioning system which
is trained on online shopping images and natu-
ral language descriptions. We learn a joint topic
model of vision and text to estimate the correct-
ness of extracted captions, and use a sentence
compression model to propose a more accurate
output caption. Our model exploits the connection
between image and sentence structure, and can be
used to improve the accuracy of extracted image
captions.
The task of domain-specific image caption
generation has been overlooked in favor of the
general-domain case, but we believe the domain-
specific case deserves more attention. While
image captioning can be viewed as a complex
grounding problem, a good image caption should
do more than label the objects in the image. When
an expert looks at images in a specific domain, he
or she makes inferences that would not be made by
a non-expert. Providing this information to non-
11About 15% of output compressions are the same for both
systems, and about 10% have no deleted words in the output
compression. We include the former in the human evaluation,
but not the latter.
strap and patent leather covered buckle.
Compression: Classic ballet flats covered.
Query Image GIST Nearest-Neighbor
Extraction: This shoe is the perfect shoe for you , fea-
turing an open toe and a lace up upper with a high heel
, and a two tone color.
Compression: This shoe is the shoe , featuring an open toe
and upper with a high heel.
Table 8: Examples of bad performance. The top
example is a parse error, while the bottom exam-
ple deletes modifiers that are not part of the image
description.
expert users in the form of an image caption will
greatly expand the utility for automatic image cap-
tioning.
</bodyText>
<sectionHeader confidence="0.983135" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995840428571429">
Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.
2010. Automatic attribute discovery and charac-
terization from noisy web data. In Proceedings of
the 11th European conference on Computer vision:
Part I, ECCV’10, pages 663–676, Berlin, Heidel-
berg. Springer-Verlag.
David M. Blei and Michael I. Jordan. 2003. Modeling
annotated data. In Proceedings of the 26th annual
international ACM SIGIR conference on Research
and development in informaion retrieval, SIGIR ’03,
pages 127–134, New York, NY, USA. ACM.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression an integer linear pro-
gramming approach. J. Artif. Int. Res., 31(1):399–
429, March.
James Clarke. 2008. Global Inference for Sentence
Compression: An Integer Linear Programming Ap-
proach. Dissertation, University of Edinburgh.
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
Philadelphia, PA, USA. AAI9926110.
</reference>
<page confidence="0.784215">
18
</page>
<reference confidence="0.997250936363637">
N. Dalal and B. Triggs. 2005. Histograms of oriented
gradients for human detection. In Computer Vision
and Pattern Recognition, 2005. CVPR 2005. IEEE
Computer Society Conference on, volume 1, pages
886 –893 vol. 1, june.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences from images.
In Proceedings of the 11th European conference on
Computer vision: Part IV, ECCV’10, pages 15–29,
Berlin, Heidelberg. Springer-Verlag.
P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and
D. Ramanan. 2010. Object detection with discrim-
inatively trained part based models. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
32(9):1627–1645.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? automatic caption genera-
tion for news images. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 1239–1249, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Yansong Feng and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In
HLT-NAACL, pages 831–839.
Sadaoki Furui, Tomonori Kikuchi, Yousuke Shinnaka,
and Chiori Hori. 2004. Speech-to-text and speech-
to-speech summarization of spontaneous speech.
IEEE TRANS. ON SPEECH AND AUDIO PRO-
CESSING, 12(4):401–408.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 423–
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kevin Knight and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artif. Intell.,
139(1):91–107, July.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and gener-
ating simple image descriptions. In CVPR, pages
1601–1608.
Polina Kuznetsova, Vicente Ordonez, Alexander C.
Berg, Tamara L. Berg, and Yejin Choi. 2012. Col-
lective generation of natural image descriptions. In
ACL.
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2013. Generalizing
image captions for image-text parallel corpus. In
ACL.
T. Leung and J. Malik. 2001. Representing and rec-
ognizing the visual appearance of materials using
three-dimensional textons. International Journal of
Computer Vision, 43(1):29–44.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
D.G. Lowe. 1999. Object recognition from local scale-
invariant features. In Computer Vision, 1999. The
Proceedings of the Seventh IEEE International Con-
ference on, volume 2, pages 1150 –1157 vol.2.
Andr´e F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, ILP ’09, pages 1–9, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
R. Mason and E. Charniak. 2013. Annotation of online
shopping images without labeled training examples.
Workshop on Vision and Language (WVL).
Rebecca Mason. 2013. Domain-independent caption-
ing of domain-specific images. NAACL Student Re-
search Workshop.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the
2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 2 - Volume
2, EMNLP ’09, pages 880–889, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Men-
sch, Alexander C. Berg, Tamara L. Berg, and Hal
Daum´e III. 2012. Midge: Generating image de-
scriptions from computer vision detections. In Euro-
pean Chapter of the Association for Computational
Linguistics (EACL).
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: A holistic representation of the
spatial envelope. International Journal of Computer
Vision, 42:145–175.
V. Ordonez, G. Kulkarni, and T.L. Berg. 2011.
Im2text: Describing images using 1 million cap-
tioned photographs. In NIPS.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.593904">
19
</page>
<reference confidence="0.998805045454545">
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of ACL,
Portland, Oregon, June. Association for Computa-
tional Linguistics.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ’05, pages 290–297, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods
in Natural Language Processing (EMNLP), Edin-
burgh, Scotland.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 53–63,
Sofia, Bulgaria. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.609049">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.457807">
<title confidence="0.999887">Domain-Specific Image Captioning</title>
<author confidence="0.935588">Mason</author>
<affiliation confidence="0.7493765">Brown Laboratory for Linguistic Information Processing Brown University, Providence, RI</affiliation>
<abstract confidence="0.994179043478261">We present a data-driven framework for image caption generation which incorporates visual and textual features with varying degrees of spatial structure. We propose the task of domain-specific image captioning, where many relevant visual details cannot be captured by off-the-shelf general-domain entity detectors. We extract previously-written descriptions from a database and adapt them to new query images, using a joint visual and textual bag-of-words model to determine the correctness of individual words. We implement our model using a large, unlabeled dataset of women’s shoes images and natural language descriptions (Berg et al., 2010). Using both automatic and human evaluations, we show that our captioning method effectively deletes inaccurate words from extracted captions while maintaining a high level of detail in the generated output.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tamara L Berg</author>
<author>Alexander C Berg</author>
<author>Jonathan Shih</author>
</authors>
<title>Automatic attribute discovery and characterization from noisy web data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th European conference on Computer vision: Part I, ECCV’10,</booktitle>
<pages>663--676</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="849" citStr="Berg et al., 2010" startWordPosition="117" endWordPosition="120">amework for image caption generation which incorporates visual and textual features with varying degrees of spatial structure. We propose the task of domain-specific image captioning, where many relevant visual details cannot be captured by off-the-shelf general-domain entity detectors. We extract previously-written descriptions from a database and adapt them to new query images, using a joint visual and textual bag-of-words model to determine the correctness of individual words. We implement our model using a large, unlabeled dataset of women’s shoes images and natural language descriptions (Berg et al., 2010). Using both automatic and human evaluations, we show that our captioning method effectively deletes inaccurate words from extracted captions while maintaining a high level of detail in the generated output. 1 Introduction Broadly, the task of image captioning is: given a query image, generate a natural language description of the image’s visual content. Both the image understanding and language generation components of this task are challenging open problems in their respective fields. A wide variety of approaches have been proposed in the literature, for both the specific task of caption gen</context>
<context position="3732" citStr="Berg et al., 2010" startWordPosition="547" endWordPosition="550">d by several recent approaches at the intersection of Natural Language Processing and Computer Vision. Previous work such as Farhadi et al. (2010) and Ordonez et al. (2011) explore extractive methods for image captioning, but these rely on generaldomain visual detection systems, and only gener11 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 11–20, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics ate extractive captions. Other models learn correspondences between domain-specific images and natural language captions (Berg et al., 2010; Feng and Lapata, 2010b) but cannot generate descriptions for new images without the use of auxiliary text. Kuznetsova et al. (2013) propose a sentence compression model for editing image captions, but their compression objective is not conditioned on a query image, and their system also requires general-domain visual detections. This paper proposes an image captioning framework which extends these ideas and culminates in the first domain-specific image caption generation system. More broadly, our goal for image caption generation is to work toward less supervised captioning methods which cou</context>
<context position="7208" citStr="Berg et al. (2010)" startWordPosition="1094" endWordPosition="1097">sentence caption. Other previous work uses topic models to learn the semantic correspondence between images and labels (e.g. Blei and Jordan (2003)), but learning from natural language descriptions is considerably more difficult because of polysemy, hypernymy, and misalginment between the visual content of an image and the content humans choose to describe. The MixLDA model (Feng and Lapata, 2010b; Feng and Lapata, 2010a) learns from news images and natural language descriptions, but to generate words for a new image it requires both a query image and query text in the form of a news article. Berg et al. (2010) use discriminative models to discover visual attributes from online shopping images and captions, but their models do not generate descriptive words for unseen images. 2.3 Sentence Compression Typical models for sentence compression (Knight and Marcu, 2002; Furui et al., 2004; Turner and Charniak, 2005; Clarke and Lapata, 2008) have a summarization objective: reduce the length of a source sentence without changing its meaning. In contrast, our objective is to change the meaning of the source sentence, letting its overall correctness relative to the query image determine the length of the outp</context>
<context position="8821" citStr="Berg et al., 2010" startWordPosition="1343" endWordPosition="1346">t grounded by a thick lug sole for excellent wet-weather traction. Available in Plus Size. Faux snake skin flats with a large crossover buckle at the toe. Padded insole for a comfortable all day fit. Glitter-covered elastic upper in a two-piece dress sandal style with round open toe. Single vamp strap with contrasting trim matching elasticized heel strap crisscrosses at instep. Explosive! These white leather joggers are sure to make a big impression. Details count, including a toe overlay, millennium trim and lightweight raised sole. Table 1: Example data from the Attribute Discovery Dataset (Berg et al., 2010). See Section 3. image, while our objective is conditioned on the query image that we are generating a caption for. Additionally, their model also relies on generaldomain trained visual detections. 3 Dataset and Preprocessing The dataset we use is the women’s shoes section of the publicly available Attribute Discovery Dataset3 from Berg et al. (2010), which consists of product images and captions scraped from the shopping website Like.com. We use the women’s shoes section of the dataset which has 14764 captioned images. Product descriptions describe many different attributes such as styles, co</context>
</contexts>
<marker>Berg, Berg, Shih, 2010</marker>
<rawString>Tamara L. Berg, Alexander C. Berg, and Jonathan Shih. 2010. Automatic attribute discovery and characterization from noisy web data. In Proceedings of the 11th European conference on Computer vision: Part I, ECCV’10, pages 663–676, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Michael I Jordan</author>
</authors>
<title>Modeling annotated data.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR ’03,</booktitle>
<pages>127--134</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6737" citStr="Blei and Jordan (2003)" startWordPosition="1013" endWordPosition="1016">AL VOC challenge.2 These object categories include entities which are commonly described in general domain images (people, cars, cats, etc) but these require labeled training data which is not typically available for the visually relevant entities in specific domains. Our caption generation system employs a multimodal topic model from our previous work (Mason and Charniak, 2013) which generates descriptive words, but lacks the spatial structure needed to generate a full sentence caption. Other previous work uses topic models to learn the semantic correspondence between images and labels (e.g. Blei and Jordan (2003)), but learning from natural language descriptions is considerably more difficult because of polysemy, hypernymy, and misalginment between the visual content of an image and the content humans choose to describe. The MixLDA model (Feng and Lapata, 2010b; Feng and Lapata, 2010a) learns from news images and natural language descriptions, but to generate words for a new image it requires both a query image and query text in the form of a news article. Berg et al. (2010) use discriminative models to discover visual attributes from online shopping images and captions, but their models do not genera</context>
</contexts>
<marker>Blei, Jordan, 2003</marker>
<rawString>David M. Blei and Michael I. Jordan. 2003. Modeling annotated data. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR ’03, pages 127–134, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression an integer linear programming approach.</title>
<date>2008</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>429</pages>
<contexts>
<context position="7538" citStr="Clarke and Lapata, 2008" startWordPosition="1143" endWordPosition="1146"> content humans choose to describe. The MixLDA model (Feng and Lapata, 2010b; Feng and Lapata, 2010a) learns from news images and natural language descriptions, but to generate words for a new image it requires both a query image and query text in the form of a news article. Berg et al. (2010) use discriminative models to discover visual attributes from online shopping images and captions, but their models do not generate descriptive words for unseen images. 2.3 Sentence Compression Typical models for sentence compression (Knight and Marcu, 2002; Furui et al., 2004; Turner and Charniak, 2005; Clarke and Lapata, 2008) have a summarization objective: reduce the length of a source sentence without changing its meaning. In contrast, our objective is to change the meaning of the source sentence, letting its overall correctness relative to the query image determine the length of the output. Our objective differs from that of Kuznetsova et al. (2013), who compress image caption sentences with the objective of creating a corpus of generally transferrable image captions. Their compression objective is to maximize the probability of a caption conditioned on the source 2http://pascallin.ecs.soton.ac.uk/ challenges/V</context>
<context position="14542" citStr="Clarke and Lapata, 2008" startWordPosition="2283" endWordPosition="2286">pic proportion for qimg using Gibbs sampling: P(zn = t|qimg, z\n, Φimg, αm) (Nt)\n + αmt (6) Et Nt − 1 + α 4.3 Sentence Compression Let w = w1, w2, ..., wn be the words in the extracted caption for qimg. For each word, we define a binary decision variable δ, such that δi = 1 if wi is included in the output compression, and δi = 0 otherwise. Our objective is to find values of δ which generate a caption for qimg which is both semantically and grammatically correct. We cast this problem as an Integer Linear Program (ILP), which has previously been used for the standard sentence compression task (Clarke and Lapata, 2008; Martins and Smith, 2009). ILP is a mathematical optimization method for determining the optimal values of integer variables in order to maximize an objective given a set of constraints. (2) θimg zn θztxt (3) n ∝ φimg img|tqn 14 4.3.1 Objective The ILP objective is a weighted linear combination of two measures which represent the correctness and fluency of the output compression: Correctness: Recall in Section 3 we defined words as either descriptive words or function words. For each descriptive word, we estimate P(wi|qimg), using topic proportions estimated using Equation 6: � P(wi|ztxt P(wi</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression an integer linear programming approach. J. Artif. Int. Res., 31(1):399– 429, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
</authors>
<title>Global Inference for Sentence Compression: An Integer Linear Programming Approach. Dissertation,</title>
<date>2008</date>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="16992" citStr="Clarke (2008)" startWordPosition="2739" endWordPosition="2740">ram sequence wi, wj ends the compression, -Yijk = 1 if the trigram sequence wi, wj, wk is in the compression, and a special “start token” S0 = 1. This language model favors shorter sentences, which is not necessarily the objective for image captioning, so we introduce a weighting factor, A, to lessen the effect. Here is the combined objective, using P to represent log P: 4.3.2 ILP Constraints The ILP constraints ensure both the mathematical validity of the model, and the grammatical correctness of its output. Table 3 summarizes the list of constraints. Sequential constraints are defined as in Clarke (2008) ensure that the ordering of the trigrams is valid, and that the mathematical validity of the model holds. 5 Implementation Details 5.1 Extraction GIST features are computed using code by Oliva and Torralba (2001)7. GIST is computed with images converted to grayscale; since color features tend to act as modifiers in this domain. Nearestneighbors are selected according to minimum distance from qimg to both a regularly-oriented and a horizontally-flipped training image. Only one sentence from the first nearestneighbor caption is extracted. In the case of multisentence captions, we select the fir</context>
</contexts>
<marker>Clarke, 2008</marker>
<rawString>James Clarke. 2008. Global Inference for Sentence Compression: An Integer Linear Programming Approach. Dissertation, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<booktitle>Ph.D. thesis,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="19677" citStr="Collins (1999)" startWordPosition="3173" endWordPosition="3174">ction and intensity of changes in light. These features are computed on the image over a densely sampled grid. 5.3 Compression The sentence compression ILP is implemented using the CPLEX optimization toolkit9. The language model weighting factor in the objective is λ = 10−3, which was hand-tuned according to observed output. The trigram language model is trained on training set captions using BerkeleyLM (Pauls and Klein, 2011) with Kneser-Ney smoothing. For the constraints, we use parses from Stanford Parser (Klein and Manning, 2003) and the “semantic head” variation of the Collins headfinder Collins (1999). 6 Evaluation 6.1 Setup We compare the following systems and baselines: KL (EXTRACTION): The top performing extractive model from Feng and Lapata (2010a), and the second-best captioning model overall. Using estimated topic distributions from our joint model, we extract the source with minimum KL Divergence from qimg. 8http://mallet.cs.umass.edu/ 9http://www-01.ibm.com/ software/integration/optimization/ cplex-optimization-studio/ ROUGE-2 Average 95% Confidence int. KL (EXTRACTION) P .06114 ( .05690 - .06554 ) R .02499 ( .02325 - .02686) F .03360 ( .03133 - .03600 ) GIST (EXTRACTION) P .10894 </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael John Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, Philadelphia, PA, USA. AAI9926110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Dalal</author>
<author>B Triggs</author>
</authors>
<title>Histograms of oriented gradients for human detection.</title>
<date>2005</date>
<journal>IEEE Computer Society Conference on,</journal>
<booktitle>In Computer Vision and Pattern Recognition,</booktitle>
<volume>1</volume>
<pages>pages</pages>
<contexts>
<context position="19044" citStr="Dalal and Triggs, 2005" startWordPosition="3071" endWordPosition="3074"> implementation from MALLET8. Briefly, the codewords represent the following attributes: SHAPE: SIFT (Lowe, 1999) describes the shapes of detected edges in the image, using descriptors which are invariant to changes in rotation and scale. COLOR: RGB (red, green, blue) and HSV (hue, saturation, value) pixel values are sampled from a central area of the image to represent colors. TEXTURE: Textons (Leung and Malik, 2001) are computed by convolving images with Gabor filters at multiple orientations and scales, then sampling the outputs at random locations. INTENSITY: HOG (histogram of gradients) (Dalal and Triggs, 2005) describes the direction and intensity of changes in light. These features are computed on the image over a densely sampled grid. 5.3 Compression The sentence compression ILP is implemented using the CPLEX optimization toolkit9. The language model weighting factor in the objective is λ = 10−3, which was hand-tuned according to observed output. The trigram language model is trained on training set captions using BerkeleyLM (Pauls and Klein, 2011) with Kneser-Ney smoothing. For the constraints, we use parses from Stanford Parser (Klein and Manning, 2003) and the “semantic head” variation of the </context>
</contexts>
<marker>Dalal, Triggs, 2005</marker>
<rawString>N. Dalal and B. Triggs. 2005. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886 –893 vol. 1, june.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Mohsen Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>Peter Young</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every picture tells a story: generating sentences from images.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th European conference on Computer vision: Part IV, ECCV’10,</booktitle>
<pages>15--29</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="3261" citStr="Farhadi et al. (2010)" startWordPosition="482" endWordPosition="485">extract an existing description from a database of human-captions, by projecting query images into a multi-dimensional space where structurally similar images are near each other. We also train a joint topic model to discover the latent topics which generate both captions and images. We combine these two approaches using sentence compression to delete modifying details in the extracted caption which are not relevant to the query image. Our captioning framework is inspired by several recent approaches at the intersection of Natural Language Processing and Computer Vision. Previous work such as Farhadi et al. (2010) and Ordonez et al. (2011) explore extractive methods for image captioning, but these rely on generaldomain visual detection systems, and only gener11 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 11–20, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics ate extractive captions. Other models learn correspondences between domain-specific images and natural language captions (Berg et al., 2010; Feng and Lapata, 2010b) but cannot generate descriptions for new images without the use of auxiliary text. Kuznetsova et al. (2</context>
<context position="5057" citStr="Farhadi et al. (2010)" startWordPosition="747" endWordPosition="750">d image data, such as in nature and medicine. 2 Related Work Our framework for domain-specific image captioning consists of three main components: extractive caption generation, image understanding through topic modeling, and sentence compression. 1 These methods have previously been applied individually to related tasks such as general domain image captioning and annotation. We briefly describe some of the related work: 2.1 Extractive Caption Generation In previous work on image caption extraction, captions are generated by retrieving human-authored descriptions from visually similar images. Farhadi et al. (2010) and Ordonez et al. (2011) retrieve whole captions to apply to a query image, while Kuznetsova et al. (2012) generate captions using text retrieved from multiple sources. The descriptions are related to visual concepts in the query image, but these models use visual similarity to approximate textual relevance; they do not model image and textual features jointly. 2.2 Image Understanding Recent improvements in state-of-the-art visual object class detections (Felzenszwalb et al., 2010) 1A research proposal for this framework and other image captioning ideas was previously presented at NAACL Stud</context>
<context position="23134" citStr="Farhadi et al., 2010" startWordPosition="3706" endWordPosition="3709">ws ROUGE-2 (bigram) scores computed without stopwords. We observe that our system very significantly improves ROUGE-2 precision of the GIST extracted caption, without significantly reducing recall. While LM-Only also improves precision against GIST extraction, it indiscriminately removes some words which are relevant to the query image. We also observe that GIST extraction strongly outperforms the KL model, which demonstrates the importance of visual structure. We also report BLEU (Papineni et al., 2002) scores, which are the most popularly accepted automatic metric for captioning evaluation (Farhadi et al., 2010; Kulkarni et al., 2011; Ordonez et al., 2011; Kuznetsova et al., 2012; Kuznetsova et al., 2013). Results are very similar to the ROUGE-2 precision scores, except the difference between our system and LM-Only is less pronounced because BLEU counts function words, while ROUGE does not. 6.3 Human Evaluation We perform human evaluation of compressions generated by our system and LM-Only. Users are shown the query image, the original uncompressed caption, and a compressed caption, and are asked two questions: does the compression improve the accuracy of the caption, and is the compression grammati</context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture tells a story: generating sentences from images. In Proceedings of the 11th European conference on Computer vision: Part IV, ECCV’10, pages 15–29, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Felzenszwalb</author>
<author>R B Girshick</author>
<author>D McAllester</author>
<author>D Ramanan</author>
</authors>
<title>Object detection with discriminatively trained part based models.</title>
<date>2010</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>32</volume>
<issue>9</issue>
<contexts>
<context position="5545" citStr="Felzenszwalb et al., 2010" startWordPosition="821" endWordPosition="824">e caption extraction, captions are generated by retrieving human-authored descriptions from visually similar images. Farhadi et al. (2010) and Ordonez et al. (2011) retrieve whole captions to apply to a query image, while Kuznetsova et al. (2012) generate captions using text retrieved from multiple sources. The descriptions are related to visual concepts in the query image, but these models use visual similarity to approximate textual relevance; they do not model image and textual features jointly. 2.2 Image Understanding Recent improvements in state-of-the-art visual object class detections (Felzenszwalb et al., 2010) 1A research proposal for this framework and other image captioning ideas was previously presented at NAACL Student Research Workshop in 2013 (Mason, 2013). This paper presents a completed project including implementation details and experimental results. have enabled much recent work in image caption generation (Farhadi et al., 2010; Ordonez et al., 2011; Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Yu and Siskind, 2013). However, these systems typically rely on a small number of detection types, e.g. the twenty object categories from the PASCAL VOC challenge.2 These objec</context>
</contexts>
<marker>Felzenszwalb, Girshick, McAllester, Ramanan, 2010</marker>
<rawString>P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. 2010. Object detection with discriminatively trained part based models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9):1627–1645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>How many words is a picture worth? automatic caption generation for news images.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1239--1249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3755" citStr="Feng and Lapata, 2010" startWordPosition="551" endWordPosition="554"> approaches at the intersection of Natural Language Processing and Computer Vision. Previous work such as Farhadi et al. (2010) and Ordonez et al. (2011) explore extractive methods for image captioning, but these rely on generaldomain visual detection systems, and only gener11 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 11–20, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics ate extractive captions. Other models learn correspondences between domain-specific images and natural language captions (Berg et al., 2010; Feng and Lapata, 2010b) but cannot generate descriptions for new images without the use of auxiliary text. Kuznetsova et al. (2013) propose a sentence compression model for editing image captions, but their compression objective is not conditioned on a query image, and their system also requires general-domain visual detections. This paper proposes an image captioning framework which extends these ideas and culminates in the first domain-specific image caption generation system. More broadly, our goal for image caption generation is to work toward less supervised captioning methods which could be used to generate </context>
<context position="6989" citStr="Feng and Lapata, 2010" startWordPosition="1053" endWordPosition="1056">ecific domains. Our caption generation system employs a multimodal topic model from our previous work (Mason and Charniak, 2013) which generates descriptive words, but lacks the spatial structure needed to generate a full sentence caption. Other previous work uses topic models to learn the semantic correspondence between images and labels (e.g. Blei and Jordan (2003)), but learning from natural language descriptions is considerably more difficult because of polysemy, hypernymy, and misalginment between the visual content of an image and the content humans choose to describe. The MixLDA model (Feng and Lapata, 2010b; Feng and Lapata, 2010a) learns from news images and natural language descriptions, but to generate words for a new image it requires both a query image and query text in the form of a news article. Berg et al. (2010) use discriminative models to discover visual attributes from online shopping images and captions, but their models do not generate descriptive words for unseen images. 2.3 Sentence Compression Typical models for sentence compression (Knight and Marcu, 2002; Furui et al., 2004; Turner and Charniak, 2005; Clarke and Lapata, 2008) have a summarization objective: reduce the length </context>
<context position="19829" citStr="Feng and Lapata (2010" startWordPosition="3195" endWordPosition="3198">ession ILP is implemented using the CPLEX optimization toolkit9. The language model weighting factor in the objective is λ = 10−3, which was hand-tuned according to observed output. The trigram language model is trained on training set captions using BerkeleyLM (Pauls and Klein, 2011) with Kneser-Ney smoothing. For the constraints, we use parses from Stanford Parser (Klein and Manning, 2003) and the “semantic head” variation of the Collins headfinder Collins (1999). 6 Evaluation 6.1 Setup We compare the following systems and baselines: KL (EXTRACTION): The top performing extractive model from Feng and Lapata (2010a), and the second-best captioning model overall. Using estimated topic distributions from our joint model, we extract the source with minimum KL Divergence from qimg. 8http://mallet.cs.umass.edu/ 9http://www-01.ibm.com/ software/integration/optimization/ cplex-optimization-studio/ ROUGE-2 Average 95% Confidence int. KL (EXTRACTION) P .06114 ( .05690 - .06554 ) R .02499 ( .02325 - .02686) F .03360 ( .03133 - .03600 ) GIST (EXTRACTION) P .10894 ( .09934 - .11921 ) R .05474 ( .04926 - .06045) F .06863 ( .06207 - .07534) LM-ONLY (COMPRESSION) P .13782 ( .12602 - .14864 ) R .02437 ( .02193 - .0270</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010a. How many words is a picture worth? automatic caption generation for news images. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1239–1249, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Topic models for image annotation and text illustration.</title>
<date>2010</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>831--839</pages>
<contexts>
<context position="3755" citStr="Feng and Lapata, 2010" startWordPosition="551" endWordPosition="554"> approaches at the intersection of Natural Language Processing and Computer Vision. Previous work such as Farhadi et al. (2010) and Ordonez et al. (2011) explore extractive methods for image captioning, but these rely on generaldomain visual detection systems, and only gener11 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 11–20, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics ate extractive captions. Other models learn correspondences between domain-specific images and natural language captions (Berg et al., 2010; Feng and Lapata, 2010b) but cannot generate descriptions for new images without the use of auxiliary text. Kuznetsova et al. (2013) propose a sentence compression model for editing image captions, but their compression objective is not conditioned on a query image, and their system also requires general-domain visual detections. This paper proposes an image captioning framework which extends these ideas and culminates in the first domain-specific image caption generation system. More broadly, our goal for image caption generation is to work toward less supervised captioning methods which could be used to generate </context>
<context position="6989" citStr="Feng and Lapata, 2010" startWordPosition="1053" endWordPosition="1056">ecific domains. Our caption generation system employs a multimodal topic model from our previous work (Mason and Charniak, 2013) which generates descriptive words, but lacks the spatial structure needed to generate a full sentence caption. Other previous work uses topic models to learn the semantic correspondence between images and labels (e.g. Blei and Jordan (2003)), but learning from natural language descriptions is considerably more difficult because of polysemy, hypernymy, and misalginment between the visual content of an image and the content humans choose to describe. The MixLDA model (Feng and Lapata, 2010b; Feng and Lapata, 2010a) learns from news images and natural language descriptions, but to generate words for a new image it requires both a query image and query text in the form of a news article. Berg et al. (2010) use discriminative models to discover visual attributes from online shopping images and captions, but their models do not generate descriptive words for unseen images. 2.3 Sentence Compression Typical models for sentence compression (Knight and Marcu, 2002; Furui et al., 2004; Turner and Charniak, 2005; Clarke and Lapata, 2008) have a summarization objective: reduce the length </context>
<context position="19829" citStr="Feng and Lapata (2010" startWordPosition="3195" endWordPosition="3198">ession ILP is implemented using the CPLEX optimization toolkit9. The language model weighting factor in the objective is λ = 10−3, which was hand-tuned according to observed output. The trigram language model is trained on training set captions using BerkeleyLM (Pauls and Klein, 2011) with Kneser-Ney smoothing. For the constraints, we use parses from Stanford Parser (Klein and Manning, 2003) and the “semantic head” variation of the Collins headfinder Collins (1999). 6 Evaluation 6.1 Setup We compare the following systems and baselines: KL (EXTRACTION): The top performing extractive model from Feng and Lapata (2010a), and the second-best captioning model overall. Using estimated topic distributions from our joint model, we extract the source with minimum KL Divergence from qimg. 8http://mallet.cs.umass.edu/ 9http://www-01.ibm.com/ software/integration/optimization/ cplex-optimization-studio/ ROUGE-2 Average 95% Confidence int. KL (EXTRACTION) P .06114 ( .05690 - .06554 ) R .02499 ( .02325 - .02686) F .03360 ( .03133 - .03600 ) GIST (EXTRACTION) P .10894 ( .09934 - .11921 ) R .05474 ( .04926 - .06045) F .06863 ( .06207 - .07534) LM-ONLY (COMPRESSION) P .13782 ( .12602 - .14864 ) R .02437 ( .02193 - .0270</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010b. Topic models for image annotation and text illustration. In HLT-NAACL, pages 831–839.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadaoki Furui</author>
<author>Tomonori Kikuchi</author>
<author>Yousuke Shinnaka</author>
<author>Chiori Hori</author>
</authors>
<title>Speech-to-text and speechto-speech summarization of spontaneous speech.</title>
<date>2004</date>
<journal>IEEE TRANS. ON SPEECH AND AUDIO PROCESSING,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="7485" citStr="Furui et al., 2004" startWordPosition="1135" endWordPosition="1138"> between the visual content of an image and the content humans choose to describe. The MixLDA model (Feng and Lapata, 2010b; Feng and Lapata, 2010a) learns from news images and natural language descriptions, but to generate words for a new image it requires both a query image and query text in the form of a news article. Berg et al. (2010) use discriminative models to discover visual attributes from online shopping images and captions, but their models do not generate descriptive words for unseen images. 2.3 Sentence Compression Typical models for sentence compression (Knight and Marcu, 2002; Furui et al., 2004; Turner and Charniak, 2005; Clarke and Lapata, 2008) have a summarization objective: reduce the length of a source sentence without changing its meaning. In contrast, our objective is to change the meaning of the source sentence, letting its overall correctness relative to the query image determine the length of the output. Our objective differs from that of Kuznetsova et al. (2013), who compress image caption sentences with the objective of creating a corpus of generally transferrable image captions. Their compression objective is to maximize the probability of a caption conditioned on the s</context>
</contexts>
<marker>Furui, Kikuchi, Shinnaka, Hori, 2004</marker>
<rawString>Sadaoki Furui, Tomonori Kikuchi, Yousuke Shinnaka, and Chiori Hori. 2004. Speech-to-text and speechto-speech summarization of spontaneous speech. IEEE TRANS. ON SPEECH AND AUDIO PROCESSING, 12(4):401–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17801" citStr="Klein and Manning, 2003" startWordPosition="2870" endWordPosition="2873">by Oliva and Torralba (2001)7. GIST is computed with images converted to grayscale; since color features tend to act as modifiers in this domain. Nearestneighbors are selected according to minimum distance from qimg to both a regularly-oriented and a horizontally-flipped training image. Only one sentence from the first nearestneighbor caption is extracted. In the case of multisentence captions, we select the first suitable sentence according to the following criteria 1.) has at least five tokens, 2.) does not contain NNP or NNPS (brand names), 3.) does not fail to parse using Stanford Parser (Klein and Manning, 2003). If the nearest-neighbor caption does not have any sentences meeting these criteria, caption sentences from the next nearest-neighbor(s) are considered. n max z = αi · P(wi|start) i=1 n−2� n−1� n -Yijk · P(wk|wi, wj) + j=i+1 k=j+1 i=1 n−1� n �Qij · P(end|wi, wj) · A + j=i+1 i=0 n Si · I(wi) (9) + i=1 7http://people.csail.mit.edu/torralba/ code/spatialenvelope/ 15 5.2 Joint Topic Model We use the Joint Topic Model that we implemented in our previous work; please see Mason and Charniak (2013) for the full model and implementation details. The topic model is trained with 200 topics using the pol</context>
<context position="19602" citStr="Klein and Manning, 2003" startWordPosition="3160" endWordPosition="3163">. INTENSITY: HOG (histogram of gradients) (Dalal and Triggs, 2005) describes the direction and intensity of changes in light. These features are computed on the image over a densely sampled grid. 5.3 Compression The sentence compression ILP is implemented using the CPLEX optimization toolkit9. The language model weighting factor in the objective is λ = 10−3, which was hand-tuned according to observed output. The trigram language model is trained on training set captions using BerkeleyLM (Pauls and Klein, 2011) with Kneser-Ney smoothing. For the constraints, we use parses from Stanford Parser (Klein and Manning, 2003) and the “semantic head” variation of the Collins headfinder Collins (1999). 6 Evaluation 6.1 Setup We compare the following systems and baselines: KL (EXTRACTION): The top performing extractive model from Feng and Lapata (2010a), and the second-best captioning model overall. Using estimated topic distributions from our joint model, we extract the source with minimum KL Divergence from qimg. 8http://mallet.cs.umass.edu/ 9http://www-01.ibm.com/ software/integration/optimization/ cplex-optimization-studio/ ROUGE-2 Average 95% Confidence int. KL (EXTRACTION) P .06114 ( .05690 - .06554 ) R .02499 </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 423– 430, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: a probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artif. Intell.,</journal>
<volume>139</volume>
<issue>1</issue>
<contexts>
<context position="7465" citStr="Knight and Marcu, 2002" startWordPosition="1131" endWordPosition="1134">ernymy, and misalginment between the visual content of an image and the content humans choose to describe. The MixLDA model (Feng and Lapata, 2010b; Feng and Lapata, 2010a) learns from news images and natural language descriptions, but to generate words for a new image it requires both a query image and query text in the form of a news article. Berg et al. (2010) use discriminative models to discover visual attributes from online shopping images and captions, but their models do not generate descriptive words for unseen images. 2.3 Sentence Compression Typical models for sentence compression (Knight and Marcu, 2002; Furui et al., 2004; Turner and Charniak, 2005; Clarke and Lapata, 2008) have a summarization objective: reduce the length of a source sentence without changing its meaning. In contrast, our objective is to change the meaning of the source sentence, letting its overall correctness relative to the query image determine the length of the output. Our objective differs from that of Kuznetsova et al. (2013), who compress image caption sentences with the objective of creating a corpus of generally transferrable image captions. Their compression objective is to maximize the probability of a caption </context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: a probabilistic approach to sentence compression. Artif. Intell., 139(1):91–107, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Baby talk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In CVPR,</booktitle>
<pages>1601--1608</pages>
<contexts>
<context position="5925" citStr="Kulkarni et al., 2011" startWordPosition="879" endWordPosition="882">e models use visual similarity to approximate textual relevance; they do not model image and textual features jointly. 2.2 Image Understanding Recent improvements in state-of-the-art visual object class detections (Felzenszwalb et al., 2010) 1A research proposal for this framework and other image captioning ideas was previously presented at NAACL Student Research Workshop in 2013 (Mason, 2013). This paper presents a completed project including implementation details and experimental results. have enabled much recent work in image caption generation (Farhadi et al., 2010; Ordonez et al., 2011; Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Yu and Siskind, 2013). However, these systems typically rely on a small number of detection types, e.g. the twenty object categories from the PASCAL VOC challenge.2 These object categories include entities which are commonly described in general domain images (people, cars, cats, etc) but these require labeled training data which is not typically available for the visually relevant entities in specific domains. Our caption generation system employs a multimodal topic model from our previous work (Mason and Charniak, 2013) which generates descriptive </context>
<context position="23157" citStr="Kulkarni et al., 2011" startWordPosition="3710" endWordPosition="3713">ores computed without stopwords. We observe that our system very significantly improves ROUGE-2 precision of the GIST extracted caption, without significantly reducing recall. While LM-Only also improves precision against GIST extraction, it indiscriminately removes some words which are relevant to the query image. We also observe that GIST extraction strongly outperforms the KL model, which demonstrates the importance of visual structure. We also report BLEU (Papineni et al., 2002) scores, which are the most popularly accepted automatic metric for captioning evaluation (Farhadi et al., 2010; Kulkarni et al., 2011; Ordonez et al., 2011; Kuznetsova et al., 2012; Kuznetsova et al., 2013). Results are very similar to the ROUGE-2 precision scores, except the difference between our system and LM-Only is less pronounced because BLEU counts function words, while ROUGE does not. 6.3 Human Evaluation We perform human evaluation of compressions generated by our system and LM-Only. Users are shown the query image, the original uncompressed caption, and a compressed caption, and are asked two questions: does the compression improve the accuracy of the caption, and is the compression grammatical. We collect 553 jud</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2011. Baby talk: Understanding and generating simple image descriptions. In CVPR, pages 1601–1608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Polina Kuznetsova</author>
<author>Vicente Ordonez</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Collective generation of natural image descriptions.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5165" citStr="Kuznetsova et al. (2012)" startWordPosition="766" endWordPosition="769">ioning consists of three main components: extractive caption generation, image understanding through topic modeling, and sentence compression. 1 These methods have previously been applied individually to related tasks such as general domain image captioning and annotation. We briefly describe some of the related work: 2.1 Extractive Caption Generation In previous work on image caption extraction, captions are generated by retrieving human-authored descriptions from visually similar images. Farhadi et al. (2010) and Ordonez et al. (2011) retrieve whole captions to apply to a query image, while Kuznetsova et al. (2012) generate captions using text retrieved from multiple sources. The descriptions are related to visual concepts in the query image, but these models use visual similarity to approximate textual relevance; they do not model image and textual features jointly. 2.2 Image Understanding Recent improvements in state-of-the-art visual object class detections (Felzenszwalb et al., 2010) 1A research proposal for this framework and other image captioning ideas was previously presented at NAACL Student Research Workshop in 2013 (Mason, 2013). This paper presents a completed project including implementatio</context>
<context position="23204" citStr="Kuznetsova et al., 2012" startWordPosition="3718" endWordPosition="3721">hat our system very significantly improves ROUGE-2 precision of the GIST extracted caption, without significantly reducing recall. While LM-Only also improves precision against GIST extraction, it indiscriminately removes some words which are relevant to the query image. We also observe that GIST extraction strongly outperforms the KL model, which demonstrates the importance of visual structure. We also report BLEU (Papineni et al., 2002) scores, which are the most popularly accepted automatic metric for captioning evaluation (Farhadi et al., 2010; Kulkarni et al., 2011; Ordonez et al., 2011; Kuznetsova et al., 2012; Kuznetsova et al., 2013). Results are very similar to the ROUGE-2 precision scores, except the difference between our system and LM-Only is less pronounced because BLEU counts function words, while ROUGE does not. 6.3 Human Evaluation We perform human evaluation of compressions generated by our system and LM-Only. Users are shown the query image, the original uncompressed caption, and a compressed caption, and are asked two questions: does the compression improve the accuracy of the caption, and is the compression grammatical. We collect 553 judgments from six women who are native English-sp</context>
</contexts>
<marker>Kuznetsova, Ordonez, Berg, Berg, Choi, 2012</marker>
<rawString>Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg, Tamara L. Berg, and Yejin Choi. 2012. Collective generation of natural image descriptions. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Polina Kuznetsova</author>
<author>Vicente Ordonez</author>
<author>Alexander Berg</author>
<author>Tamara Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Generalizing image captions for image-text parallel corpus.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3865" citStr="Kuznetsova et al. (2013)" startWordPosition="570" endWordPosition="573">arhadi et al. (2010) and Ordonez et al. (2011) explore extractive methods for image captioning, but these rely on generaldomain visual detection systems, and only gener11 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 11–20, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics ate extractive captions. Other models learn correspondences between domain-specific images and natural language captions (Berg et al., 2010; Feng and Lapata, 2010b) but cannot generate descriptions for new images without the use of auxiliary text. Kuznetsova et al. (2013) propose a sentence compression model for editing image captions, but their compression objective is not conditioned on a query image, and their system also requires general-domain visual detections. This paper proposes an image captioning framework which extends these ideas and culminates in the first domain-specific image caption generation system. More broadly, our goal for image caption generation is to work toward less supervised captioning methods which could be used to generate detailed and accurate descriptions for a variety of long-tail domains of captioned image data, such as in natu</context>
<context position="7871" citStr="Kuznetsova et al. (2013)" startWordPosition="1196" endWordPosition="1199">visual attributes from online shopping images and captions, but their models do not generate descriptive words for unseen images. 2.3 Sentence Compression Typical models for sentence compression (Knight and Marcu, 2002; Furui et al., 2004; Turner and Charniak, 2005; Clarke and Lapata, 2008) have a summarization objective: reduce the length of a source sentence without changing its meaning. In contrast, our objective is to change the meaning of the source sentence, letting its overall correctness relative to the query image determine the length of the output. Our objective differs from that of Kuznetsova et al. (2013), who compress image caption sentences with the objective of creating a corpus of generally transferrable image captions. Their compression objective is to maximize the probability of a caption conditioned on the source 2http://pascallin.ecs.soton.ac.uk/ challenges/VOC/ 12 Two adjustable buckle straps top a classic rubber rain boot grounded by a thick lug sole for excellent wet-weather traction. Available in Plus Size. Faux snake skin flats with a large crossover buckle at the toe. Padded insole for a comfortable all day fit. Glitter-covered elastic upper in a two-piece dress sandal style with</context>
<context position="21096" citStr="Kuznetsova et al. (2013)" startWordPosition="3393" endWordPosition="3396">PRESSION) P .16752 (.15679 -.17882 ) R .05060 ( .04675 - .05524 ) F .07204 ( .06685 - .07802 ) Table 4: ROUGE-2 (bigram) scores. The precision of our system compression (bolded) significantly improves over the caption that it compresses (GIST), without a significant decrease in recall. GIST (EXTRACTION): The sentence extracted using GIST nearest-neighbors, and the uncompressed source for the compression systems. LM-ONLY (COMPRESSION): We include this baseline to demonstrate that our model is effectively conditioning output compressions on qimg, as opposed to simply generalizing captions as in Kuznetsova et al. (2013)10. We modify the compression ILP to ignore the content objective and only maximize the trigram language model (still subject to the constraints). SYSTEM (COMPRESSION): Our full system. Unfortunately, we cannot compare our system against prior work in general-domain image captioning, because those models use visual detection systems which train on labeled data that is not available in our domain. 6.2 Automatic Evaluation We perform automatic evaluation using similarity measures between automatically generated and human-authored captions. Note that currently our system and baselines only genera</context>
<context position="23230" citStr="Kuznetsova et al., 2013" startWordPosition="3722" endWordPosition="3725">ficantly improves ROUGE-2 precision of the GIST extracted caption, without significantly reducing recall. While LM-Only also improves precision against GIST extraction, it indiscriminately removes some words which are relevant to the query image. We also observe that GIST extraction strongly outperforms the KL model, which demonstrates the importance of visual structure. We also report BLEU (Papineni et al., 2002) scores, which are the most popularly accepted automatic metric for captioning evaluation (Farhadi et al., 2010; Kulkarni et al., 2011; Ordonez et al., 2011; Kuznetsova et al., 2012; Kuznetsova et al., 2013). Results are very similar to the ROUGE-2 precision scores, except the difference between our system and LM-Only is less pronounced because BLEU counts function words, while ROUGE does not. 6.3 Human Evaluation We perform human evaluation of compressions generated by our system and LM-Only. Users are shown the query image, the original uncompressed caption, and a compressed caption, and are asked two questions: does the compression improve the accuracy of the caption, and is the compression grammatical. We collect 553 judgments from six women who are native English-speakers and knowledgeable Q</context>
</contexts>
<marker>Kuznetsova, Ordonez, Berg, Berg, Choi, 2013</marker>
<rawString>Polina Kuznetsova, Vicente Ordonez, Alexander Berg, Tamara Berg, and Yejin Choi. 2013. Generalizing image captions for image-text parallel corpus. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Leung</author>
<author>J Malik</author>
</authors>
<title>Representing and recognizing the visual appearance of materials using three-dimensional textons.</title>
<date>2001</date>
<journal>International Journal of Computer Vision,</journal>
<volume>43</volume>
<issue>1</issue>
<contexts>
<context position="18842" citStr="Leung and Malik, 2001" startWordPosition="3042" endWordPosition="3045">hat we implemented in our previous work; please see Mason and Charniak (2013) for the full model and implementation details. The topic model is trained with 200 topics using the polylingual topic model implementation from MALLET8. Briefly, the codewords represent the following attributes: SHAPE: SIFT (Lowe, 1999) describes the shapes of detected edges in the image, using descriptors which are invariant to changes in rotation and scale. COLOR: RGB (red, green, blue) and HSV (hue, saturation, value) pixel values are sampled from a central area of the image to represent colors. TEXTURE: Textons (Leung and Malik, 2001) are computed by convolving images with Gabor filters at multiple orientations and scales, then sampling the outputs at random locations. INTENSITY: HOG (histogram of gradients) (Dalal and Triggs, 2005) describes the direction and intensity of changes in light. These features are computed on the image over a densely sampled grid. 5.3 Compression The sentence compression ILP is implemented using the CPLEX optimization toolkit9. The language model weighting factor in the objective is λ = 10−3, which was hand-tuned according to observed output. The trigram language model is trained on training se</context>
</contexts>
<marker>Leung, Malik, 2001</marker>
<rawString>T. Leung and J. Malik. 2001. Representing and recognizing the visual appearance of materials using three-dimensional textons. International Journal of Computer Vision, 43(1):29–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="22251" citStr="Lin, 2004" startWordPosition="3569" endWordPosition="3570">ote that currently our system and baselines only generate singlesentence captions, but we compare against entire 10Technically their model is conditioned on the source image, in order to address alignment issues which are not applicable in our setup. 16 BLEU@1 KL (EXTRACTION) .2098 GIST (EXTRACTION) .4259 LM-ONLY (COMPRESSION) .4780 SYSTEM (COMPRESSION) .4841 Table 5: BLEU@1 scores of generated captions against human authored captions. Our model (bolded) has the highest BLEU@1 score with significance. held-out captions in order to increase the amount of text we have to compare against. ROUGE (Lin, 2004) is a summarization evaluation metric which has also been used to evaluate image captions (Yang et al., 2011). It is usually a recall-oriented measure, but we also report precision and f-measure because our sentence compressions do not improve recall. Table 4 shows ROUGE-2 (bigram) scores computed without stopwords. We observe that our system very significantly improves ROUGE-2 precision of the GIST extracted caption, without significantly reducing recall. While LM-Only also improves precision against GIST extraction, it indiscriminately removes some words which are relevant to the query image</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Lowe</author>
</authors>
<title>Object recognition from local scaleinvariant features. In Computer Vision,</title>
<date>1999</date>
<booktitle>The Proceedings of the Seventh IEEE International Conference on,</booktitle>
<volume>2</volume>
<pages>1150--1157</pages>
<contexts>
<context position="18534" citStr="Lowe, 1999" startWordPosition="2993" endWordPosition="2994">est-neighbor(s) are considered. n max z = αi · P(wi|start) i=1 n−2� n−1� n -Yijk · P(wk|wi, wj) + j=i+1 k=j+1 i=1 n−1� n �Qij · P(end|wi, wj) · A + j=i+1 i=0 n Si · I(wi) (9) + i=1 7http://people.csail.mit.edu/torralba/ code/spatialenvelope/ 15 5.2 Joint Topic Model We use the Joint Topic Model that we implemented in our previous work; please see Mason and Charniak (2013) for the full model and implementation details. The topic model is trained with 200 topics using the polylingual topic model implementation from MALLET8. Briefly, the codewords represent the following attributes: SHAPE: SIFT (Lowe, 1999) describes the shapes of detected edges in the image, using descriptors which are invariant to changes in rotation and scale. COLOR: RGB (red, green, blue) and HSV (hue, saturation, value) pixel values are sampled from a central area of the image to represent colors. TEXTURE: Textons (Leung and Malik, 2001) are computed by convolving images with Gabor filters at multiple orientations and scales, then sampling the outputs at random locations. INTENSITY: HOG (histogram of gradients) (Dalal and Triggs, 2005) describes the direction and intensity of changes in light. These features are computed on</context>
</contexts>
<marker>Lowe, 1999</marker>
<rawString>D.G. Lowe. 1999. Object recognition from local scaleinvariant features. In Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference on, volume 2, pages 1150 –1157 vol.2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP ’09,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14568" citStr="Martins and Smith, 2009" startWordPosition="2287" endWordPosition="2290">sing Gibbs sampling: P(zn = t|qimg, z\n, Φimg, αm) (Nt)\n + αmt (6) Et Nt − 1 + α 4.3 Sentence Compression Let w = w1, w2, ..., wn be the words in the extracted caption for qimg. For each word, we define a binary decision variable δ, such that δi = 1 if wi is included in the output compression, and δi = 0 otherwise. Our objective is to find values of δ which generate a caption for qimg which is both semantically and grammatically correct. We cast this problem as an Integer Linear Program (ILP), which has previously been used for the standard sentence compression task (Clarke and Lapata, 2008; Martins and Smith, 2009). ILP is a mathematical optimization method for determining the optimal values of integer variables in order to maximize an objective given a set of constraints. (2) θimg zn θztxt (3) n ∝ φimg img|tqn 14 4.3.1 Objective The ILP objective is a weighted linear combination of two measures which represent the correctness and fluency of the output compression: Correctness: Recall in Section 3 we defined words as either descriptive words or function words. For each descriptive word, we estimate P(wi|qimg), using topic proportions estimated using Equation 6: � P(wi|ztxt P(wi|qimg) = t )P(zt|qimg) (7)</context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>Andr´e F. T. Martins and Noah A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP ’09, pages 1–9, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mason</author>
<author>E Charniak</author>
</authors>
<title>Annotation of online shopping images without labeled training examples.</title>
<date>2013</date>
<booktitle>Workshop on Vision and Language (WVL).</booktitle>
<contexts>
<context position="6496" citStr="Mason and Charniak, 2013" startWordPosition="974" endWordPosition="978">al., 2010; Ordonez et al., 2011; Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Yu and Siskind, 2013). However, these systems typically rely on a small number of detection types, e.g. the twenty object categories from the PASCAL VOC challenge.2 These object categories include entities which are commonly described in general domain images (people, cars, cats, etc) but these require labeled training data which is not typically available for the visually relevant entities in specific domains. Our caption generation system employs a multimodal topic model from our previous work (Mason and Charniak, 2013) which generates descriptive words, but lacks the spatial structure needed to generate a full sentence caption. Other previous work uses topic models to learn the semantic correspondence between images and labels (e.g. Blei and Jordan (2003)), but learning from natural language descriptions is considerably more difficult because of polysemy, hypernymy, and misalginment between the visual content of an image and the content humans choose to describe. The MixLDA model (Feng and Lapata, 2010b; Feng and Lapata, 2010a) learns from news images and natural language descriptions, but to generate words</context>
<context position="13032" citStr="Mason and Charniak, 2013" startWordPosition="2017" endWordPosition="2020">e and clustered into discrete “codewords” using the k-means algorithm.6 Unlike text words, an individual codeword has little meaning on its own, but distributions of codewords can provide a meaningful, though unstructured, representation of an image. An image and its caption do not express exactly the same information, but they are topically related. We employ the Polylingual Topic Model (Mimno et al., 2009), which is originally used to model corresponding documents in different languages that are topically comparable, but not parallel translations. In particular, we employ our previous work (Mason and Charniak, 2013) which extends this model to topically similar images and natural language captions. The generative process for a captioned image starts with a single topic distribution drawn from concentration parameter α and base measure m: θ ∼ Dir(θ, αm) (1) Modality-specific latent topic assignments zimg and ztxt are drawn for each of the text words and codewords: �zimg ∼ P (zimg|θ) = n 6While space limits a more detailed explanation of visual bag-of-word features, Section 5.2 provides a brief overview of the specific visual attributes used in this model. �ztxt ∼ P (ztxt|θ) = n Observed words are generate</context>
<context position="18297" citStr="Mason and Charniak (2013)" startWordPosition="2954" endWordPosition="2957">okens, 2.) does not contain NNP or NNPS (brand names), 3.) does not fail to parse using Stanford Parser (Klein and Manning, 2003). If the nearest-neighbor caption does not have any sentences meeting these criteria, caption sentences from the next nearest-neighbor(s) are considered. n max z = αi · P(wi|start) i=1 n−2� n−1� n -Yijk · P(wk|wi, wj) + j=i+1 k=j+1 i=1 n−1� n �Qij · P(end|wi, wj) · A + j=i+1 i=0 n Si · I(wi) (9) + i=1 7http://people.csail.mit.edu/torralba/ code/spatialenvelope/ 15 5.2 Joint Topic Model We use the Joint Topic Model that we implemented in our previous work; please see Mason and Charniak (2013) for the full model and implementation details. The topic model is trained with 200 topics using the polylingual topic model implementation from MALLET8. Briefly, the codewords represent the following attributes: SHAPE: SIFT (Lowe, 1999) describes the shapes of detected edges in the image, using descriptors which are invariant to changes in rotation and scale. COLOR: RGB (red, green, blue) and HSV (hue, saturation, value) pixel values are sampled from a central area of the image to represent colors. TEXTURE: Textons (Leung and Malik, 2001) are computed by convolving images with Gabor filters a</context>
</contexts>
<marker>Mason, Charniak, 2013</marker>
<rawString>R. Mason and E. Charniak. 2013. Annotation of online shopping images without labeled training examples. Workshop on Vision and Language (WVL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Mason</author>
</authors>
<title>Domain-independent captioning of domain-specific images.</title>
<date>2013</date>
<journal>NAACL Student Research Workshop.</journal>
<contexts>
<context position="5700" citStr="Mason, 2013" startWordPosition="847" endWordPosition="848">ieve whole captions to apply to a query image, while Kuznetsova et al. (2012) generate captions using text retrieved from multiple sources. The descriptions are related to visual concepts in the query image, but these models use visual similarity to approximate textual relevance; they do not model image and textual features jointly. 2.2 Image Understanding Recent improvements in state-of-the-art visual object class detections (Felzenszwalb et al., 2010) 1A research proposal for this framework and other image captioning ideas was previously presented at NAACL Student Research Workshop in 2013 (Mason, 2013). This paper presents a completed project including implementation details and experimental results. have enabled much recent work in image caption generation (Farhadi et al., 2010; Ordonez et al., 2011; Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Yu and Siskind, 2013). However, these systems typically rely on a small number of detection types, e.g. the twenty object categories from the PASCAL VOC challenge.2 These object categories include entities which are commonly described in general domain images (people, cars, cats, etc) but these require labeled training data which</context>
</contexts>
<marker>Mason, 2013</marker>
<rawString>Rebecca Mason. 2013. Domain-independent captioning of domain-specific images. NAACL Student Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna M Wallach</author>
<author>Jason Naradowsky</author>
<author>David A Smith</author>
<author>Andrew McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09,</booktitle>
<pages>880--889</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12818" citStr="Mimno et al., 2009" startWordPosition="1985" endWordPosition="1988">s for an image and its caption. The bag-of-words model for Computer Vision represents images as a mixture of topics. Measures of shape, color, texture, and intensity are computed at various points on the image and clustered into discrete “codewords” using the k-means algorithm.6 Unlike text words, an individual codeword has little meaning on its own, but distributions of codewords can provide a meaningful, though unstructured, representation of an image. An image and its caption do not express exactly the same information, but they are topically related. We employ the Polylingual Topic Model (Mimno et al., 2009), which is originally used to model corresponding documents in different languages that are topically comparable, but not parallel translations. In particular, we employ our previous work (Mason and Charniak, 2013) which extends this model to topically similar images and natural language captions. The generative process for a captioned image starts with a single topic distribution drawn from concentration parameter α and base measure m: θ ∼ Dir(θ, αm) (1) Modality-specific latent topic assignments zimg and ztxt are drawn for each of the text words and codewords: �zimg ∼ P (zimg|θ) = n 6While s</context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>David Mimno, Hanna M. Wallach, Jason Naradowsky, David A. Smith, and Andrew McCallum. 2009. Polylingual topic models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09, pages 880–889, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Jesse Dodge</author>
<author>Amit Goyal</author>
<author>Kota Yamaguchi</author>
<author>Karl Stratos</author>
<author>Xufeng Han</author>
<author>Alyssa Mensch</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
<author>Hal Daum´e</author>
</authors>
<title>Midge: Generating image descriptions from computer vision detections.</title>
<date>2012</date>
<booktitle>In European Chapter of the Association for Computational Linguistics (EACL).</booktitle>
<marker>Mitchell, Dodge, Goyal, Yamaguchi, Stratos, Han, Mensch, Berg, Berg, Daum´e, 2012</marker>
<rawString>Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Yamaguchi, Karl Stratos, Xufeng Han, Alyssa Mensch, Alexander C. Berg, Tamara L. Berg, and Hal Daum´e III. 2012. Midge: Generating image descriptions from computer vision detections. In European Chapter of the Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aude Oliva</author>
<author>Antonio Torralba</author>
</authors>
<title>Modeling the shape of the scene: A holistic representation of the spatial envelope.</title>
<date>2001</date>
<journal>International Journal of Computer Vision,</journal>
<pages>42--145</pages>
<contexts>
<context position="11020" citStr="Oliva and Torralba, 2001" startWordPosition="1700" endWordPosition="1703">red” and keep the rest. As in this simple example, the basic paradigm for compression is to keep the head words of phrases (“slipper”) and remove modifiers. Thus we want to extraction stage of our scheme to be more likely to find a candidate sentence with correct head words, figuring that the compression stage can edit the mistakes. Our hypothesis is that headwords tend to describe more spatially structured visual concepts, while modifier words describe those that are more easily represented using local or unstructured features.4 Table 2 contains additional example captions with parses. GIST (Oliva and Torralba, 2001) is a commonly used feature in Computer Vision which coarsely localizes perceptual attributes (e.g. rough vs smooth, natural vs manmade). By computing the GIST of the images, we project them into a multi-dimensional Euclidean space where images with semantically similar structures are located near each other. Thus the extraction stage of our caption generation process selects a sentence from the GIST nearest-neighbor to the query image.5 4.2 Joint Topic Model The second component of our framework incorporates visual and textual features using a less structured model. We use a multi-modal topic</context>
<context position="17205" citStr="Oliva and Torralba (2001)" startWordPosition="2772" endWordPosition="2775">not necessarily the objective for image captioning, so we introduce a weighting factor, A, to lessen the effect. Here is the combined objective, using P to represent log P: 4.3.2 ILP Constraints The ILP constraints ensure both the mathematical validity of the model, and the grammatical correctness of its output. Table 3 summarizes the list of constraints. Sequential constraints are defined as in Clarke (2008) ensure that the ordering of the trigrams is valid, and that the mathematical validity of the model holds. 5 Implementation Details 5.1 Extraction GIST features are computed using code by Oliva and Torralba (2001)7. GIST is computed with images converted to grayscale; since color features tend to act as modifiers in this domain. Nearestneighbors are selected according to minimum distance from qimg to both a regularly-oriented and a horizontally-flipped training image. Only one sentence from the first nearestneighbor caption is extracted. In the case of multisentence captions, we select the first suitable sentence according to the following criteria 1.) has at least five tokens, 2.) does not contain NNP or NNPS (brand names), 3.) does not fail to parse using Stanford Parser (Klein and Manning, 2003). If</context>
</contexts>
<marker>Oliva, Torralba, 2001</marker>
<rawString>Aude Oliva and Antonio Torralba. 2001. Modeling the shape of the scene: A holistic representation of the spatial envelope. International Journal of Computer Vision, 42:145–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ordonez</author>
<author>G Kulkarni</author>
<author>T L Berg</author>
</authors>
<title>Im2text: Describing images using 1 million captioned photographs.</title>
<date>2011</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="3287" citStr="Ordonez et al. (2011)" startWordPosition="487" endWordPosition="490">ption from a database of human-captions, by projecting query images into a multi-dimensional space where structurally similar images are near each other. We also train a joint topic model to discover the latent topics which generate both captions and images. We combine these two approaches using sentence compression to delete modifying details in the extracted caption which are not relevant to the query image. Our captioning framework is inspired by several recent approaches at the intersection of Natural Language Processing and Computer Vision. Previous work such as Farhadi et al. (2010) and Ordonez et al. (2011) explore extractive methods for image captioning, but these rely on generaldomain visual detection systems, and only gener11 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 11–20, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics ate extractive captions. Other models learn correspondences between domain-specific images and natural language captions (Berg et al., 2010; Feng and Lapata, 2010b) but cannot generate descriptions for new images without the use of auxiliary text. Kuznetsova et al. (2013) propose a sentence co</context>
<context position="5083" citStr="Ordonez et al. (2011)" startWordPosition="752" endWordPosition="755">ature and medicine. 2 Related Work Our framework for domain-specific image captioning consists of three main components: extractive caption generation, image understanding through topic modeling, and sentence compression. 1 These methods have previously been applied individually to related tasks such as general domain image captioning and annotation. We briefly describe some of the related work: 2.1 Extractive Caption Generation In previous work on image caption extraction, captions are generated by retrieving human-authored descriptions from visually similar images. Farhadi et al. (2010) and Ordonez et al. (2011) retrieve whole captions to apply to a query image, while Kuznetsova et al. (2012) generate captions using text retrieved from multiple sources. The descriptions are related to visual concepts in the query image, but these models use visual similarity to approximate textual relevance; they do not model image and textual features jointly. 2.2 Image Understanding Recent improvements in state-of-the-art visual object class detections (Felzenszwalb et al., 2010) 1A research proposal for this framework and other image captioning ideas was previously presented at NAACL Student Research Workshop in 2</context>
<context position="23179" citStr="Ordonez et al., 2011" startWordPosition="3714" endWordPosition="3717">topwords. We observe that our system very significantly improves ROUGE-2 precision of the GIST extracted caption, without significantly reducing recall. While LM-Only also improves precision against GIST extraction, it indiscriminately removes some words which are relevant to the query image. We also observe that GIST extraction strongly outperforms the KL model, which demonstrates the importance of visual structure. We also report BLEU (Papineni et al., 2002) scores, which are the most popularly accepted automatic metric for captioning evaluation (Farhadi et al., 2010; Kulkarni et al., 2011; Ordonez et al., 2011; Kuznetsova et al., 2012; Kuznetsova et al., 2013). Results are very similar to the ROUGE-2 precision scores, except the difference between our system and LM-Only is less pronounced because BLEU counts function words, while ROUGE does not. 6.3 Human Evaluation We perform human evaluation of compressions generated by our system and LM-Only. Users are shown the query image, the original uncompressed caption, and a compressed caption, and are asked two questions: does the compression improve the accuracy of the caption, and is the compression grammatical. We collect 553 judgments from six women </context>
</contexts>
<marker>Ordonez, Kulkarni, Berg, 2011</marker>
<rawString>V. Ordonez, G. Kulkarni, and T.L. Berg. 2011. Im2text: Describing images using 1 million captioned photographs. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="23023" citStr="Papineni et al., 2002" startWordPosition="3689" endWordPosition="3692">, but we also report precision and f-measure because our sentence compressions do not improve recall. Table 4 shows ROUGE-2 (bigram) scores computed without stopwords. We observe that our system very significantly improves ROUGE-2 precision of the GIST extracted caption, without significantly reducing recall. While LM-Only also improves precision against GIST extraction, it indiscriminately removes some words which are relevant to the query image. We also observe that GIST extraction strongly outperforms the KL model, which demonstrates the importance of visual structure. We also report BLEU (Papineni et al., 2002) scores, which are the most popularly accepted automatic metric for captioning evaluation (Farhadi et al., 2010; Kulkarni et al., 2011; Ordonez et al., 2011; Kuznetsova et al., 2012; Kuznetsova et al., 2013). Results are very similar to the ROUGE-2 precision scores, except the difference between our system and LM-Only is less pronounced because BLEU counts function words, while ROUGE does not. 6.3 Human Evaluation We perform human evaluation of compressions generated by our system and LM-Only. Users are shown the query image, the original uncompressed caption, and a compressed caption, and are</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Faster and smaller n-gram language models.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<contexts>
<context position="19493" citStr="Pauls and Klein, 2011" startWordPosition="3144" endWordPosition="3147">mages with Gabor filters at multiple orientations and scales, then sampling the outputs at random locations. INTENSITY: HOG (histogram of gradients) (Dalal and Triggs, 2005) describes the direction and intensity of changes in light. These features are computed on the image over a densely sampled grid. 5.3 Compression The sentence compression ILP is implemented using the CPLEX optimization toolkit9. The language model weighting factor in the objective is λ = 10−3, which was hand-tuned according to observed output. The trigram language model is trained on training set captions using BerkeleyLM (Pauls and Klein, 2011) with Kneser-Ney smoothing. For the constraints, we use parses from Stanford Parser (Klein and Manning, 2003) and the “semantic head” variation of the Collins headfinder Collins (1999). 6 Evaluation 6.1 Setup We compare the following systems and baselines: KL (EXTRACTION): The top performing extractive model from Feng and Lapata (2010a), and the second-best captioning model overall. Using estimated topic distributions from our joint model, we extract the source with minimum KL Divergence from qimg. 8http://mallet.cs.umass.edu/ 9http://www-01.ibm.com/ software/integration/optimization/ cplex-op</context>
</contexts>
<marker>Pauls, Klein, 2011</marker>
<rawString>Adam Pauls and Dan Klein. 2011. Faster and smaller n-gram language models. In Proceedings of ACL, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenine Turner</author>
<author>Eugene Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>290--297</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7512" citStr="Turner and Charniak, 2005" startWordPosition="1139" endWordPosition="1142">content of an image and the content humans choose to describe. The MixLDA model (Feng and Lapata, 2010b; Feng and Lapata, 2010a) learns from news images and natural language descriptions, but to generate words for a new image it requires both a query image and query text in the form of a news article. Berg et al. (2010) use discriminative models to discover visual attributes from online shopping images and captions, but their models do not generate descriptive words for unseen images. 2.3 Sentence Compression Typical models for sentence compression (Knight and Marcu, 2002; Furui et al., 2004; Turner and Charniak, 2005; Clarke and Lapata, 2008) have a summarization objective: reduce the length of a source sentence without changing its meaning. In contrast, our objective is to change the meaning of the source sentence, letting its overall correctness relative to the query image determine the length of the output. Our objective differs from that of Kuznetsova et al. (2013), who compress image caption sentences with the objective of creating a corpus of generally transferrable image captions. Their compression objective is to maximize the probability of a caption conditioned on the source 2http://pascallin.ecs</context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 290–297, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
</authors>
<title>Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Edinburgh, Scotland.</location>
<marker>Yang, 2011</marker>
<rawString>Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation of natural images. In Empirical Methods in Natural Language Processing (EMNLP), Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haonan Yu</author>
<author>Jeffrey Mark Siskind</author>
</authors>
<title>Grounded language learning from video described with sentences.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<volume>1</volume>
<pages>53--63</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<contexts>
<context position="5990" citStr="Yu and Siskind, 2013" startWordPosition="891" endWordPosition="894">they do not model image and textual features jointly. 2.2 Image Understanding Recent improvements in state-of-the-art visual object class detections (Felzenszwalb et al., 2010) 1A research proposal for this framework and other image captioning ideas was previously presented at NAACL Student Research Workshop in 2013 (Mason, 2013). This paper presents a completed project including implementation details and experimental results. have enabled much recent work in image caption generation (Farhadi et al., 2010; Ordonez et al., 2011; Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Yu and Siskind, 2013). However, these systems typically rely on a small number of detection types, e.g. the twenty object categories from the PASCAL VOC challenge.2 These object categories include entities which are commonly described in general domain images (people, cars, cats, etc) but these require labeled training data which is not typically available for the visually relevant entities in specific domains. Our caption generation system employs a multimodal topic model from our previous work (Mason and Charniak, 2013) which generates descriptive words, but lacks the spatial structure needed to generate a full </context>
</contexts>
<marker>Yu, Siskind, 2013</marker>
<rawString>Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded language learning from video described with sentences. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 53–63, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000318">
<title confidence="0.819821">
Automated Measures of Specific Vocabulary Knowledge from Constructed
Responses (“Use These Words to Write a Sentence Based on this Picture”)
</title>
<author confidence="0.849327">
Swapna Somasundaran
</author>
<affiliation confidence="0.821406">
Educational Testing Services
</affiliation>
<address confidence="0.935127">
660 Rosedale Road,
Princeton, NJ 08541, USA
</address>
<email confidence="0.998177">
ssomasundaran@ets.org
</email>
<author confidence="0.979031">
Martin Chodorow
</author>
<affiliation confidence="0.9859075">
Hunter College and the Graduate Center
City University of New York,
</affiliation>
<address confidence="0.986953">
New York, NY 10065, USA
</address>
<email confidence="0.999714">
martin.chodorow@hunter.cuny.edu
</email>
<sectionHeader confidence="0.994821" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899">
We describe a system for automatically
scoring a vocabulary item type that asks
test-takers to use two specific words in
writing a sentence based on a picture. The
system consists of a rule-based component
and a machine learned statistical model
which uses a variety of construct-relevant
features. Specifically, in constructing the
statistical model, we investigate if gram-
mar, usage, and mechanics features devel-
oped for scoring essays can be applied to
short answers, as in our task. We also ex-
plore new features reflecting the quality of
the collocations in the response, as well as
features measuring the consistency of the
response to the picture. System accuracy
in scoring is 15 percentage points greater
than the majority class baseline and 10
percentage points less than human perfor-
mance.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982931034483">
It is often said that the best way to see if a per-
son knows the meaning of a word is to have that
person use the word in a sentence. Despite this
widespread view, most vocabulary testing contin-
ues to rely on multiple choice items (e.g. (Law-
less et al., 2012; Lawrence et al., 2012)). In
fact, few assessments use constructed sentence re-
sponses to measure vocabulary knowledge, in part
because of the considerable time and cost required
to score such responses manually. While much
progress has been made in automatically scor-
ing writing quality in essays (Attali and Burstein,
2006; Leacock et al., 2014; Dale et al., 2012),
the essay scoring engines do not measure profi-
ciency in the use of specific words, except perhaps
for some frequently confused homophones (e.g.,
its/it’s, there/their/they’re, affect/effect).
In this paper we present a system for automated
scoring of targeted vocabulary knowledge based
on short constructed responses in a picture de-
scription task. Specifically, we develop a system
for scoring a vocabulary item type that is in op-
erational use in English proficiency tests for non-
native speakers. Each task prompt in this item type
consists of two target key words, for which the vo-
cabulary proficiency is tested, and a picture that
provides the context for the sentence construction.
The task is to generate a single sentence, incorpo-
rating both key words, consistent with the picture.
Presumably, a test-taker with competent knowl-
edge of the key words will be able to use them in a
well-formed grammatical sentence in the context
of the picture.
Picture description tasks have been employed in
a number of areas of study ranging from second
language acquisition to Alzheimer’s disease (El-
lis, 2000; Forbes-McKay and Venneri, 2005). Pic-
tures and picture-based story narration have also
been used to study referring expressions (Lee et
al., 2012) and to analyze child narratives in order
to predict language impairment (Hassanali et al.,
2013). Evanini et al. (2014) employ a series of
pictures and elicit (oral) story narration to test En-
glish language proficiency. In our task, the picture
is used as a constraining factor to limit the type
and content of sentences that can be generated us-
ing the given key words.
In the course of developing our system, we ex-
amined existing features that have been developed
for essay scoring, such as detectors of errors in
grammar, usage and mechanics, as well as col-
location features, to see if they can be re-used
for scoring short responses. We also developed
new features for assessing the quality of sentence
construction using Pointwise Mutual Information
(PMI). As our task requires responses to describe
the prompt pictures, we manually constructed de-
tailed textual descriptions of the pictures, and de-
</bodyText>
<page confidence="0.827718">
1
</page>
<note confidence="0.718754">
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1–11,
Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999911681818182">
veloped features that measure the overlap between
the content of the responses and the textual de-
scription. Our automated scoring system is partly
based on deterministic scoring criteria and partly
statistical. Overall, it achieves an accuracy of
76%, which is a 15 percentage point improvement
over a simple majority class baseline.
The organization of this paper is as follows:
Section 2 describes the picture description task
and the scoring guide that is used to manually
score the picture description responses opera-
tionally. It also considers which aspects of scor-
ing may be handled best by deterministic proce-
dures and which are more amenable to statistical
modeling. Section 3 details the construction of a
reference corpus of text describing each picture,
and Section 4 presents the features used in scor-
ing. Section 5 describes our system architecture
and presents our experiments and results. Detailed
analysis is presented in Section 6, followed by re-
lated work in Section 7 and a summary with direc-
tions for future research in Section 8.
</bodyText>
<sectionHeader confidence="0.89511" genericHeader="introduction">
2 Task Description and Data
</sectionHeader>
<bodyText confidence="0.999479763157895">
The picture description task is an item type that is
in actual operational use as part of a test of En-
glish. It consists of a picture, along with two key
words, one or both of which may be in an inflected
form. Test-takers are required to use the two words
in one sentence to describe the picture. They may
change the inflections of the words as appropriate
to the context of their sentence, but they must use
some form of both words in one sentence. Requir-
ing them to produce a response based on the pic-
ture constrains the variety of sentences and words
that they are likely to generate.
Trained human scorers evaluate the responses
based on appropriate use of grammar and the rel-
evance of the sentence to the picture. The opera-
tional scoring guide is as follows:
score = 3 The response consists of ONE sen-
tence that: (a) has no grammatical errors, (b)
contains forms of both key words used appro-
priately, AND (c) is consistent with the pic-
ture.
score = 2 The response consists of one or
more sentences that: (a) have one or more
grammatical errors that do not obscure the
meaning, (b) contain BOTH key words, (but
they may not be in the same sentence and
the form of the word(s) may not be accurate),
AND (c) are consistent with the picture.
score = 1 The response: (a) has errors that in-
terfere with meaning, (b) omits one or both
key words, OR (c) is not consistent with the
picture.
score = 0 The response is blank, written in
a foreign language, or consists of keystroke
characters.
Our decisions about scoring system design are
based on the scoring guide and its criteria. Some
aspects of the scoring can be handled by simple
pattern matching or lookup, while others require
machine learning. For example, score 0 is as-
signed to responses that are blank or are not in
English. This can be detected and scored in a
straightforward way. On the other hand, the de-
termination of grammaticality for the score points
3, 2 and 1 depends on the presence and severity
of grammatical errors. A wide variety of such er-
rors appear in responses, including errors of punc-
tuation, subject-verb agreement, preposition usage
and article usage. The severity of an error depends
on how problematic the error is, and the system
will have to learn this from the behavior of the
trained human scorer(s), making this aspect of the
scoring more amenable to statistical modeling.
Similarly, statistical modeling is more suitable
for determining the consistency of the response
with respect to the picture. According to the scor-
ing guide, a response gets a score of 0 or 1 if it is
not consistent with the picture, and gets a score of
2 or 3 if it is consistent. Thus, this aspect cannot
solely determine the score of a response – it influ-
ences the score in conjunction with other language
proficiency factors. Further, measures of how rel-
evant a response is to a picture are likely to fall on
a continuous scale, making a statistical modeling
approach appropriate.
Finally, although there are some aspects of the
scoring guide, such as the number of sentences
and the presence of the key words, that can be
measured trivially, they do not act as sole deter-
minants of the score. For example, having more
than one sentence can result in the response re-
ceiving a score of 2 or 1. The number of sentences
works in conjunction with other factors such as
severity of grammar errors and relevance to the
picture. Hence its contribution to the final score
is best modeled statistically.
</bodyText>
<page confidence="0.98833">
2
</page>
<bodyText confidence="0.999763">
As a result of the heterogeneous nature of the
problem, our system is made up of a statistical
learning component as well as a non-statistical
component.
</bodyText>
<subsectionHeader confidence="0.988264">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.999988133333333">
The data set consists of about 58K responses to
434 picture prompts. The mean response length
was 11.26 words with a standard deviation of 5.10.
The data was split into 2 development sets (con-
sisting of a total of about 2K responses) and a fi-
nal train-test set (consisting of the remaining 56K
responses) used for evaluation. All 58K responses
were human scored using the scoring rubric dis-
cussed in Section 2. About 17K responses were
double annotated. The inter-annotator agreement,
using quadratic weighted kappa (QWK), was 0.83.
Score point 3, the most frequent class, was as-
signed to 61% of the responses, followed by score
point 2 (31%), score point 1 (7.6%) and score
point 0 (0.4%).
</bodyText>
<sectionHeader confidence="0.994519" genericHeader="method">
3 Reference Picture Descriptions
</sectionHeader>
<bodyText confidence="0.984468315789474">
The pictures in our task vary in their complexity.
A typical prompt picture might be a photograph
of an outdoor marketplace, the inside of an airport
terminal, a grocery store, a restaurant or a store
room. Because consistency with respect to the pic-
ture is a crucial component in our task, we needed
a reliable and exhaustive textual representation of
each picture. Therefore, we manually constructed
a reference text corpus for each of our 434 pic-
ture prompts. We chose to use manual creation of
the reference corpus instead of trying automated
image recognition because automated methods of
image recognition are error prone and would result
in a noisy reference corpus. Additionally, auto-
mated approaches would, at best, give us a (noisy)
list of items that are present in the picture, but not
the overall scene or event depicted.
Two annotators employed by a company that
specializes in annotation created the reference cor-
pora of picture descriptions. The protocol used for
creating the reference corpus is shown below:
Part-1: List the items, setting, and events
in the picture.
List, one by one, all the items and events you
see in the picture. These may be animate ob-
jects (e.g. man), inanimate objects (e.g. table)
or events (e.g. dinner). Try to capture both the
overall setting (restaurant), as well as the ob-
jects that make up the picture (e.g. man, table,
food). These are generally (but not necessar-
ily) nouns and noun phrases. Some pictures
can have many items, while some have only a
few. The goal is to list 10-15 items and to cap-
ture as many items as possible, *starting with
the most obvious ones*.
If the picture is too sparse, and you are not
able to list at least 10 items, please indicate
this as a comment.
</bodyText>
<subsectionHeader confidence="0.962775">
Part:2 Describe the picture
</subsectionHeader>
<bodyText confidence="0.999979388888889">
Describe the scene unfolding in the picture.
The scene in the picture may be greater than
the sum of its parts (many of which you will
list in part-1). For example, the objects in a
picture could be “shoe” “man” “chair”, but the
scene in the picture could be that of a shoe
purchase. The description tries to recreate the
scene (or parts of the scene) depicted in the
picture.
Generate a paragraph of 5-7 sentences de-
scribing the picture. Some of these sentences
will address what is going on, while some may
address relations between items. The propor-
tions of these will differ, based on the picture.
Make sure that you generate at least one sen-
tence containing the two key words.
If the picture is too simple, and you are not
able to generate at least 5 sentences, please
indicate this as a comment.
The human annotator was given the picture and
the two key words. The protocol for creating each
reference corpus asked the annotator to first ex-
haustively list all the items (animate and inani-
mate) in the picture. Then, the annotator was
asked to describe the scene in the picture. We used
this two step process in order to capture, as much
as possible, all objects, relationships between ob-
jects, settings and events depicted in the pictures.
The size of the reference corpus for each prompt
is much larger than the single sentence test-taker
response. This is intentional as the goal is to make
the reference corpus as exhaustive as possible. We
used a single annotator for each prompt. Double
annotation using a secondary annotator was done
in cases where we felt that the coverage of the cor-
pus created by the primary annotator was insuffi-
</bodyText>
<page confidence="0.994256">
3
</page>
<bodyText confidence="0.999236230769231">
cient1.
In order to test coverage, we used a small devel-
opment set of essays from each prompt and com-
pared the coverage of the generated reference cor-
pus over the development essays. If the cover-
age (proportion of content words in the responses
that were found in the reference corpus) was less
than 50% (this was the case for about 20% of
the prompts), we asked the secondary annotator to
create a new reference corpus for the prompt. The
two reference corpora for the prompt were then
simply combined to form a single reference cor-
pus.
</bodyText>
<sectionHeader confidence="0.999416" genericHeader="method">
4 Features for automated scoring
</sectionHeader>
<bodyText confidence="0.999995821428571">
Because the score points in the scoring guide con-
flate, to some degree, syntactic, semantic, and
other weaknesses in the response, we carried out
a scoring study on a second small development
set (comprising of a total of 80 responses from 4
prompts, picked randomly) to gather insight into
the general problems in English language profi-
ciency exhibited in the responses. For the study,
it was necessary to have test-taker responses re-
scored by an annotator using an analytic scheme
which makes the types and locations of prob-
lems explicit. This exercise revealed that, in ad-
dition to the factors stated explicitly in the scor-
ing guide, there is another factor that results in
low comprehension (readability) of the sentence
and that reflects lower English proficiency. Specif-
ically, the annotator tagged many sentences as be-
ing “awkward”. This awkwardness was due to
poor choice of words or to poor construction of the
sentence. For example, in the sentence “The man
is putting some food in bags while he is record-
ing for the payment”, “recording for the payment”
was marked as an awkward phrase. Based on our
annotation of the scores and on the descriptions in
the scoring guide, we selected features designed to
capture grammar, picture relevance and awkward
usage. We discuss each of our feature sets in the
following subsections.
</bodyText>
<sectionHeader confidence="0.8893965" genericHeader="method">
4.1 Features for Grammatical Error
Detection
</sectionHeader>
<bodyText confidence="0.956373074074074">
Essay scoring engines such as e-rater® (Attali
and Burstein, 2006) typically use a number of
1We do not conduct inter-annotator agreement studies as
the goal of the double annotation was to create a diverse de-
scription.
grammar, usage and mechanics features that de-
tect and quantify different types of English usage
errors in essays. Examples of some of these error
types are: Run-on Sentences, Subject Verb Agree-
ment Errors, Pronoun Errors, Missing Posses-
sive Errors, Wrong Article Errors, Missing Arti-
cle Errors, Preposition Errors, Non-standard Verb
or Word Form Errors, Double Negative Errors,
Fragment or Missing Comma Errors, Ill-formed
Verb Errors, Wrong Form of Word Errors, Spelling
Errors, Wrong Part of Speech Errors, and Missing
Punctuation Errors .
In addition to these, essay scoring engines of-
ten also use as features the Number of Sentences
that are Short, the Number of Sentences that are
Long, the Number of Passive Sentences, and other
features that are relevant only for longer texts such
as essays. Accordingly, we selected, from e-rater
113 grammar, word usage, mechanics and lexical
complexity features that could be applied to our
short response task. This forms our grammar fea-
ture set.
</bodyText>
<sectionHeader confidence="0.961598" genericHeader="method">
4.2 Features for Measuring Content
Relevance
</sectionHeader>
<bodyText confidence="0.998420807692308">
We generated a set of features that measure the
content overlap between a given response and the
corresponding reference corpus for the prompt.
For this, first the keywords and the stop words
were removed from the response and the reference
corpus, and then the proportion of overlap was cal-
culated between the lemmatized content words of
the response and the lemmatized version of the
corresponding reference corpus, as follows:
|Response ∩ Corpus|
|Response|
It is not always necessary for the test-taker to
use exactly the same words found in the reference
corpus. For example, the annotator might have
referred to a person in the picture as a “lady”,
while a response may refer to the same person
as a “woman” or “girl” or even just “person”.
Thus, we needed to go beyond simple lexical
match. In order to account for synonyms, we ex-
panded the content words in the reference corpus
by adding their synonyms, as provided in Lin’s
thesaurus (Lin, 1998) and then compared the ex-
panded reference to each response. Along the
same lines, we also used expansions from Word-
Net synonyms, WordNet hypernyms and WordNet
hyponyms. The following is the list of our content
</bodyText>
<page confidence="0.989729">
4
</page>
<bodyText confidence="0.997475">
relevance features. Each measures the proportion
of overlap as described by the equation above be-
tween the lemmatized response and
</bodyText>
<listItem confidence="0.997323545454545">
1. lemmas: the lemmatized reference corpus.
2. cov-lin: the reference corpus expanded using
Lin’s thesaurus.
3. cov-wn-syns: the reference corpus expanded
using WordNet Synonyms.
4. cov-wn-hyper: the reference corpus ex-
panded using WordNet Hypernyms.
5. cov-wn-hypo: the reference corpus ex-
panded using WordNet Hyponyms.
6. cov-all: the reference corpus expanded using
all of the above methods.
</listItem>
<bodyText confidence="0.999185055555556">
Mean proportions of overlap ranged from 0.65
for lemmas to 0.97 for cov-all.
The 6 features listed above, along with the
prompt id give a total of 7 features that form our
relevance feature set. We use prompt id as a fea-
ture because the extent of overlap can depend on
the prompt. Some pictures are very sparse, so,
the description of the picture in the response will
be short, and will not vary much from the refer-
ence corpus. For these, a good amount of overlap
between the response and reference corpus is ex-
pected. Other pictures are very dense with a large
number of objects and items shown. In this case,
any single response may describe just a small sub-
set of the items and satisfy the consistency criteria,
and consequently, even a small overlap between
the response and the reference corpus may be suf-
ficient.
</bodyText>
<subsectionHeader confidence="0.930982">
4.3 Features for Awkward Word Usage
</subsectionHeader>
<bodyText confidence="0.9998995">
In order to measure awkward word usage, we ex-
plored PMI-based features, and also investigated
whether some features developed for essay scor-
ing can be used effectively for this purpose.
</bodyText>
<subsubsectionHeader confidence="0.553994">
4.3.1 PMI-based ngram features
</subsubsectionHeader>
<bodyText confidence="0.999748588235294">
Non-native writing is often characterized by in-
appropriate combinations of words, indicating the
writer’s lack of knowledge of collocations. For ex-
ample, “recording for the payment” might be bet-
ter expressed as “entering the price in the cash reg-
ister”. As “recording for the payment” is an inap-
propriate construction, it is not likely to be com-
mon, for example, in a large web corpus. We use
this intuition in constructing our PMI-based fea-
tures.
We find the PMI of all adjacent word pairs
(bigrams), as well as all adjacent word triples
(trigrams) in the Google 1T web corpus (Brants
and Franz, 2006) using the TrendStream database
(Flor, 2013).
PMI between word pairs (bigram AB) is defined
as:
</bodyText>
<equation confidence="0.9240625">
p(AB)
loge p(A).p(B)
</equation>
<bodyText confidence="0.903993">
and between word triples (trigram ABC) as
</bodyText>
<equation confidence="0.9456375">
p(ABC)
loge p(A).p(B).p(C)
</equation>
<bodyText confidence="0.999959538461538">
The higher the value of the PMI, the more com-
mon is the collocation for the word pair/triple in
well formed texts. On the other hand, negative
values of PMI indicate that the given word pair (or
triple) is less likely than chance to occur together.
We hypothesized that this would be a good indica-
tor of awkward usage, as suggested in (Chodorow
and Leacock, 2000).
The PMI values for adjacent words obtained
over the entire response are then assigned to bins,
with 8 bins for word pairs and another 8 for word
triples. Each bin represents a range for PMI p tak-
ing real values R as follows:
</bodyText>
<equation confidence="0.999541625">
bine = {p ∈ R  |p &gt; 20}
bine = {p ∈ R  |10 &lt; p ≤ 20}
bin3 = {p ∈ R  |1 &lt; p ≤ 10}
bin4={p ∈ R  |0 &lt; p ≤ 1}
bins = {p ∈ R  |− 1 &lt; p ≤ 0}
bin6 = {p ∈ R  |− 10 &lt; p ≤ −1}
bin7 = {p ∈ R  |− 20 &lt; p ≤ −10}
bin8 = {p ∈ R  |p ≤ −20}
</equation>
<bodyText confidence="0.999389454545454">
Once the PMI values for the adjacent word pairs
in the response are generated, we generate two sets
of features. The first set is based on the counts
of word pairs falling into each bin (for example,
Number ofpairs falling into bine, Number ofpairs
falling into bine and so on). The second set of fea-
tures are based on percentages (for example Per-
centage of pairs falling into bine, Percentage of
pairs falling into bine etc.). These two sets result
in a total of 16 features. We similarly generate
16 more features for adjacent word triples. We
</bodyText>
<page confidence="0.965757">
5
</page>
<bodyText confidence="0.9999615">
use percentages in addition to raw counts to ac-
count for the length of the response. For example,
it is possible for a long sentence to have phrases
that are awkward as well as well formed, giving
the same counts of phrases in the high-PMI value
bins as that of a short sentence that is entirely well
formed.
In addition to binning, we also encode as fea-
tures the maximum, minimum and median PMI
value obtained over all word pairs. The first two
features capture the best and the worst word col-
locations in a response. The median PMI value
captures the overall general quality of the response
in a single number. For example, if this is a low
number, then the response generally has many bad
phrasal collocations. Finally a null-PMI feature is
used to count the number of pairs that had zero
entries in the database. This feature is an indica-
tor that the given words or word collocations were
not found even once in the database. Given the
size of the underlying database, this usually hap-
pens in cases when words are misspelled, or when
the words never occur together.
All features created for bigrams are also created
for trigrams. We thus have a total of 40 features,
called the pmi feature set.
</bodyText>
<subsubsectionHeader confidence="0.746927">
4.3.2 Features from essay scoring
</subsubsectionHeader>
<bodyText confidence="0.999884333333333">
A number of measures of collocation quality have
been proposed and implemented (e.g. (Futagi et
al., 2008; Dahlmeier and Ng, 2011)). We use e-
rater’s measure of the density of ‘good’ colloca-
tions found in the response. Another source of
difficulty for non-native writers is the selection of
appropriate prepositions. We use the mean proba-
bility assigned by e-rater to the prepositions in the
response. These two measures, one for the qual-
ity of collocations and the other for the quality of
prepositions, are combined in our colprep feature
set.
</bodyText>
<subsectionHeader confidence="0.999856">
4.4 Scoring Rubric-based Features
</subsectionHeader>
<bodyText confidence="0.990638142857143">
As seen in Section 2, some of the criteria for scor-
ing are quite straightforward (e.g. “omits one or
both key words”). While these are not sole deter-
minants of a score, they are certainly strong influ-
ences. Thus, we encode four criteria from the scor-
ing guide. These form our final feature set, rubric,
and are binary values, answering the questions: Is
the first key word from the prompt present in the
response? Is the second key word from the prompt
present in the response? Are both key words from
the prompt present in the response? Is there more
than one sentence in the response?
Table 1 provides a list of feature types and the
corresponding number of features of each type.
</bodyText>
<figureCaption confidence="0.518568666666667">
Feature set type Number of Features
grammar 113
relevance 7
pmi 40
colprep 2
rubric 4
</figureCaption>
<tableCaption confidence="0.992178">
Table 1: Feature sets and the counts of features in
each set
</tableCaption>
<sectionHeader confidence="0.641704" genericHeader="method">
5 System and Evaluation
</sectionHeader>
<figureCaption confidence="0.999864">
Figure 1: System Architecture
</figureCaption>
<bodyText confidence="0.999978882352941">
As noted earlier, the system is partly rule-based
and partly statistical. Figure 1 illustrates the sys-
tem architecture. The rule-based part captures
the straightforward deterministic scoring criteria
while the machine learning component encodes
features described in Section 4 and learns how to
weight the features for scoring based on human-
scored responses.
As described in Section 2, detection of condi-
tions that result in a score of zero are straight-
forward. Our rule-based scorer (shown as “For-
eign Language Detector” in Figure 1) assigns a
zero score to a response if it is blank or non-
English. The system determines if the response is
non-English based on the average of PMI bigram
scores over the response. If the average score is
less than a threshold value, the system tags it as
</bodyText>
<page confidence="0.998299">
6
</page>
<bodyText confidence="0.999944260869565">
a non-English sentence. The threshold was deter-
mined by manually inspecting the PMI values ob-
tained for sentences belonging to English and non-
English news texts. Responses given zero scores
by this module are filtered out and do not go to the
next stage.
Responses that pass the rule-based scorer are
then sent to the statistical scorer. Here, we encode
the features discussed in Section 4. Spell checking
and correction are carried out before features for
content relevance and PMI-based awkward word
usage are computed. This is done in order to pre-
vent misspellings from affecting the reference cor-
pus match or database search. The original text
is sent to the Grammar feature generator as it cre-
ates features based on misspellings and other word
form errors. Finally, we use all the features to train
a Logistic Regression model using sklearn. Note
that the statistical system predicts all 4 scores (0
through 3). This is because the rule-based system
is not perfect; that is, it might miss some responses
that should receive zero scores, and pass them over
to the next stage.
</bodyText>
<subsectionHeader confidence="0.98581">
5.1 Metrics
</subsectionHeader>
<bodyText confidence="0.9999626">
We report our results using overall accuracy,
quadratic weighted kappa (QWK) and score-level
precision, recall and f-measure. The precision P
of the system is calculated for each score point i
as
</bodyText>
<equation confidence="0.904739">
Pi = |Si|
</equation>
<bodyText confidence="0.9478645">
where |Si |is the number of responses given a
score of i by the system, and |Si ∩ Hi |is the num-
ber of responses given a score of i by the system
as well as the human rater.
Similarly, recall, R is calculated for each score
point i as
</bodyText>
<equation confidence="0.98474">
Ri = |Si ∩ Hi ||Hi|
</equation>
<bodyText confidence="0.9979982">
F-measure Fi is calculated as the harmonic
mean of the precision Pi and recall Ri at each
score point i. Accuracy is the ratio of the num-
ber of responses correctly classified over the total
number of responses.
</bodyText>
<subsectionHeader confidence="0.824422">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999851285714286">
All of the responses in the train-test set were
passed through the rule-based zero-scorer. A total
of 210 responses had been scored as zero by the
human scorer. The rule-based system scored 222
responses as zeros, of which 184 were correct.
The precision Prule of the rule-based system is
calculated as
</bodyText>
<equation confidence="0.962173333333333">
184
P0 rule = 222 = 82.9%
Similarly, Recall is calculated as
Rrule
0 = 184
210 = 87.6%
</equation>
<bodyText confidence="0.979373">
The corresponding F-measure is 85.2%
The remaining responses pass to the next stage
where machine learning is employed. We per-
formed 10 fold cross-validation experiments us-
ing Logistic Regression as well as Random Forest
learners. As the results are comparable, we only
report those from logistic regression.
</bodyText>
<table confidence="0.99886075">
Accuracy in % Agreement (QWK)
Baseline 61.00 -
System 76.23 0.63
Human 86.00 0.83
</table>
<tableCaption confidence="0.994047">
Table 2: Overall system and human accuracy
</tableCaption>
<bodyText confidence="0.889354928571429">
(in percentage) and agreement (using Quadratic
Weighted Kappa)
Table 2 reports the results. The system achieves
an accuracy of 76.23%, which is more than a 15
percentage point improvement over the majority
class baseline of 61%. The majority class base-
line always predicts a score of 3. Compared to hu-
man performance, system performance is 10 per-
centage points lower (human-human agreement
is 86%). Quadratic weighted kappa for system-
human agreement is also lower (0.63) than for
human-human agreement (0.83).
Table 3 reports the precision, recall and F-
measure of the system for each of the score points.
</bodyText>
<table confidence="0.9988884">
Score point Precision Recall F-measure
0 84.2 68.3 72.9
1 78.4 67.5 72.6
2 70.6 50.4 58.8
3 77.8 90.5 83.6
</table>
<tableCaption confidence="0.9010415">
Table 3: Overall system performance at each score
point using all features
</tableCaption>
<sectionHeader confidence="0.984468" genericHeader="method">
6 Analysis
</sectionHeader>
<bodyText confidence="0.999119">
In order to understand the usefulness of each fea-
ture set in scoring the responses, we constructed
</bodyText>
<equation confidence="0.632205">
|Si ∩ Hi|
</equation>
<page confidence="0.990839">
7
</page>
<bodyText confidence="0.999902705882353">
systems using first the individual features alone,
and then using feature combinations. Table 4 re-
ports the accuracy of the learner using individual
features alone. We see that, individually, each fea-
ture set performs much below the performance of
the full system (that has an accuracy of 76.23%),
which is expected, as each feature set represents
a particular aspect of the construct. However, in
general, each of the feature-sets (except colprep)
shows improvement over baseline, indicating that
they contribute towards performance improvement
in the automated system.
Grammar features are the best of the individ-
ual feature sets at 70% accuracy, indicating that
grammatical error features developed for longer
texts can be applied to single sentences. The PMI-
based feature set is the second best performer, in-
dicating its effectiveness in capturing word usage
issues. While colprep and pmi both capture awk-
ward usage, pmi alone shows better performance
(67.44%) than colprep alone (61.26%). Also,
when rubric is used alone, the resulting system
produces a four percentage point improvement
over the baseline, with 65% accuracy, indicating
the presence of responses where the test-takers are
not able to incorporate one or both words in a sin-
gle sentence. The relevance feature set by itself
does not show substantial improvement over the
baseline. This is not surprising, as according to
the scoring guide, a response gets a score of 0 or 1
if it does not describe the picture, and gets a score
of 2 or 3 if it is relevant to the picture. Hence, this
feature cannot solely and accurately determine the
score.
</bodyText>
<table confidence="0.996330833333333">
Feature Set Accuracy in %
grammar 70.30
pmi 67.44
rubric 65.00
relevance 62.50
colprep 61.26
</table>
<tableCaption confidence="0.934777222222222">
Table 4: System performance for individual fea-
tures
Table 5 reports accuracies of systems built us-
ing feature set combinations. The first feature set
combination, grammar + colprep, is a set of all
features obtained from essay scoring. Here we see
that addition of colprep does not improve the per-
formance over that obtained by grammar features
alone. Further, when colprep is combined with
</tableCaption>
<bodyText confidence="0.99965365">
pmi (colprep+pmi, row 2), there is a slight drop
in performance as compared to using pmi-based
features alone. These results indicate that colprep,
while being useful for larger texts, does not trans-
fer well to the simple single sentence responses in
our task.
Further, in Table 5 we see that the system using
a combination of the pmi feature set and the rele-
vance feature set (pmi+relevance) achieves an ac-
curacy of 69%. Thus, this feature combination is
able to improve performance over that using either
feature set alone, indicating that while content rel-
evance features by themselves do not create an im-
pact, they can improve performance when added
to other features. Finally, the feature combination
of all new features developed for this task (pmi +
relevance+ rubric) yields 73% accuracy, which is
again better than each individual feature set’s per-
formance, indicating that they can be synergisti-
cally combined to improve system performance.
</bodyText>
<table confidence="0.9719628">
Feature Set Accuracy in %
(i) grammar + colprep 70.31
(ii) colprep + pmi 67.42
(iii) pmi + relevance 69.05
(iv) pmi + relevance + rubric 73.21
</table>
<tableCaption confidence="0.909087">
Table 5: System performance for feature combi-
nations (i) typically used in essay scoring, (ii) that
measure awkwardness, (iii) newly proposed here,
(iv) newly proposed plus rubric-specific criteria
</tableCaption>
<sectionHeader confidence="0.999294" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999766055555556">
Most work in automated scoring and learner lan-
guage analysis has focused on detecting grammar
and usage errors (Leacock et al., 2014; Dale et al.,
2012; Dale and Narroway, 2012; Gamon, 2010;
Chodorow et al., 2007; Lu, 2010). This is done
either by means of handcrafted rules or with sta-
tistical classifiers using a variety of information.
In the case of the latter, the emphasis has been on
representing the contexts of function words, such
as articles and prepositions. This work is rele-
vant inasmuch as errors in using content words,
such as nouns and verbs, are often reflected in the
functional elements which accompany them, for
example, articles that indicate the definiteness or
countability of nouns, and prepositions that mark
the cases of the arguments of verbs.
Previous work (Bergsma et al., 2009; Bergsma
et al., 2010; Xu et al., 2011) has shown that mod-
</bodyText>
<page confidence="0.99527">
8
</page>
<bodyText confidence="0.999978535211268">
els which rely on large web-scale n-gram counts
can be effective for the task of context-sensitive
spelling correction. Measures of ngram associa-
tion such as PMI, log likelihood, chi-square, and
t have a long history of use for detecting colloca-
tions and measuring their quality (see (Manning
and Sch¨utze, 1999) and (Leacock et al., 2014)
for reviews). Our application of a large n-gram
database and PMI is to detect inappropriate word
usage.
Our task also differs from work focusing on
evaluating content (e.g. (Meurers et al., 2011;
Sukkarieh and Blackmore, 2009; Leacock and
Chodorow, 2003)) in that, although we are look-
ing for usage of certain content words, we focus
primarily on measuring knowledge of vocabulary.
Recent work on assessment measures of depth
of vocabulary knowledge (Lawless et al., 2012;
Lawrence et al., 2012), has argued that knowl-
edge of specific words can range from superficial
(idiomatic associations built up through word co-
occurrence) to topical (meaning-related associa-
tions between words) to deep (definitional knowl-
edge). Some of our features (e.g. awkward word
usage) capture some of this information (e.g., id-
iomatic associations between words), but assign-
ing the depth of knowledge of the key words is not
the focus of our task.
Work that is closely related to ours is that of
King and Dickinson (2013). They parse picture
descriptions from interactive learner sentences,
classify sentences into syntactic types and extract
the logical subject, verb and object in order to re-
cover simple semantic representations of the de-
scriptions. We do not explicitly model the seman-
tic representations of the pictures, but rather our
goal in this work is to ascertain if a response is
relevant to the picture and to measure other fac-
tors that reflect vocabulary proficiency.
We employ human annotators and use word
similarity measures to obtain alternative forms of
description because the proprietary nature of our
data prevents us from releasing our pictures to
the public. However, crowd sourcing has been
used by other researchers to collect human labels
for images and videos. For example, Rashtchian
et al. (2010) use Amazon Mechanical Turk and
Von Ahn and Dabbish (2004) create games to en-
tice players to correctly label images. Chen and
Dolan (2011) use crowd sourcing to collect multi-
ple paraphrased descriptions of videos to create a
paraphrasing corpus.
In a vast body of related work, automated
methods have been explored for the generation
of descriptions of images (Kulkarni et al., 2013;
Kuznetsova et al., 2012; Li et al., 2011; Yao et
al., 2010; Feng and Lapata, 2010a; Feng and La-
pata, 2010b; Leong et al., 2010; Mitchell et al.,
2012). There is also work in the opposite di-
rection, of finding or generating pictures for a
given narration. Joshi et al. (2006) found the
best set of images from an image database to
match the keywords in a story. Coyne and Sproat
(2001) developed a natural language understand-
ing system which converts English text into three-
dimensional scenes that represent the text. For a
high-stakes assessment, it would be highly unde-
sirable to have any noise in the gold-standard ref-
erence picture descriptions. Hence we chose to use
manual description for creating our reference cor-
pus.
</bodyText>
<sectionHeader confidence="0.977234" genericHeader="conclusions">
8 Summary and Future Directions
</sectionHeader>
<bodyText confidence="0.999976821428571">
We investigated different types of features for au-
tomatically scoring a vocabulary item type which
requires the test-taker to use two words in writ-
ing a sentence based on a picture. We generated a
corpus of picture descriptions for measuring the
relevance of responses, and as a foundation for
feature development, we performed preliminary
fine-grained annotations of responses. The fea-
tures used in the resulting automated scoring sys-
tem include newly developed statistical measures
of word usage and response relevance, as well as
features that are currently found in essay scoring
engines. System performance shows an overall
accuracy in scoring that is 15 percentage points
above the majority class baseline and 10 percent-
age points below human performance.
There are a number of avenues open for future
exploration. The automated scoring system might
be improved by extending the relevance feature
to include overlap with previously collected high-
scoring responses. The reference corpus could
also be expanded and diversified by using a large
number of annotators, at least some of whom are
speakers of the languages that are most promi-
nently represented in the population of test-takers.
Finally, one particular avenue we would like to ex-
plore is the use of our features to provide feedback
in low stakes practice environments.
</bodyText>
<page confidence="0.997063">
9
</page>
<sectionHeader confidence="0.983344" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999526209090909">
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v. 2.0. Journal of Technology,
Learning, and Assessment, 4:3.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2009. Web-scale n-gram models for lexical disam-
biguation. In IJCAI.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale
n-gram data. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 865–874. Association for Computa-
tional Linguistics.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Version 1. In Linguistic Data Consortium,
Philadelphia.
David L Chen and William B Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 190–200.
Association for Computational Linguistics.
Martin Chodorow and Claudia Leacock. 2000. An un-
supervised method for detecting grammatical errors.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics (NAACL), pages 140–147.
Martin Chodorow, Joel R Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involving
prepositions. In Proceedings of the fourth ACL-
SIGSEM workshop on prepositions, pages 25–30.
Association for Computational Linguistics.
Bob Coyne and Richard Sproat. 2001. Wordseye: an
automatic text-to-scene conversion system. In Pro-
ceedings of the 28th annual conference on Computer
graphics and interactive techniques, pages 487–496.
ACM.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Cor-
recting semantic collocation errors with L1 induced
paraphrases. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’11, pages 107–117, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Robert Dale and George Narroway. 2012. A frame-
work for evaluating text correction. In LREC, pages
3015–3018.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the Seventh Workshop on Building Educa-
tional Applications Using NLP, pages 54–62. Asso-
ciation for Computational Linguistics.
Rod Ellis. 2000. Task-based research and language
pedagogy. Language teaching research, 4(3):193–
220.
Keelan Evanini, Michael Heilman, Xinhao Wang, and
Daniel Blanchard. 2014. Automated scoring for
TOEFL Junior comprehensive writing and speaking.
Technical report, ETS, Princeton, NJ.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? Automatic caption genera-
tion for news images. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 1239–1249, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Yansong Feng and Mirella Lapata. 2010b. Topic
models for image annotation and text illustration.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
’10, pages 831–839, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Michael Flor. 2013. A fast and flexible architecture for
very large word n-gram datasets. Natural Language
Engineering, 19(1):61–93.
KE Forbes-McKay and Annalena Venneri. 2005. De-
tecting subtle spontaneous language decline in early
Alzheimers disease with a picture description task.
Neurological sciences, 26(4):243–254.
Yoko Futagi, Paul Deane, Martin Chodorow, and Joel
Tetreault. 2008. A computational approach to de-
tecting collocation errors in the writing of non-native
speakers of English. Computer Assisted Language
Learning, 21(4):353–367.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners’ writing: A meta-classifier
approach. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 163–171. Association for Computa-
tional Linguistics.
Khairun-nisa Hassanali, Yang Liu, and Thamar
Solorio. 2013. Using Latent Dirichlet Allocation
for child narrative analysis. ACL 2013, page 111.
Dhiraj Joshi, James Z. Wang, and Jia Li. 2006. The
story picturing engine—a system for automatic text
illustration. ACM Trans. Multimedia Comput. Com-
mun. Appl., 2(1):68–89, February.
Levi King and Markus Dickinson. 2013. Shallow se-
mantic analysis of interactive learner sentences. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 11–21, Atlanta, Georgia, June. Association
for Computational Linguistics.
Girish Kulkarni, Visruth Premraj, Vicente Ordonez,
Sagnik Dhar, Siming Li, Yejin Choi, Alexander C.
Berg, and Tamara L. Berg. 2013. Babytalk: Under-
standing and generating simple image descriptions.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 99(PrePrints):1.
</reference>
<page confidence="0.949569">
10
</page>
<reference confidence="0.999847673913044">
Polina Kuznetsova, Vicente Ordonez, Alexander C
Berg, Tamara L Berg, and Yejin Choi. 2012. Col-
lective generation of natural image descriptions. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 359–368. Association for
Computational Linguistics.
Ren´e Lawless, John Sabatini, and Paul Deane. 2012.
Approaches to assessing partial vocabulary knowl-
edge and supporting word learning: Assessing vo-
cabulary depth. In Annual Meeting of the Ameri-
can Educational Research Association, April 13-17,
2012, Vancouver, CA.
Joshua Lawrence, Elizabeth Pare-Blagoev, Ren´e Law-
less, and Chen Deane, Paul andLi. 2012. Gen-
eral vocabulary, academic vocabulary, and vocabu-
lary depth: Examiningpredictors of adolescent read-
ing comprehension. In Annual Meeting of the Amer-
ican Educational Research Association.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389–405.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2014. Automated Grammatical
Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Mor-
gan &amp; Claypool.
Choonkyu Lee, Smaranda Muresan, and Karin
Stromswold. 2012. Computational analysis of re-
ferring expressions in narratives of picture books.
NAACL-HLT 2012, page 1.
Chee Wee Leong, Rada Mihalcea, and Samer Hassan.
2010. Text mining for automatic image tagging. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 647–655.
Association for Computational Linguistics.
Siming Li, Girish Kulkarni, Tamara L Berg, Alexan-
der C Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning, pages 220–
228. Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th inter-
national conference on Computational linguistics-
Volume 2, pages 768–774. Association for Compu-
tational Linguistics.
Xiaofei Lu. 2010. Automatic analysis of syntactic
complexity in second language writing. Interna-
tional Journal of Corpus Linguistics, 15(4).
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press.
Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey M
Bailey. 2011. Integrating parallel analysis mod-
ules to evaluate the meaning of answers to reading
comprehension questions. International Journal of
Continuing Engineering Education and Life Long
Learning, 21(4):355–369.
Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa
Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi,
Tamara Berg, Karl Stratos, and Hal Daum´e III.
2012. Midge: Generating image descriptions from
computer vision detections. In Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 747–
756. Association for Computational Linguistics.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using Amazon’s Mechanical Turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon’s Mechan-
ical Turk, pages 139–147. Association for Computa-
tional Linguistics.
Jana Zuheir Sukkarieh and John Blackmore. 2009.
C-rater: Automatic content scoring for short con-
structed responses. In FLAIRS Conference.
Luis Von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 319–326. ACM.
Wei Xu, Joel Tetreault, Martin Chodorow, Ralph Gr-
ishman, and Le Zhao. 2011. Exploiting syntactic
and distributional information for spelling correc-
tion with web-scale n-gram models. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 1291–1300. Asso-
ciation for Computational Linguistics.
Benjamin Z Yao, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song-Chun Zhu. 2010. I2t: Image pars-
ing to text description. Proceedings of the IEEE,
98(8):1485–1508.
</reference>
<page confidence="0.999483">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.136447">
<title confidence="0.9701975">Automated Measures of Specific Vocabulary Knowledge from Constructed Responses (“Use These Words to Write a Sentence Based on this Picture”)</title>
<author confidence="0.642447">Swapna</author>
<affiliation confidence="0.702949">Educational Testing</affiliation>
<address confidence="0.998052">660 Rosedale Princeton, NJ 08541,</address>
<email confidence="0.990067">ssomasundaran@ets.org</email>
<author confidence="0.42694">Martin Hunter College</author>
<author confidence="0.42694">the Graduate</author>
<affiliation confidence="0.900645">City University of New</affiliation>
<address confidence="0.853923">New York, NY 10065,</address>
<email confidence="0.99905">martin.chodorow@hunter.cuny.edu</email>
<abstract confidence="0.997598571428571">We describe a system for automatically scoring a vocabulary item type that asks test-takers to use two specific words in writing a sentence based on a picture. The system consists of a rule-based component and a machine learned statistical model which uses a variety of construct-relevant features. Specifically, in constructing the statistical model, we investigate if grammar, usage, and mechanics features developed for scoring essays can be applied to short answers, as in our task. We also explore new features reflecting the quality of the collocations in the response, as well as features measuring the consistency of the response to the picture. System accuracy in scoring is 15 percentage points greater than the majority class baseline and 10 percentage points less than human performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yigal Attali</author>
<author>Jill Burstein</author>
</authors>
<title>Automated essay scoring with e-rater v.</title>
<date>2006</date>
<journal>2.0. Journal of Technology, Learning, and Assessment,</journal>
<volume>4</volume>
<contexts>
<context position="1801" citStr="Attali and Burstein, 2006" startWordPosition="281" endWordPosition="284">less than human performance. 1 Introduction It is often said that the best way to see if a person knows the meaning of a word is to have that person use the word in a sentence. Despite this widespread view, most vocabulary testing continues to rely on multiple choice items (e.g. (Lawless et al., 2012; Lawrence et al., 2012)). In fact, few assessments use constructed sentence responses to measure vocabulary knowledge, in part because of the considerable time and cost required to score such responses manually. While much progress has been made in automatically scoring writing quality in essays (Attali and Burstein, 2006; Leacock et al., 2014; Dale et al., 2012), the essay scoring engines do not measure proficiency in the use of specific words, except perhaps for some frequently confused homophones (e.g., its/it’s, there/their/they’re, affect/effect). In this paper we present a system for automated scoring of targeted vocabulary knowledge based on short constructed responses in a picture description task. Specifically, we develop a system for scoring a vocabulary item type that is in operational use in English proficiency tests for nonnative speakers. Each task prompt in this item type consists of two target </context>
<context position="15049" citStr="Attali and Burstein, 2006" startWordPosition="2543" endWordPosition="2546">as being “awkward”. This awkwardness was due to poor choice of words or to poor construction of the sentence. For example, in the sentence “The man is putting some food in bags while he is recording for the payment”, “recording for the payment” was marked as an awkward phrase. Based on our annotation of the scores and on the descriptions in the scoring guide, we selected features designed to capture grammar, picture relevance and awkward usage. We discuss each of our feature sets in the following subsections. 4.1 Features for Grammatical Error Detection Essay scoring engines such as e-rater® (Attali and Burstein, 2006) typically use a number of 1We do not conduct inter-annotator agreement studies as the goal of the double annotation was to create a diverse description. grammar, usage and mechanics features that detect and quantify different types of English usage errors in essays. Examples of some of these error types are: Run-on Sentences, Subject Verb Agreement Errors, Pronoun Errors, Missing Possessive Errors, Wrong Article Errors, Missing Article Errors, Preposition Errors, Non-standard Verb or Word Form Errors, Double Negative Errors, Fragment or Missing Comma Errors, Ill-formed Verb Errors, Wrong Form</context>
</contexts>
<marker>Attali, Burstein, 2006</marker>
<rawString>Yigal Attali and Jill Burstein. 2006. Automated essay scoring with e-rater v. 2.0. Journal of Technology, Learning, and Assessment, 4:3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Randy Goebel</author>
</authors>
<title>Web-scale n-gram models for lexical disambiguation.</title>
<date>2009</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="32352" citStr="Bergsma et al., 2009" startWordPosition="5508" endWordPosition="5511">0; Chodorow et al., 2007; Lu, 2010). This is done either by means of handcrafted rules or with statistical classifiers using a variety of information. In the case of the latter, the emphasis has been on representing the contexts of function words, such as articles and prepositions. This work is relevant inasmuch as errors in using content words, such as nouns and verbs, are often reflected in the functional elements which accompany them, for example, articles that indicate the definiteness or countability of nouns, and prepositions that mark the cases of the arguments of verbs. Previous work (Bergsma et al., 2009; Bergsma et al., 2010; Xu et al., 2011) has shown that mod8 els which rely on large web-scale n-gram counts can be effective for the task of context-sensitive spelling correction. Measures of ngram association such as PMI, log likelihood, chi-square, and t have a long history of use for detecting collocations and measuring their quality (see (Manning and Sch¨utze, 1999) and (Leacock et al., 2014) for reviews). Our application of a large n-gram database and PMI is to detect inappropriate word usage. Our task also differs from work focusing on evaluating content (e.g. (Meurers et al., 2011; Suk</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2009</marker>
<rawString>Shane Bergsma, Dekang Lin, and Randy Goebel. 2009. Web-scale n-gram models for lexical disambiguation. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Emily Pitler</author>
<author>Dekang Lin</author>
</authors>
<title>Creating robust supervised classifiers via web-scale n-gram data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>865--874</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32374" citStr="Bergsma et al., 2010" startWordPosition="5512" endWordPosition="5515">07; Lu, 2010). This is done either by means of handcrafted rules or with statistical classifiers using a variety of information. In the case of the latter, the emphasis has been on representing the contexts of function words, such as articles and prepositions. This work is relevant inasmuch as errors in using content words, such as nouns and verbs, are often reflected in the functional elements which accompany them, for example, articles that indicate the definiteness or countability of nouns, and prepositions that mark the cases of the arguments of verbs. Previous work (Bergsma et al., 2009; Bergsma et al., 2010; Xu et al., 2011) has shown that mod8 els which rely on large web-scale n-gram counts can be effective for the task of context-sensitive spelling correction. Measures of ngram association such as PMI, log likelihood, chi-square, and t have a long history of use for detecting collocations and measuring their quality (see (Manning and Sch¨utze, 1999) and (Leacock et al., 2014) for reviews). Our application of a large n-gram database and PMI is to detect inappropriate word usage. Our task also differs from work focusing on evaluating content (e.g. (Meurers et al., 2011; Sukkarieh and Blackmore, </context>
</contexts>
<marker>Bergsma, Pitler, Lin, 2010</marker>
<rawString>Shane Bergsma, Emily Pitler, and Dekang Lin. 2010. Creating robust supervised classifiers via web-scale n-gram data. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 865–874. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram Version 1.</title>
<date>2006</date>
<booktitle>In Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="19606" citStr="Brants and Franz, 2006" startWordPosition="3297" endWordPosition="3300">based ngram features Non-native writing is often characterized by inappropriate combinations of words, indicating the writer’s lack of knowledge of collocations. For example, “recording for the payment” might be better expressed as “entering the price in the cash register”. As “recording for the payment” is an inappropriate construction, it is not likely to be common, for example, in a large web corpus. We use this intuition in constructing our PMI-based features. We find the PMI of all adjacent word pairs (bigrams), as well as all adjacent word triples (trigrams) in the Google 1T web corpus (Brants and Franz, 2006) using the TrendStream database (Flor, 2013). PMI between word pairs (bigram AB) is defined as: p(AB) loge p(A).p(B) and between word triples (trigram ABC) as p(ABC) loge p(A).p(B).p(C) The higher the value of the PMI, the more common is the collocation for the word pair/triple in well formed texts. On the other hand, negative values of PMI indicate that the given word pair (or triple) is less likely than chance to occur together. We hypothesized that this would be a good indicator of awkward usage, as suggested in (Chodorow and Leacock, 2000). The PMI values for adjacent words obtained over t</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. In Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>William B Dolan</author>
</authors>
<title>Collecting highly parallel data for paraphrase evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>190--200</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="34697" citStr="Chen and Dolan (2011)" startWordPosition="5889" endWordPosition="5892">al in this work is to ascertain if a response is relevant to the picture and to measure other factors that reflect vocabulary proficiency. We employ human annotators and use word similarity measures to obtain alternative forms of description because the proprietary nature of our data prevents us from releasing our pictures to the public. However, crowd sourcing has been used by other researchers to collect human labels for images and videos. For example, Rashtchian et al. (2010) use Amazon Mechanical Turk and Von Ahn and Dabbish (2004) create games to entice players to correctly label images. Chen and Dolan (2011) use crowd sourcing to collect multiple paraphrased descriptions of videos to create a paraphrasing corpus. In a vast body of related work, automated methods have been explored for the generation of descriptions of images (Kulkarni et al., 2013; Kuznetsova et al., 2012; Li et al., 2011; Yao et al., 2010; Feng and Lapata, 2010a; Feng and Lapata, 2010b; Leong et al., 2010; Mitchell et al., 2012). There is also work in the opposite direction, of finding or generating pictures for a given narration. Joshi et al. (2006) found the best set of images from an image database to match the keywords in a </context>
</contexts>
<marker>Chen, Dolan, 2011</marker>
<rawString>David L Chen and William B Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 190–200. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>An unsupervised method for detecting grammatical errors.</title>
<date>2000</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association of Computational Linguistics (NAACL),</booktitle>
<pages>140--147</pages>
<contexts>
<context position="20155" citStr="Chodorow and Leacock, 2000" startWordPosition="3391" endWordPosition="3394">t word triples (trigrams) in the Google 1T web corpus (Brants and Franz, 2006) using the TrendStream database (Flor, 2013). PMI between word pairs (bigram AB) is defined as: p(AB) loge p(A).p(B) and between word triples (trigram ABC) as p(ABC) loge p(A).p(B).p(C) The higher the value of the PMI, the more common is the collocation for the word pair/triple in well formed texts. On the other hand, negative values of PMI indicate that the given word pair (or triple) is less likely than chance to occur together. We hypothesized that this would be a good indicator of awkward usage, as suggested in (Chodorow and Leacock, 2000). The PMI values for adjacent words obtained over the entire response are then assigned to bins, with 8 bins for word pairs and another 8 for word triples. Each bin represents a range for PMI p taking real values R as follows: bine = {p ∈ R |p &gt; 20} bine = {p ∈ R |10 &lt; p ≤ 20} bin3 = {p ∈ R |1 &lt; p ≤ 10} bin4={p ∈ R |0 &lt; p ≤ 1} bins = {p ∈ R |− 1 &lt; p ≤ 0} bin6 = {p ∈ R |− 10 &lt; p ≤ −1} bin7 = {p ∈ R |− 20 &lt; p ≤ −10} bin8 = {p ∈ R |p ≤ −20} Once the PMI values for the adjacent word pairs in the response are generated, we generate two sets of features. The first set is based on the counts of word </context>
</contexts>
<marker>Chodorow, Leacock, 2000</marker>
<rawString>Martin Chodorow and Claudia Leacock. 2000. An unsupervised method for detecting grammatical errors. In Proceedings of the Conference of the North American Chapter of the Association of Computational Linguistics (NAACL), pages 140–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Joel R Tetreault</author>
<author>Na-Rae Han</author>
</authors>
<title>Detection of grammatical errors involving prepositions.</title>
<date>2007</date>
<booktitle>In Proceedings of the fourth ACLSIGSEM workshop on prepositions,</booktitle>
<pages>25--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31756" citStr="Chodorow et al., 2007" startWordPosition="5410" endWordPosition="5413">ergistically combined to improve system performance. Feature Set Accuracy in % (i) grammar + colprep 70.31 (ii) colprep + pmi 67.42 (iii) pmi + relevance 69.05 (iv) pmi + relevance + rubric 73.21 Table 5: System performance for feature combinations (i) typically used in essay scoring, (ii) that measure awkwardness, (iii) newly proposed here, (iv) newly proposed plus rubric-specific criteria 7 Related Work Most work in automated scoring and learner language analysis has focused on detecting grammar and usage errors (Leacock et al., 2014; Dale et al., 2012; Dale and Narroway, 2012; Gamon, 2010; Chodorow et al., 2007; Lu, 2010). This is done either by means of handcrafted rules or with statistical classifiers using a variety of information. In the case of the latter, the emphasis has been on representing the contexts of function words, such as articles and prepositions. This work is relevant inasmuch as errors in using content words, such as nouns and verbs, are often reflected in the functional elements which accompany them, for example, articles that indicate the definiteness or countability of nouns, and prepositions that mark the cases of the arguments of verbs. Previous work (Bergsma et al., 2009; Be</context>
</contexts>
<marker>Chodorow, Tetreault, Han, 2007</marker>
<rawString>Martin Chodorow, Joel R Tetreault, and Na-Rae Han. 2007. Detection of grammatical errors involving prepositions. In Proceedings of the fourth ACLSIGSEM workshop on prepositions, pages 25–30. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coyne</author>
<author>Richard Sproat</author>
</authors>
<title>Wordseye: an automatic text-to-scene conversion system.</title>
<date>2001</date>
<booktitle>In Proceedings of the 28th annual conference on Computer graphics and interactive techniques,</booktitle>
<pages>487--496</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="35327" citStr="Coyne and Sproat (2001)" startWordPosition="6000" endWordPosition="6003">owd sourcing to collect multiple paraphrased descriptions of videos to create a paraphrasing corpus. In a vast body of related work, automated methods have been explored for the generation of descriptions of images (Kulkarni et al., 2013; Kuznetsova et al., 2012; Li et al., 2011; Yao et al., 2010; Feng and Lapata, 2010a; Feng and Lapata, 2010b; Leong et al., 2010; Mitchell et al., 2012). There is also work in the opposite direction, of finding or generating pictures for a given narration. Joshi et al. (2006) found the best set of images from an image database to match the keywords in a story. Coyne and Sproat (2001) developed a natural language understanding system which converts English text into threedimensional scenes that represent the text. For a high-stakes assessment, it would be highly undesirable to have any noise in the gold-standard reference picture descriptions. Hence we chose to use manual description for creating our reference corpus. 8 Summary and Future Directions We investigated different types of features for automatically scoring a vocabulary item type which requires the test-taker to use two words in writing a sentence based on a picture. We generated a corpus of picture descriptions</context>
</contexts>
<marker>Coyne, Sproat, 2001</marker>
<rawString>Bob Coyne and Richard Sproat. 2001. Wordseye: an automatic text-to-scene conversion system. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 487–496. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Correcting semantic collocation errors with L1 induced paraphrases.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>107--117</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="22508" citStr="Dahlmeier and Ng, 2011" startWordPosition="3851" endWordPosition="3854"> is used to count the number of pairs that had zero entries in the database. This feature is an indicator that the given words or word collocations were not found even once in the database. Given the size of the underlying database, this usually happens in cases when words are misspelled, or when the words never occur together. All features created for bigrams are also created for trigrams. We thus have a total of 40 features, called the pmi feature set. 4.3.2 Features from essay scoring A number of measures of collocation quality have been proposed and implemented (e.g. (Futagi et al., 2008; Dahlmeier and Ng, 2011)). We use erater’s measure of the density of ‘good’ collocations found in the response. Another source of difficulty for non-native writers is the selection of appropriate prepositions. We use the mean probability assigned by e-rater to the prepositions in the response. These two measures, one for the quality of collocations and the other for the quality of prepositions, are combined in our colprep feature set. 4.4 Scoring Rubric-based Features As seen in Section 2, some of the criteria for scoring are quite straightforward (e.g. “omits one or both key words”). While these are not sole determi</context>
</contexts>
<marker>Dahlmeier, Ng, 2011</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2011. Correcting semantic collocation errors with L1 induced paraphrases. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 107–117, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>George Narroway</author>
</authors>
<title>A framework for evaluating text correction.</title>
<date>2012</date>
<booktitle>In LREC,</booktitle>
<pages>3015--3018</pages>
<contexts>
<context position="31720" citStr="Dale and Narroway, 2012" startWordPosition="5404" endWordPosition="5407">mance, indicating that they can be synergistically combined to improve system performance. Feature Set Accuracy in % (i) grammar + colprep 70.31 (ii) colprep + pmi 67.42 (iii) pmi + relevance 69.05 (iv) pmi + relevance + rubric 73.21 Table 5: System performance for feature combinations (i) typically used in essay scoring, (ii) that measure awkwardness, (iii) newly proposed here, (iv) newly proposed plus rubric-specific criteria 7 Related Work Most work in automated scoring and learner language analysis has focused on detecting grammar and usage errors (Leacock et al., 2014; Dale et al., 2012; Dale and Narroway, 2012; Gamon, 2010; Chodorow et al., 2007; Lu, 2010). This is done either by means of handcrafted rules or with statistical classifiers using a variety of information. In the case of the latter, the emphasis has been on representing the contexts of function words, such as articles and prepositions. This work is relevant inasmuch as errors in using content words, such as nouns and verbs, are often reflected in the functional elements which accompany them, for example, articles that indicate the definiteness or countability of nouns, and prepositions that mark the cases of the arguments of verbs. Pre</context>
</contexts>
<marker>Dale, Narroway, 2012</marker>
<rawString>Robert Dale and George Narroway. 2012. A framework for evaluating text correction. In LREC, pages 3015–3018.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ilya Anisimoff</author>
<author>George Narroway</author>
</authors>
<title>HOO 2012: A report on the preposition and determiner error correction shared task.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>54--62</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1843" citStr="Dale et al., 2012" startWordPosition="289" endWordPosition="292"> often said that the best way to see if a person knows the meaning of a word is to have that person use the word in a sentence. Despite this widespread view, most vocabulary testing continues to rely on multiple choice items (e.g. (Lawless et al., 2012; Lawrence et al., 2012)). In fact, few assessments use constructed sentence responses to measure vocabulary knowledge, in part because of the considerable time and cost required to score such responses manually. While much progress has been made in automatically scoring writing quality in essays (Attali and Burstein, 2006; Leacock et al., 2014; Dale et al., 2012), the essay scoring engines do not measure proficiency in the use of specific words, except perhaps for some frequently confused homophones (e.g., its/it’s, there/their/they’re, affect/effect). In this paper we present a system for automated scoring of targeted vocabulary knowledge based on short constructed responses in a picture description task. Specifically, we develop a system for scoring a vocabulary item type that is in operational use in English proficiency tests for nonnative speakers. Each task prompt in this item type consists of two target key words, for which the vocabulary profic</context>
<context position="31695" citStr="Dale et al., 2012" startWordPosition="5400" endWordPosition="5403">eature set’s performance, indicating that they can be synergistically combined to improve system performance. Feature Set Accuracy in % (i) grammar + colprep 70.31 (ii) colprep + pmi 67.42 (iii) pmi + relevance 69.05 (iv) pmi + relevance + rubric 73.21 Table 5: System performance for feature combinations (i) typically used in essay scoring, (ii) that measure awkwardness, (iii) newly proposed here, (iv) newly proposed plus rubric-specific criteria 7 Related Work Most work in automated scoring and learner language analysis has focused on detecting grammar and usage errors (Leacock et al., 2014; Dale et al., 2012; Dale and Narroway, 2012; Gamon, 2010; Chodorow et al., 2007; Lu, 2010). This is done either by means of handcrafted rules or with statistical classifiers using a variety of information. In the case of the latter, the emphasis has been on representing the contexts of function words, such as articles and prepositions. This work is relevant inasmuch as errors in using content words, such as nouns and verbs, are often reflected in the functional elements which accompany them, for example, articles that indicate the definiteness or countability of nouns, and prepositions that mark the cases of th</context>
</contexts>
<marker>Dale, Anisimoff, Narroway, 2012</marker>
<rawString>Robert Dale, Ilya Anisimoff, and George Narroway. 2012. HOO 2012: A report on the preposition and determiner error correction shared task. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 54–62. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rod Ellis</author>
</authors>
<title>Task-based research and language pedagogy. Language teaching research,</title>
<date>2000</date>
<volume>4</volume>
<issue>3</issue>
<pages>220</pages>
<contexts>
<context position="2945" citStr="Ellis, 2000" startWordPosition="467" endWordPosition="469">peakers. Each task prompt in this item type consists of two target key words, for which the vocabulary proficiency is tested, and a picture that provides the context for the sentence construction. The task is to generate a single sentence, incorporating both key words, consistent with the picture. Presumably, a test-taker with competent knowledge of the key words will be able to use them in a well-formed grammatical sentence in the context of the picture. Picture description tasks have been employed in a number of areas of study ranging from second language acquisition to Alzheimer’s disease (Ellis, 2000; Forbes-McKay and Venneri, 2005). Pictures and picture-based story narration have also been used to study referring expressions (Lee et al., 2012) and to analyze child narratives in order to predict language impairment (Hassanali et al., 2013). Evanini et al. (2014) employ a series of pictures and elicit (oral) story narration to test English language proficiency. In our task, the picture is used as a constraining factor to limit the type and content of sentences that can be generated using the given key words. In the course of developing our system, we examined existing features that have be</context>
</contexts>
<marker>Ellis, 2000</marker>
<rawString>Rod Ellis. 2000. Task-based research and language pedagogy. Language teaching research, 4(3):193– 220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keelan Evanini</author>
<author>Michael Heilman</author>
<author>Xinhao Wang</author>
<author>Daniel Blanchard</author>
</authors>
<title>Automated scoring for TOEFL Junior comprehensive writing and speaking.</title>
<date>2014</date>
<tech>Technical report,</tech>
<location>ETS, Princeton, NJ.</location>
<contexts>
<context position="3212" citStr="Evanini et al. (2014)" startWordPosition="507" endWordPosition="510">h key words, consistent with the picture. Presumably, a test-taker with competent knowledge of the key words will be able to use them in a well-formed grammatical sentence in the context of the picture. Picture description tasks have been employed in a number of areas of study ranging from second language acquisition to Alzheimer’s disease (Ellis, 2000; Forbes-McKay and Venneri, 2005). Pictures and picture-based story narration have also been used to study referring expressions (Lee et al., 2012) and to analyze child narratives in order to predict language impairment (Hassanali et al., 2013). Evanini et al. (2014) employ a series of pictures and elicit (oral) story narration to test English language proficiency. In our task, the picture is used as a constraining factor to limit the type and content of sentences that can be generated using the given key words. In the course of developing our system, we examined existing features that have been developed for essay scoring, such as detectors of errors in grammar, usage and mechanics, as well as collocation features, to see if they can be re-used for scoring short responses. We also developed new features for assessing the quality of sentence construction </context>
</contexts>
<marker>Evanini, Heilman, Wang, Blanchard, 2014</marker>
<rawString>Keelan Evanini, Michael Heilman, Xinhao Wang, and Daniel Blanchard. 2014. Automated scoring for TOEFL Junior comprehensive writing and speaking. Technical report, ETS, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>How many words is a picture worth? Automatic caption generation for news images.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1239--1249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="35024" citStr="Feng and Lapata, 2010" startWordPosition="5944" endWordPosition="5947"> the public. However, crowd sourcing has been used by other researchers to collect human labels for images and videos. For example, Rashtchian et al. (2010) use Amazon Mechanical Turk and Von Ahn and Dabbish (2004) create games to entice players to correctly label images. Chen and Dolan (2011) use crowd sourcing to collect multiple paraphrased descriptions of videos to create a paraphrasing corpus. In a vast body of related work, automated methods have been explored for the generation of descriptions of images (Kulkarni et al., 2013; Kuznetsova et al., 2012; Li et al., 2011; Yao et al., 2010; Feng and Lapata, 2010a; Feng and Lapata, 2010b; Leong et al., 2010; Mitchell et al., 2012). There is also work in the opposite direction, of finding or generating pictures for a given narration. Joshi et al. (2006) found the best set of images from an image database to match the keywords in a story. Coyne and Sproat (2001) developed a natural language understanding system which converts English text into threedimensional scenes that represent the text. For a high-stakes assessment, it would be highly undesirable to have any noise in the gold-standard reference picture descriptions. Hence we chose to use manual des</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010a. How many words is a picture worth? Automatic caption generation for news images. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1239–1249, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Topic models for image annotation and text illustration.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>831--839</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="35024" citStr="Feng and Lapata, 2010" startWordPosition="5944" endWordPosition="5947"> the public. However, crowd sourcing has been used by other researchers to collect human labels for images and videos. For example, Rashtchian et al. (2010) use Amazon Mechanical Turk and Von Ahn and Dabbish (2004) create games to entice players to correctly label images. Chen and Dolan (2011) use crowd sourcing to collect multiple paraphrased descriptions of videos to create a paraphrasing corpus. In a vast body of related work, automated methods have been explored for the generation of descriptions of images (Kulkarni et al., 2013; Kuznetsova et al., 2012; Li et al., 2011; Yao et al., 2010; Feng and Lapata, 2010a; Feng and Lapata, 2010b; Leong et al., 2010; Mitchell et al., 2012). There is also work in the opposite direction, of finding or generating pictures for a given narration. Joshi et al. (2006) found the best set of images from an image database to match the keywords in a story. Coyne and Sproat (2001) developed a natural language understanding system which converts English text into threedimensional scenes that represent the text. For a high-stakes assessment, it would be highly undesirable to have any noise in the gold-standard reference picture descriptions. Hence we chose to use manual des</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010b. Topic models for image annotation and text illustration. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 831–839, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Flor</author>
</authors>
<title>A fast and flexible architecture for very large word n-gram datasets.</title>
<date>2013</date>
<journal>Natural Language Engineering,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="19650" citStr="Flor, 2013" startWordPosition="3305" endWordPosition="3306">erized by inappropriate combinations of words, indicating the writer’s lack of knowledge of collocations. For example, “recording for the payment” might be better expressed as “entering the price in the cash register”. As “recording for the payment” is an inappropriate construction, it is not likely to be common, for example, in a large web corpus. We use this intuition in constructing our PMI-based features. We find the PMI of all adjacent word pairs (bigrams), as well as all adjacent word triples (trigrams) in the Google 1T web corpus (Brants and Franz, 2006) using the TrendStream database (Flor, 2013). PMI between word pairs (bigram AB) is defined as: p(AB) loge p(A).p(B) and between word triples (trigram ABC) as p(ABC) loge p(A).p(B).p(C) The higher the value of the PMI, the more common is the collocation for the word pair/triple in well formed texts. On the other hand, negative values of PMI indicate that the given word pair (or triple) is less likely than chance to occur together. We hypothesized that this would be a good indicator of awkward usage, as suggested in (Chodorow and Leacock, 2000). The PMI values for adjacent words obtained over the entire response are then assigned to bins</context>
</contexts>
<marker>Flor, 2013</marker>
<rawString>Michael Flor. 2013. A fast and flexible architecture for very large word n-gram datasets. Natural Language Engineering, 19(1):61–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>KE Forbes-McKay</author>
<author>Annalena Venneri</author>
</authors>
<title>Detecting subtle spontaneous language decline in early Alzheimers disease with a picture description task. Neurological sciences,</title>
<date>2005</date>
<pages>26--4</pages>
<contexts>
<context position="2978" citStr="Forbes-McKay and Venneri, 2005" startWordPosition="470" endWordPosition="473"> task prompt in this item type consists of two target key words, for which the vocabulary proficiency is tested, and a picture that provides the context for the sentence construction. The task is to generate a single sentence, incorporating both key words, consistent with the picture. Presumably, a test-taker with competent knowledge of the key words will be able to use them in a well-formed grammatical sentence in the context of the picture. Picture description tasks have been employed in a number of areas of study ranging from second language acquisition to Alzheimer’s disease (Ellis, 2000; Forbes-McKay and Venneri, 2005). Pictures and picture-based story narration have also been used to study referring expressions (Lee et al., 2012) and to analyze child narratives in order to predict language impairment (Hassanali et al., 2013). Evanini et al. (2014) employ a series of pictures and elicit (oral) story narration to test English language proficiency. In our task, the picture is used as a constraining factor to limit the type and content of sentences that can be generated using the given key words. In the course of developing our system, we examined existing features that have been developed for essay scoring, s</context>
</contexts>
<marker>Forbes-McKay, Venneri, 2005</marker>
<rawString>KE Forbes-McKay and Annalena Venneri. 2005. Detecting subtle spontaneous language decline in early Alzheimers disease with a picture description task. Neurological sciences, 26(4):243–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoko Futagi</author>
<author>Paul Deane</author>
<author>Martin Chodorow</author>
<author>Joel Tetreault</author>
</authors>
<title>A computational approach to detecting collocation errors in the writing of non-native speakers of English.</title>
<date>2008</date>
<journal>Computer Assisted Language Learning,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="22483" citStr="Futagi et al., 2008" startWordPosition="3847" endWordPosition="3850">ly a null-PMI feature is used to count the number of pairs that had zero entries in the database. This feature is an indicator that the given words or word collocations were not found even once in the database. Given the size of the underlying database, this usually happens in cases when words are misspelled, or when the words never occur together. All features created for bigrams are also created for trigrams. We thus have a total of 40 features, called the pmi feature set. 4.3.2 Features from essay scoring A number of measures of collocation quality have been proposed and implemented (e.g. (Futagi et al., 2008; Dahlmeier and Ng, 2011)). We use erater’s measure of the density of ‘good’ collocations found in the response. Another source of difficulty for non-native writers is the selection of appropriate prepositions. We use the mean probability assigned by e-rater to the prepositions in the response. These two measures, one for the quality of collocations and the other for the quality of prepositions, are combined in our colprep feature set. 4.4 Scoring Rubric-based Features As seen in Section 2, some of the criteria for scoring are quite straightforward (e.g. “omits one or both key words”). While t</context>
</contexts>
<marker>Futagi, Deane, Chodorow, Tetreault, 2008</marker>
<rawString>Yoko Futagi, Paul Deane, Martin Chodorow, and Joel Tetreault. 2008. A computational approach to detecting collocation errors in the writing of non-native speakers of English. Computer Assisted Language Learning, 21(4):353–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
</authors>
<title>Using mostly native data to correct errors in learners’ writing: A meta-classifier approach.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>163--171</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31733" citStr="Gamon, 2010" startWordPosition="5408" endWordPosition="5409">ey can be synergistically combined to improve system performance. Feature Set Accuracy in % (i) grammar + colprep 70.31 (ii) colprep + pmi 67.42 (iii) pmi + relevance 69.05 (iv) pmi + relevance + rubric 73.21 Table 5: System performance for feature combinations (i) typically used in essay scoring, (ii) that measure awkwardness, (iii) newly proposed here, (iv) newly proposed plus rubric-specific criteria 7 Related Work Most work in automated scoring and learner language analysis has focused on detecting grammar and usage errors (Leacock et al., 2014; Dale et al., 2012; Dale and Narroway, 2012; Gamon, 2010; Chodorow et al., 2007; Lu, 2010). This is done either by means of handcrafted rules or with statistical classifiers using a variety of information. In the case of the latter, the emphasis has been on representing the contexts of function words, such as articles and prepositions. This work is relevant inasmuch as errors in using content words, such as nouns and verbs, are often reflected in the functional elements which accompany them, for example, articles that indicate the definiteness or countability of nouns, and prepositions that mark the cases of the arguments of verbs. Previous work (B</context>
</contexts>
<marker>Gamon, 2010</marker>
<rawString>Michael Gamon. 2010. Using mostly native data to correct errors in learners’ writing: A meta-classifier approach. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 163–171. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khairun-nisa Hassanali</author>
<author>Yang Liu</author>
<author>Thamar Solorio</author>
</authors>
<title>Using Latent Dirichlet Allocation for child narrative analysis.</title>
<date>2013</date>
<booktitle>ACL 2013,</booktitle>
<pages>111</pages>
<contexts>
<context position="3189" citStr="Hassanali et al., 2013" startWordPosition="503" endWordPosition="506">ntence, incorporating both key words, consistent with the picture. Presumably, a test-taker with competent knowledge of the key words will be able to use them in a well-formed grammatical sentence in the context of the picture. Picture description tasks have been employed in a number of areas of study ranging from second language acquisition to Alzheimer’s disease (Ellis, 2000; Forbes-McKay and Venneri, 2005). Pictures and picture-based story narration have also been used to study referring expressions (Lee et al., 2012) and to analyze child narratives in order to predict language impairment (Hassanali et al., 2013). Evanini et al. (2014) employ a series of pictures and elicit (oral) story narration to test English language proficiency. In our task, the picture is used as a constraining factor to limit the type and content of sentences that can be generated using the given key words. In the course of developing our system, we examined existing features that have been developed for essay scoring, such as detectors of errors in grammar, usage and mechanics, as well as collocation features, to see if they can be re-used for scoring short responses. We also developed new features for assessing the quality of</context>
</contexts>
<marker>Hassanali, Liu, Solorio, 2013</marker>
<rawString>Khairun-nisa Hassanali, Yang Liu, and Thamar Solorio. 2013. Using Latent Dirichlet Allocation for child narrative analysis. ACL 2013, page 111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dhiraj Joshi</author>
<author>James Z Wang</author>
<author>Jia Li</author>
</authors>
<title>The story picturing engine—a system for automatic text illustration.</title>
<date>2006</date>
<journal>ACM Trans. Multimedia Comput. Commun. Appl.,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="35217" citStr="Joshi et al. (2006)" startWordPosition="5979" endWordPosition="5982"> and Dabbish (2004) create games to entice players to correctly label images. Chen and Dolan (2011) use crowd sourcing to collect multiple paraphrased descriptions of videos to create a paraphrasing corpus. In a vast body of related work, automated methods have been explored for the generation of descriptions of images (Kulkarni et al., 2013; Kuznetsova et al., 2012; Li et al., 2011; Yao et al., 2010; Feng and Lapata, 2010a; Feng and Lapata, 2010b; Leong et al., 2010; Mitchell et al., 2012). There is also work in the opposite direction, of finding or generating pictures for a given narration. Joshi et al. (2006) found the best set of images from an image database to match the keywords in a story. Coyne and Sproat (2001) developed a natural language understanding system which converts English text into threedimensional scenes that represent the text. For a high-stakes assessment, it would be highly undesirable to have any noise in the gold-standard reference picture descriptions. Hence we chose to use manual description for creating our reference corpus. 8 Summary and Future Directions We investigated different types of features for automatically scoring a vocabulary item type which requires the test-</context>
</contexts>
<marker>Joshi, Wang, Li, 2006</marker>
<rawString>Dhiraj Joshi, James Z. Wang, and Jia Li. 2006. The story picturing engine—a system for automatic text illustration. ACM Trans. Multimedia Comput. Commun. Appl., 2(1):68–89, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Levi King</author>
<author>Markus Dickinson</author>
</authors>
<title>Shallow semantic analysis of interactive learner sentences.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>11--21</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="33754" citStr="King and Dickinson (2013)" startWordPosition="5738" endWordPosition="5741">lary. Recent work on assessment measures of depth of vocabulary knowledge (Lawless et al., 2012; Lawrence et al., 2012), has argued that knowledge of specific words can range from superficial (idiomatic associations built up through word cooccurrence) to topical (meaning-related associations between words) to deep (definitional knowledge). Some of our features (e.g. awkward word usage) capture some of this information (e.g., idiomatic associations between words), but assigning the depth of knowledge of the key words is not the focus of our task. Work that is closely related to ours is that of King and Dickinson (2013). They parse picture descriptions from interactive learner sentences, classify sentences into syntactic types and extract the logical subject, verb and object in order to recover simple semantic representations of the descriptions. We do not explicitly model the semantic representations of the pictures, but rather our goal in this work is to ascertain if a response is relevant to the picture and to measure other factors that reflect vocabulary proficiency. We employ human annotators and use word similarity measures to obtain alternative forms of description because the proprietary nature of ou</context>
</contexts>
<marker>King, Dickinson, 2013</marker>
<rawString>Levi King and Markus Dickinson. 2013. Shallow semantic analysis of interactive learner sentences. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 11–21, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Vicente Ordonez</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Babytalk: Understanding and generating simple image descriptions.</title>
<date>2013</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>99--1</pages>
<contexts>
<context position="34941" citStr="Kulkarni et al., 2013" startWordPosition="5928" endWordPosition="5931">cause the proprietary nature of our data prevents us from releasing our pictures to the public. However, crowd sourcing has been used by other researchers to collect human labels for images and videos. For example, Rashtchian et al. (2010) use Amazon Mechanical Turk and Von Ahn and Dabbish (2004) create games to entice players to correctly label images. Chen and Dolan (2011) use crowd sourcing to collect multiple paraphrased descriptions of videos to create a paraphrasing corpus. In a vast body of related work, automated methods have been explored for the generation of descriptions of images (Kulkarni et al., 2013; Kuznetsova et al., 2012; Li et al., 2011; Yao et al., 2010; Feng and Lapata, 2010a; Feng and Lapata, 2010b; Leong et al., 2010; Mitchell et al., 2012). There is also work in the opposite direction, of finding or generating pictures for a given narration. Joshi et al. (2006) found the best set of images from an image database to match the keywords in a story. Coyne and Sproat (2001) developed a natural language understanding system which converts English text into threedimensional scenes that represent the text. For a high-stakes assessment, it would be highly undesirable to have any noise in</context>
</contexts>
<marker>Kulkarni, Premraj, Ordonez, Dhar, Li, Choi, Berg, Berg, 2013</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2013. Babytalk: Understanding and generating simple image descriptions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 99(PrePrints):1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Polina Kuznetsova</author>
<author>Vicente Ordonez</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Collective generation of natural image descriptions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>359--368</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="34966" citStr="Kuznetsova et al., 2012" startWordPosition="5932" endWordPosition="5935">ature of our data prevents us from releasing our pictures to the public. However, crowd sourcing has been used by other researchers to collect human labels for images and videos. For example, Rashtchian et al. (2010) use Amazon Mechanical Turk and Von Ahn and Dabbish (2004) create games to entice players to correctly label images. Chen and Dolan (2011) use crowd sourcing to collect multiple paraphrased descriptions of videos to create a paraphrasing corpus. In a vast body of related work, automated methods have been explored for the generation of descriptions of images (Kulkarni et al., 2013; Kuznetsova et al., 2012; Li et al., 2011; Yao et al., 2010; Feng and Lapata, 2010a; Feng and Lapata, 2010b; Leong et al., 2010; Mitchell et al., 2012). There is also work in the opposite direction, of finding or generating pictures for a given narration. Joshi et al. (2006) found the best set of images from an image database to match the keywords in a story. Coyne and Sproat (2001) developed a natural language understanding system which converts English text into threedimensional scenes that represent the text. For a high-stakes assessment, it would be highly undesirable to have any noise in the gold-standard refere</context>
</contexts>
<marker>Kuznetsova, Ordonez, Berg, Berg, Choi, 2012</marker>
<rawString>Polina Kuznetsova, Vicente Ordonez, Alexander C Berg, Tamara L Berg, and Yejin Choi. 2012. Collective generation of natural image descriptions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 359–368. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ren´e Lawless</author>
<author>John Sabatini</author>
<author>Paul Deane</author>
</authors>
<title>Approaches to assessing partial vocabulary knowledge and supporting word learning: Assessing vocabulary depth.</title>
<date>2012</date>
<booktitle>In Annual Meeting of the American Educational Research Association,</booktitle>
<location>Vancouver, CA.</location>
<contexts>
<context position="1477" citStr="Lawless et al., 2012" startWordPosition="230" endWordPosition="234"> to short answers, as in our task. We also explore new features reflecting the quality of the collocations in the response, as well as features measuring the consistency of the response to the picture. System accuracy in scoring is 15 percentage points greater than the majority class baseline and 10 percentage points less than human performance. 1 Introduction It is often said that the best way to see if a person knows the meaning of a word is to have that person use the word in a sentence. Despite this widespread view, most vocabulary testing continues to rely on multiple choice items (e.g. (Lawless et al., 2012; Lawrence et al., 2012)). In fact, few assessments use constructed sentence responses to measure vocabulary knowledge, in part because of the considerable time and cost required to score such responses manually. While much progress has been made in automatically scoring writing quality in essays (Attali and Burstein, 2006; Leacock et al., 2014; Dale et al., 2012), the essay scoring engines do not measure proficiency in the use of specific words, except perhaps for some frequently confused homophones (e.g., its/it’s, there/their/they’re, affect/effect). In this paper we present a system for au</context>
<context position="33224" citStr="Lawless et al., 2012" startWordPosition="5651" endWordPosition="5654">and t have a long history of use for detecting collocations and measuring their quality (see (Manning and Sch¨utze, 1999) and (Leacock et al., 2014) for reviews). Our application of a large n-gram database and PMI is to detect inappropriate word usage. Our task also differs from work focusing on evaluating content (e.g. (Meurers et al., 2011; Sukkarieh and Blackmore, 2009; Leacock and Chodorow, 2003)) in that, although we are looking for usage of certain content words, we focus primarily on measuring knowledge of vocabulary. Recent work on assessment measures of depth of vocabulary knowledge (Lawless et al., 2012; Lawrence et al., 2012), has argued that knowledge of specific words can range from superficial (idiomatic associations built up through word cooccurrence) to topical (meaning-related associations between words) to deep (definitional knowledge). Some of our features (e.g. awkward word usage) capture some of this information (e.g., idiomatic associations between words), but assigning the depth of knowledge of the key words is not the focus of our task. Work that is closely related to ours is that of King and Dickinson (2013). They parse picture descriptions from interactive learner sentences, </context>
</contexts>
<marker>Lawless, Sabatini, Deane, 2012</marker>
<rawString>Ren´e Lawless, John Sabatini, and Paul Deane. 2012. Approaches to assessing partial vocabulary knowledge and supporting word learning: Assessing vocabulary depth. In Annual Meeting of the American Educational Research Association, April 13-17, 2012, Vancouver, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Lawrence</author>
<author>Elizabeth Pare-Blagoev</author>
<author>Ren´e Lawless</author>
<author>Chen Deane</author>
<author>Paul andLi</author>
</authors>
<title>General vocabulary, academic vocabulary, and vocabulary depth: Examiningpredictors of adolescent reading comprehension.</title>
<date>2012</date>
<booktitle>In Annual Meeting of the American Educational Research Association.</booktitle>
<contexts>
<context position="1501" citStr="Lawrence et al., 2012" startWordPosition="235" endWordPosition="238">in our task. We also explore new features reflecting the quality of the collocations in the response, as well as features measuring the consistency of the response to the picture. System accuracy in scoring is 15 percentage points greater than the majority class baseline and 10 percentage points less than human performance. 1 Introduction It is often said that the best way to see if a person knows the meaning of a word is to have that person use the word in a sentence. Despite this widespread view, most vocabulary testing continues to rely on multiple choice items (e.g. (Lawless et al., 2012; Lawrence et al., 2012)). In fact, few assessments use constructed sentence responses to measure vocabulary knowledge, in part because of the considerable time and cost required to score such responses manually. While much progress has been made in automatically scoring writing quality in essays (Attali and Burstein, 2006; Leacock et al., 2014; Dale et al., 2012), the essay scoring engines do not measure proficiency in the use of specific words, except perhaps for some frequently confused homophones (e.g., its/it’s, there/their/they’re, affect/effect). In this paper we present a system for automated scoring of targe</context>
<context position="33248" citStr="Lawrence et al., 2012" startWordPosition="5655" endWordPosition="5658">ory of use for detecting collocations and measuring their quality (see (Manning and Sch¨utze, 1999) and (Leacock et al., 2014) for reviews). Our application of a large n-gram database and PMI is to detect inappropriate word usage. Our task also differs from work focusing on evaluating content (e.g. (Meurers et al., 2011; Sukkarieh and Blackmore, 2009; Leacock and Chodorow, 2003)) in that, although we are looking for usage of certain content words, we focus primarily on measuring knowledge of vocabulary. Recent work on assessment measures of depth of vocabulary knowledge (Lawless et al., 2012; Lawrence et al., 2012), has argued that knowledge of specific words can range from superficial (idiomatic associations built up through word cooccurrence) to topical (meaning-related associations between words) to deep (definitional knowledge). Some of our features (e.g. awkward word usage) capture some of this information (e.g., idiomatic associations between words), but assigning the depth of knowledge of the key words is not the focus of our task. Work that is closely related to ours is that of King and Dickinson (2013). They parse picture descriptions from interactive learner sentences, classify sentences into </context>
</contexts>
<marker>Lawrence, Pare-Blagoev, Lawless, Deane, andLi, 2012</marker>
<rawString>Joshua Lawrence, Elizabeth Pare-Blagoev, Ren´e Lawless, and Chen Deane, Paul andLi. 2012. General vocabulary, academic vocabulary, and vocabulary depth: Examiningpredictors of adolescent reading comprehension. In Annual Meeting of the American Educational Research Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>C-rater: Automated scoring of short-answer questions.</title>
<date>2003</date>
<journal>Computers and the Humanities,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="33007" citStr="Leacock and Chodorow, 2003" startWordPosition="5616" endWordPosition="5619">t al., 2011) has shown that mod8 els which rely on large web-scale n-gram counts can be effective for the task of context-sensitive spelling correction. Measures of ngram association such as PMI, log likelihood, chi-square, and t have a long history of use for detecting collocations and measuring their quality (see (Manning and Sch¨utze, 1999) and (Leacock et al., 2014) for reviews). Our application of a large n-gram database and PMI is to detect inappropriate word usage. Our task also differs from work focusing on evaluating content (e.g. (Meurers et al., 2011; Sukkarieh and Blackmore, 2009; Leacock and Chodorow, 2003)) in that, although we are looking for usage of certain content words, we focus primarily on measuring knowledge of vocabulary. Recent work on assessment measures of depth of vocabulary knowledge (Lawless et al., 2012; Lawrence et al., 2012), has argued that knowledge of specific words can range from superficial (idiomatic associations built up through word cooccurrence) to topical (meaning-related associations between words) to deep (definitional knowledge). Some of our features (e.g. awkward word usage) capture some of this information (e.g., idiomatic associations between words), but assign</context>
</contexts>
<marker>Leacock, Chodorow, 2003</marker>
<rawString>Claudia Leacock and Martin Chodorow. 2003. C-rater: Automated scoring of short-answer questions. Computers and the Humanities, 37(4):389–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
<author>Michael Gamon</author>
<author>Joel Tetreault</author>
</authors>
<title>Automated Grammatical Error Detection for Language Learners. Synthesis Lectures on Human Language Technologies.</title>
<date>2014</date>
<publisher>Morgan &amp; Claypool.</publisher>
<contexts>
<context position="1823" citStr="Leacock et al., 2014" startWordPosition="285" endWordPosition="288">. 1 Introduction It is often said that the best way to see if a person knows the meaning of a word is to have that person use the word in a sentence. Despite this widespread view, most vocabulary testing continues to rely on multiple choice items (e.g. (Lawless et al., 2012; Lawrence et al., 2012)). In fact, few assessments use constructed sentence responses to measure vocabulary knowledge, in part because of the considerable time and cost required to score such responses manually. While much progress has been made in automatically scoring writing quality in essays (Attali and Burstein, 2006; Leacock et al., 2014; Dale et al., 2012), the essay scoring engines do not measure proficiency in the use of specific words, except perhaps for some frequently confused homophones (e.g., its/it’s, there/their/they’re, affect/effect). In this paper we present a system for automated scoring of targeted vocabulary knowledge based on short constructed responses in a picture description task. Specifically, we develop a system for scoring a vocabulary item type that is in operational use in English proficiency tests for nonnative speakers. Each task prompt in this item type consists of two target key words, for which t</context>
<context position="31676" citStr="Leacock et al., 2014" startWordPosition="5396" endWordPosition="5399">than each individual feature set’s performance, indicating that they can be synergistically combined to improve system performance. Feature Set Accuracy in % (i) grammar + colprep 70.31 (ii) colprep + pmi 67.42 (iii) pmi + relevance 69.05 (iv) pmi + relevance + rubric 73.21 Table 5: System performance for feature combinations (i) typically used in essay scoring, (ii) that measure awkwardness, (iii) newly proposed here, (iv) newly proposed plus rubric-specific criteria 7 Related Work Most work in automated scoring and learner language analysis has focused on detecting grammar and usage errors (Leacock et al., 2014; Dale et al., 2012; Dale and Narroway, 2012; Gamon, 2010; Chodorow et al., 2007; Lu, 2010). This is done either by means of handcrafted rules or with statistical classifiers using a variety of information. In the case of the latter, the emphasis has been on representing the contexts of function words, such as articles and prepositions. This work is relevant inasmuch as errors in using content words, such as nouns and verbs, are often reflected in the functional elements which accompany them, for example, articles that indicate the definiteness or countability of nouns, and prepositions that m</context>
</contexts>
<marker>Leacock, Chodorow, Gamon, Tetreault, 2014</marker>
<rawString>Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2014. Automated Grammatical Error Detection for Language Learners. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Choonkyu Lee</author>
<author>Smaranda Muresan</author>
<author>Karin Stromswold</author>
</authors>
<title>Computational analysis of referring expressions in narratives of picture books.</title>
<date>2012</date>
<booktitle>NAACL-HLT 2012,</booktitle>
<pages>1</pages>
<contexts>
<context position="3092" citStr="Lee et al., 2012" startWordPosition="488" endWordPosition="491">hat provides the context for the sentence construction. The task is to generate a single sentence, incorporating both key words, consistent with the picture. Presumably, a test-taker with competent knowledge of the key words will be able to use them in a well-formed grammatical sentence in the context of the picture. Picture description tasks have been employed in a number of areas of study ranging from second language acquisition to Alzheimer’s disease (Ellis, 2000; Forbes-McKay and Venneri, 2005). Pictures and picture-based story narration have also been used to study referring expressions (Lee et al., 2012) and to analyze child narratives in order to predict language impairment (Hassanali et al., 2013). Evanini et al. (2014) employ a series of pictures and elicit (oral) story narration to test English language proficiency. In our task, the picture is used as a constraining factor to limit the type and content of sentences that can be generated using the given key words. In the course of developing our system, we examined existing features that have been developed for essay scoring, such as detectors of errors in grammar, usage and mechanics, as well as collocation features, to see if they can be</context>
</contexts>
<marker>Lee, Muresan, Stromswold, 2012</marker>
<rawString>Choonkyu Lee, Smaranda Muresan, and Karin Stromswold. 2012. Computational analysis of referring expressions in narratives of picture books. NAACL-HLT 2012, page 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chee Wee Leong</author>
<author>Rada Mihalcea</author>
<author>Samer Hassan</author>
</authors>
<title>Text mining for automatic image tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>647--655</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="35069" citStr="Leong et al., 2010" startWordPosition="5953" endWordPosition="5956">ed by other researchers to collect human labels for images and videos. For example, Rashtchian et al. (2010) use Amazon Mechanical Turk and Von Ahn and Dabbish (2004) create games to entice players to correctly label images. Chen and Dolan (2011) use crowd sourcing to collect multiple paraphrased descriptions of videos to create a paraphrasing corpus. In a vast body of related work, automated methods have been explored for the generation of descriptions of images (Kulkarni et al., 2013; Kuznetsova et al., 2012; Li et al., 2011; Yao et al., 2010; Feng and Lapata, 2010a; Feng and Lapata, 2010b; Leong et al., 2010; Mitchell et al., 2012). There is also work in the opposite direction, of finding or generating pictures for a given narration. Joshi et al. (2006) found the best set of images from an image database to match the keywords in a story. Coyne and Sproat (2001) developed a natural language understanding system which converts English text into threedimensional scenes that represent the text. For a high-stakes assessment, it would be highly undesirable to have any noise in the gold-standard reference picture descriptions. Hence we chose to use manual description for creating our reference corpus. 8</context>
</contexts>
<marker>Leong, Mihalcea, Hassan, 2010</marker>
<rawString>Chee Wee Leong, Rada Mihalcea, and Samer Hassan. 2010. Text mining for automatic image tagging. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 647–655. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siming Li</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
<author>Alexander C Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Composing simple image descriptions using web-scale n-grams.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>220--228</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="34983" citStr="Li et al., 2011" startWordPosition="5936" endWordPosition="5939">s us from releasing our pictures to the public. However, crowd sourcing has been used by other researchers to collect human labels for images and videos. For example, Rashtchian et al. (2010) use Amazon Mechanical Turk and Von Ahn and Dabbish (2004) create games to entice players to correctly label images. Chen and Dolan (2011) use crowd sourcing to collect multiple paraphrased descriptions of videos to create a paraphrasing corpus. In a vast body of related work, automated methods have been explored for the generation of descriptions of images (Kulkarni et al., 2013; Kuznetsova et al., 2012; Li et al., 2011; Yao et al., 2010; Feng and Lapata, 2010a; Feng and Lapata, 2010b; Leong et al., 2010; Mitchell et al., 2012). There is also work in the opposite direction, of finding or generating pictures for a given narration. Joshi et al. (2006) found the best set of images from an image database to match the keywords in a story. Coyne and Sproat (2001) developed a natural language understanding system which converts English text into threedimensional scenes that represent the text. For a high-stakes assessment, it would be highly undesirable to have any noise in the gold-standard reference picture descr</context>
</contexts>
<marker>Li, Kulkarni, Berg, Berg, Choi, 2011</marker>
<rawString>Siming Li, Girish Kulkarni, Tamara L Berg, Alexander C Berg, and Yejin Choi. 2011. Composing simple image descriptions using web-scale n-grams. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 220– 228. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguisticsVolume 2,</booktitle>
<pages>768--774</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17194" citStr="Lin, 1998" startWordPosition="2896" endWordPosition="2897">esponse and the lemmatized version of the corresponding reference corpus, as follows: |Response ∩ Corpus| |Response| It is not always necessary for the test-taker to use exactly the same words found in the reference corpus. For example, the annotator might have referred to a person in the picture as a “lady”, while a response may refer to the same person as a “woman” or “girl” or even just “person”. Thus, we needed to go beyond simple lexical match. In order to account for synonyms, we expanded the content words in the reference corpus by adding their synonyms, as provided in Lin’s thesaurus (Lin, 1998) and then compared the expanded reference to each response. Along the same lines, we also used expansions from WordNet synonyms, WordNet hypernyms and WordNet hyponyms. The following is the list of our content 4 relevance features. Each measures the proportion of overlap as described by the equation above between the lemmatized response and 1. lemmas: the lemmatized reference corpus. 2. cov-lin: the reference corpus expanded using Lin’s thesaurus. 3. cov-wn-syns: the reference corpus expanded using WordNet Synonyms. 4. cov-wn-hyper: the reference corpus expanded using WordNet Hypernyms. 5. cov</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international conference on Computational linguisticsVolume 2, pages 768–774. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofei Lu</author>
</authors>
<title>Automatic analysis of syntactic complexity in second language writing.</title>
<date>2010</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="31767" citStr="Lu, 2010" startWordPosition="5414" endWordPosition="5415">o improve system performance. Feature Set Accuracy in % (i) grammar + colprep 70.31 (ii) colprep + pmi 67.42 (iii) pmi + relevance 69.05 (iv) pmi + relevance + rubric 73.21 Table 5: System performance for feature combinations (i) typically used in essay scoring, (ii) that measure awkwardness, (iii) newly proposed here, (iv) newly proposed plus rubric-specific criteria 7 Related Work Most work in automated scoring and learner language analysis has focused on detecting grammar and usage errors (Leacock et al., 2014; Dale et al., 2012; Dale and Narroway, 2012; Gamon, 2010; Chodorow et al., 2007; Lu, 2010). This is done either by means of handcrafted rules or with statistical classifiers using a variety of information. In the case of the latter, the emphasis has been on representing the contexts of function words, such as articles and prepositions. This work is relevant inasmuch as errors in using content words, such as nouns and verbs, are often reflected in the functional elements which accompany them, for example, articles that indicate the definiteness or countability of nouns, and prepositions that mark the cases of the arguments of verbs. Previous work (Bergsma et al., 2009; Bergsma et al</context>
</contexts>
<marker>Lu, 2010</marker>
<rawString>Xiaofei Lu. 2010. Automatic analysis of syntactic complexity in second language writing. International Journal of Corpus Linguistics, 15(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detmar Meurers</author>
<author>Ramon Ziai</author>
<author>Niels Ott</author>
<author>Stacey M Bailey</author>
</authors>
<title>Integrating parallel analysis modules to evaluate the meaning of answers to reading comprehension questions.</title>
<date>2011</date>
<booktitle>International Journal of Continuing Engineering Education and Life Long Learning,</booktitle>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="32947" citStr="Meurers et al., 2011" startWordPosition="5608" endWordPosition="5611">ork (Bergsma et al., 2009; Bergsma et al., 2010; Xu et al., 2011) has shown that mod8 els which rely on large web-scale n-gram counts can be effective for the task of context-sensitive spelling correction. Measures of ngram association such as PMI, log likelihood, chi-square, and t have a long history of use for detecting collocations and measuring their quality (see (Manning and Sch¨utze, 1999) and (Leacock et al., 2014) for reviews). Our application of a large n-gram database and PMI is to detect inappropriate word usage. Our task also differs from work focusing on evaluating content (e.g. (Meurers et al., 2011; Sukkarieh and Blackmore, 2009; Leacock and Chodorow, 2003)) in that, although we are looking for usage of certain content words, we focus primarily on measuring knowledge of vocabulary. Recent work on assessment measures of depth of vocabulary knowledge (Lawless et al., 2012; Lawrence et al., 2012), has argued that knowledge of specific words can range from superficial (idiomatic associations built up through word cooccurrence) to topical (meaning-related associations between words) to deep (definitional knowledge). Some of our features (e.g. awkward word usage) capture some of this informat</context>
</contexts>
<marker>Meurers, Ziai, Ott, Bailey, 2011</marker>
<rawString>Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey M Bailey. 2011. Integrating parallel analysis modules to evaluate the meaning of answers to reading comprehension questions. International Journal of Continuing Engineering Education and Life Long Learning, 21(4):355–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Xufeng Han</author>
<author>Jesse Dodge</author>
<author>Alyssa Mensch</author>
<author>Amit Goyal</author>
<author>Alex Berg</author>
<author>Kota Yamaguchi</author>
<author>Tamara Berg</author>
<author>Karl Stratos</author>
<author>Hal Daum´e</author>
</authors>
<title>Midge: Generating image descriptions from computer vision detections.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>747--756</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Mitchell, Han, Dodge, Mensch, Goyal, Berg, Yamaguchi, Berg, Stratos, Daum´e, 2012</marker>
<rawString>Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi, Tamara Berg, Karl Stratos, and Hal Daum´e III. 2012. Midge: Generating image descriptions from computer vision detections. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 747– 756. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyrus Rashtchian</author>
<author>Peter Young</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Collecting image annotations using Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>139--147</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="34559" citStr="Rashtchian et al. (2010)" startWordPosition="5865" endWordPosition="5868">mple semantic representations of the descriptions. We do not explicitly model the semantic representations of the pictures, but rather our goal in this work is to ascertain if a response is relevant to the picture and to measure other factors that reflect vocabulary proficiency. We employ human annotators and use word similarity measures to obtain alternative forms of description because the proprietary nature of our data prevents us from releasing our pictures to the public. However, crowd sourcing has been used by other researchers to collect human labels for images and videos. For example, Rashtchian et al. (2010) use Amazon Mechanical Turk and Von Ahn and Dabbish (2004) create games to entice players to correctly label images. Chen and Dolan (2011) use crowd sourcing to collect multiple paraphrased descriptions of videos to create a paraphrasing corpus. In a vast body of related work, automated methods have been explored for the generation of descriptions of images (Kulkarni et al., 2013; Kuznetsova et al., 2012; Li et al., 2011; Yao et al., 2010; Feng and Lapata, 2010a; Feng and Lapata, 2010b; Leong et al., 2010; Mitchell et al., 2012). There is also work in the opposite direction, of finding or gene</context>
</contexts>
<marker>Rashtchian, Young, Hodosh, Hockenmaier, 2010</marker>
<rawString>Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier. 2010. Collecting image annotations using Amazon’s Mechanical Turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 139–147. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jana Zuheir Sukkarieh</author>
<author>John Blackmore</author>
</authors>
<title>C-rater: Automatic content scoring for short constructed responses.</title>
<date>2009</date>
<booktitle>In FLAIRS Conference.</booktitle>
<contexts>
<context position="32978" citStr="Sukkarieh and Blackmore, 2009" startWordPosition="5612" endWordPosition="5615">009; Bergsma et al., 2010; Xu et al., 2011) has shown that mod8 els which rely on large web-scale n-gram counts can be effective for the task of context-sensitive spelling correction. Measures of ngram association such as PMI, log likelihood, chi-square, and t have a long history of use for detecting collocations and measuring their quality (see (Manning and Sch¨utze, 1999) and (Leacock et al., 2014) for reviews). Our application of a large n-gram database and PMI is to detect inappropriate word usage. Our task also differs from work focusing on evaluating content (e.g. (Meurers et al., 2011; Sukkarieh and Blackmore, 2009; Leacock and Chodorow, 2003)) in that, although we are looking for usage of certain content words, we focus primarily on measuring knowledge of vocabulary. Recent work on assessment measures of depth of vocabulary knowledge (Lawless et al., 2012; Lawrence et al., 2012), has argued that knowledge of specific words can range from superficial (idiomatic associations built up through word cooccurrence) to topical (meaning-related associations between words) to deep (definitional knowledge). Some of our features (e.g. awkward word usage) capture some of this information (e.g., idiomatic associatio</context>
</contexts>
<marker>Sukkarieh, Blackmore, 2009</marker>
<rawString>Jana Zuheir Sukkarieh and John Blackmore. 2009. C-rater: Automatic content scoring for short constructed responses. In FLAIRS Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGCHI conference on Human factors in computing systems,</booktitle>
<pages>319--326</pages>
<publisher>ACM.</publisher>
<marker>Von Ahn, Dabbish, 2004</marker>
<rawString>Luis Von Ahn and Laura Dabbish. 2004. Labeling images with a computer game. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 319–326. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
<author>Ralph Grishman</author>
<author>Le Zhao</author>
</authors>
<title>Exploiting syntactic and distributional information for spelling correction with web-scale n-gram models.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1291--1300</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Xu, Tetreault, Chodorow, Grishman, Le Zhao, 2011</marker>
<rawString>Wei Xu, Joel Tetreault, Martin Chodorow, Ralph Grishman, and Le Zhao. 2011. Exploiting syntactic and distributional information for spelling correction with web-scale n-gram models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1291–1300. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Z Yao</author>
<author>Xiong Yang</author>
<author>Liang Lin</author>
<author>Mun Wai Lee</author>
<author>Song-Chun Zhu</author>
</authors>
<title>I2t: Image parsing to text description.</title>
<date>2010</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>98--8</pages>
<contexts>
<context position="35001" citStr="Yao et al., 2010" startWordPosition="5940" endWordPosition="5943">ng our pictures to the public. However, crowd sourcing has been used by other researchers to collect human labels for images and videos. For example, Rashtchian et al. (2010) use Amazon Mechanical Turk and Von Ahn and Dabbish (2004) create games to entice players to correctly label images. Chen and Dolan (2011) use crowd sourcing to collect multiple paraphrased descriptions of videos to create a paraphrasing corpus. In a vast body of related work, automated methods have been explored for the generation of descriptions of images (Kulkarni et al., 2013; Kuznetsova et al., 2012; Li et al., 2011; Yao et al., 2010; Feng and Lapata, 2010a; Feng and Lapata, 2010b; Leong et al., 2010; Mitchell et al., 2012). There is also work in the opposite direction, of finding or generating pictures for a given narration. Joshi et al. (2006) found the best set of images from an image database to match the keywords in a story. Coyne and Sproat (2001) developed a natural language understanding system which converts English text into threedimensional scenes that represent the text. For a high-stakes assessment, it would be highly undesirable to have any noise in the gold-standard reference picture descriptions. Hence we </context>
</contexts>
<marker>Yao, Yang, Lin, Lee, Zhu, 2010</marker>
<rawString>Benjamin Z Yao, Xiong Yang, Liang Lin, Mun Wai Lee, and Song-Chun Zhu. 2010. I2t: Image parsing to text description. Proceedings of the IEEE, 98(8):1485–1508.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.997212">
Sentence Processing in a Vectorial Model of Working Memory
</title>
<author confidence="0.998322">
William Schuler
</author>
<affiliation confidence="0.99527">
Department of Linguistics
The Ohio State University
</affiliation>
<email confidence="0.99597">
schuler@ling.osu.edu
</email>
<sectionHeader confidence="0.993843" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999921117647059">
This paper presents a vectorial incremen-
tal parsing model defined using indepen-
dently posited operations over activation-
based working memory and weight-based
episodic memory. This model has the at-
tractive property that it hypothesizes only
one unary preterminal rule application and
only one binary branching rule applica-
tion per time step, which allows it to be
smoothly integrated into a vector-based
recurrence that propagates structural am-
biguity from one time step to the next.
Predictions of this model are calculated
on a center-embedded sentence processing
task and shown to exhibit decreased pro-
cessing accuracy in center-embedded con-
structions.
</bodyText>
<sectionHeader confidence="0.998972" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999893403225807">
Current models of memory (Marr, 1971; Ander-
son et al., 1977; Murdock, 1982; McClelland et
al., 1995; Howard and Kahana, 2002) involve a
continuous activation-based (or ‘working’) mem-
ory, typically modeled as a vector representing the
current firing pattern of neurons or neural clus-
ters in the cortex. This activation-based memory
is then supported by a durable but rapidly mu-
table weight-based (or ‘episodic’) memory, typi-
cally modeled as one or more matrices formed by
summed outer-products of cue and target vectors
and cued by simple matrix multiplication, repre-
senting variable synaptic connection strengths be-
tween neurons or neural clusters.
The lack of discrete memory units in such mod-
els makes it difficult to imagine a neural imple-
mentation of a typical e.g. chart-based computa-
tional account of sentence processing. On the
other hand, superposition in vectorial models sug-
gests a natural representation of a parallel incre-
mental processing model. This paper explores
how such an austere model of memory not only
might be used to encode a simple probabilistic in-
cremental parser, but also lends itself to naturally
implement a vectorial interpreter and coreference
resolver. This model is based on the left-corner
parser formulation of van Schijndel et al. (2013a),
which has the attractive property of generating ex-
actly one binary-branching rule application after
processing each word. This property greatly sim-
plifies a vectorial implementation because it al-
lows these single grammar rule applications to be
superposed in cases of attachment ambiguity.
Predictions of the vectorial model described in
this paper are then calculated on a simple center-
embedded sentence processing task, producing a
lower completion accuracy for center-embedded
sentences than for right-branching sentences with
the same number of words. As noted by Levy and
Gibson (2013), this kind of memory effect is not
easily explained by existing information-theoretic
models of frequency effects (Hale, 2001; Levy,
2008).
The model described in this paper also provides
an explanation for the apparent reality of linguistic
objects like categories, grammar rules, discourse
referents and dependency relations, as cognitive
states in activation-based memory (in the case of
categories and discourse referents), or cued asso-
ciations in weight-based memory (in the case of
grammar rules, and dependency relations), with-
out having to posit complex machinery specific
to language processing. In this sense, unlike ex-
isting chart-based parsers or connectionist models
based on recurrent neural networks, this model in-
tegrates familiar notions of grammar and seman-
tic relations with current ideas of activation-based
and weight-based memory. It is also anticipated
that this interface to both linguistic and neurosci-
entific theories will make the model useful as a
basis for more nuanced understanding of linguistic
phenomena such as ambiguity resolution, seman-
</bodyText>
<page confidence="0.992219">
19
</page>
<note confidence="0.8099835">
Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 19–27,
Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.716969">
tic representation, and language acquisition. S
</bodyText>
<sectionHeader confidence="0.995897" genericHeader="related work">
2 Related Work VP
</sectionHeader>
<bodyText confidence="0.999930625">
The model described in this paper is based on the NP
left-corner parser formulation of van Schijndel et N
al. (2013a), which is an implementation of a fully
parallel incremental parser. This parser differs
from chart-based fully parallel incremental parsers
used by Hale (2001), Levy (2008) and others in
that it enforces a cognitively-motivated bound on
center-embedding depth. This bound allows the
parser to represent a tractable set of incremental
hypotheses in an explicitly enumerated list as a
factored hidden Markov model, without necessi-
tating the use of a parser chart. This model has the
attractive property that, in any context, it hypoth-
esizes exactly one binary-branching rule applica-
tion at each time step.
The model described in this paper extends the
van Schijndel et al. (2013a) parser by maintain-
ing possible store configurations as superposed
sequence states in a finite-dimensional state vec-
tor. The model then exploits the uniformity of
its parsing operations to integrate probabilistically
weighted grammar rule applications into this su-
perposed state vector. These superposed states
are then used to cue more superordinate sequen-
tial states as ‘continuations’ whenever subordinate
states conclude. Interference in this cueing pro-
cess is then observed to produce a natural center-
embedding limit.
This model is defined as a recurrence over an
activation vector, similar to the simple recurrent
network of Elman (1991) and others, but unlike an
SRN, which does not encode anything in weight-
based memory during processing, this model en-
codes updates to a processing hierarchy in weight-
based memory at every time step. The model is
also similar to the ACT-R parser of Lewis and Va-
sishth (2005) in that it maintains a single state
which is updated based on content-based cued
association, but unlike the ACT-R parser, which
cues category tokens on category types and there-
fore models memory limits as interference among
grammar rules, this model cues category tokens
on other category tokens, and therefore predicts
memory limits even in cases where grammar rules
do not involve similar category types. Also unlike
Lewis and Vasishth (2005), this model is defined
purely in terms of state vectors and outer-product
associative memory and therefore has the capacity
</bodyText>
<figure confidence="0.997409365853658">
–J
wind
–J
T/T, S/N, N/N
b → xt (–F)
a/b xt b + a0 ... ; a0 → xt (+F)
a/b a0
+F
T/T, S/VP, V
T/T, S/NP +J the
T/T, S/N +J mud
T/T, S/N, N
+F
T/T, S/NP, D
room
–F
–F
+F
T/T, S/N, N
T/T, S/N +J door
T/T, S
T/T +J
a
–F:
a
+F:
b
a0
b
xt
xt
T/T the
T/T, D +F
T/T, NP/N
T/T, NP
T/T, S/VP
–J
a/b xt
a
–F
shook
</figure>
<figureCaption confidence="0.9746825">
Figure 3: Processing steps in parsing the sentence
The wind shook the mud room door.
</figureCaption>
<figure confidence="0.9818325">
matrix multiplication:1
v = M u (1)
+J: a
b
a00 b00
a
–J:
b
a0
a00 b00
a/ba00
a/b00 b → a00 b00 (+J)
a/b a00 +
a/b a0/b00 b→ a0 ...; a0 → a00 b00 (–J)
</figure>
<figureCaption confidence="0.980519">
Figure 2: Fork and join operations from the van
</figureCaption>
<bodyText confidence="0.961134454545455">
Schijndel et al. (2013a) left-corner parser formu-
lation. During the fork phase, word x either com-
pletes an existing incomplete category a, or forks
into a new complete category a0. During the join
phase, complete category a00 becomes a left child
of a grammar rule application, then either joins
onto a superordinate incomplete category a/b or
remains disjoint.
probabilistic version of this incremental parser can
reproduce the results of a state-of-the-art chart-
based parser (Petrov and Klein, 2007).
</bodyText>
<sectionHeader confidence="0.995602" genericHeader="method">
4 Vectorial Parsing
</sectionHeader>
<bodyText confidence="0.999718266666667">
This left corner parser can be implemented in a
vectorial model of working memory using vec-
tors as activation-based memory and matrices as
weight-based memory. Following Anderson et al.
(1977) and others, vectors v in activation-based
memory are cued from other vectors u through
weight-based memory matrices M using ordinary
This representation has been used to model the in-
fluence of activation in antecedent neurons on ac-
tivation in consequent neurons (Marr, 1971; An-
derson et al., 1977).
Unless they are cued from some other source,
all vectors in this model are initially randomly
generated by sampling from an exponential distri-
bution, denoted here simply by:
</bodyText>
<equation confidence="0.569427">
v ∼ Exp (2)
</equation>
<bodyText confidence="0.9997195">
Also following Anderson et al. (1977), weight-
based memory matrices M are themselves defined
and updated by simply adding outer products of
desired cue u and target v vectors:2
</bodyText>
<equation confidence="0.994552">
Mt = Mt−1 + v ⊗ u (3)
</equation>
<bodyText confidence="0.9998284">
This representation has been used to model rapid
synaptic sensitization in the hippocampus (Marr,
1971; McClelland et al., 1995), in which synapses
of activated antecedent neurons that impinge on
activated consequent neurons are strengthened.
</bodyText>
<equation confidence="0.76646725">
1That is, multiplication of an associative memory ma-
trix M by a state vector v yields:
def = �J
(M v)[i] j=1 M[i,j] · v[j] (10)
</equation>
<bodyText confidence="0.5736835">
2An outer product v ⊗ u defines a matrix by multiplying
each combination of scalars in vectors v and u:
</bodyText>
<equation confidence="0.588093">
def
(v ⊗ u)[ j,i] = v[j] · u[i] (20)
</equation>
<page confidence="0.994756">
21
</page>
<figure confidence="0.9989879">
a) NP b) S c) S d) S
N NP VP
NP
D
the
N
wind
N
wind
D
the
D
the
VP
NP
D N
VP
V NP
shook
th
</figure>
<page confidence="0.991548">
22
</page>
<bodyText confidence="0.976504">
grammar rule vector gS , NP VP and category vec-
tors cS, cVP, cNP with the following outer-product
associations:
</bodyText>
<equation confidence="0.974341285714286">
def
G =gS , NP VP ® cS · 0.8
G def
&apos;
= gS , NP VP ® cNP
�� def
G = gS , NP VP ® cVP
</equation>
<bodyText confidence="0.96074425">
Grammars with additional rules can then be en-
coded as a sum of outer products of rule and cat-
egory vectors. Grammar rules can then be cued
from category types by matrix multiplication, e.g.:
gS , NP VP = G&apos; cNP
and category types can be cued from grammar
rules using transposed versions of accessor matri-
ces:
</bodyText>
<equation confidence="0.753701">
cNP = G&apos;TgS , NP VP
</equation>
<bodyText confidence="0.920288">
The model also defines:
</bodyText>
<listItem confidence="0.894651125">
• vectors xt for observation types (i.e. words),
• a matrix P cueing category types from obser-
vation types, populated from unary rules in a
CNF PCFG, and
• a matrix D = DK of leftmost descendant cate-
gories cued from ancestor categories, derived
from accessor matrices G and G&apos; by K itera-
tions of the following recurrence:5
</listItem>
<equation confidence="0.9787648">
def
D&apos; = diag(1) (5)
0
def
D0 = diag(0) (6)
D&apos;k def G&apos;T G Dk &apos;−1
(7)
def
Dk = Dk−1 + D&apos; (8)
k
</equation>
<bodyText confidence="0.998984583333333">
where each D&apos;k cues a probabilistically-
weighted descendant at distance k from its
cue, and Dk is the superposition of all such
descendant associations from length 1 to
length K. This produces a superposed set of
category types that may occur as leftmost de-
scendants of a (possibly superposed) ancestor
category type.
In order to exclude active category types Ct at
that are not compatible with awaited category
types Ct bt in the same incomplete category, the
model also defines:
</bodyText>
<footnote confidence="0.8009985">
5Here 1 and 0 denote vectors of ones and zeros, respec-
tively.
</footnote>
<listItem confidence="0.98780675">
• a matrix E = EK of rightmost descendant
categories cued from ancestor categories, de-
rived in the same manner as D, except us-
ing G&apos;&apos; in place of G&apos;.
</listItem>
<bodyText confidence="0.985747875">
The parser proceeds in two phases, generating a
complete category token vector a&apos;&apos;
t from bt−1 dur-
ing the F phase, then generating an awaited cat-
egory token vector bt of an incomplete category
during the J phase. Since the parser proceeds in
two phases, this paper will distinguish variables
updated in each phase using a subscript for time
step t−.5 at the end of the first phase and t at the
end of the second phase.
The vectorial parser implements the F phase of
the left-corner parser (the ‘fork/no-fork’ decision)
by first defining two new category tokens for the
possibly forked or unforked complete category:
at−.5, a&apos;t−.5 — Exp
The parser then obtains:
</bodyText>
<listItem confidence="0.99956825">
• the category type of the most subordinate
awaited category token at the previous time
step: Ct−1 bt−1 (which involves no fork), and
• a superposed set of non-immediate descen-
</listItem>
<bodyText confidence="0.768768142857143">
dants of the category type of this most sub-
ordinate awaited category token: D Ct−1 bt−1
(which involves a fork),
These fork and no-fork categories are then diag-
onally multiplied (intersected) with a superposed
set of preterminal categories for the current obser-
vation (P xt):
</bodyText>
<equation confidence="0.981269">
c−t = diag(P xt) Ct−1 bt−1
c+ t = diag(P xt) D Ct−1 bt−1
</equation>
<bodyText confidence="0.999876666666667">
The B and C continuation and category matrices
are then updated with a superordinate awaited cat-
egory token and category type for a and a&apos;:
</bodyText>
<equation confidence="0.999809333333333">
at−1 = At−1 bt−1
Bt−.5 = Bt−1 + bt−1 ® a&apos;t−.5 + Bt−1 at−1 ® at−.5
Ct−.5 = Ct−1 + c+t ® a&apos;t−.5 + diag(Ct−1 at−1) ETc−t ® at−.5
</equation>
<bodyText confidence="0.999968166666667">
where the updated category for at−.5 results from
an intersection (diagonal product) of the current
category at at−1 with the set of categories that can
occur with c−t as a rightmost child, as defined by E.
The intersected fork and no-fork category types
are then used to weight superposed hypotheses for
</bodyText>
<page confidence="0.986865">
23
</page>
<figure confidence="0.851065">
xt
</figure>
<figureCaption confidence="0.989842">
Figure 5: Updates to continuation matrices during
the ‘fork’ phase of a left-corner parser.
</figureCaption>
<bodyText confidence="0.961109454545455">
the complete category token a00 tthat will result
from this phase of processing, and the b vector is
updated to encode the category token:6
These updates can be represented graphically as
shown in Figure 5.
The vectorial parser then similarly implements
the J phase (the ‘join/no-join’ decision) of the left-
corner parser by first defining a new category to-
ken a0 for a possible new active category of the
most subordinate incomplete category, and b00 for
a new awaited category token:
</bodyText>
<equation confidence="0.99226">
a0t, b00
t ∼ Exp
</equation>
<bodyText confidence="0.942053">
The parser then obtains:
</bodyText>
<listItem confidence="0.92718">
• a superposed set of grammar rules with par-
ent category matching the category of the
most subordinate awaited category token at
the previous time step: G Ct−.5 bt−.5 (which as-
sumes a join), and
• a superposed set of grammar rules with
parent category non-immediately descended
from the category of this most subordinate
awaited category token: G D Ct−.5 bt−.5 (which
assumes no join)
</listItem>
<bodyText confidence="0.6186952">
These join and no-join grammar rule vectors
are then diagonally multiplied (intersected) with
6This uses the two norm ||v||, which is the magnitude of
vector v, defined as the square root of the sum of the squares
of its scalar values:
</bodyText>
<equation confidence="0.638313">
��
||v ||def = i(v[i])2 (40)
</equation>
<bodyText confidence="0.763756">
Dividing a vector by its two norm has the effect of normaliz-
ing it to unit length.
</bodyText>
<figureCaption confidence="0.985579">
Figure 6: Updates to continuation matrices during
the ‘join’ phase of a left-corner parser.
</figureCaption>
<bodyText confidence="0.972816">
the superposed set of grammar rules whose left
child category type matches the category type
of the most subordinate complete category to-
</bodyText>
<equation confidence="0.923140166666667">
ken (G0 Ct−.5 a00
t ):
g+t = diag(G0 Ct−.5 a00
t ) G Ct−.5 bt−.5
g−t = diag(G0 Ct−.5 a00
t ) G D Ct−.5 bt−.5
</equation>
<bodyText confidence="0.9999384">
These intersected join and no-join grammar rule
vectors are then used to weight superposed hy-
potheses for the incomplete category that will re-
sult from this phase of processing in updates to the
continuation and category matrices A, B, and C:
</bodyText>
<equation confidence="0.999505571428571">
At−1 bt−.5 ||g+t  ||+ a0t ||g−t  ||t
||At−1 bt−.5 ||g+ t  ||+ a0 t ||g− t   |⊗ b00
Bt = Bt−.5 + bt−.5 ⊗ a0t
G00&gt;g+ t + G00&gt;g−
Ct = Ct−.5 + G&gt;g− t ⊗ a0 t
t + t  ||⊗ b00
||G00&gt;g+ t + G00&gt;g− t
</equation>
<bodyText confidence="0.99845625">
These updates can be represented graphically as
shown in Figure 6. Finally the the most subordi-
nate awaited category token is updated for the next
word:
</bodyText>
<equation confidence="0.9974705">
bt = b00
t
</equation>
<sectionHeader confidence="0.993552" genericHeader="method">
5 Predictions
</sectionHeader>
<bodyText confidence="0.9999576">
In order to assess the cognitive plausibility of the
memory modeling assumptions in this vectorial
parser, predictions of the implementation defined
in Section 4 were calculated on center-embedding
and right-branching sentences, exemplified by:
</bodyText>
<listItem confidence="0.96787575">
(1) If either Kim stays or Kim leaves then Pat
leaves. (center-embedded condition)
(2) If Kim stays then if Kim leaves then Pat
leaves. (right-branching condition)
</listItem>
<figure confidence="0.963042863636363">
–F:
at−1 (= a00t)
at−1
+F:
bt−1
a0t−.5(= a00
t )
bt−1
B
xt
at−.5
+J:
–J: at−.5
bt−.5
a0
t
bt−.5
A
B
a00 b00
t t
A
</figure>
<equation confidence="0.9481925">
a00 b00
t t
at−1 ||c−t  ||+ a0t−.5 ||c+t ||
||at−1 ||c−t  ||+ a0 t−.5 ||c+ t  |
bt−.5 = Bt−.5 a00
t
a00
t =
At = At−1 +
24
P(T → S T) = 1.0
P(S → NP VP) = 0.5
P(S → IF S THEN S) = 0.25
P(S → EITHER S OR S) = 0.25
P(IF → if) = 1.0
P(THEN → then) = 1.0
P(EITHER → either) = 1.0
P(OR → or) = 1.0
P(NP → kim) = 0.5
P(NP → pat) = 0.5
P(VP → leaves) = 0.5
P(VP → stays) = 0.5
</equation>
<figureCaption confidence="0.712624">
Figure 7: ‘If ...then ...’ grammar used in sen-
</figureCaption>
<bodyText confidence="0.9941224375">
tence processing experiment. Branches with arity
greater than two are decomposed into equivalent
right-branching sequences of binary branches.
both of which contain the same number of words.
These sentences were processed using the gram-
mar shown in Figure 7, which assigns the same
probability to both center-embedding and right-
branching sentences. The i f... then ... and ei-
ther ... or ... constructions used in these ex-
amples are taken from the original Chomsky and
Miller (1963) paper introducing center-embedding
effects, and are interesting because they do not in-
volve the same grammar rule (as is the case with
familiar nested object relative constructions), and
do not involve filler-gap constructions, which may
introduce overhead processing costs as a possible
confound.
This assessment consisted of 500 trials for each
sentence type. Sentences were input to an imple-
mentation of this model using the Numpy package
in Python, which consists of the equations shown
in Section 4 enclosed in a loop over the words in
each sentence. Each trial initially sampled a, b,
c, and g vectors from random exponential distri-
butions of dimension 100, and the parser initial-
ized b0 with category type T as shown in Figure 3,
with the active category token at A0 b0 also asso-
ciated with category type T.
Accuracy for this assessment was calculated by
finding the category type with the maximum co-
sine similarity for the awaited category bT at the
end of the sentence. If this category type was T
</bodyText>
<table confidence="0.945268333333333">
sentence correct incorrect
center-embedded 231 269
right-branching 297 203
</table>
<tableCaption confidence="0.897007">
Table 1: Accuracy of vectorial parser on each sen-
tence type.
</tableCaption>
<bodyText confidence="0.999625659090909">
(as it is in Figure 3), the parser was awarded a
point of accuracy; otherwise it was not. The re-
sults of this assessment are shown in Table 1. The
parser processes sentences with right-branching
structure substantially more accurately than sen-
tences with center-embedded structure. These re-
sults are strongly significant (p &lt; .001) using a χ2
test.
These predictions seem to be consistent with
observations by Chomsky and Miller (1963) that
center-embedded structures are more difficult to
parse than right-branching structures, but it is also
important to note how the model arrives at these
predictions. The decreased accuracy of center-
embedded sentences is not a result of an ex-
plicit decay factor, as in ACT-R and other models
(Lewis and Vasishth, 2005), or distance measures
as in DLT (Gibson, 2000), nor is it attributable
to cue interference (as modeled by Lewis and
Vasishth for nested object relative constructions),
since the inner and outer embeddings in these
sentences use different grammar rules. The de-
creased accuracy for center-embedding is also not
attributable to frequency effects of grammar rules
(as modeled by Hale, 2001), since the rules in
this grammar are relatively common and equally
weighted.
Instead, the decrease for center-embedded
structures emerges from this model as a necessary
result of drift due to repeated superposition of tar-
gets encoded in continuation matrices A and B.
This produces a natural decay over time as se-
quences of subordinate category token vectors bt
introduce noise in updates to At and Bt. When
these matrices are cued in concert, as happens
when cueing across incomplete categories, the dis-
tortion is magnified. This decay is therefore a
consequence of encoding hierarchic structural in-
formation using cued associations. In contrast,
right-branching parses are not similarly as badly
degraded over time because the flat treatment of
left- and right- branching structures in a left-corner
parser does not cue as often across incomplete cat-
egories using matrix B.
</bodyText>
<page confidence="0.997424">
25
</page>
<sectionHeader confidence="0.999096" genericHeader="evaluation">
6 Extensions
</sectionHeader>
<bodyText confidence="0.999970630434783">
This model is also interesting because it allows se-
mantic relations to be constructed using the same
outer product associations used to define contin-
uation and category matrices in Section 4. First,
discourse referent instances and numbered relation
types are defined as vectors i and n, respectively.
Then relation tokens are reified as vectors r, simi-
lar to the reification of grammar rules described in
Section 4, and connected to relation type vectors n
by cued association R and to source and target dis-
course referents i by cued associations R&apos; and R&amp;quot;.
Semantic relation types can then be cued from
grammar rules g using associative matrix N, al-
lowing relations of various types to be constructed
in cases of superposed grammar rules. In future
work, it would be interesting to see whether this
representation is consistent with observations of
local syntactic coherence (Tabor et al., 2004).
This model can also constrain relations to dis-
course referents introduced in a previous sentence
or earlier in the same sentence using a vector of
temporal features (Howard and Kahana, 2002).
This is a vector of features zt, that has a randomly
chosen selection of features randomly resampled
at each time step, exponentially decreasing the co-
sine similarity of the current version of the zt vec-
tor to earlier versions zt,. If discourse referents i
are cued from the current temporal features zt in
an outer product associative matrix Z, it will cue
relatively recently mentioned discourse referents
more strongly than less recently mentioned refer-
ents. If discourse referents for eventualities and
propositions j are connected to explicit predicate
type referents k (say, cued by a relation of type
‘0’), and if temporal cues are combined in a diag-
onal product with cues by semantic relations from
a common predicate type, the search for a consis-
tent discourse referent can be further constrained
to match the gender of a pronoun or other rela-
tions from a definite reference. In future work, it
would be interesting to compare the predictions of
this kind of model to human coreference resolu-
tion, particularly in the case of parsing conjunc-
tions with reflexive pronouns, which has been used
to argue for fully connected incremental parsing
(Sturt and Lombardo, 2005).
</bodyText>
<sectionHeader confidence="0.998931" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999953615384616">
This paper has presented a vectorial left-
corner parsing model defined using independently
posited operations over activation-based working
memory and weight-based episodic memory. This
model has the attractive property that it hypoth-
esizes only one unary branching rule application
and only one binary branching rule application per
time step, which allows it to be smoothly inte-
grated into a vector-based recurrence that propa-
gates structural ambiguity from one time step to
the next. Predictions of this model were calcu-
lated on a center-embedded sentence processing
task and the model was shown to exhibit decreased
processing accuracy in center-embedded construc-
tions, as observed by Chomsky and Miller (1963),
even in the absence of repeated grammar rules or
potential confounding overhead costs that may be
associated with filler-gap constructions.
This model is particularly interesting because,
unlike other vectorial or connectionist parsers,
it directly implements a recursive probabilistic
grammar with explicit categories of syntactic con-
text. This explicit implementation of a probabilis-
tic grammar allows variations of this processing
model to be evaluated without having to also posit
a human-like model of acquisition. For example,
the model can simply be defined with a PCFG de-
rived from a syntactically annotated corpus.
The model is also interesting because it serves
as an existence proof that recursive grammar is not
incompatible with current models of human mem-
ory.
Finally, the fact that this model predicts mem-
ory effects at boundaries between incomplete cat-
egories, in line with predictions of fully paral-
lel left-corner parsers (van Schijndel and Schuler,
2013; van Schijndel et al., 2013b), suggests
that measures based on incomplete categories (or
based on connected components of other kinds of
syntactic or semantic structure) are not simply ar-
bitrary but rather may naturally emerge from the
use of associative memory during sentence pro-
cessing.
Although the model may not scale to broad-
coverage parsing evaluations in its present form,
future work will explore hybridization of some of
these methods into a parser with an explicit beam
of parallel hypotheses. It is anticipated that an
algorithmic-level comprehension model such as
this will allow a more nuanced understanding of
human semantic representation and grammar ac-
quisition.
</bodyText>
<page confidence="0.99346">
26
</page>
<sectionHeader confidence="0.995873" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923576923077">
James A. Anderson, Jack W. Silverstein, Stephen A.
Ritz, and Randall S. Jones. 1977. Distinctive fea-
tures, categorical perception and probability learn-
ing: Some applications of a neural model. Psycho-
logical Review, 84:413–451.
Peter beim Graben, Sabrina Gerth, and Shravan Va-
sishth. 2008. Towards dynamical system models
of language-related brain potentials. Cognitive Neu-
rodynamics, 2(3):229–255.
Noam Chomsky and George A. Miller. 1963. Intro-
duction to the formal analysis of natural languages.
In Handbook of Mathematical Psychology, pages
269–321. Wiley, New York, NY.
Jeffrey L. Elman. 1991. Distributed representations,
simple recurrent networks, and grammatical struc-
ture. Machine Learning, 7:195–225.
Edward Gibson. 2000. The dependency locality the-
ory: A distance-based theory of linguistic complex-
ity. In Image, language, brain: Papers from the first
mind articulation project symposium, pages 95–126,
Cambridge, MA. MIT Press.
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the sec-
ond meeting of the North American chapter of the
Association for Computational Linguistics, pages
159–166, Pittsburgh, PA.
Marc W. Howard and Michael J. Kahana. 2002. A dis-
tributed representation of temporal context. Journal
of Mathematical Psychology, 45:269–299.
Roger Levy and Edward Gibson. 2013. Surprisal, the
pdc, and the primary locus of processing difficulty
in relative clauses. Frontiers in Psychology, 4(229).
Roger Levy. 2008. Expectation-based syntactic com-
prehension. Cognition, 106(3):1126–1177.
Richard L. Lewis and Shravan Vasishth. 2005.
An activation-based model of sentence processing
as skilled memory retrieval. Cognitive Science,
29(3):375–419.
David Marr. 1971. Simple memory: A theory
for archicortex. Philosophical Transactions of the
Royal Society (London) B, 262:23–81.
J. L. McClelland, B. L. McNaughton, and R. C.
O’Reilly. 1995. Why there are complementary
learning systems in the hippocampus and neocortex:
Insights from the successes and failures of connec-
tionist models of learning and memory. Psychologi-
cal Review, 102:419–457.
B.B. Murdock. 1982. A theory for the storage and
retrieval of item and associative information. Psy-
chological Review, 89:609–626.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404–411, Rochester, New York,
April. Association for Computational Linguistics.
Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures in
connectionist systems. Artificial intelligence, 46(1-
2):159–216.
Patrick Sturt and Vincent Lombardo. 2005. Processing
coordinate structures: Incrementality and connect-
edness. Cognitive Science, 29:291–305.
W. Tabor, B. Galantucci, and D Richardson. 2004.
Effects of merely local syntactic coherence on sen-
tence processing. Journal of Memory and Lan-
guage, 50(4):355–370.
Marten van Schijndel and William Schuler. 2013. An
analysis of frequency- and recency-based processing
costs. In Proceedings of NAACL-HLT 2013. Associ-
ation for Computational Linguistics.
Marten van Schijndel, Andy Exley, and William
Schuler. 2013a. A model of language processing
as hierarchic sequential prediction. Topics in Cogni-
tive Science, 5(3):522–540.
Marten van Schijndel, Luan Nguyen, and William
Schuler. 2013b. An analysis of memory-based pro-
cessing costs using incremental deep syntactic de-
pendency parsing. In Proceedings of CMCL 2013.
Association for Computational Linguistics.
</reference>
<page confidence="0.998808">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.819371">
<title confidence="0.999696">Sentence Processing in a Vectorial Model of Working Memory</title>
<author confidence="0.999932">William Schuler</author>
<affiliation confidence="0.999725">Department of Linguistics The Ohio State University</affiliation>
<email confidence="0.998751">schuler@ling.osu.edu</email>
<abstract confidence="0.989859444444445">This paper presents a vectorial incremental parsing model defined using independently posited operations over activationbased working memory and weight-based episodic memory. This model has the attractive property that it hypothesizes only one unary preterminal rule application and only one binary branching rule application per time step, which allows it to be smoothly integrated into a vector-based recurrence that propagates structural ambiguity from one time step to the next. Predictions of this model are calculated on a center-embedded sentence processing task and shown to exhibit decreased processing accuracy in center-embedded constructions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James A Anderson</author>
<author>Jack W Silverstein</author>
<author>Stephen A Ritz</author>
<author>Randall S Jones</author>
</authors>
<title>Distinctive features, categorical perception and probability learning: Some applications of a neural model.</title>
<date>1977</date>
<tech>Psychological Review, 84:413–451.</tech>
<contexts>
<context position="886" citStr="Anderson et al., 1977" startWordPosition="126" endWordPosition="130">r activationbased working memory and weight-based episodic memory. This model has the attractive property that it hypothesizes only one unary preterminal rule application and only one binary branching rule application per time step, which allows it to be smoothly integrated into a vector-based recurrence that propagates structural ambiguity from one time step to the next. Predictions of this model are calculated on a center-embedded sentence processing task and shown to exhibit decreased processing accuracy in center-embedded constructions. 1 Introduction Current models of memory (Marr, 1971; Anderson et al., 1977; Murdock, 1982; McClelland et al., 1995; Howard and Kahana, 2002) involve a continuous activation-based (or ‘working’) memory, typically modeled as a vector representing the current firing pattern of neurons or neural clusters in the cortex. This activation-based memory is then supported by a durable but rapidly mutable weight-based (or ‘episodic’) memory, typically modeled as one or more matrices formed by summed outer-products of cue and target vectors and cued by simple matrix multiplication, representing variable synaptic connection strengths between neurons or neural clusters. The lack o</context>
<context position="7612" citStr="Anderson et al. (1977)" startWordPosition="1213" endWordPosition="1216"> completes an existing incomplete category a, or forks into a new complete category a0. During the join phase, complete category a00 becomes a left child of a grammar rule application, then either joins onto a superordinate incomplete category a/b or remains disjoint. probabilistic version of this incremental parser can reproduce the results of a state-of-the-art chartbased parser (Petrov and Klein, 2007). 4 Vectorial Parsing This left corner parser can be implemented in a vectorial model of working memory using vectors as activation-based memory and matrices as weight-based memory. Following Anderson et al. (1977) and others, vectors v in activation-based memory are cued from other vectors u through weight-based memory matrices M using ordinary This representation has been used to model the influence of activation in antecedent neurons on activation in consequent neurons (Marr, 1971; Anderson et al., 1977). Unless they are cued from some other source, all vectors in this model are initially randomly generated by sampling from an exponential distribution, denoted here simply by: v ∼ Exp (2) Also following Anderson et al. (1977), weightbased memory matrices M are themselves defined and updated by simply </context>
</contexts>
<marker>Anderson, Silverstein, Ritz, Jones, 1977</marker>
<rawString>James A. Anderson, Jack W. Silverstein, Stephen A. Ritz, and Randall S. Jones. 1977. Distinctive features, categorical perception and probability learning: Some applications of a neural model. Psychological Review, 84:413–451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter beim Graben</author>
<author>Sabrina Gerth</author>
<author>Shravan Vasishth</author>
</authors>
<title>Towards dynamical system models of language-related brain potentials.</title>
<date>2008</date>
<journal>Cognitive Neurodynamics,</journal>
<volume>2</volume>
<issue>3</issue>
<marker>Graben, Gerth, Vasishth, 2008</marker>
<rawString>Peter beim Graben, Sabrina Gerth, and Shravan Vasishth. 2008. Towards dynamical system models of language-related brain potentials. Cognitive Neurodynamics, 2(3):229–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
<author>George A Miller</author>
</authors>
<title>Introduction to the formal analysis of natural languages.</title>
<date>1963</date>
<booktitle>In Handbook of Mathematical Psychology,</booktitle>
<pages>269--321</pages>
<publisher>Wiley,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="16022" citStr="Chomsky and Miller (1963)" startWordPosition="2775" endWordPosition="2778">1.0 P(OR → or) = 1.0 P(NP → kim) = 0.5 P(NP → pat) = 0.5 P(VP → leaves) = 0.5 P(VP → stays) = 0.5 Figure 7: ‘If ...then ...’ grammar used in sentence processing experiment. Branches with arity greater than two are decomposed into equivalent right-branching sequences of binary branches. both of which contain the same number of words. These sentences were processed using the grammar shown in Figure 7, which assigns the same probability to both center-embedding and rightbranching sentences. The i f... then ... and either ... or ... constructions used in these examples are taken from the original Chomsky and Miller (1963) paper introducing center-embedding effects, and are interesting because they do not involve the same grammar rule (as is the case with familiar nested object relative constructions), and do not involve filler-gap constructions, which may introduce overhead processing costs as a possible confound. This assessment consisted of 500 trials for each sentence type. Sentences were input to an implementation of this model using the Numpy package in Python, which consists of the equations shown in Section 4 enclosed in a loop over the words in each sentence. Each trial initially sampled a, b, c, and g</context>
<context position="17601" citStr="Chomsky and Miller (1963)" startWordPosition="3036" endWordPosition="3039"> the end of the sentence. If this category type was T sentence correct incorrect center-embedded 231 269 right-branching 297 203 Table 1: Accuracy of vectorial parser on each sentence type. (as it is in Figure 3), the parser was awarded a point of accuracy; otherwise it was not. The results of this assessment are shown in Table 1. The parser processes sentences with right-branching structure substantially more accurately than sentences with center-embedded structure. These results are strongly significant (p &lt; .001) using a χ2 test. These predictions seem to be consistent with observations by Chomsky and Miller (1963) that center-embedded structures are more difficult to parse than right-branching structures, but it is also important to note how the model arrives at these predictions. The decreased accuracy of centerembedded sentences is not a result of an explicit decay factor, as in ACT-R and other models (Lewis and Vasishth, 2005), or distance measures as in DLT (Gibson, 2000), nor is it attributable to cue interference (as modeled by Lewis and Vasishth for nested object relative constructions), since the inner and outer embeddings in these sentences use different grammar rules. The decreased accuracy f</context>
<context position="22198" citStr="Chomsky and Miller (1963)" startWordPosition="3770" endWordPosition="3773">y posited operations over activation-based working memory and weight-based episodic memory. This model has the attractive property that it hypothesizes only one unary branching rule application and only one binary branching rule application per time step, which allows it to be smoothly integrated into a vector-based recurrence that propagates structural ambiguity from one time step to the next. Predictions of this model were calculated on a center-embedded sentence processing task and the model was shown to exhibit decreased processing accuracy in center-embedded constructions, as observed by Chomsky and Miller (1963), even in the absence of repeated grammar rules or potential confounding overhead costs that may be associated with filler-gap constructions. This model is particularly interesting because, unlike other vectorial or connectionist parsers, it directly implements a recursive probabilistic grammar with explicit categories of syntactic context. This explicit implementation of a probabilistic grammar allows variations of this processing model to be evaluated without having to also posit a human-like model of acquisition. For example, the model can simply be defined with a PCFG derived from a syntac</context>
</contexts>
<marker>Chomsky, Miller, 1963</marker>
<rawString>Noam Chomsky and George A. Miller. 1963. Introduction to the formal analysis of natural languages. In Handbook of Mathematical Psychology, pages 269–321. Wiley, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Distributed representations, simple recurrent networks, and grammatical structure.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<pages>7--195</pages>
<contexts>
<context position="5466" citStr="Elman (1991)" startWordPosition="821" endWordPosition="822"> store configurations as superposed sequence states in a finite-dimensional state vector. The model then exploits the uniformity of its parsing operations to integrate probabilistically weighted grammar rule applications into this superposed state vector. These superposed states are then used to cue more superordinate sequential states as ‘continuations’ whenever subordinate states conclude. Interference in this cueing process is then observed to produce a natural centerembedding limit. This model is defined as a recurrence over an activation vector, similar to the simple recurrent network of Elman (1991) and others, but unlike an SRN, which does not encode anything in weightbased memory during processing, this model encodes updates to a processing hierarchy in weightbased memory at every time step. The model is also similar to the ACT-R parser of Lewis and Vasishth (2005) in that it maintains a single state which is updated based on content-based cued association, but unlike the ACT-R parser, which cues category tokens on category types and therefore models memory limits as interference among grammar rules, this model cues category tokens on other category tokens, and therefore predicts memor</context>
</contexts>
<marker>Elman, 1991</marker>
<rawString>Jeffrey L. Elman. 1991. Distributed representations, simple recurrent networks, and grammatical structure. Machine Learning, 7:195–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>The dependency locality theory: A distance-based theory of linguistic complexity. In Image, language, brain: Papers from the first mind articulation project symposium,</title>
<date>2000</date>
<pages>95--126</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="17970" citStr="Gibson, 2000" startWordPosition="3098" endWordPosition="3099">ing structure substantially more accurately than sentences with center-embedded structure. These results are strongly significant (p &lt; .001) using a χ2 test. These predictions seem to be consistent with observations by Chomsky and Miller (1963) that center-embedded structures are more difficult to parse than right-branching structures, but it is also important to note how the model arrives at these predictions. The decreased accuracy of centerembedded sentences is not a result of an explicit decay factor, as in ACT-R and other models (Lewis and Vasishth, 2005), or distance measures as in DLT (Gibson, 2000), nor is it attributable to cue interference (as modeled by Lewis and Vasishth for nested object relative constructions), since the inner and outer embeddings in these sentences use different grammar rules. The decreased accuracy for center-embedding is also not attributable to frequency effects of grammar rules (as modeled by Hale, 2001), since the rules in this grammar are relatively common and equally weighted. Instead, the decrease for center-embedded structures emerges from this model as a necessary result of drift due to repeated superposition of targets encoded in continuation matrices </context>
</contexts>
<marker>Gibson, 2000</marker>
<rawString>Edward Gibson. 2000. The dependency locality theory: A distance-based theory of linguistic complexity. In Image, language, brain: Papers from the first mind articulation project symposium, pages 95–126, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>A probabilistic earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the second meeting of the North American chapter of the Association for Computational Linguistics,</booktitle>
<pages>159--166</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="2820" citStr="Hale, 2001" startWordPosition="424" endWordPosition="425">sing each word. This property greatly simplifies a vectorial implementation because it allows these single grammar rule applications to be superposed in cases of attachment ambiguity. Predictions of the vectorial model described in this paper are then calculated on a simple centerembedded sentence processing task, producing a lower completion accuracy for center-embedded sentences than for right-branching sentences with the same number of words. As noted by Levy and Gibson (2013), this kind of memory effect is not easily explained by existing information-theoretic models of frequency effects (Hale, 2001; Levy, 2008). The model described in this paper also provides an explanation for the apparent reality of linguistic objects like categories, grammar rules, discourse referents and dependency relations, as cognitive states in activation-based memory (in the case of categories and discourse referents), or cued associations in weight-based memory (in the case of grammar rules, and dependency relations), without having to posit complex machinery specific to language processing. In this sense, unlike existing chart-based parsers or connectionist models based on recurrent neural networks, this mode</context>
<context position="4304" citStr="Hale (2001)" startWordPosition="643" endWordPosition="644"> understanding of linguistic phenomena such as ambiguity resolution, seman19 Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 19–27, Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics tic representation, and language acquisition. S 2 Related Work VP The model described in this paper is based on the NP left-corner parser formulation of van Schijndel et N al. (2013a), which is an implementation of a fully parallel incremental parser. This parser differs from chart-based fully parallel incremental parsers used by Hale (2001), Levy (2008) and others in that it enforces a cognitively-motivated bound on center-embedding depth. This bound allows the parser to represent a tractable set of incremental hypotheses in an explicitly enumerated list as a factored hidden Markov model, without necessitating the use of a parser chart. This model has the attractive property that, in any context, it hypothesizes exactly one binary-branching rule application at each time step. The model described in this paper extends the van Schijndel et al. (2013a) parser by maintaining possible store configurations as superposed sequence state</context>
<context position="18310" citStr="Hale, 2001" startWordPosition="3150" endWordPosition="3151">it is also important to note how the model arrives at these predictions. The decreased accuracy of centerembedded sentences is not a result of an explicit decay factor, as in ACT-R and other models (Lewis and Vasishth, 2005), or distance measures as in DLT (Gibson, 2000), nor is it attributable to cue interference (as modeled by Lewis and Vasishth for nested object relative constructions), since the inner and outer embeddings in these sentences use different grammar rules. The decreased accuracy for center-embedding is also not attributable to frequency effects of grammar rules (as modeled by Hale, 2001), since the rules in this grammar are relatively common and equally weighted. Instead, the decrease for center-embedded structures emerges from this model as a necessary result of drift due to repeated superposition of targets encoded in continuation matrices A and B. This produces a natural decay over time as sequences of subordinate category token vectors bt introduce noise in updates to At and Bt. When these matrices are cued in concert, as happens when cueing across incomplete categories, the distortion is magnified. This decay is therefore a consequence of encoding hierarchic structural i</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>John Hale. 2001. A probabilistic earley parser as a psycholinguistic model. In Proceedings of the second meeting of the North American chapter of the Association for Computational Linguistics, pages 159–166, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc W Howard</author>
<author>Michael J Kahana</author>
</authors>
<title>A distributed representation of temporal context.</title>
<date>2002</date>
<journal>Journal of Mathematical Psychology,</journal>
<pages>45--269</pages>
<contexts>
<context position="952" citStr="Howard and Kahana, 2002" startWordPosition="137" endWordPosition="140">ry. This model has the attractive property that it hypothesizes only one unary preterminal rule application and only one binary branching rule application per time step, which allows it to be smoothly integrated into a vector-based recurrence that propagates structural ambiguity from one time step to the next. Predictions of this model are calculated on a center-embedded sentence processing task and shown to exhibit decreased processing accuracy in center-embedded constructions. 1 Introduction Current models of memory (Marr, 1971; Anderson et al., 1977; Murdock, 1982; McClelland et al., 1995; Howard and Kahana, 2002) involve a continuous activation-based (or ‘working’) memory, typically modeled as a vector representing the current firing pattern of neurons or neural clusters in the cortex. This activation-based memory is then supported by a durable but rapidly mutable weight-based (or ‘episodic’) memory, typically modeled as one or more matrices formed by summed outer-products of cue and target vectors and cued by simple matrix multiplication, representing variable synaptic connection strengths between neurons or neural clusters. The lack of discrete memory units in such models makes it difficult to imagi</context>
<context position="20292" citStr="Howard and Kahana, 2002" startWordPosition="3467" endWordPosition="3470"> association R and to source and target discourse referents i by cued associations R&apos; and R&amp;quot;. Semantic relation types can then be cued from grammar rules g using associative matrix N, allowing relations of various types to be constructed in cases of superposed grammar rules. In future work, it would be interesting to see whether this representation is consistent with observations of local syntactic coherence (Tabor et al., 2004). This model can also constrain relations to discourse referents introduced in a previous sentence or earlier in the same sentence using a vector of temporal features (Howard and Kahana, 2002). This is a vector of features zt, that has a randomly chosen selection of features randomly resampled at each time step, exponentially decreasing the cosine similarity of the current version of the zt vector to earlier versions zt,. If discourse referents i are cued from the current temporal features zt in an outer product associative matrix Z, it will cue relatively recently mentioned discourse referents more strongly than less recently mentioned referents. If discourse referents for eventualities and propositions j are connected to explicit predicate type referents k (say, cued by a relatio</context>
</contexts>
<marker>Howard, Kahana, 2002</marker>
<rawString>Marc W. Howard and Michael J. Kahana. 2002. A distributed representation of temporal context. Journal of Mathematical Psychology, 45:269–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Edward Gibson</author>
</authors>
<title>Surprisal, the pdc, and the primary locus of processing difficulty in relative clauses.</title>
<date>2013</date>
<journal>Frontiers in Psychology,</journal>
<volume>4</volume>
<issue>229</issue>
<contexts>
<context position="2694" citStr="Levy and Gibson (2013)" startWordPosition="404" endWordPosition="407">f van Schijndel et al. (2013a), which has the attractive property of generating exactly one binary-branching rule application after processing each word. This property greatly simplifies a vectorial implementation because it allows these single grammar rule applications to be superposed in cases of attachment ambiguity. Predictions of the vectorial model described in this paper are then calculated on a simple centerembedded sentence processing task, producing a lower completion accuracy for center-embedded sentences than for right-branching sentences with the same number of words. As noted by Levy and Gibson (2013), this kind of memory effect is not easily explained by existing information-theoretic models of frequency effects (Hale, 2001; Levy, 2008). The model described in this paper also provides an explanation for the apparent reality of linguistic objects like categories, grammar rules, discourse referents and dependency relations, as cognitive states in activation-based memory (in the case of categories and discourse referents), or cued associations in weight-based memory (in the case of grammar rules, and dependency relations), without having to posit complex machinery specific to language proces</context>
</contexts>
<marker>Levy, Gibson, 2013</marker>
<rawString>Roger Levy and Edward Gibson. 2013. Surprisal, the pdc, and the primary locus of processing difficulty in relative clauses. Frontiers in Psychology, 4(229).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>106</volume>
<issue>3</issue>
<contexts>
<context position="2833" citStr="Levy, 2008" startWordPosition="426" endWordPosition="427">rd. This property greatly simplifies a vectorial implementation because it allows these single grammar rule applications to be superposed in cases of attachment ambiguity. Predictions of the vectorial model described in this paper are then calculated on a simple centerembedded sentence processing task, producing a lower completion accuracy for center-embedded sentences than for right-branching sentences with the same number of words. As noted by Levy and Gibson (2013), this kind of memory effect is not easily explained by existing information-theoretic models of frequency effects (Hale, 2001; Levy, 2008). The model described in this paper also provides an explanation for the apparent reality of linguistic objects like categories, grammar rules, discourse referents and dependency relations, as cognitive states in activation-based memory (in the case of categories and discourse referents), or cued associations in weight-based memory (in the case of grammar rules, and dependency relations), without having to posit complex machinery specific to language processing. In this sense, unlike existing chart-based parsers or connectionist models based on recurrent neural networks, this model integrates </context>
<context position="4317" citStr="Levy (2008)" startWordPosition="645" endWordPosition="646">g of linguistic phenomena such as ambiguity resolution, seman19 Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 19–27, Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics tic representation, and language acquisition. S 2 Related Work VP The model described in this paper is based on the NP left-corner parser formulation of van Schijndel et N al. (2013a), which is an implementation of a fully parallel incremental parser. This parser differs from chart-based fully parallel incremental parsers used by Hale (2001), Levy (2008) and others in that it enforces a cognitively-motivated bound on center-embedding depth. This bound allows the parser to represent a tractable set of incremental hypotheses in an explicitly enumerated list as a factored hidden Markov model, without necessitating the use of a parser chart. This model has the attractive property that, in any context, it hypothesizes exactly one binary-branching rule application at each time step. The model described in this paper extends the van Schijndel et al. (2013a) parser by maintaining possible store configurations as superposed sequence states in a finite</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>Roger Levy. 2008. Expectation-based syntactic comprehension. Cognition, 106(3):1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard L Lewis</author>
<author>Shravan Vasishth</author>
</authors>
<title>An activation-based model of sentence processing as skilled memory retrieval.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="5739" citStr="Lewis and Vasishth (2005)" startWordPosition="867" endWordPosition="871">e superposed states are then used to cue more superordinate sequential states as ‘continuations’ whenever subordinate states conclude. Interference in this cueing process is then observed to produce a natural centerembedding limit. This model is defined as a recurrence over an activation vector, similar to the simple recurrent network of Elman (1991) and others, but unlike an SRN, which does not encode anything in weightbased memory during processing, this model encodes updates to a processing hierarchy in weightbased memory at every time step. The model is also similar to the ACT-R parser of Lewis and Vasishth (2005) in that it maintains a single state which is updated based on content-based cued association, but unlike the ACT-R parser, which cues category tokens on category types and therefore models memory limits as interference among grammar rules, this model cues category tokens on other category tokens, and therefore predicts memory limits even in cases where grammar rules do not involve similar category types. Also unlike Lewis and Vasishth (2005), this model is defined purely in terms of state vectors and outer-product associative memory and therefore has the capacity –J wind –J T/T, S/N, N/N b → </context>
<context position="17923" citStr="Lewis and Vasishth, 2005" startWordPosition="3088" endWordPosition="3091">n Table 1. The parser processes sentences with right-branching structure substantially more accurately than sentences with center-embedded structure. These results are strongly significant (p &lt; .001) using a χ2 test. These predictions seem to be consistent with observations by Chomsky and Miller (1963) that center-embedded structures are more difficult to parse than right-branching structures, but it is also important to note how the model arrives at these predictions. The decreased accuracy of centerembedded sentences is not a result of an explicit decay factor, as in ACT-R and other models (Lewis and Vasishth, 2005), or distance measures as in DLT (Gibson, 2000), nor is it attributable to cue interference (as modeled by Lewis and Vasishth for nested object relative constructions), since the inner and outer embeddings in these sentences use different grammar rules. The decreased accuracy for center-embedding is also not attributable to frequency effects of grammar rules (as modeled by Hale, 2001), since the rules in this grammar are relatively common and equally weighted. Instead, the decrease for center-embedded structures emerges from this model as a necessary result of drift due to repeated superpositi</context>
</contexts>
<marker>Lewis, Vasishth, 2005</marker>
<rawString>Richard L. Lewis and Shravan Vasishth. 2005. An activation-based model of sentence processing as skilled memory retrieval. Cognitive Science, 29(3):375–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Marr</author>
</authors>
<title>Simple memory: A theory for archicortex.</title>
<date>1971</date>
<journal>Philosophical Transactions of the Royal Society (London) B,</journal>
<pages>262--23</pages>
<contexts>
<context position="863" citStr="Marr, 1971" startWordPosition="124" endWordPosition="125">erations over activationbased working memory and weight-based episodic memory. This model has the attractive property that it hypothesizes only one unary preterminal rule application and only one binary branching rule application per time step, which allows it to be smoothly integrated into a vector-based recurrence that propagates structural ambiguity from one time step to the next. Predictions of this model are calculated on a center-embedded sentence processing task and shown to exhibit decreased processing accuracy in center-embedded constructions. 1 Introduction Current models of memory (Marr, 1971; Anderson et al., 1977; Murdock, 1982; McClelland et al., 1995; Howard and Kahana, 2002) involve a continuous activation-based (or ‘working’) memory, typically modeled as a vector representing the current firing pattern of neurons or neural clusters in the cortex. This activation-based memory is then supported by a durable but rapidly mutable weight-based (or ‘episodic’) memory, typically modeled as one or more matrices formed by summed outer-products of cue and target vectors and cued by simple matrix multiplication, representing variable synaptic connection strengths between neurons or neur</context>
<context position="7886" citStr="Marr, 1971" startWordPosition="1258" endWordPosition="1259">sion of this incremental parser can reproduce the results of a state-of-the-art chartbased parser (Petrov and Klein, 2007). 4 Vectorial Parsing This left corner parser can be implemented in a vectorial model of working memory using vectors as activation-based memory and matrices as weight-based memory. Following Anderson et al. (1977) and others, vectors v in activation-based memory are cued from other vectors u through weight-based memory matrices M using ordinary This representation has been used to model the influence of activation in antecedent neurons on activation in consequent neurons (Marr, 1971; Anderson et al., 1977). Unless they are cued from some other source, all vectors in this model are initially randomly generated by sampling from an exponential distribution, denoted here simply by: v ∼ Exp (2) Also following Anderson et al. (1977), weightbased memory matrices M are themselves defined and updated by simply adding outer products of desired cue u and target v vectors:2 Mt = Mt−1 + v ⊗ u (3) This representation has been used to model rapid synaptic sensitization in the hippocampus (Marr, 1971; McClelland et al., 1995), in which synapses of activated antecedent neurons that impin</context>
</contexts>
<marker>Marr, 1971</marker>
<rawString>David Marr. 1971. Simple memory: A theory for archicortex. Philosophical Transactions of the Royal Society (London) B, 262:23–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L McClelland</author>
<author>B L McNaughton</author>
<author>R C O’Reilly</author>
</authors>
<title>Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory.</title>
<date>1995</date>
<journal>Psychological Review,</journal>
<pages>102--419</pages>
<marker>McClelland, McNaughton, O’Reilly, 1995</marker>
<rawString>J. L. McClelland, B. L. McNaughton, and R. C. O’Reilly. 1995. Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory. Psychological Review, 102:419–457.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B B Murdock</author>
</authors>
<title>A theory for the storage and retrieval of item and associative information.</title>
<date>1982</date>
<journal>Psychological Review,</journal>
<pages>89--609</pages>
<contexts>
<context position="901" citStr="Murdock, 1982" startWordPosition="131" endWordPosition="132">ng memory and weight-based episodic memory. This model has the attractive property that it hypothesizes only one unary preterminal rule application and only one binary branching rule application per time step, which allows it to be smoothly integrated into a vector-based recurrence that propagates structural ambiguity from one time step to the next. Predictions of this model are calculated on a center-embedded sentence processing task and shown to exhibit decreased processing accuracy in center-embedded constructions. 1 Introduction Current models of memory (Marr, 1971; Anderson et al., 1977; Murdock, 1982; McClelland et al., 1995; Howard and Kahana, 2002) involve a continuous activation-based (or ‘working’) memory, typically modeled as a vector representing the current firing pattern of neurons or neural clusters in the cortex. This activation-based memory is then supported by a durable but rapidly mutable weight-based (or ‘episodic’) memory, typically modeled as one or more matrices formed by summed outer-products of cue and target vectors and cued by simple matrix multiplication, representing variable synaptic connection strengths between neurons or neural clusters. The lack of discrete memo</context>
</contexts>
<marker>Murdock, 1982</marker>
<rawString>B.B. Murdock. 1982. A theory for the storage and retrieval of item and associative information. Psychological Review, 89:609–626.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="7398" citStr="Petrov and Klein, 2007" startWordPosition="1180" endWordPosition="1183"> a/b00 b → a00 b00 (+J) a/b a00 + a/b a0/b00 b→ a0 ...; a0 → a00 b00 (–J) Figure 2: Fork and join operations from the van Schijndel et al. (2013a) left-corner parser formulation. During the fork phase, word x either completes an existing incomplete category a, or forks into a new complete category a0. During the join phase, complete category a00 becomes a left child of a grammar rule application, then either joins onto a superordinate incomplete category a/b or remains disjoint. probabilistic version of this incremental parser can reproduce the results of a state-of-the-art chartbased parser (Petrov and Klein, 2007). 4 Vectorial Parsing This left corner parser can be implemented in a vectorial model of working memory using vectors as activation-based memory and matrices as weight-based memory. Following Anderson et al. (1977) and others, vectors v in activation-based memory are cued from other vectors u through weight-based memory matrices M using ordinary This representation has been used to model the influence of activation in antecedent neurons on activation in consequent neurons (Marr, 1971; Anderson et al., 1977). Unless they are cued from some other source, all vectors in this model are initially r</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of NAACL HLT 2007, pages 404–411, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Smolensky</author>
</authors>
<title>Tensor product variable binding and the representation of symbolic structures in connectionist systems.</title>
<date>1990</date>
<journal>Artificial intelligence,</journal>
<pages>46--1</pages>
<marker>Smolensky, 1990</marker>
<rawString>Paul Smolensky. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2):159–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Sturt</author>
<author>Vincent Lombardo</author>
</authors>
<title>Processing coordinate structures: Incrementality and connectedness.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<pages>29--291</pages>
<contexts>
<context position="21470" citStr="Sturt and Lombardo, 2005" startWordPosition="3661" endWordPosition="3664">dicate type referents k (say, cued by a relation of type ‘0’), and if temporal cues are combined in a diagonal product with cues by semantic relations from a common predicate type, the search for a consistent discourse referent can be further constrained to match the gender of a pronoun or other relations from a definite reference. In future work, it would be interesting to compare the predictions of this kind of model to human coreference resolution, particularly in the case of parsing conjunctions with reflexive pronouns, which has been used to argue for fully connected incremental parsing (Sturt and Lombardo, 2005). 7 Conclusion This paper has presented a vectorial leftcorner parsing model defined using independently posited operations over activation-based working memory and weight-based episodic memory. This model has the attractive property that it hypothesizes only one unary branching rule application and only one binary branching rule application per time step, which allows it to be smoothly integrated into a vector-based recurrence that propagates structural ambiguity from one time step to the next. Predictions of this model were calculated on a center-embedded sentence processing task and the mod</context>
</contexts>
<marker>Sturt, Lombardo, 2005</marker>
<rawString>Patrick Sturt and Vincent Lombardo. 2005. Processing coordinate structures: Incrementality and connectedness. Cognitive Science, 29:291–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Tabor</author>
<author>B Galantucci</author>
<author>D Richardson</author>
</authors>
<title>Effects of merely local syntactic coherence on sentence processing.</title>
<date>2004</date>
<journal>Journal of Memory and Language,</journal>
<volume>50</volume>
<issue>4</issue>
<contexts>
<context position="20100" citStr="Tabor et al., 2004" startWordPosition="3436" endWordPosition="3439"> i and n, respectively. Then relation tokens are reified as vectors r, similar to the reification of grammar rules described in Section 4, and connected to relation type vectors n by cued association R and to source and target discourse referents i by cued associations R&apos; and R&amp;quot;. Semantic relation types can then be cued from grammar rules g using associative matrix N, allowing relations of various types to be constructed in cases of superposed grammar rules. In future work, it would be interesting to see whether this representation is consistent with observations of local syntactic coherence (Tabor et al., 2004). This model can also constrain relations to discourse referents introduced in a previous sentence or earlier in the same sentence using a vector of temporal features (Howard and Kahana, 2002). This is a vector of features zt, that has a randomly chosen selection of features randomly resampled at each time step, exponentially decreasing the cosine similarity of the current version of the zt vector to earlier versions zt,. If discourse referents i are cued from the current temporal features zt in an outer product associative matrix Z, it will cue relatively recently mentioned discourse referent</context>
</contexts>
<marker>Tabor, Galantucci, Richardson, 2004</marker>
<rawString>W. Tabor, B. Galantucci, and D Richardson. 2004. Effects of merely local syntactic coherence on sentence processing. Journal of Memory and Language, 50(4):355–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marten van Schijndel</author>
<author>William Schuler</author>
</authors>
<title>An analysis of frequency- and recency-based processing costs.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT 2013. Association for Computational Linguistics.</booktitle>
<marker>van Schijndel, Schuler, 2013</marker>
<rawString>Marten van Schijndel and William Schuler. 2013. An analysis of frequency- and recency-based processing costs. In Proceedings of NAACL-HLT 2013. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marten van Schijndel</author>
<author>Andy Exley</author>
<author>William Schuler</author>
</authors>
<title>A model of language processing as hierarchic sequential prediction.</title>
<date>2013</date>
<journal>Topics in Cognitive Science,</journal>
<volume>5</volume>
<issue>3</issue>
<marker>van Schijndel, Exley, Schuler, 2013</marker>
<rawString>Marten van Schijndel, Andy Exley, and William Schuler. 2013a. A model of language processing as hierarchic sequential prediction. Topics in Cognitive Science, 5(3):522–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marten van Schijndel</author>
<author>Luan Nguyen</author>
<author>William Schuler</author>
</authors>
<title>An analysis of memory-based processing costs using incremental deep syntactic dependency parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of CMCL 2013. Association for Computational Linguistics.</booktitle>
<marker>van Schijndel, Nguyen, Schuler, 2013</marker>
<rawString>Marten van Schijndel, Luan Nguyen, and William Schuler. 2013b. An analysis of memory-based processing costs using incremental deep syntactic dependency parsing. In Proceedings of CMCL 2013. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001888">
<title confidence="0.9968215">
Automated Scoring of Speaking Items in an Assessment for Teachers of
English as a Foreign Language
</title>
<author confidence="0.9324345">
Klaus Zechner, Keelan Evanini, Su-Youn Yoon, Lawrence Davis,
Xinhao Wang, Lei Chen, Chong Min Lee, Chee Wee Leong
</author>
<affiliation confidence="0.836167">
Educational Testing Service (ETS)
</affiliation>
<address confidence="0.672091">
Princeton, NJ 08541, USA
</address>
<email confidence="0.997907">
{kzechner,kevanini,syoon,ldavis,xwang002,lchen,clee001,cleong}@ets.org
</email>
<sectionHeader confidence="0.997373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999993740740741">
This paper describes an end-to-end proto-
type system for automated scoring of spo-
ken responses in a novel assessment for
teachers of English as a Foreign Language
who are not native speakers of English.
The 21 speaking items contained in the as-
sessment elicit both restricted and moder-
ately restricted responses, and their aim is
to assess the essential speaking skills that
English teachers need in order to be effec-
tive communicators in their classrooms.
Our system consists of a state-of-the-art
automatic speech recognizer; multiple fea-
ture generation modules addressing di-
verse aspects of speaking proficiency, such
as fluency, pronunciation, prosody, gram-
matical accuracy, and content accuracy; a
filter that identifies and flags problematic
responses; and linear regression models
that predict response scores based on sub-
sets of the features. The automated speech
scoring system was trained and evaluated
on a data set involving about 1,400 test
takers, and achieved a speaker-level cor-
relation (when scores for all 21 responses
of a speaker are aggregated) with human
expert scores of 0.73.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998000735294118">
As English has become increasingly important as a
language of international business, trade, science,
and communication, efforts to promote teaching
English as a Foreign Language (EFL) have seen
substantially more emphasis in many non-English-
speaking countries worldwide in recent years. In
addition, the prevailing trend in English pedagogy
has been to promote the use of spoken English in
the classroom, as opposed to the respective native
languages of the EFL learners. However, due to
the high demand for EFL teachers in many coun-
tries, the training of these teachers has not always
caught up with these high expectations, so there is
a need for both governmental and private institu-
tions involved in the employment and training of
EFL teachers to assess their competence in the En-
glish language, as well as in English pedagogy.
Against this background, we developed a lan-
guage assessment for EFL teachers who are not
native speakers of English that addresses the four
basic English language skills of Reading, Listen-
ing, Writing and Speaking. This paper focuses
only on the speaking portion of the English assess-
ment, and, in particular, on the system that we de-
veloped to automatically compute scores for test
takers’ spoken responses.
Several significant challenges needed to be ad-
dressed during the course of building this auto-
mated speech scoring system, including, but not
limited to:
• The 21 Speaking items belong to 8 differ-
ent task types with different characteristics;
therefore, we had to select features and build
scoring models for each task type separately.
</bodyText>
<listItem confidence="0.902777466666667">
• The test takers speak a variety of native lan-
guages, and thus have very different non-
native accents in their spoken English. Fur-
thermore, the test takers also exhibit a wide
range of speaking proficiency levels, which
contributes to the diversity of their spoken re-
sponses. Our speech recognizer therefore had
to be trained and adapted to a large database
of non-native speech.
• Since content accuracy is very important for
the types of tasks contained in the test, even
small error rates by the automatic speech
recognition (ASR) system can lead to a no-
ticeable impact on feature performance. This
fact motivated the development of a set of
</listItem>
<page confidence="0.979691">
134
</page>
<note confidence="0.698">
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications, pages 134–142,
Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.8934865">
features that are robust to speech recognition
errors.
</bodyText>
<listItem confidence="0.978888">
• A significant amount of responses (more than
7%) exhibit issues that make them hard or
</listItem>
<bodyText confidence="0.949282692307692">
impossible to score automatically, e.g., high
noise levels, background speech, etc. We
therefore implemented a filter to identify
these non-scorable responses automatically.
The paper is organized as follows: Section 2
discusses related work; in Section 3, we present
the data used for system training and evaluation;
Section 4 describes the system architecture of the
automated speech scoring system. We detail the
methods we used to build our system in Section 5,
followed by an overview of the results in Section
6. Section 7 discusses our findings; finally, Sec-
tion 8 concludes the paper.
</bodyText>
<sectionHeader confidence="0.999916" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999962025641026">
Automated speech processing and scoring tech-
nology has been applied to a variety of domains
over the course of the past two decades, includ-
ing evaluation and tutoring of children’s literacy
skills (Mostow et al., 1994), preparation for high
stakes English proficiency tests for institutions of
higher education (Zechner et al., 2009), evalua-
tion of English skills of foreign-based call center
agents (Chandel et al., 2007), and evaluation of
aviation English (Pearson Education, Inc., 2011),
to name a few (for a comprehensive overview, see
(Eskenazi, 2009)).
Most of these applications elicit restricted
speech from the participants, and the most com-
mon item type by far is the Read Aloud, in which
the speaker reads a sentence or collection of sen-
tences out loud. Due to the constrained nature
of this task, it is possible to develop ASR sys-
tems that are relatively accurate, even with heav-
ily accented non-native speech. Several types of
features related to a non-native speaker’s ability
to produce English sounds and speech patterns
effectively have been extracted from these types
of responses. Some of the best performing of
these types of features include pronunciation fea-
tures, such as a phone’s spectral match to na-
tive speaker acoustic models (Witt, 1999) and a
phone’s duration compared to native speaker mod-
els (Neumeyer et al., 2000); fluency features, such
as the rate of speech, mean pause length, and num-
ber of disfluencies (Cucchiarini et al., 2000); and
prosody features, such as F0 and intensity slope
(Hoenig, 2002).
In addition to the large majority of applications
that elicit restricted speech, a small number of ap-
plications have also investigated automated scor-
ing of non-native spontaneous speech, in order
to more fully evaluate a speaker’s communicative
competence (e.g., (Cucchiarini et al., 2002) and
(Zechner et al., 2009)). In these systems, the same
types of pronunciation, fluency, and prosody fea-
tures can be extracted; furthermore, features re-
lated to additional aspects of a speaker’s profi-
ciency in the non-native language can be extracted,
such as vocabulary usage (Yoon et al., 2012), syn-
tactic complexity (Bernstein et al., 2010a; Chen
and Zechner, 2011), and topical content (Xie et al.,
2012).
As described in Section 1, the domain for the
automated speaking assessment investigated in
this study is teachers of EFL around the world.
Based on the fact that many of the item types are
designed to assess the test taker’s ability to pro-
ductively use English constructions and linguis-
tic units that commonly recur in English teach-
ing environments, several of the item types elicit
semi-restricted speech (see Table 1 below for a de-
scription of the different item types). These types
of responses fall somewhere between the heavily
restricted speech elicited by a Read Aloud task
and unconstrained spontaneous speech. In these
semi-restricted responses, the test taker may be
provided with a set of lexical items that should
be used to form a sentence; in addition, the test
taker is often asked to make the sentence conform
to a given grammatical template. Thus, the re-
sponses provided for a given prompt of this type
by multiple different speakers will often overlap
with each other; however, it is not possible to
specify a complete list of all possible responses.
These types of items have only infrequently been
examined in the context of automated speech scor-
ing. Some related item types that have been
explored previously include the Sentence Build
and Short item types described in (Bernstein et
al., 2010b); however, those item types typically
elicited a much narrower range of responses than
the semi-restricted ones in this study.
</bodyText>
<sectionHeader confidence="0.997911" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.9977005">
The data used in this study was drawn from a pilot
administration of a language assessment for teach-
</bodyText>
<page confidence="0.998189">
135
</page>
<bodyText confidence="0.999714461538462">
ers of English as a Foreign Language. This test
is designed to assess the ability of a non-native
teacher of English to use English in classroom set-
tings. The language forms and functions included
in this test are based on the materials included in a
curriculum that the test takers studied prior to tak-
ing the assessment. The assessment includes items
that cover the four language skills: Reading, Lis-
tening, Writing, and Speaking. There are a total of
8 different types of Speaking items included in the
assessment. These can be divided into the follow-
ing two categories, depending on how constrained
the test taker’s response is:
</bodyText>
<listItem confidence="0.9874952">
• Restricted Speech: In these item types, all
of the linguistic content expected in the
test taker’s response is presented in the test
prompt, and the test taker is asked to read or
repeat it aloud.
• Semi-restricted Speech: In these item types, a
portion of the linguistic content is presented
in the prompt, and the test taker is required to
provide the remaining content to formulate a
complete response.
</listItem>
<bodyText confidence="0.99923637037037">
Sets of 7 Speaking items are presented to the
test taker in thematic units, called “lessons”, based
on their instructional goals; in total, each test taker
completed three lessons, and thus responded to 21
Speaking items. Table 1 presents descriptions of
the 8 different item types included in the assess-
ment.
The numbers of responses provided by the test
takers to each type (along with their respective re-
sponse durations) are as follows: four Multiple
Choice (10 seconds each), six Read Aloud (four 40
second responses and two 60 second responses),
two Repeat Aloud (15 seconds each), one Incom-
plete Sentence (20 seconds), one Key Words (15
seconds), five Chart (four 20 seconds and one 40
seconds), one Keyword Chart (15 seconds), and
one Visuals (15 seconds). Thus, each test taker
provided a total of approximately 9 minutes of au-
dio.
The responses were all double-scored by trained
human raters on a three-point scale (1 - 3). For
the Restricted Speech items, the raters assessed
the test taker’s pronunciation, pacing, and intona-
tion. For the Semi-restricted Speech items, the re-
sponses were also scored holistically on a 3-point
scale, but raters were also asked to take into ac-
count the appropriateness of the language used
</bodyText>
<table confidence="0.843996833333333">
Restricted Speech
Type Description
Multiple The test taker selects the correct
Choice option and reads it aloud
(MC)
Read Aloud The test taker reads aloud a set
(RA) of classroom instructions
Repeat The test taker listens to a student
Aloud (RP) utterance twice and then repeats
it
Semi-restricted Speech
Type Description
</table>
<bodyText confidence="0.9775145">
Incomplete The test taker is given a sentence
Sentence fragment and completes the sen-
(IS) tence according to the instruc-
tions
Key Words The test taker uses the key words
(KW) provided to speak a sentence as
instructed
Chart (CH) The test taker uses an example
from a language chart and then
formulates a similar sentence us-
ing a given grammatical pattern
Keyword The test taker constructs a sen-
Chart (KC) tence using keywords provided
and information in a chart
Visuals (VI) The test taker is given two visu-
als and is asked to give instruc-
tions to students based on the
graphical information
</bodyText>
<tableCaption confidence="0.5963355">
Table 1: Types of speaking items included in the
assessment
</tableCaption>
<bodyText confidence="0.999886625">
(e.g., grammatical accuracy and content correct-
ness) in addition to aspects of fluency and pronun-
ciation. For some responses, the raters were not
able to provide a score on the 1 - 3 scale, e.g.,
because the audio response contained no speech
input, the test taker responded in their native lan-
guage, etc. These responses are labeled NS for
Non-Scoreable.
After receiving scores, all of the responses
were transcribed using standard English orthogra-
phy (disfluencies, such as filled pauses and par-
tial words are also included in the transcriptions).
Then, the responses were partitioned (with no
speaker overlap) into five sets for the training and
evaluation of the ASR system and the linear re-
gression scoring models. The amount of data and
</bodyText>
<page confidence="0.994365">
136
</page>
<bodyText confidence="0.9627465">
human score distributions in each of these parti-
tions are displayed in Table 2.
</bodyText>
<sectionHeader confidence="0.966727" genericHeader="method">
4 System Architecture
</sectionHeader>
<bodyText confidence="0.978232166666667">
The automated scoring system used for the teach-
ers’ spoken language assessment consists of the
following four components, which are invoked
one after the other in a pipeline fashion (ETS
SpeechRatersm, (Zechner et al., 2009; Higgins et
al., 2011)):
</bodyText>
<listItem confidence="0.987340230769231">
• an automated speech recognizer, generating
word hypotheses from input audio recordings
of the test takers’ responses
• a feature computation module that generates
features based on the ASR output, e.g., mea-
suring fluency, pronunciation, prosody, and
content accuracy
• a filtering model that flags responses that
should not be scored automatically due to is-
sues with audio quality, empty responses, etc.
• linear regression scoring models that predict
the score for each response based on a set of
selected features
</listItem>
<bodyText confidence="0.999953217391304">
Furthermore, we use Praat (Boersma and
Weenick, 2012) to extract power and pitch from
the speech signal; this information is used for
some of the feature computation modules, as well
as for the filtering model.
The ASR is an HMM-based triphone system
trained on approximately 800 hours of non-native
speech from a different data set; a background
Language Model (LM) was also trained on the
same data set. Subsequently, 8 adapted LMs were
trained (with an interpolation weight of 0.9 for the
in-domain data) using the responses in the ASR
Training partition for the 8 different item types
listed in Table 1. The ASR system obtained an
overall word error rate (WER) of 13.0% on the
ASR Evaluation partition and 15.6% on the Model
Evaluation partition. As would be expected, the
ASR system performed best on the responses that
were most restricted by the test item and per-
formed worse on the responses that were less re-
stricted. The WER ranged from 11.4% for the
RA responses to 41.4% for the IS responses in the
Model Evaluation partition.
</bodyText>
<sectionHeader confidence="0.998514" genericHeader="method">
5 Methodology
</sectionHeader>
<subsectionHeader confidence="0.991576">
5.1 Speech features
</subsectionHeader>
<bodyText confidence="0.9986145">
The feature computation components of our
speech scoring system compute more than 100
features based on a speaker’s response. They be-
long to the following broad dimensions of speak-
ing proficiency: fluency, pronunciation, prosody,
vocabulary usage, grammatical complexity and
accuracy, and content accuracy (Zechner et al.,
2009; Chen and Yoon, 2012; Chen et al., 2009;
Zechner et al., 2011; Yoon et al., 2012; Yoon and
Bhat, 2012; Zechner and Wang, 2013).
After initial feature generation, we selected a set
of about 10 features for each of the 8 item types,
based on the following considerations1 (Zechner
et al., 2009; Xi et al., 2008):
</bodyText>
<listItem confidence="0.9905935">
• empirical performance, i.e., feature correla-
tion with human scores
• construct2 relevance, i.e., to what extent the
feature measures aspects of speaking profi-
ciency that are considered to be relevant and
important by content experts
• overall construct coverage, i.e., the feature set
should include features from all relevant con-
struct dimensions
• feature independence, i.e., the inter-
correlation between any two features of the
set should be low
</listItem>
<bodyText confidence="0.9984974">
Furthermore, some features were transformed
(e.g., by applying the inverse or log function), in
order to increase the normality of their distribu-
tions (an assumption of linear regression classi-
fiers). All feature values that exceeded a thresh-
old of 4 standard deviations from the mean were
replaced by the respective threshold (outlier trun-
cation).
The composition of feature sets is slightly dif-
ferent for the two item type categories: for the 3
restricted item types, features related to fluency,
pronunciation, prosody and read/repeat accuracy
were chosen, whereas for the 5 semi-restricted
item types, vocabulary and grammar features were
also added to the set. Further, while accuracy
</bodyText>
<footnote confidence="0.9977998">
1While automated feature selection is conceivable in prin-
ciple, in our experience it typically does not result in a feature
set that meets all of these criteria well.
2A construct is the set of knowledge, skills, and abilities
measured by a test.
</footnote>
<page confidence="0.956777">
137
</page>
<table confidence="0.999777428571429">
Partition Spk. Resp. Dur. 1 2 3 NS
ASR Training 773 16,049 116.7 1,587 (9.9) 4,086 (25.5) 8,796 (54.8) 1,580 (9.8)
ASR Development 25 525 3.8 53 (10.1) 133 (25.3) 327 (62.3) 12 (2.3)
ASR Evaluation 25 525 3.8 31 (5.9) 114 (21.7) 326 (62.1) 54 (10.3)
Model Training 300 6,300 45.8 675 (10.7) 1,715 (27.2) 3,577 (56.8) 333 (5.3)
Model Evaluation 300 6,300 45.7 647 (10.3) 1,637 (26.0) 3,487 (55.3) 529 (8.4)
Total 1,423 29,699 215.8 2,993 (9.38) 7,685 (25.14) 16,513 (58.26) 2,508 (7.22)
</table>
<tableCaption confidence="0.90217">
Table 2: Amount of data contained in each partition (speakers, responses, hours of speech) and distribu-
tion of human scores (percentages of scores per partition in brackets).
</tableCaption>
<bodyText confidence="0.995502333333333">
features for the restricted items were based only
on string alignment measures, content accuracy
features for the semi-restricted items were more
diverse, e.g., based on regular expressions, key-
words, and language model scores (Zechner and
Wang, 2013). Table 3 lists the features that were
used in the scoring models for restricted and semi-
restricted item types, along with sub-constructs
they measure and their description.
</bodyText>
<subsectionHeader confidence="0.992492">
5.2 Filtering model
</subsectionHeader>
<bodyText confidence="0.9999804375">
In order to automatically identify responses that
have technical issues (e.g., loud background noise)
or are otherwise not scorable (e.g., empty re-
sponses), a decision tree-based filtering model was
developed using a combination of features derived
from ASR output and from pitch and energy in-
formation (Yoon et al., 2011; Jeon and Yoon,
2012). The filtering model was tested on the scor-
ing model evaluation data, and obtained an ac-
curacy rate (the exact agreement between the fil-
tering model and a human rater concerning the
distinction between scorable and non-scorable re-
sponses) of 97%; it correctly identified 90% of the
non-scorable responses in the data set with a false
positive rate of 21% (recall=0.90, precision=0.79,
F-score=0.84).
</bodyText>
<subsectionHeader confidence="0.999088">
5.3 Scoring models
</subsectionHeader>
<bodyText confidence="0.999324384615385">
We used the Model Training set to train 8 linear
regression models for the 8 different item types,
using the previously determined feature sets. We
used the features as independent variables in these
models and the summed scores of two human
raters as the dependent variable. These trained
scoring models were then employed to score re-
sponses of the Model Evaluation data (exclud-
ing responses marked as non-scorable by human
raters) and rounded to the nearest integer to predict
the final scores for each response. These scores
were then evaluated against the first human rater
score (H1).
</bodyText>
<table confidence="0.999737888888889">
Item N S-H1 H1-H2 WER (%)
RA 1653 0.34 0.51 11.4
RP 543 0.41 0.73 21.8
MC 1036 0.67 0.83 17.1
CH 1372 0.44 0.67 26.3
KW 275 0.45 0.67 28.7
KC 274 0.57 0.74 28.8
IS 260 0.46 0.69 41.4
VI 272 0.43 0.80 30.4
</table>
<tableCaption confidence="0.995531">
Table 4: Correlations between system and first hu-
</tableCaption>
<bodyText confidence="0.99350225">
man rater (S-H1) and between two human raters
(H1-H2), for all responses of each item type in the
Model Evaluation partition (N). The last column
provides the average ASR word error rate (WER)
in percent.
Additionally, for responses flagged as non-
scorable by the automatic filtering model, the sec-
ond human rater score (H2) was used as final
item score in order to mimic the operational sce-
nario where human raters score responses that are
flagged by the filtering model.
We also compute the agreement between sys-
tem and human raters based on a set of all 21 re-
sponses of a speaker. Score imputation was used
for responses that were labeled as non-scorable by
both the system and H2; in this case, the response
was given the mean score of the total scorable
responses from the same speaker. Similarly, the
same score imputation rule was applied to the H1
scores.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.999941">
Table 4 presents the Pearson correlation coeffi-
cients between human and automated scores for
the responses from the 8 different item types along
with the human-human correlation for each item
type. Furthermore, we also provide the word error
rates of the ASR system for the same 8 item types
in the last column of the table.
</bodyText>
<page confidence="0.995449">
138
</page>
<table confidence="0.999921833333333">
Feature Sub-construct Description
Content Ed1 Read/repeat accu- Correctly read words per minute
racy / Fluency
Content Ed2 Read/repeat accu- Read/repeat word error rate
racy
Content RegEx Content accuracy Matching of regular expressions
Content WER Content accuracy Response discrepancy from high scoring responses
Content NGram Content accuracy N-grams in response matching high scoring response n-
grams
Fluency Rate Fluency Speaking rate
Fluency Chunk Fluency Average length of contiguous word chunks
Fluency Sil1 Fluency Frequency of long silences
Fluency Sil2 Fluency / Grammar Proportion of long within-clause-silences to all within-
clause-silences
Fluency Sil3 Fluency Mean length of silences within a clause
Fluency Disfl1 Fluency Frequency of interruption points (repair, repetition, false
start)
Fluency Disfl2 Fluency Number of disfluencies per second
Fluency Disfl3 Fluency Frequency of repetitions
Pron Vowels Pronunciation Average vowel duration differences relative to a native-
speaker model
Prosody1 Prosody Percentage of stressed syllables
Prosody2 Prosody Mean deviation of time intervals between stressed syllables
Prosody3 Prosody Mean distance between stressed syllables
Vocab1 Vocabulary / Flu- Number of word types divided by utterance duration
ency
Grammar POS Grammar Part-of-speech based distributional similarity score be-
tween a response and responses with different score levels
Grammar LM Grammar Global language model score (normalized by response
length)
</table>
<tableCaption confidence="0.9811005">
Table 3: List of features used for item type scoring models, with the sub-constructs they represent and
descriptions.
</tableCaption>
<page confidence="0.95602">
139
</page>
<table confidence="0.8850435">
Comparison Pearson r
S-H1 0.725
S-H2 0.742
H1-H2 0.934
</table>
<tableCaption confidence="0.953355666666667">
Table 5: Speaker-level performance (Pearson r
correlations) computed over the sum of all 21
scores from each speaker, N=272
</tableCaption>
<table confidence="0.999800285714286">
Sub-construct Restricted Semi-restricted
Content 0.33 –0.67 0.34 –0.61
Fluency 0.19 –0.33 0.20 –0.33
Pronunciation 0.20 –0.22 0.13 –0.31
Prosody 0.18 –0.24 0.12 –0.27
Grammar – 0.23 –0.49
Vocabulary – 0.21 –0.32
</table>
<tableCaption confidence="0.983367">
Table 6: Range of Pearson r correlations for dif-
</tableCaption>
<bodyText confidence="0.944174210526316">
ferent features with human scores (H1) by sub-
construct for restricted and semi-restricted item
types.
Table 5 presents the Pearson correlation coeffi-
cients between the speaker-level scores produced
by the automated scoring system (S) and the two
sets of human scores (H1 and H2). These speaker-
level scores were computed based on the sum of
all 21 scores from each speaker in the Model Eval-
uation partition. Responses that received a non-
scorable rating from the human raters were im-
puted, as described above. Furthermore, 28 speak-
ers were excluded from this analysis because they
had more than 7 non-scorable responses each.3
Finally, Table 6 provides an overview of Pear-
son correlation ranges with human rater scores
(H1) for the different features used in the scoring
models, summarized by the sub-constructs that the
features represent.
</bodyText>
<sectionHeader confidence="0.997742" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.99965375">
When looking at Table 4, we see that the inter-rater
reliability for human raters ranges between 0.51
(for RA items) and 0.83 (for MC items). Inter-
rater reliability varies less for the 5 semi-restricted
item types (0.67–0.80), compared to the 3 re-
stricted item types (0.51–0.83). As for automated
score correlations with human raters, the Pearson
r coefficients range from 0.34 (RA) to 0.67 (MC).
</bodyText>
<footnote confidence="0.962816">
3In an operational setting, these test takers would not re-
ceive a test score; instead, they would have the opportunity to
take the test again.
</footnote>
<bodyText confidence="0.999954352941177">
Again, the variability of Pearson r coefficients is
larger for the 3 restricted item types (0.34–0.67)
than for the 5 semi-restricted item types (0.43–
0.57). The degradation in correlation between the
inter-human results and the machine-human re-
sults varies from 0.16 (MC) to 0.37 (VI).
Speech recognition word error rate does not
seem to have a strong influence on model perfor-
mance (RA items have the lowest WER with S-
H1 r=0.34, but r=0.46 for IS items that have the
highest WER). However, we found other factors
that affect model performance negatively; for ex-
ample, multiple repeats of responses by test tak-
ers contribute to the large performance difference
between S-H1 and H1-H2 for the RP items. In
general, we conjecture that using features for a
larger set of sub-construct areas—in the case of
semi-restricted item types—may contribute to the
lower variation of scoring model performance for
this subset of the data.
As for speaker-level results (Table 5), the over-
all degradation between the inter-human correla-
tion and the system-human correlations is of a
similar magnitude (around 0.2) as observed for
most of the individual item types. Still, the
speaker-level correlation of 0.73 is 0.26 higher
than the average item type correlation between the
system and H1.
When we look into more detail at the Pearson
r correlations between individual features used in
the item type scoring models and human scores
(Table 6), we can see that features related to con-
tent accuracy exhibit a substantially stronger per-
formance (r=0.33–0.67) than features related to
most other sub-constructs of speaking proficiency,
namely fluency, pronunciation, prosody, and vo-
cabulary (r ∼ 0.2). One exception is features
related to grammar, where correlations with hu-
man scores are as high as 0.49. Since related work
on scoring speech using features indicative of flu-
ency, pronunciation, etc. showed higher correla-
tions (e.g., (Cucchiarini et al., 1997; Franco et al.,
2000; Zechner et al., 2009)), we conjecture that
the reason behind this difference is likely to be
found in the fact that the responses in this assess-
ment for teachers of English are quite short (6–
14 words on average for all items except for Read
Aloud items that are about 46 words on average).
Since content features are less reliant on longer
stretches of speech, they still work fairly well for
most items in our corpus.
</bodyText>
<page confidence="0.989116">
140
</page>
<bodyText confidence="0.999893888888889">
Finally, while the proportion of words contained
in responses in restricted items is much larger than
those contained in responses in semi-restricted
items, these two item type categories are more
evenly distributed over the whole test, i.e., each
test taker responds to 9 semi-restricted and 12 re-
stricted items, and the item scores are then aggre-
gated for a final score with equal weight given to
each item score.
</bodyText>
<sectionHeader confidence="0.998507" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999982684210526">
This paper presented an overview of an automated
speech scoring system that was developed for a
language assessment for teachers of English as a
Foreign Language (EFL) whose native language
is not English. We described the main compo-
nents of this prototype system and their perfor-
mance: the ASR system, features generated from
ASR output, a filtering model to flag non-scorable
responses, and finally a set of linear regression
models, one for each of 8 different types of test
items.
We found that overall, the correlation between
our speech scoring system’s predicted scores and
human rater scores range between 0.34 and 0.67,
evaluated on responses from 8 item types. Further-
more, we found that correlations based on com-
plete sets of 21 spoken responses per test taker im-
prove to around r = 0.73.
Given the many significant challenges of this
work, including 8 different item types in the as-
sessment, responses from speakers from different
native languages and speaking proficiency levels,
sub-optimal audio conditions for a part of the data,
and a relatively small data set for both ASR system
adaptation and linear regression model training,
we find that the overall performance achieved by
our automated speech scoring system was a good
starting point for an eventual deployment in a low-
stakes assessment context.
Future work will aim at improving the perfor-
mance of the prediction models by the addition of
more features addressing different aspects of the
construct as well as an improved filtering model
for flagging the different types of problematic re-
sponses. Furthermore, agreement between human
raters, in particular for read-aloud items, could be
improved by refining rater rubrics and additional
rater training and monitoring.
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998828">
The authors would like to thank Anastassia Louk-
ina and Jidong Tao for their comments on an ear-
lier version of this paper, and are also indebted
to the anonymous reviewers of BEA-9 and ASRU
2013 for their valuable comments and suggestions.
</bodyText>
<sectionHeader confidence="0.998899" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.971482652173913">
Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010a. Fluency and structural complexity as pre-
dictors of L2 oral proficiency. In Proceedings of In-
terspeech.
Jared Bernstein, Alistair Van Moere, and Jian Cheng.
2010b. Validating automated speaking tests. Lan-
guage Testing, 27(3):355–377.
Paul Boersma and David Weenick. 2012. Praat: Doing
phonetics by computer, version 5.3.32. http://
www.praat.org.
Abhishek Chandel, Abhinav Parate, Maymon Ma-
dathingal, Himanshu Pant, Nitendra Rajput, Shajith
Ikbal, Om Deshmuck, and Ashish Verma. 2007.
Sensei: Spoken language assessment for call cen-
ter agents. In Proceedings of the IEEE Workshop on
Automatic Speech Recognition and Understanding
(ASRU).
Lei Chen and Su-Youn Yoon. 2012. Application of
structural events detected on ASR outputs for auto-
mated speaking assessment. In Proceedings of In-
terspeech.
Miao Chen and Klaus Zechner. 2011. Computing
and evaluating syntactic complexity features for au-
tomated scoring of spontaneous non-native speech.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, pages 722–
731.
Lei Chen, Klaus Zechner, and Xiaoming Xi. 2009. Im-
proved pronunciation features for construct-driven
assessment of non-native spontaneous speech. In
Proceedings of NAACL-HLT.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 1997.
Automatic evaluation of Dutch pronunciation by us-
ing speech recognition technology. In Proceedings
of the IEEE Workshop on Auotmatic Speech Recog-
nition and Understanding (ASRU).
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learn-
ers’ fluency by means of automatic speech recogni-
tion technology. Journal of the Acoustical Society of
America, 107(2):989–999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learn-
ers’ fluency: Comparisons between read and spon-
taneous speech. Journal of the Acoustical Society of
America, 111(6):2862–2873.
</reference>
<page confidence="0.986095">
141
</page>
<bodyText confidence="0.75695275">
Maxine Eskenazi. 2009. An overview of spoken lan- Conference on Empirical Methods in Natural Lan-
guage technology for education. Speech Communi- guage Processing and Computational Natural Lan-
cation, 51(10):832–844. guage Learning, pages 600–608, Jeju Island, Korea.
Association for Computational Linguistics.
</bodyText>
<reference confidence="0.999661342105263">
Horacio Franco, Leonardo Neumeyer, Vassilios Di-
galakis, and Orith Ronen. 2000. Combination of
machine scores for automatic grading of pronuncia-
tion quality. Speech Communication, 30(1-2):121–
130.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David M. Williamson. 2011. A three-stage ap-
proach to the automated scoring of spontaneous spo-
ken responses. Computer Speech and Language,
25(2):282–306.
Florian Hoenig. 2002. Automatic assessment of non-
native prosody – Annotation, modelling, and evalu-
ation. In Proceedings of the International Sympo-
sium on Automatic Detection of Errors in Pronun-
ciation Training (ISADEPT), pages 21–30, Stock-
holm, Sweden.
Je Hun Jeon and Su-Youn Yoon. 2012. Acoustic
feature-based non-scorable response detection for an
automated speaking proficiency assessment. In Pro-
ceedings of Interspeech.
Jack Mostow, Steven F. Roth, Alexander G. Haupt-
mann, and Matthew Kane. 1994. A prototype read-
ing coach that listens. In Proceedings of the Twelfth
National Conference on Artificial Intelligence.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, 30:83–93.
Pearson Education, Inc. 2011. VersantTM
Aviation English Test. http://www.
versanttest.com/technology/
VersantAviationEnglishTestValidation.
pdf.
Silke Witt. 1999. Use of speech recognition in
computer-assisted language learning. Ph.D. thesis,
Cambridge University.
Xiaoming Xi, Derrick Higgins, Klaus Zechner, and
David M. Williamson. 2008. Automated scoring of
spontaneous speech using SpeechRater v1.0. Edu-
cational Testing Service Research Report RR-08-62.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 103–111, Montr´eal, Canada. Asso-
ciation for Computational Linguistics.
Su-Youn Yoon and Suma Bhat. 2012. Assessment of
ESL learners’ syntactic competence based on sim-
ilarity measures. In Proceedings of the 2012 Joint
Su-Youn Yoon, Keelan Evanini, and Klaus Zechner.
2011. Non-scorable response detection for auto-
mated speaking proficiency assessment. In Proceed-
ings of NAACL-HLT Workshop on Innovative Use of
NLP for Building Educational Applications.
Su-Youn Yoon, Suma Bhat, and Klaus Zechner. 2012.
Vocabulary profile as a measure of vocabulary so-
phistication. In Proceedings of the 7th Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, NAACL-HLT, Montr´eal, Canada. Associ-
ation for Computational Linguistics.
Klaus Zechner and Xinhao Wang. 2013. Automated
content scoring of spoken responses in an assess-
ment for teachers of english. In Proceedings of
the 8th Workshop on Innovative Use of NLP for
Building Educational Applications, NAACL-HLT,
Atlanta. Association for Computational Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring
of non-native spontaneous speech in tests of spoken
English. Speech Communication, 51(10):883–895.
Klaus Zechner, Xiaoming Xi, and Lei Chen. 2011.
Evaluating prosodic features for automated scoring
of non-native read speech. In Proceedings of the
IEEE Workshop on Automatic Speech Recognition
and Understanding (ASRU).
</reference>
<page confidence="0.997702">
142
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.857071">
<title confidence="0.998585">Automated Scoring of Speaking Items in an Assessment for Teachers English as a Foreign Language</title>
<author confidence="0.9964115">Klaus Zechner</author>
<author confidence="0.9964115">Keelan Evanini</author>
<author confidence="0.9964115">Su-Youn Yoon</author>
<author confidence="0.9964115">Lawrence Xinhao Wang</author>
<author confidence="0.9964115">Lei Chen</author>
<author confidence="0.9964115">Chong Min Lee</author>
<author confidence="0.9964115">Chee Wee</author>
<affiliation confidence="0.939822">Educational Testing Service</affiliation>
<address confidence="0.993562">Princeton, NJ 08541,</address>
<abstract confidence="0.997220785714286">This paper describes an end-to-end prototype system for automated scoring of spoken responses in a novel assessment for teachers of English as a Foreign Language who are not native speakers of English. The 21 speaking items contained in the assessment elicit both restricted and moderately restricted responses, and their aim is to assess the essential speaking skills that English teachers need in order to be effective communicators in their classrooms. Our system consists of a state-of-the-art automatic speech recognizer; multiple feature generation modules addressing diverse aspects of speaking proficiency, such as fluency, pronunciation, prosody, grammatical accuracy, and content accuracy; a filter that identifies and flags problematic responses; and linear regression models that predict response scores based on subsets of the features. The automated speech scoring system was trained and evaluated on a data set involving about 1,400 test takers, and achieved a speaker-level correlation (when scores for all 21 responses of a speaker are aggregated) with human expert scores of 0.73.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jared Bernstein</author>
<author>Jian Cheng</author>
<author>Masanori Suzuki</author>
</authors>
<title>Fluency and structural complexity as predictors of L2 oral proficiency.</title>
<date>2010</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="6817" citStr="Bernstein et al., 2010" startWordPosition="1072" endWordPosition="1075">n to the large majority of applications that elicit restricted speech, a small number of applications have also investigated automated scoring of non-native spontaneous speech, in order to more fully evaluate a speaker’s communicative competence (e.g., (Cucchiarini et al., 2002) and (Zechner et al., 2009)). In these systems, the same types of pronunciation, fluency, and prosody features can be extracted; furthermore, features related to additional aspects of a speaker’s proficiency in the non-native language can be extracted, such as vocabulary usage (Yoon et al., 2012), syntactic complexity (Bernstein et al., 2010a; Chen and Zechner, 2011), and topical content (Xie et al., 2012). As described in Section 1, the domain for the automated speaking assessment investigated in this study is teachers of EFL around the world. Based on the fact that many of the item types are designed to assess the test taker’s ability to productively use English constructions and linguistic units that commonly recur in English teaching environments, several of the item types elicit semi-restricted speech (see Table 1 below for a description of the different item types). These types of responses fall somewhere between the heavil</context>
<context position="8200" citStr="Bernstein et al., 2010" startWordPosition="1302" endWordPosition="1305"> set of lexical items that should be used to form a sentence; in addition, the test taker is often asked to make the sentence conform to a given grammatical template. Thus, the responses provided for a given prompt of this type by multiple different speakers will often overlap with each other; however, it is not possible to specify a complete list of all possible responses. These types of items have only infrequently been examined in the context of automated speech scoring. Some related item types that have been explored previously include the Sentence Build and Short item types described in (Bernstein et al., 2010b); however, those item types typically elicited a much narrower range of responses than the semi-restricted ones in this study. 3 Data The data used in this study was drawn from a pilot administration of a language assessment for teach135 ers of English as a Foreign Language. This test is designed to assess the ability of a non-native teacher of English to use English in classroom settings. The language forms and functions included in this test are based on the materials included in a curriculum that the test takers studied prior to taking the assessment. The assessment includes items that co</context>
</contexts>
<marker>Bernstein, Cheng, Suzuki, 2010</marker>
<rawString>Jared Bernstein, Jian Cheng, and Masanori Suzuki. 2010a. Fluency and structural complexity as predictors of L2 oral proficiency. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jared Bernstein</author>
<author>Alistair Van Moere</author>
<author>Jian Cheng</author>
</authors>
<title>Validating automated speaking tests.</title>
<date>2010</date>
<journal>Language Testing,</journal>
<volume>27</volume>
<issue>3</issue>
<marker>Bernstein, Van Moere, Cheng, 2010</marker>
<rawString>Jared Bernstein, Alistair Van Moere, and Jian Cheng. 2010b. Validating automated speaking tests. Language Testing, 27(3):355–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Boersma</author>
<author>David Weenick</author>
</authors>
<title>Praat: Doing phonetics by computer, version 5.3.32.</title>
<date>2012</date>
<note>http:// www.praat.org.</note>
<contexts>
<context position="13358" citStr="Boersma and Weenick, 2012" startWordPosition="2157" endWordPosition="2160">atersm, (Zechner et al., 2009; Higgins et al., 2011)): • an automated speech recognizer, generating word hypotheses from input audio recordings of the test takers’ responses • a feature computation module that generates features based on the ASR output, e.g., measuring fluency, pronunciation, prosody, and content accuracy • a filtering model that flags responses that should not be scored automatically due to issues with audio quality, empty responses, etc. • linear regression scoring models that predict the score for each response based on a set of selected features Furthermore, we use Praat (Boersma and Weenick, 2012) to extract power and pitch from the speech signal; this information is used for some of the feature computation modules, as well as for the filtering model. The ASR is an HMM-based triphone system trained on approximately 800 hours of non-native speech from a different data set; a background Language Model (LM) was also trained on the same data set. Subsequently, 8 adapted LMs were trained (with an interpolation weight of 0.9 for the in-domain data) using the responses in the ASR Training partition for the 8 different item types listed in Table 1. The ASR system obtained an overall word error</context>
</contexts>
<marker>Boersma, Weenick, 2012</marker>
<rawString>Paul Boersma and David Weenick. 2012. Praat: Doing phonetics by computer, version 5.3.32. http:// www.praat.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhishek Chandel</author>
</authors>
<title>Abhinav Parate, Maymon Madathingal, Himanshu Pant, Nitendra Rajput, Shajith Ikbal, Om Deshmuck, and Ashish Verma.</title>
<date>2007</date>
<booktitle>In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).</booktitle>
<marker>Chandel, 2007</marker>
<rawString>Abhishek Chandel, Abhinav Parate, Maymon Madathingal, Himanshu Pant, Nitendra Rajput, Shajith Ikbal, Om Deshmuck, and Ashish Verma. 2007. Sensei: Spoken language assessment for call center agents. In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Chen</author>
<author>Su-Youn Yoon</author>
</authors>
<title>Application of structural events detected on ASR outputs for automated speaking assessment.</title>
<date>2012</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="14726" citStr="Chen and Yoon, 2012" startWordPosition="2383" endWordPosition="2386"> on the responses that were most restricted by the test item and performed worse on the responses that were less restricted. The WER ranged from 11.4% for the RA responses to 41.4% for the IS responses in the Model Evaluation partition. 5 Methodology 5.1 Speech features The feature computation components of our speech scoring system compute more than 100 features based on a speaker’s response. They belong to the following broad dimensions of speaking proficiency: fluency, pronunciation, prosody, vocabulary usage, grammatical complexity and accuracy, and content accuracy (Zechner et al., 2009; Chen and Yoon, 2012; Chen et al., 2009; Zechner et al., 2011; Yoon et al., 2012; Yoon and Bhat, 2012; Zechner and Wang, 2013). After initial feature generation, we selected a set of about 10 features for each of the 8 item types, based on the following considerations1 (Zechner et al., 2009; Xi et al., 2008): • empirical performance, i.e., feature correlation with human scores • construct2 relevance, i.e., to what extent the feature measures aspects of speaking proficiency that are considered to be relevant and important by content experts • overall construct coverage, i.e., the feature set should include feature</context>
</contexts>
<marker>Chen, Yoon, 2012</marker>
<rawString>Lei Chen and Su-Youn Yoon. 2012. Application of structural events detected on ASR outputs for automated speaking assessment. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miao Chen</author>
<author>Klaus Zechner</author>
</authors>
<title>Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>722--731</pages>
<contexts>
<context position="6843" citStr="Chen and Zechner, 2011" startWordPosition="1076" endWordPosition="1079">f applications that elicit restricted speech, a small number of applications have also investigated automated scoring of non-native spontaneous speech, in order to more fully evaluate a speaker’s communicative competence (e.g., (Cucchiarini et al., 2002) and (Zechner et al., 2009)). In these systems, the same types of pronunciation, fluency, and prosody features can be extracted; furthermore, features related to additional aspects of a speaker’s proficiency in the non-native language can be extracted, such as vocabulary usage (Yoon et al., 2012), syntactic complexity (Bernstein et al., 2010a; Chen and Zechner, 2011), and topical content (Xie et al., 2012). As described in Section 1, the domain for the automated speaking assessment investigated in this study is teachers of EFL around the world. Based on the fact that many of the item types are designed to assess the test taker’s ability to productively use English constructions and linguistic units that commonly recur in English teaching environments, several of the item types elicit semi-restricted speech (see Table 1 below for a description of the different item types). These types of responses fall somewhere between the heavily restricted speech elicit</context>
</contexts>
<marker>Chen, Zechner, 2011</marker>
<rawString>Miao Chen and Klaus Zechner. 2011. Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 722– 731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Chen</author>
<author>Klaus Zechner</author>
<author>Xiaoming Xi</author>
</authors>
<title>Improved pronunciation features for construct-driven assessment of non-native spontaneous speech.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="14745" citStr="Chen et al., 2009" startWordPosition="2387" endWordPosition="2390">t were most restricted by the test item and performed worse on the responses that were less restricted. The WER ranged from 11.4% for the RA responses to 41.4% for the IS responses in the Model Evaluation partition. 5 Methodology 5.1 Speech features The feature computation components of our speech scoring system compute more than 100 features based on a speaker’s response. They belong to the following broad dimensions of speaking proficiency: fluency, pronunciation, prosody, vocabulary usage, grammatical complexity and accuracy, and content accuracy (Zechner et al., 2009; Chen and Yoon, 2012; Chen et al., 2009; Zechner et al., 2011; Yoon et al., 2012; Yoon and Bhat, 2012; Zechner and Wang, 2013). After initial feature generation, we selected a set of about 10 features for each of the 8 item types, based on the following considerations1 (Zechner et al., 2009; Xi et al., 2008): • empirical performance, i.e., feature correlation with human scores • construct2 relevance, i.e., to what extent the feature measures aspects of speaking proficiency that are considered to be relevant and important by content experts • overall construct coverage, i.e., the feature set should include features from all relevant</context>
</contexts>
<marker>Chen, Zechner, Xi, 2009</marker>
<rawString>Lei Chen, Klaus Zechner, and Xiaoming Xi. 2009. Improved pronunciation features for construct-driven assessment of non-native spontaneous speech. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catia Cucchiarini</author>
<author>Helmer Strik</author>
<author>Lou Boves</author>
</authors>
<title>Automatic evaluation of Dutch pronunciation by using speech recognition technology.</title>
<date>1997</date>
<booktitle>In Proceedings of the IEEE Workshop on Auotmatic Speech Recognition and Understanding (ASRU).</booktitle>
<contexts>
<context position="25702" citStr="Cucchiarini et al., 1997" startWordPosition="4141" endWordPosition="4144">son r correlations between individual features used in the item type scoring models and human scores (Table 6), we can see that features related to content accuracy exhibit a substantially stronger performance (r=0.33–0.67) than features related to most other sub-constructs of speaking proficiency, namely fluency, pronunciation, prosody, and vocabulary (r ∼ 0.2). One exception is features related to grammar, where correlations with human scores are as high as 0.49. Since related work on scoring speech using features indicative of fluency, pronunciation, etc. showed higher correlations (e.g., (Cucchiarini et al., 1997; Franco et al., 2000; Zechner et al., 2009)), we conjecture that the reason behind this difference is likely to be found in the fact that the responses in this assessment for teachers of English are quite short (6– 14 words on average for all items except for Read Aloud items that are about 46 words on average). Since content features are less reliant on longer stretches of speech, they still work fairly well for most items in our corpus. 140 Finally, while the proportion of words contained in responses in restricted items is much larger than those contained in responses in semi-restricted it</context>
</contexts>
<marker>Cucchiarini, Strik, Boves, 1997</marker>
<rawString>Catia Cucchiarini, Helmer Strik, and Lou Boves. 1997. Automatic evaluation of Dutch pronunciation by using speech recognition technology. In Proceedings of the IEEE Workshop on Auotmatic Speech Recognition and Understanding (ASRU).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catia Cucchiarini</author>
<author>Helmer Strik</author>
<author>Lou Boves</author>
</authors>
<title>Quantitative assessment of second language learners’ fluency by means of automatic speech recognition technology.</title>
<date>2000</date>
<journal>Journal of the Acoustical Society of America,</journal>
<volume>107</volume>
<issue>2</issue>
<contexts>
<context position="6114" citStr="Cucchiarini et al., 2000" startWordPosition="963" endWordPosition="966">tems that are relatively accurate, even with heavily accented non-native speech. Several types of features related to a non-native speaker’s ability to produce English sounds and speech patterns effectively have been extracted from these types of responses. Some of the best performing of these types of features include pronunciation features, such as a phone’s spectral match to native speaker acoustic models (Witt, 1999) and a phone’s duration compared to native speaker models (Neumeyer et al., 2000); fluency features, such as the rate of speech, mean pause length, and number of disfluencies (Cucchiarini et al., 2000); and prosody features, such as F0 and intensity slope (Hoenig, 2002). In addition to the large majority of applications that elicit restricted speech, a small number of applications have also investigated automated scoring of non-native spontaneous speech, in order to more fully evaluate a speaker’s communicative competence (e.g., (Cucchiarini et al., 2002) and (Zechner et al., 2009)). In these systems, the same types of pronunciation, fluency, and prosody features can be extracted; furthermore, features related to additional aspects of a speaker’s proficiency in the non-native language can b</context>
</contexts>
<marker>Cucchiarini, Strik, Boves, 2000</marker>
<rawString>Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000. Quantitative assessment of second language learners’ fluency by means of automatic speech recognition technology. Journal of the Acoustical Society of America, 107(2):989–999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catia Cucchiarini</author>
<author>Helmer Strik</author>
<author>Lou Boves</author>
</authors>
<title>Quantitative assessment of second language learners’ fluency: Comparisons between read and spontaneous speech.</title>
<date>2002</date>
<journal>Journal of the Acoustical Society of America,</journal>
<volume>111</volume>
<issue>6</issue>
<contexts>
<context position="6474" citStr="Cucchiarini et al., 2002" startWordPosition="1017" endWordPosition="1020">’s spectral match to native speaker acoustic models (Witt, 1999) and a phone’s duration compared to native speaker models (Neumeyer et al., 2000); fluency features, such as the rate of speech, mean pause length, and number of disfluencies (Cucchiarini et al., 2000); and prosody features, such as F0 and intensity slope (Hoenig, 2002). In addition to the large majority of applications that elicit restricted speech, a small number of applications have also investigated automated scoring of non-native spontaneous speech, in order to more fully evaluate a speaker’s communicative competence (e.g., (Cucchiarini et al., 2002) and (Zechner et al., 2009)). In these systems, the same types of pronunciation, fluency, and prosody features can be extracted; furthermore, features related to additional aspects of a speaker’s proficiency in the non-native language can be extracted, such as vocabulary usage (Yoon et al., 2012), syntactic complexity (Bernstein et al., 2010a; Chen and Zechner, 2011), and topical content (Xie et al., 2012). As described in Section 1, the domain for the automated speaking assessment investigated in this study is teachers of EFL around the world. Based on the fact that many of the item types are</context>
</contexts>
<marker>Cucchiarini, Strik, Boves, 2002</marker>
<rawString>Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002. Quantitative assessment of second language learners’ fluency: Comparisons between read and spontaneous speech. Journal of the Acoustical Society of America, 111(6):2862–2873.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horacio Franco</author>
<author>Leonardo Neumeyer</author>
<author>Vassilios Digalakis</author>
<author>Orith Ronen</author>
</authors>
<title>Combination of machine scores for automatic grading of pronunciation quality.</title>
<date>2000</date>
<journal>Speech Communication,</journal>
<pages>30--1</pages>
<contexts>
<context position="25723" citStr="Franco et al., 2000" startWordPosition="4145" endWordPosition="4148"> individual features used in the item type scoring models and human scores (Table 6), we can see that features related to content accuracy exhibit a substantially stronger performance (r=0.33–0.67) than features related to most other sub-constructs of speaking proficiency, namely fluency, pronunciation, prosody, and vocabulary (r ∼ 0.2). One exception is features related to grammar, where correlations with human scores are as high as 0.49. Since related work on scoring speech using features indicative of fluency, pronunciation, etc. showed higher correlations (e.g., (Cucchiarini et al., 1997; Franco et al., 2000; Zechner et al., 2009)), we conjecture that the reason behind this difference is likely to be found in the fact that the responses in this assessment for teachers of English are quite short (6– 14 words on average for all items except for Read Aloud items that are about 46 words on average). Since content features are less reliant on longer stretches of speech, they still work fairly well for most items in our corpus. 140 Finally, while the proportion of words contained in responses in restricted items is much larger than those contained in responses in semi-restricted items, these two item t</context>
</contexts>
<marker>Franco, Neumeyer, Digalakis, Ronen, 2000</marker>
<rawString>Horacio Franco, Leonardo Neumeyer, Vassilios Digalakis, and Orith Ronen. 2000. Combination of machine scores for automatic grading of pronunciation quality. Speech Communication, 30(1-2):121– 130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derrick Higgins</author>
<author>Xiaoming Xi</author>
<author>Klaus Zechner</author>
<author>David M Williamson</author>
</authors>
<title>A three-stage approach to the automated scoring of spontaneous spoken responses.</title>
<date>2011</date>
<journal>Computer Speech and Language,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="12784" citStr="Higgins et al., 2011" startWordPosition="2068" endWordPosition="2071">s filled pauses and partial words are also included in the transcriptions). Then, the responses were partitioned (with no speaker overlap) into five sets for the training and evaluation of the ASR system and the linear regression scoring models. The amount of data and 136 human score distributions in each of these partitions are displayed in Table 2. 4 System Architecture The automated scoring system used for the teachers’ spoken language assessment consists of the following four components, which are invoked one after the other in a pipeline fashion (ETS SpeechRatersm, (Zechner et al., 2009; Higgins et al., 2011)): • an automated speech recognizer, generating word hypotheses from input audio recordings of the test takers’ responses • a feature computation module that generates features based on the ASR output, e.g., measuring fluency, pronunciation, prosody, and content accuracy • a filtering model that flags responses that should not be scored automatically due to issues with audio quality, empty responses, etc. • linear regression scoring models that predict the score for each response based on a set of selected features Furthermore, we use Praat (Boersma and Weenick, 2012) to extract power and pitc</context>
</contexts>
<marker>Higgins, Xi, Zechner, Williamson, 2011</marker>
<rawString>Derrick Higgins, Xiaoming Xi, Klaus Zechner, and David M. Williamson. 2011. A three-stage approach to the automated scoring of spontaneous spoken responses. Computer Speech and Language, 25(2):282–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Hoenig</author>
</authors>
<title>Automatic assessment of nonnative prosody – Annotation, modelling, and evaluation.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Symposium on Automatic Detection of Errors in Pronunciation Training (ISADEPT),</booktitle>
<pages>21--30</pages>
<location>Stockholm,</location>
<contexts>
<context position="6183" citStr="Hoenig, 2002" startWordPosition="976" endWordPosition="977">Several types of features related to a non-native speaker’s ability to produce English sounds and speech patterns effectively have been extracted from these types of responses. Some of the best performing of these types of features include pronunciation features, such as a phone’s spectral match to native speaker acoustic models (Witt, 1999) and a phone’s duration compared to native speaker models (Neumeyer et al., 2000); fluency features, such as the rate of speech, mean pause length, and number of disfluencies (Cucchiarini et al., 2000); and prosody features, such as F0 and intensity slope (Hoenig, 2002). In addition to the large majority of applications that elicit restricted speech, a small number of applications have also investigated automated scoring of non-native spontaneous speech, in order to more fully evaluate a speaker’s communicative competence (e.g., (Cucchiarini et al., 2002) and (Zechner et al., 2009)). In these systems, the same types of pronunciation, fluency, and prosody features can be extracted; furthermore, features related to additional aspects of a speaker’s proficiency in the non-native language can be extracted, such as vocabulary usage (Yoon et al., 2012), syntactic </context>
</contexts>
<marker>Hoenig, 2002</marker>
<rawString>Florian Hoenig. 2002. Automatic assessment of nonnative prosody – Annotation, modelling, and evaluation. In Proceedings of the International Symposium on Automatic Detection of Errors in Pronunciation Training (ISADEPT), pages 21–30, Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Je Hun Jeon</author>
<author>Su-Youn Yoon</author>
</authors>
<title>Acoustic feature-based non-scorable response detection for an automated speaking proficiency assessment.</title>
<date>2012</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="17858" citStr="Jeon and Yoon, 2012" startWordPosition="2887" endWordPosition="2890">n regular expressions, keywords, and language model scores (Zechner and Wang, 2013). Table 3 lists the features that were used in the scoring models for restricted and semirestricted item types, along with sub-constructs they measure and their description. 5.2 Filtering model In order to automatically identify responses that have technical issues (e.g., loud background noise) or are otherwise not scorable (e.g., empty responses), a decision tree-based filtering model was developed using a combination of features derived from ASR output and from pitch and energy information (Yoon et al., 2011; Jeon and Yoon, 2012). The filtering model was tested on the scoring model evaluation data, and obtained an accuracy rate (the exact agreement between the filtering model and a human rater concerning the distinction between scorable and non-scorable responses) of 97%; it correctly identified 90% of the non-scorable responses in the data set with a false positive rate of 21% (recall=0.90, precision=0.79, F-score=0.84). 5.3 Scoring models We used the Model Training set to train 8 linear regression models for the 8 different item types, using the previously determined feature sets. We used the features as independent</context>
</contexts>
<marker>Jeon, Yoon, 2012</marker>
<rawString>Je Hun Jeon and Su-Youn Yoon. 2012. Acoustic feature-based non-scorable response detection for an automated speaking proficiency assessment. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack Mostow</author>
<author>Steven F Roth</author>
<author>Alexander G Hauptmann</author>
<author>Matthew Kane</author>
</authors>
<title>A prototype reading coach that listens.</title>
<date>1994</date>
<booktitle>In Proceedings of the Twelfth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="4864" citStr="Mostow et al., 1994" startWordPosition="761" endWordPosition="764">s: Section 2 discusses related work; in Section 3, we present the data used for system training and evaluation; Section 4 describes the system architecture of the automated speech scoring system. We detail the methods we used to build our system in Section 5, followed by an overview of the results in Section 6. Section 7 discusses our findings; finally, Section 8 concludes the paper. 2 Related Work Automated speech processing and scoring technology has been applied to a variety of domains over the course of the past two decades, including evaluation and tutoring of children’s literacy skills (Mostow et al., 1994), preparation for high stakes English proficiency tests for institutions of higher education (Zechner et al., 2009), evaluation of English skills of foreign-based call center agents (Chandel et al., 2007), and evaluation of aviation English (Pearson Education, Inc., 2011), to name a few (for a comprehensive overview, see (Eskenazi, 2009)). Most of these applications elicit restricted speech from the participants, and the most common item type by far is the Read Aloud, in which the speaker reads a sentence or collection of sentences out loud. Due to the constrained nature of this task, it is po</context>
</contexts>
<marker>Mostow, Roth, Hauptmann, Kane, 1994</marker>
<rawString>Jack Mostow, Steven F. Roth, Alexander G. Hauptmann, and Matthew Kane. 1994. A prototype reading coach that listens. In Proceedings of the Twelfth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonardo Neumeyer</author>
<author>Horacio Franco</author>
<author>Vassilios Digalakis</author>
<author>Mitchel Weintraub</author>
</authors>
<title>Automatic scoring of pronunciation quality.</title>
<date>2000</date>
<journal>Speech Communication,</journal>
<pages>30--83</pages>
<contexts>
<context position="5994" citStr="Neumeyer et al., 2000" startWordPosition="943" endWordPosition="946">ce or collection of sentences out loud. Due to the constrained nature of this task, it is possible to develop ASR systems that are relatively accurate, even with heavily accented non-native speech. Several types of features related to a non-native speaker’s ability to produce English sounds and speech patterns effectively have been extracted from these types of responses. Some of the best performing of these types of features include pronunciation features, such as a phone’s spectral match to native speaker acoustic models (Witt, 1999) and a phone’s duration compared to native speaker models (Neumeyer et al., 2000); fluency features, such as the rate of speech, mean pause length, and number of disfluencies (Cucchiarini et al., 2000); and prosody features, such as F0 and intensity slope (Hoenig, 2002). In addition to the large majority of applications that elicit restricted speech, a small number of applications have also investigated automated scoring of non-native spontaneous speech, in order to more fully evaluate a speaker’s communicative competence (e.g., (Cucchiarini et al., 2002) and (Zechner et al., 2009)). In these systems, the same types of pronunciation, fluency, and prosody features can be ex</context>
</contexts>
<marker>Neumeyer, Franco, Digalakis, Weintraub, 2000</marker>
<rawString>Leonardo Neumeyer, Horacio Franco, Vassilios Digalakis, and Mitchel Weintraub. 2000. Automatic scoring of pronunciation quality. Speech Communication, 30:83–93.</rawString>
</citation>
<citation valid="true">
<date>2011</date>
<journal>VersantTM Aviation English Test.</journal>
<institution>Pearson Education, Inc.</institution>
<note>http://www. versanttest.com/technology/ VersantAviationEnglishTestValidation. pdf.</note>
<marker>2011</marker>
<rawString>Pearson Education, Inc. 2011. VersantTM Aviation English Test. http://www. versanttest.com/technology/ VersantAviationEnglishTestValidation. pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silke Witt</author>
</authors>
<title>Use of speech recognition in computer-assisted language learning.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Cambridge University.</institution>
<contexts>
<context position="5913" citStr="Witt, 1999" startWordPosition="931" endWordPosition="932">tem type by far is the Read Aloud, in which the speaker reads a sentence or collection of sentences out loud. Due to the constrained nature of this task, it is possible to develop ASR systems that are relatively accurate, even with heavily accented non-native speech. Several types of features related to a non-native speaker’s ability to produce English sounds and speech patterns effectively have been extracted from these types of responses. Some of the best performing of these types of features include pronunciation features, such as a phone’s spectral match to native speaker acoustic models (Witt, 1999) and a phone’s duration compared to native speaker models (Neumeyer et al., 2000); fluency features, such as the rate of speech, mean pause length, and number of disfluencies (Cucchiarini et al., 2000); and prosody features, such as F0 and intensity slope (Hoenig, 2002). In addition to the large majority of applications that elicit restricted speech, a small number of applications have also investigated automated scoring of non-native spontaneous speech, in order to more fully evaluate a speaker’s communicative competence (e.g., (Cucchiarini et al., 2002) and (Zechner et al., 2009)). In these </context>
</contexts>
<marker>Witt, 1999</marker>
<rawString>Silke Witt. 1999. Use of speech recognition in computer-assisted language learning. Ph.D. thesis, Cambridge University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoming Xi</author>
<author>Derrick Higgins</author>
<author>Klaus Zechner</author>
<author>David M Williamson</author>
</authors>
<title>Automated scoring of spontaneous speech using SpeechRater v1.0.</title>
<date>2008</date>
<journal>Educational Testing Service Research Report</journal>
<pages>08--62</pages>
<contexts>
<context position="15015" citStr="Xi et al., 2008" startWordPosition="2435" endWordPosition="2438">on components of our speech scoring system compute more than 100 features based on a speaker’s response. They belong to the following broad dimensions of speaking proficiency: fluency, pronunciation, prosody, vocabulary usage, grammatical complexity and accuracy, and content accuracy (Zechner et al., 2009; Chen and Yoon, 2012; Chen et al., 2009; Zechner et al., 2011; Yoon et al., 2012; Yoon and Bhat, 2012; Zechner and Wang, 2013). After initial feature generation, we selected a set of about 10 features for each of the 8 item types, based on the following considerations1 (Zechner et al., 2009; Xi et al., 2008): • empirical performance, i.e., feature correlation with human scores • construct2 relevance, i.e., to what extent the feature measures aspects of speaking proficiency that are considered to be relevant and important by content experts • overall construct coverage, i.e., the feature set should include features from all relevant construct dimensions • feature independence, i.e., the intercorrelation between any two features of the set should be low Furthermore, some features were transformed (e.g., by applying the inverse or log function), in order to increase the normality of their distributi</context>
</contexts>
<marker>Xi, Higgins, Zechner, Williamson, 2008</marker>
<rawString>Xiaoming Xi, Derrick Higgins, Klaus Zechner, and David M. Williamson. 2008. Automated scoring of spontaneous speech using SpeechRater v1.0. Educational Testing Service Research Report RR-08-62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Xie</author>
<author>Keelan Evanini</author>
<author>Klaus Zechner</author>
</authors>
<title>Exploring content features for automated speech scoring.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>103--111</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="6883" citStr="Xie et al., 2012" startWordPosition="1083" endWordPosition="1086">a small number of applications have also investigated automated scoring of non-native spontaneous speech, in order to more fully evaluate a speaker’s communicative competence (e.g., (Cucchiarini et al., 2002) and (Zechner et al., 2009)). In these systems, the same types of pronunciation, fluency, and prosody features can be extracted; furthermore, features related to additional aspects of a speaker’s proficiency in the non-native language can be extracted, such as vocabulary usage (Yoon et al., 2012), syntactic complexity (Bernstein et al., 2010a; Chen and Zechner, 2011), and topical content (Xie et al., 2012). As described in Section 1, the domain for the automated speaking assessment investigated in this study is teachers of EFL around the world. Based on the fact that many of the item types are designed to assess the test taker’s ability to productively use English constructions and linguistic units that commonly recur in English teaching environments, several of the item types elicit semi-restricted speech (see Table 1 below for a description of the different item types). These types of responses fall somewhere between the heavily restricted speech elicited by a Read Aloud task and unconstraine</context>
</contexts>
<marker>Xie, Evanini, Zechner, 2012</marker>
<rawString>Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012. Exploring content features for automated speech scoring. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 103–111, Montr´eal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su-Youn Yoon</author>
<author>Suma Bhat</author>
</authors>
<title>Assessment of ESL learners’ syntactic competence based on similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint</booktitle>
<contexts>
<context position="14807" citStr="Yoon and Bhat, 2012" startWordPosition="2399" endWordPosition="2402">on the responses that were less restricted. The WER ranged from 11.4% for the RA responses to 41.4% for the IS responses in the Model Evaluation partition. 5 Methodology 5.1 Speech features The feature computation components of our speech scoring system compute more than 100 features based on a speaker’s response. They belong to the following broad dimensions of speaking proficiency: fluency, pronunciation, prosody, vocabulary usage, grammatical complexity and accuracy, and content accuracy (Zechner et al., 2009; Chen and Yoon, 2012; Chen et al., 2009; Zechner et al., 2011; Yoon et al., 2012; Yoon and Bhat, 2012; Zechner and Wang, 2013). After initial feature generation, we selected a set of about 10 features for each of the 8 item types, based on the following considerations1 (Zechner et al., 2009; Xi et al., 2008): • empirical performance, i.e., feature correlation with human scores • construct2 relevance, i.e., to what extent the feature measures aspects of speaking proficiency that are considered to be relevant and important by content experts • overall construct coverage, i.e., the feature set should include features from all relevant construct dimensions • feature independence, i.e., the interc</context>
</contexts>
<marker>Yoon, Bhat, 2012</marker>
<rawString>Su-Youn Yoon and Suma Bhat. 2012. Assessment of ESL learners’ syntactic competence based on similarity measures. In Proceedings of the 2012 Joint</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su-Youn Yoon</author>
<author>Keelan Evanini</author>
<author>Klaus Zechner</author>
</authors>
<title>Non-scorable response detection for automated speaking proficiency assessment.</title>
<date>2011</date>
<booktitle>In Proceedings of NAACL-HLT Workshop on Innovative Use of NLP for Building Educational Applications.</booktitle>
<contexts>
<context position="17836" citStr="Yoon et al., 2011" startWordPosition="2883" endWordPosition="2886">erse, e.g., based on regular expressions, keywords, and language model scores (Zechner and Wang, 2013). Table 3 lists the features that were used in the scoring models for restricted and semirestricted item types, along with sub-constructs they measure and their description. 5.2 Filtering model In order to automatically identify responses that have technical issues (e.g., loud background noise) or are otherwise not scorable (e.g., empty responses), a decision tree-based filtering model was developed using a combination of features derived from ASR output and from pitch and energy information (Yoon et al., 2011; Jeon and Yoon, 2012). The filtering model was tested on the scoring model evaluation data, and obtained an accuracy rate (the exact agreement between the filtering model and a human rater concerning the distinction between scorable and non-scorable responses) of 97%; it correctly identified 90% of the non-scorable responses in the data set with a false positive rate of 21% (recall=0.90, precision=0.79, F-score=0.84). 5.3 Scoring models We used the Model Training set to train 8 linear regression models for the 8 different item types, using the previously determined feature sets. We used the f</context>
</contexts>
<marker>Yoon, Evanini, Zechner, 2011</marker>
<rawString>Su-Youn Yoon, Keelan Evanini, and Klaus Zechner. 2011. Non-scorable response detection for automated speaking proficiency assessment. In Proceedings of NAACL-HLT Workshop on Innovative Use of NLP for Building Educational Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su-Youn Yoon</author>
<author>Suma Bhat</author>
<author>Klaus Zechner</author>
</authors>
<title>Vocabulary profile as a measure of vocabulary sophistication.</title>
<date>2012</date>
<booktitle>In Proceedings of the 7th Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>NAACL-HLT, Montr´eal, Canada.</location>
<contexts>
<context position="6771" citStr="Yoon et al., 2012" startWordPosition="1065" endWordPosition="1068">intensity slope (Hoenig, 2002). In addition to the large majority of applications that elicit restricted speech, a small number of applications have also investigated automated scoring of non-native spontaneous speech, in order to more fully evaluate a speaker’s communicative competence (e.g., (Cucchiarini et al., 2002) and (Zechner et al., 2009)). In these systems, the same types of pronunciation, fluency, and prosody features can be extracted; furthermore, features related to additional aspects of a speaker’s proficiency in the non-native language can be extracted, such as vocabulary usage (Yoon et al., 2012), syntactic complexity (Bernstein et al., 2010a; Chen and Zechner, 2011), and topical content (Xie et al., 2012). As described in Section 1, the domain for the automated speaking assessment investigated in this study is teachers of EFL around the world. Based on the fact that many of the item types are designed to assess the test taker’s ability to productively use English constructions and linguistic units that commonly recur in English teaching environments, several of the item types elicit semi-restricted speech (see Table 1 below for a description of the different item types). These types </context>
<context position="14786" citStr="Yoon et al., 2012" startWordPosition="2395" endWordPosition="2398">nd performed worse on the responses that were less restricted. The WER ranged from 11.4% for the RA responses to 41.4% for the IS responses in the Model Evaluation partition. 5 Methodology 5.1 Speech features The feature computation components of our speech scoring system compute more than 100 features based on a speaker’s response. They belong to the following broad dimensions of speaking proficiency: fluency, pronunciation, prosody, vocabulary usage, grammatical complexity and accuracy, and content accuracy (Zechner et al., 2009; Chen and Yoon, 2012; Chen et al., 2009; Zechner et al., 2011; Yoon et al., 2012; Yoon and Bhat, 2012; Zechner and Wang, 2013). After initial feature generation, we selected a set of about 10 features for each of the 8 item types, based on the following considerations1 (Zechner et al., 2009; Xi et al., 2008): • empirical performance, i.e., feature correlation with human scores • construct2 relevance, i.e., to what extent the feature measures aspects of speaking proficiency that are considered to be relevant and important by content experts • overall construct coverage, i.e., the feature set should include features from all relevant construct dimensions • feature independe</context>
</contexts>
<marker>Yoon, Bhat, Zechner, 2012</marker>
<rawString>Su-Youn Yoon, Suma Bhat, and Klaus Zechner. 2012. Vocabulary profile as a measure of vocabulary sophistication. In Proceedings of the 7th Workshop on Innovative Use of NLP for Building Educational Applications, NAACL-HLT, Montr´eal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
<author>Xinhao Wang</author>
</authors>
<title>Automated content scoring of spoken responses in an assessment for teachers of english.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications, NAACL-HLT, Atlanta. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14832" citStr="Zechner and Wang, 2013" startWordPosition="2403" endWordPosition="2406"> were less restricted. The WER ranged from 11.4% for the RA responses to 41.4% for the IS responses in the Model Evaluation partition. 5 Methodology 5.1 Speech features The feature computation components of our speech scoring system compute more than 100 features based on a speaker’s response. They belong to the following broad dimensions of speaking proficiency: fluency, pronunciation, prosody, vocabulary usage, grammatical complexity and accuracy, and content accuracy (Zechner et al., 2009; Chen and Yoon, 2012; Chen et al., 2009; Zechner et al., 2011; Yoon et al., 2012; Yoon and Bhat, 2012; Zechner and Wang, 2013). After initial feature generation, we selected a set of about 10 features for each of the 8 item types, based on the following considerations1 (Zechner et al., 2009; Xi et al., 2008): • empirical performance, i.e., feature correlation with human scores • construct2 relevance, i.e., to what extent the feature measures aspects of speaking proficiency that are considered to be relevant and important by content experts • overall construct coverage, i.e., the feature set should include features from all relevant construct dimensions • feature independence, i.e., the intercorrelation between any tw</context>
<context position="17321" citStr="Zechner and Wang, 2013" startWordPosition="2803" endWordPosition="2806">) 1,715 (27.2) 3,577 (56.8) 333 (5.3) Model Evaluation 300 6,300 45.7 647 (10.3) 1,637 (26.0) 3,487 (55.3) 529 (8.4) Total 1,423 29,699 215.8 2,993 (9.38) 7,685 (25.14) 16,513 (58.26) 2,508 (7.22) Table 2: Amount of data contained in each partition (speakers, responses, hours of speech) and distribution of human scores (percentages of scores per partition in brackets). features for the restricted items were based only on string alignment measures, content accuracy features for the semi-restricted items were more diverse, e.g., based on regular expressions, keywords, and language model scores (Zechner and Wang, 2013). Table 3 lists the features that were used in the scoring models for restricted and semirestricted item types, along with sub-constructs they measure and their description. 5.2 Filtering model In order to automatically identify responses that have technical issues (e.g., loud background noise) or are otherwise not scorable (e.g., empty responses), a decision tree-based filtering model was developed using a combination of features derived from ASR output and from pitch and energy information (Yoon et al., 2011; Jeon and Yoon, 2012). The filtering model was tested on the scoring model evaluatio</context>
</contexts>
<marker>Zechner, Wang, 2013</marker>
<rawString>Klaus Zechner and Xinhao Wang. 2013. Automated content scoring of spoken responses in an assessment for teachers of english. In Proceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications, NAACL-HLT, Atlanta. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
<author>Derrick Higgins</author>
<author>Xiaoming Xi</author>
<author>David M Williamson</author>
</authors>
<title>Automatic scoring of non-native spontaneous speech in tests of spoken English.</title>
<date>2009</date>
<journal>Speech Communication,</journal>
<volume>51</volume>
<issue>10</issue>
<contexts>
<context position="4979" citStr="Zechner et al., 2009" startWordPosition="777" endWordPosition="780">ction 4 describes the system architecture of the automated speech scoring system. We detail the methods we used to build our system in Section 5, followed by an overview of the results in Section 6. Section 7 discusses our findings; finally, Section 8 concludes the paper. 2 Related Work Automated speech processing and scoring technology has been applied to a variety of domains over the course of the past two decades, including evaluation and tutoring of children’s literacy skills (Mostow et al., 1994), preparation for high stakes English proficiency tests for institutions of higher education (Zechner et al., 2009), evaluation of English skills of foreign-based call center agents (Chandel et al., 2007), and evaluation of aviation English (Pearson Education, Inc., 2011), to name a few (for a comprehensive overview, see (Eskenazi, 2009)). Most of these applications elicit restricted speech from the participants, and the most common item type by far is the Read Aloud, in which the speaker reads a sentence or collection of sentences out loud. Due to the constrained nature of this task, it is possible to develop ASR systems that are relatively accurate, even with heavily accented non-native speech. Several t</context>
<context position="6501" citStr="Zechner et al., 2009" startWordPosition="1022" endWordPosition="1025">aker acoustic models (Witt, 1999) and a phone’s duration compared to native speaker models (Neumeyer et al., 2000); fluency features, such as the rate of speech, mean pause length, and number of disfluencies (Cucchiarini et al., 2000); and prosody features, such as F0 and intensity slope (Hoenig, 2002). In addition to the large majority of applications that elicit restricted speech, a small number of applications have also investigated automated scoring of non-native spontaneous speech, in order to more fully evaluate a speaker’s communicative competence (e.g., (Cucchiarini et al., 2002) and (Zechner et al., 2009)). In these systems, the same types of pronunciation, fluency, and prosody features can be extracted; furthermore, features related to additional aspects of a speaker’s proficiency in the non-native language can be extracted, such as vocabulary usage (Yoon et al., 2012), syntactic complexity (Bernstein et al., 2010a; Chen and Zechner, 2011), and topical content (Xie et al., 2012). As described in Section 1, the domain for the automated speaking assessment investigated in this study is teachers of EFL around the world. Based on the fact that many of the item types are designed to assess the tes</context>
<context position="12761" citStr="Zechner et al., 2009" startWordPosition="2064" endWordPosition="2067"> (disfluencies, such as filled pauses and partial words are also included in the transcriptions). Then, the responses were partitioned (with no speaker overlap) into five sets for the training and evaluation of the ASR system and the linear regression scoring models. The amount of data and 136 human score distributions in each of these partitions are displayed in Table 2. 4 System Architecture The automated scoring system used for the teachers’ spoken language assessment consists of the following four components, which are invoked one after the other in a pipeline fashion (ETS SpeechRatersm, (Zechner et al., 2009; Higgins et al., 2011)): • an automated speech recognizer, generating word hypotheses from input audio recordings of the test takers’ responses • a feature computation module that generates features based on the ASR output, e.g., measuring fluency, pronunciation, prosody, and content accuracy • a filtering model that flags responses that should not be scored automatically due to issues with audio quality, empty responses, etc. • linear regression scoring models that predict the score for each response based on a set of selected features Furthermore, we use Praat (Boersma and Weenick, 2012) to</context>
<context position="14705" citStr="Zechner et al., 2009" startWordPosition="2379" endWordPosition="2382"> system performed best on the responses that were most restricted by the test item and performed worse on the responses that were less restricted. The WER ranged from 11.4% for the RA responses to 41.4% for the IS responses in the Model Evaluation partition. 5 Methodology 5.1 Speech features The feature computation components of our speech scoring system compute more than 100 features based on a speaker’s response. They belong to the following broad dimensions of speaking proficiency: fluency, pronunciation, prosody, vocabulary usage, grammatical complexity and accuracy, and content accuracy (Zechner et al., 2009; Chen and Yoon, 2012; Chen et al., 2009; Zechner et al., 2011; Yoon et al., 2012; Yoon and Bhat, 2012; Zechner and Wang, 2013). After initial feature generation, we selected a set of about 10 features for each of the 8 item types, based on the following considerations1 (Zechner et al., 2009; Xi et al., 2008): • empirical performance, i.e., feature correlation with human scores • construct2 relevance, i.e., to what extent the feature measures aspects of speaking proficiency that are considered to be relevant and important by content experts • overall construct coverage, i.e., the feature set s</context>
<context position="25746" citStr="Zechner et al., 2009" startWordPosition="4149" endWordPosition="4152">used in the item type scoring models and human scores (Table 6), we can see that features related to content accuracy exhibit a substantially stronger performance (r=0.33–0.67) than features related to most other sub-constructs of speaking proficiency, namely fluency, pronunciation, prosody, and vocabulary (r ∼ 0.2). One exception is features related to grammar, where correlations with human scores are as high as 0.49. Since related work on scoring speech using features indicative of fluency, pronunciation, etc. showed higher correlations (e.g., (Cucchiarini et al., 1997; Franco et al., 2000; Zechner et al., 2009)), we conjecture that the reason behind this difference is likely to be found in the fact that the responses in this assessment for teachers of English are quite short (6– 14 words on average for all items except for Read Aloud items that are about 46 words on average). Since content features are less reliant on longer stretches of speech, they still work fairly well for most items in our corpus. 140 Finally, while the proportion of words contained in responses in restricted items is much larger than those contained in responses in semi-restricted items, these two item type categories are more</context>
</contexts>
<marker>Zechner, Higgins, Xi, Williamson, 2009</marker>
<rawString>Klaus Zechner, Derrick Higgins, Xiaoming Xi, and David M. Williamson. 2009. Automatic scoring of non-native spontaneous speech in tests of spoken English. Speech Communication, 51(10):883–895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
<author>Xiaoming Xi</author>
<author>Lei Chen</author>
</authors>
<title>Evaluating prosodic features for automated scoring of non-native read speech.</title>
<date>2011</date>
<booktitle>In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).</booktitle>
<contexts>
<context position="14767" citStr="Zechner et al., 2011" startWordPosition="2391" endWordPosition="2394">ted by the test item and performed worse on the responses that were less restricted. The WER ranged from 11.4% for the RA responses to 41.4% for the IS responses in the Model Evaluation partition. 5 Methodology 5.1 Speech features The feature computation components of our speech scoring system compute more than 100 features based on a speaker’s response. They belong to the following broad dimensions of speaking proficiency: fluency, pronunciation, prosody, vocabulary usage, grammatical complexity and accuracy, and content accuracy (Zechner et al., 2009; Chen and Yoon, 2012; Chen et al., 2009; Zechner et al., 2011; Yoon et al., 2012; Yoon and Bhat, 2012; Zechner and Wang, 2013). After initial feature generation, we selected a set of about 10 features for each of the 8 item types, based on the following considerations1 (Zechner et al., 2009; Xi et al., 2008): • empirical performance, i.e., feature correlation with human scores • construct2 relevance, i.e., to what extent the feature measures aspects of speaking proficiency that are considered to be relevant and important by content experts • overall construct coverage, i.e., the feature set should include features from all relevant construct dimensions </context>
</contexts>
<marker>Zechner, Xi, Chen, 2011</marker>
<rawString>Klaus Zechner, Xiaoming Xi, and Lei Chen. 2011. Evaluating prosodic features for automated scoring of non-native read speech. In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
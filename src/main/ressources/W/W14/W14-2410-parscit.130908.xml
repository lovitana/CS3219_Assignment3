<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002190">
<title confidence="0.989088">
Software Requirements: A new Domain for Semantic Parsers
</title>
<author confidence="0.993373">
Michael Rotht Themistoklis Diamantopoulost Ewan Kleint Andreas Symeonidist
</author>
<affiliation confidence="0.9959735">
tILCC, School of Informatics $Electrical &amp; Computer Engineering Department
University of Edinburgh Aristotle University of Thessaloniki
</affiliation>
<email confidence="0.9862755">
{mroth,ewan}@inf.ed.ac.uk thdiaman@issel.ee.auth.gr
asymeon@eng.auth.gr
</email>
<sectionHeader confidence="0.993764" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999835611111111">
Software requirements are commonly
written in natural language, making them
prone to ambiguity, incompleteness and
inconsistency. By converting require-
ments to formal semantic representations,
emerging problems can be detected at an
early stage of the development process,
thus reducing the number of ensuing errors
and the development costs. In this paper,
we treat the mapping from requirements to
formal representations as a semantic pars-
ing task. We describe a novel data set for
this task that involves two contributions:
first, we establish an ontology for formally
representing requirements; and second, we
introduce an iterative annotation scheme,
in which formal representations are de-
rived through step-wise refinements.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956142857143">
During the process of software development, de-
velopers and customers typically discuss and
agree on requirements that specify the function-
ality of a system that is being developed.1 Such
requirements play a crucial role in the develop-
ment lifecycle, as they form the basis for actual
implementations, corresponding work plans, cost
estimations and follow-up directives (van Lam-
sweerde, 2009). In general, software requirements
can be expressed in various different ways, includ-
ing the use of UML diagrams and storyboards.
Most commonly, however, expectations are ex-
pressed in natural language (Mich et al., 2004), as
shown in Example (1):
</bodyText>
<listItem confidence="0.790502">
(1) A user should be able to login to his account.
</listItem>
<footnote confidence="0.7988768">
1Although software engineering can also involve non-
functional requirements, which describe general quality cri-
teria of a system, this paper is only concerned with functional
requirements, i.e., requirements that specify the behavior of a
system.
</footnote>
<bodyText confidence="0.9992442">
While requirements expressed in natural lan-
guage have the advantage of being intelligible to
both clients and developers, they can of course
also be ambiguous, vague and incomplete. Al-
though formal languages could be used as an alter-
native that eliminates some of these problems, cus-
tomers are rarely equipped with the mathematical
and technical expertise for understanding highly
formalised requirements. To benefit from the ad-
vantages of both natural language and formal rep-
resentations, we propose to induce the latter au-
tomatically from text in a semantic parsing task.
Given the software requirement in Example (1),
for instance, we would like to construct a represen-
tation that explicitly specifies the types of the en-
tities involved (e.g., object(account)) and that cap-
tures explicit and inferable relationships among
them (e.g., owns(user, account)). We expect such
formal representations to be helpful in detecting
errors at an early stage of the development process
(e.g., via logical inference and verification tools),
thus avoiding the costs of finding and fixing prob-
lems at a later and hence more expensive stage
(Boehm and Basili, 2001).
Given the benefits of formal representations,
we believe that software requirements constitute
a useful application domain for semantic parsers.
Requirement texts naturally occur in the real world
and appropriate data sets can thus be constructed
without setting up artificial tasks to collect them.
Parsing requirements of different software projects
also poses interesting challenges as texts exhibit a
considerable amount of lexical variety, while fre-
quently also containing more than one relation per
sentence.
</bodyText>
<sectionHeader confidence="0.999802" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99992">
A range of methods have been proposed in previ-
ous work to (semi-)automatically process require-
ments written in plain, natural language text and
map them to formal representations. To the best
</bodyText>
<page confidence="0.947034">
50
</page>
<bodyText confidence="0.969774">
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 50–54,
Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics
of our knowledge, Abbott (1983) was the first to
introduce a technique for extracting data types,
variables and operators from informal texts de-
scribing a problem. The proposed method fol-
lows a simple rule-based setup, in which common
nouns are identified as data types, proper nouns
as objects and verbs as operators between them.
Booch (1986) described a method of similar com-
plexity that extends Abbot’s approach to object-
oriented development. Saeki et al. (1989) imple-
mented a first prototype that automatically con-
structs object-oriented models from informal re-
quirements. As proposed by Abbott and Booch,
the system is based on automatically extracted
nouns and verbs. Although Saeki et al. found re-
sulting object diagrams of reasonable quality, they
concluded that human intervention was still nec-
essary to distinguish between words that are rele-
vant for the model and irrelevant nouns and verbs.
Nanduri and Rugaber (1995) proposed to further
automate object-oriented analysis of requirement
texts by applying a syntactic parser and a set of
post-processing rules. In a similar setting, Mich
(1996) employed a full NLP pipeline that con-
tains a semantic analysis module, thus omitting the
need for additional post-processing rules. More
recent approaches include those by Harmain and
Gaizauskas (2003) and Kof (2004), who relied on
a combination of NLP components and human in-
teraction. Whereas most approaches in previous
work aim to derive class diagrams, Ghosh et al.
(2014) proposed a pipeline architecture that con-
verts syntactic parses to logical expressions via a
set of heuristic post-processing rules.
Despite this seemingly long tradition, previ-
ous methods for processing software requirements
have tended to depend on domain-specific heuris-
tics and knowledge bases or have required addi-
tional user intervention. In contrast, we propose
to utilize annotated data to learn how to perform
semantic parsing of requirements automatically.
</bodyText>
<sectionHeader confidence="0.986284" genericHeader="method">
3 Data Set
</sectionHeader>
<bodyText confidence="0.945759444444445">
Given our conviction that mapping natural lan-
guage software requirements to formal representa-
tions provides an attractive challenge for semantic
parsing research, we believe that there is a more
general benefit in building a corpus of annotated
requirements. One immediate obstacle is that soft-
ware requirements can drastically differ in quality,
style and granularity. To cover a range of possible
#sentences #tokens #types
</bodyText>
<table confidence="0.8599368">
student projects 270 3130 604
industrial prototypes 55 927 286
Our dataset (total) 325 4057 765
GEOQUERY880 880 6656 279
FREE917 917 6769 2035
</table>
<tableCaption confidence="0.7932665">
Table 1: Statistics on our requirements collection
and existing semantic parsing data sets.
</tableCaption>
<bodyText confidence="0.999824423076923">
differences, we asked lecturers from several uni-
versities to provide requirement documents writ-
ten by students. We received requirement docu-
ments on student projects from various domains,
including embedded systems, virtual reality and
web applications.2 From these documents, we ex-
tracted lists of requirements, each of which is ex-
pressed within a single sentence. We addition-
ally collected single sentence requirements within
the S-CASE project, describing industrial proto-
types of cloud-based web services.3 Table 1 gives
an overview of the quantity of requirements col-
lected. We observe that the number of require-
ments received for student projects is much higher.
The token counts reveal however that require-
ments written for industrial prototypes are longer
on average (16.6 vs. 11.6 words). This observa-
tion might be related to the fact that students in
software engineering classes are often provided
with explicit guidelines on how to concisely ex-
press requirements in natural language. As a con-
sequence, we also find their requirement texts to
be more regimented and stylised than those writ-
ten by senior software engineers. Examples (2)
and (3) show examples of a student-written and
developer-written requirement, respectively.
</bodyText>
<listItem confidence="0.964827">
(2) The user must be able to vote on polls.
(3) For each user contact, back-end must perform
a check to determine whether the contact is a
registered user or not.
</listItem>
<bodyText confidence="0.9326585">
In comparison to two extant data sets, namely
GeoQuery880 (Tang, 2003) and Free917 (Cai and
Yates, 2013), we find that our collection is still rel-
atively small in terms of example sentences. The
</bodyText>
<footnote confidence="0.9987936">
2The majority of collected requirements are
from a software development course organized
jointly by several European universities, cf.
http://www.fer.unizg.hr/rasip/dsd
3http://www.scasefp7.eu/
</footnote>
<page confidence="0.996352">
51
</page>
<figureCaption confidence="0.858111">
Figure 1: Class hierarchy of our conceptual ontol-
ogy for modeling software requirements.
</figureCaption>
<bodyText confidence="0.999127333333333">
difference in total number of tokens is not as cru-
cial, however, given that sentences in our data set
are much longer on average. We further observe
that the token/type ratio in our texts lies some-
where between ratios reported in previous work.
Based on the observed lexical variety and average
sentence length, we expect our texts to be chal-
lenging but not too difficult to parse using existing
methods.
</bodyText>
<sectionHeader confidence="0.986957" genericHeader="method">
4 Modeling Requirements Conceptually
</sectionHeader>
<bodyText confidence="0.999883764705882">
Different representations have been proposed for
modeling requirements in previous work: whereas
early work focused on deriving simple class dia-
grams, more recent approaches suggest represent-
ing requirements via logical forms (cf. Section 2).
In this paper, we propose to model requirements
using a formal ontology that captures general con-
cepts from different application domains. Our pro-
posed ontology covers the same properties as ear-
lier work and provides a means to represent re-
quirements in logical form. In practice, such logi-
cal forms can be induced by semantic parsers and
in subsequent steps be utilized for automatic infer-
ence. The class hierarchy of our ontology is shown
in Figure 1. At the highest level of the class hierar-
chy, we distinguish between “things” (ThingType)
and “operations” (OperationType).
</bodyText>
<subsectionHeader confidence="0.971726">
4.1 ThingType
</subsectionHeader>
<bodyText confidence="0.99994">
We define the following subclasses of ThingType:
</bodyText>
<listItem confidence="0.967774666666667">
• A Participant is a thing that is involved in an
operation. We further subdivide Participants
into Actors, which can be users of a system
or the system itself, and Objects.
• A Property is an attribute of an Object or a
characteristic of an OperationType.
</listItem>
<subsectionHeader confidence="0.965513">
4.2 OperationType
</subsectionHeader>
<bodyText confidence="0.991843">
We further divide operations into the following
subclasses:
</bodyText>
<listItem confidence="0.998694222222222">
• An Action describes an operation that is per-
formed by an Actor on one or several Ob-
ject(s).
• A State is an operation that describes the sta-
tus of an Actor.
• Ownership is used to model operations that
express possession.
• Emergence represent operations that undergo
passive transformation.
</listItem>
<subsectionHeader confidence="0.993498">
4.3 Relations
</subsectionHeader>
<bodyText confidence="0.998577076923077">
In addition to the class hierarchy, we define a set
of relations between classes, which describe and
constrain how different operations and things can
interact with each other.
On the level of OperationType, every opera-
tion can be assigned one Actor via the relations
HAS ACTOR or HAS OWNER, respectively. Ob-
jects can participate in Actions, States and Owner-
ships via the relations ACTS ON, HAS STATE and
OWNS, respectively. Every instance of Opera-
tionType and Object can further have an arbitrary
number of properties assigned to it via the relation
HAS PROPERTY.
</bodyText>
<sectionHeader confidence="0.998272" genericHeader="method">
5 Annotation Process
</sectionHeader>
<bodyText confidence="0.999969857142857">
In preliminary annotation experiments, we found
that class diagrams may be too simple to repre-
sent requirements conceptually. Logical forms, on
the other hand, can be difficult to use for anno-
tators without sufficient background knowledge.
To keep the same level of expressiveness as log-
ical forms and the simplicity of object-oriented
annotations, we propose a multi-step annotation
scheme, in which decisions in one iteration are fur-
ther refined in later iterations.
By adopting the class hierarchy introduced in
Section 4, we can naturally divide each annotation
iteration according to a level in the ontology. This
means that in the first iteration, we ask annotators
</bodyText>
<figure confidence="0.893498266666667">
Action
OperationType
Status
level 1 level 2 level 3
ThingType
Ownership
Participant
Property
Concept
Actor
Object
Emergence
52
A user that is logged in to his account must be able to update his password.
Actor(user) ∧ Action(login) ∧ Action(update)
</figure>
<table confidence="0.9751656">
∧ Object(account) ∧ HAS ACTOR(login,user) ∧ HAS ACTOR(update,user)
∧ Object(password) ∧ ACTS ON(login,account) ∧ ACTS ON(update,password)
∧ Ownership(o1) ∧ Ownership(o2)
∧ HAS OWNER(o1,user) ∧ HAS OWNER(o2,user)
∧ OWNS(o1,account) ∧ OWNS(o2,password)
The system must be able to forward and rewind a playing program.
Actor(system) ∧ Action(forward) ∧ Action(rewind)
∧ Object(program) ∧ HAS ACTOR(forward,system) ∧ HAS ACTOR(rewind,system)
∧ ACTS ON(forward,program) ∧ ACTS ON(rewind,program)
∧ Property(playing) ∧ HAS PROPERTY(program,playing)
</table>
<tableCaption confidence="0.995808">
Table 2: Example requirements from different domains and logical forms derived from annotations.
</tableCaption>
<figureCaption confidence="0.966742">
Figure 2: Annotation process: instances are
</figureCaption>
<bodyText confidence="0.972307869565217">
marked in text (dashed), class assignments are re-
fined (dotted), and relations are added (solid).
to simply mark all instances of ThingType and Op-
erationType that are explicitly expressed in a given
requirement. We then resolve conflicting annota-
tions and present the resulting instances from the
first level to annotators for the next iteration. In
each iteration, we add one layer of sophistication
from the class hierarchy, resulting in step-wise re-
finements. In the final iteration, we add relations
between instances of concepts, including implicit
but inferable cases.
An illustration of the overall annotation process,
based on Example (1), is depicted in Figure 2. The
last iteration in this example involves the addition
of an Ownership instance that is indicated (by the
phrase “his account”) but not explicitly realized in
text. Although identifying and annotating such in-
stances can be more challenging than the previous
annotation steps, we can directly populate our on-
tology at this stage (e.g., via conversion to RDF
tuples) and run verification tools to check whether
they are consistent with the annotation schema.
</bodyText>
<sectionHeader confidence="0.998932" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999991466666666">
The annotation scheme introduced in Section 4 is
designed with the goal of covering a wide range
of different application domains. Although this
means that many of the more fine-grained distinc-
tions within a domain are not considered here, we
believe that the scheme already provides sufficient
information for a range of tasks. By storing pro-
cessed requirements in a relational database, for
example, they can be retrieved using structured
queries and utilized for probabilistic inference.
Given the hierarchical structure of our annota-
tion process, as defined in Section 5, it is possible
to extend existing annotations with additional lev-
els of granularity provided by domain ontologies.
As an example, we have defined a domain ontol-
ogy for web services, which contains subclasses
of Action to further distinguish between the HTTP
methods get, put, post and delete. Similar exten-
sions can be defined for other domains.
Regarding the task of semantic parsing itself,
we are currently in the process of annotating sev-
eral hundreds of instances of requirements (cf.
Section 3) following the proposed ontology. We
will release an initial version of this data set at
the Semantic Parsing workshop. The initial re-
lease will serve as a basis for training and eval-
uating parsers in this domain, for which we are
also planning to collect more examples through-
out the year. We believe that requirements form
an interesting domain for the parsing community
</bodyText>
<figure confidence="0.994297222222222">
A user should be able login to his account
ThingType OperationType ThingType
Participant Action Participant
Actor Object
HAS OWNER
HAS ACTOR ACTS ON
(implicit)
Ownership
OWNS
</figure>
<page confidence="0.995825">
53
</page>
<bodyText confidence="0.9999295">
as the texts involve a fair amount of variation and
challenging semantic phenomena (such as infer-
able relations), while also serving a practical and
valuable purpose.
</bodyText>
<sectionHeader confidence="0.990292" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999201166666667">
Parts of this work have been supported by the FP7
Collaborative Project S-CASE (Grant Agreement
No 610717), funded by the European Commis-
sion. We thank our project partners for data sup-
port and useful discussions on the proposed ontol-
ogy.
</bodyText>
<sectionHeader confidence="0.998951" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999906509090909">
Russell J Abbott. 1983. Program design by informal
english descriptions. Communications of the ACM,
26(11):882–894.
Barry Boehm and Victor R. Basili. 2001. Software
defect reduction top 10 list. Computer, 34:135–137.
Grady Booch. 1986. Object-oriented develop-
ment. IEEE Transactions on Software Engineering,
(2):211–221.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 423–433, Sofia,
Bulgaria, August.
Shalini Ghosh, Daniel Elenius, Wenchao Li, Patrick
Lincoln, Natarajan Shankar, and Wilfried Steiner.
2014. Automatically extracting requirements spec-
ifications from natural language. arXiv preprint
arXiv:1403.3142.
H. M. Harmain and Robert Gaizauskas. 2003. Cm-
builder: A natural language-based case tool for
object-oriented analysis. Automated Software Engi-
neering, 10(2):157–181.
Leonid Kof. 2004. Natural language processing for
requirements engineering: Applicability to large re-
quirements documents. In 19th International Con-
ference on Automated Software Engineering, Work-
shop Proceedings.
Luisa Mich, Franch Mariangela, and Novi Inverardi
Pierluigi. 2004. Market research for requirements
analysis using linguistic tools. Requirements Engi-
neering, 9(1):40–56.
Luisa Mich. 1996. NL-OOPS: From natural language
to object oriented requirements using the natural lan-
guage processing system LOLITA. Natural Lan-
guage Engineering, 2(2):161–187.
Sastry Nanduri and Spencer Rugaber. 1995. Re-
quirements validation via automated natural lan-
guage parsing. In Proceedings of the Twenty-Eighth
Hawaii International Conference on System Sci-
ences, volume 3, pages 362–368.
Motoshi Saeki, Hisayuki Horai, and Hajime Enomoto.
1989. Software development process from natural
language specification. In Proceedings of the 11th
International Conference on Software Engineering,
pages 64–73.
Lappoon R. Tang. 2003. Integrating Top-down and
Bottom-up Approaches in Inductive Logic Program-
ming: Applications in Natural Language Processing
and Relational Data Mining. Ph.D. thesis, Depart-
ment of Computer Sciences, University of Texas,
Austin, Texas, USA, August.
Axel van Lamsweerde. 2009. Requirements Engineer-
ing: From System Goals to UML Models to Software
Specifications. Wiley.
</reference>
<page confidence="0.999023">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.477583">
<title confidence="0.977001">Software Requirements: A new Domain for Semantic Parsers</title>
<affiliation confidence="0.995937">School of Informatics &amp; Computer Engineering Department University of Edinburgh Aristotle University of Thessaloniki</affiliation>
<email confidence="0.60808">thdiaman@issel.ee.auth.grasymeon@eng.auth.gr</email>
<abstract confidence="0.998123052631579">Software requirements are commonly written in natural language, making them prone to ambiguity, incompleteness and inconsistency. By converting requirements to formal semantic representations, emerging problems can be detected at an early stage of the development process, thus reducing the number of ensuing errors and the development costs. In this paper, we treat the mapping from requirements to formal representations as a semantic parsing task. We describe a novel data set for this task that involves two contributions: first, we establish an ontology for formally representing requirements; and second, we introduce an iterative annotation scheme, in which formal representations are derived through step-wise refinements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Russell J Abbott</author>
</authors>
<title>Program design by informal english descriptions.</title>
<date>1983</date>
<journal>Communications of the ACM,</journal>
<volume>26</volume>
<issue>11</issue>
<contexts>
<context position="4099" citStr="Abbott (1983)" startWordPosition="601" endWordPosition="602">rsing requirements of different software projects also poses interesting challenges as texts exhibit a considerable amount of lexical variety, while frequently also containing more than one relation per sentence. 2 Related Work A range of methods have been proposed in previous work to (semi-)automatically process requirements written in plain, natural language text and map them to formal representations. To the best 50 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 50–54, Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics of our knowledge, Abbott (1983) was the first to introduce a technique for extracting data types, variables and operators from informal texts describing a problem. The proposed method follows a simple rule-based setup, in which common nouns are identified as data types, proper nouns as objects and verbs as operators between them. Booch (1986) described a method of similar complexity that extends Abbot’s approach to objectoriented development. Saeki et al. (1989) implemented a first prototype that automatically constructs object-oriented models from informal requirements. As proposed by Abbott and Booch, the system is based </context>
</contexts>
<marker>Abbott, 1983</marker>
<rawString>Russell J Abbott. 1983. Program design by informal english descriptions. Communications of the ACM, 26(11):882–894.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barry Boehm</author>
<author>Victor R Basili</author>
</authors>
<title>Software defect reduction top 10 list.</title>
<date>2001</date>
<journal>Computer,</journal>
<pages>34--135</pages>
<contexts>
<context position="3182" citStr="Boehm and Basili, 2001" startWordPosition="464" endWordPosition="467">tically from text in a semantic parsing task. Given the software requirement in Example (1), for instance, we would like to construct a representation that explicitly specifies the types of the entities involved (e.g., object(account)) and that captures explicit and inferable relationships among them (e.g., owns(user, account)). We expect such formal representations to be helpful in detecting errors at an early stage of the development process (e.g., via logical inference and verification tools), thus avoiding the costs of finding and fixing problems at a later and hence more expensive stage (Boehm and Basili, 2001). Given the benefits of formal representations, we believe that software requirements constitute a useful application domain for semantic parsers. Requirement texts naturally occur in the real world and appropriate data sets can thus be constructed without setting up artificial tasks to collect them. Parsing requirements of different software projects also poses interesting challenges as texts exhibit a considerable amount of lexical variety, while frequently also containing more than one relation per sentence. 2 Related Work A range of methods have been proposed in previous work to (semi-)aut</context>
</contexts>
<marker>Boehm, Basili, 2001</marker>
<rawString>Barry Boehm and Victor R. Basili. 2001. Software defect reduction top 10 list. Computer, 34:135–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grady Booch</author>
</authors>
<title>Object-oriented development.</title>
<date>1986</date>
<journal>IEEE Transactions on Software Engineering,</journal>
<pages>2--211</pages>
<contexts>
<context position="4412" citStr="Booch (1986)" startWordPosition="652" endWordPosition="653"> requirements written in plain, natural language text and map them to formal representations. To the best 50 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 50–54, Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics of our knowledge, Abbott (1983) was the first to introduce a technique for extracting data types, variables and operators from informal texts describing a problem. The proposed method follows a simple rule-based setup, in which common nouns are identified as data types, proper nouns as objects and verbs as operators between them. Booch (1986) described a method of similar complexity that extends Abbot’s approach to objectoriented development. Saeki et al. (1989) implemented a first prototype that automatically constructs object-oriented models from informal requirements. As proposed by Abbott and Booch, the system is based on automatically extracted nouns and verbs. Although Saeki et al. found resulting object diagrams of reasonable quality, they concluded that human intervention was still necessary to distinguish between words that are relevant for the model and irrelevant nouns and verbs. Nanduri and Rugaber (1995) proposed to f</context>
</contexts>
<marker>Booch, 1986</marker>
<rawString>Grady Booch. 1986. Object-oriented development. IEEE Transactions on Software Engineering, (2):211–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Large-scale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>423--433</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="8198" citStr="Cai and Yates, 2013" startWordPosition="1233" endWordPosition="1236">es are often provided with explicit guidelines on how to concisely express requirements in natural language. As a consequence, we also find their requirement texts to be more regimented and stylised than those written by senior software engineers. Examples (2) and (3) show examples of a student-written and developer-written requirement, respectively. (2) The user must be able to vote on polls. (3) For each user contact, back-end must perform a check to determine whether the contact is a registered user or not. In comparison to two extant data sets, namely GeoQuery880 (Tang, 2003) and Free917 (Cai and Yates, 2013), we find that our collection is still relatively small in terms of example sentences. The 2The majority of collected requirements are from a software development course organized jointly by several European universities, cf. http://www.fer.unizg.hr/rasip/dsd 3http://www.scasefp7.eu/ 51 Figure 1: Class hierarchy of our conceptual ontology for modeling software requirements. difference in total number of tokens is not as crucial, however, given that sentences in our data set are much longer on average. We further observe that the token/type ratio in our texts lies somewhere between ratios repor</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 423–433, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalini Ghosh</author>
<author>Daniel Elenius</author>
<author>Wenchao Li</author>
<author>Patrick Lincoln</author>
<author>Natarajan Shankar</author>
<author>Wilfried Steiner</author>
</authors>
<title>Automatically extracting requirements specifications from natural language. arXiv preprint arXiv:1403.3142.</title>
<date>2014</date>
<contexts>
<context position="5552" citStr="Ghosh et al. (2014)" startWordPosition="827" endWordPosition="830"> model and irrelevant nouns and verbs. Nanduri and Rugaber (1995) proposed to further automate object-oriented analysis of requirement texts by applying a syntactic parser and a set of post-processing rules. In a similar setting, Mich (1996) employed a full NLP pipeline that contains a semantic analysis module, thus omitting the need for additional post-processing rules. More recent approaches include those by Harmain and Gaizauskas (2003) and Kof (2004), who relied on a combination of NLP components and human interaction. Whereas most approaches in previous work aim to derive class diagrams, Ghosh et al. (2014) proposed a pipeline architecture that converts syntactic parses to logical expressions via a set of heuristic post-processing rules. Despite this seemingly long tradition, previous methods for processing software requirements have tended to depend on domain-specific heuristics and knowledge bases or have required additional user intervention. In contrast, we propose to utilize annotated data to learn how to perform semantic parsing of requirements automatically. 3 Data Set Given our conviction that mapping natural language software requirements to formal representations provides an attractive</context>
</contexts>
<marker>Ghosh, Elenius, Li, Lincoln, Shankar, Steiner, 2014</marker>
<rawString>Shalini Ghosh, Daniel Elenius, Wenchao Li, Patrick Lincoln, Natarajan Shankar, and Wilfried Steiner. 2014. Automatically extracting requirements specifications from natural language. arXiv preprint arXiv:1403.3142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H M Harmain</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Cmbuilder: A natural language-based case tool for object-oriented analysis.</title>
<date>2003</date>
<journal>Automated Software Engineering,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="5376" citStr="Harmain and Gaizauskas (2003)" startWordPosition="797" endWordPosition="800">ough Saeki et al. found resulting object diagrams of reasonable quality, they concluded that human intervention was still necessary to distinguish between words that are relevant for the model and irrelevant nouns and verbs. Nanduri and Rugaber (1995) proposed to further automate object-oriented analysis of requirement texts by applying a syntactic parser and a set of post-processing rules. In a similar setting, Mich (1996) employed a full NLP pipeline that contains a semantic analysis module, thus omitting the need for additional post-processing rules. More recent approaches include those by Harmain and Gaizauskas (2003) and Kof (2004), who relied on a combination of NLP components and human interaction. Whereas most approaches in previous work aim to derive class diagrams, Ghosh et al. (2014) proposed a pipeline architecture that converts syntactic parses to logical expressions via a set of heuristic post-processing rules. Despite this seemingly long tradition, previous methods for processing software requirements have tended to depend on domain-specific heuristics and knowledge bases or have required additional user intervention. In contrast, we propose to utilize annotated data to learn how to perform sema</context>
</contexts>
<marker>Harmain, Gaizauskas, 2003</marker>
<rawString>H. M. Harmain and Robert Gaizauskas. 2003. Cmbuilder: A natural language-based case tool for object-oriented analysis. Automated Software Engineering, 10(2):157–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonid Kof</author>
</authors>
<title>Natural language processing for requirements engineering: Applicability to large requirements documents.</title>
<date>2004</date>
<booktitle>In 19th International Conference on Automated Software Engineering, Workshop Proceedings.</booktitle>
<contexts>
<context position="5391" citStr="Kof (2004)" startWordPosition="802" endWordPosition="803">object diagrams of reasonable quality, they concluded that human intervention was still necessary to distinguish between words that are relevant for the model and irrelevant nouns and verbs. Nanduri and Rugaber (1995) proposed to further automate object-oriented analysis of requirement texts by applying a syntactic parser and a set of post-processing rules. In a similar setting, Mich (1996) employed a full NLP pipeline that contains a semantic analysis module, thus omitting the need for additional post-processing rules. More recent approaches include those by Harmain and Gaizauskas (2003) and Kof (2004), who relied on a combination of NLP components and human interaction. Whereas most approaches in previous work aim to derive class diagrams, Ghosh et al. (2014) proposed a pipeline architecture that converts syntactic parses to logical expressions via a set of heuristic post-processing rules. Despite this seemingly long tradition, previous methods for processing software requirements have tended to depend on domain-specific heuristics and knowledge bases or have required additional user intervention. In contrast, we propose to utilize annotated data to learn how to perform semantic parsing of</context>
</contexts>
<marker>Kof, 2004</marker>
<rawString>Leonid Kof. 2004. Natural language processing for requirements engineering: Applicability to large requirements documents. In 19th International Conference on Automated Software Engineering, Workshop Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Mich</author>
<author>Franch Mariangela</author>
<author>Novi Inverardi Pierluigi</author>
</authors>
<title>Market research for requirements analysis using linguistic tools.</title>
<date>2004</date>
<journal>Requirements Engineering,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="1707" citStr="Mich et al., 2004" startWordPosition="233" endWordPosition="236">ction During the process of software development, developers and customers typically discuss and agree on requirements that specify the functionality of a system that is being developed.1 Such requirements play a crucial role in the development lifecycle, as they form the basis for actual implementations, corresponding work plans, cost estimations and follow-up directives (van Lamsweerde, 2009). In general, software requirements can be expressed in various different ways, including the use of UML diagrams and storyboards. Most commonly, however, expectations are expressed in natural language (Mich et al., 2004), as shown in Example (1): (1) A user should be able to login to his account. 1Although software engineering can also involve nonfunctional requirements, which describe general quality criteria of a system, this paper is only concerned with functional requirements, i.e., requirements that specify the behavior of a system. While requirements expressed in natural language have the advantage of being intelligible to both clients and developers, they can of course also be ambiguous, vague and incomplete. Although formal languages could be used as an alternative that eliminates some of these proble</context>
</contexts>
<marker>Mich, Mariangela, Pierluigi, 2004</marker>
<rawString>Luisa Mich, Franch Mariangela, and Novi Inverardi Pierluigi. 2004. Market research for requirements analysis using linguistic tools. Requirements Engineering, 9(1):40–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Mich</author>
</authors>
<title>NL-OOPS: From natural language to object oriented requirements using the natural language processing system LOLITA.</title>
<date>1996</date>
<journal>Natural Language Engineering,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="5174" citStr="Mich (1996)" startWordPosition="769" endWordPosition="770">e that automatically constructs object-oriented models from informal requirements. As proposed by Abbott and Booch, the system is based on automatically extracted nouns and verbs. Although Saeki et al. found resulting object diagrams of reasonable quality, they concluded that human intervention was still necessary to distinguish between words that are relevant for the model and irrelevant nouns and verbs. Nanduri and Rugaber (1995) proposed to further automate object-oriented analysis of requirement texts by applying a syntactic parser and a set of post-processing rules. In a similar setting, Mich (1996) employed a full NLP pipeline that contains a semantic analysis module, thus omitting the need for additional post-processing rules. More recent approaches include those by Harmain and Gaizauskas (2003) and Kof (2004), who relied on a combination of NLP components and human interaction. Whereas most approaches in previous work aim to derive class diagrams, Ghosh et al. (2014) proposed a pipeline architecture that converts syntactic parses to logical expressions via a set of heuristic post-processing rules. Despite this seemingly long tradition, previous methods for processing software requirem</context>
</contexts>
<marker>Mich, 1996</marker>
<rawString>Luisa Mich. 1996. NL-OOPS: From natural language to object oriented requirements using the natural language processing system LOLITA. Natural Language Engineering, 2(2):161–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sastry Nanduri</author>
<author>Spencer Rugaber</author>
</authors>
<title>Requirements validation via automated natural language parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the Twenty-Eighth Hawaii International Conference on System Sciences,</booktitle>
<volume>3</volume>
<pages>362--368</pages>
<contexts>
<context position="4998" citStr="Nanduri and Rugaber (1995)" startWordPosition="741" endWordPosition="744"> as operators between them. Booch (1986) described a method of similar complexity that extends Abbot’s approach to objectoriented development. Saeki et al. (1989) implemented a first prototype that automatically constructs object-oriented models from informal requirements. As proposed by Abbott and Booch, the system is based on automatically extracted nouns and verbs. Although Saeki et al. found resulting object diagrams of reasonable quality, they concluded that human intervention was still necessary to distinguish between words that are relevant for the model and irrelevant nouns and verbs. Nanduri and Rugaber (1995) proposed to further automate object-oriented analysis of requirement texts by applying a syntactic parser and a set of post-processing rules. In a similar setting, Mich (1996) employed a full NLP pipeline that contains a semantic analysis module, thus omitting the need for additional post-processing rules. More recent approaches include those by Harmain and Gaizauskas (2003) and Kof (2004), who relied on a combination of NLP components and human interaction. Whereas most approaches in previous work aim to derive class diagrams, Ghosh et al. (2014) proposed a pipeline architecture that convert</context>
</contexts>
<marker>Nanduri, Rugaber, 1995</marker>
<rawString>Sastry Nanduri and Spencer Rugaber. 1995. Requirements validation via automated natural language parsing. In Proceedings of the Twenty-Eighth Hawaii International Conference on System Sciences, volume 3, pages 362–368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Motoshi Saeki</author>
<author>Hisayuki Horai</author>
<author>Hajime Enomoto</author>
</authors>
<title>Software development process from natural language specification.</title>
<date>1989</date>
<booktitle>In Proceedings of the 11th International Conference on Software Engineering,</booktitle>
<pages>64--73</pages>
<contexts>
<context position="4534" citStr="Saeki et al. (1989)" startWordPosition="669" endWordPosition="672">dings of the ACL 2014 Workshop on Semantic Parsing, pages 50–54, Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics of our knowledge, Abbott (1983) was the first to introduce a technique for extracting data types, variables and operators from informal texts describing a problem. The proposed method follows a simple rule-based setup, in which common nouns are identified as data types, proper nouns as objects and verbs as operators between them. Booch (1986) described a method of similar complexity that extends Abbot’s approach to objectoriented development. Saeki et al. (1989) implemented a first prototype that automatically constructs object-oriented models from informal requirements. As proposed by Abbott and Booch, the system is based on automatically extracted nouns and verbs. Although Saeki et al. found resulting object diagrams of reasonable quality, they concluded that human intervention was still necessary to distinguish between words that are relevant for the model and irrelevant nouns and verbs. Nanduri and Rugaber (1995) proposed to further automate object-oriented analysis of requirement texts by applying a syntactic parser and a set of post-processing </context>
</contexts>
<marker>Saeki, Horai, Enomoto, 1989</marker>
<rawString>Motoshi Saeki, Hisayuki Horai, and Hajime Enomoto. 1989. Software development process from natural language specification. In Proceedings of the 11th International Conference on Software Engineering, pages 64–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lappoon R Tang</author>
</authors>
<title>Integrating Top-down and Bottom-up Approaches in Inductive Logic Programming: Applications in Natural Language Processing and Relational Data Mining.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Sciences, University of Texas,</institution>
<location>Austin, Texas, USA,</location>
<contexts>
<context position="8164" citStr="Tang, 2003" startWordPosition="1229" endWordPosition="1230">oftware engineering classes are often provided with explicit guidelines on how to concisely express requirements in natural language. As a consequence, we also find their requirement texts to be more regimented and stylised than those written by senior software engineers. Examples (2) and (3) show examples of a student-written and developer-written requirement, respectively. (2) The user must be able to vote on polls. (3) For each user contact, back-end must perform a check to determine whether the contact is a registered user or not. In comparison to two extant data sets, namely GeoQuery880 (Tang, 2003) and Free917 (Cai and Yates, 2013), we find that our collection is still relatively small in terms of example sentences. The 2The majority of collected requirements are from a software development course organized jointly by several European universities, cf. http://www.fer.unizg.hr/rasip/dsd 3http://www.scasefp7.eu/ 51 Figure 1: Class hierarchy of our conceptual ontology for modeling software requirements. difference in total number of tokens is not as crucial, however, given that sentences in our data set are much longer on average. We further observe that the token/type ratio in our texts l</context>
</contexts>
<marker>Tang, 2003</marker>
<rawString>Lappoon R. Tang. 2003. Integrating Top-down and Bottom-up Approaches in Inductive Logic Programming: Applications in Natural Language Processing and Relational Data Mining. Ph.D. thesis, Department of Computer Sciences, University of Texas, Austin, Texas, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Axel van Lamsweerde</author>
</authors>
<title>Requirements Engineering: From System Goals to UML Models to Software Specifications.</title>
<date>2009</date>
<publisher>Wiley.</publisher>
<marker>van Lamsweerde, 2009</marker>
<rawString>Axel van Lamsweerde. 2009. Requirements Engineering: From System Goals to UML Models to Software Specifications. Wiley.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
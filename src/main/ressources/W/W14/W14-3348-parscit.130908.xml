<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004822">
<title confidence="0.9953745">
Meteor Universal: Language Specific Translation Evaluation
for Any Target Language
</title>
<author confidence="0.986995">
Michael Denkowski Alon Lavie
</author>
<affiliation confidence="0.825296">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
</affiliation>
<email confidence="0.999593">
{mdenkows,alavie}@cs.cmu.edu
</email>
<sectionHeader confidence="0.994819" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993458">
This paper describes Meteor Universal, re-
leased for the 2014 ACL Workshop on
Statistical Machine Translation. Meteor
Universal brings language specific evalu-
ation to previously unsupported target lan-
guages by (1) automatically extracting lin-
guistic resources (paraphrase tables and
function word lists) from the bitext used to
train MT systems and (2) using a univer-
sal parameter set learned from pooling hu-
man judgments of translation quality from
several language directions. Meteor Uni-
versal is shown to significantly outperform
baseline BLEU on two new languages,
Russian (WMT13) and Hindi (WMT14).
</bodyText>
<sectionHeader confidence="0.998774" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987595">
Recent WMT evaluations have seen a variety of
metrics employ language specific resources to
replicate human translation rankings far better
than simple baselines (Callison-Burch et al., 2011;
Callison-Burch et al., 2012; Mach´aˇcek and Bojar,
2013; Snover et al., 2009; Denkowski and Lavie,
2011; Dahlmeier et al., 2011; Chen et al., 2012;
Wang and Manning, 2012, inter alia). While the
wealth of linguistic resources for the WMT lan-
guages allows the development of sophisticated
metrics, most of the world’s 7,000+ languages lack
the prerequisites for building advanced metrics.
Researchers working on low resource languages
are usually limited to baseline BLEU (Papineni et
al., 2002) for evaluating translation quality.
Meteor Universal brings language specific eval-
uation to any target language by combining lin-
guistic resources automatically learned from MT
system training data with a universal metric pa-
rameter set that generalizes across languages.
Given only the bitext used to build a standard
phrase-based translation system, Meteor Universal
learns a paraphrase table and function word list,
two of the most consistently beneficial language
specific resources employed in versions of Me-
teor. Whereas previous versions of Meteor require
human ranking judgments in the target language
to learn parameters, Meteor Universal uses a sin-
gle parameter set learned from pooling judgments
from several languages. This universal parameter
set captures general preferences shown by human
evaluators across languages. We show this ap-
proach to significantly outperform baseline BLEU
for two new languages, Russian and Hindi. The
following sections review Meteor’s scoring func-
tion (§2), describe the automatic extraction of lan-
guage specific resources (§3), discuss training of
the universal parameter set (§4), report experimen-
tal results (§5), describe released software (§6),
and conclude (§7).
</bodyText>
<sectionHeader confidence="0.952214" genericHeader="method">
2 Meteor Scoring
</sectionHeader>
<bodyText confidence="0.97138475">
Meteor evaluates translation hypotheses by align-
ing them to reference translations and calculating
sentence-level similarity scores. For a hypothesis-
reference pair, the space of possible alignments is
constructed by exhaustively identifying all possi-
ble matches between the sentences according to
the following matchers:
Exact: Match words if their surface forms are
identical.
Stem: Stem words using a language appropriate
Snowball Stemmer (Porter, 2001) and match if the
stems are identical.
Synonym: Match words if they share member-
ship in any synonym set according to the WordNet
database (Miller and Fellbaum, 2007).
Paraphrase: Match phrases if they are listed as
</bodyText>
<page confidence="0.985947">
376
</page>
<bodyText confidence="0.871064">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376–380,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
paraphrases in a language appropriate paraphrase
table (described in §3.2).
All matches are generalized to phrase matches
with a span in each sentence. Any word occur-
ring within the span is considered covered by the
match. The final alignment is then resolved as the
largest subset of all matches meeting the following
criteria in order of importance:
</bodyText>
<listItem confidence="0.986383153846154">
1. Require each word in each sentence to be
covered by zero or one matches.
2. Maximize the number of covered words
across both sentences.
3. Minimize the number of chunks, where a
chunk is defined as a series of matches that
is contiguous and identically ordered in both
sentences.
4. Minimize the sum of absolute distances be-
tween match start indices in the two sen-
tences. (Break ties by preferring to align
phrases that occur at similar positions in both
sentences.)
</listItem>
<bodyText confidence="0.992304642857143">
Alignment resolution is conducted as a beam
search using a heuristic based on the above cri-
teria.
The Meteor score for an aligned sentence pair is
calculated as follows. Content and function words
are identified in the hypothesis (hc, hf) and ref-
erence (rc, rf) according to a function word list
(described in §3.1). For each of the matchers
(mi), count the number of content and function
words covered by matches of this type in the hy-
pothesis (mi(hc), mi(hf)) and reference (mi(rc),
mi(rf)). Calculate weighted precision and re-
call using matcher weights (wi...wn) and content-
function word weight (δ):
</bodyText>
<equation confidence="0.99983975">
P = Ei wi · (δ · mi(hc) + (1 − δ) · mi(hf))
δ · |hc |+ (1 − δ) · |hf|
R = Ei wi · (δ · mi (rc) + (1 − δ) · mi (rf ))
δ · |rc |+ (1 − δ)·|rf|
</equation>
<bodyText confidence="0.953141">
The parameterized harmonic mean of P and R
(van Rijsbergen, 1979) is then calculated:
</bodyText>
<equation confidence="0.996717">
P · R
α · P + (1 − α) · R
</equation>
<bodyText confidence="0.9962016">
To account for gaps and differences in word order,
a fragmentation penalty is calculated using the to-
tal number of matched words (m, averaged over
hypothesis and reference) and number of chunks
(ch):
</bodyText>
<equation confidence="0.920133">
Pen = γ ·
The Meteor score is then calculated:
Score = (1 − Pen) · Fmean
</equation>
<bodyText confidence="0.999279">
The parameters α, Q, γ, δ, and wi...wn are tuned
to maximize correlation with human judgments.
</bodyText>
<sectionHeader confidence="0.824746" genericHeader="method">
3 Language Specific Resources
</sectionHeader>
<bodyText confidence="0.999876727272727">
Meteor uses language specific resources to dra-
matically improve evaluation accuracy. While
some resources such as WordNet and the Snowball
stemmers are limited to one or a few languages,
other resources can be learned from data for any
language. Meteor Universal uses the same bitext
used to build statistical translation systems to learn
function words and paraphrases. Used in con-
junction with the universal parameter set, these re-
sources bring language specific evaluation to new
target languages.
</bodyText>
<subsectionHeader confidence="0.999788">
3.1 Function Word Lists
</subsectionHeader>
<bodyText confidence="0.999964545454545">
The function word list is used to discriminate be-
tween content and function words in the target lan-
guage. Meteor Universal counts words in the tar-
get side of the training bitext and considers any
word with relative frequency above 10−3 to be a
function word. This list is used only during the
scoring stage of evaluation, where the tunable δ
parameter controls the relative weight of content
versus function words. When tuned to match hu-
man judgments, this parameter usually reflects a
greater importance for content words.
</bodyText>
<subsectionHeader confidence="0.999877">
3.2 Paraphrase Tables
</subsectionHeader>
<bodyText confidence="0.999985142857143">
Paraphrase tables allow many-to-many matches
that can encapsulate any local language phenom-
ena, including morphology, synonymy, and true
paraphrasing. Identifying these matches allows
far more sophisticated evaluation than is possible
with simple surface form matches. In Meteor Uni-
versal, paraphrases act as the catch-all for non-
exact matches. Paraphrases are automatically ex-
tracted from the training bitext using the transla-
tion pivot approach (Bannard and Callison-Burch,
2005). First, a standard phrase table is learned
from the bitext (Koehn et al., 2003). Paraphrase
extraction then proceeds as follows. For each tar-
get language phrase (e1) in the table, find each
</bodyText>
<equation confidence="0.87288975">
Fmean =
(ch
)β
m
</equation>
<page confidence="0.982615">
377
</page>
<bodyText confidence="0.9991656">
source phrase f that e1 translates. Each alternate
phrase (e2 =6 e1) that translates f is considered
a paraphrase with probability P(f|e1) · P(e2|f).
The total probability of e2 being a paraphrase of
e1 is the sum over all possible pivot phrases f:
</bodyText>
<equation confidence="0.9453915">
�P(e2|e1) = P(f|e1) · P(e2|f)
f
</equation>
<bodyText confidence="0.999681">
To improve paraphrase precision, we apply
several language independent pruning techniques.
The following are applied to each paraphrase in-
stance (e1, f, e2):
</bodyText>
<listItem confidence="0.972664285714286">
• Discard instances with very low probability
(P(f|e1) · P(e2|f) &lt; 0.001).
• Discard instances where e1, f, or e2 contain
punctuation characters.
• Discard instances where e1, f, or e2 con-
tain only function words (relative frequency
above 10−3 in the bitext).
</listItem>
<bodyText confidence="0.9446605">
The following are applied to each final paraphrase
(e1, e2) after summing over all instances:
</bodyText>
<listItem confidence="0.98776925">
• Discard paraphrases with very low probabil-
ity (P(e2|e1) &lt; 0.01).
• Discard paraphrases where e2 is a sub-phrase
of e1.
</listItem>
<bodyText confidence="0.999928125">
This constitutes the full Meteor paraphrasing
pipeline that has been used to build tables for
fully supported languages (Denkowski and Lavie,
2011). Paraphrases for new languages have the
added advantage of being extracted from the same
bitext that MT systems use for phrase extraction,
resulting in ideal paraphrase coverage for evalu-
ated systems.
</bodyText>
<sectionHeader confidence="0.98231" genericHeader="method">
4 Universal Parameter Set
</sectionHeader>
<bodyText confidence="0.9997781">
Traditionally, building a version of Meteor for a
new target language has required a set of human-
scored machine translations, most frequently in
the form of WMT rankings. The general lack of
availability of these judgments has severely lim-
ited the number of languages for which Meteor
versions could be trained. Meteor Universal ad-
dresses this problem with the introduction of a
”universal” parameter set that captures general hu-
man preferences that apply to all languages for
</bodyText>
<table confidence="0.9991199">
Direction Judgments
cs-en 11,021
de-en 11,934
es-en 9,796
fr-en 11,594
en-cs 18,805
en-de 14,553
en-es 11,834
en-fr 11,562
Total 101,099
</table>
<tableCaption confidence="0.845081333333333">
Table 1: Binary ranking judgments per language
direction used to learn parameters for Meteor Uni-
versal
</tableCaption>
<bodyText confidence="0.998193818181818">
which judgment data does exist. We learn this pa-
rameter set by pooling over 100,000 binary rank-
ing judgments from WMT12 (Callison-Burch et
al., 2012) that cover 8 language directions (de-
tails in Table 1). Data for each language is scored
using the same resources (function word list and
paraphrase table only) and scoring parameters are
tuned to maximize agreement (Kendall’s r) over
all judgments from all languages, leading to a sin-
gle parameter set. The universal parameter set en-
codes the following general human preferences:
</bodyText>
<listItem confidence="0.974244857142857">
• Prefer recall over precision.
• Prefer word choice over word order.
• Prefer correct translations of content words
over function words.
• Prefer exact matches over paraphrase
matches, while still giving significant credit
to paraphrases.
</listItem>
<bodyText confidence="0.9991012">
Table 2 compares the universal parameters to those
learned for specific languages in previous versions
of Meteor. Notably, the universal parameter set is
more balanced, showing a normalizing effect from
generalizing across several language directions.
</bodyText>
<sectionHeader confidence="0.999365" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999993888888889">
We evaluate the Universal version of Meteor
against full language dedicated versions of Meteor
and baseline BLEU on the WMT13 rankings. Re-
sults for English, Czech, German, Spanish, and
French are biased in favor of Meteor Universal
since rankings for these target languages are in-
cluded in the training data while Russian consti-
tutes a true held out test. We also report the re-
sults of the WMT14 Hindi evaluation task. Shown
</bodyText>
<page confidence="0.996614">
378
</page>
<table confidence="0.999740571428571">
Language α Q γ S wexact wstem wsyn wpar
English 0.85 0.20 0.60 0.75 1.00 0.60 0.80 0.60
Czech 0.95 0.20 0.60 0.80 1.00 – – 0.40
German 0.95 1.00 0.55 0.55 1.00 0.80 – 0.20
Spanish 0.65 1.30 0.50 0.80 1.00 0.80 – 0.60
French 0.90 1.40 0.60 0.65 1.00 0.20 – 0.40
Universal 0.70 1.40 0.30 0.70 1.00 – – 0.60
</table>
<tableCaption confidence="0.996269">
Table 2: Comparison of parameters for language specific and universal versions of Meteor.
</tableCaption>
<table confidence="0.999899777777778">
WMT13 r M-Full M-Universal BLEU
English 0.214 0.206 0.124
Czech 0.092 0.085 0.044
German 0.163 0.157 0.097
Spanish 0.106 0.101 0.068
French 0.150 0.137 0.099
Russian – 0.128 0.068
WMT14 r M-Full M-Universal BLEU
Hindi – 0.264 0.227
</table>
<tableCaption confidence="0.690167333333333">
Table 3: Sentence-level correlation with human
rankings (Kendall’s r) for Meteor (language spe-
cific versions), Meteor Universal, and BLEU
</tableCaption>
<bodyText confidence="0.999921428571429">
in Table 3, Meteor Universal significantly out-
performs baseline BLEU in all cases while suf-
fering only slight degradation compared to ver-
sions of Meteor tuned for individual languages.
For Russian, correlation is nearly double that of
BLEU. This provides substantial evidence that
Meteor Universal will further generalize, bringing
improved evaluation accuracy to new target lan-
guages currently limited to baseline language in-
dependent metrics.
For the WMT14 evaluation, we use the tradi-
tional language specific versions of Meteor for all
language directions except Hindi. This includes
Russian, for which additional language specific re-
sources (a Snowball word stemmer) help signifi-
cantly. For Hindi, we use the release version of
Meteor Universal to extract linguistic resources
from the constrained training bitext provided for
the shared translation task. These resources are
used with the universal parameter set to score all
system outputs for the English–Hindi direction.
</bodyText>
<sectionHeader confidence="0.968212" genericHeader="method">
6 Software
</sectionHeader>
<footnote confidence="0.665077">
Meteor Universal is included in Meteor version
1.5 which is publicly released for WMT14.
</footnote>
<bodyText confidence="0.988249705882353">
Meteor 1.5 can be downloaded from the official
webpage1 and a full tutorial for Meteor Universal
is available online.2 Building a version of Meteor
for a new language requires a training bitext
(corpus.f, corpus.e) and a standard Moses format
phrase table (phrase-table.gz) (Koehn et al.,
2007). To extract linguistic resources for Meteor,
run the new language script:
$ python scripts/new_language.py out
corpus.f corpus.e phrase-table.gz
To use the resulting files to score translations with
Meteor, use the new language option:
$ java -jar meteor-*.jar test ref -new
out/meteor-files
Meteor 1.5, including Meteor Universal, is free
software released under the terms of the GNU
Lesser General Public License.
</bodyText>
<sectionHeader confidence="0.998441" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999980375">
This paper describes Meteor Universal, a version
of the Meteor metric that brings language specific
evaluation to any target language using the same
resources used to build statistical translation sys-
tems. Held out tests show Meteor Universal to sig-
nificantly outperform baseline BLEU on WMT13
Russian and WMT14 Hindi. Meteor version 1.5 is
freely available open source software.
</bodyText>
<sectionHeader confidence="0.994221" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9972205">
This work is supported in part by the National Sci-
ence Foundation under grant IIS-0915327, by the
Qatar National Research Fund (a member of the
Qatar Foundation) under grant NPRP 09-1140-1-
177, and by the NSF-sponsored XSEDE program
under grant TG-CCR110017.
</bodyText>
<footnote confidence="0.96287">
1http://www.cs.cmu.edu/~alavie/METEOR/
2http://www.cs.cmu.edu/~mdenkows/meteor-
universal.html
</footnote>
<page confidence="0.997963">
379
</page>
<sectionHeader confidence="0.983613" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999847939024391">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL’05), pages
597–604, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 22–64, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10–51, Montr´eal, Canada, June. Association for
Computational Linguistics.
Boxing Chen, Roland Kuhn, and George Foster. 2012.
Improving amber, an mt evaluation metric. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, pages 59–63, Montr´eal, Canada,
June. Association for Computational Linguistics.
Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng.
2011. Tesla at wmt 2011: Translation evaluation and
tunable metric. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 78–
84, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85–91, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL/HLT 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Matouˇs Mach´aˇcek and Ondˇrej Bojar. 2013. Results of
the WMT13 metrics shared task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 45–51, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
George Miller and Christiane Fellbaum. 2007. Word-
Net. http://wordnet.princeton.edu/.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July.
Martin Porter. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/texts/.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with
a tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
259–268, Athens, Greece, March. Association for
Computational Linguistics.
C. J. van Rijsbergen, 1979. Information Retrieval,
chapter 7. Butterworths, London, UK, 2nd edition.
Mengqiu Wang and Christopher Manning. 2012.
Spede: Probabilistic edit distance metrics for mt
evaluation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 76–
83, Montr´eal, Canada, June. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.998273">
380
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.432179">
<title confidence="0.999336">Meteor Universal: Language Specific Translation for Any Target Language</title>
<author confidence="0.988689">Michael Denkowski Alon</author>
<affiliation confidence="0.846612">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.999436">Pittsburgh, PA 15213</address>
<abstract confidence="0.998429333333333">This paper describes Meteor Universal, released for the 2014 ACL Workshop on Statistical Machine Translation. Meteor Universal brings language specific evaluation to previously unsupported target languages by (1) automatically extracting linguistic resources (paraphrase tables and function word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages,</abstract>
<note confidence="0.639688">Russian (WMT13) and Hindi (WMT14).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>597--604</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="7245" citStr="Bannard and Callison-Burch, 2005" startWordPosition="1138" endWordPosition="1141">rsus function words. When tuned to match human judgments, this parameter usually reflects a greater importance for content words. 3.2 Paraphrase Tables Paraphrase tables allow many-to-many matches that can encapsulate any local language phenomena, including morphology, synonymy, and true paraphrasing. Identifying these matches allows far more sophisticated evaluation than is possible with simple surface form matches. In Meteor Universal, paraphrases act as the catch-all for nonexact matches. Paraphrases are automatically extracted from the training bitext using the translation pivot approach (Bannard and Callison-Burch, 2005). First, a standard phrase table is learned from the bitext (Koehn et al., 2003). Paraphrase extraction then proceeds as follows. For each target language phrase (e1) in the table, find each Fmean = (ch )β m 377 source phrase f that e1 translates. Each alternate phrase (e2 =6 e1) that translates f is considered a paraphrase with probability P(f|e1) · P(e2|f). The total probability of e2 being a paraphrase of e1 is the sum over all possible pivot phrases f: �P(e2|e1) = P(f|e1) · P(e2|f) f To improve paraphrase precision, we apply several language independent pruning techniques. The following ar</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 597–604, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>22--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="1042" citStr="Callison-Burch et al., 2011" startWordPosition="141" endWordPosition="144">pported target languages by (1) automatically extracting linguistic resources (paraphrase tables and function word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14). 1 Introduction Recent WMT evaluations have seen a variety of metrics employ language specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´aˇcek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia). While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world’s 7,000+ languages lack the prerequisites for building advanced metrics. Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality. Meteor Universal brings language specific evaluation to any target </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22–64, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1071" citStr="Callison-Burch et al., 2012" startWordPosition="145" endWordPosition="148">1) automatically extracting linguistic resources (paraphrase tables and function word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14). 1 Introduction Recent WMT evaluations have seen a variety of metrics employ language specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´aˇcek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia). While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world’s 7,000+ languages lack the prerequisites for building advanced metrics. Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality. Meteor Universal brings language specific evaluation to any target language by combining linguis</context>
<context position="9612" citStr="Callison-Burch et al., 2012" startWordPosition="1522" endWordPosition="1525">mited the number of languages for which Meteor versions could be trained. Meteor Universal addresses this problem with the introduction of a ”universal” parameter set that captures general human preferences that apply to all languages for Direction Judgments cs-en 11,021 de-en 11,934 es-en 9,796 fr-en 11,594 en-cs 18,805 en-de 14,553 en-es 11,834 en-fr 11,562 Total 101,099 Table 1: Binary ranking judgments per language direction used to learn parameters for Meteor Universal which judgment data does exist. We learn this parameter set by pooling over 100,000 binary ranking judgments from WMT12 (Callison-Burch et al., 2012) that cover 8 language directions (details in Table 1). Data for each language is scored using the same resources (function word list and paraphrase table only) and scoring parameters are tuned to maximize agreement (Kendall’s r) over all judgments from all languages, leading to a single parameter set. The universal parameter set encodes the following general human preferences: • Prefer recall over precision. • Prefer word choice over word order. • Prefer correct translations of content words over function words. • Prefer exact matches over paraphrase matches, while still giving significant cr</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
<author>George Foster</author>
</authors>
<title>Improving amber, an mt evaluation metric.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>59--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1190" citStr="Chen et al., 2012" startWordPosition="165" endWordPosition="168">tems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14). 1 Introduction Recent WMT evaluations have seen a variety of metrics employ language specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´aˇcek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia). While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world’s 7,000+ languages lack the prerequisites for building advanced metrics. Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality. Meteor Universal brings language specific evaluation to any target language by combining linguistic resources automatically learned from MT system training data with a universal metric parameter set that generalizes</context>
</contexts>
<marker>Chen, Kuhn, Foster, 2012</marker>
<rawString>Boxing Chen, Roland Kuhn, and George Foster. 2012. Improving amber, an mt evaluation metric. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 59–63, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Chang Liu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Tesla at wmt 2011: Translation evaluation and tunable metric.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>78--84</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="1171" citStr="Dahlmeier et al., 2011" startWordPosition="161" endWordPosition="164">ext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14). 1 Introduction Recent WMT evaluations have seen a variety of metrics employ language specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´aˇcek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia). While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world’s 7,000+ languages lack the prerequisites for building advanced metrics. Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality. Meteor Universal brings language specific evaluation to any target language by combining linguistic resources automatically learned from MT system training data with a universal metric parameter s</context>
</contexts>
<marker>Dahlmeier, Liu, Ng, 2011</marker>
<rawString>Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng. 2011. Tesla at wmt 2011: Translation evaluation and tunable metric. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 78– 84, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>85--91</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="1147" citStr="Denkowski and Lavie, 2011" startWordPosition="157" endWordPosition="160">on word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14). 1 Introduction Recent WMT evaluations have seen a variety of metrics employ language specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´aˇcek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia). While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world’s 7,000+ languages lack the prerequisites for building advanced metrics. Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality. Meteor Universal brings language specific evaluation to any target language by combining linguistic resources automatically learned from MT system training data with a univ</context>
<context position="8518" citStr="Denkowski and Lavie, 2011" startWordPosition="1349" endWordPosition="1352">2): • Discard instances with very low probability (P(f|e1) · P(e2|f) &lt; 0.001). • Discard instances where e1, f, or e2 contain punctuation characters. • Discard instances where e1, f, or e2 contain only function words (relative frequency above 10−3 in the bitext). The following are applied to each final paraphrase (e1, e2) after summing over all instances: • Discard paraphrases with very low probability (P(e2|e1) &lt; 0.01). • Discard paraphrases where e2 is a sub-phrase of e1. This constitutes the full Meteor paraphrasing pipeline that has been used to build tables for fully supported languages (Denkowski and Lavie, 2011). Paraphrases for new languages have the added advantage of being extracted from the same bitext that MT systems use for phrase extraction, resulting in ideal paraphrase coverage for evaluated systems. 4 Universal Parameter Set Traditionally, building a version of Meteor for a new target language has required a set of humanscored machine translations, most frequently in the form of WMT rankings. The general lack of availability of these judgments has severely limited the number of languages for which Meteor versions could be trained. Meteor Universal addresses this problem with the introductio</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 85–91, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL/HLT</booktitle>
<contexts>
<context position="7325" citStr="Koehn et al., 2003" startWordPosition="1152" endWordPosition="1155">eater importance for content words. 3.2 Paraphrase Tables Paraphrase tables allow many-to-many matches that can encapsulate any local language phenomena, including morphology, synonymy, and true paraphrasing. Identifying these matches allows far more sophisticated evaluation than is possible with simple surface form matches. In Meteor Universal, paraphrases act as the catch-all for nonexact matches. Paraphrases are automatically extracted from the training bitext using the translation pivot approach (Bannard and Callison-Burch, 2005). First, a standard phrase table is learned from the bitext (Koehn et al., 2003). Paraphrase extraction then proceeds as follows. For each target language phrase (e1) in the table, find each Fmean = (ch )β m 377 source phrase f that e1 translates. Each alternate phrase (e2 =6 e1) that translates f is considered a paraphrase with probability P(f|e1) · P(e2|f). The total probability of e2 being a paraphrase of e1 is the sum over all possible pivot phrases f: �P(e2|e1) = P(f|e1) · P(e2|f) f To improve paraphrase precision, we apply several language independent pruning techniques. The following are applied to each paraphrase instance (e1, f, e2): • Discard instances with very</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of NAACL/HLT 2003.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="13066" citStr="Koehn et al., 2007" startWordPosition="2069" endWordPosition="2072">tract linguistic resources from the constrained training bitext provided for the shared translation task. These resources are used with the universal parameter set to score all system outputs for the English–Hindi direction. 6 Software Meteor Universal is included in Meteor version 1.5 which is publicly released for WMT14. Meteor 1.5 can be downloaded from the official webpage1 and a full tutorial for Meteor Universal is available online.2 Building a version of Meteor for a new language requires a training bitext (corpus.f, corpus.e) and a standard Moses format phrase table (phrase-table.gz) (Koehn et al., 2007). To extract linguistic resources for Meteor, run the new language script: $ python scripts/new_language.py out corpus.f corpus.e phrase-table.gz To use the resulting files to score translations with Meteor, use the new language option: $ java -jar meteor-*.jar test ref -new out/meteor-files Meteor 1.5, including Meteor Universal, is free software released under the terms of the GNU Lesser General Public License. 7 Conclusion This paper describes Meteor Universal, a version of the Meteor metric that brings language specific evaluation to any target language using the same resources used to bui</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matouˇs Mach´aˇcek</author>
<author>Ondˇrej Bojar</author>
</authors>
<title>Results of the WMT13 metrics shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>45--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Mach´aˇcek, Bojar, 2013</marker>
<rawString>Matouˇs Mach´aˇcek and Ondˇrej Bojar. 2013. Results of the WMT13 metrics shared task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 45–51, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Christiane Fellbaum</author>
</authors>
<date>2007</date>
<note>WordNet. http://wordnet.princeton.edu/.</note>
<contexts>
<context position="3386" citStr="Miller and Fellbaum, 2007" startWordPosition="486" endWordPosition="489">oring Meteor evaluates translation hypotheses by aligning them to reference translations and calculating sentence-level similarity scores. For a hypothesisreference pair, the space of possible alignments is constructed by exhaustively identifying all possible matches between the sentences according to the following matchers: Exact: Match words if their surface forms are identical. Stem: Stem words using a language appropriate Snowball Stemmer (Porter, 2001) and match if the stems are identical. Synonym: Match words if they share membership in any synonym set according to the WordNet database (Miller and Fellbaum, 2007). Paraphrase: Match phrases if they are listed as 376 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376–380, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics paraphrases in a language appropriate paraphrase table (described in §3.2). All matches are generalized to phrase matches with a span in each sentence. Any word occurring within the span is considered covered by the match. The final alignment is then resolved as the largest subset of all matches meeting the following criteria in order of importance: 1. Require each w</context>
</contexts>
<marker>Miller, Fellbaum, 2007</marker>
<rawString>George Miller and Christiane Fellbaum. 2007. WordNet. http://wordnet.princeton.edu/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evalution of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="1538" citStr="Papineni et al., 2002" startWordPosition="217" endWordPosition="220">guage specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´aˇcek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia). While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world’s 7,000+ languages lack the prerequisites for building advanced metrics. Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality. Meteor Universal brings language specific evaluation to any target language by combining linguistic resources automatically learned from MT system training data with a universal metric parameter set that generalizes across languages. Given only the bitext used to build a standard phrase-based translation system, Meteor Universal learns a paraphrase table and function word list, two of the most consistently beneficial language specific resources employed in versions of Meteor. Whereas previous versions of Meteor require human ranking judgments in the target </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evalution of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
</authors>
<title>Snowball: A language for stemming algorithms.</title>
<date>2001</date>
<note>http://snowball.tartarus.org/texts/.</note>
<contexts>
<context position="3221" citStr="Porter, 2001" startWordPosition="460" endWordPosition="461"> discuss training of the universal parameter set (§4), report experimental results (§5), describe released software (§6), and conclude (§7). 2 Meteor Scoring Meteor evaluates translation hypotheses by aligning them to reference translations and calculating sentence-level similarity scores. For a hypothesisreference pair, the space of possible alignments is constructed by exhaustively identifying all possible matches between the sentences according to the following matchers: Exact: Match words if their surface forms are identical. Stem: Stem words using a language appropriate Snowball Stemmer (Porter, 2001) and match if the stems are identical. Synonym: Match words if they share membership in any synonym set according to the WordNet database (Miller and Fellbaum, 2007). Paraphrase: Match phrases if they are listed as 376 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376–380, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics paraphrases in a language appropriate paraphrase table (described in §3.2). All matches are generalized to phrase matches with a span in each sentence. Any word occurring within the span is considered cov</context>
</contexts>
<marker>Porter, 2001</marker>
<rawString>Martin Porter. 2001. Snowball: A language for stemming algorithms. http://snowball.tartarus.org/texts/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>259--268</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="1120" citStr="Snover et al., 2009" startWordPosition="153" endWordPosition="156">ase tables and function word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14). 1 Introduction Recent WMT evaluations have seen a variety of metrics employ language specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´aˇcek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia). While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world’s 7,000+ languages lack the prerequisites for building advanced metrics. Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality. Meteor Universal brings language specific evaluation to any target language by combining linguistic resources automatically learned from MT syste</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 259–268, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<date>1979</date>
<booktitle>Information Retrieval, chapter 7.</booktitle>
<location>Butterworths, London, UK,</location>
<note>2nd edition.</note>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. J. van Rijsbergen, 1979. Information Retrieval, chapter 7. Butterworths, London, UK, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher Manning</author>
</authors>
<title>Spede: Probabilistic edit distance metrics for mt evaluation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>76--83</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1214" citStr="Wang and Manning, 2012" startWordPosition="169" endWordPosition="172">a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14). 1 Introduction Recent WMT evaluations have seen a variety of metrics employ language specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´aˇcek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia). While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world’s 7,000+ languages lack the prerequisites for building advanced metrics. Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality. Meteor Universal brings language specific evaluation to any target language by combining linguistic resources automatically learned from MT system training data with a universal metric parameter set that generalizes across languages. Given</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Mengqiu Wang and Christopher Manning. 2012. Spede: Probabilistic edit distance metrics for mt evaluation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 76– 83, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008590">
<title confidence="0.998917">
A Two-Stage Approach for Computing Associative Responses to a Set of
Stimulus Words
</title>
<author confidence="0.996907">
Urmi Ghosh, Sambhav Jain and Soma Paul
</author>
<affiliation confidence="0.8455145">
Language Technologies Research Center
IIIT-Hyderabad, India
</affiliation>
<email confidence="0.944977">
{urmi.ghosh, sambhav.fain}@research.iiit.ac.in,
soma@iiit.ac.in
</email>
<sectionHeader confidence="0.997274" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9982558">
This paper describes the system submitted by the IIIT-H team for the CogALex-2014 shared task
on multiword association. The task involves generating a ranked list of responses to a set of
stimulus words. The two-stage approach combines the strength of neural network based word
embeddings and frequency based association measures. The system achieves an accuracy of
34.9% over the test set.
</bodyText>
<sectionHeader confidence="0.99964" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925791666667">
Research in psychology gives evidence that word associations reveal the respondents’ perception, learn-
ing and verbal memories and thus determine language production. Hence, it is possible to simulate
human derived word associations by analyzing the statistical distribution of words in a corpus. Church
and Hanks (1990) and Wettler and Rapp (1989) were amongst the first to devise association measures by
utilizing frequencies and co-occurrences from large corpora. Wettler and Rapp (1993) demonstrate that
corpus-based computations of word associations are similar to association norms collected from human
subjects.
The CogALex-2014 shared task on multi-word association involves generating a ranked list of re-
sponse words for a given set of stimulus words. For example, the stimulus word bank can invoke as-
sociative responses such as river, loan, finance and money. Priming1 bank with bed and bridge, results
in strengthening association with the word river and it emerges as the best response amongst the afore-
mentioned response choices. This task is motivated by the tip-of-the-tongue problem, where associated
concepts from the memory can help recall the target word. Other practical applications include query ex-
pansion for information retrieval and natural language generation where missing words can be predicted
from their context.
The participating systems are distinguished into two categories - Unrestricted systems that allows
usage of any kind of data and Restricted systems that can only make use of the ukWaC (Baroni et al.,
2009) corpus, consisting of two billion tokens. Our proposed system falls in the restricted track since
we only used ukWaC for extracting information on word associations. It follows a two-staged approach:
Candidate Response Generation, which involves selection of words that are semantically similar to the
primes and Re-ranking by Association Measure, that re-ranks the responses using a proposed weighted
Pointwise Mutual Information (wPMI) measure. Our system was evaluated on test-datasets derived
from the Edinburgh Associative Thesaurus (Kiss et al., 1972) and it achieved an accuracy of 34.9%.
When ignoring the inflectional variations of the response word, an accuracy of 39.55% was achieved.
</bodyText>
<sectionHeader confidence="0.987135" genericHeader="method">
2 Observations on Training Data
</sectionHeader>
<bodyText confidence="0.9914995">
The training set consists of 2000 sets of five words (multiword stimuli or primes) and the word that is
most closely associated to all of them (associative response). For example, a set of primes such as wheel,
driver, bus, drive and lorry are given along with the expected associative response - car.
In this section, our initial observations on the given training data are enlisted.
</bodyText>
<footnote confidence="0.998077333333333">
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1The phenomenon of providing multiple stimulus words is called priming.
</footnote>
<page confidence="0.934514">
15
</page>
<note confidence="0.340206">
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 15–21,
Dublin, Ireland, August 23, 2014.
</note>
<subsectionHeader confidence="0.926304">
2.1 Relation between the Associative Response and the Prime Words
</subsectionHeader>
<bodyText confidence="0.932399">
It is observed that a response largely exhibits two kind of relations with a priming word.
</bodyText>
<table confidence="0.65115275">
Primes Associative Response
presents, Christmas, birthday, shops, present gifts
butterfly, light, ball, fly, insect moth
mouse, cat, catcher, race, tail rat
</table>
<tableCaption confidence="0.99874">
Table 1: Some examples of primes and their associative responses from the training set
</tableCaption>
<bodyText confidence="0.9953342">
Type A relation depicts a synonymous/antonymous behavior or “of the same kind” nature. Word pairs
with paradigmatic relation are highly semantically related and belong to the same part of speech. And
hence, they tend to show a substitutive nature amongst themselves without affecting the grammar of the
sentence. From Table - 1, we observe that present/presents , butterfly/insect and mouse/cat can be substi-
tuted in place of gifts, moth and rat respectively. Type B relation depicts contextual co-occurrence, where
the words tend to occur together or form a collocation. This kind of relationship can be demonstrated by
taking examples from Table - 1, such as Christmas gifts, gift shops, birthday gifts, moth ball, rat catcher,
rat race and rat tail. In theory, the above have been formally categorized as paradigmatic (Type A) and
syntagmatic (Type B ) relations by De Saussure et al. (1916) and we will be referring to them accordingly
in rest of the paper.
Type C relation, depicting associations based on the phonological component of the words was also
observed. According to McCarthy (1990), responses can be affected by phonological shapes and or-
thographic patterns especially when instantaneous paradigmatic or syntagmatic association is difficult.
Examples from the training data set include ajar-Ajax, hypothalamus-hippopotamus and cravat-caravan.
Such examples were very few and hence, have not been dealt with in this paper.
</bodyText>
<subsectionHeader confidence="0.999801">
2.2 Context Window Size
</subsectionHeader>
<bodyText confidence="0.999654">
Words exhibiting syntagmatic associations often occur in close proximity in the corpus. We tested this
phenomenon on 500 randomly chosen sets of primes by calculating the distance of each prime from the
associative responses in the corpus. Figure - 1 testifies that a majority of primes occur within a context
window size of ±2 from the associative response.
</bodyText>
<figure confidence="0.996768571428571">
f 2,000
1,500
1,000
500
0
1 2 3 4 5 6 7 8 9 10
d
</figure>
<figureCaption confidence="0.964747">
Figure 1: Co-occurrence frequency f of an association at distance d from the response, averaged over the
2500 stimulus word and response word pairs from randomly chosen 500 training datasets
</figureCaption>
<bodyText confidence="0.968683">
Next, a mechanism to interpret the above associations in a quantitative manner is required.
</bodyText>
<page confidence="0.990724">
16
</page>
<sectionHeader confidence="0.98436" genericHeader="method">
3 Word Representation
</sectionHeader>
<bodyText confidence="0.9997198">
In order to have a quantitative comparison of association, first we need a representation for words in
a context. Traditionally co-occurrence vectors serve as a simple mechanism for such a representation.
However, such vectors are unable to effectively capture deeper semantics of words and also tend to suffer
from sparsity due to high dimensional space (equal to the vocabulary size). Several efforts have been
made to represent word vectors in a lower dimensional space. Largely, these can be categorized into:
</bodyText>
<listItem confidence="0.941190307692308">
1. Clustering: Clustering algorithms like Brown et al. (1992), are used to form clusters and derive
a vector based representation for each cluster, where semantically similar clusters are closer in
distance.
2. Topic Modeling: In this approach a word (or a document) is represented as a distribution of topics.
Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer and Dutnais, 1997) , which falls
in this category, utilizes SVD (Singular Value Decomposition) to produce a low rank representation
of a word. Latent Dirichlet Allocation (Blei et al., 2003) is an improvement with dirichlet priors
over the probabilistic version of LSA (Hofmann, 1999).
3. Neural Network based Word Embeddings: Here, a neural network is trained to output a vector
corresponding to a word which effectively signifies its position in the semantic space. There has
been different suggestions on the nature of the neural-net and how the context needs to be fed to
the neural-net. Some notable works include Collobert and Weston (2008), Mnih and Hinton (2008),
Turian et al. (2010) and Mikolov et al. (2013a).
</listItem>
<sectionHeader confidence="0.99665" genericHeader="method">
4 Methodology
</sectionHeader>
<bodyText confidence="0.999810333333333">
Our system follows a two-staged approach, where we first generate response candidates which are seman-
tically similar to prime words, followed by a re-ranking step where we give weightage to the responses
likely to occur in proximity.
</bodyText>
<subsectionHeader confidence="0.998212">
4.1 Candidate Response Generation
</subsectionHeader>
<bodyText confidence="0.9957844">
The complete vocabulary (of ukWaC Corpus) is represented in a semantic space by generating word
embeddings induced by the algorithm described in Mikolov et al. (2013a). Our choice is motivated by
the fact that this approach models semantic similarity and outperforms other approaches in terms of
accuracy as well as computational efficiency(Mikolov et al., 2013a; Mikolov et al., 2013c).
The word2vec2 utility is used to learn this model and thereby create 300-dimensional word embed-
dings. word2vec implements two classification networks - the Skip-gram architecture and the Continuous
Bag-of-words (CBOW) architecture. We applied CBOW architecture as it works better on large corpora
and is significantly faster than Skip-gram(Mikolov et al., 2013b). The CBOW architecture predicts the
current word based on its context. The architecture employs a feed forward neural network, which con-
sists of:
</bodyText>
<listItem confidence="0.99962">
1. An input layer, where the context words are fed to the network.
2. A projection layer, which projects words onto continuous space and reduces number of parameters
that are needed to be estimated.
3. An output layer.
</listItem>
<bodyText confidence="0.9885138">
This log-linear classifier learns to predict words based on its neighbors in a window of ±5. We also
applied a minimum word count of 25 so that infrequent words are filtered out.
With the vector representation available, a response r to a set of primes S, is searched in the vocabulary
by measuring its cosine similarity with each prime xi in S. The overall similarity of the response r, with
the prime word set S, is defined as the average of these similarities.
</bodyText>
<footnote confidence="0.91715">
2word2vec : https://code.google.com/p/word2vec/
</footnote>
<page confidence="0.912583">
17
</page>
<equation confidence="0.986211666666667">
|S|1 xi.r
sim(r,S) = |S |× E |xi|.|r|
i=1
</equation>
<bodyText confidence="0.9997252">
Using the best similarity score as the selection criterion for response, the approach resulted in an
accuracy of 20.8% over the test set. Error analysis revealed that the above approach is biased towards
finding a paradigmatic candidate. However, it is further observed that much of the correct answers
(&gt; 80%) exist in a k-best(k=500) list but with a relatively lower similarity score. This confirmed that our
broader selection is correct but a better re-ranking approach is required.
</bodyText>
<subsectionHeader confidence="0.998373">
4.2 Re-ranking by Association Measures
</subsectionHeader>
<bodyText confidence="0.99985625">
To give due weightage to responses with high syntagmatic associativity, we utilize word co-occurrences
from the corpus. Since we are dealing with semantically related candidates, applying even a basic lexical
association measure like Pointwise Mutual Information (PMI) (Church and Hanks, 1990) tend to improve
the results.
</bodyText>
<sectionHeader confidence="0.617623" genericHeader="method">
PMI
</sectionHeader>
<bodyText confidence="0.9996818">
For each prime word, we calculate co-occurrence frequency information for its neighbors within a win-
dow of ±2 as mentioned in Section 2. Also, a threshold of 3 is set to the observed frequency measures
as PMI tends to give very high association score to infrequent words.
For each candidate response r, we calculate its PMIi with each of the primes (xi) in the set S. The
total association score ScorePMI for a candidate is defined as the average of the individual measures.
</bodyText>
<equation confidence="0.99632775">
PMIi = p(xir) 1
ScorePMI = × PMIi
p(xi)p(r)  |S|
iES
</equation>
<bodyText confidence="0.969356875">
Ranking the candidates based on PMI improved the results to 30.45%
Weighted PMI
It should be duly noted that only some primes exhibit a syntagmatic relation with the response, while
the rest exhibit a paradigmatic relation. For example, the expected response for primes Avenue, column,
dimension, sixth, fourth is fifth. The first three words share a syntagmatic relation with the response
while the last two words share a paradigmatic relation with the response. As PMI deals with word
co-occurrences, ideally, only primes exhibiting syntagmatic associations should be considered for re-
ranking. However, a clear distinction between the two categories of primes is a difficult task as the target
response is unknown.
In order to take effective contribution of each prime, we propose a weighed extension of PMI which
gives more weightage to syntagmatic primes as to the paradigmatic ones. Since, primes sharing a
paradigmatic relation with the response word are highly semantically related, they are expected to be
closer in the semantic space too. On the other hand, the primes showcasing syntagmatic relations are
expected to be distant.
Using the vector representation described in Section 4.1, we calculate an average vector of the five
primes, pavg, and compute its cosine distance from individual primes. The cosine distance thus obtained
is used as the weight w for the PMI associativity of a prime. In a nutshell, larger the distance of a
prime from pavg, the greater is its contribution in the PMI based re-ranking score. This ranking schema
assumes that the prime set consists of at least two words demonstrating paradigmatic relation with the
target response. Table - 2 displays the primes along with their distance from pavg.
iES
Next, a ranked list of candidate responses for each set is generated by sorting the previously ranked
list according to the new score. The new ranking scheme based on weighted PMI (wPMI) improves the
results to 34.9%. Table -3 displays some sets which show improvement upon implementing the wPMI
</bodyText>
<equation confidence="0.972978">
1 � ScorewPMI = |S |× wiPMIi
</equation>
<page confidence="0.994915">
18
</page>
<table confidence="0.999474">
Primes Cosine Distance
Avenue 0.612
column 0.422
dimension 0.390
sixth 0.270
fourth 0.212
</table>
<tableCaption confidence="0.999814">
Table 2: An example demonstrating Cosine Distance between the primes and the pavg of the prime set
</tableCaption>
<bodyText confidence="0.99860725">
ranking scheme. Taking a case from Table - 3, we observe that the correct response skeleton is generated
for primes cupboard, body, skull, bone and bones when ranked according to the wPMI scheme. This is
due to larger weights being assigned to primes cupboard and body which have a closer proximity to the
word skeleton than the word vertebral which is generated by the simple PMI ranking scheme.
</bodyText>
<table confidence="0.99843925">
cupboard 0.615 pit 0.553 boat 0.499
Primes(with weights) body 0.410 band 0.549 sailing 0.476
skull 0.248 hand 0.426 drab 0.338
bone 0.244 limb 0.0.340 dark 0.318
bones 0.172 leg 0.270 dull 0.307
PMI vertebral amputated drizzly
wPMI skeleton arm dingy
Expected Response skeleton arm dingy
</table>
<tableCaption confidence="0.999942">
Table 3: Comparison between results from PMI and wPMI re-ranking approaches
</tableCaption>
<sectionHeader confidence="0.998815" genericHeader="evaluation">
5 Results and Evaluation
</sectionHeader>
<bodyText confidence="0.999192285714286">
The system was evaluated on the test set derived from the Edinburgh Associative Thesaurus (EAT) which
lists the associations to thousands of English stimulus words as collected from native speakers. For
example, for the stimulus word visual the top associations are aid, eyes, aids, see, eye, seen and sight.
For the shared task, top five associations for 2000 randomly selected stimulus words were provided as
prime sets and the system was evaluated based on its ability to predict the corresponding stimulus word
for each set. Table - 4 displays the top ten responses generated by our system for some prime sets and
their corresponding stimulus word.
</bodyText>
<table confidence="0.896418923076923">
Primes knight, plate, soldier, ants, flies, fly, babies, baby, rash, butterfly, moth, caterpillar,
protection, sword bees, bite wet, washing cocoon, insect
armor mosquitoes nappy larva
armour wasps shaving larvae
helmet beetles nappies pupa
shield insects clothes species
Top 10 guard spiders skin pests
Responses bulletproof sting bathing beetle
guards moths dry silkworm
warrior butterflies eczema wings
enemy arachnids bedding pupate
gallant bedbugs dirty pollinated
Target armour insects nappies chrysalis
</table>
<tableCaption confidence="0.999376">
Table 4: Top ten responses for some prime sets and their corresponding target response
</tableCaption>
<page confidence="0.999157">
19
</page>
<bodyText confidence="0.999515076923077">
As we have considered exact string match(ignoring capitalization), the evaluation does not account
for spelling variations. For example, the response output armor instead of the expected response armour
results in counting it as incorrect.
We achieved an accuracy of 34.9% by considering the top response for each list of ranked responses.
However, it was observed that the correct response was present within the top ten responses in 59.8%
of the cases. For example, the primes ants, flies, fly, bees, bite generate the response output mosquitoes.
The expected output insects ranks 4th in our list of responses.
For primes babies, baby, rash, wet, washing, our system outputs nappy while the expected response is
nappies. Such inflected forms of the responses are challenging to predict and hence, another evaluation
is presented which ignores the inflectional variation of the response word. Under this evaluation, we
achieved an accuracy of 39.55% for the best response and 63.15% if the expected response occurs in the
top ten responses. Table - 5 displays accuracy of our system when the target response lies within the
top-n responses for both evaluation methods.
</bodyText>
<table confidence="0.9983872">
Exact Match Ignoring Inflections
n=1 34.9 39.55
n=3 48.15 49.65
n=5 53.2 55.45
n=10 59.8 63.15
</table>
<tableCaption confidence="0.998383">
Table 5: Evaluation results in %
</tableCaption>
<sectionHeader confidence="0.99866" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999994588235294">
There exist some word associations that are asymmetric in nature. Rapp (2013) observed that the primary
response of a given stimulus word may have stronger association with another word and need not gen-
erate the stimulus word back. For example, the strongest association to bitter is sweet but the strongest
association to sweet is sour. Therefore, the EAT data set chosen for evaluation, may not be the best judge
for certain cases. Taking a case from our test data, for primes butterfly, moth, caterpillar, cocoon, insect,
our system outputs larva instead of the original stimulus word chrysalis which does not feature even in
the top ten responses (Refer Table - 4).
In this work, we proposed a system to generate a ranked list of responses for multiple stimulus words.
Candidate responses were generated by computing its semantic similarity with the stimulus words and
then re-ranked using a lexical association measure, PMI. This system scored 34.9% when the top ranked
response was considered and 59.8% when the top ten responses were taken into account. When ignoring
inflectional variations, the accuracy improved to 39.55% and 63.15% for the two evaluation methods
respectively.
In future, a more sophisticated re-ranking approach in place of PMI measure can be used such as
product-of-rank algorithm (Rapp, 2008). Since, the re-ranking methodologies discussed by far, take
into account word co-occurrences, it is biased towards syntagmatic responses. A better trade-off can be
worked out to give due weightage to paradigmatic responses too.
</bodyText>
<sectionHeader confidence="0.999319" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993185857142857">
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection
of very large linguistically processed web-crawled corpora. Language resources and evaluation, 43(3):209–
226.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. the Journal of machine
Learning research, 3:993–1022.
Peter F. Brown, Peter V. Desouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational linguistics, 18(4):467–479.
</reference>
<page confidence="0.959186">
20
</page>
<reference confidence="0.999445421052632">
Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography.
Comput. Linguist., 16(1):22–29, March.
Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of the 25th international conference on Machine learning,
pages 160–167. ACM.
Ferdinand De Saussure, Charles Bally, Albert Sechehaye, and Albert Riedlinger. 1916. Cours de linguistique
g´en´erale: Publi´e par Charles Bally et Albert Sechehaye avec la collaboration de Albert Riedlinger. Libraire
Payot &amp; Cie.
Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCI-
ENCE, 41(6):391–407.
Thomas Hofmann. 1999. Probabilistic latent semantic analysis. In Proceedings of the Fifteenth conference on
Uncertainty in artificial intelligence, pages 289–296. Morgan Kaufmann Publishers Inc.
George R. Kiss, Christine A. Armstrong, and Robert Milroy. 1972. An associative thesaurus of English. Medical
Research Council, Speech and Communication Unit, University of Edinburgh, Scotland.
Thomas K. Landauer and Susan T. Dutnais. 1997. A solution to platos problem: The latent semantic analysis
theory of acquisition, induction, and representation of knowledge. Psychological review, pages 211–240.
Michael McCarthy. 1990. Vocabulary. Oxford University Press.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine
translation. arXiv preprint arXiv:1309.4168.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word
representations. Proceedings of NAACL-HLT, pages 746–751.
Andriy Mnih and Geoffrey E. Hinton. 2008. A scalable hierarchical distributed language model. In NIPS, pages
1081–1088.
Reinhard Rapp. 2008. The computation of associative responses to multiword stimuli. In Proceedings of the
workshop on Cognitive Aspects of the Lexicon, pages 102–109. Association for Computational Linguistics.
Reinhard Rapp. 2013. From stimulus to associations and back. Natural Language Processing and Cognitive
Science, page 78.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 384–394. Association for Computational Linguistics.
Manfred Wettler and Reinhard Rapp. 1989. A connectionist system to simulate lexical decisions in information
retrieval. Pfeifer, R., Schreter, Z., Fogelman, F. Steels, L.(eds.), Connectionism in perspective. Amsterdam:
Elsevier, 463:469.
Manfred Wettler and Reinhard Rapp. 1993. Computation of word associations based on the co-occurrences of
words in large corpora.
</reference>
<page confidence="0.999436">
21
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.270911">
<title confidence="0.98176">A Two-Stage Approach for Computing Associative Responses to a Set Stimulus Words</title>
<author confidence="0.709735">Urmi Ghosh</author>
<author confidence="0.709735">Sambhav Jain</author>
<author confidence="0.709735">Soma</author>
<affiliation confidence="0.720015">Language Technologies Research</affiliation>
<email confidence="0.639025">IIIT-Hyderabad,soma@iiit.ac.in</email>
<abstract confidence="0.988706666666667">This paper describes the system submitted by the IIIT-H team for the CogALex-2014 shared task on multiword association. The task involves generating a ranked list of responses to a set of stimulus words. The two-stage approach combines the strength of neural network based word embeddings and frequency based association measures. The system achieves an accuracy of 34.9% over the test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation,</title>
<date>2009</date>
<volume>43</volume>
<issue>3</issue>
<pages>226</pages>
<contexts>
<context position="2209" citStr="Baroni et al., 2009" startWordPosition="321" endWordPosition="324">g association with the word river and it emerges as the best response amongst the aforementioned response choices. This task is motivated by the tip-of-the-tongue problem, where associated concepts from the memory can help recall the target word. Other practical applications include query expansion for information retrieval and natural language generation where missing words can be predicted from their context. The participating systems are distinguished into two categories - Unrestricted systems that allows usage of any kind of data and Restricted systems that can only make use of the ukWaC (Baroni et al., 2009) corpus, consisting of two billion tokens. Our proposed system falls in the restricted track since we only used ukWaC for extracting information on word associations. It follows a two-staged approach: Candidate Response Generation, which involves selection of words that are semantically similar to the primes and Re-ranking by Association Measure, that re-ranks the responses using a proposed weighted Pointwise Mutual Information (wPMI) measure. Our system was evaluated on test-datasets derived from the Edinburgh Associative Thesaurus (Kiss et al., 1972) and it achieved an accuracy of 34.9%. Whe</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation, 43(3):209– 226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="7411" citStr="Blei et al., 2003" startWordPosition="1127" endWordPosition="1130">nal space. Largely, these can be categorized into: 1. Clustering: Clustering algorithms like Brown et al. (1992), are used to form clusters and derive a vector based representation for each cluster, where semantically similar clusters are closer in distance. 2. Topic Modeling: In this approach a word (or a document) is represented as a distribution of topics. Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer and Dutnais, 1997) , which falls in this category, utilizes SVD (Singular Value Decomposition) to produce a low rank representation of a word. Latent Dirichlet Allocation (Blei et al., 2003) is an improvement with dirichlet priors over the probabilistic version of LSA (Hofmann, 1999). 3. Neural Network based Word Embeddings: Here, a neural network is trained to output a vector corresponding to a word which effectively signifies its position in the semantic space. There has been different suggestions on the nature of the neural-net and how the context needs to be fed to the neural-net. Some notable works include Collobert and Weston (2008), Mnih and Hinton (2008), Turian et al. (2010) and Mikolov et al. (2013a). 4 Methodology Our system follows a two-staged approach, where we firs</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="6905" citStr="Brown et al. (1992)" startWordPosition="1048" endWordPosition="1051">anner is required. 16 3 Word Representation In order to have a quantitative comparison of association, first we need a representation for words in a context. Traditionally co-occurrence vectors serve as a simple mechanism for such a representation. However, such vectors are unable to effectively capture deeper semantics of words and also tend to suffer from sparsity due to high dimensional space (equal to the vocabulary size). Several efforts have been made to represent word vectors in a lower dimensional space. Largely, these can be categorized into: 1. Clustering: Clustering algorithms like Brown et al. (1992), are used to form clusters and derive a vector based representation for each cluster, where semantically similar clusters are closer in distance. 2. Topic Modeling: In this approach a word (or a document) is represented as a distribution of topics. Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer and Dutnais, 1997) , which falls in this category, utilizes SVD (Singular Value Decomposition) to produce a low rank representation of a word. Latent Dirichlet Allocation (Blei et al., 2003) is an improvement with dirichlet priors over the probabilistic version of LSA (Hofmann, 1999)</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. Desouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Comput. Linguist.,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="981" citStr="Church and Hanks (1990)" startWordPosition="135" endWordPosition="138">word association. The task involves generating a ranked list of responses to a set of stimulus words. The two-stage approach combines the strength of neural network based word embeddings and frequency based association measures. The system achieves an accuracy of 34.9% over the test set. 1 Introduction Research in psychology gives evidence that word associations reveal the respondents’ perception, learning and verbal memories and thus determine language production. Hence, it is possible to simulate human derived word associations by analyzing the statistical distribution of words in a corpus. Church and Hanks (1990) and Wettler and Rapp (1989) were amongst the first to devise association measures by utilizing frequencies and co-occurrences from large corpora. Wettler and Rapp (1993) demonstrate that corpus-based computations of word associations are similar to association norms collected from human subjects. The CogALex-2014 shared task on multi-word association involves generating a ranked list of response words for a given set of stimulus words. For example, the stimulus word bank can invoke associative responses such as river, loan, finance and money. Priming1 bank with bed and bridge, results in stre</context>
<context position="10715" citStr="Church and Hanks, 1990" startWordPosition="1646" endWordPosition="1649"> towards finding a paradigmatic candidate. However, it is further observed that much of the correct answers (&gt; 80%) exist in a k-best(k=500) list but with a relatively lower similarity score. This confirmed that our broader selection is correct but a better re-ranking approach is required. 4.2 Re-ranking by Association Measures To give due weightage to responses with high syntagmatic associativity, we utilize word co-occurrences from the corpus. Since we are dealing with semantically related candidates, applying even a basic lexical association measure like Pointwise Mutual Information (PMI) (Church and Hanks, 1990) tend to improve the results. PMI For each prime word, we calculate co-occurrence frequency information for its neighbors within a window of ±2 as mentioned in Section 2. Also, a threshold of 3 is set to the observed frequency measures as PMI tends to give very high association score to infrequent words. For each candidate response r, we calculate its PMIi with each of the primes (xi) in the set S. The total association score ScorePMI for a candidate is defined as the average of the individual measures. PMIi = p(xir) 1 ScorePMI = × PMIi p(xi)p(r) |S| iES Ranking the candidates based on PMI imp</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Comput. Linguist., 16(1):22–29, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7867" citStr="Collobert and Weston (2008)" startWordPosition="1200" endWordPosition="1203">, which falls in this category, utilizes SVD (Singular Value Decomposition) to produce a low rank representation of a word. Latent Dirichlet Allocation (Blei et al., 2003) is an improvement with dirichlet priors over the probabilistic version of LSA (Hofmann, 1999). 3. Neural Network based Word Embeddings: Here, a neural network is trained to output a vector corresponding to a word which effectively signifies its position in the semantic space. There has been different suggestions on the nature of the neural-net and how the context needs to be fed to the neural-net. Some notable works include Collobert and Weston (2008), Mnih and Hinton (2008), Turian et al. (2010) and Mikolov et al. (2013a). 4 Methodology Our system follows a two-staged approach, where we first generate response candidates which are semantically similar to prime words, followed by a re-ranking step where we give weightage to the responses likely to occur in proximity. 4.1 Candidate Response Generation The complete vocabulary (of ukWaC Corpus) is represented in a semantic space by generating word embeddings induced by the algorithm described in Mikolov et al. (2013a). Our choice is motivated by the fact that this approach models semantic sim</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferdinand De Saussure</author>
<author>Charles Bally</author>
<author>Albert Sechehaye</author>
<author>Albert Riedlinger</author>
</authors>
<title>Cours de linguistique g´en´erale: Publi´e par Charles Bally et Albert Sechehaye avec la collaboration de Albert Riedlinger. Libraire Payot &amp; Cie.</title>
<date>1916</date>
<marker>De Saussure, Bally, Sechehaye, Riedlinger, 1916</marker>
<rawString>Ferdinand De Saussure, Charles Bally, Albert Sechehaye, and Albert Riedlinger. 1916. Cours de linguistique g´en´erale: Publi´e par Charles Bally et Albert Sechehaye avec la collaboration de Albert Riedlinger. Libraire Payot &amp; Cie.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="7210" citStr="Deerwester et al., 1990" startWordPosition="1096" endWordPosition="1099">ure deeper semantics of words and also tend to suffer from sparsity due to high dimensional space (equal to the vocabulary size). Several efforts have been made to represent word vectors in a lower dimensional space. Largely, these can be categorized into: 1. Clustering: Clustering algorithms like Brown et al. (1992), are used to form clusters and derive a vector based representation for each cluster, where semantically similar clusters are closer in distance. 2. Topic Modeling: In this approach a word (or a document) is represented as a distribution of topics. Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer and Dutnais, 1997) , which falls in this category, utilizes SVD (Singular Value Decomposition) to produce a low rank representation of a word. Latent Dirichlet Allocation (Blei et al., 2003) is an improvement with dirichlet priors over the probabilistic version of LSA (Hofmann, 1999). 3. Neural Network based Word Embeddings: Here, a neural network is trained to output a vector corresponding to a word which effectively signifies its position in the semantic space. There has been different suggestions on the nature of the neural-net and how the context needs to be fed to the neural-ne</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic analysis.</title>
<date>1999</date>
<booktitle>In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence,</booktitle>
<pages>289--296</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="7505" citStr="Hofmann, 1999" startWordPosition="1143" endWordPosition="1144"> et al. (1992), are used to form clusters and derive a vector based representation for each cluster, where semantically similar clusters are closer in distance. 2. Topic Modeling: In this approach a word (or a document) is represented as a distribution of topics. Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer and Dutnais, 1997) , which falls in this category, utilizes SVD (Singular Value Decomposition) to produce a low rank representation of a word. Latent Dirichlet Allocation (Blei et al., 2003) is an improvement with dirichlet priors over the probabilistic version of LSA (Hofmann, 1999). 3. Neural Network based Word Embeddings: Here, a neural network is trained to output a vector corresponding to a word which effectively signifies its position in the semantic space. There has been different suggestions on the nature of the neural-net and how the context needs to be fed to the neural-net. Some notable works include Collobert and Weston (2008), Mnih and Hinton (2008), Turian et al. (2010) and Mikolov et al. (2013a). 4 Methodology Our system follows a two-staged approach, where we first generate response candidates which are semantically similar to prime words, followed by a re</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic analysis. In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, pages 289–296. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George R Kiss</author>
<author>Christine A Armstrong</author>
<author>Robert Milroy</author>
</authors>
<title>An associative thesaurus of</title>
<date>1972</date>
<institution>English. Medical Research Council, Speech and Communication Unit, University of Edinburgh,</institution>
<location>Scotland.</location>
<contexts>
<context position="2767" citStr="Kiss et al., 1972" startWordPosition="400" endWordPosition="403">ms that can only make use of the ukWaC (Baroni et al., 2009) corpus, consisting of two billion tokens. Our proposed system falls in the restricted track since we only used ukWaC for extracting information on word associations. It follows a two-staged approach: Candidate Response Generation, which involves selection of words that are semantically similar to the primes and Re-ranking by Association Measure, that re-ranks the responses using a proposed weighted Pointwise Mutual Information (wPMI) measure. Our system was evaluated on test-datasets derived from the Edinburgh Associative Thesaurus (Kiss et al., 1972) and it achieved an accuracy of 34.9%. When ignoring the inflectional variations of the response word, an accuracy of 39.55% was achieved. 2 Observations on Training Data The training set consists of 2000 sets of five words (multiword stimuli or primes) and the word that is most closely associated to all of them (associative response). For example, a set of primes such as wheel, driver, bus, drive and lorry are given along with the expected associative response - car. In this section, our initial observations on the given training data are enlisted. This work is licenced under a Creative Commo</context>
</contexts>
<marker>Kiss, Armstrong, Milroy, 1972</marker>
<rawString>George R. Kiss, Christine A. Armstrong, and Robert Milroy. 1972. An associative thesaurus of English. Medical Research Council, Speech and Communication Unit, University of Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dutnais</author>
</authors>
<title>A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review,</title>
<date>1997</date>
<pages>211--240</pages>
<publisher>Vocabulary. Oxford University Press.</publisher>
<contexts>
<context position="7239" citStr="Landauer and Dutnais, 1997" startWordPosition="1100" endWordPosition="1103">ords and also tend to suffer from sparsity due to high dimensional space (equal to the vocabulary size). Several efforts have been made to represent word vectors in a lower dimensional space. Largely, these can be categorized into: 1. Clustering: Clustering algorithms like Brown et al. (1992), are used to form clusters and derive a vector based representation for each cluster, where semantically similar clusters are closer in distance. 2. Topic Modeling: In this approach a word (or a document) is represented as a distribution of topics. Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer and Dutnais, 1997) , which falls in this category, utilizes SVD (Singular Value Decomposition) to produce a low rank representation of a word. Latent Dirichlet Allocation (Blei et al., 2003) is an improvement with dirichlet priors over the probabilistic version of LSA (Hofmann, 1999). 3. Neural Network based Word Embeddings: Here, a neural network is trained to output a vector corresponding to a word which effectively signifies its position in the semantic space. There has been different suggestions on the nature of the neural-net and how the context needs to be fed to the neural-net. Some notable works include</context>
</contexts>
<marker>Landauer, Dutnais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dutnais. 1997. A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, pages 211–240. Michael McCarthy. 1990. Vocabulary. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="7938" citStr="Mikolov et al. (2013" startWordPosition="1213" endWordPosition="1216"> produce a low rank representation of a word. Latent Dirichlet Allocation (Blei et al., 2003) is an improvement with dirichlet priors over the probabilistic version of LSA (Hofmann, 1999). 3. Neural Network based Word Embeddings: Here, a neural network is trained to output a vector corresponding to a word which effectively signifies its position in the semantic space. There has been different suggestions on the nature of the neural-net and how the context needs to be fed to the neural-net. Some notable works include Collobert and Weston (2008), Mnih and Hinton (2008), Turian et al. (2010) and Mikolov et al. (2013a). 4 Methodology Our system follows a two-staged approach, where we first generate response candidates which are semantically similar to prime words, followed by a re-ranking step where we give weightage to the responses likely to occur in proximity. 4.1 Candidate Response Generation The complete vocabulary (of ukWaC Corpus) is represented in a semantic space by generating word embeddings induced by the algorithm described in Mikolov et al. (2013a). Our choice is motivated by the fact that this approach models semantic similarity and outperforms other approaches in terms of accuracy as well a</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</title>
<date>2013</date>
<contexts>
<context position="7938" citStr="Mikolov et al. (2013" startWordPosition="1213" endWordPosition="1216"> produce a low rank representation of a word. Latent Dirichlet Allocation (Blei et al., 2003) is an improvement with dirichlet priors over the probabilistic version of LSA (Hofmann, 1999). 3. Neural Network based Word Embeddings: Here, a neural network is trained to output a vector corresponding to a word which effectively signifies its position in the semantic space. There has been different suggestions on the nature of the neural-net and how the context needs to be fed to the neural-net. Some notable works include Collobert and Weston (2008), Mnih and Hinton (2008), Turian et al. (2010) and Mikolov et al. (2013a). 4 Methodology Our system follows a two-staged approach, where we first generate response candidates which are semantically similar to prime words, followed by a re-ranking step where we give weightage to the responses likely to occur in proximity. 4.1 Candidate Response Generation The complete vocabulary (of ukWaC Corpus) is represented in a semantic space by generating word embeddings induced by the algorithm described in Mikolov et al. (2013a). Our choice is motivated by the fact that this approach models semantic similarity and outperforms other approaches in terms of accuracy as well a</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>Proceedings of NAACL-HLT,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="7938" citStr="Mikolov et al. (2013" startWordPosition="1213" endWordPosition="1216"> produce a low rank representation of a word. Latent Dirichlet Allocation (Blei et al., 2003) is an improvement with dirichlet priors over the probabilistic version of LSA (Hofmann, 1999). 3. Neural Network based Word Embeddings: Here, a neural network is trained to output a vector corresponding to a word which effectively signifies its position in the semantic space. There has been different suggestions on the nature of the neural-net and how the context needs to be fed to the neural-net. Some notable works include Collobert and Weston (2008), Mnih and Hinton (2008), Turian et al. (2010) and Mikolov et al. (2013a). 4 Methodology Our system follows a two-staged approach, where we first generate response candidates which are semantically similar to prime words, followed by a re-ranking step where we give weightage to the responses likely to occur in proximity. 4.1 Candidate Response Generation The complete vocabulary (of ukWaC Corpus) is represented in a semantic space by generating word embeddings induced by the algorithm described in Mikolov et al. (2013a). Our choice is motivated by the fact that this approach models semantic similarity and outperforms other approaches in terms of accuracy as well a</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. Proceedings of NAACL-HLT, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2008</date>
<booktitle>In NIPS,</booktitle>
<pages>1081--1088</pages>
<contexts>
<context position="7891" citStr="Mnih and Hinton (2008)" startWordPosition="1204" endWordPosition="1207">y, utilizes SVD (Singular Value Decomposition) to produce a low rank representation of a word. Latent Dirichlet Allocation (Blei et al., 2003) is an improvement with dirichlet priors over the probabilistic version of LSA (Hofmann, 1999). 3. Neural Network based Word Embeddings: Here, a neural network is trained to output a vector corresponding to a word which effectively signifies its position in the semantic space. There has been different suggestions on the nature of the neural-net and how the context needs to be fed to the neural-net. Some notable works include Collobert and Weston (2008), Mnih and Hinton (2008), Turian et al. (2010) and Mikolov et al. (2013a). 4 Methodology Our system follows a two-staged approach, where we first generate response candidates which are semantically similar to prime words, followed by a re-ranking step where we give weightage to the responses likely to occur in proximity. 4.1 Candidate Response Generation The complete vocabulary (of ukWaC Corpus) is represented in a semantic space by generating word embeddings induced by the algorithm described in Mikolov et al. (2013a). Our choice is motivated by the fact that this approach models semantic similarity and outperforms </context>
</contexts>
<marker>Mnih, Hinton, 2008</marker>
<rawString>Andriy Mnih and Geoffrey E. Hinton. 2008. A scalable hierarchical distributed language model. In NIPS, pages 1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>The computation of associative responses to multiword stimuli.</title>
<date>2008</date>
<booktitle>In Proceedings of the workshop on Cognitive Aspects of the Lexicon,</booktitle>
<pages>102--109</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Rapp, 2008</marker>
<rawString>Reinhard Rapp. 2008. The computation of associative responses to multiword stimuli. In Proceedings of the workshop on Cognitive Aspects of the Lexicon, pages 102–109. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>From stimulus to associations and back.</title>
<date>2013</date>
<booktitle>Natural Language Processing and Cognitive Science,</booktitle>
<pages>78</pages>
<contexts>
<context position="16961" citStr="Rapp (2013)" startWordPosition="2651" endWordPosition="2652">to predict and hence, another evaluation is presented which ignores the inflectional variation of the response word. Under this evaluation, we achieved an accuracy of 39.55% for the best response and 63.15% if the expected response occurs in the top ten responses. Table - 5 displays accuracy of our system when the target response lies within the top-n responses for both evaluation methods. Exact Match Ignoring Inflections n=1 34.9 39.55 n=3 48.15 49.65 n=5 53.2 55.45 n=10 59.8 63.15 Table 5: Evaluation results in % 6 Conclusion There exist some word associations that are asymmetric in nature. Rapp (2013) observed that the primary response of a given stimulus word may have stronger association with another word and need not generate the stimulus word back. For example, the strongest association to bitter is sweet but the strongest association to sweet is sour. Therefore, the EAT data set chosen for evaluation, may not be the best judge for certain cases. Taking a case from our test data, for primes butterfly, moth, caterpillar, cocoon, insect, our system outputs larva instead of the original stimulus word chrysalis which does not feature even in the top ten responses (Refer Table - 4). In this</context>
</contexts>
<marker>Rapp, 2013</marker>
<rawString>Reinhard Rapp. 2013. From stimulus to associations and back. Natural Language Processing and Cognitive Science, page 78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7913" citStr="Turian et al. (2010)" startWordPosition="1208" endWordPosition="1211">r Value Decomposition) to produce a low rank representation of a word. Latent Dirichlet Allocation (Blei et al., 2003) is an improvement with dirichlet priors over the probabilistic version of LSA (Hofmann, 1999). 3. Neural Network based Word Embeddings: Here, a neural network is trained to output a vector corresponding to a word which effectively signifies its position in the semantic space. There has been different suggestions on the nature of the neural-net and how the context needs to be fed to the neural-net. Some notable works include Collobert and Weston (2008), Mnih and Hinton (2008), Turian et al. (2010) and Mikolov et al. (2013a). 4 Methodology Our system follows a two-staged approach, where we first generate response candidates which are semantically similar to prime words, followed by a re-ranking step where we give weightage to the responses likely to occur in proximity. 4.1 Candidate Response Generation The complete vocabulary (of ukWaC Corpus) is represented in a semantic space by generating word embeddings induced by the algorithm described in Mikolov et al. (2013a). Our choice is motivated by the fact that this approach models semantic similarity and outperforms other approaches in te</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Wettler</author>
<author>Reinhard Rapp</author>
</authors>
<title>A connectionist system to simulate lexical decisions in information retrieval.</title>
<date>1989</date>
<booktitle>Connectionism in perspective.</booktitle>
<pages>463--469</pages>
<editor>Pfeifer, R., Schreter, Z., Fogelman, F. Steels, L.(eds.),</editor>
<publisher>Elsevier,</publisher>
<location>Amsterdam:</location>
<contexts>
<context position="1009" citStr="Wettler and Rapp (1989)" startWordPosition="140" endWordPosition="143">nvolves generating a ranked list of responses to a set of stimulus words. The two-stage approach combines the strength of neural network based word embeddings and frequency based association measures. The system achieves an accuracy of 34.9% over the test set. 1 Introduction Research in psychology gives evidence that word associations reveal the respondents’ perception, learning and verbal memories and thus determine language production. Hence, it is possible to simulate human derived word associations by analyzing the statistical distribution of words in a corpus. Church and Hanks (1990) and Wettler and Rapp (1989) were amongst the first to devise association measures by utilizing frequencies and co-occurrences from large corpora. Wettler and Rapp (1993) demonstrate that corpus-based computations of word associations are similar to association norms collected from human subjects. The CogALex-2014 shared task on multi-word association involves generating a ranked list of response words for a given set of stimulus words. For example, the stimulus word bank can invoke associative responses such as river, loan, finance and money. Priming1 bank with bed and bridge, results in strengthening association with t</context>
</contexts>
<marker>Wettler, Rapp, 1989</marker>
<rawString>Manfred Wettler and Reinhard Rapp. 1989. A connectionist system to simulate lexical decisions in information retrieval. Pfeifer, R., Schreter, Z., Fogelman, F. Steels, L.(eds.), Connectionism in perspective. Amsterdam: Elsevier, 463:469.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Wettler</author>
<author>Reinhard Rapp</author>
</authors>
<title>Computation of word associations based on the co-occurrences of words in large corpora.</title>
<date>1993</date>
<contexts>
<context position="1151" citStr="Wettler and Rapp (1993)" startWordPosition="160" endWordPosition="163"> word embeddings and frequency based association measures. The system achieves an accuracy of 34.9% over the test set. 1 Introduction Research in psychology gives evidence that word associations reveal the respondents’ perception, learning and verbal memories and thus determine language production. Hence, it is possible to simulate human derived word associations by analyzing the statistical distribution of words in a corpus. Church and Hanks (1990) and Wettler and Rapp (1989) were amongst the first to devise association measures by utilizing frequencies and co-occurrences from large corpora. Wettler and Rapp (1993) demonstrate that corpus-based computations of word associations are similar to association norms collected from human subjects. The CogALex-2014 shared task on multi-word association involves generating a ranked list of response words for a given set of stimulus words. For example, the stimulus word bank can invoke associative responses such as river, loan, finance and money. Priming1 bank with bed and bridge, results in strengthening association with the word river and it emerges as the best response amongst the aforementioned response choices. This task is motivated by the tip-of-the-tongue</context>
</contexts>
<marker>Wettler, Rapp, 1993</marker>
<rawString>Manfred Wettler and Reinhard Rapp. 1993. Computation of word associations based on the co-occurrences of words in large corpora.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006573">
<title confidence="0.9975875">
Assessing Conformance of Manually Simplified Corpora with
User Requirements: the Case of Autistic Readers
</title>
<author confidence="0.82558">
Sanja ˇStajner and Richard Evans and Iustin Dornescu
</author>
<affiliation confidence="0.927381666666667">
Research Group in Computational Linguistics
Research Institute of Information and Language Processing
University of Wolverhampton, UK
</affiliation>
<email confidence="0.993884">
{SanjaStajner, R.J.Evans, I.Dornescu2}@wlv.ac.uk
</email>
<sectionHeader confidence="0.993817" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987916666667">
In the state of the art, there are scarce resources available to support development and evaluation
of automatic text simplification (TS) systems for specific target populations. These comprise
parallel corpora consisting of texts in their original form and in a form that is more accessible
for different categories of target reader, including neurotypical second language learners and
young readers. In this paper, we investigate the potential to exploit resources developed for such
readers to support the development of a text simplification system for use by people with autistic
spectrum disorders (ASD). We analysed four corpora in terms of nineteen linguistic features
which pose obstacles to reading comprehension for people with ASD. The results indicate that the
Britannica TS parallel corpus (aimed at young readers) and the Weekly Reader TS parallel corpus
(aimed at second language learners) may be suitable for training a TS system to assist people
with ASD. Two sets of classification experiments intended to discriminate between original and
simplified texts according to the nineteen features lent further support for those findings.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998737578947368">
As a fundamental human right, people with reading and comprehension difficulties are entitled to access
written information (UN, 2006). This entitlement enables better inclusion into society. However, the
vast majority of texts that such people encounter in their everyday life – especially newswire texts – are
lexically and syntactically very complex. Since the late nineties, several initiatives have emerged which
propose guidelines for producing plain, easy-to-read and more accessible documents. These include
the “Federal Plain Language Guidelines”1, “Make it Simple, European Guidelines for the Production of
Easy-to-Read Information for people with Learning Disability” (Freyhoff et al., 1998), “Am I making
myself clear? Mencap’s guidelines for accessible writing”2, and the W3C – Web Accessibility Initiative
guidelines3. However, manual adaptation of texts cannot match the speed with which new texts are pub-
lished on the web in order to provide up to date information. The aim of Automatic Text Simplification
(ATS) is to automatically (or at least semi-automatically) convert complex sentences into a more accessi-
ble form while preserving their original meaning. In the last twenty years, many ATS systems have been
proposed for different target populations in various languages (Carroll et al., 1998; Devlin and Unthank,
2006; Saggion et al., 2011; Inui et al., 2003; Alu´ısio et al., 2008). Due to the scarcity of parallel corpora
of original and manually simplified texts, most of these systems are rule-based.
The emergence of Simple English Wikipedia (SEW)4, together with the existing English Wikipedia
(EW)5 provided a large amount of parallel TS training data, which motivated a shift in English TS from
rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata,
2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has
</bodyText>
<footnote confidence="0.972284857142857">
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1http://www.plainlanguage.gov/howto/guidelines/bigdoc/fullbigdoc.pdf
2http://www.easy-read-online.co.uk/media/10609/making-myself-clear.pdf
3http://www.w3.org/WAI/
4http://simple.wikipedia.org/wiki/Main Page
5http://wikipedia.org/wiki/Main Page
</footnote>
<page confidence="0.978753">
53
</page>
<note confidence="0.993155">
Proceedings of the Workshop on Automatic Text Simplification: Methods and Applications in the Multilingual Society, pages 53–63,
Dublin, Ireland, August 24th 2014.
</note>
<bodyText confidence="0.999880523809524">
ever been made of the quality of the simplifications made in SEW and the usefulness of the transfor-
mations learned from EW–SEW parallel corpora for any of the specified target populations. The only
instructions given to the authors of SEW are to use Basic English vocabulary and shorter sentences. The
main page states that SEW is for everyone, including children and adults who are learning English. All
previously mentioned studies conducted on that corpus evaluated the quality of the generated output in
terms of grammaticality, meaning preservation, and simplicity, but not usefulness. Also, there have been
no comparisons of the types of transformations present in EW–SEW with any of the other TS corpora
in English which were simplified with a specific target population in mind, e.g. Encyclopedia Britan-
nica and its manually simplified versions for children – Britannica Elementary (Barzilay and Elhadad,
2003)6, Guardian Weekly and its manually simplified versions for language learners (Allen, 2009), and
the FIRST corpus of various texts simplified for people with autism spectrum disorder (ASD)7.
In this study, we compare the original and simplified texts of the four aforementioned TS corpora in
terms of nineteen features which measure the complexity of texts for people with ASD. Although these
features were derived from user requirements for people with ASD, many of them are known to present
reading obstacles for other target populations as well (e.g. children or language learners). Given the lack
of parallel TS corpora for people with ASD, our main goal is to investigate whether the EW–SEW or the
other two corpora aimed at children and language learners could be used as training material for a TS
system to assist people with ASD and thus enable data-driven approaches (instead of the currently used
rule-based ones). In order to further support the results of this analysis, we conduct several classification
experiments in which we try to distinguish between original and simplified texts in each of the four
corpora, using the nineteen features.
</bodyText>
<sectionHeader confidence="0.948121" genericHeader="introduction">
2 The FIRST Project and User Requirements
</sectionHeader>
<bodyText confidence="0.996737666666667">
Autistic Spectrum Disorders (ASD) are neurodevelopmental disorders characterised by qualitative im-
pairment in communication and stereotyped repetitive behaviour. People with ASD show a diverse range
of reading abilities: 5-10% have the capacity to read words from an early age without the need for
formal learning (hyperlexia) but many demonstrate reduced comprehension of what has been read (Volk-
mar and Wiesner, 2009). They may have difficulty inferring contextual information or may have trouble
understanding mental verbs, emotional language, and long sentences with complex syntactic structure
(Tager-Flusberg, 1981; Kover et al., 2012). To address these difficulties, a tool is being developed in the
FIRST project8 to assist in the process of making texts more accessible for people with ASD. To achieve
this, three modues are exploited:
</bodyText>
<listItem confidence="0.994206142857143">
1. Structural complexity processor, which detects syntactically complex sentences and generates
alternatives to such sentences in the form of sequences of shorter sentences (Evans et al., 2014;
Dornescu et al., 2013).
2. Meaning disambiguator, which resolves pronominal references, performs word sense disambigua-
tion, and detects lexicalised (conventional) metaphors (Barbu et al., 2013).
3. Personalised document generator, which aggregates the output of processors 1 and 2 and gener-
ates additional elements such as glossaries, illustrative images, and document summaries.
</listItem>
<bodyText confidence="0.999158">
The system, named Open Book, is deployed as an editing tool for healthcare and educational service
providers. It functions semi-automatically, exploiting the three processors and requiring the user to
authorise the application of the conversion operations. The system is required to assess the readability
of texts, not only to decide which texts should be converted, but also to assess the readability of texts
that are undergoing conversion. It is expected that people working to improve the accessibiity of a
given text will benefit from relevant feedback concerning the effects of the changes being introduced.
Automatic assessment of readability is one method by which such feedback can be delivered. In the
</bodyText>
<footnote confidence="0.999953666666667">
6http://www.cs.columbia.edu/ noemie/alignment/
7Available at: http://www.first-asd.eu/?q=system/files/FIRST D7.2 20130228 annex.pdf
8www.first-asd.eu
</footnote>
<page confidence="0.999257">
54
</page>
<bodyText confidence="0.99984745">
context of improving the accessibility of texts, relevant feedback should indicate the extent to which
different versions of a text meet the particular requirements of intended readers.
User requirements were obtained through consulatation of 94 subjects meeting the strict DSM-IV cri-
teria for ASD and with IQ &gt; 70. 43 user requirements were derived and assigned a reference code. The
requirements link linguistic phenomena to editing operations, such as deletion, explanation, or trans-
formation, that will convert the text to a more accessible form. The linguistic phenomena of concern
include instances of syntactic complexity such as long sentences containing more than 15 words (possi-
bly containing multiple copulative coordinated clauses (UR301), subordinate adjective clauses (UR302),
explicative clauses (UR303), non-initial adverbial clauses (UR307)), sentences containing passive verbs
(UR313), rarely used conjunctions and antithetic conjuncts (UR304, UR305, UR306), uncommon syn-
onyms of polysemic words (UR401, UR425, UR504, UR505, UR511), rarely-used symbols and punc-
tuation marks (UR311), anaphors, words containing more than 7 characters, adjectives ending with -ly,
long numerical expressions (UR417), negation (UR314), words more than 7 characters long and adverbs
with suffix -ly (UR317-319), anaphors, including pronouns (UR418-420).
Additional linguistic phenomena such as phraseological units (UR402, UR410, UR425, UR507), and
non-lexicalised metaphors (UR422, UR508), were also found to pose obstacles to reading comprehension
for people with ASD. At present, there is a scarcity of resources enabling accurate detection of these
items. For this reason, changes in the prevalence of these items in original and converted versions of
texts are not captured in this study. The full set of user requirements is detailed in Martos et al. (2013).
More generally, it is infrequent linguistic phenomena that cause the greatest difficulty.
</bodyText>
<sectionHeader confidence="0.999949" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999974571428571">
There have been several studies analysing the existing TS corpora. However, their main focus was on
determining necessary transformations in TS: for children (Bautista et al., 2011); for people with intel-
lectual disability (Drndarevi´c and Saggion, 2012); for language learners (Petersen and Ostendorf, 2007);
and for people with low literacy (Gasperin et al., 2009). Unfortunately, those studies are not directly
comparable (neither among themselves nor with our study), either because they focus on different types
of transformations (the study of Bautista et al. (2011) focuses on general transformations while the other
three studies focus on sentence transformations), or because they treat different languages (Spanish,
English, and Brazilian Portuguese).
Two previous studies most relevant to ours are those by Napoles and Dredze (2010), and by ˇStajner
et al. (2013). Napoles and Dredze (2010) built a statistical classification system that discriminates
simple English from ordinary English, based on EW–SEW corpus. They used four different groups of
features: lexical, part-of-speech, surface, and syntactic parse features. The accuracy of the best classifier
(SVM) on the document classification task when using all features was 99.90%, while the accuracy
of the best classifier (maximum entropy) on the sentence classification task when using all features
was 80.80%. However, this study only demonstrated that it is fairly easy to discriminate sentences and
documents of EW from those of SEW. It did not investigate whether the simple English used in SEW
complies with the user requirements of any specific population with reading difficulties. ˇStajner et al.
(2013) analysed a corpus of 37 newswire texts in Spanish and their manual simplifications aimed at
people with Down’s syndrome, compiled in the Simplext project9. They built a classification system that
discriminates the original texts from those which are simple with an F-measure of 1.00 using the SVM,
and only seven features: average number of punctuation marks (not counting end of sentence markers),
numerical expressions, average word length in characters, the ratio of simple and complex sentences,
sentence complexity index, lexical density and lexical richness. They reported the average sentence
length as being the feature with the best discriminative power, leading to an F-measure of 0.99 when
used on its own.
In spite of the many linguistic phenomena that pose obstacles to reading comprehension for different
target populations, there have been almost no studies investigating whether a TS system built with a
specific target population in mind could be successfully applied – or adapted – to a different target
</bodyText>
<footnote confidence="0.933592">
9www.simplext.es
</footnote>
<page confidence="0.99512">
55
</page>
<table confidence="0.979337153846154">
Corpus Aimed at Version Code Texts SentPerText WordsPerText
Original Learn.-O 100 39.41 ± 14.43 746.83 ± 174.25
Weekly Reader Language learners
Simple Learn.-S 100 38.40±12.59 621.11 ± 157.17
Original Brit.-O 20 27.10 ± 8.91 628.30 ± 198.19
Enc. Britannica Children
Simple Brit.-S 20 26.45 ± 9.35 382.35 ± 127.69
Original Wiki-O 110 34.55 ± 1.87 716.57 ± 117.82
Wikipedia Various
Simple Wiki-S 110 34.49 ± 1.82 675.07 ± 107.03
Original FIRST-O 25 13.64 ± 3.95 285.68 ± 34.46
FIRST People with ASD
Simple FIRST-S 25 22.92 ± 4.79 311.36 ± 76.82
</table>
<tableCaption confidence="0.99986">
Table 1: Corpora characteristics
</tableCaption>
<bodyText confidence="0.999795222222222">
population. The only exception to this is the study by ˇStajner and Saggion (2013), which demonstrated
that two classifiers – one which discriminates sentences which should be split from those which should
be left unsplit, and another which discriminates sentences which should be deleted from those which
should be preserved – can successfully be trained on one type of corpora and applied to the other. Both
corpora consisted of texts in Spanish, one containing newswire texts manually simplified for people with
Down’s syndrome, and the other various text genres manually simplified for people with ASD.
Motivated by those previous studies and the lack of parallel corpora aimed specifically to people with
ASD, in this paper, we investigate whether some of already existing corpora for TS in English could
potentially be used for building a data-driven TS system for this target population.
</bodyText>
<sectionHeader confidence="0.998599" genericHeader="method">
4 Methodology
</sectionHeader>
<bodyText confidence="0.668839">
The corpora, features, and experimental settings used in this study are described in Sections 4.1–4.3.
</bodyText>
<subsectionHeader confidence="0.985506">
4.1 Corpora
</subsectionHeader>
<bodyText confidence="0.99894">
Four parallel corpora of original and manually simplified texts for different target populations were used
in this study (Table 1):
</bodyText>
<listItem confidence="0.988022058823529">
1. The corpus of 100 texts from Weekly Reader and their manual simplifications provided by Macmil-
lan English Campus and Onestopenglish10 aimed at foreign language learners. The corpus is divided
into three sub-corpora – advanced, intermediate and elementary – each representing a different level
of simplification. Given that the other three corpora used in this study contain original texts and only
one level of simplification, we only used the texts from the advanced (henceforth original) and ele-
mentary (henceforth simplified) levels. A more detailed description of this corpus can be found in
(Allen, 2009).
2. The corpus of 20 texts from the Encyclopedia Britannica and their manually simplified versions
aimed at children – Britannica Elementary (Barzilay and Elhadad, 2003)11.
3. The corpus of 110 randomly selected corresponding articles from EW and SEW. Here, it is impor-
tant to note that, in general, articles from SEW do not represent direct simplifications of the articles
from EW, they just have a matching topic. For this reason, we did not use complete EW and SEW
articles. We only used those sentences in original and simplified versions, which existed in the
sentence-aligned parallel corpora version 2.012 (Kauchak, 2013).
4. The corpus of 25 texts on various topics manually simplified for people with autism, compiled in
the FIRST project13, for the purpose of a piloting task14. The texts were simplified by carers of
people with ASD in accordance with specified guidelines.
</listItem>
<footnote confidence="0.9964322">
10http://www.onestopenglish.com/
11http://www.cs.columbia.edu/ noemie/alignment/
12http://www.cs.middlebury.edu/ dkauchak/simplification/
13www.first-asd.eu
14http://www.first-asd.eu/?q=system/files/FIRST D7.2 20130228 annex.pdf
</footnote>
<page confidence="0.995715">
56
</page>
<subsectionHeader confidence="0.980655">
4.2 Text Features Relevant to User Requirements
</subsectionHeader>
<bodyText confidence="0.9996912">
In this paper, a set of 15 text complexity measures and 4 formulae exploiting these measures was used
to estimate the accessibility of the texts. These features quantify the occurrence of linguistic phenomena
identified as potential obstacles to reading comprehension for people with ASD. The set of features is
presented in Table 2. The set of formulae is presented in Table 3. In every case, accessible texts are
expected to have smaller values of each metric.
</bodyText>
<table confidence="0.999946956521739">
# Code Linguistic feature Explanation/relevance
1 Illative Illative conjunctions Indicators of syntactic complexity, linking clauses.
2 CompConj Comparative conjunctions [UR304-306]
3 AdvConj Adversative conjunctions
4 LongSent Long sentences Motivated by the assumption that deriving the propositions in
5 Semicol Semicolons/suspension complex sentences is more difficult than deriving connections be-
points tween related propositions expressed in simple sentences
6 Passive Passive verbs (Arya et al., 2011). [UR309-310, UR313]
7 UnPunc Unusual punctuation Indicates syntactic complexity, ellipsis, alternatives, and mathe-
matical expressions [UR311]
8 Negations Negation The sum of adverbial and morphological negations (“Make it Sim-
ple” (Freyhoff et al., 1998), though contrary to the findings of Tat-
tamanti (2008)) [UR314]
9 Senses Possible senses The sum over all tokens in the text of the total number of possible
senses of each token. [UR401, UR425, UR504-505, UR511]
10 PolyW Polysemic words Words with two or more senses listed in WordNet. [UR401,
UR425, UR504, UR505, UR511]
11 Infreq Infrequent words Words that are not among the 5000 most frequent words in English
[UR304-306, UR401, UR425, UR504-505, UR511]
12 NumExp Numerical expressions Numbers written as sequences of words rather than digits [UR417]
13 Pron Pronouns Studies have shown that people with ASD can have
14 DefDescr Definite descriptions difficulty processing anaphora (Fine et al., 1994) [UR418-420]
15 SylLongW Long words Words with more than three syllables [UR317-319]
</table>
<tableCaption confidence="0.9867955">
Table 2: Complexity measures (1 – words such as therefore and hence; 2 – words such as equally and
correspondingly; 3 – words such as although and conversely; 4 – sentences more than 15 words long; 8 –
negative adverbials and negative prefixes such as un- and dis-; 11 – derived from Wiktionary frequency
lists for English16)
</tableCaption>
<table confidence="0.999947583333333">
# Code Metric Formula Relevance
16 PolyType Polysemic type ratio ptyp Indicates the proportion of the text vocabulary that is
polysemous. [UR401, UR425, UR504-505, UR511]
typ
17 CommaInd Comma index 10×c Indicates the average syntactic complexity of the
sentences in the text [UR301-303, UR307]
w
18 WordsPerSent Words per sentence w Indicates the average length of the sentences in the text
s [UR309]
19 TypeTokRat Type-token ratio typ Indicate the range of vocabulary used in the text
[UR401, UR425, UR504, UR505, UR511]
tok
</table>
<tableCaption confidence="0.49744325">
Table 3: Text complexity formulae (w – the number of words in the text; s – the number of sentences in
the text; ptyp – the number of polysemic word types in the text; c – the number of commas in the text;
typ – the number of word types in the text; tok – the number of word tokens in the text)
Scores for these measures, and the text complexity formulae that exploit them where obtained auto-
</tableCaption>
<figureCaption confidence="0.6997388">
matically by the tokeniser, part-of-speech tagger, and lemmatiser distributed with LT TTT2 (Grover et al.,
2000). Detection of the features used to derive complexity measures also involved the use of additional
resources such as WordNet, gazetteers of rare illative, comparative, and adversative conjunctions, nega-
tives (words and prefixes) and a set of lexico-syntactic patterns used to detect passive verbs (presented in
Figure 1).
</figureCaption>
<page confidence="0.871556">
57
</page>
<figure confidence="0.928177">
am/are/is/was/were wRB* w{V BN|V BD}
am/are/is/was/were wRB* being wRB* w{V BN|V BD}
have/has/had wRB* been wRB* w{V BN|V BD}
will wRB* be wRB* w{V BN|V BD}
am/is/are wRB* going wRB* to wRB* be wRB* w{V BN|V BD}
wMD wRB* be w{V BN|V BD}
wMD wRB* have wRB* been wRB* w{V BN|V BD}
</figure>
<figureCaption confidence="0.875004333333333">
Figure 1: Lexico-syntactic patterns used to detect passive verbs (‘*’ indicates zero or more repetitions of
the item it is attached to, while RB, VBN, VBD, and MD are Penn treebank tags returned by the LT
TTT PoS tagger: RB – adverb; VBN – past participle; VBD – past tense; and MD – modal verb)
</figureCaption>
<subsectionHeader confidence="0.965852">
4.3 Experiments
</subsectionHeader>
<bodyText confidence="0.990855">
Two sets of experiments were performed in this study:
</bodyText>
<listItem confidence="0.962553538461538">
1. Analysis of differences between original and simplified texts in terms of nineteen selected features
(Section 4.2) across four corpora (Section 4.1). Statistical difference was measured using the t-
test for related samples in the cases where the features were normally distributed, and using the
related samples Wilcoxon signed rank test otherwise. Normality of the data was tested using the
Shapiro-Wilk test of normality, which is preferred over the Kolmogorov-Smirnov test when the
dataset contains less than 2,000 elements. All tests were performed in SPSS. Features 1–15 were
first normalised (as an average per sentence) in order to allow a fair comparison across the four TS
corpora (text length in words and sentences differed significantly across different corpora).
2. Classification experiments with the aim of discriminating original from simplified texts using the
nineteen selected features. All experiments were conducted using the Weka Experimenter (Witten
and Frank, 2005; Hall et al., 2009) in 10-fold cross-validation setup with 10 repetitions, using four
different classification algorithms: NB – NaiveBayes (John and Langley, 1995), SMO – Weka im-
plementation of Support Vector Machines (Keerthi et al., 2001) with normalisation, JRip – a propo-
</listItem>
<bodyText confidence="0.967993333333333">
sitional rule learner (Cohen, 1995), and J48 – Weka implementation of C4.5 (Quinlan, 1993). The
statistical significance of the observed differences in F-measures obtained by different algorithms
was calculated using the corrected paired t-test provided in the Weka Experimenter.
The TS system in FIRST is not only supposed to decide which texts should be converted, but also
to assess the readability of texts that are undergoing conversion. It is expected that people working to
improve the accessibility of a given text will benefit from relevant feedback concerning the effects of the
changes being introduced. Automatic assessment of readability is one method by which such feedback
can be delivered. Deriving a subset of features which, when trained with an appropriate classification
algorithm, can categorize a given text as either ‘original’ or ‘simplified’, would facilitate automatic eval-
uation of TS systems. The resulting classifier would be suitable for assessing whether those systems
perform an appropriate level of simplification. This could serve as a rough estimation, an efficient first
step offering a quick evaluation prior to being tested with real users.
</bodyText>
<sectionHeader confidence="0.999605" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.903748">
The results of the two sets of experiments are presented and discussed in the next two subsections.
</bodyText>
<subsectionHeader confidence="0.980155">
5.1 Analysis of the Features across the Corpora
</subsectionHeader>
<bodyText confidence="0.992721">
Mean values (with standard deviations) of each of the first eight features on each sub-corpus are dis-
played in Table 4. The number of unusual punctuation marks (UnPunc) is the only feature whose value
does not differ significantly between the original and simplified versions of the texts in any of the four
corpora. This feature was thus excluded from further classification experiments. The number of com-
parative conjunctions per sentence (CompConj) significantly decreases only when simplifying texts for
</bodyText>
<page confidence="0.997579">
58
</page>
<table confidence="0.999517333333333">
Corpus Illative CompConj AdvConj LongSent Semicol UnPunc Passive Negations
Lear.-O 0.24±0.12 0.04±0.13 0.21±0.08 0.62±0.15 0.03±0.05 0.00±0.01 0.21±0.10 0.33±0.15
Lear.-S 0.20±0.13 0.03±0.09 0.19±0.09 0.51±0.14 *0.03±0.05 0.00±0.01 0.09±0.09 0.26±0.14
Brit.-O 0.13±0.09 0.15±0.26 0.14±0.07 0.72±0.11 0.13±0.20 0±0 0.33±0.10 0.28±0.16
Brit.-S 0.08±0.05 *0.02±0.10 0.06±0.04 0.38±0.11 0.00±0.02 0±0 0.25±0.12 0.14±0.09
Wiki-O 0.20±0.11 0.11±0.19 0.16±0.10 0.65±0.12 0.04±0.04 0.04±0.10 0.34±0.15 0.32±0.23
Wiki-S 0.18±0.11 0.11±0.20 0.14±0.09 0.62±0.12 0.03±0.04 0.03±0.10 0.33±0.15 0.29±0.24
FIRST-O 0.18±0.14 0.06±0.19 0.18±0.15 0.68±0.15 0.03±0.10 0.01±0.02 0.27±0.23 0.42±0.28
FIRST-S 0.11±0.10 0.01±0.06 0.09±0.07 0.33±0.19 0.00±0.01 0±0 0.20±0.15 0.22±0.13
</table>
<tableCaption confidence="0.995284">
Table 4: Mean values (with standard deviation) of features 1–8 across the corpora (O – the original texts
</tableCaption>
<bodyText confidence="0.8641885">
in the corpora; S – the simplified texts in the corpora; bold – significantly different from the value on the
original texts at a 0.01 level of significance; *bold – significantly different from the value on the original
texts at a 0.05 level of significance (but not at 0.01); ‘0.00’ – a value different from zero which rounded
at two decimals gives 0.00; ‘0’ – a value equal to zero)
</bodyText>
<table confidence="0.998801444444445">
Corpus Senses PolyW Infreq NumExp Pron DefDescr SylLongW
Lear.-O 73.95±12.32 9.37±1.72 5.64±1.33 0.18±0.11 0.97±0.40 1.86±0.54 1.12±0.28
Lear.-S 64.21±11.16 7.85±1.45 4.14±1.01 0.16±0.10 0.90±0.37 1.62±0.45 0.92±0.27
Brit.-O 67.51± 8.83 9.87±1.15 9.37±1.10 0.18±0.12 0.40±0.18 2.86±0.44 1.45±0.20
Brit.-S 48.68± 4.17 6.48±0.57 5.39±0.58 0.09±0.06 0.28±0.13 1.86±0.20 1.17±0.19
Wiki-O 67.70±12.96 9.13±1.61 7.86±1.63 0.18±0.16 0.67±0.43 2.08±0.58 1.24±0.38
Wiki-S 68.20±13.56 8.71±1.56 7.16±1.51 *0.17±0.16 0.68±0.44 1.97±0.54 1.10±0.42
FIRST-O 82.28±24.20 10.16±2.65 7.11±2.72 0.19±0.19 1.05±0.73 2.12±0.92 1.17±0.58
FIRST-S 57.13±15.96 6.47±1.77 3.92±1.56 0.09±0.07 *0.82±0.44 1.62±0.54 *0.92±0.43
</table>
<tableCaption confidence="0.984217">
Table 5: Mean values (with standard deviation) of features 9–15 across the corpora (O – the original texts
</tableCaption>
<bodyText confidence="0.99901952">
in the corpora; S – the simplified texts in the corpora; bold – significantly different from the value on the
original texts at a 0.01 level of significance; *bold – significantly different from the value on the original
texts at a 0.05 level of significance (but not at 0.01))
children (Brit.-S), while the average number of passive constructions per sentence (Passive) decreases
when simplifying for both children (Brit.-S) and language learners (Lear.-S). It is interesting to note that
the average number of passive constructions per sentence (Passive) does not decrease in the EW–SEW
corpus and that its value on the simplified versions of Wikipedia articles (Wiki-S) is significantly higher
than on Brit.-S and Lear.-S, although SEW claims to provide articles simplified for both those target
populations. It can also be observed that the fact that all four corpora were reported to have significant
differences between original and simplified texts in terms of features Illative, AdvConj, LongSent, and
Negations does not necessarily mean that the average number of occurrences of those features is similar
in all four simplified corpora. The values of Illative, AdvConj, and LongSent in the simplified versions
of the texts in the FIRST corpus seem to correspond best to those in the simplified versions of the texts
in the Britannica corpus (Brit.-S). The value of Negations in FIRST-S, however, seems to correspond
best to that in Lear.-S. This suggests that if we wish to build a component of our TS system (to assist
people with ASD) which would remove negations (Negations), we should train it on the sentence pairs
from the corpora with simplifications aimed at second language learners. If we wish to build a com-
ponent which would remove illative conjunctions (Illative), adversative conjuctions (AdvConj), or long
sentences (LongSent), we should probably train it on the sentence pairs from the corpora with simplifi-
cations aimed at young readers.
The number of occurrences per sentence of features 9–15 in the original versions of the texts was sig-
nificantly higher than in the simplified versions of the texts in all four corpora, with only two exceptions
– features Senses and Pron in the EW–SEW corpus (Wiki-O and Wiki-S), as can be observed in Table
5. Again, the mean values of all features in the simplified versions of the texts in the FIRST corpora
FIRST-S, seems to correspond better to the simplified versions of Encyclopedia Britannica (Brit.-S) and
</bodyText>
<page confidence="0.996396">
59
</page>
<table confidence="0.999823">
Corpus PolyType CommaInd WordsPerSent TypeTokRat
Lear.-O 0.76±0.04 0.56±0.12 19.91±3.46 0.51±0.04
Lear.-S 0.77±0.04 0.46±0.15 16.69±2.78 0.47±0.05
Brit.-O 0.69±0.03 0.78±0.15 23.46±2.78 0.51±0.04
Brit.-S *0.71±0.02 *0.67±0.14 14.61±1.21 0.55±0.04
Wiki-O 0.71±0.05 0.65±0.15 20.73±3.16 0.48±0.05
Wiki-S 0.71±0.05 0.60±0.16 19.57±2.90 *0.48±0.05
FIRST-O 0.73±0.04 0.51±0.18 22.20±5.43 0.59±0.05
FIRST-S 0.75±0.06 0.19±0.15 13.86±3.41 0.53±0.08
</table>
<tableCaption confidence="0.990175">
Table 6: Mean values (with standard deviation) of features 16–19 across the corpora (O – the original
</tableCaption>
<bodyText confidence="0.981895111111111">
texts in the corpora; S – the simplified texts in the corpora; bold and *bold – used in the same way as in
the previous two tables)
Weekly Readers (Lear.-S) than to those in the simplified versions of the Wikipedia articles (Wiki-S). It is
also interesting to note that many of the features (LongSent, Negations, Senses, PolyW, Infreq, DefDesc)
seem to have a significantly higher number of occurrences per sentence in the simplified versions of
the Wikipedia articles (Wiki-S) than in the simplified versions of Encyclopedia Britannica (Brit.-S) and
Weekly Reader (Lear.-S).
The comma index (CommaInd), type-token ratio (TypeTokRat), and the average number of words per
sentence (WordsPerSent) were found to be significantly higher in original texts than in their simplified
versions in all four corpora (Table 6). However, the values of those three text complexity formulae were
not similar in the simplified texts across the four corpora. In terms of the average number of words
per sentence (WordsPerSent) and the type-token ratio (TypeTokRat), the simplified versions of the texts
in the FIRST corpora (FIRST-S) seem to correspond better to the texts simplified for young readers
(Brit.-S), than to those simplified for second language learners (Lear.-S) and those aimed at various
target populations (Brit.-S). The comma index (CommaInd) obtained for simplified texts in the FIRST
corpora was several times lower than that obtained for simplified texts in the three other corpora. The
polysemic type ratio (PolyType) was not significantly different in original and in simplified texts of the
FIRST corpora (Table 6). The higher polysemic type ratio (PolyType) for simplified rather than original
versions of the texts in the other three corpora was unexpected, as it is usually assumed that polysemous
words can pose an obstacle for various target populations. However, it is important to bear in mind that
polysemous words usually pose an obstacle when conveying one of their infrequently used meanings.
Findings in cognitive psychology indicate that the words with the highest number of possible meanings
are actually understood more quickly, due to their high frequency (Jastrzembski, 1981). A common
lexical simplification strategy is to replace infrequent words with their more frequent synonyms, and
long words with their shorter synonyms. This strategy leads to a higher polysemic type ratio (PolyType)
in simplified versions of the texts as the shorter words are usually more frequent (Balota et al., 2004),
and frequent words tend to be more polysemous than infrequent ones (Glanzer and Bowles, 1976).
</bodyText>
<subsectionHeader confidence="0.999798">
5.2 Classification between Original and Simplified Texts
</subsectionHeader>
<bodyText confidence="0.989648">
Classification experiments were conducted using two different sets of features on each of the corpora:
</bodyText>
<listItem confidence="0.9895916">
1. all – all 18 features (UnPunc was excluded as it was not reported as significant for any of the
corpora)
2. best – 11 features which were reported as significant for all four corpora (Illative, AdvConj,
LongSent, Negations, PolyW, NumExp, DefDescr, SylLongW, CommaInd, WordsPerSent, Type-
TokRat)
</listItem>
<bodyText confidence="0.9990565">
As can be observed from Table 7, use of the SMO-n classification algorithm using the subset of 11
best features achieves perfect 1.00 F-measure for discriminating original from simplified versions of the
Encyclopedia Britannica. The same classification algorithm performs less well on the FIRST and Weekly
Readers corpora (though still quite well), while it performs significantly worse on the Wikipedia corpus.
</bodyText>
<page confidence="0.996375">
60
</page>
<bodyText confidence="0.990237">
The baseline (which chooses majority class) would be 0.50 in all cases. These results indicate that the
Encyclopedia Britannica TS parallel corpus, and possibly the Weekly Readers TS parallel corpus, may
serve as suitable training material for building a TS system (or at least some of its components) aimed at
people with ASD.
</bodyText>
<table confidence="0.999892777777778">
Dataset SMO-n NB JRip J48
Brit-all 0.98±0.09 0.94±0.12 0.94±0.14 0.97±0.11
Brit-best 1.00±0.00 0.99±0.05 0.94±0.13 0.97±0.11
FIRST-all 0.88±0.15 0.86±0.19 0.79±0.23 0.75±0.25
FIRST-best 0.88±0.15 0.85±0.20 0.78±0.25 0.76±0.25
Lear-all 0.81±0.08 0.74±0.10* 0.75±0.07* 0.72±0.10*
Lear-best 0.77±0.08 0.74±0.11 0.70±0.10* 0.73±0.10
Wiki-all 0.54±0.12 0.50±0.12 0.51±0.14 0.35±0.20*
Wiki-best 0.55±0.13 0.55±0.12 0.51±0.12 0.33±0.20*
</table>
<tableCaption confidence="0.974892">
Table 7: F-measure with standard deviation in a 10-fold cross-validation setup with 10 repetitions for
four classification algorithms: SMO-n, NB, JRip, and J48 (* – statistically significant degradation in
comparison with SMO-n)
</tableCaption>
<sectionHeader confidence="0.998923" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999253733333333">
Automatic Text Simplification (ATS) aims to convert complex texts into a simpler form, which is more
accessible to a wider audience. Due to the lack of parallel corpora for TS consisting of original and
manually simplified texts, most of the ATS systems for specific target populations are still rule-based.
Our main goal was to explore whether some of the existing TS parallel corpora in English, aimed at dif-
ferent audiences (children – Encyclopedia Britannica, language learners – Weekly Reader, and various –
Wikipedia) could be used as training material to build a TS system aimed at people with ASD. We anal-
ysed the four corpora (FIRST, Britannica, Weekly Reader, and Wikipedia) in terms of nineteen linguistic
features which pose obstacles to reading comprehension for people with ASD. The preliminary results
indicate that the Britannica TS parallel corpus, and possibly the Weekly Reader TS parallel corpus, could
be used to train a TS system aimed at people with ASD. Two sets of classification experiments which
tried to discriminate original from simplified texts according to the nineteen features derived from user
requirements further supported those findings. The results of the classification experiments indicated
that the SVM classifier trained on the Britannica corpus might be suitable for discriminating original
from simplified texts for people with ASD, and thus might be used as the initial evaluation of the texts
simplified by the TS system developed in the FIRST project.
</bodyText>
<sectionHeader confidence="0.99496" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.994887">
The research described in this paper was partially funded by the European Commission under the Sev-
enth (FP7-2007-2013) Framework Programme for Research and Technological Development (FP7-ICT-
2011.5.5 FIRST 287607).
</bodyText>
<sectionHeader confidence="0.998938" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997836">
D. Allen. 2009. A Corpus-Based Study of the Role of Relative Clauses in the Simplification of News Texts for
Learners of English. System, 37(4):585–599.
S. M. Alu´ısio, L. Specia, T. A.S. Pardo, E. G. Maziero, and R. P.M. Fortes. 2008. Towards brazilian portuguese
automatic text simplification systems. In Proceedings of the eighth ACM symposium on Document engineering,
DocEng ’08, pages 240–248, New York, NY, USA. ACM.
D. J. Arya, Elfrieda H. Hiebert, and P. D. Pearson. 2011. The effects of syntactic and lexical complexity on
the comprehension of elementary science texts. International Electronic Journal of Elementary Education, 4
(1):107–125.
</reference>
<page confidence="0.993226">
61
</page>
<reference confidence="0.999568229166667">
D. Balota, M. J. Cortese, S. D. Sergent-Marshall, D. H. Spieler, and M. J. Yap. 2004. Visual word recognition of
single-syllabe words. Journal of Experimental Psychology: General, 133:283–316.
E. Barbu, M. Martin-Valdivia, L. Alfonso, and U. Lopez. 2013. Open book: a tool for helping asd users’ semantic
comprehension. In Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual
Accessibility (NLP4ITA), pages 11–19, Atlanta, US. Association for Computational Linguistics.
R. Barzilay and N. Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of the
2003 conference on Empirical methods in natural language processing, EMNLP ’03, pages 25–32, Stroudsburg,
PA, USA. Association for Computational Linguistics.
S. Bautista, C. Le´on, R. Herv´as, and P. Gerv´as. 2011. Empirical identification of text simplification strategies for
reading-impaired people. In European Conference for the Advancement of Assistive Technology.
O. Biran, S. Brody, and N. Elhadad. 2011. Putting it Simply: a Context-Aware Approach to Lexical Simplification.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, pages 496–501, Portland, Oregon, USA. Association for Computational Linguistics.
J. Carroll, G. Minnen, Y. Canning, S. Devlin, and J. Tait. 1998. Practical Simplification of English Newspaper
Text to Assist Aphasic Readers. In Proceedings ofAAAI-98 Workshop on Integrating Artificial Intelligence and
Assistive Technology, pages 7–10.
W. Cohen. 1995. Fast Effective Rule Induction. In Proceedings of the Twelfth International Conference on
Machine Learning, pages 115–123.
W. Coster and D. Kauchak. 2011. Learning to Simplify Sentences Using Wikipedia. In Proceedings of the 49th
Annual Meeting of the Association for Computational Linguistics, pages 1–9.
S. Devlin and G. Unthank. 2006. Helping aphasic people process online information. In Proceedings of the 8th
international ACM SIGACCESS conference on Computers and accessibility, Assets ’06, pages 225–226, New
York, NY, USA. ACM.
I. Dornescu, R. Evans, and C. Orasan. 2013. A Tagging Approach to Identify Complex Constituents for Text
Simplification. In Proceedings of Recent Advances in Natural Language Processing, pages 221 – 229, Hissar,
Bulgaria.
B Drndarevi´c and H. Saggion. 2012. Reducing Text Complexity through Automatic Lexical Simplification: an
Empirical Study for Spanish. SEPLN Journal, 49.
R. Evans, C. Orasan, and I. Dornescu. 2014. An evaluation of syntactic simplification rules for people with
autism. In Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader
Populations (PITR), pages 131–140, Gothenburg, Sweden, April. Association for Computational Linguistics.
J. Fine, G. Bartolucci, P. Szatmari, and G. Ginsberg. 1994. Cohesive discourse in pervasive developmental
disorders. Journal of Autism and Developmental Disorders, 24:315–329.
G. Freyhoff, G. Hess, L. Kerr, B. Tronbacke, and K. Van Der Veken, 1998. Make it Simple, European Guide-
lines for the Production of Easy-toRead Information for People with Learning Disability. ILSMH European
Association, Brussels.
C. Gasperin, L. Specia, T. Pereira, and S.M. Aluisio. 2009. Learning When to Simplify Sentences for Natural Text
Simplification. In Proceedings of the Encontro Nacional de Inteligncia Artificial (ENIA-2009), Bento Gonalves,
Brazil., pages 809–818.
M. Glanzer and N. Bowles. 1976. Analysis of the word frequency effect in recognition memory. Journal of
Experimental Psychology: Human Learning and Memory, 2:21–31.
C. Grover, C. Matheson, A. Mikheev, and M. Moens. 2000. Lt ttt - a flexible tokenisation tool. In In Proceedings
of Second International Conference on Language Resources and Evaluation, pages 1147–1154.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. 2009. The weka data mining
software: an update. SIGKDD Explor. Newsl., 11:10–18, November.
K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura. 2003. Text simplification for reading assistance: a project
note. In Proceedings of the second international workshop on Paraphrasing - Volume 16, PARAPHRASE ’03,
pages 9–16, Stroudsburg, PA, USA. Association for Computational Linguistics.
</reference>
<page confidence="0.980858">
62
</page>
<reference confidence="0.999688770833333">
J. Jastrzembski. 1981. Multiple meaning, number or related meanings, frequency of occurrence and the lexicon.
Cognitive Psychology, 13:278–305.
G. H. John and P. Langley. 1995. Estimating Continuous Distributions in Bayesian Classifiers. In Proceedings of
the Eleventh Conference on Uncertainty in Artificial Intelligence, pages 338–345.
D. Kauchak. 2013. Improving text simplification language modeling using unsimplified text data. In Proceedings
of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
1537–1546, Sofia, Bulgaria, August. Association for Computational Linguistics.
S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and K. R. K. Murthy. 2001. Improvements to Platt’s SMO
Algorithm for SVM Classifier Design. Neural Computation, 13(3):637–649.
S. T. Kover, E. Haebig, A. Oakes, A. McDuffie, R. J. Hagerman, and L. Abbeduto. 2012. Syntactic comprehension
in boys with autism spectrum disorders: Evidence from specific constructions. In Proceedings of the 2012
International Meeting for Autism Research, Athens, Greece. International Society for Autism Research.
J. Martos, S. Freire, A. Gonz´alez, D. Gil, R. Evans, V. Jordanova, A. Cerga, A. Shishkova, and C. Orasan. 2013.
FIRST Deliverable - User preferences: Updated. Technical Report D2.2, Deletrea, Madrid, Spain.
C. Napoles and M. Dredze. 2010. Learning simple wikipedia: a cogitation in ascertaining abecedarian language.
In Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing: Writing Pro-
cesses and Authoring Aids, CL&amp;W ’10, pages 42–50, Stroudsburg, PA, USA. Association for Computational
Linguistics.
S. E. Petersen and M. Ostendorf. 2007. Text Simplification for Language Learners: A Corpus Analysis. In
Proceedings of Workshop on Speech and Language Technology for Education.
R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, San Mateo, CA.
H. Saggion, E. G´omez Martinez, E. Etayo, A. Anula, and L. Bourg. 2011. Text Simplification in Simplext:
Making Text More Accessible. Revista de la Sociedad Espa˜nola para el Procesamiento del Lenguaje Natural,
47:341–342.
H. Tager-Flusberg. 1981. Sentence comprehension in autistic children. Applied Psycholinguistics, 2:1:5–24.
M. Tattamanti, R. Manenti, P. A. Della Rosa, A. Falini, D. Perani, S. Cappa, and A. Moro. 2008. Negation in the
brain: Modulating action representations. NeuroImage, 43 (2008):358–367.
UN. 2006. Convention on the rigths of persons with disabilities.
F. R. Volkmar and L. Wiesner. 2009. A Practical Guide to Autism. Wiley, Hoboken, NJ, 2nd edition.
S. ˇStajner and H. Saggion. 2013. Adapting Text Simplification Decisions to Different Text Genres and Target
Users. Procesamiento del Lenguaje Natural, 51:135–142.
S. ˇStajner, B. Drndarevi´c, and H. Saggion. 2013. Corpus-based Sentence Deletion and Split Decisions for Spanish
Text Simplification. Computaci´on y Systemas, 17(2):251–262.
I. H. Witten and E. Frank. 2005. Data mining: practical machine learning tools and techniques. Morgan Kauf-
mann Publishers.
K. Woodsend and M. Lapata. 2011. Learning to Simplify Sentences with Quasi-Synchronous Grammar and
Integer Programming. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language
Processing (EMNLP).
S. Wubben, A. van den Bosch, and E. Krahmer. 2012. Sentence simplification by monolingual machine transla-
tion. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers
- Volume 1, ACL ’12, pages 1015–1024, Stroudsburg, PA, USA. Association for Computational Linguistics.
M. Yatskar, B. Pang, C. Danescu-Niculescu-Mizil, and L. Lee. 2010. For the sake of simplicity: unsupervised
extraction of lexical simplifications from wikipedia. In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages
365–368, Stroudsburg, PA, USA. Association for Computational Linguistics.
Z. Zhu, D. Berndard, and I. Gurevych. 2010. A Monolingual Tree-based Translation Model for Sentence Sim-
plification. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),
pages 1353–1361.
</reference>
<page confidence="0.99946">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.556949">
<title confidence="0.9797585">Assessing Conformance of Manually Simplified Corpora User Requirements: the Case of Autistic Readers</title>
<author confidence="0.605118">Evans</author>
<affiliation confidence="0.978681666666667">Research Group in Computational Research Institute of Information and Language University of Wolverhampton,</affiliation>
<email confidence="0.968067">R.J.Evans,</email>
<abstract confidence="0.998608230769231">In the state of the art, there are scarce resources available to support development and evaluation of automatic text simplification (TS) systems for specific target populations. These comprise parallel corpora consisting of texts in their original form and in a form that is more accessible for different categories of target reader, including neurotypical second language learners and young readers. In this paper, we investigate the potential to exploit resources developed for such readers to support the development of a text simplification system for use by people with autistic spectrum disorders (ASD). We analysed four corpora in terms of nineteen linguistic features which pose obstacles to reading comprehension for people with ASD. The results indicate that the Britannica TS parallel corpus (aimed at young readers) and the Weekly Reader TS parallel corpus (aimed at second language learners) may be suitable for training a TS system to assist people with ASD. Two sets of classification experiments intended to discriminate between original and simplified texts according to the nineteen features lent further support for those findings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Allen</author>
</authors>
<title>A Corpus-Based Study of the Role of Relative Clauses in the Simplification of News Texts for Learners of English.</title>
<date>2009</date>
<journal>System,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="5077" citStr="Allen, 2009" startWordPosition="715" endWordPosition="716">sh. All previously mentioned studies conducted on that corpus evaluated the quality of the generated output in terms of grammaticality, meaning preservation, and simplicity, but not usefulness. Also, there have been no comparisons of the types of transformations present in EW–SEW with any of the other TS corpora in English which were simplified with a specific target population in mind, e.g. Encyclopedia Britannica and its manually simplified versions for children – Britannica Elementary (Barzilay and Elhadad, 2003)6, Guardian Weekly and its manually simplified versions for language learners (Allen, 2009), and the FIRST corpus of various texts simplified for people with autism spectrum disorder (ASD)7. In this study, we compare the original and simplified texts of the four aforementioned TS corpora in terms of nineteen features which measure the complexity of texts for people with ASD. Although these features were derived from user requirements for people with ASD, many of them are known to present reading obstacles for other target populations as well (e.g. children or language learners). Given the lack of parallel TS corpora for people with ASD, our main goal is to investigate whether the EW</context>
<context position="15505" citStr="Allen, 2009" startWordPosition="2274" endWordPosition="2275">1. The corpus of 100 texts from Weekly Reader and their manual simplifications provided by Macmillan English Campus and Onestopenglish10 aimed at foreign language learners. The corpus is divided into three sub-corpora – advanced, intermediate and elementary – each representing a different level of simplification. Given that the other three corpora used in this study contain original texts and only one level of simplification, we only used the texts from the advanced (henceforth original) and elementary (henceforth simplified) levels. A more detailed description of this corpus can be found in (Allen, 2009). 2. The corpus of 20 texts from the Encyclopedia Britannica and their manually simplified versions aimed at children – Britannica Elementary (Barzilay and Elhadad, 2003)11. 3. The corpus of 110 randomly selected corresponding articles from EW and SEW. Here, it is important to note that, in general, articles from SEW do not represent direct simplifications of the articles from EW, they just have a matching topic. For this reason, we did not use complete EW and SEW articles. We only used those sentences in original and simplified versions, which existed in the sentence-aligned parallel corpora </context>
</contexts>
<marker>Allen, 2009</marker>
<rawString>D. Allen. 2009. A Corpus-Based Study of the Role of Relative Clauses in the Simplification of News Texts for Learners of English. System, 37(4):585–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Alu´ısio</author>
<author>L Specia</author>
<author>T A S Pardo</author>
<author>E G Maziero</author>
<author>R P M Fortes</author>
</authors>
<title>Towards brazilian portuguese automatic text simplification systems.</title>
<date>2008</date>
<booktitle>In Proceedings of the eighth ACM symposium on Document engineering, DocEng ’08,</booktitle>
<pages>240--248</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Alu´ısio, Specia, Pardo, Maziero, Fortes, 2008</marker>
<rawString>S. M. Alu´ısio, L. Specia, T. A.S. Pardo, E. G. Maziero, and R. P.M. Fortes. 2008. Towards brazilian portuguese automatic text simplification systems. In Proceedings of the eighth ACM symposium on Document engineering, DocEng ’08, pages 240–248, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Arya</author>
<author>Elfrieda H Hiebert</author>
<author>P D Pearson</author>
</authors>
<title>The effects of syntactic and lexical complexity on the comprehension of elementary science texts.</title>
<date>2011</date>
<journal>International Electronic Journal of Elementary Education,</journal>
<volume>4</volume>
<pages>1--107</pages>
<contexts>
<context position="17642" citStr="Arya et al., 2011" startWordPosition="2573" endWordPosition="2576"> is presented in Table 3. In every case, accessible texts are expected to have smaller values of each metric. # Code Linguistic feature Explanation/relevance 1 Illative Illative conjunctions Indicators of syntactic complexity, linking clauses. 2 CompConj Comparative conjunctions [UR304-306] 3 AdvConj Adversative conjunctions 4 LongSent Long sentences Motivated by the assumption that deriving the propositions in 5 Semicol Semicolons/suspension complex sentences is more difficult than deriving connections bepoints tween related propositions expressed in simple sentences 6 Passive Passive verbs (Arya et al., 2011). [UR309-310, UR313] 7 UnPunc Unusual punctuation Indicates syntactic complexity, ellipsis, alternatives, and mathematical expressions [UR311] 8 Negations Negation The sum of adverbial and morphological negations (“Make it Simple” (Freyhoff et al., 1998), though contrary to the findings of Tattamanti (2008)) [UR314] 9 Senses Possible senses The sum over all tokens in the text of the total number of possible senses of each token. [UR401, UR425, UR504-505, UR511] 10 PolyW Polysemic words Words with two or more senses listed in WordNet. [UR401, UR425, UR504, UR505, UR511] 11 Infreq Infrequent wor</context>
</contexts>
<marker>Arya, Hiebert, Pearson, 2011</marker>
<rawString>D. J. Arya, Elfrieda H. Hiebert, and P. D. Pearson. 2011. The effects of syntactic and lexical complexity on the comprehension of elementary science texts. International Electronic Journal of Elementary Education, 4 (1):107–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Balota</author>
<author>M J Cortese</author>
<author>S D Sergent-Marshall</author>
<author>D H Spieler</author>
<author>M J Yap</author>
</authors>
<title>Visual word recognition of single-syllabe words.</title>
<date>2004</date>
<journal>Journal of Experimental Psychology: General,</journal>
<pages>133--283</pages>
<contexts>
<context position="31746" citStr="Balota et al., 2004" startWordPosition="4718" endWordPosition="4721">in mind that polysemous words usually pose an obstacle when conveying one of their infrequently used meanings. Findings in cognitive psychology indicate that the words with the highest number of possible meanings are actually understood more quickly, due to their high frequency (Jastrzembski, 1981). A common lexical simplification strategy is to replace infrequent words with their more frequent synonyms, and long words with their shorter synonyms. This strategy leads to a higher polysemic type ratio (PolyType) in simplified versions of the texts as the shorter words are usually more frequent (Balota et al., 2004), and frequent words tend to be more polysemous than infrequent ones (Glanzer and Bowles, 1976). 5.2 Classification between Original and Simplified Texts Classification experiments were conducted using two different sets of features on each of the corpora: 1. all – all 18 features (UnPunc was excluded as it was not reported as significant for any of the corpora) 2. best – 11 features which were reported as significant for all four corpora (Illative, AdvConj, LongSent, Negations, PolyW, NumExp, DefDescr, SylLongW, CommaInd, WordsPerSent, TypeTokRat) As can be observed from Table 7, use of the S</context>
</contexts>
<marker>Balota, Cortese, Sergent-Marshall, Spieler, Yap, 2004</marker>
<rawString>D. Balota, M. J. Cortese, S. D. Sergent-Marshall, D. H. Spieler, and M. J. Yap. 2004. Visual word recognition of single-syllabe words. Journal of Experimental Psychology: General, 133:283–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Barbu</author>
<author>M Martin-Valdivia</author>
<author>L Alfonso</author>
<author>U Lopez</author>
</authors>
<title>Open book: a tool for helping asd users’ semantic comprehension.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA),</booktitle>
<pages>11--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, US.</location>
<contexts>
<context position="7417" citStr="Barbu et al., 2013" startWordPosition="1068" endWordPosition="1071">g, 1981; Kover et al., 2012). To address these difficulties, a tool is being developed in the FIRST project8 to assist in the process of making texts more accessible for people with ASD. To achieve this, three modues are exploited: 1. Structural complexity processor, which detects syntactically complex sentences and generates alternatives to such sentences in the form of sequences of shorter sentences (Evans et al., 2014; Dornescu et al., 2013). 2. Meaning disambiguator, which resolves pronominal references, performs word sense disambiguation, and detects lexicalised (conventional) metaphors (Barbu et al., 2013). 3. Personalised document generator, which aggregates the output of processors 1 and 2 and generates additional elements such as glossaries, illustrative images, and document summaries. The system, named Open Book, is deployed as an editing tool for healthcare and educational service providers. It functions semi-automatically, exploiting the three processors and requiring the user to authorise the application of the conversion operations. The system is required to assess the readability of texts, not only to decide which texts should be converted, but also to assess the readability of texts t</context>
</contexts>
<marker>Barbu, Martin-Valdivia, Alfonso, Lopez, 2013</marker>
<rawString>E. Barbu, M. Martin-Valdivia, L. Alfonso, and U. Lopez. 2013. Open book: a tool for helping asd users’ semantic comprehension. In Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA), pages 11–19, Atlanta, US. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>N Elhadad</author>
</authors>
<title>Sentence alignment for monolingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 conference on Empirical methods in natural language processing, EMNLP ’03,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4986" citStr="Barzilay and Elhadad, 2003" startWordPosition="701" endWordPosition="704">ences. The main page states that SEW is for everyone, including children and adults who are learning English. All previously mentioned studies conducted on that corpus evaluated the quality of the generated output in terms of grammaticality, meaning preservation, and simplicity, but not usefulness. Also, there have been no comparisons of the types of transformations present in EW–SEW with any of the other TS corpora in English which were simplified with a specific target population in mind, e.g. Encyclopedia Britannica and its manually simplified versions for children – Britannica Elementary (Barzilay and Elhadad, 2003)6, Guardian Weekly and its manually simplified versions for language learners (Allen, 2009), and the FIRST corpus of various texts simplified for people with autism spectrum disorder (ASD)7. In this study, we compare the original and simplified texts of the four aforementioned TS corpora in terms of nineteen features which measure the complexity of texts for people with ASD. Although these features were derived from user requirements for people with ASD, many of them are known to present reading obstacles for other target populations as well (e.g. children or language learners). Given the lack</context>
<context position="15675" citStr="Barzilay and Elhadad, 2003" startWordPosition="2297" endWordPosition="2300">anguage learners. The corpus is divided into three sub-corpora – advanced, intermediate and elementary – each representing a different level of simplification. Given that the other three corpora used in this study contain original texts and only one level of simplification, we only used the texts from the advanced (henceforth original) and elementary (henceforth simplified) levels. A more detailed description of this corpus can be found in (Allen, 2009). 2. The corpus of 20 texts from the Encyclopedia Britannica and their manually simplified versions aimed at children – Britannica Elementary (Barzilay and Elhadad, 2003)11. 3. The corpus of 110 randomly selected corresponding articles from EW and SEW. Here, it is important to note that, in general, articles from SEW do not represent direct simplifications of the articles from EW, they just have a matching topic. For this reason, we did not use complete EW and SEW articles. We only used those sentences in original and simplified versions, which existed in the sentence-aligned parallel corpora version 2.012 (Kauchak, 2013). 4. The corpus of 25 texts on various topics manually simplified for people with autism, compiled in the FIRST project13, for the purpose of</context>
</contexts>
<marker>Barzilay, Elhadad, 2003</marker>
<rawString>R. Barzilay and N. Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of the 2003 conference on Empirical methods in natural language processing, EMNLP ’03, pages 25–32, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bautista</author>
<author>C Le´on</author>
<author>R Herv´as</author>
<author>P Gerv´as</author>
</authors>
<title>Empirical identification of text simplification strategies for reading-impaired people.</title>
<date>2011</date>
<booktitle>In European Conference for the Advancement of Assistive Technology.</booktitle>
<marker>Bautista, Le´on, Herv´as, Gerv´as, 2011</marker>
<rawString>S. Bautista, C. Le´on, R. Herv´as, and P. Gerv´as. 2011. Empirical identification of text simplification strategies for reading-impaired people. In European Conference for the Advancement of Assistive Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Biran</author>
<author>S Brody</author>
<author>N Elhadad</author>
</authors>
<title>Putting it Simply: a Context-Aware Approach to Lexical Simplification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>496--501</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="3322" citStr="Biran et al., 2011" startWordPosition="483" endWordPosition="486">st twenty years, many ATS systems have been proposed for different target populations in various languages (Carroll et al., 1998; Devlin and Unthank, 2006; Saggion et al., 2011; Inui et al., 2003; Alu´ısio et al., 2008). Due to the scarcity of parallel corpora of original and manually simplified texts, most of these systems are rule-based. The emergence of Simple English Wikipedia (SEW)4, together with the existing English Wikipedia (EW)5 provided a large amount of parallel TS training data, which motivated a shift in English TS from rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://www.plainlanguage.gov/howto/guidelines/bigdoc/fullbigdoc.pdf 2http://www.easy-read-online.co.uk/media/10609/making-myself-clear.pdf 3http://www.w3.org/WAI/ 4http://simple.wikipedia.org/wiki/Main Page 5http://wikipedia.org/wiki/Main Page 53 Proceedings of the</context>
</contexts>
<marker>Biran, Brody, Elhadad, 2011</marker>
<rawString>O. Biran, S. Brody, and N. Elhadad. 2011. Putting it Simply: a Context-Aware Approach to Lexical Simplification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 496–501, Portland, Oregon, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>G Minnen</author>
<author>Y Canning</author>
<author>S Devlin</author>
<author>J Tait</author>
</authors>
<title>Practical Simplification of English Newspaper Text to Assist Aphasic Readers.</title>
<date>1998</date>
<booktitle>In Proceedings ofAAAI-98 Workshop on Integrating Artificial Intelligence and Assistive Technology,</booktitle>
<pages>7--10</pages>
<contexts>
<context position="2832" citStr="Carroll et al., 1998" startWordPosition="405" endWordPosition="408">., 1998), “Am I making myself clear? Mencap’s guidelines for accessible writing”2, and the W3C – Web Accessibility Initiative guidelines3. However, manual adaptation of texts cannot match the speed with which new texts are published on the web in order to provide up to date information. The aim of Automatic Text Simplification (ATS) is to automatically (or at least semi-automatically) convert complex sentences into a more accessible form while preserving their original meaning. In the last twenty years, many ATS systems have been proposed for different target populations in various languages (Carroll et al., 1998; Devlin and Unthank, 2006; Saggion et al., 2011; Inui et al., 2003; Alu´ısio et al., 2008). Due to the scarcity of parallel corpora of original and manually simplified texts, most of these systems are rule-based. The emergence of Simple English Wikipedia (SEW)4, together with the existing English Wikipedia (EW)5 provided a large amount of parallel TS training data, which motivated a shift in English TS from rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no ass</context>
</contexts>
<marker>Carroll, Minnen, Canning, Devlin, Tait, 1998</marker>
<rawString>J. Carroll, G. Minnen, Y. Canning, S. Devlin, and J. Tait. 1998. Practical Simplification of English Newspaper Text to Assist Aphasic Readers. In Proceedings ofAAAI-98 Workshop on Integrating Artificial Intelligence and Assistive Technology, pages 7–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
</authors>
<title>Fast Effective Rule Induction.</title>
<date>1995</date>
<booktitle>In Proceedings of the Twelfth International Conference on Machine Learning,</booktitle>
<pages>115--123</pages>
<contexts>
<context position="22314" citStr="Cohen, 1995" startWordPosition="3327" endWordPosition="3328">ra (text length in words and sentences differed significantly across different corpora). 2. Classification experiments with the aim of discriminating original from simplified texts using the nineteen selected features. All experiments were conducted using the Weka Experimenter (Witten and Frank, 2005; Hall et al., 2009) in 10-fold cross-validation setup with 10 repetitions, using four different classification algorithms: NB – NaiveBayes (John and Langley, 1995), SMO – Weka implementation of Support Vector Machines (Keerthi et al., 2001) with normalisation, JRip – a propositional rule learner (Cohen, 1995), and J48 – Weka implementation of C4.5 (Quinlan, 1993). The statistical significance of the observed differences in F-measures obtained by different algorithms was calculated using the corrected paired t-test provided in the Weka Experimenter. The TS system in FIRST is not only supposed to decide which texts should be converted, but also to assess the readability of texts that are undergoing conversion. It is expected that people working to improve the accessibility of a given text will benefit from relevant feedback concerning the effects of the changes being introduced. Automatic assessment</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>W. Cohen. 1995. Fast Effective Rule Induction. In Proceedings of the Twelfth International Conference on Machine Learning, pages 115–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Coster</author>
<author>D Kauchak</author>
</authors>
<title>Learning to Simplify Sentences Using Wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="3375" citStr="Coster and Kauchak, 2011" startWordPosition="491" endWordPosition="494">posed for different target populations in various languages (Carroll et al., 1998; Devlin and Unthank, 2006; Saggion et al., 2011; Inui et al., 2003; Alu´ısio et al., 2008). Due to the scarcity of parallel corpora of original and manually simplified texts, most of these systems are rule-based. The emergence of Simple English Wikipedia (SEW)4, together with the existing English Wikipedia (EW)5 provided a large amount of parallel TS training data, which motivated a shift in English TS from rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://www.plainlanguage.gov/howto/guidelines/bigdoc/fullbigdoc.pdf 2http://www.easy-read-online.co.uk/media/10609/making-myself-clear.pdf 3http://www.w3.org/WAI/ 4http://simple.wikipedia.org/wiki/Main Page 5http://wikipedia.org/wiki/Main Page 53 Proceedings of the Workshop on Automatic Text Simplification: Methods a</context>
</contexts>
<marker>Coster, Kauchak, 2011</marker>
<rawString>W. Coster and D. Kauchak. 2011. Learning to Simplify Sentences Using Wikipedia. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Devlin</author>
<author>G Unthank</author>
</authors>
<title>Helping aphasic people process online information.</title>
<date>2006</date>
<booktitle>In Proceedings of the 8th international ACM SIGACCESS conference on Computers and accessibility, Assets ’06,</booktitle>
<pages>225--226</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2858" citStr="Devlin and Unthank, 2006" startWordPosition="409" endWordPosition="412"> myself clear? Mencap’s guidelines for accessible writing”2, and the W3C – Web Accessibility Initiative guidelines3. However, manual adaptation of texts cannot match the speed with which new texts are published on the web in order to provide up to date information. The aim of Automatic Text Simplification (ATS) is to automatically (or at least semi-automatically) convert complex sentences into a more accessible form while preserving their original meaning. In the last twenty years, many ATS systems have been proposed for different target populations in various languages (Carroll et al., 1998; Devlin and Unthank, 2006; Saggion et al., 2011; Inui et al., 2003; Alu´ısio et al., 2008). Due to the scarcity of parallel corpora of original and manually simplified texts, most of these systems are rule-based. The emergence of Simple English Wikipedia (SEW)4, together with the existing English Wikipedia (EW)5 provided a large amount of parallel TS training data, which motivated a shift in English TS from rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has This work is l</context>
</contexts>
<marker>Devlin, Unthank, 2006</marker>
<rawString>S. Devlin and G. Unthank. 2006. Helping aphasic people process online information. In Proceedings of the 8th international ACM SIGACCESS conference on Computers and accessibility, Assets ’06, pages 225–226, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dornescu</author>
<author>R Evans</author>
<author>C Orasan</author>
</authors>
<title>A Tagging Approach to Identify Complex Constituents for Text Simplification.</title>
<date>2013</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing,</booktitle>
<pages>221--229</pages>
<location>Hissar, Bulgaria.</location>
<contexts>
<context position="7246" citStr="Dornescu et al., 2013" startWordPosition="1047" endWordPosition="1050">iculty inferring contextual information or may have trouble understanding mental verbs, emotional language, and long sentences with complex syntactic structure (Tager-Flusberg, 1981; Kover et al., 2012). To address these difficulties, a tool is being developed in the FIRST project8 to assist in the process of making texts more accessible for people with ASD. To achieve this, three modues are exploited: 1. Structural complexity processor, which detects syntactically complex sentences and generates alternatives to such sentences in the form of sequences of shorter sentences (Evans et al., 2014; Dornescu et al., 2013). 2. Meaning disambiguator, which resolves pronominal references, performs word sense disambiguation, and detects lexicalised (conventional) metaphors (Barbu et al., 2013). 3. Personalised document generator, which aggregates the output of processors 1 and 2 and generates additional elements such as glossaries, illustrative images, and document summaries. The system, named Open Book, is deployed as an editing tool for healthcare and educational service providers. It functions semi-automatically, exploiting the three processors and requiring the user to authorise the application of the conversi</context>
</contexts>
<marker>Dornescu, Evans, Orasan, 2013</marker>
<rawString>I. Dornescu, R. Evans, and C. Orasan. 2013. A Tagging Approach to Identify Complex Constituents for Text Simplification. In Proceedings of Recent Advances in Natural Language Processing, pages 221 – 229, Hissar, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Drndarevi´c</author>
<author>H Saggion</author>
</authors>
<title>Reducing Text Complexity through Automatic Lexical Simplification: an Empirical Study for Spanish.</title>
<date>2012</date>
<journal>SEPLN Journal,</journal>
<volume>49</volume>
<marker>Drndarevi´c, Saggion, 2012</marker>
<rawString>B Drndarevi´c and H. Saggion. 2012. Reducing Text Complexity through Automatic Lexical Simplification: an Empirical Study for Spanish. SEPLN Journal, 49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Evans</author>
<author>C Orasan</author>
<author>I Dornescu</author>
</authors>
<title>An evaluation of syntactic simplification rules for people with autism.</title>
<date>2014</date>
<booktitle>In Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR),</booktitle>
<pages>131--140</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="7222" citStr="Evans et al., 2014" startWordPosition="1043" endWordPosition="1046">. They may have difficulty inferring contextual information or may have trouble understanding mental verbs, emotional language, and long sentences with complex syntactic structure (Tager-Flusberg, 1981; Kover et al., 2012). To address these difficulties, a tool is being developed in the FIRST project8 to assist in the process of making texts more accessible for people with ASD. To achieve this, three modues are exploited: 1. Structural complexity processor, which detects syntactically complex sentences and generates alternatives to such sentences in the form of sequences of shorter sentences (Evans et al., 2014; Dornescu et al., 2013). 2. Meaning disambiguator, which resolves pronominal references, performs word sense disambiguation, and detects lexicalised (conventional) metaphors (Barbu et al., 2013). 3. Personalised document generator, which aggregates the output of processors 1 and 2 and generates additional elements such as glossaries, illustrative images, and document summaries. The system, named Open Book, is deployed as an editing tool for healthcare and educational service providers. It functions semi-automatically, exploiting the three processors and requiring the user to authorise the app</context>
</contexts>
<marker>Evans, Orasan, Dornescu, 2014</marker>
<rawString>R. Evans, C. Orasan, and I. Dornescu. 2014. An evaluation of syntactic simplification rules for people with autism. In Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR), pages 131–140, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fine</author>
<author>G Bartolucci</author>
<author>P Szatmari</author>
<author>G Ginsberg</author>
</authors>
<title>Cohesive discourse in pervasive developmental disorders.</title>
<date>1994</date>
<journal>Journal of Autism and Developmental Disorders,</journal>
<pages>24--315</pages>
<contexts>
<context position="18601" citStr="Fine et al., 1994" startWordPosition="2719" endWordPosition="2722">s The sum over all tokens in the text of the total number of possible senses of each token. [UR401, UR425, UR504-505, UR511] 10 PolyW Polysemic words Words with two or more senses listed in WordNet. [UR401, UR425, UR504, UR505, UR511] 11 Infreq Infrequent words Words that are not among the 5000 most frequent words in English [UR304-306, UR401, UR425, UR504-505, UR511] 12 NumExp Numerical expressions Numbers written as sequences of words rather than digits [UR417] 13 Pron Pronouns Studies have shown that people with ASD can have 14 DefDescr Definite descriptions difficulty processing anaphora (Fine et al., 1994) [UR418-420] 15 SylLongW Long words Words with more than three syllables [UR317-319] Table 2: Complexity measures (1 – words such as therefore and hence; 2 – words such as equally and correspondingly; 3 – words such as although and conversely; 4 – sentences more than 15 words long; 8 – negative adverbials and negative prefixes such as un- and dis-; 11 – derived from Wiktionary frequency lists for English16) # Code Metric Formula Relevance 16 PolyType Polysemic type ratio ptyp Indicates the proportion of the text vocabulary that is polysemous. [UR401, UR425, UR504-505, UR511] typ 17 CommaInd Co</context>
</contexts>
<marker>Fine, Bartolucci, Szatmari, Ginsberg, 1994</marker>
<rawString>J. Fine, G. Bartolucci, P. Szatmari, and G. Ginsberg. 1994. Cohesive discourse in pervasive developmental disorders. Journal of Autism and Developmental Disorders, 24:315–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Freyhoff</author>
<author>G Hess</author>
<author>L Kerr</author>
<author>B Tronbacke</author>
<author>K Van Der Veken</author>
</authors>
<title>Make it Simple, European Guidelines for the Production of Easy-toRead Information for People with Learning Disability.</title>
<date>1998</date>
<booktitle>ILSMH European Association,</booktitle>
<location>Brussels.</location>
<marker>Freyhoff, Hess, Kerr, Tronbacke, Van Der Veken, 1998</marker>
<rawString>G. Freyhoff, G. Hess, L. Kerr, B. Tronbacke, and K. Van Der Veken, 1998. Make it Simple, European Guidelines for the Production of Easy-toRead Information for People with Learning Disability. ILSMH European Association, Brussels.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gasperin</author>
<author>L Specia</author>
<author>T Pereira</author>
<author>S M Aluisio</author>
</authors>
<title>Learning When to Simplify Sentences for Natural Text Simplification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Encontro Nacional de Inteligncia Artificial (ENIA-2009), Bento Gonalves, Brazil.,</booktitle>
<pages>809--818</pages>
<contexts>
<context position="10804" citStr="Gasperin et al., 2009" startWordPosition="1549" endWordPosition="1552">ems in original and converted versions of texts are not captured in this study. The full set of user requirements is detailed in Martos et al. (2013). More generally, it is infrequent linguistic phenomena that cause the greatest difficulty. 3 Related Work There have been several studies analysing the existing TS corpora. However, their main focus was on determining necessary transformations in TS: for children (Bautista et al., 2011); for people with intellectual disability (Drndarevi´c and Saggion, 2012); for language learners (Petersen and Ostendorf, 2007); and for people with low literacy (Gasperin et al., 2009). Unfortunately, those studies are not directly comparable (neither among themselves nor with our study), either because they focus on different types of transformations (the study of Bautista et al. (2011) focuses on general transformations while the other three studies focus on sentence transformations), or because they treat different languages (Spanish, English, and Brazilian Portuguese). Two previous studies most relevant to ours are those by Napoles and Dredze (2010), and by ˇStajner et al. (2013). Napoles and Dredze (2010) built a statistical classification system that discriminates sim</context>
</contexts>
<marker>Gasperin, Specia, Pereira, Aluisio, 2009</marker>
<rawString>C. Gasperin, L. Specia, T. Pereira, and S.M. Aluisio. 2009. Learning When to Simplify Sentences for Natural Text Simplification. In Proceedings of the Encontro Nacional de Inteligncia Artificial (ENIA-2009), Bento Gonalves, Brazil., pages 809–818.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Glanzer</author>
<author>N Bowles</author>
</authors>
<title>Analysis of the word frequency effect in recognition memory.</title>
<date>1976</date>
<journal>Journal of Experimental Psychology: Human Learning and Memory,</journal>
<pages>2--21</pages>
<contexts>
<context position="31841" citStr="Glanzer and Bowles, 1976" startWordPosition="4733" endWordPosition="4736">ently used meanings. Findings in cognitive psychology indicate that the words with the highest number of possible meanings are actually understood more quickly, due to their high frequency (Jastrzembski, 1981). A common lexical simplification strategy is to replace infrequent words with their more frequent synonyms, and long words with their shorter synonyms. This strategy leads to a higher polysemic type ratio (PolyType) in simplified versions of the texts as the shorter words are usually more frequent (Balota et al., 2004), and frequent words tend to be more polysemous than infrequent ones (Glanzer and Bowles, 1976). 5.2 Classification between Original and Simplified Texts Classification experiments were conducted using two different sets of features on each of the corpora: 1. all – all 18 features (UnPunc was excluded as it was not reported as significant for any of the corpora) 2. best – 11 features which were reported as significant for all four corpora (Illative, AdvConj, LongSent, Negations, PolyW, NumExp, DefDescr, SylLongW, CommaInd, WordsPerSent, TypeTokRat) As can be observed from Table 7, use of the SMO-n classification algorithm using the subset of 11 best features achieves perfect 1.00 F-meas</context>
</contexts>
<marker>Glanzer, Bowles, 1976</marker>
<rawString>M. Glanzer and N. Bowles. 1976. Analysis of the word frequency effect in recognition memory. Journal of Experimental Psychology: Human Learning and Memory, 2:21–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Grover</author>
<author>C Matheson</author>
<author>A Mikheev</author>
<author>M Moens</author>
</authors>
<title>Lt ttt - a flexible tokenisation tool. In</title>
<date>2000</date>
<booktitle>In Proceedings of Second International Conference on Language Resources and Evaluation,</booktitle>
<pages>1147--1154</pages>
<contexts>
<context position="20043" citStr="Grover et al., 2000" startWordPosition="2965" endWordPosition="2968">kRat Type-token ratio typ Indicate the range of vocabulary used in the text [UR401, UR425, UR504, UR505, UR511] tok Table 3: Text complexity formulae (w – the number of words in the text; s – the number of sentences in the text; ptyp – the number of polysemic word types in the text; c – the number of commas in the text; typ – the number of word types in the text; tok – the number of word tokens in the text) Scores for these measures, and the text complexity formulae that exploit them where obtained automatically by the tokeniser, part-of-speech tagger, and lemmatiser distributed with LT TTT2 (Grover et al., 2000). Detection of the features used to derive complexity measures also involved the use of additional resources such as WordNet, gazetteers of rare illative, comparative, and adversative conjunctions, negatives (words and prefixes) and a set of lexico-syntactic patterns used to detect passive verbs (presented in Figure 1). 57 am/are/is/was/were wRB* w{V BN|V BD} am/are/is/was/were wRB* being wRB* w{V BN|V BD} have/has/had wRB* been wRB* w{V BN|V BD} will wRB* be wRB* w{V BN|V BD} am/is/are wRB* going wRB* to wRB* be wRB* w{V BN|V BD} wMD wRB* be w{V BN|V BD} wMD wRB* have wRB* been wRB* w{V BN|V </context>
</contexts>
<marker>Grover, Matheson, Mikheev, Moens, 2000</marker>
<rawString>C. Grover, C. Matheson, A. Mikheev, and M. Moens. 2000. Lt ttt - a flexible tokenisation tool. In In Proceedings of Second International Conference on Language Resources and Evaluation, pages 1147–1154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hall</author>
<author>E Frank</author>
<author>G Holmes</author>
<author>B Pfahringer</author>
<author>P Reutemann</author>
<author>I H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<pages>11--10</pages>
<contexts>
<context position="22023" citStr="Hall et al., 2009" startWordPosition="3282" endWordPosition="3285">e Shapiro-Wilk test of normality, which is preferred over the Kolmogorov-Smirnov test when the dataset contains less than 2,000 elements. All tests were performed in SPSS. Features 1–15 were first normalised (as an average per sentence) in order to allow a fair comparison across the four TS corpora (text length in words and sentences differed significantly across different corpora). 2. Classification experiments with the aim of discriminating original from simplified texts using the nineteen selected features. All experiments were conducted using the Weka Experimenter (Witten and Frank, 2005; Hall et al., 2009) in 10-fold cross-validation setup with 10 repetitions, using four different classification algorithms: NB – NaiveBayes (John and Langley, 1995), SMO – Weka implementation of Support Vector Machines (Keerthi et al., 2001) with normalisation, JRip – a propositional rule learner (Cohen, 1995), and J48 – Weka implementation of C4.5 (Quinlan, 1993). The statistical significance of the observed differences in F-measures obtained by different algorithms was calculated using the corrected paired t-test provided in the Weka Experimenter. The TS system in FIRST is not only supposed to decide which text</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. 2009. The weka data mining software: an update. SIGKDD Explor. Newsl., 11:10–18, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Inui</author>
<author>A Fujita</author>
<author>T Takahashi</author>
<author>R Iida</author>
<author>T Iwakura</author>
</authors>
<title>Text simplification for reading assistance: a project note.</title>
<date>2003</date>
<booktitle>In Proceedings of the second international workshop on Paraphrasing - Volume 16, PARAPHRASE ’03,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2899" citStr="Inui et al., 2003" startWordPosition="417" endWordPosition="420">e writing”2, and the W3C – Web Accessibility Initiative guidelines3. However, manual adaptation of texts cannot match the speed with which new texts are published on the web in order to provide up to date information. The aim of Automatic Text Simplification (ATS) is to automatically (or at least semi-automatically) convert complex sentences into a more accessible form while preserving their original meaning. In the last twenty years, many ATS systems have been proposed for different target populations in various languages (Carroll et al., 1998; Devlin and Unthank, 2006; Saggion et al., 2011; Inui et al., 2003; Alu´ısio et al., 2008). Due to the scarcity of parallel corpora of original and manually simplified texts, most of these systems are rule-based. The emergence of Simple English Wikipedia (SEW)4, together with the existing English Wikipedia (EW)5 provided a large amount of parallel TS training data, which motivated a shift in English TS from rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has This work is licensed under a Creative Commons Attribut</context>
</contexts>
<marker>Inui, Fujita, Takahashi, Iida, Iwakura, 2003</marker>
<rawString>K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura. 2003. Text simplification for reading assistance: a project note. In Proceedings of the second international workshop on Paraphrasing - Volume 16, PARAPHRASE ’03, pages 9–16, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jastrzembski</author>
</authors>
<title>Multiple meaning, number or related meanings, frequency of occurrence and the lexicon. Cognitive Psychology,</title>
<date>1981</date>
<pages>13--278</pages>
<contexts>
<context position="31425" citStr="Jastrzembski, 1981" startWordPosition="4670" endWordPosition="4671">fied texts of the FIRST corpora (Table 6). The higher polysemic type ratio (PolyType) for simplified rather than original versions of the texts in the other three corpora was unexpected, as it is usually assumed that polysemous words can pose an obstacle for various target populations. However, it is important to bear in mind that polysemous words usually pose an obstacle when conveying one of their infrequently used meanings. Findings in cognitive psychology indicate that the words with the highest number of possible meanings are actually understood more quickly, due to their high frequency (Jastrzembski, 1981). A common lexical simplification strategy is to replace infrequent words with their more frequent synonyms, and long words with their shorter synonyms. This strategy leads to a higher polysemic type ratio (PolyType) in simplified versions of the texts as the shorter words are usually more frequent (Balota et al., 2004), and frequent words tend to be more polysemous than infrequent ones (Glanzer and Bowles, 1976). 5.2 Classification between Original and Simplified Texts Classification experiments were conducted using two different sets of features on each of the corpora: 1. all – all 18 featur</context>
</contexts>
<marker>Jastrzembski, 1981</marker>
<rawString>J. Jastrzembski. 1981. Multiple meaning, number or related meanings, frequency of occurrence and the lexicon. Cognitive Psychology, 13:278–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H John</author>
<author>P Langley</author>
</authors>
<title>Estimating Continuous Distributions in Bayesian Classifiers.</title>
<date>1995</date>
<booktitle>In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>338--345</pages>
<contexts>
<context position="22167" citStr="John and Langley, 1995" startWordPosition="3301" endWordPosition="3304">ll tests were performed in SPSS. Features 1–15 were first normalised (as an average per sentence) in order to allow a fair comparison across the four TS corpora (text length in words and sentences differed significantly across different corpora). 2. Classification experiments with the aim of discriminating original from simplified texts using the nineteen selected features. All experiments were conducted using the Weka Experimenter (Witten and Frank, 2005; Hall et al., 2009) in 10-fold cross-validation setup with 10 repetitions, using four different classification algorithms: NB – NaiveBayes (John and Langley, 1995), SMO – Weka implementation of Support Vector Machines (Keerthi et al., 2001) with normalisation, JRip – a propositional rule learner (Cohen, 1995), and J48 – Weka implementation of C4.5 (Quinlan, 1993). The statistical significance of the observed differences in F-measures obtained by different algorithms was calculated using the corrected paired t-test provided in the Weka Experimenter. The TS system in FIRST is not only supposed to decide which texts should be converted, but also to assess the readability of texts that are undergoing conversion. It is expected that people working to improve</context>
</contexts>
<marker>John, Langley, 1995</marker>
<rawString>G. H. John and P. Langley. 1995. Estimating Continuous Distributions in Bayesian Classifiers. In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, pages 338–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kauchak</author>
</authors>
<title>Improving text simplification language modeling using unsimplified text data.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1537--1546</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="16134" citStr="Kauchak, 2013" startWordPosition="2374" endWordPosition="2375"> of 20 texts from the Encyclopedia Britannica and their manually simplified versions aimed at children – Britannica Elementary (Barzilay and Elhadad, 2003)11. 3. The corpus of 110 randomly selected corresponding articles from EW and SEW. Here, it is important to note that, in general, articles from SEW do not represent direct simplifications of the articles from EW, they just have a matching topic. For this reason, we did not use complete EW and SEW articles. We only used those sentences in original and simplified versions, which existed in the sentence-aligned parallel corpora version 2.012 (Kauchak, 2013). 4. The corpus of 25 texts on various topics manually simplified for people with autism, compiled in the FIRST project13, for the purpose of a piloting task14. The texts were simplified by carers of people with ASD in accordance with specified guidelines. 10http://www.onestopenglish.com/ 11http://www.cs.columbia.edu/ noemie/alignment/ 12http://www.cs.middlebury.edu/ dkauchak/simplification/ 13www.first-asd.eu 14http://www.first-asd.eu/?q=system/files/FIRST D7.2 20130228 annex.pdf 56 4.2 Text Features Relevant to User Requirements In this paper, a set of 15 text complexity measures and 4 formu</context>
</contexts>
<marker>Kauchak, 2013</marker>
<rawString>D. Kauchak. 2013. Improving text simplification language modeling using unsimplified text data. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1537–1546, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Keerthi</author>
<author>S K Shevade</author>
<author>C Bhattacharyya</author>
<author>K R K Murthy</author>
</authors>
<date>2001</date>
<booktitle>Improvements to Platt’s SMO Algorithm for SVM Classifier Design. Neural Computation,</booktitle>
<volume>13</volume>
<issue>3</issue>
<contexts>
<context position="22244" citStr="Keerthi et al., 2001" startWordPosition="3314" endWordPosition="3317">rage per sentence) in order to allow a fair comparison across the four TS corpora (text length in words and sentences differed significantly across different corpora). 2. Classification experiments with the aim of discriminating original from simplified texts using the nineteen selected features. All experiments were conducted using the Weka Experimenter (Witten and Frank, 2005; Hall et al., 2009) in 10-fold cross-validation setup with 10 repetitions, using four different classification algorithms: NB – NaiveBayes (John and Langley, 1995), SMO – Weka implementation of Support Vector Machines (Keerthi et al., 2001) with normalisation, JRip – a propositional rule learner (Cohen, 1995), and J48 – Weka implementation of C4.5 (Quinlan, 1993). The statistical significance of the observed differences in F-measures obtained by different algorithms was calculated using the corrected paired t-test provided in the Weka Experimenter. The TS system in FIRST is not only supposed to decide which texts should be converted, but also to assess the readability of texts that are undergoing conversion. It is expected that people working to improve the accessibility of a given text will benefit from relevant feedback concer</context>
</contexts>
<marker>Keerthi, Shevade, Bhattacharyya, Murthy, 2001</marker>
<rawString>S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and K. R. K. Murthy. 2001. Improvements to Platt’s SMO Algorithm for SVM Classifier Design. Neural Computation, 13(3):637–649.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S T Kover</author>
<author>E Haebig</author>
<author>A Oakes</author>
<author>A McDuffie</author>
<author>R J Hagerman</author>
<author>L Abbeduto</author>
</authors>
<title>Syntactic comprehension in boys with autism spectrum disorders: Evidence from specific constructions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 International Meeting for Autism Research,</booktitle>
<institution>Greece. International Society for Autism Research.</institution>
<location>Athens,</location>
<contexts>
<context position="6826" citStr="Kover et al., 2012" startWordPosition="982" endWordPosition="985">rders (ASD) are neurodevelopmental disorders characterised by qualitative impairment in communication and stereotyped repetitive behaviour. People with ASD show a diverse range of reading abilities: 5-10% have the capacity to read words from an early age without the need for formal learning (hyperlexia) but many demonstrate reduced comprehension of what has been read (Volkmar and Wiesner, 2009). They may have difficulty inferring contextual information or may have trouble understanding mental verbs, emotional language, and long sentences with complex syntactic structure (Tager-Flusberg, 1981; Kover et al., 2012). To address these difficulties, a tool is being developed in the FIRST project8 to assist in the process of making texts more accessible for people with ASD. To achieve this, three modues are exploited: 1. Structural complexity processor, which detects syntactically complex sentences and generates alternatives to such sentences in the form of sequences of shorter sentences (Evans et al., 2014; Dornescu et al., 2013). 2. Meaning disambiguator, which resolves pronominal references, performs word sense disambiguation, and detects lexicalised (conventional) metaphors (Barbu et al., 2013). 3. Pers</context>
</contexts>
<marker>Kover, Haebig, Oakes, McDuffie, Hagerman, Abbeduto, 2012</marker>
<rawString>S. T. Kover, E. Haebig, A. Oakes, A. McDuffie, R. J. Hagerman, and L. Abbeduto. 2012. Syntactic comprehension in boys with autism spectrum disorders: Evidence from specific constructions. In Proceedings of the 2012 International Meeting for Autism Research, Athens, Greece. International Society for Autism Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Martos</author>
<author>S Freire</author>
<author>A Gonz´alez</author>
<author>D Gil</author>
<author>R Evans</author>
<author>V Jordanova</author>
<author>A Cerga</author>
<author>A Shishkova</author>
<author>C Orasan</author>
</authors>
<title>FIRST Deliverable - User preferences: Updated.</title>
<date>2013</date>
<tech>Technical Report D2.2,</tech>
<location>Deletrea, Madrid,</location>
<marker>Martos, Freire, Gonz´alez, Gil, Evans, Jordanova, Cerga, Shishkova, Orasan, 2013</marker>
<rawString>J. Martos, S. Freire, A. Gonz´alez, D. Gil, R. Evans, V. Jordanova, A. Cerga, A. Shishkova, and C. Orasan. 2013. FIRST Deliverable - User preferences: Updated. Technical Report D2.2, Deletrea, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Napoles</author>
<author>M Dredze</author>
</authors>
<title>Learning simple wikipedia: a cogitation in ascertaining abecedarian language.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing: Writing Processes and Authoring Aids, CL&amp;W ’10,</booktitle>
<pages>42--50</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11281" citStr="Napoles and Dredze (2010)" startWordPosition="1617" endWordPosition="1620">lity (Drndarevi´c and Saggion, 2012); for language learners (Petersen and Ostendorf, 2007); and for people with low literacy (Gasperin et al., 2009). Unfortunately, those studies are not directly comparable (neither among themselves nor with our study), either because they focus on different types of transformations (the study of Bautista et al. (2011) focuses on general transformations while the other three studies focus on sentence transformations), or because they treat different languages (Spanish, English, and Brazilian Portuguese). Two previous studies most relevant to ours are those by Napoles and Dredze (2010), and by ˇStajner et al. (2013). Napoles and Dredze (2010) built a statistical classification system that discriminates simple English from ordinary English, based on EW–SEW corpus. They used four different groups of features: lexical, part-of-speech, surface, and syntactic parse features. The accuracy of the best classifier (SVM) on the document classification task when using all features was 99.90%, while the accuracy of the best classifier (maximum entropy) on the sentence classification task when using all features was 80.80%. However, this study only demonstrated that it is fairly easy to</context>
</contexts>
<marker>Napoles, Dredze, 2010</marker>
<rawString>C. Napoles and M. Dredze. 2010. Learning simple wikipedia: a cogitation in ascertaining abecedarian language. In Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing: Writing Processes and Authoring Aids, CL&amp;W ’10, pages 42–50, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Petersen</author>
<author>M Ostendorf</author>
</authors>
<title>Text Simplification for Language Learners: A Corpus Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of Workshop on Speech and Language Technology for</booktitle>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="10746" citStr="Petersen and Ostendorf, 2007" startWordPosition="1539" endWordPosition="1542">ese items. For this reason, changes in the prevalence of these items in original and converted versions of texts are not captured in this study. The full set of user requirements is detailed in Martos et al. (2013). More generally, it is infrequent linguistic phenomena that cause the greatest difficulty. 3 Related Work There have been several studies analysing the existing TS corpora. However, their main focus was on determining necessary transformations in TS: for children (Bautista et al., 2011); for people with intellectual disability (Drndarevi´c and Saggion, 2012); for language learners (Petersen and Ostendorf, 2007); and for people with low literacy (Gasperin et al., 2009). Unfortunately, those studies are not directly comparable (neither among themselves nor with our study), either because they focus on different types of transformations (the study of Bautista et al. (2011) focuses on general transformations while the other three studies focus on sentence transformations), or because they treat different languages (Spanish, English, and Brazilian Portuguese). Two previous studies most relevant to ours are those by Napoles and Dredze (2010), and by ˇStajner et al. (2013). Napoles and Dredze (2010) built </context>
</contexts>
<marker>Petersen, Ostendorf, 2007</marker>
<rawString>S. E. Petersen and M. Ostendorf. 2007. Text Simplification for Language Learners: A Corpus Analysis. In Proceedings of Workshop on Speech and Language Technology for Education. R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>E G´omez Martinez</author>
<author>E Etayo</author>
<author>A Anula</author>
<author>L Bourg</author>
</authors>
<title>Text Simplification in Simplext: Making Text More Accessible.</title>
<date>2011</date>
<journal>Applied Psycholinguistics,</journal>
<booktitle>Revista de la Sociedad Espa˜nola para el Procesamiento del Lenguaje Natural, 47:341–342. H. Tager-Flusberg.</booktitle>
<pages>2--1</pages>
<contexts>
<context position="2880" citStr="Saggion et al., 2011" startWordPosition="413" endWordPosition="416">idelines for accessible writing”2, and the W3C – Web Accessibility Initiative guidelines3. However, manual adaptation of texts cannot match the speed with which new texts are published on the web in order to provide up to date information. The aim of Automatic Text Simplification (ATS) is to automatically (or at least semi-automatically) convert complex sentences into a more accessible form while preserving their original meaning. In the last twenty years, many ATS systems have been proposed for different target populations in various languages (Carroll et al., 1998; Devlin and Unthank, 2006; Saggion et al., 2011; Inui et al., 2003; Alu´ısio et al., 2008). Due to the scarcity of parallel corpora of original and manually simplified texts, most of these systems are rule-based. The emergence of Simple English Wikipedia (SEW)4, together with the existing English Wikipedia (EW)5 provided a large amount of parallel TS training data, which motivated a shift in English TS from rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has This work is licensed under a Creati</context>
</contexts>
<marker>Saggion, Martinez, Etayo, Anula, Bourg, 2011</marker>
<rawString>H. Saggion, E. G´omez Martinez, E. Etayo, A. Anula, and L. Bourg. 2011. Text Simplification in Simplext: Making Text More Accessible. Revista de la Sociedad Espa˜nola para el Procesamiento del Lenguaje Natural, 47:341–342. H. Tager-Flusberg. 1981. Sentence comprehension in autistic children. Applied Psycholinguistics, 2:1:5–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tattamanti</author>
<author>R Manenti</author>
<author>P A Della Rosa</author>
<author>A Falini</author>
<author>D Perani</author>
<author>S Cappa</author>
<author>A Moro</author>
</authors>
<title>Negation in the brain: Modulating action representations.</title>
<date>2008</date>
<journal>NeuroImage,</journal>
<volume>43</volume>
<pages>2008--358</pages>
<marker>Tattamanti, Manenti, Rosa, Falini, Perani, Cappa, Moro, 2008</marker>
<rawString>M. Tattamanti, R. Manenti, P. A. Della Rosa, A. Falini, D. Perani, S. Cappa, and A. Moro. 2008. Negation in the brain: Modulating action representations. NeuroImage, 43 (2008):358–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>UN</author>
</authors>
<title>Convention on the rigths of persons with disabilities.</title>
<date>2006</date>
<publisher>Wiley,</publisher>
<location>Hoboken, NJ,</location>
<note>2nd edition.</note>
<contexts>
<context position="1652" citStr="UN, 2006" startWordPosition="234" endWordPosition="235">acles to reading comprehension for people with ASD. The results indicate that the Britannica TS parallel corpus (aimed at young readers) and the Weekly Reader TS parallel corpus (aimed at second language learners) may be suitable for training a TS system to assist people with ASD. Two sets of classification experiments intended to discriminate between original and simplified texts according to the nineteen features lent further support for those findings. 1 Introduction As a fundamental human right, people with reading and comprehension difficulties are entitled to access written information (UN, 2006). This entitlement enables better inclusion into society. However, the vast majority of texts that such people encounter in their everyday life – especially newswire texts – are lexically and syntactically very complex. Since the late nineties, several initiatives have emerged which propose guidelines for producing plain, easy-to-read and more accessible documents. These include the “Federal Plain Language Guidelines”1, “Make it Simple, European Guidelines for the Production of Easy-to-Read Information for people with Learning Disability” (Freyhoff et al., 1998), “Am I making myself clear? Men</context>
</contexts>
<marker>UN, 2006</marker>
<rawString>UN. 2006. Convention on the rigths of persons with disabilities. F. R. Volkmar and L. Wiesner. 2009. A Practical Guide to Autism. Wiley, Hoboken, NJ, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S ˇStajner</author>
<author>H Saggion</author>
</authors>
<title>Adapting Text Simplification Decisions to Different Text Genres and Target Users.</title>
<date>2013</date>
<booktitle>Procesamiento del Lenguaje Natural,</booktitle>
<pages>51--135</pages>
<marker>ˇStajner, Saggion, 2013</marker>
<rawString>S. ˇStajner and H. Saggion. 2013. Adapting Text Simplification Decisions to Different Text Genres and Target Users. Procesamiento del Lenguaje Natural, 51:135–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S ˇStajner</author>
<author>B Drndarevi´c</author>
<author>H Saggion</author>
</authors>
<title>Corpus-based Sentence Deletion and Split Decisions for Spanish Text Simplification. Computaci´on y Systemas,</title>
<date>2013</date>
<marker>ˇStajner, Drndarevi´c, Saggion, 2013</marker>
<rawString>S. ˇStajner, B. Drndarevi´c, and H. Saggion. 2013. Corpus-based Sentence Deletion and Split Decisions for Spanish Text Simplification. Computaci´on y Systemas, 17(2):251–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Data mining: practical machine learning tools and techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="22003" citStr="Witten and Frank, 2005" startWordPosition="3278" endWordPosition="3281">data was tested using the Shapiro-Wilk test of normality, which is preferred over the Kolmogorov-Smirnov test when the dataset contains less than 2,000 elements. All tests were performed in SPSS. Features 1–15 were first normalised (as an average per sentence) in order to allow a fair comparison across the four TS corpora (text length in words and sentences differed significantly across different corpora). 2. Classification experiments with the aim of discriminating original from simplified texts using the nineteen selected features. All experiments were conducted using the Weka Experimenter (Witten and Frank, 2005; Hall et al., 2009) in 10-fold cross-validation setup with 10 repetitions, using four different classification algorithms: NB – NaiveBayes (John and Langley, 1995), SMO – Weka implementation of Support Vector Machines (Keerthi et al., 2001) with normalisation, JRip – a propositional rule learner (Cohen, 1995), and J48 – Weka implementation of C4.5 (Quinlan, 1993). The statistical significance of the observed differences in F-measures obtained by different algorithms was calculated using the corrected paired t-test provided in the Weka Experimenter. The TS system in FIRST is not only supposed </context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>I. H. Witten and E. Frank. 2005. Data mining: practical machine learning tools and techniques. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Woodsend</author>
<author>M Lapata</author>
</authors>
<title>Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="3349" citStr="Woodsend and Lapata, 2011" startWordPosition="487" endWordPosition="490">y ATS systems have been proposed for different target populations in various languages (Carroll et al., 1998; Devlin and Unthank, 2006; Saggion et al., 2011; Inui et al., 2003; Alu´ısio et al., 2008). Due to the scarcity of parallel corpora of original and manually simplified texts, most of these systems are rule-based. The emergence of Simple English Wikipedia (SEW)4, together with the existing English Wikipedia (EW)5 provided a large amount of parallel TS training data, which motivated a shift in English TS from rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://www.plainlanguage.gov/howto/guidelines/bigdoc/fullbigdoc.pdf 2http://www.easy-read-online.co.uk/media/10609/making-myself-clear.pdf 3http://www.w3.org/WAI/ 4http://simple.wikipedia.org/wiki/Main Page 5http://wikipedia.org/wiki/Main Page 53 Proceedings of the Workshop on Automatic Text</context>
</contexts>
<marker>Woodsend, Lapata, 2011</marker>
<rawString>K. Woodsend and M. Lapata. 2011. Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wubben</author>
<author>A van den Bosch</author>
<author>E Krahmer</author>
</authors>
<title>Sentence simplification by monolingual machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>1015--1024</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Wubben, van den Bosch, Krahmer, 2012</marker>
<rawString>S. Wubben, A. van den Bosch, and E. Krahmer. 2012. Sentence simplification by monolingual machine translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 1015–1024, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yatskar</author>
<author>B Pang</author>
<author>C Danescu-Niculescu-Mizil</author>
<author>L Lee</author>
</authors>
<title>For the sake of simplicity: unsupervised extraction of lexical simplifications from wikipedia.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>365--368</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3302" citStr="Yatskar et al., 2010" startWordPosition="479" endWordPosition="482">nal meaning. In the last twenty years, many ATS systems have been proposed for different target populations in various languages (Carroll et al., 1998; Devlin and Unthank, 2006; Saggion et al., 2011; Inui et al., 2003; Alu´ısio et al., 2008). Due to the scarcity of parallel corpora of original and manually simplified texts, most of these systems are rule-based. The emergence of Simple English Wikipedia (SEW)4, together with the existing English Wikipedia (EW)5 provided a large amount of parallel TS training data, which motivated a shift in English TS from rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://www.plainlanguage.gov/howto/guidelines/bigdoc/fullbigdoc.pdf 2http://www.easy-read-online.co.uk/media/10609/making-myself-clear.pdf 3http://www.w3.org/WAI/ 4http://simple.wikipedia.org/wiki/Main Page 5http://wikipedia.org/wiki/Main Page 5</context>
</contexts>
<marker>Yatskar, Pang, Danescu-Niculescu-Mizil, Lee, 2010</marker>
<rawString>M. Yatskar, B. Pang, C. Danescu-Niculescu-Mizil, and L. Lee. 2010. For the sake of simplicity: unsupervised extraction of lexical simplifications from wikipedia. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 365–368, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhu</author>
<author>D Berndard</author>
<author>I Gurevych</author>
</authors>
<title>A Monolingual Tree-based Translation Model for Sentence Simplification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1353--1361</pages>
<contexts>
<context position="3415" citStr="Zhu et al., 2010" startWordPosition="499" endWordPosition="502">us languages (Carroll et al., 1998; Devlin and Unthank, 2006; Saggion et al., 2011; Inui et al., 2003; Alu´ısio et al., 2008). Due to the scarcity of parallel corpora of original and manually simplified texts, most of these systems are rule-based. The emergence of Simple English Wikipedia (SEW)4, together with the existing English Wikipedia (EW)5 provided a large amount of parallel TS training data, which motivated a shift in English TS from rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://www.plainlanguage.gov/howto/guidelines/bigdoc/fullbigdoc.pdf 2http://www.easy-read-online.co.uk/media/10609/making-myself-clear.pdf 3http://www.w3.org/WAI/ 4http://simple.wikipedia.org/wiki/Main Page 5http://wikipedia.org/wiki/Main Page 53 Proceedings of the Workshop on Automatic Text Simplification: Methods and Applications in the Multilingual Soci</context>
</contexts>
<marker>Zhu, Berndard, Gurevych, 2010</marker>
<rawString>Z. Zhu, D. Berndard, and I. Gurevych. 2010. A Monolingual Tree-based Translation Model for Sentence Simplification. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353–1361.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
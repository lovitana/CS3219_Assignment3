<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000519">
<title confidence="0.981255">
Predicting MOOC Dropout over Weeks Using Machine Learning Methods
</title>
<author confidence="0.999546">
Marius Kloft, Felix Stiehler, Zhilin Zheng, Niels Pinkwart
</author>
<affiliation confidence="0.999815">
Department of Computer Science
Humboldt University of Berlin
</affiliation>
<address confidence="0.607219">
Berlin, Germany
</address>
<email confidence="0.585766">
{kloft, felix.stiehler, zhilin.zheng, pinkwart}@hu-berlin.de
</email>
<sectionHeader confidence="0.979137" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999959722222223">
With high dropout rates as observed in
many current larger-scale online courses,
mechanisms that are able to predict stu-
dent dropout become increasingly impor-
tant. While this problem is partially solved
for students that are active in online fo-
rums, this is not yet the case for the more
general student population. In this pa-
per, we present an approach that works on
click-stream data. Among other features,
the machine learning algorithm takes the
weekly history of student data into ac-
count and thus is able to notice changes
in student behavior over time. In the later
phases of a course (i.e., once such his-
tory data is available), this approach is able
to predict dropout significantly better than
baseline methods.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968547169812">
In the past few years, with their dramatically in-
creasing popularity, Massive Open Online Courses
(MOOCs) have become a way of online learning
used across the world by millions of people. As
a result of efforts conducted (sometimes jointly)
by academia and industry, many MOOC providers
(such as Coursera, Udacity, Edx, or iversity) have
emerged, which are able to deliver well-designed
online courses to learners. In typical MOOC plat-
forms, learners can not only access lecture videos,
assignments and examinations, but can also use
collaborative learning features such as online dis-
cussion forums. Despite all the MOOC features
and benefits, however, one of the critical issues re-
lated to MOOCs is their high dropout rate, which
puts the efficacy of the learning technology into
question. According to the online data provided
by Jordan (2014), most MOOCs have comple-
tion rates of less than 13%. While discussions
are still ongoing as to whether these numbers are
actually a problem indicating partial MOOC fail-
ures or whether they merely indicate that the com-
munity of MOOC learners is diverse and by far
not every participant intends to complete a course,
researchers and MOOC providers are certainly
interested in methods for increasing completion
rates. The analysis of MOOC data can be of help
here. For instance, a linguistic analysis of the
MOOC forum data can discover valuable indica-
tors for predicting dropout of students (Wen et
al., 2014). However, only few MOOC students
(roughly 5-10%) use the discussion forums (Rose
and Siemens, 2014), so that dropout predictors for
the remaining 90% would be desirable. In order
to get insights into the learning behaviors of this
majority of participants, the clickstream data of
the MOOC platform usage is the primary source
for analysis in addition to the forum data. That is
also the motivation of the shared task proposed by
the MOOC workshop at the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP 2014) (Rose and Siemens, 2014). Ad-
dressing this task, we propose a machine learning
method based on support vector machines for pre-
dicting dropout between MOOC course weeks in
this paper.
The rest of this paper is organized as follows.
We begin with the description of the data set and
features extracted from the data set. We then de-
scribe our prediction model. Next, the prediction
results and some experimental findings are pre-
sented. Finally, we conclude our work in this pa-
per.
</bodyText>
<sectionHeader confidence="0.992818" genericHeader="introduction">
2 Dataset
</sectionHeader>
<bodyText confidence="0.9004652">
The dataset we used in this paper was prepared for
the shared task launched by the Modeling Large
Scale Social Interaction in Massively Open On-
line Courses Workshop at the Conference on Em-
pirical Methods in Natural Language Processing
</bodyText>
<page confidence="0.972602">
60
</page>
<bodyText confidence="0.9580745">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 60–65,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
(EMNLP 2014) (Rose and Siemens, 2014). The
data was collected from a psychology MOOC
course which was launched in March 2013. The
whole course lasted for 12 weeks with 11,607 par-
ticipants in the beginning week and 3,861 partici-
pants staying until the last course week. Overall,
20,828 students participated, with approximately
81.4% lost at last. Note that the data cover the
whole life cycle of this online course up to 19
weeks. The original dataset for this task had two
types of data: clickstream data and forum data. In
this paper, we only make use of clickstream data
to train our prediction model and we do not further
consider forum data. Obviously, this will lower
the prediction quality for the 5% of students that
use the forum, but it will hopefully shed light on
the utility of the clickstream data for the larger set
of all participants. The clickstream data includes
3,475,485 web log records which can be gener-
ally classified into two types: the page view log
and the lecture video log. In the following sec-
tion, we will describe attributes extracted from the
raw clickstream data which (we believed) could be
correlated to drop-out over the 12 course weeks.
</bodyText>
<subsectionHeader confidence="0.996212">
2.1 Attributes description
</subsectionHeader>
<bodyText confidence="0.999996363636364">
Our model is an attempt to predict the participants’
drop-out during the next week (defined as no ac-
tivity in that week and in any future week) using
the data of the current and past weeks. Conse-
quently, all attributes are computed for each par-
ticipant and for each week. Note that this results
in having more data for later course weeks, since
the approach allows for comparing a student’s cur-
rent activity with the activity of that student in the
past weeks. The complete attributes list is shown
in Table 1.
</bodyText>
<subsectionHeader confidence="0.999766">
2.2 Attribute Generation
</subsectionHeader>
<bodyText confidence="0.986627153846154">
The attributes required for the predictions are ex-
tracted by parsing the clickstream file where each
line represents a web request. For each line
the corresponding Coursera ID is taken from the
database containing the forum data and the course
week is calculated from the timestamp relative to
the start date of the course. Then the request is
analysed regarding its type and every present at-
tribute is saved.
After collecting the raw attributes, the data
needs to be post-processed. There are 3 kinds of
attributes: attributes that need to be summed up,
attributes that need to be averaged and attributes
</bodyText>
<figureCaption confidence="0.920818">
Figure 1: Several basic properties of the analyzed
data set.
</figureCaption>
<bodyText confidence="0.998716333333333">
that need to be decided by majority vote. After
the post-processing the data consists of lists of at-
tributes each correlated to a unique tuple consist-
ing of the Coursera ID and the course week num-
ber. Invalid attributes are getting replaced with
the median of that week. Note that every missing
week is getting replaced by the median of the at-
tributes of active users in that week that were also
active in the original week.
</bodyText>
<subsectionHeader confidence="0.999033">
2.3 A First Glance on the Data Set
</subsectionHeader>
<bodyText confidence="0.9999905">
We have visualized several basic properties of the
data in Figure 1. We observe that the number of
active user quickly decreases over time. Further-
more the dropout probability is especially high in
the first two weeks, and then of course at the end
of the course starting around week 11 and 12.
</bodyText>
<sectionHeader confidence="0.996464" genericHeader="method">
3 Methodology &amp; Results
</sectionHeader>
<bodyText confidence="0.9999655">
In this section we concisely describe the employed
feature extraction and selection pipeline, as well
as the employed machine learning algorithms. For
each week of the course (i = 1, ... ,19) we com-
puted the dropout label of each of the nz partici-
pants (user ids) being active in that week, based
on checking whether there is any activity associ-
ated to the same user id in proceeding next week.
</bodyText>
<page confidence="0.986202">
61
</page>
<figure confidence="0.733155">
Attributes
ID
</figure>
<bodyText confidence="0.994017093023256">
Number of requests: total number of requests including page views and video click actions
Number of sessions: number of sessions is supposed to be a reflection of high engagement,
because more sessions indicate more often logging into the learning platform
Number of active days: we define a day as an active day if the student had at least one
session on that day
Number of page views: the page views include lecture pages, wiki pages, homework pages
and forum pages
Number of page views per session: the average number of pages viewed by each partici-
pant per session
Number of video views: total number of video click actions
Number of video views per session: average number of video click actions per session
Number of forum views: number of course discussion forum views
Number of wiki views: number of course wiki page views
Number of homework page views
Number of straight-through video plays: this is a video action attribute. Straight-trough
playing video means that the participates played video without any jump (e.g. pause, re-
sume, jump backward and jump forward). Since the lecture videos are the most important
learning resource for the learning participants, the video playing should be investigated as
other researchers did (Brotherton and Abowd, 2004). In this paper, five video behaviors
are taken into account including the number of full plays as well as four others: start-stop
during video plays, skip-ahead during video plays, relisten during video plays and the use
of low play rate
Number of start-stop during video plays: start-stop during video plays stands for a lecture
video being paused and resumed
Number of skip-ahead during video plays: skip-ahead means that the participant played
a video with a forward jump
Number of relisten during video plays: relisten means that a backward jump was made
as the participant was playing a video
Number of slow play rate use: this attribute is considered as an indicator of weak under-
standing of the lecturer’s lecture presentation, possibly because of language difficulties or a
lack of relevant background knowledge
Most common request time: our attempt with this attribute is to separate day time learning
from night time learning. We define night time from 19:00 to 6:59 in the morning and the
other half day as day time
Number of requests from outside of Coursera: this is to discover how many requests
from third-party tools (such as e-mail clients and social networks) to the course were made,
which could be an indicator of the participant’s social behavior
Number of screen pixels: the screen pixels is an indicator of the device that the student
used. Typically, mobile devices come with fewer pixels
Most active day: through this attribute, we can investigate if starting late or early could
have an impact on dropout
Country: this information could reflect geographical differences in learning across the
world
</bodyText>
<sectionHeader confidence="0.423355" genericHeader="method">
Operating System
Browser
</sectionHeader>
<tableCaption confidence="0.995275">
Table 1: Attributes list.
</tableCaption>
<figure confidence="0.99264805">
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</figure>
<page confidence="0.798994">
21
22
</page>
<bodyText confidence="0.99565275">
This resulted in label vectors yi E {−1,1}ni for
i = 1,... ,19, where +1 indicates dropout (and
thus −1 indicates no dropout). We experimented
on the 22 numerical features described in the pre-
</bodyText>
<page confidence="0.99827">
62
</page>
<bodyText confidence="0.9685474">
vious section. The features with ids 1–19 could be
represented a single real number, while all other
features had to be embedded into a multidimen-
sional space. For simplicity we thus first focused
on features 1–19. For each week i of the course,
this results in a matrix Xpreliminary E R19xni,
i
the rows and columns of which correspond to the
features and user ids, respectively. We then en-
riched the matrices by considering also the “his-
tory” of the features, that is, for the data of week
i, all the features of the previous weeks were ap-
pended (as additional rows) to the actual data ma-
trix, resulting in Xi E R19ixni. We can write
this as Xi = (x1, ... , xni), where xj is the fea-
ture vector of the jth user. Box plots of these fea-
tures showed that the distribution is highly skewed
and non-normal, and furthermore all features are
non-negative. We thus tried two standard features
transformations: 1. logarithmic transformation 2.
box-cox transformation. Subsequent box plots in-
dicated that both lead to fairly non-skewed distri-
butions. The logarithmic transformation is how-
ever much faster and lead to better results in later
pipeline steps, which is why it was taken for the
remaining experiments.
Subsequently, all features were centered and
normalized to unit standard deviation. We then
performed simple t-tests for each feature and com-
puted also the Fisher score fj = 1 /µ2 −IL , where
</bodyText>
<equation confidence="0.512631">
V a++σ−
</equation>
<bodyText confidence="0.999428875">
µ± and σ2± are the mean and variance of the pos-
itive (dropout) and negative class, respectively.
Both t-tests and Fisher scores lead to comparable
results; however, we have made superior experi-
ences with the Fisher score, which is why we focus
on this approach in the following methodology.
We found that the video features (id 11–15), the
most common request time (id 17), and the most
active day feature (id 19) consistently achieved
scores very close to zero, which is why they were
discarded. The remaining features are shown in
Figure 2 (a similar plot was generated using t-tests
and found to be consistent with the Fisher scores,
but is omitted due to space constraints). The re-
sults indicate that features related to a more bal-
anced behaviour pattern over the course of a week
(especially the number of sessions and number of
active days) were (weakly) predictive of dropout
in the beginning of the course. From week 6 to
12 we could also measure a rising importance of
the number of wiki page views (id 9) and home-
work submission page views (id 10). Past week 12
features related to activity in a more general way
like the number of requests (id 1) or the number of
page views (id 4) became the most predicative.
We proceeded with an exploratory analysis,
where we performed a principal component anal-
ysis (PCA) for each week, the result is shown in
Figure 3. The plot indicates that the users that have
dropped out can be better separated from the users
that did not drop out when the week id increases.
To follow up on this we trained, for each week,
a linear support vector machine (SVM) (Cortes
and Vapnik, 1995) using the -s 2 option in LI-
BLINEAR (Fan et al., 2008), which is one of the
fastest solvers to train linear SVMs (Fan et al.,
2008). The SVM computes an affine-linear pre-
diction function f(x) := (w, x)+b, based on max-
imizing the (soft) margin between positive and
negative examples: (w, b) := argminw,b 2||w||2 +
</bodyText>
<equation confidence="0.5389325">
1
C Eni=1 max(0, 1− yi((w, xi) + b). Note that this
</equation>
<bodyText confidence="0.999872333333333">
is very similar to regularized logistic regression,
whichuses theterm 1/(1+exp(−yi((w, xi)+b)))
instead of max(0,1− yi((w, xi) + b), but with ad-
ditional sparsity properties (only a subset of data
points are active in the final solution) that make
it more robust to outliers. The prediction accuracy
was estimated via 5-fold cross validation. The reg-
ularization parameter was found to have little in-
fluence on the prediction accuracy, which is why it
was set to the default value C = 1. We compared
our SVM to the trivial baseline of a classifier that
constantly predicts either -1 or 1; if the dropout
probability in week i is denoted by pi, then the
classification accuracy of such a classifier is given
by acctrivial := max(pi, 1 −pi). The result of this
experiment is shown in Figure 4. Note that we
found it beneficial to use the “history” features,
that is the information about the previous weeks
only within the weeks 1–12. For the weeks 13–19
we switched the history features off (also the PCA
above is computed without the history features).
We observe from the figure that for weeks 1–8 we
can not predict the dropout well, while then the
prediction accuracy steadily increases. Our hy-
pothesis here is that this could result from the more
and more history features being available for the
later weeks.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.998651333333333">
We proposed a machine learning framework for
the prediction of dropout in Massive Open On-
line Courses solely from clickstream data. At the
</bodyText>
<page confidence="0.998789">
63
</page>
<figureCaption confidence="0.955138">
Figure 4: SVM classification accuracies per week.
</figureCaption>
<bodyText confidence="0.9344504">
The baseline accuracy is computed as max(pi,1−
pi), where pi denotes the weekwise dropout prob-
ability.
heart of our approach lies the extraction of numer-
ical features capturing the activity level of users
(e.g., number of requests) as well technical fea-
tures (e.g., number of screen pixels in the em-
ployed device/computer). We detected significant
signals in the data and achieved an increase in pre-
diction accuracy up to 15% for some weeks of the
course. We found the prediction is better at the end
of the course, while at the beginning we still detect
rather weak signals. While this paper focuses on
clickstream data, the approach could in principle
also combined with forum data (e.g., using mul-
tiple kernel learning (Kloft et al., 2011)), which
we would like to tackle in future work. Further-
more, another interesting direction is to explore
non-scalar features (e.g., country, OS, browser,
etc.) and non-linear support vector machines.
</bodyText>
<sectionHeader confidence="0.99928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99988162962963">
Katy Jordan. MOOC Completion Rates: The Data.
Availabe at:
http://www.katyjordan.com/MOOCproject.html.
[Accessed: 27/08/2014].
Miaomiao Wen, Diyi Yang and Carolyn P. Rose. Lin-
guistic Reflections of Student Engagement in Mas-
sive Open Online Courses. ICWSM’14, 2014.
Carolyn Rose and George Siemens. Shared Task on
Prediction of Dropout Over Time in Massively Open
Online Courses. Proceedings of the 2014 Empirical
Methods in Natural Language Processing Workshop
on Modeling Large Scale Social Interaction in Mas-
sively Open Online Courses, Qatar, October 2014.
Jason A. Brotherton and Gregory D. Abowd. Lessons
learned from eClass: Assessing automated capture
and access in the classroom. ACM Transactions
on Computer-Human Interaction, Vol. 11, No. 2, pp.
121–155, June 2004.
C. Cortes and V. Vapnik. Support-vector networks.
Machine learning, 20(3):273–297, 1995.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. LIBLINEAR: A library for large linear
classification. Journal of Machine Learning Re-
search (JMLR), 9:1871–1874, 2008.
M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien. tp-
norm multiple kernel learning. Journal of Machine
Learning Research, 12:953–997, Mar 2011.
</reference>
<page confidence="0.999248">
64
</page>
<figureCaption confidence="0.977325">
Figure 2: Fisher scores indicate which features are predictive of the dropout. Features are ordered from
left to right with increasing ids; i.e., pink indicates the number of requests (feature id 1), cyan the number
of sessions (feature id 2), etc. In particular, we observe that features related to a more balanced behaviour
pattern such as the number of active days (feature id 3) are the most important ones in the first couple of
weeks while more general features like the number of requests rise in importance past week 12.
Figure 3: Result of principal component analysis. The data becomes more non-isotropic within the later
weeks (from week 13), and can also be separated better.
</figureCaption>
<page confidence="0.998817">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.820143">
<title confidence="0.999982">Predicting MOOC Dropout over Weeks Using Machine Learning Methods</title>
<author confidence="0.999587">Marius Kloft</author>
<author confidence="0.999587">Felix Stiehler</author>
<author confidence="0.999587">Zhilin Zheng</author>
<author confidence="0.999587">Niels</author>
<affiliation confidence="0.949276">Department of Computer Humboldt University of Berlin,</affiliation>
<email confidence="0.97874">felix.stiehler,zhilin.zheng,</email>
<abstract confidence="0.999529894736842">With high dropout rates as observed in many current larger-scale online courses, mechanisms that are able to predict student dropout become increasingly important. While this problem is partially solved for students that are active in online forums, this is not yet the case for the more general student population. In this paper, we present an approach that works on click-stream data. Among other features, the machine learning algorithm takes the weekly history of student data into account and thus is able to notice changes in student behavior over time. In the later phases of a course (i.e., once such history data is available), this approach is able to predict dropout significantly better than baseline methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Katy Jordan</author>
</authors>
<title>MOOC Completion Rates: The Data. Availabe at: http://www.katyjordan.com/MOOCproject.html.</title>
<pages>27--08</pages>
<location>Accessed:</location>
<marker>Jordan, </marker>
<rawString>Katy Jordan. MOOC Completion Rates: The Data. Availabe at: http://www.katyjordan.com/MOOCproject.html. [Accessed: 27/08/2014].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miaomiao Wen</author>
<author>Diyi Yang</author>
<author>Carolyn P Rose</author>
</authors>
<date>2014</date>
<booktitle>Linguistic Reflections of Student Engagement in Massive Open Online Courses. ICWSM’14,</booktitle>
<contexts>
<context position="2461" citStr="Wen et al., 2014" startWordPosition="386" endWordPosition="389">dan (2014), most MOOCs have completion rates of less than 13%. While discussions are still ongoing as to whether these numbers are actually a problem indicating partial MOOC failures or whether they merely indicate that the community of MOOC learners is diverse and by far not every participant intends to complete a course, researchers and MOOC providers are certainly interested in methods for increasing completion rates. The analysis of MOOC data can be of help here. For instance, a linguistic analysis of the MOOC forum data can discover valuable indicators for predicting dropout of students (Wen et al., 2014). However, only few MOOC students (roughly 5-10%) use the discussion forums (Rose and Siemens, 2014), so that dropout predictors for the remaining 90% would be desirable. In order to get insights into the learning behaviors of this majority of participants, the clickstream data of the MOOC platform usage is the primary source for analysis in addition to the forum data. That is also the motivation of the shared task proposed by the MOOC workshop at the Conference on Empirical Methods in Natural Language Processing (EMNLP 2014) (Rose and Siemens, 2014). Addressing this task, we propose a machine</context>
</contexts>
<marker>Wen, Yang, Rose, 2014</marker>
<rawString>Miaomiao Wen, Diyi Yang and Carolyn P. Rose. Linguistic Reflections of Student Engagement in Massive Open Online Courses. ICWSM’14, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn Rose</author>
<author>George Siemens</author>
</authors>
<title>Shared Task on Prediction of Dropout Over Time in Massively Open Online Courses.</title>
<date>2014</date>
<booktitle>Proceedings of the 2014 Empirical Methods in Natural Language Processing Workshop on Modeling Large Scale Social Interaction in Massively Open Online Courses,</booktitle>
<location>Qatar,</location>
<contexts>
<context position="2561" citStr="Rose and Siemens, 2014" startWordPosition="401" endWordPosition="404">oing as to whether these numbers are actually a problem indicating partial MOOC failures or whether they merely indicate that the community of MOOC learners is diverse and by far not every participant intends to complete a course, researchers and MOOC providers are certainly interested in methods for increasing completion rates. The analysis of MOOC data can be of help here. For instance, a linguistic analysis of the MOOC forum data can discover valuable indicators for predicting dropout of students (Wen et al., 2014). However, only few MOOC students (roughly 5-10%) use the discussion forums (Rose and Siemens, 2014), so that dropout predictors for the remaining 90% would be desirable. In order to get insights into the learning behaviors of this majority of participants, the clickstream data of the MOOC platform usage is the primary source for analysis in addition to the forum data. That is also the motivation of the shared task proposed by the MOOC workshop at the Conference on Empirical Methods in Natural Language Processing (EMNLP 2014) (Rose and Siemens, 2014). Addressing this task, we propose a machine learning method based on support vector machines for predicting dropout between MOOC course weeks i</context>
<context position="3947" citStr="Rose and Siemens, 2014" startWordPosition="628" endWordPosition="631">ibe our prediction model. Next, the prediction results and some experimental findings are presented. Finally, we conclude our work in this paper. 2 Dataset The dataset we used in this paper was prepared for the shared task launched by the Modeling Large Scale Social Interaction in Massively Open Online Courses Workshop at the Conference on Empirical Methods in Natural Language Processing 60 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 60–65, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics (EMNLP 2014) (Rose and Siemens, 2014). The data was collected from a psychology MOOC course which was launched in March 2013. The whole course lasted for 12 weeks with 11,607 participants in the beginning week and 3,861 participants staying until the last course week. Overall, 20,828 students participated, with approximately 81.4% lost at last. Note that the data cover the whole life cycle of this online course up to 19 weeks. The original dataset for this task had two types of data: clickstream data and forum data. In this paper, we only make use of clickstream data to train our prediction model and we do not further consider fo</context>
</contexts>
<marker>Rose, Siemens, 2014</marker>
<rawString>Carolyn Rose and George Siemens. Shared Task on Prediction of Dropout Over Time in Massively Open Online Courses. Proceedings of the 2014 Empirical Methods in Natural Language Processing Workshop on Modeling Large Scale Social Interaction in Massively Open Online Courses, Qatar, October 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason A Brotherton</author>
<author>Gregory D Abowd</author>
</authors>
<title>Lessons learned from eClass: Assessing automated capture and access in the classroom.</title>
<date>2004</date>
<journal>ACM Transactions on Computer-Human Interaction,</journal>
<volume>11</volume>
<pages>121--155</pages>
<contexts>
<context position="8774" citStr="Brotherton and Abowd, 2004" startWordPosition="1456" endWordPosition="1459">eo views per session: average number of video click actions per session Number of forum views: number of course discussion forum views Number of wiki views: number of course wiki page views Number of homework page views Number of straight-through video plays: this is a video action attribute. Straight-trough playing video means that the participates played video without any jump (e.g. pause, resume, jump backward and jump forward). Since the lecture videos are the most important learning resource for the learning participants, the video playing should be investigated as other researchers did (Brotherton and Abowd, 2004). In this paper, five video behaviors are taken into account including the number of full plays as well as four others: start-stop during video plays, skip-ahead during video plays, relisten during video plays and the use of low play rate Number of start-stop during video plays: start-stop during video plays stands for a lecture video being paused and resumed Number of skip-ahead during video plays: skip-ahead means that the participant played a video with a forward jump Number of relisten during video plays: relisten means that a backward jump was made as the participant was playing a video N</context>
</contexts>
<marker>Brotherton, Abowd, 2004</marker>
<rawString>Jason A. Brotherton and Gregory D. Abowd. Lessons learned from eClass: Assessing automated capture and access in the classroom. ACM Transactions on Computer-Human Interaction, Vol. 11, No. 2, pp. 121–155, June 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<journal>Machine learning,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="13734" citStr="Cortes and Vapnik, 1995" startWordPosition="2318" endWordPosition="2321"> views (id 9) and homework submission page views (id 10). Past week 12 features related to activity in a more general way like the number of requests (id 1) or the number of page views (id 4) became the most predicative. We proceeded with an exploratory analysis, where we performed a principal component analysis (PCA) for each week, the result is shown in Figure 3. The plot indicates that the users that have dropped out can be better separated from the users that did not drop out when the week id increases. To follow up on this we trained, for each week, a linear support vector machine (SVM) (Cortes and Vapnik, 1995) using the -s 2 option in LIBLINEAR (Fan et al., 2008), which is one of the fastest solvers to train linear SVMs (Fan et al., 2008). The SVM computes an affine-linear prediction function f(x) := (w, x)+b, based on maximizing the (soft) margin between positive and negative examples: (w, b) := argminw,b 2||w||2 + 1 C Eni=1 max(0, 1− yi((w, xi) + b). Note that this is very similar to regularized logistic regression, whichuses theterm 1/(1+exp(−yi((w, xi)+b))) instead of max(0,1− yi((w, xi) + b), but with additional sparsity properties (only a subset of data points are active in the final solution</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and V. Vapnik. Support-vector networks. Machine learning, 20(3):273–297, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R-E Fan</author>
<author>K-W Chang</author>
<author>C-J Hsieh</author>
<author>X-R Wang</author>
<author>C-J Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<volume>9</volume>
<contexts>
<context position="13788" citStr="Fan et al., 2008" startWordPosition="2330" endWordPosition="2333">t week 12 features related to activity in a more general way like the number of requests (id 1) or the number of page views (id 4) became the most predicative. We proceeded with an exploratory analysis, where we performed a principal component analysis (PCA) for each week, the result is shown in Figure 3. The plot indicates that the users that have dropped out can be better separated from the users that did not drop out when the week id increases. To follow up on this we trained, for each week, a linear support vector machine (SVM) (Cortes and Vapnik, 1995) using the -s 2 option in LIBLINEAR (Fan et al., 2008), which is one of the fastest solvers to train linear SVMs (Fan et al., 2008). The SVM computes an affine-linear prediction function f(x) := (w, x)+b, based on maximizing the (soft) margin between positive and negative examples: (w, b) := argminw,b 2||w||2 + 1 C Eni=1 max(0, 1− yi((w, xi) + b). Note that this is very similar to regularized logistic regression, whichuses theterm 1/(1+exp(−yi((w, xi)+b))) instead of max(0,1− yi((w, xi) + b), but with additional sparsity properties (only a subset of data points are active in the final solution) that make it more robust to outliers. The prediction</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research (JMLR), 9:1871–1874, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kloft</author>
<author>U Brefeld</author>
<author>S Sonnenburg</author>
<author>A Zien</author>
</authors>
<title>tpnorm multiple kernel learning.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--953</pages>
<marker>Kloft, Brefeld, Sonnenburg, Zien, 2011</marker>
<rawString>M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien. tpnorm multiple kernel learning. Journal of Machine Learning Research, 12:953–997, Mar 2011.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
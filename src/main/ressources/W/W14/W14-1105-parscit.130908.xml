<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000910">
<title confidence="0.7782825">
Towards Cross-Domain PDTB-Style Discourse Parsing
Evgeny A. Stepanov and Giuseppe Riccardi
</title>
<author confidence="0.50891">
Signals and Interactive Systems Lab
</author>
<affiliation confidence="0.9983775">
Department of Information Engineering and Computer Science
University of Trento, Trento, Italy
</affiliation>
<email confidence="0.998119">
{stepanov,riccardi}@disi.unitn.it
</email>
<sectionHeader confidence="0.997374" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999889">
Discourse relation parsing is an impor-
tant task with the goal of understanding
text beyond the sentence boundaries. With
the availability of annotated corpora (Penn
Discourse Treebank) statistical discourse
parsers were developed. In the litera-
ture it was shown that the discourse pars-
ing subtasks of discourse connective de-
tection and relation sense classification do
not generalize well across domains. The
biomedical domain is of particular interest
due to the availability of Biomedical Dis-
course Relation Bank (BioDRB). In this
paper we present cross-domain evaluation
of PDTB trained discourse relation parser
and evaluate feature-level domain adapta-
tion techniques on the argument span ex-
traction subtask. We demonstrate that the
subtask generalizes well across domains.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999909431034483">
Discourse analysis is one of the most challeng-
ing tasks in Natural Language Processing that has
applications in many language technology areas
such as opinion mining, summarization, informa-
tion extraction, etc. (see (Webber et al., 2011)
and (Taboada and Mann, 2006) for detailed re-
view). The release of the large discourse rela-
tion annotated corpora, such as Penn Discourse
Treebank (PDTB) (Prasad et al., 2008), marked
the development of statistical discourse parsers
(Lin et al., 2012; Ghosh et al., 2011; Xu et al.,
2012; Stepanov and Riccardi, 2013). Recently,
PDTB-style discourse annotation was applied to
biomedical domain and Biomedical Discourse Re-
lation Bank (BioDRB) (Prasad et al., 2011) was
released. This milestone marks the beginning of
the research on cross-domain evaluation and do-
main adaptation of PDTB-style discourse parsers.
In this paper we address the question of how
well PDTB-trained discourse parser (news-wire
domain) can extract argument spans of explicit dis-
course relations in BioDRB (biomedical domain).
The use cases of discourse parsing in biomed-
ical domain are discussed in detail in (Prasad et
al., 2011). Here, on the other hand, we provide
very general connection between the two. The
goal of Biomedical Text Mining (BioNLP) is to
retrieve and organize biomedical knowledge from
scientific publications; and detecting discourse re-
lations such as contrast and causality is an impor-
tant step towards this goal (Prasad et al., 2011). To
illustrate this point consider a quote from (Brunner
and Wirth, 2006), given below.
The addition of an anti-Oct2 antibody
did not interfere with complex formation
(Figure 3, lane 6), since HeLa cells do
not express Oct2. (Cause:Reason)
In the example, the discourse connective since sig-
nals a causal relation between the clauses it con-
nects. That is, the reason why ‘the addition of an
anti-Oct2 antibody did not interfere with complex
formation’ is ‘HeLa cells’ not expressing Oct2’.
PDTB adopts non-hierarchical binary view on
discourse relations: Argument 1 (Arg]) (in italics
in the example) and Argument 2 (Arg2), which is
syntactically attached to a discourse connective (in
bold). Thus, a discourse relation is a triplet of a
connective and its two arguments. In the literature
(Lin et al., 2012; Stepanov and Riccardi, 2013)
PDTB-style discourse parsing is partitioned into
discourse relation detection, argument position
classification, argument span extraction, and rela-
tion sense classification. For the explicit discourse
relations (i.e. signaled by a connective), discourse
relation detection is cast as classification of con-
nectives as discourse and non-discourse. Argu-
ment position classification, on the other hand, in-
volves detection of the location of Arg] with re-
</bodyText>
<page confidence="0.978746">
30
</page>
<note confidence="0.9988765">
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 30–37,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.988946">
Figure 1: Discourse Parser Architecture. (CRF
Argument Span Extraction models are in bold.)
</figureCaption>
<bodyText confidence="0.99935171875">
spect to Arg2, that is to detect whether a relation is
inter- or intra- sentential. Argument span extrac-
tion is the extraction (labeling) of text segments
that belong to each of the arguments. Finally, re-
lation sense classification is the annotation of re-
lations with the senses from the sense hierarchy
(PDTB or BioDRB).
To the best of our knowledge, the only subtasks
that were addressed cross-domain are the detec-
tion of explicit discourse connectives (Ramesh and
Yu, 2010; Ramesh et al., 2012; Faiz and Mercer,
2013) and relation sense classification (Prasad et
al., 2011). While the discourse parser of Faiz and
Mercer (2013)1 provides models for both domains
and does identification of argument head words in
the style of Wellner and Pustejovsky (2007); there
is no decision made on arguments spans. More-
over, there is no cross-domain evaluation available
for each of the models. In this paper we address
the task of cross-domain argument span extraction
of explicit discourse relations. Additionally, we
provide evaluation for cross-domain argument po-
sition classification as far as the data allows, since
BioDRB lacks manual sentence segmentation.
The paper is structured as follows. In Section 2
we present the comparative analysis of PDTB and
BioDRB corpora and the relevant works on cross-
domain discourse parsing. In Section 3 we de-
scribe the PDTB discourse parser used for cross-
domain experiments. In Section 4 we present the
evaluation methodology and the experimental re-
sults. Section 5 provides concluding remarks.
</bodyText>
<sectionHeader confidence="0.9933795" genericHeader="introduction">
2 PDTB vs. BioDRB Corpora Analysis
and Related Cross-Domain Works
</sectionHeader>
<bodyText confidence="0.981748">
The two corpora used in our experiments are Penn
Discourse Treebank (PDTB) (Prasad et al., 2008)
</bodyText>
<footnote confidence="0.971752">
1Made available on https://code.google.com/
p/discourse-parser/
</footnote>
<bodyText confidence="0.998851941176471">
and Biomedical Discourse Relation Bank (Bio-
DRB) (Prasad et al., 2011). Both corpora follow
the same discourse relation annotation style over
different domain corpora: PDTB is annotated on
top of Wall Street Journal (WSJ) corpus (financial
news-wire domain); and it is aligned with Penn
Treebank (PTB) syntactic tree annotation; Bio-
DRB, on the other hand, is a corpus annotated over
24 open access full-text articles from the GENIA
corpus (Kim et al., 2003) (biomedical domain),
and, unlike PDTB, there is no reference tokeniza-
tion or syntactic parse trees.
The detailed comparison of the corpora is out
of the scope of this paper, and it is available in
(Prasad et al., 2011). Similarly, the review of
PDTB-style discourse parsing literature is not in
its scope. Here, on the other hand, we focus on the
corpus differences relevant for discourse parsing
tasks and cross-domain application of discourse
parsing subtasks.
Discourse relations in both corpora are binary:
Arg] and Arg2, where Arg2 is an argument syn-
tactically attached to a discourse connective. With
respect to Arg2, Arg] can appear in the same sen-
tence (SS case), one or several of the preceding
(PS case) or following (FS case) sentences. A
discourse connective is a member of a well de-
fined list of connectives and a relation expressed
via such connective is an Explicit relation. There
are other types of discourse and non-discourse re-
lations annotated in the corpora; however, they are
out of the scope of this paper. Discourse relations
are annotated using a hierarchy of senses: even
though the organization of senses and the number
of levels are different between corpora, the most
general top level senses are mapped to the PDTB
top level senses: Comparison, Contingency, Ex-
pansion, and Temporal (Prasad et al., 2011).
The difference between the two corpora with re-
spect to discourse connectives is that in case of
PDTB the annotated connectives belong to one of
the three syntactic classes: subordinating conjunc-
tions (e.g. because), coordinating conjunctions
(e.g. but), and discourse adverbials (e.g. how-
ever), while BioDRB is also annotated for a forth
syntactic class – subordinators (e.g. by).
There are 100 unique connective types in PDTB
(after connectives like ] year after are stemmed
to after) in 18,459 explicit discourse relations.
Whereas in BioDRB there are 123 unique con-
nective types in 2,636 relations. According to
</bodyText>
<figure confidence="0.993770105263158">
PS
Argument
Position
Classif.
SS
SS Arg2
Extraction
SS Arg1
Extraction
PS Arg2
Extraction
Discourse
Connective
Detection
PS Arg1
Candidate
Heuristic
PS Arg1
Extraction
</figure>
<page confidence="0.999821">
31
</page>
<bodyText confidence="0.999983451612903">
the discourse connective analysis in (Ramesh et
al., 2012), the subordinators comprise 33% of all
connective types in BioDRB. Additionally, 11%
of connective types in common syntactic classes
that occur in BioDRB do not occur in PDTB; e.g.
In summary, as a consequence. Thus, only 56%
of connective types of BioDRB are common to
both corpora. While in-domain discourse connec-
tive detection has good performance (Ramesh and
Yu, 2010), this difference makes the cross-domain
identification of discourse connectives a hard task,
which is exemplified by experiments in (Ramesh
and Yu, 2010) (F1 = 0.55).
With respect to relation sense classification, the
connective surface provides already high baselines
(Prasad et al., 2011). However, cross-domain
sense classification experiments indicate that there
are significant differences in the semantic usage of
connectives between two domains, since the per-
formance of the classifier trained on PDTB does
not generalize well to BioDRB (F1 = 0.57).
To sum up, the corpora differences with respect
to discourse connective usage affect the cross-
domain generalization of connective detection and
sense classification tasks negatively. The exper-
iments in this paper are intended to evaluate the
generalization of argument span extraction, as-
suming that the connective is already identified.
In the following section, we present the PDTB-
trained discourse parser optimized for in-domain
performance.
</bodyText>
<sectionHeader confidence="0.973032" genericHeader="method">
3 PDTB-Style Discourse Parser
</sectionHeader>
<bodyText confidence="0.999946">
The discourse parser (see Figure 1) is a combi-
nation of argument position classification model
for classifying discourse connectives as inter- or
intra-sentential, and specific Conditional Random
Fields argument extraction models for each of the
arguments in these configurations. In the follow-
ing subsections we provide descriptions for each
of the components.
</bodyText>
<subsectionHeader confidence="0.999223">
3.1 Argument Position Classification
</subsectionHeader>
<bodyText confidence="0.999051137931034">
Discourse connectives have a very strong prefer-
ence on the location of the Arg] with respect to
their syntactic category (Subordinating Conjunc-
tion, Coordinating Conjunction, and Discourse
Adverbial) and position in the sentence (sentence
initial or sentence medial); thus, classification of
discourse connectives into inter-sentential or intra-
sentential is an easy task yielding high supervised
machine learning performance (Stepanov and Ric-
cardi, 2013; Lin et al., 2012). With respect to the
decision made in this step a specific argument span
extraction model is applied.
For Argument Position Classification the un-
igram BoosTexter (Schapire and Singer, 2000)
model with 100 iterations is trained on PDTB sec-
tions 02-22 and tested on sections 23-24. Sim-
ilar to the previously published results, it has a
high performace: F1 = 98.12. The features
are connective surface string, POS-tags, and IOB-
chains. The results obtained with automatic sen-
tence splitting, tokenization, and syntactic parsing
using Stanford Parser (Klein and Manning, 2003)
are also high F1 = 97.81.
Since, unlike PTB for PDTB, for BioDRB there
is no manual sentence splitting, tokenization, and
syntactic tree annotation; the precise cross-domain
evaluation of Argument Span Extraction step is not
possible. However, in Section 4 we estimate the
performance using automatic sentence splitting.
</bodyText>
<subsectionHeader confidence="0.999748">
3.2 Argument Span Extraction
</subsectionHeader>
<bodyText confidence="0.9999675">
Argument span extraction is cast as token-level se-
quence labeling using Conditional Random Fields
(CRF) (Lafferty et al., 2001). Previously, it was
observed that in PDTB for inter-sentential dis-
course relations Arg] precedes Arg2 in most of the
cases. Thus, the CRF models are trained for the
configurations where both of the arguments are in
the same sentence (SS), and for Arg] in one of the
previous sentences (PS); the following sentence
Arg] case (FS) is ignored due to too few training
instances being available (in PDTB 8 / 18,459).
Consequently, there are 4 CRF models SS Arg1
and Arg2, and PS Arg1 and Arg2.
Same sentence case models are applied in a cas-
cade, such that output of Arg2 model is used as a
feature for Arg] span extraction. For the case of
Arg] in the previous sentences; based on the ob-
servation that in PDTB Arg2 span is fully located
in the sentence containing the connective in 98.5%
of instances; and Arg1 span is fully located in the
sentence immediately preceding Arg2 in 71.7% of
instances; the sentences in these positions are se-
lected and CRF models are trained to label the
spans.
The features used for training the models are
presented in Table 1. The feature sets are opti-
mized for each of the arguments in (Ghosh et al.,
2011) (see the Table columns Arg1 and Arg2). Be-
</bodyText>
<page confidence="0.996436">
32
</page>
<bodyText confidence="0.997172666666667">
sides the features commonly used in NLP tasks
such that token, lemma, inflectional affixes, and
part-of-speech tag, the rest of the features are:
</bodyText>
<listItem confidence="0.992562722222222">
• IOB-Chain (IOB) is the path string of the syn-
tactic tree nodes from the root node to the to-
ken, prefixed with the information whether a
token is at the beginning (B-) or inside (I-)
the constituent. The chunklink tool (Buch-
holz, 2000) is used to extract this feature from
syntactic trees.
• PDTB Level 1 Connective sense (CONN) is
the most general sense of a connective in
PDTB sense hierarchy. It’s general purpose is
to label the discourse connective tokens, i.e.
the value of the feature is ‘NULL’ for all to-
kens except the discourse connective.
• Boolean Main Verb (BMV) is a boolean fea-
ture that indicates whether a token is a main
verb of a sentence or not (Yamada and Mat-
sumoto, 2003).
• Arg2 Label (ARG2) is an output of Arg2 span
</listItem>
<bodyText confidence="0.817759466666667">
extraction model, that is used as a feature for
Arg1 span extraction. Arg2 span is easier to
identify (Ghosh et al., 2011; Stepanov and
Riccardi, 2013) since it is syntactically at-
tached to the discourse connective. Thus, this
feature serves to constrain the Arg1 search
space for intra-sentential argument span ex-
traction. The value of the feature is either
ARG2 suffixed for whether a token is Inside
(I), Begin (B), or End (E) of the span, or ‘O’
if it does not belong to the Arg2 span.
These features are expanded during training
with n-grams (feature of CRF++2): tokens with
2-grams in the window of +1 tokens, and the rest
of the features with 2 &amp; 3-grams in the window of
</bodyText>
<sectionHeader confidence="0.713757" genericHeader="method">
+2 tokens.
</sectionHeader>
<bodyText confidence="0.99985725">
The in-domain performance of argument span
extraction models is provided in the following
section, after the description of the evaluation
methodology.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.99864375">
In this Section we first describe the evaluation
methodology and then the experiments on cross-
domain evaluation of argument position classifi-
cation and argument span extraction models.
</bodyText>
<footnote confidence="0.953095">
2https://code.google.com/p/crfpp/
</footnote>
<table confidence="0.999941333333333">
Feature ABBR Arg2 Arg1
Token TOK Y Y
POS-Tag POS
Lemma LEM Y Y
Inflection INFL Y Y
IOB-Chain IOB Y Y
Connective Sense CONN Y Y
Boolean Main Verb BMV Y
Arg2 Label ARG2 Y
</table>
<tableCaption confidence="0.934793">
Table 1: Feature sets for Arg2 and Arg1 argument
span extraction.
</tableCaption>
<bodyText confidence="0.9926725">
The experimental settings for PDTB are the fol-
lowing: Sections 02-22 are used for training and
Sections 23-24 for testing. For BioDRB, on the
other hand, 12 fold cross-validation is used (2 doc-
uments in each fold, since in BioDRB there are 24
documents).
</bodyText>
<subsectionHeader confidence="0.959597">
4.1 Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.99891975">
The performance of Argument Span Extraction is
evaluated in terms of precision (p), recall (r), and
F-measure (Fl) using the equations 1 – 3. An
argument span is considered to be correct, if it
exactly matches the reference string. Following
(Ghosh et al., 2011) and (Lin et al., 2012), argu-
ment initial and final punctuation marks are re-
moved.
</bodyText>
<equation confidence="0.963591875">
Exact Match
p = (1)
Exact Match + No Match
r = Exact Match (2)
References in Gold
_ 2 ∗ p ∗ r
F1 (3)
p + r
</equation>
<bodyText confidence="0.999952857142857">
In the equations, Exact Match is the count of cor-
rectly tagged argument spans; No Match is the
count of argument spans that do not match the ref-
erence string exactly, i.e. even a single token dif-
ference is counted as an error; and References in
Gold is the total number of arguments in the refer-
ence.
Since argument span extraction is applied after
argument position classification, the classification
error is propagated. Thus, for the evaluation of
argument span extraction, misclassified instances
are reflected in the counts of Exact Matches and
No Matches. For example, misclassified same sen-
tence relation results in that both its arguments are
</bodyText>
<page confidence="0.9953">
33
</page>
<table confidence="0.9998155">
Arg2 Arg1
P R F1 P R F1
Gold
SS 90.36 87.49 88.90 70.27 66.67 68.42
PS 79.01 77.10 78.04 46.23 36.61 40.86
ALL 85.93 83.45 84.67 61.94 54.98 58.25
Auto
SS 86.83 85.14 85.98 64.26 63.01 63.63
PS 75.00 73.67 74.33 37.66 37.00 37.33
ALL 82.24 80.69 81.46 53.93 52.92 53.42
</table>
<tableCaption confidence="0.998532">
Table 2: In-domain performance of the PDTB-
</tableCaption>
<bodyText confidence="0.963581066666667">
trained argument span extraction models on the
test set with ‘Gold’ and ’Automatic’ sentence
splitting, tokenization, and syntactic features. The
results are reported together with the error prop-
agation from argument position classification for
Same Sentence (SS), Previous Sentence (PS) mod-
els and joined results (ALL) as precision (P), recall
(R) and F-measure (F1).
considered as not recalled for the SS, and for the
PS they are considered as No Match.
However, we do not propagate error in cross-
domain evaluation on BioDRB, since there is no
reference information. Additionally, while Arg]
span extraction models are trained on Gold Arg2
features, for testing they are always automatic.
</bodyText>
<subsectionHeader confidence="0.9316355">
4.2 Cross-Domain Argument Position
Classification
</subsectionHeader>
<bodyText confidence="0.9999914">
As it was mentioned above, there is no manual
sentence splitting for BioDRB; thus, there is no
references for whether a discourse relation has its
Arg] in the same or different sentences. In order
to evaluate cross-domain argument position clas-
sification we evaluate classifier decisions against
automatic sentence splitting using Stanford Parser
(Klein and Manning, 2003) on whole of BioDRB.
The BoosTexter model described in Section 3.1
has a high in-domain performance of 97.81. On
BioDRB its performance is 95.26, which is still
high. Thus, we can conclude that argument posi-
tion classification generalizes well cross-domain,
and that it is little affected by the presence of ‘sub-
ordinators’ that were not annotated in PDTB.
</bodyText>
<sectionHeader confidence="0.6841675" genericHeader="method">
4.3 In-Domain Argument Span Extraction:
PDTB
</sectionHeader>
<bodyText confidence="0.9999456">
The in-domain performance of the argument span
extraction models trained on PDTB sections 02-22
and tested on sections 23-24 is given on Table 2.
The results are for 2 settings: ‘Gold’ and ‘Auto’.
In the ‘Gold’ settings the sentence splitting, tok-
enization and syntactic features are extracted from
PTB, and in the ‘Auto’ they are extracted from au-
tomatic parse trees obtained using Stanford Parser
(Klein and Manning, 2003).
The general trend in the literature, is that the ar-
gument span extraction for Arg] has lower perfor-
mance than for Arg2, which is expected since Arg2
position is signaled by a discourse connective. Ad-
ditionally, Previous Sentence Arg] model perfor-
mance is much lower than that of the other models
due to the fact that it only considers immediately
previous sentence; which, as was mentioned ear-
lier, covers only 71.7% of the inter-sentential re-
lations. In the next subsections, these models are
evaluated on biomedical domain.
</bodyText>
<sectionHeader confidence="0.701113" genericHeader="method">
4.4 In-Domain Argument Span Extraction:
BioDRB
</sectionHeader>
<bodyText confidence="0.999982133333333">
In order to evaluate PDTB-BioDRB cross-domain
performance we first evaluate the in-domain Bio-
DRB argument span extraction. Since there is no
gold sentence splitting, tokenization and syntactic
parse trees, the models are trained using the fea-
tures extracted from automatic parse trees. We use
exactly the same feature sets as for PDTB models,
which are optimized for PDTB. An important as-
pect is that in BioDRB the connective senses are
different: there are 16 top level senses that are
mapped to 4 top level PDTB senses. For the in-
domain BioDRB models, the 16 senses were kept
as is.
Since we do not have gold argument position
information, we do not train in-domain argument
classification model. Thus, the reported results are
without error propagation. Later, this will allow us
to assess cross-domain argument span extraction
performance better.
The results reported in Table 3 are average
precision, recall and f-measure of 12-fold cross-
validation. With respect to automatic sentence
splitting, there are 717 inter-sentential and 1,919
intra-sentential relations (27% to 73%). Thus,
BioDRB is less affected by PS Arg] performance
than PDTB models, where the ratio is 619 to
976 (39% to 61%). Additionally, BioDRB PS
Arg] performance is generally higher than that
of PDTB. Overall, in-domain BioDRB argument
extraction model performance is in-line with the
</bodyText>
<page confidence="0.99814">
34
</page>
<table confidence="0.999201">
Arg2 Arg1
P R F1 P R F1
SS 80.94 79.88 80.41 66.51 61.82 64.07
PS 82.99 82.99 82.99 57.50 55.62 56.53
ALL 81.45 80.67 81.06 63.87 60.00 61.87
</table>
<tableCaption confidence="0.814251666666667">
Table 3: In-domain performance of the BioDRB-
trained argument span extraction models. Both
training and testing are on automatic sentence
</tableCaption>
<bodyText confidence="0.972249222222222">
splitting, tokenization, and syntactic features. The
results are reported for Same Sentence (SS) and
Previous Sentence (PS) models, and the joined re-
sults for each of the arguments (ALL) as average
precision (P), recall (R), and F-measure (F1) of
12-fold cross-validation.
PDTB models, with the exception that previous
sentence Arg2 has higher performance than the
same sentence one.
</bodyText>
<subsectionHeader confidence="0.8714425">
4.5 Cross-Domain Argument Span
Extraction: PDTB - BioDRB
</subsectionHeader>
<bodyText confidence="0.9997769">
Similar to in-domain BioDRB argument span ex-
traction, we perform 12 fold cross-validation for
PDTB-BioDRB cross-domain argument span ex-
traction. The cross-domain performance of the
models described in Section 4.3 is given in the
Table 4 under the ‘Gold’. To make the cross-
domain evaluation settings closer to the BioDRB
in-domain evaluation, we additionally train PDTB
models on the automatic features, i.e. features ex-
tracted from PDTB with automatic sentence split-
ting, tokenization and syntactic parsing. Similar
to the in-domain BioDRB evaluation, results are
reported without error propagation from argument
position classification step.
The first observation from cross-domain eval-
uation is that argument span extraction general-
izes to biomedical domain much better that the
discourse parsing subtasks of discourse connective
detection and relation sense classification. Unlike
those subtasks, the difference between in-domain
BioDRB argument span extraction models and the
models trained on PDTB is much less: e.g. for
discourse connective detection the in-domain and
cross-domain difference for BioDRB is 14 points
(f-measures 69 and 55 in (Ramesh and Yu, 2010)),
and for argument span extraction 2 and 4 points
for Arg2 and Arg1 respectively (see Tables 3 &amp; 4).
The difference between the models trained on
automatic and gold parse trees is also not high, and
gold feature trained models perform better with
</bodyText>
<table confidence="0.9992987">
Arg2 Arg1
P R F1 P R F1
Gold
SS 80.37 76.58 78.42 60.82 56.40 58.52
PS 80.73 80.50 80.62 57.74 52.95 55.19
ALL 80.53 77.71 79.09 59.76 55.29 57.43
Auto
SS 77.60 75.05 76.30 60.76 55.21 57.83
PS 81.39 81.23 81.31 57.71 51.72 54.47
ALL 78.72 76.80 77.74 59.60 54.12 56.71
</table>
<tableCaption confidence="0.999053">
Table 4: Cross-domain performance of the PDTB-
</tableCaption>
<bodyText confidence="0.921061357142857">
trained argument span extraction models on Bio-
DRB. For the ‘Gold’ setting the models from in-
domain PDTB section are used. For ‘Auto’, the
models are trained on automatic sentence splitting,
tokenization, and syntactic features. The results
are reported for Same Sentence (SS) and Previ-
ous Sentence (PS) models, and the joined results
for each of the arguments (ALL) as average preci-
sion (P), recall (R), and F-measure (F1) of 12-fold
cross-validation.
the exception of PS Arg2. Since training on auto-
matic parse trees does not improve cross-domain
performance, the rest of the experiments is using
gold features for training.
</bodyText>
<subsectionHeader confidence="0.994776">
4.6 Feature-Level Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999984227272727">
The two major differences between PDTB and
BioDRB are vocabulary and connective senses.
The out-of-vocabulary rate of PDTB on the whole
BioDRB is 22.7% and of BioDRB on PDTB is
33.1%, which are very high. Thus, PDTB lexi-
cal features might not be very effective, and the
models generalize well due to syntactic features.
To test this hypothesis we train additional PDTB
models on only syntactic features: POS-tags and
IOB-chain and ‘connective labels’ – ‘CONN’ suf-
fixed for the Beginning (B), Inside (I) or End (E)
of the connective span, simulating discourse con-
nective detection output. Moreover, we reduce the
feature set to unigrams only (recall that features
were enriched by 2 and 3 grams), such that the
models become very general.
Even though BioDRB connective senses can be
mapped to PDTB, in (Prasad et al., 2011) it was
observed that relation sense classification does not
generalize well. To reduce the dependency of ar-
gument span extraction models on relation sense
classification, the connective sense feature in the
</bodyText>
<page confidence="0.997305">
35
</page>
<table confidence="0.999784214285714">
Arg2 Arg1
P R F1 P R F1
Baseline
SS 80.37 76.58 78.42 60.82 56.40 58.52
PS 80.73 80.50 80.62 57.74 52.95 55.19
ALL 80.53 77.71 79.09 59.76 55.29 57.43
Syntactic
SS 82.00 75.03 78.33 61.07 51.80 56.01
PS 75.56 74.47 75.01 56.64 46.66 51.11
ALL 80.31 74.98 77.54 59.69 50.42 54.63
No Relation Sense
SS 81.35 74.00 77.47 62.46 56.11 59.10
PS 80.35 80.13 80.24 57.58 52.25 54.74
ALL 81.16 75.67 78.30 60.86 54.87 57.69
</table>
<tableCaption confidence="0.997331">
Table 5: Cross-domain performance of the PDTB-
</tableCaption>
<bodyText confidence="0.974393176470588">
trained argument span extraction models on Bio-
DRB. For the ‘Syntactic’ setting the models are
trained on only syntactic features (POS-tag + IOB-
chain) and ‘connective labels’. For ‘No Relation
Sense’, the models are trained by replacing con-
nective sense with ‘connective labels’. The ‘Base-
line’ is repeated from Table 4. The results are re-
ported for Same Sentence (SS) and Previous Sen-
tence (PS) models, and the joined results for each
of the arguments (ALL) as average precision (P),
recall (R), and F-measure (F1) of 12-fold cross-
validation.
‘Baseline’ models (i.e. the models from Section
4.3) is also replaced by ‘connective labels’. We
train these models using gold features only, and,
similar to previous experiments, do 12-fold cross-
validation.
The performance of the adapted models is given
in Table 5. The ‘Syntactic’ section gives the re-
sults of the models trained on syntactic features
and the ‘No Relation Sense’ section gives the re-
sults for the models with ‘connective labels’ in-
stead of connective senses, and the ‘Baseline’
repeats the performance of the PDTB-optimized
models.
The PDTB-optimized baseline, outperforms the
adapted models on Arg2; however, ‘No Relation
Sense’ Arg] yields the best performance, and,
though insignificantly, outperforms the baseline.
Thus, the effect of replacing connective senses
with ‘connective labels’ is negative for all cases
except SS Arg]. Overall, the difference in perfor-
mance between the ‘Baseline’ and ‘No Relation
Sense’ models is an acceptable price to pay for the
</bodyText>
<table confidence="0.9982736">
Arg2 Arg1
P R F1 P R F1
SS 81.72 76.14 78.82 61.53 56.36 58.82
PS 80.31 79.84 80.07 58.55 52.82 55.44
ALL 81.27 77.10 79.12 60.56 55.30 57.80
</table>
<tableCaption confidence="0.989884">
Table 6: Cross-domain performance of the PDTB-
</tableCaption>
<bodyText confidence="0.993315736842106">
trained argument span extraction model on uni-
gram and bigrams of token, POS-tag, IOB-chain
and ‘connective label’. The results are reported for
Same Sentence (SS) and Previous Sentence (PS)
models, and the joined results for each of the argu-
ments (ALL) as average precision (P), recall (R),
and F-measure (F1) of 12-fold cross-validation.
independence from relation sense classification.
The most general models – unigrams of Part-of-
Speech tags and IOB-chains together with ‘con-
nective labels’ in the window of ±2 tokens –
all have the performance lower than the baseline,
which is expected given its feature set. However,
for the easiest case of intra-sentential Arg2 it out-
performs the model trained by replacing the con-
nective sense in the baseline (i.e. ‘No Relation
Sense’). Degraded performance of Arg] models
indicates that lexical features are helpful.
Introducing the tokens back into the ‘Syntactic’
model, and increasing the features to include also
2-grams, boosts the performance of the models to
outperform the ‘No Relation Sense’ models in all
but Previous Sentence Arg2 category. However,
the models now yield performance comparable to
the PDTB optimized baseline (insignificantly bet-
ter), while being unaffected by poor cross-domain
generalization of relation sense classification (see
Table 6).
The cross-domain argument extraction exper-
iments indicate that models trained on PDTB-
optimized feature set already have good general-
ization. However, they are dependent on relation
sense classification task, which does not gener-
alize well. By replacing connective senses with
‘connective labels’ we obtain models independent
of this task while maintaining comparable perfor-
mance. The in-domain trained BioDRB models,
however, perform better, as expected.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.988521">
In this paper we presented cross-domain discourse
parser evaluation on subtasks of argument posi-
tion classification and argument span extraction.
</bodyText>
<page confidence="0.993731">
36
</page>
<bodyText confidence="0.99998775">
The observed cross-domain performances are in-
dicative of good model generalization. However,
since these models are applied later in the pipeline,
they are affected by the cross-domain performance
of the other tasks. Specifically, discourse connec-
tive detection, which was shown not to generalize
well in the literature. Additionally, we have pre-
sented feature-level domain adaptation techniques
to reduce the dependence of the cross-domain ar-
gument span extraction on other discourse parsing
subtasks.
The syntactic parser (Stanford) that provides
sentence splitting and tokenization is trained on
Penn Treebank, i.e. it is in-domain for PDTB
and out-of-domain for BioDRB; and it is known
that domain-optimized tokenization improves per-
formance on various NLP tasks. Thus, the fu-
ture direction of this work is to evaluate argument
span extraction using tools optimized for biomed-
ical domain.
</bodyText>
<sectionHeader confidence="0.999075" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99992825">
The research leading to these results has re-
ceived funding from the European Union – Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement No. 610916 – SENSEI.
</bodyText>
<sectionHeader confidence="0.999507" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999655092105263">
Cornelia Brunner and Thomas Wirth. 2006. Btk ex-
pression is controlled by oct and bob. 1/obf. 1. Nu-
cleic acids research, 34(6):1807–1815.
Sabine Buchholz. 2000. Readme for perl script chun-
klink.pl.
Syeed Ibn Faiz and Robert E Mercer. 2013. Iden-
tifying explicit discourse connectives in text. In
Advances in Artificial Intelligence, pages 64–76.
Springer.
Sucheta Ghosh, Richard Johansson, Giuseppe Ric-
cardi, and Sara Tonelli. 2011. Shallow discourse
parsing with conditional random fields. In Proceed-
ings of the 5th International Joint Conference on
Natural Language Processing (IJCNLP 2011).
J-D Kim, Tomoko Ohta, Yuka Tateisi, and Junichi Tsu-
jii. 2003. Genia corpusa semantically annotated
corpus for bio-textmining. Bioinformatics, 19(suppl
1):i180–i182.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS 2002).
John Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of 18th International
Conference on Machine Learning.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012.
A pdtb-styled end-to-end discourse parser. Natural
Language Engineering, 1:1 – 35.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
Rashmi Prasad, Susan McRoy, Nadya Frid, Aravind
Joshi, and Hong Yu. 2011. The biomedical
discourse relation bank. BMC Bioinformatics,
12(1):188.
Balaji Polepalli Ramesh and Hong Yu. 2010. Identify-
ing discourse connectives in biomedical text. AMIA
Annual Symposium Proceedings, 2010:657.
Balaji Polepalli Ramesh, Rashmi Prasad, Tim Miller,
Brian Harrington, and Hong Yu. 2012. Automatic
discourse connective detection in biomedical text.
Journal of the American Medical Informatics Asso-
ciation, 19(5):800–808.
Robert E. Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135–168.
Evgeny A. Stepanov and Giuseppe Riccardi. 2013.
Comparative evaluation of argument extraction al-
gorithms in discourse relation parsing. In 13th
International Conference on Parsing Technologies
(IWPT 2013), pages 36–44.
Maite Taboada and William C. Mann. 2006. Applica-
tions of rhetorical structure theory. Discourse Stud-
ies, (8):567–88.
Bonnie L. Webber, Markus Egg, and Valia Kordoni.
2011. Discourse structure and language technology.
Natural Language Engineering, pages 1 – 54.
Ben Wellner and James Pustejovsky. 2007. Automat-
ically identifying the arguments of discourse con-
nectives. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL 2007).
Fan Xu, Qiao Ming Zhu, and Guo Dong Zhou. 2012.
A unified framework for discourse argument identi-
fication via shallow semantic parsing. In Proceed-
ings of 24th International Conference on Computa-
tional Linguistics (COLING 2012): Posters.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of 8th International Work-
shop on Parsing Technologies.
</reference>
<page confidence="0.999611">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.342316">
<title confidence="0.999834">Towards Cross-Domain PDTB-Style Discourse Parsing</title>
<author confidence="0.799427">A Stepanov</author>
<affiliation confidence="0.779940666666667">Signals and Interactive Systems Department of Information Engineering and Computer University of Trento, Trento,</affiliation>
<abstract confidence="0.99948775">Discourse relation parsing is an important task with the goal of understanding text beyond the sentence boundaries. With the availability of annotated corpora (Penn Discourse Treebank) statistical discourse parsers were developed. In the literature it was shown that the discourse parsing subtasks of discourse connective detection and relation sense classification do not generalize well across domains. The biomedical domain is of particular interest due to the availability of Biomedical Discourse Relation Bank (BioDRB). In this paper we present cross-domain evaluation of PDTB trained discourse relation parser and evaluate feature-level domain adaptation techniques on the argument span extraction subtask. We demonstrate that the subtask generalizes well across domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cornelia Brunner</author>
<author>Thomas Wirth</author>
</authors>
<title>Btk expression is controlled by oct and bob.</title>
<date>2006</date>
<booktitle>1/obf. 1. Nucleic acids research,</booktitle>
<pages>34--6</pages>
<contexts>
<context position="2600" citStr="Brunner and Wirth, 2006" startWordPosition="382" endWordPosition="385">se parser (news-wire domain) can extract argument spans of explicit discourse relations in BioDRB (biomedical domain). The use cases of discourse parsing in biomedical domain are discussed in detail in (Prasad et al., 2011). Here, on the other hand, we provide very general connection between the two. The goal of Biomedical Text Mining (BioNLP) is to retrieve and organize biomedical knowledge from scientific publications; and detecting discourse relations such as contrast and causality is an important step towards this goal (Prasad et al., 2011). To illustrate this point consider a quote from (Brunner and Wirth, 2006), given below. The addition of an anti-Oct2 antibody did not interfere with complex formation (Figure 3, lane 6), since HeLa cells do not express Oct2. (Cause:Reason) In the example, the discourse connective since signals a causal relation between the clauses it connects. That is, the reason why ‘the addition of an anti-Oct2 antibody did not interfere with complex formation’ is ‘HeLa cells’ not expressing Oct2’. PDTB adopts non-hierarchical binary view on discourse relations: Argument 1 (Arg]) (in italics in the example) and Argument 2 (Arg2), which is syntactically attached to a discourse con</context>
</contexts>
<marker>Brunner, Wirth, 2006</marker>
<rawString>Cornelia Brunner and Thomas Wirth. 2006. Btk expression is controlled by oct and bob. 1/obf. 1. Nucleic acids research, 34(6):1807–1815.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
</authors>
<title>Readme for perl script chunklink.pl.</title>
<date>2000</date>
<contexts>
<context position="13406" citStr="Buchholz, 2000" startWordPosition="2080" endWordPosition="2082"> models are trained to label the spans. The features used for training the models are presented in Table 1. The feature sets are optimized for each of the arguments in (Ghosh et al., 2011) (see the Table columns Arg1 and Arg2). Be32 sides the features commonly used in NLP tasks such that token, lemma, inflectional affixes, and part-of-speech tag, the rest of the features are: • IOB-Chain (IOB) is the path string of the syntactic tree nodes from the root node to the token, prefixed with the information whether a token is at the beginning (B-) or inside (I-) the constituent. The chunklink tool (Buchholz, 2000) is used to extract this feature from syntactic trees. • PDTB Level 1 Connective sense (CONN) is the most general sense of a connective in PDTB sense hierarchy. It’s general purpose is to label the discourse connective tokens, i.e. the value of the feature is ‘NULL’ for all tokens except the discourse connective. • Boolean Main Verb (BMV) is a boolean feature that indicates whether a token is a main verb of a sentence or not (Yamada and Matsumoto, 2003). • Arg2 Label (ARG2) is an output of Arg2 span extraction model, that is used as a feature for Arg1 span extraction. Arg2 span is easier to id</context>
</contexts>
<marker>Buchholz, 2000</marker>
<rawString>Sabine Buchholz. 2000. Readme for perl script chunklink.pl.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Syeed Ibn Faiz</author>
<author>Robert E Mercer</author>
</authors>
<title>Identifying explicit discourse connectives in text.</title>
<date>2013</date>
<booktitle>In Advances in Artificial Intelligence,</booktitle>
<pages>64--76</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4642" citStr="Faiz and Mercer, 2013" startWordPosition="696" endWordPosition="699">nguistics Figure 1: Discourse Parser Architecture. (CRF Argument Span Extraction models are in bold.) spect to Arg2, that is to detect whether a relation is inter- or intra- sentential. Argument span extraction is the extraction (labeling) of text segments that belong to each of the arguments. Finally, relation sense classification is the annotation of relations with the senses from the sense hierarchy (PDTB or BioDRB). To the best of our knowledge, the only subtasks that were addressed cross-domain are the detection of explicit discourse connectives (Ramesh and Yu, 2010; Ramesh et al., 2012; Faiz and Mercer, 2013) and relation sense classification (Prasad et al., 2011). While the discourse parser of Faiz and Mercer (2013)1 provides models for both domains and does identification of argument head words in the style of Wellner and Pustejovsky (2007); there is no decision made on arguments spans. Moreover, there is no cross-domain evaluation available for each of the models. In this paper we address the task of cross-domain argument span extraction of explicit discourse relations. Additionally, we provide evaluation for cross-domain argument position classification as far as the data allows, since BioDRB </context>
</contexts>
<marker>Faiz, Mercer, 2013</marker>
<rawString>Syeed Ibn Faiz and Robert E Mercer. 2013. Identifying explicit discourse connectives in text. In Advances in Artificial Intelligence, pages 64–76. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sucheta Ghosh</author>
<author>Richard Johansson</author>
<author>Giuseppe Riccardi</author>
<author>Sara Tonelli</author>
</authors>
<title>Shallow discourse parsing with conditional random fields.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP</booktitle>
<contexts>
<context position="1565" citStr="Ghosh et al., 2011" startWordPosition="222" endWordPosition="225">ument span extraction subtask. We demonstrate that the subtask generalizes well across domains. 1 Introduction Discourse analysis is one of the most challenging tasks in Natural Language Processing that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc. (see (Webber et al., 2011) and (Taboada and Mann, 2006) for detailed review). The release of the large discourse relation annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008), marked the development of statistical discourse parsers (Lin et al., 2012; Ghosh et al., 2011; Xu et al., 2012; Stepanov and Riccardi, 2013). Recently, PDTB-style discourse annotation was applied to biomedical domain and Biomedical Discourse Relation Bank (BioDRB) (Prasad et al., 2011) was released. This milestone marks the beginning of the research on cross-domain evaluation and domain adaptation of PDTB-style discourse parsers. In this paper we address the question of how well PDTB-trained discourse parser (news-wire domain) can extract argument spans of explicit discourse relations in BioDRB (biomedical domain). The use cases of discourse parsing in biomedical domain are discussed </context>
<context position="12979" citStr="Ghosh et al., 2011" startWordPosition="2003" endWordPosition="2006">re applied in a cascade, such that output of Arg2 model is used as a feature for Arg] span extraction. For the case of Arg] in the previous sentences; based on the observation that in PDTB Arg2 span is fully located in the sentence containing the connective in 98.5% of instances; and Arg1 span is fully located in the sentence immediately preceding Arg2 in 71.7% of instances; the sentences in these positions are selected and CRF models are trained to label the spans. The features used for training the models are presented in Table 1. The feature sets are optimized for each of the arguments in (Ghosh et al., 2011) (see the Table columns Arg1 and Arg2). Be32 sides the features commonly used in NLP tasks such that token, lemma, inflectional affixes, and part-of-speech tag, the rest of the features are: • IOB-Chain (IOB) is the path string of the syntactic tree nodes from the root node to the token, prefixed with the information whether a token is at the beginning (B-) or inside (I-) the constituent. The chunklink tool (Buchholz, 2000) is used to extract this feature from syntactic trees. • PDTB Level 1 Connective sense (CONN) is the most general sense of a connective in PDTB sense hierarchy. It’s general</context>
<context position="15777" citStr="Ghosh et al., 2011" startWordPosition="2486" endWordPosition="2489">l ARG2 Y Table 1: Feature sets for Arg2 and Arg1 argument span extraction. The experimental settings for PDTB are the following: Sections 02-22 are used for training and Sections 23-24 for testing. For BioDRB, on the other hand, 12 fold cross-validation is used (2 documents in each fold, since in BioDRB there are 24 documents). 4.1 Evaluation Methodology The performance of Argument Span Extraction is evaluated in terms of precision (p), recall (r), and F-measure (Fl) using the equations 1 – 3. An argument span is considered to be correct, if it exactly matches the reference string. Following (Ghosh et al., 2011) and (Lin et al., 2012), argument initial and final punctuation marks are removed. Exact Match p = (1) Exact Match + No Match r = Exact Match (2) References in Gold _ 2 ∗ p ∗ r F1 (3) p + r In the equations, Exact Match is the count of correctly tagged argument spans; No Match is the count of argument spans that do not match the reference string exactly, i.e. even a single token difference is counted as an error; and References in Gold is the total number of arguments in the reference. Since argument span extraction is applied after argument position classification, the classification error is</context>
</contexts>
<marker>Ghosh, Johansson, Riccardi, Tonelli, 2011</marker>
<rawString>Sucheta Ghosh, Richard Johansson, Giuseppe Riccardi, and Sara Tonelli. 2011. Shallow discourse parsing with conditional random fields. In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-D Kim</author>
<author>Tomoko Ohta</author>
<author>Yuka Tateisi</author>
<author>Junichi Tsujii</author>
</authors>
<title>Genia corpusa semantically annotated corpus for bio-textmining. Bioinformatics,</title>
<date>2003</date>
<pages>1--180</pages>
<contexts>
<context position="6335" citStr="Kim et al., 2003" startWordPosition="957" endWordPosition="960">The two corpora used in our experiments are Penn Discourse Treebank (PDTB) (Prasad et al., 2008) 1Made available on https://code.google.com/ p/discourse-parser/ and Biomedical Discourse Relation Bank (BioDRB) (Prasad et al., 2011). Both corpora follow the same discourse relation annotation style over different domain corpora: PDTB is annotated on top of Wall Street Journal (WSJ) corpus (financial news-wire domain); and it is aligned with Penn Treebank (PTB) syntactic tree annotation; BioDRB, on the other hand, is a corpus annotated over 24 open access full-text articles from the GENIA corpus (Kim et al., 2003) (biomedical domain), and, unlike PDTB, there is no reference tokenization or syntactic parse trees. The detailed comparison of the corpora is out of the scope of this paper, and it is available in (Prasad et al., 2011). Similarly, the review of PDTB-style discourse parsing literature is not in its scope. Here, on the other hand, we focus on the corpus differences relevant for discourse parsing tasks and cross-domain application of discourse parsing subtasks. Discourse relations in both corpora are binary: Arg] and Arg2, where Arg2 is an argument syntactically attached to a discourse connectiv</context>
</contexts>
<marker>Kim, Ohta, Tateisi, Tsujii, 2003</marker>
<rawString>J-D Kim, Tomoko Ohta, Yuka Tateisi, and Junichi Tsujii. 2003. Genia corpusa semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl 1):i180–i182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 15 (NIPS</booktitle>
<contexts>
<context position="11365" citStr="Klein and Manning, 2003" startWordPosition="1728" endWordPosition="1731">ance (Stepanov and Riccardi, 2013; Lin et al., 2012). With respect to the decision made in this step a specific argument span extraction model is applied. For Argument Position Classification the unigram BoosTexter (Schapire and Singer, 2000) model with 100 iterations is trained on PDTB sections 02-22 and tested on sections 23-24. Similar to the previously published results, it has a high performace: F1 = 98.12. The features are connective surface string, POS-tags, and IOBchains. The results obtained with automatic sentence splitting, tokenization, and syntactic parsing using Stanford Parser (Klein and Manning, 2003) are also high F1 = 97.81. Since, unlike PTB for PDTB, for BioDRB there is no manual sentence splitting, tokenization, and syntactic tree annotation; the precise cross-domain evaluation of Argument Span Extraction step is not possible. However, in Section 4 we estimate the performance using automatic sentence splitting. 3.2 Argument Span Extraction Argument span extraction is cast as token-level sequence labeling using Conditional Random Fields (CRF) (Lafferty et al., 2001). Previously, it was observed that in PDTB for inter-sentential discourse relations Arg] precedes Arg2 in most of the case</context>
<context position="18046" citStr="Klein and Manning, 2003" startWordPosition="2865" endWordPosition="2868">r in crossdomain evaluation on BioDRB, since there is no reference information. Additionally, while Arg] span extraction models are trained on Gold Arg2 features, for testing they are always automatic. 4.2 Cross-Domain Argument Position Classification As it was mentioned above, there is no manual sentence splitting for BioDRB; thus, there is no references for whether a discourse relation has its Arg] in the same or different sentences. In order to evaluate cross-domain argument position classification we evaluate classifier decisions against automatic sentence splitting using Stanford Parser (Klein and Manning, 2003) on whole of BioDRB. The BoosTexter model described in Section 3.1 has a high in-domain performance of 97.81. On BioDRB its performance is 95.26, which is still high. Thus, we can conclude that argument position classification generalizes well cross-domain, and that it is little affected by the presence of ‘subordinators’ that were not annotated in PDTB. 4.3 In-Domain Argument Span Extraction: PDTB The in-domain performance of the argument span extraction models trained on PDTB sections 02-22 and tested on sections 23-24 is given on Table 2. The results are for 2 settings: ‘Gold’ and ‘Auto’. I</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Advances in Neural Information Processing Systems 15 (NIPS 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of 18th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="11843" citStr="Lafferty et al., 2001" startWordPosition="1799" endWordPosition="1802">ins. The results obtained with automatic sentence splitting, tokenization, and syntactic parsing using Stanford Parser (Klein and Manning, 2003) are also high F1 = 97.81. Since, unlike PTB for PDTB, for BioDRB there is no manual sentence splitting, tokenization, and syntactic tree annotation; the precise cross-domain evaluation of Argument Span Extraction step is not possible. However, in Section 4 we estimate the performance using automatic sentence splitting. 3.2 Argument Span Extraction Argument span extraction is cast as token-level sequence labeling using Conditional Random Fields (CRF) (Lafferty et al., 2001). Previously, it was observed that in PDTB for inter-sentential discourse relations Arg] precedes Arg2 in most of the cases. Thus, the CRF models are trained for the configurations where both of the arguments are in the same sentence (SS), and for Arg] in one of the previous sentences (PS); the following sentence Arg] case (FS) is ignored due to too few training instances being available (in PDTB 8 / 18,459). Consequently, there are 4 CRF models SS Arg1 and Arg2, and PS Arg1 and Arg2. Same sentence case models are applied in a cascade, such that output of Arg2 model is used as a feature for Ar</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of 18th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A pdtb-styled end-to-end discourse parser.</title>
<date>2012</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<contexts>
<context position="1545" citStr="Lin et al., 2012" startWordPosition="218" endWordPosition="221">hniques on the argument span extraction subtask. We demonstrate that the subtask generalizes well across domains. 1 Introduction Discourse analysis is one of the most challenging tasks in Natural Language Processing that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc. (see (Webber et al., 2011) and (Taboada and Mann, 2006) for detailed review). The release of the large discourse relation annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008), marked the development of statistical discourse parsers (Lin et al., 2012; Ghosh et al., 2011; Xu et al., 2012; Stepanov and Riccardi, 2013). Recently, PDTB-style discourse annotation was applied to biomedical domain and Biomedical Discourse Relation Bank (BioDRB) (Prasad et al., 2011) was released. This milestone marks the beginning of the research on cross-domain evaluation and domain adaptation of PDTB-style discourse parsers. In this paper we address the question of how well PDTB-trained discourse parser (news-wire domain) can extract argument spans of explicit discourse relations in BioDRB (biomedical domain). The use cases of discourse parsing in biomedical d</context>
<context position="3333" citStr="Lin et al., 2012" startWordPosition="500" endWordPosition="503">ce HeLa cells do not express Oct2. (Cause:Reason) In the example, the discourse connective since signals a causal relation between the clauses it connects. That is, the reason why ‘the addition of an anti-Oct2 antibody did not interfere with complex formation’ is ‘HeLa cells’ not expressing Oct2’. PDTB adopts non-hierarchical binary view on discourse relations: Argument 1 (Arg]) (in italics in the example) and Argument 2 (Arg2), which is syntactically attached to a discourse connective (in bold). Thus, a discourse relation is a triplet of a connective and its two arguments. In the literature (Lin et al., 2012; Stepanov and Riccardi, 2013) PDTB-style discourse parsing is partitioned into discourse relation detection, argument position classification, argument span extraction, and relation sense classification. For the explicit discourse relations (i.e. signaled by a connective), discourse relation detection is cast as classification of connectives as discourse and non-discourse. Argument position classification, on the other hand, involves detection of the location of Arg] with re30 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pag</context>
<context position="10793" citStr="Lin et al., 2012" startWordPosition="1639" endWordPosition="1642">s in these configurations. In the following subsections we provide descriptions for each of the components. 3.1 Argument Position Classification Discourse connectives have a very strong preference on the location of the Arg] with respect to their syntactic category (Subordinating Conjunction, Coordinating Conjunction, and Discourse Adverbial) and position in the sentence (sentence initial or sentence medial); thus, classification of discourse connectives into inter-sentential or intrasentential is an easy task yielding high supervised machine learning performance (Stepanov and Riccardi, 2013; Lin et al., 2012). With respect to the decision made in this step a specific argument span extraction model is applied. For Argument Position Classification the unigram BoosTexter (Schapire and Singer, 2000) model with 100 iterations is trained on PDTB sections 02-22 and tested on sections 23-24. Similar to the previously published results, it has a high performace: F1 = 98.12. The features are connective surface string, POS-tags, and IOBchains. The results obtained with automatic sentence splitting, tokenization, and syntactic parsing using Stanford Parser (Klein and Manning, 2003) are also high F1 = 97.81. S</context>
<context position="15800" citStr="Lin et al., 2012" startWordPosition="2491" endWordPosition="2494"> sets for Arg2 and Arg1 argument span extraction. The experimental settings for PDTB are the following: Sections 02-22 are used for training and Sections 23-24 for testing. For BioDRB, on the other hand, 12 fold cross-validation is used (2 documents in each fold, since in BioDRB there are 24 documents). 4.1 Evaluation Methodology The performance of Argument Span Extraction is evaluated in terms of precision (p), recall (r), and F-measure (Fl) using the equations 1 – 3. An argument span is considered to be correct, if it exactly matches the reference string. Following (Ghosh et al., 2011) and (Lin et al., 2012), argument initial and final punctuation marks are removed. Exact Match p = (1) Exact Match + No Match r = Exact Match (2) References in Gold _ 2 ∗ p ∗ r F1 (3) p + r In the equations, Exact Match is the count of correctly tagged argument spans; No Match is the count of argument spans that do not match the reference string exactly, i.e. even a single token difference is counted as an error; and References in Gold is the total number of arguments in the reference. Since argument span extraction is applied after argument position classification, the classification error is propagated. Thus, for </context>
</contexts>
<marker>Lin, Ng, Kan, 2012</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012. A pdtb-styled end-to-end discourse parser. Natural Language Engineering, 1:1 – 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The penn discourse treebank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="1470" citStr="Prasad et al., 2008" startWordPosition="207" endWordPosition="210">ined discourse relation parser and evaluate feature-level domain adaptation techniques on the argument span extraction subtask. We demonstrate that the subtask generalizes well across domains. 1 Introduction Discourse analysis is one of the most challenging tasks in Natural Language Processing that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc. (see (Webber et al., 2011) and (Taboada and Mann, 2006) for detailed review). The release of the large discourse relation annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008), marked the development of statistical discourse parsers (Lin et al., 2012; Ghosh et al., 2011; Xu et al., 2012; Stepanov and Riccardi, 2013). Recently, PDTB-style discourse annotation was applied to biomedical domain and Biomedical Discourse Relation Bank (BioDRB) (Prasad et al., 2011) was released. This milestone marks the beginning of the research on cross-domain evaluation and domain adaptation of PDTB-style discourse parsers. In this paper we address the question of how well PDTB-trained discourse parser (news-wire domain) can extract argument spans of explicit discourse relations in Bio</context>
<context position="5814" citStr="Prasad et al., 2008" startWordPosition="879" endWordPosition="882">ication as far as the data allows, since BioDRB lacks manual sentence segmentation. The paper is structured as follows. In Section 2 we present the comparative analysis of PDTB and BioDRB corpora and the relevant works on crossdomain discourse parsing. In Section 3 we describe the PDTB discourse parser used for crossdomain experiments. In Section 4 we present the evaluation methodology and the experimental results. Section 5 provides concluding remarks. 2 PDTB vs. BioDRB Corpora Analysis and Related Cross-Domain Works The two corpora used in our experiments are Penn Discourse Treebank (PDTB) (Prasad et al., 2008) 1Made available on https://code.google.com/ p/discourse-parser/ and Biomedical Discourse Relation Bank (BioDRB) (Prasad et al., 2011). Both corpora follow the same discourse relation annotation style over different domain corpora: PDTB is annotated on top of Wall Street Journal (WSJ) corpus (financial news-wire domain); and it is aligned with Penn Treebank (PTB) syntactic tree annotation; BioDRB, on the other hand, is a corpus annotated over 24 open access full-text articles from the GENIA corpus (Kim et al., 2003) (biomedical domain), and, unlike PDTB, there is no reference tokenization or s</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The penn discourse treebank 2.0. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Susan McRoy</author>
<author>Nadya Frid</author>
<author>Aravind Joshi</author>
<author>Hong Yu</author>
</authors>
<title>The biomedical discourse relation bank.</title>
<date>2011</date>
<journal>BMC Bioinformatics,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="1758" citStr="Prasad et al., 2011" startWordPosition="250" endWordPosition="253">essing that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc. (see (Webber et al., 2011) and (Taboada and Mann, 2006) for detailed review). The release of the large discourse relation annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008), marked the development of statistical discourse parsers (Lin et al., 2012; Ghosh et al., 2011; Xu et al., 2012; Stepanov and Riccardi, 2013). Recently, PDTB-style discourse annotation was applied to biomedical domain and Biomedical Discourse Relation Bank (BioDRB) (Prasad et al., 2011) was released. This milestone marks the beginning of the research on cross-domain evaluation and domain adaptation of PDTB-style discourse parsers. In this paper we address the question of how well PDTB-trained discourse parser (news-wire domain) can extract argument spans of explicit discourse relations in BioDRB (biomedical domain). The use cases of discourse parsing in biomedical domain are discussed in detail in (Prasad et al., 2011). Here, on the other hand, we provide very general connection between the two. The goal of Biomedical Text Mining (BioNLP) is to retrieve and organize biomedic</context>
<context position="4698" citStr="Prasad et al., 2011" startWordPosition="704" endWordPosition="707">gument Span Extraction models are in bold.) spect to Arg2, that is to detect whether a relation is inter- or intra- sentential. Argument span extraction is the extraction (labeling) of text segments that belong to each of the arguments. Finally, relation sense classification is the annotation of relations with the senses from the sense hierarchy (PDTB or BioDRB). To the best of our knowledge, the only subtasks that were addressed cross-domain are the detection of explicit discourse connectives (Ramesh and Yu, 2010; Ramesh et al., 2012; Faiz and Mercer, 2013) and relation sense classification (Prasad et al., 2011). While the discourse parser of Faiz and Mercer (2013)1 provides models for both domains and does identification of argument head words in the style of Wellner and Pustejovsky (2007); there is no decision made on arguments spans. Moreover, there is no cross-domain evaluation available for each of the models. In this paper we address the task of cross-domain argument span extraction of explicit discourse relations. Additionally, we provide evaluation for cross-domain argument position classification as far as the data allows, since BioDRB lacks manual sentence segmentation. The paper is structu</context>
<context position="5948" citStr="Prasad et al., 2011" startWordPosition="895" endWordPosition="898">e present the comparative analysis of PDTB and BioDRB corpora and the relevant works on crossdomain discourse parsing. In Section 3 we describe the PDTB discourse parser used for crossdomain experiments. In Section 4 we present the evaluation methodology and the experimental results. Section 5 provides concluding remarks. 2 PDTB vs. BioDRB Corpora Analysis and Related Cross-Domain Works The two corpora used in our experiments are Penn Discourse Treebank (PDTB) (Prasad et al., 2008) 1Made available on https://code.google.com/ p/discourse-parser/ and Biomedical Discourse Relation Bank (BioDRB) (Prasad et al., 2011). Both corpora follow the same discourse relation annotation style over different domain corpora: PDTB is annotated on top of Wall Street Journal (WSJ) corpus (financial news-wire domain); and it is aligned with Penn Treebank (PTB) syntactic tree annotation; BioDRB, on the other hand, is a corpus annotated over 24 open access full-text articles from the GENIA corpus (Kim et al., 2003) (biomedical domain), and, unlike PDTB, there is no reference tokenization or syntactic parse trees. The detailed comparison of the corpora is out of the scope of this paper, and it is available in (Prasad et al.,</context>
<context position="7669" citStr="Prasad et al., 2011" startWordPosition="1177" endWordPosition="1180"> or following (FS case) sentences. A discourse connective is a member of a well defined list of connectives and a relation expressed via such connective is an Explicit relation. There are other types of discourse and non-discourse relations annotated in the corpora; however, they are out of the scope of this paper. Discourse relations are annotated using a hierarchy of senses: even though the organization of senses and the number of levels are different between corpora, the most general top level senses are mapped to the PDTB top level senses: Comparison, Contingency, Expansion, and Temporal (Prasad et al., 2011). The difference between the two corpora with respect to discourse connectives is that in case of PDTB the annotated connectives belong to one of the three syntactic classes: subordinating conjunctions (e.g. because), coordinating conjunctions (e.g. but), and discourse adverbials (e.g. however), while BioDRB is also annotated for a forth syntactic class – subordinators (e.g. by). There are 100 unique connective types in PDTB (after connectives like ] year after are stemmed to after) in 18,459 explicit discourse relations. Whereas in BioDRB there are 123 unique connective types in 2,636 relatio</context>
<context position="9180" citStr="Prasad et al., 2011" startWordPosition="1409" endWordPosition="1412">ctive types in BioDRB. Additionally, 11% of connective types in common syntactic classes that occur in BioDRB do not occur in PDTB; e.g. In summary, as a consequence. Thus, only 56% of connective types of BioDRB are common to both corpora. While in-domain discourse connective detection has good performance (Ramesh and Yu, 2010), this difference makes the cross-domain identification of discourse connectives a hard task, which is exemplified by experiments in (Ramesh and Yu, 2010) (F1 = 0.55). With respect to relation sense classification, the connective surface provides already high baselines (Prasad et al., 2011). However, cross-domain sense classification experiments indicate that there are significant differences in the semantic usage of connectives between two domains, since the performance of the classifier trained on PDTB does not generalize well to BioDRB (F1 = 0.57). To sum up, the corpora differences with respect to discourse connective usage affect the crossdomain generalization of connective detection and sense classification tasks negatively. The experiments in this paper are intended to evaluate the generalization of argument span extraction, assuming that the connective is already identif</context>
<context position="24748" citStr="Prasad et al., 2011" startWordPosition="3935" endWordPosition="3938">h. Thus, PDTB lexical features might not be very effective, and the models generalize well due to syntactic features. To test this hypothesis we train additional PDTB models on only syntactic features: POS-tags and IOB-chain and ‘connective labels’ – ‘CONN’ suffixed for the Beginning (B), Inside (I) or End (E) of the connective span, simulating discourse connective detection output. Moreover, we reduce the feature set to unigrams only (recall that features were enriched by 2 and 3 grams), such that the models become very general. Even though BioDRB connective senses can be mapped to PDTB, in (Prasad et al., 2011) it was observed that relation sense classification does not generalize well. To reduce the dependency of argument span extraction models on relation sense classification, the connective sense feature in the 35 Arg2 Arg1 P R F1 P R F1 Baseline SS 80.37 76.58 78.42 60.82 56.40 58.52 PS 80.73 80.50 80.62 57.74 52.95 55.19 ALL 80.53 77.71 79.09 59.76 55.29 57.43 Syntactic SS 82.00 75.03 78.33 61.07 51.80 56.01 PS 75.56 74.47 75.01 56.64 46.66 51.11 ALL 80.31 74.98 77.54 59.69 50.42 54.63 No Relation Sense SS 81.35 74.00 77.47 62.46 56.11 59.10 PS 80.35 80.13 80.24 57.58 52.25 54.74 ALL 81.16 75.6</context>
</contexts>
<marker>Prasad, McRoy, Frid, Joshi, Yu, 2011</marker>
<rawString>Rashmi Prasad, Susan McRoy, Nadya Frid, Aravind Joshi, and Hong Yu. 2011. The biomedical discourse relation bank. BMC Bioinformatics, 12(1):188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balaji Polepalli Ramesh</author>
<author>Hong Yu</author>
</authors>
<title>Identifying discourse connectives in biomedical text.</title>
<date>2010</date>
<booktitle>AMIA Annual Symposium Proceedings,</booktitle>
<pages>2010--657</pages>
<contexts>
<context position="4597" citStr="Ramesh and Yu, 2010" startWordPosition="688" endWordPosition="691">4. c�2014 Association for Computational Linguistics Figure 1: Discourse Parser Architecture. (CRF Argument Span Extraction models are in bold.) spect to Arg2, that is to detect whether a relation is inter- or intra- sentential. Argument span extraction is the extraction (labeling) of text segments that belong to each of the arguments. Finally, relation sense classification is the annotation of relations with the senses from the sense hierarchy (PDTB or BioDRB). To the best of our knowledge, the only subtasks that were addressed cross-domain are the detection of explicit discourse connectives (Ramesh and Yu, 2010; Ramesh et al., 2012; Faiz and Mercer, 2013) and relation sense classification (Prasad et al., 2011). While the discourse parser of Faiz and Mercer (2013)1 provides models for both domains and does identification of argument head words in the style of Wellner and Pustejovsky (2007); there is no decision made on arguments spans. Moreover, there is no cross-domain evaluation available for each of the models. In this paper we address the task of cross-domain argument span extraction of explicit discourse relations. Additionally, we provide evaluation for cross-domain argument position classifica</context>
<context position="8889" citStr="Ramesh and Yu, 2010" startWordPosition="1367" endWordPosition="1370">ns. According to PS Argument Position Classif. SS SS Arg2 Extraction SS Arg1 Extraction PS Arg2 Extraction Discourse Connective Detection PS Arg1 Candidate Heuristic PS Arg1 Extraction 31 the discourse connective analysis in (Ramesh et al., 2012), the subordinators comprise 33% of all connective types in BioDRB. Additionally, 11% of connective types in common syntactic classes that occur in BioDRB do not occur in PDTB; e.g. In summary, as a consequence. Thus, only 56% of connective types of BioDRB are common to both corpora. While in-domain discourse connective detection has good performance (Ramesh and Yu, 2010), this difference makes the cross-domain identification of discourse connectives a hard task, which is exemplified by experiments in (Ramesh and Yu, 2010) (F1 = 0.55). With respect to relation sense classification, the connective surface provides already high baselines (Prasad et al., 2011). However, cross-domain sense classification experiments indicate that there are significant differences in the semantic usage of connectives between two domains, since the performance of the classifier trained on PDTB does not generalize well to BioDRB (F1 = 0.57). To sum up, the corpora differences with re</context>
<context position="22701" citStr="Ramesh and Yu, 2010" startWordPosition="3592" endWordPosition="3595">n, results are reported without error propagation from argument position classification step. The first observation from cross-domain evaluation is that argument span extraction generalizes to biomedical domain much better that the discourse parsing subtasks of discourse connective detection and relation sense classification. Unlike those subtasks, the difference between in-domain BioDRB argument span extraction models and the models trained on PDTB is much less: e.g. for discourse connective detection the in-domain and cross-domain difference for BioDRB is 14 points (f-measures 69 and 55 in (Ramesh and Yu, 2010)), and for argument span extraction 2 and 4 points for Arg2 and Arg1 respectively (see Tables 3 &amp; 4). The difference between the models trained on automatic and gold parse trees is also not high, and gold feature trained models perform better with Arg2 Arg1 P R F1 P R F1 Gold SS 80.37 76.58 78.42 60.82 56.40 58.52 PS 80.73 80.50 80.62 57.74 52.95 55.19 ALL 80.53 77.71 79.09 59.76 55.29 57.43 Auto SS 77.60 75.05 76.30 60.76 55.21 57.83 PS 81.39 81.23 81.31 57.71 51.72 54.47 ALL 78.72 76.80 77.74 59.60 54.12 56.71 Table 4: Cross-domain performance of the PDTBtrained argument span extraction mode</context>
</contexts>
<marker>Ramesh, Yu, 2010</marker>
<rawString>Balaji Polepalli Ramesh and Hong Yu. 2010. Identifying discourse connectives in biomedical text. AMIA Annual Symposium Proceedings, 2010:657.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balaji Polepalli Ramesh</author>
<author>Rashmi Prasad</author>
<author>Tim Miller</author>
<author>Brian Harrington</author>
<author>Hong Yu</author>
</authors>
<title>Automatic discourse connective detection in biomedical text.</title>
<date>2012</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>19</volume>
<issue>5</issue>
<contexts>
<context position="4618" citStr="Ramesh et al., 2012" startWordPosition="692" endWordPosition="695"> for Computational Linguistics Figure 1: Discourse Parser Architecture. (CRF Argument Span Extraction models are in bold.) spect to Arg2, that is to detect whether a relation is inter- or intra- sentential. Argument span extraction is the extraction (labeling) of text segments that belong to each of the arguments. Finally, relation sense classification is the annotation of relations with the senses from the sense hierarchy (PDTB or BioDRB). To the best of our knowledge, the only subtasks that were addressed cross-domain are the detection of explicit discourse connectives (Ramesh and Yu, 2010; Ramesh et al., 2012; Faiz and Mercer, 2013) and relation sense classification (Prasad et al., 2011). While the discourse parser of Faiz and Mercer (2013)1 provides models for both domains and does identification of argument head words in the style of Wellner and Pustejovsky (2007); there is no decision made on arguments spans. Moreover, there is no cross-domain evaluation available for each of the models. In this paper we address the task of cross-domain argument span extraction of explicit discourse relations. Additionally, we provide evaluation for cross-domain argument position classification as far as the da</context>
<context position="8515" citStr="Ramesh et al., 2012" startWordPosition="1307" endWordPosition="1310">ting conjunctions (e.g. but), and discourse adverbials (e.g. however), while BioDRB is also annotated for a forth syntactic class – subordinators (e.g. by). There are 100 unique connective types in PDTB (after connectives like ] year after are stemmed to after) in 18,459 explicit discourse relations. Whereas in BioDRB there are 123 unique connective types in 2,636 relations. According to PS Argument Position Classif. SS SS Arg2 Extraction SS Arg1 Extraction PS Arg2 Extraction Discourse Connective Detection PS Arg1 Candidate Heuristic PS Arg1 Extraction 31 the discourse connective analysis in (Ramesh et al., 2012), the subordinators comprise 33% of all connective types in BioDRB. Additionally, 11% of connective types in common syntactic classes that occur in BioDRB do not occur in PDTB; e.g. In summary, as a consequence. Thus, only 56% of connective types of BioDRB are common to both corpora. While in-domain discourse connective detection has good performance (Ramesh and Yu, 2010), this difference makes the cross-domain identification of discourse connectives a hard task, which is exemplified by experiments in (Ramesh and Yu, 2010) (F1 = 0.55). With respect to relation sense classification, the connect</context>
</contexts>
<marker>Ramesh, Prasad, Miller, Harrington, Yu, 2012</marker>
<rawString>Balaji Polepalli Ramesh, Rashmi Prasad, Tim Miller, Brian Harrington, and Hong Yu. 2012. Automatic discourse connective detection in biomedical text. Journal of the American Medical Informatics Association, 19(5):800–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Boostexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="10983" citStr="Schapire and Singer, 2000" startWordPosition="1668" endWordPosition="1671">rong preference on the location of the Arg] with respect to their syntactic category (Subordinating Conjunction, Coordinating Conjunction, and Discourse Adverbial) and position in the sentence (sentence initial or sentence medial); thus, classification of discourse connectives into inter-sentential or intrasentential is an easy task yielding high supervised machine learning performance (Stepanov and Riccardi, 2013; Lin et al., 2012). With respect to the decision made in this step a specific argument span extraction model is applied. For Argument Position Classification the unigram BoosTexter (Schapire and Singer, 2000) model with 100 iterations is trained on PDTB sections 02-22 and tested on sections 23-24. Similar to the previously published results, it has a high performace: F1 = 98.12. The features are connective surface string, POS-tags, and IOBchains. The results obtained with automatic sentence splitting, tokenization, and syntactic parsing using Stanford Parser (Klein and Manning, 2003) are also high F1 = 97.81. Since, unlike PTB for PDTB, for BioDRB there is no manual sentence splitting, tokenization, and syntactic tree annotation; the precise cross-domain evaluation of Argument Span Extraction step</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Robert E. Schapire and Yoram Singer. 2000. Boostexter: A boosting-based system for text categorization. Machine Learning, 39(2/3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny A Stepanov</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Comparative evaluation of argument extraction algorithms in discourse relation parsing.</title>
<date>2013</date>
<booktitle>In 13th International Conference on Parsing Technologies (IWPT</booktitle>
<pages>36--44</pages>
<contexts>
<context position="1612" citStr="Stepanov and Riccardi, 2013" startWordPosition="230" endWordPosition="233">onstrate that the subtask generalizes well across domains. 1 Introduction Discourse analysis is one of the most challenging tasks in Natural Language Processing that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc. (see (Webber et al., 2011) and (Taboada and Mann, 2006) for detailed review). The release of the large discourse relation annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008), marked the development of statistical discourse parsers (Lin et al., 2012; Ghosh et al., 2011; Xu et al., 2012; Stepanov and Riccardi, 2013). Recently, PDTB-style discourse annotation was applied to biomedical domain and Biomedical Discourse Relation Bank (BioDRB) (Prasad et al., 2011) was released. This milestone marks the beginning of the research on cross-domain evaluation and domain adaptation of PDTB-style discourse parsers. In this paper we address the question of how well PDTB-trained discourse parser (news-wire domain) can extract argument spans of explicit discourse relations in BioDRB (biomedical domain). The use cases of discourse parsing in biomedical domain are discussed in detail in (Prasad et al., 2011). Here, on th</context>
<context position="3363" citStr="Stepanov and Riccardi, 2013" startWordPosition="504" endWordPosition="507">ot express Oct2. (Cause:Reason) In the example, the discourse connective since signals a causal relation between the clauses it connects. That is, the reason why ‘the addition of an anti-Oct2 antibody did not interfere with complex formation’ is ‘HeLa cells’ not expressing Oct2’. PDTB adopts non-hierarchical binary view on discourse relations: Argument 1 (Arg]) (in italics in the example) and Argument 2 (Arg2), which is syntactically attached to a discourse connective (in bold). Thus, a discourse relation is a triplet of a connective and its two arguments. In the literature (Lin et al., 2012; Stepanov and Riccardi, 2013) PDTB-style discourse parsing is partitioned into discourse relation detection, argument position classification, argument span extraction, and relation sense classification. For the explicit discourse relations (i.e. signaled by a connective), discourse relation detection is cast as classification of connectives as discourse and non-discourse. Argument position classification, on the other hand, involves detection of the location of Arg] with re30 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 30–37, Gothenburg, Sweden, </context>
<context position="10774" citStr="Stepanov and Riccardi, 2013" startWordPosition="1634" endWordPosition="1638">dels for each of the arguments in these configurations. In the following subsections we provide descriptions for each of the components. 3.1 Argument Position Classification Discourse connectives have a very strong preference on the location of the Arg] with respect to their syntactic category (Subordinating Conjunction, Coordinating Conjunction, and Discourse Adverbial) and position in the sentence (sentence initial or sentence medial); thus, classification of discourse connectives into inter-sentential or intrasentential is an easy task yielding high supervised machine learning performance (Stepanov and Riccardi, 2013; Lin et al., 2012). With respect to the decision made in this step a specific argument span extraction model is applied. For Argument Position Classification the unigram BoosTexter (Schapire and Singer, 2000) model with 100 iterations is trained on PDTB sections 02-22 and tested on sections 23-24. Similar to the previously published results, it has a high performace: F1 = 98.12. The features are connective surface string, POS-tags, and IOBchains. The results obtained with automatic sentence splitting, tokenization, and syntactic parsing using Stanford Parser (Klein and Manning, 2003) are also</context>
<context position="14062" citStr="Stepanov and Riccardi, 2013" startWordPosition="2198" endWordPosition="2201">ture from syntactic trees. • PDTB Level 1 Connective sense (CONN) is the most general sense of a connective in PDTB sense hierarchy. It’s general purpose is to label the discourse connective tokens, i.e. the value of the feature is ‘NULL’ for all tokens except the discourse connective. • Boolean Main Verb (BMV) is a boolean feature that indicates whether a token is a main verb of a sentence or not (Yamada and Matsumoto, 2003). • Arg2 Label (ARG2) is an output of Arg2 span extraction model, that is used as a feature for Arg1 span extraction. Arg2 span is easier to identify (Ghosh et al., 2011; Stepanov and Riccardi, 2013) since it is syntactically attached to the discourse connective. Thus, this feature serves to constrain the Arg1 search space for intra-sentential argument span extraction. The value of the feature is either ARG2 suffixed for whether a token is Inside (I), Begin (B), or End (E) of the span, or ‘O’ if it does not belong to the Arg2 span. These features are expanded during training with n-grams (feature of CRF++2): tokens with 2-grams in the window of +1 tokens, and the rest of the features with 2 &amp; 3-grams in the window of +2 tokens. The in-domain performance of argument span extraction models </context>
</contexts>
<marker>Stepanov, Riccardi, 2013</marker>
<rawString>Evgeny A. Stepanov and Giuseppe Riccardi. 2013. Comparative evaluation of argument extraction algorithms in discourse relation parsing. In 13th International Conference on Parsing Technologies (IWPT 2013), pages 36–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>William C Mann</author>
</authors>
<date>2006</date>
<booktitle>Applications of rhetorical structure theory. Discourse Studies,</booktitle>
<pages>8--567</pages>
<contexts>
<context position="1324" citStr="Taboada and Mann, 2006" startWordPosition="183" endWordPosition="186">ticular interest due to the availability of Biomedical Discourse Relation Bank (BioDRB). In this paper we present cross-domain evaluation of PDTB trained discourse relation parser and evaluate feature-level domain adaptation techniques on the argument span extraction subtask. We demonstrate that the subtask generalizes well across domains. 1 Introduction Discourse analysis is one of the most challenging tasks in Natural Language Processing that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc. (see (Webber et al., 2011) and (Taboada and Mann, 2006) for detailed review). The release of the large discourse relation annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008), marked the development of statistical discourse parsers (Lin et al., 2012; Ghosh et al., 2011; Xu et al., 2012; Stepanov and Riccardi, 2013). Recently, PDTB-style discourse annotation was applied to biomedical domain and Biomedical Discourse Relation Bank (BioDRB) (Prasad et al., 2011) was released. This milestone marks the beginning of the research on cross-domain evaluation and domain adaptation of PDTB-style discourse parsers. In this paper we a</context>
</contexts>
<marker>Taboada, Mann, 2006</marker>
<rawString>Maite Taboada and William C. Mann. 2006. Applications of rhetorical structure theory. Discourse Studies, (8):567–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie L Webber</author>
<author>Markus Egg</author>
<author>Valia Kordoni</author>
</authors>
<date>2011</date>
<booktitle>Discourse structure and language technology. Natural Language Engineering,</booktitle>
<pages>1--54</pages>
<contexts>
<context position="1295" citStr="Webber et al., 2011" startWordPosition="178" endWordPosition="181">iomedical domain is of particular interest due to the availability of Biomedical Discourse Relation Bank (BioDRB). In this paper we present cross-domain evaluation of PDTB trained discourse relation parser and evaluate feature-level domain adaptation techniques on the argument span extraction subtask. We demonstrate that the subtask generalizes well across domains. 1 Introduction Discourse analysis is one of the most challenging tasks in Natural Language Processing that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc. (see (Webber et al., 2011) and (Taboada and Mann, 2006) for detailed review). The release of the large discourse relation annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008), marked the development of statistical discourse parsers (Lin et al., 2012; Ghosh et al., 2011; Xu et al., 2012; Stepanov and Riccardi, 2013). Recently, PDTB-style discourse annotation was applied to biomedical domain and Biomedical Discourse Relation Bank (BioDRB) (Prasad et al., 2011) was released. This milestone marks the beginning of the research on cross-domain evaluation and domain adaptation of PDTB-style discours</context>
</contexts>
<marker>Webber, Egg, Kordoni, 2011</marker>
<rawString>Bonnie L. Webber, Markus Egg, and Valia Kordoni. 2011. Discourse structure and language technology. Natural Language Engineering, pages 1 – 54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Wellner</author>
<author>James Pustejovsky</author>
</authors>
<title>Automatically identifying the arguments of discourse connectives.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<contexts>
<context position="4880" citStr="Wellner and Pustejovsky (2007)" startWordPosition="733" endWordPosition="736">labeling) of text segments that belong to each of the arguments. Finally, relation sense classification is the annotation of relations with the senses from the sense hierarchy (PDTB or BioDRB). To the best of our knowledge, the only subtasks that were addressed cross-domain are the detection of explicit discourse connectives (Ramesh and Yu, 2010; Ramesh et al., 2012; Faiz and Mercer, 2013) and relation sense classification (Prasad et al., 2011). While the discourse parser of Faiz and Mercer (2013)1 provides models for both domains and does identification of argument head words in the style of Wellner and Pustejovsky (2007); there is no decision made on arguments spans. Moreover, there is no cross-domain evaluation available for each of the models. In this paper we address the task of cross-domain argument span extraction of explicit discourse relations. Additionally, we provide evaluation for cross-domain argument position classification as far as the data allows, since BioDRB lacks manual sentence segmentation. The paper is structured as follows. In Section 2 we present the comparative analysis of PDTB and BioDRB corpora and the relevant works on crossdomain discourse parsing. In Section 3 we describe the PDTB</context>
</contexts>
<marker>Wellner, Pustejovsky, 2007</marker>
<rawString>Ben Wellner and James Pustejovsky. 2007. Automatically identifying the arguments of discourse connectives. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Xu</author>
<author>Qiao Ming Zhu</author>
<author>Guo Dong Zhou</author>
</authors>
<title>A unified framework for discourse argument identification via shallow semantic parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of 24th International Conference on Computational Linguistics (COLING 2012): Posters.</booktitle>
<contexts>
<context position="1582" citStr="Xu et al., 2012" startWordPosition="226" endWordPosition="229">n subtask. We demonstrate that the subtask generalizes well across domains. 1 Introduction Discourse analysis is one of the most challenging tasks in Natural Language Processing that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc. (see (Webber et al., 2011) and (Taboada and Mann, 2006) for detailed review). The release of the large discourse relation annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008), marked the development of statistical discourse parsers (Lin et al., 2012; Ghosh et al., 2011; Xu et al., 2012; Stepanov and Riccardi, 2013). Recently, PDTB-style discourse annotation was applied to biomedical domain and Biomedical Discourse Relation Bank (BioDRB) (Prasad et al., 2011) was released. This milestone marks the beginning of the research on cross-domain evaluation and domain adaptation of PDTB-style discourse parsers. In this paper we address the question of how well PDTB-trained discourse parser (news-wire domain) can extract argument spans of explicit discourse relations in BioDRB (biomedical domain). The use cases of discourse parsing in biomedical domain are discussed in detail in (Pra</context>
</contexts>
<marker>Xu, Zhu, Zhou, 2012</marker>
<rawString>Fan Xu, Qiao Ming Zhu, and Guo Dong Zhou. 2012. A unified framework for discourse argument identification via shallow semantic parsing. In Proceedings of 24th International Conference on Computational Linguistics (COLING 2012): Posters.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of 8th International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="13863" citStr="Yamada and Matsumoto, 2003" startWordPosition="2161" endWordPosition="2165">rom the root node to the token, prefixed with the information whether a token is at the beginning (B-) or inside (I-) the constituent. The chunklink tool (Buchholz, 2000) is used to extract this feature from syntactic trees. • PDTB Level 1 Connective sense (CONN) is the most general sense of a connective in PDTB sense hierarchy. It’s general purpose is to label the discourse connective tokens, i.e. the value of the feature is ‘NULL’ for all tokens except the discourse connective. • Boolean Main Verb (BMV) is a boolean feature that indicates whether a token is a main verb of a sentence or not (Yamada and Matsumoto, 2003). • Arg2 Label (ARG2) is an output of Arg2 span extraction model, that is used as a feature for Arg1 span extraction. Arg2 span is easier to identify (Ghosh et al., 2011; Stepanov and Riccardi, 2013) since it is syntactically attached to the discourse connective. Thus, this feature serves to constrain the Arg1 search space for intra-sentential argument span extraction. The value of the feature is either ARG2 suffixed for whether a token is Inside (I), Begin (B), or End (E) of the span, or ‘O’ if it does not belong to the Arg2 span. These features are expanded during training with n-grams (feat</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of 8th International Workshop on Parsing Technologies.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
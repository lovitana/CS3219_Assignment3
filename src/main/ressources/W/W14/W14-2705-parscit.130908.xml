<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000940">
<title confidence="0.998014">
A Unified Topic-Style Model for Online Discussions
</title>
<author confidence="0.99959">
Ying Ding, Jing Jiang, Qiming Diao
</author>
<affiliation confidence="0.9463255">
School of Information Systems
Singapore Management University
</affiliation>
<email confidence="0.983966">
{ying.ding.2011, jingjiang, qiming.diao.2010}@smu.edu.sg
</email>
<sectionHeader confidence="0.993518" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986466666667">
Forums have become major places for
online communications for many years,
where people often share and express
opinions. We observe that, when editing
posts, while some people seriously state
their opinions, there are also many peo-
ple playing jokes and writing meaningless
posts on the discussed topics. We design
a unified probabilistic graphical model to
capture both topic-driven words and style-
driven words. The model can help us sepa-
rate serious and unserious posts/users and
identify slang words. An extensive set
of experiments demonstrates the effective-
ness of our model.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991794745454546">
With the fast growth of the popularity of online
social media, people nowadays are very used to
sharing their thoughts and interacting with their
friends on the Internet. Large online social net-
work sites such as Facebook, Twitter and Flickr
have attracted hundreds of millions of users. A-
mong these online social media platforms, forums
have always played an important role with its spe-
cial characteristics. Unlike personal blogs, forum-
s allow many users to engage in online conversa-
tions with a topic focus. Unlike Facebook, forums
are usually open to public and users who post in
forums do not need to reveal too much personal
information. Unlike Wikipedia or Freebase, fo-
rums encourage users to exchange not only factual
information but more importantly subjective opin-
ions. All these characteristics make online forums
a valuable source from which we can retrieve and
summarize the general public’s opinions about a
given topic. This is especially important for busi-
nesses who want to find out how their products
and services have been received and policy mak-
ers who are concerned about people’s opinions on
social issues.
While the freedom with which users can post in
online forums has promoted the popularity of on-
line forums, it has also led to the diversity in post
quality. There are posts which contribute positive-
ly to a discussion by offering relevant, serious and
meaningful opinions, but there are also many posts
which appear irrelevant, disrespectful or meaning-
less. These posts are uninformative, hard to con-
sume and sometimes even destructive. Let us look
at some examples. Table 1 shows two forum posts
in response to a piece of news about GDP bonuses
for senior civil servants in Singapore. We can see
that User A’s post is clearly written. User B’s post,
on the other hand, is hard to comprehend. We see
broken sentences, many punctuation marks such
as “?” and colloquial expressions such as “ha.”
User B is not seriously contributing to the online
discussion but rather trying to make a joke of the
issue. Generally speaking, User B’s post is less
useful than User A’s post in helping us understand
the public’s response to the news.
Senior civil servants to get bumper
GDP bonuses
User A let us ensure this will be the LAST time
they accord themselves ceiling salary s-
cales and bonuses. i suspect MANY cit-
izens are eagerly looking forward to the
GE.
User B Fever night, fever night, fe..ver..
Fever like to do it
Got it?????? Ha..ha..ha...
</bodyText>
<tableCaption confidence="0.988669">
Table 1: Two example online posts.
</tableCaption>
<bodyText confidence="0.999885142857143">
In this work, we opt for a fully unsupervised
approach to modeling this phenomenon in online
discussions. Our solution is based on the observa-
tion that the writing styles of serious posts and un-
serious posts are different, and the writing styles
are often characterized by the words used in the
posts. Moreover, the same user usually exhibits
</bodyText>
<page confidence="0.990538">
33
</page>
<note confidence="0.765907">
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 33–41,
Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.7546443">
User Post
User A Re: Creativity, Art in the eyes of beholder. your
take?
The difference is, the human can get tired or
sick, and then it will affect his work, but the
robot can work 24 hours a day 365 days a year
and yet produce the same every time.
Re: Diesel oil spill turns Manila Bay red, poses
risk to health - ST
The question is, will this environmental haz-
ard turn up on the shores of it neighbors? And
maybe even affect Singapore waters?
User B Re: Will PAP know who i vote in GE?
Hey! Who are you???
You make. ha..ha..ha.. he..he..he..
very angry lah
Re: Gender discrimination must end for Singa-
pore to flourish, says AWARE
Hao nan bu gen nu dou Let you win lah
ha..ha..ha..
</table>
<tableCaption confidence="0.986772">
Table 2: Sample posts of two example users.
</tableCaption>
<bodyText confidence="0.999978586206897">
the same writing style in most of his posts. For
example, Table 2 shows two example users, each
with two sample posts. We can see that their writ-
ing styles are consistent in the two posts. If we
treat each writing style as a latent factor associat-
ed with a word distribution, we can associate ob-
served words with the underlying writing styles.
However, not all words in a post are style-driven.
Many words in forum posts are chosen based on
the topic of the corresponding thread. Our model
therefore jointly considers both topics and writing
styles.
We apply our topic-style model to a real on-
line forum dataset from Singapore. By setting the
number of styles to two, we clearly find that one
writing style corresponds to the more serious posts
while the other corresponds to posts that are not so
serious. This topic-style model also automatically
learns a meaningful slang lexicon. Moreover, we
find that topics discovered by our topic-style mod-
el are more distinctive from each other than topics
produced by standard LDA.
Our contributions in this paper can be summa-
rized as follows: 1) We propose a principled topic-
style model to jointly model topics and writing
styles at the same time in online forums. 2) An
extensive set of experiments shows that our mod-
el is effective in separating the more serious posts
and unserious posts and identifying slang words.
</bodyText>
<sectionHeader confidence="0.999789" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999696301886792">
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) has been shown to be useful for many ap-
plications. Many extensions of LDA have been
designed for different tasks, which are not detailed
here. Our model is also an extension of LDA. We
introduce two types of word distributions, one rep-
resenting topics and the other representing writing
styles. We use switch variables to alternate be-
tween these two types of word distributions. We
also assume an author-level distribution over writ-
ing styles. It is worth pointing out that although
our model bears similarity to a number of oth-
er LDA extensions, our objectives are different
from existing work. E.g., the author topic mod-
el (Rosen-Zvi et al., 2004) also assumes an author-
level distribution over topics, but the author-level
distribution is meant to capture an author’s topical
interests. In contrast, our user-level distribution is
over writing styles and is meant to identify serious
versus unserious users. Similar to the models by
Mei et al. (2007) and Paul et al. (2010) , we also
use switch variables to alternate between different
types of word distributions, but our goal is to iden-
tify words associated with writing styles instead of
sentiment words or perspective words.
Another body of related research is around s-
tudying text quality, formality and sarcasm. Pitler
and Nenkova (2008) investigated different fea-
tures for text readability judgement and empirical-
ly demonstrated that discourse relations are high-
ly correlated with perceived readability. Brooke
et al. (2010) applied Latent Semantic Analysis
to determine the formality level of lexical items.
Agichtein et al. (2008) presented a general classifi-
cation framework incorporating community feed-
back to identify high quality content in social me-
dia. Davidov et al. (2010) proposed the first robust
algorithm for recognition of sarcasm. Gonz´alez-
Ib´a˜nez et al. (2011) took a closer look at sarcasm
in Twitter messages and found that automatic clas-
sification can be as good as human classification.
All these studies mainly rely on supervised tech-
niques and human annotation needs to be done,
which is very time consuming. Our method is ful-
ly unsupervised, which can automatically uncover
different styles and separate serious posts from un-
serious posts.
Our work is also related to spam/spammer de-
tection in social media, which has been studied
over different platforms for a few years. Jindal
and Liu (2008) first studied opinion spam in on-
line reviews and proposed a classification method
for opinion spam detection. Bhattarai et al. (2009)
</bodyText>
<page confidence="0.998377">
34
</page>
<bodyText confidence="0.999781555555556">
investigated different content attributes of com-
ment spam in the Blogsphere and built a detection
system with good performance based on these at-
tributes. Ding et al. (2013) proposed to utilize both
content and social features to detect spams in on-
line question answer website. Existing work on
spam detection need annotated data to learn the s-
pam features but our model does not as it is fully
unsupervised.
</bodyText>
<sectionHeader confidence="0.988338" genericHeader="method">
3 A Topic-Style Model
</sectionHeader>
<bodyText confidence="0.999971206896552">
Writing styles can be reflected in many differen-
t ways. Besides choices of words or expression-
s, many other linguistic features such as sentence
length, sentence complexity and use of punctua-
tion marks may all be associated with one’s writ-
ing style. In this work, however, we try to take
an approach that does not rely on heavy linguistic
analysis or feature engineering. Part of the reason
is that we want our approach to be independent of
language, culture or social norms so that it is ro-
bust and can be easily applied to any online forum.
To this end, we represent a writing style simply
as a distribution over words, much like a topic in
LDA. We assume that there are S latent writing
styles shared by all users contributing to a forum.
Meanwhile, we also assume a different set of T
latent topics. We mix writing styles and topics to
explain the generation of words in forum posts.
A key assumption we have is that the same us-
er tends to maintain a consistent writing style, and
therefore we associate each user with a multinomi-
al distribution over our latent writing styles. This
is similar to associating a document with a distri-
bution over topics in LDA, where the assumption
is that a single document tends to have focused
topics. Another assumption of our model is that
each word in a post is generated from either the
background or a topic or a writing style, as deter-
mined by a binary switch variable.
</bodyText>
<subsectionHeader confidence="0.999224">
3.1 Model Description
</subsectionHeader>
<bodyText confidence="0.999820444444444">
We now formally describe the topic-style model
we propose. The model is depicted in Figure 1.
We assume that there are T latent topics, where
Ot is the word distribution for topic t. There are
S latent writing styles, where ψs is the word dis-
tribution for writing style s. There are E threads,
where each thread a has a topic distribution Be, and
there are U users, where each user u has a writing
style distribution Tru.
</bodyText>
<figureCaption confidence="0.995716">
Figure 1: Topic-Style Model
</figureCaption>
<table confidence="0.948042277777778">
Notation Description
γ, αE, αU, Hyper-parameters of Dirichlet distributions
βB, βT, βS
λ A global multinomial distribution over
switching variables x
θe, πu Thread-specific topic distributions and user-
specific style distributions
φB, φt, ψs Word distributions of background, topics
and styles
xe,p,n, Hidden variables: xe,p,n for switching,
ye,p,n, ye,p,n for style of style words, ze,p,n for
ze,p,n topic of topic words
e, p, n Indices: e for threads, p for posts, n for
words
E, Pe, U, Number of threads, numbers of posts in
Ne,p threads, number of users and numbers of
words in posts
S, K, V Numbers of styles, topics and word types
</table>
<tableCaption confidence="0.999928">
Table 3: Notation used in our model.
</tableCaption>
<bodyText confidence="0.999683833333333">
For each word in a post, first a binary switch
variable x is sampled from a global Bernoulli dis-
tribution parameterized by A. If x = 0, we draw a
word from the background word distribution. Oth-
erwise, if x = 1, we draw a topic from the corre-
sponding thread’s topic distribution; if x = 2, we
draw a writing style from the corresponding user’s
writing style distribution. We then draw the word
from the corresponding word distribution.
The generative process of our model is de-
scribed as follows. The notation we use in the
model is also summarized in Table 3.
</bodyText>
<listItem confidence="0.999641363636364">
• Draw a global multinomial switching variable distribu-
tion λ — Dirichlet(γ).
• Draw a multinomial background word distribution
φB — Dirichlet(βB).
• For each topic t = 1, 2, ... , T, draw a multinomial
topic-word distribution φt — Dirichlet(βT).
• For each writing style s = 1, 2, ... , S, draw a multi-
nomial style-word distribution ψs — Dirichlet(βS).
• For each user u = 1, 2, ... , U, draw a multinomial
style distribution πu — Dirichlet(αu).
• For each thread e = 1, 2, ... , E
</listItem>
<page confidence="0.993934">
35
</page>
<bodyText confidence="0.98343725">
– draw a multinomial topic distribution θe —
Dir(αE).
– for each post p = 1, 2, ... , Pe in the thread,
where ue,p E {1, 2, ... , U} is the user who has
written the post
* for each word n = 1, 2, ... ,Ne,p in the
thread, where we,p,n E {1, 2, ... ,V } is the
word type
</bodyText>
<equation confidence="0.952193111111111">
� draw xe,p,n — Multinomial(λ).
� If x = 0, draw we,p,n —
Multinomial(φB)
� If x = 1, draw ye,p,n —
Multinomial(πue,p), and then draw
we,p,n — Multinomial(ψye,p,n).
� If x = 2, draw ze,p,n —
Multinomial(θe), and then draw
we,p,n — Multinomial(φze,p,n).
</equation>
<subsectionHeader confidence="0.999673">
3.2 Parameters Estimation
</subsectionHeader>
<bodyText confidence="0.999966">
We use Gibbs sampling to estimate the parameters.
The sampling probability that assign the nth word
in post p of thread e to the background topic is as
follows:
</bodyText>
<equation confidence="0.981627">
a(γ + n0) x VβB + n0
</equation>
<bodyText confidence="0.9897764">
where n0 is the number of words assigned as back-
ground words and nwe,p,n Bis the number of times
word type of we,p,n assigned to background. The
probability to assign this word to style s is as fol-
lows:
</bodyText>
<equation confidence="0.9995676">
P(xe,p,n = 1, ye,p,n = s|W, U, X−i, Y −i, Z−i)
αU + ns βS + nwe,p,n
ue,p s
a(γ + n1) x SαU + nue,p x
VβS + ns
</equation>
<bodyText confidence="0.998917222222222">
where n1 is the number of words assigned as style
words, n∗ue,p and nsue,p are the number of words
written by user ue,p and assigned as style words,
and the number of these words assigned to style
s, respectively. n∗s and nwe,p,n
s are the number of
words assigned to style s and the number of times
word type of term we,p,n assigned to style s. The
probability to assign this word topic t is as follows:
</bodyText>
<equation confidence="0.999228666666667">
P(xe,p,n = 2, ze,p,n = t|W, U, X−i, Y −i, Z−i)
αE + ne βT + nt e,p,n
a(γ + n2) x KαE + ne x VβT + nt
</equation>
<bodyText confidence="0.999958">
where n2 is the number of words assigned as topic
words, n∗e is the number of words in thread e as-
signed as topic words, nte is the number of words
in thread e assigned to topic t, n∗t is the number of
words assigned to topic t, and nwe,p,n is the num-
</bodyText>
<equation confidence="0.438693">
t
</equation>
<bodyText confidence="0.9984255">
ber of times word type of we,p,n is assigned to top-
ic t.
After running Gibbs sampling for a number of
iterations, we can estimate the parameters based
on the sampled topic assignments. They can be
calculated by the equations below:
</bodyText>
<equation confidence="0.976020333333333">
φw t = βT + nw t φw s = βS + nw s
VβT + n∗ VβS + n∗
t s
θt e = αE + nt e θs u = αU + ns u
KαE + n∗ SαU + n∗
e u
</equation>
<sectionHeader confidence="0.999379" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.999965">
4.1 Data Set and Experiment Setup
</subsectionHeader>
<bodyText confidence="0.998545">
To evaluate our model, we use forum threads from
AsiaOne1, a popular online forum site in Singa-
pore. We crawled all the threads between January
2011 and June 2013 under a category called “Sin-
gapore,” which is the largest category on AsiaOne.
In the preprocessing stage, we removed the URL-
s, HTML tags and tokenized the text. Emoticons
are kept in our data set as they frequently occur
and indicate users’ emotions. All stop words and
words occurring less than 4 times are deleted. We
also removed users who have fewer than 8 post-
s and threads attracting fewer than 21 posts. The
detailed statistics of the processed dataset are giv-
en in Table 4.
</bodyText>
<table confidence="0.9374865">
#Users #Words #Tokens #Posts/User #Posts/Thread
580 29,619 2,940,886 205.3 69.5
</table>
<tableCaption confidence="0.999404">
Table 4: Detailed statistics of the dataset.
</tableCaption>
<bodyText confidence="0.999953">
We fix the hyper-parameters γ, αE, αU, QT and
QS to be 10,1,1, 0.01 and 0.01 respectively. we
set QB,v to be H · pB(v), where H is set to be 20
and pB(v) is the probability of word v as estimated
from the entire corpus. The number of topics K is
set to be 40 empirically.
</bodyText>
<subsectionHeader confidence="0.994067">
4.2 Model Development
</subsectionHeader>
<bodyText confidence="0.9999632">
Before we evaluate the effectiveness of our model,
we first show how we choose the number of styles
to use. Note that although we are interested in sep-
arating serious and unserious posts, our model can
generally handle any arbitrary number of writing
styles. We therefore vary the number of writing
styles to see which number empirically gives the
most meaningful results.
Assuming that different styles are characterized
by words, we expect to see that the discovered
</bodyText>
<footnote confidence="0.927136">
1http://www.asiaone.com
</footnote>
<equation confidence="0.874222333333333">
P(xe,p,n = 0|W, U, X−i, Y −i, Z−i)
βB + nwe,p,n
B
</equation>
<page confidence="0.856651">
36
</page>
<figure confidence="0.995709857142857">
Serious Unserious
Style Style
singapore lah
people ha
years dont
government stupid
time leh
made ah
year lor
public liao
Average Divergence
19
18
17
16
15
14
13
12
2 3 4 5 6 7 8 9
Number of Styles
</figure>
<figureCaption confidence="0.9567365">
Figure 2: Average Divergence over different num-
bers of styles.
</figureCaption>
<table confidence="0.9974822">
Style No. Top Words
2 Style 1 singapore, people, years, government
styles Style 2 BIGGRIN, TONGUE, lah, ha
3 Style 1 people, make, WINK, good
styles Style 2 singapore, years, government, mr
Style 3 BIGGRIN, TONGUE, lah, ha
4 Style 1 ha, lah, WINK, dont
styles Style 2 singapore, year, mr, years
Style 3 people, good, make, singapore
Style 4 BIGGRIN, TONGUE, EEK, MAD
</table>
<tableCaption confidence="0.999252">
Table 5: Sample style words
</tableCaption>
<bodyText confidence="0.895448166666667">
word distributions for different styles are very d-
ifferent from each other. To measure the distinc-
tion among a set of styles, we define a metric
called Average Divergence (AD) based on KL-
divergence. Average Divergence can be calculated
as follows.
</bodyText>
<equation confidence="0.7593985">
2 AD(S) = N(N − 1) ESKL (si ||sj),
i�=j
</equation>
<bodyText confidence="0.99957435">
where S is a set of style-word distributions, N is
the size of S and si is the i-th distribution in S.
SKL(si||sj) is the symmetric KL divergence be-
tween si and sj (i.e., DKL(si||sj) + DKL(sj||si)).
The higher Average Divergence is, the more dis-
tinctive distributions in S are.
Figure 2 shows the Average Divergence over d-
ifferent numbers of styles. We can clearly see that
the Average Divergence reaches the highest value
when there are only two styles and decreases with
the increase of style number. This means the styles
are mostly distinct from each other when the num-
ber is 2 and their difference decreases when there
are more styles.
To get a better understanding of the differences
of using different numbers of styles, we compare
the top words in each style when the number of
styles is set to be 2, 3 and 4. The results are shown
in Table 5 where all uppercase words represent e-
moticons. From the top words of the first row, we
</bodyText>
<tableCaption confidence="0.978455">
Table 6: Top words of different styles
</tableCaption>
<bodyText confidence="0.999916052631579">
can see that Style 1 is dominated by formal words
while Style 2 is dominated by emoticons like BIG-
GRIN and slang words like “lah” and “ha.” These
two styles are well distinguished from each other
and humans can easily tell the difference between
them. Also, Style 2 is an unserious style character-
ized by emoticons, slang and urban words. Table 6
shows the top words of these 2 styles excluding e-
moticons. From this table, we can observe that
Style 2 has many slang words with high probabil-
ity while top words in Style 1 are all very formal.
However, styles in the second and third rows of
Table 5 are not easily distinguishable from each
other. In these results, there often exist two styles
very similar to the styles in row 1 while the other
styles look like the combination of these two styles
and humans cannot tell their meanings very clear-
ly. Based on these observations, we fix the number
of styles to 2 in the following experiments.
</bodyText>
<figure confidence="0.472009">
Word Length
</figure>
<figureCaption confidence="0.997237">
Figure 3: Word length distribution
</figureCaption>
<bodyText confidence="0.9993991">
One previous work uses word length as an in-
dicator of formality (Karlgren and Cutting, 1994).
Here, we borrow this idea and compare the word
length of Style 1 and Style 2. We calculate the
distributions of word length and show the results
in Figure 3. It shows that the majority of word-
s in Style 1 are longer compared with those in
Style 2. To have a quantitative view of the differ-
ence between the word lengths of these two styles,
we heuristically extract words labeled with Style 1
</bodyText>
<figure confidence="0.9982074">
0.25
Style 1
Style 2
0.2
0.15
0.1
0.05
0
0 2 4 6 8 10 12 14 16 18
Probability
</figure>
<page confidence="0.997244">
37
</page>
<bodyText confidence="0.999882571428572">
and Style 2 in our dataset in the final iteration of
Gibbs sampling and apply Mann-Whitney U test
on these two word length populations. The null
hypothesis that the two input populations are the
same is rejected at the 1% significance level. This
verifies the intuition that serious posts tend to use
longer words than unserious posts.
</bodyText>
<subsectionHeader confidence="0.998605">
4.3 Post Identification
</subsectionHeader>
<bodyText confidence="0.999989684210527">
Our model can also be used to separate serious
posts and unserious posts. We treat this as a re-
trieval problem and use precision/recall for evalu-
ation.
We use a simple scoring function, which is the
proportion of words assigned to the unserious style
when we terminate the Gibbs sampling at the 800-
th iteration, to score each post. When applying this
method to our data, emoticons are all removed.
For comparison, we rank post according to the
number of emoticons inside a post as the baseline.
After getting the result of each method, we ask t-
wo annotators to label the first and last 50 posts
in the ranking list. The first 50 posts are used for
evaluation of unserious post retrieval and the last
50 post are used for evaluation of serious post re-
trieval. This evaluation is based on the assumption
that if a method can separate serious and unserious
posts very well, posts ranked at the top position
should be unserious ones and those ranked near
to the bottom should be serious ones. The results
are shown in Table 7 where our method is denot-
ed as TSM and the baseline method is denoted as
EMO. In serious post retrieval, the baseline have
a perfect performance and our method is compet-
itive. We can see that EMO has a perfect perfor-
mance in identifying serious posts. When posts
are ranked in reverse order according to the num-
ber of emoticons they contain, the last 50 ones do
not contain any emoticons. They can be regarded
as a random sample of posts without emoticons.
Compared with identifying serious posts, identi-
fying unserious posts looks much more difficult.
EMO’s poor performance on this task tells us that
emoticon is not a promising sign to detect unse-
rious posts. However, the word style a post uses
matters more, which also proves the value of our
proposed model.
</bodyText>
<subsectionHeader confidence="0.989717">
4.4 User Identification
</subsectionHeader>
<bodyText confidence="0.999357333333333">
In this section, we evaluate the performance of
TSM on identifying serious and unserious users.
This identification task is very important as many
</bodyText>
<table confidence="0.974061923076923">
P@5 P@15 P@25 P@35
Serious EMO 1.0 1.0 1.0 1.0
TSM 1.0 1.0 1.0 0.97
Unserious EMO 0.4 0.67 0.64 0.6
TSM 1.0 0.93 0.96 0.97
Table 7: Precision for Serious and Unserious Post
Retrieval. P@N stands for the precision of the first
N results in ranking list.
P@5 P@15 P@25 P@35
Serious Baseline 0.6 0.8 0.8 0.83
TSM 1.0 1.0 1.0 0.94
Unserious Baseline 1.0 0.87 0.92 0.91
TSM 1.0 1.0 1.0 1.0
</table>
<tableCaption confidence="0.8529635">
Table 8: Precision for serious and unserious user
retrieval.
</tableCaption>
<bodyText confidence="0.999897818181818">
research tasks such as opinion mining and expert
finding are more interested in the serious users.
We treat this task as a retrieval problem as well,
which means we will rank users by a scoring func-
tion and do evaluation on this ranking result.
We rank user according to their style distribu-
tion π,, and pick the first 50 and last 50 users for
evaluation. For each user, 10 posts are sampled
to be shown to the annotators. We mix these 100
users and ask two graduate students to do the an-
notations. The evaluation strategy is the same as
that in Section 4.3. We choose a simple base-
line which ranks users by the number of emoticons
they use per post. The evaluation result is shown
in Table 8 for serious and unserious user retrieval
respectively.
In both serious and unserious user retrieval
tasks, our method gets almost perfect perfor-
mance, which is better than the baseline. This
means the user style distributions learned by our
model can help separate serious and unserious
users.
</bodyText>
<subsectionHeader confidence="0.989047">
4.5 Perplexity
</subsectionHeader>
<bodyText confidence="0.999976583333333">
Perplexity is a widely used criterion in statistical
natural language processing. It measures the pre-
dictive power of a model on unseen data, which
is algebraically equivalent to the inverse of the ge-
ometric mean per-word likelihood. A lower per-
plexity means the test data, which is unseen in the
training phase, can be generated by the model with
a higher probability. So it also indicates that the
model has a better generalization performance.
In this experiment, we leave 10% data for test-
ing and use the remaining 90% data for training.
We choose LDA as a baseline for comparison and
</bodyText>
<page confidence="0.997962">
38
</page>
<bodyText confidence="0.999931818181818">
treat each thread as a document. The perplexity for
both models is calculated over different numbers
of topics, which ranges from 10 to 100. The result
is show in Figure 4. We can clearly see that our
proposed model has a substantially lower perplexi-
ty than LDA over different numbers of topics. This
proves that our model fits the forum discussion da-
ta better and has a stronger generalization power.
It also indicates that separating topic-driven words
and style-driven words can better fit the generation
of user generated content in forum discussions.
</bodyText>
<subsectionHeader confidence="0.992282">
4.6 Topic Distinction
</subsectionHeader>
<bodyText confidence="0.999661666666667">
In traditional topic modeling, like LDA, all words
are regarded as topic-driven words, which are gen-
erated by mixture of topics. However, this may not
be true to user-generated content in online forum-
s as not all words are driven by discussed topics.
Take the following post for example:
</bodyText>
<listItem confidence="0.9912965">
• Okay lah. Let them be. I mean its their KKB
right? Let it rot lor.
</listItem>
<bodyText confidence="0.999941137931034">
In this post, the words “lah” and “lor” are not relat-
ed to the topics under discussion. They appear in
the post because the authors are used to using these
words, which means these words are style driven.
Style-driven words are related to a user’s charac-
teristics and should not be clustered into any top-
ic. Without separating these two types of words,
style-driven words may appear in different topics
and make topics less distinct to each other.
Figure 5 compares the Average Divergence
among discovered topics between TSM (Topic
Style model) and LDA over different numbers of
topics. We can clearly see that the Average Diver-
gence of TSM is substantially larger than that of L-
DA over different numbers of topics. This proves
that in TSM, the learned topics are more distinct
from each other. This is because LDA mixes these
two kinds of words, which introduces noise into
the learned topics and decreases their distinction
between each other. But topic driven words and
style driven words are well separated in TSM. Fig-
ure 5 also plots the Average Divergence between
the learned two styles, which is the curve denot-
ed by DIFF. We can see the AD between differ-
ent styles is even larger than that among topics in
TSM. Different topics may still have some over-
lap in frequently used words but styles may share
few words with each other. So AD of styles can
get higher value. This also proves the effective-
</bodyText>
<table confidence="0.997357333333333">
P@5 P@10 P@20 P@30 P@40 P@50
E 0 0.2 0.25 0.23 0.225 0.2
T 0.8 0.9 0.8 0.8 0.675 0.62
</table>
<tableCaption confidence="0.966879">
Table 9: Slang identification precision. E: Emoti-
con; T:TSM.
</tableCaption>
<table confidence="0.999403">
#Word/Post #Post
Formal User 34.9 158.3
Informal User 14.5 381
</table>
<tableCaption confidence="0.916322">
Table 10: Mean Value of average post length and
</tableCaption>
<bodyText confidence="0.954337666666667">
number of post for different type of users
ness of our model in identifying writing styles and
uncovering more distinct topics.
</bodyText>
<subsectionHeader confidence="0.99732">
4.7 Discovering Slang
</subsectionHeader>
<bodyText confidence="0.99999435">
By looking at Table 5, we notice that the unse-
rious style contains many slang words with high
probability. This indicates that the unserious style
in the dataset we use is also characterized by slang
words. In this section, we will show the useful-
ness of our model in slang discovery. The base-
line method is denoted as Emoticon as it ranks
words according to their probability of occurring
in a post containing emoticons. We ask two Sin-
gaporean annotators to help us identify Singapore-
an slang in the top 50 words. The result is shown
in Table 9. It tells us the unserious style learned
in our model has very good performance in iden-
tifying local slang words. For people preferring
unserious writing style, they would write posts in
a very flexible way and use many informal words,
abbreviations and slang expressions. So our un-
serious style will be characterized by these slang
words and performs very well in identifying these
slang words.
</bodyText>
<subsectionHeader confidence="0.999778">
4.8 Analysis of Users
</subsectionHeader>
<bodyText confidence="0.999973071428571">
In this subsection, we analyze users in our dataset
based on the result learned by TSM. Figure 8
shows the distribution of the histogram of serious
style probability. The majority of users have a
high serious style probability, which means most
users in our dataset are more eager to give serious
comments and express their opinions. This satis-
fies our observation that most people use forums
mainly to discuss and seek knowledge on differ-
ent topics and they are very eager to express their
thoughts in a serious way.
We heuristically split all users into two sets ac-
cording to user-style probability by setting 0.5 as
threshold. Users with probability of serious style
</bodyText>
<page confidence="0.995821">
39
</page>
<figure confidence="0.781322">
Number of topics
</figure>
<figureCaption confidence="0.997517666666667">
Figure 4: Perplexity
Figure 6: Box plot of post number for serious
and unserious users
</figureCaption>
<figure confidence="0.835825">
Number of topics
</figure>
<figureCaption confidence="0.997723">
Figure 5: Average Divergence
</figureCaption>
<figure confidence="0.565349">
Serious User Unserious User
</figure>
<figureCaption confidence="0.9853865">
Figure 7: Box plot of average post length for se-
rious and unserious users
</figureCaption>
<figure confidence="0.999511307692308">
4550
4500
4450
4400
4350
4300
4250
4200
4150
4100
TSM
LDA
4050
4000
10 20 30 40 50 60 70 80 90 1
Number ol Posts
2000
1800
1600
1400
1200
1000
800
600
400
200
0
Serious User Unserious User
3
2.8
2.6
2.4
2.2
2
1.8
1.6
1.4
1.2
1
10 20 30 40 50 60 70 80 90 100
TSM
LDA
DIFF
Perplexity
Log(AD)
Number ol Words Per Post 200
150
100
50
0
200
180
160
140
120
100
80
60
40
20
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Seriouness
Score
Number of User
</figure>
<figureCaption confidence="0.999972">
Figure 8: Seriousness Score of Users
</figureCaption>
<bodyText confidence="0.99999044">
larger than 0.5 are regarded as serious users and
the remaining are unserious users. Next, we ex-
tract the number of posts each user edit and the
average number of words per post for each user
and compare the difference between these two us-
er sets. Figure 6 and Figure 7 show the box plots
of post number and average post length respective-
ly. We can see that serious users edit fewer posts
but use more words in each post. To see the dif-
ference between serious and unserious users more
clearly, we apply Mann-Whitney U test on the post
number populations and average post length pop-
ulations. The Mann-Whitney U test on both data
set reject the null hypothesis that two input popu-
lations are the same at the 1% significance level.
The mean value for post number and average post
length are also computed and shown in Table 10.
We can find that serious users tend to publish few-
er but longer posts than unserious users. This re-
sult is intuitive as serious users often spend more
effort editing their posts to express their opinions
more clearly. However, for unserious users, they
may just use a few words to play a joke or show
some emotions and they can post many posts with-
out spending too much time.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999976370370371">
In this paper, we propose a unified probabilistic
graphical model, called Topic-Style Model, which
models topics and styles at the same time. Tra-
ditional topic modeling methods treat a corpus as
a mixture of topics. But user-generated content
in forum discussions contains not only words re-
lated to topics but also words related to different
writing styles. The proposed Topic-Style Model
can perform well in separating topic-driven word-
s and style-driven words. In this model, we as-
sume that writing style is a consistent writing pat-
tern a user will express in her posts across differ-
ent threads and use a latent variable at user lev-
el to capture the user specific preference of writ-
ing styles. Our model can successfully discover
writing styles which are different from each other
both in word distribution and formality. Words be-
longing to different writing styles and user specific
style distribution are captured by our model at the
same time. An extensive set of experiments shows
that our method has good performances in sepa-
rating serious and unserious posts and users. At
the same time, the model can identify slang words
with promising accuracy, which is proven by our
experiments. An analysis based on the learned
parameters in our model reveal the difference be-
tween serious and unserious users in average post
</bodyText>
<page confidence="0.993661">
40
</page>
<bodyText confidence="0.876494">
length and post number. Proceedings of the 16th International Conference on
</bodyText>
<note confidence="0.580402">
World Wide Web, WWW ’07, pages 171–180, New
York, NY, USA. ACM.
</note>
<sectionHeader confidence="0.97699" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999957957746479">
Eugene Agichtein, Carlos Castillo, Debora Donato,
Aristides Gionis, and Gilad Mishne. 2008. Finding
high-quality content in social media. In Proceedings
of the 2008 International Conference on Web Search
and Data Mining, pages 183–194, New York, NY,
USA. ACM.
Archana Bhattarai, Vasile Rus, and Dipankar Dasgup-
ta. 2009. Characterizing comment spam in the blo-
gosphere through content analysis. In Computation-
al Intelligence in Cyber Security, 2009. CICS’09.
IEEE Symposium on, pages 37–44. IEEE.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.
Julian Brooke, Tong Wang, and Graeme Hirst. 2010.
Automatic acquisition of lexical formality. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 90–98,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 107–116, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Zhuoye Ding, Yeyun Gong, Yaqian Zhou, Qi Zhang,
and Xuanjing Huang. 2013. Detecting spammer-
s in community question answering. In Proceeding
of International Joint Conference on Natural Lan-
guage Processing, pages 118–126, Nagoya, Japan.
Association for Computational Linguistics.
Roberto Gonz´alez-Ib´a˜nez, Smaranda Muresan, and Ni-
na Wacholder. 2011. Identifying sarcasm in twitter:
A closer look. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: Short Pa-
pers - Volume 2, pages 581–586, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nitin Jindal and Bing Liu. 2008. Opinion spam and
analysis. In Proceedings of the 2008 International
Conference on Web Search and Data Mining, WSD-
M ’08, pages 219–230, New York, NY, USA. ACM.
Jussi Karlgren and Douglass Cutting. 1994. Recogniz-
ing text genres with simple metrics using discrim-
inant analysis. In Proceedings of the 15th Con-
ference on Computational Linguistics - Volume 2,
pages 1071–1075, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: Modeling facets and opinions in weblogs. In
Michael J. Paul, ChengXiang Zhai, and Roxana Gir-
ju. 2010. Summarizing contrastive viewpoints in
opinionated text. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’10, pages 66–76, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 186–195, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model
for authors and documents. In Proceedings of the
20th Conference on Uncertainty in Artificial Intelli-
gence, pages 487–494, Arlington, Virginia, United
States. AUAI Press.
</reference>
<page confidence="0.999448">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.778158">
<title confidence="0.999827">A Unified Topic-Style Model for Online Discussions</title>
<author confidence="0.996662">Ying Ding</author>
<author confidence="0.996662">Jing Jiang</author>
<author confidence="0.996662">Qiming</author>
<affiliation confidence="0.935673">School of Information Singapore Management</affiliation>
<email confidence="0.910703">jingjiang,</email>
<abstract confidence="0.999126875">Forums have become major places for online communications for many years, where people often share and express opinions. We observe that, when editing posts, while some people seriously state their opinions, there are also many people playing jokes and writing meaningless posts on the discussed topics. We design a unified probabilistic graphical model to capture both topic-driven words and styledriven words. The model can help us separate serious and unserious posts/users and identify slang words. An extensive set of experiments demonstrates the effectiveness of our model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Carlos Castillo</author>
<author>Debora Donato</author>
<author>Aristides Gionis</author>
<author>Gilad Mishne</author>
</authors>
<title>Finding high-quality content in social media.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 International Conference on Web Search and Data Mining,</booktitle>
<pages>183--194</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7602" citStr="Agichtein et al. (2008)" startWordPosition="1262" endWordPosition="1265"> (2010) , we also use switch variables to alternate between different types of word distributions, but our goal is to identify words associated with writing styles instead of sentiment words or perspective words. Another body of related research is around studying text quality, formality and sarcasm. Pitler and Nenkova (2008) investigated different features for text readability judgement and empirically demonstrated that discourse relations are highly correlated with perceived readability. Brooke et al. (2010) applied Latent Semantic Analysis to determine the formality level of lexical items. Agichtein et al. (2008) presented a general classification framework incorporating community feedback to identify high quality content in social media. Davidov et al. (2010) proposed the first robust algorithm for recognition of sarcasm. Gonz´alezIb´a˜nez et al. (2011) took a closer look at sarcasm in Twitter messages and found that automatic classification can be as good as human classification. All these studies mainly rely on supervised techniques and human annotation needs to be done, which is very time consuming. Our method is fully unsupervised, which can automatically uncover different styles and separate ser</context>
</contexts>
<marker>Agichtein, Castillo, Donato, Gionis, Mishne, 2008</marker>
<rawString>Eugene Agichtein, Carlos Castillo, Debora Donato, Aristides Gionis, and Gilad Mishne. 2008. Finding high-quality content in social media. In Proceedings of the 2008 International Conference on Web Search and Data Mining, pages 183–194, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Archana Bhattarai</author>
<author>Vasile Rus</author>
<author>Dipankar Dasgupta</author>
</authors>
<title>Characterizing comment spam in the blogosphere through content analysis.</title>
<date>2009</date>
<booktitle>In Computational Intelligence in Cyber Security, 2009. CICS’09. IEEE Symposium on,</booktitle>
<pages>37--44</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="8523" citStr="Bhattarai et al. (2009)" startWordPosition="1410" endWordPosition="1413">nd found that automatic classification can be as good as human classification. All these studies mainly rely on supervised techniques and human annotation needs to be done, which is very time consuming. Our method is fully unsupervised, which can automatically uncover different styles and separate serious posts from unserious posts. Our work is also related to spam/spammer detection in social media, which has been studied over different platforms for a few years. Jindal and Liu (2008) first studied opinion spam in online reviews and proposed a classification method for opinion spam detection. Bhattarai et al. (2009) 34 investigated different content attributes of comment spam in the Blogsphere and built a detection system with good performance based on these attributes. Ding et al. (2013) proposed to utilize both content and social features to detect spams in online question answer website. Existing work on spam detection need annotated data to learn the spam features but our model does not as it is fully unsupervised. 3 A Topic-Style Model Writing styles can be reflected in many different ways. Besides choices of words or expressions, many other linguistic features such as sentence length, sentence comp</context>
</contexts>
<marker>Bhattarai, Rus, Dasgupta, 2009</marker>
<rawString>Archana Bhattarai, Vasile Rus, and Dipankar Dasgupta. 2009. Characterizing comment spam in the blogosphere through content analysis. In Computational Intelligence in Cyber Security, 2009. CICS’09. IEEE Symposium on, pages 37–44. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="6010" citStr="Blei et al., 2003" startWordPosition="1007" endWordPosition="1010">pic-style model also automatically learns a meaningful slang lexicon. Moreover, we find that topics discovered by our topic-style model are more distinctive from each other than topics produced by standard LDA. Our contributions in this paper can be summarized as follows: 1) We propose a principled topicstyle model to jointly model topics and writing styles at the same time in online forums. 2) An extensive set of experiments shows that our model is effective in separating the more serious posts and unserious posts and identifying slang words. 2 Related Work Latent Dirichlet Allocation (LDA) (Blei et al., 2003) has been shown to be useful for many applications. Many extensions of LDA have been designed for different tasks, which are not detailed here. Our model is also an extension of LDA. We introduce two types of word distributions, one representing topics and the other representing writing styles. We use switch variables to alternate between these two types of word distributions. We also assume an author-level distribution over writing styles. It is worth pointing out that although our model bears similarity to a number of other LDA extensions, our objectives are different from existing work. E.g</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Brooke</author>
<author>Tong Wang</author>
<author>Graeme Hirst</author>
</authors>
<title>Automatic acquisition of lexical formality.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>90--98</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7494" citStr="Brooke et al. (2010)" startWordPosition="1246" endWordPosition="1249">nt to identify serious versus unserious users. Similar to the models by Mei et al. (2007) and Paul et al. (2010) , we also use switch variables to alternate between different types of word distributions, but our goal is to identify words associated with writing styles instead of sentiment words or perspective words. Another body of related research is around studying text quality, formality and sarcasm. Pitler and Nenkova (2008) investigated different features for text readability judgement and empirically demonstrated that discourse relations are highly correlated with perceived readability. Brooke et al. (2010) applied Latent Semantic Analysis to determine the formality level of lexical items. Agichtein et al. (2008) presented a general classification framework incorporating community feedback to identify high quality content in social media. Davidov et al. (2010) proposed the first robust algorithm for recognition of sarcasm. Gonz´alezIb´a˜nez et al. (2011) took a closer look at sarcasm in Twitter messages and found that automatic classification can be as good as human classification. All these studies mainly rely on supervised techniques and human annotation needs to be done, which is very time co</context>
</contexts>
<marker>Brooke, Wang, Hirst, 2010</marker>
<rawString>Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Automatic acquisition of lexical formality. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 90–98, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Semi-supervised recognition of sarcastic sentences in twitter and amazon.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>107--116</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7752" citStr="Davidov et al. (2010)" startWordPosition="1285" endWordPosition="1288">ting styles instead of sentiment words or perspective words. Another body of related research is around studying text quality, formality and sarcasm. Pitler and Nenkova (2008) investigated different features for text readability judgement and empirically demonstrated that discourse relations are highly correlated with perceived readability. Brooke et al. (2010) applied Latent Semantic Analysis to determine the formality level of lexical items. Agichtein et al. (2008) presented a general classification framework incorporating community feedback to identify high quality content in social media. Davidov et al. (2010) proposed the first robust algorithm for recognition of sarcasm. Gonz´alezIb´a˜nez et al. (2011) took a closer look at sarcasm in Twitter messages and found that automatic classification can be as good as human classification. All these studies mainly rely on supervised techniques and human annotation needs to be done, which is very time consuming. Our method is fully unsupervised, which can automatically uncover different styles and separate serious posts from unserious posts. Our work is also related to spam/spammer detection in social media, which has been studied over different platforms f</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Semi-supervised recognition of sarcastic sentences in twitter and amazon. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 107–116, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuoye Ding</author>
<author>Yeyun Gong</author>
<author>Yaqian Zhou</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
</authors>
<title>Detecting spammers in community question answering.</title>
<date>2013</date>
<booktitle>In Proceeding of International Joint Conference on Natural Language Processing,</booktitle>
<pages>118--126</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Nagoya, Japan.</location>
<contexts>
<context position="8699" citStr="Ding et al. (2013)" startWordPosition="1439" endWordPosition="1442"> very time consuming. Our method is fully unsupervised, which can automatically uncover different styles and separate serious posts from unserious posts. Our work is also related to spam/spammer detection in social media, which has been studied over different platforms for a few years. Jindal and Liu (2008) first studied opinion spam in online reviews and proposed a classification method for opinion spam detection. Bhattarai et al. (2009) 34 investigated different content attributes of comment spam in the Blogsphere and built a detection system with good performance based on these attributes. Ding et al. (2013) proposed to utilize both content and social features to detect spams in online question answer website. Existing work on spam detection need annotated data to learn the spam features but our model does not as it is fully unsupervised. 3 A Topic-Style Model Writing styles can be reflected in many different ways. Besides choices of words or expressions, many other linguistic features such as sentence length, sentence complexity and use of punctuation marks may all be associated with one’s writing style. In this work, however, we try to take an approach that does not rely on heavy linguistic ana</context>
</contexts>
<marker>Ding, Gong, Zhou, Zhang, Huang, 2013</marker>
<rawString>Zhuoye Ding, Yeyun Gong, Yaqian Zhou, Qi Zhang, and Xuanjing Huang. 2013. Detecting spammers in community question answering. In Proceeding of International Joint Conference on Natural Language Processing, pages 118–126, Nagoya, Japan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Gonz´alez-Ib´a˜nez</author>
<author>Smaranda Muresan</author>
<author>Nina Wacholder</author>
</authors>
<title>Identifying sarcasm in twitter: A closer look.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers -</booktitle>
<volume>2</volume>
<pages>581--586</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Gonz´alez-Ib´a˜nez, Muresan, Wacholder, 2011</marker>
<rawString>Roberto Gonz´alez-Ib´a˜nez, Smaranda Muresan, and Nina Wacholder. 2011. Identifying sarcasm in twitter: A closer look. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, pages 581–586, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Opinion spam and analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 International Conference on Web Search and Data Mining, WSDM ’08,</booktitle>
<pages>219--230</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8389" citStr="Jindal and Liu (2008)" startWordPosition="1389" endWordPosition="1392">rst robust algorithm for recognition of sarcasm. Gonz´alezIb´a˜nez et al. (2011) took a closer look at sarcasm in Twitter messages and found that automatic classification can be as good as human classification. All these studies mainly rely on supervised techniques and human annotation needs to be done, which is very time consuming. Our method is fully unsupervised, which can automatically uncover different styles and separate serious posts from unserious posts. Our work is also related to spam/spammer detection in social media, which has been studied over different platforms for a few years. Jindal and Liu (2008) first studied opinion spam in online reviews and proposed a classification method for opinion spam detection. Bhattarai et al. (2009) 34 investigated different content attributes of comment spam in the Blogsphere and built a detection system with good performance based on these attributes. Ding et al. (2013) proposed to utilize both content and social features to detect spams in online question answer website. Existing work on spam detection need annotated data to learn the spam features but our model does not as it is fully unsupervised. 3 A Topic-Style Model Writing styles can be reflected </context>
</contexts>
<marker>Jindal, Liu, 2008</marker>
<rawString>Nitin Jindal and Bing Liu. 2008. Opinion spam and analysis. In Proceedings of the 2008 International Conference on Web Search and Data Mining, WSDM ’08, pages 219–230, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jussi Karlgren</author>
<author>Douglass Cutting</author>
</authors>
<title>Recognizing text genres with simple metrics using discriminant analysis.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th Conference on Computational Linguistics -</booktitle>
<volume>2</volume>
<pages>1071--1075</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="19368" citStr="Karlgren and Cutting, 1994" startWordPosition="3427" endWordPosition="3430"> many slang words with high probability while top words in Style 1 are all very formal. However, styles in the second and third rows of Table 5 are not easily distinguishable from each other. In these results, there often exist two styles very similar to the styles in row 1 while the other styles look like the combination of these two styles and humans cannot tell their meanings very clearly. Based on these observations, we fix the number of styles to 2 in the following experiments. Word Length Figure 3: Word length distribution One previous work uses word length as an indicator of formality (Karlgren and Cutting, 1994). Here, we borrow this idea and compare the word length of Style 1 and Style 2. We calculate the distributions of word length and show the results in Figure 3. It shows that the majority of words in Style 1 are longer compared with those in Style 2. To have a quantitative view of the difference between the word lengths of these two styles, we heuristically extract words labeled with Style 1 0.25 Style 1 Style 2 0.2 0.15 0.1 0.05 0 0 2 4 6 8 10 12 14 16 18 Probability 37 and Style 2 in our dataset in the final iteration of Gibbs sampling and apply Mann-Whitney U test on these two word length po</context>
</contexts>
<marker>Karlgren, Cutting, 1994</marker>
<rawString>Jussi Karlgren and Douglass Cutting. 1994. Recognizing text genres with simple metrics using discriminant analysis. In Proceedings of the 15th Conference on Computational Linguistics - Volume 2, pages 1071–1075, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xu Ling</author>
<author>Matthew Wondra</author>
<author>Hang Su</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Topic sentiment mixture: Modeling facets and opinions in weblogs. In</title>
<date>2007</date>
<contexts>
<context position="6963" citStr="Mei et al. (2007)" startWordPosition="1164" endWordPosition="1167">ween these two types of word distributions. We also assume an author-level distribution over writing styles. It is worth pointing out that although our model bears similarity to a number of other LDA extensions, our objectives are different from existing work. E.g., the author topic model (Rosen-Zvi et al., 2004) also assumes an authorlevel distribution over topics, but the author-level distribution is meant to capture an author’s topical interests. In contrast, our user-level distribution is over writing styles and is meant to identify serious versus unserious users. Similar to the models by Mei et al. (2007) and Paul et al. (2010) , we also use switch variables to alternate between different types of word distributions, but our goal is to identify words associated with writing styles instead of sentiment words or perspective words. Another body of related research is around studying text quality, formality and sarcasm. Pitler and Nenkova (2008) investigated different features for text readability judgement and empirically demonstrated that discourse relations are highly correlated with perceived readability. Brooke et al. (2010) applied Latent Semantic Analysis to determine the formality level of</context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. Topic sentiment mixture: Modeling facets and opinions in weblogs. In</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Paul</author>
<author>ChengXiang Zhai</author>
<author>Roxana Girju</author>
</authors>
<title>Summarizing contrastive viewpoints in opinionated text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>66--76</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6986" citStr="Paul et al. (2010)" startWordPosition="1169" endWordPosition="1172">f word distributions. We also assume an author-level distribution over writing styles. It is worth pointing out that although our model bears similarity to a number of other LDA extensions, our objectives are different from existing work. E.g., the author topic model (Rosen-Zvi et al., 2004) also assumes an authorlevel distribution over topics, but the author-level distribution is meant to capture an author’s topical interests. In contrast, our user-level distribution is over writing styles and is meant to identify serious versus unserious users. Similar to the models by Mei et al. (2007) and Paul et al. (2010) , we also use switch variables to alternate between different types of word distributions, but our goal is to identify words associated with writing styles instead of sentiment words or perspective words. Another body of related research is around studying text quality, formality and sarcasm. Pitler and Nenkova (2008) investigated different features for text readability judgement and empirically demonstrated that discourse relations are highly correlated with perceived readability. Brooke et al. (2010) applied Latent Semantic Analysis to determine the formality level of lexical items. Agichte</context>
</contexts>
<marker>Paul, Zhai, Girju, 2010</marker>
<rawString>Michael J. Paul, ChengXiang Zhai, and Roxana Girju. 2010. Summarizing contrastive viewpoints in opinionated text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 66–76, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Revisiting readability: A unified framework for predicting text quality.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>186--195</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7306" citStr="Pitler and Nenkova (2008)" startWordPosition="1220" endWordPosition="1223">orlevel distribution over topics, but the author-level distribution is meant to capture an author’s topical interests. In contrast, our user-level distribution is over writing styles and is meant to identify serious versus unserious users. Similar to the models by Mei et al. (2007) and Paul et al. (2010) , we also use switch variables to alternate between different types of word distributions, but our goal is to identify words associated with writing styles instead of sentiment words or perspective words. Another body of related research is around studying text quality, formality and sarcasm. Pitler and Nenkova (2008) investigated different features for text readability judgement and empirically demonstrated that discourse relations are highly correlated with perceived readability. Brooke et al. (2010) applied Latent Semantic Analysis to determine the formality level of lexical items. Agichtein et al. (2008) presented a general classification framework incorporating community feedback to identify high quality content in social media. Davidov et al. (2010) proposed the first robust algorithm for recognition of sarcasm. Gonz´alezIb´a˜nez et al. (2011) took a closer look at sarcasm in Twitter messages and fou</context>
</contexts>
<marker>Pitler, Nenkova, 2008</marker>
<rawString>Emily Pitler and Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 186–195, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michal Rosen-Zvi</author>
<author>Thomas Griffiths</author>
<author>Mark Steyvers</author>
<author>Padhraic Smyth</author>
</authors>
<title>The author-topic model for authors and documents.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>487--494</pages>
<publisher>AUAI Press.</publisher>
<location>Arlington, Virginia, United States.</location>
<contexts>
<context position="6660" citStr="Rosen-Zvi et al., 2004" startWordPosition="1117" endWordPosition="1120">l for many applications. Many extensions of LDA have been designed for different tasks, which are not detailed here. Our model is also an extension of LDA. We introduce two types of word distributions, one representing topics and the other representing writing styles. We use switch variables to alternate between these two types of word distributions. We also assume an author-level distribution over writing styles. It is worth pointing out that although our model bears similarity to a number of other LDA extensions, our objectives are different from existing work. E.g., the author topic model (Rosen-Zvi et al., 2004) also assumes an authorlevel distribution over topics, but the author-level distribution is meant to capture an author’s topical interests. In contrast, our user-level distribution is over writing styles and is meant to identify serious versus unserious users. Similar to the models by Mei et al. (2007) and Paul et al. (2010) , we also use switch variables to alternate between different types of word distributions, but our goal is to identify words associated with writing styles instead of sentiment words or perspective words. Another body of related research is around studying text quality, fo</context>
</contexts>
<marker>Rosen-Zvi, Griffiths, Steyvers, Smyth, 2004</marker>
<rawString>Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and Padhraic Smyth. 2004. The author-topic model for authors and documents. In Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence, pages 487–494, Arlington, Virginia, United States. AUAI Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>